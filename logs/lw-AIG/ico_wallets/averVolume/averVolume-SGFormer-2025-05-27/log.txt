Namespace(seed=15, model='SGFormer', dataset='ico_wallets/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 306], edge_attr=[306, 2], x=[90, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d40cb8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 1.29s
Val loss: 0.7018 score: 0.4490 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.7317;  Loss pred: 0.7317; Loss self: 0.0000; time: 0.16s
Val loss: 0.7003 score: 0.4490 time: 0.11s
Test loss: 0.6932 score: 0.4898 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7034;  Loss pred: 0.7034; Loss self: 0.0000; time: 0.16s
Val loss: 0.6975 score: 0.4490 time: 0.09s
Test loss: 0.6914 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.17s
Val loss: 0.6930 score: 0.4898 time: 0.10s
Test loss: 0.6887 score: 0.5306 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.16s
Val loss: 0.6875 score: 0.5102 time: 0.10s
Test loss: 0.6853 score: 0.6735 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.16s
Val loss: 0.6812 score: 0.6939 time: 0.10s
Test loss: 0.6819 score: 0.6531 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.17s
Val loss: 0.6747 score: 0.6122 time: 0.12s
Test loss: 0.6785 score: 0.6122 time: 0.10s
Epoch 8/1000, LR 0.000180
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.16s
Val loss: 0.6681 score: 0.5714 time: 0.09s
Test loss: 0.6750 score: 0.5510 time: 0.12s
Epoch 9/1000, LR 0.000210
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.23s
Val loss: 0.6615 score: 0.5510 time: 0.10s
Test loss: 0.6714 score: 0.5306 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.16s
Val loss: 0.6551 score: 0.5306 time: 0.09s
Test loss: 0.6682 score: 0.5306 time: 0.10s
Epoch 11/1000, LR 0.000270
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.17s
Val loss: 0.6492 score: 0.5510 time: 0.09s
Test loss: 0.6646 score: 0.5306 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.16s
Val loss: 0.6428 score: 0.5510 time: 0.09s
Test loss: 0.6604 score: 0.5306 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.14s
Val loss: 0.6358 score: 0.5510 time: 0.10s
Test loss: 0.6555 score: 0.5306 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.15s
Val loss: 0.6279 score: 0.5510 time: 0.10s
Test loss: 0.6496 score: 0.5714 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.17s
Val loss: 0.6192 score: 0.6531 time: 0.13s
Test loss: 0.6428 score: 0.6122 time: 0.11s
Epoch 16/1000, LR 0.000270
Train loss: 0.4973;  Loss pred: 0.4973; Loss self: 0.0000; time: 0.18s
Val loss: 0.6102 score: 0.7959 time: 0.10s
Test loss: 0.6356 score: 0.6327 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.17s
Val loss: 0.6006 score: 0.8980 time: 0.15s
Test loss: 0.6277 score: 0.6939 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4645;  Loss pred: 0.4645; Loss self: 0.0000; time: 0.16s
Val loss: 0.5910 score: 0.8980 time: 0.08s
Test loss: 0.6195 score: 0.7755 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4541;  Loss pred: 0.4541; Loss self: 0.0000; time: 0.16s
Val loss: 0.5819 score: 0.9592 time: 0.10s
Test loss: 0.6117 score: 0.8776 time: 0.10s
Epoch 20/1000, LR 0.000270
Train loss: 0.4436;  Loss pred: 0.4436; Loss self: 0.0000; time: 0.18s
Val loss: 0.5735 score: 0.9796 time: 0.09s
Test loss: 0.6045 score: 0.8776 time: 0.10s
Epoch 21/1000, LR 0.000270
Train loss: 0.4206;  Loss pred: 0.4206; Loss self: 0.0000; time: 0.16s
Val loss: 0.5653 score: 0.9592 time: 0.10s
Test loss: 0.5976 score: 0.8571 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.3990;  Loss pred: 0.3990; Loss self: 0.0000; time: 0.17s
Val loss: 0.5571 score: 0.9796 time: 0.10s
Test loss: 0.5911 score: 0.8367 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3996;  Loss pred: 0.3996; Loss self: 0.0000; time: 0.16s
Val loss: 0.5495 score: 0.9796 time: 0.09s
Test loss: 0.5851 score: 0.8163 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4085;  Loss pred: 0.4085; Loss self: 0.0000; time: 0.16s
Val loss: 0.5426 score: 0.9592 time: 0.19s
Test loss: 0.5796 score: 0.7959 time: 0.10s
Epoch 25/1000, LR 0.000270
Train loss: 0.3701;  Loss pred: 0.3701; Loss self: 0.0000; time: 0.17s
Val loss: 0.5361 score: 0.9388 time: 0.10s
Test loss: 0.5743 score: 0.7551 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3688;  Loss pred: 0.3688; Loss self: 0.0000; time: 0.17s
Val loss: 0.5291 score: 0.9388 time: 0.09s
Test loss: 0.5689 score: 0.7551 time: 0.10s
Epoch 27/1000, LR 0.000270
Train loss: 0.3676;  Loss pred: 0.3676; Loss self: 0.0000; time: 0.18s
Val loss: 0.5223 score: 0.9388 time: 0.10s
Test loss: 0.5636 score: 0.7551 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3354;  Loss pred: 0.3354; Loss self: 0.0000; time: 0.17s
Val loss: 0.5153 score: 0.9388 time: 0.10s
Test loss: 0.5583 score: 0.7551 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3437;  Loss pred: 0.3437; Loss self: 0.0000; time: 0.17s
Val loss: 0.5076 score: 0.9388 time: 0.10s
Test loss: 0.5525 score: 0.7551 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3432;  Loss pred: 0.3432; Loss self: 0.0000; time: 0.35s
Val loss: 0.4988 score: 0.9388 time: 0.13s
Test loss: 0.5462 score: 0.7551 time: 0.20s
Epoch 31/1000, LR 0.000270
Train loss: 0.3215;  Loss pred: 0.3215; Loss self: 0.0000; time: 0.17s
Val loss: 0.4901 score: 0.9388 time: 0.10s
Test loss: 0.5401 score: 0.7551 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.2993;  Loss pred: 0.2993; Loss self: 0.0000; time: 0.17s
Val loss: 0.4811 score: 0.9388 time: 0.09s
Test loss: 0.5340 score: 0.7551 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.17s
Val loss: 0.4715 score: 0.9388 time: 0.10s
Test loss: 0.5276 score: 0.7959 time: 0.20s
Epoch 34/1000, LR 0.000270
Train loss: 0.2838;  Loss pred: 0.2838; Loss self: 0.0000; time: 0.19s
Val loss: 0.4621 score: 0.9388 time: 0.11s
Test loss: 0.5213 score: 0.8163 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.16s
Val loss: 0.4529 score: 0.9388 time: 0.10s
Test loss: 0.5151 score: 0.7959 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.2786;  Loss pred: 0.2786; Loss self: 0.0000; time: 0.19s
Val loss: 0.4438 score: 0.9592 time: 0.13s
Test loss: 0.5092 score: 0.7959 time: 0.10s
Epoch 37/1000, LR 0.000270
Train loss: 0.2666;  Loss pred: 0.2666; Loss self: 0.0000; time: 0.32s
Val loss: 0.4349 score: 0.9592 time: 0.10s
Test loss: 0.5037 score: 0.8163 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2611;  Loss pred: 0.2611; Loss self: 0.0000; time: 0.17s
Val loss: 0.4268 score: 0.9592 time: 0.10s
Test loss: 0.4991 score: 0.8367 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.16s
Val loss: 0.4192 score: 0.9796 time: 0.10s
Test loss: 0.4947 score: 0.8367 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.16s
Val loss: 0.4126 score: 0.9796 time: 0.08s
Test loss: 0.4912 score: 0.8367 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.2301;  Loss pred: 0.2301; Loss self: 0.0000; time: 0.14s
Val loss: 0.4063 score: 0.9796 time: 0.09s
Test loss: 0.4879 score: 0.8163 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2205;  Loss pred: 0.2205; Loss self: 0.0000; time: 0.15s
Val loss: 0.4003 score: 0.9796 time: 0.10s
Test loss: 0.4846 score: 0.8163 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.17s
Val loss: 0.3948 score: 0.9796 time: 0.10s
Test loss: 0.4809 score: 0.8163 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2364;  Loss pred: 0.2364; Loss self: 0.0000; time: 0.15s
Val loss: 0.3898 score: 0.9796 time: 0.09s
Test loss: 0.4772 score: 0.8163 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2059;  Loss pred: 0.2059; Loss self: 0.0000; time: 0.16s
Val loss: 0.3851 score: 0.9796 time: 0.09s
Test loss: 0.4734 score: 0.8163 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.16s
Val loss: 0.3820 score: 0.9592 time: 0.09s
Test loss: 0.4704 score: 0.8163 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.1932;  Loss pred: 0.1932; Loss self: 0.0000; time: 0.16s
Val loss: 0.3781 score: 0.9592 time: 0.09s
Test loss: 0.4672 score: 0.8163 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.16s
Val loss: 0.3739 score: 0.9592 time: 0.09s
Test loss: 0.4638 score: 0.8163 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.1840;  Loss pred: 0.1840; Loss self: 0.0000; time: 0.16s
Val loss: 0.3687 score: 0.9592 time: 0.11s
Test loss: 0.4601 score: 0.8163 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.14s
Val loss: 0.3620 score: 0.9796 time: 0.08s
Test loss: 0.4561 score: 0.8163 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.14s
Val loss: 0.3550 score: 0.9796 time: 0.09s
Test loss: 0.4522 score: 0.8163 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1848;  Loss pred: 0.1848; Loss self: 0.0000; time: 0.14s
Val loss: 0.3478 score: 0.9796 time: 0.08s
Test loss: 0.4486 score: 0.8163 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 0.14s
Val loss: 0.3418 score: 0.9592 time: 0.09s
Test loss: 0.4471 score: 0.8367 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.14s
Val loss: 0.3370 score: 0.9592 time: 0.18s
Test loss: 0.4457 score: 0.8571 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 0.15s
Val loss: 0.3320 score: 0.9592 time: 0.09s
Test loss: 0.4425 score: 0.8571 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.15s
Val loss: 0.3267 score: 0.9592 time: 0.09s
Test loss: 0.4375 score: 0.8571 time: 0.10s
Epoch 57/1000, LR 0.000269
Train loss: 0.1701;  Loss pred: 0.1701; Loss self: 0.0000; time: 0.16s
Val loss: 0.3213 score: 0.9592 time: 0.10s
Test loss: 0.4319 score: 0.8367 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1453;  Loss pred: 0.1453; Loss self: 0.0000; time: 0.15s
Val loss: 0.3161 score: 0.9592 time: 0.09s
Test loss: 0.4259 score: 0.8163 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.15s
Val loss: 0.3126 score: 0.9796 time: 0.09s
Test loss: 0.4216 score: 0.8163 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1481;  Loss pred: 0.1481; Loss self: 0.0000; time: 0.15s
Val loss: 0.3084 score: 0.9796 time: 0.08s
Test loss: 0.4180 score: 0.8163 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.15s
Val loss: 0.3024 score: 0.9796 time: 0.08s
Test loss: 0.4140 score: 0.8163 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 0.15s
Val loss: 0.2949 score: 0.9796 time: 0.10s
Test loss: 0.4100 score: 0.8163 time: 0.10s
Epoch 63/1000, LR 0.000268
Train loss: 0.1317;  Loss pred: 0.1317; Loss self: 0.0000; time: 0.17s
Val loss: 0.2882 score: 0.9592 time: 0.10s
Test loss: 0.4069 score: 0.8163 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 0.15s
Val loss: 0.2822 score: 0.9592 time: 0.09s
Test loss: 0.4045 score: 0.8367 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1288;  Loss pred: 0.1288; Loss self: 0.0000; time: 0.16s
Val loss: 0.2771 score: 0.9592 time: 0.09s
Test loss: 0.4023 score: 0.8367 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1173;  Loss pred: 0.1173; Loss self: 0.0000; time: 0.15s
Val loss: 0.2724 score: 0.9592 time: 0.10s
Test loss: 0.3988 score: 0.8163 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 0.15s
Val loss: 0.2696 score: 0.9796 time: 0.09s
Test loss: 0.3950 score: 0.8163 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1180;  Loss pred: 0.1180; Loss self: 0.0000; time: 0.16s
Val loss: 0.2676 score: 0.9796 time: 0.09s
Test loss: 0.3929 score: 0.8163 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.16s
Val loss: 0.2639 score: 0.9796 time: 0.10s
Test loss: 0.3903 score: 0.8163 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.16s
Val loss: 0.2592 score: 0.9796 time: 0.09s
Test loss: 0.3872 score: 0.8163 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 0.16s
Val loss: 0.2535 score: 0.9796 time: 0.10s
Test loss: 0.3837 score: 0.8163 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.16s
Val loss: 0.2475 score: 0.9796 time: 0.11s
Test loss: 0.3802 score: 0.8163 time: 0.10s
Epoch 73/1000, LR 0.000267
Train loss: 0.1062;  Loss pred: 0.1062; Loss self: 0.0000; time: 0.16s
Val loss: 0.2395 score: 0.9592 time: 0.09s
Test loss: 0.3761 score: 0.8163 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1018;  Loss pred: 0.1018; Loss self: 0.0000; time: 0.15s
Val loss: 0.2318 score: 0.9592 time: 0.10s
Test loss: 0.3722 score: 0.8163 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.17s
Val loss: 0.2262 score: 0.9592 time: 0.13s
Test loss: 0.3693 score: 0.8367 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.15s
Val loss: 0.2218 score: 0.9592 time: 1.66s
Test loss: 0.3660 score: 0.8367 time: 3.12s
Epoch 77/1000, LR 0.000267
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 1.55s
Val loss: 0.2183 score: 0.9592 time: 1.01s
Test loss: 0.3634 score: 0.8163 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.15s
Val loss: 0.2185 score: 0.9796 time: 0.09s
Test loss: 0.3635 score: 0.8163 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.16s
Val loss: 0.2173 score: 0.9796 time: 0.09s
Test loss: 0.3626 score: 0.8163 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0883;  Loss pred: 0.0883; Loss self: 0.0000; time: 0.17s
Val loss: 0.2079 score: 0.9796 time: 0.13s
Test loss: 0.3572 score: 0.8163 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.15s
Val loss: 0.1997 score: 0.9592 time: 0.09s
Test loss: 0.3527 score: 0.8367 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.15s
Val loss: 0.1926 score: 0.9592 time: 0.08s
Test loss: 0.3498 score: 0.8367 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0842;  Loss pred: 0.0842; Loss self: 0.0000; time: 0.15s
Val loss: 0.1871 score: 0.9592 time: 0.08s
Test loss: 0.3495 score: 0.8367 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.16s
Val loss: 0.1830 score: 0.9592 time: 0.09s
Test loss: 0.3489 score: 0.8367 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.15s
Val loss: 0.1768 score: 0.9592 time: 0.08s
Test loss: 0.3449 score: 0.8571 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.14s
Val loss: 0.1709 score: 0.9592 time: 0.08s
Test loss: 0.3409 score: 0.8776 time: 0.20s
Epoch 87/1000, LR 0.000266
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.14s
Val loss: 0.1672 score: 0.9592 time: 0.09s
Test loss: 0.3378 score: 0.8776 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.33s
Val loss: 0.1643 score: 0.9592 time: 0.09s
Test loss: 0.3395 score: 0.8367 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.14s
Val loss: 0.1602 score: 0.9592 time: 0.09s
Test loss: 0.3400 score: 0.8367 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0700;  Loss pred: 0.0700; Loss self: 0.0000; time: 0.16s
Val loss: 0.1523 score: 0.9592 time: 0.08s
Test loss: 0.3332 score: 0.8776 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0664;  Loss pred: 0.0664; Loss self: 0.0000; time: 0.16s
Val loss: 0.1489 score: 0.9796 time: 0.09s
Test loss: 0.3228 score: 0.8776 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.15s
Val loss: 0.1427 score: 0.9592 time: 0.08s
Test loss: 0.3222 score: 0.8776 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 0.15s
Val loss: 0.1386 score: 0.9592 time: 0.09s
Test loss: 0.3220 score: 0.8776 time: 0.19s
Epoch 94/1000, LR 0.000265
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.16s
Val loss: 0.1380 score: 0.9796 time: 0.09s
Test loss: 0.3157 score: 0.8776 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.16s
Val loss: 0.1334 score: 0.9796 time: 0.09s
Test loss: 0.3150 score: 0.8776 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0691;  Loss pred: 0.0691; Loss self: 0.0000; time: 0.15s
Val loss: 0.1282 score: 0.9592 time: 0.08s
Test loss: 0.3181 score: 0.8776 time: 0.19s
Epoch 97/1000, LR 0.000265
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.18s
Val loss: 0.1265 score: 0.9592 time: 0.08s
Test loss: 0.3100 score: 0.8776 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.16s
Val loss: 0.1246 score: 0.9796 time: 0.09s
Test loss: 0.3051 score: 0.8776 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.15s
Val loss: 0.1201 score: 0.9592 time: 0.09s
Test loss: 0.3065 score: 0.8776 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.15s
Val loss: 0.1151 score: 0.9592 time: 0.20s
Test loss: 0.3054 score: 0.8776 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.16s
Val loss: 0.1126 score: 0.9592 time: 0.09s
Test loss: 0.3021 score: 0.8776 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.16s
Val loss: 0.1103 score: 0.9592 time: 0.10s
Test loss: 0.3036 score: 0.8776 time: 0.09s
Epoch 103/1000, LR 0.000264
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.16s
Val loss: 0.1087 score: 0.9592 time: 0.09s
Test loss: 0.3055 score: 0.8776 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.17s
Val loss: 0.1071 score: 0.9592 time: 0.09s
Test loss: 0.3043 score: 0.8776 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.15s
Val loss: 0.1051 score: 0.9592 time: 0.09s
Test loss: 0.3030 score: 0.8776 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.16s
Val loss: 0.1020 score: 0.9592 time: 0.09s
Test loss: 0.3030 score: 0.8776 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.15s
Val loss: 0.1002 score: 0.9592 time: 0.11s
Test loss: 0.3020 score: 0.8776 time: 0.17s
Epoch 108/1000, LR 0.000264
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.15s
Val loss: 0.0987 score: 0.9592 time: 0.09s
Test loss: 0.3009 score: 0.8776 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.16s
Val loss: 0.1008 score: 0.9796 time: 0.08s
Test loss: 0.2934 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.15s
Val loss: 0.0961 score: 0.9592 time: 0.09s
Test loss: 0.2979 score: 0.8980 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 0.15s
Val loss: 0.0949 score: 0.9592 time: 0.09s
Test loss: 0.2992 score: 0.8980 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.15s
Val loss: 0.0964 score: 0.9796 time: 0.09s
Test loss: 0.2958 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.16s
Val loss: 0.0956 score: 0.9592 time: 0.10s
Test loss: 0.2970 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.16s
Val loss: 0.0937 score: 0.9592 time: 0.09s
Test loss: 0.2992 score: 0.8980 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.29s
Val loss: 0.0922 score: 0.9592 time: 0.09s
Test loss: 0.3012 score: 0.8980 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.15s
Val loss: 0.0913 score: 0.9592 time: 0.09s
Test loss: 0.3011 score: 0.8980 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.16s
Val loss: 0.0928 score: 0.9796 time: 0.10s
Test loss: 0.2987 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.16s
Val loss: 0.0924 score: 0.9796 time: 0.09s
Test loss: 0.2995 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.16s
Val loss: 0.0925 score: 0.9796 time: 0.09s
Test loss: 0.2980 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.16s
Val loss: 0.0911 score: 0.9796 time: 0.09s
Test loss: 0.2980 score: 0.8980 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.15s
Val loss: 0.0887 score: 0.9592 time: 0.08s
Test loss: 0.2993 score: 0.8980 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.27s
Val loss: 0.0859 score: 0.9592 time: 0.10s
Test loss: 0.3026 score: 0.8980 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.15s
Val loss: 0.0871 score: 0.9592 time: 0.09s
Test loss: 0.3011 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.15s
Val loss: 0.0882 score: 0.9592 time: 0.09s
Test loss: 0.2997 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.15s
Val loss: 0.0886 score: 0.9796 time: 0.10s
Test loss: 0.2989 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.15s
Val loss: 0.0889 score: 0.9796 time: 0.09s
Test loss: 0.2993 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.16s
Val loss: 0.0861 score: 0.9592 time: 0.09s
Test loss: 0.3027 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.15s
Val loss: 0.0873 score: 0.9592 time: 0.08s
Test loss: 0.3031 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.15s
Val loss: 0.0853 score: 0.9592 time: 0.19s
Test loss: 0.3047 score: 0.8980 time: 0.08s
Epoch 130/1000, LR 0.000260
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.14s
Val loss: 0.0811 score: 0.9592 time: 0.09s
Test loss: 0.3058 score: 0.8980 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.15s
Val loss: 0.0839 score: 0.9592 time: 0.09s
Test loss: 0.3019 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.15s
Val loss: 0.0854 score: 0.9796 time: 0.09s
Test loss: 0.3011 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.15s
Val loss: 0.0830 score: 0.9592 time: 0.09s
Test loss: 0.3019 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.15s
Val loss: 0.0821 score: 0.9592 time: 0.09s
Test loss: 0.3028 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.16s
Val loss: 0.0800 score: 0.9592 time: 0.08s
Test loss: 0.3041 score: 0.8980 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.16s
Val loss: 0.0814 score: 0.9592 time: 0.09s
Test loss: 0.3040 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.25s
Val loss: 0.0836 score: 0.9592 time: 0.08s
Test loss: 0.3024 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.15s
Val loss: 0.0825 score: 0.9592 time: 0.09s
Test loss: 0.3029 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.17s
Val loss: 0.0807 score: 0.9592 time: 0.09s
Test loss: 0.3037 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.16s
Val loss: 0.0794 score: 0.9592 time: 0.09s
Test loss: 0.3039 score: 0.8980 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.16s
Val loss: 0.0769 score: 0.9592 time: 0.10s
Test loss: 0.3048 score: 0.8776 time: 0.09s
Epoch 142/1000, LR 0.000259
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.16s
Val loss: 0.0749 score: 0.9592 time: 0.09s
Test loss: 0.3049 score: 0.8776 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.14s
Val loss: 0.0767 score: 0.9592 time: 0.08s
Test loss: 0.3043 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.18s
Val loss: 0.0762 score: 0.9592 time: 0.09s
Test loss: 0.3059 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.14s
Val loss: 0.0734 score: 0.9592 time: 0.09s
Test loss: 0.3080 score: 0.8776 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.16s
Val loss: 0.0750 score: 0.9592 time: 0.09s
Test loss: 0.3085 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.16s
Val loss: 0.0763 score: 0.9592 time: 0.09s
Test loss: 0.3085 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.16s
Val loss: 0.0751 score: 0.9592 time: 0.08s
Test loss: 0.3092 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.15s
Val loss: 0.0733 score: 0.9592 time: 0.08s
Test loss: 0.3085 score: 0.8776 time: 0.08s
Epoch 150/1000, LR 0.000257
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.16s
Val loss: 0.0732 score: 0.9592 time: 0.09s
Test loss: 0.3097 score: 0.8776 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.14s
Val loss: 0.0721 score: 0.9592 time: 0.08s
Test loss: 0.3108 score: 0.8776 time: 0.08s
Epoch 152/1000, LR 0.000257
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.17s
Val loss: 0.0764 score: 0.9592 time: 0.09s
Test loss: 0.3097 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.15s
Val loss: 0.0786 score: 0.9592 time: 0.10s
Test loss: 0.3097 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.15s
Val loss: 0.0796 score: 0.9592 time: 0.08s
Test loss: 0.3099 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.16s
Val loss: 0.0772 score: 0.9592 time: 0.09s
Test loss: 0.3109 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.16s
Val loss: 0.0736 score: 0.9592 time: 0.09s
Test loss: 0.3125 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.16s
Val loss: 0.0708 score: 0.9592 time: 0.10s
Test loss: 0.3135 score: 0.8776 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.17s
Val loss: 0.0725 score: 0.9592 time: 0.09s
Test loss: 0.3135 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.15s
Val loss: 0.0719 score: 0.9592 time: 0.09s
Test loss: 0.3144 score: 0.8776 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.14s
Val loss: 0.0710 score: 0.9592 time: 0.09s
Test loss: 0.3149 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.16s
Val loss: 0.0706 score: 0.9592 time: 0.09s
Test loss: 0.3146 score: 0.8776 time: 0.08s
Epoch 162/1000, LR 0.000255
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.16s
Val loss: 0.0697 score: 0.9592 time: 0.10s
Test loss: 0.3144 score: 0.8776 time: 0.09s
Epoch 163/1000, LR 0.000255
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.15s
Val loss: 0.0691 score: 0.9592 time: 0.08s
Test loss: 0.3149 score: 0.8776 time: 0.08s
Epoch 164/1000, LR 0.000254
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.16s
Val loss: 0.0671 score: 0.9592 time: 0.09s
Test loss: 0.3154 score: 0.8776 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.15s
Val loss: 0.0693 score: 0.9592 time: 0.08s
Test loss: 0.3150 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.15s
Val loss: 0.0727 score: 0.9592 time: 0.08s
Test loss: 0.3141 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.21s
Val loss: 0.0743 score: 0.9592 time: 0.09s
Test loss: 0.3135 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.15s
Val loss: 0.0718 score: 0.9592 time: 0.09s
Test loss: 0.3142 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.16s
Val loss: 0.0676 score: 0.9592 time: 0.10s
Test loss: 0.3150 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.16s
Val loss: 0.0668 score: 0.9592 time: 0.11s
Test loss: 0.3150 score: 0.8980 time: 0.11s
Epoch 171/1000, LR 0.000253
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.16s
Val loss: 0.0649 score: 0.9592 time: 0.09s
Test loss: 0.3143 score: 0.8980 time: 0.09s
Epoch 172/1000, LR 0.000253
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.17s
Val loss: 0.0661 score: 0.9592 time: 0.08s
Test loss: 0.3147 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.15s
Val loss: 0.0668 score: 0.9592 time: 0.08s
Test loss: 0.3149 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.29s
Val loss: 0.0656 score: 0.9592 time: 0.09s
Test loss: 0.3156 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.15s
Val loss: 0.0630 score: 0.9592 time: 0.09s
Test loss: 0.3163 score: 0.8776 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.15s
Val loss: 0.0627 score: 0.9592 time: 0.09s
Test loss: 0.3159 score: 0.8980 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.17s
Val loss: 0.0643 score: 0.9592 time: 0.09s
Test loss: 0.3165 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.16s
Val loss: 0.0639 score: 0.9592 time: 0.10s
Test loss: 0.3157 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.16s
Val loss: 0.0648 score: 0.9592 time: 0.09s
Test loss: 0.3161 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.16s
Val loss: 0.0656 score: 0.9592 time: 0.09s
Test loss: 0.3168 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.16s
Val loss: 0.0664 score: 0.9592 time: 0.16s
Test loss: 0.3174 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.16s
Val loss: 0.0676 score: 0.9592 time: 0.09s
Test loss: 0.3173 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 183/1000, LR 0.000250
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.16s
Val loss: 0.0671 score: 0.9592 time: 0.08s
Test loss: 0.3173 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 184/1000, LR 0.000250
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.16s
Val loss: 0.0659 score: 0.9592 time: 0.09s
Test loss: 0.3177 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.23s
Val loss: 0.0646 score: 0.9592 time: 0.09s
Test loss: 0.3178 score: 0.8980 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.15s
Val loss: 0.0633 score: 0.9592 time: 0.10s
Test loss: 0.3180 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.23s
Val loss: 0.0630 score: 0.9592 time: 0.09s
Test loss: 0.3181 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 188/1000, LR 0.000249
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.28s
Val loss: 0.0610 score: 0.9592 time: 0.10s
Test loss: 0.3187 score: 0.8980 time: 0.09s
Epoch 189/1000, LR 0.000249
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.16s
Val loss: 0.0606 score: 0.9592 time: 0.09s
Test loss: 0.3192 score: 0.8980 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.16s
Val loss: 0.0615 score: 0.9592 time: 0.09s
Test loss: 0.3192 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.15s
Val loss: 0.0624 score: 0.9592 time: 0.10s
Test loss: 0.3191 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.15s
Val loss: 0.0626 score: 0.9592 time: 0.09s
Test loss: 0.3193 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.15s
Val loss: 0.0620 score: 0.9592 time: 0.09s
Test loss: 0.3198 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.15s
Val loss: 0.0594 score: 0.9592 time: 0.09s
Test loss: 0.3202 score: 0.8776 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.15s
Val loss: 0.0616 score: 0.9592 time: 0.09s
Test loss: 0.3204 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.16s
Val loss: 0.0615 score: 0.9592 time: 0.09s
Test loss: 0.3206 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.15s
Val loss: 0.0606 score: 0.9592 time: 0.08s
Test loss: 0.3200 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.14s
Val loss: 0.0619 score: 0.9592 time: 0.09s
Test loss: 0.3197 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.16s
Val loss: 0.0643 score: 0.9592 time: 0.09s
Test loss: 0.3194 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.16s
Val loss: 0.0636 score: 0.9592 time: 0.10s
Test loss: 0.3197 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.15s
Val loss: 0.0630 score: 0.9592 time: 0.09s
Test loss: 0.3199 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 202/1000, LR 0.000246
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.15s
Val loss: 0.0623 score: 0.9592 time: 0.08s
Test loss: 0.3206 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.14s
Val loss: 0.0618 score: 0.9592 time: 0.08s
Test loss: 0.3214 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 204/1000, LR 0.000245
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.14s
Val loss: 0.0615 score: 0.9592 time: 0.08s
Test loss: 0.3224 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 205/1000, LR 0.000245
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.14s
Val loss: 0.0604 score: 0.9592 time: 0.08s
Test loss: 0.3232 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 206/1000, LR 0.000245
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.17s
Val loss: 0.0582 score: 0.9592 time: 0.09s
Test loss: 0.3238 score: 0.8980 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.16s
Val loss: 0.0558 score: 0.9592 time: 0.08s
Test loss: 0.3238 score: 0.8980 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.15s
Val loss: 0.0558 score: 0.9592 time: 0.09s
Test loss: 0.3247 score: 0.8980 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 209/1000, LR 0.000244
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.15s
Val loss: 0.0563 score: 0.9592 time: 0.08s
Test loss: 0.3248 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 210/1000, LR 0.000244
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.15s
Val loss: 0.0570 score: 0.9592 time: 0.09s
Test loss: 0.3246 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 211/1000, LR 0.000244
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.15s
Val loss: 0.0571 score: 0.9592 time: 0.08s
Test loss: 0.3248 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 212/1000, LR 0.000243
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.16s
Val loss: 0.0564 score: 0.9592 time: 0.11s
Test loss: 0.3251 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 213/1000, LR 0.000243
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.17s
Val loss: 0.0550 score: 0.9592 time: 0.08s
Test loss: 0.3249 score: 0.8980 time: 0.08s
Epoch 214/1000, LR 0.000243
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.15s
Val loss: 0.0558 score: 0.9592 time: 0.08s
Test loss: 0.3256 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 215/1000, LR 0.000243
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.16s
Val loss: 0.0572 score: 0.9592 time: 0.09s
Test loss: 0.3267 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 216/1000, LR 0.000242
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.16s
Val loss: 0.0579 score: 0.9592 time: 0.08s
Test loss: 0.3274 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 217/1000, LR 0.000242
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.15s
Val loss: 0.0558 score: 0.9592 time: 0.08s
Test loss: 0.3279 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 218/1000, LR 0.000242
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.15s
Val loss: 0.0572 score: 0.9592 time: 0.09s
Test loss: 0.3280 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 219/1000, LR 0.000242
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.16s
Val loss: 0.0595 score: 0.9592 time: 0.09s
Test loss: 0.3278 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 220/1000, LR 0.000241
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.15s
Val loss: 0.0598 score: 0.9592 time: 0.08s
Test loss: 0.3281 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 221/1000, LR 0.000241
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.16s
Val loss: 0.0575 score: 0.9592 time: 0.08s
Test loss: 0.3287 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 222/1000, LR 0.000241
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.29s
Val loss: 0.0552 score: 0.9592 time: 0.08s
Test loss: 0.3286 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 223/1000, LR 0.000241
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.15s
Val loss: 0.0523 score: 0.9592 time: 0.08s
Test loss: 0.3271 score: 0.8980 time: 0.07s
Epoch 224/1000, LR 0.000240
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.0510 score: 0.9592 time: 0.10s
Test loss: 0.3273 score: 0.8980 time: 0.21s
Epoch 225/1000, LR 0.000240
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.15s
Val loss: 0.0492 score: 0.9796 time: 0.10s
Test loss: 0.3249 score: 0.8980 time: 0.08s
Epoch 226/1000, LR 0.000240
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.16s
Val loss: 0.0484 score: 0.9796 time: 0.09s
Test loss: 0.3244 score: 0.8980 time: 0.09s
Epoch 227/1000, LR 0.000240
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.15s
Val loss: 0.0496 score: 0.9796 time: 0.09s
Test loss: 0.3269 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 228/1000, LR 0.000239
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.15s
Val loss: 0.0512 score: 0.9592 time: 0.09s
Test loss: 0.3294 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 229/1000, LR 0.000239
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.15s
Val loss: 0.0531 score: 0.9592 time: 0.09s
Test loss: 0.3323 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 230/1000, LR 0.000239
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.0550 score: 0.9592 time: 0.09s
Test loss: 0.3342 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 231/1000, LR 0.000238
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.15s
Val loss: 0.0550 score: 0.9592 time: 0.09s
Test loss: 0.3344 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 232/1000, LR 0.000238
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.16s
Val loss: 0.0555 score: 0.9592 time: 0.15s
Test loss: 0.3341 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 233/1000, LR 0.000238
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.14s
Val loss: 0.0542 score: 0.9592 time: 0.09s
Test loss: 0.3335 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 234/1000, LR 0.000238
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.14s
Val loss: 0.0540 score: 0.9592 time: 0.08s
Test loss: 0.3337 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 235/1000, LR 0.000237
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.14s
Val loss: 0.0536 score: 0.9592 time: 0.08s
Test loss: 0.3338 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 236/1000, LR 0.000237
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.16s
Val loss: 0.0521 score: 0.9592 time: 0.09s
Test loss: 0.3335 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 237/1000, LR 0.000237
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.0540 score: 0.9592 time: 0.08s
Test loss: 0.3343 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 238/1000, LR 0.000236
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.15s
Val loss: 0.0543 score: 0.9592 time: 0.09s
Test loss: 0.3343 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 239/1000, LR 0.000236
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.15s
Val loss: 0.0545 score: 0.9592 time: 0.09s
Test loss: 0.3345 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 240/1000, LR 0.000236
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.16s
Val loss: 0.0543 score: 0.9592 time: 0.12s
Test loss: 0.3348 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 241/1000, LR 0.000236
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.15s
Val loss: 0.0534 score: 0.9592 time: 0.09s
Test loss: 0.3351 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 242/1000, LR 0.000235
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.15s
Val loss: 0.0523 score: 0.9592 time: 0.08s
Test loss: 0.3348 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 243/1000, LR 0.000235
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.0514 score: 0.9592 time: 0.10s
Test loss: 0.3342 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 244/1000, LR 0.000235
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.15s
Val loss: 0.0516 score: 0.9592 time: 0.09s
Test loss: 0.3352 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 245/1000, LR 0.000234
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.16s
Val loss: 0.0520 score: 0.9592 time: 0.09s
Test loss: 0.3363 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 246/1000, LR 0.000234
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.16s
Val loss: 0.0530 score: 0.9592 time: 0.09s
Test loss: 0.3368 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 225,   Train_Loss: 0.0113,   Val_Loss: 0.0484,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0484,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3244


[0.2394612398929894, 0.09545059502124786, 0.09643882303498685, 0.09361927886493504, 0.1004986569751054, 0.10370176495052874, 0.10904351901262999, 0.12588524306192994, 0.10665453900583088, 0.10348750092089176, 0.10505772102624178, 0.08799063391052186, 0.09275238704867661, 0.0965933590196073, 0.12278804602101445, 0.10502863698638976, 0.10589498491026461, 0.09529308113269508, 0.10316031705588102, 0.10280511411838233, 0.09568627900443971, 0.10198194393888116, 0.08405745588243008, 0.10280540003441274, 0.09598211501725018, 0.10248268395662308, 0.09528246708214283, 0.09724306012503803, 0.09967087605036795, 0.21277527301572263, 0.23618996515870094, 0.23367126216180623, 0.2087712469510734, 0.09462057007476687, 0.11106297606602311, 0.1028166098985821, 0.09246257110498846, 0.09126918110996485, 0.09118396299891174, 0.078358432976529, 0.0819196819793433, 0.09542880300432444, 0.09598940191790462, 0.088874327018857, 0.0872696558944881, 0.09007284184917808, 0.09695405419915915, 0.08869276591576636, 0.09139166004024446, 0.0798952009063214, 0.07884702598676085, 0.07728837616741657, 0.07853528601117432, 0.09089345601387322, 0.07991645601578057, 0.10195423197001219, 0.08559147105552256, 0.08758446108549833, 0.08273334591649473, 0.08972880197688937, 0.09071883698925376, 0.1075486580375582, 0.09110534703359008, 0.08778659580275416, 0.08685945789329708, 0.0889496048912406, 0.08693338301964104, 0.08762196497991681, 0.08902284409850836, 0.08839694713242352, 0.0992884139996022, 0.11004513711668551, 0.09017838607542217, 0.09191150404512882, 0.08700799779035151, 3.129479216877371, 0.08843301702290773, 0.0944723158609122, 0.08645662991330028, 0.0835436200723052, 0.08425115118734539, 0.08253813907504082, 0.08085088105872273, 0.08504074811935425, 0.08285718504339457, 0.20892707398161292, 0.08619164698757231, 0.08630726905539632, 0.08724456909112632, 0.08509374316781759, 0.0829878649674356, 0.08191635506227612, 0.1979307031724602, 0.08643840509466827, 0.08463943377137184, 0.1988642259966582, 0.07945046504028141, 0.08956967992708087, 0.08241633116267622, 0.09186432301066816, 0.08360415301285684, 0.09717888804152608, 0.0900319879874587, 0.08704181993380189, 0.09471106203272939, 0.09132047882303596, 0.17789843794889748, 0.08340414194390178, 0.09120183112099767, 0.08294900297187269, 0.08324207901023328, 0.09168130112811923, 0.09330852003768086, 0.08687654114328325, 0.08763326914049685, 0.08774863881990314, 0.08660778007470071, 0.09915313287638128, 0.09602722013369203, 0.09168196399696171, 0.09127156902104616, 0.08618036098778248, 0.09319070004858077, 0.08927634102292359, 0.08745685615576804, 0.09235594794154167, 0.09581388998776674, 0.08467029593884945, 0.08327589393593371, 0.08608053205534816, 0.0847567121963948, 0.0854678291361779, 0.08420614106580615, 0.08527310402132571, 0.08849721401929855, 0.10325196292251348, 0.08862095605581999, 0.0875789790879935, 0.08554799715057015, 0.08519505988806486, 0.09864136087708175, 0.08249109983444214, 0.09880341100506485, 0.083911461988464, 0.08731775707565248, 0.08498109178617597, 0.0885558039881289, 0.09119477402418852, 0.0821380119305104, 0.07915899017825723, 0.08197101694531739, 0.08587450091727078, 0.08725975407287478, 0.08066029800102115, 0.1002293189521879, 0.08538345689885318, 0.08471331396140158, 0.08634182205423713, 0.15393957099877298, 0.09633121686056256, 0.08731711190193892, 0.10084254713729024, 0.08991986815817654, 0.08281821990385652, 0.08672154787927866, 0.10763410618528724, 0.08480639988556504, 0.08878928888589144, 0.09049938898533583, 0.11246427497826517, 0.09297169395722449, 0.08185418089851737, 0.09106800495646894, 0.10219845501706004, 0.08633057097904384, 0.08831665385514498, 0.08613896812312305, 0.08809350593946874, 0.10732985800132155, 0.09506871202029288, 0.0787265570834279, 0.08879346493631601, 0.09304943610914052, 0.0914131950121373, 0.22475219098851085, 0.10402209591120481, 0.08798702014610171, 0.09114812081679702, 0.08622777415439487, 0.08934874599799514, 0.08569853915832937, 0.08383411401882768, 0.08579481905326247, 0.08624948980286717, 0.08515932015143335, 0.08432129304856062, 0.08481513406150043, 0.10486973193474114, 0.0953565330710262, 0.08757792599499226, 0.0828182059340179, 0.08940726215951145, 0.08434918499551713, 0.09780599712394178, 0.08096312102861702, 0.08526447601616383, 0.0802910819184035, 0.24515607999637723, 0.08581281290389597, 0.08389152213931084, 0.083328538807109, 0.0958866011351347, 0.08837056998163462, 0.0873304340057075, 0.08362050587311387, 0.08847036492079496, 0.0881443137768656, 0.08280913811177015, 0.08513349900022149, 0.10327559593133628, 0.08089207299053669, 0.08249981212429702, 0.07974560908041894, 0.2153674610890448, 0.0865905589889735, 0.09498741896823049, 0.08621739689260721, 0.08553462103009224, 0.08461728296242654, 0.09195353300310671, 0.08685902017168701, 0.08710444020107388, 0.08146408596076071, 0.08210279699414968, 0.07909905700944364, 0.08206697390414774, 0.08727898309007287, 0.08774171187542379, 0.08193140220828354, 0.08939554402604699, 0.08232937986031175, 0.08919593412429094, 0.087561507942155, 0.08683314686641097, 0.08949702279642224, 0.08501844992861152]
[0.004886964079448763, 0.001947971326964242, 0.0019681392456119768, 0.0019105975278558172, 0.002050992999491947, 0.0021163625500107905, 0.002225377939033265, 0.002569086593100611, 0.0021766232450169567, 0.0021119898147120768, 0.002144035122984526, 0.0017957272226637115, 0.0018929058581362574, 0.001971293041216476, 0.002505878490224785, 0.0021434415711508114, 0.0021611221410258083, 0.0019447567578101037, 0.0021053125929771637, 0.0020980635534363743, 0.001952781204172239, 0.002081264162017983, 0.0017154582833148995, 0.0020980693884574027, 0.0019588186738214324, 0.002091483346053532, 0.001944540144533527, 0.0019845522474497557, 0.002034099511231999, 0.004342352510524952, 0.0048202033705857334, 0.004768801268608291, 0.0042606376928790496, 0.001931032042342181, 0.002266591348286186, 0.002098298161195553, 0.0018869912470405807, 0.001862636349182956, 0.0018608972040594233, 0.001599151693398551, 0.0016718302444763938, 0.0019475265919249884, 0.001958967386079686, 0.0018137617758950408, 0.0017810133856017978, 0.0018382212622281239, 0.0019786541673297783, 0.0018100564472605378, 0.0018651359191886624, 0.0016305143042106408, 0.0016091229793216502, 0.001577313799335032, 0.0016027609390035576, 0.0018549684900790453, 0.0016309480819547055, 0.0020806986116329016, 0.0017467647154188277, 0.0017874379813367007, 0.001688435630948872, 0.001831200040344681, 0.0018514048365153829, 0.0021948705721950654, 0.0018592927966038792, 0.0017915631796480442, 0.0017726419978223893, 0.0018152980590049102, 0.0017741506738702254, 0.0017882033669370779, 0.0018167927367042523, 0.001804019329233133, 0.002026294163257188, 0.0022458191248303167, 0.0018403752260290238, 0.0018757449805128332, 0.001775673424292888, 0.06386692279341574, 0.0018047554494470966, 0.0019280064461410654, 0.0017644210186387812, 0.0017049718382103102, 0.0017194112487213344, 0.0016844518178579758, 0.00165001798079026, 0.001735525471823556, 0.0016909629600692767, 0.0042638178363594475, 0.0017590132038280064, 0.001761372837865231, 0.0017805014100229861, 0.0017366070034248488, 0.001693629897294604, 0.0016717623482097167, 0.004039402105560412, 0.0017640490835646586, 0.0017273353830892214, 0.004058453591768534, 0.0016214380620465595, 0.001827952651573079, 0.001681965942095433, 0.0018747821022585339, 0.0017062072043440171, 0.001983242613092369, 0.0018373875099481369, 0.0017763636721184058, 0.0019328788169944773, 0.0018636832412864482, 0.00363058036630403, 0.0017021253457939138, 0.0018612618596121973, 0.0016928367953443406, 0.0016988179389843528, 0.0018710469617983516, 0.0019042555109730788, 0.0017729906355772092, 0.0017884340640917725, 0.0017907885473449619, 0.0017675057158102186, 0.002023533324007781, 0.001959739186401878, 0.0018710604897339124, 0.0018626850820621665, 0.0017587828773016833, 0.0019018510213996075, 0.0018219661433249712, 0.001784833799097307, 0.0018848152641130953, 0.0019553855099544233, 0.0017279652232418256, 0.0016995080395088512, 0.0017567455521499623, 0.0017297288203345878, 0.001744241410942406, 0.0017184926748123703, 0.001740267429006647, 0.0018060655922305826, 0.0021071829167859896, 0.0018085909399146937, 0.0017873261038366022, 0.0017458774928687786, 0.0017386746915931605, 0.002013088997491464, 0.001683491833355962, 0.002016396142960507, 0.001712478816091102, 0.0017819950423602546, 0.0017343079956362443, 0.0018072613058801815, 0.001861117837228337, 0.0016762859577655184, 0.0016154895954746374, 0.001672877896843212, 0.0017525408350463425, 0.0017808113076096894, 0.0016461285306330845, 0.0020454963051466917, 0.0017425195285480242, 0.0017288431420694201, 0.0017620780011068803, 0.0031416238979341425, 0.0019659432012359705, 0.0017819818755497737, 0.0020580111660671477, 0.0018350993501668681, 0.001690167753139929, 0.0017698275077403808, 0.0021966144119446376, 0.0017307428548074498, 0.0018120263037937029, 0.0018469263058231802, 0.0022951892852707176, 0.001897381509331112, 0.0016704934877248443, 0.0018585307133973253, 0.0020856827554502047, 0.0017618483873274252, 0.0018023806909213261, 0.001757938124961695, 0.0017978266518258927, 0.002190405265333093, 0.0019401777963325077, 0.0016066644302740389, 0.0018121115293125718, 0.0018989680838600105, 0.0018655754084109652, 0.004586779407928793, 0.0021228999165552004, 0.0017956534723694228, 0.0018601657309550413, 0.001759750492946834, 0.0018234437958774517, 0.0017489497787414156, 0.001710900286098524, 0.001750914674556377, 0.0017601936694462688, 0.0017379453092129255, 0.0017208427152767473, 0.0017309211032959272, 0.0021401986109130843, 0.0019460516953270655, 0.0017873046121426991, 0.0016901674680411816, 0.0018246380032553356, 0.0017214119386840233, 0.001996040757631465, 0.0016523085924207556, 0.0017400913472686494, 0.0016385935085388471, 0.005003185306048515, 0.001751281895997877, 0.0017120718803940987, 0.0017005824246348776, 0.001956869410921116, 0.0018034810200333595, 0.0017822537552185205, 0.0017065409361859973, 0.0018055176514447952, 0.0017988635464666449, 0.0016899824104442888, 0.0017374183469432958, 0.0021076652230884955, 0.0016508586324599324, 0.001683669635189735, 0.0016274614098044683, 0.00439525430793969, 0.0017671542650810918, 0.0019385187544536833, 0.0017595387120940247, 0.001745604510818209, 0.001726883325763807, 0.0018766027143491165, 0.0017726330647283063, 0.0017776416367566098, 0.001662532366546137, 0.0016755672855948915, 0.0016142664695804824, 0.0016748362021254642, 0.0017812037365320995, 0.0017906471811310978, 0.0016720694328221132, 0.0018243988576744283, 0.001680191425720648, 0.0018203251862100192, 0.001786969549839898, 0.0017721050380900198, 0.001826469852988209, 0.0017350704067063574]
[204.6260180641224, 513.3545787649864, 508.0941311594334, 523.3964691256864, 487.5687046458524, 472.5088336092988, 449.3618735316545, 389.2433998470669, 459.4272354158423, 473.4871319141887, 466.4102697198292, 556.8774518641195, 528.2882905674946, 507.2812509817945, 399.06164800125526, 466.53942587439, 462.72257408150716, 514.2031238529036, 474.98884647143086, 476.6299849983672, 512.0901398802065, 480.47721103812466, 582.9346068781284, 476.6286594244845, 510.51177598236484, 478.1295542644042, 514.2604038343927, 503.89199945985183, 491.61803268628046, 230.28991717650968, 207.46012628892123, 209.69630388725264, 234.70665005647734, 517.8577973191389, 441.1911307947591, 476.5766936717074, 529.9441645891718, 536.873448453129, 537.3751961250555, 625.3315455488646, 598.146853308778, 513.4718078542757, 510.4730211977722, 551.3403211436231, 561.4780933620573, 544.004152572956, 505.3940281790193, 552.4689583650652, 536.1539551685873, 613.3034205327727, 621.456540519709, 633.9892546566083, 623.9233660271904, 539.0927152392693, 613.1403022967422, 480.6078085548462, 572.4869475395984, 559.4599703270093, 592.2642129022219, 546.0899835999202, 540.1303811445948, 455.6077304366568, 537.8389040319878, 558.1717749950922, 564.1297008806374, 550.8737229346066, 563.6499845971637, 559.2205106474265, 550.4205184208558, 554.3177857329766, 493.51176059873734, 445.2718337571176, 543.3674534718114, 533.1215119267426, 563.1666196717574, 15.6575572496988, 554.0916916507215, 518.6704650295726, 566.7581543386294, 586.5199515844723, 581.5944270131214, 593.6649474911337, 606.0540016182489, 576.1943666256176, 591.3790092475066, 234.53159548059506, 568.5005648756793, 567.7389695710259, 561.6395439906391, 575.8355218122755, 590.4477723246354, 598.1711461984389, 247.56138999468675, 566.8776505806034, 578.9263682027797, 246.3992694232668, 616.7364781962822, 547.0601216828189, 594.5423596117388, 533.3953203389923, 586.0952863485701, 504.2247445665516, 544.251005618421, 562.9477880548233, 517.3630085899262, 536.571868999438, 275.4380564829669, 587.5007986169051, 537.269914405464, 590.7243998654872, 588.6445963702593, 534.4601286965308, 525.1396119048109, 564.0187714101734, 559.1483745909492, 558.4132205237789, 565.7690331946696, 494.18509106606376, 510.2719825876528, 534.456264501749, 536.8594023917916, 568.5750145203799, 525.8035402079398, 548.8576193710517, 560.2762568177259, 530.5559749223236, 511.40810592552066, 578.7153506040509, 588.4055719377442, 569.2343998117243, 578.1253039459482, 573.3151350074324, 581.9053026275964, 574.6243268891177, 553.6897465417904, 474.56724901949383, 552.9166258276, 559.4949896683321, 572.7778747848035, 575.1507195884312, 496.7490266183525, 594.0034755063509, 495.9342951984539, 583.9488293832425, 561.1687890419149, 576.5988524046113, 553.323416346246, 537.311490974288, 596.5569271563877, 619.0073911965964, 597.7722593424423, 570.6001138475938, 561.5418072239553, 607.4859777902094, 488.87890801068227, 573.8816602148856, 578.4214748383725, 567.5117670000037, 318.306720501323, 508.661694484006, 561.1729354382361, 485.90601279923885, 544.9296246053755, 591.6572471236883, 565.026815114172, 455.24603433458833, 577.7865829243899, 551.8683685255426, 541.4401196447832, 435.6939126622186, 527.0421341633777, 598.6255003974701, 538.0594427584342, 479.45930290062023, 567.5857282571943, 554.8217449493582, 568.8482352140749, 556.2271529262229, 456.53652126695874, 515.4166808270287, 622.4075053615496, 551.8424135733809, 526.6017941530179, 536.0276488913233, 218.01789688673085, 471.05376574826283, 556.9003236913344, 537.586508212138, 568.2623781087426, 548.4128451125603, 571.7717067437025, 584.4875987953478, 571.1300582099276, 568.1193026416151, 575.3920993364725, 581.1106332510924, 577.7270830518235, 467.24635503494915, 513.860963920557, 559.501717393968, 591.6573469248874, 548.0539143741939, 580.9184760066642, 500.99177392881325, 605.213823003199, 574.6824737503921, 610.2794834648843, 199.87266887577942, 571.0102995327328, 584.0876259061108, 588.033832123547, 511.0203033575403, 554.4832404066568, 561.087329496125, 585.9806693151655, 553.8577810080056, 555.9065344140278, 591.7221349878455, 575.5666168481166, 474.45865170875504, 605.7453862720561, 593.9407465095185, 614.4538936380343, 227.518120667916, 565.8815530482913, 515.8577897183263, 568.3307750642788, 572.8674472382492, 579.0779174717529, 532.8778394881739, 564.1325437835445, 562.5430791689525, 601.4920491908805, 596.8127980279593, 619.4764116359805, 597.0733130385778, 561.4180901882354, 558.457305569448, 598.0612888259092, 548.1257542962429, 595.1702792264232, 549.3523946026561, 559.6066256918559, 564.3006359700908, 547.5042461631342, 576.345487845808]
Elapsed: 0.10885444376523024~0.19512911599519323
Time per graph: 0.002221519260514903~0.003982226857044759
Speed: 529.2209265495648~88.74676608530497
Total Time: 0.0860
best val loss: 0.048370059579610825 test_score: 0.8980

Testing...
Test loss: 0.6045 score: 0.8776 time: 0.08s
test Score 0.8776
Epoch Time List: [1.6468671369366348, 0.3625374410767108, 0.3446607287041843, 0.3508823369629681, 0.3606036838609725, 0.36298969807103276, 0.3916797188576311, 0.37404541322030127, 0.4218095289543271, 0.35315572493709624, 0.35673578176647425, 0.3355580123607069, 0.32381758093833923, 0.34335721214301884, 0.40835621021687984, 0.37103253905661404, 0.4127646731212735, 0.3285720518324524, 0.3519704029895365, 0.3680481552146375, 0.3486543719191104, 0.36052900715731084, 0.3294205910060555, 0.44964528689160943, 0.3643657409120351, 0.36292105820029974, 0.36129339318722486, 0.3596871639601886, 0.3661671478766948, 0.6826463029719889, 0.5047453679144382, 0.48902602097950876, 0.47005920903757215, 0.3907252917997539, 0.366746474057436, 0.4112151151057333, 0.5025370516814291, 0.35438661370426416, 0.34019099711440504, 0.31484659295529127, 0.3116508359089494, 0.3412766430992633, 0.35118512087501585, 0.3203880980145186, 0.32365983026102185, 0.3277209587395191, 0.3366870472673327, 0.3314850579481572, 0.353385770926252, 0.3044655539561063, 0.3045341409742832, 0.29658078122884035, 0.2944740492384881, 0.4024378666654229, 0.3069890718907118, 0.33451387216337025, 0.34206120134331286, 0.3166959087830037, 0.3189078269060701, 0.32396027096547186, 0.3176552599761635, 0.35139480885118246, 0.3557017759885639, 0.3201450798660517, 0.33116723597049713, 0.334884470095858, 0.3290913768578321, 0.3272226902190596, 0.3420929505955428, 0.33435797109268606, 0.3474288329016417, 0.37654860503971577, 0.33514810376800597, 0.33629975211806595, 0.38132777716964483, 4.934736586175859, 2.6449583880603313, 0.3323659591842443, 0.33420270308852196, 0.3755028247833252, 0.3224214508663863, 0.3143256369512528, 0.31103151896968484, 0.33049133606255054, 0.31024729483760893, 0.42670373409055173, 0.30975280003622174, 0.4988886397331953, 0.31787070678547025, 0.3217624758835882, 0.33461211808025837, 0.3120733411051333, 0.42808966618031263, 0.3359708893112838, 0.32973859715275466, 0.4316256493330002, 0.34111255616880953, 0.3346774810925126, 0.3174678520299494, 0.4332678900100291, 0.32320048310793936, 0.349380720872432, 0.33280800306238234, 0.3458822616375983, 0.3232066018972546, 0.32853517099283636, 0.4387583010829985, 0.3217755891382694, 0.32743463874794543, 0.3175818568561226, 0.32052710209973156, 0.3255198916886002, 0.3427574180532247, 0.33294892287813127, 0.46673375298269093, 0.3215505820699036, 0.34192029596306384, 0.3480273690074682, 0.33743947208859026, 0.33478575362823904, 0.3214109791442752, 0.44881214783526957, 0.3293634089641273, 0.32061013602651656, 0.3391737702768296, 0.32914240122772753, 0.33453374495729804, 0.3151097579393536, 0.4171629927586764, 0.3168445110786706, 0.3170872167684138, 0.32431772095151246, 0.32035361998714507, 0.318181409034878, 0.32250176509842277, 0.3442632502410561, 0.42108577815815806, 0.31959534296765924, 0.3502377748955041, 0.33160578389652073, 0.35447746119461954, 0.3295757200103253, 0.31307718716561794, 0.3474174439907074, 0.3204757629428059, 0.32392207300290465, 0.3267728630453348, 0.3283728687092662, 0.3142463231924921, 0.32584513910114765, 0.3008857660461217, 0.3364989091642201, 0.3337217168882489, 0.3061957322061062, 0.34094861312769353, 0.33240644610486925, 0.33795351115986705, 0.34164069476537406, 0.394276782637462, 0.31954616704024374, 0.3367699566297233, 0.3527671080082655, 0.3194517670199275, 0.3247833517380059, 0.318045194959268, 0.3231408742722124, 0.3762339698150754, 0.3249653750099242, 0.33733928203582764, 0.3713695900514722, 0.33764017559587955, 0.32593846833333373, 0.3227188987657428, 0.48592003411613405, 0.3216946462634951, 0.3275471639353782, 0.339575934689492, 0.33445127913728356, 0.3530180782545358, 0.3459909060038626, 0.3982119639404118, 0.33653374621644616, 0.3268450200557709, 0.3352506582159549, 0.5393593520857394, 0.34283977816812694, 0.3965936698950827, 0.46377471601590514, 0.32787263486534357, 0.33436236483976245, 0.32971241883933544, 0.31260556797496974, 0.32625557482242584, 0.3229276221245527, 0.32174919196404517, 0.32830481370911, 0.3116384190507233, 0.3308674667496234, 0.33602419518865645, 0.3390928921289742, 0.31809527589939535, 0.318866616114974, 0.29743430111557245, 0.30576892592944205, 0.30150343128480017, 0.33679333329200745, 0.3190713129006326, 0.48341950587928295, 0.3117968551814556, 0.3151617511175573, 0.3066605688072741, 0.35626407293602824, 0.3419248959980905, 0.31768706208094954, 0.32579530286602676, 0.3226159301120788, 0.3195519687142223, 0.31447100290097296, 0.3312310769688338, 0.33332077390514314, 0.32099627796560526, 0.44301877659745514, 0.3098689268808812, 0.46362223709002137, 0.3268974057864398, 0.33915742323733866, 0.3212234459351748, 0.3256098323035985, 0.3241121752653271, 0.32494653132744133, 0.32661836920306087, 0.39569193683564663, 0.30732294800691307, 0.30255943443626165, 0.2974578207358718, 0.32894459227100015, 0.3160535120405257, 0.3229030813090503, 0.3101215388160199, 0.35792749002575874, 0.3237544430885464, 0.3190615160856396, 0.3304854198358953, 0.3208736930973828, 0.3354801987297833, 0.3305049678310752]
Total Epoch List: [246]
Total Time List: [0.0859517918433994]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d4206920>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7171;  Loss pred: 0.7171; Loss self: 0.0000; time: 0.16s
Val loss: 0.6995 score: 0.4286 time: 0.11s
Test loss: 0.7061 score: 0.3673 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.16s
Val loss: 0.6986 score: 0.4490 time: 0.09s
Test loss: 0.7048 score: 0.3878 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7063;  Loss pred: 0.7063; Loss self: 0.0000; time: 0.16s
Val loss: 0.6968 score: 0.4490 time: 0.08s
Test loss: 0.7025 score: 0.4286 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.15s
Val loss: 0.6941 score: 0.4898 time: 0.10s
Test loss: 0.6990 score: 0.4490 time: 0.10s
Epoch 5/1000, LR 0.000090
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.17s
Val loss: 0.6907 score: 0.5306 time: 0.08s
Test loss: 0.6949 score: 0.4898 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5102 time: 0.09s
Test loss: 0.6900 score: 0.4898 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.6295;  Loss pred: 0.6295; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5102 time: 0.08s
Test loss: 0.6846 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.4898 time: 0.11s
Epoch 9/1000, LR 0.000210
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.16s
Val loss: 0.6707 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6713 score: 0.4898 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.15s
Val loss: 0.6645 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6641 score: 0.4898 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.16s
Val loss: 0.6587 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6575 score: 0.4898 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.16s
Val loss: 0.6530 score: 0.6122 time: 0.08s
Test loss: 0.6508 score: 0.5306 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.16s
Val loss: 0.6474 score: 0.6122 time: 0.08s
Test loss: 0.6444 score: 0.5510 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5454;  Loss pred: 0.5454; Loss self: 0.0000; time: 0.16s
Val loss: 0.6417 score: 0.6122 time: 0.08s
Test loss: 0.6375 score: 0.5714 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.17s
Val loss: 0.6360 score: 0.6122 time: 0.13s
Test loss: 0.6307 score: 0.5918 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4911;  Loss pred: 0.4911; Loss self: 0.0000; time: 0.15s
Val loss: 0.6306 score: 0.6122 time: 0.08s
Test loss: 0.6246 score: 0.5918 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.15s
Val loss: 0.6250 score: 0.6122 time: 0.08s
Test loss: 0.6181 score: 0.5918 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4661;  Loss pred: 0.4661; Loss self: 0.0000; time: 0.15s
Val loss: 0.6189 score: 0.6122 time: 0.07s
Test loss: 0.6110 score: 0.6122 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4525;  Loss pred: 0.4525; Loss self: 0.0000; time: 0.16s
Val loss: 0.6126 score: 0.6122 time: 0.08s
Test loss: 0.6035 score: 0.6122 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4408;  Loss pred: 0.4408; Loss self: 0.0000; time: 0.15s
Val loss: 0.6063 score: 0.6122 time: 0.08s
Test loss: 0.5960 score: 0.6531 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4282;  Loss pred: 0.4282; Loss self: 0.0000; time: 0.16s
Val loss: 0.5995 score: 0.6327 time: 0.09s
Test loss: 0.5880 score: 0.6735 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.4220;  Loss pred: 0.4220; Loss self: 0.0000; time: 0.16s
Val loss: 0.5926 score: 0.6531 time: 0.08s
Test loss: 0.5796 score: 0.6939 time: 0.10s
Epoch 23/1000, LR 0.000270
Train loss: 0.4015;  Loss pred: 0.4015; Loss self: 0.0000; time: 0.26s
Val loss: 0.5860 score: 0.6735 time: 0.08s
Test loss: 0.5713 score: 0.7143 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.16s
Val loss: 0.5796 score: 0.6735 time: 0.08s
Test loss: 0.5633 score: 0.7143 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.16s
Val loss: 0.5735 score: 0.6939 time: 0.08s
Test loss: 0.5554 score: 0.7551 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3655;  Loss pred: 0.3655; Loss self: 0.0000; time: 0.16s
Val loss: 0.5676 score: 0.7347 time: 0.08s
Test loss: 0.5476 score: 0.7551 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3645;  Loss pred: 0.3645; Loss self: 0.0000; time: 0.17s
Val loss: 0.5616 score: 0.7143 time: 0.08s
Test loss: 0.5396 score: 0.7551 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3556;  Loss pred: 0.3556; Loss self: 0.0000; time: 0.15s
Val loss: 0.5558 score: 0.7347 time: 0.08s
Test loss: 0.5320 score: 0.7551 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.16s
Val loss: 0.5505 score: 0.7347 time: 0.09s
Test loss: 0.5253 score: 0.7551 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.3342;  Loss pred: 0.3342; Loss self: 0.0000; time: 0.16s
Val loss: 0.5458 score: 0.7347 time: 0.16s
Test loss: 0.5195 score: 0.7551 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.16s
Val loss: 0.5418 score: 0.7347 time: 0.07s
Test loss: 0.5146 score: 0.7551 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.17s
Val loss: 0.5397 score: 0.7347 time: 0.09s
Test loss: 0.5129 score: 0.7551 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.2957;  Loss pred: 0.2957; Loss self: 0.0000; time: 0.17s
Val loss: 0.5383 score: 0.7143 time: 0.09s
Test loss: 0.5125 score: 0.7551 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.2986;  Loss pred: 0.2986; Loss self: 0.0000; time: 0.15s
Val loss: 0.5377 score: 0.7143 time: 0.08s
Test loss: 0.5131 score: 0.7551 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.2829;  Loss pred: 0.2829; Loss self: 0.0000; time: 0.16s
Val loss: 0.5367 score: 0.6939 time: 0.08s
Test loss: 0.5129 score: 0.7347 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.2678;  Loss pred: 0.2678; Loss self: 0.0000; time: 0.16s
Val loss: 0.5353 score: 0.6735 time: 0.08s
Test loss: 0.5121 score: 0.7347 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2732;  Loss pred: 0.2732; Loss self: 0.0000; time: 0.16s
Val loss: 0.5338 score: 0.6735 time: 0.07s
Test loss: 0.5108 score: 0.7347 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2552;  Loss pred: 0.2552; Loss self: 0.0000; time: 0.22s
Val loss: 0.5315 score: 0.6735 time: 0.07s
Test loss: 0.5081 score: 0.7347 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 0.15s
Val loss: 0.5284 score: 0.6735 time: 0.08s
Test loss: 0.5032 score: 0.7347 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2471;  Loss pred: 0.2471; Loss self: 0.0000; time: 0.16s
Val loss: 0.5244 score: 0.6939 time: 0.08s
Test loss: 0.4974 score: 0.7347 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 0.16s
Val loss: 0.5215 score: 0.6939 time: 0.08s
Test loss: 0.4935 score: 0.7347 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2334;  Loss pred: 0.2334; Loss self: 0.0000; time: 0.16s
Val loss: 0.5191 score: 0.6939 time: 0.08s
Test loss: 0.4906 score: 0.7347 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2291;  Loss pred: 0.2291; Loss self: 0.0000; time: 0.16s
Val loss: 0.5180 score: 0.6939 time: 0.08s
Test loss: 0.4901 score: 0.7347 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2277;  Loss pred: 0.2277; Loss self: 0.0000; time: 0.16s
Val loss: 0.5168 score: 0.6735 time: 0.08s
Test loss: 0.4894 score: 0.7347 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2153;  Loss pred: 0.2153; Loss self: 0.0000; time: 0.17s
Val loss: 0.5154 score: 0.6735 time: 0.10s
Test loss: 0.4883 score: 0.7347 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.15s
Val loss: 0.5134 score: 0.6735 time: 0.08s
Test loss: 0.4863 score: 0.7347 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2096;  Loss pred: 0.2096; Loss self: 0.0000; time: 0.16s
Val loss: 0.5100 score: 0.6939 time: 0.08s
Test loss: 0.4823 score: 0.7347 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.15s
Val loss: 0.5067 score: 0.6939 time: 0.08s
Test loss: 0.4782 score: 0.7347 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.15s
Val loss: 0.5027 score: 0.6939 time: 0.10s
Test loss: 0.4735 score: 0.7347 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.16s
Val loss: 0.4978 score: 0.7143 time: 0.08s
Test loss: 0.4670 score: 0.7347 time: 0.10s
Epoch 51/1000, LR 0.000269
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.62s
Val loss: 0.4924 score: 0.7143 time: 3.35s
Test loss: 0.4596 score: 0.7143 time: 2.34s
Epoch 52/1000, LR 0.000269
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 0.42s
Val loss: 0.4871 score: 0.7347 time: 0.08s
Test loss: 0.4520 score: 0.7551 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.1814;  Loss pred: 0.1814; Loss self: 0.0000; time: 0.15s
Val loss: 0.4808 score: 0.7551 time: 0.08s
Test loss: 0.4435 score: 0.7551 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.26s
Val loss: 0.4746 score: 0.7755 time: 0.08s
Test loss: 0.4351 score: 0.7551 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 0.15s
Val loss: 0.4690 score: 0.7959 time: 0.07s
Test loss: 0.4274 score: 0.7551 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1605;  Loss pred: 0.1605; Loss self: 0.0000; time: 0.15s
Val loss: 0.4644 score: 0.7959 time: 0.07s
Test loss: 0.4215 score: 0.7755 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.15s
Val loss: 0.4600 score: 0.7959 time: 0.07s
Test loss: 0.4157 score: 0.7755 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.15s
Val loss: 0.4561 score: 0.7959 time: 0.07s
Test loss: 0.4108 score: 0.7959 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.15s
Val loss: 0.4529 score: 0.7959 time: 0.08s
Test loss: 0.4072 score: 0.7959 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 0.15s
Val loss: 0.4504 score: 0.7959 time: 0.07s
Test loss: 0.4046 score: 0.7959 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1468;  Loss pred: 0.1468; Loss self: 0.0000; time: 0.15s
Val loss: 0.4474 score: 0.7959 time: 0.08s
Test loss: 0.4007 score: 0.7959 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.27s
Val loss: 0.4439 score: 0.8163 time: 0.07s
Test loss: 0.3962 score: 0.7959 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 0.15s
Val loss: 0.4411 score: 0.8163 time: 0.07s
Test loss: 0.3924 score: 0.7959 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 0.15s
Val loss: 0.4376 score: 0.8163 time: 0.07s
Test loss: 0.3878 score: 0.7959 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 0.15s
Val loss: 0.4336 score: 0.8163 time: 0.07s
Test loss: 0.3822 score: 0.7959 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1363;  Loss pred: 0.1363; Loss self: 0.0000; time: 0.15s
Val loss: 0.4303 score: 0.8163 time: 0.08s
Test loss: 0.3782 score: 0.7959 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1316;  Loss pred: 0.1316; Loss self: 0.0000; time: 0.15s
Val loss: 0.4276 score: 0.8163 time: 0.08s
Test loss: 0.3753 score: 0.7959 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.14s
Val loss: 0.4265 score: 0.8163 time: 0.07s
Test loss: 0.3749 score: 0.7959 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1311;  Loss pred: 0.1311; Loss self: 0.0000; time: 0.15s
Val loss: 0.4230 score: 0.8163 time: 0.21s
Test loss: 0.3708 score: 0.7959 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1258;  Loss pred: 0.1258; Loss self: 0.0000; time: 0.15s
Val loss: 0.4179 score: 0.8163 time: 0.07s
Test loss: 0.3638 score: 0.8367 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1252;  Loss pred: 0.1252; Loss self: 0.0000; time: 0.15s
Val loss: 0.4128 score: 0.8163 time: 0.08s
Test loss: 0.3569 score: 0.8571 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1169;  Loss pred: 0.1169; Loss self: 0.0000; time: 0.15s
Val loss: 0.4080 score: 0.8163 time: 0.08s
Test loss: 0.3503 score: 0.8571 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.15s
Val loss: 0.4044 score: 0.8163 time: 0.09s
Test loss: 0.3462 score: 0.8571 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1168;  Loss pred: 0.1168; Loss self: 0.0000; time: 0.16s
Val loss: 0.4004 score: 0.8163 time: 0.10s
Test loss: 0.3410 score: 0.8571 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1070;  Loss pred: 0.1070; Loss self: 0.0000; time: 0.15s
Val loss: 0.3956 score: 0.8163 time: 0.09s
Test loss: 0.3342 score: 0.8776 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.16s
Val loss: 0.3895 score: 0.8163 time: 0.08s
Test loss: 0.3244 score: 0.8980 time: 0.10s
Epoch 77/1000, LR 0.000267
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.26s
Val loss: 0.3843 score: 0.8163 time: 0.09s
Test loss: 0.3159 score: 0.8980 time: 0.10s
Epoch 78/1000, LR 0.000267
Train loss: 0.1079;  Loss pred: 0.1079; Loss self: 0.0000; time: 0.15s
Val loss: 0.3794 score: 0.7959 time: 0.07s
Test loss: 0.3073 score: 0.8980 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 0.18s
Val loss: 0.3751 score: 0.8163 time: 0.10s
Test loss: 0.3002 score: 0.8980 time: 0.11s
Epoch 80/1000, LR 0.000267
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.16s
Val loss: 0.3709 score: 0.8163 time: 0.08s
Test loss: 0.2932 score: 0.8980 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0999;  Loss pred: 0.0999; Loss self: 0.0000; time: 0.15s
Val loss: 0.3666 score: 0.8163 time: 0.08s
Test loss: 0.2861 score: 0.8980 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0923;  Loss pred: 0.0923; Loss self: 0.0000; time: 0.16s
Val loss: 0.3626 score: 0.8163 time: 0.08s
Test loss: 0.2804 score: 0.8980 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.17s
Val loss: 0.3593 score: 0.8163 time: 0.10s
Test loss: 0.2772 score: 0.8980 time: 0.10s
Epoch 84/1000, LR 0.000266
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.17s
Val loss: 0.3559 score: 0.8163 time: 0.09s
Test loss: 0.2724 score: 0.8980 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 0.16s
Val loss: 0.3529 score: 0.8163 time: 0.10s
Test loss: 0.2697 score: 0.8980 time: 0.10s
Epoch 86/1000, LR 0.000266
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.16s
Val loss: 0.3499 score: 0.8163 time: 0.08s
Test loss: 0.2667 score: 0.8980 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 0.15s
Val loss: 0.3458 score: 0.8163 time: 0.07s
Test loss: 0.2611 score: 0.8980 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.35s
Val loss: 0.3414 score: 0.8367 time: 0.08s
Test loss: 0.2536 score: 0.8980 time: 0.10s
Epoch 89/1000, LR 0.000266
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.15s
Val loss: 0.3368 score: 0.8571 time: 0.09s
Test loss: 0.2440 score: 0.8980 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0877;  Loss pred: 0.0877; Loss self: 0.0000; time: 0.16s
Val loss: 0.3326 score: 0.8571 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.15s
Val loss: 0.3284 score: 0.8571 time: 0.07s
Test loss: 0.2197 score: 0.9388 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0829;  Loss pred: 0.0829; Loss self: 0.0000; time: 0.16s
Val loss: 0.3249 score: 0.8571 time: 0.08s
Test loss: 0.2096 score: 0.9388 time: 0.10s
Epoch 93/1000, LR 0.000265
Train loss: 0.0790;  Loss pred: 0.0790; Loss self: 0.0000; time: 0.16s
Val loss: 0.3221 score: 0.8776 time: 0.09s
Test loss: 0.2035 score: 0.9388 time: 0.10s
Epoch 94/1000, LR 0.000265
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.15s
Val loss: 0.3191 score: 0.8776 time: 0.09s
Test loss: 0.1993 score: 0.9388 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0803;  Loss pred: 0.0803; Loss self: 0.0000; time: 0.15s
Val loss: 0.3160 score: 0.8776 time: 0.07s
Test loss: 0.1938 score: 0.9592 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.15s
Val loss: 0.3132 score: 0.8571 time: 0.08s
Test loss: 0.1884 score: 0.9592 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0736;  Loss pred: 0.0736; Loss self: 0.0000; time: 0.16s
Val loss: 0.3107 score: 0.8571 time: 0.08s
Test loss: 0.1828 score: 0.9796 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.15s
Val loss: 0.3091 score: 0.8571 time: 0.08s
Test loss: 0.1755 score: 0.9796 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.15s
Val loss: 0.3085 score: 0.8571 time: 0.08s
Test loss: 0.1679 score: 0.9796 time: 0.10s
Epoch 100/1000, LR 0.000265
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.15s
Val loss: 0.3085 score: 0.8571 time: 0.08s
Test loss: 0.1612 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.15s
Val loss: 0.3086 score: 0.8571 time: 0.08s
Test loss: 0.1551 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.18s
Val loss: 0.3092 score: 0.8571 time: 0.08s
Test loss: 0.1495 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.17s
Val loss: 0.3100 score: 0.8571 time: 0.09s
Test loss: 0.1440 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.15s
Val loss: 0.3071 score: 0.8571 time: 0.08s
Test loss: 0.1431 score: 0.9796 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0662;  Loss pred: 0.0662; Loss self: 0.0000; time: 0.15s
Val loss: 0.3050 score: 0.8571 time: 0.08s
Test loss: 0.1388 score: 0.9796 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.16s
Val loss: 0.3038 score: 0.8571 time: 0.08s
Test loss: 0.1341 score: 0.9796 time: 0.10s
Epoch 107/1000, LR 0.000264
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.14s
Val loss: 0.3060 score: 0.8776 time: 0.08s
Test loss: 0.1288 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.15s
Val loss: 0.3064 score: 0.8776 time: 0.08s
Test loss: 0.1248 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.16s
Val loss: 0.3066 score: 0.8776 time: 0.08s
Test loss: 0.1219 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.15s
Val loss: 0.3110 score: 0.8776 time: 0.08s
Test loss: 0.1171 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.15s
Val loss: 0.3120 score: 0.8776 time: 0.08s
Test loss: 0.1160 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.16s
Val loss: 0.3062 score: 0.8776 time: 0.08s
Test loss: 0.1172 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.15s
Val loss: 0.3043 score: 0.8571 time: 0.08s
Test loss: 0.1168 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.16s
Val loss: 0.3055 score: 0.8571 time: 0.10s
Test loss: 0.1145 score: 0.9796 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.25s
Val loss: 0.3121 score: 0.8776 time: 0.09s
Test loss: 0.1097 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.17s
Val loss: 0.3304 score: 0.8776 time: 0.08s
Test loss: 0.1053 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.17s
Val loss: 0.3310 score: 0.8776 time: 0.09s
Test loss: 0.1042 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0551;  Loss pred: 0.0551; Loss self: 0.0000; time: 0.16s
Val loss: 0.3201 score: 0.8776 time: 0.08s
Test loss: 0.1037 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.15s
Val loss: 0.3094 score: 0.8776 time: 0.08s
Test loss: 0.1048 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0533;  Loss pred: 0.0533; Loss self: 0.0000; time: 0.16s
Val loss: 0.3071 score: 0.8776 time: 0.08s
Test loss: 0.1036 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.15s
Val loss: 0.3094 score: 0.8776 time: 0.08s
Test loss: 0.1018 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.15s
Val loss: 0.3185 score: 0.8776 time: 0.08s
Test loss: 0.0983 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.16s
Val loss: 0.3320 score: 0.8776 time: 0.08s
Test loss: 0.0968 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.16s
Val loss: 0.3460 score: 0.8776 time: 0.08s
Test loss: 0.0964 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.16s
Val loss: 0.3386 score: 0.8776 time: 0.09s
Test loss: 0.0948 score: 0.9796 time: 0.22s
     INFO: Early stopping counter 19 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.14s
Val loss: 0.3180 score: 0.8776 time: 0.07s
Test loss: 0.0961 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 105,   Train_Loss: 0.0657,   Val_Loss: 0.3038,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3038,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.1341


[0.2394612398929894, 0.09545059502124786, 0.09643882303498685, 0.09361927886493504, 0.1004986569751054, 0.10370176495052874, 0.10904351901262999, 0.12588524306192994, 0.10665453900583088, 0.10348750092089176, 0.10505772102624178, 0.08799063391052186, 0.09275238704867661, 0.0965933590196073, 0.12278804602101445, 0.10502863698638976, 0.10589498491026461, 0.09529308113269508, 0.10316031705588102, 0.10280511411838233, 0.09568627900443971, 0.10198194393888116, 0.08405745588243008, 0.10280540003441274, 0.09598211501725018, 0.10248268395662308, 0.09528246708214283, 0.09724306012503803, 0.09967087605036795, 0.21277527301572263, 0.23618996515870094, 0.23367126216180623, 0.2087712469510734, 0.09462057007476687, 0.11106297606602311, 0.1028166098985821, 0.09246257110498846, 0.09126918110996485, 0.09118396299891174, 0.078358432976529, 0.0819196819793433, 0.09542880300432444, 0.09598940191790462, 0.088874327018857, 0.0872696558944881, 0.09007284184917808, 0.09695405419915915, 0.08869276591576636, 0.09139166004024446, 0.0798952009063214, 0.07884702598676085, 0.07728837616741657, 0.07853528601117432, 0.09089345601387322, 0.07991645601578057, 0.10195423197001219, 0.08559147105552256, 0.08758446108549833, 0.08273334591649473, 0.08972880197688937, 0.09071883698925376, 0.1075486580375582, 0.09110534703359008, 0.08778659580275416, 0.08685945789329708, 0.0889496048912406, 0.08693338301964104, 0.08762196497991681, 0.08902284409850836, 0.08839694713242352, 0.0992884139996022, 0.11004513711668551, 0.09017838607542217, 0.09191150404512882, 0.08700799779035151, 3.129479216877371, 0.08843301702290773, 0.0944723158609122, 0.08645662991330028, 0.0835436200723052, 0.08425115118734539, 0.08253813907504082, 0.08085088105872273, 0.08504074811935425, 0.08285718504339457, 0.20892707398161292, 0.08619164698757231, 0.08630726905539632, 0.08724456909112632, 0.08509374316781759, 0.0829878649674356, 0.08191635506227612, 0.1979307031724602, 0.08643840509466827, 0.08463943377137184, 0.1988642259966582, 0.07945046504028141, 0.08956967992708087, 0.08241633116267622, 0.09186432301066816, 0.08360415301285684, 0.09717888804152608, 0.0900319879874587, 0.08704181993380189, 0.09471106203272939, 0.09132047882303596, 0.17789843794889748, 0.08340414194390178, 0.09120183112099767, 0.08294900297187269, 0.08324207901023328, 0.09168130112811923, 0.09330852003768086, 0.08687654114328325, 0.08763326914049685, 0.08774863881990314, 0.08660778007470071, 0.09915313287638128, 0.09602722013369203, 0.09168196399696171, 0.09127156902104616, 0.08618036098778248, 0.09319070004858077, 0.08927634102292359, 0.08745685615576804, 0.09235594794154167, 0.09581388998776674, 0.08467029593884945, 0.08327589393593371, 0.08608053205534816, 0.0847567121963948, 0.0854678291361779, 0.08420614106580615, 0.08527310402132571, 0.08849721401929855, 0.10325196292251348, 0.08862095605581999, 0.0875789790879935, 0.08554799715057015, 0.08519505988806486, 0.09864136087708175, 0.08249109983444214, 0.09880341100506485, 0.083911461988464, 0.08731775707565248, 0.08498109178617597, 0.0885558039881289, 0.09119477402418852, 0.0821380119305104, 0.07915899017825723, 0.08197101694531739, 0.08587450091727078, 0.08725975407287478, 0.08066029800102115, 0.1002293189521879, 0.08538345689885318, 0.08471331396140158, 0.08634182205423713, 0.15393957099877298, 0.09633121686056256, 0.08731711190193892, 0.10084254713729024, 0.08991986815817654, 0.08281821990385652, 0.08672154787927866, 0.10763410618528724, 0.08480639988556504, 0.08878928888589144, 0.09049938898533583, 0.11246427497826517, 0.09297169395722449, 0.08185418089851737, 0.09106800495646894, 0.10219845501706004, 0.08633057097904384, 0.08831665385514498, 0.08613896812312305, 0.08809350593946874, 0.10732985800132155, 0.09506871202029288, 0.0787265570834279, 0.08879346493631601, 0.09304943610914052, 0.0914131950121373, 0.22475219098851085, 0.10402209591120481, 0.08798702014610171, 0.09114812081679702, 0.08622777415439487, 0.08934874599799514, 0.08569853915832937, 0.08383411401882768, 0.08579481905326247, 0.08624948980286717, 0.08515932015143335, 0.08432129304856062, 0.08481513406150043, 0.10486973193474114, 0.0953565330710262, 0.08757792599499226, 0.0828182059340179, 0.08940726215951145, 0.08434918499551713, 0.09780599712394178, 0.08096312102861702, 0.08526447601616383, 0.0802910819184035, 0.24515607999637723, 0.08581281290389597, 0.08389152213931084, 0.083328538807109, 0.0958866011351347, 0.08837056998163462, 0.0873304340057075, 0.08362050587311387, 0.08847036492079496, 0.0881443137768656, 0.08280913811177015, 0.08513349900022149, 0.10327559593133628, 0.08089207299053669, 0.08249981212429702, 0.07974560908041894, 0.2153674610890448, 0.0865905589889735, 0.09498741896823049, 0.08621739689260721, 0.08553462103009224, 0.08461728296242654, 0.09195353300310671, 0.08685902017168701, 0.08710444020107388, 0.08146408596076071, 0.08210279699414968, 0.07909905700944364, 0.08206697390414774, 0.08727898309007287, 0.08774171187542379, 0.08193140220828354, 0.08939554402604699, 0.08232937986031175, 0.08919593412429094, 0.087561507942155, 0.08683314686641097, 0.08949702279642224, 0.08501844992861152, 0.09377385582774878, 0.1071053131017834, 0.09638045099563897, 0.10971526009961963, 0.10653248685412109, 0.10048661706969142, 0.092984575079754, 0.11306942021474242, 0.09685623389668763, 0.09618054190650582, 0.09484837995842099, 0.09866203600540757, 0.0962987020611763, 0.09290683595463634, 0.08964207512326539, 0.08864587405696511, 0.08757643192075193, 0.0886092078872025, 0.08993486501276493, 0.09432363510131836, 0.10444240900687873, 0.10341274295933545, 0.09494237089529634, 0.09439187194220722, 0.09262269199825823, 0.09415961592458189, 0.08907612087205052, 0.09581966581754386, 0.10124349896796048, 0.09503383305855095, 0.08971589081920683, 0.09559466899372637, 0.10706321895122528, 0.09037789003923535, 0.09459559619426727, 0.09123083995655179, 0.09775713598355651, 0.08627334609627724, 0.09496837807819247, 0.09675388014875352, 0.09539868705905974, 0.09699613507837057, 0.09559524105861783, 0.09462325600907207, 0.09433700609952211, 0.09245835919864476, 0.09115956909954548, 0.09441674803383648, 0.08968709711916745, 0.10313707403838634, 2.3511948930099607, 0.08931005699560046, 0.09250502986833453, 0.09291360597126186, 0.08753556990996003, 0.08644743403419852, 0.08849788084626198, 0.08786026411689818, 0.08872468397021294, 0.08808946399949491, 0.08852309198118746, 0.08741566282697022, 0.0907421549782157, 0.08821761701256037, 0.08866588003002107, 0.09078714507631958, 0.088500265032053, 0.08714445098303258, 0.08826917107217014, 0.0885246298275888, 0.08922947198152542, 0.09116909001022577, 0.09239553101360798, 0.0968009380158037, 0.09209364908747375, 0.10452739405445755, 0.10435785911977291, 0.08825975004583597, 0.11152748204767704, 0.08750258618965745, 0.09261424420401454, 0.09528317302465439, 0.10240871296264231, 0.09995200298726559, 0.10073212208226323, 0.09091373183764517, 0.09043520991690457, 0.10783850797452033, 0.08742421213537455, 0.08818155690096319, 0.0869767488911748, 0.10258430405519903, 0.10181283112615347, 0.09100496908649802, 0.09182876721024513, 0.09324555890634656, 0.09149208106100559, 0.09071994107216597, 0.10916547197848558, 0.08744606701657176, 0.08697231789119542, 0.09447206603363156, 0.09625788405537605, 0.08956268103793263, 0.09156626299954951, 0.1067745708860457, 0.0916756740771234, 0.09234579396434128, 0.09228096716105938, 0.09212888311594725, 0.09220808115787804, 0.09140539518557489, 0.09367072000168264, 0.13304937887005508, 0.09388152090832591, 0.10614901897497475, 0.09913949482142925, 0.09175232890993357, 0.09160783304832876, 0.09403908997774124, 0.087428268045187, 0.09496230306103826, 0.08721553906798363, 0.09008760284632444, 0.22741566691547632, 0.08871576003730297]
[0.004886964079448763, 0.001947971326964242, 0.0019681392456119768, 0.0019105975278558172, 0.002050992999491947, 0.0021163625500107905, 0.002225377939033265, 0.002569086593100611, 0.0021766232450169567, 0.0021119898147120768, 0.002144035122984526, 0.0017957272226637115, 0.0018929058581362574, 0.001971293041216476, 0.002505878490224785, 0.0021434415711508114, 0.0021611221410258083, 0.0019447567578101037, 0.0021053125929771637, 0.0020980635534363743, 0.001952781204172239, 0.002081264162017983, 0.0017154582833148995, 0.0020980693884574027, 0.0019588186738214324, 0.002091483346053532, 0.001944540144533527, 0.0019845522474497557, 0.002034099511231999, 0.004342352510524952, 0.0048202033705857334, 0.004768801268608291, 0.0042606376928790496, 0.001931032042342181, 0.002266591348286186, 0.002098298161195553, 0.0018869912470405807, 0.001862636349182956, 0.0018608972040594233, 0.001599151693398551, 0.0016718302444763938, 0.0019475265919249884, 0.001958967386079686, 0.0018137617758950408, 0.0017810133856017978, 0.0018382212622281239, 0.0019786541673297783, 0.0018100564472605378, 0.0018651359191886624, 0.0016305143042106408, 0.0016091229793216502, 0.001577313799335032, 0.0016027609390035576, 0.0018549684900790453, 0.0016309480819547055, 0.0020806986116329016, 0.0017467647154188277, 0.0017874379813367007, 0.001688435630948872, 0.001831200040344681, 0.0018514048365153829, 0.0021948705721950654, 0.0018592927966038792, 0.0017915631796480442, 0.0017726419978223893, 0.0018152980590049102, 0.0017741506738702254, 0.0017882033669370779, 0.0018167927367042523, 0.001804019329233133, 0.002026294163257188, 0.0022458191248303167, 0.0018403752260290238, 0.0018757449805128332, 0.001775673424292888, 0.06386692279341574, 0.0018047554494470966, 0.0019280064461410654, 0.0017644210186387812, 0.0017049718382103102, 0.0017194112487213344, 0.0016844518178579758, 0.00165001798079026, 0.001735525471823556, 0.0016909629600692767, 0.0042638178363594475, 0.0017590132038280064, 0.001761372837865231, 0.0017805014100229861, 0.0017366070034248488, 0.001693629897294604, 0.0016717623482097167, 0.004039402105560412, 0.0017640490835646586, 0.0017273353830892214, 0.004058453591768534, 0.0016214380620465595, 0.001827952651573079, 0.001681965942095433, 0.0018747821022585339, 0.0017062072043440171, 0.001983242613092369, 0.0018373875099481369, 0.0017763636721184058, 0.0019328788169944773, 0.0018636832412864482, 0.00363058036630403, 0.0017021253457939138, 0.0018612618596121973, 0.0016928367953443406, 0.0016988179389843528, 0.0018710469617983516, 0.0019042555109730788, 0.0017729906355772092, 0.0017884340640917725, 0.0017907885473449619, 0.0017675057158102186, 0.002023533324007781, 0.001959739186401878, 0.0018710604897339124, 0.0018626850820621665, 0.0017587828773016833, 0.0019018510213996075, 0.0018219661433249712, 0.001784833799097307, 0.0018848152641130953, 0.0019553855099544233, 0.0017279652232418256, 0.0016995080395088512, 0.0017567455521499623, 0.0017297288203345878, 0.001744241410942406, 0.0017184926748123703, 0.001740267429006647, 0.0018060655922305826, 0.0021071829167859896, 0.0018085909399146937, 0.0017873261038366022, 0.0017458774928687786, 0.0017386746915931605, 0.002013088997491464, 0.001683491833355962, 0.002016396142960507, 0.001712478816091102, 0.0017819950423602546, 0.0017343079956362443, 0.0018072613058801815, 0.001861117837228337, 0.0016762859577655184, 0.0016154895954746374, 0.001672877896843212, 0.0017525408350463425, 0.0017808113076096894, 0.0016461285306330845, 0.0020454963051466917, 0.0017425195285480242, 0.0017288431420694201, 0.0017620780011068803, 0.0031416238979341425, 0.0019659432012359705, 0.0017819818755497737, 0.0020580111660671477, 0.0018350993501668681, 0.001690167753139929, 0.0017698275077403808, 0.0021966144119446376, 0.0017307428548074498, 0.0018120263037937029, 0.0018469263058231802, 0.0022951892852707176, 0.001897381509331112, 0.0016704934877248443, 0.0018585307133973253, 0.0020856827554502047, 0.0017618483873274252, 0.0018023806909213261, 0.001757938124961695, 0.0017978266518258927, 0.002190405265333093, 0.0019401777963325077, 0.0016066644302740389, 0.0018121115293125718, 0.0018989680838600105, 0.0018655754084109652, 0.004586779407928793, 0.0021228999165552004, 0.0017956534723694228, 0.0018601657309550413, 0.001759750492946834, 0.0018234437958774517, 0.0017489497787414156, 0.001710900286098524, 0.001750914674556377, 0.0017601936694462688, 0.0017379453092129255, 0.0017208427152767473, 0.0017309211032959272, 0.0021401986109130843, 0.0019460516953270655, 0.0017873046121426991, 0.0016901674680411816, 0.0018246380032553356, 0.0017214119386840233, 0.001996040757631465, 0.0016523085924207556, 0.0017400913472686494, 0.0016385935085388471, 0.005003185306048515, 0.001751281895997877, 0.0017120718803940987, 0.0017005824246348776, 0.001956869410921116, 0.0018034810200333595, 0.0017822537552185205, 0.0017065409361859973, 0.0018055176514447952, 0.0017988635464666449, 0.0016899824104442888, 0.0017374183469432958, 0.0021076652230884955, 0.0016508586324599324, 0.001683669635189735, 0.0016274614098044683, 0.00439525430793969, 0.0017671542650810918, 0.0019385187544536833, 0.0017595387120940247, 0.001745604510818209, 0.001726883325763807, 0.0018766027143491165, 0.0017726330647283063, 0.0017776416367566098, 0.001662532366546137, 0.0016755672855948915, 0.0016142664695804824, 0.0016748362021254642, 0.0017812037365320995, 0.0017906471811310978, 0.0016720694328221132, 0.0018243988576744283, 0.001680191425720648, 0.0018203251862100192, 0.001786969549839898, 0.0017721050380900198, 0.001826469852988209, 0.0017350704067063574, 0.001913752159749975, 0.0021858227163629265, 0.001966947979502836, 0.002239086940808564, 0.0021741323847779812, 0.0020507472871365597, 0.0018976443893827346, 0.002307539188055968, 0.001976657834626278, 0.001962868202173588, 0.0019356812236412447, 0.002013510938885869, 0.001965279633901557, 0.0018960578766252314, 0.0018294301045564364, 0.0018090994705503083, 0.001787274120831672, 0.0018083511813714796, 0.001835405408423774, 0.0019249721449248645, 0.00213147773483426, 0.002110464142027254, 0.0019375994060264559, 0.001926364733514433, 0.001890259020372617, 0.0019216248147873854, 0.0018178800177969495, 0.0019555033840315075, 0.002066193856488989, 0.0019394659807867541, 0.0018309365473307517, 0.0019509116121168648, 0.002184963652065822, 0.001844446735494599, 0.001930522371311577, 0.0018618538766643222, 0.0019950435915011533, 0.0017606805325770865, 0.001938130164861071, 0.001974568982627623, 0.0019469119807971375, 0.001979512960783073, 0.001950923286910568, 0.0019310868573280014, 0.0019252450224392268, 0.0018869052897682603, 0.0018603993693784792, 0.001926872408853806, 0.0018303489207993357, 0.002104838245681354, 0.047983569245101235, 0.0018226542244000094, 0.0018878577524149905, 0.001896196040229834, 0.0017864402022440822, 0.0017642333476367046, 0.001806079200944122, 0.001793066614630575, 0.0018107078361267947, 0.0017977441632549983, 0.001806593713901785, 0.0017839931189177595, 0.0018518807138411366, 0.001800359530868579, 0.0018095077557147158, 0.0018527988791085628, 0.001806127857797, 0.0017784581833271956, 0.0018014116545340844, 0.0018066250985222204, 0.001821009632276029, 0.001860593673678077, 0.0018856230819103668, 0.0019755293472612998, 0.0018794622262749746, 0.002133212123560358, 0.002129752226934141, 0.00180121938869053, 0.002276071062197491, 0.00178576706509505, 0.0018900866164084599, 0.0019445545515235588, 0.0020899737339314757, 0.0020398367956584815, 0.002055757593515576, 0.001855382282400922, 0.0018456165289164198, 0.002200785877031027, 0.0017841675945994805, 0.0017996236102237385, 0.0017750356916566283, 0.0020935572256163067, 0.002077812880125581, 0.0018572442670713883, 0.0018740564736784721, 0.00190297058992544, 0.0018671853277756243, 0.0018514273688197136, 0.0022278667750711342, 0.001784613612583097, 0.0017749452630856208, 0.001928001347625134, 0.0019644466133750217, 0.0018278098171006661, 0.0018686992448887654, 0.0021790728752254223, 0.0018709321240229265, 0.0018846080400885977, 0.0018832850441032527, 0.001880181288080556, 0.0018817975746505723, 0.0018654162282770385, 0.001911647346973115, 0.0027152934463276547, 0.0019159494062923655, 0.0021663065096933624, 0.002023254996355699, 0.0018724965083659912, 0.0018695476132311992, 0.001919165101586556, 0.0017842503682691225, 0.001938006184919148, 0.0017799089605710944, 0.0018385225070678458, 0.004641136059499517, 0.0018105257150469994]
[204.6260180641224, 513.3545787649864, 508.0941311594334, 523.3964691256864, 487.5687046458524, 472.5088336092988, 449.3618735316545, 389.2433998470669, 459.4272354158423, 473.4871319141887, 466.4102697198292, 556.8774518641195, 528.2882905674946, 507.2812509817945, 399.06164800125526, 466.53942587439, 462.72257408150716, 514.2031238529036, 474.98884647143086, 476.6299849983672, 512.0901398802065, 480.47721103812466, 582.9346068781284, 476.6286594244845, 510.51177598236484, 478.1295542644042, 514.2604038343927, 503.89199945985183, 491.61803268628046, 230.28991717650968, 207.46012628892123, 209.69630388725264, 234.70665005647734, 517.8577973191389, 441.1911307947591, 476.5766936717074, 529.9441645891718, 536.873448453129, 537.3751961250555, 625.3315455488646, 598.146853308778, 513.4718078542757, 510.4730211977722, 551.3403211436231, 561.4780933620573, 544.004152572956, 505.3940281790193, 552.4689583650652, 536.1539551685873, 613.3034205327727, 621.456540519709, 633.9892546566083, 623.9233660271904, 539.0927152392693, 613.1403022967422, 480.6078085548462, 572.4869475395984, 559.4599703270093, 592.2642129022219, 546.0899835999202, 540.1303811445948, 455.6077304366568, 537.8389040319878, 558.1717749950922, 564.1297008806374, 550.8737229346066, 563.6499845971637, 559.2205106474265, 550.4205184208558, 554.3177857329766, 493.51176059873734, 445.2718337571176, 543.3674534718114, 533.1215119267426, 563.1666196717574, 15.6575572496988, 554.0916916507215, 518.6704650295726, 566.7581543386294, 586.5199515844723, 581.5944270131214, 593.6649474911337, 606.0540016182489, 576.1943666256176, 591.3790092475066, 234.53159548059506, 568.5005648756793, 567.7389695710259, 561.6395439906391, 575.8355218122755, 590.4477723246354, 598.1711461984389, 247.56138999468675, 566.8776505806034, 578.9263682027797, 246.3992694232668, 616.7364781962822, 547.0601216828189, 594.5423596117388, 533.3953203389923, 586.0952863485701, 504.2247445665516, 544.251005618421, 562.9477880548233, 517.3630085899262, 536.571868999438, 275.4380564829669, 587.5007986169051, 537.269914405464, 590.7243998654872, 588.6445963702593, 534.4601286965308, 525.1396119048109, 564.0187714101734, 559.1483745909492, 558.4132205237789, 565.7690331946696, 494.18509106606376, 510.2719825876528, 534.456264501749, 536.8594023917916, 568.5750145203799, 525.8035402079398, 548.8576193710517, 560.2762568177259, 530.5559749223236, 511.40810592552066, 578.7153506040509, 588.4055719377442, 569.2343998117243, 578.1253039459482, 573.3151350074324, 581.9053026275964, 574.6243268891177, 553.6897465417904, 474.56724901949383, 552.9166258276, 559.4949896683321, 572.7778747848035, 575.1507195884312, 496.7490266183525, 594.0034755063509, 495.9342951984539, 583.9488293832425, 561.1687890419149, 576.5988524046113, 553.323416346246, 537.311490974288, 596.5569271563877, 619.0073911965964, 597.7722593424423, 570.6001138475938, 561.5418072239553, 607.4859777902094, 488.87890801068227, 573.8816602148856, 578.4214748383725, 567.5117670000037, 318.306720501323, 508.661694484006, 561.1729354382361, 485.90601279923885, 544.9296246053755, 591.6572471236883, 565.026815114172, 455.24603433458833, 577.7865829243899, 551.8683685255426, 541.4401196447832, 435.6939126622186, 527.0421341633777, 598.6255003974701, 538.0594427584342, 479.45930290062023, 567.5857282571943, 554.8217449493582, 568.8482352140749, 556.2271529262229, 456.53652126695874, 515.4166808270287, 622.4075053615496, 551.8424135733809, 526.6017941530179, 536.0276488913233, 218.01789688673085, 471.05376574826283, 556.9003236913344, 537.586508212138, 568.2623781087426, 548.4128451125603, 571.7717067437025, 584.4875987953478, 571.1300582099276, 568.1193026416151, 575.3920993364725, 581.1106332510924, 577.7270830518235, 467.24635503494915, 513.860963920557, 559.501717393968, 591.6573469248874, 548.0539143741939, 580.9184760066642, 500.99177392881325, 605.213823003199, 574.6824737503921, 610.2794834648843, 199.87266887577942, 571.0102995327328, 584.0876259061108, 588.033832123547, 511.0203033575403, 554.4832404066568, 561.087329496125, 585.9806693151655, 553.8577810080056, 555.9065344140278, 591.7221349878455, 575.5666168481166, 474.45865170875504, 605.7453862720561, 593.9407465095185, 614.4538936380343, 227.518120667916, 565.8815530482913, 515.8577897183263, 568.3307750642788, 572.8674472382492, 579.0779174717529, 532.8778394881739, 564.1325437835445, 562.5430791689525, 601.4920491908805, 596.8127980279593, 619.4764116359805, 597.0733130385778, 561.4180901882354, 558.457305569448, 598.0612888259092, 548.1257542962429, 595.1702792264232, 549.3523946026561, 559.6066256918559, 564.3006359700908, 547.5042461631342, 576.345487845808, 522.5337016108954, 457.4936441615622, 508.40185425379633, 446.6106169324925, 459.9535920634006, 487.62712318215046, 526.9691231902938, 433.362087706285, 505.90445269910236, 509.4585560521317, 516.6139898381006, 496.64493034903876, 508.8334416892914, 527.4100608046242, 546.6183143643304, 552.7612031724329, 559.511262623033, 552.9899337592079, 544.8387562826182, 519.4880365601508, 469.1580792317112, 473.82941983531043, 516.1025529269522, 519.1124933935091, 529.0280269647253, 520.3929467941655, 550.0913097729514, 511.3772792038737, 483.98169264681917, 515.6058471282621, 546.1685722849651, 512.5808846434286, 457.67351738530203, 542.1680012526056, 517.9945152982663, 537.0990777168772, 501.2421805017096, 567.9622063727327, 515.9612177398219, 506.4396376110738, 513.6339032597473, 505.1747676380009, 512.5778172362556, 517.8430976344979, 519.4144061377863, 529.9683059995102, 537.5189953617751, 518.9757222144495, 546.3439176194273, 475.0958901719745, 20.840467179337487, 548.650416855224, 529.700926206319, 527.371631827051, 559.7724450803472, 566.8184434556571, 553.6855745181348, 557.7037639541517, 552.2702117085084, 556.252675124473, 553.5278863780907, 560.5402786568142, 539.9915839750923, 555.4446113980092, 552.6364818508457, 539.7239880030206, 553.6706582997599, 562.2847977955649, 555.1202011394997, 553.5182705133334, 549.1459145936136, 537.4628615301965, 530.3286799962576, 506.1934419685094, 532.0670913306747, 468.77663452005294, 469.5381872846016, 555.1794558057644, 439.35359339550877, 559.9834488753849, 529.0762821759981, 514.2565937358248, 478.4749127535146, 490.23529829855295, 486.43867504333906, 538.9724853392311, 541.8243629336752, 454.38314123909737, 560.4854628157761, 555.6717495363794, 563.36895348099, 477.6559187225549, 481.27529170940596, 538.4321371883186, 533.6018492746701, 525.4941959135484, 535.5654766157031, 540.1238076314605, 448.8598740236927, 560.3453839806665, 563.3976555770337, 518.6718366311186, 509.04921171766944, 547.102871778112, 535.131590990462, 458.91076492636864, 534.4929338482757, 530.6143127527934, 530.9870659946547, 531.8636060998577, 531.4067854432686, 536.0733893280395, 523.1090355568933, 368.28431982276805, 521.9344502082349, 461.6151941220675, 494.2530732909134, 534.0463896900062, 534.8887575383372, 521.0599125491128, 560.4594611742111, 515.9942252927945, 561.8264878441558, 543.915016626499, 215.464486966115, 552.3257646600402]
Elapsed: 0.11039400221629729~0.1970186462193771
Time per graph: 0.002252938820740761~0.0040207886983546345
Speed: 524.4136128240009~81.01705299976584
Total Time: 0.0900
best val loss: 0.30376601219177246 test_score: 0.9796

Testing...
Test loss: 0.2035 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [1.6468671369366348, 0.3625374410767108, 0.3446607287041843, 0.3508823369629681, 0.3606036838609725, 0.36298969807103276, 0.3916797188576311, 0.37404541322030127, 0.4218095289543271, 0.35315572493709624, 0.35673578176647425, 0.3355580123607069, 0.32381758093833923, 0.34335721214301884, 0.40835621021687984, 0.37103253905661404, 0.4127646731212735, 0.3285720518324524, 0.3519704029895365, 0.3680481552146375, 0.3486543719191104, 0.36052900715731084, 0.3294205910060555, 0.44964528689160943, 0.3643657409120351, 0.36292105820029974, 0.36129339318722486, 0.3596871639601886, 0.3661671478766948, 0.6826463029719889, 0.5047453679144382, 0.48902602097950876, 0.47005920903757215, 0.3907252917997539, 0.366746474057436, 0.4112151151057333, 0.5025370516814291, 0.35438661370426416, 0.34019099711440504, 0.31484659295529127, 0.3116508359089494, 0.3412766430992633, 0.35118512087501585, 0.3203880980145186, 0.32365983026102185, 0.3277209587395191, 0.3366870472673327, 0.3314850579481572, 0.353385770926252, 0.3044655539561063, 0.3045341409742832, 0.29658078122884035, 0.2944740492384881, 0.4024378666654229, 0.3069890718907118, 0.33451387216337025, 0.34206120134331286, 0.3166959087830037, 0.3189078269060701, 0.32396027096547186, 0.3176552599761635, 0.35139480885118246, 0.3557017759885639, 0.3201450798660517, 0.33116723597049713, 0.334884470095858, 0.3290913768578321, 0.3272226902190596, 0.3420929505955428, 0.33435797109268606, 0.3474288329016417, 0.37654860503971577, 0.33514810376800597, 0.33629975211806595, 0.38132777716964483, 4.934736586175859, 2.6449583880603313, 0.3323659591842443, 0.33420270308852196, 0.3755028247833252, 0.3224214508663863, 0.3143256369512528, 0.31103151896968484, 0.33049133606255054, 0.31024729483760893, 0.42670373409055173, 0.30975280003622174, 0.4988886397331953, 0.31787070678547025, 0.3217624758835882, 0.33461211808025837, 0.3120733411051333, 0.42808966618031263, 0.3359708893112838, 0.32973859715275466, 0.4316256493330002, 0.34111255616880953, 0.3346774810925126, 0.3174678520299494, 0.4332678900100291, 0.32320048310793936, 0.349380720872432, 0.33280800306238234, 0.3458822616375983, 0.3232066018972546, 0.32853517099283636, 0.4387583010829985, 0.3217755891382694, 0.32743463874794543, 0.3175818568561226, 0.32052710209973156, 0.3255198916886002, 0.3427574180532247, 0.33294892287813127, 0.46673375298269093, 0.3215505820699036, 0.34192029596306384, 0.3480273690074682, 0.33743947208859026, 0.33478575362823904, 0.3214109791442752, 0.44881214783526957, 0.3293634089641273, 0.32061013602651656, 0.3391737702768296, 0.32914240122772753, 0.33453374495729804, 0.3151097579393536, 0.4171629927586764, 0.3168445110786706, 0.3170872167684138, 0.32431772095151246, 0.32035361998714507, 0.318181409034878, 0.32250176509842277, 0.3442632502410561, 0.42108577815815806, 0.31959534296765924, 0.3502377748955041, 0.33160578389652073, 0.35447746119461954, 0.3295757200103253, 0.31307718716561794, 0.3474174439907074, 0.3204757629428059, 0.32392207300290465, 0.3267728630453348, 0.3283728687092662, 0.3142463231924921, 0.32584513910114765, 0.3008857660461217, 0.3364989091642201, 0.3337217168882489, 0.3061957322061062, 0.34094861312769353, 0.33240644610486925, 0.33795351115986705, 0.34164069476537406, 0.394276782637462, 0.31954616704024374, 0.3367699566297233, 0.3527671080082655, 0.3194517670199275, 0.3247833517380059, 0.318045194959268, 0.3231408742722124, 0.3762339698150754, 0.3249653750099242, 0.33733928203582764, 0.3713695900514722, 0.33764017559587955, 0.32593846833333373, 0.3227188987657428, 0.48592003411613405, 0.3216946462634951, 0.3275471639353782, 0.339575934689492, 0.33445127913728356, 0.3530180782545358, 0.3459909060038626, 0.3982119639404118, 0.33653374621644616, 0.3268450200557709, 0.3352506582159549, 0.5393593520857394, 0.34283977816812694, 0.3965936698950827, 0.46377471601590514, 0.32787263486534357, 0.33436236483976245, 0.32971241883933544, 0.31260556797496974, 0.32625557482242584, 0.3229276221245527, 0.32174919196404517, 0.32830481370911, 0.3116384190507233, 0.3308674667496234, 0.33602419518865645, 0.3390928921289742, 0.31809527589939535, 0.318866616114974, 0.29743430111557245, 0.30576892592944205, 0.30150343128480017, 0.33679333329200745, 0.3190713129006326, 0.48341950587928295, 0.3117968551814556, 0.3151617511175573, 0.3066605688072741, 0.35626407293602824, 0.3419248959980905, 0.31768706208094954, 0.32579530286602676, 0.3226159301120788, 0.3195519687142223, 0.31447100290097296, 0.3312310769688338, 0.33332077390514314, 0.32099627796560526, 0.44301877659745514, 0.3098689268808812, 0.46362223709002137, 0.3268974057864398, 0.33915742323733866, 0.3212234459351748, 0.3256098323035985, 0.3241121752653271, 0.32494653132744133, 0.32661836920306087, 0.39569193683564663, 0.30732294800691307, 0.30255943443626165, 0.2974578207358718, 0.32894459227100015, 0.3160535120405257, 0.3229030813090503, 0.3101215388160199, 0.35792749002575874, 0.3237544430885464, 0.3190615160856396, 0.3304854198358953, 0.3208736930973828, 0.3354801987297833, 0.3305049678310752, 0.36681159026920795, 0.35212023090571165, 0.3337019928731024, 0.3564072020817548, 0.35377576923929155, 0.3390311428811401, 0.32947858585976064, 0.4897268540225923, 0.3405740880407393, 0.31802593800239265, 0.32820741669274867, 0.3272103010676801, 0.3328960840590298, 0.32938775909133255, 0.3788847429677844, 0.308204643195495, 0.3127678958699107, 0.3091896758414805, 0.3205132931470871, 0.3254796650726348, 0.35638964967802167, 0.3389011637773365, 0.4276899192482233, 0.33167952485382557, 0.3225037697702646, 0.32579553220421076, 0.3253000369295478, 0.3212986937724054, 0.34334721812047064, 0.41502430802211165, 0.31693003699183464, 0.34896466019563377, 0.35949543537572026, 0.31386540620587766, 0.3251198809593916, 0.32050535921007395, 0.32562368595972657, 0.3758470139000565, 0.3153000446036458, 0.33573311590589583, 0.3383994069881737, 0.3357512338552624, 0.32786035910248756, 0.3256170868407935, 0.3542753567453474, 0.3188794357702136, 0.32267710380256176, 0.321560253854841, 0.3343553419690579, 0.32914014626294374, 6.3174513899721205, 0.5842817903030664, 0.31525842077098787, 0.42829459300264716, 0.3089941218495369, 0.3089463522192091, 0.30809602700173855, 0.3074867441318929, 0.305380400037393, 0.3073995718732476, 0.31662490777671337, 0.43061256501823664, 0.3123949831351638, 0.31002334295772016, 0.306510248221457, 0.31114858598448336, 0.3132651059422642, 0.29677155311219394, 0.44384762505069375, 0.30943772662431, 0.30851198686286807, 0.31137112295255065, 0.3289407747797668, 0.3460774119012058, 0.3234752328135073, 0.3418551650829613, 0.45722442329861224, 0.30815884098410606, 0.37888143700547516, 0.31895234412513673, 0.3151295662391931, 0.3330032422672957, 0.36659270082600415, 0.35981327877379954, 0.35335372295230627, 0.3270696171093732, 0.3123186598531902, 0.5250158330891281, 0.3198447530157864, 0.31803426682017744, 0.30921968491747975, 0.3404535478912294, 0.3499032137915492, 0.3245255737565458, 0.3119264147244394, 0.3168616278562695, 0.32044471125118434, 0.3175971021410078, 0.33683574735186994, 0.31072208704426885, 0.3090542189311236, 0.34465873916633427, 0.347028368152678, 0.31833501206710935, 0.31268232106231153, 0.34046002686955035, 0.3085485880728811, 0.3198970502708107, 0.3232050302904099, 0.3174084909260273, 0.32034864579327404, 0.3244651770219207, 0.3187113357707858, 0.3921713759191334, 0.42367313895374537, 0.3530024476349354, 0.34871961595490575, 0.32124227681197226, 0.31511024688370526, 0.32464348687790334, 0.31003272510133684, 0.3166051679290831, 0.31551403109915555, 0.32104796706698835, 0.4731155917979777, 0.29737632418982685]
Total Epoch List: [246, 126]
Total Time List: [0.0859517918433994, 0.09003407694399357]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d40bbdf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7182;  Loss pred: 0.7182; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.08s
Test loss: 0.6920 score: 0.5208 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
Test loss: 0.6896 score: 0.5208 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.08s
Test loss: 0.6862 score: 0.5208 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.15s
Val loss: 0.6872 score: 0.5306 time: 0.13s
Test loss: 0.6822 score: 0.5833 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.15s
Val loss: 0.6833 score: 0.5918 time: 0.08s
Test loss: 0.6774 score: 0.6875 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.15s
Val loss: 0.6787 score: 0.7143 time: 0.08s
Test loss: 0.6720 score: 0.8333 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 0.16s
Val loss: 0.6736 score: 0.8367 time: 0.08s
Test loss: 0.6659 score: 0.9167 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.15s
Val loss: 0.6677 score: 0.8367 time: 0.08s
Test loss: 0.6589 score: 0.9167 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.16s
Val loss: 0.6618 score: 0.7959 time: 0.09s
Test loss: 0.6520 score: 0.9167 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.16s
Val loss: 0.6558 score: 0.7551 time: 0.08s
Test loss: 0.6448 score: 0.8333 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.18s
Val loss: 0.6498 score: 0.7347 time: 0.12s
Test loss: 0.6376 score: 0.8125 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5076;  Loss pred: 0.5076; Loss self: 0.0000; time: 0.15s
Val loss: 0.6436 score: 0.7347 time: 0.08s
Test loss: 0.6302 score: 0.7708 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.4934;  Loss pred: 0.4934; Loss self: 0.0000; time: 0.15s
Val loss: 0.6371 score: 0.6939 time: 0.08s
Test loss: 0.6227 score: 0.7500 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4667;  Loss pred: 0.4667; Loss self: 0.0000; time: 0.15s
Val loss: 0.6306 score: 0.6531 time: 0.08s
Test loss: 0.6153 score: 0.7292 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4383;  Loss pred: 0.4383; Loss self: 0.0000; time: 0.16s
Val loss: 0.6242 score: 0.6735 time: 0.09s
Test loss: 0.6080 score: 0.7083 time: 0.10s
Epoch 18/1000, LR 0.000270
Train loss: 0.4261;  Loss pred: 0.4261; Loss self: 0.0000; time: 0.16s
Val loss: 0.6179 score: 0.6531 time: 0.08s
Test loss: 0.6007 score: 0.7083 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4048;  Loss pred: 0.4048; Loss self: 0.0000; time: 0.16s
Val loss: 0.6120 score: 0.6531 time: 0.08s
Test loss: 0.5939 score: 0.6667 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4044;  Loss pred: 0.4044; Loss self: 0.0000; time: 0.16s
Val loss: 0.6059 score: 0.6531 time: 0.08s
Test loss: 0.5868 score: 0.6250 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3982;  Loss pred: 0.3982; Loss self: 0.0000; time: 0.17s
Val loss: 0.5998 score: 0.6735 time: 0.10s
Test loss: 0.5796 score: 0.6458 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 0.15s
Val loss: 0.5944 score: 0.6735 time: 0.08s
Test loss: 0.5730 score: 0.6458 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.3659;  Loss pred: 0.3659; Loss self: 0.0000; time: 0.16s
Val loss: 0.5889 score: 0.6735 time: 0.09s
Test loss: 0.5661 score: 0.6458 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.3489;  Loss pred: 0.3489; Loss self: 0.0000; time: 0.16s
Val loss: 0.5834 score: 0.6735 time: 0.08s
Test loss: 0.5594 score: 0.6458 time: 0.10s
Epoch 25/1000, LR 0.000270
Train loss: 0.3525;  Loss pred: 0.3525; Loss self: 0.0000; time: 0.17s
Val loss: 0.5780 score: 0.6735 time: 0.08s
Test loss: 0.5528 score: 0.6667 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3300;  Loss pred: 0.3300; Loss self: 0.0000; time: 0.15s
Val loss: 0.5713 score: 0.6735 time: 0.09s
Test loss: 0.5450 score: 0.7083 time: 0.10s
Epoch 27/1000, LR 0.000270
Train loss: 0.3220;  Loss pred: 0.3220; Loss self: 0.0000; time: 0.16s
Val loss: 0.5648 score: 0.6939 time: 0.08s
Test loss: 0.5373 score: 0.7083 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.15s
Val loss: 0.5576 score: 0.6735 time: 0.09s
Test loss: 0.5289 score: 0.7083 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.3174;  Loss pred: 0.3174; Loss self: 0.0000; time: 0.15s
Val loss: 0.5494 score: 0.6735 time: 0.09s
Test loss: 0.5196 score: 0.7292 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.2812;  Loss pred: 0.2812; Loss self: 0.0000; time: 0.15s
Val loss: 0.5420 score: 0.6939 time: 0.08s
Test loss: 0.5112 score: 0.7500 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.15s
Val loss: 0.5354 score: 0.6939 time: 0.08s
Test loss: 0.5035 score: 0.7708 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.2607;  Loss pred: 0.2607; Loss self: 0.0000; time: 0.16s
Val loss: 0.5303 score: 0.6939 time: 0.08s
Test loss: 0.4973 score: 0.7917 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.2862;  Loss pred: 0.2862; Loss self: 0.0000; time: 0.14s
Val loss: 0.5247 score: 0.6939 time: 0.08s
Test loss: 0.4904 score: 0.7917 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.2655;  Loss pred: 0.2655; Loss self: 0.0000; time: 0.15s
Val loss: 0.5200 score: 0.6939 time: 0.08s
Test loss: 0.4842 score: 0.7917 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2541;  Loss pred: 0.2541; Loss self: 0.0000; time: 0.15s
Val loss: 0.5144 score: 0.6939 time: 0.10s
Test loss: 0.4770 score: 0.7917 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 0.17s
Val loss: 0.5096 score: 0.6939 time: 0.13s
Test loss: 0.4706 score: 0.8125 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.2314;  Loss pred: 0.2314; Loss self: 0.0000; time: 0.15s
Val loss: 0.5059 score: 0.6939 time: 0.08s
Test loss: 0.4655 score: 0.8333 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.15s
Val loss: 0.5036 score: 0.6939 time: 0.08s
Test loss: 0.4619 score: 0.8333 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2282;  Loss pred: 0.2282; Loss self: 0.0000; time: 0.15s
Val loss: 0.5008 score: 0.6939 time: 0.08s
Test loss: 0.4578 score: 0.8333 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2309;  Loss pred: 0.2309; Loss self: 0.0000; time: 0.15s
Val loss: 0.4994 score: 0.6939 time: 0.08s
Test loss: 0.4550 score: 0.8333 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.15s
Val loss: 0.4997 score: 0.6939 time: 0.08s
Test loss: 0.4538 score: 0.8333 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.2006;  Loss pred: 0.2006; Loss self: 0.0000; time: 0.15s
Val loss: 0.5035 score: 0.6939 time: 0.08s
Test loss: 0.4557 score: 0.8125 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1969;  Loss pred: 0.1969; Loss self: 0.0000; time: 0.15s
Val loss: 0.5049 score: 0.6939 time: 0.08s
Test loss: 0.4558 score: 0.8125 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1935;  Loss pred: 0.1935; Loss self: 0.0000; time: 0.27s
Val loss: 0.5080 score: 0.6939 time: 0.08s
Test loss: 0.4575 score: 0.8125 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1911;  Loss pred: 0.1911; Loss self: 0.0000; time: 0.15s
Val loss: 0.5088 score: 0.6939 time: 0.08s
Test loss: 0.4574 score: 0.7708 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1832;  Loss pred: 0.1832; Loss self: 0.0000; time: 0.16s
Val loss: 0.5104 score: 0.6735 time: 0.08s
Test loss: 0.4582 score: 0.7708 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1787;  Loss pred: 0.1787; Loss self: 0.0000; time: 0.15s
Val loss: 0.5097 score: 0.6735 time: 0.08s
Test loss: 0.4570 score: 0.7708 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1818;  Loss pred: 0.1818; Loss self: 0.0000; time: 0.16s
Val loss: 0.5047 score: 0.6939 time: 0.08s
Test loss: 0.4514 score: 0.7917 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1755;  Loss pred: 0.1755; Loss self: 0.0000; time: 0.15s
Val loss: 0.4981 score: 0.6939 time: 0.08s
Test loss: 0.4441 score: 0.8125 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.1685;  Loss pred: 0.1685; Loss self: 0.0000; time: 0.16s
Val loss: 0.4915 score: 0.6939 time: 0.08s
Test loss: 0.4369 score: 0.8333 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.1568;  Loss pred: 0.1568; Loss self: 0.0000; time: 0.16s
Val loss: 0.4868 score: 0.6939 time: 0.15s
Test loss: 0.4316 score: 0.8333 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.1605;  Loss pred: 0.1605; Loss self: 0.0000; time: 0.15s
Val loss: 0.4791 score: 0.7143 time: 0.08s
Test loss: 0.4235 score: 0.8333 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1546;  Loss pred: 0.1546; Loss self: 0.0000; time: 0.16s
Val loss: 0.4714 score: 0.6939 time: 0.09s
Test loss: 0.4154 score: 0.8333 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.1586;  Loss pred: 0.1586; Loss self: 0.0000; time: 0.15s
Val loss: 0.4665 score: 0.7143 time: 0.08s
Test loss: 0.4097 score: 0.8542 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.15s
Val loss: 0.4634 score: 0.7143 time: 0.08s
Test loss: 0.4064 score: 0.8542 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1432;  Loss pred: 0.1432; Loss self: 0.0000; time: 0.15s
Val loss: 0.4617 score: 0.7143 time: 0.08s
Test loss: 0.4038 score: 0.8542 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1415;  Loss pred: 0.1415; Loss self: 0.0000; time: 0.16s
Val loss: 0.4646 score: 0.7347 time: 0.08s
Test loss: 0.4051 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.1371;  Loss pred: 0.1371; Loss self: 0.0000; time: 0.15s
Val loss: 0.4695 score: 0.7143 time: 0.08s
Test loss: 0.4077 score: 0.8542 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.1353;  Loss pred: 0.1353; Loss self: 0.0000; time: 0.26s
Val loss: 0.4716 score: 0.7143 time: 0.08s
Test loss: 0.4077 score: 0.8333 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.1340;  Loss pred: 0.1340; Loss self: 0.0000; time: 0.15s
Val loss: 0.4678 score: 0.7143 time: 0.08s
Test loss: 0.4022 score: 0.8542 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.1358;  Loss pred: 0.1358; Loss self: 0.0000; time: 0.16s
Val loss: 0.4605 score: 0.7143 time: 0.08s
Test loss: 0.3933 score: 0.8542 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 0.15s
Val loss: 0.4510 score: 0.7551 time: 0.08s
Test loss: 0.3818 score: 0.8542 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 0.16s
Val loss: 0.4419 score: 0.7959 time: 0.08s
Test loss: 0.3712 score: 0.8542 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.16s
Val loss: 0.4325 score: 0.8163 time: 0.09s
Test loss: 0.3605 score: 0.8542 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1137;  Loss pred: 0.1137; Loss self: 0.0000; time: 0.15s
Val loss: 0.4285 score: 0.8163 time: 0.08s
Test loss: 0.3555 score: 0.8542 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1235;  Loss pred: 0.1235; Loss self: 0.0000; time: 0.16s
Val loss: 0.4262 score: 0.8163 time: 0.09s
Test loss: 0.3519 score: 0.8542 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.1169;  Loss pred: 0.1169; Loss self: 0.0000; time: 0.16s
Val loss: 0.4276 score: 0.8163 time: 0.08s
Test loss: 0.3515 score: 0.8542 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.1110;  Loss pred: 0.1110; Loss self: 0.0000; time: 0.15s
Val loss: 0.4296 score: 0.8163 time: 0.08s
Test loss: 0.3512 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.15s
Val loss: 0.4301 score: 0.8163 time: 0.10s
Test loss: 0.3495 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 0.16s
Val loss: 0.4258 score: 0.8163 time: 0.08s
Test loss: 0.3445 score: 0.8542 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.16s
Val loss: 0.4188 score: 0.8163 time: 0.08s
Test loss: 0.3364 score: 0.8542 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.15s
Val loss: 0.4025 score: 0.8163 time: 0.08s
Test loss: 0.3203 score: 0.8542 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.15s
Val loss: 0.3918 score: 0.8367 time: 0.22s
Test loss: 0.3095 score: 0.8958 time: 0.10s
Epoch 74/1000, LR 0.000267
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.16s
Val loss: 0.3879 score: 0.8367 time: 0.08s
Test loss: 0.3042 score: 0.8958 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.16s
Val loss: 0.3887 score: 0.8367 time: 0.08s
Test loss: 0.3028 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 0.15s
Val loss: 0.3835 score: 0.8367 time: 0.08s
Test loss: 0.2946 score: 0.8958 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0895;  Loss pred: 0.0895; Loss self: 0.0000; time: 0.15s
Val loss: 0.3787 score: 0.8367 time: 0.08s
Test loss: 0.2854 score: 0.9375 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0864;  Loss pred: 0.0864; Loss self: 0.0000; time: 0.15s
Val loss: 0.3748 score: 0.8367 time: 0.08s
Test loss: 0.2793 score: 0.9375 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.15s
Val loss: 0.3739 score: 0.8367 time: 0.08s
Test loss: 0.2748 score: 0.9167 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.15s
Val loss: 0.3668 score: 0.8571 time: 0.08s
Test loss: 0.2661 score: 0.9167 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.17s
Val loss: 0.3553 score: 0.8571 time: 0.11s
Test loss: 0.2577 score: 0.9167 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.15s
Val loss: 0.3547 score: 0.8571 time: 0.08s
Test loss: 0.2547 score: 0.9167 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.16s
Val loss: 0.3583 score: 0.8571 time: 0.08s
Test loss: 0.2541 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.14s
Val loss: 0.3515 score: 0.8571 time: 0.08s
Test loss: 0.2438 score: 0.9167 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.15s
Val loss: 0.3561 score: 0.8571 time: 0.08s
Test loss: 0.2431 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.15s
Val loss: 0.3672 score: 0.8367 time: 0.09s
Test loss: 0.2505 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.17s
Val loss: 0.3671 score: 0.8367 time: 0.08s
Test loss: 0.2486 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.15s
Val loss: 0.3554 score: 0.8571 time: 0.08s
Test loss: 0.2371 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.26s
Val loss: 0.3473 score: 0.8571 time: 0.08s
Test loss: 0.2271 score: 0.9167 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 0.15s
Val loss: 0.3459 score: 0.8571 time: 0.08s
Test loss: 0.2220 score: 0.9167 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0696;  Loss pred: 0.0696; Loss self: 0.0000; time: 0.15s
Val loss: 0.3412 score: 0.8571 time: 0.08s
Test loss: 0.2160 score: 0.9375 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.15s
Val loss: 0.3372 score: 0.8571 time: 0.08s
Test loss: 0.2109 score: 0.9375 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.15s
Val loss: 0.3386 score: 0.8571 time: 0.08s
Test loss: 0.2078 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.15s
Val loss: 0.3422 score: 0.8571 time: 0.08s
Test loss: 0.2062 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.15s
Val loss: 0.3442 score: 0.8571 time: 0.08s
Test loss: 0.2036 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.15s
Val loss: 0.3356 score: 0.8776 time: 0.20s
Test loss: 0.1961 score: 0.9375 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.15s
Val loss: 0.3307 score: 0.8776 time: 0.08s
Test loss: 0.1920 score: 0.9167 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.15s
Val loss: 0.3275 score: 0.8776 time: 0.08s
Test loss: 0.1890 score: 0.9167 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.15s
Val loss: 0.3335 score: 0.8776 time: 0.08s
Test loss: 0.1872 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.15s
Val loss: 0.3358 score: 0.8776 time: 0.08s
Test loss: 0.1851 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.15s
Val loss: 0.3381 score: 0.8776 time: 0.08s
Test loss: 0.1828 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.16s
Val loss: 0.3331 score: 0.8571 time: 0.08s
Test loss: 0.1808 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.14s
Val loss: 0.3343 score: 0.8776 time: 0.08s
Test loss: 0.1782 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.15s
Val loss: 0.3399 score: 0.8776 time: 0.23s
Test loss: 0.1762 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.15s
Val loss: 0.3400 score: 0.8776 time: 0.08s
Test loss: 0.1749 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.15s
Val loss: 0.3339 score: 0.8571 time: 0.08s
Test loss: 0.1731 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.15s
Val loss: 0.3359 score: 0.8571 time: 0.08s
Test loss: 0.1728 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.15s
Val loss: 0.3405 score: 0.8571 time: 0.08s
Test loss: 0.1720 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.15s
Val loss: 0.3458 score: 0.8571 time: 0.08s
Test loss: 0.1709 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.15s
Val loss: 0.3468 score: 0.8571 time: 0.08s
Test loss: 0.1703 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.15s
Val loss: 0.3421 score: 0.8776 time: 0.09s
Test loss: 0.1725 score: 0.9375 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.15s
Val loss: 0.3436 score: 0.8980 time: 0.10s
Test loss: 0.1728 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.16s
Val loss: 0.3510 score: 0.8571 time: 0.08s
Test loss: 0.1701 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.16s
Val loss: 0.3500 score: 0.8776 time: 0.08s
Test loss: 0.1694 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.15s
Val loss: 0.3561 score: 0.8571 time: 0.08s
Test loss: 0.1676 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.15s
Val loss: 0.3579 score: 0.8571 time: 0.08s
Test loss: 0.1677 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.16s
Val loss: 0.3516 score: 0.8776 time: 0.08s
Test loss: 0.1685 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.15s
Val loss: 0.3507 score: 0.8980 time: 0.08s
Test loss: 0.1682 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.0571,   Val_Loss: 0.3275,   Val_Precision: 0.8800,   Val_Recall: 0.8800,   Val_accuracy: 0.8800,   Val_Score: 0.8776,   Val_Loss: 0.3275,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9167,   Test_loss: 0.1890


[0.2394612398929894, 0.09545059502124786, 0.09643882303498685, 0.09361927886493504, 0.1004986569751054, 0.10370176495052874, 0.10904351901262999, 0.12588524306192994, 0.10665453900583088, 0.10348750092089176, 0.10505772102624178, 0.08799063391052186, 0.09275238704867661, 0.0965933590196073, 0.12278804602101445, 0.10502863698638976, 0.10589498491026461, 0.09529308113269508, 0.10316031705588102, 0.10280511411838233, 0.09568627900443971, 0.10198194393888116, 0.08405745588243008, 0.10280540003441274, 0.09598211501725018, 0.10248268395662308, 0.09528246708214283, 0.09724306012503803, 0.09967087605036795, 0.21277527301572263, 0.23618996515870094, 0.23367126216180623, 0.2087712469510734, 0.09462057007476687, 0.11106297606602311, 0.1028166098985821, 0.09246257110498846, 0.09126918110996485, 0.09118396299891174, 0.078358432976529, 0.0819196819793433, 0.09542880300432444, 0.09598940191790462, 0.088874327018857, 0.0872696558944881, 0.09007284184917808, 0.09695405419915915, 0.08869276591576636, 0.09139166004024446, 0.0798952009063214, 0.07884702598676085, 0.07728837616741657, 0.07853528601117432, 0.09089345601387322, 0.07991645601578057, 0.10195423197001219, 0.08559147105552256, 0.08758446108549833, 0.08273334591649473, 0.08972880197688937, 0.09071883698925376, 0.1075486580375582, 0.09110534703359008, 0.08778659580275416, 0.08685945789329708, 0.0889496048912406, 0.08693338301964104, 0.08762196497991681, 0.08902284409850836, 0.08839694713242352, 0.0992884139996022, 0.11004513711668551, 0.09017838607542217, 0.09191150404512882, 0.08700799779035151, 3.129479216877371, 0.08843301702290773, 0.0944723158609122, 0.08645662991330028, 0.0835436200723052, 0.08425115118734539, 0.08253813907504082, 0.08085088105872273, 0.08504074811935425, 0.08285718504339457, 0.20892707398161292, 0.08619164698757231, 0.08630726905539632, 0.08724456909112632, 0.08509374316781759, 0.0829878649674356, 0.08191635506227612, 0.1979307031724602, 0.08643840509466827, 0.08463943377137184, 0.1988642259966582, 0.07945046504028141, 0.08956967992708087, 0.08241633116267622, 0.09186432301066816, 0.08360415301285684, 0.09717888804152608, 0.0900319879874587, 0.08704181993380189, 0.09471106203272939, 0.09132047882303596, 0.17789843794889748, 0.08340414194390178, 0.09120183112099767, 0.08294900297187269, 0.08324207901023328, 0.09168130112811923, 0.09330852003768086, 0.08687654114328325, 0.08763326914049685, 0.08774863881990314, 0.08660778007470071, 0.09915313287638128, 0.09602722013369203, 0.09168196399696171, 0.09127156902104616, 0.08618036098778248, 0.09319070004858077, 0.08927634102292359, 0.08745685615576804, 0.09235594794154167, 0.09581388998776674, 0.08467029593884945, 0.08327589393593371, 0.08608053205534816, 0.0847567121963948, 0.0854678291361779, 0.08420614106580615, 0.08527310402132571, 0.08849721401929855, 0.10325196292251348, 0.08862095605581999, 0.0875789790879935, 0.08554799715057015, 0.08519505988806486, 0.09864136087708175, 0.08249109983444214, 0.09880341100506485, 0.083911461988464, 0.08731775707565248, 0.08498109178617597, 0.0885558039881289, 0.09119477402418852, 0.0821380119305104, 0.07915899017825723, 0.08197101694531739, 0.08587450091727078, 0.08725975407287478, 0.08066029800102115, 0.1002293189521879, 0.08538345689885318, 0.08471331396140158, 0.08634182205423713, 0.15393957099877298, 0.09633121686056256, 0.08731711190193892, 0.10084254713729024, 0.08991986815817654, 0.08281821990385652, 0.08672154787927866, 0.10763410618528724, 0.08480639988556504, 0.08878928888589144, 0.09049938898533583, 0.11246427497826517, 0.09297169395722449, 0.08185418089851737, 0.09106800495646894, 0.10219845501706004, 0.08633057097904384, 0.08831665385514498, 0.08613896812312305, 0.08809350593946874, 0.10732985800132155, 0.09506871202029288, 0.0787265570834279, 0.08879346493631601, 0.09304943610914052, 0.0914131950121373, 0.22475219098851085, 0.10402209591120481, 0.08798702014610171, 0.09114812081679702, 0.08622777415439487, 0.08934874599799514, 0.08569853915832937, 0.08383411401882768, 0.08579481905326247, 0.08624948980286717, 0.08515932015143335, 0.08432129304856062, 0.08481513406150043, 0.10486973193474114, 0.0953565330710262, 0.08757792599499226, 0.0828182059340179, 0.08940726215951145, 0.08434918499551713, 0.09780599712394178, 0.08096312102861702, 0.08526447601616383, 0.0802910819184035, 0.24515607999637723, 0.08581281290389597, 0.08389152213931084, 0.083328538807109, 0.0958866011351347, 0.08837056998163462, 0.0873304340057075, 0.08362050587311387, 0.08847036492079496, 0.0881443137768656, 0.08280913811177015, 0.08513349900022149, 0.10327559593133628, 0.08089207299053669, 0.08249981212429702, 0.07974560908041894, 0.2153674610890448, 0.0865905589889735, 0.09498741896823049, 0.08621739689260721, 0.08553462103009224, 0.08461728296242654, 0.09195353300310671, 0.08685902017168701, 0.08710444020107388, 0.08146408596076071, 0.08210279699414968, 0.07909905700944364, 0.08206697390414774, 0.08727898309007287, 0.08774171187542379, 0.08193140220828354, 0.08939554402604699, 0.08232937986031175, 0.08919593412429094, 0.087561507942155, 0.08683314686641097, 0.08949702279642224, 0.08501844992861152, 0.09377385582774878, 0.1071053131017834, 0.09638045099563897, 0.10971526009961963, 0.10653248685412109, 0.10048661706969142, 0.092984575079754, 0.11306942021474242, 0.09685623389668763, 0.09618054190650582, 0.09484837995842099, 0.09866203600540757, 0.0962987020611763, 0.09290683595463634, 0.08964207512326539, 0.08864587405696511, 0.08757643192075193, 0.0886092078872025, 0.08993486501276493, 0.09432363510131836, 0.10444240900687873, 0.10341274295933545, 0.09494237089529634, 0.09439187194220722, 0.09262269199825823, 0.09415961592458189, 0.08907612087205052, 0.09581966581754386, 0.10124349896796048, 0.09503383305855095, 0.08971589081920683, 0.09559466899372637, 0.10706321895122528, 0.09037789003923535, 0.09459559619426727, 0.09123083995655179, 0.09775713598355651, 0.08627334609627724, 0.09496837807819247, 0.09675388014875352, 0.09539868705905974, 0.09699613507837057, 0.09559524105861783, 0.09462325600907207, 0.09433700609952211, 0.09245835919864476, 0.09115956909954548, 0.09441674803383648, 0.08968709711916745, 0.10313707403838634, 2.3511948930099607, 0.08931005699560046, 0.09250502986833453, 0.09291360597126186, 0.08753556990996003, 0.08644743403419852, 0.08849788084626198, 0.08786026411689818, 0.08872468397021294, 0.08808946399949491, 0.08852309198118746, 0.08741566282697022, 0.0907421549782157, 0.08821761701256037, 0.08866588003002107, 0.09078714507631958, 0.088500265032053, 0.08714445098303258, 0.08826917107217014, 0.0885246298275888, 0.08922947198152542, 0.09116909001022577, 0.09239553101360798, 0.0968009380158037, 0.09209364908747375, 0.10452739405445755, 0.10435785911977291, 0.08825975004583597, 0.11152748204767704, 0.08750258618965745, 0.09261424420401454, 0.09528317302465439, 0.10240871296264231, 0.09995200298726559, 0.10073212208226323, 0.09091373183764517, 0.09043520991690457, 0.10783850797452033, 0.08742421213537455, 0.08818155690096319, 0.0869767488911748, 0.10258430405519903, 0.10181283112615347, 0.09100496908649802, 0.09182876721024513, 0.09324555890634656, 0.09149208106100559, 0.09071994107216597, 0.10916547197848558, 0.08744606701657176, 0.08697231789119542, 0.09447206603363156, 0.09625788405537605, 0.08956268103793263, 0.09156626299954951, 0.1067745708860457, 0.0916756740771234, 0.09234579396434128, 0.09228096716105938, 0.09212888311594725, 0.09220808115787804, 0.09140539518557489, 0.09367072000168264, 0.13304937887005508, 0.09388152090832591, 0.10614901897497475, 0.09913949482142925, 0.09175232890993357, 0.09160783304832876, 0.09403908997774124, 0.087428268045187, 0.09496230306103826, 0.08721553906798363, 0.09008760284632444, 0.22741566691547632, 0.08871576003730297, 0.08641513390466571, 0.08504298888146877, 0.08904120093211532, 0.084835923044011, 0.08307801885530353, 0.08796215197071433, 0.08470051782205701, 0.08990358794108033, 0.09478123299777508, 0.09142093197442591, 0.08962909504771233, 0.09914968581870198, 0.08853345504030585, 0.08919577091000974, 0.08599340100772679, 0.08616292313672602, 0.10247077816165984, 0.08882053405977786, 0.08766057388857007, 0.08855578699149191, 0.08432780089788139, 0.08828192297369242, 0.08766533597372472, 0.1054498478770256, 0.08741900091990829, 0.10029405285604298, 0.0897119368892163, 0.2134324659127742, 0.08386353496462107, 0.08336913003586233, 0.08431266993284225, 0.08259995095431805, 0.08367911796085536, 0.0844716529827565, 0.0987656069919467, 0.08407480898313224, 0.08376680896617472, 0.08354410505853593, 0.08759113494306803, 0.08660957007668912, 0.08474560896866024, 0.08324133907444775, 0.08500061579979956, 0.08734522690065205, 0.08988685207441449, 0.089745539939031, 0.09114738809876144, 0.08692355989478528, 0.08930963999591768, 0.0891915641259402, 0.08705534692853689, 0.09157253708690405, 0.09137636912055314, 0.08859989419579506, 0.0889387740753591, 0.09085073182359338, 0.08669656794518232, 0.09384137904271483, 0.08998151402920485, 0.09800207684747875, 0.08752794307656586, 0.08952063182368875, 0.0884153270162642, 0.08860671194270253, 0.08970619505271316, 0.21313098492100835, 0.09122988302260637, 0.08623139513656497, 0.08886190713383257, 0.0887345049995929, 0.08352786302566528, 0.08292865613475442, 0.10509921098127961, 0.08608317095786333, 0.08340331097133458, 0.08320828108116984, 0.08463150495663285, 0.0884409889113158, 0.08859399985522032, 0.08872104110196233, 0.08997980295680463, 0.08309222897514701, 0.08625778811983764, 0.08437822689302266, 0.08660442801192403, 0.09527395595796406, 0.09704184299334884, 0.09401189908385277, 0.08823452680371702, 0.08779695490375161, 0.08372044400312006, 0.08590430207550526, 0.08355353493243456, 0.0834840890020132, 0.09575590211898088, 0.08572010602802038, 0.08220721990801394, 0.0846445329952985, 0.08533711591735482, 0.08757514110766351, 0.08707874384708703, 0.08939712005667388, 0.08885557181201875, 0.08199282200075686, 0.08199262293055654, 0.08326780097559094, 0.08299926901236176, 0.0982219900470227, 0.08423061203211546, 0.09027518006041646, 0.22883803397417068, 0.08984057395718992, 0.09012267808429897, 0.08726482093334198, 0.08694930607452989, 0.08645747508853674, 0.0862845960073173, 0.08270089188590646]
[0.004886964079448763, 0.001947971326964242, 0.0019681392456119768, 0.0019105975278558172, 0.002050992999491947, 0.0021163625500107905, 0.002225377939033265, 0.002569086593100611, 0.0021766232450169567, 0.0021119898147120768, 0.002144035122984526, 0.0017957272226637115, 0.0018929058581362574, 0.001971293041216476, 0.002505878490224785, 0.0021434415711508114, 0.0021611221410258083, 0.0019447567578101037, 0.0021053125929771637, 0.0020980635534363743, 0.001952781204172239, 0.002081264162017983, 0.0017154582833148995, 0.0020980693884574027, 0.0019588186738214324, 0.002091483346053532, 0.001944540144533527, 0.0019845522474497557, 0.002034099511231999, 0.004342352510524952, 0.0048202033705857334, 0.004768801268608291, 0.0042606376928790496, 0.001931032042342181, 0.002266591348286186, 0.002098298161195553, 0.0018869912470405807, 0.001862636349182956, 0.0018608972040594233, 0.001599151693398551, 0.0016718302444763938, 0.0019475265919249884, 0.001958967386079686, 0.0018137617758950408, 0.0017810133856017978, 0.0018382212622281239, 0.0019786541673297783, 0.0018100564472605378, 0.0018651359191886624, 0.0016305143042106408, 0.0016091229793216502, 0.001577313799335032, 0.0016027609390035576, 0.0018549684900790453, 0.0016309480819547055, 0.0020806986116329016, 0.0017467647154188277, 0.0017874379813367007, 0.001688435630948872, 0.001831200040344681, 0.0018514048365153829, 0.0021948705721950654, 0.0018592927966038792, 0.0017915631796480442, 0.0017726419978223893, 0.0018152980590049102, 0.0017741506738702254, 0.0017882033669370779, 0.0018167927367042523, 0.001804019329233133, 0.002026294163257188, 0.0022458191248303167, 0.0018403752260290238, 0.0018757449805128332, 0.001775673424292888, 0.06386692279341574, 0.0018047554494470966, 0.0019280064461410654, 0.0017644210186387812, 0.0017049718382103102, 0.0017194112487213344, 0.0016844518178579758, 0.00165001798079026, 0.001735525471823556, 0.0016909629600692767, 0.0042638178363594475, 0.0017590132038280064, 0.001761372837865231, 0.0017805014100229861, 0.0017366070034248488, 0.001693629897294604, 0.0016717623482097167, 0.004039402105560412, 0.0017640490835646586, 0.0017273353830892214, 0.004058453591768534, 0.0016214380620465595, 0.001827952651573079, 0.001681965942095433, 0.0018747821022585339, 0.0017062072043440171, 0.001983242613092369, 0.0018373875099481369, 0.0017763636721184058, 0.0019328788169944773, 0.0018636832412864482, 0.00363058036630403, 0.0017021253457939138, 0.0018612618596121973, 0.0016928367953443406, 0.0016988179389843528, 0.0018710469617983516, 0.0019042555109730788, 0.0017729906355772092, 0.0017884340640917725, 0.0017907885473449619, 0.0017675057158102186, 0.002023533324007781, 0.001959739186401878, 0.0018710604897339124, 0.0018626850820621665, 0.0017587828773016833, 0.0019018510213996075, 0.0018219661433249712, 0.001784833799097307, 0.0018848152641130953, 0.0019553855099544233, 0.0017279652232418256, 0.0016995080395088512, 0.0017567455521499623, 0.0017297288203345878, 0.001744241410942406, 0.0017184926748123703, 0.001740267429006647, 0.0018060655922305826, 0.0021071829167859896, 0.0018085909399146937, 0.0017873261038366022, 0.0017458774928687786, 0.0017386746915931605, 0.002013088997491464, 0.001683491833355962, 0.002016396142960507, 0.001712478816091102, 0.0017819950423602546, 0.0017343079956362443, 0.0018072613058801815, 0.001861117837228337, 0.0016762859577655184, 0.0016154895954746374, 0.001672877896843212, 0.0017525408350463425, 0.0017808113076096894, 0.0016461285306330845, 0.0020454963051466917, 0.0017425195285480242, 0.0017288431420694201, 0.0017620780011068803, 0.0031416238979341425, 0.0019659432012359705, 0.0017819818755497737, 0.0020580111660671477, 0.0018350993501668681, 0.001690167753139929, 0.0017698275077403808, 0.0021966144119446376, 0.0017307428548074498, 0.0018120263037937029, 0.0018469263058231802, 0.0022951892852707176, 0.001897381509331112, 0.0016704934877248443, 0.0018585307133973253, 0.0020856827554502047, 0.0017618483873274252, 0.0018023806909213261, 0.001757938124961695, 0.0017978266518258927, 0.002190405265333093, 0.0019401777963325077, 0.0016066644302740389, 0.0018121115293125718, 0.0018989680838600105, 0.0018655754084109652, 0.004586779407928793, 0.0021228999165552004, 0.0017956534723694228, 0.0018601657309550413, 0.001759750492946834, 0.0018234437958774517, 0.0017489497787414156, 0.001710900286098524, 0.001750914674556377, 0.0017601936694462688, 0.0017379453092129255, 0.0017208427152767473, 0.0017309211032959272, 0.0021401986109130843, 0.0019460516953270655, 0.0017873046121426991, 0.0016901674680411816, 0.0018246380032553356, 0.0017214119386840233, 0.001996040757631465, 0.0016523085924207556, 0.0017400913472686494, 0.0016385935085388471, 0.005003185306048515, 0.001751281895997877, 0.0017120718803940987, 0.0017005824246348776, 0.001956869410921116, 0.0018034810200333595, 0.0017822537552185205, 0.0017065409361859973, 0.0018055176514447952, 0.0017988635464666449, 0.0016899824104442888, 0.0017374183469432958, 0.0021076652230884955, 0.0016508586324599324, 0.001683669635189735, 0.0016274614098044683, 0.00439525430793969, 0.0017671542650810918, 0.0019385187544536833, 0.0017595387120940247, 0.001745604510818209, 0.001726883325763807, 0.0018766027143491165, 0.0017726330647283063, 0.0017776416367566098, 0.001662532366546137, 0.0016755672855948915, 0.0016142664695804824, 0.0016748362021254642, 0.0017812037365320995, 0.0017906471811310978, 0.0016720694328221132, 0.0018243988576744283, 0.001680191425720648, 0.0018203251862100192, 0.001786969549839898, 0.0017721050380900198, 0.001826469852988209, 0.0017350704067063574, 0.001913752159749975, 0.0021858227163629265, 0.001966947979502836, 0.002239086940808564, 0.0021741323847779812, 0.0020507472871365597, 0.0018976443893827346, 0.002307539188055968, 0.001976657834626278, 0.001962868202173588, 0.0019356812236412447, 0.002013510938885869, 0.001965279633901557, 0.0018960578766252314, 0.0018294301045564364, 0.0018090994705503083, 0.001787274120831672, 0.0018083511813714796, 0.001835405408423774, 0.0019249721449248645, 0.00213147773483426, 0.002110464142027254, 0.0019375994060264559, 0.001926364733514433, 0.001890259020372617, 0.0019216248147873854, 0.0018178800177969495, 0.0019555033840315075, 0.002066193856488989, 0.0019394659807867541, 0.0018309365473307517, 0.0019509116121168648, 0.002184963652065822, 0.001844446735494599, 0.001930522371311577, 0.0018618538766643222, 0.0019950435915011533, 0.0017606805325770865, 0.001938130164861071, 0.001974568982627623, 0.0019469119807971375, 0.001979512960783073, 0.001950923286910568, 0.0019310868573280014, 0.0019252450224392268, 0.0018869052897682603, 0.0018603993693784792, 0.001926872408853806, 0.0018303489207993357, 0.002104838245681354, 0.047983569245101235, 0.0018226542244000094, 0.0018878577524149905, 0.001896196040229834, 0.0017864402022440822, 0.0017642333476367046, 0.001806079200944122, 0.001793066614630575, 0.0018107078361267947, 0.0017977441632549983, 0.001806593713901785, 0.0017839931189177595, 0.0018518807138411366, 0.001800359530868579, 0.0018095077557147158, 0.0018527988791085628, 0.001806127857797, 0.0017784581833271956, 0.0018014116545340844, 0.0018066250985222204, 0.001821009632276029, 0.001860593673678077, 0.0018856230819103668, 0.0019755293472612998, 0.0018794622262749746, 0.002133212123560358, 0.002129752226934141, 0.00180121938869053, 0.002276071062197491, 0.00178576706509505, 0.0018900866164084599, 0.0019445545515235588, 0.0020899737339314757, 0.0020398367956584815, 0.002055757593515576, 0.001855382282400922, 0.0018456165289164198, 0.002200785877031027, 0.0017841675945994805, 0.0017996236102237385, 0.0017750356916566283, 0.0020935572256163067, 0.002077812880125581, 0.0018572442670713883, 0.0018740564736784721, 0.00190297058992544, 0.0018671853277756243, 0.0018514273688197136, 0.0022278667750711342, 0.001784613612583097, 0.0017749452630856208, 0.001928001347625134, 0.0019644466133750217, 0.0018278098171006661, 0.0018686992448887654, 0.0021790728752254223, 0.0018709321240229265, 0.0018846080400885977, 0.0018832850441032527, 0.001880181288080556, 0.0018817975746505723, 0.0018654162282770385, 0.001911647346973115, 0.0027152934463276547, 0.0019159494062923655, 0.0021663065096933624, 0.002023254996355699, 0.0018724965083659912, 0.0018695476132311992, 0.001919165101586556, 0.0017842503682691225, 0.001938006184919148, 0.0017799089605710944, 0.0018385225070678458, 0.004641136059499517, 0.0018105257150469994, 0.0018003152896805357, 0.0017717289350305994, 0.0018550250194190692, 0.0017674150634168957, 0.0017307920594854902, 0.0018325448327232152, 0.0017645941212928544, 0.0018729914154391736, 0.0019746090207869806, 0.0019046027494672064, 0.0018672728134940069, 0.0020656184545562914, 0.0018444469800063719, 0.0018582452272918697, 0.0017915291876609747, 0.0017950608986817922, 0.0021348078783679134, 0.0018504277929120387, 0.0018262619560118765, 0.0018449122289894149, 0.0017568291853725289, 0.001839206728618592, 0.0018263611661192651, 0.0021968718307713666, 0.0018212291858314227, 0.0020894594345008954, 0.0018689986851920064, 0.004446509706516129, 0.0017471569784296055, 0.0017368568757471319, 0.0017565139569342136, 0.0017208323115482926, 0.0017433149575178202, 0.001759826103807427, 0.0020576168123322227, 0.0017515585204819217, 0.0017451418534619734, 0.0017405021887194987, 0.0018248153113139172, 0.0018043660432643567, 0.0017655335201804216, 0.0017341945640509948, 0.0017708461624958243, 0.0018196922270969178, 0.001872642751550302, 0.0018696987487298127, 0.0018989039187241967, 0.0018109074978080268, 0.0018606174999149516, 0.0018581575859570876, 0.0018136530610111852, 0.001907761189310501, 0.0019036743566781904, 0.0018458311290790637, 0.0018528911265699814, 0.0018927235796581954, 0.001806178498857965, 0.001955028730056559, 0.0018746148756084342, 0.002041709934322474, 0.001823498814095122, 0.0018650131629935156, 0.0018419859795055042, 0.0018459731654729694, 0.0018688790635981907, 0.004440228852521007, 0.001900622562970966, 0.0017964873986784369, 0.0018512897319548454, 0.0018486355208248522, 0.0017401638130346935, 0.0017276803361407171, 0.0021895668954433254, 0.001793399394955486, 0.0017375689785694703, 0.001733505855857705, 0.0017631563532631844, 0.0018425206023190792, 0.00184570833031709, 0.0018483550229575485, 0.0018745792282667633, 0.001731088103648896, 0.0017970372524966176, 0.001757879726937972, 0.001804258916915084, 0.001984874082457585, 0.002021705062361434, 0.0019585812309135995, 0.0018382193084107712, 0.001829103227161492, 0.001744175916731668, 0.0017896729599063594, 0.00174069864442572, 0.0017392518542086084, 0.0019949146274787686, 0.0017858355422504246, 0.0017126504147502903, 0.0017634277707353856, 0.0017778565816115588, 0.0018244821064096566, 0.001814140496814313, 0.0018624400011807059, 0.001851157746083724, 0.0017081837916824345, 0.0017081796443865944, 0.0017347458536581446, 0.0017291514377575368, 0.002046291459312973, 0.0017548044173357387, 0.001880732917925343, 0.004767459041128556, 0.0018716786241081234, 0.0018775557934228952, 0.001818017102777958, 0.0018114438765527059, 0.0018011973976778488, 0.0017975957501524438, 0.0017229352476230513]
[204.6260180641224, 513.3545787649864, 508.0941311594334, 523.3964691256864, 487.5687046458524, 472.5088336092988, 449.3618735316545, 389.2433998470669, 459.4272354158423, 473.4871319141887, 466.4102697198292, 556.8774518641195, 528.2882905674946, 507.2812509817945, 399.06164800125526, 466.53942587439, 462.72257408150716, 514.2031238529036, 474.98884647143086, 476.6299849983672, 512.0901398802065, 480.47721103812466, 582.9346068781284, 476.6286594244845, 510.51177598236484, 478.1295542644042, 514.2604038343927, 503.89199945985183, 491.61803268628046, 230.28991717650968, 207.46012628892123, 209.69630388725264, 234.70665005647734, 517.8577973191389, 441.1911307947591, 476.5766936717074, 529.9441645891718, 536.873448453129, 537.3751961250555, 625.3315455488646, 598.146853308778, 513.4718078542757, 510.4730211977722, 551.3403211436231, 561.4780933620573, 544.004152572956, 505.3940281790193, 552.4689583650652, 536.1539551685873, 613.3034205327727, 621.456540519709, 633.9892546566083, 623.9233660271904, 539.0927152392693, 613.1403022967422, 480.6078085548462, 572.4869475395984, 559.4599703270093, 592.2642129022219, 546.0899835999202, 540.1303811445948, 455.6077304366568, 537.8389040319878, 558.1717749950922, 564.1297008806374, 550.8737229346066, 563.6499845971637, 559.2205106474265, 550.4205184208558, 554.3177857329766, 493.51176059873734, 445.2718337571176, 543.3674534718114, 533.1215119267426, 563.1666196717574, 15.6575572496988, 554.0916916507215, 518.6704650295726, 566.7581543386294, 586.5199515844723, 581.5944270131214, 593.6649474911337, 606.0540016182489, 576.1943666256176, 591.3790092475066, 234.53159548059506, 568.5005648756793, 567.7389695710259, 561.6395439906391, 575.8355218122755, 590.4477723246354, 598.1711461984389, 247.56138999468675, 566.8776505806034, 578.9263682027797, 246.3992694232668, 616.7364781962822, 547.0601216828189, 594.5423596117388, 533.3953203389923, 586.0952863485701, 504.2247445665516, 544.251005618421, 562.9477880548233, 517.3630085899262, 536.571868999438, 275.4380564829669, 587.5007986169051, 537.269914405464, 590.7243998654872, 588.6445963702593, 534.4601286965308, 525.1396119048109, 564.0187714101734, 559.1483745909492, 558.4132205237789, 565.7690331946696, 494.18509106606376, 510.2719825876528, 534.456264501749, 536.8594023917916, 568.5750145203799, 525.8035402079398, 548.8576193710517, 560.2762568177259, 530.5559749223236, 511.40810592552066, 578.7153506040509, 588.4055719377442, 569.2343998117243, 578.1253039459482, 573.3151350074324, 581.9053026275964, 574.6243268891177, 553.6897465417904, 474.56724901949383, 552.9166258276, 559.4949896683321, 572.7778747848035, 575.1507195884312, 496.7490266183525, 594.0034755063509, 495.9342951984539, 583.9488293832425, 561.1687890419149, 576.5988524046113, 553.323416346246, 537.311490974288, 596.5569271563877, 619.0073911965964, 597.7722593424423, 570.6001138475938, 561.5418072239553, 607.4859777902094, 488.87890801068227, 573.8816602148856, 578.4214748383725, 567.5117670000037, 318.306720501323, 508.661694484006, 561.1729354382361, 485.90601279923885, 544.9296246053755, 591.6572471236883, 565.026815114172, 455.24603433458833, 577.7865829243899, 551.8683685255426, 541.4401196447832, 435.6939126622186, 527.0421341633777, 598.6255003974701, 538.0594427584342, 479.45930290062023, 567.5857282571943, 554.8217449493582, 568.8482352140749, 556.2271529262229, 456.53652126695874, 515.4166808270287, 622.4075053615496, 551.8424135733809, 526.6017941530179, 536.0276488913233, 218.01789688673085, 471.05376574826283, 556.9003236913344, 537.586508212138, 568.2623781087426, 548.4128451125603, 571.7717067437025, 584.4875987953478, 571.1300582099276, 568.1193026416151, 575.3920993364725, 581.1106332510924, 577.7270830518235, 467.24635503494915, 513.860963920557, 559.501717393968, 591.6573469248874, 548.0539143741939, 580.9184760066642, 500.99177392881325, 605.213823003199, 574.6824737503921, 610.2794834648843, 199.87266887577942, 571.0102995327328, 584.0876259061108, 588.033832123547, 511.0203033575403, 554.4832404066568, 561.087329496125, 585.9806693151655, 553.8577810080056, 555.9065344140278, 591.7221349878455, 575.5666168481166, 474.45865170875504, 605.7453862720561, 593.9407465095185, 614.4538936380343, 227.518120667916, 565.8815530482913, 515.8577897183263, 568.3307750642788, 572.8674472382492, 579.0779174717529, 532.8778394881739, 564.1325437835445, 562.5430791689525, 601.4920491908805, 596.8127980279593, 619.4764116359805, 597.0733130385778, 561.4180901882354, 558.457305569448, 598.0612888259092, 548.1257542962429, 595.1702792264232, 549.3523946026561, 559.6066256918559, 564.3006359700908, 547.5042461631342, 576.345487845808, 522.5337016108954, 457.4936441615622, 508.40185425379633, 446.6106169324925, 459.9535920634006, 487.62712318215046, 526.9691231902938, 433.362087706285, 505.90445269910236, 509.4585560521317, 516.6139898381006, 496.64493034903876, 508.8334416892914, 527.4100608046242, 546.6183143643304, 552.7612031724329, 559.511262623033, 552.9899337592079, 544.8387562826182, 519.4880365601508, 469.1580792317112, 473.82941983531043, 516.1025529269522, 519.1124933935091, 529.0280269647253, 520.3929467941655, 550.0913097729514, 511.3772792038737, 483.98169264681917, 515.6058471282621, 546.1685722849651, 512.5808846434286, 457.67351738530203, 542.1680012526056, 517.9945152982663, 537.0990777168772, 501.2421805017096, 567.9622063727327, 515.9612177398219, 506.4396376110738, 513.6339032597473, 505.1747676380009, 512.5778172362556, 517.8430976344979, 519.4144061377863, 529.9683059995102, 537.5189953617751, 518.9757222144495, 546.3439176194273, 475.0958901719745, 20.840467179337487, 548.650416855224, 529.700926206319, 527.371631827051, 559.7724450803472, 566.8184434556571, 553.6855745181348, 557.7037639541517, 552.2702117085084, 556.252675124473, 553.5278863780907, 560.5402786568142, 539.9915839750923, 555.4446113980092, 552.6364818508457, 539.7239880030206, 553.6706582997599, 562.2847977955649, 555.1202011394997, 553.5182705133334, 549.1459145936136, 537.4628615301965, 530.3286799962576, 506.1934419685094, 532.0670913306747, 468.77663452005294, 469.5381872846016, 555.1794558057644, 439.35359339550877, 559.9834488753849, 529.0762821759981, 514.2565937358248, 478.4749127535146, 490.23529829855295, 486.43867504333906, 538.9724853392311, 541.8243629336752, 454.38314123909737, 560.4854628157761, 555.6717495363794, 563.36895348099, 477.6559187225549, 481.27529170940596, 538.4321371883186, 533.6018492746701, 525.4941959135484, 535.5654766157031, 540.1238076314605, 448.8598740236927, 560.3453839806665, 563.3976555770337, 518.6718366311186, 509.04921171766944, 547.102871778112, 535.131590990462, 458.91076492636864, 534.4929338482757, 530.6143127527934, 530.9870659946547, 531.8636060998577, 531.4067854432686, 536.0733893280395, 523.1090355568933, 368.28431982276805, 521.9344502082349, 461.6151941220675, 494.2530732909134, 534.0463896900062, 534.8887575383372, 521.0599125491128, 560.4594611742111, 515.9942252927945, 561.8264878441558, 543.915016626499, 215.464486966115, 552.3257646600402, 555.4582609679715, 564.4204258495835, 539.0762871290901, 565.7980520244787, 577.7701570327682, 545.6892416181549, 566.7025566578087, 533.9052767444334, 506.4293687878777, 525.0438708437966, 535.5403842295643, 484.1165113500144, 542.1679293793229, 538.1421059573278, 558.1823655943907, 557.0841639603162, 468.4262270778728, 540.4155751607518, 547.5665726420558, 542.031205759728, 569.2073016125092, 543.7126694023618, 547.5368281755822, 455.1926908038506, 549.0797137338225, 478.5926845422906, 535.0458552608708, 224.89549466957237, 572.3584156123329, 575.7526794312496, 569.3094529948291, 581.1141465029008, 573.6198130393074, 568.237962737611, 485.99913939590226, 570.9201196000355, 573.0193210461501, 574.5468213031711, 548.000662751987, 554.2112720049068, 566.401027547644, 576.636567043578, 564.7017912558839, 549.5434805452622, 534.0046835800002, 534.8455202632799, 526.619588352771, 552.2093211334251, 537.4559790207874, 538.1674878155862, 551.3733698563381, 524.1746218568467, 525.2999266875383, 541.7613693073471, 539.6971174723964, 528.3391672969958, 553.6551346571191, 511.5014345446831, 533.4429023323711, 489.7855386748865, 548.3962985170526, 536.1892451176641, 542.8922972955842, 541.719684069071, 535.0801020129572, 225.21361695855717, 526.1433908460215, 556.6418115349082, 540.1639639323571, 540.9395138928223, 574.6585422070624, 578.810778291194, 456.7113259161366, 557.6002773352229, 575.5167203913215, 576.8656602000456, 567.1646749587676, 542.7347725400492, 541.7974138027548, 541.0216043884807, 533.4530463802271, 577.6713489579978, 556.4714914010289, 568.8671327599227, 554.2441778310824, 503.81029650094655, 494.6319908958229, 510.57366639500583, 544.00473078729, 546.7160000323535, 573.3366631238978, 558.761305781992, 574.4819777980085, 574.9598585048043, 501.2745839975264, 559.9619765321995, 583.8903207493182, 567.0773799728579, 562.4750670796732, 548.100744034081, 551.2252230497203, 536.9300484128581, 540.2024771338812, 585.4170990670003, 585.4185204033964, 576.4533161392203, 578.3183463079773, 488.68893795595596, 569.8640772276302, 531.7076074273805, 209.75534165539037, 534.2797567485774, 532.6073416848726, 550.0498309240241, 552.0458088401084, 555.1862340514296, 556.2986004585267, 580.4048651158496]
Elapsed: 0.10583786359722061~0.17216576647374907
Time per graph: 0.0021693222755664343~0.0035131116918982485
Speed: 527.5952621826316~76.31710721243554
Total Time: 0.0841
best val loss: 0.32753080129623413 test_score: 0.9167

Testing...
Test loss: 0.1728 score: 0.9375 time: 0.08s
test Score 0.9375
Epoch Time List: [1.6468671369366348, 0.3625374410767108, 0.3446607287041843, 0.3508823369629681, 0.3606036838609725, 0.36298969807103276, 0.3916797188576311, 0.37404541322030127, 0.4218095289543271, 0.35315572493709624, 0.35673578176647425, 0.3355580123607069, 0.32381758093833923, 0.34335721214301884, 0.40835621021687984, 0.37103253905661404, 0.4127646731212735, 0.3285720518324524, 0.3519704029895365, 0.3680481552146375, 0.3486543719191104, 0.36052900715731084, 0.3294205910060555, 0.44964528689160943, 0.3643657409120351, 0.36292105820029974, 0.36129339318722486, 0.3596871639601886, 0.3661671478766948, 0.6826463029719889, 0.5047453679144382, 0.48902602097950876, 0.47005920903757215, 0.3907252917997539, 0.366746474057436, 0.4112151151057333, 0.5025370516814291, 0.35438661370426416, 0.34019099711440504, 0.31484659295529127, 0.3116508359089494, 0.3412766430992633, 0.35118512087501585, 0.3203880980145186, 0.32365983026102185, 0.3277209587395191, 0.3366870472673327, 0.3314850579481572, 0.353385770926252, 0.3044655539561063, 0.3045341409742832, 0.29658078122884035, 0.2944740492384881, 0.4024378666654229, 0.3069890718907118, 0.33451387216337025, 0.34206120134331286, 0.3166959087830037, 0.3189078269060701, 0.32396027096547186, 0.3176552599761635, 0.35139480885118246, 0.3557017759885639, 0.3201450798660517, 0.33116723597049713, 0.334884470095858, 0.3290913768578321, 0.3272226902190596, 0.3420929505955428, 0.33435797109268606, 0.3474288329016417, 0.37654860503971577, 0.33514810376800597, 0.33629975211806595, 0.38132777716964483, 4.934736586175859, 2.6449583880603313, 0.3323659591842443, 0.33420270308852196, 0.3755028247833252, 0.3224214508663863, 0.3143256369512528, 0.31103151896968484, 0.33049133606255054, 0.31024729483760893, 0.42670373409055173, 0.30975280003622174, 0.4988886397331953, 0.31787070678547025, 0.3217624758835882, 0.33461211808025837, 0.3120733411051333, 0.42808966618031263, 0.3359708893112838, 0.32973859715275466, 0.4316256493330002, 0.34111255616880953, 0.3346774810925126, 0.3174678520299494, 0.4332678900100291, 0.32320048310793936, 0.349380720872432, 0.33280800306238234, 0.3458822616375983, 0.3232066018972546, 0.32853517099283636, 0.4387583010829985, 0.3217755891382694, 0.32743463874794543, 0.3175818568561226, 0.32052710209973156, 0.3255198916886002, 0.3427574180532247, 0.33294892287813127, 0.46673375298269093, 0.3215505820699036, 0.34192029596306384, 0.3480273690074682, 0.33743947208859026, 0.33478575362823904, 0.3214109791442752, 0.44881214783526957, 0.3293634089641273, 0.32061013602651656, 0.3391737702768296, 0.32914240122772753, 0.33453374495729804, 0.3151097579393536, 0.4171629927586764, 0.3168445110786706, 0.3170872167684138, 0.32431772095151246, 0.32035361998714507, 0.318181409034878, 0.32250176509842277, 0.3442632502410561, 0.42108577815815806, 0.31959534296765924, 0.3502377748955041, 0.33160578389652073, 0.35447746119461954, 0.3295757200103253, 0.31307718716561794, 0.3474174439907074, 0.3204757629428059, 0.32392207300290465, 0.3267728630453348, 0.3283728687092662, 0.3142463231924921, 0.32584513910114765, 0.3008857660461217, 0.3364989091642201, 0.3337217168882489, 0.3061957322061062, 0.34094861312769353, 0.33240644610486925, 0.33795351115986705, 0.34164069476537406, 0.394276782637462, 0.31954616704024374, 0.3367699566297233, 0.3527671080082655, 0.3194517670199275, 0.3247833517380059, 0.318045194959268, 0.3231408742722124, 0.3762339698150754, 0.3249653750099242, 0.33733928203582764, 0.3713695900514722, 0.33764017559587955, 0.32593846833333373, 0.3227188987657428, 0.48592003411613405, 0.3216946462634951, 0.3275471639353782, 0.339575934689492, 0.33445127913728356, 0.3530180782545358, 0.3459909060038626, 0.3982119639404118, 0.33653374621644616, 0.3268450200557709, 0.3352506582159549, 0.5393593520857394, 0.34283977816812694, 0.3965936698950827, 0.46377471601590514, 0.32787263486534357, 0.33436236483976245, 0.32971241883933544, 0.31260556797496974, 0.32625557482242584, 0.3229276221245527, 0.32174919196404517, 0.32830481370911, 0.3116384190507233, 0.3308674667496234, 0.33602419518865645, 0.3390928921289742, 0.31809527589939535, 0.318866616114974, 0.29743430111557245, 0.30576892592944205, 0.30150343128480017, 0.33679333329200745, 0.3190713129006326, 0.48341950587928295, 0.3117968551814556, 0.3151617511175573, 0.3066605688072741, 0.35626407293602824, 0.3419248959980905, 0.31768706208094954, 0.32579530286602676, 0.3226159301120788, 0.3195519687142223, 0.31447100290097296, 0.3312310769688338, 0.33332077390514314, 0.32099627796560526, 0.44301877659745514, 0.3098689268808812, 0.46362223709002137, 0.3268974057864398, 0.33915742323733866, 0.3212234459351748, 0.3256098323035985, 0.3241121752653271, 0.32494653132744133, 0.32661836920306087, 0.39569193683564663, 0.30732294800691307, 0.30255943443626165, 0.2974578207358718, 0.32894459227100015, 0.3160535120405257, 0.3229030813090503, 0.3101215388160199, 0.35792749002575874, 0.3237544430885464, 0.3190615160856396, 0.3304854198358953, 0.3208736930973828, 0.3354801987297833, 0.3305049678310752, 0.36681159026920795, 0.35212023090571165, 0.3337019928731024, 0.3564072020817548, 0.35377576923929155, 0.3390311428811401, 0.32947858585976064, 0.4897268540225923, 0.3405740880407393, 0.31802593800239265, 0.32820741669274867, 0.3272103010676801, 0.3328960840590298, 0.32938775909133255, 0.3788847429677844, 0.308204643195495, 0.3127678958699107, 0.3091896758414805, 0.3205132931470871, 0.3254796650726348, 0.35638964967802167, 0.3389011637773365, 0.4276899192482233, 0.33167952485382557, 0.3225037697702646, 0.32579553220421076, 0.3253000369295478, 0.3212986937724054, 0.34334721812047064, 0.41502430802211165, 0.31693003699183464, 0.34896466019563377, 0.35949543537572026, 0.31386540620587766, 0.3251198809593916, 0.32050535921007395, 0.32562368595972657, 0.3758470139000565, 0.3153000446036458, 0.33573311590589583, 0.3383994069881737, 0.3357512338552624, 0.32786035910248756, 0.3256170868407935, 0.3542753567453474, 0.3188794357702136, 0.32267710380256176, 0.321560253854841, 0.3343553419690579, 0.32914014626294374, 6.3174513899721205, 0.5842817903030664, 0.31525842077098787, 0.42829459300264716, 0.3089941218495369, 0.3089463522192091, 0.30809602700173855, 0.3074867441318929, 0.305380400037393, 0.3073995718732476, 0.31662490777671337, 0.43061256501823664, 0.3123949831351638, 0.31002334295772016, 0.306510248221457, 0.31114858598448336, 0.3132651059422642, 0.29677155311219394, 0.44384762505069375, 0.30943772662431, 0.30851198686286807, 0.31137112295255065, 0.3289407747797668, 0.3460774119012058, 0.3234752328135073, 0.3418551650829613, 0.45722442329861224, 0.30815884098410606, 0.37888143700547516, 0.31895234412513673, 0.3151295662391931, 0.3330032422672957, 0.36659270082600415, 0.35981327877379954, 0.35335372295230627, 0.3270696171093732, 0.3123186598531902, 0.5250158330891281, 0.3198447530157864, 0.31803426682017744, 0.30921968491747975, 0.3404535478912294, 0.3499032137915492, 0.3245255737565458, 0.3119264147244394, 0.3168616278562695, 0.32044471125118434, 0.3175971021410078, 0.33683574735186994, 0.31072208704426885, 0.3090542189311236, 0.34465873916633427, 0.347028368152678, 0.31833501206710935, 0.31268232106231153, 0.34046002686955035, 0.3085485880728811, 0.3198970502708107, 0.3232050302904099, 0.3174084909260273, 0.32034864579327404, 0.3244651770219207, 0.3187113357707858, 0.3921713759191334, 0.42367313895374537, 0.3530024476349354, 0.34871961595490575, 0.32124227681197226, 0.31511024688370526, 0.32464348687790334, 0.31003272510133684, 0.3166051679290831, 0.31551403109915555, 0.32104796706698835, 0.4731155917979777, 0.29737632418982685, 0.2994488407857716, 0.292776808841154, 0.30367506109178066, 0.3195971930399537, 0.295500225154683, 0.370134016033262, 0.3128554690629244, 0.3144838318694383, 0.33189249597489834, 0.31603226182051003, 0.3316729450598359, 0.33804751932621, 0.38645042199641466, 0.3138215800281614, 0.3130815299227834, 0.3109033221844584, 0.3384902700781822, 0.32813083566725254, 0.3228072968777269, 0.32218716805800796, 0.34079479495994747, 0.31935781706124544, 0.32721086288802326, 0.33893609791994095, 0.3362206951715052, 0.33439201582223177, 0.3331768657080829, 0.4573276189621538, 0.3150769709609449, 0.3107251387555152, 0.30641904124058783, 0.31376908882521093, 0.3010865629184991, 0.3113368300255388, 0.3438878918532282, 0.37616324913688004, 0.30306399310939014, 0.30710929399356246, 0.3183114167768508, 0.3121282309293747, 0.31061888486146927, 0.30883635696955025, 0.3079367878381163, 0.43536130827851593, 0.3224262180738151, 0.3284784951247275, 0.3154296351131052, 0.32014504284597933, 0.31667350395582616, 0.3251102410722524, 0.39299532514996827, 0.3239191297907382, 0.33026589220389724, 0.3207861727569252, 0.3179082814604044, 0.3246278108563274, 0.322720481781289, 0.31793634314090014, 0.4270516321994364, 0.3317444429267198, 0.3247313518077135, 0.3202987969852984, 0.3220296520739794, 0.3269374929368496, 0.3227285537868738, 0.4486759561114013, 0.3303197359200567, 0.3158905920572579, 0.3389097759500146, 0.32335248589515686, 0.3178618950769305, 0.30811244319193065, 0.462548921816051, 0.3236744040623307, 0.3188240716699511, 0.3024794280063361, 0.3041686862707138, 0.32050148700363934, 0.3170648878440261, 0.31232440983876586, 0.3640090299304575, 0.30814341525547206, 0.3192727130372077, 0.299662470119074, 0.31410559988580644, 0.3282697612885386, 0.33856375701725483, 0.3170419249217957, 0.43051543715409935, 0.3157785921357572, 0.30575135583058, 0.30801678891293705, 0.30709373694844544, 0.3045131149701774, 0.3162933001294732, 0.43512407317757607, 0.3020845747087151, 0.30440119304694235, 0.3079689198639244, 0.320467809215188, 0.3161286620888859, 0.3273836220614612, 0.30791266704909503, 0.45875714416615665, 0.3039922129828483, 0.30160332517698407, 0.30941813392564654, 0.3191598551347852, 0.30871278513222933, 0.31849629315547645, 0.46079995413310826, 0.33569498313590884, 0.32613121811300516, 0.33044050517491996, 0.3154948656447232, 0.31829059310257435, 0.3173431591130793, 0.31148850615136325]
Total Epoch List: [246, 126, 118]
Total Time List: [0.0859517918433994, 0.09003407694399357, 0.0840744290035218]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d407c5e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7052 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7041 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6811 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6728 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.15s
Val loss: 0.6711 score: 0.5510 time: 0.08s
Test loss: 0.6637 score: 0.5510 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.15s
Val loss: 0.6630 score: 0.6327 time: 0.08s
Test loss: 0.6531 score: 0.6122 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.15s
Val loss: 0.6549 score: 0.6735 time: 0.08s
Test loss: 0.6424 score: 0.7143 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5244;  Loss pred: 0.5244; Loss self: 0.0000; time: 0.15s
Val loss: 0.6466 score: 0.7755 time: 0.08s
Test loss: 0.6315 score: 0.8163 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.15s
Val loss: 0.6381 score: 0.7959 time: 0.08s
Test loss: 0.6205 score: 0.8776 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5101;  Loss pred: 0.5101; Loss self: 0.0000; time: 0.15s
Val loss: 0.6304 score: 0.8367 time: 0.08s
Test loss: 0.6103 score: 0.8571 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.15s
Val loss: 0.6228 score: 0.8163 time: 0.08s
Test loss: 0.6006 score: 0.8571 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4451;  Loss pred: 0.4451; Loss self: 0.0000; time: 0.15s
Val loss: 0.6153 score: 0.8367 time: 0.09s
Test loss: 0.5909 score: 0.8980 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4434;  Loss pred: 0.4434; Loss self: 0.0000; time: 0.16s
Val loss: 0.6078 score: 0.8367 time: 0.09s
Test loss: 0.5813 score: 0.9184 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4284;  Loss pred: 0.4284; Loss self: 0.0000; time: 0.15s
Val loss: 0.6004 score: 0.8367 time: 0.08s
Test loss: 0.5718 score: 0.9184 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4047;  Loss pred: 0.4047; Loss self: 0.0000; time: 0.15s
Val loss: 0.5930 score: 0.8367 time: 0.08s
Test loss: 0.5626 score: 0.9184 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.3950;  Loss pred: 0.3950; Loss self: 0.0000; time: 0.15s
Val loss: 0.5855 score: 0.8367 time: 0.08s
Test loss: 0.5534 score: 0.9184 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3795;  Loss pred: 0.3795; Loss self: 0.0000; time: 0.16s
Val loss: 0.5784 score: 0.8367 time: 0.09s
Test loss: 0.5449 score: 0.9184 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.3625;  Loss pred: 0.3625; Loss self: 0.0000; time: 0.15s
Val loss: 0.5712 score: 0.8367 time: 0.09s
Test loss: 0.5365 score: 0.9184 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.3451;  Loss pred: 0.3451; Loss self: 0.0000; time: 0.15s
Val loss: 0.5641 score: 0.8367 time: 0.08s
Test loss: 0.5282 score: 0.9184 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 0.15s
Val loss: 0.5572 score: 0.8571 time: 0.10s
Test loss: 0.5203 score: 0.9184 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.3337;  Loss pred: 0.3337; Loss self: 0.0000; time: 0.16s
Val loss: 0.5505 score: 0.8571 time: 0.10s
Test loss: 0.5125 score: 0.9184 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3417;  Loss pred: 0.3417; Loss self: 0.0000; time: 1.46s
Val loss: 0.5441 score: 0.8367 time: 2.68s
Test loss: 0.5047 score: 0.9184 time: 1.73s
Epoch 27/1000, LR 0.000270
Train loss: 0.3037;  Loss pred: 0.3037; Loss self: 0.0000; time: 0.14s
Val loss: 0.5379 score: 0.8571 time: 0.08s
Test loss: 0.4972 score: 0.9184 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3076;  Loss pred: 0.3076; Loss self: 0.0000; time: 0.16s
Val loss: 0.5323 score: 0.8367 time: 0.08s
Test loss: 0.4905 score: 0.9184 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.2879;  Loss pred: 0.2879; Loss self: 0.0000; time: 0.15s
Val loss: 0.5272 score: 0.8367 time: 0.08s
Test loss: 0.4848 score: 0.9184 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.2962;  Loss pred: 0.2962; Loss self: 0.0000; time: 0.15s
Val loss: 0.5223 score: 0.8367 time: 0.08s
Test loss: 0.4787 score: 0.8980 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.2691;  Loss pred: 0.2691; Loss self: 0.0000; time: 0.15s
Val loss: 0.5178 score: 0.8367 time: 0.09s
Test loss: 0.4733 score: 0.9184 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.2566;  Loss pred: 0.2566; Loss self: 0.0000; time: 0.15s
Val loss: 0.5127 score: 0.8367 time: 0.08s
Test loss: 0.4660 score: 0.9388 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.15s
Val loss: 0.5078 score: 0.8367 time: 0.08s
Test loss: 0.4585 score: 0.9184 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.2412;  Loss pred: 0.2412; Loss self: 0.0000; time: 0.25s
Val loss: 0.5031 score: 0.8571 time: 0.08s
Test loss: 0.4512 score: 0.9184 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2430;  Loss pred: 0.2430; Loss self: 0.0000; time: 0.15s
Val loss: 0.4986 score: 0.8776 time: 0.08s
Test loss: 0.4443 score: 0.9184 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.2249;  Loss pred: 0.2249; Loss self: 0.0000; time: 0.15s
Val loss: 0.4943 score: 0.8571 time: 0.08s
Test loss: 0.4375 score: 0.9184 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.2274;  Loss pred: 0.2274; Loss self: 0.0000; time: 0.15s
Val loss: 0.4904 score: 0.8571 time: 0.08s
Test loss: 0.4315 score: 0.9184 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2249;  Loss pred: 0.2249; Loss self: 0.0000; time: 0.16s
Val loss: 0.4865 score: 0.8571 time: 0.08s
Test loss: 0.4255 score: 0.9184 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.16s
Val loss: 0.4832 score: 0.8571 time: 0.09s
Test loss: 0.4205 score: 0.9184 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.15s
Val loss: 0.4799 score: 0.8571 time: 0.08s
Test loss: 0.4160 score: 0.9184 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.16s
Val loss: 0.4763 score: 0.8776 time: 0.14s
Test loss: 0.4112 score: 0.9184 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 0.14s
Val loss: 0.4731 score: 0.8776 time: 0.08s
Test loss: 0.4065 score: 0.9388 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2023;  Loss pred: 0.2023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4695 score: 0.8571 time: 0.08s
Test loss: 0.4006 score: 0.9184 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 0.14s
Val loss: 0.4654 score: 0.8571 time: 0.08s
Test loss: 0.3934 score: 0.9184 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.1748;  Loss pred: 0.1748; Loss self: 0.0000; time: 0.15s
Val loss: 0.4612 score: 0.8571 time: 0.08s
Test loss: 0.3859 score: 0.9184 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.1679;  Loss pred: 0.1679; Loss self: 0.0000; time: 0.16s
Val loss: 0.4573 score: 0.8367 time: 0.09s
Test loss: 0.3784 score: 0.9184 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.1735;  Loss pred: 0.1735; Loss self: 0.0000; time: 0.15s
Val loss: 0.4538 score: 0.8367 time: 0.08s
Test loss: 0.3718 score: 0.9184 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.14s
Val loss: 0.4506 score: 0.8367 time: 0.08s
Test loss: 0.3666 score: 0.9184 time: 0.21s
Epoch 49/1000, LR 0.000269
Train loss: 0.1637;  Loss pred: 0.1637; Loss self: 0.0000; time: 0.15s
Val loss: 0.4476 score: 0.8163 time: 0.09s
Test loss: 0.3609 score: 0.9388 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.1528;  Loss pred: 0.1528; Loss self: 0.0000; time: 0.16s
Val loss: 0.4449 score: 0.8163 time: 0.08s
Test loss: 0.3556 score: 0.9388 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 0.16s
Val loss: 0.4422 score: 0.8163 time: 0.10s
Test loss: 0.3508 score: 0.9388 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.1529;  Loss pred: 0.1529; Loss self: 0.0000; time: 0.16s
Val loss: 0.4391 score: 0.8163 time: 0.09s
Test loss: 0.3468 score: 0.9388 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 0.15s
Val loss: 0.4367 score: 0.8163 time: 0.09s
Test loss: 0.3437 score: 0.9388 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1375;  Loss pred: 0.1375; Loss self: 0.0000; time: 0.15s
Val loss: 0.4341 score: 0.8163 time: 0.09s
Test loss: 0.3399 score: 0.9388 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1331;  Loss pred: 0.1331; Loss self: 0.0000; time: 0.15s
Val loss: 0.4315 score: 0.8163 time: 0.08s
Test loss: 0.3363 score: 0.9388 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1337;  Loss pred: 0.1337; Loss self: 0.0000; time: 0.16s
Val loss: 0.4292 score: 0.8163 time: 0.13s
Test loss: 0.3324 score: 0.9388 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1277;  Loss pred: 0.1277; Loss self: 0.0000; time: 0.15s
Val loss: 0.4272 score: 0.8163 time: 0.08s
Test loss: 0.3282 score: 0.9184 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.14s
Val loss: 0.4254 score: 0.8163 time: 0.09s
Test loss: 0.3242 score: 0.9184 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.15s
Val loss: 0.4230 score: 0.8163 time: 0.08s
Test loss: 0.3203 score: 0.9184 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1278;  Loss pred: 0.1278; Loss self: 0.0000; time: 0.15s
Val loss: 0.4204 score: 0.8163 time: 0.08s
Test loss: 0.3164 score: 0.9184 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1191;  Loss pred: 0.1191; Loss self: 0.0000; time: 0.15s
Val loss: 0.4177 score: 0.8163 time: 0.08s
Test loss: 0.3126 score: 0.9184 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1177;  Loss pred: 0.1177; Loss self: 0.0000; time: 0.15s
Val loss: 0.4132 score: 0.8163 time: 0.08s
Test loss: 0.3082 score: 0.9184 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.15s
Val loss: 0.4094 score: 0.8163 time: 0.10s
Test loss: 0.3038 score: 0.9388 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1096;  Loss pred: 0.1096; Loss self: 0.0000; time: 0.23s
Val loss: 0.4059 score: 0.8163 time: 0.08s
Test loss: 0.2996 score: 0.9184 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1066;  Loss pred: 0.1066; Loss self: 0.0000; time: 0.15s
Val loss: 0.4039 score: 0.8163 time: 0.08s
Test loss: 0.2953 score: 0.9388 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 0.15s
Val loss: 0.4056 score: 0.8163 time: 0.08s
Test loss: 0.2915 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 0.15s
Val loss: 0.4080 score: 0.7959 time: 0.08s
Test loss: 0.2895 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.15s
Val loss: 0.4052 score: 0.7959 time: 0.09s
Test loss: 0.2846 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0944;  Loss pred: 0.0944; Loss self: 0.0000; time: 0.17s
Val loss: 0.4004 score: 0.8163 time: 0.10s
Test loss: 0.2786 score: 0.9184 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0998;  Loss pred: 0.0998; Loss self: 0.0000; time: 0.16s
Val loss: 0.3925 score: 0.8163 time: 0.08s
Test loss: 0.2720 score: 0.9184 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.16s
Val loss: 0.3845 score: 0.8163 time: 0.09s
Test loss: 0.2661 score: 0.9388 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 0.14s
Val loss: 0.3823 score: 0.8163 time: 0.08s
Test loss: 0.2618 score: 0.9388 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.15s
Val loss: 0.3896 score: 0.8163 time: 0.08s
Test loss: 0.2605 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0886;  Loss pred: 0.0886; Loss self: 0.0000; time: 0.15s
Val loss: 0.3908 score: 0.8163 time: 0.08s
Test loss: 0.2609 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.15s
Val loss: 0.3820 score: 0.8163 time: 0.08s
Test loss: 0.2551 score: 0.9184 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.14s
Val loss: 0.3713 score: 0.8163 time: 0.08s
Test loss: 0.2501 score: 0.9388 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.14s
Val loss: 0.3658 score: 0.8163 time: 0.08s
Test loss: 0.2469 score: 0.9388 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.14s
Val loss: 0.3676 score: 0.8163 time: 0.08s
Test loss: 0.2409 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.16s
Val loss: 0.3671 score: 0.8163 time: 0.10s
Test loss: 0.2366 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.15s
Val loss: 0.3554 score: 0.8163 time: 0.08s
Test loss: 0.2348 score: 0.9388 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.15s
Val loss: 0.3544 score: 0.8163 time: 0.09s
Test loss: 0.2306 score: 0.9388 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.15s
Val loss: 0.3511 score: 0.8163 time: 0.09s
Test loss: 0.2263 score: 0.9388 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.15s
Val loss: 0.3458 score: 0.8163 time: 0.08s
Test loss: 0.2231 score: 0.9388 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.15s
Val loss: 0.3409 score: 0.8163 time: 0.08s
Test loss: 0.2193 score: 0.9388 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.15s
Val loss: 0.3347 score: 0.8367 time: 0.08s
Test loss: 0.2163 score: 0.9388 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.15s
Val loss: 0.3314 score: 0.8367 time: 0.08s
Test loss: 0.2134 score: 0.9388 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.28s
Val loss: 0.3294 score: 0.8367 time: 0.08s
Test loss: 0.2112 score: 0.9388 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.15s
Val loss: 0.3246 score: 0.8367 time: 0.08s
Test loss: 0.2086 score: 0.9388 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.14s
Val loss: 0.3187 score: 0.8367 time: 0.08s
Test loss: 0.2070 score: 0.9388 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.15s
Val loss: 0.3254 score: 0.8367 time: 0.08s
Test loss: 0.2044 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.15s
Val loss: 0.3126 score: 0.8367 time: 0.08s
Test loss: 0.2030 score: 0.9388 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.16s
Val loss: 0.3064 score: 0.8571 time: 0.09s
Test loss: 0.2021 score: 0.9388 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0507;  Loss pred: 0.0507; Loss self: 0.0000; time: 0.15s
Val loss: 0.3084 score: 0.8571 time: 0.08s
Test loss: 0.1986 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.15s
Val loss: 0.3092 score: 0.8367 time: 0.08s
Test loss: 0.1963 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.22s
Val loss: 0.3010 score: 0.8571 time: 0.08s
Test loss: 0.1954 score: 0.9388 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.15s
Val loss: 0.2959 score: 0.8571 time: 0.08s
Test loss: 0.1941 score: 0.9388 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.15s
Val loss: 0.2923 score: 0.8571 time: 0.08s
Test loss: 0.1931 score: 0.9388 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.15s
Val loss: 0.2893 score: 0.8571 time: 0.08s
Test loss: 0.1934 score: 0.9388 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.15s
Val loss: 0.2942 score: 0.8571 time: 0.09s
Test loss: 0.1915 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.15s
Val loss: 0.2806 score: 0.8571 time: 0.09s
Test loss: 0.1938 score: 0.9388 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.15s
Val loss: 0.2880 score: 0.8571 time: 0.08s
Test loss: 0.1907 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.30s
Val loss: 0.2926 score: 0.8571 time: 0.09s
Test loss: 0.1889 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.17s
Val loss: 0.2813 score: 0.8571 time: 0.09s
Test loss: 0.1916 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0404;  Loss pred: 0.0404; Loss self: 0.0000; time: 0.16s
Val loss: 0.2769 score: 0.8571 time: 0.08s
Test loss: 0.1925 score: 0.9388 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.16s
Val loss: 0.2789 score: 0.8571 time: 0.08s
Test loss: 0.1909 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.16s
Val loss: 0.2826 score: 0.8571 time: 0.09s
Test loss: 0.1894 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.16s
Val loss: 0.2822 score: 0.8776 time: 0.09s
Test loss: 0.1895 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.16s
Val loss: 0.2753 score: 0.8776 time: 0.09s
Test loss: 0.1910 score: 0.9388 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.16s
Val loss: 0.2738 score: 0.8776 time: 0.23s
Test loss: 0.1920 score: 0.9388 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.15s
Val loss: 0.2757 score: 0.8776 time: 0.08s
Test loss: 0.1922 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.16s
Val loss: 0.2749 score: 0.8776 time: 0.08s
Test loss: 0.1924 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.15s
Val loss: 0.2715 score: 0.8776 time: 0.08s
Test loss: 0.1923 score: 0.9388 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.15s
Val loss: 0.2718 score: 0.8776 time: 0.08s
Test loss: 0.1917 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.16s
Val loss: 0.2725 score: 0.8776 time: 0.08s
Test loss: 0.1925 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.16s
Val loss: 0.2698 score: 0.8776 time: 0.08s
Test loss: 0.1939 score: 0.9388 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.15s
Val loss: 0.2724 score: 0.8776 time: 0.08s
Test loss: 0.1926 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.18s
Val loss: 0.2712 score: 0.8776 time: 0.10s
Test loss: 0.1928 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.16s
Val loss: 0.2711 score: 0.8776 time: 0.08s
Test loss: 0.1939 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.16s
Val loss: 0.2722 score: 0.8776 time: 0.10s
Test loss: 0.1958 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.17s
Val loss: 0.2743 score: 0.8776 time: 0.09s
Test loss: 0.1960 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.15s
Val loss: 0.2701 score: 0.8776 time: 0.09s
Test loss: 0.1967 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.16s
Val loss: 0.2661 score: 0.8776 time: 0.08s
Test loss: 0.1977 score: 0.9388 time: 0.09s
Epoch 123/1000, LR 0.000262
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.16s
Val loss: 0.2647 score: 0.8776 time: 0.08s
Test loss: 0.1981 score: 0.9388 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.17s
Val loss: 0.2680 score: 0.8776 time: 0.15s
Test loss: 0.1984 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.15s
Val loss: 0.2704 score: 0.8776 time: 0.08s
Test loss: 0.1980 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.17s
Val loss: 0.2734 score: 0.8776 time: 0.08s
Test loss: 0.1971 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.16s
Val loss: 0.2697 score: 0.8776 time: 0.08s
Test loss: 0.1977 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.15s
Val loss: 0.2699 score: 0.8776 time: 0.08s
Test loss: 0.1978 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.15s
Val loss: 0.2701 score: 0.8776 time: 0.08s
Test loss: 0.1990 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.15s
Val loss: 0.2713 score: 0.8776 time: 0.08s
Test loss: 0.1995 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.16s
Val loss: 0.2724 score: 0.8776 time: 0.08s
Test loss: 0.1999 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.26s
Val loss: 0.2725 score: 0.8776 time: 0.08s
Test loss: 0.2007 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.15s
Val loss: 0.2699 score: 0.8776 time: 0.08s
Test loss: 0.2033 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.16s
Val loss: 0.2697 score: 0.8776 time: 0.09s
Test loss: 0.2036 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.17s
Val loss: 0.2737 score: 0.8776 time: 0.09s
Test loss: 0.2014 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.16s
Val loss: 0.2743 score: 0.8776 time: 0.08s
Test loss: 0.2002 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.16s
Val loss: 0.2742 score: 0.8776 time: 0.09s
Test loss: 0.1998 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.15s
Val loss: 0.2731 score: 0.8776 time: 0.08s
Test loss: 0.2002 score: 0.9388 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.15s
Val loss: 0.2719 score: 0.8776 time: 0.08s
Test loss: 0.2007 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.17s
Val loss: 0.2706 score: 0.8776 time: 0.08s
Test loss: 0.2013 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.15s
Val loss: 0.2705 score: 0.8776 time: 0.08s
Test loss: 0.2010 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.16s
Val loss: 0.2689 score: 0.8776 time: 0.08s
Test loss: 0.2028 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.15s
Val loss: 0.2698 score: 0.8776 time: 0.08s
Test loss: 0.2029 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 122,   Train_Loss: 0.0273,   Val_Loss: 0.2647,   Val_Precision: 0.8462,   Val_Recall: 0.9167,   Val_accuracy: 0.8800,   Val_Score: 0.8776,   Val_Loss: 0.2647,   Test_Precision: 0.9583,   Test_Recall: 0.9200,   Test_accuracy: 0.9388,   Test_Score: 0.9388,   Test_loss: 0.1981


[0.08784167980775237, 0.08701143995858729, 0.08159107412211597, 0.08523665205575526, 0.0843327809125185, 0.08390744007192552, 0.08476514206267893, 0.08275979408062994, 0.0836453118827194, 0.08332366403192282, 0.08705194410867989, 0.09070615703240037, 0.08762322412803769, 0.08916717790998518, 0.08695923094637692, 0.0887101641856134, 0.08727181889116764, 0.08795737591572106, 0.08737895009107888, 0.08942959015257657, 0.08944672020152211, 0.08596696192398667, 0.08611232205294073, 0.09936061687767506, 0.0988610740751028, 1.7382950740866363, 0.08944874000735581, 0.08598288614302874, 0.08859614911489189, 0.0887419618666172, 0.0892189231235534, 0.08874173904769123, 0.0969245231244713, 0.08796709100715816, 0.08850480592809618, 0.08603157592006028, 0.08776853000745177, 0.0905919810757041, 0.0882463229354471, 0.08811196591705084, 0.08165757800452411, 0.08253940101712942, 0.08544808579608798, 0.08535583200864494, 0.08158652600832283, 0.08389338804408908, 0.08053363882936537, 0.2178816010709852, 0.08978643408045173, 0.08931656298227608, 0.09048899402841926, 0.098601542878896, 0.08752691396512091, 0.09131372510455549, 0.08376379893161356, 0.08173949900083244, 0.07954814308322966, 0.08570010797120631, 0.0844830540008843, 0.08478369005024433, 0.08665402210317552, 0.08480208204127848, 0.09239123412407935, 0.08455205010250211, 0.08538991515524685, 0.09008996794000268, 0.10840894002467394, 0.09663272392936051, 0.09117019199766219, 0.08750048093497753, 0.2208327439147979, 0.08474329183809459, 0.08242558408528566, 0.08623540494590998, 0.08082906110212207, 0.0814104110468179, 0.08065528608858585, 0.08190023293718696, 0.07962578698061407, 0.0891187519300729, 0.09900699881836772, 0.08708423911593854, 0.08537957305088639, 0.08714143396355212, 0.086063914000988, 0.0840301108546555, 0.08762198290787637, 0.08693409711122513, 0.08544559706933796, 0.08495110692456365, 0.09539997996762395, 0.08890177891589701, 0.08621465601027012, 0.10862519894726574, 0.08644559909589589, 0.08501654816791415, 0.085684719029814, 0.08804524200968444, 0.08893372607417405, 0.08809660212136805, 0.08680886309593916, 0.09109386708587408, 0.08947808900848031, 0.0873291571624577, 0.08818918792530894, 0.09098908095620573, 0.09054515487514436, 0.09178929799236357, 0.08709343988448381, 0.08595106797292829, 0.08437032392248511, 0.0854486299213022, 0.08718623593449593, 0.08812647103331983, 0.09557637595571578, 0.10450924816541374, 0.09348579193465412, 0.08952593593858182, 0.1069197878241539, 0.09032967896200716, 0.09081554599106312, 0.09320780215784907, 0.09381936094723642, 0.08472861698828638, 0.08715720195323229, 0.0884755291044712, 0.084273518063128, 0.08463544817641377, 0.08705387893132865, 0.08771857596002519, 0.09776769299060106, 0.0854302030056715, 0.08572163502685726, 0.09210917796008289, 0.09830486308783293, 0.09170945291407406, 0.0890005910769105, 0.22446245816536248, 0.0976351669523865, 0.08771290211006999, 0.0843653199262917, 0.08514425298199058, 0.08538098284043372]
[0.0017926873430153545, 0.0017757436726242304, 0.001665123961675836, 0.001739523511341944, 0.0017210771614799695, 0.0017123967361617454, 0.001729900858422019, 0.001688975389400611, 0.0017070471812799877, 0.0017004829394269962, 0.0017765702879322426, 0.001851146061885722, 0.001788229063837504, 0.0018197383246935752, 0.001774678182579121, 0.00181041151399211, 0.0017810575283911763, 0.00179504848807594, 0.001783243879409773, 0.0018250936765831951, 0.0018254432694188186, 0.001754427794367075, 0.0017573943276110353, 0.0020277676913811236, 0.0020175729403082206, 0.035475409675237476, 0.001825484489946037, 0.001754752778429158, 0.0018080846758141201, 0.001811060446257494, 0.0018207943494602734, 0.001811055898932474, 0.001978051492336149, 0.0017952467552481257, 0.0018062205291448198, 0.001755746447348169, 0.0017911944899479954, 0.0018488159403204918, 0.0018009453660295326, 0.001798203386062262, 0.0016664811837657982, 0.0016844775717781515, 0.0017438384856344486, 0.0017419557552784681, 0.0016650311430269967, 0.0017121099600834505, 0.0016435436495788852, 0.004446563287162963, 0.0018323762057235046, 0.0018227869996382874, 0.001846714163845291, 0.002012276385283592, 0.00178626355030859, 0.0018635454102970508, 0.0017094652843186442, 0.0016681530408333152, 0.001623431491494483, 0.0017489817953307409, 0.0017241439592017203, 0.0017302793887804967, 0.0017684494306770514, 0.0017306547355362956, 0.0018855353902873335, 0.0017255520429082063, 0.0017426513296989153, 0.001838570774285769, 0.0022124273474423252, 0.001972096406721643, 0.0018606161632175957, 0.001785724100713827, 0.004506790692138733, 0.0017294549354713183, 0.001682154777250728, 0.001759906223385918, 0.0016495726755535115, 0.0016614369601391408, 0.0016460262467058338, 0.0016714333252487133, 0.0016250160608288584, 0.001818750039389243, 0.002020550996293219, 0.0017772293697130314, 0.0017424402663446202, 0.0017783966115010636, 0.0017564064081834288, 0.0017149002215235817, 0.0017882037328138035, 0.0017741652471678598, 0.0017437876952926116, 0.0017336960596849723, 0.001946938366686203, 0.0018143220186917757, 0.0017594827757197984, 0.002216840794842158, 0.0017641958999162425, 0.001735031595263554, 0.0017486677353023266, 0.0017968416736670295, 0.0018149740015137561, 0.0017978898392115928, 0.0017716094509375338, 0.0018590585119566138, 0.0018260834491526593, 0.0017822276971930144, 0.0017997793454144681, 0.0018569200195144026, 0.0018478603035743748, 0.0018732509794359912, 0.0017774171404996697, 0.0017541034280189446, 0.001721843345356839, 0.001743849590230657, 0.0017793109374386923, 0.0017984994088432618, 0.001950538284810526, 0.002132841799294158, 0.0019078733047888595, 0.0018270599171139148, 0.0021820364862072225, 0.00184346283595933, 0.001853378489613533, 0.0019022000440377363, 0.0019146808356578862, 0.0017291554487405382, 0.0017787184072088223, 0.001805623042948392, 0.0017198677155740407, 0.0017272540444166077, 0.001776609774108748, 0.001790175019592351, 0.0019952590406245117, 0.00174347353072799, 0.0017494211229970868, 0.001879779142042508, 0.0020062216956700596, 0.0018716214880423279, 0.0018163385934063367, 0.004580866493170663, 0.0019925544275997244, 0.0017900592267361222, 0.001721741222985545, 0.0017376378159589913, 0.0017424690375598718]
[557.8217550852843, 563.1443408283018, 600.5558883397287, 574.8700684295761, 581.0314739985807, 583.9768196717379, 578.0678095692605, 592.0749386140452, 585.8068897956144, 588.0682345081129, 562.8823170086358, 540.2058868230646, 559.2124746334342, 549.5295595142172, 563.4824442067072, 552.3606054597585, 561.4641773549544, 557.0880155286895, 560.7757926700328, 547.9170810958732, 547.8121488368018, 569.9864099341625, 569.0242561323041, 493.1531379311476, 495.64502973916376, 28.18853986901297, 547.7997789121512, 569.8808472012752, 553.0714426025061, 552.1626857162455, 549.2108432214894, 552.1640721246923, 505.5480122102203, 557.0264906908507, 553.6422512446272, 569.5583217670026, 558.2866660275588, 540.886725493426, 555.2639290800128, 556.1106200505038, 600.0667812763836, 593.655870967988, 573.4476032258096, 574.0673934856287, 600.5893668643447, 584.074634990885, 608.4414005409736, 224.8927847011549, 545.7394594387646, 548.6104521254756, 541.5023177803358, 496.9496274534222, 559.8278035888056, 536.6115547678546, 584.9782438831909, 599.465382085361, 615.9791806671371, 571.7612399795706, 579.9979721316314, 577.9413466311943, 565.4671163637117, 577.816001924913, 530.3533442814943, 579.524682613816, 573.838256085785, 543.9007374565011, 451.9922433412555, 507.0746017241477, 537.4563651380319, 559.9969220330615, 221.88738468469725, 578.2168586702584, 594.4756175376294, 568.2120937535414, 606.2176070323495, 601.8886205085101, 607.5237269158279, 598.2888966577219, 615.3785332373504, 549.8281667863558, 494.91450690160246, 562.6735732830421, 573.9077656290915, 562.3042652763185, 569.3443131047641, 583.1243051047963, 559.220396227707, 563.6453546795163, 573.4643057176738, 576.8023722576313, 513.626942234466, 551.1700732822799, 568.3488430802646, 451.0923844087781, 566.8304750325495, 576.3583802911083, 571.8639280704234, 556.5320610352834, 550.9720795812848, 556.2076041536104, 564.4584925141379, 537.9066842535926, 547.6200994341309, 561.0955331773752, 555.6236671722176, 538.5261559415504, 541.166449685438, 533.8312970219748, 562.6141310412247, 570.0918110224455, 580.7729272797216, 573.4439515897303, 562.0153166930419, 556.0190874030748, 512.6789911212327, 468.85802797513617, 524.1438189265235, 547.3274251342746, 458.2874788396332, 542.4573690847445, 539.5551991156001, 525.7070638466255, 522.2802575638666, 578.3170048293623, 562.2025363583026, 553.8254531616441, 581.4400671311106, 578.9536306095362, 562.8698066246195, 558.6045995813944, 501.18805610674104, 573.5676409050205, 571.61765503712, 531.9773890636067, 498.44939976387263, 534.2960670140502, 550.5581413235368, 218.2993111654399, 501.8683485623137, 558.6407338171347, 580.8073749119937, 575.493921009141, 573.8982894068439]
Elapsed: 0.10256857499219738~0.13867681913945562
Time per graph: 0.0020932362243305585~0.00283013916611134
Speed: 546.3528501176637~71.49119502693169
Total Time: 0.0861
best val loss: 0.26465892791748047 test_score: 0.9388

Testing...
Test loss: 0.4443 score: 0.9184 time: 0.08s
test Score 0.9184
Epoch Time List: [0.31099370401352644, 0.31831427616998553, 0.32284665503539145, 0.3025944430846721, 0.30491525470279157, 0.3056305758655071, 0.2985287131741643, 0.3058923401404172, 0.3031960120424628, 0.3046769257634878, 0.3124565358739346, 0.31756879622116685, 0.3129382347688079, 0.32057230011560023, 0.31602387269958854, 0.3197938709054142, 0.32514972309581935, 0.31694327807053924, 0.3176526839379221, 0.3222224081400782, 0.3351619769819081, 0.31867693923413754, 0.31132117682136595, 0.3431999038439244, 0.3483598700258881, 5.876168746035546, 0.3112674979493022, 0.3210631359834224, 0.31363874091766775, 0.3196337940171361, 0.32438020198605955, 0.31843655509874225, 0.32003137306310236, 0.41443767095915973, 0.32060050778090954, 0.31182512012310326, 0.3165409779176116, 0.3263592498842627, 0.3247044701129198, 0.3177550849504769, 0.3683477931190282, 0.2985377749428153, 0.301327157067135, 0.304742360021919, 0.3064157357439399, 0.32410791679285467, 0.29763513430953026, 0.43848468898795545, 0.3254619671497494, 0.32580348500050604, 0.34521875297650695, 0.3362231468781829, 0.31495678797364235, 0.32560492190532386, 0.30831063189543784, 0.36611390789039433, 0.29712144122458994, 0.3092544819228351, 0.30963098094798625, 0.31437275116331875, 0.3127486682496965, 0.31106371292844415, 0.3404822109732777, 0.3898457877803594, 0.309959001140669, 0.31834367010742426, 0.3376716561615467, 0.3347922791726887, 0.35782345198094845, 0.3228419590741396, 0.4608151069842279, 0.2999815260991454, 0.3077787689398974, 0.30966246384195983, 0.3038583716843277, 0.2966262467671186, 0.29173134872689843, 0.2993877022527158, 0.3276151609607041, 0.3139248692896217, 0.3278150318656117, 0.3235171341802925, 0.3104537967592478, 0.31026187911629677, 0.31178554194048047, 0.31133450707420707, 0.44833043892867863, 0.3136424000840634, 0.3074743398465216, 0.30735858296975493, 0.3212160796392709, 0.33497832086868584, 0.31563008087687194, 0.3351059730630368, 0.3795883981510997, 0.31321933399885893, 0.3077023511286825, 0.3139203598257154, 0.3294808058999479, 0.32183202798478305, 0.3096358261536807, 0.47451919014565647, 0.335008829832077, 0.32396014919504523, 0.323829913046211, 0.33295877603814006, 0.3311936550308019, 0.3334300257265568, 0.4693582581821829, 0.3178354010451585, 0.3156597388442606, 0.31423775223083794, 0.3135733709204942, 0.3230154509656131, 0.3325871452689171, 0.3313327331561595, 0.37444654991850257, 0.32887426810339093, 0.3570574370678514, 0.3397505700122565, 0.3237180607393384, 0.33320533693768084, 0.33087202766910195, 0.39472166216000915, 0.3151025311090052, 0.33206799114122987, 0.3132225959561765, 0.31313096499070525, 0.3152324981056154, 0.31988766603171825, 0.3304290061350912, 0.42051080986857414, 0.3124681932386011, 0.3353935570921749, 0.3497506428975612, 0.33152873418293893, 0.33052214211784303, 0.45828465023078024, 0.3272271358873695, 0.33621210884302855, 0.3116574778687209, 0.3167343698441982, 0.3089325102046132]
Total Epoch List: [143]
Total Time List: [0.08611331391148269]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d4207b20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8111;  Loss pred: 0.8111; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7678 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7872 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8134;  Loss pred: 0.8134; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7676 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7872 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.8219;  Loss pred: 0.8219; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7639 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7833 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.8241;  Loss pred: 0.8241; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7564 score: 0.5102 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7753 score: 0.4898 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7996;  Loss pred: 0.7996; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7454 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7635 score: 0.4898 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.7422;  Loss pred: 0.7422; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7320 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7492 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.7320;  Loss pred: 0.7320; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7166 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7327 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7144 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4898 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6686 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6800 score: 0.4898 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5954;  Loss pred: 0.5954; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6583 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6684 score: 0.4898 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6519 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6608 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.14s
Val loss: 0.6478 score: 0.5714 time: 0.09s
Test loss: 0.6559 score: 0.5714 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.13s
Val loss: 0.6455 score: 0.7143 time: 0.09s
Test loss: 0.6529 score: 0.6122 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.4991;  Loss pred: 0.4991; Loss self: 0.0000; time: 0.14s
Val loss: 0.6441 score: 0.8980 time: 0.09s
Test loss: 0.6511 score: 0.8163 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.4997;  Loss pred: 0.4997; Loss self: 0.0000; time: 0.13s
Val loss: 0.6432 score: 0.9184 time: 0.10s
Test loss: 0.6500 score: 0.8980 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.14s
Val loss: 0.6424 score: 0.9592 time: 0.09s
Test loss: 0.6491 score: 0.8980 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4762;  Loss pred: 0.4762; Loss self: 0.0000; time: 0.13s
Val loss: 0.6412 score: 0.8980 time: 0.10s
Test loss: 0.6480 score: 0.8367 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4664;  Loss pred: 0.4664; Loss self: 0.0000; time: 0.13s
Val loss: 0.6396 score: 0.8571 time: 0.08s
Test loss: 0.6466 score: 0.7755 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4677;  Loss pred: 0.4677; Loss self: 0.0000; time: 0.14s
Val loss: 0.6372 score: 0.8367 time: 0.12s
Test loss: 0.6447 score: 0.7959 time: 0.12s
Epoch 21/1000, LR 0.000270
Train loss: 0.4449;  Loss pred: 0.4449; Loss self: 0.0000; time: 0.13s
Val loss: 0.6341 score: 0.8367 time: 0.08s
Test loss: 0.6423 score: 0.7959 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4523;  Loss pred: 0.4523; Loss self: 0.0000; time: 0.13s
Val loss: 0.6302 score: 0.8367 time: 0.09s
Test loss: 0.6391 score: 0.7959 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.4289;  Loss pred: 0.4289; Loss self: 0.0000; time: 0.12s
Val loss: 0.6252 score: 0.8367 time: 0.09s
Test loss: 0.6351 score: 0.7959 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4329;  Loss pred: 0.4329; Loss self: 0.0000; time: 0.13s
Val loss: 0.6194 score: 0.8980 time: 0.09s
Test loss: 0.6306 score: 0.7959 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.4099;  Loss pred: 0.4099; Loss self: 0.0000; time: 0.13s
Val loss: 0.6130 score: 0.8980 time: 0.09s
Test loss: 0.6253 score: 0.8163 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3991;  Loss pred: 0.3991; Loss self: 0.0000; time: 0.13s
Val loss: 0.6058 score: 0.8980 time: 0.09s
Test loss: 0.6194 score: 0.8367 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3897;  Loss pred: 0.3897; Loss self: 0.0000; time: 0.14s
Val loss: 0.5982 score: 0.8980 time: 0.09s
Test loss: 0.6132 score: 0.8571 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3920;  Loss pred: 0.3920; Loss self: 0.0000; time: 0.13s
Val loss: 0.5901 score: 0.9184 time: 0.08s
Test loss: 0.6064 score: 0.8776 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3721;  Loss pred: 0.3721; Loss self: 0.0000; time: 0.13s
Val loss: 0.5817 score: 0.9388 time: 0.09s
Test loss: 0.5995 score: 0.8980 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3723;  Loss pred: 0.3723; Loss self: 0.0000; time: 0.15s
Val loss: 0.5732 score: 0.9388 time: 0.09s
Test loss: 0.5922 score: 0.8980 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3624;  Loss pred: 0.3624; Loss self: 0.0000; time: 0.14s
Val loss: 0.5647 score: 0.8980 time: 0.09s
Test loss: 0.5848 score: 0.8980 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3504;  Loss pred: 0.3504; Loss self: 0.0000; time: 0.14s
Val loss: 0.5564 score: 0.8980 time: 0.09s
Test loss: 0.5775 score: 0.9184 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.14s
Val loss: 0.5482 score: 0.8980 time: 0.09s
Test loss: 0.5701 score: 0.8980 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3461;  Loss pred: 0.3461; Loss self: 0.0000; time: 0.14s
Val loss: 0.5405 score: 0.9184 time: 0.09s
Test loss: 0.5630 score: 0.8571 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3366;  Loss pred: 0.3366; Loss self: 0.0000; time: 0.14s
Val loss: 0.5334 score: 0.9184 time: 0.09s
Test loss: 0.5563 score: 0.8571 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3264;  Loss pred: 0.3264; Loss self: 0.0000; time: 0.14s
Val loss: 0.5271 score: 0.9184 time: 0.09s
Test loss: 0.5505 score: 0.8571 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.3229;  Loss pred: 0.3229; Loss self: 0.0000; time: 0.14s
Val loss: 0.5214 score: 0.9184 time: 0.09s
Test loss: 0.5450 score: 0.8571 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.3169;  Loss pred: 0.3169; Loss self: 0.0000; time: 0.15s
Val loss: 0.5158 score: 0.9184 time: 0.09s
Test loss: 0.5398 score: 0.8776 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.3161;  Loss pred: 0.3161; Loss self: 0.0000; time: 0.14s
Val loss: 0.5113 score: 0.8980 time: 0.09s
Test loss: 0.5354 score: 0.8776 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.3009;  Loss pred: 0.3009; Loss self: 0.0000; time: 0.14s
Val loss: 0.5073 score: 0.8776 time: 0.09s
Test loss: 0.5313 score: 0.8571 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.14s
Val loss: 0.5038 score: 0.8776 time: 0.09s
Test loss: 0.5277 score: 0.8571 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2799;  Loss pred: 0.2799; Loss self: 0.0000; time: 0.14s
Val loss: 0.5010 score: 0.8776 time: 0.09s
Test loss: 0.5249 score: 0.8367 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.14s
Val loss: 0.4989 score: 0.8571 time: 0.09s
Test loss: 0.5225 score: 0.8163 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2796;  Loss pred: 0.2796; Loss self: 0.0000; time: 0.14s
Val loss: 0.4967 score: 0.8367 time: 0.09s
Test loss: 0.5202 score: 0.8367 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2663;  Loss pred: 0.2663; Loss self: 0.0000; time: 0.14s
Val loss: 0.4942 score: 0.8367 time: 0.09s
Test loss: 0.5174 score: 0.8367 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2661;  Loss pred: 0.2661; Loss self: 0.0000; time: 0.14s
Val loss: 0.4916 score: 0.8367 time: 0.09s
Test loss: 0.5146 score: 0.8367 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2637;  Loss pred: 0.2637; Loss self: 0.0000; time: 0.14s
Val loss: 0.4890 score: 0.8367 time: 0.09s
Test loss: 0.5117 score: 0.8367 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2576;  Loss pred: 0.2576; Loss self: 0.0000; time: 0.14s
Val loss: 0.4860 score: 0.8367 time: 0.09s
Test loss: 0.5087 score: 0.8367 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2603;  Loss pred: 0.2603; Loss self: 0.0000; time: 0.15s
Val loss: 0.4827 score: 0.8367 time: 0.10s
Test loss: 0.5053 score: 0.8367 time: 0.10s
Epoch 50/1000, LR 0.000269
Train loss: 0.2479;  Loss pred: 0.2479; Loss self: 0.0000; time: 0.16s
Val loss: 0.4791 score: 0.8367 time: 0.10s
Test loss: 0.5017 score: 0.8367 time: 0.10s
Epoch 51/1000, LR 0.000269
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 0.15s
Val loss: 0.4754 score: 0.8571 time: 0.14s
Test loss: 0.4981 score: 0.8571 time: 0.10s
Epoch 52/1000, LR 0.000269
Train loss: 0.2398;  Loss pred: 0.2398; Loss self: 0.0000; time: 0.16s
Val loss: 0.4723 score: 0.8571 time: 0.10s
Test loss: 0.4951 score: 0.8571 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2390;  Loss pred: 0.2390; Loss self: 0.0000; time: 0.13s
Val loss: 0.4687 score: 0.8571 time: 0.09s
Test loss: 0.4915 score: 0.8776 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.2349;  Loss pred: 0.2349; Loss self: 0.0000; time: 0.14s
Val loss: 0.4653 score: 0.8571 time: 0.09s
Test loss: 0.4881 score: 0.8776 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2287;  Loss pred: 0.2287; Loss self: 0.0000; time: 0.13s
Val loss: 0.4619 score: 0.8571 time: 0.09s
Test loss: 0.4847 score: 0.8776 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.12s
Val loss: 0.4586 score: 0.8571 time: 0.09s
Test loss: 0.4814 score: 0.8776 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2242;  Loss pred: 0.2242; Loss self: 0.0000; time: 0.13s
Val loss: 0.4559 score: 0.8571 time: 0.09s
Test loss: 0.4787 score: 0.8776 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.13s
Val loss: 0.4528 score: 0.8571 time: 0.08s
Test loss: 0.4756 score: 0.8776 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 0.13s
Val loss: 0.4500 score: 0.8571 time: 0.09s
Test loss: 0.4727 score: 0.8776 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.13s
Val loss: 0.4475 score: 0.8571 time: 0.09s
Test loss: 0.4702 score: 0.8776 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.2122;  Loss pred: 0.2122; Loss self: 0.0000; time: 0.13s
Val loss: 0.4443 score: 0.8571 time: 0.10s
Test loss: 0.4670 score: 0.8776 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1948;  Loss pred: 0.1948; Loss self: 0.0000; time: 0.14s
Val loss: 0.4410 score: 0.8367 time: 0.09s
Test loss: 0.4635 score: 0.8776 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.13s
Val loss: 0.4385 score: 0.8367 time: 0.08s
Test loss: 0.4609 score: 0.8776 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.2057;  Loss pred: 0.2057; Loss self: 0.0000; time: 0.13s
Val loss: 0.4355 score: 0.8367 time: 0.09s
Test loss: 0.4577 score: 0.8571 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1957;  Loss pred: 0.1957; Loss self: 0.0000; time: 0.13s
Val loss: 0.4330 score: 0.8367 time: 0.09s
Test loss: 0.4550 score: 0.8571 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.13s
Val loss: 0.4313 score: 0.8163 time: 0.08s
Test loss: 0.4529 score: 0.8163 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.23s
Val loss: 0.4291 score: 0.8367 time: 0.09s
Test loss: 0.4506 score: 0.7959 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 0.25s
Val loss: 0.4280 score: 0.8367 time: 0.09s
Test loss: 0.4492 score: 0.7959 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1785;  Loss pred: 0.1785; Loss self: 0.0000; time: 0.13s
Val loss: 0.4275 score: 0.8163 time: 0.09s
Test loss: 0.4485 score: 0.7959 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1742;  Loss pred: 0.1742; Loss self: 0.0000; time: 0.14s
Val loss: 0.4240 score: 0.8163 time: 0.09s
Test loss: 0.4449 score: 0.7959 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.15s
Val loss: 0.4208 score: 0.8163 time: 0.09s
Test loss: 0.4417 score: 0.7959 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1732;  Loss pred: 0.1732; Loss self: 0.0000; time: 0.14s
Val loss: 0.4157 score: 0.8163 time: 0.09s
Test loss: 0.4368 score: 0.7959 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1662;  Loss pred: 0.1662; Loss self: 0.0000; time: 0.14s
Val loss: 0.4082 score: 0.8163 time: 0.09s
Test loss: 0.4297 score: 0.8163 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1683;  Loss pred: 0.1683; Loss self: 0.0000; time: 0.13s
Val loss: 0.3991 score: 0.8163 time: 0.08s
Test loss: 0.4214 score: 0.8163 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1649;  Loss pred: 0.1649; Loss self: 0.0000; time: 0.12s
Val loss: 0.3910 score: 0.8163 time: 0.20s
Test loss: 0.4139 score: 0.8163 time: 0.19s
Epoch 76/1000, LR 0.000267
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.14s
Val loss: 0.3807 score: 0.8367 time: 0.09s
Test loss: 0.4045 score: 0.8163 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1555;  Loss pred: 0.1555; Loss self: 0.0000; time: 0.14s
Val loss: 0.3703 score: 0.8571 time: 0.09s
Test loss: 0.3951 score: 0.8163 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.1563;  Loss pred: 0.1563; Loss self: 0.0000; time: 0.13s
Val loss: 0.3596 score: 0.8571 time: 0.09s
Test loss: 0.3856 score: 0.8367 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.13s
Val loss: 0.3514 score: 0.8571 time: 0.09s
Test loss: 0.3783 score: 0.8367 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 0.13s
Val loss: 0.3434 score: 0.8571 time: 0.09s
Test loss: 0.3713 score: 0.8367 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.13s
Val loss: 0.3362 score: 0.8571 time: 0.10s
Test loss: 0.3650 score: 0.8367 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1407;  Loss pred: 0.1407; Loss self: 0.0000; time: 0.14s
Val loss: 0.3303 score: 0.8571 time: 0.19s
Test loss: 0.3599 score: 0.8367 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1540;  Loss pred: 0.1540; Loss self: 0.0000; time: 0.14s
Val loss: 0.3248 score: 0.8571 time: 0.09s
Test loss: 0.3553 score: 0.8367 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.14s
Val loss: 0.3186 score: 0.8571 time: 0.09s
Test loss: 0.3501 score: 0.8367 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.1447;  Loss pred: 0.1447; Loss self: 0.0000; time: 0.13s
Val loss: 0.3129 score: 0.8571 time: 0.08s
Test loss: 0.3455 score: 0.8367 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.14s
Val loss: 0.3078 score: 0.8571 time: 0.09s
Test loss: 0.3414 score: 0.8367 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1429;  Loss pred: 0.1429; Loss self: 0.0000; time: 0.13s
Val loss: 0.3020 score: 0.8776 time: 0.09s
Test loss: 0.3368 score: 0.8367 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.13s
Val loss: 0.2971 score: 0.8776 time: 0.09s
Test loss: 0.3329 score: 0.8367 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 0.14s
Val loss: 0.2926 score: 0.8776 time: 0.09s
Test loss: 0.3292 score: 0.8367 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1350;  Loss pred: 0.1350; Loss self: 0.0000; time: 0.14s
Val loss: 0.2866 score: 0.8980 time: 0.19s
Test loss: 0.3247 score: 0.8367 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.1223;  Loss pred: 0.1223; Loss self: 0.0000; time: 0.13s
Val loss: 0.2811 score: 0.9184 time: 0.09s
Test loss: 0.3206 score: 0.8367 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.1303;  Loss pred: 0.1303; Loss self: 0.0000; time: 0.13s
Val loss: 0.2778 score: 0.9184 time: 0.08s
Test loss: 0.3182 score: 0.8367 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 0.13s
Val loss: 0.2736 score: 0.9184 time: 0.09s
Test loss: 0.3151 score: 0.8367 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.1201;  Loss pred: 0.1201; Loss self: 0.0000; time: 0.13s
Val loss: 0.2698 score: 0.9184 time: 0.09s
Test loss: 0.3125 score: 0.8571 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 0.13s
Val loss: 0.2652 score: 0.9184 time: 0.09s
Test loss: 0.3093 score: 0.8776 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.1264;  Loss pred: 0.1264; Loss self: 0.0000; time: 0.14s
Val loss: 0.2603 score: 0.9184 time: 0.08s
Test loss: 0.3058 score: 0.8776 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 0.13s
Val loss: 0.2564 score: 0.9184 time: 0.09s
Test loss: 0.3033 score: 0.8776 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.13s
Val loss: 0.2521 score: 0.9184 time: 0.09s
Test loss: 0.3006 score: 0.8776 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.1177;  Loss pred: 0.1177; Loss self: 0.0000; time: 0.14s
Val loss: 0.2460 score: 0.9184 time: 0.09s
Test loss: 0.2964 score: 0.8776 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.1186;  Loss pred: 0.1186; Loss self: 0.0000; time: 0.13s
Val loss: 0.2403 score: 0.9184 time: 0.09s
Test loss: 0.2926 score: 0.8776 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.13s
Val loss: 0.2336 score: 0.9184 time: 0.09s
Test loss: 0.2879 score: 0.8776 time: 0.46s
Epoch 102/1000, LR 0.000264
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 1.49s
Val loss: 0.2293 score: 0.9184 time: 3.27s
Test loss: 0.2851 score: 0.8776 time: 1.81s
Epoch 103/1000, LR 0.000264
Train loss: 0.1146;  Loss pred: 0.1146; Loss self: 0.0000; time: 0.12s
Val loss: 0.2260 score: 0.9184 time: 0.20s
Test loss: 0.2831 score: 0.8776 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.13s
Val loss: 0.2215 score: 0.9184 time: 0.08s
Test loss: 0.2801 score: 0.8776 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.13s
Val loss: 0.2183 score: 0.9388 time: 0.08s
Test loss: 0.2780 score: 0.8776 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.1033;  Loss pred: 0.1033; Loss self: 0.0000; time: 0.13s
Val loss: 0.2137 score: 0.9388 time: 0.08s
Test loss: 0.2747 score: 0.8776 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.1038;  Loss pred: 0.1038; Loss self: 0.0000; time: 0.13s
Val loss: 0.2098 score: 0.9388 time: 0.08s
Test loss: 0.2720 score: 0.8980 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.12s
Val loss: 0.2042 score: 0.9388 time: 0.08s
Test loss: 0.2681 score: 0.8980 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.13s
Val loss: 0.1988 score: 0.9388 time: 0.09s
Test loss: 0.2643 score: 0.8980 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.13s
Val loss: 0.1952 score: 0.9388 time: 0.08s
Test loss: 0.2618 score: 0.8980 time: 0.10s
Epoch 111/1000, LR 0.000263
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.13s
Val loss: 0.1921 score: 0.9388 time: 0.09s
Test loss: 0.2597 score: 0.8980 time: 0.20s
Epoch 112/1000, LR 0.000263
Train loss: 0.1003;  Loss pred: 0.1003; Loss self: 0.0000; time: 0.12s
Val loss: 0.1903 score: 0.9388 time: 0.08s
Test loss: 0.2584 score: 0.8980 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.12s
Val loss: 0.1899 score: 0.9388 time: 0.08s
Test loss: 0.2583 score: 0.8980 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.13s
Val loss: 0.1896 score: 0.9388 time: 0.08s
Test loss: 0.2585 score: 0.8980 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.1028;  Loss pred: 0.1028; Loss self: 0.0000; time: 0.13s
Val loss: 0.1903 score: 0.9388 time: 0.08s
Test loss: 0.2593 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0916;  Loss pred: 0.0916; Loss self: 0.0000; time: 0.13s
Val loss: 0.1915 score: 0.9388 time: 0.08s
Test loss: 0.2603 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0955;  Loss pred: 0.0955; Loss self: 0.0000; time: 0.12s
Val loss: 0.1912 score: 0.9388 time: 0.08s
Test loss: 0.2602 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.13s
Val loss: 0.1877 score: 0.9388 time: 0.08s
Test loss: 0.2575 score: 0.8980 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.13s
Val loss: 0.1849 score: 0.9388 time: 0.08s
Test loss: 0.2555 score: 0.8980 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.24s
Val loss: 0.1832 score: 0.9388 time: 0.08s
Test loss: 0.2538 score: 0.8980 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.13s
Val loss: 0.1805 score: 0.9388 time: 0.08s
Test loss: 0.2518 score: 0.8980 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.13s
Val loss: 0.1801 score: 0.9388 time: 0.08s
Test loss: 0.2512 score: 0.8980 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0999;  Loss pred: 0.0999; Loss self: 0.0000; time: 0.14s
Val loss: 0.1797 score: 0.9388 time: 0.09s
Test loss: 0.2508 score: 0.8980 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 0.0872;  Loss pred: 0.0872; Loss self: 0.0000; time: 0.13s
Val loss: 0.1783 score: 0.9388 time: 0.09s
Test loss: 0.2497 score: 0.8980 time: 0.08s
Epoch 125/1000, LR 0.000261
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.13s
Val loss: 0.1779 score: 0.9388 time: 0.09s
Test loss: 0.2492 score: 0.8980 time: 0.08s
Epoch 126/1000, LR 0.000261
Train loss: 0.0855;  Loss pred: 0.0855; Loss self: 0.0000; time: 0.13s
Val loss: 0.1787 score: 0.9388 time: 0.09s
Test loss: 0.2495 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0871;  Loss pred: 0.0871; Loss self: 0.0000; time: 0.13s
Val loss: 0.1792 score: 0.9388 time: 0.09s
Test loss: 0.2495 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.23s
Val loss: 0.1817 score: 0.9388 time: 0.08s
Test loss: 0.2511 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.13s
Val loss: 0.1845 score: 0.9388 time: 0.08s
Test loss: 0.2530 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.13s
Val loss: 0.1874 score: 0.9388 time: 0.08s
Test loss: 0.2553 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.12s
Val loss: 0.1897 score: 0.9388 time: 0.08s
Test loss: 0.2568 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 0.12s
Val loss: 0.1908 score: 0.9388 time: 0.08s
Test loss: 0.2576 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.12s
Val loss: 0.1912 score: 0.9388 time: 0.08s
Test loss: 0.2578 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.13s
Val loss: 0.1902 score: 0.9388 time: 0.08s
Test loss: 0.2569 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.13s
Val loss: 0.1872 score: 0.9388 time: 0.08s
Test loss: 0.2548 score: 0.8980 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.13s
Val loss: 0.1848 score: 0.9388 time: 0.09s
Test loss: 0.2530 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.13s
Val loss: 0.1824 score: 0.9388 time: 0.09s
Test loss: 0.2510 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.12s
Val loss: 0.1812 score: 0.9388 time: 0.09s
Test loss: 0.2499 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0750;  Loss pred: 0.0750; Loss self: 0.0000; time: 0.13s
Val loss: 0.1798 score: 0.9388 time: 0.08s
Test loss: 0.2485 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.13s
Val loss: 0.1779 score: 0.9388 time: 0.09s
Test loss: 0.2468 score: 0.8980 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.13s
Val loss: 0.1772 score: 0.9388 time: 0.09s
Test loss: 0.2460 score: 0.8980 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.0688;  Loss pred: 0.0688; Loss self: 0.0000; time: 0.13s
Val loss: 0.1779 score: 0.9388 time: 0.09s
Test loss: 0.2463 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.12s
Val loss: 0.1801 score: 0.9388 time: 0.09s
Test loss: 0.2482 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0730;  Loss pred: 0.0730; Loss self: 0.0000; time: 0.13s
Val loss: 0.1832 score: 0.9388 time: 0.19s
Test loss: 0.2509 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.13s
Val loss: 0.1880 score: 0.9388 time: 0.08s
Test loss: 0.2549 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0696;  Loss pred: 0.0696; Loss self: 0.0000; time: 0.13s
Val loss: 0.1909 score: 0.9388 time: 0.10s
Test loss: 0.2573 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.13s
Val loss: 0.1924 score: 0.9388 time: 0.09s
Test loss: 0.2585 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0639;  Loss pred: 0.0639; Loss self: 0.0000; time: 0.13s
Val loss: 0.1921 score: 0.9388 time: 0.08s
Test loss: 0.2585 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0644;  Loss pred: 0.0644; Loss self: 0.0000; time: 0.13s
Val loss: 0.1919 score: 0.9388 time: 0.09s
Test loss: 0.2584 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.14s
Val loss: 0.1915 score: 0.9388 time: 0.09s
Test loss: 0.2581 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.13s
Val loss: 0.1910 score: 0.9388 time: 0.11s
Test loss: 0.2575 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.25s
Val loss: 0.1883 score: 0.9388 time: 0.09s
Test loss: 0.2555 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.13s
Val loss: 0.1856 score: 0.9388 time: 0.09s
Test loss: 0.2531 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.13s
Val loss: 0.1827 score: 0.9388 time: 0.09s
Test loss: 0.2509 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.13s
Val loss: 0.1783 score: 0.9388 time: 0.09s
Test loss: 0.2475 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.13s
Val loss: 0.1733 score: 0.9388 time: 0.09s
Test loss: 0.2437 score: 0.8980 time: 0.09s
Epoch 157/1000, LR 0.000256
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.14s
Val loss: 0.1705 score: 0.9388 time: 0.09s
Test loss: 0.2416 score: 0.8980 time: 0.09s
Epoch 158/1000, LR 0.000256
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.13s
Val loss: 0.1677 score: 0.9388 time: 0.09s
Test loss: 0.2396 score: 0.8980 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.13s
Val loss: 0.1660 score: 0.9388 time: 0.09s
Test loss: 0.2386 score: 0.9184 time: 0.21s
Epoch 160/1000, LR 0.000255
Train loss: 0.0625;  Loss pred: 0.0625; Loss self: 0.0000; time: 0.13s
Val loss: 0.1678 score: 0.9388 time: 0.08s
Test loss: 0.2400 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.13s
Val loss: 0.1680 score: 0.9388 time: 0.09s
Test loss: 0.2402 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0570;  Loss pred: 0.0570; Loss self: 0.0000; time: 0.13s
Val loss: 0.1713 score: 0.9388 time: 0.08s
Test loss: 0.2426 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.13s
Val loss: 0.1751 score: 0.9388 time: 0.08s
Test loss: 0.2451 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.12s
Val loss: 0.1788 score: 0.9388 time: 0.09s
Test loss: 0.2474 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.13s
Val loss: 0.1827 score: 0.9388 time: 0.08s
Test loss: 0.2499 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.14s
Val loss: 0.1870 score: 0.9388 time: 0.09s
Test loss: 0.2531 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.13s
Val loss: 0.1903 score: 0.9388 time: 0.09s
Test loss: 0.2555 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.13s
Val loss: 0.1931 score: 0.9388 time: 0.21s
Test loss: 0.2570 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.13s
Val loss: 0.1938 score: 0.9388 time: 0.09s
Test loss: 0.2569 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.13s
Val loss: 0.1914 score: 0.9388 time: 0.08s
Test loss: 0.2549 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.13s
Val loss: 0.1862 score: 0.9388 time: 0.09s
Test loss: 0.2503 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.14s
Val loss: 0.1816 score: 0.9388 time: 0.11s
Test loss: 0.2464 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.14s
Val loss: 0.1752 score: 0.9388 time: 0.09s
Test loss: 0.2412 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.13s
Val loss: 0.1699 score: 0.9388 time: 0.08s
Test loss: 0.2372 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.13s
Val loss: 0.1681 score: 0.9388 time: 0.08s
Test loss: 0.2355 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.13s
Val loss: 0.1729 score: 0.9388 time: 0.08s
Test loss: 0.2385 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.13s
Val loss: 0.1771 score: 0.9388 time: 0.09s
Test loss: 0.2415 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.13s
Val loss: 0.1800 score: 0.9388 time: 0.09s
Test loss: 0.2433 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.13s
Val loss: 0.1788 score: 0.9388 time: 0.09s
Test loss: 0.2422 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 158,   Train_Loss: 0.0582,   Val_Loss: 0.1660,   Val_Precision: 0.8929,   Val_Recall: 1.0000,   Val_accuracy: 0.9434,   Val_Score: 0.9388,   Val_Loss: 0.1660,   Test_Precision: 0.9167,   Test_Recall: 0.9167,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2386


[0.08784167980775237, 0.08701143995858729, 0.08159107412211597, 0.08523665205575526, 0.0843327809125185, 0.08390744007192552, 0.08476514206267893, 0.08275979408062994, 0.0836453118827194, 0.08332366403192282, 0.08705194410867989, 0.09070615703240037, 0.08762322412803769, 0.08916717790998518, 0.08695923094637692, 0.0887101641856134, 0.08727181889116764, 0.08795737591572106, 0.08737895009107888, 0.08942959015257657, 0.08944672020152211, 0.08596696192398667, 0.08611232205294073, 0.09936061687767506, 0.0988610740751028, 1.7382950740866363, 0.08944874000735581, 0.08598288614302874, 0.08859614911489189, 0.0887419618666172, 0.0892189231235534, 0.08874173904769123, 0.0969245231244713, 0.08796709100715816, 0.08850480592809618, 0.08603157592006028, 0.08776853000745177, 0.0905919810757041, 0.0882463229354471, 0.08811196591705084, 0.08165757800452411, 0.08253940101712942, 0.08544808579608798, 0.08535583200864494, 0.08158652600832283, 0.08389338804408908, 0.08053363882936537, 0.2178816010709852, 0.08978643408045173, 0.08931656298227608, 0.09048899402841926, 0.098601542878896, 0.08752691396512091, 0.09131372510455549, 0.08376379893161356, 0.08173949900083244, 0.07954814308322966, 0.08570010797120631, 0.0844830540008843, 0.08478369005024433, 0.08665402210317552, 0.08480208204127848, 0.09239123412407935, 0.08455205010250211, 0.08538991515524685, 0.09008996794000268, 0.10840894002467394, 0.09663272392936051, 0.09117019199766219, 0.08750048093497753, 0.2208327439147979, 0.08474329183809459, 0.08242558408528566, 0.08623540494590998, 0.08082906110212207, 0.0814104110468179, 0.08065528608858585, 0.08190023293718696, 0.07962578698061407, 0.0891187519300729, 0.09900699881836772, 0.08708423911593854, 0.08537957305088639, 0.08714143396355212, 0.086063914000988, 0.0840301108546555, 0.08762198290787637, 0.08693409711122513, 0.08544559706933796, 0.08495110692456365, 0.09539997996762395, 0.08890177891589701, 0.08621465601027012, 0.10862519894726574, 0.08644559909589589, 0.08501654816791415, 0.085684719029814, 0.08804524200968444, 0.08893372607417405, 0.08809660212136805, 0.08680886309593916, 0.09109386708587408, 0.08947808900848031, 0.0873291571624577, 0.08818918792530894, 0.09098908095620573, 0.09054515487514436, 0.09178929799236357, 0.08709343988448381, 0.08595106797292829, 0.08437032392248511, 0.0854486299213022, 0.08718623593449593, 0.08812647103331983, 0.09557637595571578, 0.10450924816541374, 0.09348579193465412, 0.08952593593858182, 0.1069197878241539, 0.09032967896200716, 0.09081554599106312, 0.09320780215784907, 0.09381936094723642, 0.08472861698828638, 0.08715720195323229, 0.0884755291044712, 0.084273518063128, 0.08463544817641377, 0.08705387893132865, 0.08771857596002519, 0.09776769299060106, 0.0854302030056715, 0.08572163502685726, 0.09210917796008289, 0.09830486308783293, 0.09170945291407406, 0.0890005910769105, 0.22446245816536248, 0.0976351669523865, 0.08771290211006999, 0.0843653199262917, 0.08514425298199058, 0.08538098284043372, 0.08521124208346009, 0.08753331401385367, 0.08588876505382359, 0.09547983296215534, 0.09250634582713246, 0.08744988311082125, 0.08839769987389445, 0.08820500504225492, 0.0866165969055146, 0.08544057887047529, 0.09350721701048315, 0.0887324979994446, 0.088514813920483, 0.08784758299589157, 0.10192561987787485, 0.09684650180861354, 0.0876771358307451, 0.09448133897967637, 0.08380458108149469, 0.12158624897710979, 0.08246714598499238, 0.09050965798087418, 0.08660729904659092, 0.08618874405510724, 0.08781790500506759, 0.09007783816196024, 0.0865955341141671, 0.08617683500051498, 0.09394376212731004, 0.0902209710329771, 0.09038711991161108, 0.09356861608102918, 0.09335894393734634, 0.09303109999746084, 0.0939684840850532, 0.09335941704921424, 0.09567757602781057, 0.09460263093933463, 0.09432801092043519, 0.09325731103308499, 0.09345184406265616, 0.09204088780097663, 0.09233120898716152, 0.09331486118026078, 0.09251614194363356, 0.09336951584555209, 0.09236824908293784, 0.09646049910224974, 0.10632627503946424, 0.10517141595482826, 0.10619928198866546, 0.09102268400602043, 0.09372516488656402, 0.0879439131822437, 0.08636288205161691, 0.08544010599143803, 0.085882370127365, 0.09209557296708226, 0.21959308092482388, 0.09051003912463784, 0.0909148019272834, 0.09619981911964715, 0.08958501601591706, 0.08443198399618268, 0.09110909211449325, 0.0865722328890115, 0.09402977209538221, 0.09799296385608613, 0.09716963092796504, 0.09024795913137496, 0.09186062612570822, 0.08793165814131498, 0.08656610106118023, 0.08479302609339356, 0.18998764292337, 0.09250542894005775, 0.08724838891066611, 0.09461148316040635, 0.08487916900776327, 0.0874958629719913, 0.08731139404699206, 0.0910896179266274, 0.09733779705129564, 0.08601994509808719, 0.08624502783641219, 0.08624658989720047, 0.08862893097102642, 0.09136353898793459, 0.09214081591926515, 0.08571560611017048, 0.09011264401488006, 0.0827395839150995, 0.09137877798639238, 0.09057537885382771, 0.09088261099532247, 0.08501621009781957, 0.21145720104686916, 0.08989653293974698, 0.09100094507448375, 0.08923120005056262, 0.47101362398825586, 1.8164139280561358, 0.08963786298409104, 0.08676059311255813, 0.0833775419741869, 0.08405410102568567, 0.08444754802621901, 0.08542404603213072, 0.08651060308329761, 0.1002768122125417, 0.20180454687215388, 0.08331124810501933, 0.08461242914199829, 0.08417473104782403, 0.08408125885762274, 0.08364233491010964, 0.08504149899818003, 0.0858849841170013, 0.08822396909818053, 0.08476362703368068, 0.08336037211120129, 0.0897249870467931, 0.08821745892055333, 0.08784958207979798, 0.08748126402497292, 0.08586661703884602, 0.09467535116709769, 0.08241935889236629, 0.0825924789533019, 0.08281635702587664, 0.08371940511278808, 0.08510741614736617, 0.08630538592115045, 0.08367034094408154, 0.20623521716333926, 0.08922272198833525, 0.08884102385491133, 0.08629390900023282, 0.08708605682477355, 0.08801389508880675, 0.08720717113465071, 0.08568150107748806, 0.0893222470767796, 0.08661505789496005, 0.08709025499410927, 0.09582461416721344, 0.08959177508950233, 0.0856072991155088, 0.08918603602796793, 0.08650876209139824, 0.10136479698121548, 0.08862386993132532, 0.09006556589156389, 0.08988284110091627, 0.08939940785057843, 0.0906850821338594, 0.09117833687923849, 0.08944071689620614, 0.21806444693356752, 0.0837727680336684, 0.08562987810000777, 0.08436079300008714, 0.08752406807616353, 0.08610587986186147, 0.09922775696031749, 0.08899240591563284, 0.09000203292816877, 0.10370277287438512, 0.08626945200376213, 0.0862581159453839, 0.10117674805223942, 0.09312824998050928, 0.09129368490539491, 0.0857703359797597, 0.08616189402528107, 0.08570443000644445, 0.09005002491176128, 0.0869295580778271, 0.09011990902945399]
[0.0017926873430153545, 0.0017757436726242304, 0.001665123961675836, 0.001739523511341944, 0.0017210771614799695, 0.0017123967361617454, 0.001729900858422019, 0.001688975389400611, 0.0017070471812799877, 0.0017004829394269962, 0.0017765702879322426, 0.001851146061885722, 0.001788229063837504, 0.0018197383246935752, 0.001774678182579121, 0.00181041151399211, 0.0017810575283911763, 0.00179504848807594, 0.001783243879409773, 0.0018250936765831951, 0.0018254432694188186, 0.001754427794367075, 0.0017573943276110353, 0.0020277676913811236, 0.0020175729403082206, 0.035475409675237476, 0.001825484489946037, 0.001754752778429158, 0.0018080846758141201, 0.001811060446257494, 0.0018207943494602734, 0.001811055898932474, 0.001978051492336149, 0.0017952467552481257, 0.0018062205291448198, 0.001755746447348169, 0.0017911944899479954, 0.0018488159403204918, 0.0018009453660295326, 0.001798203386062262, 0.0016664811837657982, 0.0016844775717781515, 0.0017438384856344486, 0.0017419557552784681, 0.0016650311430269967, 0.0017121099600834505, 0.0016435436495788852, 0.004446563287162963, 0.0018323762057235046, 0.0018227869996382874, 0.001846714163845291, 0.002012276385283592, 0.00178626355030859, 0.0018635454102970508, 0.0017094652843186442, 0.0016681530408333152, 0.001623431491494483, 0.0017489817953307409, 0.0017241439592017203, 0.0017302793887804967, 0.0017684494306770514, 0.0017306547355362956, 0.0018855353902873335, 0.0017255520429082063, 0.0017426513296989153, 0.001838570774285769, 0.0022124273474423252, 0.001972096406721643, 0.0018606161632175957, 0.001785724100713827, 0.004506790692138733, 0.0017294549354713183, 0.001682154777250728, 0.001759906223385918, 0.0016495726755535115, 0.0016614369601391408, 0.0016460262467058338, 0.0016714333252487133, 0.0016250160608288584, 0.001818750039389243, 0.002020550996293219, 0.0017772293697130314, 0.0017424402663446202, 0.0017783966115010636, 0.0017564064081834288, 0.0017149002215235817, 0.0017882037328138035, 0.0017741652471678598, 0.0017437876952926116, 0.0017336960596849723, 0.001946938366686203, 0.0018143220186917757, 0.0017594827757197984, 0.002216840794842158, 0.0017641958999162425, 0.001735031595263554, 0.0017486677353023266, 0.0017968416736670295, 0.0018149740015137561, 0.0017978898392115928, 0.0017716094509375338, 0.0018590585119566138, 0.0018260834491526593, 0.0017822276971930144, 0.0017997793454144681, 0.0018569200195144026, 0.0018478603035743748, 0.0018732509794359912, 0.0017774171404996697, 0.0017541034280189446, 0.001721843345356839, 0.001743849590230657, 0.0017793109374386923, 0.0017984994088432618, 0.001950538284810526, 0.002132841799294158, 0.0019078733047888595, 0.0018270599171139148, 0.0021820364862072225, 0.00184346283595933, 0.001853378489613533, 0.0019022000440377363, 0.0019146808356578862, 0.0017291554487405382, 0.0017787184072088223, 0.001805623042948392, 0.0017198677155740407, 0.0017272540444166077, 0.001776609774108748, 0.001790175019592351, 0.0019952590406245117, 0.00174347353072799, 0.0017494211229970868, 0.001879779142042508, 0.0020062216956700596, 0.0018716214880423279, 0.0018163385934063367, 0.004580866493170663, 0.0019925544275997244, 0.0017900592267361222, 0.001721741222985545, 0.0017376378159589913, 0.0017424690375598718, 0.0017390049404787775, 0.001786394163548034, 0.0017528319398739509, 0.0019485680196358233, 0.001887884608716989, 0.0017846914920575765, 0.0018040346913039684, 0.0018001021437194882, 0.001767685651132951, 0.0017436852830709244, 0.00190831055123435, 0.0018108673061111144, 0.001806424773887408, 0.0017928078162426852, 0.002080114691385201, 0.00197645922058395, 0.0017893293026682673, 0.0019281905914219667, 0.0017102975730917283, 0.0024813520199410163, 0.001683002979285559, 0.0018471358771606976, 0.001767495898910019, 0.001758953960308311, 0.001792202142960563, 0.001838323227795107, 0.001767255798248308, 0.0017587109183778568, 0.0019172196352512253, 0.0018412443067954512, 0.0018446351002369607, 0.0019095635934903914, 0.0019052845701499252, 0.0018985938774992008, 0.001917724165001086, 0.0019052942254941683, 0.0019526035924042975, 0.0019306659375374414, 0.0019250614473558202, 0.0019032104292466324, 0.0019071804910746155, 0.0018783854653260537, 0.0018843103874930922, 0.0019043849220461383, 0.0018880845294619094, 0.0019055003233786141, 0.0018850663078150579, 0.0019685816143316273, 0.0021699239803972294, 0.0021463554276495564, 0.0021673322854829685, 0.0018576057960412332, 0.001912758467072735, 0.0017947737384131368, 0.0017625077969717737, 0.0017436756324783272, 0.001752701431170714, 0.0018795014891241277, 0.004481491447445385, 0.001847143655604854, 0.0018554041209649674, 0.0019632616146866766, 0.0018282656329778992, 0.0017231017142078097, 0.0018593692268263928, 0.001766780263041051, 0.0019189749407220859, 0.0019998564052262475, 0.0019830536924074498, 0.0018417950843137745, 0.0018747066556266984, 0.0017945236355370404, 0.001766655123697556, 0.001730469920273338, 0.0038772988351708166, 0.0018878658967358725, 0.001780579365523798, 0.0019308465951103338, 0.0017322279389339443, 0.001785629856571251, 0.001781865184632491, 0.0018589717944209672, 0.001986485654108074, 0.0017555090836344324, 0.0017601026089063712, 0.0017601344876979686, 0.0018087536932862535, 0.0018645620201619304, 0.0018804248146788807, 0.0017492980838810302, 0.0018390335513240829, 0.001688562937042847, 0.0018648730201304567, 0.0018484771194658717, 0.0018547471631698462, 0.0017350246958738687, 0.004315453082589167, 0.001834623121219326, 0.0018571621443772195, 0.0018210448989910738, 0.009612522938535834, 0.03706967200114563, 0.0018293441425324703, 0.0017706243492358802, 0.0017015824892691203, 0.001715389816850728, 0.0017234193474738573, 0.0017433478782067494, 0.0017655225119040329, 0.002046465555357994, 0.004118460140248038, 0.0017002295531636598, 0.0017267842682040467, 0.001717851654037225, 0.0017159440583188314, 0.0017069864267369313, 0.0017355407958812251, 0.0017527547778979856, 0.0018004891652689905, 0.001729869939462871, 0.0017012320839020672, 0.0018311221846284307, 0.0018003563045010883, 0.0017928486138734283, 0.0017853319188769982, 0.0017523799395682861, 0.0019321500238183202, 0.0016820277324972712, 0.001685560794965345, 0.0016901297352219723, 0.0017085592880160833, 0.0017368860438237994, 0.0017613344065540908, 0.0017075579784506438, 0.004208881982925291, 0.0018208718773129644, 0.0018130821194879863, 0.0017611001836782207, 0.0017772664658117052, 0.0017962019405878928, 0.001779738186421443, 0.001748602062805879, 0.0018229030015669307, 0.0017676542427542867, 0.001777352142736924, 0.001955604370759458, 0.0018284035732551497, 0.0017470877370511998, 0.0018201231842442434, 0.0017654849406407804, 0.0020686693261472546, 0.0018086504067617412, 0.0018380727732972223, 0.0018343436959370667, 0.0018244777112362944, 0.001850715961915498, 0.0018607823852905814, 0.0018253207529837989, 0.004450294835378929, 0.0017096483272177223, 0.00174754853265322, 0.0017216488367364723, 0.001786205470942113, 0.0017572628543237035, 0.0020250562644962755, 0.0018161715492986295, 0.001836776182207526, 0.0021163831198854105, 0.001760601061301268, 0.0017603697131711002, 0.0020648315929028454, 0.0019005765302144751, 0.0018631364266407124, 0.0017504150199950958, 0.0017584060005159403, 0.0017490700001315195, 0.0018377556104441078, 0.001774072613833206, 0.0018391818169276326]
[557.8217550852843, 563.1443408283018, 600.5558883397287, 574.8700684295761, 581.0314739985807, 583.9768196717379, 578.0678095692605, 592.0749386140452, 585.8068897956144, 588.0682345081129, 562.8823170086358, 540.2058868230646, 559.2124746334342, 549.5295595142172, 563.4824442067072, 552.3606054597585, 561.4641773549544, 557.0880155286895, 560.7757926700328, 547.9170810958732, 547.8121488368018, 569.9864099341625, 569.0242561323041, 493.1531379311476, 495.64502973916376, 28.18853986901297, 547.7997789121512, 569.8808472012752, 553.0714426025061, 552.1626857162455, 549.2108432214894, 552.1640721246923, 505.5480122102203, 557.0264906908507, 553.6422512446272, 569.5583217670026, 558.2866660275588, 540.886725493426, 555.2639290800128, 556.1106200505038, 600.0667812763836, 593.655870967988, 573.4476032258096, 574.0673934856287, 600.5893668643447, 584.074634990885, 608.4414005409736, 224.8927847011549, 545.7394594387646, 548.6104521254756, 541.5023177803358, 496.9496274534222, 559.8278035888056, 536.6115547678546, 584.9782438831909, 599.465382085361, 615.9791806671371, 571.7612399795706, 579.9979721316314, 577.9413466311943, 565.4671163637117, 577.816001924913, 530.3533442814943, 579.524682613816, 573.838256085785, 543.9007374565011, 451.9922433412555, 507.0746017241477, 537.4563651380319, 559.9969220330615, 221.88738468469725, 578.2168586702584, 594.4756175376294, 568.2120937535414, 606.2176070323495, 601.8886205085101, 607.5237269158279, 598.2888966577219, 615.3785332373504, 549.8281667863558, 494.91450690160246, 562.6735732830421, 573.9077656290915, 562.3042652763185, 569.3443131047641, 583.1243051047963, 559.220396227707, 563.6453546795163, 573.4643057176738, 576.8023722576313, 513.626942234466, 551.1700732822799, 568.3488430802646, 451.0923844087781, 566.8304750325495, 576.3583802911083, 571.8639280704234, 556.5320610352834, 550.9720795812848, 556.2076041536104, 564.4584925141379, 537.9066842535926, 547.6200994341309, 561.0955331773752, 555.6236671722176, 538.5261559415504, 541.166449685438, 533.8312970219748, 562.6141310412247, 570.0918110224455, 580.7729272797216, 573.4439515897303, 562.0153166930419, 556.0190874030748, 512.6789911212327, 468.85802797513617, 524.1438189265235, 547.3274251342746, 458.2874788396332, 542.4573690847445, 539.5551991156001, 525.7070638466255, 522.2802575638666, 578.3170048293623, 562.2025363583026, 553.8254531616441, 581.4400671311106, 578.9536306095362, 562.8698066246195, 558.6045995813944, 501.18805610674104, 573.5676409050205, 571.61765503712, 531.9773890636067, 498.44939976387263, 534.2960670140502, 550.5581413235368, 218.2993111654399, 501.8683485623137, 558.6407338171347, 580.8073749119937, 575.493921009141, 573.8982894068439, 575.0414945484187, 559.7868714561052, 570.5053503714176, 513.1973787534985, 529.6933908898184, 560.3209319091317, 554.3130654972014, 555.5240315050872, 565.7114427325224, 573.497987113151, 524.0237231582519, 552.2215772659381, 553.5796532772356, 557.7842705392545, 480.74272257270326, 505.9552909493106, 558.8686210575041, 518.6209311718186, 584.6935736406889, 403.00609988572717, 594.1760129411676, 541.3786892262294, 565.7721755488548, 568.5197126050526, 557.9727732877757, 543.9739785039894, 565.8490417692749, 568.5982781765794, 521.5886493197551, 543.1109800634907, 542.1126378173876, 523.6798624612194, 524.8559798714534, 526.7055855658741, 521.451425731726, 524.8533200905672, 512.1367203717325, 517.955996714531, 519.4639378257541, 525.4279740342955, 524.3342225237121, 532.3720921288209, 530.6981305401661, 525.1039264297286, 529.6373040485602, 524.7965522393168, 530.4853181313711, 507.97995507009745, 460.8456374664971, 465.90605969444954, 461.3967164601896, 538.3273470243861, 522.8051618720001, 557.1732963310226, 567.3733765706654, 573.5011612100564, 570.5478310313534, 532.0559764312893, 223.13999964677757, 541.376409444747, 538.9661415001683, 509.3564670746101, 546.9664702777293, 580.3487929670748, 537.8167959178389, 566.0013420564032, 521.1115469927464, 500.03590127105554, 504.2727808272244, 542.9485660575455, 533.4167865669142, 557.2509496096627, 566.0414342257307, 577.8777130330263, 257.9115106963233, 529.6986410576111, 561.6149548637654, 517.9075347220204, 577.2912314388744, 560.0264782311549, 561.2096855723962, 537.931776588079, 503.4015714797582, 569.6353321793692, 568.1486948203228, 568.1384047578503, 552.8668738655838, 536.3189795709524, 531.79472648619, 571.6578604953243, 543.7638694954812, 592.2195602322552, 536.2295390653704, 540.9858685667452, 539.1570451526962, 576.360672201462, 231.72537874053873, 545.0710766881541, 538.4559463629063, 549.135279725413, 104.03096111126874, 26.976230055909188, 546.6440003003701, 564.7725337288815, 587.6882292256836, 582.9578735846135, 580.2418323002893, 573.6089810305816, 566.4045591361772, 488.647364419025, 242.80919711408788, 588.1558746813187, 579.1111364710639, 582.1224420920402, 582.7695810664911, 585.8277396567213, 576.1892790841874, 570.5304658757018, 555.4046196387977, 578.0781417072908, 587.8092762666037, 546.1132022726921, 555.4456067945497, 557.7715777348941, 560.119935921504, 570.6525037295044, 517.5581542181685, 594.5205187047189, 593.2743588881111, 591.6705559107068, 585.2884398065949, 575.7430106344077, 567.7513573111988, 585.6316521137115, 237.59278688659546, 549.1874592932295, 551.5469979277053, 567.8268671299593, 562.6618288458419, 556.7302748112511, 561.8803977065419, 571.8854056453296, 548.5755408490853, 565.7214945168467, 562.634705838378, 511.35087185945, 546.9252054783926, 572.3810995822324, 549.4133631483974, 566.4166127846162, 483.40253677102993, 552.8984464114479, 544.0481000140993, 545.1541072782188, 548.1020644107427, 540.331428797424, 537.4083546281202, 547.8489182601628, 224.7042133141844, 584.9156133924909, 572.2301734772124, 580.8385419035758, 559.8460066705327, 569.0668288694112, 493.81343991878964, 550.6087794328574, 544.4321467616985, 472.50424112915056, 567.987843458924, 568.0624885318078, 484.30099744558305, 526.1561342584571, 536.7293482651876, 571.2930868262326, 568.6968764361509, 571.7324063215343, 544.1419927203173, 563.6747854640056, 543.7200339825606]
Elapsed: 0.10417462720807573~0.13562048931212584
Time per graph: 0.0021260128001648108~0.002767765088002568
Speed: 538.4153987665235~76.95210359947932
Total Time: 0.0912
best val loss: 0.16598066687583923 test_score: 0.9184

Testing...
Test loss: 0.6491 score: 0.8980 time: 0.09s
test Score 0.8980
Epoch Time List: [0.31099370401352644, 0.31831427616998553, 0.32284665503539145, 0.3025944430846721, 0.30491525470279157, 0.3056305758655071, 0.2985287131741643, 0.3058923401404172, 0.3031960120424628, 0.3046769257634878, 0.3124565358739346, 0.31756879622116685, 0.3129382347688079, 0.32057230011560023, 0.31602387269958854, 0.3197938709054142, 0.32514972309581935, 0.31694327807053924, 0.3176526839379221, 0.3222224081400782, 0.3351619769819081, 0.31867693923413754, 0.31132117682136595, 0.3431999038439244, 0.3483598700258881, 5.876168746035546, 0.3112674979493022, 0.3210631359834224, 0.31363874091766775, 0.3196337940171361, 0.32438020198605955, 0.31843655509874225, 0.32003137306310236, 0.41443767095915973, 0.32060050778090954, 0.31182512012310326, 0.3165409779176116, 0.3263592498842627, 0.3247044701129198, 0.3177550849504769, 0.3683477931190282, 0.2985377749428153, 0.301327157067135, 0.304742360021919, 0.3064157357439399, 0.32410791679285467, 0.29763513430953026, 0.43848468898795545, 0.3254619671497494, 0.32580348500050604, 0.34521875297650695, 0.3362231468781829, 0.31495678797364235, 0.32560492190532386, 0.30831063189543784, 0.36611390789039433, 0.29712144122458994, 0.3092544819228351, 0.30963098094798625, 0.31437275116331875, 0.3127486682496965, 0.31106371292844415, 0.3404822109732777, 0.3898457877803594, 0.309959001140669, 0.31834367010742426, 0.3376716561615467, 0.3347922791726887, 0.35782345198094845, 0.3228419590741396, 0.4608151069842279, 0.2999815260991454, 0.3077787689398974, 0.30966246384195983, 0.3038583716843277, 0.2966262467671186, 0.29173134872689843, 0.2993877022527158, 0.3276151609607041, 0.3139248692896217, 0.3278150318656117, 0.3235171341802925, 0.3104537967592478, 0.31026187911629677, 0.31178554194048047, 0.31133450707420707, 0.44833043892867863, 0.3136424000840634, 0.3074743398465216, 0.30735858296975493, 0.3212160796392709, 0.33497832086868584, 0.31563008087687194, 0.3351059730630368, 0.3795883981510997, 0.31321933399885893, 0.3077023511286825, 0.3139203598257154, 0.3294808058999479, 0.32183202798478305, 0.3096358261536807, 0.47451919014565647, 0.335008829832077, 0.32396014919504523, 0.323829913046211, 0.33295877603814006, 0.3311936550308019, 0.3334300257265568, 0.4693582581821829, 0.3178354010451585, 0.3156597388442606, 0.31423775223083794, 0.3135733709204942, 0.3230154509656131, 0.3325871452689171, 0.3313327331561595, 0.37444654991850257, 0.32887426810339093, 0.3570574370678514, 0.3397505700122565, 0.3237180607393384, 0.33320533693768084, 0.33087202766910195, 0.39472166216000915, 0.3151025311090052, 0.33206799114122987, 0.3132225959561765, 0.31313096499070525, 0.3152324981056154, 0.31988766603171825, 0.3304290061350912, 0.42051080986857414, 0.3124681932386011, 0.3353935570921749, 0.3497506428975612, 0.33152873418293893, 0.33052214211784303, 0.45828465023078024, 0.3272271358873695, 0.33621210884302855, 0.3116574778687209, 0.3167343698441982, 0.3089325102046132, 0.2914740249980241, 0.28849512804299593, 0.45039400993846357, 0.32406573998741806, 0.3018438077997416, 0.2997501250356436, 0.2976975392084569, 0.2912299772724509, 0.30419317493215203, 0.301942857215181, 0.3019552391488105, 0.3046345319598913, 0.31004221085458994, 0.3051629248075187, 0.3255393172148615, 0.31598114292137325, 0.3063589399680495, 0.32572071393951774, 0.29299249593168497, 0.3668699648696929, 0.2859703837893903, 0.30093860998749733, 0.2921442009974271, 0.29466563183814287, 0.2988655222579837, 0.30477060307748616, 0.31054464913904667, 0.30049380427226424, 0.31259335088543594, 0.3197790728881955, 0.3177669928409159, 0.3221442948561162, 0.3206210918724537, 0.31962851993739605, 0.32324764621444046, 0.3193573197349906, 0.32034189673140645, 0.33549115993082523, 0.3250704661477357, 0.3199213547632098, 0.3167396471835673, 0.31765452795661986, 0.31624169973656535, 0.31947224494069815, 0.31908880895935, 0.32224763091653585, 0.3177333581261337, 0.3196346261538565, 0.3561338731087744, 0.3571885130368173, 0.3887932829093188, 0.3459007570054382, 0.3101152388844639, 0.30400685709901154, 0.3011587290093303, 0.2890193702187389, 0.3028468661941588, 0.2988487984985113, 0.43750572274439037, 0.3077991418540478, 0.31975482893176377, 0.31513111013919115, 0.29900511517189443, 0.29771555098704994, 0.30406362493522465, 0.2969177851919085, 0.40981703205034137, 0.43301268573850393, 0.30755215510725975, 0.31099836295470595, 0.32239570282399654, 0.30745279882103205, 0.30298840207979083, 0.2914887226652354, 0.5107318570371717, 0.31936140172183514, 0.30738158198073506, 0.3033778890967369, 0.2961338860914111, 0.3030166528187692, 0.3137114872224629, 0.40809832606464624, 0.3202470720279962, 0.30958354799076915, 0.29679664806462824, 0.3091357182711363, 0.30167607171460986, 0.30675531504675746, 0.320741840172559, 0.4099143941421062, 0.3062814446166158, 0.2910180080216378, 0.30418614274822176, 0.3108539888635278, 0.3106184438802302, 0.3024594741873443, 0.429633857216686, 0.30769198085181415, 0.31082337093539536, 0.3079152989666909, 0.683507754933089, 6.570069010835141, 0.41173000494018197, 0.2936436247546226, 0.2921580739784986, 0.28756809211336076, 0.29086446901783347, 0.2864970581140369, 0.29404400824569166, 0.30867861886508763, 0.4199128597974777, 0.2838671070057899, 0.28441694215871394, 0.29601920512504876, 0.289231134345755, 0.28985345899127424, 0.2843660097569227, 0.29395299102179706, 0.2951267200987786, 0.40354839293286204, 0.28933091019280255, 0.2903956619556993, 0.3076659229118377, 0.30215911590494215, 0.29785724193789065, 0.29596763383597136, 0.3023356138728559, 0.387938927160576, 0.28560684411786497, 0.28861570078879595, 0.28475914197042584, 0.28439716971479356, 0.28702032077126205, 0.2889123761560768, 0.41134637617506087, 0.30565471504814923, 0.29793788911774755, 0.29070035414770246, 0.2941639421042055, 0.2995782170910388, 0.294794546905905, 0.2999537349678576, 0.28840844705700874, 0.3964940598234534, 0.2928131460212171, 0.31630554306320846, 0.3032731548883021, 0.2925563370808959, 0.30016519385389984, 0.30675979796797037, 0.3338101329281926, 0.4232708867639303, 0.30850440193898976, 0.3068141331896186, 0.30944803869351745, 0.30838763900101185, 0.31143598025664687, 0.30696212290786207, 0.4390196150634438, 0.293471448821947, 0.29391726618632674, 0.29538822593167424, 0.29746828111819923, 0.29734497191384435, 0.31210936699062586, 0.31050030793994665, 0.30755983176641166, 0.43839083914645016, 0.3011879369150847, 0.2928312746807933, 0.31148346117697656, 0.3352951018605381, 0.31200438598170877, 0.29342539492063224, 0.2984660828951746, 0.29138379730284214, 0.30413670116104186, 0.29774928698316216, 0.3090697000734508]
Total Epoch List: [143, 179]
Total Time List: [0.08611331391148269, 0.09115174692124128]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d40b84c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.15s
Val loss: 0.6842 score: 0.7143 time: 0.09s
Test loss: 0.6865 score: 0.6667 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7733;  Loss pred: 0.7733; Loss self: 0.0000; time: 0.15s
Val loss: 0.6831 score: 0.7551 time: 0.08s
Test loss: 0.6858 score: 0.6458 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7826;  Loss pred: 0.7826; Loss self: 0.0000; time: 0.15s
Val loss: 0.6812 score: 0.7551 time: 0.08s
Test loss: 0.6845 score: 0.6875 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7447;  Loss pred: 0.7447; Loss self: 0.0000; time: 0.16s
Val loss: 0.6787 score: 0.7551 time: 0.08s
Test loss: 0.6825 score: 0.7292 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7513;  Loss pred: 0.7513; Loss self: 0.0000; time: 0.15s
Val loss: 0.6757 score: 0.7755 time: 0.08s
Test loss: 0.6802 score: 0.7708 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.15s
Val loss: 0.6727 score: 0.6735 time: 0.08s
Test loss: 0.6777 score: 0.7292 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.15s
Val loss: 0.6698 score: 0.5714 time: 0.09s
Test loss: 0.6752 score: 0.6042 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.15s
Val loss: 0.6673 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6728 score: 0.5000 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6661 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6715 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6670 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6719 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6692 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6735 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6787 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6781 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6826 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4898;  Loss pred: 0.4898; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4862;  Loss pred: 0.4862; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4744;  Loss pred: 0.4744; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6780 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4704;  Loss pred: 0.4704; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6747 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4797;  Loss pred: 0.4797; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6707 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6763 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.4599;  Loss pred: 0.4599; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6656 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6720 score: 0.5000 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4462;  Loss pred: 0.4462; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6594 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6666 score: 0.5000 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4225;  Loss pred: 0.4225; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6519 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6603 score: 0.5000 time: 0.10s
Epoch 26/1000, LR 0.000270
Train loss: 0.4253;  Loss pred: 0.4253; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6434 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6531 score: 0.5000 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.4036;  Loss pred: 0.4036; Loss self: 0.0000; time: 0.15s
Val loss: 0.6343 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6452 score: 0.5000 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3989;  Loss pred: 0.3989; Loss self: 0.0000; time: 0.15s
Val loss: 0.6243 score: 0.5102 time: 0.08s
Test loss: 0.6365 score: 0.5417 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3660;  Loss pred: 0.3660; Loss self: 0.0000; time: 0.15s
Val loss: 0.6137 score: 0.5306 time: 0.08s
Test loss: 0.6273 score: 0.5417 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3755;  Loss pred: 0.3755; Loss self: 0.0000; time: 0.15s
Val loss: 0.6027 score: 0.5714 time: 0.09s
Test loss: 0.6176 score: 0.5417 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3811;  Loss pred: 0.3811; Loss self: 0.0000; time: 0.15s
Val loss: 0.5913 score: 0.5714 time: 0.09s
Test loss: 0.6079 score: 0.5833 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3774;  Loss pred: 0.3774; Loss self: 0.0000; time: 0.15s
Val loss: 0.5794 score: 0.6327 time: 0.09s
Test loss: 0.5976 score: 0.6250 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.3639;  Loss pred: 0.3639; Loss self: 0.0000; time: 0.15s
Val loss: 0.5674 score: 0.7143 time: 0.09s
Test loss: 0.5872 score: 0.6667 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3526;  Loss pred: 0.3526; Loss self: 0.0000; time: 0.16s
Val loss: 0.5552 score: 0.7347 time: 0.08s
Test loss: 0.5765 score: 0.7083 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3421;  Loss pred: 0.3421; Loss self: 0.0000; time: 0.14s
Val loss: 0.5429 score: 0.7755 time: 0.08s
Test loss: 0.5656 score: 0.7500 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3575;  Loss pred: 0.3575; Loss self: 0.0000; time: 0.15s
Val loss: 0.5312 score: 0.8163 time: 0.09s
Test loss: 0.5553 score: 0.8125 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.3527;  Loss pred: 0.3527; Loss self: 0.0000; time: 0.15s
Val loss: 0.5201 score: 0.8571 time: 0.10s
Test loss: 0.5456 score: 0.8333 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.3012;  Loss pred: 0.3012; Loss self: 0.0000; time: 0.16s
Val loss: 0.5095 score: 0.8980 time: 0.09s
Test loss: 0.5365 score: 0.8958 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 0.16s
Val loss: 0.4989 score: 0.9388 time: 0.18s
Test loss: 0.5277 score: 0.9167 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.3011;  Loss pred: 0.3011; Loss self: 0.0000; time: 0.15s
Val loss: 0.4890 score: 0.9388 time: 0.08s
Test loss: 0.5195 score: 0.9167 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2799;  Loss pred: 0.2799; Loss self: 0.0000; time: 0.15s
Val loss: 0.4801 score: 0.9388 time: 0.08s
Test loss: 0.5124 score: 0.9167 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2884;  Loss pred: 0.2884; Loss self: 0.0000; time: 0.15s
Val loss: 0.4724 score: 0.9388 time: 0.08s
Test loss: 0.5066 score: 0.8958 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2803;  Loss pred: 0.2803; Loss self: 0.0000; time: 0.15s
Val loss: 0.4659 score: 0.9388 time: 0.08s
Test loss: 0.5014 score: 0.8958 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2632;  Loss pred: 0.2632; Loss self: 0.0000; time: 0.15s
Val loss: 0.4601 score: 0.9388 time: 0.11s
Test loss: 0.4967 score: 0.8958 time: 0.10s
Epoch 45/1000, LR 0.000269
Train loss: 0.2709;  Loss pred: 0.2709; Loss self: 0.0000; time: 0.15s
Val loss: 0.4553 score: 0.9388 time: 0.10s
Test loss: 0.4923 score: 0.8958 time: 0.19s
Epoch 46/1000, LR 0.000269
Train loss: 0.2623;  Loss pred: 0.2623; Loss self: 0.0000; time: 0.15s
Val loss: 0.4513 score: 0.9388 time: 0.08s
Test loss: 0.4885 score: 0.8958 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2504;  Loss pred: 0.2504; Loss self: 0.0000; time: 0.15s
Val loss: 0.4475 score: 0.9388 time: 0.08s
Test loss: 0.4849 score: 0.8958 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2572;  Loss pred: 0.2572; Loss self: 0.0000; time: 0.15s
Val loss: 0.4441 score: 0.9388 time: 0.08s
Test loss: 0.4815 score: 0.9167 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.15s
Val loss: 0.4409 score: 0.9388 time: 0.09s
Test loss: 0.4782 score: 0.9167 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2553;  Loss pred: 0.2553; Loss self: 0.0000; time: 0.15s
Val loss: 0.4378 score: 0.9388 time: 0.09s
Test loss: 0.4750 score: 0.9167 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.2313;  Loss pred: 0.2313; Loss self: 0.0000; time: 0.16s
Val loss: 0.4347 score: 0.9388 time: 0.09s
Test loss: 0.4718 score: 0.9167 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.15s
Val loss: 0.4317 score: 0.9388 time: 0.08s
Test loss: 0.4689 score: 0.9167 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.24s
Val loss: 0.4289 score: 0.9388 time: 0.08s
Test loss: 0.4660 score: 0.9167 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2103;  Loss pred: 0.2103; Loss self: 0.0000; time: 0.15s
Val loss: 0.4253 score: 0.9388 time: 0.08s
Test loss: 0.4628 score: 0.9167 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2141;  Loss pred: 0.2141; Loss self: 0.0000; time: 0.15s
Val loss: 0.4216 score: 0.9388 time: 0.09s
Test loss: 0.4594 score: 0.9167 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2033;  Loss pred: 0.2033; Loss self: 0.0000; time: 0.15s
Val loss: 0.4175 score: 0.9388 time: 0.08s
Test loss: 0.4556 score: 0.9167 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2071;  Loss pred: 0.2071; Loss self: 0.0000; time: 0.15s
Val loss: 0.4132 score: 0.9388 time: 0.09s
Test loss: 0.4516 score: 0.9167 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.18s
Val loss: 0.4085 score: 0.9388 time: 0.10s
Test loss: 0.4473 score: 0.9167 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.16s
Val loss: 0.4030 score: 0.9388 time: 0.09s
Test loss: 0.4427 score: 0.9167 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.1957;  Loss pred: 0.1957; Loss self: 0.0000; time: 0.28s
Val loss: 0.3969 score: 0.9388 time: 0.08s
Test loss: 0.4379 score: 0.9167 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1984;  Loss pred: 0.1984; Loss self: 0.0000; time: 0.16s
Val loss: 0.3898 score: 0.9388 time: 0.09s
Test loss: 0.4326 score: 0.9167 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.16s
Val loss: 0.3835 score: 0.9388 time: 0.08s
Test loss: 0.4277 score: 0.9167 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1862;  Loss pred: 0.1862; Loss self: 0.0000; time: 0.16s
Val loss: 0.3767 score: 0.9592 time: 0.08s
Test loss: 0.4226 score: 0.9167 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.15s
Val loss: 0.3704 score: 0.9592 time: 0.09s
Test loss: 0.4175 score: 0.8958 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1657;  Loss pred: 0.1657; Loss self: 0.0000; time: 0.15s
Val loss: 0.3649 score: 0.9592 time: 0.08s
Test loss: 0.4127 score: 0.8958 time: 0.10s
Epoch 66/1000, LR 0.000268
Train loss: 0.1671;  Loss pred: 0.1671; Loss self: 0.0000; time: 0.15s
Val loss: 0.3599 score: 0.9592 time: 0.08s
Test loss: 0.4081 score: 0.8958 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1668;  Loss pred: 0.1668; Loss self: 0.0000; time: 0.16s
Val loss: 0.3551 score: 0.9592 time: 0.20s
Test loss: 0.4035 score: 0.8958 time: 0.10s
Epoch 68/1000, LR 0.000268
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 0.16s
Val loss: 0.3490 score: 0.9592 time: 0.08s
Test loss: 0.3985 score: 0.8958 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1593;  Loss pred: 0.1593; Loss self: 0.0000; time: 0.15s
Val loss: 0.3431 score: 0.9592 time: 0.08s
Test loss: 0.3934 score: 0.8958 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1679;  Loss pred: 0.1679; Loss self: 0.0000; time: 0.15s
Val loss: 0.3362 score: 0.9592 time: 0.09s
Test loss: 0.3876 score: 0.8958 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1487;  Loss pred: 0.1487; Loss self: 0.0000; time: 0.15s
Val loss: 0.3297 score: 0.9592 time: 0.08s
Test loss: 0.3823 score: 0.9167 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.16s
Val loss: 0.3234 score: 0.9592 time: 0.09s
Test loss: 0.3775 score: 0.9167 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 0.15s
Val loss: 0.3185 score: 0.9796 time: 0.09s
Test loss: 0.3730 score: 0.9167 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1333;  Loss pred: 0.1333; Loss self: 0.0000; time: 0.15s
Val loss: 0.3136 score: 0.9796 time: 0.09s
Test loss: 0.3684 score: 0.9167 time: 0.10s
Epoch 75/1000, LR 0.000267
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 0.25s
Val loss: 0.3085 score: 0.9796 time: 0.09s
Test loss: 0.3640 score: 0.9167 time: 0.16s
Epoch 76/1000, LR 0.000267
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.15s
Val loss: 0.3032 score: 0.9796 time: 0.08s
Test loss: 0.3594 score: 0.9167 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1294;  Loss pred: 0.1294; Loss self: 0.0000; time: 0.15s
Val loss: 0.2980 score: 0.9592 time: 0.09s
Test loss: 0.3544 score: 0.9167 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.15s
Val loss: 0.2930 score: 0.9796 time: 0.09s
Test loss: 0.3491 score: 0.9167 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.15s
Val loss: 0.2886 score: 0.9796 time: 0.09s
Test loss: 0.3436 score: 0.9167 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 0.15s
Val loss: 0.2844 score: 0.9796 time: 0.08s
Test loss: 0.3383 score: 0.9167 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 0.15s
Val loss: 0.2815 score: 0.9592 time: 0.08s
Test loss: 0.3333 score: 0.8958 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 0.28s
Val loss: 0.2789 score: 0.9592 time: 0.08s
Test loss: 0.3287 score: 0.8958 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1175;  Loss pred: 0.1175; Loss self: 0.0000; time: 0.16s
Val loss: 0.2764 score: 0.9592 time: 0.09s
Test loss: 0.3247 score: 0.8958 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1078;  Loss pred: 0.1078; Loss self: 0.0000; time: 0.16s
Val loss: 0.2727 score: 0.9592 time: 0.09s
Test loss: 0.3201 score: 0.8958 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1020;  Loss pred: 0.1020; Loss self: 0.0000; time: 0.16s
Val loss: 0.2676 score: 0.9592 time: 0.08s
Test loss: 0.3150 score: 0.8958 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.1116;  Loss pred: 0.1116; Loss self: 0.0000; time: 0.16s
Val loss: 0.2625 score: 0.9592 time: 0.09s
Test loss: 0.3098 score: 0.8958 time: 0.10s
Epoch 87/1000, LR 0.000266
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 0.15s
Val loss: 0.2569 score: 0.9592 time: 0.08s
Test loss: 0.3043 score: 0.8958 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1133;  Loss pred: 0.1133; Loss self: 0.0000; time: 0.16s
Val loss: 0.2514 score: 0.9592 time: 0.09s
Test loss: 0.2989 score: 0.8958 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 0.18s
Val loss: 0.2469 score: 0.9592 time: 0.17s
Test loss: 0.2942 score: 0.8958 time: 0.11s
Epoch 90/1000, LR 0.000266
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 0.15s
Val loss: 0.2426 score: 0.9592 time: 0.08s
Test loss: 0.2899 score: 0.8958 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0984;  Loss pred: 0.0984; Loss self: 0.0000; time: 0.17s
Val loss: 0.2388 score: 0.9592 time: 0.08s
Test loss: 0.2860 score: 0.8958 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.16s
Val loss: 0.2345 score: 0.9592 time: 0.09s
Test loss: 0.2818 score: 0.8958 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.15s
Val loss: 0.2307 score: 0.9592 time: 0.09s
Test loss: 0.2777 score: 0.8958 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.16s
Val loss: 0.2276 score: 0.9592 time: 0.09s
Test loss: 0.2738 score: 0.8958 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 0.16s
Val loss: 0.2233 score: 0.9592 time: 0.09s
Test loss: 0.2694 score: 0.8958 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.16s
Val loss: 0.2204 score: 0.9592 time: 0.09s
Test loss: 0.2653 score: 0.8958 time: 0.21s
Epoch 97/1000, LR 0.000265
Train loss: 0.0876;  Loss pred: 0.0876; Loss self: 0.0000; time: 0.15s
Val loss: 0.2171 score: 0.9592 time: 0.08s
Test loss: 0.2609 score: 0.8958 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.16s
Val loss: 0.2139 score: 0.9592 time: 0.09s
Test loss: 0.2571 score: 0.8958 time: 0.10s
Epoch 99/1000, LR 0.000265
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.15s
Val loss: 0.2120 score: 0.9592 time: 0.08s
Test loss: 0.2532 score: 0.9167 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.15s
Val loss: 0.2100 score: 0.9592 time: 0.09s
Test loss: 0.2500 score: 0.9167 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.15s
Val loss: 0.2098 score: 0.9592 time: 0.09s
Test loss: 0.2475 score: 0.8958 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0872;  Loss pred: 0.0872; Loss self: 0.0000; time: 0.15s
Val loss: 0.2075 score: 0.9592 time: 0.08s
Test loss: 0.2448 score: 0.8958 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.16s
Val loss: 0.2032 score: 0.9592 time: 0.09s
Test loss: 0.2412 score: 0.8958 time: 0.22s
Epoch 104/1000, LR 0.000264
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.15s
Val loss: 0.2031 score: 0.9592 time: 0.09s
Test loss: 0.2401 score: 0.8958 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0700;  Loss pred: 0.0700; Loss self: 0.0000; time: 0.15s
Val loss: 0.1976 score: 0.9592 time: 0.08s
Test loss: 0.2360 score: 0.9167 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 0.16s
Val loss: 0.1943 score: 0.9592 time: 0.08s
Test loss: 0.2328 score: 0.9167 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.15s
Val loss: 0.1938 score: 0.9592 time: 0.09s
Test loss: 0.2305 score: 0.9167 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.16s
Val loss: 0.1957 score: 0.9592 time: 0.08s
Test loss: 0.2293 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0614;  Loss pred: 0.0614; Loss self: 0.0000; time: 0.16s
Val loss: 0.1876 score: 0.9592 time: 0.09s
Test loss: 0.2287 score: 0.9167 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.16s
Val loss: 0.1861 score: 0.9592 time: 0.08s
Test loss: 0.2253 score: 0.9167 time: 0.09s
Epoch 111/1000, LR 0.000263
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.20s
Val loss: 0.1976 score: 0.9592 time: 0.08s
Test loss: 0.2287 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.15s
Val loss: 0.1839 score: 0.9592 time: 0.08s
Test loss: 0.2250 score: 0.9167 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.16s
Val loss: 0.1822 score: 0.9592 time: 0.08s
Test loss: 0.2171 score: 0.8750 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.16s
Val loss: 0.1857 score: 0.9592 time: 0.10s
Test loss: 0.2178 score: 0.8958 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.15s
Val loss: 0.1970 score: 0.9592 time: 0.09s
Test loss: 0.2253 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.16s
Val loss: 0.1932 score: 0.9592 time: 0.09s
Test loss: 0.2226 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.15s
Val loss: 0.1842 score: 0.9592 time: 0.08s
Test loss: 0.2172 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.15s
Val loss: 0.1808 score: 0.9592 time: 0.09s
Test loss: 0.2159 score: 0.8958 time: 0.09s
Epoch 119/1000, LR 0.000262
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.26s
Val loss: 0.1826 score: 0.9592 time: 0.08s
Test loss: 0.2163 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.15s
Val loss: 0.1845 score: 0.9592 time: 0.09s
Test loss: 0.2165 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.16s
Val loss: 0.1830 score: 0.9592 time: 0.09s
Test loss: 0.2151 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.17s
Val loss: 0.1819 score: 0.9592 time: 0.10s
Test loss: 0.2141 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.15s
Val loss: 0.1834 score: 0.9592 time: 0.08s
Test loss: 0.2154 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.16s
Val loss: 0.1912 score: 0.9592 time: 0.09s
Test loss: 0.2194 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.16s
Val loss: 0.1890 score: 0.9592 time: 0.17s
Test loss: 0.2175 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.15s
Val loss: 0.1833 score: 0.9592 time: 0.08s
Test loss: 0.2142 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.15s
Val loss: 0.1800 score: 0.9592 time: 0.09s
Test loss: 0.2106 score: 0.8958 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.15s
Val loss: 0.1781 score: 0.9592 time: 0.09s
Test loss: 0.2088 score: 0.8958 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.15s
Val loss: 0.1728 score: 0.9592 time: 0.10s
Test loss: 0.2029 score: 0.8958 time: 0.09s
Epoch 130/1000, LR 0.000260
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.15s
Val loss: 0.1723 score: 0.9592 time: 0.08s
Test loss: 0.2037 score: 0.8958 time: 0.09s
Epoch 131/1000, LR 0.000260
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.15s
Val loss: 0.1739 score: 0.9592 time: 0.08s
Test loss: 0.2053 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.15s
Val loss: 0.1777 score: 0.9592 time: 0.09s
Test loss: 0.2091 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.16s
Val loss: 0.1694 score: 0.9592 time: 0.16s
Test loss: 0.2014 score: 0.8958 time: 0.09s
Epoch 134/1000, LR 0.000260
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.16s
Val loss: 0.1732 score: 0.9592 time: 0.08s
Test loss: 0.2050 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.14s
Val loss: 0.1757 score: 0.9592 time: 0.08s
Test loss: 0.2083 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.14s
Val loss: 0.1718 score: 0.9592 time: 0.08s
Test loss: 0.2059 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.14s
Val loss: 0.1746 score: 0.9592 time: 0.08s
Test loss: 0.2087 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.16s
Val loss: 0.1766 score: 0.9592 time: 0.10s
Test loss: 0.2110 score: 0.8958 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.17s
Val loss: 0.1751 score: 0.9592 time: 0.09s
Test loss: 0.2091 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.18s
Val loss: 0.1712 score: 0.9592 time: 0.09s
Test loss: 0.2036 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.15s
Val loss: 0.1705 score: 0.9592 time: 0.09s
Test loss: 0.2029 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.16s
Val loss: 0.1725 score: 0.9592 time: 0.08s
Test loss: 0.2036 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.16s
Val loss: 0.1735 score: 0.9592 time: 0.08s
Test loss: 0.2053 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.15s
Val loss: 0.1745 score: 0.9592 time: 0.57s
Test loss: 0.2083 score: 0.8958 time: 3.22s
     INFO: Early stopping counter 11 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 1.26s
Val loss: 0.1689 score: 0.9592 time: 1.78s
Test loss: 0.2045 score: 0.8958 time: 0.09s
Epoch 146/1000, LR 0.000258
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.15s
Val loss: 0.1632 score: 0.9592 time: 0.09s
Test loss: 0.2000 score: 0.8750 time: 0.09s
Epoch 147/1000, LR 0.000258
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.16s
Val loss: 0.1665 score: 0.9592 time: 0.09s
Test loss: 0.2018 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.16s
Val loss: 0.1663 score: 0.9592 time: 0.08s
Test loss: 0.1997 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.15s
Val loss: 0.1656 score: 0.9592 time: 0.09s
Test loss: 0.1993 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.16s
Val loss: 0.1656 score: 0.9592 time: 0.09s
Test loss: 0.1997 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.16s
Val loss: 0.1639 score: 0.9592 time: 0.09s
Test loss: 0.1988 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.16s
Val loss: 0.1650 score: 0.9592 time: 0.08s
Test loss: 0.1993 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.16s
Val loss: 0.1656 score: 0.9592 time: 0.09s
Test loss: 0.1988 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.16s
Val loss: 0.1642 score: 0.9592 time: 0.08s
Test loss: 0.1974 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.16s
Val loss: 0.1613 score: 0.9592 time: 0.09s
Test loss: 0.1958 score: 0.9167 time: 0.09s
Epoch 156/1000, LR 0.000256
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.16s
Val loss: 0.1606 score: 0.9592 time: 0.09s
Test loss: 0.1959 score: 0.9167 time: 0.09s
Epoch 157/1000, LR 0.000256
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.16s
Val loss: 0.1628 score: 0.9592 time: 0.09s
Test loss: 0.1975 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.16s
Val loss: 0.1663 score: 0.9592 time: 0.09s
Test loss: 0.1994 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.15s
Val loss: 0.1630 score: 0.9592 time: 0.09s
Test loss: 0.1952 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.15s
Val loss: 0.1579 score: 0.9796 time: 0.09s
Test loss: 0.1918 score: 0.8958 time: 0.09s
Epoch 161/1000, LR 0.000255
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.15s
Val loss: 0.1612 score: 0.9592 time: 0.08s
Test loss: 0.1939 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.16s
Val loss: 0.1598 score: 0.9592 time: 0.09s
Test loss: 0.1923 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.15s
Val loss: 0.1607 score: 0.9592 time: 0.09s
Test loss: 0.1932 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.15s
Val loss: 0.1597 score: 0.9592 time: 0.09s
Test loss: 0.1929 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.16s
Val loss: 0.1617 score: 0.9592 time: 0.09s
Test loss: 0.1943 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.15s
Val loss: 0.1575 score: 0.9592 time: 0.09s
Test loss: 0.1917 score: 0.9167 time: 0.09s
Epoch 167/1000, LR 0.000254
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.16s
Val loss: 0.1561 score: 0.9796 time: 0.09s
Test loss: 0.1908 score: 0.9167 time: 0.09s
Epoch 168/1000, LR 0.000254
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.16s
Val loss: 0.1566 score: 0.9796 time: 0.09s
Test loss: 0.1911 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.15s
Val loss: 0.1548 score: 0.9796 time: 0.09s
Test loss: 0.1897 score: 0.8958 time: 0.09s
Epoch 170/1000, LR 0.000253
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.15s
Val loss: 0.1543 score: 0.9796 time: 0.09s
Test loss: 0.1894 score: 0.8958 time: 0.11s
Epoch 171/1000, LR 0.000253
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.15s
Val loss: 0.1578 score: 0.9592 time: 0.09s
Test loss: 0.1910 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.15s
Val loss: 0.1622 score: 0.9592 time: 0.09s
Test loss: 0.1933 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.16s
Val loss: 0.1613 score: 0.9592 time: 0.09s
Test loss: 0.1928 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.15s
Val loss: 0.1586 score: 0.9592 time: 0.09s
Test loss: 0.1916 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.16s
Val loss: 0.1576 score: 0.9592 time: 0.09s
Test loss: 0.1914 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.16s
Val loss: 0.1571 score: 0.9592 time: 0.09s
Test loss: 0.1912 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.15s
Val loss: 0.1572 score: 0.9592 time: 0.19s
Test loss: 0.1912 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.15s
Val loss: 0.1595 score: 0.9592 time: 0.08s
Test loss: 0.1927 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.15s
Val loss: 0.1611 score: 0.9592 time: 0.08s
Test loss: 0.1937 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.16s
Val loss: 0.1612 score: 0.9592 time: 0.09s
Test loss: 0.1941 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.15s
Val loss: 0.1595 score: 0.9592 time: 0.09s
Test loss: 0.1932 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.16s
Val loss: 0.1560 score: 0.9592 time: 0.08s
Test loss: 0.1915 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 183/1000, LR 0.000250
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.15s
Val loss: 0.1525 score: 0.9796 time: 0.10s
Test loss: 0.1891 score: 0.8958 time: 0.22s
Epoch 184/1000, LR 0.000250
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.15s
Val loss: 0.1525 score: 0.9796 time: 0.09s
Test loss: 0.1884 score: 0.8958 time: 0.09s
Epoch 185/1000, LR 0.000250
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.16s
Val loss: 0.1511 score: 0.9796 time: 0.08s
Test loss: 0.1876 score: 0.8958 time: 0.09s
Epoch 186/1000, LR 0.000250
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.15s
Val loss: 0.1502 score: 0.9796 time: 0.09s
Test loss: 0.1872 score: 0.8958 time: 0.09s
Epoch 187/1000, LR 0.000249
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.16s
Val loss: 0.1491 score: 0.9796 time: 0.08s
Test loss: 0.1869 score: 0.8958 time: 0.09s
Epoch 188/1000, LR 0.000249
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.16s
Val loss: 0.1496 score: 0.9796 time: 0.08s
Test loss: 0.1876 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 189/1000, LR 0.000249
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.16s
Val loss: 0.1517 score: 0.9796 time: 0.08s
Test loss: 0.1885 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 190/1000, LR 0.000249
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.16s
Val loss: 0.1552 score: 0.9592 time: 0.22s
Test loss: 0.1895 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.15s
Val loss: 0.1594 score: 0.9592 time: 0.09s
Test loss: 0.1911 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.15s
Val loss: 0.1573 score: 0.9592 time: 0.09s
Test loss: 0.1894 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.15s
Val loss: 0.1534 score: 0.9796 time: 0.09s
Test loss: 0.1878 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.15s
Val loss: 0.1512 score: 0.9796 time: 0.09s
Test loss: 0.1869 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.16s
Val loss: 0.1508 score: 0.9796 time: 0.09s
Test loss: 0.1863 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.16s
Val loss: 0.1510 score: 0.9796 time: 0.09s
Test loss: 0.1864 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.16s
Val loss: 0.1506 score: 0.9796 time: 0.08s
Test loss: 0.1862 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.17s
Val loss: 0.1505 score: 0.9796 time: 0.08s
Test loss: 0.1861 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.15s
Val loss: 0.1514 score: 0.9796 time: 0.08s
Test loss: 0.1864 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.15s
Val loss: 0.1541 score: 0.9592 time: 0.08s
Test loss: 0.1877 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.15s
Val loss: 0.1540 score: 0.9592 time: 0.08s
Test loss: 0.1883 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 202/1000, LR 0.000246
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.15s
Val loss: 0.1517 score: 0.9796 time: 0.08s
Test loss: 0.1882 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.14s
Val loss: 0.1486 score: 0.9796 time: 0.08s
Test loss: 0.1875 score: 0.8958 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.15s
Val loss: 0.1466 score: 0.9796 time: 0.09s
Test loss: 0.1866 score: 0.8958 time: 0.09s
Epoch 205/1000, LR 0.000245
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.14s
Val loss: 0.1457 score: 0.9796 time: 0.08s
Test loss: 0.1861 score: 0.8958 time: 0.09s
Epoch 206/1000, LR 0.000245
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.17s
Val loss: 0.1454 score: 0.9796 time: 0.11s
Test loss: 0.1859 score: 0.8958 time: 0.09s
Epoch 207/1000, LR 0.000245
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.15s
Val loss: 0.1459 score: 0.9796 time: 0.09s
Test loss: 0.1863 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 208/1000, LR 0.000244
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.16s
Val loss: 0.1466 score: 0.9796 time: 0.09s
Test loss: 0.1867 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 209/1000, LR 0.000244
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.18s
Val loss: 0.1471 score: 0.9796 time: 0.09s
Test loss: 0.1877 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 210/1000, LR 0.000244
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.16s
Val loss: 0.1472 score: 0.9796 time: 0.08s
Test loss: 0.1885 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 211/1000, LR 0.000244
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.15s
Val loss: 0.1473 score: 0.9796 time: 0.08s
Test loss: 0.1886 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 212/1000, LR 0.000243
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.15s
Val loss: 0.1483 score: 0.9796 time: 0.10s
Test loss: 0.1892 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 213/1000, LR 0.000243
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.16s
Val loss: 0.1498 score: 0.9796 time: 0.09s
Test loss: 0.1900 score: 0.8958 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 214/1000, LR 0.000243
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.16s
Val loss: 0.1538 score: 0.9592 time: 0.08s
Test loss: 0.1917 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 215/1000, LR 0.000243
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.14s
Val loss: 0.1530 score: 0.9796 time: 0.08s
Test loss: 0.1923 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 216/1000, LR 0.000242
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.14s
Val loss: 0.1500 score: 0.9796 time: 0.08s
Test loss: 0.1922 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 217/1000, LR 0.000242
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.15s
Val loss: 0.1483 score: 0.9796 time: 0.08s
Test loss: 0.1920 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 218/1000, LR 0.000242
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.16s
Val loss: 0.1481 score: 0.9796 time: 0.08s
Test loss: 0.1916 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 219/1000, LR 0.000242
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.14s
Val loss: 0.1478 score: 0.9796 time: 0.09s
Test loss: 0.1903 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 220/1000, LR 0.000241
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.18s
Val loss: 0.1478 score: 0.9796 time: 0.11s
Test loss: 0.1896 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 221/1000, LR 0.000241
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.17s
Val loss: 0.1478 score: 0.9796 time: 0.10s
Test loss: 0.1891 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 222/1000, LR 0.000241
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.16s
Val loss: 0.1487 score: 0.9796 time: 0.09s
Test loss: 0.1890 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 223/1000, LR 0.000241
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.16s
Val loss: 0.1521 score: 0.9796 time: 0.08s
Test loss: 0.1892 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 224/1000, LR 0.000240
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.14s
Val loss: 0.1533 score: 0.9592 time: 0.08s
Test loss: 0.1892 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 225/1000, LR 0.000240
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.16s
Val loss: 0.1546 score: 0.9592 time: 0.08s
Test loss: 0.1888 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 226/1000, LR 0.000240
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.14s
Val loss: 0.1523 score: 0.9796 time: 0.08s
Test loss: 0.1876 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 205,   Train_Loss: 0.0150,   Val_Loss: 0.1454,   Val_Precision: 1.0000,   Val_Recall: 0.9600,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.1454,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.1859


[0.08784167980775237, 0.08701143995858729, 0.08159107412211597, 0.08523665205575526, 0.0843327809125185, 0.08390744007192552, 0.08476514206267893, 0.08275979408062994, 0.0836453118827194, 0.08332366403192282, 0.08705194410867989, 0.09070615703240037, 0.08762322412803769, 0.08916717790998518, 0.08695923094637692, 0.0887101641856134, 0.08727181889116764, 0.08795737591572106, 0.08737895009107888, 0.08942959015257657, 0.08944672020152211, 0.08596696192398667, 0.08611232205294073, 0.09936061687767506, 0.0988610740751028, 1.7382950740866363, 0.08944874000735581, 0.08598288614302874, 0.08859614911489189, 0.0887419618666172, 0.0892189231235534, 0.08874173904769123, 0.0969245231244713, 0.08796709100715816, 0.08850480592809618, 0.08603157592006028, 0.08776853000745177, 0.0905919810757041, 0.0882463229354471, 0.08811196591705084, 0.08165757800452411, 0.08253940101712942, 0.08544808579608798, 0.08535583200864494, 0.08158652600832283, 0.08389338804408908, 0.08053363882936537, 0.2178816010709852, 0.08978643408045173, 0.08931656298227608, 0.09048899402841926, 0.098601542878896, 0.08752691396512091, 0.09131372510455549, 0.08376379893161356, 0.08173949900083244, 0.07954814308322966, 0.08570010797120631, 0.0844830540008843, 0.08478369005024433, 0.08665402210317552, 0.08480208204127848, 0.09239123412407935, 0.08455205010250211, 0.08538991515524685, 0.09008996794000268, 0.10840894002467394, 0.09663272392936051, 0.09117019199766219, 0.08750048093497753, 0.2208327439147979, 0.08474329183809459, 0.08242558408528566, 0.08623540494590998, 0.08082906110212207, 0.0814104110468179, 0.08065528608858585, 0.08190023293718696, 0.07962578698061407, 0.0891187519300729, 0.09900699881836772, 0.08708423911593854, 0.08537957305088639, 0.08714143396355212, 0.086063914000988, 0.0840301108546555, 0.08762198290787637, 0.08693409711122513, 0.08544559706933796, 0.08495110692456365, 0.09539997996762395, 0.08890177891589701, 0.08621465601027012, 0.10862519894726574, 0.08644559909589589, 0.08501654816791415, 0.085684719029814, 0.08804524200968444, 0.08893372607417405, 0.08809660212136805, 0.08680886309593916, 0.09109386708587408, 0.08947808900848031, 0.0873291571624577, 0.08818918792530894, 0.09098908095620573, 0.09054515487514436, 0.09178929799236357, 0.08709343988448381, 0.08595106797292829, 0.08437032392248511, 0.0854486299213022, 0.08718623593449593, 0.08812647103331983, 0.09557637595571578, 0.10450924816541374, 0.09348579193465412, 0.08952593593858182, 0.1069197878241539, 0.09032967896200716, 0.09081554599106312, 0.09320780215784907, 0.09381936094723642, 0.08472861698828638, 0.08715720195323229, 0.0884755291044712, 0.084273518063128, 0.08463544817641377, 0.08705387893132865, 0.08771857596002519, 0.09776769299060106, 0.0854302030056715, 0.08572163502685726, 0.09210917796008289, 0.09830486308783293, 0.09170945291407406, 0.0890005910769105, 0.22446245816536248, 0.0976351669523865, 0.08771290211006999, 0.0843653199262917, 0.08514425298199058, 0.08538098284043372, 0.08521124208346009, 0.08753331401385367, 0.08588876505382359, 0.09547983296215534, 0.09250634582713246, 0.08744988311082125, 0.08839769987389445, 0.08820500504225492, 0.0866165969055146, 0.08544057887047529, 0.09350721701048315, 0.0887324979994446, 0.088514813920483, 0.08784758299589157, 0.10192561987787485, 0.09684650180861354, 0.0876771358307451, 0.09448133897967637, 0.08380458108149469, 0.12158624897710979, 0.08246714598499238, 0.09050965798087418, 0.08660729904659092, 0.08618874405510724, 0.08781790500506759, 0.09007783816196024, 0.0865955341141671, 0.08617683500051498, 0.09394376212731004, 0.0902209710329771, 0.09038711991161108, 0.09356861608102918, 0.09335894393734634, 0.09303109999746084, 0.0939684840850532, 0.09335941704921424, 0.09567757602781057, 0.09460263093933463, 0.09432801092043519, 0.09325731103308499, 0.09345184406265616, 0.09204088780097663, 0.09233120898716152, 0.09331486118026078, 0.09251614194363356, 0.09336951584555209, 0.09236824908293784, 0.09646049910224974, 0.10632627503946424, 0.10517141595482826, 0.10619928198866546, 0.09102268400602043, 0.09372516488656402, 0.0879439131822437, 0.08636288205161691, 0.08544010599143803, 0.085882370127365, 0.09209557296708226, 0.21959308092482388, 0.09051003912463784, 0.0909148019272834, 0.09619981911964715, 0.08958501601591706, 0.08443198399618268, 0.09110909211449325, 0.0865722328890115, 0.09402977209538221, 0.09799296385608613, 0.09716963092796504, 0.09024795913137496, 0.09186062612570822, 0.08793165814131498, 0.08656610106118023, 0.08479302609339356, 0.18998764292337, 0.09250542894005775, 0.08724838891066611, 0.09461148316040635, 0.08487916900776327, 0.0874958629719913, 0.08731139404699206, 0.0910896179266274, 0.09733779705129564, 0.08601994509808719, 0.08624502783641219, 0.08624658989720047, 0.08862893097102642, 0.09136353898793459, 0.09214081591926515, 0.08571560611017048, 0.09011264401488006, 0.0827395839150995, 0.09137877798639238, 0.09057537885382771, 0.09088261099532247, 0.08501621009781957, 0.21145720104686916, 0.08989653293974698, 0.09100094507448375, 0.08923120005056262, 0.47101362398825586, 1.8164139280561358, 0.08963786298409104, 0.08676059311255813, 0.0833775419741869, 0.08405410102568567, 0.08444754802621901, 0.08542404603213072, 0.08651060308329761, 0.1002768122125417, 0.20180454687215388, 0.08331124810501933, 0.08461242914199829, 0.08417473104782403, 0.08408125885762274, 0.08364233491010964, 0.08504149899818003, 0.0858849841170013, 0.08822396909818053, 0.08476362703368068, 0.08336037211120129, 0.0897249870467931, 0.08821745892055333, 0.08784958207979798, 0.08748126402497292, 0.08586661703884602, 0.09467535116709769, 0.08241935889236629, 0.0825924789533019, 0.08281635702587664, 0.08371940511278808, 0.08510741614736617, 0.08630538592115045, 0.08367034094408154, 0.20623521716333926, 0.08922272198833525, 0.08884102385491133, 0.08629390900023282, 0.08708605682477355, 0.08801389508880675, 0.08720717113465071, 0.08568150107748806, 0.0893222470767796, 0.08661505789496005, 0.08709025499410927, 0.09582461416721344, 0.08959177508950233, 0.0856072991155088, 0.08918603602796793, 0.08650876209139824, 0.10136479698121548, 0.08862386993132532, 0.09006556589156389, 0.08988284110091627, 0.08939940785057843, 0.0906850821338594, 0.09117833687923849, 0.08944071689620614, 0.21806444693356752, 0.0837727680336684, 0.08562987810000777, 0.08436079300008714, 0.08752406807616353, 0.08610587986186147, 0.09922775696031749, 0.08899240591563284, 0.09000203292816877, 0.10370277287438512, 0.08626945200376213, 0.0862581159453839, 0.10117674805223942, 0.09312824998050928, 0.09129368490539491, 0.0857703359797597, 0.08616189402528107, 0.08570443000644445, 0.09005002491176128, 0.0869295580778271, 0.09011990902945399, 0.09556770394556224, 0.08897756296209991, 0.09312321804463863, 0.08788181305862963, 0.09074115892872214, 0.08866783510893583, 0.09254667605273426, 0.09176110289990902, 0.0887922930996865, 0.09066237299703062, 0.10144289117306471, 0.0926810810342431, 0.09280634298920631, 0.09480001009069383, 0.09154019900597632, 0.09176898701116443, 0.08794794394634664, 0.08982769306749105, 0.0911499890498817, 0.0948498579673469, 0.08994901995174587, 0.08955205092206597, 0.0912750530987978, 0.09666316490620375, 0.10486894915811718, 0.08941077603958547, 0.09570039086975157, 0.09445503307506442, 0.09474457101896405, 0.0944341691210866, 0.09498983202502131, 0.23459040303714573, 0.09589428617618978, 0.09191477997228503, 0.08793836715631187, 0.09720266191288829, 0.09438609005883336, 0.09269635495729744, 0.09787189704366028, 0.09138902183622122, 0.0876792089547962, 0.09311209502629936, 0.0888465999159962, 0.10150388488546014, 0.19786616787314415, 0.0900834291242063, 0.09250910300761461, 0.08908850606530905, 0.09282821998931468, 0.09716565092094243, 0.09024793887510896, 0.10103691695258021, 0.08955716690979898, 0.08775415690615773, 0.08728280896320939, 0.0888398359529674, 0.09640950500033796, 0.1001816710922867, 0.10219615092501044, 0.091066082008183, 0.09316518087871373, 0.09205739782191813, 0.09143530484288931, 0.09187067486345768, 0.11190366698428988, 0.09110402502119541, 0.11060619493946433, 0.09084622701629996, 0.0876785300206393, 0.08910849108360708, 0.09230487910099328, 0.09725034702569246, 0.0959406178444624, 0.10411175899207592, 0.16930562094785273, 0.09457016689702868, 0.09480748302303255, 0.09428228694014251, 0.09551031282171607, 0.08858683588914573, 0.08970597898587584, 0.09377862210385501, 0.09801475307904184, 0.09671085211448371, 0.09600607003085315, 0.10029383888468146, 0.0931483400054276, 0.09764493606053293, 0.11042487993836403, 0.09467995911836624, 0.09551698109135032, 0.09698134218342602, 0.09889913583174348, 0.09682015981525183, 0.09756761812604964, 0.21266293711960316, 0.09552881587296724, 0.10161703289486468, 0.09514626511372626, 0.09438834991306067, 0.09281972493045032, 0.08914127410389483, 0.22555931308306754, 0.09323893790133297, 0.0874291229993105, 0.0907631351146847, 0.0968012420926243, 0.09213253296911716, 0.09566475311294198, 0.09576624911278486, 0.09387653111480176, 0.09408922703005373, 0.09292537183500826, 0.10233995807357132, 0.09623352694325149, 0.09620884410105646, 0.09105255105532706, 0.09470365708693862, 0.09176804102025926, 0.09635214507579803, 0.09657118306495249, 0.1166136919055134, 0.09240488102659583, 0.09196683391928673, 0.0968355699442327, 0.08939549000933766, 0.09456651308573782, 0.09057461889460683, 0.09092452400363982, 0.09347240999341011, 0.09272932796739042, 0.09549657092429698, 0.09132344298996031, 0.08702423493377864, 0.09024204197339714, 0.09010654990561306, 0.09078946709632874, 0.10557951382361352, 0.1011718730442226, 0.09470938588492572, 0.09666908578947186, 0.09356316807679832, 0.08780625904910266, 3.22555757407099, 0.09165240707807243, 0.09655248699709773, 0.09485138603486121, 0.09382480406202376, 0.09690281795337796, 0.09832543903030455, 0.09623631415888667, 0.0951203869190067, 0.09572102106176317, 0.09486221219412982, 0.09475393895991147, 0.09520127903670073, 0.09775704517960548, 0.09138050489127636, 0.09408730105496943, 0.09414457390084863, 0.09610744519159198, 0.09485876094549894, 0.0972132480237633, 0.09619502210989594, 0.09449674398638308, 0.09385292395018041, 0.0977425891906023, 0.09532325412146747, 0.09533009282313287, 0.11349511495791376, 0.09515177016146481, 0.09614916518330574, 0.0972859668545425, 0.09519427898339927, 0.09623602102510631, 0.0967140628490597, 0.0897471469361335, 0.09076078282669187, 0.09225829294882715, 0.09325795713812113, 0.09373861411586404, 0.08974598720669746, 0.22425150708295405, 0.09916698094457388, 0.09210569108836353, 0.09541600802913308, 0.09614133602008224, 0.097524096025154, 0.09368271799758077, 0.09265585686080158, 0.09273892384953797, 0.09615807211957872, 0.09379496891051531, 0.09676086809486151, 0.091765854973346, 0.09777665720321238, 0.08975978498347104, 0.09013437991961837, 0.08947036694735289, 0.09178184997290373, 0.08761207200586796, 0.0953442701138556, 0.0903106911573559, 0.09028221503831446, 0.09136796207167208, 0.09452143288217485, 0.09309835499152541, 0.09922590106725693, 0.09882344398647547, 0.0920965289697051, 0.0883199512027204, 0.0902393558062613, 0.18114404496736825, 0.09683672012761235, 0.0888662631623447, 0.08709761803038418, 0.08858627709560096, 0.08553832396864891, 0.09375243098475039, 0.09660936309956014, 0.09102419996634126, 0.09102925099432468, 0.09071343089453876, 0.08915553614497185, 0.09072549804113805, 0.08925473201088607]
[0.0017926873430153545, 0.0017757436726242304, 0.001665123961675836, 0.001739523511341944, 0.0017210771614799695, 0.0017123967361617454, 0.001729900858422019, 0.001688975389400611, 0.0017070471812799877, 0.0017004829394269962, 0.0017765702879322426, 0.001851146061885722, 0.001788229063837504, 0.0018197383246935752, 0.001774678182579121, 0.00181041151399211, 0.0017810575283911763, 0.00179504848807594, 0.001783243879409773, 0.0018250936765831951, 0.0018254432694188186, 0.001754427794367075, 0.0017573943276110353, 0.0020277676913811236, 0.0020175729403082206, 0.035475409675237476, 0.001825484489946037, 0.001754752778429158, 0.0018080846758141201, 0.001811060446257494, 0.0018207943494602734, 0.001811055898932474, 0.001978051492336149, 0.0017952467552481257, 0.0018062205291448198, 0.001755746447348169, 0.0017911944899479954, 0.0018488159403204918, 0.0018009453660295326, 0.001798203386062262, 0.0016664811837657982, 0.0016844775717781515, 0.0017438384856344486, 0.0017419557552784681, 0.0016650311430269967, 0.0017121099600834505, 0.0016435436495788852, 0.004446563287162963, 0.0018323762057235046, 0.0018227869996382874, 0.001846714163845291, 0.002012276385283592, 0.00178626355030859, 0.0018635454102970508, 0.0017094652843186442, 0.0016681530408333152, 0.001623431491494483, 0.0017489817953307409, 0.0017241439592017203, 0.0017302793887804967, 0.0017684494306770514, 0.0017306547355362956, 0.0018855353902873335, 0.0017255520429082063, 0.0017426513296989153, 0.001838570774285769, 0.0022124273474423252, 0.001972096406721643, 0.0018606161632175957, 0.001785724100713827, 0.004506790692138733, 0.0017294549354713183, 0.001682154777250728, 0.001759906223385918, 0.0016495726755535115, 0.0016614369601391408, 0.0016460262467058338, 0.0016714333252487133, 0.0016250160608288584, 0.001818750039389243, 0.002020550996293219, 0.0017772293697130314, 0.0017424402663446202, 0.0017783966115010636, 0.0017564064081834288, 0.0017149002215235817, 0.0017882037328138035, 0.0017741652471678598, 0.0017437876952926116, 0.0017336960596849723, 0.001946938366686203, 0.0018143220186917757, 0.0017594827757197984, 0.002216840794842158, 0.0017641958999162425, 0.001735031595263554, 0.0017486677353023266, 0.0017968416736670295, 0.0018149740015137561, 0.0017978898392115928, 0.0017716094509375338, 0.0018590585119566138, 0.0018260834491526593, 0.0017822276971930144, 0.0017997793454144681, 0.0018569200195144026, 0.0018478603035743748, 0.0018732509794359912, 0.0017774171404996697, 0.0017541034280189446, 0.001721843345356839, 0.001743849590230657, 0.0017793109374386923, 0.0017984994088432618, 0.001950538284810526, 0.002132841799294158, 0.0019078733047888595, 0.0018270599171139148, 0.0021820364862072225, 0.00184346283595933, 0.001853378489613533, 0.0019022000440377363, 0.0019146808356578862, 0.0017291554487405382, 0.0017787184072088223, 0.001805623042948392, 0.0017198677155740407, 0.0017272540444166077, 0.001776609774108748, 0.001790175019592351, 0.0019952590406245117, 0.00174347353072799, 0.0017494211229970868, 0.001879779142042508, 0.0020062216956700596, 0.0018716214880423279, 0.0018163385934063367, 0.004580866493170663, 0.0019925544275997244, 0.0017900592267361222, 0.001721741222985545, 0.0017376378159589913, 0.0017424690375598718, 0.0017390049404787775, 0.001786394163548034, 0.0017528319398739509, 0.0019485680196358233, 0.001887884608716989, 0.0017846914920575765, 0.0018040346913039684, 0.0018001021437194882, 0.001767685651132951, 0.0017436852830709244, 0.00190831055123435, 0.0018108673061111144, 0.001806424773887408, 0.0017928078162426852, 0.002080114691385201, 0.00197645922058395, 0.0017893293026682673, 0.0019281905914219667, 0.0017102975730917283, 0.0024813520199410163, 0.001683002979285559, 0.0018471358771606976, 0.001767495898910019, 0.001758953960308311, 0.001792202142960563, 0.001838323227795107, 0.001767255798248308, 0.0017587109183778568, 0.0019172196352512253, 0.0018412443067954512, 0.0018446351002369607, 0.0019095635934903914, 0.0019052845701499252, 0.0018985938774992008, 0.001917724165001086, 0.0019052942254941683, 0.0019526035924042975, 0.0019306659375374414, 0.0019250614473558202, 0.0019032104292466324, 0.0019071804910746155, 0.0018783854653260537, 0.0018843103874930922, 0.0019043849220461383, 0.0018880845294619094, 0.0019055003233786141, 0.0018850663078150579, 0.0019685816143316273, 0.0021699239803972294, 0.0021463554276495564, 0.0021673322854829685, 0.0018576057960412332, 0.001912758467072735, 0.0017947737384131368, 0.0017625077969717737, 0.0017436756324783272, 0.001752701431170714, 0.0018795014891241277, 0.004481491447445385, 0.001847143655604854, 0.0018554041209649674, 0.0019632616146866766, 0.0018282656329778992, 0.0017231017142078097, 0.0018593692268263928, 0.001766780263041051, 0.0019189749407220859, 0.0019998564052262475, 0.0019830536924074498, 0.0018417950843137745, 0.0018747066556266984, 0.0017945236355370404, 0.001766655123697556, 0.001730469920273338, 0.0038772988351708166, 0.0018878658967358725, 0.001780579365523798, 0.0019308465951103338, 0.0017322279389339443, 0.001785629856571251, 0.001781865184632491, 0.0018589717944209672, 0.001986485654108074, 0.0017555090836344324, 0.0017601026089063712, 0.0017601344876979686, 0.0018087536932862535, 0.0018645620201619304, 0.0018804248146788807, 0.0017492980838810302, 0.0018390335513240829, 0.001688562937042847, 0.0018648730201304567, 0.0018484771194658717, 0.0018547471631698462, 0.0017350246958738687, 0.004315453082589167, 0.001834623121219326, 0.0018571621443772195, 0.0018210448989910738, 0.009612522938535834, 0.03706967200114563, 0.0018293441425324703, 0.0017706243492358802, 0.0017015824892691203, 0.001715389816850728, 0.0017234193474738573, 0.0017433478782067494, 0.0017655225119040329, 0.002046465555357994, 0.004118460140248038, 0.0017002295531636598, 0.0017267842682040467, 0.001717851654037225, 0.0017159440583188314, 0.0017069864267369313, 0.0017355407958812251, 0.0017527547778979856, 0.0018004891652689905, 0.001729869939462871, 0.0017012320839020672, 0.0018311221846284307, 0.0018003563045010883, 0.0017928486138734283, 0.0017853319188769982, 0.0017523799395682861, 0.0019321500238183202, 0.0016820277324972712, 0.001685560794965345, 0.0016901297352219723, 0.0017085592880160833, 0.0017368860438237994, 0.0017613344065540908, 0.0017075579784506438, 0.004208881982925291, 0.0018208718773129644, 0.0018130821194879863, 0.0017611001836782207, 0.0017772664658117052, 0.0017962019405878928, 0.001779738186421443, 0.001748602062805879, 0.0018229030015669307, 0.0017676542427542867, 0.001777352142736924, 0.001955604370759458, 0.0018284035732551497, 0.0017470877370511998, 0.0018201231842442434, 0.0017654849406407804, 0.0020686693261472546, 0.0018086504067617412, 0.0018380727732972223, 0.0018343436959370667, 0.0018244777112362944, 0.001850715961915498, 0.0018607823852905814, 0.0018253207529837989, 0.004450294835378929, 0.0017096483272177223, 0.00174754853265322, 0.0017216488367364723, 0.001786205470942113, 0.0017572628543237035, 0.0020250562644962755, 0.0018161715492986295, 0.001836776182207526, 0.0021163831198854105, 0.001760601061301268, 0.0017603697131711002, 0.0020648315929028454, 0.0019005765302144751, 0.0018631364266407124, 0.0017504150199950958, 0.0017584060005159403, 0.0017490700001315195, 0.0018377556104441078, 0.001774072613833206, 0.0018391818169276326, 0.0019909938321992136, 0.0018536992283770815, 0.0019400670425966382, 0.0018308711053881173, 0.0018904408110150446, 0.0018472465647694964, 0.0019280557510986303, 0.0019116896437481046, 0.001849839439576802, 0.0018887994374381378, 0.0021133935661055148, 0.0019308558548800647, 0.0019334654789417982, 0.001975000210222788, 0.0019070874792911734, 0.0019118538960659255, 0.001832248832215555, 0.001871410272239397, 0.0018989581052058686, 0.0019760387076530606, 0.0018739379156613722, 0.0018656677275430411, 0.0019015636062249541, 0.0020138159355459115, 0.002184769774127441, 0.0018627245008246973, 0.0019937581431198246, 0.001967813189063842, 0.0019738452295617512, 0.0019673785233559706, 0.0019789548338546106, 0.0048873000632738695, 0.0019977976286706203, 0.001914891249422605, 0.0018320493157564972, 0.002025055456518506, 0.001966376876225695, 0.0019311740616103634, 0.0020389978550762558, 0.0019039379549212754, 0.001826650186558254, 0.0019398353130479034, 0.001850970831583254, 0.002114664268447086, 0.004122211830690503, 0.001876738106754298, 0.0019272729793253045, 0.0018560105430272718, 0.001933921249777389, 0.0020242843941863007, 0.0018801653932314366, 0.002104935769845421, 0.001865774310620812, 0.0018282116022116195, 0.0018183918534001957, 0.0018508299156868209, 0.0020085313541737073, 0.002087118147755973, 0.002129086477604384, 0.0018972100418371458, 0.001940941268306536, 0.0019178624546232943, 0.0019049021842268605, 0.0019139723929887016, 0.0023313263955060393, 0.0018980005212749045, 0.0023042957279055067, 0.0018926297295062493, 0.001826636042096652, 0.0018564268975751474, 0.0019230183146040265, 0.002026048896368593, 0.001998762871759633, 0.0021689949790015817, 0.0035272004364135987, 0.001970211810354764, 0.001975155896313178, 0.001964214311252969, 0.0019897981837857515, 0.0018455590810238693, 0.0018688745622057468, 0.001953721293830313, 0.0020419740224800384, 0.002014809419051744, 0.002000126458976107, 0.002089454976764197, 0.0019405904167797416, 0.0020342695012611025, 0.0023005183320492506, 0.001972499148299297, 0.001989937106069798, 0.0020204446288213753, 0.0020603986631613225, 0.0020170866628177464, 0.0020326587109593675, 0.004430477856658399, 0.001990183664020151, 0.002117021518643014, 0.0019822138565359637, 0.0019664239565220973, 0.0019337442693843816, 0.0018571098771644756, 0.00469915235589724, 0.001942477872944437, 0.0018214400624856353, 0.001890898648222598, 0.00201669254359634, 0.001919427770189941, 0.001993015689852958, 0.0019951301898496845, 0.0019557610648917034, 0.0019601922297927863, 0.001935945246562672, 0.0021320824598660693, 0.0020048651446510726, 0.0020043509187720097, 0.0018969281469859804, 0.0019729928559778878, 0.001911834187922068, 0.0020073363557457924, 0.0020118996471865103, 0.002429451914698196, 0.0019251016880540799, 0.0019159757066518068, 0.0020174077071715146, 0.0018624060418612014, 0.0019701356892862045, 0.0018869712269709755, 0.0018942609167424962, 0.0019473418748627107, 0.0019318609993206337, 0.001989511894256187, 0.0019025717289575066, 0.0018130048944537218, 0.0018800425411124404, 0.0018772197897002723, 0.0018914472311735153, 0.002199573204658615, 0.002107747355087971, 0.0019731122059359527, 0.002013939287280664, 0.0019492326682666317, 0.0018292970635229722, 0.06719911612647896, 0.0019094251474598423, 0.0020115101457728692, 0.0019760705423929417, 0.001954683417958828, 0.002018808707362041, 0.002048446646464678, 0.0020049232116434723, 0.0019816747274793065, 0.0019941879387867325, 0.0019762960873777047, 0.0019740403949981555, 0.001983359979931265, 0.0020366051079084477, 0.0019037605185682576, 0.001960152105311863, 0.001961345289601013, 0.0020022384414914995, 0.0019762241863645613, 0.0020252760004950687, 0.002004062960622832, 0.001968682166382981, 0.0019552692489620918, 0.0020363039414708814, 0.0019859011275305725, 0.001986043600481935, 0.002364481561623203, 0.001982328545030517, 0.002003107607985536, 0.002026790976136302, 0.001983214145487485, 0.002004917104689715, 0.0020148763093554103, 0.0018697322278361146, 0.0018908496422227472, 0.0019220477697672322, 0.0019428741070441902, 0.0019528877940805007, 0.0018697080668061972, 0.004671906397561543, 0.0020659787696786225, 0.001918868564340907, 0.001987833500606939, 0.00200294450041838, 0.0020317520005240417, 0.001951723291616266, 0.0019303303512666996, 0.001932060913532041, 0.00200329316915789, 0.0019540618523024023, 0.002015851418642948, 0.0019117886452780415, 0.0020370136917335913, 0.00186999552048898, 0.001877799581658716, 0.0018639659780698519, 0.0019121218744354944, 0.0018252515001222491, 0.001986338960705325, 0.0018814727324449148, 0.0018808794799648847, 0.0019034992098265018, 0.001969196518378643, 0.0019395490623234461, 0.0020672062722345195, 0.002058821749718239, 0.0019186776868688564, 0.0018399989833900083, 0.0018799865792971104, 0.0037738342701535053, 0.002017431669325257, 0.0018513804825488478, 0.0018145337089663371, 0.0018455474394916866, 0.001782048416013519, 0.0019531756455156333, 0.0020126950645741695, 0.0018963374992987763, 0.001896442729048431, 0.001889863143636224, 0.0018574070030202468, 0.0018901145425237094, 0.0018594735835601266]
[557.8217550852843, 563.1443408283018, 600.5558883397287, 574.8700684295761, 581.0314739985807, 583.9768196717379, 578.0678095692605, 592.0749386140452, 585.8068897956144, 588.0682345081129, 562.8823170086358, 540.2058868230646, 559.2124746334342, 549.5295595142172, 563.4824442067072, 552.3606054597585, 561.4641773549544, 557.0880155286895, 560.7757926700328, 547.9170810958732, 547.8121488368018, 569.9864099341625, 569.0242561323041, 493.1531379311476, 495.64502973916376, 28.18853986901297, 547.7997789121512, 569.8808472012752, 553.0714426025061, 552.1626857162455, 549.2108432214894, 552.1640721246923, 505.5480122102203, 557.0264906908507, 553.6422512446272, 569.5583217670026, 558.2866660275588, 540.886725493426, 555.2639290800128, 556.1106200505038, 600.0667812763836, 593.655870967988, 573.4476032258096, 574.0673934856287, 600.5893668643447, 584.074634990885, 608.4414005409736, 224.8927847011549, 545.7394594387646, 548.6104521254756, 541.5023177803358, 496.9496274534222, 559.8278035888056, 536.6115547678546, 584.9782438831909, 599.465382085361, 615.9791806671371, 571.7612399795706, 579.9979721316314, 577.9413466311943, 565.4671163637117, 577.816001924913, 530.3533442814943, 579.524682613816, 573.838256085785, 543.9007374565011, 451.9922433412555, 507.0746017241477, 537.4563651380319, 559.9969220330615, 221.88738468469725, 578.2168586702584, 594.4756175376294, 568.2120937535414, 606.2176070323495, 601.8886205085101, 607.5237269158279, 598.2888966577219, 615.3785332373504, 549.8281667863558, 494.91450690160246, 562.6735732830421, 573.9077656290915, 562.3042652763185, 569.3443131047641, 583.1243051047963, 559.220396227707, 563.6453546795163, 573.4643057176738, 576.8023722576313, 513.626942234466, 551.1700732822799, 568.3488430802646, 451.0923844087781, 566.8304750325495, 576.3583802911083, 571.8639280704234, 556.5320610352834, 550.9720795812848, 556.2076041536104, 564.4584925141379, 537.9066842535926, 547.6200994341309, 561.0955331773752, 555.6236671722176, 538.5261559415504, 541.166449685438, 533.8312970219748, 562.6141310412247, 570.0918110224455, 580.7729272797216, 573.4439515897303, 562.0153166930419, 556.0190874030748, 512.6789911212327, 468.85802797513617, 524.1438189265235, 547.3274251342746, 458.2874788396332, 542.4573690847445, 539.5551991156001, 525.7070638466255, 522.2802575638666, 578.3170048293623, 562.2025363583026, 553.8254531616441, 581.4400671311106, 578.9536306095362, 562.8698066246195, 558.6045995813944, 501.18805610674104, 573.5676409050205, 571.61765503712, 531.9773890636067, 498.44939976387263, 534.2960670140502, 550.5581413235368, 218.2993111654399, 501.8683485623137, 558.6407338171347, 580.8073749119937, 575.493921009141, 573.8982894068439, 575.0414945484187, 559.7868714561052, 570.5053503714176, 513.1973787534985, 529.6933908898184, 560.3209319091317, 554.3130654972014, 555.5240315050872, 565.7114427325224, 573.497987113151, 524.0237231582519, 552.2215772659381, 553.5796532772356, 557.7842705392545, 480.74272257270326, 505.9552909493106, 558.8686210575041, 518.6209311718186, 584.6935736406889, 403.00609988572717, 594.1760129411676, 541.3786892262294, 565.7721755488548, 568.5197126050526, 557.9727732877757, 543.9739785039894, 565.8490417692749, 568.5982781765794, 521.5886493197551, 543.1109800634907, 542.1126378173876, 523.6798624612194, 524.8559798714534, 526.7055855658741, 521.451425731726, 524.8533200905672, 512.1367203717325, 517.955996714531, 519.4639378257541, 525.4279740342955, 524.3342225237121, 532.3720921288209, 530.6981305401661, 525.1039264297286, 529.6373040485602, 524.7965522393168, 530.4853181313711, 507.97995507009745, 460.8456374664971, 465.90605969444954, 461.3967164601896, 538.3273470243861, 522.8051618720001, 557.1732963310226, 567.3733765706654, 573.5011612100564, 570.5478310313534, 532.0559764312893, 223.13999964677757, 541.376409444747, 538.9661415001683, 509.3564670746101, 546.9664702777293, 580.3487929670748, 537.8167959178389, 566.0013420564032, 521.1115469927464, 500.03590127105554, 504.2727808272244, 542.9485660575455, 533.4167865669142, 557.2509496096627, 566.0414342257307, 577.8777130330263, 257.9115106963233, 529.6986410576111, 561.6149548637654, 517.9075347220204, 577.2912314388744, 560.0264782311549, 561.2096855723962, 537.931776588079, 503.4015714797582, 569.6353321793692, 568.1486948203228, 568.1384047578503, 552.8668738655838, 536.3189795709524, 531.79472648619, 571.6578604953243, 543.7638694954812, 592.2195602322552, 536.2295390653704, 540.9858685667452, 539.1570451526962, 576.360672201462, 231.72537874053873, 545.0710766881541, 538.4559463629063, 549.135279725413, 104.03096111126874, 26.976230055909188, 546.6440003003701, 564.7725337288815, 587.6882292256836, 582.9578735846135, 580.2418323002893, 573.6089810305816, 566.4045591361772, 488.647364419025, 242.80919711408788, 588.1558746813187, 579.1111364710639, 582.1224420920402, 582.7695810664911, 585.8277396567213, 576.1892790841874, 570.5304658757018, 555.4046196387977, 578.0781417072908, 587.8092762666037, 546.1132022726921, 555.4456067945497, 557.7715777348941, 560.119935921504, 570.6525037295044, 517.5581542181685, 594.5205187047189, 593.2743588881111, 591.6705559107068, 585.2884398065949, 575.7430106344077, 567.7513573111988, 585.6316521137115, 237.59278688659546, 549.1874592932295, 551.5469979277053, 567.8268671299593, 562.6618288458419, 556.7302748112511, 561.8803977065419, 571.8854056453296, 548.5755408490853, 565.7214945168467, 562.634705838378, 511.35087185945, 546.9252054783926, 572.3810995822324, 549.4133631483974, 566.4166127846162, 483.40253677102993, 552.8984464114479, 544.0481000140993, 545.1541072782188, 548.1020644107427, 540.331428797424, 537.4083546281202, 547.8489182601628, 224.7042133141844, 584.9156133924909, 572.2301734772124, 580.8385419035758, 559.8460066705327, 569.0668288694112, 493.81343991878964, 550.6087794328574, 544.4321467616985, 472.50424112915056, 567.987843458924, 568.0624885318078, 484.30099744558305, 526.1561342584571, 536.7293482651876, 571.2930868262326, 568.6968764361509, 571.7324063215343, 544.1419927203173, 563.6747854640056, 543.7200339825606, 502.26172669526517, 539.4618418628262, 515.4461047189241, 546.1880943213722, 528.9771539914358, 541.3462496408985, 518.6572013958556, 523.0974615939102, 540.5874578113522, 529.4368370610827, 473.17263383306494, 517.9050510024298, 517.2060276697096, 506.32906002941434, 524.3597951635026, 523.0525209367345, 545.7773979262414, 534.3563700777189, 526.6045613426467, 506.0629612806012, 533.6356085452641, 536.0011245501538, 525.8830137085093, 496.56971242951107, 457.7141316408877, 536.8480414346099, 501.5653495640169, 508.17831974982096, 506.62533466315796, 508.29059488470557, 505.3172426639969, 204.61195078129236, 500.5511998056693, 522.2228679051768, 545.8368349582751, 493.8136369456317, 508.54951158672134, 517.8197138615884, 490.43700438939464, 525.2271994553248, 547.4501945466551, 515.5076790662102, 540.2570277915379, 472.8883042670195, 242.58821260830064, 532.8393963979545, 518.8678566697271, 538.7900428458429, 517.0841367584686, 494.0017335864356, 531.8681024552324, 475.07387841740956, 535.9705052789922, 546.9826352651315, 549.9364716851917, 540.2981611246066, 497.87622081278954, 479.1295600947075, 469.68500834460457, 527.0897675787438, 515.2139409516993, 521.4138258921282, 524.9613383197775, 522.473575723045, 428.9403671350529, 526.8702451821724, 433.9720756714467, 528.3653661410469, 547.4544336988876, 538.6692044303998, 520.0158482140678, 493.5715035270664, 500.30947348928834, 461.0430220821967, 283.5109651485483, 507.55964142755596, 506.2891500699251, 509.1094155413732, 502.5635303864935, 541.8412286455904, 535.0813908129531, 511.84373285888614, 489.7221947933844, 496.3248585916593, 499.9683872548309, 478.5937055933289, 515.3070897152126, 491.57695151997854, 434.68464739823315, 506.97106807990633, 502.52844522058194, 494.94056196103185, 485.3429668148175, 495.7645194099203, 491.9665040709285, 225.70928742982824, 502.4661884622297, 472.36175503827104, 504.48643404579934, 508.5373358493066, 517.1314613996791, 538.4711008736047, 212.80433666830183, 514.8063789700652, 549.0161442014985, 528.8490744546132, 495.86140593186997, 520.98860688102, 501.752196478583, 501.2204241545466, 511.3099028052147, 510.154047547526, 516.5435343667542, 469.0250113791644, 498.78666536149507, 498.9146314821271, 527.1680962660051, 506.8442072510005, 523.0579128239561, 498.1726142395636, 497.04268371358603, 411.6154734119227, 519.4530793907381, 521.9272856791663, 495.68562489633763, 536.9398388552514, 507.57925225054305, 529.9497871015399, 527.910379801147, 513.5205137364495, 517.6355857650547, 502.63584896730015, 525.604362127224, 551.5704910997005, 531.9028575854937, 532.7026731162182, 528.695690537223, 454.6336525113312, 474.4401636116693, 506.81354916946884, 496.53929803924586, 513.0223889020169, 546.6580687961849, 14.881148110904434, 523.717832736374, 497.138929227611, 506.05480854395023, 511.5917957928178, 495.3416320988089, 488.1747844035162, 498.77221940100225, 504.62368325805, 501.4572501167577, 505.99705498930257, 506.5752466534173, 504.1949066828785, 491.01320433541474, 525.2761522505258, 510.1644904444283, 509.8541319073018, 499.44101525444887, 506.01546469259046, 493.7598627325633, 498.98631911704774, 507.9540095785392, 511.43851443008487, 491.08582448535213, 503.5497417957961, 503.51361861206834, 422.9256917163294, 504.45724676007507, 499.22430328427004, 493.39078956544057, 504.2319823481264, 498.7737386552758, 496.3083814906312, 534.8359434106368, 528.8627808737197, 520.2784320605642, 514.7013882033558, 512.0621896614601, 534.8428547501443, 214.04538424013387, 484.0320794562458, 521.1404358711145, 503.0602410587572, 499.2649570625235, 492.18605407651825, 512.3677133411046, 518.0460429189187, 517.582024974502, 499.17806110244106, 511.7545275354182, 496.0683067967333, 523.0703731136371, 490.9147169987623, 534.7606392867256, 532.5381951127449, 536.4904787776793, 522.9792166334715, 547.8697044944346, 503.4387482612293, 531.4985345020289, 531.6661756651573, 525.3482611590614, 507.82133254194446, 515.5837608985611, 483.74466226781675, 485.71470557703964, 521.1922809359022, 543.4785611444214, 531.9186908099525, 264.9824895355894, 495.67973736352434, 540.1374862844356, 551.1057717244931, 541.8446465269009, 561.1519816263027, 511.9867239262052, 496.8462523713557, 527.3322920470528, 527.3030314507653, 529.1388444540659, 538.384962678586, 529.0684651654947, 537.7866127495136]
Elapsed: 0.10715106356996078~0.1696096614639279
Time per graph: 0.002206288272304039~0.003507144689702272
Speed: 522.8842777526386~73.68879547312022
Total Time: 0.0913
best val loss: 0.14541013538837433 test_score: 0.8958

Testing...
Test loss: 0.3730 score: 0.9167 time: 0.09s
test Score 0.9167
Epoch Time List: [0.31099370401352644, 0.31831427616998553, 0.32284665503539145, 0.3025944430846721, 0.30491525470279157, 0.3056305758655071, 0.2985287131741643, 0.3058923401404172, 0.3031960120424628, 0.3046769257634878, 0.3124565358739346, 0.31756879622116685, 0.3129382347688079, 0.32057230011560023, 0.31602387269958854, 0.3197938709054142, 0.32514972309581935, 0.31694327807053924, 0.3176526839379221, 0.3222224081400782, 0.3351619769819081, 0.31867693923413754, 0.31132117682136595, 0.3431999038439244, 0.3483598700258881, 5.876168746035546, 0.3112674979493022, 0.3210631359834224, 0.31363874091766775, 0.3196337940171361, 0.32438020198605955, 0.31843655509874225, 0.32003137306310236, 0.41443767095915973, 0.32060050778090954, 0.31182512012310326, 0.3165409779176116, 0.3263592498842627, 0.3247044701129198, 0.3177550849504769, 0.3683477931190282, 0.2985377749428153, 0.301327157067135, 0.304742360021919, 0.3064157357439399, 0.32410791679285467, 0.29763513430953026, 0.43848468898795545, 0.3254619671497494, 0.32580348500050604, 0.34521875297650695, 0.3362231468781829, 0.31495678797364235, 0.32560492190532386, 0.30831063189543784, 0.36611390789039433, 0.29712144122458994, 0.3092544819228351, 0.30963098094798625, 0.31437275116331875, 0.3127486682496965, 0.31106371292844415, 0.3404822109732777, 0.3898457877803594, 0.309959001140669, 0.31834367010742426, 0.3376716561615467, 0.3347922791726887, 0.35782345198094845, 0.3228419590741396, 0.4608151069842279, 0.2999815260991454, 0.3077787689398974, 0.30966246384195983, 0.3038583716843277, 0.2966262467671186, 0.29173134872689843, 0.2993877022527158, 0.3276151609607041, 0.3139248692896217, 0.3278150318656117, 0.3235171341802925, 0.3104537967592478, 0.31026187911629677, 0.31178554194048047, 0.31133450707420707, 0.44833043892867863, 0.3136424000840634, 0.3074743398465216, 0.30735858296975493, 0.3212160796392709, 0.33497832086868584, 0.31563008087687194, 0.3351059730630368, 0.3795883981510997, 0.31321933399885893, 0.3077023511286825, 0.3139203598257154, 0.3294808058999479, 0.32183202798478305, 0.3096358261536807, 0.47451919014565647, 0.335008829832077, 0.32396014919504523, 0.323829913046211, 0.33295877603814006, 0.3311936550308019, 0.3334300257265568, 0.4693582581821829, 0.3178354010451585, 0.3156597388442606, 0.31423775223083794, 0.3135733709204942, 0.3230154509656131, 0.3325871452689171, 0.3313327331561595, 0.37444654991850257, 0.32887426810339093, 0.3570574370678514, 0.3397505700122565, 0.3237180607393384, 0.33320533693768084, 0.33087202766910195, 0.39472166216000915, 0.3151025311090052, 0.33206799114122987, 0.3132225959561765, 0.31313096499070525, 0.3152324981056154, 0.31988766603171825, 0.3304290061350912, 0.42051080986857414, 0.3124681932386011, 0.3353935570921749, 0.3497506428975612, 0.33152873418293893, 0.33052214211784303, 0.45828465023078024, 0.3272271358873695, 0.33621210884302855, 0.3116574778687209, 0.3167343698441982, 0.3089325102046132, 0.2914740249980241, 0.28849512804299593, 0.45039400993846357, 0.32406573998741806, 0.3018438077997416, 0.2997501250356436, 0.2976975392084569, 0.2912299772724509, 0.30419317493215203, 0.301942857215181, 0.3019552391488105, 0.3046345319598913, 0.31004221085458994, 0.3051629248075187, 0.3255393172148615, 0.31598114292137325, 0.3063589399680495, 0.32572071393951774, 0.29299249593168497, 0.3668699648696929, 0.2859703837893903, 0.30093860998749733, 0.2921442009974271, 0.29466563183814287, 0.2988655222579837, 0.30477060307748616, 0.31054464913904667, 0.30049380427226424, 0.31259335088543594, 0.3197790728881955, 0.3177669928409159, 0.3221442948561162, 0.3206210918724537, 0.31962851993739605, 0.32324764621444046, 0.3193573197349906, 0.32034189673140645, 0.33549115993082523, 0.3250704661477357, 0.3199213547632098, 0.3167396471835673, 0.31765452795661986, 0.31624169973656535, 0.31947224494069815, 0.31908880895935, 0.32224763091653585, 0.3177333581261337, 0.3196346261538565, 0.3561338731087744, 0.3571885130368173, 0.3887932829093188, 0.3459007570054382, 0.3101152388844639, 0.30400685709901154, 0.3011587290093303, 0.2890193702187389, 0.3028468661941588, 0.2988487984985113, 0.43750572274439037, 0.3077991418540478, 0.31975482893176377, 0.31513111013919115, 0.29900511517189443, 0.29771555098704994, 0.30406362493522465, 0.2969177851919085, 0.40981703205034137, 0.43301268573850393, 0.30755215510725975, 0.31099836295470595, 0.32239570282399654, 0.30745279882103205, 0.30298840207979083, 0.2914887226652354, 0.5107318570371717, 0.31936140172183514, 0.30738158198073506, 0.3033778890967369, 0.2961338860914111, 0.3030166528187692, 0.3137114872224629, 0.40809832606464624, 0.3202470720279962, 0.30958354799076915, 0.29679664806462824, 0.3091357182711363, 0.30167607171460986, 0.30675531504675746, 0.320741840172559, 0.4099143941421062, 0.3062814446166158, 0.2910180080216378, 0.30418614274822176, 0.3108539888635278, 0.3106184438802302, 0.3024594741873443, 0.429633857216686, 0.30769198085181415, 0.31082337093539536, 0.3079152989666909, 0.683507754933089, 6.570069010835141, 0.41173000494018197, 0.2936436247546226, 0.2921580739784986, 0.28756809211336076, 0.29086446901783347, 0.2864970581140369, 0.29404400824569166, 0.30867861886508763, 0.4199128597974777, 0.2838671070057899, 0.28441694215871394, 0.29601920512504876, 0.289231134345755, 0.28985345899127424, 0.2843660097569227, 0.29395299102179706, 0.2951267200987786, 0.40354839293286204, 0.28933091019280255, 0.2903956619556993, 0.3076659229118377, 0.30215911590494215, 0.29785724193789065, 0.29596763383597136, 0.3023356138728559, 0.387938927160576, 0.28560684411786497, 0.28861570078879595, 0.28475914197042584, 0.28439716971479356, 0.28702032077126205, 0.2889123761560768, 0.41134637617506087, 0.30565471504814923, 0.29793788911774755, 0.29070035414770246, 0.2941639421042055, 0.2995782170910388, 0.294794546905905, 0.2999537349678576, 0.28840844705700874, 0.3964940598234534, 0.2928131460212171, 0.31630554306320846, 0.3032731548883021, 0.2925563370808959, 0.30016519385389984, 0.30675979796797037, 0.3338101329281926, 0.4232708867639303, 0.30850440193898976, 0.3068141331896186, 0.30944803869351745, 0.30838763900101185, 0.31143598025664687, 0.30696212290786207, 0.4390196150634438, 0.293471448821947, 0.29391726618632674, 0.29538822593167424, 0.29746828111819923, 0.29734497191384435, 0.31210936699062586, 0.31050030793994665, 0.30755983176641166, 0.43839083914645016, 0.3011879369150847, 0.2928312746807933, 0.31148346117697656, 0.3352951018605381, 0.31200438598170877, 0.29342539492063224, 0.2984660828951746, 0.29138379730284214, 0.30413670116104186, 0.29774928698316216, 0.3090697000734508, 0.3271569258067757, 0.3217040151357651, 0.32301387027837336, 0.316797993844375, 0.3206526949070394, 0.3172436289023608, 0.3229105907958001, 0.323873768793419, 0.3107134068850428, 0.31573964189738035, 0.33664001896977425, 0.33641405729576945, 0.3305672740098089, 0.3389505119994283, 0.3296310598962009, 0.31927487812936306, 0.31461839005351067, 0.3171413289383054, 0.314914639107883, 0.3159207517746836, 0.33204270200803876, 0.3159672466572374, 0.31231268495321274, 0.32865289598703384, 0.33755784085951746, 0.39572564675472677, 0.3265863771084696, 0.3221612167544663, 0.32931439811363816, 0.32826290000230074, 0.33202556311152875, 0.4656533298548311, 0.33701445278711617, 0.3261621128767729, 0.3029704599175602, 0.3306112459395081, 0.3419527211226523, 0.3298835039604455, 0.42938520503230393, 0.32487577316351235, 0.313734243856743, 0.3227759669534862, 0.3170628040097654, 0.36359346681274474, 0.4390988328959793, 0.321896743029356, 0.3189065200276673, 0.3171027069911361, 0.3307789450045675, 0.332042328780517, 0.3339769900776446, 0.33173772506415844, 0.4085530638694763, 0.3147447449155152, 0.3186207271646708, 0.3189847192261368, 0.335368521977216, 0.3701232310850173, 0.3406533976085484, 0.4499540510587394, 0.33217833610251546, 0.3269642267841846, 0.32829198706895113, 0.3203661891166121, 0.33423095499165356, 0.3192787538282573, 0.4601377977523953, 0.33170014014467597, 0.31220579287037253, 0.3180914658587426, 0.3214934621937573, 0.33478664024733007, 0.3322771810926497, 0.3350225880276412, 0.5011773267760873, 0.3210748650599271, 0.3288726732134819, 0.3283588767517358, 0.3301366460509598, 0.317929367069155, 0.3126627702731639, 0.449911201139912, 0.3423301503062248, 0.3420590788591653, 0.33263950794935226, 0.3444514072034508, 0.3215211941860616, 0.3389920527115464, 0.4500805970747024, 0.3259183878544718, 0.34884907118976116, 0.34026198484934866, 0.335369034903124, 0.3414891269057989, 0.3401159290224314, 0.45895032305270433, 0.3236682510469109, 0.3451386208180338, 0.3256839872337878, 0.33063179114833474, 0.3241300666704774, 0.3197757541202009, 0.4648967438843101, 0.32754264306277037, 0.3108123999554664, 0.32533528516069055, 0.32904691505245864, 0.3322312608361244, 0.34560275077819824, 0.33118345914408565, 0.37047255714423954, 0.32328702276572585, 0.32922665728256106, 0.35539029887877405, 0.3305998067371547, 0.337412232067436, 0.31849664100445807, 0.3312781439162791, 0.43092935625463724, 0.3350587519817054, 0.3412393669132143, 0.3740996897686273, 0.3250424559228122, 0.32785213622264564, 0.4280907451175153, 0.31355041009373963, 0.3294883850030601, 0.32453757687471807, 0.3330074781551957, 0.32034206995740533, 0.3197296839207411, 0.33038677694275975, 0.41093761404044926, 0.322546245995909, 0.3093488379381597, 0.31129237613640726, 0.31190667604096234, 0.35603925096802413, 0.352588701993227, 0.36422947025857866, 0.3279174838680774, 0.32730799191631377, 0.31818685214966536, 3.937826187815517, 3.1202841757331043, 0.33372526802122593, 0.335335582960397, 0.3270320389419794, 0.33567935111932456, 0.34303002688102424, 0.345148844178766, 0.3329308128450066, 0.33956830902025104, 0.3332909890450537, 0.3405873221345246, 0.3392371437512338, 0.33759060874581337, 0.33548220503143966, 0.32190409884788096, 0.32928015873767436, 0.32920751301571727, 0.33494103723205626, 0.3359298149589449, 0.3301542710978538, 0.3372681029140949, 0.3283808759879321, 0.34032377414405346, 0.3346951387356967, 0.3336350880563259, 0.35341056692413986, 0.3317903862334788, 0.3324661087244749, 0.33781100693158805, 0.3325884803198278, 0.3385068660136312, 0.343330305069685, 0.42824174696579576, 0.32104819524101913, 0.3242114339955151, 0.3309284928254783, 0.32927454402670264, 0.32671669824048877, 0.46907930891029537, 0.33742127707228065, 0.3242719261907041, 0.3311026031151414, 0.32981622871011496, 0.33319767704233527, 0.3319203157443553, 0.4606559360399842, 0.32346115075051785, 0.32739882613532245, 0.329778108978644, 0.3356361670885235, 0.33467910112813115, 0.3366118159610778, 0.32286340091377497, 0.333051566965878, 0.31511502992361784, 0.32337409793399274, 0.31802066299133, 0.3243386228568852, 0.3077724128961563, 0.3228021669201553, 0.307150995824486, 0.3649574180599302, 0.32821720535866916, 0.3433292231056839, 0.3632208639755845, 0.3355456260032952, 0.3185409570578486, 0.329976768232882, 0.41832182719372213, 0.32917943596839905, 0.308956831227988, 0.3059870698489249, 0.31409879121929407, 0.31602881918661296, 0.3167128001805395, 0.38145820889621973, 0.3533392909448594, 0.33331097522750497, 0.3297991321887821, 0.3079606357496232, 0.32137357303872705, 0.30720309307798743]
Total Epoch List: [143, 179, 226]
Total Time List: [0.08611331391148269, 0.09115174692124128, 0.09131161007098854]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d407ce80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7219;  Loss pred: 0.7219; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7225 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7212 score: 0.5102 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.7141;  Loss pred: 0.7141; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7216 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7205 score: 0.5102 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7186 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7181 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7133 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7138 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7036;  Loss pred: 0.7036; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7061 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7081 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7014 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6666 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6775 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.23s
Val loss: 0.6551 score: 0.5714 time: 0.09s
Test loss: 0.6689 score: 0.5510 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.13s
Val loss: 0.6449 score: 0.7143 time: 0.09s
Test loss: 0.6612 score: 0.5918 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.13s
Val loss: 0.6349 score: 0.7755 time: 0.09s
Test loss: 0.6540 score: 0.6531 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 0.13s
Val loss: 0.6255 score: 0.8367 time: 0.09s
Test loss: 0.6471 score: 0.6939 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.13s
Val loss: 0.6163 score: 0.8571 time: 0.09s
Test loss: 0.6401 score: 0.7143 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.14s
Val loss: 0.6074 score: 0.8571 time: 0.09s
Test loss: 0.6334 score: 0.7551 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4900;  Loss pred: 0.4900; Loss self: 0.0000; time: 0.13s
Val loss: 0.5989 score: 0.8571 time: 0.09s
Test loss: 0.6268 score: 0.7347 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4726;  Loss pred: 0.4726; Loss self: 0.0000; time: 0.14s
Val loss: 0.5910 score: 0.8367 time: 0.20s
Test loss: 0.6209 score: 0.6939 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4473;  Loss pred: 0.4473; Loss self: 0.0000; time: 0.12s
Val loss: 0.5841 score: 0.7959 time: 0.08s
Test loss: 0.6158 score: 0.6531 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4291;  Loss pred: 0.4291; Loss self: 0.0000; time: 0.13s
Val loss: 0.5785 score: 0.7755 time: 0.09s
Test loss: 0.6117 score: 0.5714 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4208;  Loss pred: 0.4208; Loss self: 0.0000; time: 0.13s
Val loss: 0.5735 score: 0.7347 time: 0.09s
Test loss: 0.6081 score: 0.5714 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4168;  Loss pred: 0.4168; Loss self: 0.0000; time: 0.13s
Val loss: 0.5699 score: 0.7143 time: 0.09s
Test loss: 0.6053 score: 0.5714 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.3746;  Loss pred: 0.3746; Loss self: 0.0000; time: 0.14s
Val loss: 0.5672 score: 0.7143 time: 0.08s
Test loss: 0.6036 score: 0.5714 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3971;  Loss pred: 0.3971; Loss self: 0.0000; time: 0.13s
Val loss: 0.5660 score: 0.6939 time: 0.08s
Test loss: 0.6032 score: 0.5714 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.3604;  Loss pred: 0.3604; Loss self: 0.0000; time: 0.13s
Val loss: 0.5662 score: 0.6531 time: 0.08s
Test loss: 0.6038 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.14s
Val loss: 0.5669 score: 0.6531 time: 0.15s
Test loss: 0.6048 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3622;  Loss pred: 0.3622; Loss self: 0.0000; time: 0.12s
Val loss: 0.5667 score: 0.6531 time: 0.09s
Test loss: 0.6051 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.13s
Val loss: 0.5667 score: 0.6531 time: 0.09s
Test loss: 0.6056 score: 0.5714 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.3213;  Loss pred: 0.3213; Loss self: 0.0000; time: 0.14s
Val loss: 0.5649 score: 0.6531 time: 0.09s
Test loss: 0.6047 score: 0.5714 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3051;  Loss pred: 0.3051; Loss self: 0.0000; time: 0.14s
Val loss: 0.5620 score: 0.6531 time: 0.09s
Test loss: 0.6027 score: 0.5714 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.2955;  Loss pred: 0.2955; Loss self: 0.0000; time: 0.14s
Val loss: 0.5590 score: 0.6531 time: 0.09s
Test loss: 0.6007 score: 0.5918 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.13s
Val loss: 0.5539 score: 0.6531 time: 0.09s
Test loss: 0.5966 score: 0.6122 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.13s
Val loss: 0.5478 score: 0.6531 time: 0.09s
Test loss: 0.5913 score: 0.6122 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.2702;  Loss pred: 0.2702; Loss self: 0.0000; time: 0.13s
Val loss: 0.5419 score: 0.6531 time: 0.09s
Test loss: 0.5861 score: 0.6327 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 0.14s
Val loss: 0.5369 score: 0.6531 time: 0.09s
Test loss: 0.5814 score: 0.6327 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.2466;  Loss pred: 0.2466; Loss self: 0.0000; time: 0.13s
Val loss: 0.5323 score: 0.6939 time: 0.08s
Test loss: 0.5770 score: 0.6327 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.2544;  Loss pred: 0.2544; Loss self: 0.0000; time: 0.12s
Val loss: 0.5276 score: 0.6939 time: 0.08s
Test loss: 0.5724 score: 0.6327 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.13s
Val loss: 0.5246 score: 0.6939 time: 0.09s
Test loss: 0.5696 score: 0.6327 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2303;  Loss pred: 0.2303; Loss self: 0.0000; time: 0.12s
Val loss: 0.5206 score: 0.6939 time: 0.08s
Test loss: 0.5657 score: 0.6327 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2311;  Loss pred: 0.2311; Loss self: 0.0000; time: 0.13s
Val loss: 0.5190 score: 0.6939 time: 0.10s
Test loss: 0.5642 score: 0.6327 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 0.13s
Val loss: 0.5177 score: 0.6939 time: 0.08s
Test loss: 0.5629 score: 0.6327 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2098;  Loss pred: 0.2098; Loss self: 0.0000; time: 0.13s
Val loss: 0.5161 score: 0.6939 time: 0.21s
Test loss: 0.5615 score: 0.6327 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 0.12s
Val loss: 0.5144 score: 0.6939 time: 0.08s
Test loss: 0.5602 score: 0.6327 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.15s
Val loss: 0.5151 score: 0.6939 time: 0.09s
Test loss: 0.5612 score: 0.6327 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 0.12s
Val loss: 0.5157 score: 0.6939 time: 0.26s
Test loss: 0.5624 score: 0.6327 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.14s
Val loss: 0.5177 score: 0.6939 time: 0.10s
Test loss: 0.5653 score: 0.6327 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.13s
Val loss: 0.5177 score: 0.6939 time: 0.09s
Test loss: 0.5665 score: 0.6327 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1806;  Loss pred: 0.1806; Loss self: 0.0000; time: 0.13s
Val loss: 0.5180 score: 0.6939 time: 0.09s
Test loss: 0.5679 score: 0.6327 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1733;  Loss pred: 0.1733; Loss self: 0.0000; time: 0.14s
Val loss: 0.5162 score: 0.6939 time: 0.08s
Test loss: 0.5669 score: 0.6531 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1738;  Loss pred: 0.1738; Loss self: 0.0000; time: 0.13s
Val loss: 0.5138 score: 0.6939 time: 0.11s
Test loss: 0.5653 score: 0.6531 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.1663;  Loss pred: 0.1663; Loss self: 0.0000; time: 0.13s
Val loss: 0.5107 score: 0.6939 time: 0.09s
Test loss: 0.5630 score: 0.6531 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.13s
Val loss: 0.5050 score: 0.6939 time: 0.09s
Test loss: 0.5578 score: 0.6735 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.13s
Val loss: 0.4983 score: 0.6939 time: 0.09s
Test loss: 0.5517 score: 0.6735 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1554;  Loss pred: 0.1554; Loss self: 0.0000; time: 0.14s
Val loss: 0.4926 score: 0.6939 time: 0.10s
Test loss: 0.5464 score: 0.6735 time: 0.11s
Epoch 54/1000, LR 0.000269
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 0.15s
Val loss: 0.4880 score: 0.6939 time: 0.10s
Test loss: 0.5422 score: 0.6735 time: 0.10s
Epoch 55/1000, LR 0.000269
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.14s
Val loss: 0.4836 score: 0.6939 time: 0.09s
Test loss: 0.5384 score: 0.6939 time: 0.19s
Epoch 56/1000, LR 0.000269
Train loss: 0.1434;  Loss pred: 0.1434; Loss self: 0.0000; time: 0.13s
Val loss: 0.4783 score: 0.6939 time: 0.09s
Test loss: 0.5336 score: 0.6939 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1479;  Loss pred: 0.1479; Loss self: 0.0000; time: 0.13s
Val loss: 0.4734 score: 0.6939 time: 0.09s
Test loss: 0.5294 score: 0.6735 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.14s
Val loss: 0.4699 score: 0.7143 time: 0.10s
Test loss: 0.5268 score: 0.6735 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1293;  Loss pred: 0.1293; Loss self: 0.0000; time: 0.14s
Val loss: 0.4671 score: 0.7143 time: 0.10s
Test loss: 0.5250 score: 0.6735 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.1246;  Loss pred: 0.1246; Loss self: 0.0000; time: 0.13s
Val loss: 0.4641 score: 0.7347 time: 0.09s
Test loss: 0.5228 score: 0.6735 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1207;  Loss pred: 0.1207; Loss self: 0.0000; time: 0.13s
Val loss: 0.4614 score: 0.7551 time: 0.09s
Test loss: 0.5210 score: 0.6735 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.13s
Val loss: 0.4559 score: 0.7551 time: 0.09s
Test loss: 0.5166 score: 0.6735 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1183;  Loss pred: 0.1183; Loss self: 0.0000; time: 0.26s
Val loss: 0.4506 score: 0.7551 time: 0.09s
Test loss: 0.5124 score: 0.6735 time: 0.10s
Epoch 64/1000, LR 0.000268
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 0.13s
Val loss: 0.4467 score: 0.7551 time: 0.09s
Test loss: 0.5100 score: 0.6735 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.13s
Val loss: 0.4423 score: 0.7551 time: 0.08s
Test loss: 0.5068 score: 0.6735 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1133;  Loss pred: 0.1133; Loss self: 0.0000; time: 0.12s
Val loss: 0.4374 score: 0.7551 time: 0.09s
Test loss: 0.5026 score: 0.6735 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.14s
Val loss: 0.4332 score: 0.7551 time: 0.09s
Test loss: 0.4997 score: 0.6735 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1084;  Loss pred: 0.1084; Loss self: 0.0000; time: 0.14s
Val loss: 0.4275 score: 0.7551 time: 0.09s
Test loss: 0.4952 score: 0.6735 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1020;  Loss pred: 0.1020; Loss self: 0.0000; time: 0.13s
Val loss: 0.4216 score: 0.7755 time: 0.09s
Test loss: 0.4906 score: 0.7143 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.13s
Val loss: 0.4151 score: 0.7755 time: 0.09s
Test loss: 0.4852 score: 0.7143 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.30s
Val loss: 0.4090 score: 0.7959 time: 0.09s
Test loss: 0.4805 score: 0.7143 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.13s
Val loss: 0.4021 score: 0.7959 time: 0.09s
Test loss: 0.4747 score: 0.7143 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.14s
Val loss: 0.3943 score: 0.7959 time: 0.09s
Test loss: 0.4680 score: 0.7143 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.13s
Val loss: 0.3863 score: 0.8163 time: 0.09s
Test loss: 0.4609 score: 0.7551 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.14s
Val loss: 0.3786 score: 0.8163 time: 0.09s
Test loss: 0.4538 score: 0.7755 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.13s
Val loss: 0.3711 score: 0.8367 time: 0.09s
Test loss: 0.4469 score: 0.7755 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0920;  Loss pred: 0.0920; Loss self: 0.0000; time: 0.14s
Val loss: 0.3635 score: 0.8367 time: 0.09s
Test loss: 0.4404 score: 0.7755 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.13s
Val loss: 0.3562 score: 0.8571 time: 0.09s
Test loss: 0.4344 score: 0.7755 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.13s
Val loss: 0.3473 score: 0.8571 time: 0.09s
Test loss: 0.4270 score: 0.7755 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0922;  Loss pred: 0.0922; Loss self: 0.0000; time: 0.13s
Val loss: 0.3387 score: 0.8776 time: 0.09s
Test loss: 0.4198 score: 0.7755 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.13s
Val loss: 0.3302 score: 0.8776 time: 0.08s
Test loss: 0.4128 score: 0.8163 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.14s
Val loss: 0.3212 score: 0.8776 time: 0.09s
Test loss: 0.4055 score: 0.8163 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.14s
Val loss: 0.3138 score: 0.8776 time: 0.08s
Test loss: 0.3999 score: 0.8163 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.13s
Val loss: 0.3051 score: 0.8776 time: 0.08s
Test loss: 0.3932 score: 0.8163 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.14s
Val loss: 0.2968 score: 0.8776 time: 0.09s
Test loss: 0.3866 score: 0.8163 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0713;  Loss pred: 0.0713; Loss self: 0.0000; time: 0.14s
Val loss: 0.2892 score: 0.8776 time: 0.08s
Test loss: 0.3809 score: 0.8163 time: 0.10s
Epoch 87/1000, LR 0.000266
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 0.14s
Val loss: 0.2813 score: 0.8776 time: 0.09s
Test loss: 0.3748 score: 0.8163 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0791;  Loss pred: 0.0791; Loss self: 0.0000; time: 0.14s
Val loss: 0.2733 score: 0.8776 time: 0.08s
Test loss: 0.3688 score: 0.8367 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.13s
Val loss: 0.2661 score: 0.8776 time: 0.09s
Test loss: 0.3635 score: 0.8367 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.14s
Val loss: 0.2579 score: 0.8776 time: 0.08s
Test loss: 0.3576 score: 0.8367 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.13s
Val loss: 0.2497 score: 0.8776 time: 0.08s
Test loss: 0.3517 score: 0.8367 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0696;  Loss pred: 0.0696; Loss self: 0.0000; time: 0.13s
Val loss: 0.2417 score: 0.8776 time: 0.09s
Test loss: 0.3459 score: 0.8367 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.13s
Val loss: 0.2339 score: 0.8980 time: 0.09s
Test loss: 0.3402 score: 0.8571 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.14s
Val loss: 0.2257 score: 0.8980 time: 0.10s
Test loss: 0.3344 score: 0.8571 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0675;  Loss pred: 0.0675; Loss self: 0.0000; time: 0.14s
Val loss: 0.2189 score: 0.9184 time: 0.09s
Test loss: 0.3297 score: 0.8776 time: 0.10s
Epoch 96/1000, LR 0.000265
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.14s
Val loss: 0.2120 score: 0.9184 time: 0.09s
Test loss: 0.3248 score: 0.8776 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0645;  Loss pred: 0.0645; Loss self: 0.0000; time: 0.13s
Val loss: 0.2044 score: 0.9388 time: 0.09s
Test loss: 0.3193 score: 0.8776 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.14s
Val loss: 0.1975 score: 0.9388 time: 0.09s
Test loss: 0.3142 score: 0.8776 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.14s
Val loss: 0.1899 score: 0.9388 time: 0.11s
Test loss: 0.3088 score: 0.8776 time: 0.11s
Epoch 100/1000, LR 0.000265
Train loss: 0.0561;  Loss pred: 0.0561; Loss self: 0.0000; time: 0.13s
Val loss: 0.1830 score: 0.9592 time: 0.08s
Test loss: 0.3042 score: 0.8980 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.13s
Val loss: 0.1764 score: 0.9592 time: 0.09s
Test loss: 0.3000 score: 0.8980 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.13s
Val loss: 0.1702 score: 0.9592 time: 0.09s
Test loss: 0.2961 score: 0.8776 time: 0.26s
Epoch 103/1000, LR 0.000264
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.13s
Val loss: 0.1651 score: 0.9592 time: 0.09s
Test loss: 0.2930 score: 0.8776 time: 0.09s
Epoch 104/1000, LR 0.000264
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.14s
Val loss: 0.1598 score: 0.9592 time: 0.09s
Test loss: 0.2904 score: 0.8776 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.13s
Val loss: 0.1553 score: 0.9592 time: 0.09s
Test loss: 0.2885 score: 0.8776 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.13s
Val loss: 0.1521 score: 0.9592 time: 0.09s
Test loss: 0.2872 score: 0.8776 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.13s
Val loss: 0.1487 score: 0.9592 time: 0.09s
Test loss: 0.2858 score: 0.8776 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.14s
Val loss: 0.1447 score: 0.9592 time: 0.09s
Test loss: 0.2842 score: 0.8776 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.13s
Val loss: 0.1411 score: 0.9592 time: 0.09s
Test loss: 0.2827 score: 0.8776 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.13s
Val loss: 0.1381 score: 0.9796 time: 0.09s
Test loss: 0.2813 score: 0.8776 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.12s
Val loss: 0.1357 score: 0.9796 time: 0.09s
Test loss: 0.2803 score: 0.8776 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.13s
Val loss: 0.1327 score: 0.9796 time: 0.09s
Test loss: 0.2794 score: 0.8776 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.13s
Val loss: 0.1306 score: 0.9796 time: 0.09s
Test loss: 0.2789 score: 0.8776 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.13s
Val loss: 0.1286 score: 0.9796 time: 0.18s
Test loss: 0.2784 score: 0.8776 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.13s
Val loss: 0.1265 score: 0.9796 time: 0.09s
Test loss: 0.2777 score: 0.8776 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.12s
Val loss: 0.1247 score: 0.9796 time: 0.09s
Test loss: 0.2770 score: 0.8776 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0454;  Loss pred: 0.0454; Loss self: 0.0000; time: 0.12s
Val loss: 0.1227 score: 0.9796 time: 0.08s
Test loss: 0.2765 score: 0.8776 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.13s
Val loss: 0.1210 score: 0.9796 time: 0.09s
Test loss: 0.2760 score: 0.8776 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.13s
Val loss: 0.1193 score: 0.9796 time: 0.09s
Test loss: 0.2755 score: 0.8776 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.13s
Val loss: 0.1180 score: 0.9796 time: 0.09s
Test loss: 0.2752 score: 0.8776 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.0452;  Loss pred: 0.0452; Loss self: 0.0000; time: 0.14s
Val loss: 0.1165 score: 0.9796 time: 0.18s
Test loss: 0.2749 score: 0.8776 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.14s
Val loss: 0.1150 score: 0.9796 time: 0.09s
Test loss: 0.2746 score: 0.8776 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.13s
Val loss: 0.1135 score: 0.9796 time: 0.09s
Test loss: 0.2741 score: 0.8776 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.13s
Val loss: 0.1121 score: 0.9796 time: 0.09s
Test loss: 0.2741 score: 0.8776 time: 0.08s
Epoch 125/1000, LR 0.000261
Train loss: 0.0396;  Loss pred: 0.0396; Loss self: 0.0000; time: 0.13s
Val loss: 0.1111 score: 0.9796 time: 0.09s
Test loss: 0.2740 score: 0.8776 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.13s
Val loss: 0.1105 score: 0.9796 time: 0.09s
Test loss: 0.2744 score: 0.8776 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.13s
Val loss: 0.1098 score: 0.9796 time: 0.09s
Test loss: 0.2745 score: 0.8776 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.13s
Val loss: 0.1093 score: 0.9796 time: 0.09s
Test loss: 0.2744 score: 0.8776 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.13s
Val loss: 0.1084 score: 0.9796 time: 0.08s
Test loss: 0.2744 score: 0.8776 time: 0.08s
Epoch 130/1000, LR 0.000260
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.13s
Val loss: 0.1081 score: 0.9796 time: 0.09s
Test loss: 0.2742 score: 0.8776 time: 0.18s
Epoch 131/1000, LR 0.000260
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.13s
Val loss: 0.1072 score: 0.9796 time: 0.09s
Test loss: 0.2741 score: 0.8776 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.13s
Val loss: 0.1065 score: 0.9796 time: 0.09s
Test loss: 0.2740 score: 0.8776 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.13s
Val loss: 0.1057 score: 0.9796 time: 0.08s
Test loss: 0.2739 score: 0.8776 time: 0.08s
Epoch 134/1000, LR 0.000260
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.13s
Val loss: 0.1048 score: 0.9796 time: 0.08s
Test loss: 0.2739 score: 0.8776 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.13s
Val loss: 0.1045 score: 0.9796 time: 0.08s
Test loss: 0.2741 score: 0.8776 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.13s
Val loss: 0.1040 score: 0.9796 time: 0.08s
Test loss: 0.2738 score: 0.8776 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.13s
Val loss: 0.1037 score: 0.9796 time: 0.08s
Test loss: 0.2736 score: 0.8776 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.12s
Val loss: 0.1034 score: 0.9796 time: 0.10s
Test loss: 0.2733 score: 0.8776 time: 0.18s
Epoch 139/1000, LR 0.000259
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.12s
Val loss: 0.1032 score: 0.9796 time: 0.08s
Test loss: 0.2730 score: 0.8776 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.12s
Val loss: 0.1030 score: 0.9796 time: 0.08s
Test loss: 0.2728 score: 0.8776 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.13s
Val loss: 0.1025 score: 0.9796 time: 0.08s
Test loss: 0.2727 score: 0.8776 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.13s
Val loss: 0.1017 score: 0.9796 time: 0.08s
Test loss: 0.2727 score: 0.8776 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.12s
Val loss: 0.1014 score: 0.9796 time: 0.08s
Test loss: 0.2725 score: 0.8776 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.12s
Val loss: 0.1017 score: 0.9796 time: 0.08s
Test loss: 0.2724 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.13s
Val loss: 0.1019 score: 0.9796 time: 0.08s
Test loss: 0.2723 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.14s
Val loss: 0.1017 score: 0.9796 time: 2.60s
Test loss: 0.2728 score: 0.8776 time: 3.20s
     INFO: Early stopping counter 3 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 1.30s
Val loss: 0.1019 score: 0.9796 time: 0.09s
Test loss: 0.2728 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.13s
Val loss: 0.1020 score: 0.9796 time: 0.08s
Test loss: 0.2728 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0302;  Loss pred: 0.0302; Loss self: 0.0000; time: 0.14s
Val loss: 0.1017 score: 0.9796 time: 0.09s
Test loss: 0.2730 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.13s
Val loss: 0.1013 score: 0.9796 time: 0.08s
Test loss: 0.2730 score: 0.8776 time: 0.08s
Epoch 151/1000, LR 0.000257
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.14s
Val loss: 0.1009 score: 0.9796 time: 0.08s
Test loss: 0.2731 score: 0.8776 time: 0.08s
Epoch 152/1000, LR 0.000257
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.13s
Val loss: 0.1006 score: 0.9796 time: 0.08s
Test loss: 0.2729 score: 0.8776 time: 0.18s
Epoch 153/1000, LR 0.000257
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.0995 score: 0.9796 time: 0.09s
Test loss: 0.2734 score: 0.8776 time: 0.09s
Epoch 154/1000, LR 0.000256
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.13s
Val loss: 0.0987 score: 0.9796 time: 0.09s
Test loss: 0.2738 score: 0.8776 time: 0.08s
Epoch 155/1000, LR 0.000256
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.13s
Val loss: 0.0977 score: 0.9796 time: 0.08s
Test loss: 0.2742 score: 0.8776 time: 0.08s
Epoch 156/1000, LR 0.000256
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.13s
Val loss: 0.0966 score: 0.9796 time: 0.09s
Test loss: 0.2749 score: 0.8776 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.13s
Val loss: 0.0963 score: 0.9796 time: 0.09s
Test loss: 0.2751 score: 0.8776 time: 0.09s
Epoch 158/1000, LR 0.000256
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.13s
Val loss: 0.0964 score: 0.9796 time: 0.08s
Test loss: 0.2750 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.13s
Val loss: 0.0967 score: 0.9796 time: 0.09s
Test loss: 0.2748 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.13s
Val loss: 0.0966 score: 0.9796 time: 0.21s
Test loss: 0.2750 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.13s
Val loss: 0.0964 score: 0.9796 time: 0.08s
Test loss: 0.2752 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.13s
Val loss: 0.0962 score: 0.9796 time: 0.08s
Test loss: 0.2752 score: 0.8776 time: 0.08s
Epoch 163/1000, LR 0.000255
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.13s
Val loss: 0.0956 score: 0.9796 time: 0.09s
Test loss: 0.2755 score: 0.8776 time: 0.08s
Epoch 164/1000, LR 0.000254
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.13s
Val loss: 0.0952 score: 0.9796 time: 0.09s
Test loss: 0.2758 score: 0.8776 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.13s
Val loss: 0.0947 score: 0.9796 time: 0.09s
Test loss: 0.2762 score: 0.8776 time: 0.08s
Epoch 166/1000, LR 0.000254
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.13s
Val loss: 0.0944 score: 0.9796 time: 0.09s
Test loss: 0.2761 score: 0.8776 time: 0.08s
Epoch 167/1000, LR 0.000254
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.13s
Val loss: 0.0943 score: 0.9796 time: 0.08s
Test loss: 0.2767 score: 0.8776 time: 0.08s
Epoch 168/1000, LR 0.000254
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.13s
Val loss: 0.0943 score: 0.9796 time: 0.09s
Test loss: 0.2767 score: 0.8776 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.12s
Val loss: 0.0940 score: 0.9796 time: 0.08s
Test loss: 0.2766 score: 0.8776 time: 0.08s
Epoch 170/1000, LR 0.000253
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.12s
Val loss: 0.0936 score: 0.9796 time: 0.08s
Test loss: 0.2774 score: 0.8776 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.13s
Val loss: 0.0936 score: 0.9796 time: 0.08s
Test loss: 0.2778 score: 0.8776 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9796 time: 0.09s
Test loss: 0.2782 score: 0.8776 time: 0.09s
Epoch 173/1000, LR 0.000253
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.14s
Val loss: 0.0936 score: 0.9796 time: 0.08s
Test loss: 0.2781 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.14s
Val loss: 0.0938 score: 0.9796 time: 0.09s
Test loss: 0.2778 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.13s
Val loss: 0.0938 score: 0.9796 time: 0.09s
Test loss: 0.2775 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.13s
Val loss: 0.0934 score: 0.9796 time: 0.09s
Test loss: 0.2777 score: 0.8776 time: 0.22s
Epoch 177/1000, LR 0.000252
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.13s
Val loss: 0.0928 score: 0.9796 time: 0.08s
Test loss: 0.2783 score: 0.8776 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.13s
Val loss: 0.0926 score: 0.9796 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.08s
Epoch 179/1000, LR 0.000251
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.13s
Val loss: 0.0922 score: 0.9796 time: 0.08s
Test loss: 0.2789 score: 0.8776 time: 0.08s
Epoch 180/1000, LR 0.000251
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.13s
Val loss: 0.0916 score: 0.9796 time: 0.08s
Test loss: 0.2797 score: 0.8776 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.13s
Val loss: 0.0917 score: 0.9796 time: 0.08s
Test loss: 0.2801 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.13s
Val loss: 0.0917 score: 0.9796 time: 0.08s
Test loss: 0.2809 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 183/1000, LR 0.000250
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.13s
Val loss: 0.0916 score: 0.9796 time: 0.08s
Test loss: 0.2817 score: 0.8776 time: 0.08s
Epoch 184/1000, LR 0.000250
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.12s
Val loss: 0.0924 score: 0.9796 time: 0.09s
Test loss: 0.2815 score: 0.8776 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.13s
Val loss: 0.0933 score: 0.9796 time: 0.09s
Test loss: 0.2819 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9796 time: 0.08s
Test loss: 0.2823 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.13s
Val loss: 0.0934 score: 0.9796 time: 0.08s
Test loss: 0.2826 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 188/1000, LR 0.000249
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.13s
Val loss: 0.0924 score: 0.9796 time: 0.08s
Test loss: 0.2827 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 189/1000, LR 0.000249
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.13s
Val loss: 0.0910 score: 0.9796 time: 0.08s
Test loss: 0.2835 score: 0.8776 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.13s
Val loss: 0.0895 score: 0.9796 time: 0.09s
Test loss: 0.2847 score: 0.8776 time: 0.08s
Epoch 191/1000, LR 0.000249
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.14s
Val loss: 0.0891 score: 0.9796 time: 0.08s
Test loss: 0.2853 score: 0.8776 time: 0.08s
Epoch 192/1000, LR 0.000248
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.13s
Val loss: 0.0897 score: 0.9796 time: 0.08s
Test loss: 0.2849 score: 0.8776 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.12s
Val loss: 0.0912 score: 0.9796 time: 0.08s
Test loss: 0.2836 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.13s
Val loss: 0.0924 score: 0.9796 time: 0.08s
Test loss: 0.2828 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.13s
Val loss: 0.0936 score: 0.9796 time: 0.08s
Test loss: 0.2827 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.13s
Val loss: 0.0940 score: 0.9796 time: 0.08s
Test loss: 0.2828 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.12s
Val loss: 0.0939 score: 0.9796 time: 0.08s
Test loss: 0.2826 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.12s
Val loss: 0.0928 score: 0.9796 time: 0.08s
Test loss: 0.2825 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.12s
Val loss: 0.0919 score: 0.9796 time: 0.08s
Test loss: 0.2832 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.13s
Val loss: 0.0915 score: 0.9796 time: 0.08s
Test loss: 0.2832 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.26s
Val loss: 0.0918 score: 0.9796 time: 0.09s
Test loss: 0.2829 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 202/1000, LR 0.000246
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.13s
Val loss: 0.0924 score: 0.9796 time: 0.09s
Test loss: 0.2828 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9796 time: 0.09s
Test loss: 0.2824 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 204/1000, LR 0.000245
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.13s
Val loss: 0.0960 score: 0.9796 time: 0.09s
Test loss: 0.2814 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 205/1000, LR 0.000245
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.13s
Val loss: 0.0969 score: 0.9796 time: 0.09s
Test loss: 0.2816 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 206/1000, LR 0.000245
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.13s
Val loss: 0.0973 score: 0.9796 time: 0.09s
Test loss: 0.2822 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 207/1000, LR 0.000245
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.14s
Val loss: 0.0968 score: 0.9796 time: 0.09s
Test loss: 0.2821 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 208/1000, LR 0.000244
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.13s
Val loss: 0.0953 score: 0.9796 time: 0.09s
Test loss: 0.2817 score: 0.8776 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 209/1000, LR 0.000244
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.13s
Val loss: 0.0929 score: 0.9796 time: 0.09s
Test loss: 0.2823 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 210/1000, LR 0.000244
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.13s
Val loss: 0.0915 score: 0.9796 time: 0.09s
Test loss: 0.2830 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 211/1000, LR 0.000244
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.14s
Val loss: 0.0896 score: 0.9796 time: 0.09s
Test loss: 0.2843 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 190,   Train_Loss: 0.0194,   Val_Loss: 0.0891,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0891,   Test_Precision: 0.8800,   Test_Recall: 0.8800,   Test_accuracy: 0.8800,   Test_Score: 0.8776,   Test_loss: 0.2853


[0.2135941891465336, 0.09214408788830042, 0.09436085098423064, 0.08510692813433707, 0.08713073004037142, 0.08993108198046684, 0.08891682093963027, 0.08514036214910448, 0.10019451007246971, 0.09260286879725754, 0.09034177614375949, 0.09141899202950299, 0.09240424702875316, 0.09323805896565318, 0.08973067696206272, 0.08917326899245381, 0.08307387004606426, 0.08701081899926066, 0.08922558394260705, 0.0891643378417939, 0.09143853606656194, 0.09105851012282073, 0.08602680498734117, 0.08505977084860206, 0.08808791497722268, 0.08823519293218851, 0.09534473204985261, 0.09016354591585696, 0.09045739215798676, 0.09721409203484654, 0.09323020791634917, 0.2206021868623793, 0.10376631305553019, 0.09195093088783324, 0.08393859211355448, 0.08649680204689503, 0.08701924397610128, 0.0872768119443208, 0.08916755695827305, 0.08481184486299753, 0.09709411906078458, 0.08507582382299006, 0.09009638987481594, 0.1008870939258486, 0.11207112902775407, 0.10149517795071006, 0.23612062889151275, 0.08516365592367947, 0.09975953120738268, 0.08872788702137768, 0.09022864792495966, 0.09167136810719967, 0.11678183008916676, 0.10215716692619026, 0.1956126489676535, 0.09068594500422478, 0.09095925092697144, 0.09103723894804716, 0.10353299486450851, 0.08945778594352305, 0.08758439496159554, 0.09074271004647017, 0.10078648501075804, 0.09001925401389599, 0.09031062992289662, 0.09114779299125075, 0.090903433971107, 0.09180487296544015, 0.09078593202866614, 0.09178500296548009, 0.0924446601420641, 0.0910842688754201, 0.09738172707147896, 0.09217219310812652, 0.09260898199863732, 0.0920626800507307, 0.09136405703611672, 0.08876906102523208, 0.08858553995378315, 0.09145450894720852, 0.08971925196237862, 0.08866410097107291, 0.08697885903529823, 0.0879341580439359, 0.08845911920070648, 0.10398306418210268, 0.0893523448612541, 0.0877744008321315, 0.0874137501232326, 0.08899673284031451, 0.08999751694500446, 0.09410237101837993, 0.09207112505100667, 0.09194637089967728, 0.10971475997939706, 0.09203088516369462, 0.09201181400567293, 0.0921189240179956, 0.11399492505006492, 0.08591836201958358, 0.09165044198743999, 0.2642267090268433, 0.09379065223038197, 0.09035956207662821, 0.09125912399031222, 0.0891556041315198, 0.08996579213999212, 0.0901355859823525, 0.09106242912821472, 0.08906117407605052, 0.088781813159585, 0.09045314299874008, 0.09062883094884455, 0.08836075803264976, 0.08905764808878303, 0.08740788092836738, 0.08622875111177564, 0.08872182085178792, 0.08823371818289161, 0.08866132702678442, 0.0859353260602802, 0.0886246629524976, 0.09120388492010534, 0.08848224091343582, 0.0914007150568068, 0.09038051404058933, 0.09138604300096631, 0.08963072882033885, 0.0835422680247575, 0.1897975387983024, 0.08780545298941433, 0.08801006805151701, 0.08330072509124875, 0.08582389098592103, 0.08371770894154906, 0.08453939110040665, 0.08301667310297489, 0.18688556784763932, 0.08322318200953305, 0.08398510701954365, 0.08362322999164462, 0.08506592595949769, 0.08471717895008624, 0.08691698010079563, 0.0867964739445597, 3.2059317571111023, 0.09026755602099001, 0.08513317885808647, 0.08699018601328135, 0.08692675898782909, 0.08328140783123672, 0.1897415709681809, 0.09188266913406551, 0.08727464801631868, 0.0871808270458132, 0.08912333589978516, 0.09186836797744036, 0.08876875904388726, 0.08893432910554111, 0.08621299685910344, 0.08676721900701523, 0.0867406609468162, 0.08964907005429268, 0.08806596789509058, 0.08704129699617624, 0.08990840404294431, 0.0867208659183234, 0.21097653498873115, 0.08636145712807775, 0.08248711004853249, 0.08377858786843717, 0.09522142400965095, 0.08591939997859299, 0.09091826295480132, 0.08961707609705627, 0.2200973650906235, 0.08461625105701387, 0.0825939429923892, 0.08336602291092277, 0.08450167998671532, 0.08508054492995143, 0.08601560909301043, 0.08459465391933918, 0.1921884089242667, 0.08631638600490987, 0.08467529504559934, 0.08398154890164733, 0.08981377794407308, 0.0842126349452883, 0.08757091197185218, 0.08485263283364475, 0.22260588803328574, 0.0815732148475945, 0.08281856402754784, 0.08433097298257053, 0.0847642500884831, 0.08391227503307164, 0.08807893702760339, 0.08547589997760952, 0.08495455491356552, 0.09118189802393317, 0.08972287294454873, 0.08865553583018482, 0.09099534107372165, 0.08927050489000976, 0.0909909950569272, 0.08949862094596028, 0.21659167297184467, 0.08811767981387675, 0.09033615910448134, 0.09180692699737847]
[0.0043590650846231345, 0.0018804915895571513, 0.0019257316527394007, 0.001736876084374226, 0.0017781781640892125, 0.0018353282036829967, 0.0018146289987679646, 0.0017375584112062138, 0.0020447859198463206, 0.001889854465250154, 0.0018437097172195815, 0.0018656937148878161, 0.0018858009597704727, 0.0019028175299112893, 0.0018312383053482187, 0.0018198626324990575, 0.0016953851029809033, 0.0017757309999849116, 0.0018209302845430008, 0.0018196803641182427, 0.0018660925727869784, 0.001858336941282056, 0.0017556490813743096, 0.0017359136907877971, 0.0017977125505555648, 0.001800718223105888, 0.0019458108581602573, 0.0018400723656297338, 0.0018460692277140155, 0.001983961061935644, 0.0019026573044152893, 0.004502085446171007, 0.0021176798582761263, 0.0018765496099557803, 0.0017130324921133568, 0.0017652408580998986, 0.0017759029382877812, 0.0017811594274351184, 0.0018197460603729195, 0.001730853976795868, 0.001981512633893563, 0.0017362413025100011, 0.001838701834179917, 0.002058920284200992, 0.002287165898525593, 0.002071330162259389, 0.00481878834472475, 0.0017380337943608056, 0.002035908800150667, 0.0018107732045179118, 0.0018414009780604013, 0.0018708442470857076, 0.0023833026548809545, 0.0020848401413508214, 0.003992094876890888, 0.0018507335715147915, 0.0018563112434075804, 0.0018579028356744318, 0.00211291826254099, 0.0018256691008882255, 0.0017874366318692966, 0.0018518920417646973, 0.0020568670410358782, 0.0018371276329366528, 0.0018430740800591148, 0.0018601590406377704, 0.0018551721218593267, 0.0018735688360293908, 0.0018527741230340029, 0.0018731633258261243, 0.0018866257171849816, 0.001858862630110614, 0.0019873821851322236, 0.0018810651654719698, 0.001889979224461986, 0.001878830205116953, 0.0018645725925738107, 0.0018116134903108586, 0.0018078681623221052, 0.0018664185499430311, 0.0018310051420893595, 0.001809471448389243, 0.0017750787558224127, 0.0017945746539578754, 0.0018052881469531935, 0.0021221033506551566, 0.00182351724206641, 0.0017913143026965614, 0.0017839540841476042, 0.0018162598538839696, 0.0018366840192858055, 0.0019204565513955088, 0.0018790025520613607, 0.0018764565489730056, 0.0022390767342734094, 0.0018781813298713189, 0.0018777921225647538, 0.0018799780411835835, 0.0023264270418380598, 0.0017534359595833384, 0.0018704171834171427, 0.005392381816874354, 0.001914094943477183, 0.001844072695441392, 0.0018624311018431065, 0.001819502125133057, 0.0018360365742855534, 0.0018395017547418876, 0.0018584169209839739, 0.0018175749811438881, 0.0018118737379507143, 0.0018459825101783689, 0.001849567978547848, 0.0018032807761765256, 0.0018175030222200618, 0.0017838343046605587, 0.001759770430852564, 0.0018106494051385292, 0.0018006881261814613, 0.0018094148372813147, 0.0017537821644955144, 0.001808666590867298, 0.0018613037738797007, 0.0018057600186415473, 0.0018653207154450367, 0.0018445002865426394, 0.0018650212857340062, 0.001829198547353854, 0.0017049442454032144, 0.003873419159149029, 0.0017919480201921292, 0.0017961238377860614, 0.0017000147977805867, 0.0017515079793045108, 0.0017085246722765115, 0.0017252936959266663, 0.001694217818428059, 0.0038139911805640677, 0.0016984322859088378, 0.001713981775909054, 0.001706596530441727, 0.0017360393052958713, 0.001728922019389515, 0.0017738159204244005, 0.0017713566111134632, 0.06542717871655311, 0.0018421950208365309, 0.001737411813430336, 0.0017753099186383948, 0.0017740154895475324, 0.001699620567984423, 0.003872276958534304, 0.0018751565129401125, 0.0017811152656391567, 0.0017792005519553715, 0.0018188435897915339, 0.0018748646526008236, 0.0018116073274262706, 0.001814986308276349, 0.001759448915491907, 0.0017707595715717394, 0.001770217570343188, 0.001829572858250871, 0.001797264650920216, 0.0017763529999219642, 0.0018348653886315165, 0.0017698135901698653, 0.004305643571198595, 0.0017624787168995459, 0.0016834104091537241, 0.0017097670993558606, 0.001943294367543897, 0.0017534571424202652, 0.001855474754179619, 0.001828919920348087, 0.004491782961033133, 0.0017268622664696708, 0.0016855906733140654, 0.0017013474063453625, 0.0017245240813615371, 0.0017363376516316617, 0.0017554205937349067, 0.0017264215085579424, 0.003922212427025851, 0.0017615588980593852, 0.001728067245828558, 0.0017139091612581089, 0.0018329342437565935, 0.0017186252029650674, 0.0017871614688133098, 0.001731686384360097, 0.00454297730680175, 0.001664759486685602, 0.001690174776072405, 0.0017210402649504189, 0.0017298826548670018, 0.0017124954088381967, 0.0017975293270939467, 0.0017444061219920308, 0.0017337664268074595, 0.001860855061712922, 0.0018310790396846679, 0.0018092966495956086, 0.0018570477770147274, 0.0018218470385716278, 0.0018569590827944328, 0.0018265024682849037, 0.004420238223915197, 0.0017983199962015664, 0.0018435950837649253, 0.0018736107550485401]
[229.40698993634217, 531.7758428451659, 519.2831506806649, 575.746312011825, 562.3733437938164, 544.861675417659, 551.0768320571016, 575.5202205293343, 489.04875092017295, 529.1412743084602, 542.3847315335826, 535.9936585626167, 530.2786568322213, 525.5364659409148, 546.0785726682608, 549.4920232670462, 589.8364909788074, 563.1483597507151, 549.1698438367014, 549.5470631648911, 535.8790954869493, 538.1155471784928, 569.5899087175252, 576.0655067742322, 556.2624568043207, 555.333970172854, 513.9245655898379, 543.4568871739817, 541.6914950899746, 504.042150416175, 525.580722119222, 222.11928493060773, 472.21490826004214, 532.8929193742788, 583.7600889673182, 566.4949320719882, 563.0938371914289, 561.4320563319853, 549.5272234825285, 577.7494886375023, 504.6649629657194, 575.9568088573562, 543.8619690320871, 485.69146055505075, 437.22232857906965, 482.7815566153919, 207.52104646694545, 575.3628055130935, 491.1811373505509, 552.2502749129388, 543.0647707450051, 534.5180399478695, 419.585820521796, 479.6530823471551, 250.4950485492512, 540.32628758202, 538.702765256287, 538.2412797906048, 473.27907459960267, 547.7443856137344, 559.4603927044969, 539.9882808757492, 486.1762963037127, 544.3279944581192, 542.5717885240543, 537.5884417157911, 539.0335420725062, 533.7407309353393, 539.7311995929941, 533.8562773531605, 530.0468401820006, 537.9633673847613, 503.1744812251441, 531.6136933241727, 529.1063452216871, 532.2460737944929, 536.3159385602812, 551.9941231108893, 553.1376794177049, 535.785502148231, 546.1481112275305, 552.6475705876326, 563.3552859105056, 557.2351073801989, 553.9281923983781, 471.2305834168115, 548.3907565726112, 558.2493248084082, 560.5525438609103, 550.5820094308401, 544.4594658088494, 520.7095152834077, 532.1972548163649, 532.9193476647808, 446.6126527479214, 532.4299544967331, 532.5403104972904, 531.9211065733656, 429.84369680036116, 570.308823960486, 534.6400839694269, 185.446808842561, 522.4401241995761, 542.2779711841256, 536.9326140496559, 549.6008969634327, 544.6514595653545, 543.6254667451059, 538.0923885855125, 550.1836294922213, 551.9148376922948, 541.716941783687, 540.6667998140465, 554.5448125500943, 550.2054124666654, 560.5901833972677, 568.255939790695, 552.2880338745049, 555.3432520936313, 552.6648612556543, 570.1962422953799, 552.8934990281855, 537.2578157490115, 553.7834428033735, 536.1008387029119, 542.1522605856657, 536.1869098488254, 546.6875104655069, 586.529443819732, 258.1698388200504, 558.05190146798, 556.7544837179046, 588.2301738229137, 570.9365939612106, 585.3002981031332, 579.6114611448189, 590.2428773460941, 262.19253077876954, 588.7782564524767, 583.4367751486882, 585.9615803514877, 576.0238244315391, 578.3950859467343, 563.7563562744107, 564.5390621662606, 15.284168133433507, 542.8306931075655, 575.568781258374, 563.2819315102839, 563.6929361056779, 588.3666147826737, 258.245990849402, 533.288817812904, 561.4459767381467, 562.0501853492475, 549.7998869240948, 533.3718349283474, 551.9960009328779, 550.9683436398356, 568.3597808353645, 564.7294054225513, 564.9023129999396, 546.5756640902682, 556.4010839961666, 562.9511702031806, 544.999107943184, 565.0312584072884, 232.25331671418922, 567.3827379652812, 594.0322066219818, 584.874981146111, 514.5900779118124, 570.3019342803658, 538.9456244270706, 546.7707956342211, 222.62874423700896, 579.0849793969734, 593.2638426587191, 587.7694327862669, 579.8701281170196, 575.924849098495, 569.6640472197936, 579.2328206309751, 254.95814380412932, 567.6790035812293, 578.6811840881395, 583.4614941120578, 545.5733087022821, 581.8604302291999, 559.5465308817386, 577.4717691561258, 220.11996373012894, 600.687371357719, 591.6547886981135, 581.0439304444797, 578.0738925767671, 583.9431713737715, 556.3191570380068, 573.2610012042645, 576.7789619974302, 537.3873659346136, 546.1260701079354, 552.7009626771306, 538.489107484104, 548.8935013907777, 538.5148274215911, 547.4944695470385, 226.23215069939295, 556.0745596513481, 542.4184566373626, 533.7287893472263]
Elapsed: 0.11201760542283268~0.21564928182127452
Time per graph: 0.00228607358005781~0.004401005751454581
Speed: 526.0820437108887~88.40035430455242
Total Time: 0.0926
best val loss: 0.08906220644712448 test_score: 0.8776

Testing...
Test loss: 0.2813 score: 0.8776 time: 0.09s
test Score 0.8776
Epoch Time List: [0.4336077037733048, 0.2944593238644302, 0.31582182995043695, 0.29263413907028735, 0.30103284190408885, 0.2895073061808944, 0.30209682881832123, 0.2975220929365605, 0.30395884974859655, 0.4059344017878175, 0.30219995020888746, 0.2989963530562818, 0.305264457128942, 0.31163162691518664, 0.309203227981925, 0.30081028677523136, 0.4127152687869966, 0.28759340615943074, 0.30536343017593026, 0.2976354949641973, 0.30571971694007516, 0.3065792159177363, 0.2968894769437611, 0.2922814358025789, 0.3674967128317803, 0.29558586422353983, 0.309013371123001, 0.31331815803423524, 0.31581354117952287, 0.32013092702254653, 0.3121164459735155, 0.43614139989949763, 0.3086449848487973, 0.3113256108481437, 0.2861402190756053, 0.28759698709473014, 0.30035373801365495, 0.2863979700487107, 0.31223518773913383, 0.2912184961605817, 0.43546973215416074, 0.28766257618553936, 0.3207822951953858, 0.4793594880029559, 0.340936197899282, 0.313765455968678, 0.44515777681954205, 0.29499886301346123, 0.329655644018203, 0.3006967888213694, 0.3048808048479259, 0.3092139249201864, 0.34837648808024824, 0.34121672296896577, 0.42159207770600915, 0.30647029005922377, 0.30552181880921125, 0.3168223579414189, 0.33544458425603807, 0.3045391689520329, 0.29862224077805877, 0.29999327822588384, 0.44581230194307864, 0.3062024973332882, 0.3014312069863081, 0.2955679530277848, 0.3131753809284419, 0.31034270487725735, 0.3057723480742425, 0.30282597709447145, 0.4822118189185858, 0.30392721644602716, 0.3146809369791299, 0.3114135756623, 0.3151313029229641, 0.3095252166967839, 0.31006460706703365, 0.31298755086027086, 0.30339726619422436, 0.3059654908720404, 0.30288409208878875, 0.30900910403579473, 0.30263738869689405, 0.3004622326698154, 0.30697516398504376, 0.31841497984714806, 0.30720837297849357, 0.309722047066316, 0.29747380083426833, 0.30473236995749176, 0.2986379947979003, 0.3132455302402377, 0.308694401755929, 0.3266336170490831, 0.331674826098606, 0.3160508219152689, 0.3096298899035901, 0.3109611268155277, 0.3515328059438616, 0.2908832929097116, 0.3008618119638413, 0.47567184595391154, 0.30562205286696553, 0.30920709925703704, 0.30616347794421017, 0.3049945109523833, 0.3067305281292647, 0.30932541005313396, 0.30358776124194264, 0.2958197249099612, 0.29385331901721656, 0.30290292389690876, 0.30834223609417677, 0.3928548078984022, 0.29641120904125273, 0.28972971998155117, 0.286998167168349, 0.2990966278593987, 0.29989532893523574, 0.2970852069556713, 0.3928489270620048, 0.31042591924779117, 0.301480463007465, 0.3041665852069855, 0.309337645303458, 0.2998354348819703, 0.3004754660651088, 0.30107871000654995, 0.2929465267807245, 0.40037986915558577, 0.30194369913078845, 0.29752740799449384, 0.2864937838166952, 0.2930958040524274, 0.28986814827658236, 0.2925203659106046, 0.2850896348245442, 0.39646251290105283, 0.27952436497434974, 0.27956098364666104, 0.2856336042750627, 0.2886789399199188, 0.279861006885767, 0.2839453858323395, 0.288852317025885, 5.943802120164037, 1.479110548272729, 0.2970658040139824, 0.30998126207850873, 0.29425204591825604, 0.2980236259754747, 0.39372654631733894, 0.31885244976729155, 0.3002551980316639, 0.297075220849365, 0.30359467188827693, 0.30159143591299653, 0.29502607230097055, 0.3037884358782321, 0.42628949298523366, 0.2947206380777061, 0.2966636240016669, 0.30107450392097235, 0.2954095769673586, 0.30124051100574434, 0.2981315420474857, 0.2971778579521924, 0.4212320151273161, 0.285719740903005, 0.2832523500546813, 0.28581255418248475, 0.30714985099621117, 0.3048179259058088, 0.3138995640911162, 0.2973983231931925, 0.43346114712767303, 0.2884948200080544, 0.2867229809053242, 0.28955725324340165, 0.290547801181674, 0.2915098911616951, 0.2988624048884958, 0.29213425097987056, 0.40172881493344903, 0.31024895003065467, 0.2921906551346183, 0.2888672580011189, 0.2975449289660901, 0.2907384599093348, 0.29889760608784854, 0.29757329425774515, 0.43216754612512887, 0.27986040501855314, 0.2836647860240191, 0.28686855896376073, 0.29610991897061467, 0.28361411788500845, 0.2865388428326696, 0.28440860169939697, 0.29302490898407996, 0.4327394438441843, 0.297303105937317, 0.30100032687187195, 0.3052109368145466, 0.3051949969958514, 0.29977859696373343, 0.31443379214033484, 0.429582686861977, 0.29909132700413465, 0.3005962206516415, 0.3103500222787261]
Total Epoch List: [211]
Total Time List: [0.09264870616607368]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d4248940>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7083;  Loss pred: 0.7083; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4898 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5102 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.15s
Val loss: 0.6764 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.15s
Val loss: 0.6698 score: 0.6939 time: 0.09s
Test loss: 0.6741 score: 0.5714 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.15s
Val loss: 0.6632 score: 0.8367 time: 0.08s
Test loss: 0.6673 score: 0.8367 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.15s
Val loss: 0.6571 score: 0.7755 time: 0.08s
Test loss: 0.6612 score: 0.7551 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 0.15s
Val loss: 0.6527 score: 0.6122 time: 0.08s
Test loss: 0.6566 score: 0.5714 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.15s
Val loss: 0.6503 score: 0.5714 time: 0.08s
Test loss: 0.6541 score: 0.5510 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.16s
Val loss: 0.6493 score: 0.5510 time: 0.14s
Test loss: 0.6532 score: 0.5510 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.15s
Val loss: 0.6495 score: 0.5510 time: 0.08s
Test loss: 0.6540 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.15s
Val loss: 0.6504 score: 0.5510 time: 0.08s
Test loss: 0.6555 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.15s
Val loss: 0.6511 score: 0.5306 time: 0.10s
Test loss: 0.6566 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5013;  Loss pred: 0.5013; Loss self: 0.0000; time: 0.15s
Val loss: 0.6514 score: 0.5510 time: 0.08s
Test loss: 0.6575 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4782;  Loss pred: 0.4782; Loss self: 0.0000; time: 0.15s
Val loss: 0.6507 score: 0.5510 time: 0.09s
Test loss: 0.6574 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.15s
Val loss: 0.6491 score: 0.5510 time: 0.08s
Test loss: 0.6566 score: 0.5306 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4626;  Loss pred: 0.4626; Loss self: 0.0000; time: 0.15s
Val loss: 0.6462 score: 0.5510 time: 0.08s
Test loss: 0.6541 score: 0.5306 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4554;  Loss pred: 0.4554; Loss self: 0.0000; time: 0.29s
Val loss: 0.6425 score: 0.5510 time: 0.08s
Test loss: 0.6509 score: 0.5306 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4284;  Loss pred: 0.4284; Loss self: 0.0000; time: 0.14s
Val loss: 0.6371 score: 0.5510 time: 0.08s
Test loss: 0.6463 score: 0.5306 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4270;  Loss pred: 0.4270; Loss self: 0.0000; time: 0.15s
Val loss: 0.6310 score: 0.5510 time: 0.08s
Test loss: 0.6413 score: 0.5306 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4125;  Loss pred: 0.4125; Loss self: 0.0000; time: 0.14s
Val loss: 0.6237 score: 0.5510 time: 0.08s
Test loss: 0.6351 score: 0.5510 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.3988;  Loss pred: 0.3988; Loss self: 0.0000; time: 0.15s
Val loss: 0.6163 score: 0.5510 time: 0.08s
Test loss: 0.6287 score: 0.5510 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.15s
Val loss: 0.6082 score: 0.5714 time: 0.08s
Test loss: 0.6217 score: 0.5510 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3624;  Loss pred: 0.3624; Loss self: 0.0000; time: 0.15s
Val loss: 0.5993 score: 0.5918 time: 0.08s
Test loss: 0.6137 score: 0.5510 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3800;  Loss pred: 0.3800; Loss self: 0.0000; time: 0.15s
Val loss: 0.5899 score: 0.6327 time: 0.08s
Test loss: 0.6047 score: 0.5510 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3667;  Loss pred: 0.3667; Loss self: 0.0000; time: 0.15s
Val loss: 0.5811 score: 0.6327 time: 0.08s
Test loss: 0.5960 score: 0.5510 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3652;  Loss pred: 0.3652; Loss self: 0.0000; time: 0.15s
Val loss: 0.5727 score: 0.6327 time: 0.08s
Test loss: 0.5877 score: 0.5714 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.15s
Val loss: 0.5654 score: 0.6327 time: 0.08s
Test loss: 0.5801 score: 0.6122 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3374;  Loss pred: 0.3374; Loss self: 0.0000; time: 0.15s
Val loss: 0.5577 score: 0.6327 time: 0.08s
Test loss: 0.5721 score: 0.6122 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3326;  Loss pred: 0.3326; Loss self: 0.0000; time: 0.15s
Val loss: 0.5519 score: 0.6531 time: 0.08s
Test loss: 0.5662 score: 0.6122 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3321;  Loss pred: 0.3321; Loss self: 0.0000; time: 0.15s
Val loss: 0.5476 score: 0.6735 time: 0.08s
Test loss: 0.5623 score: 0.6122 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3118;  Loss pred: 0.3118; Loss self: 0.0000; time: 0.15s
Val loss: 0.5428 score: 0.6939 time: 0.08s
Test loss: 0.5578 score: 0.6122 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.2995;  Loss pred: 0.2995; Loss self: 0.0000; time: 0.16s
Val loss: 0.5382 score: 0.7143 time: 0.09s
Test loss: 0.5538 score: 0.6122 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3062;  Loss pred: 0.3062; Loss self: 0.0000; time: 0.14s
Val loss: 0.5345 score: 0.7143 time: 0.10s
Test loss: 0.5515 score: 0.6122 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.3106;  Loss pred: 0.3106; Loss self: 0.0000; time: 0.15s
Val loss: 0.5321 score: 0.7143 time: 0.08s
Test loss: 0.5505 score: 0.6122 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.14s
Val loss: 0.5302 score: 0.7143 time: 0.08s
Test loss: 0.5501 score: 0.6122 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2784;  Loss pred: 0.2784; Loss self: 0.0000; time: 0.14s
Val loss: 0.5274 score: 0.7143 time: 0.08s
Test loss: 0.5490 score: 0.6122 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.14s
Val loss: 0.5252 score: 0.7347 time: 0.08s
Test loss: 0.5482 score: 0.6122 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2693;  Loss pred: 0.2693; Loss self: 0.0000; time: 0.15s
Val loss: 0.5222 score: 0.7347 time: 0.08s
Test loss: 0.5464 score: 0.6327 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2679;  Loss pred: 0.2679; Loss self: 0.0000; time: 0.14s
Val loss: 0.5185 score: 0.7347 time: 0.08s
Test loss: 0.5438 score: 0.6327 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 0.15s
Val loss: 0.5149 score: 0.7347 time: 0.08s
Test loss: 0.5416 score: 0.6327 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.15s
Val loss: 0.5122 score: 0.7551 time: 0.08s
Test loss: 0.5396 score: 0.6327 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 0.15s
Val loss: 0.5111 score: 0.7551 time: 0.08s
Test loss: 0.5390 score: 0.6327 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2354;  Loss pred: 0.2354; Loss self: 0.0000; time: 0.14s
Val loss: 0.5118 score: 0.7551 time: 0.08s
Test loss: 0.5404 score: 0.6327 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.2384;  Loss pred: 0.2384; Loss self: 0.0000; time: 0.15s
Val loss: 0.5125 score: 0.7347 time: 0.08s
Test loss: 0.5414 score: 0.6327 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.15s
Val loss: 0.5127 score: 0.7347 time: 0.08s
Test loss: 0.5418 score: 0.6327 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.2332;  Loss pred: 0.2332; Loss self: 0.0000; time: 0.15s
Val loss: 0.5104 score: 0.7347 time: 0.08s
Test loss: 0.5394 score: 0.6327 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.14s
Val loss: 0.5074 score: 0.7551 time: 0.08s
Test loss: 0.5362 score: 0.6327 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2120;  Loss pred: 0.2120; Loss self: 0.0000; time: 0.14s
Val loss: 0.5043 score: 0.7551 time: 0.08s
Test loss: 0.5333 score: 0.6327 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2201;  Loss pred: 0.2201; Loss self: 0.0000; time: 0.14s
Val loss: 0.5017 score: 0.7551 time: 0.08s
Test loss: 0.5315 score: 0.6327 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.15s
Val loss: 0.4997 score: 0.7551 time: 0.08s
Test loss: 0.5307 score: 0.6327 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.15s
Val loss: 0.4965 score: 0.7551 time: 0.09s
Test loss: 0.5289 score: 0.6327 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1987;  Loss pred: 0.1987; Loss self: 0.0000; time: 0.15s
Val loss: 0.4937 score: 0.7551 time: 0.09s
Test loss: 0.5272 score: 0.6531 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1890;  Loss pred: 0.1890; Loss self: 0.0000; time: 0.14s
Val loss: 0.4902 score: 0.7551 time: 0.08s
Test loss: 0.5245 score: 0.6531 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1890;  Loss pred: 0.1890; Loss self: 0.0000; time: 0.15s
Val loss: 0.4854 score: 0.7551 time: 0.08s
Test loss: 0.5200 score: 0.6531 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 0.15s
Val loss: 0.4829 score: 0.7551 time: 0.08s
Test loss: 0.5184 score: 0.6531 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.2110;  Loss pred: 0.2110; Loss self: 0.0000; time: 0.14s
Val loss: 0.4802 score: 0.7347 time: 0.09s
Test loss: 0.5163 score: 0.6531 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.15s
Val loss: 0.4775 score: 0.7347 time: 0.08s
Test loss: 0.5142 score: 0.6531 time: 0.10s
Epoch 61/1000, LR 0.000268
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.16s
Val loss: 0.4741 score: 0.7347 time: 0.08s
Test loss: 0.5112 score: 0.6531 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.14s
Val loss: 0.4696 score: 0.7347 time: 0.08s
Test loss: 0.5064 score: 0.6531 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.14s
Val loss: 0.4647 score: 0.7347 time: 0.08s
Test loss: 0.5011 score: 0.6531 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1737;  Loss pred: 0.1737; Loss self: 0.0000; time: 0.14s
Val loss: 0.4586 score: 0.7551 time: 0.09s
Test loss: 0.4946 score: 0.6531 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.15s
Val loss: 0.4543 score: 0.7551 time: 0.08s
Test loss: 0.4901 score: 0.6735 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.15s
Val loss: 0.4514 score: 0.7551 time: 0.08s
Test loss: 0.4871 score: 0.6735 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.15s
Val loss: 0.4478 score: 0.7551 time: 0.08s
Test loss: 0.4836 score: 0.6735 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1636;  Loss pred: 0.1636; Loss self: 0.0000; time: 0.14s
Val loss: 0.4457 score: 0.7551 time: 0.08s
Test loss: 0.4810 score: 0.6939 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.15s
Val loss: 0.4436 score: 0.7551 time: 0.08s
Test loss: 0.4784 score: 0.6939 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.16s
Val loss: 0.4403 score: 0.7755 time: 0.08s
Test loss: 0.4738 score: 0.6939 time: 0.10s
Epoch 71/1000, LR 0.000268
Train loss: 0.1535;  Loss pred: 0.1535; Loss self: 0.0000; time: 0.16s
Val loss: 0.4353 score: 0.7755 time: 0.08s
Test loss: 0.4672 score: 0.6939 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1511;  Loss pred: 0.1511; Loss self: 0.0000; time: 0.17s
Val loss: 0.4330 score: 0.7755 time: 0.12s
Test loss: 0.4638 score: 0.6939 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1360;  Loss pred: 0.1360; Loss self: 0.0000; time: 0.14s
Val loss: 0.4287 score: 0.7959 time: 0.08s
Test loss: 0.4582 score: 0.6939 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1450;  Loss pred: 0.1450; Loss self: 0.0000; time: 0.15s
Val loss: 0.4262 score: 0.7959 time: 0.08s
Test loss: 0.4543 score: 0.6939 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1394;  Loss pred: 0.1394; Loss self: 0.0000; time: 0.15s
Val loss: 0.4245 score: 0.8163 time: 0.08s
Test loss: 0.4512 score: 0.6939 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1403;  Loss pred: 0.1403; Loss self: 0.0000; time: 0.14s
Val loss: 0.4212 score: 0.8163 time: 0.09s
Test loss: 0.4462 score: 0.6939 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.15s
Val loss: 0.4180 score: 0.8163 time: 0.08s
Test loss: 0.4413 score: 0.7143 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1359;  Loss pred: 0.1359; Loss self: 0.0000; time: 0.15s
Val loss: 0.4133 score: 0.8163 time: 0.08s
Test loss: 0.4353 score: 0.7143 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1264;  Loss pred: 0.1264; Loss self: 0.0000; time: 0.14s
Val loss: 0.4075 score: 0.8163 time: 0.08s
Test loss: 0.4278 score: 0.7143 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1245;  Loss pred: 0.1245; Loss self: 0.0000; time: 0.27s
Val loss: 0.4010 score: 0.8163 time: 0.08s
Test loss: 0.4188 score: 0.7143 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.14s
Val loss: 0.3965 score: 0.8163 time: 0.08s
Test loss: 0.4124 score: 0.7143 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.15s
Val loss: 0.3913 score: 0.8163 time: 0.08s
Test loss: 0.4050 score: 0.7143 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 0.14s
Val loss: 0.3878 score: 0.8163 time: 0.08s
Test loss: 0.3998 score: 0.7347 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.15s
Val loss: 0.3817 score: 0.8367 time: 0.08s
Test loss: 0.3911 score: 0.7347 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.1146;  Loss pred: 0.1146; Loss self: 0.0000; time: 0.14s
Val loss: 0.3778 score: 0.8367 time: 0.08s
Test loss: 0.3853 score: 0.7551 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.15s
Val loss: 0.3722 score: 0.8367 time: 0.08s
Test loss: 0.3769 score: 0.7959 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1049;  Loss pred: 0.1049; Loss self: 0.0000; time: 0.15s
Val loss: 0.3660 score: 0.8367 time: 0.08s
Test loss: 0.3676 score: 0.7959 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.16s
Val loss: 0.3581 score: 0.8367 time: 0.09s
Test loss: 0.3556 score: 0.8367 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.15s
Val loss: 0.3523 score: 0.8367 time: 0.09s
Test loss: 0.3462 score: 0.8367 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.15s
Val loss: 0.3451 score: 0.8367 time: 0.09s
Test loss: 0.3348 score: 0.8571 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.15s
Val loss: 0.3369 score: 0.8367 time: 0.09s
Test loss: 0.3217 score: 0.8776 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0955;  Loss pred: 0.0955; Loss self: 0.0000; time: 0.15s
Val loss: 0.3274 score: 0.8367 time: 0.09s
Test loss: 0.3058 score: 0.8776 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 0.16s
Val loss: 0.3193 score: 0.8367 time: 0.09s
Test loss: 0.2914 score: 0.8776 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.15s
Val loss: 0.3143 score: 0.8571 time: 0.08s
Test loss: 0.2820 score: 0.8776 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 0.16s
Val loss: 0.3119 score: 0.8571 time: 0.13s
Test loss: 0.2775 score: 0.9184 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.15s
Val loss: 0.3114 score: 0.8571 time: 0.09s
Test loss: 0.2760 score: 0.8776 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.15s
Val loss: 0.3136 score: 0.8571 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.14s
Val loss: 0.3128 score: 0.8571 time: 0.08s
Test loss: 0.2759 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.15s
Val loss: 0.3129 score: 0.8571 time: 0.09s
Test loss: 0.2735 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.15s
Val loss: 0.3104 score: 0.8571 time: 0.08s
Test loss: 0.2675 score: 0.8980 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.14s
Val loss: 0.3069 score: 0.8571 time: 0.08s
Test loss: 0.2603 score: 0.9184 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.14s
Val loss: 0.3024 score: 0.8571 time: 0.08s
Test loss: 0.2509 score: 0.9184 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0843;  Loss pred: 0.0843; Loss self: 0.0000; time: 0.14s
Val loss: 0.2950 score: 0.8776 time: 0.08s
Test loss: 0.2375 score: 0.9388 time: 0.20s
Epoch 104/1000, LR 0.000264
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.14s
Val loss: 0.2872 score: 0.8776 time: 0.08s
Test loss: 0.2227 score: 0.9388 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.15s
Val loss: 0.2805 score: 0.8571 time: 0.08s
Test loss: 0.2091 score: 0.9388 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.14s
Val loss: 0.2750 score: 0.8571 time: 0.08s
Test loss: 0.1967 score: 0.9388 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0872;  Loss pred: 0.0872; Loss self: 0.0000; time: 0.14s
Val loss: 0.2702 score: 0.8776 time: 0.08s
Test loss: 0.1846 score: 0.9592 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.14s
Val loss: 0.2667 score: 0.8776 time: 0.08s
Test loss: 0.1747 score: 0.9592 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.15s
Val loss: 0.2644 score: 0.8776 time: 0.08s
Test loss: 0.1680 score: 0.9592 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.14s
Val loss: 0.2631 score: 0.8776 time: 0.08s
Test loss: 0.1641 score: 0.9592 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.28s
Val loss: 0.2632 score: 0.8776 time: 0.08s
Test loss: 0.1642 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.15s
Val loss: 0.2637 score: 0.8776 time: 0.09s
Test loss: 0.1653 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.15s
Val loss: 0.2640 score: 0.8776 time: 0.08s
Test loss: 0.1652 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.15s
Val loss: 0.2638 score: 0.8776 time: 0.08s
Test loss: 0.1634 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.14s
Val loss: 0.2637 score: 0.8776 time: 0.09s
Test loss: 0.1617 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 0.15s
Val loss: 0.2632 score: 0.8776 time: 0.08s
Test loss: 0.1595 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.14s
Val loss: 0.2630 score: 0.8776 time: 0.08s
Test loss: 0.1579 score: 0.9592 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.14s
Val loss: 0.2621 score: 0.8776 time: 0.08s
Test loss: 0.1550 score: 0.9592 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.28s
Val loss: 0.2605 score: 0.8776 time: 0.08s
Test loss: 0.1506 score: 0.9592 time: 0.09s
Epoch 120/1000, LR 0.000262
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.14s
Val loss: 0.2580 score: 0.8776 time: 0.08s
Test loss: 0.1448 score: 0.9592 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0686;  Loss pred: 0.0686; Loss self: 0.0000; time: 0.15s
Val loss: 0.2555 score: 0.8776 time: 0.08s
Test loss: 0.1389 score: 0.9592 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.15s
Val loss: 0.2521 score: 0.8776 time: 0.08s
Test loss: 0.1306 score: 0.9592 time: 0.10s
Epoch 123/1000, LR 0.000262
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.15s
Val loss: 0.2500 score: 0.8776 time: 0.08s
Test loss: 0.1244 score: 0.9592 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.15s
Val loss: 0.2494 score: 0.8776 time: 0.08s
Test loss: 0.1216 score: 0.9592 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.15s
Val loss: 0.2492 score: 0.8776 time: 0.08s
Test loss: 0.1206 score: 0.9592 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0533;  Loss pred: 0.0533; Loss self: 0.0000; time: 0.16s
Val loss: 0.2489 score: 0.8776 time: 0.18s
Test loss: 0.1186 score: 0.9592 time: 0.11s
Epoch 127/1000, LR 0.000261
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.16s
Val loss: 0.2489 score: 0.8776 time: 0.08s
Test loss: 0.1178 score: 0.9592 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 0.16s
Val loss: 0.2491 score: 0.8776 time: 0.08s
Test loss: 0.1179 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.16s
Val loss: 0.2494 score: 0.8776 time: 0.09s
Test loss: 0.1182 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.13s
Val loss: 0.2500 score: 0.8776 time: 0.08s
Test loss: 0.1207 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0601;  Loss pred: 0.0601; Loss self: 0.0000; time: 0.15s
Val loss: 0.2509 score: 0.8776 time: 0.08s
Test loss: 0.1229 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.14s
Val loss: 0.2518 score: 0.8776 time: 0.08s
Test loss: 0.1251 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 0.14s
Val loss: 0.2501 score: 0.8776 time: 0.08s
Test loss: 0.1196 score: 0.9592 time: 0.35s
     INFO: Early stopping counter 6 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.14s
Val loss: 0.2484 score: 0.8776 time: 0.08s
Test loss: 0.1138 score: 0.9592 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.14s
Val loss: 0.2470 score: 0.8776 time: 0.08s
Test loss: 0.1083 score: 0.9592 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.14s
Val loss: 0.2467 score: 0.8980 time: 0.08s
Test loss: 0.1038 score: 0.9592 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.15s
Val loss: 0.2466 score: 0.8980 time: 0.08s
Test loss: 0.1003 score: 0.9796 time: 0.09s
Epoch 138/1000, LR 0.000259
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.15s
Val loss: 0.2466 score: 0.8980 time: 0.09s
Test loss: 0.0996 score: 0.9796 time: 0.09s
Epoch 139/1000, LR 0.000259
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.15s
Val loss: 0.2468 score: 0.8980 time: 0.08s
Test loss: 0.1011 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.14s
Val loss: 0.2470 score: 0.8980 time: 0.08s
Test loss: 0.1009 score: 0.9592 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.16s
Val loss: 0.2469 score: 0.8980 time: 0.08s
Test loss: 0.0988 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.15s
Val loss: 0.2465 score: 0.8980 time: 0.09s
Test loss: 0.0958 score: 0.9796 time: 0.10s
Epoch 143/1000, LR 0.000258
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.16s
Val loss: 0.2461 score: 0.8980 time: 0.08s
Test loss: 0.0942 score: 0.9796 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.14s
Val loss: 0.2464 score: 0.8980 time: 0.08s
Test loss: 0.0951 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.15s
Val loss: 0.2473 score: 0.8980 time: 0.08s
Test loss: 0.0978 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.15s
Val loss: 0.2471 score: 0.8980 time: 0.08s
Test loss: 0.0954 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.15s
Val loss: 0.2472 score: 0.8980 time: 0.08s
Test loss: 0.0942 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0536;  Loss pred: 0.0536; Loss self: 0.0000; time: 0.15s
Val loss: 0.2470 score: 0.8980 time: 0.11s
Test loss: 0.0907 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.14s
Val loss: 0.2469 score: 0.8980 time: 0.08s
Test loss: 0.0863 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.14s
Val loss: 0.2470 score: 0.8980 time: 0.08s
Test loss: 0.0835 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.14s
Val loss: 0.2473 score: 0.8980 time: 0.08s
Test loss: 0.0794 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.15s
Val loss: 0.2478 score: 0.9184 time: 0.08s
Test loss: 0.0756 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.15s
Val loss: 0.2482 score: 0.9184 time: 0.08s
Test loss: 0.0752 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.14s
Val loss: 0.2481 score: 0.9184 time: 0.08s
Test loss: 0.0760 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.13s
Val loss: 0.2480 score: 0.9184 time: 0.08s
Test loss: 0.0767 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.14s
Val loss: 0.2480 score: 0.8980 time: 0.23s
Test loss: 0.0825 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.14s
Val loss: 0.2483 score: 0.8980 time: 0.08s
Test loss: 0.0886 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.15s
Val loss: 0.2483 score: 0.8980 time: 0.08s
Test loss: 0.0894 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.14s
Val loss: 0.2480 score: 0.8980 time: 0.08s
Test loss: 0.0862 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.15s
Val loss: 0.2476 score: 0.8980 time: 0.08s
Test loss: 0.0810 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.14s
Val loss: 0.2475 score: 0.9184 time: 0.09s
Test loss: 0.0761 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.15s
Val loss: 0.2478 score: 0.9184 time: 0.08s
Test loss: 0.0726 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.15s
Val loss: 0.2485 score: 0.9184 time: 0.08s
Test loss: 0.0694 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 142,   Train_Loss: 0.0505,   Val_Loss: 0.2461,   Val_Precision: 0.8846,   Val_Recall: 0.9200,   Val_accuracy: 0.9020,   Val_Score: 0.8980,   Val_Loss: 0.2461,   Test_Precision: 0.9600,   Test_Recall: 1.0000,   Test_accuracy: 0.9796,   Test_Score: 0.9796,   Test_loss: 0.0942


[0.2135941891465336, 0.09214408788830042, 0.09436085098423064, 0.08510692813433707, 0.08713073004037142, 0.08993108198046684, 0.08891682093963027, 0.08514036214910448, 0.10019451007246971, 0.09260286879725754, 0.09034177614375949, 0.09141899202950299, 0.09240424702875316, 0.09323805896565318, 0.08973067696206272, 0.08917326899245381, 0.08307387004606426, 0.08701081899926066, 0.08922558394260705, 0.0891643378417939, 0.09143853606656194, 0.09105851012282073, 0.08602680498734117, 0.08505977084860206, 0.08808791497722268, 0.08823519293218851, 0.09534473204985261, 0.09016354591585696, 0.09045739215798676, 0.09721409203484654, 0.09323020791634917, 0.2206021868623793, 0.10376631305553019, 0.09195093088783324, 0.08393859211355448, 0.08649680204689503, 0.08701924397610128, 0.0872768119443208, 0.08916755695827305, 0.08481184486299753, 0.09709411906078458, 0.08507582382299006, 0.09009638987481594, 0.1008870939258486, 0.11207112902775407, 0.10149517795071006, 0.23612062889151275, 0.08516365592367947, 0.09975953120738268, 0.08872788702137768, 0.09022864792495966, 0.09167136810719967, 0.11678183008916676, 0.10215716692619026, 0.1956126489676535, 0.09068594500422478, 0.09095925092697144, 0.09103723894804716, 0.10353299486450851, 0.08945778594352305, 0.08758439496159554, 0.09074271004647017, 0.10078648501075804, 0.09001925401389599, 0.09031062992289662, 0.09114779299125075, 0.090903433971107, 0.09180487296544015, 0.09078593202866614, 0.09178500296548009, 0.0924446601420641, 0.0910842688754201, 0.09738172707147896, 0.09217219310812652, 0.09260898199863732, 0.0920626800507307, 0.09136405703611672, 0.08876906102523208, 0.08858553995378315, 0.09145450894720852, 0.08971925196237862, 0.08866410097107291, 0.08697885903529823, 0.0879341580439359, 0.08845911920070648, 0.10398306418210268, 0.0893523448612541, 0.0877744008321315, 0.0874137501232326, 0.08899673284031451, 0.08999751694500446, 0.09410237101837993, 0.09207112505100667, 0.09194637089967728, 0.10971475997939706, 0.09203088516369462, 0.09201181400567293, 0.0921189240179956, 0.11399492505006492, 0.08591836201958358, 0.09165044198743999, 0.2642267090268433, 0.09379065223038197, 0.09035956207662821, 0.09125912399031222, 0.0891556041315198, 0.08996579213999212, 0.0901355859823525, 0.09106242912821472, 0.08906117407605052, 0.088781813159585, 0.09045314299874008, 0.09062883094884455, 0.08836075803264976, 0.08905764808878303, 0.08740788092836738, 0.08622875111177564, 0.08872182085178792, 0.08823371818289161, 0.08866132702678442, 0.0859353260602802, 0.0886246629524976, 0.09120388492010534, 0.08848224091343582, 0.0914007150568068, 0.09038051404058933, 0.09138604300096631, 0.08963072882033885, 0.0835422680247575, 0.1897975387983024, 0.08780545298941433, 0.08801006805151701, 0.08330072509124875, 0.08582389098592103, 0.08371770894154906, 0.08453939110040665, 0.08301667310297489, 0.18688556784763932, 0.08322318200953305, 0.08398510701954365, 0.08362322999164462, 0.08506592595949769, 0.08471717895008624, 0.08691698010079563, 0.0867964739445597, 3.2059317571111023, 0.09026755602099001, 0.08513317885808647, 0.08699018601328135, 0.08692675898782909, 0.08328140783123672, 0.1897415709681809, 0.09188266913406551, 0.08727464801631868, 0.0871808270458132, 0.08912333589978516, 0.09186836797744036, 0.08876875904388726, 0.08893432910554111, 0.08621299685910344, 0.08676721900701523, 0.0867406609468162, 0.08964907005429268, 0.08806596789509058, 0.08704129699617624, 0.08990840404294431, 0.0867208659183234, 0.21097653498873115, 0.08636145712807775, 0.08248711004853249, 0.08377858786843717, 0.09522142400965095, 0.08591939997859299, 0.09091826295480132, 0.08961707609705627, 0.2200973650906235, 0.08461625105701387, 0.0825939429923892, 0.08336602291092277, 0.08450167998671532, 0.08508054492995143, 0.08601560909301043, 0.08459465391933918, 0.1921884089242667, 0.08631638600490987, 0.08467529504559934, 0.08398154890164733, 0.08981377794407308, 0.0842126349452883, 0.08757091197185218, 0.08485263283364475, 0.22260588803328574, 0.0815732148475945, 0.08281856402754784, 0.08433097298257053, 0.0847642500884831, 0.08391227503307164, 0.08807893702760339, 0.08547589997760952, 0.08495455491356552, 0.09118189802393317, 0.08972287294454873, 0.08865553583018482, 0.09099534107372165, 0.08927050489000976, 0.0909909950569272, 0.08949862094596028, 0.21659167297184467, 0.08811767981387675, 0.09033615910448134, 0.09180692699737847, 0.08587196096777916, 0.08577988808974624, 0.08783271489664912, 0.09800068801268935, 0.09141327813267708, 0.09182012197561562, 0.09370028297416866, 0.09256998379714787, 0.09132830193266273, 0.09122697985731065, 0.08575646206736565, 0.09185927198268473, 0.09084170311689377, 0.09121716418303549, 0.09378380514681339, 0.09195919195190072, 0.09341589920222759, 0.0892265981528908, 0.09211079915985465, 0.0904333449434489, 0.0899563180282712, 0.09274207893759012, 0.09254855196923018, 0.08961033704690635, 0.09073073719628155, 0.09087232500314713, 0.09134464804083109, 0.09056929918006063, 0.09363429108634591, 0.09106827504001558, 0.08410309301689267, 0.08848976087756455, 0.08745169802568853, 0.09345672582276165, 0.09297298779711127, 0.09845311916433275, 0.0852180770598352, 0.08465267112478614, 0.09785462496802211, 0.08650915208272636, 0.0842873309738934, 0.08552998187951744, 0.09156275191344321, 0.1006992431357503, 0.08942836406640708, 0.08971804101020098, 0.09173612506128848, 0.09170051082037389, 0.09246320091187954, 0.08446094510145485, 0.08441164111718535, 0.08585918508470058, 0.08529632305726409, 0.09747973107732832, 0.08960936008952558, 0.08652628399431705, 0.0848455389495939, 0.08754171500913799, 0.08806771389208734, 0.1051397961564362, 0.08934234804473817, 0.08754319278523326, 0.09165462688542902, 0.11022772896103561, 0.2068454520776868, 0.08766055596061051, 0.08464787295088172, 0.08887969190254807, 0.08852774393744767, 0.1020104200579226, 0.09102718019858003, 0.089943733997643, 0.09370374493300915, 0.08891586819663644, 0.08888817415572703, 0.09004339296370745, 0.09133117785677314, 0.09036775398999453, 0.103517462965101, 0.08436593087390065, 0.08631683513522148, 0.08564235479570925, 0.09005191712640226, 0.08659853902645409, 0.08798746694810688, 0.08587566786445677, 0.2272084781434387, 0.09340476593934, 0.09426429402083158, 0.09261739091016352, 0.09488877607509494, 0.09342541010119021, 0.09252547495998442, 0.09575245901942253, 0.0951496809720993, 0.09263342316262424, 0.09122346201911569, 0.09259261190891266, 0.09403687412850559, 0.08967565186321735, 0.08461574814282358, 0.08508491003885865, 0.20969060994684696, 0.0861392819788307, 0.08469193312339485, 0.08562674699351192, 0.0841122071724385, 0.08524556900374591, 0.08726196992211044, 0.087464485084638, 0.09431528695859015, 0.09422383294440806, 0.0912144840694964, 0.09266044688411057, 0.09434278402477503, 0.08806231897324324, 0.08554552681744099, 0.08754335390403867, 0.09049902600236237, 0.09183505503460765, 0.09006226202473044, 0.10212200204841793, 0.09150462900288403, 0.0902151248883456, 0.09301866893656552, 0.11699911602772772, 0.10060007800348103, 0.08887090184725821, 0.08989766519516706, 0.08733155601657927, 0.08711476600728929, 0.0855160029605031, 0.3538832440972328, 0.08978812699206173, 0.0894116759300232, 0.08764563291333616, 0.09185615996830165, 0.09261512197554111, 0.08587002800777555, 0.21628849091939628, 0.08941716607660055, 0.10211513889953494, 0.086557931965217, 0.0890719520393759, 0.09174216794781387, 0.0897476370446384, 0.09745783288963139, 0.08411588100716472, 0.08411416597664356, 0.08376197097823024, 0.08751470409333706, 0.09108872804790735, 0.08925767708569765, 0.08272364595904946, 0.08425851305946708, 0.09343592100776732, 0.08950078999623656, 0.09126100689172745, 0.0901307980529964, 0.09152810694649816, 0.09408437693491578, 0.08985023782588542, 0.09981696889735758]
[0.0043590650846231345, 0.0018804915895571513, 0.0019257316527394007, 0.001736876084374226, 0.0017781781640892125, 0.0018353282036829967, 0.0018146289987679646, 0.0017375584112062138, 0.0020447859198463206, 0.001889854465250154, 0.0018437097172195815, 0.0018656937148878161, 0.0018858009597704727, 0.0019028175299112893, 0.0018312383053482187, 0.0018198626324990575, 0.0016953851029809033, 0.0017757309999849116, 0.0018209302845430008, 0.0018196803641182427, 0.0018660925727869784, 0.001858336941282056, 0.0017556490813743096, 0.0017359136907877971, 0.0017977125505555648, 0.001800718223105888, 0.0019458108581602573, 0.0018400723656297338, 0.0018460692277140155, 0.001983961061935644, 0.0019026573044152893, 0.004502085446171007, 0.0021176798582761263, 0.0018765496099557803, 0.0017130324921133568, 0.0017652408580998986, 0.0017759029382877812, 0.0017811594274351184, 0.0018197460603729195, 0.001730853976795868, 0.001981512633893563, 0.0017362413025100011, 0.001838701834179917, 0.002058920284200992, 0.002287165898525593, 0.002071330162259389, 0.00481878834472475, 0.0017380337943608056, 0.002035908800150667, 0.0018107732045179118, 0.0018414009780604013, 0.0018708442470857076, 0.0023833026548809545, 0.0020848401413508214, 0.003992094876890888, 0.0018507335715147915, 0.0018563112434075804, 0.0018579028356744318, 0.00211291826254099, 0.0018256691008882255, 0.0017874366318692966, 0.0018518920417646973, 0.0020568670410358782, 0.0018371276329366528, 0.0018430740800591148, 0.0018601590406377704, 0.0018551721218593267, 0.0018735688360293908, 0.0018527741230340029, 0.0018731633258261243, 0.0018866257171849816, 0.001858862630110614, 0.0019873821851322236, 0.0018810651654719698, 0.001889979224461986, 0.001878830205116953, 0.0018645725925738107, 0.0018116134903108586, 0.0018078681623221052, 0.0018664185499430311, 0.0018310051420893595, 0.001809471448389243, 0.0017750787558224127, 0.0017945746539578754, 0.0018052881469531935, 0.0021221033506551566, 0.00182351724206641, 0.0017913143026965614, 0.0017839540841476042, 0.0018162598538839696, 0.0018366840192858055, 0.0019204565513955088, 0.0018790025520613607, 0.0018764565489730056, 0.0022390767342734094, 0.0018781813298713189, 0.0018777921225647538, 0.0018799780411835835, 0.0023264270418380598, 0.0017534359595833384, 0.0018704171834171427, 0.005392381816874354, 0.001914094943477183, 0.001844072695441392, 0.0018624311018431065, 0.001819502125133057, 0.0018360365742855534, 0.0018395017547418876, 0.0018584169209839739, 0.0018175749811438881, 0.0018118737379507143, 0.0018459825101783689, 0.001849567978547848, 0.0018032807761765256, 0.0018175030222200618, 0.0017838343046605587, 0.001759770430852564, 0.0018106494051385292, 0.0018006881261814613, 0.0018094148372813147, 0.0017537821644955144, 0.001808666590867298, 0.0018613037738797007, 0.0018057600186415473, 0.0018653207154450367, 0.0018445002865426394, 0.0018650212857340062, 0.001829198547353854, 0.0017049442454032144, 0.003873419159149029, 0.0017919480201921292, 0.0017961238377860614, 0.0017000147977805867, 0.0017515079793045108, 0.0017085246722765115, 0.0017252936959266663, 0.001694217818428059, 0.0038139911805640677, 0.0016984322859088378, 0.001713981775909054, 0.001706596530441727, 0.0017360393052958713, 0.001728922019389515, 0.0017738159204244005, 0.0017713566111134632, 0.06542717871655311, 0.0018421950208365309, 0.001737411813430336, 0.0017753099186383948, 0.0017740154895475324, 0.001699620567984423, 0.003872276958534304, 0.0018751565129401125, 0.0017811152656391567, 0.0017792005519553715, 0.0018188435897915339, 0.0018748646526008236, 0.0018116073274262706, 0.001814986308276349, 0.001759448915491907, 0.0017707595715717394, 0.001770217570343188, 0.001829572858250871, 0.001797264650920216, 0.0017763529999219642, 0.0018348653886315165, 0.0017698135901698653, 0.004305643571198595, 0.0017624787168995459, 0.0016834104091537241, 0.0017097670993558606, 0.001943294367543897, 0.0017534571424202652, 0.001855474754179619, 0.001828919920348087, 0.004491782961033133, 0.0017268622664696708, 0.0016855906733140654, 0.0017013474063453625, 0.0017245240813615371, 0.0017363376516316617, 0.0017554205937349067, 0.0017264215085579424, 0.003922212427025851, 0.0017615588980593852, 0.001728067245828558, 0.0017139091612581089, 0.0018329342437565935, 0.0017186252029650674, 0.0017871614688133098, 0.001731686384360097, 0.00454297730680175, 0.001664759486685602, 0.001690174776072405, 0.0017210402649504189, 0.0017298826548670018, 0.0017124954088381967, 0.0017975293270939467, 0.0017444061219920308, 0.0017337664268074595, 0.001860855061712922, 0.0018310790396846679, 0.0018092966495956086, 0.0018570477770147274, 0.0018218470385716278, 0.0018569590827944328, 0.0018265024682849037, 0.004420238223915197, 0.0017983199962015664, 0.0018435950837649253, 0.0018736107550485401, 0.001752488999342432, 0.0017506099610152294, 0.0017925043856459005, 0.002000014041075293, 0.0018655771047485117, 0.0018738800403186862, 0.0019122506729422175, 0.0018891833427989362, 0.0018638428965849535, 0.001861775099128789, 0.0017501318789258295, 0.0018746790200547905, 0.001853912308508036, 0.0018615747792456224, 0.0019139552070778242, 0.0018767182031000148, 0.0019064469224944407, 0.0018209509827120571, 0.0018798122277521357, 0.0018455784682336511, 0.001835843225066759, 0.0018926954885222474, 0.001888745958555718, 0.0018287823887123745, 0.001851647697883297, 0.0018545372449621862, 0.0018641764906292058, 0.001848353044491033, 0.0019109038997213452, 0.0018585362253064404, 0.0017163896534059728, 0.0018059134872972357, 0.0017847285311365006, 0.0019072801188318705, 0.0018974079142267607, 0.0020092473298843417, 0.001739144429792555, 0.0017276055331589008, 0.001997033162612696, 0.0017654928996474768, 0.0017201496117121102, 0.001745509834275866, 0.0018686275900702696, 0.002055086594607149, 0.001825068654416471, 0.0018309804287796117, 0.0018721658175773158, 0.001871438996334161, 0.0018870041002424396, 0.001723692757172548, 0.001722686553411946, 0.0017522282670347058, 0.0017407412868829407, 0.0019893822668842517, 0.0018287624508066445, 0.0017658425304962664, 0.001731541611216202, 0.0017865656124313874, 0.0017973002835119866, 0.002145710125641555, 0.00182331322540282, 0.0017865957711272094, 0.0018705025894985516, 0.0022495454890007266, 0.004221335756687486, 0.0017889909379716431, 0.001727507611242484, 0.0018138712633173077, 0.0018066886517846463, 0.002081845307304543, 0.0018576975550730617, 0.0018355864081151631, 0.001912321325163452, 0.0018146095550333966, 0.0018140443705250413, 0.0018376202645654582, 0.0018639015889137375, 0.001844239877346827, 0.0021126012850020614, 0.001721753691304095, 0.0017615680639841119, 0.0017478031590961072, 0.0018377942270694338, 0.001767317122988859, 0.0017956625907776915, 0.0017525646502950362, 0.004636907717213035, 0.001906219713047755, 0.0019237611024659506, 0.0018901508349012963, 0.001936505634185611, 0.0019066410224732695, 0.0018882749991833555, 0.001954131816722909, 0.001941830223920394, 0.0018904780237270252, 0.001861703306512565, 0.0018896451409982176, 0.0019191198801735835, 0.001830115344147293, 0.0017268520029147668, 0.0017364267354869113, 0.004279400202996877, 0.0017579445301802183, 0.0017284067984366296, 0.0017474846325206514, 0.0017165756565803777, 0.0017397054898723656, 0.001780856529022662, 0.0017849894915232245, 0.0019248017746651051, 0.0019229353662124093, 0.0018615200830509467, 0.0018910295282471546, 0.001925362939281123, 0.001797190183127413, 0.0017458270779069588, 0.0017865990592660953, 0.0018469188980073954, 0.001874184796624646, 0.0018380053474434785, 0.002084122490784039, 0.001867441408222123, 0.0018411249977213387, 0.0018983401823788881, 0.002387737061790362, 0.002053062816397572, 0.001813691874433841, 0.0018346462284727972, 0.001782276653399577, 0.0017778523674956998, 0.001745224550214349, 0.007222107022392506, 0.0018324107549400354, 0.0018247280802045549, 0.0017886863859864523, 0.0018746155095571767, 0.001890104530113084, 0.0017524495511790927, 0.00441405083508972, 0.001824840124012256, 0.0020839824265211212, 0.0017664884074534081, 0.0018177949395791, 0.0018722891417921198, 0.0018315844294824162, 0.0019889353650945183, 0.00171665063279928, 0.0017166156321763992, 0.001709427979147556, 0.0017860143692517768, 0.0018589536336307622, 0.0018215852466468908, 0.0016882376726336625, 0.0017195614910095322, 0.0019068555307707616, 0.0018265467346170728, 0.001862469528402601, 0.0018394040418978855, 0.0018679205499285338, 0.001920089325202363, 0.0018336783229772533, 0.002037080997905257]
[229.40698993634217, 531.7758428451659, 519.2831506806649, 575.746312011825, 562.3733437938164, 544.861675417659, 551.0768320571016, 575.5202205293343, 489.04875092017295, 529.1412743084602, 542.3847315335826, 535.9936585626167, 530.2786568322213, 525.5364659409148, 546.0785726682608, 549.4920232670462, 589.8364909788074, 563.1483597507151, 549.1698438367014, 549.5470631648911, 535.8790954869493, 538.1155471784928, 569.5899087175252, 576.0655067742322, 556.2624568043207, 555.333970172854, 513.9245655898379, 543.4568871739817, 541.6914950899746, 504.042150416175, 525.580722119222, 222.11928493060773, 472.21490826004214, 532.8929193742788, 583.7600889673182, 566.4949320719882, 563.0938371914289, 561.4320563319853, 549.5272234825285, 577.7494886375023, 504.6649629657194, 575.9568088573562, 543.8619690320871, 485.69146055505075, 437.22232857906965, 482.7815566153919, 207.52104646694545, 575.3628055130935, 491.1811373505509, 552.2502749129388, 543.0647707450051, 534.5180399478695, 419.585820521796, 479.6530823471551, 250.4950485492512, 540.32628758202, 538.702765256287, 538.2412797906048, 473.27907459960267, 547.7443856137344, 559.4603927044969, 539.9882808757492, 486.1762963037127, 544.3279944581192, 542.5717885240543, 537.5884417157911, 539.0335420725062, 533.7407309353393, 539.7311995929941, 533.8562773531605, 530.0468401820006, 537.9633673847613, 503.1744812251441, 531.6136933241727, 529.1063452216871, 532.2460737944929, 536.3159385602812, 551.9941231108893, 553.1376794177049, 535.785502148231, 546.1481112275305, 552.6475705876326, 563.3552859105056, 557.2351073801989, 553.9281923983781, 471.2305834168115, 548.3907565726112, 558.2493248084082, 560.5525438609103, 550.5820094308401, 544.4594658088494, 520.7095152834077, 532.1972548163649, 532.9193476647808, 446.6126527479214, 532.4299544967331, 532.5403104972904, 531.9211065733656, 429.84369680036116, 570.308823960486, 534.6400839694269, 185.446808842561, 522.4401241995761, 542.2779711841256, 536.9326140496559, 549.6008969634327, 544.6514595653545, 543.6254667451059, 538.0923885855125, 550.1836294922213, 551.9148376922948, 541.716941783687, 540.6667998140465, 554.5448125500943, 550.2054124666654, 560.5901833972677, 568.255939790695, 552.2880338745049, 555.3432520936313, 552.6648612556543, 570.1962422953799, 552.8934990281855, 537.2578157490115, 553.7834428033735, 536.1008387029119, 542.1522605856657, 536.1869098488254, 546.6875104655069, 586.529443819732, 258.1698388200504, 558.05190146798, 556.7544837179046, 588.2301738229137, 570.9365939612106, 585.3002981031332, 579.6114611448189, 590.2428773460941, 262.19253077876954, 588.7782564524767, 583.4367751486882, 585.9615803514877, 576.0238244315391, 578.3950859467343, 563.7563562744107, 564.5390621662606, 15.284168133433507, 542.8306931075655, 575.568781258374, 563.2819315102839, 563.6929361056779, 588.3666147826737, 258.245990849402, 533.288817812904, 561.4459767381467, 562.0501853492475, 549.7998869240948, 533.3718349283474, 551.9960009328779, 550.9683436398356, 568.3597808353645, 564.7294054225513, 564.9023129999396, 546.5756640902682, 556.4010839961666, 562.9511702031806, 544.999107943184, 565.0312584072884, 232.25331671418922, 567.3827379652812, 594.0322066219818, 584.874981146111, 514.5900779118124, 570.3019342803658, 538.9456244270706, 546.7707956342211, 222.62874423700896, 579.0849793969734, 593.2638426587191, 587.7694327862669, 579.8701281170196, 575.924849098495, 569.6640472197936, 579.2328206309751, 254.95814380412932, 567.6790035812293, 578.6811840881395, 583.4614941120578, 545.5733087022821, 581.8604302291999, 559.5465308817386, 577.4717691561258, 220.11996373012894, 600.687371357719, 591.6547886981135, 581.0439304444797, 578.0738925767671, 583.9431713737715, 556.3191570380068, 573.2610012042645, 576.7789619974302, 537.3873659346136, 546.1260701079354, 552.7009626771306, 538.489107484104, 548.8935013907777, 538.5148274215911, 547.4944695470385, 226.23215069939295, 556.0745596513481, 542.4184566373626, 533.7287893472263, 570.6169912480019, 571.2294698814984, 557.8786908460845, 499.99648975582056, 536.0271614904947, 533.6520900398365, 522.9439916793884, 529.3292489644977, 536.525906680365, 537.1218040610526, 571.3855121670977, 533.4246499279506, 539.3998386065871, 537.1796025326666, 522.4782671517028, 532.8450474600676, 524.5359774777135, 549.1636015982357, 531.9680259744836, 541.8355367772959, 544.708821726123, 528.3470088369927, 529.4518277961944, 546.8119149507388, 540.0595378608715, 539.2180732506075, 536.4298954668585, 541.0221834949082, 523.3125538891953, 538.0578470215813, 582.6182871794979, 553.7363816339945, 560.309303378037, 524.3068336561167, 527.0347996875116, 497.69880747213085, 574.9953729370711, 578.835840014656, 500.7428112469155, 566.4140593256841, 581.3447814022837, 572.8985195977743, 535.1521112681394, 486.5975003798613, 547.9245931818, 546.1554827576839, 534.1407211964025, 534.3481684195073, 529.9405549100404, 580.1497951643921, 580.4886547813379, 570.7018992978004, 574.4679048720965, 502.66860052301007, 546.8178765147504, 566.3019112576043, 577.5200512204954, 559.7331511598233, 556.390052999917, 466.0461765314202, 548.4521178631129, 559.7237025637165, 534.6156726081209, 444.5342425345714, 236.89184126512308, 558.9743238910978, 578.8686507035213, 551.3070415874746, 553.4987995923926, 480.343086247241, 538.3007569069395, 544.7850319543556, 522.9246711007246, 551.0827369040254, 551.2544324980147, 544.1820703019239, 536.5090120357639, 542.2288132271746, 473.3500860286679, 580.8031689146998, 567.6760497907275, 572.1468088644258, 544.130558944355, 565.8294071800854, 556.8974957410599, 570.5923600773613, 215.66096652901248, 524.5984988798339, 519.8150636886054, 529.0583066362638, 516.3940565659885, 524.482578636021, 529.5838796957444, 511.73620502070645, 514.9780797937541, 528.9667414533217, 537.1425170175207, 529.1998896003, 521.0721906072645, 546.4136472042591, 579.08842119191, 575.8953024410731, 233.67760727302323, 568.8461625677595, 578.5674997949067, 572.2510981727802, 582.5551563466295, 574.8099352571254, 561.5275479540186, 560.2273877515368, 519.5340180803757, 520.0382797939236, 537.1953862356647, 528.8124722869487, 519.3825951450858, 556.4241388520339, 572.7944151255131, 559.7226724225318, 541.442291309532, 533.5653142640853, 544.0680580124109, 479.81824697060085, 535.4920350363436, 543.1461748863582, 526.775974760677, 418.8065830205713, 487.0771571201413, 551.3615703395916, 545.0642115523403, 561.080120806478, 562.4764003372281, 572.9921687597047, 138.46374706154998, 545.7291697858019, 548.0268599187109, 559.0694980598875, 533.4427219351348, 529.0712677886501, 570.6298360070762, 226.54927126132065, 547.9932114827193, 479.8504955098598, 566.0948556359974, 550.1170556848091, 534.1055383373206, 545.9753773308654, 502.7815471280928, 582.529712740289, 582.5415901241427, 584.9910099743846, 559.9059096142292, 537.9370318381091, 548.9723864643526, 592.3336602482002, 581.5436116872523, 524.4235779077582, 547.4812010269452, 536.92153603054, 543.6543452237967, 535.3546755713244, 520.8091034486678, 545.3519232186533, 490.8984969317893]
Elapsed: 0.10473027133089018~0.16327260445000738
Time per graph: 0.0021373524761406156~0.0033320939683674972
Speed: 528.6733208157289~78.7081699740356
Total Time: 0.1056
best val loss: 0.24608300626277924 test_score: 0.9796

Testing...
Test loss: 0.0756 score: 0.9796 time: 0.17s
test Score 0.9796
Epoch Time List: [0.4336077037733048, 0.2944593238644302, 0.31582182995043695, 0.29263413907028735, 0.30103284190408885, 0.2895073061808944, 0.30209682881832123, 0.2975220929365605, 0.30395884974859655, 0.4059344017878175, 0.30219995020888746, 0.2989963530562818, 0.305264457128942, 0.31163162691518664, 0.309203227981925, 0.30081028677523136, 0.4127152687869966, 0.28759340615943074, 0.30536343017593026, 0.2976354949641973, 0.30571971694007516, 0.3065792159177363, 0.2968894769437611, 0.2922814358025789, 0.3674967128317803, 0.29558586422353983, 0.309013371123001, 0.31331815803423524, 0.31581354117952287, 0.32013092702254653, 0.3121164459735155, 0.43614139989949763, 0.3086449848487973, 0.3113256108481437, 0.2861402190756053, 0.28759698709473014, 0.30035373801365495, 0.2863979700487107, 0.31223518773913383, 0.2912184961605817, 0.43546973215416074, 0.28766257618553936, 0.3207822951953858, 0.4793594880029559, 0.340936197899282, 0.313765455968678, 0.44515777681954205, 0.29499886301346123, 0.329655644018203, 0.3006967888213694, 0.3048808048479259, 0.3092139249201864, 0.34837648808024824, 0.34121672296896577, 0.42159207770600915, 0.30647029005922377, 0.30552181880921125, 0.3168223579414189, 0.33544458425603807, 0.3045391689520329, 0.29862224077805877, 0.29999327822588384, 0.44581230194307864, 0.3062024973332882, 0.3014312069863081, 0.2955679530277848, 0.3131753809284419, 0.31034270487725735, 0.3057723480742425, 0.30282597709447145, 0.4822118189185858, 0.30392721644602716, 0.3146809369791299, 0.3114135756623, 0.3151313029229641, 0.3095252166967839, 0.31006460706703365, 0.31298755086027086, 0.30339726619422436, 0.3059654908720404, 0.30288409208878875, 0.30900910403579473, 0.30263738869689405, 0.3004622326698154, 0.30697516398504376, 0.31841497984714806, 0.30720837297849357, 0.309722047066316, 0.29747380083426833, 0.30473236995749176, 0.2986379947979003, 0.3132455302402377, 0.308694401755929, 0.3266336170490831, 0.331674826098606, 0.3160508219152689, 0.3096298899035901, 0.3109611268155277, 0.3515328059438616, 0.2908832929097116, 0.3008618119638413, 0.47567184595391154, 0.30562205286696553, 0.30920709925703704, 0.30616347794421017, 0.3049945109523833, 0.3067305281292647, 0.30932541005313396, 0.30358776124194264, 0.2958197249099612, 0.29385331901721656, 0.30290292389690876, 0.30834223609417677, 0.3928548078984022, 0.29641120904125273, 0.28972971998155117, 0.286998167168349, 0.2990966278593987, 0.29989532893523574, 0.2970852069556713, 0.3928489270620048, 0.31042591924779117, 0.301480463007465, 0.3041665852069855, 0.309337645303458, 0.2998354348819703, 0.3004754660651088, 0.30107871000654995, 0.2929465267807245, 0.40037986915558577, 0.30194369913078845, 0.29752740799449384, 0.2864937838166952, 0.2930958040524274, 0.28986814827658236, 0.2925203659106046, 0.2850896348245442, 0.39646251290105283, 0.27952436497434974, 0.27956098364666104, 0.2856336042750627, 0.2886789399199188, 0.279861006885767, 0.2839453858323395, 0.288852317025885, 5.943802120164037, 1.479110548272729, 0.2970658040139824, 0.30998126207850873, 0.29425204591825604, 0.2980236259754747, 0.39372654631733894, 0.31885244976729155, 0.3002551980316639, 0.297075220849365, 0.30359467188827693, 0.30159143591299653, 0.29502607230097055, 0.3037884358782321, 0.42628949298523366, 0.2947206380777061, 0.2966636240016669, 0.30107450392097235, 0.2954095769673586, 0.30124051100574434, 0.2981315420474857, 0.2971778579521924, 0.4212320151273161, 0.285719740903005, 0.2832523500546813, 0.28581255418248475, 0.30714985099621117, 0.3048179259058088, 0.3138995640911162, 0.2973983231931925, 0.43346114712767303, 0.2884948200080544, 0.2867229809053242, 0.28955725324340165, 0.290547801181674, 0.2915098911616951, 0.2988624048884958, 0.29213425097987056, 0.40172881493344903, 0.31024895003065467, 0.2921906551346183, 0.2888672580011189, 0.2975449289660901, 0.2907384599093348, 0.29889760608784854, 0.29757329425774515, 0.43216754612512887, 0.27986040501855314, 0.2836647860240191, 0.28686855896376073, 0.29610991897061467, 0.28361411788500845, 0.2865388428326696, 0.28440860169939697, 0.29302490898407996, 0.4327394438441843, 0.297303105937317, 0.30100032687187195, 0.3052109368145466, 0.3051949969958514, 0.29977859696373343, 0.31443379214033484, 0.429582686861977, 0.29909132700413465, 0.3005962206516415, 0.3103500222787261, 0.304413533071056, 0.29488060204312205, 0.3076683562248945, 0.41929599922150373, 0.30814831610769033, 0.3181879478506744, 0.32267633080482483, 0.3240608503110707, 0.3158291489817202, 0.32096412498503923, 0.306225482840091, 0.38928817003034055, 0.3161594192497432, 0.31966716912575066, 0.334387760842219, 0.31981777609325945, 0.32150080194696784, 0.31503683025948703, 0.3151343420613557, 0.4583409719634801, 0.31042546313256025, 0.3151857580523938, 0.3165269708260894, 0.3161790033336729, 0.3153876119758934, 0.3190334721002728, 0.3174782220739871, 0.3183078649453819, 0.32097873627208173, 0.3181976121850312, 0.30990117811597884, 0.30844829697161913, 0.3147796532139182, 0.31219154107384384, 0.33962268312461674, 0.3297652800101787, 0.3065492119640112, 0.3013809851836413, 0.3056148109026253, 0.3019223981536925, 0.3076121099293232, 0.2963723901193589, 0.30853358772583306, 0.32440465316176414, 0.3122482227627188, 0.3118080389685929, 0.31304313987493515, 0.318316335324198, 0.3185691460967064, 0.30260337609797716, 0.29639596212655306, 0.30358241335488856, 0.30666679702699184, 0.3255507196299732, 0.32630234211683273, 0.3044919571839273, 0.3059149431064725, 0.30711042787879705, 0.316974374698475, 0.32257872889749706, 0.328731823945418, 0.30420882487669587, 0.3115074245724827, 0.33416126878000796, 0.4300718589220196, 0.3116004227194935, 0.30418983404524624, 0.30933444295078516, 0.3083725271280855, 0.3362148830201477, 0.3297664262354374, 0.3720514359883964, 0.3144044801592827, 0.3137409989722073, 0.3131014108657837, 0.32432592078112066, 0.3193431382533163, 0.31941739516332746, 0.32061029225587845, 0.42572396202012897, 0.30107520311139524, 0.3072466431185603, 0.30488836392760277, 0.30712355370633304, 0.30405154218897223, 0.3059776790905744, 0.4461837827693671, 0.33575572771951556, 0.3234772540163249, 0.3237532547209412, 0.3297412400133908, 0.31857844395563006, 0.32926988904364407, 0.32637173496186733, 0.3824745360761881, 0.3245510619599372, 0.3168561172205955, 0.3160572899505496, 0.3220704449340701, 0.31748519092798233, 0.3002994360867888, 0.2989718932658434, 0.42426206753589213, 0.30066454876214266, 0.306606647092849, 0.3010451658628881, 0.2999467640183866, 0.2963874079287052, 0.30899744178168476, 0.3042030627839267, 0.45069832215085626, 0.3251193780452013, 0.3185153517406434, 0.3191123199649155, 0.31642672792077065, 0.31297692796215415, 0.29707556986249983, 0.3043534259777516, 0.4503689147531986, 0.3085226092953235, 0.31938828504644334, 0.33101091301068664, 0.3221872141584754, 0.3205220070667565, 0.3206370281986892, 0.4504637052305043, 0.337733615655452, 0.32362004765309393, 0.32943943282589316, 0.29492790601216257, 0.31335937697440386, 0.3036916048731655, 0.5718698010314256, 0.3106601149775088, 0.31088853790424764, 0.299417024012655, 0.31164205889217556, 0.32374982303008437, 0.3130806959234178, 0.42915420583449304, 0.3260998309124261, 0.33956109872087836, 0.3299607632216066, 0.30739625403657556, 0.3123747541103512, 0.32083822204731405, 0.3189558554440737, 0.33660720800980926, 0.29459333093836904, 0.2966247880831361, 0.30156823014840484, 0.3172736221458763, 0.3130755459424108, 0.2933800492901355, 0.2872146179433912, 0.4541881906334311, 0.3109732170123607, 0.32174411485902965, 0.3103538400027901, 0.31297818617895246, 0.31924390117637813, 0.31342369806952775, 0.3203746350482106]
Total Epoch List: [211, 163]
Total Time List: [0.09264870616607368, 0.105589693877846]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7418d40b8a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4898 time: 0.09s
Test loss: 0.7093 score: 0.4583 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7115;  Loss pred: 0.7115; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.09s
Test loss: 0.7084 score: 0.4583 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.09s
Test loss: 0.7064 score: 0.4583 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.09s
Test loss: 0.7033 score: 0.4583 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.09s
Test loss: 0.6994 score: 0.4583 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4898 time: 0.09s
Test loss: 0.6950 score: 0.4583 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6374;  Loss pred: 0.6374; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6702 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6630 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6671 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5874;  Loss pred: 0.5874; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6559 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6590 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6478 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6502 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6390 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6411 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.14s
Val loss: 0.6290 score: 0.5918 time: 0.09s
Test loss: 0.6308 score: 0.6250 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.14s
Val loss: 0.6188 score: 0.6735 time: 0.09s
Test loss: 0.6204 score: 0.6458 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5034;  Loss pred: 0.5034; Loss self: 0.0000; time: 0.14s
Val loss: 0.6079 score: 0.7959 time: 0.09s
Test loss: 0.6094 score: 0.7292 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4877;  Loss pred: 0.4877; Loss self: 0.0000; time: 0.14s
Val loss: 0.5973 score: 0.8367 time: 0.09s
Test loss: 0.5985 score: 0.8333 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4693;  Loss pred: 0.4693; Loss self: 0.0000; time: 0.14s
Val loss: 0.5867 score: 0.8776 time: 0.09s
Test loss: 0.5876 score: 0.9167 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4527;  Loss pred: 0.4527; Loss self: 0.0000; time: 0.14s
Val loss: 0.5760 score: 0.9184 time: 0.09s
Test loss: 0.5766 score: 0.9375 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4563;  Loss pred: 0.4563; Loss self: 0.0000; time: 0.14s
Val loss: 0.5655 score: 0.9388 time: 0.09s
Test loss: 0.5658 score: 0.9167 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4401;  Loss pred: 0.4401; Loss self: 0.0000; time: 0.14s
Val loss: 0.5553 score: 0.9388 time: 0.16s
Test loss: 0.5555 score: 0.8750 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4101;  Loss pred: 0.4101; Loss self: 0.0000; time: 0.14s
Val loss: 0.5452 score: 0.9184 time: 0.09s
Test loss: 0.5454 score: 0.8542 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.4251;  Loss pred: 0.4251; Loss self: 0.0000; time: 0.14s
Val loss: 0.5361 score: 0.8571 time: 0.09s
Test loss: 0.5359 score: 0.8750 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4002;  Loss pred: 0.4002; Loss self: 0.0000; time: 0.14s
Val loss: 0.5275 score: 0.8571 time: 0.09s
Test loss: 0.5271 score: 0.8750 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.3945;  Loss pred: 0.3945; Loss self: 0.0000; time: 0.14s
Val loss: 0.5197 score: 0.8571 time: 0.09s
Test loss: 0.5191 score: 0.8750 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3735;  Loss pred: 0.3735; Loss self: 0.0000; time: 0.14s
Val loss: 0.5125 score: 0.8571 time: 0.09s
Test loss: 0.5116 score: 0.8333 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3508;  Loss pred: 0.3508; Loss self: 0.0000; time: 0.14s
Val loss: 0.5059 score: 0.8571 time: 0.09s
Test loss: 0.5046 score: 0.8333 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3499;  Loss pred: 0.3499; Loss self: 0.0000; time: 0.14s
Val loss: 0.4995 score: 0.8571 time: 0.09s
Test loss: 0.4976 score: 0.8333 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3437;  Loss pred: 0.3437; Loss self: 0.0000; time: 0.14s
Val loss: 0.4936 score: 0.8571 time: 0.09s
Test loss: 0.4909 score: 0.8333 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3327;  Loss pred: 0.3327; Loss self: 0.0000; time: 0.14s
Val loss: 0.4879 score: 0.8571 time: 0.09s
Test loss: 0.4845 score: 0.8542 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3379;  Loss pred: 0.3379; Loss self: 0.0000; time: 0.14s
Val loss: 0.4811 score: 0.8571 time: 0.09s
Test loss: 0.4774 score: 0.8542 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3203;  Loss pred: 0.3203; Loss self: 0.0000; time: 0.14s
Val loss: 0.4745 score: 0.8571 time: 0.09s
Test loss: 0.4707 score: 0.8542 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.14s
Val loss: 0.4681 score: 0.8571 time: 0.09s
Test loss: 0.4643 score: 0.8542 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3034;  Loss pred: 0.3034; Loss self: 0.0000; time: 0.14s
Val loss: 0.4629 score: 0.8571 time: 0.09s
Test loss: 0.4590 score: 0.8542 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3093;  Loss pred: 0.3093; Loss self: 0.0000; time: 0.14s
Val loss: 0.4580 score: 0.8571 time: 0.09s
Test loss: 0.4543 score: 0.8542 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.14s
Val loss: 0.4536 score: 0.8776 time: 0.08s
Test loss: 0.4501 score: 0.8750 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.14s
Val loss: 0.4494 score: 0.8776 time: 0.08s
Test loss: 0.4457 score: 0.8750 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.2787;  Loss pred: 0.2787; Loss self: 0.0000; time: 0.13s
Val loss: 0.4468 score: 0.8571 time: 0.11s
Test loss: 0.4423 score: 0.8542 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2669;  Loss pred: 0.2669; Loss self: 0.0000; time: 0.14s
Val loss: 0.4454 score: 0.8571 time: 0.08s
Test loss: 0.4395 score: 0.8542 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2619;  Loss pred: 0.2619; Loss self: 0.0000; time: 0.14s
Val loss: 0.4439 score: 0.8571 time: 0.09s
Test loss: 0.4366 score: 0.8542 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2569;  Loss pred: 0.2569; Loss self: 0.0000; time: 0.13s
Val loss: 0.4442 score: 0.8367 time: 0.09s
Test loss: 0.4351 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.2567;  Loss pred: 0.2567; Loss self: 0.0000; time: 0.13s
Val loss: 0.4436 score: 0.8163 time: 0.09s
Test loss: 0.4329 score: 0.8542 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2389;  Loss pred: 0.2389; Loss self: 0.0000; time: 0.14s
Val loss: 0.4421 score: 0.8163 time: 0.09s
Test loss: 0.4301 score: 0.8542 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2297;  Loss pred: 0.2297; Loss self: 0.0000; time: 0.14s
Val loss: 0.4385 score: 0.8163 time: 0.09s
Test loss: 0.4259 score: 0.8542 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2383;  Loss pred: 0.2383; Loss self: 0.0000; time: 0.14s
Val loss: 0.4352 score: 0.8163 time: 0.09s
Test loss: 0.4217 score: 0.8750 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2242;  Loss pred: 0.2242; Loss self: 0.0000; time: 0.14s
Val loss: 0.4328 score: 0.8163 time: 0.09s
Test loss: 0.4183 score: 0.8750 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2260;  Loss pred: 0.2260; Loss self: 0.0000; time: 0.15s
Val loss: 0.4306 score: 0.8163 time: 0.10s
Test loss: 0.4150 score: 0.8750 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2267;  Loss pred: 0.2267; Loss self: 0.0000; time: 0.15s
Val loss: 0.4270 score: 0.8163 time: 0.09s
Test loss: 0.4110 score: 0.8542 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2128;  Loss pred: 0.2128; Loss self: 0.0000; time: 0.15s
Val loss: 0.4241 score: 0.8163 time: 0.10s
Test loss: 0.4077 score: 0.8542 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2145;  Loss pred: 0.2145; Loss self: 0.0000; time: 0.15s
Val loss: 0.4212 score: 0.8163 time: 0.09s
Test loss: 0.4046 score: 0.8542 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.15s
Val loss: 0.4178 score: 0.8367 time: 0.09s
Test loss: 0.4011 score: 0.8542 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.1951;  Loss pred: 0.1951; Loss self: 0.0000; time: 0.15s
Val loss: 0.4155 score: 0.8367 time: 0.10s
Test loss: 0.3985 score: 0.8542 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.1951;  Loss pred: 0.1951; Loss self: 0.0000; time: 0.15s
Val loss: 0.4104 score: 0.8367 time: 0.10s
Test loss: 0.3939 score: 0.8750 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 0.15s
Val loss: 0.4043 score: 0.8571 time: 0.10s
Test loss: 0.3889 score: 0.8750 time: 0.17s
Epoch 55/1000, LR 0.000269
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.15s
Val loss: 0.3988 score: 0.8571 time: 0.09s
Test loss: 0.3843 score: 0.8750 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1835;  Loss pred: 0.1835; Loss self: 0.0000; time: 0.15s
Val loss: 0.3926 score: 0.8571 time: 0.09s
Test loss: 0.3793 score: 0.8750 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1759;  Loss pred: 0.1759; Loss self: 0.0000; time: 0.14s
Val loss: 0.3862 score: 0.8571 time: 0.09s
Test loss: 0.3744 score: 0.8750 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1754;  Loss pred: 0.1754; Loss self: 0.0000; time: 0.14s
Val loss: 0.3852 score: 0.8571 time: 0.09s
Test loss: 0.3723 score: 0.8750 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 0.15s
Val loss: 0.3858 score: 0.8571 time: 0.09s
Test loss: 0.3711 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 0.14s
Val loss: 0.3870 score: 0.8571 time: 0.08s
Test loss: 0.3704 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.15s
Val loss: 0.3889 score: 0.8367 time: 0.09s
Test loss: 0.3700 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.14s
Val loss: 0.3888 score: 0.8367 time: 0.10s
Test loss: 0.3687 score: 0.8750 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.1563;  Loss pred: 0.1563; Loss self: 0.0000; time: 0.14s
Val loss: 0.3851 score: 0.8367 time: 0.09s
Test loss: 0.3652 score: 0.8750 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.15s
Val loss: 0.3793 score: 0.8571 time: 0.09s
Test loss: 0.3603 score: 0.8750 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.15s
Val loss: 0.3713 score: 0.8571 time: 0.11s
Test loss: 0.3539 score: 0.8750 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 0.14s
Val loss: 0.3606 score: 0.8571 time: 0.09s
Test loss: 0.3462 score: 0.8750 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1553;  Loss pred: 0.1553; Loss self: 0.0000; time: 0.15s
Val loss: 0.3538 score: 0.8571 time: 0.10s
Test loss: 0.3409 score: 0.8750 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1494;  Loss pred: 0.1494; Loss self: 0.0000; time: 0.15s
Val loss: 0.3459 score: 0.8980 time: 0.10s
Test loss: 0.3352 score: 0.8750 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.15s
Val loss: 0.3338 score: 0.9184 time: 0.09s
Test loss: 0.3284 score: 0.8958 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 0.23s
Val loss: 0.3229 score: 0.9388 time: 0.09s
Test loss: 0.3221 score: 0.8958 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.14s
Val loss: 0.3175 score: 0.9388 time: 0.09s
Test loss: 0.3182 score: 0.9167 time: 0.10s
Epoch 72/1000, LR 0.000267
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.14s
Val loss: 0.3156 score: 0.9388 time: 0.09s
Test loss: 0.3154 score: 0.8958 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1403;  Loss pred: 0.1403; Loss self: 0.0000; time: 0.15s
Val loss: 0.3147 score: 0.9388 time: 0.09s
Test loss: 0.3126 score: 0.9167 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 0.14s
Val loss: 0.3176 score: 0.9184 time: 0.09s
Test loss: 0.3110 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.14s
Val loss: 0.3202 score: 0.8980 time: 0.09s
Test loss: 0.3096 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.1251;  Loss pred: 0.1251; Loss self: 0.0000; time: 0.15s
Val loss: 0.3196 score: 0.8980 time: 0.09s
Test loss: 0.3069 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.14s
Val loss: 0.3191 score: 0.8776 time: 0.23s
Test loss: 0.3048 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.15s
Val loss: 0.3134 score: 0.8980 time: 0.10s
Test loss: 0.3000 score: 0.8958 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1167;  Loss pred: 0.1167; Loss self: 0.0000; time: 0.15s
Val loss: 0.3063 score: 0.9184 time: 0.10s
Test loss: 0.2947 score: 0.8958 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1231;  Loss pred: 0.1231; Loss self: 0.0000; time: 0.17s
Val loss: 0.3014 score: 0.9184 time: 0.10s
Test loss: 0.2907 score: 0.9167 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.15s
Val loss: 0.2972 score: 0.9184 time: 0.10s
Test loss: 0.2866 score: 0.9167 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1231;  Loss pred: 0.1231; Loss self: 0.0000; time: 0.16s
Val loss: 0.2935 score: 0.9184 time: 0.10s
Test loss: 0.2827 score: 0.9167 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1133;  Loss pred: 0.1133; Loss self: 0.0000; time: 0.16s
Val loss: 0.2939 score: 0.9184 time: 0.11s
Test loss: 0.2809 score: 0.9167 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.14s
Val loss: 0.2923 score: 0.9184 time: 0.09s
Test loss: 0.2781 score: 0.9167 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 0.14s
Val loss: 0.2913 score: 0.9184 time: 0.09s
Test loss: 0.2759 score: 0.9167 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1134;  Loss pred: 0.1134; Loss self: 0.0000; time: 0.15s
Val loss: 0.2864 score: 0.9184 time: 0.09s
Test loss: 0.2717 score: 0.9167 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 0.14s
Val loss: 0.2801 score: 0.9184 time: 0.09s
Test loss: 0.2666 score: 0.9167 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.15s
Val loss: 0.2723 score: 0.9388 time: 0.09s
Test loss: 0.2609 score: 0.9167 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.14s
Val loss: 0.2643 score: 0.9388 time: 0.10s
Test loss: 0.2555 score: 0.9167 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.26s
Val loss: 0.2571 score: 0.9388 time: 0.09s
Test loss: 0.2505 score: 0.9375 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.15s
Val loss: 0.2518 score: 0.9592 time: 0.10s
Test loss: 0.2461 score: 0.9375 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 0.15s
Val loss: 0.2476 score: 0.9592 time: 0.10s
Test loss: 0.2417 score: 0.9375 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.15s
Val loss: 0.2441 score: 0.9592 time: 0.10s
Test loss: 0.2380 score: 0.9375 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.15s
Val loss: 0.2418 score: 0.9592 time: 0.09s
Test loss: 0.2344 score: 0.9375 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0956;  Loss pred: 0.0956; Loss self: 0.0000; time: 0.15s
Val loss: 0.2408 score: 0.9592 time: 0.10s
Test loss: 0.2319 score: 0.9375 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.15s
Val loss: 0.2392 score: 0.9592 time: 0.09s
Test loss: 0.2294 score: 0.9375 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.15s
Val loss: 0.2359 score: 0.9592 time: 0.19s
Test loss: 0.2268 score: 0.9375 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0929;  Loss pred: 0.0929; Loss self: 0.0000; time: 0.15s
Val loss: 0.2325 score: 0.9592 time: 0.10s
Test loss: 0.2241 score: 0.9375 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.14s
Val loss: 0.2283 score: 0.9592 time: 0.09s
Test loss: 0.2215 score: 0.9375 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 0.14s
Val loss: 0.2255 score: 0.9592 time: 0.09s
Test loss: 0.2194 score: 0.9375 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.14s
Val loss: 0.2225 score: 0.9592 time: 0.09s
Test loss: 0.2171 score: 0.9375 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0825;  Loss pred: 0.0825; Loss self: 0.0000; time: 0.15s
Val loss: 0.2189 score: 0.9592 time: 0.09s
Test loss: 0.2147 score: 0.9583 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.14s
Val loss: 0.2167 score: 0.9592 time: 0.09s
Test loss: 0.2126 score: 0.9583 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0790;  Loss pred: 0.0790; Loss self: 0.0000; time: 0.14s
Val loss: 0.2175 score: 0.9592 time: 0.09s
Test loss: 0.2105 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.14s
Val loss: 0.2187 score: 0.9592 time: 0.23s
Test loss: 0.2086 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.14s
Val loss: 0.2160 score: 0.9592 time: 0.09s
Test loss: 0.2059 score: 0.9375 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.14s
Val loss: 0.2124 score: 0.9592 time: 0.09s
Test loss: 0.2035 score: 0.9375 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0739;  Loss pred: 0.0739; Loss self: 0.0000; time: 0.15s
Val loss: 0.2098 score: 0.9592 time: 0.09s
Test loss: 0.2017 score: 0.9583 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0786;  Loss pred: 0.0786; Loss self: 0.0000; time: 0.15s
Val loss: 0.2080 score: 0.9592 time: 0.11s
Test loss: 0.2005 score: 0.9583 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.15s
Val loss: 0.2070 score: 0.9592 time: 0.09s
Test loss: 0.1991 score: 0.9583 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.15s
Val loss: 0.2054 score: 0.9592 time: 0.10s
Test loss: 0.1973 score: 0.9583 time: 0.09s
Epoch 112/1000, LR 0.000263
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.26s
Val loss: 0.2043 score: 0.9592 time: 0.10s
Test loss: 0.1940 score: 0.9375 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.14s
Val loss: 0.2029 score: 0.9592 time: 0.09s
Test loss: 0.1915 score: 0.9375 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.14s
Val loss: 0.2002 score: 0.9592 time: 0.09s
Test loss: 0.1886 score: 0.9375 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0662;  Loss pred: 0.0662; Loss self: 0.0000; time: 0.14s
Val loss: 0.1969 score: 0.9592 time: 0.09s
Test loss: 0.1868 score: 0.9375 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.14s
Val loss: 0.1942 score: 0.9592 time: 0.09s
Test loss: 0.1858 score: 0.9583 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0667;  Loss pred: 0.0667; Loss self: 0.0000; time: 0.14s
Val loss: 0.1926 score: 0.9592 time: 0.09s
Test loss: 0.1850 score: 0.9583 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.15s
Val loss: 0.1912 score: 0.9592 time: 0.21s
Test loss: 0.1841 score: 0.9583 time: 0.07s
Epoch 119/1000, LR 0.000262
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.14s
Val loss: 0.1899 score: 0.9592 time: 0.09s
Test loss: 0.1823 score: 0.9375 time: 0.07s
Epoch 120/1000, LR 0.000262
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.27s
Val loss: 0.1880 score: 0.9592 time: 0.10s
Test loss: 0.1804 score: 0.9375 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.15s
Val loss: 0.1864 score: 0.9592 time: 0.09s
Test loss: 0.1789 score: 0.9583 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.14s
Val loss: 0.1850 score: 0.9592 time: 0.09s
Test loss: 0.1761 score: 0.9583 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.14s
Val loss: 0.1808 score: 0.9592 time: 0.09s
Test loss: 0.1731 score: 0.9375 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.15s
Val loss: 0.1800 score: 0.9592 time: 0.09s
Test loss: 0.1717 score: 0.9375 time: 0.08s
Epoch 125/1000, LR 0.000261
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.14s
Val loss: 0.1774 score: 0.9592 time: 0.09s
Test loss: 0.1696 score: 0.9375 time: 0.08s
Epoch 126/1000, LR 0.000261
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.14s
Val loss: 0.1797 score: 0.9592 time: 0.23s
Test loss: 0.1705 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.13s
Val loss: 0.1780 score: 0.9592 time: 0.09s
Test loss: 0.1688 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.14s
Val loss: 0.1790 score: 0.9592 time: 0.09s
Test loss: 0.1690 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.14s
Val loss: 0.1776 score: 0.9592 time: 0.09s
Test loss: 0.1687 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.14s
Val loss: 0.1773 score: 0.9592 time: 0.10s
Test loss: 0.1689 score: 0.9583 time: 0.09s
Epoch 131/1000, LR 0.000260
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.16s
Val loss: 0.1786 score: 0.9388 time: 0.10s
Test loss: 0.1700 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.16s
Val loss: 0.1785 score: 0.9388 time: 0.11s
Test loss: 0.1710 score: 0.9583 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.14s
Val loss: 0.1789 score: 0.9388 time: 0.09s
Test loss: 0.1714 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0454;  Loss pred: 0.0454; Loss self: 0.0000; time: 0.13s
Val loss: 0.1778 score: 0.9592 time: 0.09s
Test loss: 0.1718 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.13s
Val loss: 0.1773 score: 0.9388 time: 0.09s
Test loss: 0.1715 score: 0.9583 time: 0.07s
Epoch 136/1000, LR 0.000260
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.13s
Val loss: 0.1793 score: 0.9388 time: 0.09s
Test loss: 0.1694 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.14s
Val loss: 0.1760 score: 0.9388 time: 0.09s
Test loss: 0.1682 score: 0.9583 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.15s
Val loss: 0.1746 score: 0.9388 time: 0.10s
Test loss: 0.1652 score: 0.9375 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.14s
Val loss: 0.1733 score: 0.9592 time: 0.10s
Test loss: 0.1642 score: 0.9375 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.14s
Val loss: 0.1735 score: 0.9592 time: 0.21s
Test loss: 0.1628 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0396;  Loss pred: 0.0396; Loss self: 0.0000; time: 0.14s
Val loss: 0.1735 score: 0.9592 time: 0.09s
Test loss: 0.1637 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.13s
Val loss: 0.1756 score: 0.9388 time: 0.10s
Test loss: 0.1650 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.14s
Val loss: 0.1731 score: 0.9388 time: 0.09s
Test loss: 0.1647 score: 0.9583 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.14s
Val loss: 0.1699 score: 0.9592 time: 0.09s
Test loss: 0.1639 score: 0.9375 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.0384;  Loss pred: 0.0384; Loss self: 0.0000; time: 0.13s
Val loss: 0.1683 score: 0.9592 time: 0.09s
Test loss: 0.1595 score: 0.9375 time: 0.07s
Epoch 146/1000, LR 0.000258
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.14s
Val loss: 0.1676 score: 0.9592 time: 0.09s
Test loss: 0.1586 score: 0.9375 time: 0.09s
Epoch 147/1000, LR 0.000258
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.15s
Val loss: 0.1743 score: 0.9388 time: 0.09s
Test loss: 0.1628 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.14s
Val loss: 0.1750 score: 0.9388 time: 0.09s
Test loss: 0.1635 score: 0.9583 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.14s
Val loss: 0.1751 score: 0.9388 time: 0.09s
Test loss: 0.1644 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.14s
Val loss: 0.1749 score: 0.9388 time: 0.09s
Test loss: 0.1639 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0311;  Loss pred: 0.0311; Loss self: 0.0000; time: 0.14s
Val loss: 0.1748 score: 0.9388 time: 0.09s
Test loss: 0.1644 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.14s
Val loss: 0.1736 score: 0.9592 time: 0.09s
Test loss: 0.1637 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.14s
Val loss: 0.1709 score: 0.9592 time: 0.09s
Test loss: 0.1616 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.14s
Val loss: 0.1716 score: 0.9592 time: 0.09s
Test loss: 0.1617 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.14s
Val loss: 0.1734 score: 0.9388 time: 0.09s
Test loss: 0.1617 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.14s
Val loss: 0.1718 score: 0.9592 time: 0.09s
Test loss: 0.1609 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.15s
Val loss: 0.1700 score: 0.9592 time: 0.10s
Test loss: 0.1611 score: 0.9375 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.14s
Val loss: 0.1666 score: 0.9592 time: 0.09s
Test loss: 0.1559 score: 0.9375 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.14s
Val loss: 0.1652 score: 0.9592 time: 0.09s
Test loss: 0.1539 score: 0.9375 time: 0.08s
Epoch 160/1000, LR 0.000255
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.13s
Val loss: 0.1680 score: 0.9592 time: 0.09s
Test loss: 0.1576 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.14s
Val loss: 0.1656 score: 0.9592 time: 0.09s
Test loss: 0.1549 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.13s
Val loss: 0.1639 score: 0.9592 time: 0.09s
Test loss: 0.1520 score: 0.9375 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.14s
Val loss: 0.1674 score: 0.9592 time: 0.09s
Test loss: 0.1598 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.14s
Val loss: 0.1723 score: 0.9592 time: 0.09s
Test loss: 0.1632 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.14s
Val loss: 0.1737 score: 0.9592 time: 0.19s
Test loss: 0.1631 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.15s
Val loss: 0.1751 score: 0.9388 time: 0.09s
Test loss: 0.1626 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.14s
Val loss: 0.1709 score: 0.9592 time: 0.09s
Test loss: 0.1574 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.14s
Val loss: 0.1710 score: 0.9592 time: 0.10s
Test loss: 0.1583 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.14s
Val loss: 0.1746 score: 0.9592 time: 0.09s
Test loss: 0.1631 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.13s
Val loss: 0.1738 score: 0.9592 time: 0.09s
Test loss: 0.1632 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.15s
Val loss: 0.1703 score: 0.9592 time: 0.09s
Test loss: 0.1578 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.14s
Val loss: 0.1725 score: 0.9592 time: 0.09s
Test loss: 0.1610 score: 0.9375 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.15s
Val loss: 0.1727 score: 0.9592 time: 0.10s
Test loss: 0.1577 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.14s
Val loss: 0.1734 score: 0.9592 time: 0.09s
Test loss: 0.1582 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.14s
Val loss: 0.1755 score: 0.9592 time: 0.09s
Test loss: 0.1619 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.14s
Val loss: 0.1778 score: 0.9592 time: 0.09s
Test loss: 0.1641 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.14s
Val loss: 0.1772 score: 0.9592 time: 0.09s
Test loss: 0.1639 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.14s
Val loss: 0.1767 score: 0.9592 time: 0.09s
Test loss: 0.1625 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.14s
Val loss: 0.1777 score: 0.9592 time: 0.09s
Test loss: 0.1664 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.14s
Val loss: 0.1783 score: 0.9592 time: 0.20s
Test loss: 0.1683 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.13s
Val loss: 0.1773 score: 0.9592 time: 0.08s
Test loss: 0.1667 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.13s
Val loss: 0.1774 score: 0.9592 time: 0.09s
Test loss: 0.1655 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 161,   Train_Loss: 0.0237,   Val_Loss: 0.1639,   Val_Precision: 1.0000,   Val_Recall: 0.9200,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1639,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9375,   Test_loss: 0.1520


[0.2135941891465336, 0.09214408788830042, 0.09436085098423064, 0.08510692813433707, 0.08713073004037142, 0.08993108198046684, 0.08891682093963027, 0.08514036214910448, 0.10019451007246971, 0.09260286879725754, 0.09034177614375949, 0.09141899202950299, 0.09240424702875316, 0.09323805896565318, 0.08973067696206272, 0.08917326899245381, 0.08307387004606426, 0.08701081899926066, 0.08922558394260705, 0.0891643378417939, 0.09143853606656194, 0.09105851012282073, 0.08602680498734117, 0.08505977084860206, 0.08808791497722268, 0.08823519293218851, 0.09534473204985261, 0.09016354591585696, 0.09045739215798676, 0.09721409203484654, 0.09323020791634917, 0.2206021868623793, 0.10376631305553019, 0.09195093088783324, 0.08393859211355448, 0.08649680204689503, 0.08701924397610128, 0.0872768119443208, 0.08916755695827305, 0.08481184486299753, 0.09709411906078458, 0.08507582382299006, 0.09009638987481594, 0.1008870939258486, 0.11207112902775407, 0.10149517795071006, 0.23612062889151275, 0.08516365592367947, 0.09975953120738268, 0.08872788702137768, 0.09022864792495966, 0.09167136810719967, 0.11678183008916676, 0.10215716692619026, 0.1956126489676535, 0.09068594500422478, 0.09095925092697144, 0.09103723894804716, 0.10353299486450851, 0.08945778594352305, 0.08758439496159554, 0.09074271004647017, 0.10078648501075804, 0.09001925401389599, 0.09031062992289662, 0.09114779299125075, 0.090903433971107, 0.09180487296544015, 0.09078593202866614, 0.09178500296548009, 0.0924446601420641, 0.0910842688754201, 0.09738172707147896, 0.09217219310812652, 0.09260898199863732, 0.0920626800507307, 0.09136405703611672, 0.08876906102523208, 0.08858553995378315, 0.09145450894720852, 0.08971925196237862, 0.08866410097107291, 0.08697885903529823, 0.0879341580439359, 0.08845911920070648, 0.10398306418210268, 0.0893523448612541, 0.0877744008321315, 0.0874137501232326, 0.08899673284031451, 0.08999751694500446, 0.09410237101837993, 0.09207112505100667, 0.09194637089967728, 0.10971475997939706, 0.09203088516369462, 0.09201181400567293, 0.0921189240179956, 0.11399492505006492, 0.08591836201958358, 0.09165044198743999, 0.2642267090268433, 0.09379065223038197, 0.09035956207662821, 0.09125912399031222, 0.0891556041315198, 0.08996579213999212, 0.0901355859823525, 0.09106242912821472, 0.08906117407605052, 0.088781813159585, 0.09045314299874008, 0.09062883094884455, 0.08836075803264976, 0.08905764808878303, 0.08740788092836738, 0.08622875111177564, 0.08872182085178792, 0.08823371818289161, 0.08866132702678442, 0.0859353260602802, 0.0886246629524976, 0.09120388492010534, 0.08848224091343582, 0.0914007150568068, 0.09038051404058933, 0.09138604300096631, 0.08963072882033885, 0.0835422680247575, 0.1897975387983024, 0.08780545298941433, 0.08801006805151701, 0.08330072509124875, 0.08582389098592103, 0.08371770894154906, 0.08453939110040665, 0.08301667310297489, 0.18688556784763932, 0.08322318200953305, 0.08398510701954365, 0.08362322999164462, 0.08506592595949769, 0.08471717895008624, 0.08691698010079563, 0.0867964739445597, 3.2059317571111023, 0.09026755602099001, 0.08513317885808647, 0.08699018601328135, 0.08692675898782909, 0.08328140783123672, 0.1897415709681809, 0.09188266913406551, 0.08727464801631868, 0.0871808270458132, 0.08912333589978516, 0.09186836797744036, 0.08876875904388726, 0.08893432910554111, 0.08621299685910344, 0.08676721900701523, 0.0867406609468162, 0.08964907005429268, 0.08806596789509058, 0.08704129699617624, 0.08990840404294431, 0.0867208659183234, 0.21097653498873115, 0.08636145712807775, 0.08248711004853249, 0.08377858786843717, 0.09522142400965095, 0.08591939997859299, 0.09091826295480132, 0.08961707609705627, 0.2200973650906235, 0.08461625105701387, 0.0825939429923892, 0.08336602291092277, 0.08450167998671532, 0.08508054492995143, 0.08601560909301043, 0.08459465391933918, 0.1921884089242667, 0.08631638600490987, 0.08467529504559934, 0.08398154890164733, 0.08981377794407308, 0.0842126349452883, 0.08757091197185218, 0.08485263283364475, 0.22260588803328574, 0.0815732148475945, 0.08281856402754784, 0.08433097298257053, 0.0847642500884831, 0.08391227503307164, 0.08807893702760339, 0.08547589997760952, 0.08495455491356552, 0.09118189802393317, 0.08972287294454873, 0.08865553583018482, 0.09099534107372165, 0.08927050489000976, 0.0909909950569272, 0.08949862094596028, 0.21659167297184467, 0.08811767981387675, 0.09033615910448134, 0.09180692699737847, 0.08587196096777916, 0.08577988808974624, 0.08783271489664912, 0.09800068801268935, 0.09141327813267708, 0.09182012197561562, 0.09370028297416866, 0.09256998379714787, 0.09132830193266273, 0.09122697985731065, 0.08575646206736565, 0.09185927198268473, 0.09084170311689377, 0.09121716418303549, 0.09378380514681339, 0.09195919195190072, 0.09341589920222759, 0.0892265981528908, 0.09211079915985465, 0.0904333449434489, 0.0899563180282712, 0.09274207893759012, 0.09254855196923018, 0.08961033704690635, 0.09073073719628155, 0.09087232500314713, 0.09134464804083109, 0.09056929918006063, 0.09363429108634591, 0.09106827504001558, 0.08410309301689267, 0.08848976087756455, 0.08745169802568853, 0.09345672582276165, 0.09297298779711127, 0.09845311916433275, 0.0852180770598352, 0.08465267112478614, 0.09785462496802211, 0.08650915208272636, 0.0842873309738934, 0.08552998187951744, 0.09156275191344321, 0.1006992431357503, 0.08942836406640708, 0.08971804101020098, 0.09173612506128848, 0.09170051082037389, 0.09246320091187954, 0.08446094510145485, 0.08441164111718535, 0.08585918508470058, 0.08529632305726409, 0.09747973107732832, 0.08960936008952558, 0.08652628399431705, 0.0848455389495939, 0.08754171500913799, 0.08806771389208734, 0.1051397961564362, 0.08934234804473817, 0.08754319278523326, 0.09165462688542902, 0.11022772896103561, 0.2068454520776868, 0.08766055596061051, 0.08464787295088172, 0.08887969190254807, 0.08852774393744767, 0.1020104200579226, 0.09102718019858003, 0.089943733997643, 0.09370374493300915, 0.08891586819663644, 0.08888817415572703, 0.09004339296370745, 0.09133117785677314, 0.09036775398999453, 0.103517462965101, 0.08436593087390065, 0.08631683513522148, 0.08564235479570925, 0.09005191712640226, 0.08659853902645409, 0.08798746694810688, 0.08587566786445677, 0.2272084781434387, 0.09340476593934, 0.09426429402083158, 0.09261739091016352, 0.09488877607509494, 0.09342541010119021, 0.09252547495998442, 0.09575245901942253, 0.0951496809720993, 0.09263342316262424, 0.09122346201911569, 0.09259261190891266, 0.09403687412850559, 0.08967565186321735, 0.08461574814282358, 0.08508491003885865, 0.20969060994684696, 0.0861392819788307, 0.08469193312339485, 0.08562674699351192, 0.0841122071724385, 0.08524556900374591, 0.08726196992211044, 0.087464485084638, 0.09431528695859015, 0.09422383294440806, 0.0912144840694964, 0.09266044688411057, 0.09434278402477503, 0.08806231897324324, 0.08554552681744099, 0.08754335390403867, 0.09049902600236237, 0.09183505503460765, 0.09006226202473044, 0.10212200204841793, 0.09150462900288403, 0.0902151248883456, 0.09301866893656552, 0.11699911602772772, 0.10060007800348103, 0.08887090184725821, 0.08989766519516706, 0.08733155601657927, 0.08711476600728929, 0.0855160029605031, 0.3538832440972328, 0.08978812699206173, 0.0894116759300232, 0.08764563291333616, 0.09185615996830165, 0.09261512197554111, 0.08587002800777555, 0.21628849091939628, 0.08941716607660055, 0.10211513889953494, 0.086557931965217, 0.0890719520393759, 0.09174216794781387, 0.0897476370446384, 0.09745783288963139, 0.08411588100716472, 0.08411416597664356, 0.08376197097823024, 0.08751470409333706, 0.09108872804790735, 0.08925767708569765, 0.08272364595904946, 0.08425851305946708, 0.09343592100776732, 0.08950078999623656, 0.09126100689172745, 0.0901307980529964, 0.09152810694649816, 0.09408437693491578, 0.08985023782588542, 0.09981696889735758, 0.08313989010639489, 0.08186119492165744, 0.08250791393220425, 0.0823823120445013, 0.08530342602171004, 0.08203415595926344, 0.08434204710647464, 0.08334691799245775, 0.08271224796772003, 0.08399714599363506, 0.08402677183039486, 0.08313891687430441, 0.08361106785014272, 0.0847735630813986, 0.08448694087564945, 0.0835450200829655, 0.08510922105051577, 0.08365692291408777, 0.08402222511358559, 0.08485256391577423, 0.08409705804660916, 0.08253874816000462, 0.08413508185185492, 0.08511833706870675, 0.08397437492385507, 0.08660887088626623, 0.08310009492561221, 0.08237856393679976, 0.08476001117378473, 0.08536793617531657, 0.08359938091598451, 0.08300172304734588, 0.08467296906746924, 0.08328933105804026, 0.08305981988087296, 0.07659385609440506, 0.07952031097374856, 0.09617682616226375, 0.08357861801050603, 0.08288326091133058, 0.08168175700120628, 0.08758388389833272, 0.08210800099186599, 0.08230414590798318, 0.08056268701329827, 0.08293121703900397, 0.09047753014601767, 0.09246186981908977, 0.08647741586901248, 0.08548864210024476, 0.08537407009862363, 0.08918948099017143, 0.08749546320177615, 0.17446956713683903, 0.08438183693215251, 0.08367017307318747, 0.08313393406569958, 0.08271708409301937, 0.08088392112404108, 0.0824615799356252, 0.08580520702525973, 0.18449454591609538, 0.09748294507153332, 0.09388748183846474, 0.08976176986470819, 0.08491648081690073, 0.0852297421079129, 0.08356391591951251, 0.0967357971239835, 0.08798843482509255, 0.10187092702835798, 0.08432394987903535, 0.08209848497062922, 0.08186713722534478, 0.08121598395518959, 0.08190852007828653, 0.09952935599721968, 0.08951203594915569, 0.09291325393132865, 0.09294479992240667, 0.0933887620922178, 0.09376027691178024, 0.23360799299553037, 0.08557935198768973, 0.08943367912434042, 0.08212045882828534, 0.0843171130400151, 0.08490679110400379, 0.08671044884249568, 0.08666996494866908, 0.08753674104809761, 0.09006978594698012, 0.08853270579129457, 0.08689470798708498, 0.08691110415384173, 0.08799710706807673, 0.0839428249746561, 0.08899162895977497, 0.08526215213350952, 0.08752413094043732, 0.0874140930827707, 0.08475571009330451, 0.08071470819413662, 0.08392868097871542, 0.08775008795782924, 0.08215664001181722, 0.08595102606341243, 0.08316045883111656, 0.09990307409316301, 0.08143421309068799, 0.10429301089607179, 0.0844224898610264, 0.08544849790632725, 0.08351498492993414, 0.08297595009207726, 0.08245836803689599, 0.08354722894728184, 0.07918269094079733, 0.07964134099893272, 0.09379888908006251, 0.08230153191834688, 0.08890546998009086, 0.0833393638022244, 0.08315848396159708, 0.08233389793895185, 0.08095416682772338, 0.07905029808171093, 0.08813789393752813, 0.07917206385172904, 0.09545667190104723, 0.09463881189003587, 0.17692453297786415, 0.08012316492386162, 0.07775323814712465, 0.07807313883677125, 0.08055677893571556, 0.08367004105821252, 0.07956733903847635, 0.08023333689197898, 0.09850538382306695, 0.0767294408287853, 0.07749624084681273, 0.07912588096223772, 0.0794245470315218, 0.07976852590218186, 0.09692234406247735, 0.0891814490314573, 0.20821041893213987, 0.08469644002616405, 0.08260650211013854, 0.08261827891692519, 0.08533716783858836, 0.08393645216710865, 0.08372807805426419, 0.08730867179110646, 0.08384256204590201, 0.20788247208110988, 0.08712984807789326, 0.08300084294751287, 0.08377502905204892, 0.08543955883942544, 0.0794430470559746, 0.07987019303254783, 0.0808646259829402, 0.09104704903438687, 0.08392060385085642, 0.08320362004451454, 0.093606511130929, 0.07873422210104764, 0.09014888689853251, 0.08340710122138262, 0.21339131612330675, 0.09954487206414342, 0.08352064993232489, 0.08192693791352212, 0.08430237392894924, 0.08252745703794062, 0.08440903201699257, 0.08303818106651306, 0.0773159028030932, 0.07975258608348668, 0.08833061694167554]
[0.0043590650846231345, 0.0018804915895571513, 0.0019257316527394007, 0.001736876084374226, 0.0017781781640892125, 0.0018353282036829967, 0.0018146289987679646, 0.0017375584112062138, 0.0020447859198463206, 0.001889854465250154, 0.0018437097172195815, 0.0018656937148878161, 0.0018858009597704727, 0.0019028175299112893, 0.0018312383053482187, 0.0018198626324990575, 0.0016953851029809033, 0.0017757309999849116, 0.0018209302845430008, 0.0018196803641182427, 0.0018660925727869784, 0.001858336941282056, 0.0017556490813743096, 0.0017359136907877971, 0.0017977125505555648, 0.001800718223105888, 0.0019458108581602573, 0.0018400723656297338, 0.0018460692277140155, 0.001983961061935644, 0.0019026573044152893, 0.004502085446171007, 0.0021176798582761263, 0.0018765496099557803, 0.0017130324921133568, 0.0017652408580998986, 0.0017759029382877812, 0.0017811594274351184, 0.0018197460603729195, 0.001730853976795868, 0.001981512633893563, 0.0017362413025100011, 0.001838701834179917, 0.002058920284200992, 0.002287165898525593, 0.002071330162259389, 0.00481878834472475, 0.0017380337943608056, 0.002035908800150667, 0.0018107732045179118, 0.0018414009780604013, 0.0018708442470857076, 0.0023833026548809545, 0.0020848401413508214, 0.003992094876890888, 0.0018507335715147915, 0.0018563112434075804, 0.0018579028356744318, 0.00211291826254099, 0.0018256691008882255, 0.0017874366318692966, 0.0018518920417646973, 0.0020568670410358782, 0.0018371276329366528, 0.0018430740800591148, 0.0018601590406377704, 0.0018551721218593267, 0.0018735688360293908, 0.0018527741230340029, 0.0018731633258261243, 0.0018866257171849816, 0.001858862630110614, 0.0019873821851322236, 0.0018810651654719698, 0.001889979224461986, 0.001878830205116953, 0.0018645725925738107, 0.0018116134903108586, 0.0018078681623221052, 0.0018664185499430311, 0.0018310051420893595, 0.001809471448389243, 0.0017750787558224127, 0.0017945746539578754, 0.0018052881469531935, 0.0021221033506551566, 0.00182351724206641, 0.0017913143026965614, 0.0017839540841476042, 0.0018162598538839696, 0.0018366840192858055, 0.0019204565513955088, 0.0018790025520613607, 0.0018764565489730056, 0.0022390767342734094, 0.0018781813298713189, 0.0018777921225647538, 0.0018799780411835835, 0.0023264270418380598, 0.0017534359595833384, 0.0018704171834171427, 0.005392381816874354, 0.001914094943477183, 0.001844072695441392, 0.0018624311018431065, 0.001819502125133057, 0.0018360365742855534, 0.0018395017547418876, 0.0018584169209839739, 0.0018175749811438881, 0.0018118737379507143, 0.0018459825101783689, 0.001849567978547848, 0.0018032807761765256, 0.0018175030222200618, 0.0017838343046605587, 0.001759770430852564, 0.0018106494051385292, 0.0018006881261814613, 0.0018094148372813147, 0.0017537821644955144, 0.001808666590867298, 0.0018613037738797007, 0.0018057600186415473, 0.0018653207154450367, 0.0018445002865426394, 0.0018650212857340062, 0.001829198547353854, 0.0017049442454032144, 0.003873419159149029, 0.0017919480201921292, 0.0017961238377860614, 0.0017000147977805867, 0.0017515079793045108, 0.0017085246722765115, 0.0017252936959266663, 0.001694217818428059, 0.0038139911805640677, 0.0016984322859088378, 0.001713981775909054, 0.001706596530441727, 0.0017360393052958713, 0.001728922019389515, 0.0017738159204244005, 0.0017713566111134632, 0.06542717871655311, 0.0018421950208365309, 0.001737411813430336, 0.0017753099186383948, 0.0017740154895475324, 0.001699620567984423, 0.003872276958534304, 0.0018751565129401125, 0.0017811152656391567, 0.0017792005519553715, 0.0018188435897915339, 0.0018748646526008236, 0.0018116073274262706, 0.001814986308276349, 0.001759448915491907, 0.0017707595715717394, 0.001770217570343188, 0.001829572858250871, 0.001797264650920216, 0.0017763529999219642, 0.0018348653886315165, 0.0017698135901698653, 0.004305643571198595, 0.0017624787168995459, 0.0016834104091537241, 0.0017097670993558606, 0.001943294367543897, 0.0017534571424202652, 0.001855474754179619, 0.001828919920348087, 0.004491782961033133, 0.0017268622664696708, 0.0016855906733140654, 0.0017013474063453625, 0.0017245240813615371, 0.0017363376516316617, 0.0017554205937349067, 0.0017264215085579424, 0.003922212427025851, 0.0017615588980593852, 0.001728067245828558, 0.0017139091612581089, 0.0018329342437565935, 0.0017186252029650674, 0.0017871614688133098, 0.001731686384360097, 0.00454297730680175, 0.001664759486685602, 0.001690174776072405, 0.0017210402649504189, 0.0017298826548670018, 0.0017124954088381967, 0.0017975293270939467, 0.0017444061219920308, 0.0017337664268074595, 0.001860855061712922, 0.0018310790396846679, 0.0018092966495956086, 0.0018570477770147274, 0.0018218470385716278, 0.0018569590827944328, 0.0018265024682849037, 0.004420238223915197, 0.0017983199962015664, 0.0018435950837649253, 0.0018736107550485401, 0.001752488999342432, 0.0017506099610152294, 0.0017925043856459005, 0.002000014041075293, 0.0018655771047485117, 0.0018738800403186862, 0.0019122506729422175, 0.0018891833427989362, 0.0018638428965849535, 0.001861775099128789, 0.0017501318789258295, 0.0018746790200547905, 0.001853912308508036, 0.0018615747792456224, 0.0019139552070778242, 0.0018767182031000148, 0.0019064469224944407, 0.0018209509827120571, 0.0018798122277521357, 0.0018455784682336511, 0.001835843225066759, 0.0018926954885222474, 0.001888745958555718, 0.0018287823887123745, 0.001851647697883297, 0.0018545372449621862, 0.0018641764906292058, 0.001848353044491033, 0.0019109038997213452, 0.0018585362253064404, 0.0017163896534059728, 0.0018059134872972357, 0.0017847285311365006, 0.0019072801188318705, 0.0018974079142267607, 0.0020092473298843417, 0.001739144429792555, 0.0017276055331589008, 0.001997033162612696, 0.0017654928996474768, 0.0017201496117121102, 0.001745509834275866, 0.0018686275900702696, 0.002055086594607149, 0.001825068654416471, 0.0018309804287796117, 0.0018721658175773158, 0.001871438996334161, 0.0018870041002424396, 0.001723692757172548, 0.001722686553411946, 0.0017522282670347058, 0.0017407412868829407, 0.0019893822668842517, 0.0018287624508066445, 0.0017658425304962664, 0.001731541611216202, 0.0017865656124313874, 0.0017973002835119866, 0.002145710125641555, 0.00182331322540282, 0.0017865957711272094, 0.0018705025894985516, 0.0022495454890007266, 0.004221335756687486, 0.0017889909379716431, 0.001727507611242484, 0.0018138712633173077, 0.0018066886517846463, 0.002081845307304543, 0.0018576975550730617, 0.0018355864081151631, 0.001912321325163452, 0.0018146095550333966, 0.0018140443705250413, 0.0018376202645654582, 0.0018639015889137375, 0.001844239877346827, 0.0021126012850020614, 0.001721753691304095, 0.0017615680639841119, 0.0017478031590961072, 0.0018377942270694338, 0.001767317122988859, 0.0017956625907776915, 0.0017525646502950362, 0.004636907717213035, 0.001906219713047755, 0.0019237611024659506, 0.0018901508349012963, 0.001936505634185611, 0.0019066410224732695, 0.0018882749991833555, 0.001954131816722909, 0.001941830223920394, 0.0018904780237270252, 0.001861703306512565, 0.0018896451409982176, 0.0019191198801735835, 0.001830115344147293, 0.0017268520029147668, 0.0017364267354869113, 0.004279400202996877, 0.0017579445301802183, 0.0017284067984366296, 0.0017474846325206514, 0.0017165756565803777, 0.0017397054898723656, 0.001780856529022662, 0.0017849894915232245, 0.0019248017746651051, 0.0019229353662124093, 0.0018615200830509467, 0.0018910295282471546, 0.001925362939281123, 0.001797190183127413, 0.0017458270779069588, 0.0017865990592660953, 0.0018469188980073954, 0.001874184796624646, 0.0018380053474434785, 0.002084122490784039, 0.001867441408222123, 0.0018411249977213387, 0.0018983401823788881, 0.002387737061790362, 0.002053062816397572, 0.001813691874433841, 0.0018346462284727972, 0.001782276653399577, 0.0017778523674956998, 0.001745224550214349, 0.007222107022392506, 0.0018324107549400354, 0.0018247280802045549, 0.0017886863859864523, 0.0018746155095571767, 0.001890104530113084, 0.0017524495511790927, 0.00441405083508972, 0.001824840124012256, 0.0020839824265211212, 0.0017664884074534081, 0.0018177949395791, 0.0018722891417921198, 0.0018315844294824162, 0.0019889353650945183, 0.00171665063279928, 0.0017166156321763992, 0.001709427979147556, 0.0017860143692517768, 0.0018589536336307622, 0.0018215852466468908, 0.0016882376726336625, 0.0017195614910095322, 0.0019068555307707616, 0.0018265467346170728, 0.001862469528402601, 0.0018394040418978855, 0.0018679205499285338, 0.001920089325202363, 0.0018336783229772533, 0.002037080997905257, 0.0017320810438832268, 0.0017054415608678635, 0.0017189148735875885, 0.0017162981675937772, 0.0017771547087856259, 0.0017090449158179883, 0.0017571259813848883, 0.0017363941248428698, 0.001723171832660834, 0.0017499405415340636, 0.0017505577464665596, 0.0017320607682146754, 0.0017418972468779732, 0.0017661158975291376, 0.0017601446015760303, 0.0017405212517284478, 0.0017731087718857452, 0.001742852560710162, 0.0017504630231996998, 0.0017677617482452963, 0.001752022042637691, 0.0017195572533334296, 0.0017528142052469775, 0.0017732986889313906, 0.0017494661442469805, 0.001804351476797213, 0.001731251977616921, 0.0017162200820166618, 0.0017658335661205153, 0.0017784986703190953, 0.0017416537690830107, 0.0017292025634863724, 0.001764020188905609, 0.0017351943970425054, 0.0017304129141848534, 0.0015957053353001054, 0.0016566731452864285, 0.0020036838783804947, 0.001741221208552209, 0.001726734602319387, 0.001701703270858464, 0.0018246642478819315, 0.0017105833539972082, 0.0017146697064163163, 0.0016783893127770473, 0.0017277336883125827, 0.0018849485447087015, 0.0019262889545643702, 0.0018016128306044266, 0.0017810133770884324, 0.0017786264603879924, 0.0018581141872952382, 0.001822822150037003, 0.0036347826486841464, 0.0017579549360865105, 0.0017431286056914057, 0.0017319569597020745, 0.0017232725852712367, 0.0016850816900841892, 0.0017179495819921915, 0.001787608479692911, 0.003843636373251987, 0.0020308946889902777, 0.001955989204968015, 0.0018700368721814205, 0.0017690933503520985, 0.0017756196272481854, 0.001740914914989844, 0.002015329106749656, 0.0018330923921894282, 0.002122310979757458, 0.0017567489558132365, 0.0017103851035547752, 0.0017055653588613495, 0.0016919996657331164, 0.0017064275016309693, 0.002073528249942077, 0.0018648340822740768, 0.0019356927902360137, 0.0019363499983834724, 0.0019455992102545376, 0.001953339102328755, 0.004866833187406883, 0.0017829031664102029, 0.0018632016484237586, 0.0017108428922559444, 0.0017566065216669813, 0.0017688914813334122, 0.00180646768421866, 0.001805624269763939, 0.0018236821051687002, 0.001876453873895419, 0.0018444313706519704, 0.001810306416397604, 0.001810648003205036, 0.0018332730639182653, 0.0017488088536386688, 0.001853992269995312, 0.0017762948361147817, 0.0018234193945924442, 0.0018211269392243896, 0.0017657439602771774, 0.0016815564207111795, 0.0017485141870565712, 0.0018281268324547757, 0.0017115966669128586, 0.0017906463763210922, 0.0017325095589815949, 0.0020813140436075628, 0.0016965461060559999, 0.002172771060334829, 0.0017588018721047167, 0.001780177039715151, 0.001739895519373628, 0.001728665626918276, 0.0017178826674353331, 0.0017405672697350383, 0.0016496393945999444, 0.0016591946041444317, 0.0019541435225013024, 0.0017146152482988934, 0.001852197291251893, 0.001736236745879675, 0.001732468415866606, 0.0017152895403948303, 0.0016865451422442372, 0.0016468812100356445, 0.0018362061236985028, 0.0016494179969110216, 0.0019886806646051505, 0.001971641914375747, 0.0036859277703721696, 0.0016692326025804505, 0.001619859128065097, 0.0016265237257660676, 0.0016782662278274074, 0.0017431258553794275, 0.001657652896634924, 0.0016715278519162287, 0.0020521954963138946, 0.0015985300172663603, 0.0016145050176419318, 0.0016484558533799525, 0.0016546780631567042, 0.0016618442896287888, 0.0020192155013016113, 0.0018579468548220273, 0.004337717061086248, 0.0017645091672117512, 0.0017209687939612195, 0.001721214144102608, 0.001777857663303924, 0.0017486760868147637, 0.0017443349594638373, 0.001818930662314718, 0.0017467200426229585, 0.004330884835023123, 0.0018152051682894428, 0.0017291842280731846, 0.0017453131052510191, 0.0017799908091546968, 0.0016550634803328042, 0.0016639623548447464, 0.0016846797079779208, 0.0018968135215497266, 0.0017483459135595087, 0.0017334087509273861, 0.0019501356485610206, 0.0016402962937718257, 0.001878101810386094, 0.0017376479421121378, 0.004445652419235557, 0.0020738515013363212, 0.0017400135402567685, 0.001706811206531711, 0.0017562994568531092, 0.001719322021623763, 0.0017585215003540118, 0.0017299621055523555, 0.0016107479750644416, 0.0016615122100726392, 0.0018402211862849072]
[229.40698993634217, 531.7758428451659, 519.2831506806649, 575.746312011825, 562.3733437938164, 544.861675417659, 551.0768320571016, 575.5202205293343, 489.04875092017295, 529.1412743084602, 542.3847315335826, 535.9936585626167, 530.2786568322213, 525.5364659409148, 546.0785726682608, 549.4920232670462, 589.8364909788074, 563.1483597507151, 549.1698438367014, 549.5470631648911, 535.8790954869493, 538.1155471784928, 569.5899087175252, 576.0655067742322, 556.2624568043207, 555.333970172854, 513.9245655898379, 543.4568871739817, 541.6914950899746, 504.042150416175, 525.580722119222, 222.11928493060773, 472.21490826004214, 532.8929193742788, 583.7600889673182, 566.4949320719882, 563.0938371914289, 561.4320563319853, 549.5272234825285, 577.7494886375023, 504.6649629657194, 575.9568088573562, 543.8619690320871, 485.69146055505075, 437.22232857906965, 482.7815566153919, 207.52104646694545, 575.3628055130935, 491.1811373505509, 552.2502749129388, 543.0647707450051, 534.5180399478695, 419.585820521796, 479.6530823471551, 250.4950485492512, 540.32628758202, 538.702765256287, 538.2412797906048, 473.27907459960267, 547.7443856137344, 559.4603927044969, 539.9882808757492, 486.1762963037127, 544.3279944581192, 542.5717885240543, 537.5884417157911, 539.0335420725062, 533.7407309353393, 539.7311995929941, 533.8562773531605, 530.0468401820006, 537.9633673847613, 503.1744812251441, 531.6136933241727, 529.1063452216871, 532.2460737944929, 536.3159385602812, 551.9941231108893, 553.1376794177049, 535.785502148231, 546.1481112275305, 552.6475705876326, 563.3552859105056, 557.2351073801989, 553.9281923983781, 471.2305834168115, 548.3907565726112, 558.2493248084082, 560.5525438609103, 550.5820094308401, 544.4594658088494, 520.7095152834077, 532.1972548163649, 532.9193476647808, 446.6126527479214, 532.4299544967331, 532.5403104972904, 531.9211065733656, 429.84369680036116, 570.308823960486, 534.6400839694269, 185.446808842561, 522.4401241995761, 542.2779711841256, 536.9326140496559, 549.6008969634327, 544.6514595653545, 543.6254667451059, 538.0923885855125, 550.1836294922213, 551.9148376922948, 541.716941783687, 540.6667998140465, 554.5448125500943, 550.2054124666654, 560.5901833972677, 568.255939790695, 552.2880338745049, 555.3432520936313, 552.6648612556543, 570.1962422953799, 552.8934990281855, 537.2578157490115, 553.7834428033735, 536.1008387029119, 542.1522605856657, 536.1869098488254, 546.6875104655069, 586.529443819732, 258.1698388200504, 558.05190146798, 556.7544837179046, 588.2301738229137, 570.9365939612106, 585.3002981031332, 579.6114611448189, 590.2428773460941, 262.19253077876954, 588.7782564524767, 583.4367751486882, 585.9615803514877, 576.0238244315391, 578.3950859467343, 563.7563562744107, 564.5390621662606, 15.284168133433507, 542.8306931075655, 575.568781258374, 563.2819315102839, 563.6929361056779, 588.3666147826737, 258.245990849402, 533.288817812904, 561.4459767381467, 562.0501853492475, 549.7998869240948, 533.3718349283474, 551.9960009328779, 550.9683436398356, 568.3597808353645, 564.7294054225513, 564.9023129999396, 546.5756640902682, 556.4010839961666, 562.9511702031806, 544.999107943184, 565.0312584072884, 232.25331671418922, 567.3827379652812, 594.0322066219818, 584.874981146111, 514.5900779118124, 570.3019342803658, 538.9456244270706, 546.7707956342211, 222.62874423700896, 579.0849793969734, 593.2638426587191, 587.7694327862669, 579.8701281170196, 575.924849098495, 569.6640472197936, 579.2328206309751, 254.95814380412932, 567.6790035812293, 578.6811840881395, 583.4614941120578, 545.5733087022821, 581.8604302291999, 559.5465308817386, 577.4717691561258, 220.11996373012894, 600.687371357719, 591.6547886981135, 581.0439304444797, 578.0738925767671, 583.9431713737715, 556.3191570380068, 573.2610012042645, 576.7789619974302, 537.3873659346136, 546.1260701079354, 552.7009626771306, 538.489107484104, 548.8935013907777, 538.5148274215911, 547.4944695470385, 226.23215069939295, 556.0745596513481, 542.4184566373626, 533.7287893472263, 570.6169912480019, 571.2294698814984, 557.8786908460845, 499.99648975582056, 536.0271614904947, 533.6520900398365, 522.9439916793884, 529.3292489644977, 536.525906680365, 537.1218040610526, 571.3855121670977, 533.4246499279506, 539.3998386065871, 537.1796025326666, 522.4782671517028, 532.8450474600676, 524.5359774777135, 549.1636015982357, 531.9680259744836, 541.8355367772959, 544.708821726123, 528.3470088369927, 529.4518277961944, 546.8119149507388, 540.0595378608715, 539.2180732506075, 536.4298954668585, 541.0221834949082, 523.3125538891953, 538.0578470215813, 582.6182871794979, 553.7363816339945, 560.309303378037, 524.3068336561167, 527.0347996875116, 497.69880747213085, 574.9953729370711, 578.835840014656, 500.7428112469155, 566.4140593256841, 581.3447814022837, 572.8985195977743, 535.1521112681394, 486.5975003798613, 547.9245931818, 546.1554827576839, 534.1407211964025, 534.3481684195073, 529.9405549100404, 580.1497951643921, 580.4886547813379, 570.7018992978004, 574.4679048720965, 502.66860052301007, 546.8178765147504, 566.3019112576043, 577.5200512204954, 559.7331511598233, 556.390052999917, 466.0461765314202, 548.4521178631129, 559.7237025637165, 534.6156726081209, 444.5342425345714, 236.89184126512308, 558.9743238910978, 578.8686507035213, 551.3070415874746, 553.4987995923926, 480.343086247241, 538.3007569069395, 544.7850319543556, 522.9246711007246, 551.0827369040254, 551.2544324980147, 544.1820703019239, 536.5090120357639, 542.2288132271746, 473.3500860286679, 580.8031689146998, 567.6760497907275, 572.1468088644258, 544.130558944355, 565.8294071800854, 556.8974957410599, 570.5923600773613, 215.66096652901248, 524.5984988798339, 519.8150636886054, 529.0583066362638, 516.3940565659885, 524.482578636021, 529.5838796957444, 511.73620502070645, 514.9780797937541, 528.9667414533217, 537.1425170175207, 529.1998896003, 521.0721906072645, 546.4136472042591, 579.08842119191, 575.8953024410731, 233.67760727302323, 568.8461625677595, 578.5674997949067, 572.2510981727802, 582.5551563466295, 574.8099352571254, 561.5275479540186, 560.2273877515368, 519.5340180803757, 520.0382797939236, 537.1953862356647, 528.8124722869487, 519.3825951450858, 556.4241388520339, 572.7944151255131, 559.7226724225318, 541.442291309532, 533.5653142640853, 544.0680580124109, 479.81824697060085, 535.4920350363436, 543.1461748863582, 526.775974760677, 418.8065830205713, 487.0771571201413, 551.3615703395916, 545.0642115523403, 561.080120806478, 562.4764003372281, 572.9921687597047, 138.46374706154998, 545.7291697858019, 548.0268599187109, 559.0694980598875, 533.4427219351348, 529.0712677886501, 570.6298360070762, 226.54927126132065, 547.9932114827193, 479.8504955098598, 566.0948556359974, 550.1170556848091, 534.1055383373206, 545.9753773308654, 502.7815471280928, 582.529712740289, 582.5415901241427, 584.9910099743846, 559.9059096142292, 537.9370318381091, 548.9723864643526, 592.3336602482002, 581.5436116872523, 524.4235779077582, 547.4812010269452, 536.92153603054, 543.6543452237967, 535.3546755713244, 520.8091034486678, 545.3519232186533, 490.8984969317893, 577.3401905941174, 586.3584088399493, 581.7623754182056, 582.6493431511286, 562.6972120414463, 585.122129175509, 569.1111568516246, 575.9061181403688, 580.325177702012, 571.4479870975287, 571.2465081591655, 577.3469489934535, 574.0866757739666, 566.2142566062836, 568.1351402064364, 574.5405285956358, 563.9811927254046, 573.7720003076616, 571.2774201719975, 565.6870904648848, 570.769074625618, 581.545044843061, 570.5111226315607, 563.9207913713685, 571.6029448688921, 554.2157461333613, 577.6166687049831, 582.6758528690212, 566.3047861282707, 562.2719975498107, 574.1669313106387, 578.301247705663, 566.8869360392048, 576.3043044078617, 577.8967504244907, 626.6821184826892, 603.6193698468543, 499.080723656001, 574.3095679563197, 579.1277933834061, 587.6465169485903, 548.0460315703555, 584.5958910235205, 583.2026986060272, 595.8093228950614, 578.792904696247, 530.5184604678634, 519.1329149401419, 555.0582139584941, 561.4780960459608, 562.2315996478869, 538.1800574138282, 548.5998729934789, 275.11961419811917, 568.8427953825485, 573.6811367416884, 577.3815535069743, 580.2912484925324, 593.4430395181842, 582.0892594766178, 559.4066102057133, 260.17029263200817, 492.3938229890104, 511.25026531848994, 534.7488142485062, 565.2612960197789, 563.1836822787193, 574.4106109894715, 496.19687258564454, 545.526239845232, 471.18448216965925, 569.2332969323319, 584.6636514324476, 586.3158481757679, 591.0166652229899, 586.0196223069659, 482.26977376745873, 536.2407355728652, 516.6109028479012, 516.4355621839194, 513.9804717895485, 511.94388051097127, 205.47242148909856, 560.8829569883237, 536.7105599364328, 584.5072066678105, 569.2794531190866, 565.3258046368044, 553.5665037000225, 553.8250768698055, 548.3411813746425, 532.9201073960069, 542.172517726436, 552.3926728326674, 552.2884614954953, 545.4724774402642, 571.8177820974228, 539.3765746404804, 562.9696037327113, 548.4201840594725, 549.1105416440085, 566.3335242800576, 594.6871527373852, 571.9141471098891, 547.0079987050012, 584.2497939679105, 558.4575565693287, 577.1973925430003, 480.4656957326295, 589.4328461987534, 460.24177063822714, 568.5688739933644, 561.7418816726293, 574.7471551395256, 578.4808724303256, 582.1119328789335, 574.525338599655, 606.1930887886627, 602.7020564689293, 511.73313960071914, 583.2212217826253, 539.8992886573676, 575.9583204151943, 577.2110999782847, 582.9919535157995, 592.9280959947083, 607.2083365249862, 544.6011681879107, 606.2744567312644, 502.845940928655, 507.1914898485082, 271.3021150436243, 599.0776830347728, 617.3376330536153, 614.8081237050602, 595.8530198719102, 573.68204189842, 603.2626022190922, 598.2550627879796, 487.28301070545007, 625.5747400415388, 619.3848820987573, 606.6283170092939, 604.3471671415374, 601.7410934591068, 495.24183989048595, 538.2285275839013, 230.53601374119611, 566.7298411264045, 581.0680609136798, 580.9852326779312, 562.4747248559968, 571.8611969021164, 573.2843881701332, 549.7735679090863, 572.5015890344695, 230.89969788925617, 550.9019131662947, 578.3073797256939, 572.9630958430093, 561.80065360837, 604.2064318879887, 600.9751344965395, 593.5846412017838, 527.1999533106366, 571.9691922773284, 576.8979760053668, 512.7848417815893, 609.6459546954908, 532.4524977665739, 575.4905673150826, 224.93886289291885, 482.1946023404439, 574.7081714389608, 585.8878803778355, 569.378983804831, 581.6246098305537, 568.6595243781142, 578.0473438062462, 620.8295869252871, 601.8613609564032, 543.412937234371]
Elapsed: 0.0997810506583493~0.13473792460185308
Time per graph: 0.0020488194821629804~0.0027494192927691005
Speed: 536.5684643810931~76.59769439103636
Total Time: 0.0910
best val loss: 0.16394956409931183 test_score: 0.9375

Testing...
Test loss: 0.2461 score: 0.9375 time: 0.07s
test Score 0.9375
Epoch Time List: [0.4336077037733048, 0.2944593238644302, 0.31582182995043695, 0.29263413907028735, 0.30103284190408885, 0.2895073061808944, 0.30209682881832123, 0.2975220929365605, 0.30395884974859655, 0.4059344017878175, 0.30219995020888746, 0.2989963530562818, 0.305264457128942, 0.31163162691518664, 0.309203227981925, 0.30081028677523136, 0.4127152687869966, 0.28759340615943074, 0.30536343017593026, 0.2976354949641973, 0.30571971694007516, 0.3065792159177363, 0.2968894769437611, 0.2922814358025789, 0.3674967128317803, 0.29558586422353983, 0.309013371123001, 0.31331815803423524, 0.31581354117952287, 0.32013092702254653, 0.3121164459735155, 0.43614139989949763, 0.3086449848487973, 0.3113256108481437, 0.2861402190756053, 0.28759698709473014, 0.30035373801365495, 0.2863979700487107, 0.31223518773913383, 0.2912184961605817, 0.43546973215416074, 0.28766257618553936, 0.3207822951953858, 0.4793594880029559, 0.340936197899282, 0.313765455968678, 0.44515777681954205, 0.29499886301346123, 0.329655644018203, 0.3006967888213694, 0.3048808048479259, 0.3092139249201864, 0.34837648808024824, 0.34121672296896577, 0.42159207770600915, 0.30647029005922377, 0.30552181880921125, 0.3168223579414189, 0.33544458425603807, 0.3045391689520329, 0.29862224077805877, 0.29999327822588384, 0.44581230194307864, 0.3062024973332882, 0.3014312069863081, 0.2955679530277848, 0.3131753809284419, 0.31034270487725735, 0.3057723480742425, 0.30282597709447145, 0.4822118189185858, 0.30392721644602716, 0.3146809369791299, 0.3114135756623, 0.3151313029229641, 0.3095252166967839, 0.31006460706703365, 0.31298755086027086, 0.30339726619422436, 0.3059654908720404, 0.30288409208878875, 0.30900910403579473, 0.30263738869689405, 0.3004622326698154, 0.30697516398504376, 0.31841497984714806, 0.30720837297849357, 0.309722047066316, 0.29747380083426833, 0.30473236995749176, 0.2986379947979003, 0.3132455302402377, 0.308694401755929, 0.3266336170490831, 0.331674826098606, 0.3160508219152689, 0.3096298899035901, 0.3109611268155277, 0.3515328059438616, 0.2908832929097116, 0.3008618119638413, 0.47567184595391154, 0.30562205286696553, 0.30920709925703704, 0.30616347794421017, 0.3049945109523833, 0.3067305281292647, 0.30932541005313396, 0.30358776124194264, 0.2958197249099612, 0.29385331901721656, 0.30290292389690876, 0.30834223609417677, 0.3928548078984022, 0.29641120904125273, 0.28972971998155117, 0.286998167168349, 0.2990966278593987, 0.29989532893523574, 0.2970852069556713, 0.3928489270620048, 0.31042591924779117, 0.301480463007465, 0.3041665852069855, 0.309337645303458, 0.2998354348819703, 0.3004754660651088, 0.30107871000654995, 0.2929465267807245, 0.40037986915558577, 0.30194369913078845, 0.29752740799449384, 0.2864937838166952, 0.2930958040524274, 0.28986814827658236, 0.2925203659106046, 0.2850896348245442, 0.39646251290105283, 0.27952436497434974, 0.27956098364666104, 0.2856336042750627, 0.2886789399199188, 0.279861006885767, 0.2839453858323395, 0.288852317025885, 5.943802120164037, 1.479110548272729, 0.2970658040139824, 0.30998126207850873, 0.29425204591825604, 0.2980236259754747, 0.39372654631733894, 0.31885244976729155, 0.3002551980316639, 0.297075220849365, 0.30359467188827693, 0.30159143591299653, 0.29502607230097055, 0.3037884358782321, 0.42628949298523366, 0.2947206380777061, 0.2966636240016669, 0.30107450392097235, 0.2954095769673586, 0.30124051100574434, 0.2981315420474857, 0.2971778579521924, 0.4212320151273161, 0.285719740903005, 0.2832523500546813, 0.28581255418248475, 0.30714985099621117, 0.3048179259058088, 0.3138995640911162, 0.2973983231931925, 0.43346114712767303, 0.2884948200080544, 0.2867229809053242, 0.28955725324340165, 0.290547801181674, 0.2915098911616951, 0.2988624048884958, 0.29213425097987056, 0.40172881493344903, 0.31024895003065467, 0.2921906551346183, 0.2888672580011189, 0.2975449289660901, 0.2907384599093348, 0.29889760608784854, 0.29757329425774515, 0.43216754612512887, 0.27986040501855314, 0.2836647860240191, 0.28686855896376073, 0.29610991897061467, 0.28361411788500845, 0.2865388428326696, 0.28440860169939697, 0.29302490898407996, 0.4327394438441843, 0.297303105937317, 0.30100032687187195, 0.3052109368145466, 0.3051949969958514, 0.29977859696373343, 0.31443379214033484, 0.429582686861977, 0.29909132700413465, 0.3005962206516415, 0.3103500222787261, 0.304413533071056, 0.29488060204312205, 0.3076683562248945, 0.41929599922150373, 0.30814831610769033, 0.3181879478506744, 0.32267633080482483, 0.3240608503110707, 0.3158291489817202, 0.32096412498503923, 0.306225482840091, 0.38928817003034055, 0.3161594192497432, 0.31966716912575066, 0.334387760842219, 0.31981777609325945, 0.32150080194696784, 0.31503683025948703, 0.3151343420613557, 0.4583409719634801, 0.31042546313256025, 0.3151857580523938, 0.3165269708260894, 0.3161790033336729, 0.3153876119758934, 0.3190334721002728, 0.3174782220739871, 0.3183078649453819, 0.32097873627208173, 0.3181976121850312, 0.30990117811597884, 0.30844829697161913, 0.3147796532139182, 0.31219154107384384, 0.33962268312461674, 0.3297652800101787, 0.3065492119640112, 0.3013809851836413, 0.3056148109026253, 0.3019223981536925, 0.3076121099293232, 0.2963723901193589, 0.30853358772583306, 0.32440465316176414, 0.3122482227627188, 0.3118080389685929, 0.31304313987493515, 0.318316335324198, 0.3185691460967064, 0.30260337609797716, 0.29639596212655306, 0.30358241335488856, 0.30666679702699184, 0.3255507196299732, 0.32630234211683273, 0.3044919571839273, 0.3059149431064725, 0.30711042787879705, 0.316974374698475, 0.32257872889749706, 0.328731823945418, 0.30420882487669587, 0.3115074245724827, 0.33416126878000796, 0.4300718589220196, 0.3116004227194935, 0.30418983404524624, 0.30933444295078516, 0.3083725271280855, 0.3362148830201477, 0.3297664262354374, 0.3720514359883964, 0.3144044801592827, 0.3137409989722073, 0.3131014108657837, 0.32432592078112066, 0.3193431382533163, 0.31941739516332746, 0.32061029225587845, 0.42572396202012897, 0.30107520311139524, 0.3072466431185603, 0.30488836392760277, 0.30712355370633304, 0.30405154218897223, 0.3059776790905744, 0.4461837827693671, 0.33575572771951556, 0.3234772540163249, 0.3237532547209412, 0.3297412400133908, 0.31857844395563006, 0.32926988904364407, 0.32637173496186733, 0.3824745360761881, 0.3245510619599372, 0.3168561172205955, 0.3160572899505496, 0.3220704449340701, 0.31748519092798233, 0.3002994360867888, 0.2989718932658434, 0.42426206753589213, 0.30066454876214266, 0.306606647092849, 0.3010451658628881, 0.2999467640183866, 0.2963874079287052, 0.30899744178168476, 0.3042030627839267, 0.45069832215085626, 0.3251193780452013, 0.3185153517406434, 0.3191123199649155, 0.31642672792077065, 0.31297692796215415, 0.29707556986249983, 0.3043534259777516, 0.4503689147531986, 0.3085226092953235, 0.31938828504644334, 0.33101091301068664, 0.3221872141584754, 0.3205220070667565, 0.3206370281986892, 0.4504637052305043, 0.337733615655452, 0.32362004765309393, 0.32943943282589316, 0.29492790601216257, 0.31335937697440386, 0.3036916048731655, 0.5718698010314256, 0.3106601149775088, 0.31088853790424764, 0.299417024012655, 0.31164205889217556, 0.32374982303008437, 0.3130806959234178, 0.42915420583449304, 0.3260998309124261, 0.33956109872087836, 0.3299607632216066, 0.30739625403657556, 0.3123747541103512, 0.32083822204731405, 0.3189558554440737, 0.33660720800980926, 0.29459333093836904, 0.2966247880831361, 0.30156823014840484, 0.3172736221458763, 0.3130755459424108, 0.2933800492901355, 0.2872146179433912, 0.4541881906334311, 0.3109732170123607, 0.32174411485902965, 0.3103538400027901, 0.31297818617895246, 0.31924390117637813, 0.31342369806952775, 0.3203746350482106, 0.3041446888819337, 0.3096724571660161, 0.3035108740441501, 0.3084770238492638, 0.31105149793438613, 0.30896379775367677, 0.30681556928902864, 0.44806498219259083, 0.302952469792217, 0.3063961456064135, 0.3066349010914564, 0.310496523976326, 0.3091195987071842, 0.3124951939098537, 0.31036859191954136, 0.31206390098668635, 0.311551604187116, 0.308413052232936, 0.3102419590577483, 0.30628104298375547, 0.3822053619660437, 0.30477313604205847, 0.30839322693645954, 0.31028686510398984, 0.30630744714289904, 0.3119636867195368, 0.3114513859618455, 0.3069102999288589, 0.30780777824111283, 0.30956510198302567, 0.3126481350045651, 0.3058180247899145, 0.30940675106830895, 0.309177263174206, 0.30620257603004575, 0.2906932230107486, 0.29834835208021104, 0.3301993610803038, 0.2988270178902894, 0.31354284519329667, 0.30116765620186925, 0.3070740180555731, 0.30777446972206235, 0.30780157493427396, 0.30741012189537287, 0.3075459797400981, 0.3303949742112309, 0.3247312316671014, 0.3242962670046836, 0.3201491853687912, 0.321997543098405, 0.3278810391202569, 0.3243223913013935, 0.41793007403612137, 0.3173278677277267, 0.3161862569395453, 0.3116946257650852, 0.3106192178092897, 0.3135207088198513, 0.30125550902448595, 0.31917155371047556, 0.4198715949896723, 0.32104125106707215, 0.3242949219420552, 0.3474840889684856, 0.3128944330383092, 0.32178943627513945, 0.32390438369475305, 0.330425692955032, 0.4053477009292692, 0.325456861872226, 0.3114098769146949, 0.3153359468560666, 0.30518788285553455, 0.30579539109021425, 0.3105742228217423, 0.46604744996875525, 0.33299512369558215, 0.33784281788393855, 0.35357055230997503, 0.33879268215969205, 0.3452374041080475, 0.49642354785464704, 0.31268370314501226, 0.31836295896209776, 0.3158766080159694, 0.3073822809383273, 0.31462931097485125, 0.31044244416989386, 0.4331432858016342, 0.32589640095829964, 0.32867231126874685, 0.32661953615024686, 0.3213943319860846, 0.328551558079198, 0.3217649273574352, 0.4177408979739994, 0.32961376779712737, 0.31228106771595776, 0.3191287817899138, 0.31690096110105515, 0.319815058959648, 0.30686901323497295, 0.3117374759167433, 0.4561441042460501, 0.302721711108461, 0.31418808130547404, 0.31416055420413613, 0.34917495818808675, 0.32498848903924227, 0.33907336508855224, 0.43841843283735216, 0.31471480475738645, 0.30725920596159995, 0.30808125832118094, 0.30456537287682295, 0.31335397996008396, 0.43698089895769954, 0.3017011003103107, 0.4595016110688448, 0.3160599779803306, 0.3166681530419737, 0.3161657298915088, 0.31702036084607244, 0.3076706128194928, 0.44634125009179115, 0.2940105979796499, 0.3196987158153206, 0.3077974838670343, 0.329341301927343, 0.34523737500421703, 0.4370402011554688, 0.3015548540279269, 0.2964754307176918, 0.2914913047570735, 0.2924366800580174, 0.3051454620435834, 0.32088470575399697, 0.31022766744717956, 0.44551712181419134, 0.2954340260475874, 0.30530167114920914, 0.2975034569390118, 0.29948574723675847, 0.2938194489106536, 0.3198895340319723, 0.31703938520513475, 0.4279136792756617, 0.3070459458976984, 0.31183358491398394, 0.3050764419604093, 0.3148084878921509, 0.3147949231788516, 0.31549355178140104, 0.3167507331818342, 0.3157158219255507, 0.4461894743144512, 0.31792243383824825, 0.3123898352496326, 0.30518032889813185, 0.31169946398586035, 0.2916458370164037, 0.29955258197151124, 0.30147446412593126, 0.41976943775080144, 0.31683179805986583, 0.307828821009025, 0.3302851233165711, 0.2986859732773155, 0.3028320809826255, 0.31363721983507276, 0.4411208361852914, 0.3456782440189272, 0.31346684601157904, 0.30532897589728236, 0.3168501961044967, 0.31030191108584404, 0.3115484002046287, 0.303733830107376, 0.4102215110324323, 0.29002560093067586, 0.3059794257860631]
Total Epoch List: [211, 163, 182]
Total Time List: [0.09264870616607368, 0.105589693877846, 0.09096853481605649]
T-times Epoch Time: 0.35608196984532126 ~ 0.015298676911047358
T-times Total Epoch: 177.11111111111111 ~ 9.802997777827898
T-times Total Time: 0.0908715448394004 ~ 0.004078947537846885
T-times Inference Elapsed: 0.1042566592751769 ~ 0.0032098211695116034
T-times Time Per Graph: 0.0021414766766778177 ~ 6.723410840846317e-05
T-times Speed: 529.0160014387878 ~ 5.676155738178256
T-times cross validation test micro f1 score:0.9255354646055496 ~ 0.0065141374483667835
T-times cross validation test precision:0.9439016458581676 ~ 0.010215043118571151
T-times cross validation test recall:0.9088888888888889 ~ 0.017521298033355334
T-times cross validation test f1_score:0.9255354646055496 ~ 0.007651583590746492
