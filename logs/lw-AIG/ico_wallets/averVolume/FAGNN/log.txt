Namespace(seed=15, model='FAGNN', dataset='ico_wallets/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=10, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 306], edge_attr=[306, 2], x=[90, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d3160>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 3.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 4.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 3.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.38s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.39s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.41s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 9.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 1.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.38s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.82s
Val loss: 0.6928 score: 0.7143 time: 0.63s
Test loss: 0.6929 score: 0.6735 time: 0.44s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.89s
Val loss: 0.6927 score: 0.6531 time: 0.66s
Test loss: 0.6929 score: 0.5918 time: 0.43s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 18.42s
Val loss: 0.6925 score: 0.6122 time: 0.51s
Test loss: 0.6928 score: 0.5714 time: 0.42s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.85s
Val loss: 0.6922 score: 0.6122 time: 0.59s
Test loss: 0.6927 score: 0.5918 time: 0.62s
Epoch 10/1000, LR 0.000240
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.69s
Val loss: 0.6920 score: 0.7143 time: 9.98s
Test loss: 0.6925 score: 0.5918 time: 2.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 2.99s
Val loss: 0.6916 score: 0.8571 time: 0.88s
Test loss: 0.6923 score: 0.6122 time: 1.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 2.28s
Val loss: 0.6913 score: 0.7551 time: 10.14s
Test loss: 0.6921 score: 0.7143 time: 9.99s
Epoch 13/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 8.81s
Val loss: 0.6909 score: 0.7551 time: 0.74s
Test loss: 0.6919 score: 0.6735 time: 1.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 3.89s
Val loss: 0.6905 score: 0.6735 time: 9.24s
Test loss: 0.6916 score: 0.6122 time: 5.74s
Epoch 15/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 2.52s
Val loss: 0.6900 score: 0.6531 time: 10.69s
Test loss: 0.6913 score: 0.5918 time: 2.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.73s
Val loss: 0.6894 score: 0.6122 time: 8.40s
Test loss: 0.6909 score: 0.5714 time: 5.72s
Epoch 17/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 3.95s
Val loss: 0.6887 score: 0.6122 time: 0.82s
Test loss: 0.6905 score: 0.5918 time: 7.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 7.87s
Val loss: 0.6880 score: 0.6122 time: 1.30s
Test loss: 0.6901 score: 0.5918 time: 1.12s
Epoch 19/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 13.63s
Val loss: 0.6872 score: 0.6122 time: 1.44s
Test loss: 0.6896 score: 0.5714 time: 1.12s
Epoch 20/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 22.00s
Val loss: 0.6863 score: 0.6122 time: 1.27s
Test loss: 0.6891 score: 0.5714 time: 1.53s
Epoch 21/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 2.48s
Val loss: 0.6853 score: 0.6122 time: 0.88s
Test loss: 0.6885 score: 0.5918 time: 10.17s
Epoch 22/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 5.59s
Val loss: 0.6842 score: 0.6122 time: 10.32s
Test loss: 0.6878 score: 0.5918 time: 8.59s
Epoch 23/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 2.55s
Val loss: 0.6830 score: 0.6531 time: 0.83s
Test loss: 0.6871 score: 0.6122 time: 1.15s
Epoch 24/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 22.66s
Val loss: 0.6816 score: 0.6531 time: 1.00s
Test loss: 0.6863 score: 0.6122 time: 1.44s
Epoch 25/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 1.59s
Val loss: 0.6802 score: 0.6531 time: 1.20s
Test loss: 0.6854 score: 0.6122 time: 9.33s
Epoch 26/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 9.47s
Val loss: 0.6786 score: 0.6531 time: 1.45s
Test loss: 0.6845 score: 0.6122 time: 0.67s
Epoch 27/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 2.51s
Val loss: 0.6769 score: 0.6531 time: 9.00s
Test loss: 0.6834 score: 0.6122 time: 10.72s
Epoch 28/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 4.06s
Val loss: 0.6749 score: 0.6735 time: 11.55s
Test loss: 0.6823 score: 0.6122 time: 2.32s
Epoch 29/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 1.62s
Val loss: 0.6728 score: 0.7551 time: 1.21s
Test loss: 0.6810 score: 0.6122 time: 2.92s
Epoch 30/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 17.34s
Val loss: 0.6705 score: 0.7551 time: 1.65s
Test loss: 0.6795 score: 0.6327 time: 0.72s
Epoch 31/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 24.30s
Val loss: 0.6679 score: 0.7551 time: 0.81s
Test loss: 0.6779 score: 0.6327 time: 1.50s
Epoch 32/1000, LR 0.000270
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 1.91s
Val loss: 0.6651 score: 0.7755 time: 0.72s
Test loss: 0.6762 score: 0.6122 time: 0.82s
Epoch 33/1000, LR 0.000270
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 23.29s
Val loss: 0.6621 score: 0.7959 time: 2.19s
Test loss: 0.6743 score: 0.6327 time: 0.53s
Epoch 34/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 18.02s
Val loss: 0.6587 score: 0.7959 time: 0.63s
Test loss: 0.6722 score: 0.6531 time: 1.16s
Epoch 35/1000, LR 0.000270
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 2.48s
Val loss: 0.6550 score: 0.7959 time: 0.89s
Test loss: 0.6699 score: 0.6735 time: 9.49s
Epoch 36/1000, LR 0.000270
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 8.78s
Val loss: 0.6510 score: 0.7959 time: 1.47s
Test loss: 0.6673 score: 0.6939 time: 0.56s
Epoch 37/1000, LR 0.000270
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 11.97s
Val loss: 0.6466 score: 0.7959 time: 9.92s
Test loss: 0.6646 score: 0.7143 time: 2.83s
Epoch 38/1000, LR 0.000270
Train loss: 0.6536;  Loss pred: 0.6536; Loss self: 0.0000; time: 1.50s
Val loss: 0.6419 score: 0.8163 time: 1.87s
Test loss: 0.6616 score: 0.7143 time: 1.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 2.74s
Val loss: 0.6369 score: 0.8571 time: 8.84s
Test loss: 0.6584 score: 0.7347 time: 5.13s
Epoch 40/1000, LR 0.000269
Train loss: 0.6452;  Loss pred: 0.6452; Loss self: 0.0000; time: 20.95s
Val loss: 0.6315 score: 0.8571 time: 0.70s
Test loss: 0.6549 score: 0.7551 time: 1.16s
Epoch 41/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 1.68s
Val loss: 0.6257 score: 0.8776 time: 9.33s
Test loss: 0.6510 score: 0.7551 time: 4.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 2.11s
Val loss: 0.6194 score: 0.9184 time: 1.04s
Test loss: 0.6469 score: 0.7551 time: 9.41s
Epoch 43/1000, LR 0.000269
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 2.05s
Val loss: 0.6127 score: 0.9184 time: 1.74s
Test loss: 0.6426 score: 0.7755 time: 9.67s
Epoch 44/1000, LR 0.000269
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 10.06s
Val loss: 0.6054 score: 0.9184 time: 1.18s
Test loss: 0.6378 score: 0.7755 time: 0.86s
Epoch 45/1000, LR 0.000269
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 17.60s
Val loss: 0.5977 score: 0.9388 time: 8.18s
Test loss: 0.6327 score: 0.7959 time: 3.76s
Epoch 46/1000, LR 0.000269
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 18.23s
Val loss: 0.5895 score: 0.9592 time: 0.82s
Test loss: 0.6272 score: 0.7959 time: 1.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 1.76s
Val loss: 0.5806 score: 0.9592 time: 7.17s
Test loss: 0.6213 score: 0.7959 time: 0.43s
Epoch 48/1000, LR 0.000269
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 3.19s
Val loss: 0.5712 score: 0.9796 time: 11.37s
Test loss: 0.6149 score: 0.8163 time: 1.51s
Epoch 49/1000, LR 0.000269
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 3.21s
Val loss: 0.5612 score: 0.9796 time: 9.52s
Test loss: 0.6081 score: 0.8163 time: 0.62s
Epoch 50/1000, LR 0.000269
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 2.23s
Val loss: 0.5505 score: 0.9796 time: 9.16s
Test loss: 0.6008 score: 0.8367 time: 8.40s
Epoch 51/1000, LR 0.000269
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 2.27s
Val loss: 0.5392 score: 0.9796 time: 0.88s
Test loss: 0.5931 score: 0.8776 time: 11.34s
Epoch 52/1000, LR 0.000269
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 3.73s
Val loss: 0.5273 score: 0.9796 time: 8.79s
Test loss: 0.5851 score: 0.8776 time: 9.40s
Epoch 53/1000, LR 0.000269
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 3.40s
Val loss: 0.5149 score: 0.9796 time: 1.44s
Test loss: 0.5766 score: 0.8776 time: 0.51s
Epoch 54/1000, LR 0.000269
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 10.77s
Val loss: 0.5019 score: 0.9796 time: 1.24s
Test loss: 0.5677 score: 0.8776 time: 3.57s
Epoch 55/1000, LR 0.000269
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 14.27s
Val loss: 0.4884 score: 0.9796 time: 1.26s
Test loss: 0.5584 score: 0.8776 time: 0.99s
Epoch 56/1000, LR 0.000269
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 5.41s
Val loss: 0.4744 score: 0.9796 time: 9.87s
Test loss: 0.5486 score: 0.8776 time: 1.11s
Epoch 57/1000, LR 0.000269
Train loss: 0.4864;  Loss pred: 0.4864; Loss self: 0.0000; time: 4.06s
Val loss: 0.4598 score: 0.9796 time: 9.42s
Test loss: 0.5385 score: 0.8776 time: 9.42s
Epoch 58/1000, LR 0.000269
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 1.55s
Val loss: 0.4447 score: 0.9796 time: 4.84s
Test loss: 0.5279 score: 0.8776 time: 9.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.4634;  Loss pred: 0.4634; Loss self: 0.0000; time: 1.25s
Val loss: 0.4294 score: 0.9796 time: 2.69s
Test loss: 0.5170 score: 0.8776 time: 10.69s
Epoch 60/1000, LR 0.000268
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 3.85s
Val loss: 0.4138 score: 0.9796 time: 0.49s
Test loss: 0.5058 score: 0.8776 time: 0.39s
Epoch 61/1000, LR 0.000268
Train loss: 0.4275;  Loss pred: 0.4275; Loss self: 0.0000; time: 0.74s
Val loss: 0.3981 score: 0.9796 time: 0.47s
Test loss: 0.4944 score: 0.8776 time: 0.38s
Epoch 62/1000, LR 0.000268
Train loss: 0.4221;  Loss pred: 0.4221; Loss self: 0.0000; time: 0.66s
Val loss: 0.3824 score: 0.9796 time: 2.30s
Test loss: 0.4830 score: 0.8776 time: 3.13s
Epoch 63/1000, LR 0.000268
Train loss: 0.4005;  Loss pred: 0.4005; Loss self: 0.0000; time: 16.81s
Val loss: 0.3669 score: 0.9796 time: 0.74s
Test loss: 0.4715 score: 0.8776 time: 0.62s
Epoch 64/1000, LR 0.000268
Train loss: 0.3837;  Loss pred: 0.3837; Loss self: 0.0000; time: 1.50s
Val loss: 0.3517 score: 0.9796 time: 0.85s
Test loss: 0.4601 score: 0.8776 time: 0.43s
Epoch 65/1000, LR 0.000268
Train loss: 0.3708;  Loss pred: 0.3708; Loss self: 0.0000; time: 15.08s
Val loss: 0.3368 score: 0.9796 time: 1.37s
Test loss: 0.4488 score: 0.8776 time: 0.72s
Epoch 66/1000, LR 0.000268
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 9.89s
Val loss: 0.3224 score: 0.9796 time: 7.66s
Test loss: 0.4377 score: 0.8776 time: 1.01s
Epoch 67/1000, LR 0.000268
Train loss: 0.3476;  Loss pred: 0.3476; Loss self: 0.0000; time: 2.24s
Val loss: 0.3085 score: 0.9796 time: 8.85s
Test loss: 0.4265 score: 0.8776 time: 0.60s
Epoch 68/1000, LR 0.000268
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 1.45s
Val loss: 0.2950 score: 0.9796 time: 4.65s
Test loss: 0.4158 score: 0.8776 time: 8.97s
Epoch 69/1000, LR 0.000268
Train loss: 0.2979;  Loss pred: 0.2979; Loss self: 0.0000; time: 9.72s
Val loss: 0.2820 score: 0.9796 time: 4.96s
Test loss: 0.4054 score: 0.8776 time: 5.46s
Epoch 70/1000, LR 0.000268
Train loss: 0.2932;  Loss pred: 0.2932; Loss self: 0.0000; time: 18.15s
Val loss: 0.2695 score: 0.9796 time: 0.92s
Test loss: 0.3953 score: 0.8776 time: 0.67s
Epoch 71/1000, LR 0.000268
Train loss: 0.2716;  Loss pred: 0.2716; Loss self: 0.0000; time: 1.84s
Val loss: 0.2575 score: 0.9796 time: 9.87s
Test loss: 0.3856 score: 0.8776 time: 9.49s
Epoch 72/1000, LR 0.000267
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 2.96s
Val loss: 0.2460 score: 0.9796 time: 0.78s
Test loss: 0.3765 score: 0.8776 time: 1.03s
Epoch 73/1000, LR 0.000267
Train loss: 0.2507;  Loss pred: 0.2507; Loss self: 0.0000; time: 16.34s
Val loss: 0.2350 score: 0.9796 time: 1.20s
Test loss: 0.3678 score: 0.8776 time: 0.98s
Epoch 74/1000, LR 0.000267
Train loss: 0.2359;  Loss pred: 0.2359; Loss self: 0.0000; time: 2.36s
Val loss: 0.2244 score: 0.9796 time: 0.78s
Test loss: 0.3598 score: 0.8776 time: 5.82s
Epoch 75/1000, LR 0.000267
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 14.99s
Val loss: 0.2143 score: 0.9796 time: 0.48s
Test loss: 0.3525 score: 0.8776 time: 0.40s
Epoch 76/1000, LR 0.000267
Train loss: 0.2109;  Loss pred: 0.2109; Loss self: 0.0000; time: 0.63s
Val loss: 0.2046 score: 0.9796 time: 0.59s
Test loss: 0.3458 score: 0.8776 time: 1.48s
Epoch 77/1000, LR 0.000267
Train loss: 0.2069;  Loss pred: 0.2069; Loss self: 0.0000; time: 22.13s
Val loss: 0.1957 score: 0.9592 time: 1.40s
Test loss: 0.3394 score: 0.8776 time: 1.51s
Epoch 78/1000, LR 0.000267
Train loss: 0.1866;  Loss pred: 0.1866; Loss self: 0.0000; time: 1.58s
Val loss: 0.1872 score: 0.9592 time: 11.16s
Test loss: 0.3337 score: 0.8776 time: 10.83s
Epoch 79/1000, LR 0.000267
Train loss: 0.1786;  Loss pred: 0.1786; Loss self: 0.0000; time: 8.35s
Val loss: 0.1793 score: 0.9592 time: 9.58s
Test loss: 0.3281 score: 0.8776 time: 7.46s
Epoch 80/1000, LR 0.000267
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 21.72s
Val loss: 0.1721 score: 0.9592 time: 1.06s
Test loss: 0.3231 score: 0.8776 time: 0.55s
Epoch 81/1000, LR 0.000267
Train loss: 0.1533;  Loss pred: 0.1533; Loss self: 0.0000; time: 2.16s
Val loss: 0.1655 score: 0.9592 time: 10.67s
Test loss: 0.3190 score: 0.8776 time: 9.99s
Epoch 82/1000, LR 0.000267
Train loss: 0.1420;  Loss pred: 0.1420; Loss self: 0.0000; time: 5.51s
Val loss: 0.1595 score: 0.9592 time: 1.25s
Test loss: 0.3155 score: 0.8980 time: 0.52s
Epoch 83/1000, LR 0.000266
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 11.78s
Val loss: 0.1543 score: 0.9592 time: 0.61s
Test loss: 0.3125 score: 0.8980 time: 1.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 1.97s
Val loss: 0.1497 score: 0.9592 time: 8.85s
Test loss: 0.3103 score: 0.8980 time: 9.96s
Epoch 85/1000, LR 0.000266
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 22.34s
Val loss: 0.1457 score: 0.9592 time: 9.02s
Test loss: 0.3087 score: 0.8980 time: 2.14s
Epoch 86/1000, LR 0.000266
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 2.51s
Val loss: 0.1423 score: 0.9592 time: 10.21s
Test loss: 0.3079 score: 0.8980 time: 9.96s
Epoch 87/1000, LR 0.000266
Train loss: 0.1097;  Loss pred: 0.1097; Loss self: 0.0000; time: 4.24s
Val loss: 0.1392 score: 0.9592 time: 0.78s
Test loss: 0.3079 score: 0.8980 time: 1.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 20.35s
Val loss: 0.1367 score: 0.9592 time: 0.47s
Test loss: 0.3087 score: 0.8980 time: 9.81s
Epoch 89/1000, LR 0.000266
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 9.64s
Val loss: 0.1345 score: 0.9592 time: 11.38s
Test loss: 0.3100 score: 0.8980 time: 4.28s
Epoch 90/1000, LR 0.000266
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 1.74s
Val loss: 0.1328 score: 0.9592 time: 1.04s
Test loss: 0.3121 score: 0.8980 time: 0.58s
Epoch 91/1000, LR 0.000266
Train loss: 0.0923;  Loss pred: 0.0923; Loss self: 0.0000; time: 15.70s
Val loss: 0.1318 score: 0.9592 time: 4.92s
Test loss: 0.3139 score: 0.8980 time: 0.65s
Epoch 92/1000, LR 0.000266
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 1.38s
Val loss: 0.1313 score: 0.9592 time: 1.20s
Test loss: 0.3161 score: 0.8980 time: 9.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 18.51s
Val loss: 0.1317 score: 0.9592 time: 8.23s
Test loss: 0.3181 score: 0.8980 time: 9.47s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 2.56s
Val loss: 0.1325 score: 0.9592 time: 1.70s
Test loss: 0.3201 score: 0.8980 time: 10.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 091,   Train_Loss: 0.0770,   Val_Loss: 0.1313,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1313,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3161


[4.20998081099242, 0.3801914050709456, 0.39033605495933443, 0.41736693296115845, 0.3842876589624211, 0.44220690499059856, 0.4389886950375512, 0.4204101220238954, 0.6276955739594996, 2.0759688779944554, 1.103972650016658, 9.992660070885904, 1.2440602959832177, 5.746186608914286, 2.5089665750274435, 5.734940523048863, 7.054660225985572, 1.1251596040092409, 1.1248534000478685, 1.5360058010555804, 10.171286393073387, 8.597733428003266, 1.1504607269307598, 1.441576577955857, 9.332677235943265, 0.6781181229744107, 10.72648668999318, 2.324371569091454, 2.931617929949425, 0.7277763220481575, 1.5108333199750632, 0.8245339489076287, 0.544463471043855, 1.171911892015487, 9.492495149956085, 0.5649784629931673, 2.8314255910227075, 1.2156774729955941, 5.137492283945903, 1.1685249030124396, 4.078842686954886, 9.419127741944976, 9.67569983692374, 0.8611436169594526, 3.763740206020884, 1.0709673109231517, 0.4388017780147493, 1.5168438979890198, 0.6291808818932623, 8.407945835962892, 11.346492246957496, 9.41009756398853, 0.5146344359964132, 3.5777930190088227, 1.0005330480635166, 1.1160023880656809, 9.421909576980397, 9.21827711199876, 10.69503390591126, 0.39239069807808846, 0.38862070499453694, 3.138231493998319, 0.6210656720213592, 0.4350298239151016, 0.729491326957941, 1.010295914951712, 0.6096510200295597, 8.972448851098306, 5.460545056965202, 0.6745391159784049, 9.495890646008775, 1.0324437720701098, 0.9835861730389297, 5.835210973047651, 0.40411011897958815, 1.4841283910209313, 1.5122076099505648, 10.834568899008445, 7.46605524700135, 0.5550562969874591, 9.993400639970787, 0.5274411250138655, 1.246264089946635, 9.96703715599142, 2.1428094840375707, 9.963717818027362, 1.2508718410972506, 9.819924705079757, 4.2843708449508995, 0.5802636609878391, 0.6583686569938436, 9.259317980962805, 9.482170381001197, 10.229828840005212]
[0.08591797573453917, 0.0077590082667539926, 0.007966041937945601, 0.008517692509411397, 0.00784260528494737, 0.009024630714093849, 0.008958952959950025, 0.008579798408650927, 0.012810113754275501, 0.042366711795805215, 0.022530054081972613, 0.20393183818134497, 0.025388985632310564, 0.11726911446763849, 0.05120339949035599, 0.11703960251120128, 0.14397265767317494, 0.022962440898147772, 0.022956191837711602, 0.0313470571643996, 0.2075772733280283, 0.17546394751027075, 0.02347879034552571, 0.029419930162364428, 0.19046280073353603, 0.013839145366824707, 0.2189078916325139, 0.04743615447125417, 0.059828937345906635, 0.014852578000982806, 0.030833333060715576, 0.01682722344709446, 0.011111499409058265, 0.023916569224805857, 0.1937243908154303, 0.011530172714146272, 0.0577841957351573, 0.02480974434684886, 0.10484678130501843, 0.02384744700025387, 0.08324168748887521, 0.19222709677438726, 0.19746326197803551, 0.01757435952978475, 0.0768110246126711, 0.021856475733125543, 0.008955138326831619, 0.030955997918143262, 0.012840426161086984, 0.17159073134618147, 0.2315610662644387, 0.19204280742833732, 0.010502743591763536, 0.07301618406140455, 0.020419041797214627, 0.022775558940115934, 0.19228386891796728, 0.18812810432650529, 0.2182659980798216, 0.00800797343016507, 0.007931034795806877, 0.06404554069384324, 0.012674809633088964, 0.008878159671736767, 0.01488757810118247, 0.020618283978606364, 0.012441857551623667, 0.18311120104282258, 0.11143969504010616, 0.013766104407722548, 0.1937936866532403, 0.021070281062655304, 0.020073187204876115, 0.11908593822546228, 0.008247145285297717, 0.030288334510631253, 0.030861379794909487, 0.22111365100017236, 0.15236847442859897, 0.011327679530356308, 0.2039469518361385, 0.010764104592119704, 0.025433961019319083, 0.2034089215508453, 0.04373080579668512, 0.20334117995974207, 0.025527996757086748, 0.20040662663428074, 0.0874361396928755, 0.011842115530364064, 0.013436095040690686, 0.1889656730808736, 0.19351368124492238, 0.2087720171429635]
[11.639007919480093, 128.8824506457645, 125.53285656664451, 117.40268845054887, 127.50864842316373, 110.80785814740219, 111.62018647384194, 116.55285501716565, 78.06331927897519, 23.603436698597207, 44.38515754829672, 4.903599207058375, 39.38715845060697, 8.527394485237282, 19.529953283440626, 8.544116508805597, 6.945763287012799, 43.54937719537749, 43.56123206625396, 31.900921185535896, 4.817483070122658, 5.699176464392864, 42.59163207658896, 33.99056335216099, 5.250369080726867, 72.25879730963783, 4.568131338447701, 21.080966852108343, 16.71431993214932, 67.32837894767019, 32.43243271918888, 59.42751061362229, 89.9968548965394, 41.812017041424866, 5.16197261372598, 86.728969703386, 17.305769982216365, 40.306743431921426, 9.537727220169186, 41.93320987313042, 12.013211530984933, 5.202180216942454, 5.0642331640972955, 56.901077863191304, 13.018964465617026, 45.75303046155817, 111.66773348478314, 32.303917407033474, 77.87903512349988, 5.827820606361986, 4.3185152674069025, 5.207172366365035, 95.21321655269394, 13.695593831074879, 48.97389456034174, 43.9067160823281, 5.200644264270671, 5.315526904286732, 4.58156565290711, 124.8755392011068, 126.08695154492301, 15.613889572426249, 78.89664846637157, 112.63595575820248, 67.17009262376756, 48.50064151980858, 80.3738505967302, 5.461162366392534, 8.973463177910787, 72.64219203793175, 5.160126819762318, 47.460211708916745, 49.81769909250301, 8.397297068833822, 121.25407827878475, 33.0160114828697, 32.402958216565146, 4.522561114959024, 6.5630374245730705, 88.27933358462035, 4.903235821849652, 92.9013641071539, 39.31750934274145, 4.916205210546942, 22.86717525053705, 4.917843007491065, 39.17267811946087, 4.989854960359599, 11.436918458575114, 84.44437123045505, 74.42638631027387, 5.291966438645286, 5.167593286256286, 4.789913963015537]
Elapsed: 3.7990187489838796~3.8122431788134636
Time per graph: 0.07753099487722201~0.07780088120027477
Speed: 42.88810398222985~39.40581067602575
Total Time: 10.2305
best val loss: 0.131316140294075 test_score: 0.8980

Testing...
Test loss: 0.6149 score: 0.8163 time: 3.54s
test Score 0.8163
Epoch Time List: [9.233843691996299, 4.273922999971546, 1.8734583469340578, 1.5833287599962205, 11.893710596952587, 1.8830520879710093, 1.9836342639755458, 19.344367160927504, 2.0642795800231397, 12.737888444913551, 4.96762684697751, 22.41127654199954, 10.781780915916897, 18.864039964042604, 15.717427596100606, 14.846924169105478, 11.82442497194279, 10.296903591020964, 16.192355770152062, 24.800162309082225, 13.522720113047399, 24.508441373938695, 4.52520399610512, 25.09761512989644, 12.116193930967711, 11.589682840975001, 22.232915799948387, 17.93188690394163, 5.754702292964794, 19.709493588889018, 26.612625295994803, 3.4461163041414693, 26.007582439109683, 19.811232370091602, 12.861963052069768, 10.804033781052567, 24.716521229944192, 4.5801692879758775, 16.70970431691967, 22.81190911110025, 15.081666366080754, 12.556934696971439, 13.466582978959195, 12.09752483794, 29.544525985023938, 20.111976254964247, 9.357704854919575, 16.07004031992983, 13.354963763034903, 19.80196435994003, 14.489574330858886, 21.921089339070022, 5.348641948075965, 15.57797521003522, 16.524153239908628, 16.390400325064547, 22.891237517120317, 15.60264636296779, 14.637129032984376, 4.735733990906738, 1.6008901899913326, 6.096560187987052, 18.164203705848195, 2.7867725040996447, 17.176505322102457, 18.557462551048957, 11.698768873000517, 15.065711592906155, 20.143220031051897, 19.742097233072855, 21.19160205195658, 4.760558368987404, 18.515061816899106, 8.955553437001072, 15.873269472038373, 2.6983683368889615, 25.04117266391404, 23.571269833948463, 25.39393110992387, 23.331935737864114, 22.8173843229888, 7.278773032128811, 13.63293563714251, 20.77856893907301, 33.50516891491134, 22.672362343990244, 6.25969225098379, 30.632361622992903, 25.29447036003694, 3.3521978730568662, 21.265450199018233, 11.829835025127977, 36.21029985300265, 14.480679856031202]
Total Epoch List: [94]
Total Time List: [10.230493623996153]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d2b30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 5.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.60s
Epoch 2/1000, LR 0.000000
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 22.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 1.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 13.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 1.38s
Epoch 4/1000, LR 0.000060
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 8.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 9.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 7.97s
Epoch 5/1000, LR 0.000090
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 3.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 6.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4898 time: 4.49s
Epoch 6/1000, LR 0.000120
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4898 time: 9.45s
Epoch 7/1000, LR 0.000150
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 9.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 10.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 3.83s
Epoch 8/1000, LR 0.000180
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 2.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.49s
Epoch 9/1000, LR 0.000210
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.49s
Epoch 10/1000, LR 0.000240
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 1.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 4.89s
Epoch 11/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 5.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.50s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.46s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 7.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 2.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.48s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.47s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.52s
Epoch 16/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 10.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4898 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4898 time: 0.47s
Epoch 18/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4898 time: 0.56s
Epoch 19/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 11.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 2.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4898 time: 0.48s
Epoch 20/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4898 time: 0.61s
Epoch 21/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4898 time: 1.39s
Epoch 22/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 10.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4898 time: 0.49s
Epoch 23/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4898 time: 0.47s
Epoch 24/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4898 time: 0.67s
Epoch 25/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 3.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5102 time: 8.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4898 time: 3.83s
Epoch 26/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 2.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.4898 time: 0.51s
Epoch 27/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.4898 time: 0.59s
Epoch 28/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6857 score: 0.5102 time: 4.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4898 time: 0.66s
Epoch 29/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.4898 time: 0.50s
Epoch 30/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.4898 time: 0.55s
Epoch 31/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 13.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.4898 time: 0.56s
Epoch 32/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6752 score: 0.4898 time: 0.64s
Epoch 33/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6793 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6730 score: 0.4898 time: 0.48s
Epoch 34/1000, LR 0.000270
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.4898 time: 6.01s
Epoch 35/1000, LR 0.000270
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 3.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6757 score: 0.5102 time: 0.41s
Test loss: 0.6677 score: 0.5306 time: 0.49s
Epoch 36/1000, LR 0.000270
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6736 score: 0.5102 time: 0.37s
Test loss: 0.6647 score: 0.5306 time: 0.48s
Epoch 37/1000, LR 0.000270
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.73s
Val loss: 0.6713 score: 0.5306 time: 0.46s
Test loss: 0.6614 score: 0.5510 time: 6.19s
Epoch 38/1000, LR 0.000270
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.81s
Val loss: 0.6688 score: 0.5306 time: 0.39s
Test loss: 0.6579 score: 0.5714 time: 0.50s
Epoch 39/1000, LR 0.000269
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.71s
Val loss: 0.6661 score: 0.5714 time: 0.38s
Test loss: 0.6542 score: 0.5714 time: 0.47s
Epoch 40/1000, LR 0.000269
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.57s
Val loss: 0.6633 score: 0.5714 time: 0.36s
Test loss: 0.6501 score: 0.5714 time: 0.49s
Epoch 41/1000, LR 0.000269
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.64s
Val loss: 0.6602 score: 0.5510 time: 3.84s
Test loss: 0.6458 score: 0.5918 time: 0.57s
Epoch 42/1000, LR 0.000269
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.80s
Val loss: 0.6568 score: 0.5510 time: 0.45s
Test loss: 0.6412 score: 0.5918 time: 0.49s
Epoch 43/1000, LR 0.000269
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.80s
Val loss: 0.6532 score: 0.5510 time: 0.53s
Test loss: 0.6363 score: 0.5918 time: 0.51s
Epoch 44/1000, LR 0.000269
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.67s
Val loss: 0.6493 score: 0.5714 time: 0.39s
Test loss: 0.6311 score: 0.5918 time: 0.48s
Epoch 45/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.69s
Val loss: 0.6451 score: 0.6122 time: 0.42s
Test loss: 0.6255 score: 0.6122 time: 3.71s
Epoch 46/1000, LR 0.000269
Train loss: 0.6222;  Loss pred: 0.6222; Loss self: 0.0000; time: 9.56s
Val loss: 0.6405 score: 0.6122 time: 0.38s
Test loss: 0.6195 score: 0.6122 time: 0.48s
Epoch 47/1000, LR 0.000269
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.90s
Val loss: 0.6356 score: 0.6122 time: 0.38s
Test loss: 0.6130 score: 0.6122 time: 0.58s
Epoch 48/1000, LR 0.000269
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.75s
Val loss: 0.6303 score: 0.6327 time: 0.40s
Test loss: 0.6060 score: 0.6122 time: 0.57s
Epoch 49/1000, LR 0.000269
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.67s
Val loss: 0.6244 score: 0.6327 time: 0.38s
Test loss: 0.5983 score: 0.6122 time: 4.03s
Epoch 50/1000, LR 0.000269
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.75s
Val loss: 0.6181 score: 0.6735 time: 0.39s
Test loss: 0.5900 score: 0.6735 time: 0.46s
Epoch 51/1000, LR 0.000269
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 0.67s
Val loss: 0.6112 score: 0.6735 time: 0.38s
Test loss: 0.5812 score: 0.7347 time: 0.45s
Epoch 52/1000, LR 0.000269
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.60s
Val loss: 0.6039 score: 0.7347 time: 0.37s
Test loss: 0.5718 score: 0.7347 time: 0.63s
Epoch 53/1000, LR 0.000269
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 5.08s
Val loss: 0.5961 score: 0.7347 time: 4.64s
Test loss: 0.5619 score: 0.7551 time: 2.00s
Epoch 54/1000, LR 0.000269
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.75s
Val loss: 0.5880 score: 0.7755 time: 0.44s
Test loss: 0.5515 score: 0.7959 time: 0.55s
Epoch 55/1000, LR 0.000269
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.84s
Val loss: 0.5793 score: 0.7755 time: 0.38s
Test loss: 0.5407 score: 0.8367 time: 0.47s
Epoch 56/1000, LR 0.000269
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.78s
Val loss: 0.5703 score: 0.7755 time: 0.37s
Test loss: 0.5294 score: 0.8367 time: 0.66s
Epoch 57/1000, LR 0.000269
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 11.83s
Val loss: 0.5610 score: 0.7755 time: 3.23s
Test loss: 0.5178 score: 0.8571 time: 0.53s
Epoch 58/1000, LR 0.000269
Train loss: 0.4971;  Loss pred: 0.4971; Loss self: 0.0000; time: 0.65s
Val loss: 0.5516 score: 0.7755 time: 0.39s
Test loss: 0.5059 score: 0.8776 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.78s
Val loss: 0.5420 score: 0.7959 time: 0.42s
Test loss: 0.4938 score: 0.9184 time: 0.57s
Epoch 60/1000, LR 0.000268
Train loss: 0.4740;  Loss pred: 0.4740; Loss self: 0.0000; time: 0.62s
Val loss: 0.5324 score: 0.8163 time: 0.36s
Test loss: 0.4817 score: 0.9184 time: 0.47s
Epoch 61/1000, LR 0.000268
Train loss: 0.4592;  Loss pred: 0.4592; Loss self: 0.0000; time: 0.78s
Val loss: 0.5226 score: 0.8571 time: 9.47s
Test loss: 0.4697 score: 0.9184 time: 7.28s
Epoch 62/1000, LR 0.000268
Train loss: 0.4577;  Loss pred: 0.4577; Loss self: 0.0000; time: 0.60s
Val loss: 0.5129 score: 0.8571 time: 0.51s
Test loss: 0.4579 score: 0.9184 time: 0.46s
Epoch 63/1000, LR 0.000268
Train loss: 0.4327;  Loss pred: 0.4327; Loss self: 0.0000; time: 0.74s
Val loss: 0.5031 score: 0.8571 time: 0.37s
Test loss: 0.4462 score: 0.9184 time: 0.50s
Epoch 64/1000, LR 0.000268
Train loss: 0.4222;  Loss pred: 0.4222; Loss self: 0.0000; time: 0.72s
Val loss: 0.4933 score: 0.8571 time: 0.37s
Test loss: 0.4346 score: 0.9184 time: 0.51s
Epoch 65/1000, LR 0.000268
Train loss: 0.4087;  Loss pred: 0.4087; Loss self: 0.0000; time: 0.69s
Val loss: 0.4837 score: 0.8571 time: 5.32s
Test loss: 0.4235 score: 0.9184 time: 4.64s
Epoch 66/1000, LR 0.000268
Train loss: 0.3940;  Loss pred: 0.3940; Loss self: 0.0000; time: 1.11s
Val loss: 0.4742 score: 0.8571 time: 0.38s
Test loss: 0.4128 score: 0.9388 time: 0.49s
Epoch 67/1000, LR 0.000268
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.73s
Val loss: 0.4649 score: 0.8571 time: 0.51s
Test loss: 0.4025 score: 0.9388 time: 0.49s
Epoch 68/1000, LR 0.000268
Train loss: 0.3744;  Loss pred: 0.3744; Loss self: 0.0000; time: 0.62s
Val loss: 0.4559 score: 0.8571 time: 0.43s
Test loss: 0.3926 score: 0.9388 time: 0.54s
Epoch 69/1000, LR 0.000268
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 8.32s
Val loss: 0.4473 score: 0.8571 time: 0.38s
Test loss: 0.3833 score: 0.9388 time: 0.53s
Epoch 70/1000, LR 0.000268
Train loss: 0.3550;  Loss pred: 0.3550; Loss self: 0.0000; time: 0.61s
Val loss: 0.4390 score: 0.8776 time: 0.40s
Test loss: 0.3743 score: 0.9388 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.3444;  Loss pred: 0.3444; Loss self: 0.0000; time: 0.97s
Val loss: 0.4311 score: 0.8776 time: 0.40s
Test loss: 0.3660 score: 0.9388 time: 0.55s
Epoch 72/1000, LR 0.000267
Train loss: 0.3286;  Loss pred: 0.3286; Loss self: 0.0000; time: 4.71s
Val loss: 0.4233 score: 0.8776 time: 0.39s
Test loss: 0.3581 score: 0.9388 time: 0.59s
Epoch 73/1000, LR 0.000267
Train loss: 0.3179;  Loss pred: 0.3179; Loss self: 0.0000; time: 0.76s
Val loss: 0.4155 score: 0.8776 time: 0.39s
Test loss: 0.3502 score: 0.9388 time: 0.51s
Epoch 74/1000, LR 0.000267
Train loss: 0.3121;  Loss pred: 0.3121; Loss self: 0.0000; time: 2.90s
Val loss: 0.4079 score: 0.8776 time: 3.80s
Test loss: 0.3427 score: 0.9592 time: 4.14s
Epoch 75/1000, LR 0.000267
Train loss: 0.3046;  Loss pred: 0.3046; Loss self: 0.0000; time: 0.79s
Val loss: 0.4006 score: 0.8571 time: 0.38s
Test loss: 0.3357 score: 0.9592 time: 0.48s
Epoch 76/1000, LR 0.000267
Train loss: 0.2844;  Loss pred: 0.2844; Loss self: 0.0000; time: 0.69s
Val loss: 0.3933 score: 0.8571 time: 0.37s
Test loss: 0.3287 score: 0.9388 time: 0.63s
Epoch 77/1000, LR 0.000267
Train loss: 0.2816;  Loss pred: 0.2816; Loss self: 0.0000; time: 0.63s
Val loss: 0.3863 score: 0.8571 time: 0.40s
Test loss: 0.3220 score: 0.9388 time: 0.52s
Epoch 78/1000, LR 0.000267
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 3.30s
Val loss: 0.3794 score: 0.8571 time: 0.38s
Test loss: 0.3156 score: 0.9388 time: 0.46s
Epoch 79/1000, LR 0.000267
Train loss: 0.2522;  Loss pred: 0.2522; Loss self: 0.0000; time: 0.73s
Val loss: 0.3727 score: 0.8571 time: 0.37s
Test loss: 0.3091 score: 0.9388 time: 0.49s
Epoch 80/1000, LR 0.000267
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.62s
Val loss: 0.3663 score: 0.8571 time: 0.40s
Test loss: 0.3031 score: 0.9388 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2261;  Loss pred: 0.2261; Loss self: 0.0000; time: 10.95s
Val loss: 0.3602 score: 0.8776 time: 3.85s
Test loss: 0.2974 score: 0.9388 time: 4.79s
Epoch 82/1000, LR 0.000267
Train loss: 0.2226;  Loss pred: 0.2226; Loss self: 0.0000; time: 0.95s
Val loss: 0.3542 score: 0.8776 time: 0.43s
Test loss: 0.2920 score: 0.9388 time: 0.50s
Epoch 83/1000, LR 0.000266
Train loss: 0.2070;  Loss pred: 0.2070; Loss self: 0.0000; time: 0.73s
Val loss: 0.3483 score: 0.8776 time: 0.53s
Test loss: 0.2866 score: 0.9388 time: 0.47s
Epoch 84/1000, LR 0.000266
Train loss: 0.1995;  Loss pred: 0.1995; Loss self: 0.0000; time: 0.59s
Val loss: 0.3425 score: 0.8776 time: 0.36s
Test loss: 0.2815 score: 0.9388 time: 0.51s
Epoch 85/1000, LR 0.000266
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.73s
Val loss: 0.3369 score: 0.8776 time: 0.42s
Test loss: 0.2767 score: 0.9388 time: 4.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.1825;  Loss pred: 0.1825; Loss self: 0.0000; time: 9.09s
Val loss: 0.3316 score: 0.8571 time: 0.41s
Test loss: 0.2723 score: 0.9388 time: 0.47s
Epoch 87/1000, LR 0.000266
Train loss: 0.1657;  Loss pred: 0.1657; Loss self: 0.0000; time: 0.76s
Val loss: 0.3270 score: 0.8571 time: 0.42s
Test loss: 0.2688 score: 0.9388 time: 0.56s
Epoch 88/1000, LR 0.000266
Train loss: 0.1534;  Loss pred: 0.1534; Loss self: 0.0000; time: 0.64s
Val loss: 0.3227 score: 0.8571 time: 0.40s
Test loss: 0.2656 score: 0.9388 time: 3.95s
Epoch 89/1000, LR 0.000266
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 3.91s
Val loss: 0.3190 score: 0.8571 time: 0.48s
Test loss: 0.2632 score: 0.9388 time: 0.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1414;  Loss pred: 0.1414; Loss self: 0.0000; time: 0.60s
Val loss: 0.3152 score: 0.8571 time: 0.37s
Test loss: 0.2610 score: 0.9388 time: 0.52s
Epoch 91/1000, LR 0.000266
Train loss: 0.1299;  Loss pred: 0.1299; Loss self: 0.0000; time: 0.61s
Val loss: 0.3120 score: 0.8571 time: 0.49s
Test loss: 0.2596 score: 0.9388 time: 0.46s
Epoch 92/1000, LR 0.000266
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.60s
Val loss: 0.3095 score: 0.8571 time: 4.31s
Test loss: 0.2590 score: 0.9388 time: 3.73s
Epoch 93/1000, LR 0.000265
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.78s
Val loss: 0.3080 score: 0.8571 time: 0.35s
Test loss: 0.2594 score: 0.9388 time: 0.48s
Epoch 94/1000, LR 0.000265
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.59s
Val loss: 0.3077 score: 0.8571 time: 0.38s
Test loss: 0.2610 score: 0.9388 time: 0.47s
Epoch 95/1000, LR 0.000265
Train loss: 0.1070;  Loss pred: 0.1070; Loss self: 0.0000; time: 0.79s
Val loss: 0.3071 score: 0.8571 time: 2.20s
Test loss: 0.2626 score: 0.9388 time: 3.02s
Epoch 96/1000, LR 0.000265
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 8.39s
Val loss: 0.3081 score: 0.8571 time: 0.38s
Test loss: 0.2656 score: 0.9388 time: 0.59s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.86s
Val loss: 0.3086 score: 0.8571 time: 0.41s
Test loss: 0.2684 score: 0.9388 time: 0.50s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1070,   Val_Loss: 0.3071,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3071,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2626


[4.20998081099242, 0.3801914050709456, 0.39033605495933443, 0.41736693296115845, 0.3842876589624211, 0.44220690499059856, 0.4389886950375512, 0.4204101220238954, 0.6276955739594996, 2.0759688779944554, 1.103972650016658, 9.992660070885904, 1.2440602959832177, 5.746186608914286, 2.5089665750274435, 5.734940523048863, 7.054660225985572, 1.1251596040092409, 1.1248534000478685, 1.5360058010555804, 10.171286393073387, 8.597733428003266, 1.1504607269307598, 1.441576577955857, 9.332677235943265, 0.6781181229744107, 10.72648668999318, 2.324371569091454, 2.931617929949425, 0.7277763220481575, 1.5108333199750632, 0.8245339489076287, 0.544463471043855, 1.171911892015487, 9.492495149956085, 0.5649784629931673, 2.8314255910227075, 1.2156774729955941, 5.137492283945903, 1.1685249030124396, 4.078842686954886, 9.419127741944976, 9.67569983692374, 0.8611436169594526, 3.763740206020884, 1.0709673109231517, 0.4388017780147493, 1.5168438979890198, 0.6291808818932623, 8.407945835962892, 11.346492246957496, 9.41009756398853, 0.5146344359964132, 3.5777930190088227, 1.0005330480635166, 1.1160023880656809, 9.421909576980397, 9.21827711199876, 10.69503390591126, 0.39239069807808846, 0.38862070499453694, 3.138231493998319, 0.6210656720213592, 0.4350298239151016, 0.729491326957941, 1.010295914951712, 0.6096510200295597, 8.972448851098306, 5.460545056965202, 0.6745391159784049, 9.495890646008775, 1.0324437720701098, 0.9835861730389297, 5.835210973047651, 0.40411011897958815, 1.4841283910209313, 1.5122076099505648, 10.834568899008445, 7.46605524700135, 0.5550562969874591, 9.993400639970787, 0.5274411250138655, 1.246264089946635, 9.96703715599142, 2.1428094840375707, 9.963717818027362, 1.2508718410972506, 9.819924705079757, 4.2843708449508995, 0.5802636609878391, 0.6583686569938436, 9.259317980962805, 9.482170381001197, 10.229828840005212, 0.6071437239879742, 1.2517227079952136, 1.3844665710348636, 7.976436157012358, 4.490878814016469, 9.452557650045492, 3.840755420969799, 0.4938006700249389, 0.49492414097767323, 4.899126550066285, 0.4998156710062176, 0.4637824500678107, 0.4855843320256099, 0.4752942699706182, 0.520623543066904, 0.466454474022612, 0.47361462307162583, 0.5646954899420962, 0.48370779491961, 0.6166399670764804, 1.3979586299974471, 0.4993271860294044, 0.4778230939991772, 0.6727059360127896, 3.8333134619751945, 0.5103551289066672, 0.5956580019555986, 0.6662038530921564, 0.5077555420575663, 0.5588193919975311, 0.5598046509549022, 0.6430651869159192, 0.48609543195925653, 6.013729300000705, 0.4988483639899641, 0.487226419034414, 6.192579412017949, 0.5005399460205808, 0.4749131969874725, 0.49116037297062576, 0.575866905041039, 0.4953831110615283, 0.51065267494414, 0.48185404704418033, 3.713547701947391, 0.4880414210492745, 0.5846331840148196, 0.5722087599569932, 4.0329966300632805, 0.4690501670120284, 0.45878724299836904, 0.6339313520584255, 2.0021149369422346, 0.5503998580388725, 0.47388584807049483, 0.6623190869577229, 0.5371718959650025, 0.4643954470520839, 0.5758318859152496, 0.4789380069123581, 7.2886938150040805, 0.46737052395474166, 0.5083874700358137, 0.5135433069663122, 4.641962586087175, 0.4982703800778836, 0.4913730879779905, 0.5481601220089942, 0.531053998041898, 0.47825528401881456, 0.5528499009087682, 0.591942387050949, 0.5123313979711384, 4.14940255205147, 0.4853993789292872, 0.6360672489972785, 0.5237605769652873, 0.46540731901768595, 0.4956611959496513, 0.498724611941725, 4.790384551975876, 0.4994853580137715, 0.47716370096895844, 0.5158337809843943, 4.216001939959824, 0.47553391999099404, 0.5640538999577984, 3.9582338080508634, 0.4682652170304209, 0.5215823460603133, 0.4685269830515608, 3.736323565011844, 0.4818846059497446, 0.47638791508506984, 3.0255380660528317, 0.5986179850297049, 0.5003632719162852]
[0.08591797573453917, 0.0077590082667539926, 0.007966041937945601, 0.008517692509411397, 0.00784260528494737, 0.009024630714093849, 0.008958952959950025, 0.008579798408650927, 0.012810113754275501, 0.042366711795805215, 0.022530054081972613, 0.20393183818134497, 0.025388985632310564, 0.11726911446763849, 0.05120339949035599, 0.11703960251120128, 0.14397265767317494, 0.022962440898147772, 0.022956191837711602, 0.0313470571643996, 0.2075772733280283, 0.17546394751027075, 0.02347879034552571, 0.029419930162364428, 0.19046280073353603, 0.013839145366824707, 0.2189078916325139, 0.04743615447125417, 0.059828937345906635, 0.014852578000982806, 0.030833333060715576, 0.01682722344709446, 0.011111499409058265, 0.023916569224805857, 0.1937243908154303, 0.011530172714146272, 0.0577841957351573, 0.02480974434684886, 0.10484678130501843, 0.02384744700025387, 0.08324168748887521, 0.19222709677438726, 0.19746326197803551, 0.01757435952978475, 0.0768110246126711, 0.021856475733125543, 0.008955138326831619, 0.030955997918143262, 0.012840426161086984, 0.17159073134618147, 0.2315610662644387, 0.19204280742833732, 0.010502743591763536, 0.07301618406140455, 0.020419041797214627, 0.022775558940115934, 0.19228386891796728, 0.18812810432650529, 0.2182659980798216, 0.00800797343016507, 0.007931034795806877, 0.06404554069384324, 0.012674809633088964, 0.008878159671736767, 0.01488757810118247, 0.020618283978606364, 0.012441857551623667, 0.18311120104282258, 0.11143969504010616, 0.013766104407722548, 0.1937936866532403, 0.021070281062655304, 0.020073187204876115, 0.11908593822546228, 0.008247145285297717, 0.030288334510631253, 0.030861379794909487, 0.22111365100017236, 0.15236847442859897, 0.011327679530356308, 0.2039469518361385, 0.010764104592119704, 0.025433961019319083, 0.2034089215508453, 0.04373080579668512, 0.20334117995974207, 0.025527996757086748, 0.20040662663428074, 0.0874361396928755, 0.011842115530364064, 0.013436095040690686, 0.1889656730808736, 0.19351368124492238, 0.2087720171429635, 0.012390688244652535, 0.02554536138765742, 0.028254419817038034, 0.16278441136759914, 0.09165058804115242, 0.19290933979684677, 0.0783827636932612, 0.010077564694386507, 0.01010049267301374, 0.09998217449114867, 0.01020031981645342, 0.009464947960567566, 0.009909884327053264, 0.00969988306062486, 0.010624970266671509, 0.00951947906168596, 0.00966560455248216, 0.01152439775392033, 0.009871587651420613, 0.012584489124009803, 0.028529767959131574, 0.010190350735293967, 0.009751491714268923, 0.013728692571689583, 0.0782308869790856, 0.010415410794013617, 0.012156285754195889, 0.013595997001880742, 0.010362358001174823, 0.011404477387704715, 0.011424584713365351, 0.013123779324814677, 0.00992031493794401, 0.12272916938776948, 0.010180578856938044, 0.009943396306824776, 0.1263791716738357, 0.010215100939195526, 0.009692106060968826, 0.010023681081033178, 0.011752385817164061, 0.010109859409418945, 0.010421483162125307, 0.00983375606212613, 0.07578668779484471, 0.009960029001005602, 0.011931289469690196, 0.011677729795040677, 0.08230605367476082, 0.00957245238800058, 0.009363004959150389, 0.012937374531804604, 0.040859488509025196, 0.011232650164058623, 0.00967113975654071, 0.013516716060361691, 0.010962691754387806, 0.009477458103103754, 0.011751671141127542, 0.009774245039027716, 0.1487488533674302, 0.009538173958260034, 0.01037525449052681, 0.010480475652373719, 0.09473393032830969, 0.010168783266895584, 0.01002802220363246, 0.011186941265489678, 0.010837836694732612, 0.009760311918751317, 0.011282651038954452, 0.012080456878590797, 0.010455742815737518, 0.08468168473574428, 0.009906109774067086, 0.012980964265250581, 0.010688991366638517, 0.009498108551381347, 0.010115534611217374, 0.010178053304933163, 0.097762950040324, 0.010193578734974928, 0.009738034713652213, 0.010527220020089679, 0.08604085591754743, 0.009704773877367226, 0.011511304080771396, 0.0807802817969564, 0.009556433000620835, 0.01064453767470027, 0.009561775164317568, 0.07625150132677233, 0.009834379713260094, 0.009722202348674894, 0.06174567481740473, 0.012216693572034793, 0.010211495345230309]
[11.639007919480093, 128.8824506457645, 125.53285656664451, 117.40268845054887, 127.50864842316373, 110.80785814740219, 111.62018647384194, 116.55285501716565, 78.06331927897519, 23.603436698597207, 44.38515754829672, 4.903599207058375, 39.38715845060697, 8.527394485237282, 19.529953283440626, 8.544116508805597, 6.945763287012799, 43.54937719537749, 43.56123206625396, 31.900921185535896, 4.817483070122658, 5.699176464392864, 42.59163207658896, 33.99056335216099, 5.250369080726867, 72.25879730963783, 4.568131338447701, 21.080966852108343, 16.71431993214932, 67.32837894767019, 32.43243271918888, 59.42751061362229, 89.9968548965394, 41.812017041424866, 5.16197261372598, 86.728969703386, 17.305769982216365, 40.306743431921426, 9.537727220169186, 41.93320987313042, 12.013211530984933, 5.202180216942454, 5.0642331640972955, 56.901077863191304, 13.018964465617026, 45.75303046155817, 111.66773348478314, 32.303917407033474, 77.87903512349988, 5.827820606361986, 4.3185152674069025, 5.207172366365035, 95.21321655269394, 13.695593831074879, 48.97389456034174, 43.9067160823281, 5.200644264270671, 5.315526904286732, 4.58156565290711, 124.8755392011068, 126.08695154492301, 15.613889572426249, 78.89664846637157, 112.63595575820248, 67.17009262376756, 48.50064151980858, 80.3738505967302, 5.461162366392534, 8.973463177910787, 72.64219203793175, 5.160126819762318, 47.460211708916745, 49.81769909250301, 8.397297068833822, 121.25407827878475, 33.0160114828697, 32.402958216565146, 4.522561114959024, 6.5630374245730705, 88.27933358462035, 4.903235821849652, 92.9013641071539, 39.31750934274145, 4.916205210546942, 22.86717525053705, 4.917843007491065, 39.17267811946087, 4.989854960359599, 11.436918458575114, 84.44437123045505, 74.42638631027387, 5.291966438645286, 5.167593286256286, 4.789913963015537, 80.70576712569387, 39.14605022903153, 35.3926927707423, 6.1430943638810955, 10.911004734099313, 5.183782190396287, 12.757906877503643, 99.23032303201474, 99.00507157158547, 10.00178286869055, 98.0361418067471, 105.65298448191731, 100.90935141090121, 103.09402636608493, 94.11791044129393, 105.04776506361618, 103.45964337462954, 86.77243022611081, 101.30082772005684, 79.46289993545399, 35.051108772860815, 98.13204922736669, 102.54841303272038, 72.84014808971212, 12.782674958898804, 96.01157551795865, 82.26196884643304, 73.55106064392847, 96.50313180519586, 87.68485972694464, 87.53053393968217, 76.19756285517404, 100.80325133379792, 8.148022226406875, 98.22624175426941, 100.56925914877168, 7.912696267553004, 97.89428474103295, 103.17674958460367, 99.76374865838471, 85.08910578305942, 98.91334384614099, 95.95563169302908, 101.69054364195738, 13.19492946712501, 100.40131408242247, 83.81323766725825, 85.6330825897926, 12.149774595579355, 104.46643759268385, 106.80331841784492, 77.29543560338686, 24.474119390386306, 89.02618575264846, 103.40042902633971, 73.98246700857614, 91.21847283535553, 105.51352368126133, 85.09428046367647, 102.30969205366618, 6.7227408975709, 104.84187061130318, 96.38317796571216, 95.41551673501674, 10.555879994996538, 98.3401822768211, 99.72056101329424, 89.38993923968079, 92.2693364152662, 102.4557420218118, 88.63165195372989, 82.77832618832637, 95.6412201048829, 11.808928968767887, 100.94780118607919, 77.03587958230129, 93.55419662149852, 105.28411994770961, 98.85784967717619, 98.25061532300225, 10.228823900951566, 98.10097375997357, 102.69012479469318, 94.99184001964854, 11.622385543889703, 103.04207111225209, 86.87113058462339, 12.379258622958623, 104.6415540123637, 93.94489742629034, 104.58309077709535, 13.1144958800817, 101.6840948953455, 102.85735311159172, 16.195466370028598, 81.85520853932998, 97.92885039771284]
Elapsed: 2.578704074108656~3.22403614002625
Time per graph: 0.05262661375731951~0.06579665591890306
Speed: 60.0599217472132~40.780593920785904
Total Time: 0.5015
best val loss: 0.3070884644985199 test_score: 0.9388

Testing...
Test loss: 0.3743 score: 0.9388 time: 0.53s
test Score 0.9388
Epoch Time List: [9.233843691996299, 4.273922999971546, 1.8734583469340578, 1.5833287599962205, 11.893710596952587, 1.8830520879710093, 1.9836342639755458, 19.344367160927504, 2.0642795800231397, 12.737888444913551, 4.96762684697751, 22.41127654199954, 10.781780915916897, 18.864039964042604, 15.717427596100606, 14.846924169105478, 11.82442497194279, 10.296903591020964, 16.192355770152062, 24.800162309082225, 13.522720113047399, 24.508441373938695, 4.52520399610512, 25.09761512989644, 12.116193930967711, 11.589682840975001, 22.232915799948387, 17.93188690394163, 5.754702292964794, 19.709493588889018, 26.612625295994803, 3.4461163041414693, 26.007582439109683, 19.811232370091602, 12.861963052069768, 10.804033781052567, 24.716521229944192, 4.5801692879758775, 16.70970431691967, 22.81190911110025, 15.081666366080754, 12.556934696971439, 13.466582978959195, 12.09752483794, 29.544525985023938, 20.111976254964247, 9.357704854919575, 16.07004031992983, 13.354963763034903, 19.80196435994003, 14.489574330858886, 21.921089339070022, 5.348641948075965, 15.57797521003522, 16.524153239908628, 16.390400325064547, 22.891237517120317, 15.60264636296779, 14.637129032984376, 4.735733990906738, 1.6008901899913326, 6.096560187987052, 18.164203705848195, 2.7867725040996447, 17.176505322102457, 18.557462551048957, 11.698768873000517, 15.065711592906155, 20.143220031051897, 19.742097233072855, 21.19160205195658, 4.760558368987404, 18.515061816899106, 8.955553437001072, 15.873269472038373, 2.6983683368889615, 25.04117266391404, 23.571269833948463, 25.39393110992387, 23.331935737864114, 22.8173843229888, 7.278773032128811, 13.63293563714251, 20.77856893907301, 33.50516891491134, 22.672362343990244, 6.25969225098379, 30.632361622992903, 25.29447036003694, 3.3521978730568662, 21.265450199018233, 11.829835025127977, 36.21029985300265, 14.480679856031202, 6.786627368885092, 24.92128497012891, 15.845115873031318, 25.321119840838946, 15.053179595968686, 11.680653774994425, 24.136583377839997, 3.469886211096309, 1.5838435859186575, 7.298535927082412, 6.022594282985665, 1.709998874925077, 11.196586000965908, 1.4482517780270427, 1.6289579261792824, 11.51035005087033, 1.6954126709606498, 1.5020259550074115, 14.44323162699584, 1.7271297071129084, 2.540792511892505, 11.393652789993212, 1.5995761000085622, 1.728476551012136, 16.551186132943258, 3.901185891008936, 2.285382463829592, 5.845565511030145, 1.5128009549807757, 1.5628420900320634, 14.435235733981244, 1.605706945876591, 1.5972592598991469, 7.169407909153961, 4.088765730964951, 1.4359356899512932, 7.3812039020704105, 1.6978958030231297, 1.5625746749574319, 1.4198061040369794, 5.047519272891805, 1.7379151321947575, 1.8361468290677294, 1.5440425740089267, 4.817973237019032, 10.432379024103284, 1.8594853788381442, 1.7171884099952877, 5.080781089956872, 1.5982886939309537, 1.5090640580747277, 1.5977851030183956, 11.716409043874592, 1.7393232170725241, 1.688392161973752, 1.813432935043238, 15.59004999906756, 1.501809053006582, 1.7768886839039624, 1.4568615849129856, 17.5330061231507, 1.5799753499450162, 1.618988123955205, 1.5933187020709738, 10.646852414007299, 1.9905598999466747, 1.7274246150627732, 1.595356015022844, 9.223113260115497, 1.4895012329798192, 1.9209724820684642, 5.680527975200675, 1.650467730127275, 10.841851214878261, 1.6529437799472362, 1.6978841189993545, 1.5489875508937985, 4.141320570022799, 1.5937002780847251, 1.513193455990404, 19.583149680052884, 1.8750452811364084, 1.729973212000914, 1.4591894760960713, 5.363501476123929, 9.975705171003938, 1.7332479441538453, 4.9946887290570885, 4.859505039989017, 1.490568722016178, 1.5649470059433952, 8.640135777881369, 1.6089777159504592, 1.44774947501719, 6.012049405951984, 9.366986174951307, 1.7734386438969523]
Total Epoch List: [94, 97]
Total Time List: [10.230493623996153, 0.5015418929979205]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d2590>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.45s
Epoch 2/1000, LR 0.000000
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 3.43s
Epoch 3/1000, LR 0.000030
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 12.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.47s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.44s
Epoch 5/1000, LR 0.000090
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 5.47s
Epoch 6/1000, LR 0.000120
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 4.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.47s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.44s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.48s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 9.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 6.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.87s
Epoch 10/1000, LR 0.000240
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.49s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 4.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 1.96s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.45s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.57s
Epoch 14/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.63s
Val loss: 0.6923 score: 0.5306 time: 0.47s
Test loss: 0.6922 score: 0.5208 time: 5.17s
Epoch 15/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 10.21s
Val loss: 0.6921 score: 0.5714 time: 0.58s
Test loss: 0.6920 score: 0.6042 time: 0.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.61s
Val loss: 0.6920 score: 0.6327 time: 0.43s
Test loss: 0.6918 score: 0.6458 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.69s
Val loss: 0.6919 score: 0.6327 time: 0.52s
Test loss: 0.6916 score: 0.6667 time: 0.55s
Epoch 18/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 5.17s
Val loss: 0.6917 score: 0.6327 time: 5.39s
Test loss: 0.6914 score: 0.6875 time: 1.43s
Epoch 19/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.59s
Val loss: 0.6915 score: 0.6531 time: 0.41s
Test loss: 0.6911 score: 0.6875 time: 0.56s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.62s
Val loss: 0.6912 score: 0.6531 time: 0.42s
Test loss: 0.6909 score: 0.6875 time: 0.45s
Epoch 21/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.60s
Val loss: 0.6910 score: 0.6531 time: 0.44s
Test loss: 0.6905 score: 0.7083 time: 0.62s
Epoch 22/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 10.24s
Val loss: 0.6907 score: 0.6735 time: 0.45s
Test loss: 0.6902 score: 0.7083 time: 0.47s
Epoch 23/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.62s
Val loss: 0.6903 score: 0.6735 time: 0.44s
Test loss: 0.6897 score: 0.7083 time: 0.57s
Epoch 24/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.65s
Val loss: 0.6899 score: 0.6939 time: 0.43s
Test loss: 0.6892 score: 0.7083 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.61s
Val loss: 0.6894 score: 0.6735 time: 0.42s
Test loss: 0.6887 score: 0.6875 time: 0.56s
Epoch 26/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 7.46s
Val loss: 0.6888 score: 0.6735 time: 0.40s
Test loss: 0.6880 score: 0.6875 time: 0.45s
Epoch 27/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.76s
Val loss: 0.6882 score: 0.6735 time: 0.41s
Test loss: 0.6873 score: 0.6875 time: 0.57s
Epoch 28/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.87s
Val loss: 0.6875 score: 0.6122 time: 0.41s
Test loss: 0.6865 score: 0.6875 time: 0.59s
Epoch 29/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 8.80s
Val loss: 0.6867 score: 0.6531 time: 1.46s
Test loss: 0.6856 score: 0.7083 time: 1.20s
Epoch 30/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.58s
Val loss: 0.6858 score: 0.6531 time: 0.41s
Test loss: 0.6846 score: 0.7083 time: 0.44s
Epoch 31/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.70s
Val loss: 0.6849 score: 0.6531 time: 0.54s
Test loss: 0.6835 score: 0.7292 time: 0.43s
Epoch 32/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.62s
Val loss: 0.6840 score: 0.6531 time: 0.47s
Test loss: 0.6824 score: 0.7500 time: 4.16s
Epoch 33/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 11.93s
Val loss: 0.6830 score: 0.6735 time: 0.52s
Test loss: 0.6812 score: 0.7500 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.60s
Val loss: 0.6819 score: 0.6531 time: 0.41s
Test loss: 0.6799 score: 0.7292 time: 0.49s
Epoch 35/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.64s
Val loss: 0.6808 score: 0.6531 time: 0.55s
Test loss: 0.6785 score: 0.7083 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 11.47s
Val loss: 0.6795 score: 0.6531 time: 0.43s
Test loss: 0.6769 score: 0.7083 time: 0.47s
Epoch 37/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.61s
Val loss: 0.6781 score: 0.6531 time: 0.44s
Test loss: 0.6752 score: 0.6875 time: 0.56s
Epoch 38/1000, LR 0.000270
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.62s
Val loss: 0.6765 score: 0.6531 time: 0.45s
Test loss: 0.6734 score: 0.6875 time: 0.52s
Epoch 39/1000, LR 0.000269
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 8.47s
Val loss: 0.6747 score: 0.6531 time: 0.55s
Test loss: 0.6713 score: 0.6875 time: 0.45s
Epoch 40/1000, LR 0.000269
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.63s
Val loss: 0.6728 score: 0.6531 time: 0.43s
Test loss: 0.6691 score: 0.6875 time: 0.46s
Epoch 41/1000, LR 0.000269
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.65s
Val loss: 0.6708 score: 0.6531 time: 0.59s
Test loss: 0.6666 score: 0.7083 time: 0.45s
Epoch 42/1000, LR 0.000269
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.68s
Val loss: 0.6685 score: 0.6531 time: 0.50s
Test loss: 0.6638 score: 0.7083 time: 4.94s
Epoch 43/1000, LR 0.000269
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 3.16s
Val loss: 0.6659 score: 0.6531 time: 0.43s
Test loss: 0.6608 score: 0.7083 time: 0.55s
Epoch 44/1000, LR 0.000269
Train loss: 0.6547;  Loss pred: 0.6547; Loss self: 0.0000; time: 0.62s
Val loss: 0.6631 score: 0.6735 time: 0.41s
Test loss: 0.6575 score: 0.7083 time: 0.47s
Epoch 45/1000, LR 0.000269
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.63s
Val loss: 0.6601 score: 0.6735 time: 0.49s
Test loss: 0.6538 score: 0.7292 time: 3.74s
Epoch 46/1000, LR 0.000269
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 3.02s
Val loss: 0.6567 score: 0.6735 time: 0.42s
Test loss: 0.6497 score: 0.7292 time: 0.43s
Epoch 47/1000, LR 0.000269
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.64s
Val loss: 0.6530 score: 0.6735 time: 0.42s
Test loss: 0.6453 score: 0.7292 time: 0.55s
Epoch 48/1000, LR 0.000269
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.60s
Val loss: 0.6490 score: 0.6735 time: 0.44s
Test loss: 0.6406 score: 0.7500 time: 0.51s
Epoch 49/1000, LR 0.000269
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 0.71s
Val loss: 0.6448 score: 0.6735 time: 5.05s
Test loss: 0.6355 score: 0.7500 time: 3.97s
Epoch 50/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.59s
Val loss: 0.6402 score: 0.6531 time: 0.42s
Test loss: 0.6300 score: 0.7500 time: 0.47s
Epoch 51/1000, LR 0.000269
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.68s
Val loss: 0.6352 score: 0.6531 time: 0.46s
Test loss: 0.6240 score: 0.7500 time: 0.60s
Epoch 52/1000, LR 0.000269
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.65s
Val loss: 0.6298 score: 0.6531 time: 3.35s
Test loss: 0.6175 score: 0.7500 time: 5.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.61s
Val loss: 0.6240 score: 0.6531 time: 0.41s
Test loss: 0.6104 score: 0.7500 time: 0.58s
Epoch 54/1000, LR 0.000269
Train loss: 0.5961;  Loss pred: 0.5961; Loss self: 0.0000; time: 0.59s
Val loss: 0.6178 score: 0.6531 time: 0.41s
Test loss: 0.6030 score: 0.7500 time: 0.46s
Epoch 55/1000, LR 0.000269
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.71s
Val loss: 0.6114 score: 0.6531 time: 5.39s
Test loss: 0.5951 score: 0.7500 time: 1.58s
Epoch 56/1000, LR 0.000269
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.64s
Val loss: 0.6045 score: 0.6531 time: 0.41s
Test loss: 0.5868 score: 0.7500 time: 0.41s
Epoch 57/1000, LR 0.000269
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.60s
Val loss: 0.5973 score: 0.6735 time: 0.51s
Test loss: 0.5780 score: 0.7500 time: 0.43s
Epoch 58/1000, LR 0.000269
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.58s
Val loss: 0.5898 score: 0.6939 time: 0.41s
Test loss: 0.5689 score: 0.7708 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.78s
Val loss: 0.5819 score: 0.6939 time: 5.92s
Test loss: 0.5593 score: 0.7917 time: 2.13s
Epoch 60/1000, LR 0.000268
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.57s
Val loss: 0.5736 score: 0.6939 time: 0.40s
Test loss: 0.5494 score: 0.7917 time: 0.56s
Epoch 61/1000, LR 0.000268
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.77s
Val loss: 0.5650 score: 0.6939 time: 0.40s
Test loss: 0.5390 score: 0.7708 time: 0.53s
Epoch 62/1000, LR 0.000268
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.64s
Val loss: 0.5559 score: 0.7143 time: 2.66s
Test loss: 0.5281 score: 0.7917 time: 5.48s
Epoch 63/1000, LR 0.000268
Train loss: 0.5015;  Loss pred: 0.5015; Loss self: 0.0000; time: 9.28s
Val loss: 0.5463 score: 0.7755 time: 0.58s
Test loss: 0.5169 score: 0.7917 time: 0.51s
Epoch 64/1000, LR 0.000268
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.71s
Val loss: 0.5364 score: 0.7959 time: 0.39s
Test loss: 0.5053 score: 0.8333 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.4782;  Loss pred: 0.4782; Loss self: 0.0000; time: 0.80s
Val loss: 0.5262 score: 0.7959 time: 0.64s
Test loss: 0.4935 score: 0.8333 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.4681;  Loss pred: 0.4681; Loss self: 0.0000; time: 11.37s
Val loss: 0.5157 score: 0.7959 time: 0.42s
Test loss: 0.4814 score: 0.8750 time: 0.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4412;  Loss pred: 0.4412; Loss self: 0.0000; time: 0.65s
Val loss: 0.5049 score: 0.8163 time: 0.41s
Test loss: 0.4692 score: 0.8958 time: 0.56s
Epoch 68/1000, LR 0.000268
Train loss: 0.4422;  Loss pred: 0.4422; Loss self: 0.0000; time: 0.70s
Val loss: 0.4938 score: 0.8571 time: 0.46s
Test loss: 0.4571 score: 0.8958 time: 0.52s
Epoch 69/1000, LR 0.000268
Train loss: 0.4230;  Loss pred: 0.4230; Loss self: 0.0000; time: 0.65s
Val loss: 0.4827 score: 0.8571 time: 0.48s
Test loss: 0.4450 score: 0.9167 time: 3.41s
Epoch 70/1000, LR 0.000268
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 0.59s
Val loss: 0.4716 score: 0.8571 time: 0.41s
Test loss: 0.4332 score: 0.9167 time: 0.44s
Epoch 71/1000, LR 0.000268
Train loss: 0.4003;  Loss pred: 0.4003; Loss self: 0.0000; time: 0.62s
Val loss: 0.4607 score: 0.8571 time: 0.42s
Test loss: 0.4215 score: 0.9167 time: 0.62s
Epoch 72/1000, LR 0.000267
Train loss: 0.3847;  Loss pred: 0.3847; Loss self: 0.0000; time: 0.63s
Val loss: 0.4501 score: 0.8571 time: 2.66s
Test loss: 0.4101 score: 0.9167 time: 5.47s
Epoch 73/1000, LR 0.000267
Train loss: 0.3675;  Loss pred: 0.3675; Loss self: 0.0000; time: 10.83s
Val loss: 0.4401 score: 0.8571 time: 0.42s
Test loss: 0.3991 score: 0.9375 time: 0.54s
Epoch 74/1000, LR 0.000267
Train loss: 0.3543;  Loss pred: 0.3543; Loss self: 0.0000; time: 0.78s
Val loss: 0.4304 score: 0.8571 time: 0.41s
Test loss: 0.3886 score: 0.9375 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.3468;  Loss pred: 0.3468; Loss self: 0.0000; time: 0.60s
Val loss: 0.4213 score: 0.8776 time: 0.42s
Test loss: 0.3787 score: 0.9375 time: 0.74s
Epoch 76/1000, LR 0.000267
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 15.74s
Val loss: 0.4123 score: 0.8980 time: 0.56s
Test loss: 0.3690 score: 0.9375 time: 0.56s
Epoch 77/1000, LR 0.000267
Train loss: 0.3173;  Loss pred: 0.3173; Loss self: 0.0000; time: 0.65s
Val loss: 0.4035 score: 0.8980 time: 0.43s
Test loss: 0.3590 score: 0.9375 time: 0.80s
Epoch 78/1000, LR 0.000267
Train loss: 0.3044;  Loss pred: 0.3044; Loss self: 0.0000; time: 12.29s
Val loss: 0.3946 score: 0.8980 time: 4.23s
Test loss: 0.3488 score: 0.9375 time: 0.58s
Epoch 79/1000, LR 0.000267
Train loss: 0.2852;  Loss pred: 0.2852; Loss self: 0.0000; time: 0.92s
Val loss: 0.3859 score: 0.8980 time: 0.53s
Test loss: 0.3385 score: 0.9375 time: 0.41s
Epoch 80/1000, LR 0.000267
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 11.90s
Val loss: 0.3777 score: 0.8980 time: 2.68s
Test loss: 0.3291 score: 0.9375 time: 0.44s
Epoch 81/1000, LR 0.000267
Train loss: 0.2630;  Loss pred: 0.2630; Loss self: 0.0000; time: 0.60s
Val loss: 0.3698 score: 0.8980 time: 0.54s
Test loss: 0.3198 score: 0.9375 time: 0.45s
Epoch 82/1000, LR 0.000267
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.67s
Val loss: 0.3619 score: 0.8980 time: 0.46s
Test loss: 0.3102 score: 0.9375 time: 0.46s
Epoch 83/1000, LR 0.000266
Train loss: 0.2315;  Loss pred: 0.2315; Loss self: 0.0000; time: 0.75s
Val loss: 0.3542 score: 0.8980 time: 0.68s
Test loss: 0.3009 score: 0.9375 time: 4.98s
Epoch 84/1000, LR 0.000266
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 3.71s
Val loss: 0.3470 score: 0.9184 time: 0.44s
Test loss: 0.2924 score: 0.9375 time: 0.66s
Epoch 85/1000, LR 0.000266
Train loss: 0.2122;  Loss pred: 0.2122; Loss self: 0.0000; time: 0.67s
Val loss: 0.3400 score: 0.9184 time: 0.44s
Test loss: 0.2839 score: 0.9375 time: 0.56s
Epoch 86/1000, LR 0.000266
Train loss: 0.2084;  Loss pred: 0.2084; Loss self: 0.0000; time: 0.61s
Val loss: 0.3335 score: 0.9184 time: 0.43s
Test loss: 0.2764 score: 0.9167 time: 0.47s
Epoch 87/1000, LR 0.000266
Train loss: 0.1889;  Loss pred: 0.1889; Loss self: 0.0000; time: 12.67s
Val loss: 0.3272 score: 0.9184 time: 0.54s
Test loss: 0.2688 score: 0.8958 time: 0.46s
Epoch 88/1000, LR 0.000266
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.62s
Val loss: 0.3215 score: 0.9184 time: 0.46s
Test loss: 0.2621 score: 0.8958 time: 0.45s
Epoch 89/1000, LR 0.000266
Train loss: 0.1696;  Loss pred: 0.1696; Loss self: 0.0000; time: 0.66s
Val loss: 0.3161 score: 0.9184 time: 0.54s
Test loss: 0.2558 score: 0.8958 time: 0.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.64s
Val loss: 0.3112 score: 0.9184 time: 0.50s
Test loss: 0.2498 score: 0.8958 time: 0.63s
Epoch 91/1000, LR 0.000266
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 10.15s
Val loss: 0.3069 score: 0.9184 time: 0.42s
Test loss: 0.2449 score: 0.8958 time: 0.55s
Epoch 92/1000, LR 0.000266
Train loss: 0.1367;  Loss pred: 0.1367; Loss self: 0.0000; time: 0.82s
Val loss: 0.3035 score: 0.9184 time: 0.40s
Test loss: 0.2414 score: 0.8958 time: 0.44s
Epoch 93/1000, LR 0.000265
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.72s
Val loss: 0.3012 score: 0.9184 time: 6.66s
Test loss: 0.2392 score: 0.8958 time: 4.77s
Epoch 94/1000, LR 0.000265
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 2.28s
Val loss: 0.2999 score: 0.9184 time: 0.45s
Test loss: 0.2379 score: 0.8958 time: 0.46s
Epoch 95/1000, LR 0.000265
Train loss: 0.1162;  Loss pred: 0.1162; Loss self: 0.0000; time: 0.60s
Val loss: 0.3002 score: 0.8980 time: 0.42s
Test loss: 0.2378 score: 0.8958 time: 0.55s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.63s
Val loss: 0.3017 score: 0.8980 time: 1.27s
Test loss: 0.2384 score: 0.8958 time: 6.14s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.1240,   Val_Loss: 0.2999,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2999,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2379


[4.20998081099242, 0.3801914050709456, 0.39033605495933443, 0.41736693296115845, 0.3842876589624211, 0.44220690499059856, 0.4389886950375512, 0.4204101220238954, 0.6276955739594996, 2.0759688779944554, 1.103972650016658, 9.992660070885904, 1.2440602959832177, 5.746186608914286, 2.5089665750274435, 5.734940523048863, 7.054660225985572, 1.1251596040092409, 1.1248534000478685, 1.5360058010555804, 10.171286393073387, 8.597733428003266, 1.1504607269307598, 1.441576577955857, 9.332677235943265, 0.6781181229744107, 10.72648668999318, 2.324371569091454, 2.931617929949425, 0.7277763220481575, 1.5108333199750632, 0.8245339489076287, 0.544463471043855, 1.171911892015487, 9.492495149956085, 0.5649784629931673, 2.8314255910227075, 1.2156774729955941, 5.137492283945903, 1.1685249030124396, 4.078842686954886, 9.419127741944976, 9.67569983692374, 0.8611436169594526, 3.763740206020884, 1.0709673109231517, 0.4388017780147493, 1.5168438979890198, 0.6291808818932623, 8.407945835962892, 11.346492246957496, 9.41009756398853, 0.5146344359964132, 3.5777930190088227, 1.0005330480635166, 1.1160023880656809, 9.421909576980397, 9.21827711199876, 10.69503390591126, 0.39239069807808846, 0.38862070499453694, 3.138231493998319, 0.6210656720213592, 0.4350298239151016, 0.729491326957941, 1.010295914951712, 0.6096510200295597, 8.972448851098306, 5.460545056965202, 0.6745391159784049, 9.495890646008775, 1.0324437720701098, 0.9835861730389297, 5.835210973047651, 0.40411011897958815, 1.4841283910209313, 1.5122076099505648, 10.834568899008445, 7.46605524700135, 0.5550562969874591, 9.993400639970787, 0.5274411250138655, 1.246264089946635, 9.96703715599142, 2.1428094840375707, 9.963717818027362, 1.2508718410972506, 9.819924705079757, 4.2843708449508995, 0.5802636609878391, 0.6583686569938436, 9.259317980962805, 9.482170381001197, 10.229828840005212, 0.6071437239879742, 1.2517227079952136, 1.3844665710348636, 7.976436157012358, 4.490878814016469, 9.452557650045492, 3.840755420969799, 0.4938006700249389, 0.49492414097767323, 4.899126550066285, 0.4998156710062176, 0.4637824500678107, 0.4855843320256099, 0.4752942699706182, 0.520623543066904, 0.466454474022612, 0.47361462307162583, 0.5646954899420962, 0.48370779491961, 0.6166399670764804, 1.3979586299974471, 0.4993271860294044, 0.4778230939991772, 0.6727059360127896, 3.8333134619751945, 0.5103551289066672, 0.5956580019555986, 0.6662038530921564, 0.5077555420575663, 0.5588193919975311, 0.5598046509549022, 0.6430651869159192, 0.48609543195925653, 6.013729300000705, 0.4988483639899641, 0.487226419034414, 6.192579412017949, 0.5005399460205808, 0.4749131969874725, 0.49116037297062576, 0.575866905041039, 0.4953831110615283, 0.51065267494414, 0.48185404704418033, 3.713547701947391, 0.4880414210492745, 0.5846331840148196, 0.5722087599569932, 4.0329966300632805, 0.4690501670120284, 0.45878724299836904, 0.6339313520584255, 2.0021149369422346, 0.5503998580388725, 0.47388584807049483, 0.6623190869577229, 0.5371718959650025, 0.4643954470520839, 0.5758318859152496, 0.4789380069123581, 7.2886938150040805, 0.46737052395474166, 0.5083874700358137, 0.5135433069663122, 4.641962586087175, 0.4982703800778836, 0.4913730879779905, 0.5481601220089942, 0.531053998041898, 0.47825528401881456, 0.5528499009087682, 0.591942387050949, 0.5123313979711384, 4.14940255205147, 0.4853993789292872, 0.6360672489972785, 0.5237605769652873, 0.46540731901768595, 0.4956611959496513, 0.498724611941725, 4.790384551975876, 0.4994853580137715, 0.47716370096895844, 0.5158337809843943, 4.216001939959824, 0.47553391999099404, 0.5640538999577984, 3.9582338080508634, 0.4682652170304209, 0.5215823460603133, 0.4685269830515608, 3.736323565011844, 0.4818846059497446, 0.47638791508506984, 3.0255380660528317, 0.5986179850297049, 0.5003632719162852, 0.45117256906814873, 3.4303726970683783, 0.47663572093006223, 0.44114941300358623, 5.475988326012157, 0.4801854739198461, 0.44910659606102854, 0.4866477239411324, 0.870314986910671, 0.49598485708702356, 1.964051941060461, 0.4521478411043063, 0.5821702349931002, 5.1720173510257155, 0.49992604507133365, 0.463926060940139, 0.5527102160267532, 1.4347801390103996, 0.5690963550005108, 0.452154609025456, 0.6248578419908881, 0.4851888220291585, 0.5791963289957494, 0.4582811880391091, 0.562431352911517, 0.4591403719969094, 0.5715694889659062, 0.5925364309223369, 1.2091815760359168, 0.44680655002593994, 0.43284290307201445, 4.162114172941074, 0.43691938708070666, 0.49140522500965744, 0.47975563793443143, 0.4741989820031449, 0.5637180580524728, 0.5278673049760982, 0.44962163001764566, 0.4658278200076893, 0.45569464995060116, 4.947017204016447, 0.558341005933471, 0.4759672300424427, 3.749222892918624, 0.4370046639814973, 0.5542079819133505, 0.5113390430342406, 3.971861313912086, 0.48371114302426577, 0.6014912460232154, 5.22203217598144, 0.5870527080260217, 0.4653196199797094, 1.5845968149369583, 0.4123709730338305, 0.42950889794155955, 0.4685347619233653, 2.1319044349947944, 0.5609971529338509, 0.5337723989505321, 5.4879478310467675, 0.5171576250577345, 0.442242912016809, 0.4802640840644017, 0.44031640503089875, 0.5609406050061807, 0.5236222899984568, 3.4149101990042254, 0.44752725306898355, 0.6288767249789089, 5.476948196999729, 0.54147074802313, 0.4753103859256953, 0.7398343390086666, 0.5725822150707245, 0.8091133499983698, 0.5879436740651727, 0.4184814909240231, 0.44326926895882934, 0.4518854459747672, 0.46163579006679356, 4.989131813985296, 0.6653299940517172, 0.5621849249582738, 0.473987158969976, 0.4672675870824605, 0.4573816350894049, 0.46770746097899973, 0.6327111759455875, 0.5506183069664985, 0.44284923397935927, 4.778905684943311, 0.460908638080582, 0.5592619719682261, 6.143868921906687]
[0.08591797573453917, 0.0077590082667539926, 0.007966041937945601, 0.008517692509411397, 0.00784260528494737, 0.009024630714093849, 0.008958952959950025, 0.008579798408650927, 0.012810113754275501, 0.042366711795805215, 0.022530054081972613, 0.20393183818134497, 0.025388985632310564, 0.11726911446763849, 0.05120339949035599, 0.11703960251120128, 0.14397265767317494, 0.022962440898147772, 0.022956191837711602, 0.0313470571643996, 0.2075772733280283, 0.17546394751027075, 0.02347879034552571, 0.029419930162364428, 0.19046280073353603, 0.013839145366824707, 0.2189078916325139, 0.04743615447125417, 0.059828937345906635, 0.014852578000982806, 0.030833333060715576, 0.01682722344709446, 0.011111499409058265, 0.023916569224805857, 0.1937243908154303, 0.011530172714146272, 0.0577841957351573, 0.02480974434684886, 0.10484678130501843, 0.02384744700025387, 0.08324168748887521, 0.19222709677438726, 0.19746326197803551, 0.01757435952978475, 0.0768110246126711, 0.021856475733125543, 0.008955138326831619, 0.030955997918143262, 0.012840426161086984, 0.17159073134618147, 0.2315610662644387, 0.19204280742833732, 0.010502743591763536, 0.07301618406140455, 0.020419041797214627, 0.022775558940115934, 0.19228386891796728, 0.18812810432650529, 0.2182659980798216, 0.00800797343016507, 0.007931034795806877, 0.06404554069384324, 0.012674809633088964, 0.008878159671736767, 0.01488757810118247, 0.020618283978606364, 0.012441857551623667, 0.18311120104282258, 0.11143969504010616, 0.013766104407722548, 0.1937936866532403, 0.021070281062655304, 0.020073187204876115, 0.11908593822546228, 0.008247145285297717, 0.030288334510631253, 0.030861379794909487, 0.22111365100017236, 0.15236847442859897, 0.011327679530356308, 0.2039469518361385, 0.010764104592119704, 0.025433961019319083, 0.2034089215508453, 0.04373080579668512, 0.20334117995974207, 0.025527996757086748, 0.20040662663428074, 0.0874361396928755, 0.011842115530364064, 0.013436095040690686, 0.1889656730808736, 0.19351368124492238, 0.2087720171429635, 0.012390688244652535, 0.02554536138765742, 0.028254419817038034, 0.16278441136759914, 0.09165058804115242, 0.19290933979684677, 0.0783827636932612, 0.010077564694386507, 0.01010049267301374, 0.09998217449114867, 0.01020031981645342, 0.009464947960567566, 0.009909884327053264, 0.00969988306062486, 0.010624970266671509, 0.00951947906168596, 0.00966560455248216, 0.01152439775392033, 0.009871587651420613, 0.012584489124009803, 0.028529767959131574, 0.010190350735293967, 0.009751491714268923, 0.013728692571689583, 0.0782308869790856, 0.010415410794013617, 0.012156285754195889, 0.013595997001880742, 0.010362358001174823, 0.011404477387704715, 0.011424584713365351, 0.013123779324814677, 0.00992031493794401, 0.12272916938776948, 0.010180578856938044, 0.009943396306824776, 0.1263791716738357, 0.010215100939195526, 0.009692106060968826, 0.010023681081033178, 0.011752385817164061, 0.010109859409418945, 0.010421483162125307, 0.00983375606212613, 0.07578668779484471, 0.009960029001005602, 0.011931289469690196, 0.011677729795040677, 0.08230605367476082, 0.00957245238800058, 0.009363004959150389, 0.012937374531804604, 0.040859488509025196, 0.011232650164058623, 0.00967113975654071, 0.013516716060361691, 0.010962691754387806, 0.009477458103103754, 0.011751671141127542, 0.009774245039027716, 0.1487488533674302, 0.009538173958260034, 0.01037525449052681, 0.010480475652373719, 0.09473393032830969, 0.010168783266895584, 0.01002802220363246, 0.011186941265489678, 0.010837836694732612, 0.009760311918751317, 0.011282651038954452, 0.012080456878590797, 0.010455742815737518, 0.08468168473574428, 0.009906109774067086, 0.012980964265250581, 0.010688991366638517, 0.009498108551381347, 0.010115534611217374, 0.010178053304933163, 0.097762950040324, 0.010193578734974928, 0.009738034713652213, 0.010527220020089679, 0.08604085591754743, 0.009704773877367226, 0.011511304080771396, 0.0807802817969564, 0.009556433000620835, 0.01064453767470027, 0.009561775164317568, 0.07625150132677233, 0.009834379713260094, 0.009722202348674894, 0.06174567481740473, 0.012216693572034793, 0.010211495345230309, 0.009399428522253098, 0.07146609785559122, 0.00992991085270963, 0.009190612770908047, 0.11408309012525326, 0.010003864039996794, 0.009356387417938095, 0.010138494248773592, 0.018131562227305647, 0.010333017855979657, 0.04091774877209294, 0.009419746689673048, 0.012128546562356254, 0.1077503614797024, 0.010415125938986117, 0.00966512626958623, 0.011514796167224025, 0.02989125289604999, 0.011856174062510641, 0.009419887688030334, 0.013017871708143502, 0.010108100458940802, 0.012066590187411444, 0.009547524750814773, 0.011717319852323271, 0.009565424416602278, 0.011907697686789712, 0.012344508977548685, 0.0251912828340816, 0.009308469792207083, 0.009017560480666967, 0.08671071193627237, 0.009102487230848055, 0.010237608854367863, 0.009994909123633988, 0.009879145458398852, 0.011744126209426517, 0.01099723552033538, 0.009367117292034285, 0.009704746250160193, 0.009493638540637525, 0.10306285841700931, 0.011632104290280646, 0.009915983959217556, 0.078108810269138, 0.00910426383294786, 0.011545999623194803, 0.010652896729880013, 0.0827471107065018, 0.010077315479672203, 0.012531067625483653, 0.10879233699961333, 0.01223026475054212, 0.009694158749577278, 0.033012433644519966, 0.008591061938204803, 0.008948102040449157, 0.009761140873403443, 0.04441467572905822, 0.011687440686121894, 0.011120258311469419, 0.114332246480141, 0.010774117188702803, 0.009213394000350187, 0.010005501751341702, 0.009173258438143725, 0.01168626260429543, 0.010908797708301185, 0.0711439624792547, 0.009323484438937157, 0.013101598437060602, 0.11410308743749435, 0.011280640583815208, 0.00990229970678532, 0.015413215396013888, 0.01192879614730676, 0.016856528124966037, 0.01224882654302443, 0.00871836439425048, 0.009234776436642278, 0.009414280124474317, 0.0096174122930582, 0.10394024612469366, 0.01386104154274411, 0.011712185936630704, 0.009874732478541167, 0.009734741397551261, 0.009528784064362602, 0.009743905437062494, 0.01318148283219974, 0.011471214728468718, 0.009226025707903318, 0.09956053510298564, 0.009602263293345459, 0.011651291082671378, 0.12799726920638932]
[11.639007919480093, 128.8824506457645, 125.53285656664451, 117.40268845054887, 127.50864842316373, 110.80785814740219, 111.62018647384194, 116.55285501716565, 78.06331927897519, 23.603436698597207, 44.38515754829672, 4.903599207058375, 39.38715845060697, 8.527394485237282, 19.529953283440626, 8.544116508805597, 6.945763287012799, 43.54937719537749, 43.56123206625396, 31.900921185535896, 4.817483070122658, 5.699176464392864, 42.59163207658896, 33.99056335216099, 5.250369080726867, 72.25879730963783, 4.568131338447701, 21.080966852108343, 16.71431993214932, 67.32837894767019, 32.43243271918888, 59.42751061362229, 89.9968548965394, 41.812017041424866, 5.16197261372598, 86.728969703386, 17.305769982216365, 40.306743431921426, 9.537727220169186, 41.93320987313042, 12.013211530984933, 5.202180216942454, 5.0642331640972955, 56.901077863191304, 13.018964465617026, 45.75303046155817, 111.66773348478314, 32.303917407033474, 77.87903512349988, 5.827820606361986, 4.3185152674069025, 5.207172366365035, 95.21321655269394, 13.695593831074879, 48.97389456034174, 43.9067160823281, 5.200644264270671, 5.315526904286732, 4.58156565290711, 124.8755392011068, 126.08695154492301, 15.613889572426249, 78.89664846637157, 112.63595575820248, 67.17009262376756, 48.50064151980858, 80.3738505967302, 5.461162366392534, 8.973463177910787, 72.64219203793175, 5.160126819762318, 47.460211708916745, 49.81769909250301, 8.397297068833822, 121.25407827878475, 33.0160114828697, 32.402958216565146, 4.522561114959024, 6.5630374245730705, 88.27933358462035, 4.903235821849652, 92.9013641071539, 39.31750934274145, 4.916205210546942, 22.86717525053705, 4.917843007491065, 39.17267811946087, 4.989854960359599, 11.436918458575114, 84.44437123045505, 74.42638631027387, 5.291966438645286, 5.167593286256286, 4.789913963015537, 80.70576712569387, 39.14605022903153, 35.3926927707423, 6.1430943638810955, 10.911004734099313, 5.183782190396287, 12.757906877503643, 99.23032303201474, 99.00507157158547, 10.00178286869055, 98.0361418067471, 105.65298448191731, 100.90935141090121, 103.09402636608493, 94.11791044129393, 105.04776506361618, 103.45964337462954, 86.77243022611081, 101.30082772005684, 79.46289993545399, 35.051108772860815, 98.13204922736669, 102.54841303272038, 72.84014808971212, 12.782674958898804, 96.01157551795865, 82.26196884643304, 73.55106064392847, 96.50313180519586, 87.68485972694464, 87.53053393968217, 76.19756285517404, 100.80325133379792, 8.148022226406875, 98.22624175426941, 100.56925914877168, 7.912696267553004, 97.89428474103295, 103.17674958460367, 99.76374865838471, 85.08910578305942, 98.91334384614099, 95.95563169302908, 101.69054364195738, 13.19492946712501, 100.40131408242247, 83.81323766725825, 85.6330825897926, 12.149774595579355, 104.46643759268385, 106.80331841784492, 77.29543560338686, 24.474119390386306, 89.02618575264846, 103.40042902633971, 73.98246700857614, 91.21847283535553, 105.51352368126133, 85.09428046367647, 102.30969205366618, 6.7227408975709, 104.84187061130318, 96.38317796571216, 95.41551673501674, 10.555879994996538, 98.3401822768211, 99.72056101329424, 89.38993923968079, 92.2693364152662, 102.4557420218118, 88.63165195372989, 82.77832618832637, 95.6412201048829, 11.808928968767887, 100.94780118607919, 77.03587958230129, 93.55419662149852, 105.28411994770961, 98.85784967717619, 98.25061532300225, 10.228823900951566, 98.10097375997357, 102.69012479469318, 94.99184001964854, 11.622385543889703, 103.04207111225209, 86.87113058462339, 12.379258622958623, 104.6415540123637, 93.94489742629034, 104.58309077709535, 13.1144958800817, 101.6840948953455, 102.85735311159172, 16.195466370028598, 81.85520853932998, 97.92885039771284, 106.38944672354337, 13.992648682465655, 100.70583863571387, 108.8066731704113, 8.765540965817873, 99.96137452507006, 106.87885776114796, 98.63397615686033, 55.15244563394691, 96.77714816115466, 24.4392721987195, 106.15996724161478, 82.45011014788294, 9.280711324466193, 96.01420144683792, 103.46476311921073, 86.84478522046466, 33.45460303981256, 84.34424079197787, 106.1583782225642, 76.81747235029492, 98.93055614771632, 82.87345343370134, 104.73918906726702, 85.34374862197889, 104.54319185924932, 83.97929022915912, 81.0076773259049, 39.69627138825529, 107.4290428312058, 110.89473723452497, 11.532600501941966, 109.86008270476118, 97.67905906791421, 100.05093469388305, 101.22332991360504, 85.1489486886936, 90.93194359172034, 106.75642984105593, 103.04236444961076, 105.33369221078931, 9.702816469089525, 85.96896787071991, 100.84727890976816, 12.80265307529739, 109.83864465582069, 86.61008424000778, 93.87118127176882, 12.085014104564085, 99.23277702451449, 79.80166015275205, 9.191823869024471, 81.76437880918922, 103.15490243478897, 30.291617115177427, 116.4000454417584, 111.75554273739642, 102.44704107536636, 22.515080513033034, 85.56193155165593, 89.92596862328291, 8.746438828818915, 92.81502906322103, 108.53763552953357, 99.94501273920656, 109.0125179338518, 85.57055697451446, 91.66913043396502, 14.05600651343529, 107.25603786324292, 76.32656464048628, 8.764004747442087, 88.64744803896514, 100.9866424578902, 64.87938916746836, 83.83075606718087, 59.324197283479144, 81.64047359863127, 114.70041337793499, 108.28632472705483, 106.22161086967219, 103.97807326215963, 9.620912373060317, 72.1446506682951, 85.38115817239787, 101.26856622932371, 102.72486542390807, 104.94518432209765, 102.62825377967451, 75.8639989696151, 87.17472592664902, 108.38903246750841, 10.044140471579404, 104.14211415063134, 85.82739826037567, 7.812666677970676]
Elapsed: 2.1147136656504175~2.8505156543460486
Time per graph: 0.0433268828988111~0.058236854298593675
Speed: 66.5674752438915~39.72803040230906
Total Time: 6.1534
best val loss: 0.29992610216140747 test_score: 0.8958

Testing...
Test loss: 0.2924 score: 0.9375 time: 4.08s
test Score 0.9375
Epoch Time List: [9.233843691996299, 4.273922999971546, 1.8734583469340578, 1.5833287599962205, 11.893710596952587, 1.8830520879710093, 1.9836342639755458, 19.344367160927504, 2.0642795800231397, 12.737888444913551, 4.96762684697751, 22.41127654199954, 10.781780915916897, 18.864039964042604, 15.717427596100606, 14.846924169105478, 11.82442497194279, 10.296903591020964, 16.192355770152062, 24.800162309082225, 13.522720113047399, 24.508441373938695, 4.52520399610512, 25.09761512989644, 12.116193930967711, 11.589682840975001, 22.232915799948387, 17.93188690394163, 5.754702292964794, 19.709493588889018, 26.612625295994803, 3.4461163041414693, 26.007582439109683, 19.811232370091602, 12.861963052069768, 10.804033781052567, 24.716521229944192, 4.5801692879758775, 16.70970431691967, 22.81190911110025, 15.081666366080754, 12.556934696971439, 13.466582978959195, 12.09752483794, 29.544525985023938, 20.111976254964247, 9.357704854919575, 16.07004031992983, 13.354963763034903, 19.80196435994003, 14.489574330858886, 21.921089339070022, 5.348641948075965, 15.57797521003522, 16.524153239908628, 16.390400325064547, 22.891237517120317, 15.60264636296779, 14.637129032984376, 4.735733990906738, 1.6008901899913326, 6.096560187987052, 18.164203705848195, 2.7867725040996447, 17.176505322102457, 18.557462551048957, 11.698768873000517, 15.065711592906155, 20.143220031051897, 19.742097233072855, 21.19160205195658, 4.760558368987404, 18.515061816899106, 8.955553437001072, 15.873269472038373, 2.6983683368889615, 25.04117266391404, 23.571269833948463, 25.39393110992387, 23.331935737864114, 22.8173843229888, 7.278773032128811, 13.63293563714251, 20.77856893907301, 33.50516891491134, 22.672362343990244, 6.25969225098379, 30.632361622992903, 25.29447036003694, 3.3521978730568662, 21.265450199018233, 11.829835025127977, 36.21029985300265, 14.480679856031202, 6.786627368885092, 24.92128497012891, 15.845115873031318, 25.321119840838946, 15.053179595968686, 11.680653774994425, 24.136583377839997, 3.469886211096309, 1.5838435859186575, 7.298535927082412, 6.022594282985665, 1.709998874925077, 11.196586000965908, 1.4482517780270427, 1.6289579261792824, 11.51035005087033, 1.6954126709606498, 1.5020259550074115, 14.44323162699584, 1.7271297071129084, 2.540792511892505, 11.393652789993212, 1.5995761000085622, 1.728476551012136, 16.551186132943258, 3.901185891008936, 2.285382463829592, 5.845565511030145, 1.5128009549807757, 1.5628420900320634, 14.435235733981244, 1.605706945876591, 1.5972592598991469, 7.169407909153961, 4.088765730964951, 1.4359356899512932, 7.3812039020704105, 1.6978958030231297, 1.5625746749574319, 1.4198061040369794, 5.047519272891805, 1.7379151321947575, 1.8361468290677294, 1.5440425740089267, 4.817973237019032, 10.432379024103284, 1.8594853788381442, 1.7171884099952877, 5.080781089956872, 1.5982886939309537, 1.5090640580747277, 1.5977851030183956, 11.716409043874592, 1.7393232170725241, 1.688392161973752, 1.813432935043238, 15.59004999906756, 1.501809053006582, 1.7768886839039624, 1.4568615849129856, 17.5330061231507, 1.5799753499450162, 1.618988123955205, 1.5933187020709738, 10.646852414007299, 1.9905598999466747, 1.7274246150627732, 1.595356015022844, 9.223113260115497, 1.4895012329798192, 1.9209724820684642, 5.680527975200675, 1.650467730127275, 10.841851214878261, 1.6529437799472362, 1.6978841189993545, 1.5489875508937985, 4.141320570022799, 1.5937002780847251, 1.513193455990404, 19.583149680052884, 1.8750452811364084, 1.729973212000914, 1.4591894760960713, 5.363501476123929, 9.975705171003938, 1.7332479441538453, 4.9946887290570885, 4.859505039989017, 1.490568722016178, 1.5649470059433952, 8.640135777881369, 1.6089777159504592, 1.44774947501719, 6.012049405951984, 9.366986174951307, 1.7734386438969523, 1.619526029098779, 4.526420175097883, 13.157738286070526, 1.4801105810329318, 6.737741863005795, 5.590930305072106, 1.6646185300778598, 1.5679503470892087, 17.167369784903713, 2.2516188990557566, 7.990250890958123, 1.6913166230078787, 1.618440615129657, 6.266509752953425, 11.283074520993978, 1.4981560310116038, 1.7580999140627682, 11.996987504069693, 1.5643621319904923, 1.485140549018979, 1.6604788821423426, 11.156133694923483, 1.6293571359710768, 1.5302802079822868, 1.585026926943101, 8.315534672001377, 1.7382640229770914, 1.8749867209699005, 11.462055586860515, 1.4339620871469378, 1.667720377095975, 5.249635557061993, 12.885375879937783, 1.5030475158710033, 1.660698945983313, 12.37107464298606, 1.6128394670085981, 1.5932928890688345, 9.463870396954007, 1.5220544668845832, 1.6885960269719362, 6.115608236985281, 4.136029709945433, 1.5085402029799297, 4.865666038007475, 3.8738380318973213, 1.6078121088212356, 1.543340367032215, 9.731596364988945, 1.478002900024876, 1.7300986849004403, 9.219501892919652, 1.5998934609815478, 1.4614216649206355, 7.673993958043866, 1.4569311270024627, 1.5444855790119618, 1.45623084588442, 8.830321265966631, 1.5331605720566586, 1.6973129141842946, 8.776289420900866, 10.374458482838236, 1.5380954799475148, 1.9185941190226004, 12.2247132321354, 1.6171430981485173, 1.6798262619413435, 4.536607171059586, 1.444272780092433, 1.6618775801034644, 8.755153738078661, 11.786881532869302, 1.6660237709293142, 1.7530536219710484, 16.860952736926265, 1.8794956740457565, 17.097178387921304, 1.8602978580165654, 15.018692826968618, 1.5822944819228724, 1.5904840959701687, 6.401152914040722, 4.809705258929171, 1.6602968979859725, 1.5040637771598995, 13.678454806096852, 1.5351842400850728, 1.6636598339537159, 1.766062177135609, 11.116802967037074, 1.6520024229539558, 12.149229281931184, 3.1781167939770967, 1.5777212739922106, 8.03250788000878]
Total Epoch List: [94, 97, 96]
Total Time List: [10.230493623996153, 0.5015418929979205, 6.153427204932086]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d17b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.41s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.37s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 5.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 5.63s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 5.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.43s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4898 time: 0.41s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 4.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 6.18s
Epoch 7/1000, LR 0.000150
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 2.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.42s
Epoch 8/1000, LR 0.000180
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.39s
Epoch 9/1000, LR 0.000210
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 3.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 3.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 2.92s
Epoch 10/1000, LR 0.000240
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.52s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.39s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 4.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 6.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 8.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.39s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.52s
Epoch 15/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 10.27s
Val loss: 0.6910 score: 0.5714 time: 0.51s
Test loss: 0.6921 score: 0.5714 time: 0.43s
Epoch 16/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.65s
Val loss: 0.6907 score: 0.7143 time: 0.49s
Test loss: 0.6919 score: 0.5918 time: 0.48s
Epoch 17/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.61s
Val loss: 0.6903 score: 0.8163 time: 2.28s
Test loss: 0.6916 score: 0.6735 time: 3.64s
Epoch 18/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 9.09s
Val loss: 0.6899 score: 0.7755 time: 0.62s
Test loss: 0.6913 score: 0.6735 time: 0.41s
Epoch 19/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.78s
Val loss: 0.6895 score: 0.6939 time: 0.49s
Test loss: 0.6910 score: 0.6122 time: 0.37s
Epoch 20/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.63s
Val loss: 0.6890 score: 0.6122 time: 0.65s
Test loss: 0.6907 score: 0.5918 time: 0.48s
Epoch 21/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 13.80s
Val loss: 0.6884 score: 0.5918 time: 0.64s
Test loss: 0.6903 score: 0.5510 time: 0.49s
Epoch 22/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.67s
Val loss: 0.6877 score: 0.5918 time: 0.67s
Test loss: 0.6899 score: 0.5510 time: 0.39s
Epoch 23/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.71s
Val loss: 0.6870 score: 0.5918 time: 0.60s
Test loss: 0.6894 score: 0.5510 time: 0.39s
Epoch 24/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.60s
Val loss: 0.6861 score: 0.5714 time: 0.57s
Test loss: 0.6888 score: 0.5510 time: 1.88s
Epoch 25/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 6.51s
Val loss: 0.6852 score: 0.5714 time: 0.49s
Test loss: 0.6882 score: 0.5510 time: 0.39s
Epoch 26/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.81s
Val loss: 0.6841 score: 0.5714 time: 0.60s
Test loss: 0.6876 score: 0.5510 time: 0.44s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.66s
Val loss: 0.6830 score: 0.5714 time: 0.47s
Test loss: 0.6868 score: 0.5510 time: 0.38s
Epoch 28/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.63s
Val loss: 0.6817 score: 0.5918 time: 3.70s
Test loss: 0.6860 score: 0.5510 time: 4.92s
Epoch 29/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 10.31s
Val loss: 0.6802 score: 0.5918 time: 0.69s
Test loss: 0.6851 score: 0.5510 time: 0.37s
Epoch 30/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.61s
Val loss: 0.6786 score: 0.5918 time: 0.62s
Test loss: 0.6841 score: 0.5510 time: 0.43s
Epoch 31/1000, LR 0.000270
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.60s
Val loss: 0.6768 score: 0.5918 time: 0.51s
Test loss: 0.6830 score: 0.5510 time: 2.38s
Epoch 32/1000, LR 0.000270
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 8.45s
Val loss: 0.6749 score: 0.5918 time: 0.50s
Test loss: 0.6818 score: 0.5510 time: 0.51s
Epoch 33/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.72s
Val loss: 0.6727 score: 0.5918 time: 0.50s
Test loss: 0.6804 score: 0.5714 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.71s
Val loss: 0.6703 score: 0.6122 time: 4.89s
Test loss: 0.6789 score: 0.5714 time: 5.20s
Epoch 35/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 4.26s
Val loss: 0.6677 score: 0.6122 time: 0.53s
Test loss: 0.6772 score: 0.5918 time: 0.39s
Epoch 36/1000, LR 0.000270
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 0.60s
Val loss: 0.6648 score: 0.6122 time: 0.60s
Test loss: 0.6754 score: 0.6122 time: 0.39s
Epoch 37/1000, LR 0.000270
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.62s
Val loss: 0.6618 score: 0.6531 time: 0.48s
Test loss: 0.6734 score: 0.6327 time: 0.40s
Epoch 38/1000, LR 0.000270
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 12.62s
Val loss: 0.6584 score: 0.6735 time: 2.01s
Test loss: 0.6713 score: 0.6122 time: 0.39s
Epoch 39/1000, LR 0.000269
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.69s
Val loss: 0.6548 score: 0.6939 time: 0.50s
Test loss: 0.6689 score: 0.6327 time: 0.41s
Epoch 40/1000, LR 0.000269
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.64s
Val loss: 0.6508 score: 0.7347 time: 0.50s
Test loss: 0.6664 score: 0.6327 time: 0.50s
Epoch 41/1000, LR 0.000269
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.81s
Val loss: 0.6465 score: 0.7347 time: 3.90s
Test loss: 0.6636 score: 0.6122 time: 7.67s
Epoch 42/1000, LR 0.000269
Train loss: 0.6536;  Loss pred: 0.6536; Loss self: 0.0000; time: 0.72s
Val loss: 0.6419 score: 0.7755 time: 0.61s
Test loss: 0.6606 score: 0.6531 time: 0.42s
Epoch 43/1000, LR 0.000269
Train loss: 0.6469;  Loss pred: 0.6469; Loss self: 0.0000; time: 0.61s
Val loss: 0.6368 score: 0.7959 time: 0.53s
Test loss: 0.6573 score: 0.6939 time: 0.52s
Epoch 44/1000, LR 0.000269
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 13.67s
Val loss: 0.6312 score: 0.8163 time: 1.68s
Test loss: 0.6537 score: 0.6939 time: 1.11s
Epoch 45/1000, LR 0.000269
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 12.68s
Val loss: 0.6252 score: 0.8571 time: 4.87s
Test loss: 0.6498 score: 0.6939 time: 1.38s
Epoch 46/1000, LR 0.000269
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 2.85s
Val loss: 0.6187 score: 0.8571 time: 8.74s
Test loss: 0.6456 score: 0.7143 time: 4.17s
Epoch 47/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 3.79s
Val loss: 0.6117 score: 0.8776 time: 0.50s
Test loss: 0.6410 score: 0.7143 time: 0.37s
Epoch 48/1000, LR 0.000269
Train loss: 0.6185;  Loss pred: 0.6185; Loss self: 0.0000; time: 0.75s
Val loss: 0.6041 score: 0.8980 time: 0.49s
Test loss: 0.6360 score: 0.7347 time: 0.53s
Epoch 49/1000, LR 0.000269
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.60s
Val loss: 0.5959 score: 0.8980 time: 0.48s
Test loss: 0.6306 score: 0.7347 time: 0.38s
Epoch 50/1000, LR 0.000269
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.63s
Val loss: 0.5870 score: 0.9184 time: 0.72s
Test loss: 0.6249 score: 0.7347 time: 3.65s
Epoch 51/1000, LR 0.000269
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 8.30s
Val loss: 0.5776 score: 0.9184 time: 0.69s
Test loss: 0.6188 score: 0.7347 time: 0.43s
Epoch 52/1000, LR 0.000269
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 9.00s
Val loss: 0.5675 score: 0.9388 time: 0.65s
Test loss: 0.6122 score: 0.7347 time: 0.37s
Epoch 53/1000, LR 0.000269
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 0.65s
Val loss: 0.5568 score: 0.9388 time: 0.49s
Test loss: 0.6051 score: 0.7347 time: 0.37s
Epoch 54/1000, LR 0.000269
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.60s
Val loss: 0.5454 score: 0.9592 time: 0.59s
Test loss: 0.5975 score: 0.7551 time: 0.39s
Epoch 55/1000, LR 0.000269
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.64s
Val loss: 0.5335 score: 0.9592 time: 0.52s
Test loss: 0.5896 score: 0.7551 time: 5.64s
Epoch 56/1000, LR 0.000269
Train loss: 0.5495;  Loss pred: 0.5495; Loss self: 0.0000; time: 1.71s
Val loss: 0.5210 score: 0.9592 time: 0.55s
Test loss: 0.5811 score: 0.7551 time: 0.50s
Epoch 57/1000, LR 0.000269
Train loss: 0.5338;  Loss pred: 0.5338; Loss self: 0.0000; time: 0.68s
Val loss: 0.5079 score: 0.9592 time: 0.47s
Test loss: 0.5723 score: 0.7551 time: 0.38s
Epoch 58/1000, LR 0.000269
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.63s
Val loss: 0.4946 score: 0.9592 time: 2.81s
Test loss: 0.5631 score: 0.7755 time: 2.68s
Epoch 59/1000, LR 0.000268
Train loss: 0.5038;  Loss pred: 0.5038; Loss self: 0.0000; time: 3.89s
Val loss: 0.4809 score: 0.9592 time: 0.49s
Test loss: 0.5534 score: 0.7755 time: 0.36s
Epoch 60/1000, LR 0.000268
Train loss: 0.4954;  Loss pred: 0.4954; Loss self: 0.0000; time: 0.57s
Val loss: 0.4670 score: 0.9592 time: 0.59s
Test loss: 0.5435 score: 0.7959 time: 0.37s
Epoch 61/1000, LR 0.000268
Train loss: 0.4850;  Loss pred: 0.4850; Loss self: 0.0000; time: 0.68s
Val loss: 0.4530 score: 0.9592 time: 4.36s
Test loss: 0.5334 score: 0.7959 time: 0.36s
Epoch 62/1000, LR 0.000268
Train loss: 0.4686;  Loss pred: 0.4686; Loss self: 0.0000; time: 0.64s
Val loss: 0.4391 score: 0.9592 time: 0.64s
Test loss: 0.5231 score: 0.8367 time: 0.36s
Epoch 63/1000, LR 0.000268
Train loss: 0.4547;  Loss pred: 0.4547; Loss self: 0.0000; time: 0.66s
Val loss: 0.4251 score: 0.9796 time: 0.48s
Test loss: 0.5126 score: 0.8367 time: 0.38s
Epoch 64/1000, LR 0.000268
Train loss: 0.4400;  Loss pred: 0.4400; Loss self: 0.0000; time: 5.97s
Val loss: 0.4111 score: 0.9796 time: 6.22s
Test loss: 0.5021 score: 0.8367 time: 5.33s
Epoch 65/1000, LR 0.000268
Train loss: 0.4239;  Loss pred: 0.4239; Loss self: 0.0000; time: 0.62s
Val loss: 0.3971 score: 0.9796 time: 0.56s
Test loss: 0.4916 score: 0.8367 time: 0.40s
Epoch 66/1000, LR 0.000268
Train loss: 0.4062;  Loss pred: 0.4062; Loss self: 0.0000; time: 0.75s
Val loss: 0.3834 score: 0.9796 time: 0.60s
Test loss: 0.4812 score: 0.8571 time: 0.51s
Epoch 67/1000, LR 0.000268
Train loss: 0.3940;  Loss pred: 0.3940; Loss self: 0.0000; time: 0.64s
Val loss: 0.3699 score: 0.9796 time: 0.54s
Test loss: 0.4711 score: 0.8571 time: 0.47s
Epoch 68/1000, LR 0.000268
Train loss: 0.3829;  Loss pred: 0.3829; Loss self: 0.0000; time: 10.16s
Val loss: 0.3569 score: 0.9796 time: 0.64s
Test loss: 0.4612 score: 0.8571 time: 0.37s
Epoch 69/1000, LR 0.000268
Train loss: 0.3599;  Loss pred: 0.3599; Loss self: 0.0000; time: 0.66s
Val loss: 0.3443 score: 0.9796 time: 0.49s
Test loss: 0.4517 score: 0.8571 time: 0.41s
Epoch 70/1000, LR 0.000268
Train loss: 0.3558;  Loss pred: 0.3558; Loss self: 0.0000; time: 0.71s
Val loss: 0.3323 score: 0.9796 time: 0.61s
Test loss: 0.4421 score: 0.8571 time: 0.42s
Epoch 71/1000, LR 0.000268
Train loss: 0.3473;  Loss pred: 0.3473; Loss self: 0.0000; time: 0.61s
Val loss: 0.3210 score: 0.9796 time: 0.55s
Test loss: 0.4328 score: 0.8776 time: 0.39s
Epoch 72/1000, LR 0.000267
Train loss: 0.3277;  Loss pred: 0.3277; Loss self: 0.0000; time: 0.63s
Val loss: 0.3101 score: 0.9796 time: 0.59s
Test loss: 0.4236 score: 0.8776 time: 3.00s
Epoch 73/1000, LR 0.000267
Train loss: 0.3077;  Loss pred: 0.3077; Loss self: 0.0000; time: 9.39s
Val loss: 0.2997 score: 0.9796 time: 0.63s
Test loss: 0.4149 score: 0.8776 time: 0.54s
Epoch 74/1000, LR 0.000267
Train loss: 0.2970;  Loss pred: 0.2970; Loss self: 0.0000; time: 0.66s
Val loss: 0.2897 score: 0.9796 time: 0.67s
Test loss: 0.4067 score: 0.8776 time: 0.40s
Epoch 75/1000, LR 0.000267
Train loss: 0.2932;  Loss pred: 0.2932; Loss self: 0.0000; time: 1.67s
Val loss: 0.2801 score: 0.9796 time: 5.31s
Test loss: 0.3986 score: 0.8776 time: 1.65s
Epoch 76/1000, LR 0.000267
Train loss: 0.2755;  Loss pred: 0.2755; Loss self: 0.0000; time: 0.63s
Val loss: 0.2710 score: 0.9796 time: 0.58s
Test loss: 0.3907 score: 0.8776 time: 0.39s
Epoch 77/1000, LR 0.000267
Train loss: 0.2645;  Loss pred: 0.2645; Loss self: 0.0000; time: 0.75s
Val loss: 0.2622 score: 0.9796 time: 0.85s
Test loss: 0.3831 score: 0.8776 time: 0.43s
Epoch 78/1000, LR 0.000267
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 2.22s
Val loss: 0.2537 score: 0.9796 time: 5.89s
Test loss: 0.3762 score: 0.8776 time: 3.90s
Epoch 79/1000, LR 0.000267
Train loss: 0.2396;  Loss pred: 0.2396; Loss self: 0.0000; time: 0.98s
Val loss: 0.2455 score: 0.9796 time: 0.48s
Test loss: 0.3696 score: 0.8776 time: 0.37s
Epoch 80/1000, LR 0.000267
Train loss: 0.2372;  Loss pred: 0.2372; Loss self: 0.0000; time: 0.63s
Val loss: 0.2376 score: 0.9796 time: 0.48s
Test loss: 0.3633 score: 0.8776 time: 0.48s
Epoch 81/1000, LR 0.000267
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.61s
Val loss: 0.2300 score: 0.9592 time: 0.48s
Test loss: 0.3573 score: 0.8776 time: 0.48s
Epoch 82/1000, LR 0.000267
Train loss: 0.2095;  Loss pred: 0.2095; Loss self: 0.0000; time: 0.70s
Val loss: 0.2227 score: 0.9592 time: 3.23s
Test loss: 0.3517 score: 0.8776 time: 0.42s
Epoch 83/1000, LR 0.000266
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.59s
Val loss: 0.2157 score: 0.9592 time: 0.48s
Test loss: 0.3464 score: 0.8776 time: 0.39s
Epoch 84/1000, LR 0.000266
Train loss: 0.1972;  Loss pred: 0.1972; Loss self: 0.0000; time: 0.61s
Val loss: 0.2089 score: 0.9592 time: 0.58s
Test loss: 0.3418 score: 0.8776 time: 0.41s
Epoch 85/1000, LR 0.000266
Train loss: 0.1765;  Loss pred: 0.1765; Loss self: 0.0000; time: 0.66s
Val loss: 0.2024 score: 0.9592 time: 0.57s
Test loss: 0.3377 score: 0.8776 time: 5.86s
Epoch 86/1000, LR 0.000266
Train loss: 0.1765;  Loss pred: 0.1765; Loss self: 0.0000; time: 3.95s
Val loss: 0.1961 score: 0.9592 time: 0.58s
Test loss: 0.3342 score: 0.8776 time: 0.38s
Epoch 87/1000, LR 0.000266
Train loss: 0.1729;  Loss pred: 0.1729; Loss self: 0.0000; time: 0.59s
Val loss: 0.1903 score: 0.9592 time: 0.50s
Test loss: 0.3308 score: 0.8776 time: 0.38s
Epoch 88/1000, LR 0.000266
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.67s
Val loss: 0.1849 score: 0.9592 time: 0.49s
Test loss: 0.3278 score: 0.8776 time: 6.10s
Epoch 89/1000, LR 0.000266
Train loss: 0.1474;  Loss pred: 0.1474; Loss self: 0.0000; time: 2.52s
Val loss: 0.1799 score: 0.9592 time: 0.58s
Test loss: 0.3255 score: 0.8776 time: 0.35s
Epoch 90/1000, LR 0.000266
Train loss: 0.1419;  Loss pred: 0.1419; Loss self: 0.0000; time: 0.67s
Val loss: 0.1752 score: 0.9592 time: 0.61s
Test loss: 0.3238 score: 0.8776 time: 0.39s
Epoch 91/1000, LR 0.000266
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.65s
Val loss: 0.1709 score: 0.9592 time: 0.49s
Test loss: 0.3229 score: 0.8776 time: 0.40s
Epoch 92/1000, LR 0.000266
Train loss: 0.1319;  Loss pred: 0.1319; Loss self: 0.0000; time: 0.69s
Val loss: 0.1671 score: 0.9592 time: 3.01s
Test loss: 0.3219 score: 0.8776 time: 4.82s
Epoch 93/1000, LR 0.000265
Train loss: 0.1213;  Loss pred: 0.1213; Loss self: 0.0000; time: 9.16s
Val loss: 0.1637 score: 0.9592 time: 0.51s
Test loss: 0.3213 score: 0.8776 time: 0.39s
Epoch 94/1000, LR 0.000265
Train loss: 0.1124;  Loss pred: 0.1124; Loss self: 0.0000; time: 0.64s
Val loss: 0.1607 score: 0.9592 time: 0.59s
Test loss: 0.3215 score: 0.8776 time: 0.38s
Epoch 95/1000, LR 0.000265
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.63s
Val loss: 0.1581 score: 0.9592 time: 0.50s
Test loss: 0.3221 score: 0.8776 time: 1.94s
Epoch 96/1000, LR 0.000265
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 5.93s
Val loss: 0.1562 score: 0.9592 time: 0.50s
Test loss: 0.3211 score: 0.8776 time: 0.67s
Epoch 97/1000, LR 0.000265
Train loss: 0.0967;  Loss pred: 0.0967; Loss self: 0.0000; time: 0.62s
Val loss: 0.1549 score: 0.9592 time: 0.47s
Test loss: 0.3203 score: 0.8980 time: 0.40s
Epoch 98/1000, LR 0.000265
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.62s
Val loss: 0.1541 score: 0.9592 time: 0.57s
Test loss: 0.3200 score: 0.8980 time: 0.39s
Epoch 99/1000, LR 0.000265
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.62s
Val loss: 0.1537 score: 0.9592 time: 0.54s
Test loss: 0.3200 score: 0.8980 time: 0.40s
Epoch 100/1000, LR 0.000265
Train loss: 0.0807;  Loss pred: 0.0807; Loss self: 0.0000; time: 4.94s
Val loss: 0.1536 score: 0.9592 time: 0.60s
Test loss: 0.3203 score: 0.8980 time: 0.38s
Epoch 101/1000, LR 0.000265
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.70s
Val loss: 0.1538 score: 0.9592 time: 0.49s
Test loss: 0.3215 score: 0.8980 time: 0.38s
     INFO: Early stopping counter 1 of 2
Epoch 102/1000, LR 0.000264
Train loss: 0.0786;  Loss pred: 0.0786; Loss self: 0.0000; time: 0.77s
Val loss: 0.1541 score: 0.9592 time: 5.33s
Test loss: 0.3235 score: 0.8980 time: 2.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 099,   Train_Loss: 0.0807,   Val_Loss: 0.1536,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1536,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3203


[0.4101442910032347, 0.37081507593393326, 5.636755637009628, 0.43722322303801775, 0.4102330980822444, 6.18457286991179, 0.42485792201478034, 0.39547653205227107, 2.9270273640286177, 0.524497541016899, 0.3951879640808329, 6.091175285051577, 0.38991824199911207, 0.5231781259644777, 0.43277969397604465, 0.4876167200272903, 3.6480638859793544, 0.41646112501621246, 0.3787445129128173, 0.48757525195833296, 0.49554311600513756, 0.3969246610067785, 0.3895486299879849, 1.883905126946047, 0.39478070894256234, 0.44452430703677237, 0.38196667993906885, 4.927246132050641, 0.3792978960555047, 0.43546888092532754, 2.3871966269798577, 0.5183377780485898, 0.4342452730052173, 5.206334159011021, 0.3950428010430187, 0.3962346649495885, 0.40234090702142566, 0.3930591410025954, 0.4133384668966755, 0.5030691140564159, 7.670755526982248, 0.42137484799604863, 0.5247849880252033, 1.1194432090269402, 1.3867557629710063, 4.178382813115604, 0.3796927280491218, 0.5384487059200183, 0.3799441320588812, 3.655878046993166, 0.4353841149713844, 0.37201496702618897, 0.3773134519578889, 0.39172910095658153, 5.643646414973773, 0.5067320140078664, 0.37986306101083755, 2.6848251919727772, 0.3670044440077618, 0.3785388880642131, 0.36757214507088065, 0.3625989550491795, 0.38442697504069656, 5.331157470005564, 0.39985974796582013, 0.5139811700209975, 0.4777706890599802, 0.37500457104761153, 0.4103090720018372, 0.41968918696511537, 0.39428449398837984, 3.0047690300270915, 0.5470916630001739, 0.4010561479954049, 1.6504924190230668, 0.3931459200102836, 0.4384398970287293, 3.908631618018262, 0.37932693504262716, 0.485551472986117, 0.48172691802028567, 0.4244220709661022, 0.3938724569743499, 0.41504489292856306, 5.863276822026819, 0.3807974250521511, 0.3835669480031356, 6.110098542063497, 0.3564564469270408, 0.39425489702261984, 0.40006466303020716, 4.826321228058077, 0.3951798209454864, 0.3799847370246425, 1.9499718029983342, 0.6727515899110585, 0.4011653469642624, 0.40005315793678164, 0.40450484200846404, 0.3801863299449906, 0.3846632050117478, 2.302288157050498]
[0.008370291653127238, 0.007567654610896597, 0.11503582932672711, 0.00892292291914322, 0.008372104042494784, 0.12621577285534266, 0.008670569837036334, 0.008070949633719817, 0.05973525232711465, 0.010704031449324471, 0.008065060491445569, 0.12430969969493014, 0.007957515142839022, 0.010677104611519952, 0.008832238652572339, 0.009951361633210006, 0.07445028338733377, 0.008499206632983928, 0.0077294798553636185, 0.009950515346088427, 0.010113124816431378, 0.008100503285852621, 0.00794997204057112, 0.03844704340706218, 0.008056749162093108, 0.009071924633403517, 0.007795238366103446, 0.10055604351123756, 0.007740773388887851, 0.008887120018884236, 0.048718298509793014, 0.01057832200099163, 0.008862148428677904, 0.10625171753083718, 0.008062097980469769, 0.008086421733665071, 0.008211038918804606, 0.008021615122501947, 0.008435478916258685, 0.010266716613396242, 0.15654603116290303, 0.00859948669379691, 0.010709897714800068, 0.022845779776060005, 0.028301138019816457, 0.08527311863501233, 0.007748831184675955, 0.010988749100408536, 0.0077539618787526784, 0.07460975606108503, 0.008885390101456824, 0.0075921421842079385, 0.0077002745297528345, 0.007994471448093501, 0.11517645744844435, 0.010341469673629927, 0.007752307367568113, 0.05479235085658729, 0.007489886612403302, 0.0077252834298819, 0.007501472348385319, 0.007399978674473051, 0.007845448470218298, 0.10879913204092989, 0.008160403019710615, 0.010489411633081583, 0.009750422225713882, 0.0076531545111757455, 0.008373654530649739, 0.008565085448267661, 0.008046622326293466, 0.0613218169393284, 0.011165135979595385, 0.008184819346844998, 0.033683518755572794, 0.008023386122658849, 0.008947753000586313, 0.07976799220445432, 0.0077413660212781055, 0.009909213734410552, 0.009831161592250727, 0.008661674917675555, 0.008038213407639794, 0.008470303937317613, 0.11965871065360856, 0.007771376021472471, 0.007827896898023176, 0.12469588861354075, 0.007274621365857976, 0.008046018306584078, 0.008164584959800146, 0.09849635159302199, 0.008064894305009927, 0.007754790551523317, 0.03979534291833335, 0.013729624283899153, 0.008187047897229845, 0.008164350161975135, 0.008255200857315593, 0.0077589046927549095, 0.007850269490035668, 0.046985472592867304]
[119.47015007850871, 132.141337232821, 8.692943805879642, 112.07089975579663, 119.44428723344106, 7.922940036552417, 115.33267349148157, 123.90115728415347, 16.740533621988007, 93.42274494747582, 123.99163044848551, 8.044424549766523, 125.66737003320708, 93.65834993515567, 113.22157828114797, 100.48876092119572, 13.431782318375031, 117.65804070690265, 129.3748115930573, 100.49730744780997, 98.88140591078655, 123.4491197289533, 125.78660590209581, 26.00980235105189, 124.11954001310924, 110.23019264489079, 128.2834408692833, 9.944703123569552, 129.18605800236122, 112.52239171690049, 20.526168412860045, 94.53295143655662, 112.83945513302405, 9.411612567201773, 124.03719260451497, 123.66409184878889, 121.78726832116682, 124.66317377841226, 118.54691475460666, 97.40212354699433, 6.3878974929705645, 116.28601050354929, 93.37157334547597, 43.771760465269644, 35.33426815910371, 11.727025069649669, 129.05172098439755, 91.00216875120229, 128.96632916653732, 13.403072906193032, 112.54429896511158, 131.71513068868146, 129.86549974759123, 125.08644336186568, 8.682329897562818, 96.69805468268547, 128.9938533891876, 18.25072267144342, 133.51336966089625, 129.44508885356035, 133.30716338843115, 135.13552457246905, 127.4624393743772, 9.191249794380742, 122.54296725107851, 95.3342317929628, 102.55966119731646, 130.66507392993574, 119.42217061137902, 116.75306756014395, 124.27574694693448, 16.307409824294616, 89.56451599223955, 122.17740644275429, 29.688109703044443, 124.6356569049942, 111.75990217146962, 12.536356655898867, 129.17616829528225, 100.91618031483353, 101.71738004878647, 115.4511118812988, 124.40575402608316, 118.05951798191096, 8.357101581136275, 128.6773406970631, 127.7482334051354, 8.019510595888322, 137.4641991256488, 124.28507640626387, 122.48020014779516, 10.152660315093797, 123.99418543883435, 128.952547893581, 25.128568487326923, 72.83520505165669, 122.144149216271, 122.48372254505043, 121.1357563897213, 128.88417110391592, 127.38416194110253, 21.28317424121869]
Elapsed: 1.300872879630119~1.777994602988005
Time per graph: 0.02654842611490039~0.03628560414261235
Speed: 92.83871849410066~45.53715513512025
Total Time: 2.3029
best val loss: 0.15356212854385376 test_score: 0.8980

Testing...
Test loss: 0.5126 score: 0.8367 time: 0.43s
test Score 0.8367
Epoch Time List: [1.8821028281236067, 1.9516737670637667, 11.735487641999498, 6.43655739503447, 1.6652437469456345, 11.210415615933016, 3.8424208777723834, 1.7168957530520856, 10.333535420941189, 1.7506778319366276, 1.5207137078978121, 11.708370335982181, 9.174257454113103, 1.9241145099513233, 11.203940422972664, 1.6211439390899613, 6.534382140031084, 10.118911528028548, 1.6462003121851012, 1.7533943890593946, 14.935949907056056, 1.739463694859296, 1.6948367969598621, 3.0531660228734836, 7.3815570548176765, 1.8508394239470363, 1.5075421740766615, 9.25309223588556, 11.379827141063288, 1.6601341040804982, 3.4944777791388333, 9.456976382178254, 1.6471137941116467, 10.798509048996493, 5.177732676966116, 1.589254059130326, 1.5073690039571375, 15.018137508886866, 1.5998423100681975, 1.6397771949414164, 12.37247650208883, 1.746441519120708, 1.6578875362174585, 16.46759994805325, 18.926413196022622, 15.760610318975523, 4.664179718005471, 1.7705336048966274, 1.452725112088956, 4.9968408329878, 9.428953106049448, 10.01627794187516, 1.5175342400325462, 1.5798890009755269, 6.796156543889083, 2.762615147046745, 1.5262764249928296, 6.110614272998646, 4.7342858909396455, 1.5331225789850578, 5.410153753007762, 1.6405605589970946, 1.513085444807075, 17.510529264225625, 1.5793708860874176, 1.8661827839678153, 1.6447292520897463, 11.165624292101711, 1.551703173900023, 1.73282128397841, 1.5440430550370365, 4.223281029961072, 10.565859466092661, 1.7277433869894594, 8.62612360890489, 1.6012240811251104, 2.036836166982539, 12.018676346051507, 1.8366510841296986, 1.5917150379391387, 1.5611570699838921, 4.355313600040972, 1.450910335057415, 1.5952838330995291, 7.089486956014298, 4.913300023064949, 1.4732495190110058, 7.263778852066025, 3.453229266917333, 1.6706905589671806, 1.535099743050523, 8.522644947865047, 10.058009714935906, 1.6051608370617032, 3.0715189611073583, 7.095996225019917, 1.4926727749407291, 1.5791903170756996, 1.5567859879229218, 5.9143400529865175, 1.572929939138703, 8.398130613029934]
Total Epoch List: [102]
Total Time List: [2.3028912730515003]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d0ac0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4898 time: 5.80s
Epoch 2/1000, LR 0.000000
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 16.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4898 time: 0.68s
Epoch 3/1000, LR 0.000030
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4898 time: 0.62s
Epoch 4/1000, LR 0.000060
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4898 time: 0.69s
Epoch 5/1000, LR 0.000090
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 11.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4898 time: 0.56s
Epoch 6/1000, LR 0.000120
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4898 time: 0.62s
Epoch 7/1000, LR 0.000150
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 4.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4898 time: 2.34s
Epoch 8/1000, LR 0.000180
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4898 time: 0.62s
Epoch 9/1000, LR 0.000210
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 0.48s
Epoch 10/1000, LR 0.000240
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.63s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 5.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 2.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.50s
Epoch 12/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.61s
Epoch 13/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.50s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 6.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 3.74s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.78s
Epoch 16/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4898 time: 0.62s
Epoch 17/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4898 time: 0.50s
Epoch 18/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4898 time: 0.49s
Epoch 19/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 5.57s
Epoch 20/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 2.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4898 time: 0.51s
Epoch 21/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4898 time: 0.49s
Epoch 22/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4898 time: 0.56s
Epoch 23/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 7.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.4898 time: 0.50s
Epoch 24/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.4898 time: 0.56s
Epoch 25/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 8.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5102 time: 4.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.4898 time: 0.80s
Epoch 26/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.4898 time: 0.65s
Epoch 27/1000, LR 0.000270
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.4898 time: 0.50s
Epoch 28/1000, LR 0.000270
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5102 time: 3.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6803 score: 0.4898 time: 4.95s
Epoch 29/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 4.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.4898 time: 0.48s
Epoch 30/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.4898 time: 0.69s
Epoch 31/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5102 time: 5.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6745 score: 0.4898 time: 8.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6721 score: 0.4898 time: 5.95s
Epoch 33/1000, LR 0.000270
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 2.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6769 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6694 score: 0.4898 time: 0.48s
Epoch 34/1000, LR 0.000270
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6749 score: 0.5102 time: 0.40s
Test loss: 0.6665 score: 0.5306 time: 0.52s
Epoch 35/1000, LR 0.000270
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.72s
Val loss: 0.6727 score: 0.5306 time: 0.39s
Test loss: 0.6632 score: 0.5306 time: 4.32s
Epoch 36/1000, LR 0.000270
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 12.09s
Val loss: 0.6702 score: 0.5306 time: 0.44s
Test loss: 0.6596 score: 0.5510 time: 0.49s
Epoch 37/1000, LR 0.000270
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.84s
Val loss: 0.6675 score: 0.5306 time: 0.37s
Test loss: 0.6556 score: 0.5714 time: 0.51s
Epoch 38/1000, LR 0.000270
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.60s
Val loss: 0.6645 score: 0.5510 time: 0.47s
Test loss: 0.6512 score: 0.5714 time: 5.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 4.86s
Val loss: 0.6612 score: 0.5714 time: 0.42s
Test loss: 0.6465 score: 0.5714 time: 0.46s
Epoch 40/1000, LR 0.000269
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.62s
Val loss: 0.6577 score: 0.5714 time: 0.38s
Test loss: 0.6413 score: 0.5714 time: 0.49s
Epoch 41/1000, LR 0.000269
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.71s
Val loss: 0.6539 score: 0.5714 time: 0.47s
Test loss: 0.6357 score: 0.5714 time: 0.58s
Epoch 42/1000, LR 0.000269
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 10.88s
Val loss: 0.6498 score: 0.5510 time: 0.37s
Test loss: 0.6297 score: 0.5918 time: 0.48s
Epoch 43/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.60s
Val loss: 0.6455 score: 0.5510 time: 0.48s
Test loss: 0.6233 score: 0.5918 time: 0.49s
Epoch 44/1000, LR 0.000269
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.60s
Val loss: 0.6408 score: 0.5510 time: 0.41s
Test loss: 0.6166 score: 0.5918 time: 0.50s
Epoch 45/1000, LR 0.000269
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 15.19s
Val loss: 0.6358 score: 0.5510 time: 0.48s
Test loss: 0.6095 score: 0.5918 time: 0.48s
Epoch 46/1000, LR 0.000269
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.60s
Val loss: 0.6306 score: 0.5714 time: 0.40s
Test loss: 0.6021 score: 0.5918 time: 0.50s
Epoch 47/1000, LR 0.000269
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.75s
Val loss: 0.6251 score: 0.5918 time: 0.38s
Test loss: 0.5944 score: 0.6122 time: 0.47s
Epoch 48/1000, LR 0.000269
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.58s
Val loss: 0.6193 score: 0.6122 time: 0.42s
Test loss: 0.5864 score: 0.6122 time: 0.56s
Epoch 49/1000, LR 0.000269
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 14.08s
Val loss: 0.6132 score: 0.6122 time: 0.86s
Test loss: 0.5781 score: 0.6122 time: 2.73s
Epoch 50/1000, LR 0.000269
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 0.60s
Val loss: 0.6067 score: 0.6327 time: 0.39s
Test loss: 0.5694 score: 0.6122 time: 0.70s
Epoch 51/1000, LR 0.000269
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.61s
Val loss: 0.5999 score: 0.6327 time: 0.39s
Test loss: 0.5603 score: 0.6122 time: 0.64s
Epoch 52/1000, LR 0.000269
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.61s
Val loss: 0.5927 score: 0.6735 time: 0.41s
Test loss: 0.5508 score: 0.6531 time: 4.52s
Epoch 53/1000, LR 0.000269
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 9.31s
Val loss: 0.5851 score: 0.6735 time: 0.40s
Test loss: 0.5409 score: 0.7143 time: 0.54s
Epoch 54/1000, LR 0.000269
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.63s
Val loss: 0.5770 score: 0.6735 time: 0.38s
Test loss: 0.5307 score: 0.7347 time: 0.63s
Epoch 55/1000, LR 0.000269
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.63s
Val loss: 0.5686 score: 0.6939 time: 0.39s
Test loss: 0.5201 score: 0.7755 time: 0.62s
Epoch 56/1000, LR 0.000269
Train loss: 0.5024;  Loss pred: 0.5024; Loss self: 0.0000; time: 14.15s
Val loss: 0.5600 score: 0.7143 time: 2.23s
Test loss: 0.5094 score: 0.7755 time: 0.56s
Epoch 57/1000, LR 0.000269
Train loss: 0.4945;  Loss pred: 0.4945; Loss self: 0.0000; time: 0.86s
Val loss: 0.5513 score: 0.7755 time: 0.52s
Test loss: 0.4984 score: 0.8163 time: 0.60s
Epoch 58/1000, LR 0.000269
Train loss: 0.4847;  Loss pred: 0.4847; Loss self: 0.0000; time: 0.77s
Val loss: 0.5426 score: 0.7755 time: 0.37s
Test loss: 0.4874 score: 0.8571 time: 0.50s
Epoch 59/1000, LR 0.000268
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.79s
Val loss: 0.5342 score: 0.7755 time: 4.70s
Test loss: 0.4766 score: 0.8571 time: 3.99s
Epoch 60/1000, LR 0.000268
Train loss: 0.4676;  Loss pred: 0.4676; Loss self: 0.0000; time: 0.62s
Val loss: 0.5259 score: 0.7755 time: 0.41s
Test loss: 0.4661 score: 0.8980 time: 0.47s
Epoch 61/1000, LR 0.000268
Train loss: 0.4547;  Loss pred: 0.4547; Loss self: 0.0000; time: 0.74s
Val loss: 0.5176 score: 0.7959 time: 0.37s
Test loss: 0.4557 score: 0.9184 time: 0.50s
Epoch 62/1000, LR 0.000268
Train loss: 0.4353;  Loss pred: 0.4353; Loss self: 0.0000; time: 0.59s
Val loss: 0.5092 score: 0.7959 time: 0.38s
Test loss: 0.4455 score: 0.9184 time: 0.53s
Epoch 63/1000, LR 0.000268
Train loss: 0.4320;  Loss pred: 0.4320; Loss self: 0.0000; time: 9.64s
Val loss: 0.5010 score: 0.8163 time: 0.39s
Test loss: 0.4357 score: 0.9184 time: 0.49s
Epoch 64/1000, LR 0.000268
Train loss: 0.4120;  Loss pred: 0.4120; Loss self: 0.0000; time: 0.61s
Val loss: 0.4929 score: 0.8163 time: 0.37s
Test loss: 0.4263 score: 0.9184 time: 0.53s
Epoch 65/1000, LR 0.000268
Train loss: 0.4133;  Loss pred: 0.4133; Loss self: 0.0000; time: 0.77s
Val loss: 0.4852 score: 0.8163 time: 0.54s
Test loss: 0.4175 score: 0.9184 time: 0.52s
Epoch 66/1000, LR 0.000268
Train loss: 0.3952;  Loss pred: 0.3952; Loss self: 0.0000; time: 6.17s
Val loss: 0.4777 score: 0.8571 time: 2.32s
Test loss: 0.4090 score: 0.9184 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.3889;  Loss pred: 0.3889; Loss self: 0.0000; time: 0.60s
Val loss: 0.4707 score: 0.8571 time: 0.47s
Test loss: 0.4011 score: 0.9184 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.57s
Val loss: 0.4640 score: 0.8571 time: 0.38s
Test loss: 0.3936 score: 0.9184 time: 0.50s
Epoch 69/1000, LR 0.000268
Train loss: 0.3646;  Loss pred: 0.3646; Loss self: 0.0000; time: 0.61s
Val loss: 0.4577 score: 0.8571 time: 0.47s
Test loss: 0.3866 score: 0.9184 time: 4.14s
Epoch 70/1000, LR 0.000268
Train loss: 0.3495;  Loss pred: 0.3495; Loss self: 0.0000; time: 0.64s
Val loss: 0.4520 score: 0.8571 time: 0.38s
Test loss: 0.3799 score: 0.9184 time: 0.51s
Epoch 71/1000, LR 0.000268
Train loss: 0.3479;  Loss pred: 0.3479; Loss self: 0.0000; time: 0.70s
Val loss: 0.4467 score: 0.8571 time: 0.39s
Test loss: 0.3737 score: 0.9184 time: 0.51s
Epoch 72/1000, LR 0.000267
Train loss: 0.3381;  Loss pred: 0.3381; Loss self: 0.0000; time: 0.61s
Val loss: 0.4418 score: 0.8571 time: 0.42s
Test loss: 0.3680 score: 0.9388 time: 0.53s
Epoch 73/1000, LR 0.000267
Train loss: 0.3372;  Loss pred: 0.3372; Loss self: 0.0000; time: 7.68s
Val loss: 0.4372 score: 0.8776 time: 0.38s
Test loss: 0.3627 score: 0.9388 time: 0.47s
Epoch 74/1000, LR 0.000267
Train loss: 0.3302;  Loss pred: 0.3302; Loss self: 0.0000; time: 0.61s
Val loss: 0.4327 score: 0.8776 time: 0.38s
Test loss: 0.3578 score: 0.9388 time: 0.60s
Epoch 75/1000, LR 0.000267
Train loss: 0.3280;  Loss pred: 0.3280; Loss self: 0.0000; time: 0.61s
Val loss: 0.4284 score: 0.8776 time: 0.38s
Test loss: 0.3535 score: 0.9184 time: 0.49s
Epoch 76/1000, LR 0.000267
Train loss: 0.3114;  Loss pred: 0.3114; Loss self: 0.0000; time: 9.25s
Val loss: 0.4243 score: 0.8571 time: 0.41s
Test loss: 0.3494 score: 0.9388 time: 0.48s
Epoch 77/1000, LR 0.000267
Train loss: 0.3005;  Loss pred: 0.3005; Loss self: 0.0000; time: 0.69s
Val loss: 0.4204 score: 0.8571 time: 0.38s
Test loss: 0.3452 score: 0.9388 time: 0.47s
Epoch 78/1000, LR 0.000267
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.59s
Val loss: 0.4166 score: 0.8571 time: 0.38s
Test loss: 0.3409 score: 0.9592 time: 0.60s
Epoch 79/1000, LR 0.000267
Train loss: 0.2941;  Loss pred: 0.2941; Loss self: 0.0000; time: 14.54s
Val loss: 0.4127 score: 0.8571 time: 0.47s
Test loss: 0.3366 score: 0.9592 time: 0.54s
Epoch 80/1000, LR 0.000267
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.65s
Val loss: 0.4088 score: 0.8571 time: 0.38s
Test loss: 0.3324 score: 0.9592 time: 0.53s
Epoch 81/1000, LR 0.000267
Train loss: 0.2779;  Loss pred: 0.2779; Loss self: 0.0000; time: 0.76s
Val loss: 0.4048 score: 0.8571 time: 0.52s
Test loss: 0.3282 score: 0.9388 time: 3.43s
Epoch 82/1000, LR 0.000267
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 11.43s
Val loss: 0.4009 score: 0.8571 time: 0.41s
Test loss: 0.3238 score: 0.9388 time: 0.62s
Epoch 83/1000, LR 0.000266
Train loss: 0.2623;  Loss pred: 0.2623; Loss self: 0.0000; time: 0.72s
Val loss: 0.3968 score: 0.8571 time: 0.41s
Test loss: 0.3195 score: 0.9388 time: 4.46s
Epoch 84/1000, LR 0.000266
Train loss: 0.2564;  Loss pred: 0.2564; Loss self: 0.0000; time: 3.97s
Val loss: 0.3926 score: 0.8571 time: 0.41s
Test loss: 0.3152 score: 0.9388 time: 0.51s
Epoch 85/1000, LR 0.000266
Train loss: 0.2524;  Loss pred: 0.2524; Loss self: 0.0000; time: 0.73s
Val loss: 0.3884 score: 0.8571 time: 0.39s
Test loss: 0.3108 score: 0.9388 time: 0.50s
Epoch 86/1000, LR 0.000266
Train loss: 0.2491;  Loss pred: 0.2491; Loss self: 0.0000; time: 0.66s
Val loss: 0.3843 score: 0.8571 time: 3.27s
Test loss: 0.3063 score: 0.9388 time: 4.35s
Epoch 87/1000, LR 0.000266
Train loss: 0.2421;  Loss pred: 0.2421; Loss self: 0.0000; time: 8.78s
Val loss: 0.3807 score: 0.8571 time: 0.39s
Test loss: 0.3019 score: 0.9388 time: 0.50s
Epoch 88/1000, LR 0.000266
Train loss: 0.2287;  Loss pred: 0.2287; Loss self: 0.0000; time: 0.62s
Val loss: 0.3771 score: 0.8571 time: 0.40s
Test loss: 0.2976 score: 0.9388 time: 0.52s
Epoch 89/1000, LR 0.000266
Train loss: 0.2174;  Loss pred: 0.2174; Loss self: 0.0000; time: 0.75s
Val loss: 0.3733 score: 0.8571 time: 5.26s
Test loss: 0.2933 score: 0.9388 time: 4.60s
Epoch 90/1000, LR 0.000266
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 10.26s
Val loss: 0.3695 score: 0.8571 time: 0.39s
Test loss: 0.2890 score: 0.9388 time: 0.57s
Epoch 91/1000, LR 0.000266
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.68s
Val loss: 0.3652 score: 0.8571 time: 0.49s
Test loss: 0.2845 score: 0.9388 time: 0.53s
Epoch 92/1000, LR 0.000266
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 12.41s
Val loss: 0.3608 score: 0.8571 time: 0.40s
Test loss: 0.2800 score: 0.9388 time: 0.50s
Epoch 93/1000, LR 0.000265
Train loss: 0.1917;  Loss pred: 0.1917; Loss self: 0.0000; time: 0.67s
Val loss: 0.3562 score: 0.8571 time: 0.49s
Test loss: 0.2756 score: 0.9388 time: 0.65s
Epoch 94/1000, LR 0.000265
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 12.55s
Val loss: 0.3512 score: 0.8776 time: 2.10s
Test loss: 0.2711 score: 0.9388 time: 0.56s
Epoch 95/1000, LR 0.000265
Train loss: 0.1775;  Loss pred: 0.1775; Loss self: 0.0000; time: 1.20s
Val loss: 0.3466 score: 0.8776 time: 0.43s
Test loss: 0.2668 score: 0.9388 time: 0.73s
Epoch 96/1000, LR 0.000265
Train loss: 0.1658;  Loss pred: 0.1658; Loss self: 0.0000; time: 0.65s
Val loss: 0.3422 score: 0.8776 time: 0.37s
Test loss: 0.2628 score: 0.9388 time: 0.50s
Epoch 97/1000, LR 0.000265
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 0.72s
Val loss: 0.3384 score: 0.8776 time: 0.37s
Test loss: 0.2594 score: 0.9388 time: 0.52s
Epoch 98/1000, LR 0.000265
Train loss: 0.1509;  Loss pred: 0.1509; Loss self: 0.0000; time: 0.61s
Val loss: 0.3352 score: 0.8571 time: 0.40s
Test loss: 0.2564 score: 0.9388 time: 0.62s
Epoch 99/1000, LR 0.000265
Train loss: 0.1474;  Loss pred: 0.1474; Loss self: 0.0000; time: 0.62s
Val loss: 0.3328 score: 0.8571 time: 0.40s
Test loss: 0.2541 score: 0.9388 time: 0.50s
Epoch 100/1000, LR 0.000265
Train loss: 0.1375;  Loss pred: 0.1375; Loss self: 0.0000; time: 0.62s
Val loss: 0.3310 score: 0.8571 time: 0.39s
Test loss: 0.2525 score: 0.9592 time: 0.51s
Epoch 101/1000, LR 0.000265
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.75s
Val loss: 0.3294 score: 0.8571 time: 0.40s
Test loss: 0.2510 score: 0.9592 time: 0.66s
Epoch 102/1000, LR 0.000264
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.69s
Val loss: 0.3282 score: 0.8571 time: 0.57s
Test loss: 0.2501 score: 0.9592 time: 0.64s
Epoch 103/1000, LR 0.000264
Train loss: 0.1149;  Loss pred: 0.1149; Loss self: 0.0000; time: 0.71s
Val loss: 0.3277 score: 0.8571 time: 0.52s
Test loss: 0.2499 score: 0.9592 time: 0.50s
Epoch 104/1000, LR 0.000264
Train loss: 0.1049;  Loss pred: 0.1049; Loss self: 0.0000; time: 0.61s
Val loss: 0.3277 score: 0.8571 time: 0.39s
Test loss: 0.2501 score: 0.9592 time: 0.48s
Epoch 105/1000, LR 0.000264
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.75s
Val loss: 0.3281 score: 0.8571 time: 0.36s
Test loss: 0.2510 score: 0.9592 time: 0.46s
     INFO: Early stopping counter 1 of 2
Epoch 106/1000, LR 0.000264
Train loss: 0.1027;  Loss pred: 0.1027; Loss self: 0.0000; time: 0.63s
Val loss: 0.3283 score: 0.8776 time: 0.40s
Test loss: 0.2517 score: 0.9592 time: 0.47s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 103,   Train_Loss: 0.1049,   Val_Loss: 0.3277,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3277,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2501


[0.4101442910032347, 0.37081507593393326, 5.636755637009628, 0.43722322303801775, 0.4102330980822444, 6.18457286991179, 0.42485792201478034, 0.39547653205227107, 2.9270273640286177, 0.524497541016899, 0.3951879640808329, 6.091175285051577, 0.38991824199911207, 0.5231781259644777, 0.43277969397604465, 0.4876167200272903, 3.6480638859793544, 0.41646112501621246, 0.3787445129128173, 0.48757525195833296, 0.49554311600513756, 0.3969246610067785, 0.3895486299879849, 1.883905126946047, 0.39478070894256234, 0.44452430703677237, 0.38196667993906885, 4.927246132050641, 0.3792978960555047, 0.43546888092532754, 2.3871966269798577, 0.5183377780485898, 0.4342452730052173, 5.206334159011021, 0.3950428010430187, 0.3962346649495885, 0.40234090702142566, 0.3930591410025954, 0.4133384668966755, 0.5030691140564159, 7.670755526982248, 0.42137484799604863, 0.5247849880252033, 1.1194432090269402, 1.3867557629710063, 4.178382813115604, 0.3796927280491218, 0.5384487059200183, 0.3799441320588812, 3.655878046993166, 0.4353841149713844, 0.37201496702618897, 0.3773134519578889, 0.39172910095658153, 5.643646414973773, 0.5067320140078664, 0.37986306101083755, 2.6848251919727772, 0.3670044440077618, 0.3785388880642131, 0.36757214507088065, 0.3625989550491795, 0.38442697504069656, 5.331157470005564, 0.39985974796582013, 0.5139811700209975, 0.4777706890599802, 0.37500457104761153, 0.4103090720018372, 0.41968918696511537, 0.39428449398837984, 3.0047690300270915, 0.5470916630001739, 0.4010561479954049, 1.6504924190230668, 0.3931459200102836, 0.4384398970287293, 3.908631618018262, 0.37932693504262716, 0.485551472986117, 0.48172691802028567, 0.4244220709661022, 0.3938724569743499, 0.41504489292856306, 5.863276822026819, 0.3807974250521511, 0.3835669480031356, 6.110098542063497, 0.3564564469270408, 0.39425489702261984, 0.40006466303020716, 4.826321228058077, 0.3951798209454864, 0.3799847370246425, 1.9499718029983342, 0.6727515899110585, 0.4011653469642624, 0.40005315793678164, 0.40450484200846404, 0.3801863299449906, 0.3846632050117478, 2.302288157050498, 5.804760455037467, 0.6877438599476591, 0.6232581789372489, 0.6919766169739887, 0.5633005049312487, 0.6283595809945837, 2.3475486079696566, 0.6247600469505414, 0.4878035259898752, 0.631822218070738, 0.5062481219647452, 0.6185587489744648, 0.5075291399843991, 3.742853390984237, 0.7888003139523789, 0.6227379940683022, 0.508094482938759, 0.4955762739991769, 5.576299589942209, 0.5109439509687945, 0.49177912704180926, 0.5605035290354863, 0.5027952929958701, 0.5694532869383693, 0.8083322530146688, 0.6577273060102016, 0.502386000007391, 4.959606701042503, 0.48266145901288837, 0.6964710530592129, 8.217080097994767, 5.958224485977553, 0.48452882398851216, 0.5241611439269036, 4.322445811005309, 0.4994515960570425, 0.5144652479793876, 5.24579271604307, 0.4662402729736641, 0.4919551679631695, 0.5819229671033099, 0.4878603710094467, 0.494740315945819, 0.5092014129040763, 0.48005815292708576, 0.5054615970002487, 0.4792026679497212, 0.5627813440514728, 2.730263739009388, 0.7070811999728903, 0.6442159709986299, 4.5258047099923715, 0.5404060779837891, 0.6326593470294029, 0.6247981869382784, 0.5616211770102382, 0.6044132690876722, 0.5000164340017363, 3.9901206529466435, 0.47527873306535184, 0.5000542420893908, 0.535219670040533, 0.4990253110881895, 0.5345898360246792, 0.5249103930545971, 0.4869224240537733, 0.4688797239214182, 0.5050243620062247, 4.1413210140308365, 0.518320786068216, 0.5108479090267792, 0.5342630069935694, 0.47692269599065185, 0.5997065430274233, 0.49514234892558306, 0.4845469680149108, 0.47870790504384786, 0.6042935650330037, 0.54717928590253, 0.5355977499857545, 3.432187195052393, 0.6263859189348295, 4.469178342958912, 0.5117655759677291, 0.5086992239812389, 4.357184758991934, 0.5066294659627602, 0.5243014310253784, 4.602514547063038, 0.5722831689054146, 0.5301507420372218, 0.4999151430092752, 0.6583754039602354, 0.5684499049093574, 0.735715713002719, 0.5066884079715237, 0.5224965669913217, 0.6276663290336728, 0.5018688649870455, 0.5110372990602627, 0.6598782490473241, 0.642214177059941, 0.5064832769567147, 0.4843413810012862, 0.4601062840083614, 0.4722946089459583]
[0.008370291653127238, 0.007567654610896597, 0.11503582932672711, 0.00892292291914322, 0.008372104042494784, 0.12621577285534266, 0.008670569837036334, 0.008070949633719817, 0.05973525232711465, 0.010704031449324471, 0.008065060491445569, 0.12430969969493014, 0.007957515142839022, 0.010677104611519952, 0.008832238652572339, 0.009951361633210006, 0.07445028338733377, 0.008499206632983928, 0.0077294798553636185, 0.009950515346088427, 0.010113124816431378, 0.008100503285852621, 0.00794997204057112, 0.03844704340706218, 0.008056749162093108, 0.009071924633403517, 0.007795238366103446, 0.10055604351123756, 0.007740773388887851, 0.008887120018884236, 0.048718298509793014, 0.01057832200099163, 0.008862148428677904, 0.10625171753083718, 0.008062097980469769, 0.008086421733665071, 0.008211038918804606, 0.008021615122501947, 0.008435478916258685, 0.010266716613396242, 0.15654603116290303, 0.00859948669379691, 0.010709897714800068, 0.022845779776060005, 0.028301138019816457, 0.08527311863501233, 0.007748831184675955, 0.010988749100408536, 0.0077539618787526784, 0.07460975606108503, 0.008885390101456824, 0.0075921421842079385, 0.0077002745297528345, 0.007994471448093501, 0.11517645744844435, 0.010341469673629927, 0.007752307367568113, 0.05479235085658729, 0.007489886612403302, 0.0077252834298819, 0.007501472348385319, 0.007399978674473051, 0.007845448470218298, 0.10879913204092989, 0.008160403019710615, 0.010489411633081583, 0.009750422225713882, 0.0076531545111757455, 0.008373654530649739, 0.008565085448267661, 0.008046622326293466, 0.0613218169393284, 0.011165135979595385, 0.008184819346844998, 0.033683518755572794, 0.008023386122658849, 0.008947753000586313, 0.07976799220445432, 0.0077413660212781055, 0.009909213734410552, 0.009831161592250727, 0.008661674917675555, 0.008038213407639794, 0.008470303937317613, 0.11965871065360856, 0.007771376021472471, 0.007827896898023176, 0.12469588861354075, 0.007274621365857976, 0.008046018306584078, 0.008164584959800146, 0.09849635159302199, 0.008064894305009927, 0.007754790551523317, 0.03979534291833335, 0.013729624283899153, 0.008187047897229845, 0.008164350161975135, 0.008255200857315593, 0.0077589046927549095, 0.007850269490035668, 0.046985472592867304, 0.11846449908239728, 0.014035588978523654, 0.012719554672188754, 0.01412197177497936, 0.011495928672066301, 0.01282366491825681, 0.04790915526468687, 0.012750205039806967, 0.00995517399979337, 0.01289433098103547, 0.010331594325811128, 0.012623647938254384, 0.010357737550702023, 0.07638476308131097, 0.016097965590864877, 0.012708938654455148, 0.01036927516201549, 0.010113801510187283, 0.1138020324478002, 0.010427427570791724, 0.010036308715138964, 0.011438847531336454, 0.010261128428487145, 0.011621495651803454, 0.0164965765921361, 0.013423006245106155, 0.010252775510354918, 0.1012164632865817, 0.009850233857405886, 0.0142136949603921, 0.16769551220397483, 0.12159641808117455, 0.00988834334670433, 0.01069716620258987, 0.08821317981643488, 0.010192889715449847, 0.010499290775089544, 0.1070569942049606, 0.009515107611707431, 0.01003990138700346, 0.011875978920475714, 0.009956334102233606, 0.010096741141751409, 0.010391865569470944, 0.00979710516177726, 0.010315542795923444, 0.009779646284688187, 0.011485333552070874, 0.05571966814304873, 0.014430228570875312, 0.013147264714257754, 0.09236336142841574, 0.011028695469056922, 0.012911415245498017, 0.01275098340690364, 0.01146165667367833, 0.012334964675258617, 0.010204417020443599, 0.08143103373360497, 0.009699565980925548, 0.010205188614069199, 0.010922850408990468, 0.010184190022207948, 0.010909996653564883, 0.010712457001114227, 0.009937192327628026, 0.009568973957579963, 0.010306619632780095, 0.08451675538838442, 0.010577975225881959, 0.01042546753115876, 0.010903326673338152, 0.00973311624470718, 0.012238909041375987, 0.010104945896440471, 0.009888713632957364, 0.009769549082527508, 0.012332521735367422, 0.011166924202092449, 0.010930566326239887, 0.0700446366337223, 0.012783386100710807, 0.09120772128487575, 0.010444195427912839, 0.010381616815943651, 0.08892213793861091, 0.01033937685638286, 0.01070002920459956, 0.09392886830740893, 0.011679248345008462, 0.010819402898718812, 0.010202349857332147, 0.013436232733882355, 0.011601018467537907, 0.01501460638781059, 0.010340579754520893, 0.010663195244720852, 0.012809516919054548, 0.0102422217344295, 0.010429332633882913, 0.013466903041782123, 0.01310641177673349, 0.010336393407279891, 0.009884517979618085, 0.009389924163435946, 0.009638665488693026]
[119.47015007850871, 132.141337232821, 8.692943805879642, 112.07089975579663, 119.44428723344106, 7.922940036552417, 115.33267349148157, 123.90115728415347, 16.740533621988007, 93.42274494747582, 123.99163044848551, 8.044424549766523, 125.66737003320708, 93.65834993515567, 113.22157828114797, 100.48876092119572, 13.431782318375031, 117.65804070690265, 129.3748115930573, 100.49730744780997, 98.88140591078655, 123.4491197289533, 125.78660590209581, 26.00980235105189, 124.11954001310924, 110.23019264489079, 128.2834408692833, 9.944703123569552, 129.18605800236122, 112.52239171690049, 20.526168412860045, 94.53295143655662, 112.83945513302405, 9.411612567201773, 124.03719260451497, 123.66409184878889, 121.78726832116682, 124.66317377841226, 118.54691475460666, 97.40212354699433, 6.3878974929705645, 116.28601050354929, 93.37157334547597, 43.771760465269644, 35.33426815910371, 11.727025069649669, 129.05172098439755, 91.00216875120229, 128.96632916653732, 13.403072906193032, 112.54429896511158, 131.71513068868146, 129.86549974759123, 125.08644336186568, 8.682329897562818, 96.69805468268547, 128.9938533891876, 18.25072267144342, 133.51336966089625, 129.44508885356035, 133.30716338843115, 135.13552457246905, 127.4624393743772, 9.191249794380742, 122.54296725107851, 95.3342317929628, 102.55966119731646, 130.66507392993574, 119.42217061137902, 116.75306756014395, 124.27574694693448, 16.307409824294616, 89.56451599223955, 122.17740644275429, 29.688109703044443, 124.6356569049942, 111.75990217146962, 12.536356655898867, 129.17616829528225, 100.91618031483353, 101.71738004878647, 115.4511118812988, 124.40575402608316, 118.05951798191096, 8.357101581136275, 128.6773406970631, 127.7482334051354, 8.019510595888322, 137.4641991256488, 124.28507640626387, 122.48020014779516, 10.152660315093797, 123.99418543883435, 128.952547893581, 25.128568487326923, 72.83520505165669, 122.144149216271, 122.48372254505043, 121.1357563897213, 128.88417110391592, 127.38416194110253, 21.28317424121869, 8.441347473258261, 71.24745541709258, 78.61910465988997, 70.81164131568033, 86.98731773013498, 77.9808273511825, 20.872837236958866, 78.43011127099015, 100.45027842012163, 77.55346139871584, 96.79048252037235, 79.21640439366384, 96.54618058286508, 13.091616176586266, 62.1196507319826, 78.68477669057343, 96.43875626554679, 98.87478995832916, 8.787189283800267, 95.90092985168276, 99.63822640206159, 87.42139426725669, 97.45516850016033, 86.04744431882288, 60.61863771642772, 74.49895960262899, 97.53456505412, 9.879815669597402, 101.52043235482692, 70.35468277507019, 5.963188798896774, 8.223926459185883, 101.12917451772024, 93.48270196623588, 11.33617450454599, 98.10760519504616, 95.24452855164128, 9.3408189481343, 105.0960263202484, 99.60257192311559, 84.20358495886782, 100.43857405062973, 99.04185775991255, 96.22911240670625, 102.0709672385096, 96.94109362768441, 102.25318696502158, 87.06756277180074, 17.946984131935363, 69.29897160591835, 76.06144865369103, 10.826803881266585, 90.67255531768812, 77.4508433805258, 78.42532360747798, 87.24742229423936, 81.07035782646324, 97.99677904152615, 12.280330411516314, 103.09739651923873, 97.98936970369837, 91.55119429054085, 98.19141216133734, 91.6590565289721, 93.34926617637652, 100.63204646042074, 104.50441232603214, 97.02502232832096, 11.831973380954414, 94.53605048660181, 95.91895970240991, 91.71512786508495, 102.74201754692851, 81.70662896662665, 98.96144029353547, 101.12538770130564, 102.35886953968681, 81.08641699225086, 89.55017352160597, 91.48656804720197, 14.276610573757475, 78.22653498233899, 10.963984034604117, 95.74696365097023, 96.3241099848958, 11.245793490597011, 96.71762756018173, 93.45768884164688, 10.646354182903766, 85.62194847302696, 92.42654232965258, 98.01663479333907, 74.42562359598644, 86.19932834330112, 66.60181253981035, 96.7063765997067, 93.78052047720699, 78.06695649173697, 97.63506648547487, 95.88341220906031, 74.25612235399791, 76.29853365168952, 96.74554369183578, 101.168312108087, 106.49713273446518, 103.74880227694227]
Elapsed: 1.2511166670480565~1.6815814289225666
Time per graph: 0.025532993205062374~0.034317988345358506
Speed: 85.09671815337099~39.401602271314985
Total Time: 0.4730
best val loss: 0.32765454053878784 test_score: 0.9592

Testing...
Test loss: 0.3627 score: 0.9388 time: 0.62s
test Score 0.9388
Epoch Time List: [1.8821028281236067, 1.9516737670637667, 11.735487641999498, 6.43655739503447, 1.6652437469456345, 11.210415615933016, 3.8424208777723834, 1.7168957530520856, 10.333535420941189, 1.7506778319366276, 1.5207137078978121, 11.708370335982181, 9.174257454113103, 1.9241145099513233, 11.203940422972664, 1.6211439390899613, 6.534382140031084, 10.118911528028548, 1.6462003121851012, 1.7533943890593946, 14.935949907056056, 1.739463694859296, 1.6948367969598621, 3.0531660228734836, 7.3815570548176765, 1.8508394239470363, 1.5075421740766615, 9.25309223588556, 11.379827141063288, 1.6601341040804982, 3.4944777791388333, 9.456976382178254, 1.6471137941116467, 10.798509048996493, 5.177732676966116, 1.589254059130326, 1.5073690039571375, 15.018137508886866, 1.5998423100681975, 1.6397771949414164, 12.37247650208883, 1.746441519120708, 1.6578875362174585, 16.46759994805325, 18.926413196022622, 15.760610318975523, 4.664179718005471, 1.7705336048966274, 1.452725112088956, 4.9968408329878, 9.428953106049448, 10.01627794187516, 1.5175342400325462, 1.5798890009755269, 6.796156543889083, 2.762615147046745, 1.5262764249928296, 6.110614272998646, 4.7342858909396455, 1.5331225789850578, 5.410153753007762, 1.6405605589970946, 1.513085444807075, 17.510529264225625, 1.5793708860874176, 1.8661827839678153, 1.6447292520897463, 11.165624292101711, 1.551703173900023, 1.73282128397841, 1.5440430550370365, 4.223281029961072, 10.565859466092661, 1.7277433869894594, 8.62612360890489, 1.6012240811251104, 2.036836166982539, 12.018676346051507, 1.8366510841296986, 1.5917150379391387, 1.5611570699838921, 4.355313600040972, 1.450910335057415, 1.5952838330995291, 7.089486956014298, 4.913300023064949, 1.4732495190110058, 7.263778852066025, 3.453229266917333, 1.6706905589671806, 1.535099743050523, 8.522644947865047, 10.058009714935906, 1.6051608370617032, 3.0715189611073583, 7.095996225019917, 1.4926727749407291, 1.5791903170756996, 1.5567859879229218, 5.9143400529865175, 1.572929939138703, 8.398130613029934, 6.786253616097383, 17.63669297308661, 1.7069556950591505, 1.7454601790523157, 12.319949357071891, 1.6823391711805016, 8.038757875096053, 1.6357813331997022, 1.4680638851132244, 1.6181429858552292, 8.052502617938444, 1.6263452160637826, 1.4940440530190244, 11.396942109102383, 1.8348922040313482, 1.7569071321049705, 1.5217463950393721, 1.5462593770353124, 6.822858116938733, 2.942646170966327, 1.6820273119956255, 1.8048946489579976, 8.140524475020356, 1.7611692609498277, 13.030235851998441, 1.6965438141487539, 1.5933015450136736, 8.743090249947272, 5.625326100969687, 1.7265696609392762, 14.160158741055056, 7.167979795834981, 3.4147040779935196, 1.514924920978956, 5.429087582975626, 13.031868704012595, 1.7200743779540062, 6.308120957808569, 5.743355495040305, 1.4969062469899654, 1.7637199619784951, 11.726192021858878, 1.5657703230390325, 1.5107932729879394, 16.14650489401538, 1.50414161500521, 1.6039350000210106, 1.5609854379436001, 17.661052244948223, 1.69641954486724, 1.6415089230285957, 5.540547600015998, 10.23940874391701, 1.6422676290385425, 1.6381344298133627, 16.942448023008183, 1.9811028980184346, 1.6319283199263737, 9.478700976935215, 1.501161674852483, 1.6030475259758532, 1.5050187930464745, 10.523570313001983, 1.5061915550613776, 1.8291968000121415, 8.969236175063998, 1.5294731289613992, 1.4430270050652325, 5.211612824932672, 1.5303600491024554, 1.5878613268723711, 1.5641993748722598, 8.530170511105098, 1.5785874058492482, 1.4788399561075494, 10.147864007973112, 1.5460275940131396, 1.560338247101754, 15.551263881032355, 1.5627972329966724, 4.707282126066275, 12.456501519074664, 5.597761350218207, 4.882036635885015, 1.622587211895734, 8.280558021971956, 9.682040325016715, 1.539296556962654, 10.607658545020968, 11.221443670918234, 1.6952572759473696, 13.302738061873242, 1.8118196229916066, 15.209536239155568, 2.357973427977413, 1.521443040925078, 1.6148882071720436, 1.6364872041158378, 1.5185667170444503, 1.521169506944716, 1.8074868739349768, 1.9000272790435702, 1.7229035599157214, 1.4794999989680946, 1.571300788084045, 1.4975363690173253]
Total Epoch List: [102, 106]
Total Time List: [2.3028912730515003, 0.4729964069556445]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d2fb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 10.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.44s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.58s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 5.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.48s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.60s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.48s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 5.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.68s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.42s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.42s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 6.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 4.14s
Epoch 10/1000, LR 0.000240
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.45s
Epoch 11/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.46s
Epoch 12/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 7.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.43s
Epoch 13/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.42s
Epoch 14/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.53s
Epoch 15/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 9.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 3.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.45s
Epoch 16/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.48s
Epoch 18/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 4.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 4.84s
Epoch 19/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 2.80s
Val loss: 0.6907 score: 0.5102 time: 0.52s
Test loss: 0.6898 score: 0.5208 time: 0.43s
Epoch 20/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.58s
Val loss: 0.6902 score: 0.5510 time: 0.43s
Test loss: 0.6892 score: 0.5625 time: 0.51s
Epoch 21/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.87s
Val loss: 0.6896 score: 0.5918 time: 4.47s
Test loss: 0.6885 score: 0.5625 time: 6.19s
Epoch 22/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.74s
Val loss: 0.6890 score: 0.6122 time: 0.58s
Test loss: 0.6878 score: 0.6458 time: 0.43s
Epoch 23/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.64s
Val loss: 0.6883 score: 0.6327 time: 0.51s
Test loss: 0.6870 score: 0.6667 time: 0.57s
Epoch 24/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 8.77s
Val loss: 0.6876 score: 0.6939 time: 0.44s
Test loss: 0.6861 score: 0.6667 time: 0.44s
Epoch 25/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.89s
Val loss: 0.6867 score: 0.6531 time: 0.43s
Test loss: 0.6851 score: 0.7708 time: 0.43s
Epoch 26/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.62s
Val loss: 0.6857 score: 0.6939 time: 0.45s
Test loss: 0.6840 score: 0.8125 time: 2.32s
Epoch 27/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 12.98s
Val loss: 0.6846 score: 0.7347 time: 0.41s
Test loss: 0.6827 score: 0.8125 time: 0.45s
Epoch 28/1000, LR 0.000270
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 0.83s
Val loss: 0.6834 score: 0.7143 time: 0.42s
Test loss: 0.6814 score: 0.7708 time: 0.42s
Epoch 29/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.77s
Val loss: 0.6821 score: 0.6735 time: 4.69s
Test loss: 0.6798 score: 0.7083 time: 6.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 3.03s
Val loss: 0.6808 score: 0.6939 time: 0.45s
Test loss: 0.6782 score: 0.6875 time: 0.53s
Epoch 31/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.75s
Val loss: 0.6793 score: 0.6735 time: 0.44s
Test loss: 0.6765 score: 0.7083 time: 0.52s
Epoch 32/1000, LR 0.000270
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.79s
Val loss: 0.6777 score: 0.6939 time: 0.45s
Test loss: 0.6745 score: 0.7292 time: 0.48s
Epoch 33/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.83s
Val loss: 0.6760 score: 0.6735 time: 4.62s
Test loss: 0.6725 score: 0.7292 time: 6.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.89s
Val loss: 0.6742 score: 0.6939 time: 0.42s
Test loss: 0.6702 score: 0.7292 time: 0.45s
Epoch 35/1000, LR 0.000270
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.77s
Val loss: 0.6721 score: 0.6939 time: 0.53s
Test loss: 0.6678 score: 0.7500 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.64s
Val loss: 0.6699 score: 0.6939 time: 0.41s
Test loss: 0.6651 score: 0.7500 time: 0.43s
Epoch 37/1000, LR 0.000270
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.90s
Val loss: 0.6675 score: 0.6939 time: 5.84s
Test loss: 0.6622 score: 0.7500 time: 3.33s
Epoch 38/1000, LR 0.000270
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 2.69s
Val loss: 0.6648 score: 0.6735 time: 0.43s
Test loss: 0.6590 score: 0.7500 time: 0.44s
Epoch 39/1000, LR 0.000269
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.71s
Val loss: 0.6619 score: 0.6735 time: 0.42s
Test loss: 0.6555 score: 0.7500 time: 0.43s
Epoch 40/1000, LR 0.000269
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.63s
Val loss: 0.6588 score: 0.6735 time: 0.41s
Test loss: 0.6518 score: 0.7292 time: 0.45s
Epoch 41/1000, LR 0.000269
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.60s
Val loss: 0.6554 score: 0.6735 time: 1.27s
Test loss: 0.6477 score: 0.7292 time: 5.55s
Epoch 42/1000, LR 0.000269
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 9.52s
Val loss: 0.6518 score: 0.6939 time: 0.43s
Test loss: 0.6433 score: 0.7292 time: 0.47s
Epoch 43/1000, LR 0.000269
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.64s
Val loss: 0.6478 score: 0.7143 time: 0.54s
Test loss: 0.6386 score: 0.7292 time: 0.45s
Epoch 44/1000, LR 0.000269
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.71s
Val loss: 0.6436 score: 0.7143 time: 2.90s
Test loss: 0.6335 score: 0.7292 time: 5.21s
Epoch 45/1000, LR 0.000269
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 1.42s
Val loss: 0.6391 score: 0.7143 time: 0.44s
Test loss: 0.6280 score: 0.7708 time: 0.55s
Epoch 46/1000, LR 0.000269
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.61s
Val loss: 0.6342 score: 0.7143 time: 0.42s
Test loss: 0.6220 score: 0.7708 time: 0.44s
Epoch 47/1000, LR 0.000269
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 0.68s
Val loss: 0.6289 score: 0.7143 time: 0.56s
Test loss: 0.6156 score: 0.7917 time: 0.48s
Epoch 48/1000, LR 0.000269
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 8.57s
Val loss: 0.6233 score: 0.7143 time: 0.48s
Test loss: 0.6087 score: 0.7917 time: 0.54s
Epoch 49/1000, LR 0.000269
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.81s
Val loss: 0.6172 score: 0.7143 time: 0.41s
Test loss: 0.6014 score: 0.7917 time: 0.48s
Epoch 50/1000, LR 0.000269
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.70s
Val loss: 0.6107 score: 0.7347 time: 4.52s
Test loss: 0.5936 score: 0.7917 time: 3.71s
Epoch 51/1000, LR 0.000269
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 4.15s
Val loss: 0.6037 score: 0.7755 time: 0.41s
Test loss: 0.5853 score: 0.7917 time: 0.44s
Epoch 52/1000, LR 0.000269
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.63s
Val loss: 0.5963 score: 0.7959 time: 0.41s
Test loss: 0.5764 score: 0.8125 time: 0.44s
Epoch 53/1000, LR 0.000269
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.98s
Val loss: 0.5884 score: 0.7959 time: 2.21s
Test loss: 0.5670 score: 0.8333 time: 5.67s
Epoch 54/1000, LR 0.000269
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 7.56s
Val loss: 0.5800 score: 0.7959 time: 0.42s
Test loss: 0.5572 score: 0.8542 time: 0.43s
Epoch 55/1000, LR 0.000269
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.72s
Val loss: 0.5711 score: 0.7959 time: 0.42s
Test loss: 0.5467 score: 0.8542 time: 0.48s
Epoch 56/1000, LR 0.000269
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.65s
Val loss: 0.5617 score: 0.8163 time: 0.42s
Test loss: 0.5358 score: 0.8750 time: 0.48s
Epoch 57/1000, LR 0.000269
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.85s
Val loss: 0.5520 score: 0.8367 time: 6.26s
Test loss: 0.5245 score: 0.9167 time: 0.43s
Epoch 58/1000, LR 0.000269
Train loss: 0.5009;  Loss pred: 0.5009; Loss self: 0.0000; time: 0.72s
Val loss: 0.5418 score: 0.8571 time: 0.50s
Test loss: 0.5128 score: 0.9167 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.4869;  Loss pred: 0.4869; Loss self: 0.0000; time: 0.76s
Val loss: 0.5312 score: 0.8571 time: 0.43s
Test loss: 0.5007 score: 0.9167 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.62s
Val loss: 0.5203 score: 0.8571 time: 6.54s
Test loss: 0.4885 score: 0.9167 time: 6.12s
Epoch 61/1000, LR 0.000268
Train loss: 0.4570;  Loss pred: 0.4570; Loss self: 0.0000; time: 13.21s
Val loss: 0.5093 score: 0.8571 time: 0.49s
Test loss: 0.4763 score: 0.9167 time: 0.52s
Epoch 62/1000, LR 0.000268
Train loss: 0.4484;  Loss pred: 0.4484; Loss self: 0.0000; time: 0.60s
Val loss: 0.4981 score: 0.8571 time: 0.41s
Test loss: 0.4638 score: 0.9167 time: 0.44s
Epoch 63/1000, LR 0.000268
Train loss: 0.4254;  Loss pred: 0.4254; Loss self: 0.0000; time: 0.72s
Val loss: 0.4867 score: 0.8571 time: 0.43s
Test loss: 0.4512 score: 0.9167 time: 5.45s
Epoch 64/1000, LR 0.000268
Train loss: 0.4272;  Loss pred: 0.4272; Loss self: 0.0000; time: 0.65s
Val loss: 0.4754 score: 0.8571 time: 0.49s
Test loss: 0.4390 score: 0.9167 time: 0.46s
Epoch 65/1000, LR 0.000268
Train loss: 0.4005;  Loss pred: 0.4005; Loss self: 0.0000; time: 0.63s
Val loss: 0.4641 score: 0.8571 time: 0.56s
Test loss: 0.4268 score: 0.9167 time: 0.47s
Epoch 66/1000, LR 0.000268
Train loss: 0.3840;  Loss pred: 0.3840; Loss self: 0.0000; time: 0.68s
Val loss: 0.4530 score: 0.8571 time: 0.49s
Test loss: 0.4148 score: 0.9375 time: 4.41s
Epoch 67/1000, LR 0.000268
Train loss: 0.3739;  Loss pred: 0.3739; Loss self: 0.0000; time: 6.48s
Val loss: 0.4425 score: 0.8571 time: 0.60s
Test loss: 0.4033 score: 0.9375 time: 0.53s
Epoch 68/1000, LR 0.000268
Train loss: 0.3668;  Loss pred: 0.3668; Loss self: 0.0000; time: 0.61s
Val loss: 0.4324 score: 0.8776 time: 0.41s
Test loss: 0.3921 score: 0.9375 time: 0.47s
Epoch 69/1000, LR 0.000268
Train loss: 0.3454;  Loss pred: 0.3454; Loss self: 0.0000; time: 0.78s
Val loss: 0.4226 score: 0.8776 time: 0.72s
Test loss: 0.3810 score: 0.9375 time: 5.79s
Epoch 70/1000, LR 0.000268
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 10.06s
Val loss: 0.4130 score: 0.8980 time: 0.44s
Test loss: 0.3701 score: 0.9375 time: 0.54s
Epoch 71/1000, LR 0.000268
Train loss: 0.3150;  Loss pred: 0.3150; Loss self: 0.0000; time: 0.65s
Val loss: 0.4035 score: 0.8980 time: 0.54s
Test loss: 0.3590 score: 0.9375 time: 0.45s
Epoch 72/1000, LR 0.000267
Train loss: 0.2972;  Loss pred: 0.2972; Loss self: 0.0000; time: 0.63s
Val loss: 0.3942 score: 0.8980 time: 0.51s
Test loss: 0.3482 score: 0.9375 time: 0.58s
Epoch 73/1000, LR 0.000267
Train loss: 0.2888;  Loss pred: 0.2888; Loss self: 0.0000; time: 8.57s
Val loss: 0.3852 score: 0.8980 time: 0.43s
Test loss: 0.3376 score: 0.9375 time: 0.45s
Epoch 74/1000, LR 0.000267
Train loss: 0.2697;  Loss pred: 0.2697; Loss self: 0.0000; time: 0.63s
Val loss: 0.3764 score: 0.8980 time: 0.44s
Test loss: 0.3273 score: 0.9375 time: 0.46s
Epoch 75/1000, LR 0.000267
Train loss: 0.2680;  Loss pred: 0.2680; Loss self: 0.0000; time: 0.76s
Val loss: 0.3683 score: 0.9184 time: 0.42s
Test loss: 0.3181 score: 0.9375 time: 0.46s
Epoch 76/1000, LR 0.000267
Train loss: 0.2456;  Loss pred: 0.2456; Loss self: 0.0000; time: 0.72s
Val loss: 0.3600 score: 0.9184 time: 5.63s
Test loss: 0.3086 score: 0.9375 time: 2.77s
Epoch 77/1000, LR 0.000267
Train loss: 0.2352;  Loss pred: 0.2352; Loss self: 0.0000; time: 9.43s
Val loss: 0.3521 score: 0.9184 time: 0.44s
Test loss: 0.2995 score: 0.9375 time: 0.58s
Epoch 78/1000, LR 0.000267
Train loss: 0.2248;  Loss pred: 0.2248; Loss self: 0.0000; time: 0.64s
Val loss: 0.3445 score: 0.9184 time: 0.47s
Test loss: 0.2910 score: 0.9375 time: 0.45s
Epoch 79/1000, LR 0.000267
Train loss: 0.2156;  Loss pred: 0.2156; Loss self: 0.0000; time: 0.80s
Val loss: 0.3372 score: 0.9184 time: 2.92s
Test loss: 0.2832 score: 0.9375 time: 5.75s
Epoch 80/1000, LR 0.000267
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 5.06s
Val loss: 0.3302 score: 0.9184 time: 0.45s
Test loss: 0.2758 score: 0.8958 time: 0.45s
Epoch 81/1000, LR 0.000267
Train loss: 0.1920;  Loss pred: 0.1920; Loss self: 0.0000; time: 0.85s
Val loss: 0.3238 score: 0.9184 time: 0.44s
Test loss: 0.2690 score: 0.8958 time: 0.45s
Epoch 82/1000, LR 0.000267
Train loss: 0.1798;  Loss pred: 0.1798; Loss self: 0.0000; time: 0.69s
Val loss: 0.3179 score: 0.9184 time: 3.73s
Test loss: 0.2630 score: 0.8958 time: 5.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.1674;  Loss pred: 0.1674; Loss self: 0.0000; time: 11.10s
Val loss: 0.3125 score: 0.9184 time: 0.37s
Test loss: 0.2572 score: 0.8958 time: 0.40s
Epoch 84/1000, LR 0.000266
Train loss: 0.1547;  Loss pred: 0.1547; Loss self: 0.0000; time: 0.60s
Val loss: 0.3076 score: 0.9184 time: 0.40s
Test loss: 0.2518 score: 0.8958 time: 0.50s
Epoch 85/1000, LR 0.000266
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 0.71s
Val loss: 0.3034 score: 0.9184 time: 0.46s
Test loss: 0.2472 score: 0.8958 time: 4.52s
Epoch 86/1000, LR 0.000266
Train loss: 0.1372;  Loss pred: 0.1372; Loss self: 0.0000; time: 4.64s
Val loss: 0.3000 score: 0.9184 time: 0.42s
Test loss: 0.2433 score: 0.8958 time: 0.56s
Epoch 87/1000, LR 0.000266
Train loss: 0.1277;  Loss pred: 0.1277; Loss self: 0.0000; time: 0.78s
Val loss: 0.2979 score: 0.9184 time: 0.40s
Test loss: 0.2407 score: 0.8958 time: 0.44s
Epoch 88/1000, LR 0.000266
Train loss: 0.1183;  Loss pred: 0.1183; Loss self: 0.0000; time: 0.61s
Val loss: 0.2965 score: 0.9184 time: 0.43s
Test loss: 0.2383 score: 0.8958 time: 0.45s
Epoch 89/1000, LR 0.000266
Train loss: 0.1069;  Loss pred: 0.1069; Loss self: 0.0000; time: 0.68s
Val loss: 0.2964 score: 0.8980 time: 4.19s
Test loss: 0.2369 score: 0.8958 time: 5.50s
Epoch 90/1000, LR 0.000266
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.61s
Val loss: 0.2971 score: 0.8980 time: 0.51s
Test loss: 0.2357 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 1 of 2
Epoch 91/1000, LR 0.000266
Train loss: 0.0960;  Loss pred: 0.0960; Loss self: 0.0000; time: 0.61s
Val loss: 0.2986 score: 0.8980 time: 0.51s
Test loss: 0.2349 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.1069,   Val_Loss: 0.2964,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.2964,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2369


[0.4101442910032347, 0.37081507593393326, 5.636755637009628, 0.43722322303801775, 0.4102330980822444, 6.18457286991179, 0.42485792201478034, 0.39547653205227107, 2.9270273640286177, 0.524497541016899, 0.3951879640808329, 6.091175285051577, 0.38991824199911207, 0.5231781259644777, 0.43277969397604465, 0.4876167200272903, 3.6480638859793544, 0.41646112501621246, 0.3787445129128173, 0.48757525195833296, 0.49554311600513756, 0.3969246610067785, 0.3895486299879849, 1.883905126946047, 0.39478070894256234, 0.44452430703677237, 0.38196667993906885, 4.927246132050641, 0.3792978960555047, 0.43546888092532754, 2.3871966269798577, 0.5183377780485898, 0.4342452730052173, 5.206334159011021, 0.3950428010430187, 0.3962346649495885, 0.40234090702142566, 0.3930591410025954, 0.4133384668966755, 0.5030691140564159, 7.670755526982248, 0.42137484799604863, 0.5247849880252033, 1.1194432090269402, 1.3867557629710063, 4.178382813115604, 0.3796927280491218, 0.5384487059200183, 0.3799441320588812, 3.655878046993166, 0.4353841149713844, 0.37201496702618897, 0.3773134519578889, 0.39172910095658153, 5.643646414973773, 0.5067320140078664, 0.37986306101083755, 2.6848251919727772, 0.3670044440077618, 0.3785388880642131, 0.36757214507088065, 0.3625989550491795, 0.38442697504069656, 5.331157470005564, 0.39985974796582013, 0.5139811700209975, 0.4777706890599802, 0.37500457104761153, 0.4103090720018372, 0.41968918696511537, 0.39428449398837984, 3.0047690300270915, 0.5470916630001739, 0.4010561479954049, 1.6504924190230668, 0.3931459200102836, 0.4384398970287293, 3.908631618018262, 0.37932693504262716, 0.485551472986117, 0.48172691802028567, 0.4244220709661022, 0.3938724569743499, 0.41504489292856306, 5.863276822026819, 0.3807974250521511, 0.3835669480031356, 6.110098542063497, 0.3564564469270408, 0.39425489702261984, 0.40006466303020716, 4.826321228058077, 0.3951798209454864, 0.3799847370246425, 1.9499718029983342, 0.6727515899110585, 0.4011653469642624, 0.40005315793678164, 0.40450484200846404, 0.3801863299449906, 0.3846632050117478, 2.302288157050498, 5.804760455037467, 0.6877438599476591, 0.6232581789372489, 0.6919766169739887, 0.5633005049312487, 0.6283595809945837, 2.3475486079696566, 0.6247600469505414, 0.4878035259898752, 0.631822218070738, 0.5062481219647452, 0.6185587489744648, 0.5075291399843991, 3.742853390984237, 0.7888003139523789, 0.6227379940683022, 0.508094482938759, 0.4955762739991769, 5.576299589942209, 0.5109439509687945, 0.49177912704180926, 0.5605035290354863, 0.5027952929958701, 0.5694532869383693, 0.8083322530146688, 0.6577273060102016, 0.502386000007391, 4.959606701042503, 0.48266145901288837, 0.6964710530592129, 8.217080097994767, 5.958224485977553, 0.48452882398851216, 0.5241611439269036, 4.322445811005309, 0.4994515960570425, 0.5144652479793876, 5.24579271604307, 0.4662402729736641, 0.4919551679631695, 0.5819229671033099, 0.4878603710094467, 0.494740315945819, 0.5092014129040763, 0.48005815292708576, 0.5054615970002487, 0.4792026679497212, 0.5627813440514728, 2.730263739009388, 0.7070811999728903, 0.6442159709986299, 4.5258047099923715, 0.5404060779837891, 0.6326593470294029, 0.6247981869382784, 0.5616211770102382, 0.6044132690876722, 0.5000164340017363, 3.9901206529466435, 0.47527873306535184, 0.5000542420893908, 0.535219670040533, 0.4990253110881895, 0.5345898360246792, 0.5249103930545971, 0.4869224240537733, 0.4688797239214182, 0.5050243620062247, 4.1413210140308365, 0.518320786068216, 0.5108479090267792, 0.5342630069935694, 0.47692269599065185, 0.5997065430274233, 0.49514234892558306, 0.4845469680149108, 0.47870790504384786, 0.6042935650330037, 0.54717928590253, 0.5355977499857545, 3.432187195052393, 0.6263859189348295, 4.469178342958912, 0.5117655759677291, 0.5086992239812389, 4.357184758991934, 0.5066294659627602, 0.5243014310253784, 4.602514547063038, 0.5722831689054146, 0.5301507420372218, 0.4999151430092752, 0.6583754039602354, 0.5684499049093574, 0.735715713002719, 0.5066884079715237, 0.5224965669913217, 0.6276663290336728, 0.5018688649870455, 0.5110372990602627, 0.6598782490473241, 0.642214177059941, 0.5064832769567147, 0.4843413810012862, 0.4601062840083614, 0.4722946089459583, 0.4486980129731819, 0.581226950045675, 0.4827460889937356, 0.6095752600813285, 0.4917016860563308, 0.6875854099635035, 0.42762952705379575, 0.4279197290306911, 4.145552276982926, 0.4507853720569983, 0.46255106606986374, 0.4367556539364159, 0.4293226129375398, 0.5315910730278119, 0.4547855800483376, 0.4684161249315366, 0.48906276701018214, 4.841165330028161, 0.43865917494986206, 0.5163785429904237, 6.1917658150196075, 0.43445189006160945, 0.5782204730203375, 0.4463981930166483, 0.43214119295589626, 2.325936137000099, 0.4571655340259895, 0.4238234889926389, 6.083829685929231, 0.5381272049853578, 0.5276449009543285, 0.4844116159947589, 6.091694778064266, 0.4510509530082345, 0.4333114930195734, 0.43839823198504746, 3.3324320330284536, 0.45019511005375534, 0.4325760300271213, 0.4535620289389044, 5.5529747230466455, 0.47825816995464265, 0.4544558000052348, 5.2121716879773885, 0.5533737540245056, 0.4489402479957789, 0.4911098099546507, 0.5463648199802265, 0.48818331700749695, 3.712242786074057, 0.439989065984264, 0.4471928220009431, 5.67619602906052, 0.4297961600823328, 0.48829904198646545, 0.48353687801864, 0.43797983205877244, 0.46802629495505244, 0.45237618091050535, 6.126817718031816, 0.520226065069437, 0.44281427608802915, 5.450456570019014, 0.46335097402334213, 0.4701134640490636, 4.4134584999410436, 0.5332442919025198, 0.47556272498331964, 5.799189370009117, 0.5400820759823546, 0.4575103319948539, 0.5853190260240808, 0.45789119601249695, 0.4620731750037521, 0.4690121420426294, 2.78184201894328, 0.5873445778852329, 0.4517314959084615, 5.759730475954711, 0.4541831319220364, 0.45151105406694114, 5.06149336497765, 0.40755858505144715, 0.5050690929638222, 4.523217957932502, 0.5602215909166262, 0.44351271202322096, 0.45892922300845385, 5.503894893918186, 0.47819443594198674, 0.4351378740975633]
[0.008370291653127238, 0.007567654610896597, 0.11503582932672711, 0.00892292291914322, 0.008372104042494784, 0.12621577285534266, 0.008670569837036334, 0.008070949633719817, 0.05973525232711465, 0.010704031449324471, 0.008065060491445569, 0.12430969969493014, 0.007957515142839022, 0.010677104611519952, 0.008832238652572339, 0.009951361633210006, 0.07445028338733377, 0.008499206632983928, 0.0077294798553636185, 0.009950515346088427, 0.010113124816431378, 0.008100503285852621, 0.00794997204057112, 0.03844704340706218, 0.008056749162093108, 0.009071924633403517, 0.007795238366103446, 0.10055604351123756, 0.007740773388887851, 0.008887120018884236, 0.048718298509793014, 0.01057832200099163, 0.008862148428677904, 0.10625171753083718, 0.008062097980469769, 0.008086421733665071, 0.008211038918804606, 0.008021615122501947, 0.008435478916258685, 0.010266716613396242, 0.15654603116290303, 0.00859948669379691, 0.010709897714800068, 0.022845779776060005, 0.028301138019816457, 0.08527311863501233, 0.007748831184675955, 0.010988749100408536, 0.0077539618787526784, 0.07460975606108503, 0.008885390101456824, 0.0075921421842079385, 0.0077002745297528345, 0.007994471448093501, 0.11517645744844435, 0.010341469673629927, 0.007752307367568113, 0.05479235085658729, 0.007489886612403302, 0.0077252834298819, 0.007501472348385319, 0.007399978674473051, 0.007845448470218298, 0.10879913204092989, 0.008160403019710615, 0.010489411633081583, 0.009750422225713882, 0.0076531545111757455, 0.008373654530649739, 0.008565085448267661, 0.008046622326293466, 0.0613218169393284, 0.011165135979595385, 0.008184819346844998, 0.033683518755572794, 0.008023386122658849, 0.008947753000586313, 0.07976799220445432, 0.0077413660212781055, 0.009909213734410552, 0.009831161592250727, 0.008661674917675555, 0.008038213407639794, 0.008470303937317613, 0.11965871065360856, 0.007771376021472471, 0.007827896898023176, 0.12469588861354075, 0.007274621365857976, 0.008046018306584078, 0.008164584959800146, 0.09849635159302199, 0.008064894305009927, 0.007754790551523317, 0.03979534291833335, 0.013729624283899153, 0.008187047897229845, 0.008164350161975135, 0.008255200857315593, 0.0077589046927549095, 0.007850269490035668, 0.046985472592867304, 0.11846449908239728, 0.014035588978523654, 0.012719554672188754, 0.01412197177497936, 0.011495928672066301, 0.01282366491825681, 0.04790915526468687, 0.012750205039806967, 0.00995517399979337, 0.01289433098103547, 0.010331594325811128, 0.012623647938254384, 0.010357737550702023, 0.07638476308131097, 0.016097965590864877, 0.012708938654455148, 0.01036927516201549, 0.010113801510187283, 0.1138020324478002, 0.010427427570791724, 0.010036308715138964, 0.011438847531336454, 0.010261128428487145, 0.011621495651803454, 0.0164965765921361, 0.013423006245106155, 0.010252775510354918, 0.1012164632865817, 0.009850233857405886, 0.0142136949603921, 0.16769551220397483, 0.12159641808117455, 0.00988834334670433, 0.01069716620258987, 0.08821317981643488, 0.010192889715449847, 0.010499290775089544, 0.1070569942049606, 0.009515107611707431, 0.01003990138700346, 0.011875978920475714, 0.009956334102233606, 0.010096741141751409, 0.010391865569470944, 0.00979710516177726, 0.010315542795923444, 0.009779646284688187, 0.011485333552070874, 0.05571966814304873, 0.014430228570875312, 0.013147264714257754, 0.09236336142841574, 0.011028695469056922, 0.012911415245498017, 0.01275098340690364, 0.01146165667367833, 0.012334964675258617, 0.010204417020443599, 0.08143103373360497, 0.009699565980925548, 0.010205188614069199, 0.010922850408990468, 0.010184190022207948, 0.010909996653564883, 0.010712457001114227, 0.009937192327628026, 0.009568973957579963, 0.010306619632780095, 0.08451675538838442, 0.010577975225881959, 0.01042546753115876, 0.010903326673338152, 0.00973311624470718, 0.012238909041375987, 0.010104945896440471, 0.009888713632957364, 0.009769549082527508, 0.012332521735367422, 0.011166924202092449, 0.010930566326239887, 0.0700446366337223, 0.012783386100710807, 0.09120772128487575, 0.010444195427912839, 0.010381616815943651, 0.08892213793861091, 0.01033937685638286, 0.01070002920459956, 0.09392886830740893, 0.011679248345008462, 0.010819402898718812, 0.010202349857332147, 0.013436232733882355, 0.011601018467537907, 0.01501460638781059, 0.010340579754520893, 0.010663195244720852, 0.012809516919054548, 0.0102422217344295, 0.010429332633882913, 0.013466903041782123, 0.01310641177673349, 0.010336393407279891, 0.009884517979618085, 0.009389924163435946, 0.009638665488693026, 0.009347875270274622, 0.01210889479261823, 0.010057210187369492, 0.012699484585027676, 0.010243785126173558, 0.014324696040906323, 0.008908948480287412, 0.008914994354806064, 0.08636567243714428, 0.009391361917854132, 0.009636480543122161, 0.009099076123675331, 0.008944221102865413, 0.011074814021412749, 0.009474699584340366, 0.009758669269407013, 0.010188807646045461, 0.10085761104225337, 0.00913873281145546, 0.010757886312300494, 0.12899512114624181, 0.009051081042950196, 0.012046259854590366, 0.009299962354513506, 0.009002941519914506, 0.04845700285416873, 0.00952428195887478, 0.008829656020679977, 0.1267464517901923, 0.011210983437194955, 0.010992602103215177, 0.010091908666557478, 0.12691030787633886, 0.00939689485433822, 0.009027322771241112, 0.00913329649968849, 0.06942566735475945, 0.00937906479278657, 0.009012000625565028, 0.009449208936227175, 0.11568697339680512, 0.009963711874055056, 0.009467829166775724, 0.1085869101661956, 0.011528619875510534, 0.009352921833245395, 0.010231454374055223, 0.011382600416254718, 0.01017048577098952, 0.07733839137654286, 0.009166438874672167, 0.009316517125019649, 0.11825408393876084, 0.008954086668381933, 0.010172896708051363, 0.010073684958721666, 0.009124579834557759, 0.009750547811563592, 0.009424503768968862, 0.1276420357923295, 0.010838043022279939, 0.009225297418500608, 0.1135511785420628, 0.009653145292152962, 0.009794030501022158, 0.09194705208210507, 0.011109256081302496, 0.009907556770485826, 0.12081644520852326, 0.011251709916299054, 0.00953146524989279, 0.012194146375501683, 0.00953939991692702, 0.009626524479244836, 0.009771086292554779, 0.05795504206131833, 0.012236345372609017, 0.009411072831426281, 0.11999438491572316, 0.009462148581709092, 0.009406480293061273, 0.10544777843703439, 0.008490803855238482, 0.01052227277007963, 0.09423370745692712, 0.011671283144096378, 0.009239848167150436, 0.009561025479342788, 0.11466447695662889, 0.009962384082124723, 0.009065372377032569]
[119.47015007850871, 132.141337232821, 8.692943805879642, 112.07089975579663, 119.44428723344106, 7.922940036552417, 115.33267349148157, 123.90115728415347, 16.740533621988007, 93.42274494747582, 123.99163044848551, 8.044424549766523, 125.66737003320708, 93.65834993515567, 113.22157828114797, 100.48876092119572, 13.431782318375031, 117.65804070690265, 129.3748115930573, 100.49730744780997, 98.88140591078655, 123.4491197289533, 125.78660590209581, 26.00980235105189, 124.11954001310924, 110.23019264489079, 128.2834408692833, 9.944703123569552, 129.18605800236122, 112.52239171690049, 20.526168412860045, 94.53295143655662, 112.83945513302405, 9.411612567201773, 124.03719260451497, 123.66409184878889, 121.78726832116682, 124.66317377841226, 118.54691475460666, 97.40212354699433, 6.3878974929705645, 116.28601050354929, 93.37157334547597, 43.771760465269644, 35.33426815910371, 11.727025069649669, 129.05172098439755, 91.00216875120229, 128.96632916653732, 13.403072906193032, 112.54429896511158, 131.71513068868146, 129.86549974759123, 125.08644336186568, 8.682329897562818, 96.69805468268547, 128.9938533891876, 18.25072267144342, 133.51336966089625, 129.44508885356035, 133.30716338843115, 135.13552457246905, 127.4624393743772, 9.191249794380742, 122.54296725107851, 95.3342317929628, 102.55966119731646, 130.66507392993574, 119.42217061137902, 116.75306756014395, 124.27574694693448, 16.307409824294616, 89.56451599223955, 122.17740644275429, 29.688109703044443, 124.6356569049942, 111.75990217146962, 12.536356655898867, 129.17616829528225, 100.91618031483353, 101.71738004878647, 115.4511118812988, 124.40575402608316, 118.05951798191096, 8.357101581136275, 128.6773406970631, 127.7482334051354, 8.019510595888322, 137.4641991256488, 124.28507640626387, 122.48020014779516, 10.152660315093797, 123.99418543883435, 128.952547893581, 25.128568487326923, 72.83520505165669, 122.144149216271, 122.48372254505043, 121.1357563897213, 128.88417110391592, 127.38416194110253, 21.28317424121869, 8.441347473258261, 71.24745541709258, 78.61910465988997, 70.81164131568033, 86.98731773013498, 77.9808273511825, 20.872837236958866, 78.43011127099015, 100.45027842012163, 77.55346139871584, 96.79048252037235, 79.21640439366384, 96.54618058286508, 13.091616176586266, 62.1196507319826, 78.68477669057343, 96.43875626554679, 98.87478995832916, 8.787189283800267, 95.90092985168276, 99.63822640206159, 87.42139426725669, 97.45516850016033, 86.04744431882288, 60.61863771642772, 74.49895960262899, 97.53456505412, 9.879815669597402, 101.52043235482692, 70.35468277507019, 5.963188798896774, 8.223926459185883, 101.12917451772024, 93.48270196623588, 11.33617450454599, 98.10760519504616, 95.24452855164128, 9.3408189481343, 105.0960263202484, 99.60257192311559, 84.20358495886782, 100.43857405062973, 99.04185775991255, 96.22911240670625, 102.0709672385096, 96.94109362768441, 102.25318696502158, 87.06756277180074, 17.946984131935363, 69.29897160591835, 76.06144865369103, 10.826803881266585, 90.67255531768812, 77.4508433805258, 78.42532360747798, 87.24742229423936, 81.07035782646324, 97.99677904152615, 12.280330411516314, 103.09739651923873, 97.98936970369837, 91.55119429054085, 98.19141216133734, 91.6590565289721, 93.34926617637652, 100.63204646042074, 104.50441232603214, 97.02502232832096, 11.831973380954414, 94.53605048660181, 95.91895970240991, 91.71512786508495, 102.74201754692851, 81.70662896662665, 98.96144029353547, 101.12538770130564, 102.35886953968681, 81.08641699225086, 89.55017352160597, 91.48656804720197, 14.276610573757475, 78.22653498233899, 10.963984034604117, 95.74696365097023, 96.3241099848958, 11.245793490597011, 96.71762756018173, 93.45768884164688, 10.646354182903766, 85.62194847302696, 92.42654232965258, 98.01663479333907, 74.42562359598644, 86.19932834330112, 66.60181253981035, 96.7063765997067, 93.78052047720699, 78.06695649173697, 97.63506648547487, 95.88341220906031, 74.25612235399791, 76.29853365168952, 96.74554369183578, 101.168312108087, 106.49713273446518, 103.74880227694227, 106.97618133394519, 82.58392009563214, 99.43115251343418, 78.74335318922871, 97.62016556213513, 69.80950919617071, 112.24669243656228, 112.17057018784325, 11.578674394364102, 106.48082873889433, 103.77232595708703, 109.90126760210865, 111.8040339677689, 90.29497001633946, 105.544243497998, 102.47298810863022, 98.14691127161731, 9.914968138409101, 109.42436119223156, 92.95506300866991, 7.752231178311772, 110.48404000082297, 83.013317998361, 107.52731698044721, 111.07480791561294, 20.636852077077453, 104.99479166176872, 113.25469504790397, 7.889767215379991, 89.19824077897354, 90.97027169822826, 99.08928360735126, 7.879580600926426, 106.41813231935173, 110.7748139000591, 109.48949265296568, 14.403894670397365, 106.62043840118251, 110.9631525283325, 105.82896481060077, 8.644015576153162, 100.3642028834599, 105.62083265181586, 9.209213140602944, 86.74065159562006, 106.91846011643695, 97.73781550898431, 87.8533870495856, 98.32372047089612, 12.930188774307831, 109.09362007126944, 107.33624879135195, 8.456367566280932, 111.68084887218397, 98.30041813051612, 99.26854017151022, 109.59408741350192, 102.55834024156644, 106.10638231082275, 7.834409673839547, 92.26757985221909, 108.39758921967966, 8.806601682514195, 103.59317815436796, 102.1030105936095, 10.875824481105056, 90.01502825045652, 100.93305778261659, 8.277018896509071, 88.87538049229437, 104.91566341400112, 82.00656029593205, 104.82839682877406, 103.8796506627121, 102.34276620420029, 17.254754106501505, 81.72374753645755, 106.25781118819016, 8.33372328798835, 105.6842419419476, 106.30968958045379, 9.483367168300527, 117.77447895973248, 95.036502270073, 10.611914006005607, 85.68038215282478, 108.22688662300816, 104.591291191574, 8.721096773312311, 100.37757947861867, 110.30986465967291]
Elapsed: 1.3138918848930201~1.7591862735430033
Time per graph: 0.027002704222154405~0.03619028618959152
Speed: 83.92414726120619~39.18683483201509
Total Time: 0.4363
best val loss: 0.29640549421310425 test_score: 0.8958

Testing...
Test loss: 0.3181 score: 0.9375 time: 0.47s
test Score 0.9375
Epoch Time List: [1.8821028281236067, 1.9516737670637667, 11.735487641999498, 6.43655739503447, 1.6652437469456345, 11.210415615933016, 3.8424208777723834, 1.7168957530520856, 10.333535420941189, 1.7506778319366276, 1.5207137078978121, 11.708370335982181, 9.174257454113103, 1.9241145099513233, 11.203940422972664, 1.6211439390899613, 6.534382140031084, 10.118911528028548, 1.6462003121851012, 1.7533943890593946, 14.935949907056056, 1.739463694859296, 1.6948367969598621, 3.0531660228734836, 7.3815570548176765, 1.8508394239470363, 1.5075421740766615, 9.25309223588556, 11.379827141063288, 1.6601341040804982, 3.4944777791388333, 9.456976382178254, 1.6471137941116467, 10.798509048996493, 5.177732676966116, 1.589254059130326, 1.5073690039571375, 15.018137508886866, 1.5998423100681975, 1.6397771949414164, 12.37247650208883, 1.746441519120708, 1.6578875362174585, 16.46759994805325, 18.926413196022622, 15.760610318975523, 4.664179718005471, 1.7705336048966274, 1.452725112088956, 4.9968408329878, 9.428953106049448, 10.01627794187516, 1.5175342400325462, 1.5798890009755269, 6.796156543889083, 2.762615147046745, 1.5262764249928296, 6.110614272998646, 4.7342858909396455, 1.5331225789850578, 5.410153753007762, 1.6405605589970946, 1.513085444807075, 17.510529264225625, 1.5793708860874176, 1.8661827839678153, 1.6447292520897463, 11.165624292101711, 1.551703173900023, 1.73282128397841, 1.5440430550370365, 4.223281029961072, 10.565859466092661, 1.7277433869894594, 8.62612360890489, 1.6012240811251104, 2.036836166982539, 12.018676346051507, 1.8366510841296986, 1.5917150379391387, 1.5611570699838921, 4.355313600040972, 1.450910335057415, 1.5952838330995291, 7.089486956014298, 4.913300023064949, 1.4732495190110058, 7.263778852066025, 3.453229266917333, 1.6706905589671806, 1.535099743050523, 8.522644947865047, 10.058009714935906, 1.6051608370617032, 3.0715189611073583, 7.095996225019917, 1.4926727749407291, 1.5791903170756996, 1.5567859879229218, 5.9143400529865175, 1.572929939138703, 8.398130613029934, 6.786253616097383, 17.63669297308661, 1.7069556950591505, 1.7454601790523157, 12.319949357071891, 1.6823391711805016, 8.038757875096053, 1.6357813331997022, 1.4680638851132244, 1.6181429858552292, 8.052502617938444, 1.6263452160637826, 1.4940440530190244, 11.396942109102383, 1.8348922040313482, 1.7569071321049705, 1.5217463950393721, 1.5462593770353124, 6.822858116938733, 2.942646170966327, 1.6820273119956255, 1.8048946489579976, 8.140524475020356, 1.7611692609498277, 13.030235851998441, 1.6965438141487539, 1.5933015450136736, 8.743090249947272, 5.625326100969687, 1.7265696609392762, 14.160158741055056, 7.167979795834981, 3.4147040779935196, 1.514924920978956, 5.429087582975626, 13.031868704012595, 1.7200743779540062, 6.308120957808569, 5.743355495040305, 1.4969062469899654, 1.7637199619784951, 11.726192021858878, 1.5657703230390325, 1.5107932729879394, 16.14650489401538, 1.50414161500521, 1.6039350000210106, 1.5609854379436001, 17.661052244948223, 1.69641954486724, 1.6415089230285957, 5.540547600015998, 10.23940874391701, 1.6422676290385425, 1.6381344298133627, 16.942448023008183, 1.9811028980184346, 1.6319283199263737, 9.478700976935215, 1.501161674852483, 1.6030475259758532, 1.5050187930464745, 10.523570313001983, 1.5061915550613776, 1.8291968000121415, 8.969236175063998, 1.5294731289613992, 1.4430270050652325, 5.211612824932672, 1.5303600491024554, 1.5878613268723711, 1.5641993748722598, 8.530170511105098, 1.5785874058492482, 1.4788399561075494, 10.147864007973112, 1.5460275940131396, 1.560338247101754, 15.551263881032355, 1.5627972329966724, 4.707282126066275, 12.456501519074664, 5.597761350218207, 4.882036635885015, 1.622587211895734, 8.280558021971956, 9.682040325016715, 1.539296556962654, 10.607658545020968, 11.221443670918234, 1.6952572759473696, 13.302738061873242, 1.8118196229916066, 15.209536239155568, 2.357973427977413, 1.521443040925078, 1.6148882071720436, 1.6364872041158378, 1.5185667170444503, 1.521169506944716, 1.8074868739349768, 1.9000272790435702, 1.7229035599157214, 1.4794999989680946, 1.571300788084045, 1.4975363690173253, 11.051446177996695, 1.6760528309969231, 6.802755347918719, 1.72260381502565, 1.56856632407289, 6.57226499391254, 1.6577050739433616, 1.4057649939786643, 11.599608485936187, 1.5458151429193094, 1.9211562010459602, 8.486320653930306, 1.5464325769571587, 1.5290723170619458, 13.417394593008794, 1.6635648310184479, 1.6449771040352061, 9.811565960990265, 3.7543334290385246, 1.5252433880232275, 11.526947976090014, 1.7432071689981967, 1.725788108073175, 9.646786371129565, 1.7481618711026385, 3.392446151934564, 13.84283765417058, 1.6648428200278431, 11.54231123207137, 4.005398962879553, 1.71579185500741, 1.7228252911008894, 11.534995911060832, 1.7539854240603745, 1.7250866279937327, 1.4865443060407415, 10.065253814100288, 3.560293695074506, 1.5546898278407753, 1.488577756099403, 7.416320397867821, 10.424265887122601, 1.6285972651094198, 8.816055431962013, 2.413836706895381, 1.4712673848262057, 1.7211210011737421, 9.585587348090485, 1.706509281997569, 8.929047539131716, 4.9964736349647865, 1.4855025039287284, 8.86006445006933, 8.403366046957672, 1.615070270956494, 1.5411372979870066, 7.538979794946499, 1.6797994680237025, 1.6411718948511407, 13.28241559700109, 14.222562910988927, 1.4540522791212425, 6.595835941028781, 1.6022332338616252, 1.661911029019393, 5.582999368081801, 7.606855631805956, 1.4966113498667255, 7.293329416075721, 11.039080857066438, 1.6494862850522622, 1.7216711979126558, 9.445415313937701, 1.525315713020973, 1.6457395710749552, 9.11632470798213, 10.45473870204296, 1.5553653300739825, 9.477622490143403, 5.9558727160329, 1.7401075820671394, 9.48028653289657, 11.869301034021191, 1.4932250400306657, 5.686463386053219, 5.613074164837599, 1.6204178109765053, 1.4929943978786469, 10.367557295016013, 1.5956379030831158, 1.5541902858531103]
Total Epoch List: [102, 106, 91]
Total Time List: [2.3028912730515003, 0.4729964069556445, 0.4363008619984612]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d1360>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 4.67s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 6.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.44s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.55s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 8.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.48s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.50s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 3.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 3.88s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 6.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.52s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.41s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 5.14s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 6.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.42s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.38s
Epoch 12/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.71s
Val loss: 0.6917 score: 0.5306 time: 6.32s
Test loss: 0.6922 score: 0.5510 time: 5.18s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.83s
Val loss: 0.6913 score: 0.5918 time: 0.49s
Test loss: 0.6921 score: 0.5714 time: 0.49s
Epoch 14/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.55s
Val loss: 0.6909 score: 0.6122 time: 0.47s
Test loss: 0.6918 score: 0.5714 time: 0.37s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.62s
Val loss: 0.6905 score: 0.6531 time: 1.26s
Test loss: 0.6916 score: 0.6122 time: 5.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 13.28s
Val loss: 0.6899 score: 0.6735 time: 0.65s
Test loss: 0.6913 score: 0.6122 time: 0.37s
Epoch 17/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.71s
Val loss: 0.6893 score: 0.6735 time: 0.58s
Test loss: 0.6909 score: 0.6122 time: 0.35s
Epoch 18/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.63s
Val loss: 0.6887 score: 0.6531 time: 0.55s
Test loss: 0.6905 score: 0.6122 time: 0.43s
Epoch 19/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 5.85s
Val loss: 0.6879 score: 0.6531 time: 0.61s
Test loss: 0.6901 score: 0.6122 time: 0.37s
Epoch 20/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.58s
Val loss: 0.6871 score: 0.6531 time: 0.47s
Test loss: 0.6896 score: 0.6122 time: 0.37s
Epoch 21/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.71s
Val loss: 0.6862 score: 0.6122 time: 0.49s
Test loss: 0.6890 score: 0.5918 time: 0.62s
Epoch 22/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 7.78s
Val loss: 0.6851 score: 0.6122 time: 0.49s
Test loss: 0.6884 score: 0.5918 time: 0.39s
Epoch 23/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.57s
Val loss: 0.6839 score: 0.6122 time: 0.59s
Test loss: 0.6876 score: 0.5918 time: 0.37s
Epoch 24/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.64s
Val loss: 0.6826 score: 0.6122 time: 0.50s
Test loss: 0.6868 score: 0.5918 time: 0.75s
Epoch 25/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 11.43s
Val loss: 0.6812 score: 0.6122 time: 0.64s
Test loss: 0.6859 score: 0.5918 time: 0.42s
Epoch 26/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.60s
Val loss: 0.6796 score: 0.5918 time: 0.48s
Test loss: 0.6850 score: 0.5714 time: 0.39s
Epoch 27/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.63s
Val loss: 0.6779 score: 0.5918 time: 0.61s
Test loss: 0.6839 score: 0.5510 time: 0.39s
Epoch 28/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.60s
Val loss: 0.6760 score: 0.5918 time: 0.54s
Test loss: 0.6827 score: 0.5510 time: 5.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 6.10s
Val loss: 0.6739 score: 0.5918 time: 0.50s
Test loss: 0.6814 score: 0.5510 time: 0.52s
Epoch 30/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.60s
Val loss: 0.6716 score: 0.5918 time: 0.53s
Test loss: 0.6800 score: 0.5714 time: 0.38s
Epoch 31/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.61s
Val loss: 0.6691 score: 0.5918 time: 3.70s
Test loss: 0.6784 score: 0.5714 time: 3.52s
Epoch 32/1000, LR 0.000270
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 2.45s
Val loss: 0.6663 score: 0.6122 time: 0.59s
Test loss: 0.6767 score: 0.5714 time: 0.38s
Epoch 33/1000, LR 0.000270
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.61s
Val loss: 0.6633 score: 0.6122 time: 0.59s
Test loss: 0.6748 score: 0.5714 time: 0.40s
Epoch 34/1000, LR 0.000270
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 12.34s
Val loss: 0.6600 score: 0.6122 time: 0.46s
Test loss: 0.6727 score: 0.5918 time: 0.37s
Epoch 35/1000, LR 0.000270
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.75s
Val loss: 0.6565 score: 0.6122 time: 0.59s
Test loss: 0.6705 score: 0.5918 time: 0.41s
Epoch 36/1000, LR 0.000270
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.75s
Val loss: 0.6526 score: 0.6122 time: 6.29s
Test loss: 0.6680 score: 0.6122 time: 4.59s
Epoch 37/1000, LR 0.000270
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.94s
Val loss: 0.6484 score: 0.6122 time: 0.50s
Test loss: 0.6654 score: 0.6122 time: 0.50s
Epoch 38/1000, LR 0.000270
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.55s
Val loss: 0.6439 score: 0.6122 time: 0.47s
Test loss: 0.6626 score: 0.6122 time: 0.37s
Epoch 39/1000, LR 0.000269
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 3.97s
Val loss: 0.6390 score: 0.6531 time: 3.92s
Test loss: 0.6596 score: 0.6327 time: 2.76s
Epoch 40/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.63s
Val loss: 0.6338 score: 0.6531 time: 0.46s
Test loss: 0.6563 score: 0.6327 time: 0.41s
Epoch 41/1000, LR 0.000269
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 0.59s
Val loss: 0.6282 score: 0.6531 time: 0.56s
Test loss: 0.6527 score: 0.6122 time: 0.38s
Epoch 42/1000, LR 0.000269
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.64s
Val loss: 0.6222 score: 0.6939 time: 4.04s
Test loss: 0.6488 score: 0.6122 time: 3.36s
Epoch 43/1000, LR 0.000269
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 8.47s
Val loss: 0.6158 score: 0.7143 time: 0.71s
Test loss: 0.6447 score: 0.6327 time: 0.42s
Epoch 44/1000, LR 0.000269
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.70s
Val loss: 0.6089 score: 0.7347 time: 0.50s
Test loss: 0.6402 score: 0.6122 time: 0.37s
Epoch 45/1000, LR 0.000269
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.64s
Val loss: 0.6015 score: 0.7551 time: 0.51s
Test loss: 0.6354 score: 0.6122 time: 0.66s
Epoch 46/1000, LR 0.000269
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 16.27s
Val loss: 0.5937 score: 0.7755 time: 1.72s
Test loss: 0.6303 score: 0.6531 time: 0.57s
Epoch 47/1000, LR 0.000269
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.74s
Val loss: 0.5853 score: 0.7959 time: 0.63s
Test loss: 0.6247 score: 0.6939 time: 0.41s
Epoch 48/1000, LR 0.000269
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.68s
Val loss: 0.5764 score: 0.8163 time: 0.53s
Test loss: 0.6188 score: 0.6939 time: 3.63s
Epoch 49/1000, LR 0.000269
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.75s
Val loss: 0.5670 score: 0.8367 time: 0.78s
Test loss: 0.6126 score: 0.6939 time: 0.41s
Epoch 50/1000, LR 0.000269
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.64s
Val loss: 0.5571 score: 0.8776 time: 0.50s
Test loss: 0.6060 score: 0.7143 time: 0.40s
Epoch 51/1000, LR 0.000269
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 13.01s
Val loss: 0.5467 score: 0.8980 time: 0.63s
Test loss: 0.5991 score: 0.7347 time: 0.39s
Epoch 52/1000, LR 0.000269
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.60s
Val loss: 0.5358 score: 0.9184 time: 0.53s
Test loss: 0.5918 score: 0.7347 time: 0.41s
Epoch 53/1000, LR 0.000269
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.65s
Val loss: 0.5243 score: 0.9184 time: 0.53s
Test loss: 0.5841 score: 0.7347 time: 5.81s
Epoch 54/1000, LR 0.000269
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.68s
Val loss: 0.5125 score: 0.9388 time: 0.51s
Test loss: 0.5761 score: 0.7347 time: 0.38s
Epoch 55/1000, LR 0.000269
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.96s
Val loss: 0.5006 score: 0.9592 time: 0.60s
Test loss: 0.5678 score: 0.7347 time: 0.92s
Epoch 56/1000, LR 0.000269
Train loss: 0.5190;  Loss pred: 0.5190; Loss self: 0.0000; time: 15.95s
Val loss: 0.4886 score: 0.9592 time: 0.79s
Test loss: 0.5593 score: 0.7551 time: 0.70s
Epoch 57/1000, LR 0.000269
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.67s
Val loss: 0.4766 score: 0.9592 time: 8.59s
Test loss: 0.5506 score: 0.7551 time: 4.80s
Epoch 58/1000, LR 0.000269
Train loss: 0.4957;  Loss pred: 0.4957; Loss self: 0.0000; time: 1.89s
Val loss: 0.4646 score: 0.9592 time: 1.02s
Test loss: 0.5416 score: 0.7755 time: 3.17s
Epoch 59/1000, LR 0.000268
Train loss: 0.4781;  Loss pred: 0.4781; Loss self: 0.0000; time: 16.15s
Val loss: 0.4524 score: 0.9592 time: 2.22s
Test loss: 0.5325 score: 0.7755 time: 1.15s
Epoch 60/1000, LR 0.000268
Train loss: 0.4676;  Loss pred: 0.4676; Loss self: 0.0000; time: 9.40s
Val loss: 0.4402 score: 0.9796 time: 0.97s
Test loss: 0.5233 score: 0.8163 time: 0.95s
Epoch 61/1000, LR 0.000268
Train loss: 0.4517;  Loss pred: 0.4517; Loss self: 0.0000; time: 1.18s
Val loss: 0.4278 score: 0.9796 time: 9.80s
Test loss: 0.5141 score: 0.8163 time: 8.61s
Epoch 62/1000, LR 0.000268
Train loss: 0.4396;  Loss pred: 0.4396; Loss self: 0.0000; time: 2.67s
Val loss: 0.4155 score: 0.9592 time: 0.54s
Test loss: 0.5049 score: 0.8163 time: 8.92s
Epoch 63/1000, LR 0.000268
Train loss: 0.4330;  Loss pred: 0.4330; Loss self: 0.0000; time: 15.20s
Val loss: 0.4035 score: 0.9592 time: 5.95s
Test loss: 0.4959 score: 0.8163 time: 10.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.4200;  Loss pred: 0.4200; Loss self: 0.0000; time: 4.85s
Val loss: 0.3918 score: 0.9592 time: 0.74s
Test loss: 0.4873 score: 0.8163 time: 0.93s
Epoch 65/1000, LR 0.000268
Train loss: 0.4136;  Loss pred: 0.4136; Loss self: 0.0000; time: 11.82s
Val loss: 0.3808 score: 0.9592 time: 2.32s
Test loss: 0.4791 score: 0.8163 time: 1.51s
Epoch 66/1000, LR 0.000268
Train loss: 0.3955;  Loss pred: 0.3955; Loss self: 0.0000; time: 2.77s
Val loss: 0.3704 score: 0.9592 time: 6.51s
Test loss: 0.4709 score: 0.8163 time: 1.61s
Epoch 67/1000, LR 0.000268
Train loss: 0.3820;  Loss pred: 0.3820; Loss self: 0.0000; time: 1.91s
Val loss: 0.3603 score: 0.9592 time: 8.15s
Test loss: 0.4630 score: 0.8163 time: 0.93s
Epoch 68/1000, LR 0.000268
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 1.35s
Val loss: 0.3507 score: 0.9592 time: 1.47s
Test loss: 0.4553 score: 0.8163 time: 1.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.3548;  Loss pred: 0.3548; Loss self: 0.0000; time: 23.06s
Val loss: 0.3415 score: 0.9592 time: 0.61s
Test loss: 0.4479 score: 0.8163 time: 8.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.3468;  Loss pred: 0.3468; Loss self: 0.0000; time: 14.78s
Val loss: 0.3326 score: 0.9592 time: 0.97s
Test loss: 0.4404 score: 0.8367 time: 0.64s
Epoch 71/1000, LR 0.000268
Train loss: 0.3373;  Loss pred: 0.3373; Loss self: 0.0000; time: 10.58s
Val loss: 0.3241 score: 0.9796 time: 7.79s
Test loss: 0.4327 score: 0.8571 time: 1.02s
Epoch 72/1000, LR 0.000267
Train loss: 0.3272;  Loss pred: 0.3272; Loss self: 0.0000; time: 2.98s
Val loss: 0.3160 score: 0.9796 time: 0.73s
Test loss: 0.4248 score: 0.8571 time: 5.96s
Epoch 73/1000, LR 0.000267
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 4.49s
Val loss: 0.3081 score: 0.9796 time: 1.65s
Test loss: 0.4171 score: 0.8571 time: 1.20s
Epoch 74/1000, LR 0.000267
Train loss: 0.3028;  Loss pred: 0.3028; Loss self: 0.0000; time: 21.27s
Val loss: 0.3005 score: 0.9796 time: 1.38s
Test loss: 0.4098 score: 0.8571 time: 0.65s
Epoch 75/1000, LR 0.000267
Train loss: 0.2974;  Loss pred: 0.2974; Loss self: 0.0000; time: 1.48s
Val loss: 0.2931 score: 0.9796 time: 9.62s
Test loss: 0.4029 score: 0.8571 time: 1.74s
Epoch 76/1000, LR 0.000267
Train loss: 0.2836;  Loss pred: 0.2836; Loss self: 0.0000; time: 3.29s
Val loss: 0.2858 score: 0.9796 time: 11.26s
Test loss: 0.3966 score: 0.8776 time: 6.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.2758;  Loss pred: 0.2758; Loss self: 0.0000; time: 2.43s
Val loss: 0.2786 score: 0.9796 time: 7.04s
Test loss: 0.3909 score: 0.8776 time: 6.29s
Epoch 78/1000, LR 0.000267
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 2.10s
Val loss: 0.2717 score: 0.9796 time: 9.01s
Test loss: 0.3856 score: 0.8776 time: 8.67s
Epoch 79/1000, LR 0.000267
Train loss: 0.2562;  Loss pred: 0.2562; Loss self: 0.0000; time: 2.36s
Val loss: 0.2649 score: 0.9796 time: 0.84s
Test loss: 0.3805 score: 0.8776 time: 1.39s
Epoch 80/1000, LR 0.000267
Train loss: 0.2515;  Loss pred: 0.2515; Loss self: 0.0000; time: 20.22s
Val loss: 0.2582 score: 0.9796 time: 1.24s
Test loss: 0.3758 score: 0.8776 time: 1.88s
Epoch 81/1000, LR 0.000267
Train loss: 0.2412;  Loss pred: 0.2412; Loss self: 0.0000; time: 0.66s
Val loss: 0.2517 score: 0.9796 time: 4.20s
Test loss: 0.3714 score: 0.8776 time: 9.83s
Epoch 82/1000, LR 0.000267
Train loss: 0.2312;  Loss pred: 0.2312; Loss self: 0.0000; time: 21.88s
Val loss: 0.2455 score: 0.9796 time: 5.76s
Test loss: 0.3673 score: 0.8776 time: 0.51s
Epoch 83/1000, LR 0.000266
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 2.13s
Val loss: 0.2393 score: 0.9796 time: 1.89s
Test loss: 0.3634 score: 0.8776 time: 1.02s
Epoch 84/1000, LR 0.000266
Train loss: 0.2215;  Loss pred: 0.2215; Loss self: 0.0000; time: 16.19s
Val loss: 0.2333 score: 0.9796 time: 4.68s
Test loss: 0.3589 score: 0.8776 time: 1.18s
Epoch 85/1000, LR 0.000266
Train loss: 0.2085;  Loss pred: 0.2085; Loss self: 0.0000; time: 2.42s
Val loss: 0.2273 score: 0.9592 time: 0.91s
Test loss: 0.3546 score: 0.8776 time: 11.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 7.81s
Val loss: 0.2216 score: 0.9592 time: 1.60s
Test loss: 0.3503 score: 0.8776 time: 5.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 8.17s
Val loss: 0.2159 score: 0.9592 time: 0.69s
Test loss: 0.3460 score: 0.8776 time: 10.20s
Epoch 88/1000, LR 0.000266
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 11.96s
Val loss: 0.2105 score: 0.9592 time: 0.61s
Test loss: 0.3422 score: 0.8776 time: 1.98s
Epoch 89/1000, LR 0.000266
Train loss: 0.1839;  Loss pred: 0.1839; Loss self: 0.0000; time: 0.62s
Val loss: 0.2055 score: 0.9592 time: 4.02s
Test loss: 0.3376 score: 0.8776 time: 5.86s
Epoch 90/1000, LR 0.000266
Train loss: 0.1703;  Loss pred: 0.1703; Loss self: 0.0000; time: 6.96s
Val loss: 0.2007 score: 0.9592 time: 0.50s
Test loss: 0.3330 score: 0.8776 time: 0.39s
Epoch 91/1000, LR 0.000266
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.66s
Val loss: 0.1963 score: 0.9592 time: 0.64s
Test loss: 0.3293 score: 0.8980 time: 0.41s
Epoch 92/1000, LR 0.000266
Train loss: 0.1631;  Loss pred: 0.1631; Loss self: 0.0000; time: 0.73s
Val loss: 0.1922 score: 0.9592 time: 0.46s
Test loss: 0.3257 score: 0.8980 time: 0.36s
Epoch 93/1000, LR 0.000265
Train loss: 0.1479;  Loss pred: 0.1479; Loss self: 0.0000; time: 23.13s
Val loss: 0.1885 score: 0.9592 time: 0.74s
Test loss: 0.3229 score: 0.8980 time: 0.84s
Epoch 94/1000, LR 0.000265
Train loss: 0.1378;  Loss pred: 0.1378; Loss self: 0.0000; time: 10.47s
Val loss: 0.1852 score: 0.9592 time: 8.01s
Test loss: 0.3208 score: 0.8980 time: 0.61s
Epoch 95/1000, LR 0.000265
Train loss: 0.1290;  Loss pred: 0.1290; Loss self: 0.0000; time: 1.73s
Val loss: 0.1821 score: 0.9592 time: 0.95s
Test loss: 0.3191 score: 0.8980 time: 0.49s
Epoch 96/1000, LR 0.000265
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 18.11s
Val loss: 0.1794 score: 0.9592 time: 1.86s
Test loss: 0.3177 score: 0.8980 time: 0.81s
Epoch 97/1000, LR 0.000265
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 14.32s
Val loss: 0.1770 score: 0.9592 time: 9.23s
Test loss: 0.3170 score: 0.8980 time: 0.39s
Epoch 98/1000, LR 0.000265
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.68s
Val loss: 0.1750 score: 0.9592 time: 0.52s
Test loss: 0.3167 score: 0.8980 time: 0.45s
Epoch 99/1000, LR 0.000265
Train loss: 0.1105;  Loss pred: 0.1105; Loss self: 0.0000; time: 1.82s
Val loss: 0.1734 score: 0.9592 time: 10.39s
Test loss: 0.3167 score: 0.8980 time: 7.59s
Epoch 100/1000, LR 0.000265
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 2.18s
Val loss: 0.1721 score: 0.9592 time: 0.95s
Test loss: 0.3171 score: 0.8980 time: 0.40s
Epoch 101/1000, LR 0.000265
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 9.85s
Val loss: 0.1711 score: 0.9592 time: 8.48s
Test loss: 0.3178 score: 0.8980 time: 1.01s
Epoch 102/1000, LR 0.000264
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 2.38s
Val loss: 0.1704 score: 0.9592 time: 0.83s
Test loss: 0.3189 score: 0.8980 time: 8.30s
Epoch 103/1000, LR 0.000264
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 3.53s
Val loss: 0.1700 score: 0.9592 time: 1.76s
Test loss: 0.3204 score: 0.8980 time: 0.77s
Epoch 104/1000, LR 0.000264
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 1.19s
Val loss: 0.1699 score: 0.9592 time: 6.49s
Test loss: 0.3220 score: 0.8980 time: 9.46s
Epoch 105/1000, LR 0.000264
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 1.36s
Val loss: 0.1700 score: 0.9592 time: 7.59s
Test loss: 0.3239 score: 0.8980 time: 8.08s
     INFO: Early stopping counter 1 of 2
Epoch 106/1000, LR 0.000264
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 6.95s
Val loss: 0.1704 score: 0.9592 time: 1.01s
Test loss: 0.3263 score: 0.8980 time: 1.12s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 103,   Train_Loss: 0.0833,   Val_Loss: 0.1699,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1699,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3220


[4.6794644250767305, 0.4419186320155859, 0.5552356279222295, 0.48763100802898407, 0.5007772999815643, 3.8876276779919863, 0.523340743035078, 0.4115420179441571, 5.1459369889926165, 0.4227330219000578, 0.386552577954717, 5.189637662959285, 0.4979627870488912, 0.37356310605537146, 5.085736300097778, 0.3790678730001673, 0.35364183492492884, 0.43382151203695685, 0.3731481520226225, 0.3710962060140446, 0.6291189389303327, 0.3973802290856838, 0.3739706260384992, 0.7548364090034738, 0.4289310739841312, 0.3917252329410985, 0.3954915290232748, 5.092410024954006, 0.5220963769825175, 0.38083034206647426, 3.5225374619476497, 0.3858298740815371, 0.4042014180449769, 0.37407009105663747, 0.41510502400342375, 4.6050881600240245, 0.5008813680615276, 0.378263144986704, 2.7605680100386962, 0.4102721409872174, 0.3893761020153761, 3.3603761380072683, 0.42410113802179694, 0.3776211730437353, 0.6695681100245565, 0.5734185690525919, 0.41125378594733775, 3.6359464949928224, 0.4094273520167917, 0.40765903901774436, 0.39660760504193604, 0.4158900739857927, 5.809511939994991, 0.388733378960751, 0.9230677750892937, 0.7029234999790788, 4.808921469026245, 3.1775786749785766, 1.1531317549524829, 0.9532233549980447, 8.616552326944657, 8.926573418895714, 10.095034238067456, 0.9377645789645612, 1.5178989099804312, 1.6160550999920815, 0.938230742001906, 1.2574710409389809, 8.20207721798215, 0.6437072269618511, 1.0266468520276248, 5.96702057600487, 1.208889525034465, 0.6590784379513934, 1.7482946139061823, 6.098507999093272, 6.297415487933904, 8.67921952996403, 1.3998770910548046, 1.8874893529573455, 9.832505747908726, 0.5196057299617678, 1.0280329069355503, 1.18262016098015, 11.257270321948454, 5.231648370041512, 10.212363669998012, 1.9824618079001084, 5.869365707039833, 0.3910172089235857, 0.4113040450029075, 0.36075360293034464, 0.8430892250034958, 0.6153887970140204, 0.49167126207612455, 0.8183611090062186, 0.39732412493322045, 0.45484198292251676, 7.592004404054023, 0.40101104497443885, 1.0173345130169764, 8.306714701000601, 0.7725735940039158, 9.467755084042437, 8.085591883980669, 1.124471302027814]
[0.09549927398115776, 0.009018747592154814, 0.011331339345351622, 0.009951653225081307, 0.010219944897582945, 0.07933934036718339, 0.01068042332724649, 0.0083988166927379, 0.10501912222433911, 0.008627204528572609, 0.007888828121524836, 0.1059109727134548, 0.010162505858140637, 0.007623736858272887, 0.10379053673668935, 0.007736079040819741, 0.007217180304590384, 0.008853500245652181, 0.0076152684086249495, 0.007573391959470298, 0.012839162018986381, 0.008109800593585384, 0.007632053592622432, 0.015404824673540282, 0.008753695387431249, 0.00799439250900201, 0.008071255694352547, 0.10392673520314298, 0.010655028101684031, 0.0077720477972749846, 0.07188851963158469, 0.007874079062888513, 0.008249008531530141, 0.007634083490951785, 0.008471531102110689, 0.09398139102089846, 0.010222068735949543, 0.007719656020136816, 0.05633812265385094, 0.008372900836473825, 0.007946451061538287, 0.06857910485729118, 0.008655125265750957, 0.007706554551912966, 0.013664655306623603, 0.011702419776583508, 0.008392934407088526, 0.07420298969373107, 0.008355660245240646, 0.008319572224851926, 0.008094032755957879, 0.0084875525303223, 0.11856146816316308, 0.007933334264505123, 0.018838117858965178, 0.014345377550593444, 0.09814125446992338, 0.0648485443873179, 0.023533301121479243, 0.01945353785710295, 0.17584800667233993, 0.18217496773256558, 0.20602110689933584, 0.01913805263192982, 0.03097752877511084, 0.03298071632636901, 0.019147566163304205, 0.02566267430487716, 0.16738933097922756, 0.01313688218289492, 0.020951976571992343, 0.12177593012254838, 0.024671214796621734, 0.013450580366354967, 0.035679481916452704, 0.12445934692027086, 0.12851868342722253, 0.17712692918293937, 0.02856892022560826, 0.03852019087668052, 0.20066338261038216, 0.010604198570648322, 0.020980263406847964, 0.024135105326125512, 0.22974021065200925, 0.10676833408247983, 0.20841558510200026, 0.040458404242859354, 0.11978297361305781, 0.007979943039256851, 0.008393960102100154, 0.007362318427149891, 0.01720590255109175, 0.012558955041102457, 0.010034107389308664, 0.01670124712257589, 0.00810865561088205, 0.009282489447398302, 0.15493886538885762, 0.008183898877029364, 0.02076192883708115, 0.16952478981633878, 0.01576680804089624, 0.19321949151107015, 0.16501207926491163, 0.02294839391893498]
[10.471283794233894, 110.8801404831282, 88.25082097732985, 100.4858165153589, 97.84788568052885, 12.604087648977018, 93.6292475831863, 119.06439163800985, 9.522075397505477, 115.91240206350508, 126.76153981241886, 9.44189232125673, 98.40092728694015, 131.16927021357662, 9.634789754840007, 129.26444969388996, 138.55826760541984, 112.94967778321177, 131.3151351129546, 132.04123137315378, 77.88670308242963, 123.30759412148443, 131.0263335895146, 64.91472776821831, 114.23746837659239, 125.08767850389626, 123.89645897350266, 9.622163132954432, 93.85240380942305, 128.6662184901407, 13.91042693777552, 126.9989788028826, 121.22668999282828, 130.99149376414854, 118.04241617561307, 10.640404330444863, 97.82755583350209, 129.53945064281206, 17.749970231421013, 119.43292050514017, 125.8423404682012, 14.58170097263499, 115.5384779879596, 129.7596731799912, 73.18150202554139, 85.45241232937103, 119.1478393010456, 13.476545946833777, 119.6793515592734, 120.19848773147687, 123.5478073972356, 117.81959480397192, 8.43444346205135, 126.05040537295191, 53.08385941136331, 69.70886590284489, 10.189394922666926, 15.42054658971752, 42.492976010377184, 51.40453152252078, 5.686729232383703, 5.489228363514841, 4.853871601071491, 52.25192025711151, 32.28146464683324, 30.3207483459197, 52.225958718266405, 38.96710015954772, 5.974096402381204, 76.12156264155777, 47.72819388013038, 8.211803424483449, 40.533066905847356, 74.34623434549943, 28.02731279399197, 8.034752107775432, 7.780969842927774, 5.645668925740732, 35.00307299341444, 25.960411338599656, 4.983470262442694, 94.30227030715267, 47.66384389976723, 41.43342183460582, 4.35274259199977, 9.366072895990753, 4.798105667148606, 24.71674349777385, 8.348431916796125, 125.3141776928181, 119.13328010098617, 135.82677928087406, 58.11958989251325, 79.62445894003434, 99.66008546664544, 59.87576811843298, 123.32500577012682, 107.72972117735941, 6.454158532078128, 122.19114813439452, 48.16508176321187, 5.898842293704587, 63.42437844148171, 5.175461296267343, 6.060162410259631, 43.57603427640697]
Elapsed: 2.3528610088177166~2.9380321165893624
Time per graph: 0.04801757160852484~0.05995983911406862
Speed: 68.17959952915521~48.34989739643718
Total Time: 1.1251
best val loss: 0.16987629234790802 test_score: 0.8980

Testing...
Test loss: 0.5233 score: 0.8163 time: 1.33s
test Score 0.8163
Epoch Time List: [5.7616848619654775, 6.980404923087917, 1.7884972519241273, 9.621527162147686, 1.6623823221307248, 8.258319083135575, 7.519732084940188, 1.4882440948858857, 6.496882788022049, 7.860313066164963, 1.6558751289267093, 12.213574492023326, 1.807032041135244, 1.381071100011468, 6.958169581135735, 14.301530844997615, 1.6394435780821368, 1.6068459000671282, 6.833464242867194, 1.418536944896914, 1.827063697972335, 8.66207163699437, 1.5211611830163747, 1.8839318559039384, 12.497050225036219, 1.4642396649578586, 1.63688243587967, 6.230349141987972, 7.119672607048415, 1.506110452930443, 7.8331821580650285, 3.417555835098028, 1.5988553072093055, 13.166531654074788, 1.7529303249903023, 11.621138520073146, 1.932513887877576, 1.3915239770431072, 10.642679539974779, 1.4994341090787202, 1.5374875178094953, 8.033897261018865, 9.601390806026757, 1.564527663984336, 1.8097510179504752, 18.552979042055085, 1.779083393048495, 4.840514556970447, 1.9329237908823416, 1.5490123130148277, 14.03463211201597, 1.545958349131979, 6.986277188058011, 1.5681531860027462, 2.479002720909193, 17.43931692908518, 14.058413930004463, 6.073517221142538, 19.51189361384604, 11.318134693894535, 19.593921874882653, 12.122686435934156, 31.23695767892059, 6.528596274089068, 15.654594345018268, 10.893737345002592, 10.989062689943239, 4.07428459613584, 31.8660111509962, 16.38550120498985, 19.393515785923228, 9.671341035049409, 7.340897703077644, 23.30868654407095, 12.845846176962368, 20.641787351923995, 15.763106822152622, 19.784071878064424, 4.589323709020391, 23.346311567816883, 14.6877462239936, 28.148412873968482, 5.04822763602715, 22.051180717884563, 14.588233401998878, 14.639629389974289, 19.06212158105336, 14.540458886069246, 10.507414316991344, 7.845735052833334, 1.7093993078451604, 1.5414119550259784, 24.70401019300334, 19.095169157953933, 3.161128023872152, 20.78108755790163, 23.938999910023995, 1.6451355488970876, 19.79787302191835, 3.525733712129295, 19.337175732012838, 11.504625395871699, 6.060042835888453, 17.129491482977755, 17.031711726100184, 9.079908650950529]
Total Epoch List: [106]
Total Time List: [1.1251285680336878]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442a1d1b70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.88s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 11.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 9.82s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 2.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.79s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 9.88s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 7.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.78s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 10.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 4.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 1.14s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 2.02s
Val loss: 0.6929 score: 0.5102 time: 0.88s
Test loss: 0.6928 score: 0.5714 time: 9.55s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 11.03s
Val loss: 0.6929 score: 0.5918 time: 0.67s
Test loss: 0.6926 score: 0.7347 time: 0.62s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.71s
Val loss: 0.6928 score: 0.6122 time: 0.54s
Test loss: 0.6924 score: 0.7755 time: 6.88s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 17.09s
Val loss: 0.6927 score: 0.6327 time: 0.42s
Test loss: 0.6922 score: 0.7551 time: 0.74s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 6.68s
Val loss: 0.6926 score: 0.6327 time: 9.37s
Test loss: 0.6920 score: 0.7755 time: 3.73s
Epoch 12/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.51s
Val loss: 0.6925 score: 0.5918 time: 0.63s
Test loss: 0.6918 score: 0.7755 time: 0.92s
Epoch 13/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 28.27s
Val loss: 0.6923 score: 0.6122 time: 1.22s
Test loss: 0.6915 score: 0.7143 time: 1.29s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.87s
Val loss: 0.6921 score: 0.6122 time: 10.01s
Test loss: 0.6912 score: 0.7143 time: 9.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 22.04s
Val loss: 0.6919 score: 0.5918 time: 0.43s
Test loss: 0.6908 score: 0.7551 time: 0.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.68s
Val loss: 0.6916 score: 0.5714 time: 0.39s
Test loss: 0.6904 score: 0.7755 time: 0.64s
Epoch 17/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 13.35s
Val loss: 0.6913 score: 0.6122 time: 0.40s
Test loss: 0.6899 score: 0.7959 time: 0.49s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.73s
Val loss: 0.6910 score: 0.6327 time: 0.40s
Test loss: 0.6894 score: 0.7755 time: 0.53s
Epoch 19/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.75s
Val loss: 0.6906 score: 0.6531 time: 0.37s
Test loss: 0.6888 score: 0.6939 time: 3.73s
Epoch 20/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 7.93s
Val loss: 0.6902 score: 0.6327 time: 0.37s
Test loss: 0.6881 score: 0.6531 time: 0.48s
Epoch 21/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.72s
Val loss: 0.6897 score: 0.6327 time: 0.65s
Test loss: 0.6874 score: 0.6531 time: 0.55s
Epoch 22/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.83s
Val loss: 0.6892 score: 0.6122 time: 5.48s
Test loss: 0.6866 score: 0.6122 time: 5.31s
Epoch 23/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 4.41s
Val loss: 0.6886 score: 0.5918 time: 0.39s
Test loss: 0.6856 score: 0.6122 time: 0.51s
Epoch 24/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.62s
Val loss: 0.6879 score: 0.6122 time: 0.39s
Test loss: 0.6846 score: 0.6122 time: 0.50s
Epoch 25/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.71s
Val loss: 0.6872 score: 0.6122 time: 0.43s
Test loss: 0.6835 score: 0.6122 time: 3.97s
Epoch 26/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 8.43s
Val loss: 0.6864 score: 0.6122 time: 0.40s
Test loss: 0.6822 score: 0.6122 time: 0.50s
Epoch 27/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.71s
Val loss: 0.6854 score: 0.6122 time: 0.40s
Test loss: 0.6808 score: 0.6122 time: 0.51s
Epoch 28/1000, LR 0.000270
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.61s
Val loss: 0.6844 score: 0.6122 time: 0.41s
Test loss: 0.6792 score: 0.6122 time: 0.49s
Epoch 29/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.67s
Val loss: 0.6833 score: 0.5918 time: 0.56s
Test loss: 0.6775 score: 0.6122 time: 2.64s
Epoch 30/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 15.31s
Val loss: 0.6821 score: 0.5918 time: 0.40s
Test loss: 0.6757 score: 0.6122 time: 0.52s
Epoch 31/1000, LR 0.000270
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.69s
Val loss: 0.6808 score: 0.5918 time: 4.80s
Test loss: 0.6737 score: 0.6122 time: 3.35s
Epoch 32/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 8.61s
Val loss: 0.6793 score: 0.5918 time: 0.40s
Test loss: 0.6715 score: 0.5918 time: 0.49s
Epoch 33/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.72s
Val loss: 0.6777 score: 0.5714 time: 0.39s
Test loss: 0.6691 score: 0.5918 time: 0.63s
Epoch 34/1000, LR 0.000270
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.64s
Val loss: 0.6760 score: 0.5714 time: 0.41s
Test loss: 0.6665 score: 0.5918 time: 2.90s
Epoch 35/1000, LR 0.000270
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 8.55s
Val loss: 0.6741 score: 0.5714 time: 0.39s
Test loss: 0.6637 score: 0.5918 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.62s
Val loss: 0.6721 score: 0.5714 time: 0.37s
Test loss: 0.6607 score: 0.5918 time: 0.61s
Epoch 37/1000, LR 0.000270
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.63s
Val loss: 0.6699 score: 0.5714 time: 0.44s
Test loss: 0.6574 score: 0.6122 time: 4.55s
Epoch 38/1000, LR 0.000270
Train loss: 0.6586;  Loss pred: 0.6586; Loss self: 0.0000; time: 3.50s
Val loss: 0.6675 score: 0.5918 time: 0.39s
Test loss: 0.6539 score: 0.6122 time: 0.51s
Epoch 39/1000, LR 0.000269
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.75s
Val loss: 0.6649 score: 0.6122 time: 0.39s
Test loss: 0.6501 score: 0.6122 time: 0.48s
Epoch 40/1000, LR 0.000269
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.61s
Val loss: 0.6620 score: 0.6122 time: 0.42s
Test loss: 0.6460 score: 0.6122 time: 0.65s
Epoch 41/1000, LR 0.000269
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 2.14s
Val loss: 0.6589 score: 0.6122 time: 5.58s
Test loss: 0.6415 score: 0.6122 time: 4.68s
Epoch 42/1000, LR 0.000269
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.64s
Val loss: 0.6555 score: 0.6327 time: 0.38s
Test loss: 0.6366 score: 0.6122 time: 0.52s
Epoch 43/1000, LR 0.000269
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.72s
Val loss: 0.6518 score: 0.6327 time: 0.40s
Test loss: 0.6313 score: 0.6122 time: 0.53s
Epoch 44/1000, LR 0.000269
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 11.14s
Val loss: 0.6478 score: 0.6327 time: 0.39s
Test loss: 0.6257 score: 0.6122 time: 0.52s
Epoch 45/1000, LR 0.000269
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.71s
Val loss: 0.6436 score: 0.6327 time: 0.41s
Test loss: 0.6197 score: 0.6122 time: 0.51s
Epoch 46/1000, LR 0.000269
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.65s
Val loss: 0.6390 score: 0.6327 time: 0.41s
Test loss: 0.6134 score: 0.6122 time: 6.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.73s
Val loss: 0.6340 score: 0.6531 time: 0.39s
Test loss: 0.6067 score: 0.6122 time: 0.49s
Epoch 48/1000, LR 0.000269
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.57s
Val loss: 0.6287 score: 0.6531 time: 0.38s
Test loss: 0.5996 score: 0.6531 time: 0.55s
Epoch 49/1000, LR 0.000269
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.70s
Val loss: 0.6230 score: 0.6735 time: 0.54s
Test loss: 0.5920 score: 0.6939 time: 0.55s
Epoch 50/1000, LR 0.000269
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 11.80s
Val loss: 0.6169 score: 0.6531 time: 1.07s
Test loss: 0.5841 score: 0.7347 time: 0.47s
Epoch 51/1000, LR 0.000269
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.72s
Val loss: 0.6104 score: 0.6531 time: 0.44s
Test loss: 0.5756 score: 0.7347 time: 0.48s
Epoch 52/1000, LR 0.000269
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.60s
Val loss: 0.6035 score: 0.6531 time: 0.37s
Test loss: 0.5667 score: 0.7551 time: 0.48s
Epoch 53/1000, LR 0.000269
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.65s
Val loss: 0.5960 score: 0.7143 time: 5.96s
Test loss: 0.5572 score: 0.7755 time: 5.54s
Epoch 54/1000, LR 0.000269
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 3.62s
Val loss: 0.5881 score: 0.7347 time: 0.41s
Test loss: 0.5473 score: 0.7755 time: 0.57s
Epoch 55/1000, LR 0.000269
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.91s
Val loss: 0.5798 score: 0.7551 time: 0.52s
Test loss: 0.5370 score: 0.7755 time: 0.52s
Epoch 56/1000, LR 0.000269
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 13.42s
Val loss: 0.5711 score: 0.7755 time: 0.40s
Test loss: 0.5261 score: 0.8367 time: 0.47s
Epoch 57/1000, LR 0.000269
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.80s
Val loss: 0.5620 score: 0.7755 time: 0.37s
Test loss: 0.5148 score: 0.8776 time: 0.53s
Epoch 58/1000, LR 0.000269
Train loss: 0.5013;  Loss pred: 0.5013; Loss self: 0.0000; time: 0.59s
Val loss: 0.5524 score: 0.7755 time: 0.44s
Test loss: 0.5031 score: 0.8980 time: 0.49s
Epoch 59/1000, LR 0.000268
Train loss: 0.4856;  Loss pred: 0.4856; Loss self: 0.0000; time: 12.17s
Val loss: 0.5425 score: 0.8163 time: 0.46s
Test loss: 0.4910 score: 0.9388 time: 0.69s
Epoch 60/1000, LR 0.000268
Train loss: 0.4718;  Loss pred: 0.4718; Loss self: 0.0000; time: 0.63s
Val loss: 0.5323 score: 0.8367 time: 0.41s
Test loss: 0.4786 score: 0.9388 time: 0.60s
Epoch 61/1000, LR 0.000268
Train loss: 0.4644;  Loss pred: 0.4644; Loss self: 0.0000; time: 13.80s
Val loss: 0.5220 score: 0.8571 time: 0.60s
Test loss: 0.4660 score: 0.9388 time: 0.50s
Epoch 62/1000, LR 0.000268
Train loss: 0.4466;  Loss pred: 0.4466; Loss self: 0.0000; time: 0.61s
Val loss: 0.5118 score: 0.8571 time: 0.39s
Test loss: 0.4534 score: 0.9592 time: 0.55s
Epoch 63/1000, LR 0.000268
Train loss: 0.4265;  Loss pred: 0.4265; Loss self: 0.0000; time: 0.76s
Val loss: 0.5012 score: 0.8571 time: 0.38s
Test loss: 0.4409 score: 0.9592 time: 5.70s
Epoch 64/1000, LR 0.000268
Train loss: 0.4167;  Loss pred: 0.4167; Loss self: 0.0000; time: 8.67s
Val loss: 0.4905 score: 0.8571 time: 0.38s
Test loss: 0.4282 score: 0.9592 time: 0.59s
Epoch 65/1000, LR 0.000268
Train loss: 0.4013;  Loss pred: 0.4013; Loss self: 0.0000; time: 0.74s
Val loss: 0.4799 score: 0.8571 time: 0.42s
Test loss: 0.4158 score: 0.9592 time: 0.49s
Epoch 66/1000, LR 0.000268
Train loss: 0.3881;  Loss pred: 0.3881; Loss self: 0.0000; time: 0.64s
Val loss: 0.4694 score: 0.8571 time: 0.38s
Test loss: 0.4037 score: 0.9592 time: 0.52s
Epoch 67/1000, LR 0.000268
Train loss: 0.3698;  Loss pred: 0.3698; Loss self: 0.0000; time: 15.16s
Val loss: 0.4589 score: 0.8776 time: 0.45s
Test loss: 0.3920 score: 0.9592 time: 0.51s
Epoch 68/1000, LR 0.000268
Train loss: 0.3654;  Loss pred: 0.3654; Loss self: 0.0000; time: 0.74s
Val loss: 0.4489 score: 0.8776 time: 0.39s
Test loss: 0.3809 score: 0.9592 time: 0.49s
Epoch 69/1000, LR 0.000268
Train loss: 0.3490;  Loss pred: 0.3490; Loss self: 0.0000; time: 0.82s
Val loss: 0.4393 score: 0.8776 time: 0.41s
Test loss: 0.3705 score: 0.9388 time: 0.48s
Epoch 70/1000, LR 0.000268
Train loss: 0.3368;  Loss pred: 0.3368; Loss self: 0.0000; time: 0.61s
Val loss: 0.4300 score: 0.8571 time: 0.38s
Test loss: 0.3604 score: 0.9388 time: 0.53s
Epoch 71/1000, LR 0.000268
Train loss: 0.3220;  Loss pred: 0.3220; Loss self: 0.0000; time: 15.66s
Val loss: 0.4211 score: 0.8571 time: 0.41s
Test loss: 0.3509 score: 0.9388 time: 0.54s
Epoch 72/1000, LR 0.000267
Train loss: 0.3069;  Loss pred: 0.3069; Loss self: 0.0000; time: 0.62s
Val loss: 0.4123 score: 0.8571 time: 0.39s
Test loss: 0.3419 score: 0.9388 time: 0.76s
Epoch 73/1000, LR 0.000267
Train loss: 0.3022;  Loss pred: 0.3022; Loss self: 0.0000; time: 15.90s
Val loss: 0.4036 score: 0.8571 time: 0.49s
Test loss: 0.3331 score: 0.9388 time: 0.62s
Epoch 74/1000, LR 0.000267
Train loss: 0.2874;  Loss pred: 0.2874; Loss self: 0.0000; time: 0.62s
Val loss: 0.3950 score: 0.8571 time: 0.37s
Test loss: 0.3245 score: 0.9592 time: 0.50s
Epoch 75/1000, LR 0.000267
Train loss: 0.2767;  Loss pred: 0.2767; Loss self: 0.0000; time: 0.72s
Val loss: 0.3868 score: 0.8571 time: 0.38s
Test loss: 0.3165 score: 0.9388 time: 0.50s
Epoch 76/1000, LR 0.000267
Train loss: 0.2699;  Loss pred: 0.2699; Loss self: 0.0000; time: 0.76s
Val loss: 0.3789 score: 0.8571 time: 5.27s
Test loss: 0.3088 score: 0.9388 time: 3.96s
Epoch 77/1000, LR 0.000267
Train loss: 0.2459;  Loss pred: 0.2459; Loss self: 0.0000; time: 4.08s
Val loss: 0.3711 score: 0.8571 time: 0.49s
Test loss: 0.3011 score: 0.9388 time: 0.49s
Epoch 78/1000, LR 0.000267
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.62s
Val loss: 0.3636 score: 0.8571 time: 0.39s
Test loss: 0.2939 score: 0.9388 time: 0.49s
Epoch 79/1000, LR 0.000267
Train loss: 0.2300;  Loss pred: 0.2300; Loss self: 0.0000; time: 0.62s
Val loss: 0.3565 score: 0.8571 time: 0.47s
Test loss: 0.2871 score: 0.9388 time: 0.49s
Epoch 80/1000, LR 0.000267
Train loss: 0.2219;  Loss pred: 0.2219; Loss self: 0.0000; time: 4.72s
Val loss: 0.3498 score: 0.8776 time: 3.72s
Test loss: 0.2809 score: 0.9388 time: 1.52s
Epoch 81/1000, LR 0.000267
Train loss: 0.2080;  Loss pred: 0.2080; Loss self: 0.0000; time: 0.73s
Val loss: 0.3435 score: 0.8776 time: 0.44s
Test loss: 0.2754 score: 0.9388 time: 0.52s
Epoch 82/1000, LR 0.000267
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.61s
Val loss: 0.3378 score: 0.8776 time: 0.39s
Test loss: 0.2705 score: 0.9388 time: 0.50s
Epoch 83/1000, LR 0.000266
Train loss: 0.1876;  Loss pred: 0.1876; Loss self: 0.0000; time: 0.69s
Val loss: 0.3327 score: 0.8776 time: 0.39s
Test loss: 0.2661 score: 0.9388 time: 0.54s
Epoch 84/1000, LR 0.000266
Train loss: 0.1670;  Loss pred: 0.1670; Loss self: 0.0000; time: 9.51s
Val loss: 0.3285 score: 0.8571 time: 0.40s
Test loss: 0.2617 score: 0.9388 time: 0.77s
Epoch 85/1000, LR 0.000266
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 0.78s
Val loss: 0.3246 score: 0.8571 time: 0.64s
Test loss: 0.2579 score: 0.9388 time: 0.52s
Epoch 86/1000, LR 0.000266
Train loss: 0.1620;  Loss pred: 0.1620; Loss self: 0.0000; time: 9.90s
Val loss: 0.3215 score: 0.8571 time: 0.38s
Test loss: 0.2549 score: 0.9388 time: 0.56s
Epoch 87/1000, LR 0.000266
Train loss: 0.1468;  Loss pred: 0.1468; Loss self: 0.0000; time: 0.97s
Val loss: 0.3182 score: 0.8571 time: 0.37s
Test loss: 0.2522 score: 0.9592 time: 0.63s
Epoch 88/1000, LR 0.000266
Train loss: 0.1358;  Loss pred: 0.1358; Loss self: 0.0000; time: 13.11s
Val loss: 0.3153 score: 0.8571 time: 2.97s
Test loss: 0.2502 score: 0.9592 time: 0.64s
Epoch 89/1000, LR 0.000266
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.60s
Val loss: 0.3133 score: 0.8571 time: 0.38s
Test loss: 0.2490 score: 0.9592 time: 0.50s
Epoch 90/1000, LR 0.000266
Train loss: 0.1210;  Loss pred: 0.1210; Loss self: 0.0000; time: 0.64s
Val loss: 0.3109 score: 0.8571 time: 0.39s
Test loss: 0.2482 score: 0.9592 time: 0.51s
Epoch 91/1000, LR 0.000266
Train loss: 0.1191;  Loss pred: 0.1191; Loss self: 0.0000; time: 0.71s
Val loss: 0.3085 score: 0.8571 time: 0.44s
Test loss: 0.2478 score: 0.9592 time: 4.35s
Epoch 92/1000, LR 0.000266
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 7.06s
Val loss: 0.3072 score: 0.8571 time: 0.39s
Test loss: 0.2483 score: 0.9592 time: 0.48s
Epoch 93/1000, LR 0.000265
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.70s
Val loss: 0.3074 score: 0.8571 time: 0.39s
Test loss: 0.2503 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.1007;  Loss pred: 0.1007; Loss self: 0.0000; time: 0.60s
Val loss: 0.3071 score: 0.8571 time: 0.40s
Test loss: 0.2522 score: 0.9592 time: 0.51s
Epoch 95/1000, LR 0.000265
Train loss: 0.0805;  Loss pred: 0.0805; Loss self: 0.0000; time: 10.60s
Val loss: 0.3068 score: 0.8571 time: 1.49s
Test loss: 0.2542 score: 0.9592 time: 0.50s
Epoch 96/1000, LR 0.000265
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.96s
Val loss: 0.3077 score: 0.8571 time: 0.42s
Test loss: 0.2572 score: 0.9592 time: 0.53s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.76s
Val loss: 0.3099 score: 0.8571 time: 0.43s
Test loss: 0.2615 score: 0.9592 time: 4.42s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.0805,   Val_Loss: 0.3068,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3068,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2542


[4.6794644250767305, 0.4419186320155859, 0.5552356279222295, 0.48763100802898407, 0.5007772999815643, 3.8876276779919863, 0.523340743035078, 0.4115420179441571, 5.1459369889926165, 0.4227330219000578, 0.386552577954717, 5.189637662959285, 0.4979627870488912, 0.37356310605537146, 5.085736300097778, 0.3790678730001673, 0.35364183492492884, 0.43382151203695685, 0.3731481520226225, 0.3710962060140446, 0.6291189389303327, 0.3973802290856838, 0.3739706260384992, 0.7548364090034738, 0.4289310739841312, 0.3917252329410985, 0.3954915290232748, 5.092410024954006, 0.5220963769825175, 0.38083034206647426, 3.5225374619476497, 0.3858298740815371, 0.4042014180449769, 0.37407009105663747, 0.41510502400342375, 4.6050881600240245, 0.5008813680615276, 0.378263144986704, 2.7605680100386962, 0.4102721409872174, 0.3893761020153761, 3.3603761380072683, 0.42410113802179694, 0.3776211730437353, 0.6695681100245565, 0.5734185690525919, 0.41125378594733775, 3.6359464949928224, 0.4094273520167917, 0.40765903901774436, 0.39660760504193604, 0.4158900739857927, 5.809511939994991, 0.388733378960751, 0.9230677750892937, 0.7029234999790788, 4.808921469026245, 3.1775786749785766, 1.1531317549524829, 0.9532233549980447, 8.616552326944657, 8.926573418895714, 10.095034238067456, 0.9377645789645612, 1.5178989099804312, 1.6160550999920815, 0.938230742001906, 1.2574710409389809, 8.20207721798215, 0.6437072269618511, 1.0266468520276248, 5.96702057600487, 1.208889525034465, 0.6590784379513934, 1.7482946139061823, 6.098507999093272, 6.297415487933904, 8.67921952996403, 1.3998770910548046, 1.8874893529573455, 9.832505747908726, 0.5196057299617678, 1.0280329069355503, 1.18262016098015, 11.257270321948454, 5.231648370041512, 10.212363669998012, 1.9824618079001084, 5.869365707039833, 0.3910172089235857, 0.4113040450029075, 0.36075360293034464, 0.8430892250034958, 0.6153887970140204, 0.49167126207612455, 0.8183611090062186, 0.39732412493322045, 0.45484198292251676, 7.592004404054023, 0.40101104497443885, 1.0173345130169764, 8.306714701000601, 0.7725735940039158, 9.467755084042437, 8.085591883980669, 1.124471302027814, 0.8822462550597265, 9.827992627979256, 0.7933155380887911, 9.884763630921952, 0.7887512559536844, 1.1427381950197741, 9.562052170047536, 0.6227058439981192, 6.883212913060561, 0.7474470379529521, 3.732610232080333, 0.9210183909162879, 1.2944370199693367, 9.225194216007367, 0.5049109340179712, 0.6498552759876475, 0.49696423090063035, 0.5313211360480636, 3.735850899014622, 0.4845936510246247, 0.5504080910468474, 5.311011896003038, 0.5128829110180959, 0.5086477019358426, 3.976257330039516, 0.5050381129840389, 0.5151452280115336, 0.498641315032728, 2.6409414970548823, 0.5221493939170614, 3.353492791997269, 0.49337875994388014, 0.6315610909368843, 2.908995907055214, 0.4790050669107586, 0.6179560970049351, 4.5558807930210605, 0.516283500008285, 0.4870742540806532, 0.6548185650026426, 4.683393069077283, 0.5258965899702162, 0.5361843009013683, 0.5221709469333291, 0.513702897937037, 6.222314266953617, 0.48958902806043625, 0.5519881460350007, 0.5505668240366504, 0.47669666807632893, 0.4890811420045793, 0.48806897492613643, 5.545035870047286, 0.5773092659655958, 0.5215340270660818, 0.4735426000552252, 0.5368663059780374, 0.4983188349287957, 0.6969362531090155, 0.6044630029937252, 0.5040675250347704, 0.5564315270166844, 5.706018119934015, 0.5925841889111325, 0.49102983297780156, 0.5249565029516816, 0.516675321967341, 0.49767882307060063, 0.48871460498776287, 0.5399752459488809, 0.5450739159714431, 0.7616946219932288, 0.6249219910241663, 0.5011697149602696, 0.4998029089765623, 3.9634118700632825, 0.494945807964541, 0.49865708698052913, 0.48961025301832706, 1.520956826978363, 0.525555044063367, 0.5033264230005443, 0.5456923439633101, 0.7746420620242134, 0.5207828980637714, 0.5630390750011429, 0.6394898619037122, 0.6432615809608251, 0.5025372439995408, 0.5095494480337948, 4.353812930057757, 0.4870443169493228, 0.4932615009602159, 0.5130769870011136, 0.5115625869948417, 0.5372837660834193, 4.423403644934297]
[0.09549927398115776, 0.009018747592154814, 0.011331339345351622, 0.009951653225081307, 0.010219944897582945, 0.07933934036718339, 0.01068042332724649, 0.0083988166927379, 0.10501912222433911, 0.008627204528572609, 0.007888828121524836, 0.1059109727134548, 0.010162505858140637, 0.007623736858272887, 0.10379053673668935, 0.007736079040819741, 0.007217180304590384, 0.008853500245652181, 0.0076152684086249495, 0.007573391959470298, 0.012839162018986381, 0.008109800593585384, 0.007632053592622432, 0.015404824673540282, 0.008753695387431249, 0.00799439250900201, 0.008071255694352547, 0.10392673520314298, 0.010655028101684031, 0.0077720477972749846, 0.07188851963158469, 0.007874079062888513, 0.008249008531530141, 0.007634083490951785, 0.008471531102110689, 0.09398139102089846, 0.010222068735949543, 0.007719656020136816, 0.05633812265385094, 0.008372900836473825, 0.007946451061538287, 0.06857910485729118, 0.008655125265750957, 0.007706554551912966, 0.013664655306623603, 0.011702419776583508, 0.008392934407088526, 0.07420298969373107, 0.008355660245240646, 0.008319572224851926, 0.008094032755957879, 0.0084875525303223, 0.11856146816316308, 0.007933334264505123, 0.018838117858965178, 0.014345377550593444, 0.09814125446992338, 0.0648485443873179, 0.023533301121479243, 0.01945353785710295, 0.17584800667233993, 0.18217496773256558, 0.20602110689933584, 0.01913805263192982, 0.03097752877511084, 0.03298071632636901, 0.019147566163304205, 0.02566267430487716, 0.16738933097922756, 0.01313688218289492, 0.020951976571992343, 0.12177593012254838, 0.024671214796621734, 0.013450580366354967, 0.035679481916452704, 0.12445934692027086, 0.12851868342722253, 0.17712692918293937, 0.02856892022560826, 0.03852019087668052, 0.20066338261038216, 0.010604198570648322, 0.020980263406847964, 0.024135105326125512, 0.22974021065200925, 0.10676833408247983, 0.20841558510200026, 0.040458404242859354, 0.11978297361305781, 0.007979943039256851, 0.008393960102100154, 0.007362318427149891, 0.01720590255109175, 0.012558955041102457, 0.010034107389308664, 0.01670124712257589, 0.00810865561088205, 0.009282489447398302, 0.15493886538885762, 0.008183898877029364, 0.02076192883708115, 0.16952478981633878, 0.01576680804089624, 0.19321949151107015, 0.16501207926491163, 0.02294839391893498, 0.01800502561346381, 0.20057127812202563, 0.016190113022220225, 0.20172987001881534, 0.01609696440721805, 0.02332118765346478, 0.19514392183770482, 0.012708282530573862, 0.14047373291960327, 0.015254021182713308, 0.07617571902204762, 0.01879629369216914, 0.026417082040190543, 0.18826926971443605, 0.010304304775876962, 0.01326235257117648, 0.010142127161237354, 0.01084328849077681, 0.07624185508193106, 0.00988966634744132, 0.01123281818462954, 0.10838799787761301, 0.010466998184042774, 0.010380565345629441, 0.08114810877631666, 0.010306900264980386, 0.010513167918602727, 0.010176353368014857, 0.053896765246018004, 0.010656110079940028, 0.06843862840810752, 0.010068954284568983, 0.012889001855854782, 0.059367263409290084, 0.009775613610423645, 0.012611348918468063, 0.09297715904124614, 0.010536397959352756, 0.009940290899605167, 0.013363644183727399, 0.09557945038933231, 0.010732583468779922, 0.010942536753089148, 0.01065654993741488, 0.01048373261095994, 0.126986005448033, 0.009991612817559923, 0.011265064204795932, 0.011236057633401028, 0.009728503430129163, 0.009981247796011823, 0.009960591325023192, 0.1131639973479038, 0.011781821754399915, 0.01064355157277718, 0.009664134695004595, 0.01095645522404158, 0.010169772141403993, 0.0142231888389595, 0.012335979652933168, 0.010287092347648375, 0.01135574544932009, 0.11644934938640847, 0.012093554875737399, 0.01002101699954697, 0.010713398019422074, 0.010544394325864102, 0.010156710674910217, 0.009973767448729855, 0.01101990297854859, 0.01112395746880496, 0.015544788203943444, 0.012753510020901354, 0.010227953366536115, 0.010200059366868618, 0.08088595653190372, 0.010100934856419205, 0.010176675244500595, 0.009992045979965858, 0.03103993524445639, 0.010725613144150346, 0.01027196781633764, 0.011136578448230818, 0.015809021673963537, 0.010628222409464722, 0.011490593367370263, 0.013050813508239023, 0.01312778736654745, 0.010255862122439608, 0.010398968327220301, 0.08885332510321953, 0.009939679937741282, 0.01006656124408604, 0.010470958918390073, 0.010440052795813096, 0.010964974818028966, 0.09027354377416932]
[10.471283794233894, 110.8801404831282, 88.25082097732985, 100.4858165153589, 97.84788568052885, 12.604087648977018, 93.6292475831863, 119.06439163800985, 9.522075397505477, 115.91240206350508, 126.76153981241886, 9.44189232125673, 98.40092728694015, 131.16927021357662, 9.634789754840007, 129.26444969388996, 138.55826760541984, 112.94967778321177, 131.3151351129546, 132.04123137315378, 77.88670308242963, 123.30759412148443, 131.0263335895146, 64.91472776821831, 114.23746837659239, 125.08767850389626, 123.89645897350266, 9.622163132954432, 93.85240380942305, 128.6662184901407, 13.91042693777552, 126.9989788028826, 121.22668999282828, 130.99149376414854, 118.04241617561307, 10.640404330444863, 97.82755583350209, 129.53945064281206, 17.749970231421013, 119.43292050514017, 125.8423404682012, 14.58170097263499, 115.5384779879596, 129.7596731799912, 73.18150202554139, 85.45241232937103, 119.1478393010456, 13.476545946833777, 119.6793515592734, 120.19848773147687, 123.5478073972356, 117.81959480397192, 8.43444346205135, 126.05040537295191, 53.08385941136331, 69.70886590284489, 10.189394922666926, 15.42054658971752, 42.492976010377184, 51.40453152252078, 5.686729232383703, 5.489228363514841, 4.853871601071491, 52.25192025711151, 32.28146464683324, 30.3207483459197, 52.225958718266405, 38.96710015954772, 5.974096402381204, 76.12156264155777, 47.72819388013038, 8.211803424483449, 40.533066905847356, 74.34623434549943, 28.02731279399197, 8.034752107775432, 7.780969842927774, 5.645668925740732, 35.00307299341444, 25.960411338599656, 4.983470262442694, 94.30227030715267, 47.66384389976723, 41.43342183460582, 4.35274259199977, 9.366072895990753, 4.798105667148606, 24.71674349777385, 8.348431916796125, 125.3141776928181, 119.13328010098617, 135.82677928087406, 58.11958989251325, 79.62445894003434, 99.66008546664544, 59.87576811843298, 123.32500577012682, 107.72972117735941, 6.454158532078128, 122.19114813439452, 48.16508176321187, 5.898842293704587, 63.42437844148171, 5.175461296267343, 6.060162410259631, 43.57603427640697, 55.540048732406106, 4.985758725591855, 61.766091356344674, 4.957124098214756, 62.123514390799635, 42.87946286695361, 5.1244229929521925, 78.68883915621, 7.118768606884859, 65.55648428843503, 13.127542645322048, 53.20197781420159, 37.85429437205121, 5.311541291453378, 97.04681895095572, 75.40140368258147, 98.59864544214594, 92.22294517485076, 13.116155147659766, 101.11564585378783, 89.02485409835558, 9.226113772570615, 95.53837522629243, 96.33386686603083, 12.323146097667939, 97.02238056942167, 95.11880793138772, 98.267027867083, 18.55398919462763, 93.84287441648011, 14.611631227278304, 99.3151792865456, 77.585526884361, 16.84430008346174, 102.29536884863262, 79.29365894679194, 10.755329699376857, 94.90909548574315, 100.60067759583579, 74.8298881840686, 10.462500003155602, 93.17421130792098, 91.386487664087, 93.83900097807688, 95.38587420234055, 7.874883507610089, 100.08394222827907, 88.77002224046491, 88.99918749324813, 102.79073314637495, 100.18787434568722, 100.39564593798568, 8.836732736876263, 84.87651747290698, 93.95360121687972, 103.47537897178744, 91.270395356129, 98.33062000757319, 70.30772151887953, 81.06368753309565, 97.20919830457238, 88.061149702847, 8.587424534951642, 82.68867262563487, 99.79027079239641, 93.34106678265131, 94.8371209474899, 98.45707257077497, 100.26301546938, 90.74490056279134, 89.89606466981839, 64.33024283639433, 78.40978666744522, 97.77127096333922, 98.03864507378808, 12.363085545085585, 99.000737477729, 98.26391979447247, 100.07960351713842, 32.216562055444236, 93.23476304432918, 97.35232994105499, 89.79418630673429, 63.255021128026975, 94.08911118659623, 87.0277076238457, 76.62357594556819, 76.17429899483477, 97.50521097704905, 96.16338549492488, 11.254502843177963, 100.60686121320346, 99.33878866405206, 95.5022369769503, 95.7849562217772, 91.19947985249976, 11.077442606015628]
Elapsed: 1.9961782074458851~2.661308961194141
Time per graph: 0.04073833076420174~0.05431242777947227
Speed: 69.87618628446509~42.252596597240526
Total Time: 4.4241
best val loss: 0.30682870745658875 test_score: 0.9592

Testing...
Test loss: 0.3920 score: 0.9592 time: 3.88s
test Score 0.9592
Epoch Time List: [5.7616848619654775, 6.980404923087917, 1.7884972519241273, 9.621527162147686, 1.6623823221307248, 8.258319083135575, 7.519732084940188, 1.4882440948858857, 6.496882788022049, 7.860313066164963, 1.6558751289267093, 12.213574492023326, 1.807032041135244, 1.381071100011468, 6.958169581135735, 14.301530844997615, 1.6394435780821368, 1.6068459000671282, 6.833464242867194, 1.418536944896914, 1.827063697972335, 8.66207163699437, 1.5211611830163747, 1.8839318559039384, 12.497050225036219, 1.4642396649578586, 1.63688243587967, 6.230349141987972, 7.119672607048415, 1.506110452930443, 7.8331821580650285, 3.417555835098028, 1.5988553072093055, 13.166531654074788, 1.7529303249903023, 11.621138520073146, 1.932513887877576, 1.3915239770431072, 10.642679539974779, 1.4994341090787202, 1.5374875178094953, 8.033897261018865, 9.601390806026757, 1.564527663984336, 1.8097510179504752, 18.552979042055085, 1.779083393048495, 4.840514556970447, 1.9329237908823416, 1.5490123130148277, 14.03463211201597, 1.545958349131979, 6.986277188058011, 1.5681531860027462, 2.479002720909193, 17.43931692908518, 14.058413930004463, 6.073517221142538, 19.51189361384604, 11.318134693894535, 19.593921874882653, 12.122686435934156, 31.23695767892059, 6.528596274089068, 15.654594345018268, 10.893737345002592, 10.989062689943239, 4.07428459613584, 31.8660111509962, 16.38550120498985, 19.393515785923228, 9.671341035049409, 7.340897703077644, 23.30868654407095, 12.845846176962368, 20.641787351923995, 15.763106822152622, 19.784071878064424, 4.589323709020391, 23.346311567816883, 14.6877462239936, 28.148412873968482, 5.04822763602715, 22.051180717884563, 14.588233401998878, 14.639629389974289, 19.06212158105336, 14.540458886069246, 10.507414316991344, 7.845735052833334, 1.7093993078451604, 1.5414119550259784, 24.70401019300334, 19.095169157953933, 3.161128023872152, 20.78108755790163, 23.938999910023995, 1.6451355488970876, 19.79787302191835, 3.525733712129295, 19.337175732012838, 11.504625395871699, 6.060042835888453, 17.129491482977755, 17.031711726100184, 9.079908650950529, 3.1046323030022904, 22.411946469102986, 3.6209462177939713, 13.372352313948795, 9.061296463012695, 15.305980723001994, 12.445476706838235, 12.320810417993926, 9.12988627597224, 18.253223404171877, 19.780690688057803, 3.0528567171422765, 30.774202068103477, 21.095475394045934, 22.967672523926012, 1.7151471909601241, 14.237632244010456, 1.6588123941328377, 4.851707336027175, 8.784324286971241, 1.9098170389188454, 11.612712168018334, 5.300649642944336, 1.511937370058149, 5.11051680194214, 9.326734074042179, 1.6237211000407115, 1.5113828019239008, 3.8756672120653093, 16.228760375874117, 8.836274224915542, 9.505895403097384, 1.7440703440224752, 3.946936674998142, 9.420305983978324, 1.595353969023563, 5.612683487008326, 4.400810586987063, 1.6243255210574716, 1.680666503845714, 12.401820396189578, 1.5454123599920422, 1.6497603199677542, 12.046569826081395, 1.6330783209996298, 7.275506975129247, 1.6042510670376942, 1.4948133119614795, 1.7888383259996772, 13.341787109035067, 1.6349618410458788, 1.4534743300173432, 12.151018872042187, 4.610066298977472, 1.9422493390738964, 14.291332277003676, 1.6969703091308475, 1.5318402430275455, 13.323619796195999, 1.6378187180962414, 14.906046969001181, 1.5451701510464773, 6.838628159835935, 9.647045929916203, 1.6449695880291983, 1.5369511720491573, 16.119603203143924, 1.6258259990718216, 1.7112830011174083, 1.5249265439342707, 16.61894047597889, 1.7665199479088187, 17.01752338395454, 1.491996017866768, 1.592633988126181, 9.991173351067118, 5.052816613926552, 1.5064462970476598, 1.5776634368812665, 9.959461780963466, 1.685720643028617, 1.4912187670124695, 1.620807130006142, 10.683629503007978, 1.935682148905471, 10.842816506046802, 1.980727179092355, 16.71868434199132, 1.4751138450810686, 1.5389923100592569, 5.498539247084409, 7.927313215914182, 1.584173568058759, 1.5083996530156583, 12.593331743963063, 1.913247475051321, 5.617644765879959]
Total Epoch List: [106, 97]
Total Time List: [1.1251285680336878, 4.424122308962978]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb60040>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.59s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.52s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 15.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.58s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.44s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.43s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 5.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 2.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 3.55s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.46s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.50s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 3.92s
Epoch 10/1000, LR 0.000240
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 12.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.45s
Epoch 11/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.57s
Epoch 12/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 3.87s
Epoch 13/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.57s
Epoch 14/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.43s
Epoch 15/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 11.12s
Val loss: 0.6917 score: 0.5102 time: 0.44s
Test loss: 0.6913 score: 0.5417 time: 0.56s
Epoch 16/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.62s
Val loss: 0.6914 score: 0.6122 time: 0.41s
Test loss: 0.6909 score: 0.6042 time: 0.43s
Epoch 17/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.66s
Val loss: 0.6909 score: 0.6327 time: 0.49s
Test loss: 0.6905 score: 0.6458 time: 3.73s
Epoch 18/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 6.72s
Val loss: 0.6905 score: 0.6531 time: 0.44s
Test loss: 0.6900 score: 0.7083 time: 0.49s
Epoch 19/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.70s
Val loss: 0.6900 score: 0.6531 time: 0.56s
Test loss: 0.6894 score: 0.6875 time: 0.47s
Epoch 20/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 11.31s
Val loss: 0.6894 score: 0.6327 time: 0.44s
Test loss: 0.6888 score: 0.6667 time: 0.52s
Epoch 21/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.63s
Val loss: 0.6888 score: 0.6531 time: 0.53s
Test loss: 0.6881 score: 0.7083 time: 0.47s
Epoch 22/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.64s
Val loss: 0.6881 score: 0.6327 time: 0.45s
Test loss: 0.6874 score: 0.6875 time: 0.45s
Epoch 23/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.62s
Val loss: 0.6873 score: 0.6327 time: 5.61s
Test loss: 0.6865 score: 0.6458 time: 3.78s
Epoch 24/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 10.03s
Val loss: 0.6864 score: 0.6122 time: 0.46s
Test loss: 0.6855 score: 0.6250 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.75s
Val loss: 0.6855 score: 0.6122 time: 0.43s
Test loss: 0.6844 score: 0.6250 time: 0.55s
Epoch 26/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.71s
Val loss: 0.6843 score: 0.5510 time: 0.44s
Test loss: 0.6831 score: 0.6250 time: 0.45s
Epoch 27/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.69s
Val loss: 0.6831 score: 0.5714 time: 0.55s
Test loss: 0.6818 score: 0.6042 time: 0.44s
Epoch 28/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.61s
Val loss: 0.6818 score: 0.5714 time: 0.44s
Test loss: 0.6803 score: 0.6042 time: 0.43s
Epoch 29/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 4.50s
Val loss: 0.6804 score: 0.5714 time: 5.63s
Test loss: 0.6786 score: 0.6042 time: 3.11s
Epoch 30/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.89s
Val loss: 0.6788 score: 0.5714 time: 0.40s
Test loss: 0.6768 score: 0.6042 time: 0.47s
Epoch 31/1000, LR 0.000270
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.64s
Val loss: 0.6770 score: 0.5714 time: 0.43s
Test loss: 0.6748 score: 0.6042 time: 0.56s
Epoch 32/1000, LR 0.000270
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.60s
Val loss: 0.6751 score: 0.5714 time: 0.51s
Test loss: 0.6725 score: 0.6042 time: 3.98s
Epoch 33/1000, LR 0.000270
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 5.75s
Val loss: 0.6730 score: 0.5714 time: 0.45s
Test loss: 0.6701 score: 0.6042 time: 0.64s
Epoch 34/1000, LR 0.000270
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.62s
Val loss: 0.6707 score: 0.5714 time: 0.42s
Test loss: 0.6675 score: 0.6042 time: 0.45s
Epoch 35/1000, LR 0.000270
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.63s
Val loss: 0.6683 score: 0.5714 time: 1.08s
Test loss: 0.6645 score: 0.6042 time: 5.90s
Epoch 36/1000, LR 0.000270
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 7.71s
Val loss: 0.6656 score: 0.5714 time: 0.43s
Test loss: 0.6614 score: 0.6042 time: 0.47s
Epoch 37/1000, LR 0.000270
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.66s
Val loss: 0.6626 score: 0.5714 time: 0.41s
Test loss: 0.6580 score: 0.6042 time: 0.72s
Epoch 38/1000, LR 0.000270
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.67s
Val loss: 0.6595 score: 0.5714 time: 6.09s
Test loss: 0.6542 score: 0.6042 time: 4.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.66s
Val loss: 0.6561 score: 0.5714 time: 0.43s
Test loss: 0.6502 score: 0.6042 time: 0.56s
Epoch 40/1000, LR 0.000269
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.57s
Val loss: 0.6524 score: 0.5714 time: 0.41s
Test loss: 0.6458 score: 0.6042 time: 0.52s
Epoch 41/1000, LR 0.000269
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.80s
Val loss: 0.6484 score: 0.5714 time: 0.41s
Test loss: 0.6410 score: 0.6042 time: 0.56s
Epoch 42/1000, LR 0.000269
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.68s
Val loss: 0.6442 score: 0.5714 time: 4.03s
Test loss: 0.6359 score: 0.6042 time: 4.32s
Epoch 43/1000, LR 0.000269
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 1.77s
Val loss: 0.6396 score: 0.5714 time: 0.56s
Test loss: 0.6305 score: 0.6042 time: 0.45s
Epoch 44/1000, LR 0.000269
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.63s
Val loss: 0.6349 score: 0.5714 time: 0.41s
Test loss: 0.6247 score: 0.6042 time: 0.57s
Epoch 45/1000, LR 0.000269
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.62s
Val loss: 0.6298 score: 0.5714 time: 0.55s
Test loss: 0.6186 score: 0.6042 time: 5.93s
Epoch 46/1000, LR 0.000269
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 10.64s
Val loss: 0.6244 score: 0.5714 time: 0.41s
Test loss: 0.6121 score: 0.6042 time: 0.43s
Epoch 47/1000, LR 0.000269
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.60s
Val loss: 0.6187 score: 0.5714 time: 0.53s
Test loss: 0.6051 score: 0.6042 time: 0.47s
Epoch 48/1000, LR 0.000269
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.60s
Val loss: 0.6127 score: 0.5714 time: 0.44s
Test loss: 0.5978 score: 0.6042 time: 0.44s
Epoch 49/1000, LR 0.000269
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.72s
Val loss: 0.6063 score: 0.5714 time: 0.43s
Test loss: 0.5901 score: 0.6042 time: 3.64s
Epoch 50/1000, LR 0.000269
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 1.47s
Val loss: 0.5995 score: 0.5918 time: 0.44s
Test loss: 0.5819 score: 0.6250 time: 0.42s
Epoch 51/1000, LR 0.000269
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.59s
Val loss: 0.5923 score: 0.5918 time: 0.53s
Test loss: 0.5733 score: 0.6458 time: 0.43s
Epoch 52/1000, LR 0.000269
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.60s
Val loss: 0.5846 score: 0.6122 time: 0.40s
Test loss: 0.5642 score: 0.6875 time: 0.43s
Epoch 53/1000, LR 0.000269
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.65s
Val loss: 0.5765 score: 0.6939 time: 0.77s
Test loss: 0.5546 score: 0.7083 time: 5.41s
Epoch 54/1000, LR 0.000269
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 8.68s
Val loss: 0.5679 score: 0.6939 time: 0.41s
Test loss: 0.5445 score: 0.7708 time: 0.44s
Epoch 55/1000, LR 0.000269
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.64s
Val loss: 0.5588 score: 0.6939 time: 0.43s
Test loss: 0.5339 score: 0.7708 time: 0.57s
Epoch 56/1000, LR 0.000269
Train loss: 0.5028;  Loss pred: 0.5028; Loss self: 0.0000; time: 0.64s
Val loss: 0.5492 score: 0.6735 time: 0.45s
Test loss: 0.5228 score: 0.7917 time: 0.50s
Epoch 57/1000, LR 0.000269
Train loss: 0.5013;  Loss pred: 0.5013; Loss self: 0.0000; time: 5.90s
Val loss: 0.5392 score: 0.7347 time: 0.41s
Test loss: 0.5114 score: 0.8125 time: 0.55s
Epoch 58/1000, LR 0.000269
Train loss: 0.4823;  Loss pred: 0.4823; Loss self: 0.0000; time: 0.58s
Val loss: 0.5288 score: 0.7755 time: 0.41s
Test loss: 0.4998 score: 0.8542 time: 0.45s
Epoch 59/1000, LR 0.000268
Train loss: 0.4702;  Loss pred: 0.4702; Loss self: 0.0000; time: 0.71s
Val loss: 0.5183 score: 0.8163 time: 0.46s
Test loss: 0.4881 score: 0.8958 time: 6.03s
Epoch 60/1000, LR 0.000268
Train loss: 0.4535;  Loss pred: 0.4535; Loss self: 0.0000; time: 8.14s
Val loss: 0.5075 score: 0.8367 time: 0.41s
Test loss: 0.4762 score: 0.8958 time: 0.43s
Epoch 61/1000, LR 0.000268
Train loss: 0.4424;  Loss pred: 0.4424; Loss self: 0.0000; time: 0.61s
Val loss: 0.4966 score: 0.8571 time: 0.42s
Test loss: 0.4644 score: 0.8958 time: 0.59s
Epoch 62/1000, LR 0.000268
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.61s
Val loss: 0.4858 score: 0.8571 time: 0.41s
Test loss: 0.4527 score: 0.9167 time: 0.48s
Epoch 63/1000, LR 0.000268
Train loss: 0.4132;  Loss pred: 0.4132; Loss self: 0.0000; time: 0.63s
Val loss: 0.4751 score: 0.8571 time: 0.43s
Test loss: 0.4412 score: 0.9167 time: 6.22s
Epoch 64/1000, LR 0.000268
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.63s
Val loss: 0.4646 score: 0.8571 time: 0.42s
Test loss: 0.4304 score: 0.9375 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 0.62s
Val loss: 0.4545 score: 0.8571 time: 0.41s
Test loss: 0.4198 score: 0.9375 time: 0.54s
Epoch 66/1000, LR 0.000268
Train loss: 0.3732;  Loss pred: 0.3732; Loss self: 0.0000; time: 1.21s
Val loss: 0.4448 score: 0.8571 time: 6.43s
Test loss: 0.4096 score: 0.9375 time: 3.42s
Epoch 67/1000, LR 0.000268
Train loss: 0.3569;  Loss pred: 0.3569; Loss self: 0.0000; time: 0.62s
Val loss: 0.4354 score: 0.8776 time: 0.55s
Test loss: 0.3995 score: 0.9375 time: 0.42s
Epoch 68/1000, LR 0.000268
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 0.61s
Val loss: 0.4264 score: 0.8776 time: 0.39s
Test loss: 0.3898 score: 0.9375 time: 0.44s
Epoch 69/1000, LR 0.000268
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.63s
Val loss: 0.4177 score: 0.8776 time: 4.82s
Test loss: 0.3800 score: 0.9375 time: 5.15s
Epoch 70/1000, LR 0.000268
Train loss: 0.3250;  Loss pred: 0.3250; Loss self: 0.0000; time: 3.48s
Val loss: 0.4094 score: 0.8776 time: 0.46s
Test loss: 0.3706 score: 0.9375 time: 0.45s
Epoch 71/1000, LR 0.000268
Train loss: 0.3147;  Loss pred: 0.3147; Loss self: 0.0000; time: 0.61s
Val loss: 0.4017 score: 0.8776 time: 0.56s
Test loss: 0.3619 score: 0.9375 time: 0.52s
Epoch 72/1000, LR 0.000267
Train loss: 0.3076;  Loss pred: 0.3076; Loss self: 0.0000; time: 0.74s
Val loss: 0.3944 score: 0.8776 time: 0.46s
Test loss: 0.3538 score: 0.9375 time: 1.51s
Epoch 73/1000, LR 0.000267
Train loss: 0.2919;  Loss pred: 0.2919; Loss self: 0.0000; time: 15.74s
Val loss: 0.3878 score: 0.8980 time: 4.45s
Test loss: 0.3469 score: 0.9375 time: 0.54s
Epoch 74/1000, LR 0.000267
Train loss: 0.2794;  Loss pred: 0.2794; Loss self: 0.0000; time: 0.59s
Val loss: 0.3815 score: 0.8980 time: 0.39s
Test loss: 0.3402 score: 0.9375 time: 0.42s
Epoch 75/1000, LR 0.000267
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 0.78s
Val loss: 0.3756 score: 0.8980 time: 0.53s
Test loss: 0.3345 score: 0.9375 time: 0.44s
Epoch 76/1000, LR 0.000267
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.61s
Val loss: 0.3698 score: 0.8980 time: 0.45s
Test loss: 0.3287 score: 0.9167 time: 4.32s
Epoch 77/1000, LR 0.000267
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 5.62s
Val loss: 0.3643 score: 0.8980 time: 0.50s
Test loss: 0.3232 score: 0.9167 time: 0.43s
Epoch 78/1000, LR 0.000267
Train loss: 0.2458;  Loss pred: 0.2458; Loss self: 0.0000; time: 0.58s
Val loss: 0.3591 score: 0.9184 time: 0.40s
Test loss: 0.3184 score: 0.9167 time: 0.46s
Epoch 79/1000, LR 0.000267
Train loss: 0.2418;  Loss pred: 0.2418; Loss self: 0.0000; time: 0.60s
Val loss: 0.3543 score: 0.9184 time: 0.40s
Test loss: 0.3143 score: 0.9167 time: 0.54s
Epoch 80/1000, LR 0.000267
Train loss: 0.2261;  Loss pred: 0.2261; Loss self: 0.0000; time: 0.61s
Val loss: 0.3496 score: 0.9184 time: 0.50s
Test loss: 0.3098 score: 0.8958 time: 6.97s
Epoch 81/1000, LR 0.000267
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 8.98s
Val loss: 0.3451 score: 0.9184 time: 0.49s
Test loss: 0.3056 score: 0.8958 time: 0.63s
Epoch 82/1000, LR 0.000267
Train loss: 0.2093;  Loss pred: 0.2093; Loss self: 0.0000; time: 0.79s
Val loss: 0.3404 score: 0.9184 time: 0.44s
Test loss: 0.3006 score: 0.8958 time: 0.51s
Epoch 83/1000, LR 0.000266
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.75s
Val loss: 0.3359 score: 0.9184 time: 0.48s
Test loss: 0.2960 score: 0.8958 time: 7.15s
Epoch 84/1000, LR 0.000266
Train loss: 0.1937;  Loss pred: 0.1937; Loss self: 0.0000; time: 7.53s
Val loss: 0.3314 score: 0.9184 time: 0.41s
Test loss: 0.2910 score: 0.8958 time: 0.44s
Epoch 85/1000, LR 0.000266
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.63s
Val loss: 0.3268 score: 0.9184 time: 0.44s
Test loss: 0.2857 score: 0.8958 time: 0.61s
Epoch 86/1000, LR 0.000266
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 0.73s
Val loss: 0.3224 score: 0.9184 time: 1.29s
Test loss: 0.2807 score: 0.8958 time: 10.06s
Epoch 87/1000, LR 0.000266
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 3.45s
Val loss: 0.3180 score: 0.9184 time: 0.57s
Test loss: 0.2753 score: 0.8958 time: 0.82s
Epoch 88/1000, LR 0.000266
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 16.31s
Val loss: 0.3142 score: 0.9184 time: 2.61s
Test loss: 0.2709 score: 0.8958 time: 0.93s
Epoch 89/1000, LR 0.000266
Train loss: 0.1557;  Loss pred: 0.1557; Loss self: 0.0000; time: 2.82s
Val loss: 0.3114 score: 0.9184 time: 10.21s
Test loss: 0.2677 score: 0.8958 time: 6.54s
Epoch 90/1000, LR 0.000266
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 2.81s
Val loss: 0.3087 score: 0.9184 time: 0.93s
Test loss: 0.2644 score: 0.8958 time: 0.76s
Epoch 91/1000, LR 0.000266
Train loss: 0.1360;  Loss pred: 0.1360; Loss self: 0.0000; time: 28.59s
Val loss: 0.3067 score: 0.8980 time: 3.67s
Test loss: 0.2615 score: 0.8958 time: 0.52s
Epoch 92/1000, LR 0.000266
Train loss: 0.1288;  Loss pred: 0.1288; Loss self: 0.0000; time: 0.73s
Val loss: 0.3055 score: 0.8980 time: 8.58s
Test loss: 0.2591 score: 0.8958 time: 9.42s
Epoch 93/1000, LR 0.000265
Train loss: 0.1224;  Loss pred: 0.1224; Loss self: 0.0000; time: 2.49s
Val loss: 0.3053 score: 0.8980 time: 1.42s
Test loss: 0.2573 score: 0.8958 time: 1.31s
Epoch 94/1000, LR 0.000265
Train loss: 0.1172;  Loss pred: 0.1172; Loss self: 0.0000; time: 2.09s
Val loss: 0.3064 score: 0.8980 time: 5.73s
Test loss: 0.2562 score: 0.8958 time: 9.77s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000265
Train loss: 0.1098;  Loss pred: 0.1098; Loss self: 0.0000; time: 3.41s
Val loss: 0.3097 score: 0.8980 time: 0.88s
Test loss: 0.2564 score: 0.8958 time: 3.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1224,   Val_Loss: 0.3053,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3053,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2573


[4.6794644250767305, 0.4419186320155859, 0.5552356279222295, 0.48763100802898407, 0.5007772999815643, 3.8876276779919863, 0.523340743035078, 0.4115420179441571, 5.1459369889926165, 0.4227330219000578, 0.386552577954717, 5.189637662959285, 0.4979627870488912, 0.37356310605537146, 5.085736300097778, 0.3790678730001673, 0.35364183492492884, 0.43382151203695685, 0.3731481520226225, 0.3710962060140446, 0.6291189389303327, 0.3973802290856838, 0.3739706260384992, 0.7548364090034738, 0.4289310739841312, 0.3917252329410985, 0.3954915290232748, 5.092410024954006, 0.5220963769825175, 0.38083034206647426, 3.5225374619476497, 0.3858298740815371, 0.4042014180449769, 0.37407009105663747, 0.41510502400342375, 4.6050881600240245, 0.5008813680615276, 0.378263144986704, 2.7605680100386962, 0.4102721409872174, 0.3893761020153761, 3.3603761380072683, 0.42410113802179694, 0.3776211730437353, 0.6695681100245565, 0.5734185690525919, 0.41125378594733775, 3.6359464949928224, 0.4094273520167917, 0.40765903901774436, 0.39660760504193604, 0.4158900739857927, 5.809511939994991, 0.388733378960751, 0.9230677750892937, 0.7029234999790788, 4.808921469026245, 3.1775786749785766, 1.1531317549524829, 0.9532233549980447, 8.616552326944657, 8.926573418895714, 10.095034238067456, 0.9377645789645612, 1.5178989099804312, 1.6160550999920815, 0.938230742001906, 1.2574710409389809, 8.20207721798215, 0.6437072269618511, 1.0266468520276248, 5.96702057600487, 1.208889525034465, 0.6590784379513934, 1.7482946139061823, 6.098507999093272, 6.297415487933904, 8.67921952996403, 1.3998770910548046, 1.8874893529573455, 9.832505747908726, 0.5196057299617678, 1.0280329069355503, 1.18262016098015, 11.257270321948454, 5.231648370041512, 10.212363669998012, 1.9824618079001084, 5.869365707039833, 0.3910172089235857, 0.4113040450029075, 0.36075360293034464, 0.8430892250034958, 0.6153887970140204, 0.49167126207612455, 0.8183611090062186, 0.39732412493322045, 0.45484198292251676, 7.592004404054023, 0.40101104497443885, 1.0173345130169764, 8.306714701000601, 0.7725735940039158, 9.467755084042437, 8.085591883980669, 1.124471302027814, 0.8822462550597265, 9.827992627979256, 0.7933155380887911, 9.884763630921952, 0.7887512559536844, 1.1427381950197741, 9.562052170047536, 0.6227058439981192, 6.883212913060561, 0.7474470379529521, 3.732610232080333, 0.9210183909162879, 1.2944370199693367, 9.225194216007367, 0.5049109340179712, 0.6498552759876475, 0.49696423090063035, 0.5313211360480636, 3.735850899014622, 0.4845936510246247, 0.5504080910468474, 5.311011896003038, 0.5128829110180959, 0.5086477019358426, 3.976257330039516, 0.5050381129840389, 0.5151452280115336, 0.498641315032728, 2.6409414970548823, 0.5221493939170614, 3.353492791997269, 0.49337875994388014, 0.6315610909368843, 2.908995907055214, 0.4790050669107586, 0.6179560970049351, 4.5558807930210605, 0.516283500008285, 0.4870742540806532, 0.6548185650026426, 4.683393069077283, 0.5258965899702162, 0.5361843009013683, 0.5221709469333291, 0.513702897937037, 6.222314266953617, 0.48958902806043625, 0.5519881460350007, 0.5505668240366504, 0.47669666807632893, 0.4890811420045793, 0.48806897492613643, 5.545035870047286, 0.5773092659655958, 0.5215340270660818, 0.4735426000552252, 0.5368663059780374, 0.4983188349287957, 0.6969362531090155, 0.6044630029937252, 0.5040675250347704, 0.5564315270166844, 5.706018119934015, 0.5925841889111325, 0.49102983297780156, 0.5249565029516816, 0.516675321967341, 0.49767882307060063, 0.48871460498776287, 0.5399752459488809, 0.5450739159714431, 0.7616946219932288, 0.6249219910241663, 0.5011697149602696, 0.4998029089765623, 3.9634118700632825, 0.494945807964541, 0.49865708698052913, 0.48961025301832706, 1.520956826978363, 0.525555044063367, 0.5033264230005443, 0.5456923439633101, 0.7746420620242134, 0.5207828980637714, 0.5630390750011429, 0.6394898619037122, 0.6432615809608251, 0.5025372439995408, 0.5095494480337948, 4.353812930057757, 0.4870443169493228, 0.4932615009602159, 0.5130769870011136, 0.5115625869948417, 0.5372837660834193, 4.423403644934297, 0.5960325240157545, 0.5302449619630352, 0.5846601059893146, 0.441278996062465, 0.4326279821107164, 3.5541119729168713, 0.46570681605953723, 0.5049416120164096, 3.9227811659220606, 0.4581574910553172, 0.5789510230533779, 3.872628759010695, 0.5758323130430654, 0.4391767259221524, 0.5604403440374881, 0.43728394503705204, 3.7362088159425184, 0.49238542397506535, 0.4696984429610893, 0.5234465500107035, 0.4739635500591248, 0.45130737498402596, 3.782524270005524, 0.4495390939991921, 0.5556865410180762, 0.4496604730375111, 0.44769536098465323, 0.43817968806251884, 3.111063063959591, 0.47436164598912, 0.5593960189726204, 3.9842440899228677, 0.6440066039795056, 0.45263774297200143, 5.9026094059227034, 0.47138165298383683, 0.7291499270359054, 4.226698226062581, 0.5614427820546553, 0.5293418009532616, 0.5683600839693099, 4.321288704988547, 0.4511060470249504, 0.5695007930044085, 5.935032776091248, 0.4307154160924256, 0.47327878698706627, 0.4461905099451542, 3.6445628999499604, 0.4279213319532573, 0.43238784198183566, 0.4371222850168124, 5.419110814109445, 0.4410109859891236, 0.5765385329723358, 0.504884671070613, 0.5523267310345545, 0.45658180792815983, 6.042786225909367, 0.43726699601393193, 0.5964136390248314, 0.4885163160506636, 6.22247153299395, 0.4492486850358546, 0.543162698042579, 3.4285399209475145, 0.42532305198255926, 0.4423354030586779, 5.158763797953725, 0.4509986350312829, 0.5212767090415582, 1.513968972954899, 0.548536635003984, 0.4245971970958635, 0.4441808119881898, 4.327737580984831, 0.43210898397956043, 0.4610798070207238, 0.549158229958266, 6.978904385934584, 0.6359413200989366, 0.5145881200442091, 7.163818252971396, 0.4448219599435106, 0.6153063660021871, 10.070427838014439, 0.8289243709295988, 0.9377067710738629, 6.5480228289961815, 0.7695226209471002, 0.5245131829287857, 9.42213093303144, 1.311030987999402, 9.77853198198136, 3.2060223650187254]
[0.09549927398115776, 0.009018747592154814, 0.011331339345351622, 0.009951653225081307, 0.010219944897582945, 0.07933934036718339, 0.01068042332724649, 0.0083988166927379, 0.10501912222433911, 0.008627204528572609, 0.007888828121524836, 0.1059109727134548, 0.010162505858140637, 0.007623736858272887, 0.10379053673668935, 0.007736079040819741, 0.007217180304590384, 0.008853500245652181, 0.0076152684086249495, 0.007573391959470298, 0.012839162018986381, 0.008109800593585384, 0.007632053592622432, 0.015404824673540282, 0.008753695387431249, 0.00799439250900201, 0.008071255694352547, 0.10392673520314298, 0.010655028101684031, 0.0077720477972749846, 0.07188851963158469, 0.007874079062888513, 0.008249008531530141, 0.007634083490951785, 0.008471531102110689, 0.09398139102089846, 0.010222068735949543, 0.007719656020136816, 0.05633812265385094, 0.008372900836473825, 0.007946451061538287, 0.06857910485729118, 0.008655125265750957, 0.007706554551912966, 0.013664655306623603, 0.011702419776583508, 0.008392934407088526, 0.07420298969373107, 0.008355660245240646, 0.008319572224851926, 0.008094032755957879, 0.0084875525303223, 0.11856146816316308, 0.007933334264505123, 0.018838117858965178, 0.014345377550593444, 0.09814125446992338, 0.0648485443873179, 0.023533301121479243, 0.01945353785710295, 0.17584800667233993, 0.18217496773256558, 0.20602110689933584, 0.01913805263192982, 0.03097752877511084, 0.03298071632636901, 0.019147566163304205, 0.02566267430487716, 0.16738933097922756, 0.01313688218289492, 0.020951976571992343, 0.12177593012254838, 0.024671214796621734, 0.013450580366354967, 0.035679481916452704, 0.12445934692027086, 0.12851868342722253, 0.17712692918293937, 0.02856892022560826, 0.03852019087668052, 0.20066338261038216, 0.010604198570648322, 0.020980263406847964, 0.024135105326125512, 0.22974021065200925, 0.10676833408247983, 0.20841558510200026, 0.040458404242859354, 0.11978297361305781, 0.007979943039256851, 0.008393960102100154, 0.007362318427149891, 0.01720590255109175, 0.012558955041102457, 0.010034107389308664, 0.01670124712257589, 0.00810865561088205, 0.009282489447398302, 0.15493886538885762, 0.008183898877029364, 0.02076192883708115, 0.16952478981633878, 0.01576680804089624, 0.19321949151107015, 0.16501207926491163, 0.02294839391893498, 0.01800502561346381, 0.20057127812202563, 0.016190113022220225, 0.20172987001881534, 0.01609696440721805, 0.02332118765346478, 0.19514392183770482, 0.012708282530573862, 0.14047373291960327, 0.015254021182713308, 0.07617571902204762, 0.01879629369216914, 0.026417082040190543, 0.18826926971443605, 0.010304304775876962, 0.01326235257117648, 0.010142127161237354, 0.01084328849077681, 0.07624185508193106, 0.00988966634744132, 0.01123281818462954, 0.10838799787761301, 0.010466998184042774, 0.010380565345629441, 0.08114810877631666, 0.010306900264980386, 0.010513167918602727, 0.010176353368014857, 0.053896765246018004, 0.010656110079940028, 0.06843862840810752, 0.010068954284568983, 0.012889001855854782, 0.059367263409290084, 0.009775613610423645, 0.012611348918468063, 0.09297715904124614, 0.010536397959352756, 0.009940290899605167, 0.013363644183727399, 0.09557945038933231, 0.010732583468779922, 0.010942536753089148, 0.01065654993741488, 0.01048373261095994, 0.126986005448033, 0.009991612817559923, 0.011265064204795932, 0.011236057633401028, 0.009728503430129163, 0.009981247796011823, 0.009960591325023192, 0.1131639973479038, 0.011781821754399915, 0.01064355157277718, 0.009664134695004595, 0.01095645522404158, 0.010169772141403993, 0.0142231888389595, 0.012335979652933168, 0.010287092347648375, 0.01135574544932009, 0.11644934938640847, 0.012093554875737399, 0.01002101699954697, 0.010713398019422074, 0.010544394325864102, 0.010156710674910217, 0.009973767448729855, 0.01101990297854859, 0.01112395746880496, 0.015544788203943444, 0.012753510020901354, 0.010227953366536115, 0.010200059366868618, 0.08088595653190372, 0.010100934856419205, 0.010176675244500595, 0.009992045979965858, 0.03103993524445639, 0.010725613144150346, 0.01027196781633764, 0.011136578448230818, 0.015809021673963537, 0.010628222409464722, 0.011490593367370263, 0.013050813508239023, 0.01312778736654745, 0.010255862122439608, 0.010398968327220301, 0.08885332510321953, 0.009939679937741282, 0.01006656124408604, 0.010470958918390073, 0.010440052795813096, 0.010964974818028966, 0.09027354377416932, 0.012417344250328219, 0.011046770040896567, 0.012180418874777388, 0.009193312417968022, 0.009013082960639926, 0.07404399943576816, 0.009702225334573692, 0.010519616917008534, 0.08172460762337626, 0.009544947730319109, 0.012061479646945372, 0.0806797658127228, 0.011996506521730529, 0.009149515123378174, 0.011675840500781002, 0.009110082188271917, 0.07783768366546913, 0.010258029666147195, 0.009785384228356028, 0.010905136458556322, 0.009874240626231767, 0.009402236978833875, 0.07880258895844842, 0.009365397791649835, 0.011576802937876588, 0.009367926521614814, 0.009326986687180275, 0.009128743501302475, 0.06481381383249148, 0.00988253429144, 0.011654083728596257, 0.0830050852067264, 0.013416804249573033, 0.009429952978583364, 0.12297102929005632, 0.009820451103829933, 0.015190623479914697, 0.08805621304297044, 0.011696724626138652, 0.011027954186526282, 0.011840835082693957, 0.09002684802059473, 0.009398042646353133, 0.01186459985425851, 0.12364651616856766, 0.008973237835258866, 0.009859974728897214, 0.009295635623857379, 0.0759283937489575, 0.008915027749026194, 0.009008080041288244, 0.009106714271183591, 0.11289814196061343, 0.009187728874773407, 0.012011219436923662, 0.010518430647304436, 0.01150680689655322, 0.00951212099850333, 0.12589137970644515, 0.009109729083623582, 0.012425284146350654, 0.010177423251055492, 0.12963482360404063, 0.009359347604913637, 0.011315889542553728, 0.07142791501973988, 0.008860896916303318, 0.00921532089705579, 0.1074742457907026, 0.00939580489648506, 0.010859931438365797, 0.03154102026989373, 0.011427846562583, 0.008845774939497156, 0.009253766916420622, 0.09016119960385065, 0.009002270499574175, 0.009605829312931746, 0.011440796457463875, 0.14539384137363717, 0.013248777502061179, 0.010720585834254356, 0.14924621360357074, 0.009267124165489804, 0.012818882625045566, 0.20980057995863413, 0.017269257727699976, 0.019535557730705477, 0.13641714227075377, 0.016031721269731253, 0.010927357977683036, 0.196294394438155, 0.027313145583320875, 0.20371941629127832, 0.06679213260455678]
[10.471283794233894, 110.8801404831282, 88.25082097732985, 100.4858165153589, 97.84788568052885, 12.604087648977018, 93.6292475831863, 119.06439163800985, 9.522075397505477, 115.91240206350508, 126.76153981241886, 9.44189232125673, 98.40092728694015, 131.16927021357662, 9.634789754840007, 129.26444969388996, 138.55826760541984, 112.94967778321177, 131.3151351129546, 132.04123137315378, 77.88670308242963, 123.30759412148443, 131.0263335895146, 64.91472776821831, 114.23746837659239, 125.08767850389626, 123.89645897350266, 9.622163132954432, 93.85240380942305, 128.6662184901407, 13.91042693777552, 126.9989788028826, 121.22668999282828, 130.99149376414854, 118.04241617561307, 10.640404330444863, 97.82755583350209, 129.53945064281206, 17.749970231421013, 119.43292050514017, 125.8423404682012, 14.58170097263499, 115.5384779879596, 129.7596731799912, 73.18150202554139, 85.45241232937103, 119.1478393010456, 13.476545946833777, 119.6793515592734, 120.19848773147687, 123.5478073972356, 117.81959480397192, 8.43444346205135, 126.05040537295191, 53.08385941136331, 69.70886590284489, 10.189394922666926, 15.42054658971752, 42.492976010377184, 51.40453152252078, 5.686729232383703, 5.489228363514841, 4.853871601071491, 52.25192025711151, 32.28146464683324, 30.3207483459197, 52.225958718266405, 38.96710015954772, 5.974096402381204, 76.12156264155777, 47.72819388013038, 8.211803424483449, 40.533066905847356, 74.34623434549943, 28.02731279399197, 8.034752107775432, 7.780969842927774, 5.645668925740732, 35.00307299341444, 25.960411338599656, 4.983470262442694, 94.30227030715267, 47.66384389976723, 41.43342183460582, 4.35274259199977, 9.366072895990753, 4.798105667148606, 24.71674349777385, 8.348431916796125, 125.3141776928181, 119.13328010098617, 135.82677928087406, 58.11958989251325, 79.62445894003434, 99.66008546664544, 59.87576811843298, 123.32500577012682, 107.72972117735941, 6.454158532078128, 122.19114813439452, 48.16508176321187, 5.898842293704587, 63.42437844148171, 5.175461296267343, 6.060162410259631, 43.57603427640697, 55.540048732406106, 4.985758725591855, 61.766091356344674, 4.957124098214756, 62.123514390799635, 42.87946286695361, 5.1244229929521925, 78.68883915621, 7.118768606884859, 65.55648428843503, 13.127542645322048, 53.20197781420159, 37.85429437205121, 5.311541291453378, 97.04681895095572, 75.40140368258147, 98.59864544214594, 92.22294517485076, 13.116155147659766, 101.11564585378783, 89.02485409835558, 9.226113772570615, 95.53837522629243, 96.33386686603083, 12.323146097667939, 97.02238056942167, 95.11880793138772, 98.267027867083, 18.55398919462763, 93.84287441648011, 14.611631227278304, 99.3151792865456, 77.585526884361, 16.84430008346174, 102.29536884863262, 79.29365894679194, 10.755329699376857, 94.90909548574315, 100.60067759583579, 74.8298881840686, 10.462500003155602, 93.17421130792098, 91.386487664087, 93.83900097807688, 95.38587420234055, 7.874883507610089, 100.08394222827907, 88.77002224046491, 88.99918749324813, 102.79073314637495, 100.18787434568722, 100.39564593798568, 8.836732736876263, 84.87651747290698, 93.95360121687972, 103.47537897178744, 91.270395356129, 98.33062000757319, 70.30772151887953, 81.06368753309565, 97.20919830457238, 88.061149702847, 8.587424534951642, 82.68867262563487, 99.79027079239641, 93.34106678265131, 94.8371209474899, 98.45707257077497, 100.26301546938, 90.74490056279134, 89.89606466981839, 64.33024283639433, 78.40978666744522, 97.77127096333922, 98.03864507378808, 12.363085545085585, 99.000737477729, 98.26391979447247, 100.07960351713842, 32.216562055444236, 93.23476304432918, 97.35232994105499, 89.79418630673429, 63.255021128026975, 94.08911118659623, 87.0277076238457, 76.62357594556819, 76.17429899483477, 97.50521097704905, 96.16338549492488, 11.254502843177963, 100.60686121320346, 99.33878866405206, 95.5022369769503, 95.7849562217772, 91.19947985249976, 11.077442606015628, 80.5325180522049, 90.5241981409834, 82.09898282486417, 108.77472172549402, 110.94982753037928, 13.505483329104639, 103.0691378024915, 95.06049582310933, 12.236216594742793, 104.76746738209407, 82.90856754488284, 12.394681490787184, 83.35760066387621, 109.29540926653837, 85.64693907330351, 109.76849377795669, 12.847247668594571, 97.48460791648199, 102.19322784507592, 91.69990708510447, 101.27361058463718, 106.35766810081259, 12.689938404527382, 106.77603047375067, 86.37963394265218, 106.74720790056143, 107.21576362647505, 109.54410098797513, 15.42880970690071, 101.18861928626694, 85.80683160412222, 12.047454653042918, 74.5333971785288, 106.0450674856098, 8.131996664362815, 101.82831617684083, 65.8300827034662, 11.356382081886844, 85.49401922015858, 90.67865019078322, 84.45350290044627, 11.107797529146394, 106.40513537018747, 84.28434269033306, 8.08757117456263, 111.44249359697834, 101.42013823516612, 107.57736646146994, 13.170303632476486, 112.17015001543119, 111.01144699164888, 109.80908923038287, 8.857541697620393, 108.84082602237895, 83.25549335364752, 95.07121675573062, 86.90508226913434, 105.12902434245143, 7.943355631909116, 109.77274854393688, 80.48105686932747, 98.2566977251625, 7.713976632192762, 106.84505397309981, 88.37131152963902, 14.000128657313308, 112.85539256867808, 108.51494062670035, 9.304554711157676, 106.43047732654567, 92.08161263958036, 31.704744851088776, 87.50555010724376, 113.04832045126004, 108.06410070968184, 11.091245506867585, 111.0830873219486, 104.10345295785764, 87.40650213627468, 6.877870414264467, 75.47866207613684, 93.27848453997781, 6.70033748833461, 107.90834158928593, 78.00991937052278, 4.766431056564131, 57.90636840146262, 51.188710032487386, 7.330457033143613, 62.37633396783497, 91.51342914200322, 5.094388980705522, 36.61240690675566, 4.908712277921504, 14.971823192418574]
Elapsed: 1.935712310508981~2.571250560111475
Time per graph: 0.03974918815621809~0.0527598428254054
Speed: 70.53635784722927~41.530727348214086
Total Time: 3.2076
best val loss: 0.30527225136756897 test_score: 0.8958

Testing...
Test loss: 0.3184 score: 0.9167 time: 8.35s
test Score 0.9167
Epoch Time List: [5.7616848619654775, 6.980404923087917, 1.7884972519241273, 9.621527162147686, 1.6623823221307248, 8.258319083135575, 7.519732084940188, 1.4882440948858857, 6.496882788022049, 7.860313066164963, 1.6558751289267093, 12.213574492023326, 1.807032041135244, 1.381071100011468, 6.958169581135735, 14.301530844997615, 1.6394435780821368, 1.6068459000671282, 6.833464242867194, 1.418536944896914, 1.827063697972335, 8.66207163699437, 1.5211611830163747, 1.8839318559039384, 12.497050225036219, 1.4642396649578586, 1.63688243587967, 6.230349141987972, 7.119672607048415, 1.506110452930443, 7.8331821580650285, 3.417555835098028, 1.5988553072093055, 13.166531654074788, 1.7529303249903023, 11.621138520073146, 1.932513887877576, 1.3915239770431072, 10.642679539974779, 1.4994341090787202, 1.5374875178094953, 8.033897261018865, 9.601390806026757, 1.564527663984336, 1.8097510179504752, 18.552979042055085, 1.779083393048495, 4.840514556970447, 1.9329237908823416, 1.5490123130148277, 14.03463211201597, 1.545958349131979, 6.986277188058011, 1.5681531860027462, 2.479002720909193, 17.43931692908518, 14.058413930004463, 6.073517221142538, 19.51189361384604, 11.318134693894535, 19.593921874882653, 12.122686435934156, 31.23695767892059, 6.528596274089068, 15.654594345018268, 10.893737345002592, 10.989062689943239, 4.07428459613584, 31.8660111509962, 16.38550120498985, 19.393515785923228, 9.671341035049409, 7.340897703077644, 23.30868654407095, 12.845846176962368, 20.641787351923995, 15.763106822152622, 19.784071878064424, 4.589323709020391, 23.346311567816883, 14.6877462239936, 28.148412873968482, 5.04822763602715, 22.051180717884563, 14.588233401998878, 14.639629389974289, 19.06212158105336, 14.540458886069246, 10.507414316991344, 7.845735052833334, 1.7093993078451604, 1.5414119550259784, 24.70401019300334, 19.095169157953933, 3.161128023872152, 20.78108755790163, 23.938999910023995, 1.6451355488970876, 19.79787302191835, 3.525733712129295, 19.337175732012838, 11.504625395871699, 6.060042835888453, 17.129491482977755, 17.031711726100184, 9.079908650950529, 3.1046323030022904, 22.411946469102986, 3.6209462177939713, 13.372352313948795, 9.061296463012695, 15.305980723001994, 12.445476706838235, 12.320810417993926, 9.12988627597224, 18.253223404171877, 19.780690688057803, 3.0528567171422765, 30.774202068103477, 21.095475394045934, 22.967672523926012, 1.7151471909601241, 14.237632244010456, 1.6588123941328377, 4.851707336027175, 8.784324286971241, 1.9098170389188454, 11.612712168018334, 5.300649642944336, 1.511937370058149, 5.11051680194214, 9.326734074042179, 1.6237211000407115, 1.5113828019239008, 3.8756672120653093, 16.228760375874117, 8.836274224915542, 9.505895403097384, 1.7440703440224752, 3.946936674998142, 9.420305983978324, 1.595353969023563, 5.612683487008326, 4.400810586987063, 1.6243255210574716, 1.680666503845714, 12.401820396189578, 1.5454123599920422, 1.6497603199677542, 12.046569826081395, 1.6330783209996298, 7.275506975129247, 1.6042510670376942, 1.4948133119614795, 1.7888383259996772, 13.341787109035067, 1.6349618410458788, 1.4534743300173432, 12.151018872042187, 4.610066298977472, 1.9422493390738964, 14.291332277003676, 1.6969703091308475, 1.5318402430275455, 13.323619796195999, 1.6378187180962414, 14.906046969001181, 1.5451701510464773, 6.838628159835935, 9.647045929916203, 1.6449695880291983, 1.5369511720491573, 16.119603203143924, 1.6258259990718216, 1.7112830011174083, 1.5249265439342707, 16.61894047597889, 1.7665199479088187, 17.01752338395454, 1.491996017866768, 1.592633988126181, 9.991173351067118, 5.052816613926552, 1.5064462970476598, 1.5776634368812665, 9.959461780963466, 1.685720643028617, 1.4912187670124695, 1.620807130006142, 10.683629503007978, 1.935682148905471, 10.842816506046802, 1.980727179092355, 16.71868434199132, 1.4751138450810686, 1.5389923100592569, 5.498539247084409, 7.927313215914182, 1.584173568058759, 1.5083996530156583, 12.593331743963063, 1.913247475051321, 5.617644765879959, 1.655265269917436, 1.7156609860248864, 17.427323464071378, 1.5093905598623678, 1.4945253221085295, 12.1530276380945, 1.6867932039313018, 1.6884159629698843, 4.99235879315529, 12.952490441850387, 1.6242258090060204, 5.210380077944137, 1.594815567950718, 1.4838782729348168, 12.1194459941471, 1.4582654041005298, 4.880898402072489, 7.6568542319582775, 1.7334298850037158, 12.27561555604916, 1.6335215290309861, 1.5298741759033874, 10.002583327004686, 10.936632587923668, 1.7263678029412404, 1.5909935850650072, 1.6854964699596167, 1.4825506479246542, 13.230505250976421, 1.762955378042534, 1.620886452961713, 5.086193156894296, 6.837177966022864, 1.4885157219832763, 7.612193965120241, 8.605588310048915, 1.7944526019273326, 10.973901411052793, 1.6510448199696839, 1.5133295599371195, 1.7701524830190465, 9.017663946957327, 2.775673585012555, 1.604455689783208, 7.101808559964411, 11.475218688836321, 1.5904688789742067, 1.4840118369320408, 4.789796659955755, 2.3350368918618187, 1.5477348379790783, 1.4339097389020026, 6.830230224062689, 9.525172895984724, 1.636704320088029, 1.5881255010608584, 6.85701297596097, 1.444784724037163, 7.20441728818696, 8.98125784297008, 1.6152550580445677, 1.5131797299254686, 7.275279851979576, 1.4971200400032103, 1.5665763961151242, 11.057996286195703, 1.5935522121144459, 1.4455738731194288, 10.605782941100188, 4.3799009510548785, 1.692251957836561, 2.7102281810948625, 20.729574980097823, 1.4019994168775156, 1.7526848529232666, 5.382964592892677, 6.542446281993762, 1.4400992919690907, 1.5530614741146564, 8.085580637911335, 10.10023638093844, 1.7387988099362701, 8.38178956892807, 8.379633925040253, 1.685430071898736, 12.076020597014576, 4.8374703348381445, 19.852470137993805, 19.57764737715479, 4.498105172067881, 32.77529609890189, 18.73230095487088, 5.212985625839792, 17.597452439018525, 7.4906451139831915]
Total Epoch List: [106, 97, 95]
Total Time List: [1.1251285680336878, 4.424122308962978, 3.2075677909888327]
========================training times:3========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3190>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 11.87s
Epoch 2/1000, LR 0.000000
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 9.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 1.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 1.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 17.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 1.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 2.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 9.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4898 time: 10.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 9.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 12.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 4.76s
Epoch 6/1000, LR 0.000120
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 1.14s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 16.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 4.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 10.28s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 3.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 9.79s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 8.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 1.55s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 28.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.41s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 2.06s
Val loss: 0.6916 score: 0.5306 time: 9.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 9.94s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 5.37s
Val loss: 0.6914 score: 0.8367 time: 1.10s
Test loss: 0.6922 score: 0.6327 time: 0.74s
Epoch 13/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 21.21s
Val loss: 0.6911 score: 0.6122 time: 1.04s
Test loss: 0.6919 score: 0.5918 time: 0.88s
Epoch 14/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.46s
Val loss: 0.6908 score: 0.5306 time: 10.19s
Test loss: 0.6917 score: 0.5510 time: 6.77s
Epoch 15/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 9.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 10.90s
Epoch 16/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 7.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 8.55s
Epoch 17/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 12.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.35s
Epoch 18/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.37s
Epoch 19/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4898 time: 8.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5102 time: 1.28s
Epoch 20/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 10.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.4898 time: 9.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 7.28s
Epoch 21/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 2.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.4898 time: 9.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5102 time: 4.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 2.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4898 time: 9.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5102 time: 8.74s
Epoch 23/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 24.45s
Val loss: 0.6844 score: 0.5102 time: 9.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5102 time: 8.48s
Epoch 24/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 2.79s
Val loss: 0.6831 score: 0.5102 time: 6.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5102 time: 8.46s
Epoch 25/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 3.69s
Val loss: 0.6817 score: 0.5102 time: 4.13s
Test loss: 0.6859 score: 0.5306 time: 9.63s
Epoch 26/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 8.21s
Val loss: 0.6801 score: 0.5306 time: 6.76s
Test loss: 0.6850 score: 0.5510 time: 7.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 7.72s
Val loss: 0.6784 score: 0.5306 time: 0.81s
Test loss: 0.6839 score: 0.5510 time: 1.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 9.94s
Val loss: 0.6764 score: 0.5714 time: 11.07s
Test loss: 0.6827 score: 0.5510 time: 4.13s
Epoch 29/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 2.65s
Val loss: 0.6742 score: 0.5918 time: 4.34s
Test loss: 0.6813 score: 0.5714 time: 9.00s
Epoch 30/1000, LR 0.000270
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 7.02s
Val loss: 0.6718 score: 0.5918 time: 2.09s
Test loss: 0.6798 score: 0.5510 time: 9.59s
Epoch 31/1000, LR 0.000270
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 25.88s
Val loss: 0.6691 score: 0.5918 time: 5.30s
Test loss: 0.6782 score: 0.5510 time: 1.36s
Epoch 32/1000, LR 0.000270
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 11.30s
Val loss: 0.6661 score: 0.5918 time: 0.91s
Test loss: 0.6763 score: 0.5714 time: 0.97s
Epoch 33/1000, LR 0.000270
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 16.44s
Val loss: 0.6627 score: 0.5918 time: 6.02s
Test loss: 0.6743 score: 0.5714 time: 0.60s
Epoch 34/1000, LR 0.000270
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 1.57s
Val loss: 0.6591 score: 0.6122 time: 0.81s
Test loss: 0.6720 score: 0.5918 time: 10.70s
Epoch 35/1000, LR 0.000270
Train loss: 0.6641;  Loss pred: 0.6641; Loss self: 0.0000; time: 25.61s
Val loss: 0.6551 score: 0.6122 time: 10.44s
Test loss: 0.6695 score: 0.5918 time: 4.98s
Epoch 36/1000, LR 0.000270
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 2.17s
Val loss: 0.6507 score: 0.6122 time: 1.00s
Test loss: 0.6667 score: 0.5918 time: 1.53s
Epoch 37/1000, LR 0.000270
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 18.30s
Val loss: 0.6459 score: 0.6531 time: 0.95s
Test loss: 0.6637 score: 0.6327 time: 1.52s
Epoch 38/1000, LR 0.000270
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 18.20s
Val loss: 0.6407 score: 0.6531 time: 7.76s
Test loss: 0.6604 score: 0.6327 time: 4.57s
Epoch 39/1000, LR 0.000269
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 22.74s
Val loss: 0.6351 score: 0.6735 time: 4.50s
Test loss: 0.6568 score: 0.6327 time: 0.39s
Epoch 40/1000, LR 0.000269
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.60s
Val loss: 0.6289 score: 0.6735 time: 0.56s
Test loss: 0.6529 score: 0.6327 time: 0.36s
Epoch 41/1000, LR 0.000269
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.59s
Val loss: 0.6223 score: 0.7143 time: 3.19s
Test loss: 0.6486 score: 0.6327 time: 6.72s
Epoch 42/1000, LR 0.000269
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 13.21s
Val loss: 0.6151 score: 0.7347 time: 0.47s
Test loss: 0.6440 score: 0.6122 time: 0.39s
Epoch 43/1000, LR 0.000269
Train loss: 0.6241;  Loss pred: 0.6241; Loss self: 0.0000; time: 0.62s
Val loss: 0.6073 score: 0.7551 time: 0.48s
Test loss: 0.6389 score: 0.6122 time: 0.38s
Epoch 44/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.71s
Val loss: 0.5990 score: 0.7755 time: 0.49s
Test loss: 0.6335 score: 0.6735 time: 0.37s
Epoch 45/1000, LR 0.000269
Train loss: 0.6076;  Loss pred: 0.6076; Loss self: 0.0000; time: 0.57s
Val loss: 0.5900 score: 0.8163 time: 0.48s
Test loss: 0.6276 score: 0.6939 time: 0.55s
Epoch 46/1000, LR 0.000269
Train loss: 0.5961;  Loss pred: 0.5961; Loss self: 0.0000; time: 8.00s
Val loss: 0.5803 score: 0.8163 time: 0.58s
Test loss: 0.6213 score: 0.6939 time: 0.39s
Epoch 47/1000, LR 0.000269
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.62s
Val loss: 0.5700 score: 0.8571 time: 0.48s
Test loss: 0.6145 score: 0.6939 time: 0.48s
Epoch 48/1000, LR 0.000269
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 0.60s
Val loss: 0.5591 score: 0.8776 time: 0.60s
Test loss: 0.6072 score: 0.6939 time: 0.50s
Epoch 49/1000, LR 0.000269
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 9.96s
Val loss: 0.5476 score: 0.8980 time: 0.52s
Test loss: 0.5995 score: 0.7347 time: 0.47s
Epoch 50/1000, LR 0.000269
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.89s
Val loss: 0.5353 score: 0.9184 time: 0.59s
Test loss: 0.5913 score: 0.7347 time: 0.46s
Epoch 51/1000, LR 0.000269
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.65s
Val loss: 0.5225 score: 0.9184 time: 4.36s
Test loss: 0.5827 score: 0.7347 time: 5.88s
Epoch 52/1000, LR 0.000269
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 4.95s
Val loss: 0.5090 score: 0.9388 time: 0.49s
Test loss: 0.5737 score: 0.7347 time: 0.38s
Epoch 53/1000, LR 0.000269
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.78s
Val loss: 0.4952 score: 0.9592 time: 0.50s
Test loss: 0.5642 score: 0.7551 time: 0.38s
Epoch 54/1000, LR 0.000269
Train loss: 0.5077;  Loss pred: 0.5077; Loss self: 0.0000; time: 0.63s
Val loss: 0.4811 score: 0.9592 time: 0.56s
Test loss: 0.5543 score: 0.7551 time: 0.43s
Epoch 55/1000, LR 0.000269
Train loss: 0.5001;  Loss pred: 0.5001; Loss self: 0.0000; time: 0.71s
Val loss: 0.4667 score: 0.9592 time: 5.80s
Test loss: 0.5441 score: 0.7551 time: 1.03s
Epoch 56/1000, LR 0.000269
Train loss: 0.4828;  Loss pred: 0.4828; Loss self: 0.0000; time: 0.58s
Val loss: 0.4520 score: 0.9796 time: 0.55s
Test loss: 0.5334 score: 0.8571 time: 0.37s
Epoch 57/1000, LR 0.000269
Train loss: 0.4665;  Loss pred: 0.4665; Loss self: 0.0000; time: 0.61s
Val loss: 0.4371 score: 0.9796 time: 0.47s
Test loss: 0.5225 score: 0.8571 time: 0.42s
Epoch 58/1000, LR 0.000269
Train loss: 0.4575;  Loss pred: 0.4575; Loss self: 0.0000; time: 0.74s
Val loss: 0.4222 score: 0.9796 time: 0.48s
Test loss: 0.5113 score: 0.8571 time: 0.42s
Epoch 59/1000, LR 0.000268
Train loss: 0.4432;  Loss pred: 0.4432; Loss self: 0.0000; time: 8.10s
Val loss: 0.4076 score: 0.9796 time: 0.50s
Test loss: 0.5000 score: 0.8571 time: 0.49s
Epoch 60/1000, LR 0.000268
Train loss: 0.4250;  Loss pred: 0.4250; Loss self: 0.0000; time: 0.84s
Val loss: 0.3930 score: 0.9796 time: 0.47s
Test loss: 0.4887 score: 0.8776 time: 0.46s
Epoch 61/1000, LR 0.000268
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 0.62s
Val loss: 0.3787 score: 0.9796 time: 0.47s
Test loss: 0.4774 score: 0.8776 time: 0.40s
Epoch 62/1000, LR 0.000268
Train loss: 0.3910;  Loss pred: 0.3910; Loss self: 0.0000; time: 16.06s
Val loss: 0.3647 score: 0.9796 time: 0.74s
Test loss: 0.4663 score: 0.8776 time: 0.46s
Epoch 63/1000, LR 0.000268
Train loss: 0.3840;  Loss pred: 0.3840; Loss self: 0.0000; time: 0.75s
Val loss: 0.3511 score: 0.9796 time: 0.49s
Test loss: 0.4555 score: 0.8776 time: 0.38s
Epoch 64/1000, LR 0.000268
Train loss: 0.3605;  Loss pred: 0.3605; Loss self: 0.0000; time: 0.74s
Val loss: 0.3379 score: 0.9796 time: 3.46s
Test loss: 0.4452 score: 0.8776 time: 3.52s
Epoch 65/1000, LR 0.000268
Train loss: 0.3435;  Loss pred: 0.3435; Loss self: 0.0000; time: 2.32s
Val loss: 0.3250 score: 0.9796 time: 0.47s
Test loss: 0.4354 score: 0.8776 time: 0.37s
Epoch 66/1000, LR 0.000268
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.74s
Val loss: 0.3130 score: 0.9796 time: 0.47s
Test loss: 0.4261 score: 0.8776 time: 0.43s
Epoch 67/1000, LR 0.000268
Train loss: 0.3138;  Loss pred: 0.3138; Loss self: 0.0000; time: 0.62s
Val loss: 0.3016 score: 0.9796 time: 0.47s
Test loss: 0.4173 score: 0.8776 time: 0.39s
Epoch 68/1000, LR 0.000268
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.71s
Val loss: 0.2909 score: 0.9796 time: 0.47s
Test loss: 0.4090 score: 0.8776 time: 0.39s
Epoch 69/1000, LR 0.000268
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 9.17s
Val loss: 0.2808 score: 0.9796 time: 4.69s
Test loss: 0.4013 score: 0.8776 time: 1.51s
Epoch 70/1000, LR 0.000268
Train loss: 0.2736;  Loss pred: 0.2736; Loss self: 0.0000; time: 0.61s
Val loss: 0.2712 score: 0.9796 time: 0.56s
Test loss: 0.3943 score: 0.8776 time: 0.41s
Epoch 71/1000, LR 0.000268
Train loss: 0.2763;  Loss pred: 0.2763; Loss self: 0.0000; time: 0.62s
Val loss: 0.2621 score: 0.9796 time: 0.47s
Test loss: 0.3872 score: 0.8776 time: 0.38s
Epoch 72/1000, LR 0.000267
Train loss: 0.2542;  Loss pred: 0.2542; Loss self: 0.0000; time: 0.60s
Val loss: 0.2535 score: 0.9796 time: 0.61s
Test loss: 0.3807 score: 0.8776 time: 0.80s
Epoch 73/1000, LR 0.000267
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 11.19s
Val loss: 0.2451 score: 0.9796 time: 4.29s
Test loss: 0.3737 score: 0.8776 time: 0.66s
Epoch 74/1000, LR 0.000267
Train loss: 0.2253;  Loss pred: 0.2253; Loss self: 0.0000; time: 1.15s
Val loss: 0.2371 score: 0.9796 time: 0.72s
Test loss: 0.3674 score: 0.8776 time: 0.39s
Epoch 75/1000, LR 0.000267
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.67s
Val loss: 0.2293 score: 0.9796 time: 5.53s
Test loss: 0.3608 score: 0.8776 time: 5.38s
Epoch 76/1000, LR 0.000267
Train loss: 0.2105;  Loss pred: 0.2105; Loss self: 0.0000; time: 0.78s
Val loss: 0.2219 score: 0.9796 time: 0.46s
Test loss: 0.3548 score: 0.8776 time: 0.43s
Epoch 77/1000, LR 0.000267
Train loss: 0.2015;  Loss pred: 0.2015; Loss self: 0.0000; time: 0.59s
Val loss: 0.2148 score: 0.9796 time: 0.48s
Test loss: 0.3493 score: 0.8776 time: 0.39s
Epoch 78/1000, LR 0.000267
Train loss: 0.1913;  Loss pred: 0.1913; Loss self: 0.0000; time: 0.60s
Val loss: 0.2080 score: 0.9592 time: 0.57s
Test loss: 0.3443 score: 0.8776 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 0.1890;  Loss pred: 0.1890; Loss self: 0.0000; time: 0.64s
Val loss: 0.2016 score: 0.9592 time: 0.47s
Test loss: 0.3398 score: 0.8776 time: 4.43s
Epoch 80/1000, LR 0.000267
Train loss: 0.1830;  Loss pred: 0.1830; Loss self: 0.0000; time: 11.44s
Val loss: 0.1955 score: 0.9592 time: 0.54s
Test loss: 0.3355 score: 0.8776 time: 0.39s
Epoch 81/1000, LR 0.000267
Train loss: 0.1654;  Loss pred: 0.1654; Loss self: 0.0000; time: 0.61s
Val loss: 0.1897 score: 0.9592 time: 0.48s
Test loss: 0.3316 score: 0.8776 time: 0.37s
Epoch 82/1000, LR 0.000267
Train loss: 0.1547;  Loss pred: 0.1547; Loss self: 0.0000; time: 0.82s
Val loss: 0.1843 score: 0.9592 time: 0.48s
Test loss: 0.3283 score: 0.8776 time: 0.70s
Epoch 83/1000, LR 0.000266
Train loss: 0.1492;  Loss pred: 0.1492; Loss self: 0.0000; time: 3.62s
Val loss: 0.1793 score: 0.9592 time: 0.49s
Test loss: 0.3254 score: 0.8776 time: 0.37s
Epoch 84/1000, LR 0.000266
Train loss: 0.1377;  Loss pred: 0.1377; Loss self: 0.0000; time: 0.73s
Val loss: 0.1748 score: 0.9592 time: 0.52s
Test loss: 0.3228 score: 0.8980 time: 0.37s
Epoch 85/1000, LR 0.000266
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.60s
Val loss: 0.1707 score: 0.9592 time: 0.47s
Test loss: 0.3210 score: 0.8980 time: 0.43s
Epoch 86/1000, LR 0.000266
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 9.14s
Val loss: 0.1670 score: 0.9592 time: 0.63s
Test loss: 0.3198 score: 0.8980 time: 0.39s
Epoch 87/1000, LR 0.000266
Train loss: 0.1217;  Loss pred: 0.1217; Loss self: 0.0000; time: 0.85s
Val loss: 0.1638 score: 0.9592 time: 0.53s
Test loss: 0.3190 score: 0.8980 time: 0.35s
Epoch 88/1000, LR 0.000266
Train loss: 0.1141;  Loss pred: 0.1141; Loss self: 0.0000; time: 0.62s
Val loss: 0.1610 score: 0.9592 time: 0.60s
Test loss: 0.3186 score: 0.8980 time: 0.41s
Epoch 89/1000, LR 0.000266
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 7.43s
Val loss: 0.1588 score: 0.9592 time: 0.47s
Test loss: 0.3183 score: 0.8980 time: 0.36s
Epoch 90/1000, LR 0.000266
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.73s
Val loss: 0.1570 score: 0.9592 time: 0.46s
Test loss: 0.3184 score: 0.8980 time: 0.37s
Epoch 91/1000, LR 0.000266
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.69s
Val loss: 0.1557 score: 0.9592 time: 0.54s
Test loss: 0.3189 score: 0.8980 time: 5.48s
Epoch 92/1000, LR 0.000266
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 1.36s
Val loss: 0.1549 score: 0.9592 time: 0.46s
Test loss: 0.3199 score: 0.8980 time: 0.39s
Epoch 93/1000, LR 0.000265
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 0.58s
Val loss: 0.1544 score: 0.9592 time: 0.46s
Test loss: 0.3210 score: 0.8980 time: 0.37s
Epoch 94/1000, LR 0.000265
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.64s
Val loss: 0.1543 score: 0.9592 time: 0.56s
Test loss: 0.3231 score: 0.8980 time: 0.38s
Epoch 95/1000, LR 0.000265
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 4.76s
Val loss: 0.1545 score: 0.9592 time: 4.17s
Test loss: 0.3263 score: 0.8980 time: 0.77s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.60s
Val loss: 0.1549 score: 0.9592 time: 0.55s
Test loss: 0.3295 score: 0.8980 time: 0.36s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.0779,   Val_Loss: 0.1543,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1543,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3231


[11.877417591051199, 1.0780011280439794, 1.0837587109999731, 10.22304615995381, 4.769169323029928, 1.1421367699513212, 10.28698989294935, 9.79477888403926, 1.559251528000459, 0.41232472797855735, 9.940751136047766, 0.7500309470342472, 0.8796537419548258, 6.775275908992626, 10.901907104998827, 8.554148345021531, 0.351163927000016, 0.37898538692388684, 1.287496994016692, 7.293296694057062, 4.464724946999922, 8.751723380060866, 8.486773630022071, 8.462669923901558, 9.638799651060253, 7.434535554959439, 1.0404624389484525, 4.138751995982602, 9.007079810020514, 9.594208082999103, 1.3698611760046333, 0.9767638819757849, 0.6083001350052655, 10.703087502974086, 4.987016311963089, 1.5335081570083275, 1.5272411170881242, 4.574087162967771, 0.38989684102125466, 0.36243048193864524, 6.721600641030818, 0.3996888629626483, 0.3869157119188458, 0.3752349289134145, 0.552511190995574, 0.39812493906356394, 0.4848336649592966, 0.5056049859849736, 0.47628150996752083, 0.46560769295319915, 5.88473041402176, 0.3803817860316485, 0.3834101080428809, 0.4339019840117544, 1.0391025349963456, 0.3758490780601278, 0.42148831696249545, 0.42229867703281343, 0.489381036022678, 0.4629778229864314, 0.4053614380536601, 0.461117216036655, 0.3830751549685374, 3.5293954230146483, 0.3792034749640152, 0.4379368119407445, 0.3905927869491279, 0.39474215591326356, 1.5101363429566845, 0.4150685688946396, 0.3857164310757071, 0.8025839820038527, 0.6614004060393199, 0.38999362103641033, 5.379454461042769, 0.4328683150233701, 0.3938154529314488, 0.3850672410335392, 4.433615178917535, 0.3965304549783468, 0.37236048304475844, 0.7048345549264923, 0.37252267997246236, 0.37265570904128253, 0.43847418390214443, 0.3968162789242342, 0.3579201730899513, 0.41859818692319095, 0.3619057619944215, 0.37753458600491285, 5.484150427044369, 0.3923131379997358, 0.37838219897821546, 0.3792255891021341, 0.7765588139882311, 0.3608146240003407]
[0.2423962773683918, 0.0220000230213057, 0.022117524714285165, 0.20863359510109816, 0.09732998618428425, 0.023308913672475944, 0.20993856924386428, 0.19989344661304614, 0.03182145975511141, 0.008414790366909333, 0.20287247216424012, 0.015306754021107085, 0.017952117182751547, 0.13827093691821685, 0.22248790010201688, 0.17457445602084756, 0.007166610755102366, 0.007734395651507894, 0.02627544885748351, 0.14884278967463394, 0.09111683565305964, 0.1786065995930789, 0.17319946183718513, 0.17270754946737873, 0.19671019696041334, 0.1517252154073355, 0.021233927325478623, 0.08446432644862453, 0.1838179553065411, 0.19580016495916536, 0.027956350530706803, 0.01993395677501602, 0.012414288469495214, 0.21843035720355278, 0.10177584310128752, 0.031296084836904645, 0.03116818606302294, 0.09334871761158717, 0.007957078388188871, 0.007396540447727454, 0.13717552328634322, 0.008156915570666291, 0.007896239018751954, 0.0076578556921105, 0.011275738591746408, 0.008124998756399264, 0.009894564591006053, 0.010318469101734156, 0.00972003081566369, 0.009502197815371411, 0.12009653906166858, 0.007762893592482623, 0.007824696082507774, 0.008855142530852131, 0.02120617418359889, 0.007670389348165873, 0.008601802386989703, 0.008618340347608437, 0.00998736808209547, 0.00944852699972309, 0.008272682409258368, 0.00941055542931949, 0.007817860305480356, 0.07202847802070711, 0.007738846427837044, 0.008937485957974377, 0.007971281366308734, 0.008055962365576807, 0.030819109039932336, 0.008470787120298768, 0.007871763899504227, 0.016379264938854138, 0.013497967470190202, 0.007959053490538987, 0.1097847849192402, 0.0088340472453749, 0.008037050059825485, 0.007858515123133453, 0.09048194242688846, 0.008092458264864221, 0.007599193531525683, 0.01438437867196923, 0.007602503672907395, 0.007605218551862909, 0.008948452732696826, 0.008098291406617025, 0.007304493328366353, 0.008542820141289612, 0.007385831877437173, 0.007704787469488017, 0.11192143728661978, 0.008006390571423178, 0.007722085693432968, 0.007739297736778247, 0.01584813906098431, 0.007363563755108994]
[4.125475897801056, 45.45449788991402, 45.21301605482663, 4.793091925178335, 10.274325921577793, 42.902042285258375, 4.763298157178549, 5.0026652546333885, 31.425333963171575, 118.83837343499857, 4.92920497952244, 65.33063761402715, 55.70373621228382, 7.232177797359327, 4.494626447287571, 5.728214899209427, 139.53597232667286, 129.2925840695828, 38.058341283680484, 6.718498102501111, 10.974920198146945, 5.598897253955394, 5.773689995295959, 5.790134844040975, 5.083620551715698, 6.590862285581918, 47.09444393737274, 11.839317757518026, 5.440164962842536, 5.107247995467994, 35.77004798611379, 50.1656550822533, 80.55234115569587, 4.5781182286311575, 9.825514282449108, 31.952878617608757, 32.08399738046905, 10.712519952988323, 125.67426776696772, 135.1983413147216, 7.289930273585163, 122.59535989267519, 126.64256966198772, 130.5848582430522, 88.68598645342647, 123.07694191490161, 101.0655891729666, 96.91360124651987, 102.8803322710165, 105.2388110024747, 8.326634620890351, 128.8179450209614, 127.80049083765888, 112.92873000247123, 47.15607781687524, 130.37147850116864, 116.25470512, 116.0316208998983, 100.12647894621182, 105.8366028936899, 120.87977641699993, 106.26365335295766, 127.91223697090054, 13.883397615489182, 129.2182251353311, 111.88828767979886, 125.45034531419014, 124.13166231672186, 32.447401341300925, 118.05278373761442, 127.03633045485286, 61.052800826723676, 74.08522818035128, 125.64308069906942, 9.108730328484219, 113.19839844908617, 124.42376152397807, 127.25050271345238, 11.051928961494427, 123.57184520083261, 131.59291125452242, 69.51986059354063, 131.53561550567116, 131.4886604744644, 111.75116300788982, 123.48283727885008, 136.90203482240014, 117.05736319634624, 135.39436269256, 129.7894333828328, 8.93483879624507, 124.90022702230533, 129.49869241290887, 129.2106899115481, 63.09889105288374, 135.80380821802203]
Elapsed: 2.6287934924851775~3.443491330706381
Time per graph: 0.053648846785411784~0.07027533327972206
Speed: 74.07041258022453~51.76964423306592
Total Time: 0.3613
best val loss: 0.1543119251728058 test_score: 0.8980

Testing...
Test loss: 0.5334 score: 0.8571 time: 0.38s
test Score 0.8571
Epoch Time List: [14.723522186046466, 11.71239796304144, 19.443408926017582, 21.994334694114514, 26.963141032028943, 3.7034712319727987, 30.956139978021383, 14.693682887009345, 11.812120805960149, 29.427759002079256, 21.856516817817464, 7.213048678007908, 23.122112475917675, 18.416722221067175, 21.64650156116113, 16.730721357045695, 13.148554442916065, 1.549883002997376, 10.659786793868989, 26.87962238502223, 15.717305910075083, 20.79425092798192, 42.541456757928245, 17.672530901036225, 17.457530553103425, 22.400859385961667, 9.565342831076123, 25.141970356111415, 15.98553065303713, 18.69499948597513, 32.54798067396041, 13.175428448943421, 23.065254900953732, 13.080694509902969, 41.03002535097767, 4.700228482950479, 20.771166550111957, 30.524544834857807, 27.62200514099095, 1.518449618946761, 10.491414599120617, 14.072374499868602, 1.4790085880085826, 1.5730911960126832, 1.5931789659662172, 8.971248990972526, 1.5730717789847404, 1.7035630199825391, 10.95209395303391, 1.9421449588844553, 10.8842375659151, 5.818462549010292, 1.6621327782049775, 1.6195024580229074, 7.54213469394017, 1.5012669109273702, 1.4958135450724512, 1.6301564760506153, 9.086391684832051, 1.762481423211284, 1.4900371271651238, 17.263895227923058, 1.6096780250081792, 7.726920977933332, 3.1670897990697995, 1.6453504089731723, 1.4739737970521674, 1.5657295481069013, 15.366839608061127, 1.5853707620408386, 1.4694319190457463, 2.008432212052867, 16.13383638311643, 2.2514199170982465, 11.57376234815456, 1.6748396371258423, 1.4573933141073212, 1.551146587007679, 5.542630949057639, 12.379657820099965, 1.4542794320732355, 1.997862208983861, 4.483948976150714, 1.6105470900656655, 1.5107719691004604, 10.163371912902221, 1.7398797200294212, 1.6360033740056679, 8.26299884391483, 1.558066020021215, 6.707270680111833, 2.205247833044268, 1.415806228062138, 1.5729412930086255, 9.703884810907766, 1.5011860760860145]
Total Epoch List: [96]
Total Time List: [0.3612922379979864]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba0a00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4898 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.67s
Epoch 2/1000, LR 0.000000
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.43s
Epoch 3/1000, LR 0.000030
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 9.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 6.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 5.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.50s
Epoch 5/1000, LR 0.000090
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.57s
Epoch 6/1000, LR 0.000120
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.48s
Epoch 7/1000, LR 0.000150
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.60s
Epoch 8/1000, LR 0.000180
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 4.30s
Epoch 9/1000, LR 0.000210
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 4.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.48s
Epoch 10/1000, LR 0.000240
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.49s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.63s
Epoch 12/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 7.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.48s
Epoch 13/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.61s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.54s
Epoch 15/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 6.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.59s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.48s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 2.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 5.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 6.92s
Epoch 18/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 3.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.57s
Epoch 19/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.63s
Epoch 20/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5102 time: 0.51s
Epoch 21/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.61s
Val loss: 0.6912 score: 0.5306 time: 0.38s
Test loss: 0.6890 score: 0.5510 time: 0.56s
Epoch 22/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 10.45s
Val loss: 0.6908 score: 0.5918 time: 0.38s
Test loss: 0.6885 score: 0.6531 time: 0.50s
Epoch 23/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.65s
Val loss: 0.6903 score: 0.6531 time: 0.37s
Test loss: 0.6879 score: 0.7755 time: 0.59s
Epoch 24/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.61s
Val loss: 0.6898 score: 0.7143 time: 0.38s
Test loss: 0.6873 score: 0.7959 time: 0.47s
Epoch 25/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.63s
Val loss: 0.6893 score: 0.6531 time: 0.45s
Test loss: 0.6865 score: 0.7755 time: 4.60s
Epoch 26/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 3.74s
Val loss: 0.6887 score: 0.6122 time: 0.40s
Test loss: 0.6857 score: 0.8163 time: 0.48s
Epoch 27/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.62s
Val loss: 0.6880 score: 0.6327 time: 0.39s
Test loss: 0.6847 score: 0.7755 time: 0.52s
Epoch 28/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.80s
Val loss: 0.6872 score: 0.6327 time: 0.39s
Test loss: 0.6836 score: 0.7755 time: 0.50s
Epoch 29/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.64s
Val loss: 0.6864 score: 0.6531 time: 0.40s
Test loss: 0.6824 score: 0.7551 time: 0.52s
Epoch 30/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 5.20s
Val loss: 0.6854 score: 0.6531 time: 0.40s
Test loss: 0.6810 score: 0.6735 time: 0.81s
Epoch 31/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.80s
Val loss: 0.6844 score: 0.6531 time: 0.49s
Test loss: 0.6795 score: 0.6531 time: 0.54s
Epoch 32/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.72s
Val loss: 0.6832 score: 0.6327 time: 0.47s
Test loss: 0.6778 score: 0.6531 time: 0.51s
Epoch 33/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 1.12s
Val loss: 0.6819 score: 0.6531 time: 3.41s
Test loss: 0.6759 score: 0.6531 time: 3.17s
Epoch 34/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.65s
Val loss: 0.6805 score: 0.6531 time: 0.42s
Test loss: 0.6738 score: 0.6122 time: 0.57s
Epoch 35/1000, LR 0.000270
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.77s
Val loss: 0.6790 score: 0.6327 time: 0.38s
Test loss: 0.6715 score: 0.6122 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.79s
Val loss: 0.6773 score: 0.6327 time: 6.53s
Test loss: 0.6691 score: 0.6122 time: 4.36s
Epoch 37/1000, LR 0.000270
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 7.22s
Val loss: 0.6755 score: 0.6327 time: 0.39s
Test loss: 0.6665 score: 0.6122 time: 0.65s
Epoch 38/1000, LR 0.000270
Train loss: 0.6669;  Loss pred: 0.6669; Loss self: 0.0000; time: 0.68s
Val loss: 0.6735 score: 0.6327 time: 0.39s
Test loss: 0.6636 score: 0.6122 time: 0.53s
Epoch 39/1000, LR 0.000269
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.65s
Val loss: 0.6714 score: 0.6327 time: 0.42s
Test loss: 0.6605 score: 0.6122 time: 0.56s
Epoch 40/1000, LR 0.000269
Train loss: 0.6610;  Loss pred: 0.6610; Loss self: 0.0000; time: 5.45s
Val loss: 0.6691 score: 0.6531 time: 0.43s
Test loss: 0.6571 score: 0.6531 time: 0.49s
Epoch 41/1000, LR 0.000269
Train loss: 0.6576;  Loss pred: 0.6576; Loss self: 0.0000; time: 0.63s
Val loss: 0.6665 score: 0.6531 time: 0.38s
Test loss: 0.6535 score: 0.6531 time: 0.50s
Epoch 42/1000, LR 0.000269
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.79s
Val loss: 0.6638 score: 0.6531 time: 4.19s
Test loss: 0.6497 score: 0.6531 time: 3.34s
Epoch 43/1000, LR 0.000269
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 4.84s
Val loss: 0.6609 score: 0.6735 time: 0.43s
Test loss: 0.6456 score: 0.6531 time: 0.64s
Epoch 44/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.95s
Val loss: 0.6577 score: 0.6735 time: 0.40s
Test loss: 0.6411 score: 0.6531 time: 0.51s
Epoch 45/1000, LR 0.000269
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.90s
Val loss: 0.6542 score: 0.6735 time: 4.90s
Test loss: 0.6362 score: 0.6531 time: 6.37s
Epoch 46/1000, LR 0.000269
Train loss: 0.6353;  Loss pred: 0.6353; Loss self: 0.0000; time: 6.01s
Val loss: 0.6505 score: 0.6531 time: 0.55s
Test loss: 0.6309 score: 0.6939 time: 0.51s
Epoch 47/1000, LR 0.000269
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.64s
Val loss: 0.6464 score: 0.6531 time: 0.40s
Test loss: 0.6252 score: 0.6939 time: 0.50s
Epoch 48/1000, LR 0.000269
Train loss: 0.6219;  Loss pred: 0.6219; Loss self: 0.0000; time: 0.73s
Val loss: 0.6420 score: 0.6531 time: 0.47s
Test loss: 0.6190 score: 0.7143 time: 0.50s
Epoch 49/1000, LR 0.000269
Train loss: 0.6167;  Loss pred: 0.6167; Loss self: 0.0000; time: 0.66s
Val loss: 0.6372 score: 0.6531 time: 0.41s
Test loss: 0.6124 score: 0.7347 time: 0.53s
Epoch 50/1000, LR 0.000269
Train loss: 0.6108;  Loss pred: 0.6108; Loss self: 0.0000; time: 9.52s
Val loss: 0.6320 score: 0.6531 time: 0.56s
Test loss: 0.6053 score: 0.7347 time: 0.90s
Epoch 51/1000, LR 0.000269
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 0.57s
Val loss: 0.6265 score: 0.6531 time: 0.47s
Test loss: 0.5977 score: 0.7347 time: 0.49s
Epoch 52/1000, LR 0.000269
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.67s
Val loss: 0.6205 score: 0.6531 time: 0.48s
Test loss: 0.5896 score: 0.7551 time: 0.47s
Epoch 53/1000, LR 0.000269
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.59s
Val loss: 0.6141 score: 0.6735 time: 0.37s
Test loss: 0.5811 score: 0.7551 time: 0.59s
Epoch 54/1000, LR 0.000269
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 12.64s
Val loss: 0.6073 score: 0.6939 time: 0.42s
Test loss: 0.5720 score: 0.7755 time: 0.49s
Epoch 55/1000, LR 0.000269
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.67s
Val loss: 0.6000 score: 0.7143 time: 0.42s
Test loss: 0.5624 score: 0.7755 time: 0.52s
Epoch 56/1000, LR 0.000269
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.79s
Val loss: 0.5922 score: 0.7551 time: 6.53s
Test loss: 0.5522 score: 0.8163 time: 4.84s
Epoch 57/1000, LR 0.000269
Train loss: 0.5402;  Loss pred: 0.5402; Loss self: 0.0000; time: 7.41s
Val loss: 0.5840 score: 0.7755 time: 0.38s
Test loss: 0.5416 score: 0.8163 time: 0.57s
Epoch 58/1000, LR 0.000269
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.61s
Val loss: 0.5754 score: 0.7755 time: 0.36s
Test loss: 0.5304 score: 0.8571 time: 0.48s
Epoch 59/1000, LR 0.000268
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.61s
Val loss: 0.5663 score: 0.7755 time: 0.37s
Test loss: 0.5187 score: 0.8776 time: 0.50s
Epoch 60/1000, LR 0.000268
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 8.24s
Val loss: 0.5570 score: 0.7755 time: 2.66s
Test loss: 0.5067 score: 0.9184 time: 2.35s
Epoch 61/1000, LR 0.000268
Train loss: 0.4951;  Loss pred: 0.4951; Loss self: 0.0000; time: 0.78s
Val loss: 0.5472 score: 0.7959 time: 0.44s
Test loss: 0.4943 score: 0.9184 time: 0.55s
Epoch 62/1000, LR 0.000268
Train loss: 0.4801;  Loss pred: 0.4801; Loss self: 0.0000; time: 0.75s
Val loss: 0.5373 score: 0.7959 time: 0.37s
Test loss: 0.4817 score: 0.9184 time: 0.53s
Epoch 63/1000, LR 0.000268
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.63s
Val loss: 0.5272 score: 0.7959 time: 0.40s
Test loss: 0.4688 score: 0.9184 time: 0.52s
Epoch 64/1000, LR 0.000268
Train loss: 0.4518;  Loss pred: 0.4518; Loss self: 0.0000; time: 0.70s
Val loss: 0.5170 score: 0.8163 time: 0.37s
Test loss: 0.4557 score: 0.9184 time: 0.50s
Epoch 65/1000, LR 0.000268
Train loss: 0.4405;  Loss pred: 0.4405; Loss self: 0.0000; time: 6.25s
Val loss: 0.5067 score: 0.8367 time: 0.39s
Test loss: 0.4427 score: 0.9184 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.4225;  Loss pred: 0.4225; Loss self: 0.0000; time: 0.70s
Val loss: 0.4965 score: 0.8367 time: 0.38s
Test loss: 0.4296 score: 0.9388 time: 0.47s
Epoch 67/1000, LR 0.000268
Train loss: 0.4097;  Loss pred: 0.4097; Loss self: 0.0000; time: 7.96s
Val loss: 0.4863 score: 0.8367 time: 3.21s
Test loss: 0.4167 score: 0.9388 time: 0.54s
Epoch 68/1000, LR 0.000268
Train loss: 0.3924;  Loss pred: 0.3924; Loss self: 0.0000; time: 0.69s
Val loss: 0.4760 score: 0.8571 time: 0.48s
Test loss: 0.4039 score: 0.9592 time: 0.46s
Epoch 69/1000, LR 0.000268
Train loss: 0.3810;  Loss pred: 0.3810; Loss self: 0.0000; time: 0.60s
Val loss: 0.4658 score: 0.8571 time: 0.41s
Test loss: 0.3915 score: 0.9592 time: 6.30s
Epoch 70/1000, LR 0.000268
Train loss: 0.3675;  Loss pred: 0.3675; Loss self: 0.0000; time: 15.14s
Val loss: 0.4559 score: 0.8571 time: 1.85s
Test loss: 0.3794 score: 0.9592 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.3582;  Loss pred: 0.3582; Loss self: 0.0000; time: 0.67s
Val loss: 0.4460 score: 0.8571 time: 0.38s
Test loss: 0.3677 score: 0.9592 time: 0.49s
Epoch 72/1000, LR 0.000267
Train loss: 0.3320;  Loss pred: 0.3320; Loss self: 0.0000; time: 0.68s
Val loss: 0.4364 score: 0.8571 time: 0.37s
Test loss: 0.3564 score: 0.9388 time: 0.50s
Epoch 73/1000, LR 0.000267
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.63s
Val loss: 0.4271 score: 0.8571 time: 4.21s
Test loss: 0.3455 score: 0.9388 time: 0.79s
Epoch 74/1000, LR 0.000267
Train loss: 0.3126;  Loss pred: 0.3126; Loss self: 0.0000; time: 0.70s
Val loss: 0.4181 score: 0.8571 time: 0.47s
Test loss: 0.3350 score: 0.9388 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.2885;  Loss pred: 0.2885; Loss self: 0.0000; time: 0.70s
Val loss: 0.4095 score: 0.8571 time: 0.37s
Test loss: 0.3251 score: 0.9388 time: 0.48s
Epoch 76/1000, LR 0.000267
Train loss: 0.2861;  Loss pred: 0.2861; Loss self: 0.0000; time: 0.62s
Val loss: 0.4013 score: 0.8571 time: 0.46s
Test loss: 0.3160 score: 0.9592 time: 3.42s
Epoch 77/1000, LR 0.000267
Train loss: 0.2825;  Loss pred: 0.2825; Loss self: 0.0000; time: 3.54s
Val loss: 0.3935 score: 0.8571 time: 0.39s
Test loss: 0.3076 score: 0.9592 time: 0.48s
Epoch 78/1000, LR 0.000267
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.72s
Val loss: 0.3860 score: 0.8571 time: 0.60s
Test loss: 0.2997 score: 0.9592 time: 0.50s
Epoch 79/1000, LR 0.000267
Train loss: 0.2518;  Loss pred: 0.2518; Loss self: 0.0000; time: 5.12s
Val loss: 0.3789 score: 0.8571 time: 5.76s
Test loss: 0.2920 score: 0.9592 time: 2.83s
Epoch 80/1000, LR 0.000267
Train loss: 0.2487;  Loss pred: 0.2487; Loss self: 0.0000; time: 1.26s
Val loss: 0.3722 score: 0.8571 time: 0.40s
Test loss: 0.2850 score: 0.9592 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2243;  Loss pred: 0.2243; Loss self: 0.0000; time: 0.60s
Val loss: 0.3658 score: 0.8571 time: 0.42s
Test loss: 0.2783 score: 0.9592 time: 0.65s
Epoch 82/1000, LR 0.000267
Train loss: 0.2197;  Loss pred: 0.2197; Loss self: 0.0000; time: 8.69s
Val loss: 0.3598 score: 0.8571 time: 0.37s
Test loss: 0.2723 score: 0.9388 time: 0.75s
Epoch 83/1000, LR 0.000266
Train loss: 0.2024;  Loss pred: 0.2024; Loss self: 0.0000; time: 0.62s
Val loss: 0.3541 score: 0.8571 time: 0.39s
Test loss: 0.2665 score: 0.9388 time: 0.50s
Epoch 84/1000, LR 0.000266
Train loss: 0.1882;  Loss pred: 0.1882; Loss self: 0.0000; time: 0.77s
Val loss: 0.3487 score: 0.8571 time: 5.21s
Test loss: 0.2612 score: 0.9388 time: 6.10s
Epoch 85/1000, LR 0.000266
Train loss: 0.1858;  Loss pred: 0.1858; Loss self: 0.0000; time: 0.90s
Val loss: 0.3439 score: 0.8776 time: 0.43s
Test loss: 0.2572 score: 0.9388 time: 0.64s
Epoch 86/1000, LR 0.000266
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.62s
Val loss: 0.3393 score: 0.8776 time: 0.37s
Test loss: 0.2532 score: 0.9388 time: 0.50s
Epoch 87/1000, LR 0.000266
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.91s
Val loss: 0.3351 score: 0.8776 time: 4.46s
Test loss: 0.2496 score: 0.9388 time: 4.53s
Epoch 88/1000, LR 0.000266
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 8.64s
Val loss: 0.3314 score: 0.8776 time: 0.41s
Test loss: 0.2467 score: 0.9388 time: 0.48s
Epoch 89/1000, LR 0.000266
Train loss: 0.1453;  Loss pred: 0.1453; Loss self: 0.0000; time: 0.62s
Val loss: 0.3280 score: 0.8776 time: 0.38s
Test loss: 0.2435 score: 0.9388 time: 0.49s
Epoch 90/1000, LR 0.000266
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.80s
Val loss: 0.3250 score: 0.8776 time: 2.66s
Test loss: 0.2406 score: 0.9388 time: 4.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 1.94s
Val loss: 0.3227 score: 0.8776 time: 0.39s
Test loss: 0.2380 score: 0.9388 time: 0.48s
Epoch 92/1000, LR 0.000266
Train loss: 0.1251;  Loss pred: 0.1251; Loss self: 0.0000; time: 0.83s
Val loss: 0.3211 score: 0.8776 time: 0.40s
Test loss: 0.2366 score: 0.9388 time: 0.49s
Epoch 93/1000, LR 0.000265
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 11.29s
Val loss: 0.3202 score: 0.8776 time: 4.77s
Test loss: 0.2357 score: 0.9388 time: 0.66s
Epoch 94/1000, LR 0.000265
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.71s
Val loss: 0.3200 score: 0.8776 time: 0.45s
Test loss: 0.2355 score: 0.9388 time: 0.47s
Epoch 95/1000, LR 0.000265
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.62s
Val loss: 0.3206 score: 0.8776 time: 0.58s
Test loss: 0.2364 score: 0.9388 time: 3.41s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.0836;  Loss pred: 0.0836; Loss self: 0.0000; time: 4.14s
Val loss: 0.3218 score: 0.8776 time: 0.53s
Test loss: 0.2374 score: 0.9388 time: 0.48s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.1008,   Val_Loss: 0.3200,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3200,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2355


[11.877417591051199, 1.0780011280439794, 1.0837587109999731, 10.22304615995381, 4.769169323029928, 1.1421367699513212, 10.28698989294935, 9.79477888403926, 1.559251528000459, 0.41232472797855735, 9.940751136047766, 0.7500309470342472, 0.8796537419548258, 6.775275908992626, 10.901907104998827, 8.554148345021531, 0.351163927000016, 0.37898538692388684, 1.287496994016692, 7.293296694057062, 4.464724946999922, 8.751723380060866, 8.486773630022071, 8.462669923901558, 9.638799651060253, 7.434535554959439, 1.0404624389484525, 4.138751995982602, 9.007079810020514, 9.594208082999103, 1.3698611760046333, 0.9767638819757849, 0.6083001350052655, 10.703087502974086, 4.987016311963089, 1.5335081570083275, 1.5272411170881242, 4.574087162967771, 0.38989684102125466, 0.36243048193864524, 6.721600641030818, 0.3996888629626483, 0.3869157119188458, 0.3752349289134145, 0.552511190995574, 0.39812493906356394, 0.4848336649592966, 0.5056049859849736, 0.47628150996752083, 0.46560769295319915, 5.88473041402176, 0.3803817860316485, 0.3834101080428809, 0.4339019840117544, 1.0391025349963456, 0.3758490780601278, 0.42148831696249545, 0.42229867703281343, 0.489381036022678, 0.4629778229864314, 0.4053614380536601, 0.461117216036655, 0.3830751549685374, 3.5293954230146483, 0.3792034749640152, 0.4379368119407445, 0.3905927869491279, 0.39474215591326356, 1.5101363429566845, 0.4150685688946396, 0.3857164310757071, 0.8025839820038527, 0.6614004060393199, 0.38999362103641033, 5.379454461042769, 0.4328683150233701, 0.3938154529314488, 0.3850672410335392, 4.433615178917535, 0.3965304549783468, 0.37236048304475844, 0.7048345549264923, 0.37252267997246236, 0.37265570904128253, 0.43847418390214443, 0.3968162789242342, 0.3579201730899513, 0.41859818692319095, 0.3619057619944215, 0.37753458600491285, 5.484150427044369, 0.3923131379997358, 0.37838219897821546, 0.3792255891021341, 0.7765588139882311, 0.3608146240003407, 0.6744081309298053, 0.44083207298535854, 5.08154091401957, 0.5013041889760643, 0.5756542490562424, 0.4826867519877851, 0.6053222860209644, 4.300483547034673, 0.4807536529842764, 0.49906196899246424, 0.6357270040316507, 0.4876247899373993, 0.6114035110222176, 0.5461858819471672, 0.592644243966788, 0.4823639140231535, 6.9247741440776736, 0.5716841300018132, 0.6303164530545473, 0.5109775569289923, 0.5687096730107442, 0.5004408009117469, 0.59084801399149, 0.47257305099628866, 4.608954742900096, 0.48890140000730753, 0.5294237530324608, 0.5049727860605344, 0.5242502860492095, 0.8160734550328925, 0.5495451630558819, 0.5124716709833592, 3.1746160690672696, 0.5722223109332845, 0.47427798598073423, 4.367454948020168, 0.6590680370572954, 0.5314730289392173, 0.5690411570249125, 0.4909646500600502, 0.5018627970712259, 3.346357320086099, 0.6448514780495316, 0.5112965289736167, 6.380637426977046, 0.5166775990510359, 0.5017907690489665, 0.5009209850104526, 0.5353170549497008, 0.9078297989908606, 0.4928379449993372, 0.47355214203707874, 0.596419995999895, 0.49521026900038123, 0.5274228060152382, 4.844558174954727, 0.5770906229736283, 0.4888923210091889, 0.5036432929337025, 2.349409253918566, 0.5590146500617266, 0.5307541720103472, 0.5208369540050626, 0.5067776360083371, 0.48754217999521643, 0.4730925440089777, 0.5412824270315468, 0.46246295189484954, 6.304850116954185, 0.47148563305381685, 0.4963257460622117, 0.5081667099148035, 0.7954831500537694, 0.469905928010121, 0.48388858302496374, 3.4218135359697044, 0.48163887101691216, 0.5060569290071726, 2.8336732239695266, 0.4978064229944721, 0.6535982548957691, 0.757643096963875, 0.507043305085972, 6.1059938370017335, 0.644907591980882, 0.5036636739969254, 4.534334135008976, 0.48043726792093366, 0.4967032429995015, 4.255271378904581, 0.48381918598897755, 0.49203253095038235, 0.6655263809952885, 0.4750678250566125, 3.4157686779508367, 0.48647530493326485]
[0.2423962773683918, 0.0220000230213057, 0.022117524714285165, 0.20863359510109816, 0.09732998618428425, 0.023308913672475944, 0.20993856924386428, 0.19989344661304614, 0.03182145975511141, 0.008414790366909333, 0.20287247216424012, 0.015306754021107085, 0.017952117182751547, 0.13827093691821685, 0.22248790010201688, 0.17457445602084756, 0.007166610755102366, 0.007734395651507894, 0.02627544885748351, 0.14884278967463394, 0.09111683565305964, 0.1786065995930789, 0.17319946183718513, 0.17270754946737873, 0.19671019696041334, 0.1517252154073355, 0.021233927325478623, 0.08446432644862453, 0.1838179553065411, 0.19580016495916536, 0.027956350530706803, 0.01993395677501602, 0.012414288469495214, 0.21843035720355278, 0.10177584310128752, 0.031296084836904645, 0.03116818606302294, 0.09334871761158717, 0.007957078388188871, 0.007396540447727454, 0.13717552328634322, 0.008156915570666291, 0.007896239018751954, 0.0076578556921105, 0.011275738591746408, 0.008124998756399264, 0.009894564591006053, 0.010318469101734156, 0.00972003081566369, 0.009502197815371411, 0.12009653906166858, 0.007762893592482623, 0.007824696082507774, 0.008855142530852131, 0.02120617418359889, 0.007670389348165873, 0.008601802386989703, 0.008618340347608437, 0.00998736808209547, 0.00944852699972309, 0.008272682409258368, 0.00941055542931949, 0.007817860305480356, 0.07202847802070711, 0.007738846427837044, 0.008937485957974377, 0.007971281366308734, 0.008055962365576807, 0.030819109039932336, 0.008470787120298768, 0.007871763899504227, 0.016379264938854138, 0.013497967470190202, 0.007959053490538987, 0.1097847849192402, 0.0088340472453749, 0.008037050059825485, 0.007858515123133453, 0.09048194242688846, 0.008092458264864221, 0.007599193531525683, 0.01438437867196923, 0.007602503672907395, 0.007605218551862909, 0.008948452732696826, 0.008098291406617025, 0.007304493328366353, 0.008542820141289612, 0.007385831877437173, 0.007704787469488017, 0.11192143728661978, 0.008006390571423178, 0.007722085693432968, 0.007739297736778247, 0.01584813906098431, 0.007363563755108994, 0.013763431243465416, 0.008996572918068541, 0.10370491661264428, 0.010230697734205395, 0.011748045899106987, 0.009850750040567043, 0.012353516041244171, 0.0877649703476464, 0.009811299040495438, 0.010184938142703352, 0.012974020490441851, 0.009951526325253047, 0.012477622673922807, 0.011146650651983005, 0.012094780489118124, 0.009844161510676602, 0.14132192130770763, 0.011667023061261493, 0.012863601082745863, 0.010428113406714129, 0.011606319857362126, 0.010213077569627489, 0.012058122734520204, 0.009644347979516094, 0.09406030087551216, 0.009977579591985869, 0.010804566388417567, 0.010305567062459886, 0.010698985429575704, 0.016654560306793725, 0.011215207409303712, 0.010458605530272638, 0.06478808304218918, 0.011678006345577235, 0.009679142571035393, 0.08913173363306465, 0.013450368103210111, 0.010846388345698312, 0.011613084837243113, 0.010019686735919391, 0.010242097899412774, 0.06829300653236937, 0.013160234245908807, 0.010434623040277891, 0.13021709034647033, 0.010544440796959917, 0.010240627939774826, 0.010222877245111277, 0.010924837856116344, 0.01852713875491552, 0.010057917244884432, 0.009664329429328138, 0.012171836653059083, 0.010106332020415944, 0.01076373073500486, 0.09886853418274953, 0.011777359652523028, 0.009977394306309978, 0.010278434549667398, 0.04794712763099114, 0.011408462246157686, 0.010831717796129535, 0.010629325591940053, 0.010342400734864024, 0.009949840408065642, 0.009654949877734239, 0.011046580143500956, 0.009438019426425501, 0.1286704105500854, 0.009622155776608507, 0.010129096858412484, 0.010370749181934766, 0.016234350001097332, 0.009589916898165735, 0.009875277204591096, 0.06983292930550417, 0.00982936471463086, 0.010327692428717807, 0.05783006579529646, 0.010159314754989227, 0.013338739895832022, 0.015462104019670918, 0.010347822552774938, 0.12461211912248436, 0.013161379428181266, 0.01027885048973317, 0.0925374313267138, 0.009804842202468035, 0.010136800877540847, 0.086842273038869, 0.009873860938550562, 0.01004148022347719, 0.013582171040720173, 0.009695261735849234, 0.06970956485613952, 0.00992806744761765]
[4.125475897801056, 45.45449788991402, 45.21301605482663, 4.793091925178335, 10.274325921577793, 42.902042285258375, 4.763298157178549, 5.0026652546333885, 31.425333963171575, 118.83837343499857, 4.92920497952244, 65.33063761402715, 55.70373621228382, 7.232177797359327, 4.494626447287571, 5.728214899209427, 139.53597232667286, 129.2925840695828, 38.058341283680484, 6.718498102501111, 10.974920198146945, 5.598897253955394, 5.773689995295959, 5.790134844040975, 5.083620551715698, 6.590862285581918, 47.09444393737274, 11.839317757518026, 5.440164962842536, 5.107247995467994, 35.77004798611379, 50.1656550822533, 80.55234115569587, 4.5781182286311575, 9.825514282449108, 31.952878617608757, 32.08399738046905, 10.712519952988323, 125.67426776696772, 135.1983413147216, 7.289930273585163, 122.59535989267519, 126.64256966198772, 130.5848582430522, 88.68598645342647, 123.07694191490161, 101.0655891729666, 96.91360124651987, 102.8803322710165, 105.2388110024747, 8.326634620890351, 128.8179450209614, 127.80049083765888, 112.92873000247123, 47.15607781687524, 130.37147850116864, 116.25470512, 116.0316208998983, 100.12647894621182, 105.8366028936899, 120.87977641699993, 106.26365335295766, 127.91223697090054, 13.883397615489182, 129.2182251353311, 111.88828767979886, 125.45034531419014, 124.13166231672186, 32.447401341300925, 118.05278373761442, 127.03633045485286, 61.052800826723676, 74.08522818035128, 125.64308069906942, 9.108730328484219, 113.19839844908617, 124.42376152397807, 127.25050271345238, 11.051928961494427, 123.57184520083261, 131.59291125452242, 69.51986059354063, 131.53561550567116, 131.4886604744644, 111.75116300788982, 123.48283727885008, 136.90203482240014, 117.05736319634624, 135.39436269256, 129.7894333828328, 8.93483879624507, 124.90022702230533, 129.49869241290887, 129.2106899115481, 63.09889105288374, 135.80380821802203, 72.65630076619003, 111.15343688168409, 9.642744362209674, 97.7450439823466, 85.12053907416328, 101.51511264440089, 80.94861387327636, 11.394067542424882, 101.92330249771935, 98.18419964743876, 77.07710965438312, 100.4870978899382, 80.14347172798523, 89.71304755317676, 82.6802934455666, 101.5830549829397, 7.076043056495443, 85.7116673850026, 77.73872911383381, 95.8946226415366, 86.15995529070996, 97.9136791219411, 82.93164881604518, 103.68767304165387, 10.631477793415627, 100.22470788438652, 92.5534597179202, 97.0349320846887, 93.46680641658351, 60.04361457636802, 89.1646461366767, 95.61504132701826, 15.434937307047852, 85.63105468586478, 103.31493648956847, 11.21934870151607, 74.34740761937486, 92.19658822160842, 86.10976446094705, 99.80351944688233, 97.63624697019685, 14.642787757865399, 75.98648939785213, 95.83479883652494, 7.679483525083281, 94.83670298460127, 97.65026186685074, 97.81981882627164, 91.53453929205396, 53.97487508613197, 99.42416264248058, 103.47329396339916, 82.15686987129222, 98.94786733504162, 92.90459085416246, 10.11444144758524, 84.90867473727637, 100.22656911209499, 97.29108019006253, 20.856306715517185, 87.65423230784633, 92.32145988490643, 94.0793459895776, 96.68934956552401, 100.50412458770391, 103.5738157798364, 90.52575430671463, 105.95443332105262, 7.771794585288484, 103.92681465737681, 98.72548500407255, 96.42504918949743, 61.59778493948983, 104.27619035898739, 101.26298019615002, 14.319891918398744, 101.73597470765482, 96.82705085400679, 17.29204326932192, 98.43183562246669, 74.96960041274001, 64.67425123565319, 96.63868846802303, 8.024901647142965, 75.97987775193161, 97.28714324610816, 10.806437845344849, 101.99042262488265, 98.65045314400972, 11.515129268350892, 101.27750494193161, 99.58691126652613, 73.62593189276879, 103.14316696602387, 14.345233714537052, 100.72453730559225]
Elapsed: 1.934968386934391~2.773962378030965
Time per graph: 0.03948915075376309~0.05661147710267275
Speed: 75.94619150913682~43.204103174052385
Total Time: 0.4871
best val loss: 0.3200443685054779 test_score: 0.9388

Testing...
Test loss: 0.2572 score: 0.9388 time: 0.46s
test Score 0.9388
Epoch Time List: [14.723522186046466, 11.71239796304144, 19.443408926017582, 21.994334694114514, 26.963141032028943, 3.7034712319727987, 30.956139978021383, 14.693682887009345, 11.812120805960149, 29.427759002079256, 21.856516817817464, 7.213048678007908, 23.122112475917675, 18.416722221067175, 21.64650156116113, 16.730721357045695, 13.148554442916065, 1.549883002997376, 10.659786793868989, 26.87962238502223, 15.717305910075083, 20.79425092798192, 42.541456757928245, 17.672530901036225, 17.457530553103425, 22.400859385961667, 9.565342831076123, 25.141970356111415, 15.98553065303713, 18.69499948597513, 32.54798067396041, 13.175428448943421, 23.065254900953732, 13.080694509902969, 41.03002535097767, 4.700228482950479, 20.771166550111957, 30.524544834857807, 27.62200514099095, 1.518449618946761, 10.491414599120617, 14.072374499868602, 1.4790085880085826, 1.5730911960126832, 1.5931789659662172, 8.971248990972526, 1.5730717789847404, 1.7035630199825391, 10.95209395303391, 1.9421449588844553, 10.8842375659151, 5.818462549010292, 1.6621327782049775, 1.6195024580229074, 7.54213469394017, 1.5012669109273702, 1.4958135450724512, 1.6301564760506153, 9.086391684832051, 1.762481423211284, 1.4900371271651238, 17.263895227923058, 1.6096780250081792, 7.726920977933332, 3.1670897990697995, 1.6453504089731723, 1.4739737970521674, 1.5657295481069013, 15.366839608061127, 1.5853707620408386, 1.4694319190457463, 2.008432212052867, 16.13383638311643, 2.2514199170982465, 11.57376234815456, 1.6748396371258423, 1.4573933141073212, 1.551146587007679, 5.542630949057639, 12.379657820099965, 1.4542794320732355, 1.997862208983861, 4.483948976150714, 1.6105470900656655, 1.5107719691004604, 10.163371912902221, 1.7398797200294212, 1.6360033740056679, 8.26299884391483, 1.558066020021215, 6.707270680111833, 2.205247833044268, 1.415806228062138, 1.5729412930086255, 9.703884810907766, 1.5011860760860145, 2.036858494975604, 1.6640513871097937, 21.48928346799221, 1.5685223489999771, 1.5435587069950998, 1.40093051106669, 1.6058542899554595, 5.416932513937354, 5.108531912090257, 1.4834124590270221, 1.6474818600108847, 8.271497490932234, 1.7599852581042796, 1.6370291369967163, 7.5698085009353235, 1.5459920768626034, 14.519320149091072, 4.199121633078903, 1.6464758779620752, 1.5070697679184377, 1.5463385669281706, 11.328475008835085, 1.6072748880833387, 1.45467803417705, 5.676885107997805, 4.6173199019394815, 1.5387129340087995, 1.6852329320972785, 1.5605195129755884, 6.4100862729828805, 1.8308806160930544, 1.7033293079584837, 7.700263113016263, 1.6426471997983754, 1.6238590179709718, 11.676395004964434, 8.27005241299048, 1.6000218897825107, 1.6356534650549293, 6.370129428920336, 1.4999638150911778, 8.319414583034813, 5.91714943991974, 1.8577988259494305, 12.165697384160012, 7.073591416934505, 1.5415816931053996, 1.6990995241794735, 1.5955590719822794, 10.988747961004265, 1.5281697328900918, 1.6178636200493202, 1.545681563904509, 13.546801091870293, 1.6127982209436595, 12.15484475507401, 8.364258588990197, 1.4617171509889886, 1.4765417298767716, 13.24664222507272, 1.7738860248355195, 1.6444831570843235, 1.5420533349970356, 1.5695249000564218, 7.1209881220711395, 1.5568289699731395, 11.707182215061039, 1.6332054940285161, 7.305181892937981, 17.454704643925652, 1.5480571649968624, 1.5574928780552, 5.630987090873532, 1.6316743950592354, 1.557433722075075, 4.492722371942364, 4.4005500610219315, 1.818751801038161, 13.7090052269632, 2.154716912074946, 1.6676902160979807, 9.818332455935888, 1.5187863359460607, 12.085586620960385, 1.9644974829861894, 1.4882591400528327, 9.902598922024481, 9.520263925893232, 1.4948064219206572, 7.71171980293002, 2.8024553479626775, 1.7192545178113505, 16.71953340503387, 1.6292594451224431, 4.610533980070613, 5.1563435851130635]
Total Epoch List: [96, 96]
Total Time List: [0.3612922379979864, 0.487119092955254]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb611e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.46s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 5.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 12.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.44s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.47s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 3.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 4.36s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 2.89s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 2.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.44s
Epoch 8/1000, LR 0.000180
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.46s
Epoch 9/1000, LR 0.000210
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 1.70s
Epoch 10/1000, LR 0.000240
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 6.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.46s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.56s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 5.80s
Epoch 13/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 5.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.43s
Epoch 14/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.44s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 13.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.45s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.43s
Epoch 17/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.44s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 12.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 3.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.45s
Epoch 19/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.83s
Val loss: 0.6913 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.46s
Epoch 20/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.69s
Val loss: 0.6909 score: 0.5102 time: 0.53s
Test loss: 0.6902 score: 0.5417 time: 4.84s
Epoch 21/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 3.06s
Val loss: 0.6904 score: 0.5918 time: 0.42s
Test loss: 0.6896 score: 0.5625 time: 0.45s
Epoch 22/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.69s
Val loss: 0.6899 score: 0.6122 time: 0.64s
Test loss: 0.6891 score: 0.6667 time: 0.42s
Epoch 23/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.57s
Val loss: 0.6893 score: 0.6327 time: 0.44s
Test loss: 0.6884 score: 0.6667 time: 0.60s
Epoch 24/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 7.08s
Val loss: 0.6887 score: 0.6531 time: 0.48s
Test loss: 0.6877 score: 0.7292 time: 0.51s
Epoch 25/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.71s
Val loss: 0.6880 score: 0.6939 time: 0.43s
Test loss: 0.6869 score: 0.8125 time: 0.48s
Epoch 26/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.71s
Val loss: 0.6872 score: 0.7347 time: 0.44s
Test loss: 0.6861 score: 0.7500 time: 4.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 1.97s
Val loss: 0.6865 score: 0.6939 time: 0.41s
Test loss: 0.6852 score: 0.6875 time: 0.42s
Epoch 28/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 1.08s
Val loss: 0.6856 score: 0.6531 time: 0.43s
Test loss: 0.6842 score: 0.6875 time: 0.43s
Epoch 29/1000, LR 0.000270
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.67s
Val loss: 0.6847 score: 0.6735 time: 0.42s
Test loss: 0.6831 score: 0.7083 time: 0.42s
Epoch 30/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.73s
Val loss: 0.6836 score: 0.6531 time: 0.45s
Test loss: 0.6819 score: 0.7292 time: 0.43s
Epoch 31/1000, LR 0.000270
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 13.12s
Val loss: 0.6825 score: 0.6735 time: 0.42s
Test loss: 0.6805 score: 0.7500 time: 0.43s
Epoch 32/1000, LR 0.000270
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.67s
Val loss: 0.6812 score: 0.6531 time: 0.40s
Test loss: 0.6790 score: 0.7500 time: 0.45s
Epoch 33/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.60s
Val loss: 0.6798 score: 0.6735 time: 0.42s
Test loss: 0.6773 score: 0.7292 time: 3.53s
Epoch 34/1000, LR 0.000270
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 7.59s
Val loss: 0.6782 score: 0.6531 time: 0.50s
Test loss: 0.6754 score: 0.7292 time: 0.42s
Epoch 35/1000, LR 0.000270
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.59s
Val loss: 0.6764 score: 0.6531 time: 0.41s
Test loss: 0.6734 score: 0.7292 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.85s
Val loss: 0.6744 score: 0.6531 time: 0.53s
Test loss: 0.6711 score: 0.7083 time: 0.44s
Epoch 37/1000, LR 0.000270
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 3.54s
Val loss: 0.6723 score: 0.6531 time: 4.12s
Test loss: 0.6686 score: 0.7083 time: 4.00s
Epoch 38/1000, LR 0.000270
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.90s
Val loss: 0.6699 score: 0.6531 time: 0.42s
Test loss: 0.6658 score: 0.6875 time: 0.44s
Epoch 39/1000, LR 0.000269
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.62s
Val loss: 0.6673 score: 0.6531 time: 0.51s
Test loss: 0.6626 score: 0.6875 time: 0.45s
Epoch 40/1000, LR 0.000269
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.65s
Val loss: 0.6645 score: 0.6122 time: 0.65s
Test loss: 0.6593 score: 0.6667 time: 5.11s
Epoch 41/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 1.20s
Val loss: 0.6614 score: 0.6122 time: 0.41s
Test loss: 0.6556 score: 0.6667 time: 0.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.93s
Val loss: 0.6581 score: 0.6122 time: 0.41s
Test loss: 0.6516 score: 0.6667 time: 0.45s
Epoch 43/1000, LR 0.000269
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.66s
Val loss: 0.6545 score: 0.6122 time: 4.80s
Test loss: 0.6474 score: 0.6667 time: 4.79s
Epoch 44/1000, LR 0.000269
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 11.13s
Val loss: 0.6507 score: 0.6122 time: 0.41s
Test loss: 0.6428 score: 0.6667 time: 0.43s
Epoch 45/1000, LR 0.000269
Train loss: 0.6339;  Loss pred: 0.6339; Loss self: 0.0000; time: 0.60s
Val loss: 0.6466 score: 0.6122 time: 0.42s
Test loss: 0.6378 score: 0.6667 time: 0.44s
Epoch 46/1000, LR 0.000269
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.89s
Val loss: 0.6422 score: 0.6122 time: 0.58s
Test loss: 0.6325 score: 0.6667 time: 0.51s
Epoch 47/1000, LR 0.000269
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 10.39s
Val loss: 0.6375 score: 0.6327 time: 0.45s
Test loss: 0.6268 score: 0.6667 time: 0.45s
Epoch 48/1000, LR 0.000269
Train loss: 0.6177;  Loss pred: 0.6177; Loss self: 0.0000; time: 0.80s
Val loss: 0.6325 score: 0.6531 time: 0.45s
Test loss: 0.6208 score: 0.7083 time: 0.46s
Epoch 49/1000, LR 0.000269
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.63s
Val loss: 0.6272 score: 0.6735 time: 0.42s
Test loss: 0.6144 score: 0.7083 time: 0.55s
Epoch 50/1000, LR 0.000269
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 16.23s
Val loss: 0.6215 score: 0.6735 time: 3.51s
Test loss: 0.6075 score: 0.7292 time: 1.11s
Epoch 51/1000, LR 0.000269
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.64s
Val loss: 0.6155 score: 0.6735 time: 0.42s
Test loss: 0.6002 score: 0.7500 time: 0.42s
Epoch 52/1000, LR 0.000269
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.84s
Val loss: 0.6091 score: 0.6531 time: 0.42s
Test loss: 0.5925 score: 0.7500 time: 0.44s
Epoch 53/1000, LR 0.000269
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.61s
Val loss: 0.6023 score: 0.6735 time: 0.43s
Test loss: 0.5843 score: 0.7708 time: 0.58s
Epoch 54/1000, LR 0.000269
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 13.92s
Val loss: 0.5951 score: 0.6735 time: 0.41s
Test loss: 0.5756 score: 0.7708 time: 0.46s
Epoch 55/1000, LR 0.000269
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.59s
Val loss: 0.5873 score: 0.6939 time: 0.43s
Test loss: 0.5664 score: 0.7917 time: 0.49s
Epoch 56/1000, LR 0.000269
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.70s
Val loss: 0.5792 score: 0.6939 time: 0.42s
Test loss: 0.5566 score: 0.7917 time: 0.45s
Epoch 57/1000, LR 0.000269
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 12.98s
Val loss: 0.5706 score: 0.6939 time: 5.42s
Test loss: 0.5464 score: 0.8125 time: 0.42s
Epoch 58/1000, LR 0.000269
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.70s
Val loss: 0.5615 score: 0.7551 time: 0.52s
Test loss: 0.5357 score: 0.8125 time: 0.42s
Epoch 59/1000, LR 0.000268
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.78s
Val loss: 0.5519 score: 0.7755 time: 0.41s
Test loss: 0.5245 score: 0.8542 time: 0.49s
Epoch 60/1000, LR 0.000268
Train loss: 0.5016;  Loss pred: 0.5016; Loss self: 0.0000; time: 0.67s
Val loss: 0.5419 score: 0.7959 time: 3.78s
Test loss: 0.5130 score: 0.8333 time: 5.33s
Epoch 61/1000, LR 0.000268
Train loss: 0.4834;  Loss pred: 0.4834; Loss self: 0.0000; time: 1.66s
Val loss: 0.5316 score: 0.8163 time: 0.46s
Test loss: 0.5013 score: 0.8750 time: 0.63s
Epoch 62/1000, LR 0.000268
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.85s
Val loss: 0.5209 score: 0.8163 time: 0.43s
Test loss: 0.4893 score: 0.8958 time: 0.49s
Epoch 63/1000, LR 0.000268
Train loss: 0.4539;  Loss pred: 0.4539; Loss self: 0.0000; time: 0.83s
Val loss: 0.5101 score: 0.8571 time: 0.45s
Test loss: 0.4774 score: 0.8958 time: 0.49s
Epoch 64/1000, LR 0.000268
Train loss: 0.4539;  Loss pred: 0.4539; Loss self: 0.0000; time: 10.99s
Val loss: 0.4993 score: 0.8571 time: 0.52s
Test loss: 0.4657 score: 0.9167 time: 0.59s
Epoch 65/1000, LR 0.000268
Train loss: 0.4293;  Loss pred: 0.4293; Loss self: 0.0000; time: 0.69s
Val loss: 0.4887 score: 0.8571 time: 0.41s
Test loss: 0.4544 score: 0.9167 time: 0.44s
Epoch 66/1000, LR 0.000268
Train loss: 0.4206;  Loss pred: 0.4206; Loss self: 0.0000; time: 0.85s
Val loss: 0.4785 score: 0.8571 time: 4.42s
Test loss: 0.4437 score: 0.9167 time: 4.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4005;  Loss pred: 0.4005; Loss self: 0.0000; time: 1.59s
Val loss: 0.4685 score: 0.8571 time: 0.51s
Test loss: 0.4329 score: 0.9167 time: 0.45s
Epoch 68/1000, LR 0.000268
Train loss: 0.3889;  Loss pred: 0.3889; Loss self: 0.0000; time: 0.71s
Val loss: 0.4588 score: 0.8571 time: 0.40s
Test loss: 0.4222 score: 0.9375 time: 0.43s
Epoch 69/1000, LR 0.000268
Train loss: 0.3852;  Loss pred: 0.3852; Loss self: 0.0000; time: 0.63s
Val loss: 0.4493 score: 0.8571 time: 0.43s
Test loss: 0.4116 score: 0.9375 time: 5.02s
Epoch 70/1000, LR 0.000268
Train loss: 0.3705;  Loss pred: 0.3705; Loss self: 0.0000; time: 10.43s
Val loss: 0.4401 score: 0.8571 time: 0.41s
Test loss: 0.4012 score: 0.9375 time: 0.43s
Epoch 71/1000, LR 0.000268
Train loss: 0.3584;  Loss pred: 0.3584; Loss self: 0.0000; time: 0.63s
Val loss: 0.4315 score: 0.8571 time: 0.42s
Test loss: 0.3913 score: 0.9375 time: 0.43s
Epoch 72/1000, LR 0.000267
Train loss: 0.3487;  Loss pred: 0.3487; Loss self: 0.0000; time: 0.96s
Val loss: 0.4232 score: 0.8571 time: 6.35s
Test loss: 0.3819 score: 0.9375 time: 5.28s
Epoch 73/1000, LR 0.000267
Train loss: 0.3306;  Loss pred: 0.3306; Loss self: 0.0000; time: 4.40s
Val loss: 0.4155 score: 0.8776 time: 0.43s
Test loss: 0.3732 score: 0.9375 time: 0.47s
Epoch 74/1000, LR 0.000267
Train loss: 0.3150;  Loss pred: 0.3150; Loss self: 0.0000; time: 0.72s
Val loss: 0.4077 score: 0.8776 time: 0.43s
Test loss: 0.3641 score: 0.9375 time: 0.44s
Epoch 75/1000, LR 0.000267
Train loss: 0.3056;  Loss pred: 0.3056; Loss self: 0.0000; time: 0.58s
Val loss: 0.4003 score: 0.8776 time: 0.43s
Test loss: 0.3556 score: 0.9375 time: 0.49s
Epoch 76/1000, LR 0.000267
Train loss: 0.2957;  Loss pred: 0.2957; Loss self: 0.0000; time: 0.81s
Val loss: 0.3933 score: 0.8776 time: 0.41s
Test loss: 0.3475 score: 0.9375 time: 0.47s
Epoch 77/1000, LR 0.000267
Train loss: 0.2829;  Loss pred: 0.2829; Loss self: 0.0000; time: 0.69s
Val loss: 0.3865 score: 0.8980 time: 2.78s
Test loss: 0.3399 score: 0.9375 time: 5.35s
Epoch 78/1000, LR 0.000267
Train loss: 0.2734;  Loss pred: 0.2734; Loss self: 0.0000; time: 3.82s
Val loss: 0.3798 score: 0.8980 time: 0.42s
Test loss: 0.3326 score: 0.9375 time: 0.44s
Epoch 79/1000, LR 0.000267
Train loss: 0.2632;  Loss pred: 0.2632; Loss self: 0.0000; time: 0.74s
Val loss: 0.3733 score: 0.8980 time: 0.42s
Test loss: 0.3263 score: 0.9375 time: 0.44s
Epoch 80/1000, LR 0.000267
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.73s
Val loss: 0.3667 score: 0.9184 time: 7.77s
Test loss: 0.3196 score: 0.9375 time: 5.72s
Epoch 81/1000, LR 0.000267
Train loss: 0.2423;  Loss pred: 0.2423; Loss self: 0.0000; time: 3.99s
Val loss: 0.3604 score: 0.9184 time: 0.43s
Test loss: 0.3132 score: 0.9167 time: 0.53s
Epoch 82/1000, LR 0.000267
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.72s
Val loss: 0.3540 score: 0.9184 time: 0.53s
Test loss: 0.3065 score: 0.8958 time: 0.45s
Epoch 83/1000, LR 0.000266
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 10.35s
Val loss: 0.3475 score: 0.9184 time: 0.42s
Test loss: 0.2992 score: 0.8958 time: 0.49s
Epoch 84/1000, LR 0.000266
Train loss: 0.2108;  Loss pred: 0.2108; Loss self: 0.0000; time: 0.74s
Val loss: 0.3413 score: 0.9184 time: 0.52s
Test loss: 0.2919 score: 0.8958 time: 0.46s
Epoch 85/1000, LR 0.000266
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.60s
Val loss: 0.3354 score: 0.9184 time: 0.44s
Test loss: 0.2853 score: 0.8958 time: 0.47s
Epoch 86/1000, LR 0.000266
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 12.41s
Val loss: 0.3296 score: 0.9184 time: 0.51s
Test loss: 0.2782 score: 0.8958 time: 0.44s
Epoch 87/1000, LR 0.000266
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.62s
Val loss: 0.3242 score: 0.9184 time: 0.43s
Test loss: 0.2719 score: 0.8958 time: 0.49s
Epoch 88/1000, LR 0.000266
Train loss: 0.1800;  Loss pred: 0.1800; Loss self: 0.0000; time: 0.64s
Val loss: 0.3197 score: 0.9184 time: 0.53s
Test loss: 0.2670 score: 0.8958 time: 0.54s
Epoch 89/1000, LR 0.000266
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 8.99s
Val loss: 0.3163 score: 0.9184 time: 0.41s
Test loss: 0.2634 score: 0.8958 time: 0.48s
Epoch 90/1000, LR 0.000266
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.70s
Val loss: 0.3138 score: 0.9184 time: 0.67s
Test loss: 0.2605 score: 0.8958 time: 0.42s
Epoch 91/1000, LR 0.000266
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.90s
Val loss: 0.3131 score: 0.8980 time: 0.46s
Test loss: 0.2589 score: 0.8958 time: 0.41s
Epoch 92/1000, LR 0.000266
Train loss: 0.1386;  Loss pred: 0.1386; Loss self: 0.0000; time: 0.74s
Val loss: 0.3140 score: 0.8980 time: 0.47s
Test loss: 0.2578 score: 0.8958 time: 5.47s
     INFO: Early stopping counter 1 of 2
Epoch 93/1000, LR 0.000265
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 11.07s
Val loss: 0.3164 score: 0.8980 time: 0.41s
Test loss: 0.2577 score: 0.8958 time: 0.57s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1556,   Val_Loss: 0.3131,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3131,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2589


[11.877417591051199, 1.0780011280439794, 1.0837587109999731, 10.22304615995381, 4.769169323029928, 1.1421367699513212, 10.28698989294935, 9.79477888403926, 1.559251528000459, 0.41232472797855735, 9.940751136047766, 0.7500309470342472, 0.8796537419548258, 6.775275908992626, 10.901907104998827, 8.554148345021531, 0.351163927000016, 0.37898538692388684, 1.287496994016692, 7.293296694057062, 4.464724946999922, 8.751723380060866, 8.486773630022071, 8.462669923901558, 9.638799651060253, 7.434535554959439, 1.0404624389484525, 4.138751995982602, 9.007079810020514, 9.594208082999103, 1.3698611760046333, 0.9767638819757849, 0.6083001350052655, 10.703087502974086, 4.987016311963089, 1.5335081570083275, 1.5272411170881242, 4.574087162967771, 0.38989684102125466, 0.36243048193864524, 6.721600641030818, 0.3996888629626483, 0.3869157119188458, 0.3752349289134145, 0.552511190995574, 0.39812493906356394, 0.4848336649592966, 0.5056049859849736, 0.47628150996752083, 0.46560769295319915, 5.88473041402176, 0.3803817860316485, 0.3834101080428809, 0.4339019840117544, 1.0391025349963456, 0.3758490780601278, 0.42148831696249545, 0.42229867703281343, 0.489381036022678, 0.4629778229864314, 0.4053614380536601, 0.461117216036655, 0.3830751549685374, 3.5293954230146483, 0.3792034749640152, 0.4379368119407445, 0.3905927869491279, 0.39474215591326356, 1.5101363429566845, 0.4150685688946396, 0.3857164310757071, 0.8025839820038527, 0.6614004060393199, 0.38999362103641033, 5.379454461042769, 0.4328683150233701, 0.3938154529314488, 0.3850672410335392, 4.433615178917535, 0.3965304549783468, 0.37236048304475844, 0.7048345549264923, 0.37252267997246236, 0.37265570904128253, 0.43847418390214443, 0.3968162789242342, 0.3579201730899513, 0.41859818692319095, 0.3619057619944215, 0.37753458600491285, 5.484150427044369, 0.3923131379997358, 0.37838219897821546, 0.3792255891021341, 0.7765588139882311, 0.3608146240003407, 0.6744081309298053, 0.44083207298535854, 5.08154091401957, 0.5013041889760643, 0.5756542490562424, 0.4826867519877851, 0.6053222860209644, 4.300483547034673, 0.4807536529842764, 0.49906196899246424, 0.6357270040316507, 0.4876247899373993, 0.6114035110222176, 0.5461858819471672, 0.592644243966788, 0.4823639140231535, 6.9247741440776736, 0.5716841300018132, 0.6303164530545473, 0.5109775569289923, 0.5687096730107442, 0.5004408009117469, 0.59084801399149, 0.47257305099628866, 4.608954742900096, 0.48890140000730753, 0.5294237530324608, 0.5049727860605344, 0.5242502860492095, 0.8160734550328925, 0.5495451630558819, 0.5124716709833592, 3.1746160690672696, 0.5722223109332845, 0.47427798598073423, 4.367454948020168, 0.6590680370572954, 0.5314730289392173, 0.5690411570249125, 0.4909646500600502, 0.5018627970712259, 3.346357320086099, 0.6448514780495316, 0.5112965289736167, 6.380637426977046, 0.5166775990510359, 0.5017907690489665, 0.5009209850104526, 0.5353170549497008, 0.9078297989908606, 0.4928379449993372, 0.47355214203707874, 0.596419995999895, 0.49521026900038123, 0.5274228060152382, 4.844558174954727, 0.5770906229736283, 0.4888923210091889, 0.5036432929337025, 2.349409253918566, 0.5590146500617266, 0.5307541720103472, 0.5208369540050626, 0.5067776360083371, 0.48754217999521643, 0.4730925440089777, 0.5412824270315468, 0.46246295189484954, 6.304850116954185, 0.47148563305381685, 0.4963257460622117, 0.5081667099148035, 0.7954831500537694, 0.469905928010121, 0.48388858302496374, 3.4218135359697044, 0.48163887101691216, 0.5060569290071726, 2.8336732239695266, 0.4978064229944721, 0.6535982548957691, 0.757643096963875, 0.507043305085972, 6.1059938370017335, 0.644907591980882, 0.5036636739969254, 4.534334135008976, 0.48043726792093366, 0.4967032429995015, 4.255271378904581, 0.48381918598897755, 0.49203253095038235, 0.6655263809952885, 0.4750678250566125, 3.4157686779508367, 0.48647530493326485, 0.461925478070043, 5.106999431038275, 0.44222704600542784, 0.47534689004532993, 4.360553800011985, 2.8956173839978874, 0.44809853902552277, 0.4678021490108222, 1.703922949032858, 0.460584998014383, 0.5630921080010012, 5.810272969887592, 0.4306513140909374, 0.4438762729987502, 0.451211686944589, 0.4392075149808079, 0.4395825630053878, 0.45203932805452496, 0.4680546009913087, 4.844823530991562, 0.4575733670499176, 0.4235198419773951, 0.6027727780165151, 0.5164593720110133, 0.48081620200537145, 4.245397397084162, 0.4280355320079252, 0.4309679380385205, 0.42562457302119583, 0.4372207219712436, 0.43789266201201826, 0.450398400076665, 3.5362556820036843, 0.42543591605499387, 0.4718014139216393, 0.45029850699938834, 4.006762332981452, 0.44437827391084284, 0.4581561170052737, 5.116848194040358, 0.4454024439910427, 0.4507413760293275, 4.791218816069886, 0.43234326399397105, 0.44632077391725034, 0.5170473109465092, 0.4584422450279817, 0.4686491140164435, 0.5510688680224121, 1.1158515060087666, 0.42920089105609804, 0.4430896920384839, 0.5872723390348256, 0.46542011201381683, 0.49175256094895303, 0.4563170309411362, 0.42006748099811375, 0.4230061430716887, 0.49105142999906093, 5.333311497000977, 0.6336644600378349, 0.49450014997273684, 0.49915554199833423, 0.5908501528901979, 0.4475046779261902, 4.449101040023379, 0.4523625309811905, 0.4312671199440956, 5.0270251899492, 0.4389828440034762, 0.4307633239077404, 5.28729549900163, 0.4750244750175625, 0.4460065671009943, 0.4912838019663468, 0.47080312098842114, 5.357361544971354, 0.44183231703937054, 0.4419419049518183, 5.721943640965037, 0.5327848009765148, 0.4558019229443744, 0.4939707850571722, 0.46405824506655335, 0.4785535919945687, 0.44492162810638547, 0.4919559929985553, 0.5461278989678249, 0.4855531849898398, 0.42587211506906897, 0.4146306930342689, 5.478669801028445, 0.5699582030065358]
[0.2423962773683918, 0.0220000230213057, 0.022117524714285165, 0.20863359510109816, 0.09732998618428425, 0.023308913672475944, 0.20993856924386428, 0.19989344661304614, 0.03182145975511141, 0.008414790366909333, 0.20287247216424012, 0.015306754021107085, 0.017952117182751547, 0.13827093691821685, 0.22248790010201688, 0.17457445602084756, 0.007166610755102366, 0.007734395651507894, 0.02627544885748351, 0.14884278967463394, 0.09111683565305964, 0.1786065995930789, 0.17319946183718513, 0.17270754946737873, 0.19671019696041334, 0.1517252154073355, 0.021233927325478623, 0.08446432644862453, 0.1838179553065411, 0.19580016495916536, 0.027956350530706803, 0.01993395677501602, 0.012414288469495214, 0.21843035720355278, 0.10177584310128752, 0.031296084836904645, 0.03116818606302294, 0.09334871761158717, 0.007957078388188871, 0.007396540447727454, 0.13717552328634322, 0.008156915570666291, 0.007896239018751954, 0.0076578556921105, 0.011275738591746408, 0.008124998756399264, 0.009894564591006053, 0.010318469101734156, 0.00972003081566369, 0.009502197815371411, 0.12009653906166858, 0.007762893592482623, 0.007824696082507774, 0.008855142530852131, 0.02120617418359889, 0.007670389348165873, 0.008601802386989703, 0.008618340347608437, 0.00998736808209547, 0.00944852699972309, 0.008272682409258368, 0.00941055542931949, 0.007817860305480356, 0.07202847802070711, 0.007738846427837044, 0.008937485957974377, 0.007971281366308734, 0.008055962365576807, 0.030819109039932336, 0.008470787120298768, 0.007871763899504227, 0.016379264938854138, 0.013497967470190202, 0.007959053490538987, 0.1097847849192402, 0.0088340472453749, 0.008037050059825485, 0.007858515123133453, 0.09048194242688846, 0.008092458264864221, 0.007599193531525683, 0.01438437867196923, 0.007602503672907395, 0.007605218551862909, 0.008948452732696826, 0.008098291406617025, 0.007304493328366353, 0.008542820141289612, 0.007385831877437173, 0.007704787469488017, 0.11192143728661978, 0.008006390571423178, 0.007722085693432968, 0.007739297736778247, 0.01584813906098431, 0.007363563755108994, 0.013763431243465416, 0.008996572918068541, 0.10370491661264428, 0.010230697734205395, 0.011748045899106987, 0.009850750040567043, 0.012353516041244171, 0.0877649703476464, 0.009811299040495438, 0.010184938142703352, 0.012974020490441851, 0.009951526325253047, 0.012477622673922807, 0.011146650651983005, 0.012094780489118124, 0.009844161510676602, 0.14132192130770763, 0.011667023061261493, 0.012863601082745863, 0.010428113406714129, 0.011606319857362126, 0.010213077569627489, 0.012058122734520204, 0.009644347979516094, 0.09406030087551216, 0.009977579591985869, 0.010804566388417567, 0.010305567062459886, 0.010698985429575704, 0.016654560306793725, 0.011215207409303712, 0.010458605530272638, 0.06478808304218918, 0.011678006345577235, 0.009679142571035393, 0.08913173363306465, 0.013450368103210111, 0.010846388345698312, 0.011613084837243113, 0.010019686735919391, 0.010242097899412774, 0.06829300653236937, 0.013160234245908807, 0.010434623040277891, 0.13021709034647033, 0.010544440796959917, 0.010240627939774826, 0.010222877245111277, 0.010924837856116344, 0.01852713875491552, 0.010057917244884432, 0.009664329429328138, 0.012171836653059083, 0.010106332020415944, 0.01076373073500486, 0.09886853418274953, 0.011777359652523028, 0.009977394306309978, 0.010278434549667398, 0.04794712763099114, 0.011408462246157686, 0.010831717796129535, 0.010629325591940053, 0.010342400734864024, 0.009949840408065642, 0.009654949877734239, 0.011046580143500956, 0.009438019426425501, 0.1286704105500854, 0.009622155776608507, 0.010129096858412484, 0.010370749181934766, 0.016234350001097332, 0.009589916898165735, 0.009875277204591096, 0.06983292930550417, 0.00982936471463086, 0.010327692428717807, 0.05783006579529646, 0.010159314754989227, 0.013338739895832022, 0.015462104019670918, 0.010347822552774938, 0.12461211912248436, 0.013161379428181266, 0.01027885048973317, 0.0925374313267138, 0.009804842202468035, 0.010136800877540847, 0.086842273038869, 0.009873860938550562, 0.01004148022347719, 0.013582171040720173, 0.009695261735849234, 0.06970956485613952, 0.00992806744761765, 0.009623447459792564, 0.10639582147996407, 0.009213063458446413, 0.009903060209277706, 0.09084487083358302, 0.060325362166622654, 0.009335386229698392, 0.009745878104392128, 0.03549839477151787, 0.009595520791966313, 0.011731085583354192, 0.12104735353932483, 0.008971902376894528, 0.00924742235414063, 0.009400243478012271, 0.009150156562100165, 0.009157970062612245, 0.009417486001135936, 0.009751137520652264, 0.1009338235623242, 0.009532778480206616, 0.008823330041195732, 0.012557766208677398, 0.010759570250229444, 0.010017004208445238, 0.08844577910592004, 0.008917406916831775, 0.008978498709135843, 0.008867178604608247, 0.009108765041067576, 0.009122763791917047, 0.009383300001597187, 0.07367199337507675, 0.008863248251145706, 0.009829196123367486, 0.00938121889582059, 0.08347421527044692, 0.009257880706475893, 0.009544919104276536, 0.10660100404250746, 0.009279217583146723, 0.009390445333944323, 0.09981705866812263, 0.00900715133320773, 0.009298349456609381, 0.010771818978052275, 0.009550880104749618, 0.009763523208675906, 0.011480601417133585, 0.023246906375182636, 0.008941685230335375, 0.009231035250801748, 0.012234840396558866, 0.009696252333621183, 0.010244845019769855, 0.00950660481127367, 0.00875140585412737, 0.008812627980660181, 0.010230238124980437, 0.11111065618752036, 0.013201342917454895, 0.01030208645776535, 0.010399073791631963, 0.012309378185212458, 0.009323014123462295, 0.09268960500048706, 0.009424219395441469, 0.008984731665501991, 0.104729691457275, 0.009145475916739088, 0.008974235914744591, 0.11015198956253396, 0.009896343229532553, 0.009291803481270714, 0.010235079207632225, 0.00980839835392544, 0.11161169885356988, 0.00920483993832022, 0.009207123019829547, 0.1192071591867716, 0.011099683353677392, 0.009495873394674467, 0.010291058022024421, 0.009667880105553195, 0.009969866499886848, 0.009269200585549697, 0.010249083187469902, 0.011377664561829684, 0.010115691353954995, 0.008872335730605604, 0.008638139438213935, 0.11413895418809261, 0.011874129229302829]
[4.125475897801056, 45.45449788991402, 45.21301605482663, 4.793091925178335, 10.274325921577793, 42.902042285258375, 4.763298157178549, 5.0026652546333885, 31.425333963171575, 118.83837343499857, 4.92920497952244, 65.33063761402715, 55.70373621228382, 7.232177797359327, 4.494626447287571, 5.728214899209427, 139.53597232667286, 129.2925840695828, 38.058341283680484, 6.718498102501111, 10.974920198146945, 5.598897253955394, 5.773689995295959, 5.790134844040975, 5.083620551715698, 6.590862285581918, 47.09444393737274, 11.839317757518026, 5.440164962842536, 5.107247995467994, 35.77004798611379, 50.1656550822533, 80.55234115569587, 4.5781182286311575, 9.825514282449108, 31.952878617608757, 32.08399738046905, 10.712519952988323, 125.67426776696772, 135.1983413147216, 7.289930273585163, 122.59535989267519, 126.64256966198772, 130.5848582430522, 88.68598645342647, 123.07694191490161, 101.0655891729666, 96.91360124651987, 102.8803322710165, 105.2388110024747, 8.326634620890351, 128.8179450209614, 127.80049083765888, 112.92873000247123, 47.15607781687524, 130.37147850116864, 116.25470512, 116.0316208998983, 100.12647894621182, 105.8366028936899, 120.87977641699993, 106.26365335295766, 127.91223697090054, 13.883397615489182, 129.2182251353311, 111.88828767979886, 125.45034531419014, 124.13166231672186, 32.447401341300925, 118.05278373761442, 127.03633045485286, 61.052800826723676, 74.08522818035128, 125.64308069906942, 9.108730328484219, 113.19839844908617, 124.42376152397807, 127.25050271345238, 11.051928961494427, 123.57184520083261, 131.59291125452242, 69.51986059354063, 131.53561550567116, 131.4886604744644, 111.75116300788982, 123.48283727885008, 136.90203482240014, 117.05736319634624, 135.39436269256, 129.7894333828328, 8.93483879624507, 124.90022702230533, 129.49869241290887, 129.2106899115481, 63.09889105288374, 135.80380821802203, 72.65630076619003, 111.15343688168409, 9.642744362209674, 97.7450439823466, 85.12053907416328, 101.51511264440089, 80.94861387327636, 11.394067542424882, 101.92330249771935, 98.18419964743876, 77.07710965438312, 100.4870978899382, 80.14347172798523, 89.71304755317676, 82.6802934455666, 101.5830549829397, 7.076043056495443, 85.7116673850026, 77.73872911383381, 95.8946226415366, 86.15995529070996, 97.9136791219411, 82.93164881604518, 103.68767304165387, 10.631477793415627, 100.22470788438652, 92.5534597179202, 97.0349320846887, 93.46680641658351, 60.04361457636802, 89.1646461366767, 95.61504132701826, 15.434937307047852, 85.63105468586478, 103.31493648956847, 11.21934870151607, 74.34740761937486, 92.19658822160842, 86.10976446094705, 99.80351944688233, 97.63624697019685, 14.642787757865399, 75.98648939785213, 95.83479883652494, 7.679483525083281, 94.83670298460127, 97.65026186685074, 97.81981882627164, 91.53453929205396, 53.97487508613197, 99.42416264248058, 103.47329396339916, 82.15686987129222, 98.94786733504162, 92.90459085416246, 10.11444144758524, 84.90867473727637, 100.22656911209499, 97.29108019006253, 20.856306715517185, 87.65423230784633, 92.32145988490643, 94.0793459895776, 96.68934956552401, 100.50412458770391, 103.5738157798364, 90.52575430671463, 105.95443332105262, 7.771794585288484, 103.92681465737681, 98.72548500407255, 96.42504918949743, 61.59778493948983, 104.27619035898739, 101.26298019615002, 14.319891918398744, 101.73597470765482, 96.82705085400679, 17.29204326932192, 98.43183562246669, 74.96960041274001, 64.67425123565319, 96.63868846802303, 8.024901647142965, 75.97987775193161, 97.28714324610816, 10.806437845344849, 101.99042262488265, 98.65045314400972, 11.515129268350892, 101.27750494193161, 99.58691126652613, 73.62593189276879, 103.14316696602387, 14.345233714537052, 100.72453730559225, 103.91286534041673, 9.39886535100737, 108.5415295911387, 100.97888721944227, 11.007776122351265, 16.576775738833256, 107.11929591287067, 102.60748075120442, 28.17028788023817, 104.21529187214445, 85.24360281020783, 8.261229764722849, 111.45908169657693, 108.13824238840347, 106.38022327177583, 109.28774750609051, 109.19450414918228, 106.18545117873072, 102.55213793078666, 9.907481602364188, 104.90121028998523, 113.3358941953939, 79.63199691590066, 92.94051497815866, 99.83024656782212, 11.306362045863485, 112.14022297361815, 111.37719482907262, 112.77544353062912, 109.78436654051589, 109.61590399677127, 106.57231462596145, 13.573679144377545, 112.82545311429534, 101.73771969232003, 106.5959563576017, 11.979747240032317, 108.01608183398815, 104.767781588841, 9.380774683897416, 107.76770681789368, 106.49122213461246, 10.018327662056805, 111.02289314416, 107.5459687406336, 92.83483152079637, 104.70239276720724, 102.42204362369887, 87.10345073975007, 43.01647642318358, 111.83574172432516, 108.33021138264492, 81.73380016311917, 103.13262955549938, 97.61006614255884, 105.19002523531033, 114.26735505911617, 113.47352937109764, 97.74943532919107, 9.000036848961722, 75.74986925593714, 97.06771575831954, 96.1624102335624, 81.23887209845606, 107.26144857845921, 10.788696315997303, 106.10958404509384, 111.29992939462242, 9.548390586140084, 109.34368086516815, 111.4300993978784, 9.078365302083753, 101.04742497368237, 107.62173371570742, 97.70320089504605, 101.95344478436562, 8.9596342522477, 108.63849960464265, 108.61156061956399, 8.388757913719076, 90.09266013599334, 105.30890192373732, 97.171738596736, 103.43529182013786, 100.30224577343633, 107.88416873392069, 97.56970274400328, 87.89149957495198, 98.85631787381726, 112.7098917763499, 115.7656700441924, 8.761250767657057, 84.21670176304065]
Elapsed: 1.720896630630861~2.4936711394317843
Time per graph: 0.03529777944335328~0.051022187779707996
Speed: 78.79689086835891~41.59415616097394
Total Time: 0.5711
best val loss: 0.31310203671455383 test_score: 0.8958

Testing...
Test loss: 0.3196 score: 0.9375 time: 0.83s
test Score 0.9375
Epoch Time List: [14.723522186046466, 11.71239796304144, 19.443408926017582, 21.994334694114514, 26.963141032028943, 3.7034712319727987, 30.956139978021383, 14.693682887009345, 11.812120805960149, 29.427759002079256, 21.856516817817464, 7.213048678007908, 23.122112475917675, 18.416722221067175, 21.64650156116113, 16.730721357045695, 13.148554442916065, 1.549883002997376, 10.659786793868989, 26.87962238502223, 15.717305910075083, 20.79425092798192, 42.541456757928245, 17.672530901036225, 17.457530553103425, 22.400859385961667, 9.565342831076123, 25.141970356111415, 15.98553065303713, 18.69499948597513, 32.54798067396041, 13.175428448943421, 23.065254900953732, 13.080694509902969, 41.03002535097767, 4.700228482950479, 20.771166550111957, 30.524544834857807, 27.62200514099095, 1.518449618946761, 10.491414599120617, 14.072374499868602, 1.4790085880085826, 1.5730911960126832, 1.5931789659662172, 8.971248990972526, 1.5730717789847404, 1.7035630199825391, 10.95209395303391, 1.9421449588844553, 10.8842375659151, 5.818462549010292, 1.6621327782049775, 1.6195024580229074, 7.54213469394017, 1.5012669109273702, 1.4958135450724512, 1.6301564760506153, 9.086391684832051, 1.762481423211284, 1.4900371271651238, 17.263895227923058, 1.6096780250081792, 7.726920977933332, 3.1670897990697995, 1.6453504089731723, 1.4739737970521674, 1.5657295481069013, 15.366839608061127, 1.5853707620408386, 1.4694319190457463, 2.008432212052867, 16.13383638311643, 2.2514199170982465, 11.57376234815456, 1.6748396371258423, 1.4573933141073212, 1.551146587007679, 5.542630949057639, 12.379657820099965, 1.4542794320732355, 1.997862208983861, 4.483948976150714, 1.6105470900656655, 1.5107719691004604, 10.163371912902221, 1.7398797200294212, 1.6360033740056679, 8.26299884391483, 1.558066020021215, 6.707270680111833, 2.205247833044268, 1.415806228062138, 1.5729412930086255, 9.703884810907766, 1.5011860760860145, 2.036858494975604, 1.6640513871097937, 21.48928346799221, 1.5685223489999771, 1.5435587069950998, 1.40093051106669, 1.6058542899554595, 5.416932513937354, 5.108531912090257, 1.4834124590270221, 1.6474818600108847, 8.271497490932234, 1.7599852581042796, 1.6370291369967163, 7.5698085009353235, 1.5459920768626034, 14.519320149091072, 4.199121633078903, 1.6464758779620752, 1.5070697679184377, 1.5463385669281706, 11.328475008835085, 1.6072748880833387, 1.45467803417705, 5.676885107997805, 4.6173199019394815, 1.5387129340087995, 1.6852329320972785, 1.5605195129755884, 6.4100862729828805, 1.8308806160930544, 1.7033293079584837, 7.700263113016263, 1.6426471997983754, 1.6238590179709718, 11.676395004964434, 8.27005241299048, 1.6000218897825107, 1.6356534650549293, 6.370129428920336, 1.4999638150911778, 8.319414583034813, 5.91714943991974, 1.8577988259494305, 12.165697384160012, 7.073591416934505, 1.5415816931053996, 1.6990995241794735, 1.5955590719822794, 10.988747961004265, 1.5281697328900918, 1.6178636200493202, 1.545681563904509, 13.546801091870293, 1.6127982209436595, 12.15484475507401, 8.364258588990197, 1.4617171509889886, 1.4765417298767716, 13.24664222507272, 1.7738860248355195, 1.6444831570843235, 1.5420533349970356, 1.5695249000564218, 7.1209881220711395, 1.5568289699731395, 11.707182215061039, 1.6332054940285161, 7.305181892937981, 17.454704643925652, 1.5480571649968624, 1.5574928780552, 5.630987090873532, 1.6316743950592354, 1.557433722075075, 4.492722371942364, 4.4005500610219315, 1.818751801038161, 13.7090052269632, 2.154716912074946, 1.6676902160979807, 9.818332455935888, 1.5187863359460607, 12.085586620960385, 1.9644974829861894, 1.4882591400528327, 9.902598922024481, 9.520263925893232, 1.4948064219206572, 7.71171980293002, 2.8024553479626775, 1.7192545178113505, 16.71953340503387, 1.6292594451224431, 4.610533980070613, 5.1563435851130635, 1.6447975750779733, 6.311878567910753, 13.439094788976945, 1.7659342200495303, 9.005010058986954, 4.123086792998947, 3.4798589079873636, 1.5481266090646386, 2.7143429400166497, 7.377802858012728, 1.561858030850999, 7.260210325126536, 6.005474352976307, 1.72111619298812, 14.044224359095097, 1.539057117071934, 1.4627363290637732, 16.021053425152786, 1.8401682650437579, 6.058144830050878, 3.92497934109997, 1.74982657993678, 1.6072565680369735, 8.069814852089621, 1.6178161029238254, 5.3901284269522876, 2.8093535050284117, 1.9275063500972465, 1.5090171019546688, 1.6125413799891248, 13.97425742901396, 1.514175993972458, 4.547392574953847, 8.506730814930052, 1.4746439908631146, 1.8245579870417714, 11.670700562070124, 1.7520135999657214, 1.5766278367955238, 6.407084441045299, 2.0439101490192115, 1.783294747932814, 10.252496646135114, 11.967280069133267, 1.4641049719648436, 1.9800849319435656, 11.287792023969814, 1.7124030799604952, 1.5982553280191496, 20.847904301946983, 1.4825192710850388, 1.6944544239668176, 1.621101240045391, 14.787432481069118, 1.505684772040695, 1.5690895080333576, 18.816539943101816, 1.6431163880042732, 1.669262091978453, 9.77611256588716, 2.749813394038938, 1.771996570052579, 1.7720601689070463, 12.10285452613607, 1.5444698900682852, 9.715881695970893, 2.5413240930065513, 1.5417967221001163, 6.090186509070918, 11.275001146714203, 1.4813706419663504, 12.594409326906316, 5.303796902881004, 1.5895210319431499, 1.4957362330751494, 1.6797674820991233, 8.821048271027394, 4.6803858919302, 1.5916474659461528, 14.220095832948573, 4.951041846070439, 1.7015920450212434, 11.25850563601125, 1.7163740328978747, 1.517979271011427, 13.365057405084372, 1.5400186408078298, 1.7058493290096521, 9.877314767916687, 1.7925188390072435, 1.7695823379326612, 6.6773012791527435, 12.044060353073291]
Total Epoch List: [96, 96, 93]
Total Time List: [0.3612922379979864, 0.487119092955254, 0.5711031389655545]
========================training times:4========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb60130>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.37s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 12.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 1.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.52s
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.40s
Epoch 4/1000, LR 0.000060
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.53s
Epoch 5/1000, LR 0.000090
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 4.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 9.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 1.63s
Epoch 6/1000, LR 0.000120
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.51s
Epoch 7/1000, LR 0.000150
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.39s
Epoch 8/1000, LR 0.000180
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.43s
Epoch 9/1000, LR 0.000210
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 10.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.41s
Epoch 10/1000, LR 0.000240
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 9.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 9.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 4.55s
Epoch 11/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 1.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 19.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.73s
Epoch 13/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 11.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 8.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 6.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 13.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 1.44s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 11.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 1.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 2.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 8.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 4.16s
Epoch 17/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 2.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 0.84s
Epoch 18/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 16.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 1.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5102 time: 6.93s
Epoch 19/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 22.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 9.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 5.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 3.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 10.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 6.18s
Epoch 21/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 2.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4898 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5102 time: 9.36s
Epoch 22/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 13.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.4898 time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 0.63s
Epoch 23/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 15.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5102 time: 8.61s
Epoch 24/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 20.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4898 time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5102 time: 0.65s
Epoch 25/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 2.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.4898 time: 9.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5102 time: 9.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 8.63s
Val loss: 0.6831 score: 0.5102 time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5102 time: 7.66s
Epoch 27/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 10.94s
Val loss: 0.6817 score: 0.5102 time: 0.61s
Test loss: 0.6859 score: 0.5306 time: 4.73s
Epoch 28/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 23.87s
Val loss: 0.6802 score: 0.5306 time: 1.34s
Test loss: 0.6851 score: 0.5510 time: 1.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 22.82s
Val loss: 0.6786 score: 0.5306 time: 1.34s
Test loss: 0.6841 score: 0.5510 time: 8.58s
Epoch 30/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 6.54s
Val loss: 0.6769 score: 0.5714 time: 1.31s
Test loss: 0.6831 score: 0.5510 time: 9.67s
Epoch 31/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 9.95s
Val loss: 0.6751 score: 0.5918 time: 0.56s
Test loss: 0.6820 score: 0.5510 time: 11.15s
Epoch 32/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 9.90s
Val loss: 0.6732 score: 0.5918 time: 0.78s
Test loss: 0.6808 score: 0.5510 time: 0.87s
Epoch 33/1000, LR 0.000270
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 1.17s
Val loss: 0.6712 score: 0.5918 time: 8.41s
Test loss: 0.6795 score: 0.5510 time: 7.36s
Epoch 34/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 5.62s
Val loss: 0.6690 score: 0.5918 time: 1.29s
Test loss: 0.6781 score: 0.5510 time: 9.48s
Epoch 35/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 10.83s
Val loss: 0.6667 score: 0.5918 time: 1.99s
Test loss: 0.6766 score: 0.5714 time: 1.52s
Epoch 36/1000, LR 0.000270
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 17.20s
Val loss: 0.6642 score: 0.6122 time: 0.98s
Test loss: 0.6751 score: 0.5714 time: 0.45s
Epoch 37/1000, LR 0.000270
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 25.37s
Val loss: 0.6614 score: 0.6122 time: 1.40s
Test loss: 0.6733 score: 0.6122 time: 9.93s
Epoch 38/1000, LR 0.000270
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 15.71s
Val loss: 0.6583 score: 0.6122 time: 0.49s
Test loss: 0.6714 score: 0.6327 time: 0.54s
Epoch 39/1000, LR 0.000269
Train loss: 0.6639;  Loss pred: 0.6639; Loss self: 0.0000; time: 0.65s
Val loss: 0.6550 score: 0.6531 time: 0.48s
Test loss: 0.6694 score: 0.6122 time: 0.38s
Epoch 40/1000, LR 0.000269
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 15.61s
Val loss: 0.6515 score: 0.6939 time: 1.72s
Test loss: 0.6671 score: 0.6122 time: 0.69s
Epoch 41/1000, LR 0.000269
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 16.33s
Val loss: 0.6476 score: 0.7347 time: 1.11s
Test loss: 0.6646 score: 0.6122 time: 8.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 13.87s
Val loss: 0.6435 score: 0.7755 time: 1.48s
Test loss: 0.6620 score: 0.6122 time: 1.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 24.86s
Val loss: 0.6389 score: 0.7959 time: 0.51s
Test loss: 0.6590 score: 0.6735 time: 0.39s
Epoch 44/1000, LR 0.000269
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.64s
Val loss: 0.6341 score: 0.8163 time: 0.58s
Test loss: 0.6559 score: 0.7143 time: 8.99s
Epoch 45/1000, LR 0.000269
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 11.34s
Val loss: 0.6289 score: 0.8571 time: 2.91s
Test loss: 0.6524 score: 0.7143 time: 8.66s
Epoch 46/1000, LR 0.000269
Train loss: 0.6356;  Loss pred: 0.6356; Loss self: 0.0000; time: 5.54s
Val loss: 0.6232 score: 0.8980 time: 10.85s
Test loss: 0.6487 score: 0.7143 time: 2.74s
Epoch 47/1000, LR 0.000269
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 2.21s
Val loss: 0.6172 score: 0.8980 time: 9.06s
Test loss: 0.6446 score: 0.7347 time: 2.90s
Epoch 48/1000, LR 0.000269
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.74s
Val loss: 0.6106 score: 0.9184 time: 0.65s
Test loss: 0.6403 score: 0.7347 time: 10.00s
Epoch 49/1000, LR 0.000269
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 10.16s
Val loss: 0.6036 score: 0.9388 time: 0.78s
Test loss: 0.6356 score: 0.7755 time: 1.13s
Epoch 50/1000, LR 0.000269
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 18.65s
Val loss: 0.5961 score: 0.9388 time: 1.71s
Test loss: 0.6305 score: 0.7755 time: 1.17s
Epoch 51/1000, LR 0.000269
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 23.32s
Val loss: 0.5882 score: 0.9796 time: 0.58s
Test loss: 0.6251 score: 0.7755 time: 0.93s
Epoch 52/1000, LR 0.000269
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 23.82s
Val loss: 0.5797 score: 0.9796 time: 0.75s
Test loss: 0.6194 score: 0.8163 time: 10.16s
Epoch 53/1000, LR 0.000269
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 16.83s
Val loss: 0.5708 score: 0.9796 time: 1.15s
Test loss: 0.6133 score: 0.8163 time: 9.54s
Epoch 54/1000, LR 0.000269
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 15.19s
Val loss: 0.5613 score: 0.9796 time: 0.88s
Test loss: 0.6068 score: 0.8163 time: 1.15s
Epoch 55/1000, LR 0.000269
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 16.13s
Val loss: 0.5513 score: 0.9796 time: 1.23s
Test loss: 0.6000 score: 0.8163 time: 8.44s
Epoch 56/1000, LR 0.000269
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 15.33s
Val loss: 0.5408 score: 0.9796 time: 0.58s
Test loss: 0.5928 score: 0.8367 time: 0.38s
Epoch 57/1000, LR 0.000269
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.75s
Val loss: 0.5297 score: 0.9796 time: 0.51s
Test loss: 0.5852 score: 0.8571 time: 0.45s
Epoch 58/1000, LR 0.000269
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 15.71s
Val loss: 0.5183 score: 0.9796 time: 6.54s
Test loss: 0.5772 score: 0.8776 time: 1.17s
Epoch 59/1000, LR 0.000268
Train loss: 0.5304;  Loss pred: 0.5304; Loss self: 0.0000; time: 23.81s
Val loss: 0.5063 score: 0.9796 time: 7.00s
Test loss: 0.5689 score: 0.8776 time: 0.43s
Epoch 60/1000, LR 0.000268
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 2.87s
Val loss: 0.4942 score: 0.9796 time: 9.91s
Test loss: 0.5603 score: 0.8776 time: 7.68s
Epoch 61/1000, LR 0.000268
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 15.63s
Val loss: 0.4818 score: 0.9796 time: 7.98s
Test loss: 0.5514 score: 0.8776 time: 5.62s
Epoch 62/1000, LR 0.000268
Train loss: 0.4938;  Loss pred: 0.4938; Loss self: 0.0000; time: 2.33s
Val loss: 0.4690 score: 0.9796 time: 10.18s
Test loss: 0.5421 score: 0.8776 time: 9.99s
Epoch 63/1000, LR 0.000268
Train loss: 0.4790;  Loss pred: 0.4790; Loss self: 0.0000; time: 5.59s
Val loss: 0.4559 score: 0.9796 time: 1.23s
Test loss: 0.5326 score: 0.8776 time: 9.27s
Epoch 64/1000, LR 0.000268
Train loss: 0.4657;  Loss pred: 0.4657; Loss self: 0.0000; time: 7.73s
Val loss: 0.4426 score: 0.9796 time: 8.94s
Test loss: 0.5228 score: 0.8776 time: 10.12s
Epoch 65/1000, LR 0.000268
Train loss: 0.4572;  Loss pred: 0.4572; Loss self: 0.0000; time: 9.09s
Val loss: 0.4291 score: 0.9796 time: 9.30s
Test loss: 0.5129 score: 0.8776 time: 2.41s
Epoch 66/1000, LR 0.000268
Train loss: 0.4356;  Loss pred: 0.4356; Loss self: 0.0000; time: 4.93s
Val loss: 0.4153 score: 0.9796 time: 9.01s
Test loss: 0.5029 score: 0.8776 time: 1.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4314;  Loss pred: 0.4314; Loss self: 0.0000; time: 14.27s
Val loss: 0.4017 score: 0.9796 time: 2.73s
Test loss: 0.4928 score: 0.8776 time: 1.45s
Epoch 68/1000, LR 0.000268
Train loss: 0.4178;  Loss pred: 0.4178; Loss self: 0.0000; time: 18.43s
Val loss: 0.3882 score: 0.9796 time: 9.33s
Test loss: 0.4827 score: 0.8776 time: 7.51s
Epoch 69/1000, LR 0.000268
Train loss: 0.4050;  Loss pred: 0.4050; Loss self: 0.0000; time: 10.73s
Val loss: 0.3749 score: 0.9796 time: 5.77s
Test loss: 0.4726 score: 0.8776 time: 1.73s
Epoch 70/1000, LR 0.000268
Train loss: 0.3876;  Loss pred: 0.3876; Loss self: 0.0000; time: 0.60s
Val loss: 0.3620 score: 0.9796 time: 0.46s
Test loss: 0.4623 score: 0.8776 time: 0.50s
Epoch 71/1000, LR 0.000268
Train loss: 0.3689;  Loss pred: 0.3689; Loss self: 0.0000; time: 0.58s
Val loss: 0.3491 score: 0.9796 time: 0.48s
Test loss: 0.4521 score: 0.8776 time: 0.38s
Epoch 72/1000, LR 0.000267
Train loss: 0.3607;  Loss pred: 0.3607; Loss self: 0.0000; time: 0.60s
Val loss: 0.3364 score: 0.9796 time: 0.60s
Test loss: 0.4419 score: 0.8776 time: 0.39s
Epoch 73/1000, LR 0.000267
Train loss: 0.3429;  Loss pred: 0.3429; Loss self: 0.0000; time: 4.42s
Val loss: 0.3239 score: 0.9796 time: 4.47s
Test loss: 0.4319 score: 0.8776 time: 0.39s
Epoch 74/1000, LR 0.000267
Train loss: 0.3347;  Loss pred: 0.3347; Loss self: 0.0000; time: 0.61s
Val loss: 0.3117 score: 0.9796 time: 0.58s
Test loss: 0.4220 score: 0.8776 time: 0.42s
Epoch 75/1000, LR 0.000267
Train loss: 0.3151;  Loss pred: 0.3151; Loss self: 0.0000; time: 0.64s
Val loss: 0.2996 score: 0.9796 time: 0.48s
Test loss: 0.4124 score: 0.8776 time: 0.52s
Epoch 76/1000, LR 0.000267
Train loss: 0.3003;  Loss pred: 0.3003; Loss self: 0.0000; time: 6.76s
Val loss: 0.2877 score: 0.9796 time: 0.58s
Test loss: 0.4031 score: 0.8776 time: 0.36s
Epoch 77/1000, LR 0.000267
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.65s
Val loss: 0.2761 score: 0.9796 time: 0.45s
Test loss: 0.3940 score: 0.8776 time: 0.37s
Epoch 78/1000, LR 0.000267
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.66s
Val loss: 0.2647 score: 0.9796 time: 4.57s
Test loss: 0.3853 score: 0.8776 time: 5.14s
Epoch 79/1000, LR 0.000267
Train loss: 0.2622;  Loss pred: 0.2622; Loss self: 0.0000; time: 0.59s
Val loss: 0.2536 score: 0.9796 time: 0.45s
Test loss: 0.3768 score: 0.8776 time: 0.38s
Epoch 80/1000, LR 0.000267
Train loss: 0.2542;  Loss pred: 0.2542; Loss self: 0.0000; time: 0.60s
Val loss: 0.2429 score: 0.9796 time: 0.56s
Test loss: 0.3685 score: 0.8776 time: 0.37s
Epoch 81/1000, LR 0.000267
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.69s
Val loss: 0.2327 score: 0.9796 time: 0.50s
Test loss: 0.3603 score: 0.8776 time: 2.84s
Epoch 82/1000, LR 0.000267
Train loss: 0.2258;  Loss pred: 0.2258; Loss self: 0.0000; time: 10.98s
Val loss: 0.2230 score: 0.9592 time: 0.75s
Test loss: 0.3527 score: 0.8776 time: 0.37s
Epoch 83/1000, LR 0.000266
Train loss: 0.2166;  Loss pred: 0.2166; Loss self: 0.0000; time: 0.74s
Val loss: 0.2137 score: 0.9592 time: 0.46s
Test loss: 0.3456 score: 0.8776 time: 0.38s
Epoch 84/1000, LR 0.000266
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.58s
Val loss: 0.2048 score: 0.9592 time: 0.64s
Test loss: 0.3390 score: 0.8980 time: 0.41s
Epoch 85/1000, LR 0.000266
Train loss: 0.1860;  Loss pred: 0.1860; Loss self: 0.0000; time: 5.52s
Val loss: 0.1965 score: 0.9592 time: 0.48s
Test loss: 0.3330 score: 0.8980 time: 0.39s
Epoch 86/1000, LR 0.000266
Train loss: 0.1783;  Loss pred: 0.1783; Loss self: 0.0000; time: 0.65s
Val loss: 0.1886 score: 0.9592 time: 0.48s
Test loss: 0.3275 score: 0.8980 time: 0.48s
Epoch 87/1000, LR 0.000266
Train loss: 0.1685;  Loss pred: 0.1685; Loss self: 0.0000; time: 0.59s
Val loss: 0.1813 score: 0.9592 time: 0.50s
Test loss: 0.3226 score: 0.8980 time: 0.64s
Epoch 88/1000, LR 0.000266
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 12.50s
Val loss: 0.1744 score: 0.9592 time: 0.69s
Test loss: 0.3184 score: 0.8980 time: 0.39s
Epoch 89/1000, LR 0.000266
Train loss: 0.1491;  Loss pred: 0.1491; Loss self: 0.0000; time: 0.59s
Val loss: 0.1681 score: 0.9592 time: 0.61s
Test loss: 0.3148 score: 0.8980 time: 0.38s
Epoch 90/1000, LR 0.000266
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 0.77s
Val loss: 0.1626 score: 0.9592 time: 5.63s
Test loss: 0.3118 score: 0.8980 time: 5.17s
Epoch 91/1000, LR 0.000266
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.59s
Val loss: 0.1578 score: 0.9592 time: 0.46s
Test loss: 0.3094 score: 0.8980 time: 0.36s
Epoch 92/1000, LR 0.000266
Train loss: 0.1238;  Loss pred: 0.1238; Loss self: 0.0000; time: 0.58s
Val loss: 0.1537 score: 0.9592 time: 0.55s
Test loss: 0.3079 score: 0.8980 time: 0.38s
Epoch 93/1000, LR 0.000265
Train loss: 0.1212;  Loss pred: 0.1212; Loss self: 0.0000; time: 0.59s
Val loss: 0.1501 score: 0.9592 time: 0.48s
Test loss: 0.3072 score: 0.8980 time: 6.00s
Epoch 94/1000, LR 0.000265
Train loss: 0.1149;  Loss pred: 0.1149; Loss self: 0.0000; time: 4.49s
Val loss: 0.1470 score: 0.9592 time: 0.46s
Test loss: 0.3069 score: 0.8980 time: 0.47s
Epoch 95/1000, LR 0.000265
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.59s
Val loss: 0.1444 score: 0.9592 time: 0.47s
Test loss: 0.3071 score: 0.8980 time: 0.37s
Epoch 96/1000, LR 0.000265
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.66s
Val loss: 0.1423 score: 0.9592 time: 7.62s
Test loss: 0.3079 score: 0.8980 time: 3.94s
Epoch 97/1000, LR 0.000265
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.64s
Val loss: 0.1408 score: 0.9592 time: 0.54s
Test loss: 0.3091 score: 0.8980 time: 0.40s
Epoch 98/1000, LR 0.000265
Train loss: 0.0835;  Loss pred: 0.0835; Loss self: 0.0000; time: 0.66s
Val loss: 0.1395 score: 0.9592 time: 0.56s
Test loss: 0.3108 score: 0.8980 time: 0.39s
Epoch 99/1000, LR 0.000265
Train loss: 0.0854;  Loss pred: 0.0854; Loss self: 0.0000; time: 0.62s
Val loss: 0.1384 score: 0.9592 time: 4.21s
Test loss: 0.3127 score: 0.8980 time: 2.45s
Epoch 100/1000, LR 0.000265
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.86s
Val loss: 0.1374 score: 0.9592 time: 0.58s
Test loss: 0.3150 score: 0.8980 time: 0.38s
Epoch 101/1000, LR 0.000265
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.64s
Val loss: 0.1366 score: 0.9592 time: 0.52s
Test loss: 0.3179 score: 0.8980 time: 3.35s
Epoch 102/1000, LR 0.000264
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 12.88s
Val loss: 0.1363 score: 0.9592 time: 0.48s
Test loss: 0.3211 score: 0.8980 time: 0.46s
Epoch 103/1000, LR 0.000264
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.59s
Val loss: 0.1361 score: 0.9592 time: 0.47s
Test loss: 0.3248 score: 0.8980 time: 0.37s
Epoch 104/1000, LR 0.000264
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.75s
Val loss: 0.1361 score: 0.9592 time: 5.13s
Test loss: 0.3290 score: 0.8980 time: 3.93s
     INFO: Early stopping counter 1 of 2
Epoch 105/1000, LR 0.000264
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 3.05s
Val loss: 0.1364 score: 0.9592 time: 0.49s
Test loss: 0.3335 score: 0.8980 time: 0.39s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 102,   Train_Loss: 0.0712,   Val_Loss: 0.1361,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1361,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3248


[0.3788351740222424, 0.5244298030156642, 0.4059122330509126, 0.5390196479856968, 1.6352017680183053, 0.5109337409958243, 0.3984824719373137, 0.43290934804826975, 0.4170326559105888, 4.5594999990426, 1.2329500120831653, 0.7384609329747036, 6.246244639973156, 1.4508020220091566, 1.2046333010075614, 4.168678641086444, 0.8478594199987128, 6.938657262013294, 5.0501101850531995, 6.1911539969732985, 9.37384433997795, 0.6391824199818075, 8.614362410036847, 0.6571992880199105, 9.0625812090002, 7.668791029020213, 4.7337966550840065, 1.2985285860486329, 8.598210850032046, 9.671734989969991, 11.155706189922057, 0.8713208719855174, 7.360768198035657, 9.483454713015817, 1.5229522290173918, 0.45939651399385184, 9.934485525009222, 0.5410146790090948, 0.3840487999841571, 0.6957317219348624, 8.447724499972537, 1.2243740189587697, 0.3904161509126425, 8.99921638192609, 8.662375352927484, 2.7486613399814814, 2.9051960109500214, 10.010524393990636, 1.1309135099872947, 1.1770478099351749, 0.9330855660373345, 10.16052712209057, 9.546618673019111, 1.1587814869126305, 8.451134937000461, 0.3884216819424182, 0.45461041398812085, 1.171126383007504, 0.43167333595920354, 7.68049998593051, 5.621060137054883, 9.99884514696896, 9.275726168998517, 10.128007767023519, 2.413401555037126, 1.4447629740461707, 1.4513186960248277, 7.519128399086185, 1.7334296320332214, 0.5092178990598768, 0.38124984805472195, 0.39014448900707066, 0.3998645240208134, 0.426794204977341, 0.5272911339998245, 0.3622810470405966, 0.3743028889875859, 5.141137117054313, 0.38635232707019895, 0.374939419911243, 2.845544971060008, 0.3728056070394814, 0.38681738602463156, 0.4134570839814842, 0.396081323036924, 0.48923315899446607, 0.6439147699857131, 0.3894510759273544, 0.3822516080690548, 5.172495691920631, 0.3686916349688545, 0.3820609530666843, 6.001573158078827, 0.473688698024489, 0.3765930269146338, 3.9465678160777315, 0.40478840901050717, 0.3912631249986589, 2.4569340449525043, 0.3877158249961212, 3.356396505027078, 0.46631727588828653, 0.37714965594932437, 3.9389209849759936, 0.39314014895353466]
[0.00773133008208658, 0.010702649041136004, 0.008283923123488012, 0.011000400979299935, 0.0333714646534348, 0.010427219203996415, 0.008132295345659462, 0.008834884654046322, 0.008510870528787526, 0.09305102038862449, 0.025162245144554396, 0.015070631285198033, 0.12747438040761544, 0.029608204530799116, 0.024584353081786966, 0.08507507430788662, 0.017303253469361485, 0.14160525024516926, 0.10306347316435101, 0.12635008157088365, 0.19130294571383571, 0.013044539183302194, 0.1758033144905479, 0.013412230367753275, 0.18495063691837144, 0.15650593936775944, 0.09660809500171441, 0.026500583388747608, 0.17547369081698053, 0.19738234673408145, 0.22766747326371545, 0.017782058611949335, 0.15021975914358485, 0.1935398921023636, 0.03108065773504881, 0.009375439061099018, 0.20274460255120863, 0.011041115898144792, 0.007837730611921574, 0.014198606570099234, 0.17240254081576606, 0.024987224876709586, 0.007967676549237602, 0.1836574771821651, 0.17678317046790784, 0.05609512938737717, 0.05928971450918411, 0.20429641620389052, 0.023079867550761114, 0.02402138387622806, 0.0190425625721905, 0.20735769636919532, 0.1948289525105941, 0.023648601773727154, 0.17247214157143798, 0.007926973100865677, 0.009277763550777977, 0.023900538428724572, 0.008809659917534766, 0.15674489767205121, 0.11471551300112005, 0.20405806422385633, 0.18930053406119424, 0.20669403606170447, 0.04925309295994135, 0.029484958654003485, 0.02961874889846587, 0.15345159998135072, 0.0353761149394535, 0.010392202021630138, 0.007780609143973917, 0.007962132428715728, 0.008160500490220681, 0.008710085815864101, 0.010761043551016827, 0.007393490755930543, 0.007638834469134406, 0.10492116565416966, 0.00788474136877957, 0.007651824896147817, 0.058072346348163426, 0.007608277694683294, 0.007894232367849623, 0.008437899673091514, 0.008083292306876001, 0.009984350183560533, 0.01314111775481047, 0.00794798114137458, 0.007801053225899077, 0.10556113656980878, 0.00752431908099703, 0.007797162307483353, 0.12248108485875157, 0.009667116286214061, 0.00768557197784967, 0.08054220032811697, 0.008260987938989942, 0.007984961734666509, 0.05014151112147968, 0.007912567857063698, 0.06849788785769548, 0.00951667909976095, 0.007696931754067844, 0.08038614255053048, 0.008023268345990504]
[129.34385020204877, 93.43481190091025, 120.71575086985379, 90.90577715137435, 29.965721024985747, 95.9028462369653, 122.96651283499446, 113.18766901410606, 117.49679384941386, 10.746792413705226, 39.74208160897835, 66.35422107248905, 7.844713555793515, 33.77442218624833, 40.6762787970548, 11.754324144120062, 57.79259962704005, 7.061885052062991, 9.702758594262992, 7.9145180404097335, 5.227311039401713, 76.66043130753603, 5.688174895324657, 74.5588147967006, 5.406848100995474, 6.3895338671472945, 10.351099459959892, 37.73501833263826, 5.698860013396551, 5.066309204172274, 4.392370968344977, 56.236458433896495, 6.656913882042429, 5.166893445776533, 32.17435128061422, 106.66167136099725, 4.932313794876108, 90.57055547872903, 127.58795236965032, 70.42944637299088, 5.800378551663148, 40.02045064764646, 125.50710283234156, 5.444918526285352, 5.656647051601182, 17.826859674291534, 16.866331846565018, 4.8948484686191795, 43.32780497117812, 41.629574930094506, 52.51394061114372, 4.822584439882687, 5.13270736773901, 42.28579810206658, 5.798037821579434, 126.15155712976916, 107.7845964199148, 41.840061594518815, 113.51175974563988, 6.379792993914515, 8.717216824809354, 4.900565943343348, 5.282605276098878, 4.838068959577864, 20.303293456378924, 33.91559783870409, 33.76239838583445, 6.516712762340256, 28.267660304459884, 96.2259969464237, 128.52464138678656, 125.59449481064428, 122.5415035754697, 114.80943140406855, 92.9277904377135, 135.25410837876137, 130.91002351741167, 9.530965404026267, 126.82724178621774, 130.6877788726495, 17.219900053713356, 131.43579140109534, 126.67476119307575, 118.51290472070946, 123.71196809861206, 100.15674346504028, 76.09702756327083, 125.81811433778176, 128.18781913704342, 9.473183337114683, 132.90239146363956, 128.2517870687707, 8.164525984997818, 103.44346446168909, 130.11393333925784, 12.415851515431022, 121.05089698536328, 125.23541542579039, 19.943555302457145, 126.38122264029384, 14.598990294087644, 105.07867182630116, 129.92190030416964, 12.439955050354644, 124.63748648014915]
Elapsed: 3.111856731838946~3.47391451264925
Time per graph: 0.06350728024161115~0.07089621454386225
Speed: 60.66931207647994~50.47386969605564
Total Time: 0.3939
best val loss: 0.13606086373329163 test_score: 0.8980

Testing...
Test loss: 0.6251 score: 0.7755 time: 0.40s
test Score 0.7755
Epoch Time List: [1.4648569519631565, 14.679164236760698, 1.768648892058991, 2.049244328052737, 15.552218958036974, 1.5910246461862698, 1.473922613891773, 1.6390828461153433, 12.682095161871985, 23.609958324814215, 4.32757701294031, 20.985241739079356, 26.5067766798893, 16.65743023622781, 13.223418735084124, 15.343166811973788, 4.225896264077164, 25.234772425028495, 36.71487993304618, 19.74811875494197, 13.287213095929474, 15.296769196982495, 24.923783584032208, 22.129780369112268, 20.786021510954015, 17.772980110021308, 16.274315660935827, 26.49318890599534, 32.741746840882115, 17.515109239029698, 21.664561629178934, 11.545491831144318, 16.933984971954487, 16.389003560878336, 14.341399525874294, 18.641814125003293, 36.7008740111487, 16.728734908043407, 1.5040760338306427, 18.01873376907315, 25.876490044873208, 16.56735022587236, 25.75458689185325, 10.205092184012756, 22.91293591307476, 19.130165235023014, 14.173297197092324, 11.388201162102632, 12.069619476096705, 21.53226773010101, 24.823382937931456, 34.72187739692163, 27.52387919393368, 17.217075466061942, 25.797222696011886, 16.296394822071306, 1.717080986010842, 23.41631297499407, 31.23469104990363, 20.449511325918138, 29.22951340896543, 22.506461179000326, 16.09225024515763, 26.796041983994655, 20.80265552806668, 15.385184762999415, 18.45044297503773, 35.278237930033356, 18.232675703824498, 1.5593645239714533, 1.4374329330166802, 1.582406408037059, 9.29123984312173, 1.6131818380672485, 1.642197441891767, 7.700346781872213, 1.4679736291291192, 10.368916408973746, 1.4240315899951383, 1.5368347759358585, 4.025559218018316, 12.103291216073558, 1.5869333120062947, 1.626648087054491, 6.393705940106884, 1.6103178969351575, 1.730104642920196, 13.58051778213121, 1.5723858989076689, 11.567283827927895, 1.4135134479729459, 1.50696128513664, 7.070975793059915, 5.4269721179734915, 1.432181519921869, 12.219449613941833, 1.5867652621818706, 1.601573443855159, 7.277727008913644, 1.827477499959059, 4.513199654058553, 13.827869447995909, 1.427716471021995, 9.814884983934462, 3.926816009916365]
Total Epoch List: [105]
Total Time List: [0.39390551194082946]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb612a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.53s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.47s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 2.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 5.52s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 7.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.47s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.46s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.47s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 5.15s
Epoch 8/1000, LR 0.000180
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 6.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.75s
Epoch 9/1000, LR 0.000210
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.80s
Val loss: 0.6927 score: 0.6122 time: 0.38s
Test loss: 0.6922 score: 0.6327 time: 0.47s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.58s
Val loss: 0.6925 score: 0.6122 time: 0.38s
Test loss: 0.6920 score: 0.6735 time: 0.49s
Epoch 11/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.81s
Test loss: 0.6918 score: 0.5306 time: 5.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 8.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.47s
Epoch 13/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4898 time: 0.50s
Epoch 14/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4898 time: 0.55s
Epoch 15/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 5.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 4.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4898 time: 1.87s
Epoch 16/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4898 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4898 time: 0.52s
Epoch 18/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 12.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 3.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4898 time: 0.58s
Epoch 19/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4898 time: 0.48s
Epoch 20/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4898 time: 0.59s
Epoch 21/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 10.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4898 time: 0.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.4898 time: 0.55s
Epoch 23/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.66s
Val loss: 0.6885 score: 0.5306 time: 4.72s
Test loss: 0.6859 score: 0.5306 time: 0.53s
Epoch 24/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.58s
Val loss: 0.6879 score: 0.5306 time: 0.38s
Test loss: 0.6849 score: 0.5714 time: 0.47s
Epoch 25/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.68s
Val loss: 0.6873 score: 0.5306 time: 0.38s
Test loss: 0.6839 score: 0.5714 time: 0.51s
Epoch 26/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.69s
Val loss: 0.6866 score: 0.5510 time: 0.38s
Test loss: 0.6827 score: 0.5918 time: 0.52s
Epoch 27/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.76s
Val loss: 0.6858 score: 0.5510 time: 4.92s
Test loss: 0.6815 score: 0.5918 time: 2.96s
Epoch 28/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.59s
Val loss: 0.6850 score: 0.5510 time: 0.37s
Test loss: 0.6802 score: 0.5918 time: 0.50s
Epoch 29/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.92s
Val loss: 0.6840 score: 0.5714 time: 0.36s
Test loss: 0.6787 score: 0.5918 time: 0.48s
Epoch 30/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 10.22s
Val loss: 0.6830 score: 0.6122 time: 0.60s
Test loss: 0.6771 score: 0.6122 time: 0.50s
Epoch 31/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.75s
Val loss: 0.6819 score: 0.6122 time: 0.48s
Test loss: 0.6753 score: 0.6122 time: 0.48s
Epoch 32/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.61s
Val loss: 0.6806 score: 0.6327 time: 0.48s
Test loss: 0.6734 score: 0.6122 time: 0.54s
Epoch 33/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.71s
Val loss: 0.6793 score: 0.6531 time: 0.87s
Test loss: 0.6714 score: 0.6122 time: 4.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 0.73s
Val loss: 0.6777 score: 0.6327 time: 0.36s
Test loss: 0.6691 score: 0.6531 time: 0.47s
Epoch 35/1000, LR 0.000270
Train loss: 0.6710;  Loss pred: 0.6710; Loss self: 0.0000; time: 0.62s
Val loss: 0.6760 score: 0.6327 time: 0.47s
Test loss: 0.6666 score: 0.6531 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.59s
Val loss: 0.6742 score: 0.6531 time: 0.44s
Test loss: 0.6639 score: 0.6531 time: 3.97s
Epoch 37/1000, LR 0.000270
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 3.81s
Val loss: 0.6721 score: 0.6531 time: 0.48s
Test loss: 0.6609 score: 0.6939 time: 0.49s
Epoch 38/1000, LR 0.000270
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.60s
Val loss: 0.6698 score: 0.6531 time: 0.38s
Test loss: 0.6576 score: 0.7347 time: 0.49s
Epoch 39/1000, LR 0.000269
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.72s
Val loss: 0.6673 score: 0.6531 time: 0.42s
Test loss: 0.6539 score: 0.7347 time: 0.48s
Epoch 40/1000, LR 0.000269
Train loss: 0.6547;  Loss pred: 0.6547; Loss self: 0.0000; time: 0.81s
Val loss: 0.6646 score: 0.6531 time: 0.40s
Test loss: 0.6499 score: 0.7551 time: 5.11s
Epoch 41/1000, LR 0.000269
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 3.03s
Val loss: 0.6615 score: 0.6531 time: 0.41s
Test loss: 0.6455 score: 0.7551 time: 0.47s
Epoch 42/1000, LR 0.000269
Train loss: 0.6466;  Loss pred: 0.6466; Loss self: 0.0000; time: 0.70s
Val loss: 0.6581 score: 0.6327 time: 0.36s
Test loss: 0.6408 score: 0.7755 time: 0.60s
Epoch 43/1000, LR 0.000269
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.62s
Val loss: 0.6544 score: 0.6531 time: 1.57s
Test loss: 0.6356 score: 0.7755 time: 5.01s
Epoch 44/1000, LR 0.000269
Train loss: 0.6356;  Loss pred: 0.6356; Loss self: 0.0000; time: 7.16s
Val loss: 0.6504 score: 0.6531 time: 0.38s
Test loss: 0.6300 score: 0.7755 time: 0.46s
Epoch 45/1000, LR 0.000269
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.70s
Val loss: 0.6460 score: 0.6531 time: 0.39s
Test loss: 0.6240 score: 0.7755 time: 0.60s
Epoch 46/1000, LR 0.000269
Train loss: 0.6241;  Loss pred: 0.6241; Loss self: 0.0000; time: 0.98s
Val loss: 0.6413 score: 0.6531 time: 0.36s
Test loss: 0.6175 score: 0.7755 time: 0.58s
Epoch 47/1000, LR 0.000269
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 13.20s
Val loss: 0.6362 score: 0.6735 time: 4.02s
Test loss: 0.6105 score: 0.7755 time: 0.54s
Epoch 48/1000, LR 0.000269
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.61s
Val loss: 0.6308 score: 0.7347 time: 0.37s
Test loss: 0.6029 score: 0.7755 time: 0.48s
Epoch 49/1000, LR 0.000269
Train loss: 0.6023;  Loss pred: 0.6023; Loss self: 0.0000; time: 0.67s
Val loss: 0.6249 score: 0.7347 time: 0.39s
Test loss: 0.5949 score: 0.7755 time: 0.52s
Epoch 50/1000, LR 0.000269
Train loss: 0.5906;  Loss pred: 0.5906; Loss self: 0.0000; time: 0.64s
Val loss: 0.6186 score: 0.7347 time: 0.42s
Test loss: 0.5864 score: 0.8367 time: 0.56s
Epoch 51/1000, LR 0.000269
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 6.93s
Val loss: 0.6117 score: 0.7755 time: 0.37s
Test loss: 0.5774 score: 0.8571 time: 0.48s
Epoch 52/1000, LR 0.000269
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.61s
Val loss: 0.6045 score: 0.7551 time: 0.38s
Test loss: 0.5678 score: 0.8776 time: 0.48s
Epoch 53/1000, LR 0.000269
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.71s
Val loss: 0.5969 score: 0.7551 time: 0.39s
Test loss: 0.5578 score: 0.9184 time: 0.49s
Epoch 54/1000, LR 0.000269
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 7.06s
Val loss: 0.5889 score: 0.7551 time: 0.38s
Test loss: 0.5473 score: 0.9184 time: 0.47s
Epoch 55/1000, LR 0.000269
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.83s
Val loss: 0.5804 score: 0.7755 time: 0.38s
Test loss: 0.5363 score: 0.9184 time: 0.50s
Epoch 56/1000, LR 0.000269
Train loss: 0.5288;  Loss pred: 0.5288; Loss self: 0.0000; time: 0.59s
Val loss: 0.5714 score: 0.7959 time: 0.38s
Test loss: 0.5246 score: 0.9388 time: 0.57s
Epoch 57/1000, LR 0.000269
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.78s
Val loss: 0.5619 score: 0.7959 time: 0.41s
Test loss: 0.5124 score: 0.9388 time: 0.48s
Epoch 58/1000, LR 0.000269
Train loss: 0.5016;  Loss pred: 0.5016; Loss self: 0.0000; time: 0.63s
Val loss: 0.5519 score: 0.8571 time: 0.40s
Test loss: 0.4995 score: 0.9388 time: 3.94s
Epoch 59/1000, LR 0.000268
Train loss: 0.4843;  Loss pred: 0.4843; Loss self: 0.0000; time: 7.77s
Val loss: 0.5414 score: 0.8367 time: 0.48s
Test loss: 0.4861 score: 0.9388 time: 0.76s
Epoch 60/1000, LR 0.000268
Train loss: 0.4700;  Loss pred: 0.4700; Loss self: 0.0000; time: 0.75s
Val loss: 0.5303 score: 0.8367 time: 0.38s
Test loss: 0.4720 score: 0.9388 time: 0.50s
Epoch 61/1000, LR 0.000268
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 9.29s
Val loss: 0.5188 score: 0.8571 time: 0.45s
Test loss: 0.4574 score: 0.9592 time: 0.52s
Epoch 62/1000, LR 0.000268
Train loss: 0.4407;  Loss pred: 0.4407; Loss self: 0.0000; time: 0.59s
Val loss: 0.5070 score: 0.8571 time: 0.37s
Test loss: 0.4423 score: 0.9592 time: 0.51s
Epoch 63/1000, LR 0.000268
Train loss: 0.4182;  Loss pred: 0.4182; Loss self: 0.0000; time: 0.69s
Val loss: 0.4949 score: 0.8571 time: 0.37s
Test loss: 0.4269 score: 0.9796 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.3933;  Loss pred: 0.3933; Loss self: 0.0000; time: 0.76s
Val loss: 0.4828 score: 0.8571 time: 0.36s
Test loss: 0.4112 score: 0.9796 time: 0.48s
Epoch 65/1000, LR 0.000268
Train loss: 0.3855;  Loss pred: 0.3855; Loss self: 0.0000; time: 9.66s
Val loss: 0.4706 score: 0.8571 time: 5.27s
Test loss: 0.3955 score: 0.9796 time: 0.52s
Epoch 66/1000, LR 0.000268
Train loss: 0.3663;  Loss pred: 0.3663; Loss self: 0.0000; time: 0.70s
Val loss: 0.4583 score: 0.8571 time: 0.38s
Test loss: 0.3800 score: 0.9796 time: 0.59s
Epoch 67/1000, LR 0.000268
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 0.57s
Val loss: 0.4461 score: 0.8571 time: 0.36s
Test loss: 0.3647 score: 0.9796 time: 0.52s
Epoch 68/1000, LR 0.000268
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 8.39s
Val loss: 0.4342 score: 0.8571 time: 0.36s
Test loss: 0.3498 score: 0.9592 time: 0.50s
Epoch 69/1000, LR 0.000268
Train loss: 0.3256;  Loss pred: 0.3256; Loss self: 0.0000; time: 0.77s
Val loss: 0.4227 score: 0.8571 time: 0.40s
Test loss: 0.3356 score: 0.9592 time: 0.54s
Epoch 70/1000, LR 0.000268
Train loss: 0.3023;  Loss pred: 0.3023; Loss self: 0.0000; time: 0.60s
Val loss: 0.4115 score: 0.8776 time: 0.38s
Test loss: 0.3218 score: 0.9592 time: 0.62s
Epoch 71/1000, LR 0.000268
Train loss: 0.2869;  Loss pred: 0.2869; Loss self: 0.0000; time: 0.63s
Val loss: 0.4007 score: 0.8776 time: 6.39s
Test loss: 0.3087 score: 0.9592 time: 2.44s
Epoch 72/1000, LR 0.000267
Train loss: 0.2705;  Loss pred: 0.2705; Loss self: 0.0000; time: 0.61s
Val loss: 0.3902 score: 0.8776 time: 0.38s
Test loss: 0.2957 score: 0.9592 time: 0.47s
Epoch 73/1000, LR 0.000267
Train loss: 0.2521;  Loss pred: 0.2521; Loss self: 0.0000; time: 0.76s
Val loss: 0.3801 score: 0.8776 time: 0.39s
Test loss: 0.2832 score: 0.9592 time: 0.46s
Epoch 74/1000, LR 0.000267
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.64s
Val loss: 0.3709 score: 0.8776 time: 0.42s
Test loss: 0.2719 score: 0.9592 time: 5.03s
Epoch 75/1000, LR 0.000267
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 4.72s
Val loss: 0.3623 score: 0.8776 time: 0.44s
Test loss: 0.2616 score: 0.9592 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.2077;  Loss pred: 0.2077; Loss self: 0.0000; time: 0.59s
Val loss: 0.3544 score: 0.8776 time: 0.37s
Test loss: 0.2523 score: 0.9592 time: 0.51s
Epoch 77/1000, LR 0.000267
Train loss: 0.2020;  Loss pred: 0.2020; Loss self: 0.0000; time: 0.72s
Val loss: 0.3469 score: 0.8776 time: 0.38s
Test loss: 0.2432 score: 0.9592 time: 0.54s
Epoch 78/1000, LR 0.000267
Train loss: 0.1831;  Loss pred: 0.1831; Loss self: 0.0000; time: 1.39s
Val loss: 0.3404 score: 0.8776 time: 4.73s
Test loss: 0.2354 score: 0.9388 time: 2.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.1746;  Loss pred: 0.1746; Loss self: 0.0000; time: 0.67s
Val loss: 0.3346 score: 0.8776 time: 0.41s
Test loss: 0.2286 score: 0.9388 time: 0.48s
Epoch 80/1000, LR 0.000267
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 0.63s
Val loss: 0.3296 score: 0.8776 time: 0.39s
Test loss: 0.2228 score: 0.9388 time: 0.48s
Epoch 81/1000, LR 0.000267
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 0.78s
Val loss: 0.3250 score: 0.8776 time: 0.38s
Test loss: 0.2168 score: 0.9388 time: 0.54s
Epoch 82/1000, LR 0.000267
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 3.36s
Val loss: 0.3212 score: 0.8776 time: 0.51s
Test loss: 0.2118 score: 0.9388 time: 0.48s
Epoch 83/1000, LR 0.000266
Train loss: 0.1190;  Loss pred: 0.1190; Loss self: 0.0000; time: 0.61s
Val loss: 0.3180 score: 0.8776 time: 0.48s
Test loss: 0.2071 score: 0.9388 time: 0.47s
Epoch 84/1000, LR 0.000266
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.79s
Val loss: 0.3156 score: 0.8776 time: 0.51s
Test loss: 0.2034 score: 0.9388 time: 0.53s
Epoch 85/1000, LR 0.000266
Train loss: 0.1050;  Loss pred: 0.1050; Loss self: 0.0000; time: 0.70s
Val loss: 0.3139 score: 0.8776 time: 4.88s
Test loss: 0.1999 score: 0.9388 time: 1.91s
Epoch 86/1000, LR 0.000266
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 0.61s
Val loss: 0.3128 score: 0.8776 time: 0.39s
Test loss: 0.1967 score: 0.9388 time: 0.61s
Epoch 87/1000, LR 0.000266
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 0.86s
Val loss: 0.3128 score: 0.8776 time: 0.38s
Test loss: 0.1959 score: 0.9388 time: 0.48s
     INFO: Early stopping counter 1 of 2
Epoch 88/1000, LR 0.000266
Train loss: 0.0861;  Loss pred: 0.0861; Loss self: 0.0000; time: 0.60s
Val loss: 0.3141 score: 0.8776 time: 0.42s
Test loss: 0.1969 score: 0.9388 time: 4.54s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.0971,   Val_Loss: 0.3128,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3128,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1967


[0.3788351740222424, 0.5244298030156642, 0.4059122330509126, 0.5390196479856968, 1.6352017680183053, 0.5109337409958243, 0.3984824719373137, 0.43290934804826975, 0.4170326559105888, 4.5594999990426, 1.2329500120831653, 0.7384609329747036, 6.246244639973156, 1.4508020220091566, 1.2046333010075614, 4.168678641086444, 0.8478594199987128, 6.938657262013294, 5.0501101850531995, 6.1911539969732985, 9.37384433997795, 0.6391824199818075, 8.614362410036847, 0.6571992880199105, 9.0625812090002, 7.668791029020213, 4.7337966550840065, 1.2985285860486329, 8.598210850032046, 9.671734989969991, 11.155706189922057, 0.8713208719855174, 7.360768198035657, 9.483454713015817, 1.5229522290173918, 0.45939651399385184, 9.934485525009222, 0.5410146790090948, 0.3840487999841571, 0.6957317219348624, 8.447724499972537, 1.2243740189587697, 0.3904161509126425, 8.99921638192609, 8.662375352927484, 2.7486613399814814, 2.9051960109500214, 10.010524393990636, 1.1309135099872947, 1.1770478099351749, 0.9330855660373345, 10.16052712209057, 9.546618673019111, 1.1587814869126305, 8.451134937000461, 0.3884216819424182, 0.45461041398812085, 1.171126383007504, 0.43167333595920354, 7.68049998593051, 5.621060137054883, 9.99884514696896, 9.275726168998517, 10.128007767023519, 2.413401555037126, 1.4447629740461707, 1.4513186960248277, 7.519128399086185, 1.7334296320332214, 0.5092178990598768, 0.38124984805472195, 0.39014448900707066, 0.3998645240208134, 0.426794204977341, 0.5272911339998245, 0.3622810470405966, 0.3743028889875859, 5.141137117054313, 0.38635232707019895, 0.374939419911243, 2.845544971060008, 0.3728056070394814, 0.38681738602463156, 0.4134570839814842, 0.396081323036924, 0.48923315899446607, 0.6439147699857131, 0.3894510759273544, 0.3822516080690548, 5.172495691920631, 0.3686916349688545, 0.3820609530666843, 6.001573158078827, 0.473688698024489, 0.3765930269146338, 3.9465678160777315, 0.40478840901050717, 0.3912631249986589, 2.4569340449525043, 0.3877158249961212, 3.356396505027078, 0.46631727588828653, 0.37714965594932437, 3.9389209849759936, 0.39314014895353466, 0.5323877269402146, 0.47708452597726136, 5.529484294936992, 0.4716285759350285, 0.4678397160023451, 0.476372898905538, 5.155615768046118, 0.7553510280558839, 0.47694574401248246, 0.497742039966397, 5.044181952951476, 0.4788824761053547, 0.5079100700095296, 0.5567607269622386, 1.8708697970723733, 0.5379799179499969, 0.5294872319791466, 0.5802993860561401, 0.4818630249937996, 0.5942344450158998, 0.4676987669663504, 0.556512029026635, 0.5389222460798919, 0.4778234559344128, 0.5098615620518103, 0.5239875649567693, 2.9638148669619113, 0.5000090259127319, 0.484110051067546, 0.5057485869619995, 0.48571637796703726, 0.5478678430663422, 4.437000264995731, 0.4757721029454842, 0.4770124029600993, 3.976211618981324, 0.4967528630513698, 0.49372903094626963, 0.4882490400923416, 5.1193949489388615, 0.47661715804133564, 0.6125948360422626, 5.01509644894395, 0.46265231200959533, 0.6069777590455487, 0.581178336055018, 0.5428712740540504, 0.47969544609077275, 0.5271037709899247, 0.5658638370223343, 0.4809156709816307, 0.4891766590299085, 0.4910297089954838, 0.4768802559701726, 0.5070661449572071, 0.574469840968959, 0.4818210059311241, 3.9477914799936116, 0.7625952269881964, 0.5002829419681802, 0.5287734500598162, 0.5108073749579489, 0.4817248859908432, 0.48234237008728087, 0.5254940938903019, 0.5964385600527748, 0.5253524560248479, 0.5053960969671607, 0.5426859969738871, 0.6262481570011005, 2.442927932017483, 0.47109276393894106, 0.4665016660001129, 5.04005267797038, 0.47827636601869017, 0.5141533319838345, 0.5420131620485336, 2.259763756999746, 0.48822560999542475, 0.48258329601958394, 0.5402703159488738, 0.4839682609308511, 0.47638045402709395, 0.5382098610280082, 1.9115894879214466, 0.6103848259663209, 0.48003340896684676, 4.541731623001397]
[0.00773133008208658, 0.010702649041136004, 0.008283923123488012, 0.011000400979299935, 0.0333714646534348, 0.010427219203996415, 0.008132295345659462, 0.008834884654046322, 0.008510870528787526, 0.09305102038862449, 0.025162245144554396, 0.015070631285198033, 0.12747438040761544, 0.029608204530799116, 0.024584353081786966, 0.08507507430788662, 0.017303253469361485, 0.14160525024516926, 0.10306347316435101, 0.12635008157088365, 0.19130294571383571, 0.013044539183302194, 0.1758033144905479, 0.013412230367753275, 0.18495063691837144, 0.15650593936775944, 0.09660809500171441, 0.026500583388747608, 0.17547369081698053, 0.19738234673408145, 0.22766747326371545, 0.017782058611949335, 0.15021975914358485, 0.1935398921023636, 0.03108065773504881, 0.009375439061099018, 0.20274460255120863, 0.011041115898144792, 0.007837730611921574, 0.014198606570099234, 0.17240254081576606, 0.024987224876709586, 0.007967676549237602, 0.1836574771821651, 0.17678317046790784, 0.05609512938737717, 0.05928971450918411, 0.20429641620389052, 0.023079867550761114, 0.02402138387622806, 0.0190425625721905, 0.20735769636919532, 0.1948289525105941, 0.023648601773727154, 0.17247214157143798, 0.007926973100865677, 0.009277763550777977, 0.023900538428724572, 0.008809659917534766, 0.15674489767205121, 0.11471551300112005, 0.20405806422385633, 0.18930053406119424, 0.20669403606170447, 0.04925309295994135, 0.029484958654003485, 0.02961874889846587, 0.15345159998135072, 0.0353761149394535, 0.010392202021630138, 0.007780609143973917, 0.007962132428715728, 0.008160500490220681, 0.008710085815864101, 0.010761043551016827, 0.007393490755930543, 0.007638834469134406, 0.10492116565416966, 0.00788474136877957, 0.007651824896147817, 0.058072346348163426, 0.007608277694683294, 0.007894232367849623, 0.008437899673091514, 0.008083292306876001, 0.009984350183560533, 0.01314111775481047, 0.00794798114137458, 0.007801053225899077, 0.10556113656980878, 0.00752431908099703, 0.007797162307483353, 0.12248108485875157, 0.009667116286214061, 0.00768557197784967, 0.08054220032811697, 0.008260987938989942, 0.007984961734666509, 0.05014151112147968, 0.007912567857063698, 0.06849788785769548, 0.00951667909976095, 0.007696931754067844, 0.08038614255053048, 0.008023268345990504, 0.010865055651841114, 0.00973641889749513, 0.11284661826402025, 0.009625072978265887, 0.009547749306170307, 0.009721895896031387, 0.1052166483274718, 0.015415327103181305, 0.009733586612499642, 0.010158000815640755, 0.1029424888357444, 0.009773111757252137, 0.010365511632847543, 0.011362463815555888, 0.03818101626678313, 0.010979181998979529, 0.010805861877125442, 0.011842844613390614, 0.009833939285587748, 0.012127233571753058, 0.00954487279523164, 0.011357388347482346, 0.010998413185303917, 0.009751499100702301, 0.010405338001057354, 0.010693623774627946, 0.06048601769310023, 0.010204265834953713, 0.009879796960562164, 0.010321399733918359, 0.009912579142184434, 0.011180976389109023, 0.09055102581623942, 0.009709634753989473, 0.009734946999185699, 0.08114717589757804, 0.010137813531660609, 0.01007610267237285, 0.009964266124333501, 0.10447744793752778, 0.009726880776353789, 0.012501935429433932, 0.10234890712130511, 0.00944188391856317, 0.012387301205011199, 0.011860782368469755, 0.011079005592939804, 0.009789702981444342, 0.010757219816120912, 0.011548241571884374, 0.009814605530237362, 0.009983197123059357, 0.010021014469295588, 0.009732250121840256, 0.010348288672596065, 0.01172387430548896, 0.00983308175369641, 0.08056717306109411, 0.015563167897718293, 0.01020985595853429, 0.010791294899179923, 0.010424640305264264, 0.009831120122262105, 0.009843721838515937, 0.010724369263067385, 0.01217221551128112, 0.010721478694384652, 0.0103142060605543, 0.011075224428038512, 0.01278057463267552, 0.04985567208198945, 0.009614138039570225, 0.00952044216326761, 0.10285821791776285, 0.009760742163646738, 0.010492925142527235, 0.011061493103031297, 0.046117627693872364, 0.0099637879590903, 0.009848638694277224, 0.011025924815283137, 0.009876903284303084, 0.009722050082185591, 0.01098387471485731, 0.039012030365743806, 0.01245683318298614, 0.009796600182996872, 0.09268840046941626]
[129.34385020204877, 93.43481190091025, 120.71575086985379, 90.90577715137435, 29.965721024985747, 95.9028462369653, 122.96651283499446, 113.18766901410606, 117.49679384941386, 10.746792413705226, 39.74208160897835, 66.35422107248905, 7.844713555793515, 33.77442218624833, 40.6762787970548, 11.754324144120062, 57.79259962704005, 7.061885052062991, 9.702758594262992, 7.9145180404097335, 5.227311039401713, 76.66043130753603, 5.688174895324657, 74.5588147967006, 5.406848100995474, 6.3895338671472945, 10.351099459959892, 37.73501833263826, 5.698860013396551, 5.066309204172274, 4.392370968344977, 56.236458433896495, 6.656913882042429, 5.166893445776533, 32.17435128061422, 106.66167136099725, 4.932313794876108, 90.57055547872903, 127.58795236965032, 70.42944637299088, 5.800378551663148, 40.02045064764646, 125.50710283234156, 5.444918526285352, 5.656647051601182, 17.826859674291534, 16.866331846565018, 4.8948484686191795, 43.32780497117812, 41.629574930094506, 52.51394061114372, 4.822584439882687, 5.13270736773901, 42.28579810206658, 5.798037821579434, 126.15155712976916, 107.7845964199148, 41.840061594518815, 113.51175974563988, 6.379792993914515, 8.717216824809354, 4.900565943343348, 5.282605276098878, 4.838068959577864, 20.303293456378924, 33.91559783870409, 33.76239838583445, 6.516712762340256, 28.267660304459884, 96.2259969464237, 128.52464138678656, 125.59449481064428, 122.5415035754697, 114.80943140406855, 92.9277904377135, 135.25410837876137, 130.91002351741167, 9.530965404026267, 126.82724178621774, 130.6877788726495, 17.219900053713356, 131.43579140109534, 126.67476119307575, 118.51290472070946, 123.71196809861206, 100.15674346504028, 76.09702756327083, 125.81811433778176, 128.18781913704342, 9.473183337114683, 132.90239146363956, 128.2517870687707, 8.164525984997818, 103.44346446168909, 130.11393333925784, 12.415851515431022, 121.05089698536328, 125.23541542579039, 19.943555302457145, 126.38122264029384, 14.598990294087644, 105.07867182630116, 129.92190030416964, 12.439955050354644, 124.63748648014915, 92.03818480492984, 102.70716682673422, 8.861585888735823, 103.8953161454539, 104.73672568609884, 102.86059537093107, 9.504199343887507, 64.87050150195172, 102.73705262105786, 98.44456779923198, 9.714161871446546, 102.32155579904732, 96.47377142784478, 88.00908114936661, 26.19102626846483, 91.08146673340018, 92.54236370694926, 84.43917256748499, 101.68864896955003, 82.45903685150549, 104.76828989272364, 88.0484112548353, 90.92220697220216, 102.54833535573829, 96.10451865171353, 93.51366955443429, 16.532746544398016, 97.99823095303908, 101.21665495675325, 96.88608384323912, 100.88191838432375, 89.43762737698499, 11.043497199350998, 102.99048577384666, 102.72269587945853, 12.323287766196266, 98.64059906773572, 99.24472115015747, 100.35862024579244, 9.571443596114152, 102.80788086053413, 79.98761516922012, 9.770500029030973, 105.91106696767949, 80.7278343724666, 84.31147026678116, 90.26080830189841, 102.14814503518912, 92.96082232152463, 86.59326996021835, 101.88896506529443, 100.16831158128524, 99.79029598889437, 102.75116108615913, 96.6343355542604, 85.29603558883376, 101.69751712112881, 12.412003077751029, 64.25427050405396, 97.94457473850183, 92.66728500543474, 95.92657115421213, 101.71780911674016, 101.58759221408094, 93.24557700971776, 82.15431275212039, 93.27071652194277, 96.95365732748009, 90.29162402058031, 78.24374323853522, 20.057898294008837, 104.01348471221891, 105.03713828106274, 9.722120606829096, 102.45122586317629, 95.30230954827422, 90.40370867527454, 21.683682574437142, 100.36343648678977, 101.53687540401629, 90.6953400057554, 101.24630880908339, 102.85896406071511, 91.04255337575479, 25.633118569447568, 80.27722498249598, 102.07622862221274, 10.788836520379515]
Elapsed: 2.1967366590554263~2.906713749775641
Time per graph: 0.04483136038888625~0.05932068877093145
Speed: 70.10971099029527~44.11980106424042
Total Time: 4.5433
best val loss: 0.31275859475135803 test_score: 0.9388

Testing...
Test loss: 0.3218 score: 0.9592 time: 4.63s
test Score 0.9592
Epoch Time List: [1.4648569519631565, 14.679164236760698, 1.768648892058991, 2.049244328052737, 15.552218958036974, 1.5910246461862698, 1.473922613891773, 1.6390828461153433, 12.682095161871985, 23.609958324814215, 4.32757701294031, 20.985241739079356, 26.5067766798893, 16.65743023622781, 13.223418735084124, 15.343166811973788, 4.225896264077164, 25.234772425028495, 36.71487993304618, 19.74811875494197, 13.287213095929474, 15.296769196982495, 24.923783584032208, 22.129780369112268, 20.786021510954015, 17.772980110021308, 16.274315660935827, 26.49318890599534, 32.741746840882115, 17.515109239029698, 21.664561629178934, 11.545491831144318, 16.933984971954487, 16.389003560878336, 14.341399525874294, 18.641814125003293, 36.7008740111487, 16.728734908043407, 1.5040760338306427, 18.01873376907315, 25.876490044873208, 16.56735022587236, 25.75458689185325, 10.205092184012756, 22.91293591307476, 19.130165235023014, 14.173297197092324, 11.388201162102632, 12.069619476096705, 21.53226773010101, 24.823382937931456, 34.72187739692163, 27.52387919393368, 17.217075466061942, 25.797222696011886, 16.296394822071306, 1.717080986010842, 23.41631297499407, 31.23469104990363, 20.449511325918138, 29.22951340896543, 22.506461179000326, 16.09225024515763, 26.796041983994655, 20.80265552806668, 15.385184762999415, 18.45044297503773, 35.278237930033356, 18.232675703824498, 1.5593645239714533, 1.4374329330166802, 1.582406408037059, 9.29123984312173, 1.6131818380672485, 1.642197441891767, 7.700346781872213, 1.4679736291291192, 10.368916408973746, 1.4240315899951383, 1.5368347759358585, 4.025559218018316, 12.103291216073558, 1.5869333120062947, 1.626648087054491, 6.393705940106884, 1.6103178969351575, 1.730104642920196, 13.58051778213121, 1.5723858989076689, 11.567283827927895, 1.4135134479729459, 1.50696128513664, 7.070975793059915, 5.4269721179734915, 1.432181519921869, 12.219449613941833, 1.5867652621818706, 1.601573443855159, 7.277727008913644, 1.827477499959059, 4.513199654058553, 13.827869447995909, 1.427716471021995, 9.814884983934462, 3.926816009916365, 1.8130815419135615, 1.4322720399359241, 8.706856072996743, 7.857471813214943, 1.5380316189257428, 1.5050140849780291, 6.23384652484674, 7.708297906094231, 1.650624569854699, 1.4509027228923514, 6.493880560039543, 9.457935028942302, 1.6623729380080476, 1.58909041096922, 11.44321760488674, 1.562241823063232, 1.57356577005703, 16.186026719864458, 1.73164457292296, 1.679226907901466, 11.560521323117428, 1.58594948600512, 5.913447241997346, 1.4263038870412856, 1.563641051063314, 1.5872836579801515, 8.644277739804238, 1.4630745209287852, 1.7587389380205423, 11.320093167945743, 1.7138022249564528, 1.6372137300204486, 6.012105240020901, 1.5600656089372933, 1.5639515249058604, 5.001586258993484, 4.78549433092121, 1.466887115035206, 1.6181091798935086, 6.319801094126888, 3.917373013915494, 1.66522537113633, 7.197710798936896, 7.9927208460867405, 1.6953437429619953, 1.9117801840184256, 17.760334430029616, 1.4560229619964957, 1.5862863978836685, 1.6178083108970895, 7.783476858166978, 1.475801674183458, 1.587430469924584, 7.9110484848497435, 1.7138111719395965, 1.5339656791184098, 1.6660377649823204, 4.979410671978258, 9.007066341000609, 1.6254468600964174, 10.26472912915051, 1.4653030631598085, 1.5405019549652934, 1.6037454560864717, 15.446149421972223, 1.6709532081149518, 1.4563767310464755, 9.252124111982994, 1.702271484886296, 1.5947653689654544, 9.456291201990098, 1.4659453850472346, 1.6058714230312034, 6.095379068865441, 5.639991333824582, 1.4700892800465226, 1.6337756499415264, 8.374015923938714, 1.569390235003084, 1.4975544488988817, 1.6966970460489392, 4.347023052861914, 1.5601444960338995, 1.832642234978266, 7.486872715991922, 1.6048489679815248, 1.7191942100180313, 5.559270278085023]
Total Epoch List: [105, 88]
Total Time List: [0.39390551194082946, 4.543259626021609]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba2ef0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.53s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.56s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.43s
Epoch 4/1000, LR 0.000060
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 5.55s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 12.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.44s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.57s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 11.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.45s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.48s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 5.37s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 4.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.41s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.42s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.47s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 4.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.88s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.43s
Epoch 16/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.45s
Epoch 18/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 3.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 2.87s
Epoch 19/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.42s
Epoch 20/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.44s
Epoch 21/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 5.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 1.36s
Epoch 23/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.63s
Val loss: 0.6902 score: 0.5102 time: 0.41s
Test loss: 0.6891 score: 0.5417 time: 0.46s
Epoch 24/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.72s
Val loss: 0.6897 score: 0.5714 time: 0.41s
Test loss: 0.6885 score: 0.5625 time: 0.51s
Epoch 25/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 15.21s
Val loss: 0.6891 score: 0.6122 time: 0.45s
Test loss: 0.6879 score: 0.5625 time: 0.53s
Epoch 26/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.78s
Val loss: 0.6885 score: 0.6327 time: 0.39s
Test loss: 0.6872 score: 0.6667 time: 0.52s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.58s
Val loss: 0.6878 score: 0.6531 time: 0.40s
Test loss: 0.6864 score: 0.6667 time: 0.56s
Epoch 28/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 4.47s
Val loss: 0.6871 score: 0.6939 time: 3.15s
Test loss: 0.6855 score: 0.7292 time: 2.33s
Epoch 29/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.65s
Val loss: 0.6863 score: 0.6939 time: 0.41s
Test loss: 0.6845 score: 0.7708 time: 0.55s
Epoch 30/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.56s
Val loss: 0.6854 score: 0.7143 time: 0.41s
Test loss: 0.6834 score: 0.8125 time: 0.50s
Epoch 31/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.61s
Val loss: 0.6844 score: 0.7755 time: 0.41s
Test loss: 0.6822 score: 0.8333 time: 0.44s
Epoch 32/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.61s
Val loss: 0.6833 score: 0.7755 time: 0.42s
Test loss: 0.6809 score: 0.8542 time: 0.59s
Epoch 33/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 12.88s
Val loss: 0.6821 score: 0.7551 time: 2.60s
Test loss: 0.6795 score: 0.8125 time: 2.16s
Epoch 34/1000, LR 0.000270
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.67s
Val loss: 0.6808 score: 0.7143 time: 0.50s
Test loss: 0.6779 score: 0.7708 time: 0.41s
Epoch 35/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.58s
Val loss: 0.6794 score: 0.7551 time: 0.39s
Test loss: 0.6762 score: 0.7708 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.58s
Val loss: 0.6778 score: 0.7347 time: 0.50s
Test loss: 0.6743 score: 0.7708 time: 0.43s
Epoch 37/1000, LR 0.000270
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.62s
Val loss: 0.6760 score: 0.7143 time: 0.41s
Test loss: 0.6721 score: 0.7708 time: 0.45s
Epoch 38/1000, LR 0.000270
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.82s
Val loss: 0.6740 score: 0.7143 time: 0.50s
Test loss: 0.6698 score: 0.7708 time: 0.46s
Epoch 39/1000, LR 0.000269
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 14.69s
Val loss: 0.6718 score: 0.7143 time: 0.59s
Test loss: 0.6672 score: 0.7917 time: 0.98s
Epoch 40/1000, LR 0.000269
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.60s
Val loss: 0.6694 score: 0.7347 time: 0.43s
Test loss: 0.6643 score: 0.7917 time: 0.52s
Epoch 41/1000, LR 0.000269
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.58s
Val loss: 0.6667 score: 0.7347 time: 3.07s
Test loss: 0.6611 score: 0.7917 time: 4.33s
Epoch 42/1000, LR 0.000269
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 8.47s
Val loss: 0.6638 score: 0.7551 time: 0.48s
Test loss: 0.6576 score: 0.7917 time: 0.45s
Epoch 43/1000, LR 0.000269
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.57s
Val loss: 0.6607 score: 0.7551 time: 0.39s
Test loss: 0.6538 score: 0.7917 time: 0.43s
Epoch 44/1000, LR 0.000269
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.58s
Val loss: 0.6573 score: 0.7551 time: 0.52s
Test loss: 0.6497 score: 0.7917 time: 0.42s
Epoch 45/1000, LR 0.000269
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.66s
Val loss: 0.6537 score: 0.7551 time: 0.43s
Test loss: 0.6453 score: 0.7917 time: 0.46s
Epoch 46/1000, LR 0.000269
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 7.08s
Val loss: 0.6497 score: 0.7755 time: 0.42s
Test loss: 0.6405 score: 0.8125 time: 0.51s
Epoch 47/1000, LR 0.000269
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.62s
Val loss: 0.6454 score: 0.7755 time: 0.41s
Test loss: 0.6354 score: 0.8125 time: 0.43s
Epoch 48/1000, LR 0.000269
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.58s
Val loss: 0.6409 score: 0.7755 time: 0.43s
Test loss: 0.6299 score: 0.8125 time: 0.57s
Epoch 49/1000, LR 0.000269
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 6.81s
Val loss: 0.6360 score: 0.7755 time: 4.33s
Test loss: 0.6240 score: 0.8125 time: 0.73s
Epoch 50/1000, LR 0.000269
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.79s
Val loss: 0.6308 score: 0.7755 time: 0.47s
Test loss: 0.6177 score: 0.8125 time: 0.59s
Epoch 51/1000, LR 0.000269
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.73s
Val loss: 0.6252 score: 0.7755 time: 0.42s
Test loss: 0.6110 score: 0.8125 time: 0.45s
Epoch 52/1000, LR 0.000269
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 12.59s
Val loss: 0.6192 score: 0.7959 time: 1.67s
Test loss: 0.6038 score: 0.8125 time: 0.51s
Epoch 53/1000, LR 0.000269
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.67s
Val loss: 0.6128 score: 0.7959 time: 0.42s
Test loss: 0.5961 score: 0.8125 time: 0.47s
Epoch 54/1000, LR 0.000269
Train loss: 0.5817;  Loss pred: 0.5817; Loss self: 0.0000; time: 0.57s
Val loss: 0.6060 score: 0.7959 time: 0.52s
Test loss: 0.5879 score: 0.8333 time: 0.65s
Epoch 55/1000, LR 0.000269
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 5.58s
Val loss: 0.5987 score: 0.7959 time: 0.42s
Test loss: 0.5792 score: 0.8333 time: 0.44s
Epoch 56/1000, LR 0.000269
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.57s
Val loss: 0.5910 score: 0.7959 time: 0.41s
Test loss: 0.5700 score: 0.8333 time: 0.52s
Epoch 57/1000, LR 0.000269
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.62s
Val loss: 0.5827 score: 0.7959 time: 0.41s
Test loss: 0.5602 score: 0.8542 time: 0.49s
Epoch 58/1000, LR 0.000269
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.70s
Val loss: 0.5740 score: 0.7959 time: 0.61s
Test loss: 0.5499 score: 0.8542 time: 5.45s
Epoch 59/1000, LR 0.000268
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 3.31s
Val loss: 0.5648 score: 0.8163 time: 0.40s
Test loss: 0.5390 score: 0.8750 time: 0.43s
Epoch 60/1000, LR 0.000268
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.62s
Val loss: 0.5551 score: 0.8367 time: 0.50s
Test loss: 0.5277 score: 0.8958 time: 0.45s
Epoch 61/1000, LR 0.000268
Train loss: 0.5034;  Loss pred: 0.5034; Loss self: 0.0000; time: 0.59s
Val loss: 0.5450 score: 0.8367 time: 0.40s
Test loss: 0.5159 score: 0.9167 time: 0.48s
Epoch 62/1000, LR 0.000268
Train loss: 0.4896;  Loss pred: 0.4896; Loss self: 0.0000; time: 12.74s
Val loss: 0.5345 score: 0.8571 time: 0.51s
Test loss: 0.5038 score: 0.9167 time: 0.46s
Epoch 63/1000, LR 0.000268
Train loss: 0.4759;  Loss pred: 0.4759; Loss self: 0.0000; time: 0.61s
Val loss: 0.5237 score: 0.8571 time: 0.43s
Test loss: 0.4914 score: 0.9167 time: 0.41s
Epoch 64/1000, LR 0.000268
Train loss: 0.4671;  Loss pred: 0.4671; Loss self: 0.0000; time: 0.59s
Val loss: 0.5127 score: 0.8571 time: 0.39s
Test loss: 0.4788 score: 0.9167 time: 0.51s
Epoch 65/1000, LR 0.000268
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.57s
Val loss: 0.5014 score: 0.8571 time: 0.42s
Test loss: 0.4660 score: 0.9167 time: 0.46s
Epoch 66/1000, LR 0.000268
Train loss: 0.4348;  Loss pred: 0.4348; Loss self: 0.0000; time: 3.08s
Val loss: 0.4899 score: 0.8571 time: 0.51s
Test loss: 0.4531 score: 0.9167 time: 0.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4205;  Loss pred: 0.4205; Loss self: 0.0000; time: 0.74s
Val loss: 0.4783 score: 0.8571 time: 0.43s
Test loss: 0.4403 score: 0.9167 time: 0.42s
Epoch 68/1000, LR 0.000268
Train loss: 0.4106;  Loss pred: 0.4106; Loss self: 0.0000; time: 0.67s
Val loss: 0.4666 score: 0.8571 time: 1.11s
Test loss: 0.4273 score: 0.9167 time: 6.26s
Epoch 69/1000, LR 0.000268
Train loss: 0.4032;  Loss pred: 0.4032; Loss self: 0.0000; time: 2.17s
Val loss: 0.4553 score: 0.8571 time: 0.41s
Test loss: 0.4149 score: 0.9375 time: 0.43s
Epoch 70/1000, LR 0.000268
Train loss: 0.3739;  Loss pred: 0.3739; Loss self: 0.0000; time: 0.57s
Val loss: 0.4439 score: 0.8571 time: 0.40s
Test loss: 0.4023 score: 0.9375 time: 0.50s
Epoch 71/1000, LR 0.000268
Train loss: 0.3504;  Loss pred: 0.3504; Loss self: 0.0000; time: 0.60s
Val loss: 0.4326 score: 0.8776 time: 2.56s
Test loss: 0.3897 score: 0.9375 time: 4.13s
Epoch 72/1000, LR 0.000267
Train loss: 0.3428;  Loss pred: 0.3428; Loss self: 0.0000; time: 0.60s
Val loss: 0.4215 score: 0.8980 time: 0.40s
Test loss: 0.3772 score: 0.9375 time: 0.50s
Epoch 73/1000, LR 0.000267
Train loss: 0.3285;  Loss pred: 0.3285; Loss self: 0.0000; time: 0.56s
Val loss: 0.4106 score: 0.8980 time: 0.40s
Test loss: 0.3646 score: 0.9375 time: 0.44s
Epoch 74/1000, LR 0.000267
Train loss: 0.3104;  Loss pred: 0.3104; Loss self: 0.0000; time: 0.58s
Val loss: 0.3999 score: 0.8980 time: 0.43s
Test loss: 0.3520 score: 0.9375 time: 0.57s
Epoch 75/1000, LR 0.000267
Train loss: 0.2931;  Loss pred: 0.2931; Loss self: 0.0000; time: 6.88s
Val loss: 0.3900 score: 0.8980 time: 3.06s
Test loss: 0.3404 score: 0.9375 time: 0.42s
Epoch 76/1000, LR 0.000267
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 0.75s
Val loss: 0.3809 score: 0.9184 time: 0.42s
Test loss: 0.3299 score: 0.9375 time: 0.52s
Epoch 77/1000, LR 0.000267
Train loss: 0.2643;  Loss pred: 0.2643; Loss self: 0.0000; time: 0.79s
Val loss: 0.3719 score: 0.9184 time: 0.42s
Test loss: 0.3194 score: 0.9375 time: 0.42s
Epoch 78/1000, LR 0.000267
Train loss: 0.2562;  Loss pred: 0.2562; Loss self: 0.0000; time: 5.55s
Val loss: 0.3633 score: 0.9184 time: 1.17s
Test loss: 0.3095 score: 0.9375 time: 0.51s
Epoch 79/1000, LR 0.000267
Train loss: 0.2435;  Loss pred: 0.2435; Loss self: 0.0000; time: 0.59s
Val loss: 0.3556 score: 0.9184 time: 0.40s
Test loss: 0.3008 score: 0.9167 time: 0.43s
Epoch 80/1000, LR 0.000267
Train loss: 0.2304;  Loss pred: 0.2304; Loss self: 0.0000; time: 0.73s
Val loss: 0.3478 score: 0.9184 time: 0.40s
Test loss: 0.2920 score: 0.9167 time: 0.53s
Epoch 81/1000, LR 0.000267
Train loss: 0.2091;  Loss pred: 0.2091; Loss self: 0.0000; time: 0.57s
Val loss: 0.3400 score: 0.9184 time: 0.42s
Test loss: 0.2830 score: 0.8958 time: 0.45s
Epoch 82/1000, LR 0.000267
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.71s
Val loss: 0.3324 score: 0.9184 time: 4.00s
Test loss: 0.2740 score: 0.8958 time: 0.44s
Epoch 83/1000, LR 0.000266
Train loss: 0.1894;  Loss pred: 0.1894; Loss self: 0.0000; time: 0.58s
Val loss: 0.3250 score: 0.9184 time: 0.41s
Test loss: 0.2650 score: 0.8958 time: 0.51s
Epoch 84/1000, LR 0.000266
Train loss: 0.1763;  Loss pred: 0.1763; Loss self: 0.0000; time: 0.64s
Val loss: 0.3185 score: 0.9184 time: 0.50s
Test loss: 0.2571 score: 0.8958 time: 0.42s
Epoch 85/1000, LR 0.000266
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 0.60s
Val loss: 0.3119 score: 0.9184 time: 0.45s
Test loss: 0.2487 score: 0.8958 time: 3.87s
Epoch 86/1000, LR 0.000266
Train loss: 0.1559;  Loss pred: 0.1559; Loss self: 0.0000; time: 13.89s
Val loss: 0.3065 score: 0.9184 time: 0.54s
Test loss: 0.2418 score: 0.8958 time: 0.42s
Epoch 87/1000, LR 0.000266
Train loss: 0.1342;  Loss pred: 0.1342; Loss self: 0.0000; time: 0.79s
Val loss: 0.3019 score: 0.9184 time: 0.45s
Test loss: 0.2356 score: 0.8958 time: 0.46s
Epoch 88/1000, LR 0.000266
Train loss: 0.1331;  Loss pred: 0.1331; Loss self: 0.0000; time: 0.66s
Val loss: 0.2975 score: 0.9184 time: 0.43s
Test loss: 0.2291 score: 0.8958 time: 0.58s
Epoch 89/1000, LR 0.000266
Train loss: 0.1177;  Loss pred: 0.1177; Loss self: 0.0000; time: 0.65s
Val loss: 0.2941 score: 0.9184 time: 3.28s
Test loss: 0.2239 score: 0.8958 time: 2.81s
Epoch 90/1000, LR 0.000266
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 4.88s
Val loss: 0.2936 score: 0.9184 time: 0.54s
Test loss: 0.2226 score: 0.8958 time: 0.45s
Epoch 91/1000, LR 0.000266
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.64s
Val loss: 0.2947 score: 0.8980 time: 0.43s
Test loss: 0.2227 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.58s
Val loss: 0.2967 score: 0.8980 time: 0.62s
Test loss: 0.2233 score: 0.8958 time: 2.62s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.1209,   Val_Loss: 0.2936,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2936,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2226


[0.3788351740222424, 0.5244298030156642, 0.4059122330509126, 0.5390196479856968, 1.6352017680183053, 0.5109337409958243, 0.3984824719373137, 0.43290934804826975, 0.4170326559105888, 4.5594999990426, 1.2329500120831653, 0.7384609329747036, 6.246244639973156, 1.4508020220091566, 1.2046333010075614, 4.168678641086444, 0.8478594199987128, 6.938657262013294, 5.0501101850531995, 6.1911539969732985, 9.37384433997795, 0.6391824199818075, 8.614362410036847, 0.6571992880199105, 9.0625812090002, 7.668791029020213, 4.7337966550840065, 1.2985285860486329, 8.598210850032046, 9.671734989969991, 11.155706189922057, 0.8713208719855174, 7.360768198035657, 9.483454713015817, 1.5229522290173918, 0.45939651399385184, 9.934485525009222, 0.5410146790090948, 0.3840487999841571, 0.6957317219348624, 8.447724499972537, 1.2243740189587697, 0.3904161509126425, 8.99921638192609, 8.662375352927484, 2.7486613399814814, 2.9051960109500214, 10.010524393990636, 1.1309135099872947, 1.1770478099351749, 0.9330855660373345, 10.16052712209057, 9.546618673019111, 1.1587814869126305, 8.451134937000461, 0.3884216819424182, 0.45461041398812085, 1.171126383007504, 0.43167333595920354, 7.68049998593051, 5.621060137054883, 9.99884514696896, 9.275726168998517, 10.128007767023519, 2.413401555037126, 1.4447629740461707, 1.4513186960248277, 7.519128399086185, 1.7334296320332214, 0.5092178990598768, 0.38124984805472195, 0.39014448900707066, 0.3998645240208134, 0.426794204977341, 0.5272911339998245, 0.3622810470405966, 0.3743028889875859, 5.141137117054313, 0.38635232707019895, 0.374939419911243, 2.845544971060008, 0.3728056070394814, 0.38681738602463156, 0.4134570839814842, 0.396081323036924, 0.48923315899446607, 0.6439147699857131, 0.3894510759273544, 0.3822516080690548, 5.172495691920631, 0.3686916349688545, 0.3820609530666843, 6.001573158078827, 0.473688698024489, 0.3765930269146338, 3.9465678160777315, 0.40478840901050717, 0.3912631249986589, 2.4569340449525043, 0.3877158249961212, 3.356396505027078, 0.46631727588828653, 0.37714965594932437, 3.9389209849759936, 0.39314014895353466, 0.5323877269402146, 0.47708452597726136, 5.529484294936992, 0.4716285759350285, 0.4678397160023451, 0.476372898905538, 5.155615768046118, 0.7553510280558839, 0.47694574401248246, 0.497742039966397, 5.044181952951476, 0.4788824761053547, 0.5079100700095296, 0.5567607269622386, 1.8708697970723733, 0.5379799179499969, 0.5294872319791466, 0.5802993860561401, 0.4818630249937996, 0.5942344450158998, 0.4676987669663504, 0.556512029026635, 0.5389222460798919, 0.4778234559344128, 0.5098615620518103, 0.5239875649567693, 2.9638148669619113, 0.5000090259127319, 0.484110051067546, 0.5057485869619995, 0.48571637796703726, 0.5478678430663422, 4.437000264995731, 0.4757721029454842, 0.4770124029600993, 3.976211618981324, 0.4967528630513698, 0.49372903094626963, 0.4882490400923416, 5.1193949489388615, 0.47661715804133564, 0.6125948360422626, 5.01509644894395, 0.46265231200959533, 0.6069777590455487, 0.581178336055018, 0.5428712740540504, 0.47969544609077275, 0.5271037709899247, 0.5658638370223343, 0.4809156709816307, 0.4891766590299085, 0.4910297089954838, 0.4768802559701726, 0.5070661449572071, 0.574469840968959, 0.4818210059311241, 3.9477914799936116, 0.7625952269881964, 0.5002829419681802, 0.5287734500598162, 0.5108073749579489, 0.4817248859908432, 0.48234237008728087, 0.5254940938903019, 0.5964385600527748, 0.5253524560248479, 0.5053960969671607, 0.5426859969738871, 0.6262481570011005, 2.442927932017483, 0.47109276393894106, 0.4665016660001129, 5.04005267797038, 0.47827636601869017, 0.5141533319838345, 0.5420131620485336, 2.259763756999746, 0.48822560999542475, 0.48258329601958394, 0.5402703159488738, 0.4839682609308511, 0.47638045402709395, 0.5382098610280082, 1.9115894879214466, 0.6103848259663209, 0.48003340896684676, 4.541731623001397, 0.5303660520585254, 0.5601721010170877, 0.43484817806165665, 5.5520992239471525, 0.44321467902045697, 0.46002813801169395, 0.5794523479416966, 0.45696851797401905, 0.4830165409948677, 5.373300906969234, 0.4142206400865689, 0.42752612801268697, 0.4773063790053129, 0.8848324829014018, 0.435814525000751, 0.5358393450733274, 0.45404925697948784, 2.8788131359033287, 0.4227756200125441, 0.4411033010110259, 0.46370866906363517, 1.3650297169806436, 0.46734032791573554, 0.5134544459870085, 0.536171008949168, 0.5249738419661298, 0.568040027981624, 2.337987140053883, 0.5543310249922797, 0.5038121229736134, 0.4458370109787211, 0.5980402800487354, 2.166371482075192, 0.4168738960288465, 0.43351330899167806, 0.43665914493612945, 0.45672500401269644, 0.46447109594009817, 0.9884055310394615, 0.5225400179624557, 4.335009815054946, 0.4592035380192101, 0.43115375994239, 0.4245164319872856, 0.4651972639840096, 0.512143983040005, 0.4389538619434461, 0.5745582489762455, 0.7358518639812246, 0.592525499057956, 0.449658315978013, 0.5177891969215125, 0.47077224298845977, 0.6506838829955086, 0.43952666805125773, 0.5208743989933282, 0.49092658900190145, 5.45463330601342, 0.43077853706199676, 0.4502990059554577, 0.484637732966803, 0.46221532695926726, 0.4108845330774784, 0.5183973669772968, 0.46632977202534676, 0.44081941805779934, 0.4226078559877351, 6.260600631008856, 0.4372676080092788, 0.5030339960940182, 4.139076545950957, 0.5007161790272221, 0.439457074040547, 0.572005856083706, 0.4289892829256132, 0.5194336260901764, 0.41948679101187736, 0.5155855640769005, 0.43169352202676237, 0.5300894500687718, 0.45972393709234893, 0.4450526289874688, 0.5129440309247002, 0.42434892302844673, 3.878748944029212, 0.42444086901377887, 0.4624050320126116, 0.581343958969228, 2.811505528050475, 0.45855827000923455, 0.4418275849893689, 2.6251947979908437]
[0.00773133008208658, 0.010702649041136004, 0.008283923123488012, 0.011000400979299935, 0.0333714646534348, 0.010427219203996415, 0.008132295345659462, 0.008834884654046322, 0.008510870528787526, 0.09305102038862449, 0.025162245144554396, 0.015070631285198033, 0.12747438040761544, 0.029608204530799116, 0.024584353081786966, 0.08507507430788662, 0.017303253469361485, 0.14160525024516926, 0.10306347316435101, 0.12635008157088365, 0.19130294571383571, 0.013044539183302194, 0.1758033144905479, 0.013412230367753275, 0.18495063691837144, 0.15650593936775944, 0.09660809500171441, 0.026500583388747608, 0.17547369081698053, 0.19738234673408145, 0.22766747326371545, 0.017782058611949335, 0.15021975914358485, 0.1935398921023636, 0.03108065773504881, 0.009375439061099018, 0.20274460255120863, 0.011041115898144792, 0.007837730611921574, 0.014198606570099234, 0.17240254081576606, 0.024987224876709586, 0.007967676549237602, 0.1836574771821651, 0.17678317046790784, 0.05609512938737717, 0.05928971450918411, 0.20429641620389052, 0.023079867550761114, 0.02402138387622806, 0.0190425625721905, 0.20735769636919532, 0.1948289525105941, 0.023648601773727154, 0.17247214157143798, 0.007926973100865677, 0.009277763550777977, 0.023900538428724572, 0.008809659917534766, 0.15674489767205121, 0.11471551300112005, 0.20405806422385633, 0.18930053406119424, 0.20669403606170447, 0.04925309295994135, 0.029484958654003485, 0.02961874889846587, 0.15345159998135072, 0.0353761149394535, 0.010392202021630138, 0.007780609143973917, 0.007962132428715728, 0.008160500490220681, 0.008710085815864101, 0.010761043551016827, 0.007393490755930543, 0.007638834469134406, 0.10492116565416966, 0.00788474136877957, 0.007651824896147817, 0.058072346348163426, 0.007608277694683294, 0.007894232367849623, 0.008437899673091514, 0.008083292306876001, 0.009984350183560533, 0.01314111775481047, 0.00794798114137458, 0.007801053225899077, 0.10556113656980878, 0.00752431908099703, 0.007797162307483353, 0.12248108485875157, 0.009667116286214061, 0.00768557197784967, 0.08054220032811697, 0.008260987938989942, 0.007984961734666509, 0.05014151112147968, 0.007912567857063698, 0.06849788785769548, 0.00951667909976095, 0.007696931754067844, 0.08038614255053048, 0.008023268345990504, 0.010865055651841114, 0.00973641889749513, 0.11284661826402025, 0.009625072978265887, 0.009547749306170307, 0.009721895896031387, 0.1052166483274718, 0.015415327103181305, 0.009733586612499642, 0.010158000815640755, 0.1029424888357444, 0.009773111757252137, 0.010365511632847543, 0.011362463815555888, 0.03818101626678313, 0.010979181998979529, 0.010805861877125442, 0.011842844613390614, 0.009833939285587748, 0.012127233571753058, 0.00954487279523164, 0.011357388347482346, 0.010998413185303917, 0.009751499100702301, 0.010405338001057354, 0.010693623774627946, 0.06048601769310023, 0.010204265834953713, 0.009879796960562164, 0.010321399733918359, 0.009912579142184434, 0.011180976389109023, 0.09055102581623942, 0.009709634753989473, 0.009734946999185699, 0.08114717589757804, 0.010137813531660609, 0.01007610267237285, 0.009964266124333501, 0.10447744793752778, 0.009726880776353789, 0.012501935429433932, 0.10234890712130511, 0.00944188391856317, 0.012387301205011199, 0.011860782368469755, 0.011079005592939804, 0.009789702981444342, 0.010757219816120912, 0.011548241571884374, 0.009814605530237362, 0.009983197123059357, 0.010021014469295588, 0.009732250121840256, 0.010348288672596065, 0.01172387430548896, 0.00983308175369641, 0.08056717306109411, 0.015563167897718293, 0.01020985595853429, 0.010791294899179923, 0.010424640305264264, 0.009831120122262105, 0.009843721838515937, 0.010724369263067385, 0.01217221551128112, 0.010721478694384652, 0.0103142060605543, 0.011075224428038512, 0.01278057463267552, 0.04985567208198945, 0.009614138039570225, 0.00952044216326761, 0.10285821791776285, 0.009760742163646738, 0.010492925142527235, 0.011061493103031297, 0.046117627693872364, 0.0099637879590903, 0.009848638694277224, 0.011025924815283137, 0.009876903284303084, 0.009722050082185591, 0.01098387471485731, 0.039012030365743806, 0.01245683318298614, 0.009796600182996872, 0.09268840046941626, 0.011049292751219278, 0.01167025210452266, 0.00905933704295118, 0.11566873383223235, 0.00923363914625952, 0.00958391954191029, 0.012071923915452013, 0.009520177457792064, 0.010062844604059743, 0.11194376889519238, 0.008629596668470185, 0.008906794333597645, 0.00994388289594402, 0.01843401006044587, 0.00907946927084898, 0.011163319689027654, 0.009459359520405997, 0.05997527366465268, 0.008807825416928003, 0.009189652104396373, 0.009660597272159066, 0.02843811910376341, 0.009736256831577824, 0.010696967624729345, 0.011170229353107667, 0.010936955040961038, 0.011834167249617167, 0.048708065417789236, 0.011548563020672495, 0.010496085895283613, 0.009288271062056689, 0.012459172501015322, 0.04513273920989983, 0.008684872833934302, 0.009031527270659959, 0.009097065519502697, 0.009515104250264509, 0.009676481165418712, 0.020591781896655448, 0.010886250374217829, 0.09031270448031137, 0.00956674037540021, 0.008982369998799792, 0.008844092333068451, 0.009691609666333534, 0.010669666313333437, 0.009144872123821793, 0.011969963520338448, 0.015330247166275512, 0.012344281230374085, 0.00936788158287527, 0.010787274935864843, 0.009807755062259579, 0.013555914229073096, 0.009156805584401203, 0.010851549979027672, 0.010227637270872947, 0.11363819387527958, 0.008974552855458265, 0.009381229290738702, 0.010096619436808396, 0.009629485978318067, 0.008560094439114133, 0.010799945145360349, 0.00971520358386139, 0.009183737876204153, 0.008804330333077814, 0.1304291798126845, 0.009109741833526641, 0.010479874918625379, 0.08623076137397827, 0.010431587063067127, 0.009155355709178062, 0.011916788668410542, 0.008937276727616942, 0.010821533876878675, 0.008739308146080779, 0.01074136591826876, 0.008993615042224215, 0.01104353020976608, 0.00957758202275727, 0.009271929770572266, 0.01068633397759792, 0.00884060256309264, 0.08080726966727525, 0.008842518104453726, 0.009633438166929409, 0.012111332478525583, 0.05857303183438489, 0.009553297291859053, 0.009204741353945186, 0.05469155829147591]
[129.34385020204877, 93.43481190091025, 120.71575086985379, 90.90577715137435, 29.965721024985747, 95.9028462369653, 122.96651283499446, 113.18766901410606, 117.49679384941386, 10.746792413705226, 39.74208160897835, 66.35422107248905, 7.844713555793515, 33.77442218624833, 40.6762787970548, 11.754324144120062, 57.79259962704005, 7.061885052062991, 9.702758594262992, 7.9145180404097335, 5.227311039401713, 76.66043130753603, 5.688174895324657, 74.5588147967006, 5.406848100995474, 6.3895338671472945, 10.351099459959892, 37.73501833263826, 5.698860013396551, 5.066309204172274, 4.392370968344977, 56.236458433896495, 6.656913882042429, 5.166893445776533, 32.17435128061422, 106.66167136099725, 4.932313794876108, 90.57055547872903, 127.58795236965032, 70.42944637299088, 5.800378551663148, 40.02045064764646, 125.50710283234156, 5.444918526285352, 5.656647051601182, 17.826859674291534, 16.866331846565018, 4.8948484686191795, 43.32780497117812, 41.629574930094506, 52.51394061114372, 4.822584439882687, 5.13270736773901, 42.28579810206658, 5.798037821579434, 126.15155712976916, 107.7845964199148, 41.840061594518815, 113.51175974563988, 6.379792993914515, 8.717216824809354, 4.900565943343348, 5.282605276098878, 4.838068959577864, 20.303293456378924, 33.91559783870409, 33.76239838583445, 6.516712762340256, 28.267660304459884, 96.2259969464237, 128.52464138678656, 125.59449481064428, 122.5415035754697, 114.80943140406855, 92.9277904377135, 135.25410837876137, 130.91002351741167, 9.530965404026267, 126.82724178621774, 130.6877788726495, 17.219900053713356, 131.43579140109534, 126.67476119307575, 118.51290472070946, 123.71196809861206, 100.15674346504028, 76.09702756327083, 125.81811433778176, 128.18781913704342, 9.473183337114683, 132.90239146363956, 128.2517870687707, 8.164525984997818, 103.44346446168909, 130.11393333925784, 12.415851515431022, 121.05089698536328, 125.23541542579039, 19.943555302457145, 126.38122264029384, 14.598990294087644, 105.07867182630116, 129.92190030416964, 12.439955050354644, 124.63748648014915, 92.03818480492984, 102.70716682673422, 8.861585888735823, 103.8953161454539, 104.73672568609884, 102.86059537093107, 9.504199343887507, 64.87050150195172, 102.73705262105786, 98.44456779923198, 9.714161871446546, 102.32155579904732, 96.47377142784478, 88.00908114936661, 26.19102626846483, 91.08146673340018, 92.54236370694926, 84.43917256748499, 101.68864896955003, 82.45903685150549, 104.76828989272364, 88.0484112548353, 90.92220697220216, 102.54833535573829, 96.10451865171353, 93.51366955443429, 16.532746544398016, 97.99823095303908, 101.21665495675325, 96.88608384323912, 100.88191838432375, 89.43762737698499, 11.043497199350998, 102.99048577384666, 102.72269587945853, 12.323287766196266, 98.64059906773572, 99.24472115015747, 100.35862024579244, 9.571443596114152, 102.80788086053413, 79.98761516922012, 9.770500029030973, 105.91106696767949, 80.7278343724666, 84.31147026678116, 90.26080830189841, 102.14814503518912, 92.96082232152463, 86.59326996021835, 101.88896506529443, 100.16831158128524, 99.79029598889437, 102.75116108615913, 96.6343355542604, 85.29603558883376, 101.69751712112881, 12.412003077751029, 64.25427050405396, 97.94457473850183, 92.66728500543474, 95.92657115421213, 101.71780911674016, 101.58759221408094, 93.24557700971776, 82.15431275212039, 93.27071652194277, 96.95365732748009, 90.29162402058031, 78.24374323853522, 20.057898294008837, 104.01348471221891, 105.03713828106274, 9.722120606829096, 102.45122586317629, 95.30230954827422, 90.40370867527454, 21.683682574437142, 100.36343648678977, 101.53687540401629, 90.6953400057554, 101.24630880908339, 102.85896406071511, 91.04255337575479, 25.633118569447568, 80.27722498249598, 102.07622862221274, 10.788836520379515, 90.50353018202463, 85.68795181489374, 110.38335313708991, 8.645378633178565, 108.2996621548821, 104.34144356356705, 82.83683752512755, 105.04005880494603, 99.37547873854288, 8.933056389554405, 115.88027093475684, 112.2738397840695, 100.56433794165929, 54.247556376553945, 110.13859622947923, 89.57908828705253, 105.71540259599732, 16.67353792483593, 113.53540206167943, 108.81804758654522, 103.51326857211055, 35.16406961906504, 102.7088764500005, 93.48443737346565, 89.52367658608462, 91.43312706825658, 84.50108730991188, 20.530480761709303, 86.59085967751581, 95.27361056080412, 107.66266330071674, 80.26215223511096, 22.156864783882888, 115.14273370736204, 110.72324425666348, 109.92555762692433, 105.09606344798598, 103.34335208275363, 48.56306292572095, 91.85899328278582, 11.07263928983552, 104.52881135683234, 111.32919264443774, 113.06982812254866, 103.1820341953901, 93.72364332990824, 109.35090031440302, 83.54244340852638, 65.23052036629036, 81.00917188595967, 106.74771997844485, 92.7018182020432, 101.96013192132197, 73.76853992298949, 109.20839050066975, 92.15273411933387, 97.77429268516171, 8.79985826858109, 111.42616418954024, 106.59583824341826, 99.04305161333347, 103.84770300840759, 116.82114106482777, 92.59306288510173, 102.93145083044584, 108.88812523613998, 113.58047258211181, 7.666996000712012, 109.77259490710193, 95.42098620115665, 11.59678963824815, 95.86269030342321, 109.2256851361351, 83.9152247996842, 111.89090709364692, 92.40834168034195, 114.42553383913565, 93.09802939486629, 111.18999371277175, 90.55075514853702, 104.41048665768692, 107.85241311618344, 93.57746090439711, 113.11446169684814, 12.37512421985683, 113.08995788160482, 103.80509872714977, 82.56729817079042, 17.07270340431579, 104.67590083814946, 108.63966314179989, 18.284357426251237]
Elapsed: 1.7973848661195422~2.5656650241828487
Time per graph: 0.03681302882229638~0.05240598698024231
Speed: 75.7244714653651~41.393793441610995
Total Time: 2.6263
best val loss: 0.29355090856552124 test_score: 0.8958

Testing...
Test loss: 0.3299 score: 0.9375 time: 4.61s
test Score 0.9375
Epoch Time List: [1.4648569519631565, 14.679164236760698, 1.768648892058991, 2.049244328052737, 15.552218958036974, 1.5910246461862698, 1.473922613891773, 1.6390828461153433, 12.682095161871985, 23.609958324814215, 4.32757701294031, 20.985241739079356, 26.5067766798893, 16.65743023622781, 13.223418735084124, 15.343166811973788, 4.225896264077164, 25.234772425028495, 36.71487993304618, 19.74811875494197, 13.287213095929474, 15.296769196982495, 24.923783584032208, 22.129780369112268, 20.786021510954015, 17.772980110021308, 16.274315660935827, 26.49318890599534, 32.741746840882115, 17.515109239029698, 21.664561629178934, 11.545491831144318, 16.933984971954487, 16.389003560878336, 14.341399525874294, 18.641814125003293, 36.7008740111487, 16.728734908043407, 1.5040760338306427, 18.01873376907315, 25.876490044873208, 16.56735022587236, 25.75458689185325, 10.205092184012756, 22.91293591307476, 19.130165235023014, 14.173297197092324, 11.388201162102632, 12.069619476096705, 21.53226773010101, 24.823382937931456, 34.72187739692163, 27.52387919393368, 17.217075466061942, 25.797222696011886, 16.296394822071306, 1.717080986010842, 23.41631297499407, 31.23469104990363, 20.449511325918138, 29.22951340896543, 22.506461179000326, 16.09225024515763, 26.796041983994655, 20.80265552806668, 15.385184762999415, 18.45044297503773, 35.278237930033356, 18.232675703824498, 1.5593645239714533, 1.4374329330166802, 1.582406408037059, 9.29123984312173, 1.6131818380672485, 1.642197441891767, 7.700346781872213, 1.4679736291291192, 10.368916408973746, 1.4240315899951383, 1.5368347759358585, 4.025559218018316, 12.103291216073558, 1.5869333120062947, 1.626648087054491, 6.393705940106884, 1.6103178969351575, 1.730104642920196, 13.58051778213121, 1.5723858989076689, 11.567283827927895, 1.4135134479729459, 1.50696128513664, 7.070975793059915, 5.4269721179734915, 1.432181519921869, 12.219449613941833, 1.5867652621818706, 1.601573443855159, 7.277727008913644, 1.827477499959059, 4.513199654058553, 13.827869447995909, 1.427716471021995, 9.814884983934462, 3.926816009916365, 1.8130815419135615, 1.4322720399359241, 8.706856072996743, 7.857471813214943, 1.5380316189257428, 1.5050140849780291, 6.23384652484674, 7.708297906094231, 1.650624569854699, 1.4509027228923514, 6.493880560039543, 9.457935028942302, 1.6623729380080476, 1.58909041096922, 11.44321760488674, 1.562241823063232, 1.57356577005703, 16.186026719864458, 1.73164457292296, 1.679226907901466, 11.560521323117428, 1.58594948600512, 5.913447241997346, 1.4263038870412856, 1.563641051063314, 1.5872836579801515, 8.644277739804238, 1.4630745209287852, 1.7587389380205423, 11.320093167945743, 1.7138022249564528, 1.6372137300204486, 6.012105240020901, 1.5600656089372933, 1.5639515249058604, 5.001586258993484, 4.78549433092121, 1.466887115035206, 1.6181091798935086, 6.319801094126888, 3.917373013915494, 1.66522537113633, 7.197710798936896, 7.9927208460867405, 1.6953437429619953, 1.9117801840184256, 17.760334430029616, 1.4560229619964957, 1.5862863978836685, 1.6178083108970895, 7.783476858166978, 1.475801674183458, 1.587430469924584, 7.9110484848497435, 1.7138111719395965, 1.5339656791184098, 1.6660377649823204, 4.979410671978258, 9.007066341000609, 1.6254468600964174, 10.26472912915051, 1.4653030631598085, 1.5405019549652934, 1.6037454560864717, 15.446149421972223, 1.6709532081149518, 1.4563767310464755, 9.252124111982994, 1.702271484886296, 1.5947653689654544, 9.456291201990098, 1.4659453850472346, 1.6058714230312034, 6.095379068865441, 5.639991333824582, 1.4700892800465226, 1.6337756499415264, 8.374015923938714, 1.569390235003084, 1.4975544488988817, 1.6966970460489392, 4.347023052861914, 1.5601444960338995, 1.832642234978266, 7.486872715991922, 1.6048489679815248, 1.7191942100180313, 5.559270278085023, 1.7058639500755817, 1.5595742838922888, 1.450502936844714, 6.824670996051282, 13.712896589073353, 1.711916932137683, 1.67766466003377, 12.16777938103769, 1.574632311007008, 6.646976204123348, 4.931898104026914, 1.5598914319416508, 1.5036902200663462, 5.615066247875802, 1.5195877159712836, 1.687674400047399, 1.484258570941165, 6.914558242075145, 1.5249762570019811, 1.626899200025946, 1.4757474340731278, 7.5681130659068, 1.4974619600689039, 1.637317046057433, 16.1876099340152, 1.6833498009946197, 1.5372797668678686, 9.950011232984252, 1.6071680128807202, 1.466499152011238, 1.4613840230740607, 1.6196390360128134, 17.639411447918974, 1.5859439560445026, 1.394877157988958, 1.51608893298544, 1.481946327839978, 1.7806207998655736, 16.264962375047617, 1.547412435989827, 7.9803758460329846, 9.400674511911348, 1.3952623719815165, 1.5235774850007147, 1.5491130540613085, 8.013901141937822, 1.459429674083367, 1.5844375480664894, 11.86930737295188, 1.8497726559871808, 1.593992793932557, 14.77101146208588, 1.5653542840154842, 1.7373192829545587, 6.428688969928771, 1.4967203718842939, 1.5114233300555497, 6.76055394904688, 4.137160778976977, 1.563617697916925, 1.4661907189292833, 13.706091322936118, 1.4527051381301135, 1.49507716181688, 1.448390483041294, 4.025849651894532, 1.5788025299552828, 8.036403684993275, 3.015370798064396, 1.4690183700295165, 7.295132523984648, 1.490887948893942, 1.3901504260720685, 1.5735608718823642, 10.363668504054658, 1.6777627950068563, 1.6257575209019706, 7.226950260112062, 1.4174264537869021, 1.6630064260680228, 1.4436526988865808, 5.155186551855877, 1.5011995180975646, 1.561722789076157, 4.920171800884418, 14.853557381080464, 1.7028115909779444, 1.6657210839912295, 6.729119301890023, 5.878132953774184, 1.503015803056769, 3.8177389960037544]
Total Epoch List: [105, 88, 92]
Total Time List: [0.39390551194082946, 4.543259626021609, 2.62629599601496]
========================training times:5========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba0d90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 5.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 3.91s
Epoch 2/1000, LR 0.000000
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.35s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.40s
Epoch 4/1000, LR 0.000060
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 11.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 3.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.41s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.38s
Epoch 6/1000, LR 0.000120
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.35s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 3.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 4.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 4.79s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.43s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.37s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.37s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 5.43s
Val loss: 0.6923 score: 0.5714 time: 5.49s
Test loss: 0.6928 score: 0.5102 time: 4.65s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 8.47s
Val loss: 0.6922 score: 0.7551 time: 0.56s
Test loss: 0.6926 score: 0.6531 time: 0.39s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.63s
Val loss: 0.6920 score: 0.5102 time: 0.46s
Test loss: 0.6924 score: 0.5306 time: 0.39s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.36s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 5.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 3.48s
Epoch 16/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.37s
Epoch 17/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.36s
Epoch 18/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 3.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 4.58s
Epoch 19/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.36s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.37s
Epoch 21/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 2.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 4.65s
Epoch 22/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 6.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4898 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.37s
Epoch 23/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 0.45s
Epoch 24/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5102 time: 0.40s
Epoch 25/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5102 time: 4.29s
Epoch 26/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 7.65s
Val loss: 0.6847 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5102 time: 0.37s
Epoch 27/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.80s
Val loss: 0.6835 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5102 time: 0.37s
Epoch 28/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.64s
Val loss: 0.6822 score: 0.5102 time: 0.59s
Test loss: 0.6861 score: 0.5306 time: 4.84s
Epoch 29/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 5.48s
Val loss: 0.6807 score: 0.5306 time: 0.48s
Test loss: 0.6852 score: 0.5510 time: 0.38s
Epoch 30/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.76s
Val loss: 0.6789 score: 0.5714 time: 0.56s
Test loss: 0.6842 score: 0.5510 time: 0.37s
Epoch 31/1000, LR 0.000270
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.57s
Val loss: 0.6771 score: 0.5714 time: 0.46s
Test loss: 0.6830 score: 0.5714 time: 3.96s
Epoch 32/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 7.03s
Val loss: 0.6750 score: 0.5918 time: 0.43s
Test loss: 0.6817 score: 0.5714 time: 0.42s
Epoch 33/1000, LR 0.000270
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.66s
Val loss: 0.6727 score: 0.5918 time: 0.46s
Test loss: 0.6803 score: 0.5510 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.68s
Val loss: 0.6702 score: 0.5918 time: 4.28s
Test loss: 0.6787 score: 0.5714 time: 6.63s
Epoch 35/1000, LR 0.000270
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 1.50s
Val loss: 0.6674 score: 0.5918 time: 0.58s
Test loss: 0.6770 score: 0.5714 time: 0.39s
Epoch 36/1000, LR 0.000270
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.56s
Val loss: 0.6643 score: 0.6122 time: 0.55s
Test loss: 0.6751 score: 0.5918 time: 0.35s
Epoch 37/1000, LR 0.000270
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.60s
Val loss: 0.6610 score: 0.6122 time: 1.28s
Test loss: 0.6729 score: 0.5918 time: 5.56s
Epoch 38/1000, LR 0.000270
Train loss: 0.6651;  Loss pred: 0.6651; Loss self: 0.0000; time: 0.58s
Val loss: 0.6573 score: 0.6531 time: 0.54s
Test loss: 0.6706 score: 0.6327 time: 0.37s
Epoch 39/1000, LR 0.000269
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.65s
Val loss: 0.6533 score: 0.6531 time: 0.47s
Test loss: 0.6680 score: 0.6327 time: 0.52s
Epoch 40/1000, LR 0.000269
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.77s
Val loss: 0.6489 score: 0.6735 time: 0.51s
Test loss: 0.6652 score: 0.6122 time: 6.29s
Epoch 41/1000, LR 0.000269
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 1.26s
Val loss: 0.6440 score: 0.7143 time: 0.47s
Test loss: 0.6621 score: 0.6327 time: 0.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 0.67s
Val loss: 0.6388 score: 0.7551 time: 0.46s
Test loss: 0.6587 score: 0.6122 time: 0.35s
Epoch 43/1000, LR 0.000269
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 0.68s
Val loss: 0.6330 score: 0.7755 time: 0.54s
Test loss: 0.6549 score: 0.6327 time: 0.37s
Epoch 44/1000, LR 0.000269
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 12.09s
Val loss: 0.6267 score: 0.7755 time: 4.80s
Test loss: 0.6509 score: 0.6939 time: 0.48s
Epoch 45/1000, LR 0.000269
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.72s
Val loss: 0.6198 score: 0.8163 time: 0.46s
Test loss: 0.6464 score: 0.6939 time: 0.35s
Epoch 46/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.57s
Val loss: 0.6124 score: 0.8571 time: 0.55s
Test loss: 0.6416 score: 0.6939 time: 0.36s
Epoch 47/1000, LR 0.000269
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 10.06s
Val loss: 0.6044 score: 0.8571 time: 0.55s
Test loss: 0.6363 score: 0.7143 time: 0.52s
Epoch 48/1000, LR 0.000269
Train loss: 0.6138;  Loss pred: 0.6138; Loss self: 0.0000; time: 0.74s
Val loss: 0.5958 score: 0.8980 time: 0.47s
Test loss: 0.6307 score: 0.7347 time: 0.42s
Epoch 49/1000, LR 0.000269
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.70s
Val loss: 0.5867 score: 0.9184 time: 0.46s
Test loss: 0.6247 score: 0.7347 time: 0.36s
Epoch 50/1000, LR 0.000269
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 7.30s
Val loss: 0.5771 score: 0.9388 time: 5.46s
Test loss: 0.6183 score: 0.7347 time: 4.86s
Epoch 51/1000, LR 0.000269
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 4.14s
Val loss: 0.5669 score: 0.9388 time: 0.47s
Test loss: 0.6116 score: 0.7347 time: 0.38s
Epoch 52/1000, LR 0.000269
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.62s
Val loss: 0.5562 score: 0.9592 time: 0.58s
Test loss: 0.6045 score: 0.7347 time: 0.49s
Epoch 53/1000, LR 0.000269
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.75s
Val loss: 0.5449 score: 0.9592 time: 4.20s
Test loss: 0.5968 score: 0.7347 time: 4.83s
Epoch 54/1000, LR 0.000269
Train loss: 0.5603;  Loss pred: 0.5603; Loss self: 0.0000; time: 1.23s
Val loss: 0.5329 score: 0.9592 time: 0.54s
Test loss: 0.5887 score: 0.7551 time: 0.37s
Epoch 55/1000, LR 0.000269
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.73s
Val loss: 0.5204 score: 0.9592 time: 0.45s
Test loss: 0.5802 score: 0.7959 time: 0.37s
Epoch 56/1000, LR 0.000269
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.65s
Val loss: 0.5072 score: 0.9592 time: 0.47s
Test loss: 0.5712 score: 0.8163 time: 0.37s
Epoch 57/1000, LR 0.000269
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.61s
Val loss: 0.4934 score: 0.9592 time: 0.48s
Test loss: 0.5618 score: 0.8163 time: 0.43s
Epoch 58/1000, LR 0.000269
Train loss: 0.5063;  Loss pred: 0.5063; Loss self: 0.0000; time: 9.63s
Val loss: 0.4793 score: 0.9796 time: 0.59s
Test loss: 0.5521 score: 0.8571 time: 0.39s
Epoch 59/1000, LR 0.000268
Train loss: 0.4891;  Loss pred: 0.4891; Loss self: 0.0000; time: 0.60s
Val loss: 0.4648 score: 0.9796 time: 0.47s
Test loss: 0.5420 score: 0.8571 time: 0.41s
Epoch 60/1000, LR 0.000268
Train loss: 0.4791;  Loss pred: 0.4791; Loss self: 0.0000; time: 0.63s
Val loss: 0.4500 score: 0.9796 time: 0.55s
Test loss: 0.5315 score: 0.8571 time: 0.39s
Epoch 61/1000, LR 0.000268
Train loss: 0.4627;  Loss pred: 0.4627; Loss self: 0.0000; time: 0.63s
Val loss: 0.4350 score: 0.9796 time: 0.52s
Test loss: 0.5208 score: 0.8571 time: 4.85s
Epoch 62/1000, LR 0.000268
Train loss: 0.4495;  Loss pred: 0.4495; Loss self: 0.0000; time: 5.79s
Val loss: 0.4202 score: 0.9796 time: 0.56s
Test loss: 0.5100 score: 0.8571 time: 0.37s
Epoch 63/1000, LR 0.000268
Train loss: 0.4426;  Loss pred: 0.4426; Loss self: 0.0000; time: 0.62s
Val loss: 0.4056 score: 0.9796 time: 0.48s
Test loss: 0.4990 score: 0.8776 time: 0.38s
Epoch 64/1000, LR 0.000268
Train loss: 0.4149;  Loss pred: 0.4149; Loss self: 0.0000; time: 0.76s
Val loss: 0.3912 score: 0.9796 time: 0.48s
Test loss: 0.4880 score: 0.8776 time: 4.51s
Epoch 65/1000, LR 0.000268
Train loss: 0.4009;  Loss pred: 0.4009; Loss self: 0.0000; time: 3.95s
Val loss: 0.3770 score: 0.9796 time: 0.58s
Test loss: 0.4770 score: 0.8776 time: 0.46s
Epoch 66/1000, LR 0.000268
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.76s
Val loss: 0.3630 score: 0.9796 time: 0.45s
Test loss: 0.4661 score: 0.8776 time: 0.38s
Epoch 67/1000, LR 0.000268
Train loss: 0.3746;  Loss pred: 0.3746; Loss self: 0.0000; time: 0.57s
Val loss: 0.3496 score: 0.9796 time: 0.47s
Test loss: 0.4554 score: 0.8776 time: 0.63s
Epoch 68/1000, LR 0.000268
Train loss: 0.3586;  Loss pred: 0.3586; Loss self: 0.0000; time: 7.27s
Val loss: 0.3365 score: 0.9796 time: 0.72s
Test loss: 0.4450 score: 0.8776 time: 0.45s
Epoch 69/1000, LR 0.000268
Train loss: 0.3478;  Loss pred: 0.3478; Loss self: 0.0000; time: 0.86s
Val loss: 0.3237 score: 0.9796 time: 0.50s
Test loss: 0.4349 score: 0.8776 time: 0.41s
Epoch 70/1000, LR 0.000268
Train loss: 0.3332;  Loss pred: 0.3332; Loss self: 0.0000; time: 0.74s
Val loss: 0.3113 score: 0.9796 time: 3.63s
Test loss: 0.4251 score: 0.8776 time: 0.95s
Epoch 71/1000, LR 0.000268
Train loss: 0.3152;  Loss pred: 0.3152; Loss self: 0.0000; time: 0.56s
Val loss: 0.2994 score: 0.9796 time: 0.44s
Test loss: 0.4157 score: 0.8776 time: 0.47s
Epoch 72/1000, LR 0.000267
Train loss: 0.3019;  Loss pred: 0.3019; Loss self: 0.0000; time: 0.70s
Val loss: 0.2879 score: 0.9796 time: 0.46s
Test loss: 0.4067 score: 0.8776 time: 0.40s
Epoch 73/1000, LR 0.000267
Train loss: 0.2888;  Loss pred: 0.2888; Loss self: 0.0000; time: 0.60s
Val loss: 0.2771 score: 0.9796 time: 0.47s
Test loss: 0.3979 score: 0.8776 time: 0.38s
Epoch 74/1000, LR 0.000267
Train loss: 0.2806;  Loss pred: 0.2806; Loss self: 0.0000; time: 0.72s
Val loss: 0.2666 score: 0.9796 time: 0.57s
Test loss: 0.3899 score: 0.8776 time: 5.18s
Epoch 75/1000, LR 0.000267
Train loss: 0.2632;  Loss pred: 0.2632; Loss self: 0.0000; time: 7.65s
Val loss: 0.2566 score: 0.9796 time: 0.62s
Test loss: 0.3821 score: 0.8776 time: 0.35s
Epoch 76/1000, LR 0.000267
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 0.55s
Val loss: 0.2470 score: 0.9796 time: 0.54s
Test loss: 0.3747 score: 0.8776 time: 0.36s
Epoch 77/1000, LR 0.000267
Train loss: 0.2419;  Loss pred: 0.2419; Loss self: 0.0000; time: 0.65s
Val loss: 0.2378 score: 0.9796 time: 0.46s
Test loss: 0.3675 score: 0.8776 time: 0.38s
Epoch 78/1000, LR 0.000267
Train loss: 0.2227;  Loss pred: 0.2227; Loss self: 0.0000; time: 0.62s
Val loss: 0.2288 score: 0.9796 time: 5.93s
Test loss: 0.3608 score: 0.8776 time: 3.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.2210;  Loss pred: 0.2210; Loss self: 0.0000; time: 9.69s
Val loss: 0.2203 score: 0.9796 time: 0.47s
Test loss: 0.3540 score: 0.8776 time: 0.56s
Epoch 80/1000, LR 0.000267
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 0.85s
Val loss: 0.2121 score: 0.9592 time: 0.54s
Test loss: 0.3475 score: 0.8776 time: 0.46s
Epoch 81/1000, LR 0.000267
Train loss: 0.1909;  Loss pred: 0.1909; Loss self: 0.0000; time: 0.59s
Val loss: 0.2042 score: 0.9592 time: 0.47s
Test loss: 0.3414 score: 0.8776 time: 0.34s
Epoch 82/1000, LR 0.000267
Train loss: 0.1801;  Loss pred: 0.1801; Loss self: 0.0000; time: 0.67s
Val loss: 0.1968 score: 0.9592 time: 0.55s
Test loss: 0.3360 score: 0.8980 time: 0.36s
Epoch 83/1000, LR 0.000266
Train loss: 0.1899;  Loss pred: 0.1899; Loss self: 0.0000; time: 0.60s
Val loss: 0.1901 score: 0.9592 time: 0.46s
Test loss: 0.3309 score: 0.8980 time: 0.38s
Epoch 84/1000, LR 0.000266
Train loss: 0.1709;  Loss pred: 0.1709; Loss self: 0.0000; time: 0.61s
Val loss: 0.1841 score: 0.9592 time: 0.61s
Test loss: 0.3264 score: 0.8980 time: 1.96s
Epoch 85/1000, LR 0.000266
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 5.61s
Val loss: 0.1786 score: 0.9592 time: 0.46s
Test loss: 0.3228 score: 0.8980 time: 0.39s
Epoch 86/1000, LR 0.000266
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 0.92s
Val loss: 0.1736 score: 0.9592 time: 0.57s
Test loss: 0.3201 score: 0.8980 time: 0.40s
Epoch 87/1000, LR 0.000266
Train loss: 0.1437;  Loss pred: 0.1437; Loss self: 0.0000; time: 12.60s
Val loss: 0.1694 score: 0.9592 time: 1.52s
Test loss: 0.3181 score: 0.8980 time: 0.37s
Epoch 88/1000, LR 0.000266
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 0.86s
Val loss: 0.1655 score: 0.9592 time: 0.48s
Test loss: 0.3166 score: 0.8980 time: 0.38s
Epoch 89/1000, LR 0.000266
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.82s
Val loss: 0.1622 score: 0.9592 time: 0.50s
Test loss: 0.3158 score: 0.8980 time: 0.38s
Epoch 90/1000, LR 0.000266
Train loss: 0.1176;  Loss pred: 0.1176; Loss self: 0.0000; time: 0.73s
Val loss: 0.1593 score: 0.9592 time: 0.46s
Test loss: 0.3155 score: 0.8980 time: 0.39s
Epoch 91/1000, LR 0.000266
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 0.72s
Val loss: 0.1569 score: 0.9592 time: 3.48s
Test loss: 0.3156 score: 0.8980 time: 4.00s
Epoch 92/1000, LR 0.000266
Train loss: 0.1018;  Loss pred: 0.1018; Loss self: 0.0000; time: 1.12s
Val loss: 0.1548 score: 0.9592 time: 0.60s
Test loss: 0.3161 score: 0.8980 time: 0.37s
Epoch 93/1000, LR 0.000265
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.60s
Val loss: 0.1532 score: 0.9592 time: 0.47s
Test loss: 0.3169 score: 0.8980 time: 0.36s
Epoch 94/1000, LR 0.000265
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.62s
Val loss: 0.1519 score: 0.9592 time: 0.57s
Test loss: 0.3183 score: 0.8980 time: 0.38s
Epoch 95/1000, LR 0.000265
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 5.29s
Val loss: 0.1510 score: 0.9592 time: 4.24s
Test loss: 0.3201 score: 0.8980 time: 2.15s
Epoch 96/1000, LR 0.000265
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.76s
Val loss: 0.1504 score: 0.9592 time: 0.48s
Test loss: 0.3225 score: 0.8980 time: 0.37s
Epoch 97/1000, LR 0.000265
Train loss: 0.0812;  Loss pred: 0.0812; Loss self: 0.0000; time: 0.64s
Val loss: 0.1503 score: 0.9592 time: 0.46s
Test loss: 0.3252 score: 0.8980 time: 0.39s
Epoch 98/1000, LR 0.000265
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.67s
Val loss: 0.1504 score: 0.9592 time: 0.45s
Test loss: 0.3284 score: 0.8980 time: 0.42s
     INFO: Early stopping counter 1 of 2
Epoch 99/1000, LR 0.000265
Train loss: 0.0688;  Loss pred: 0.0688; Loss self: 0.0000; time: 0.71s
Val loss: 0.1509 score: 0.9592 time: 2.87s
Test loss: 0.3316 score: 0.8980 time: 0.37s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 096,   Train_Loss: 0.0812,   Val_Loss: 0.1503,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1503,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3252


[3.914231769973412, 0.35806162003427744, 0.40710097597911954, 0.4195962600642815, 0.3856795768951997, 0.35722180793527514, 4.795574533985928, 0.43692611204460263, 0.3735485679935664, 0.3713097228901461, 4.652438799967058, 0.39923579909373075, 0.3926714960252866, 0.3596554020186886, 3.4863707741023973, 0.3711123850662261, 0.35975213604979217, 4.58982814499177, 0.36586111003998667, 0.3716306200949475, 4.651735802995972, 0.3774341850075871, 0.453943429980427, 0.40428375802002847, 4.296016723965295, 0.3737571609672159, 0.377263339003548, 4.847278980072588, 0.38232274702750146, 0.3778656990034506, 3.9662171620875597, 0.4202980799600482, 0.433449330041185, 6.630433333106339, 0.39351546694524586, 0.3551650890149176, 5.5646426329622045, 0.37033731397241354, 0.5278961609583348, 6.2951631639152765, 0.44118362106382847, 0.35391705099027604, 0.3713319479720667, 0.4815561060095206, 0.3550662389025092, 0.3683647490106523, 0.5195899199461564, 0.42389267997350544, 0.36321534391026944, 4.867142342962325, 0.38438223395496607, 0.4911801740527153, 4.831325732986443, 0.3765180899063125, 0.37632263998966664, 0.3695581449428573, 0.4295752589823678, 0.391511051915586, 0.41557042801287025, 0.3935364990029484, 4.85124518093653, 0.3716130609391257, 0.38151128799654543, 4.514153510099277, 0.4618854190921411, 0.3797661609714851, 0.6387168930377811, 0.45383310597389936, 0.4194754809141159, 0.9557641790015623, 0.4763481559930369, 0.4030449789715931, 0.38241661700885743, 5.188739080913365, 0.35461943002883345, 0.36828622792381793, 0.37972088798414916, 3.0686233600135893, 0.5622355840168893, 0.4644723639357835, 0.34793845005333424, 0.3634272990748286, 0.38500103901606053, 1.9610592960380018, 0.3907672269269824, 0.4000972519861534, 0.3754606519360095, 0.38042783294804394, 0.3858682740246877, 0.39838381798472255, 4.004323008004576, 0.37346208293456584, 0.3681641329312697, 0.38906912109814584, 2.1558686330681667, 0.3704585520317778, 0.39232053398154676, 0.43002449395135045, 0.3745568949962035]
[0.07988228101986555, 0.00730738000069954, 0.008308183183247338, 0.008563188980903705, 0.007871011773371423, 0.007290240978270921, 0.09786886804052913, 0.008916859429481686, 0.0076234401631340085, 0.0075777494467376754, 0.0949477306115726, 0.008147669369259812, 0.008013704000516054, 0.007339906163646707, 0.07115042396127341, 0.007573722144208697, 0.007341880327546779, 0.09366996214268919, 0.007466553266122177, 0.007584298369284643, 0.09493338373461169, 0.007702738469542593, 0.009264151632253612, 0.008250688939184254, 0.08767381069316928, 0.007627697162596243, 0.00769925181639894, 0.09892406081780791, 0.0078025050413775806, 0.007711544877621441, 0.08094320738954204, 0.00857751183591935, 0.008845904694718063, 0.13531496598176201, 0.008030927896841752, 0.00724826712275342, 0.1135641353665756, 0.007557904366783949, 0.010773391039966017, 0.12847271763092402, 0.00900374736864956, 0.0072227969589852255, 0.007578203019838096, 0.009827675632847359, 0.007246249773520596, 0.007517647938992904, 0.0106038759172685, 0.008650871019867458, 0.007412558038985091, 0.09932943557065968, 0.007844535386836042, 0.010024085184749293, 0.0985984843466621, 0.0076840426511492355, 0.0076800538773401355, 0.007542002958017497, 0.008766842020048323, 0.00799002146766502, 0.008481029143119802, 0.008031357122509152, 0.09900500369258225, 0.007583940019165831, 0.0077859446529907235, 0.09212558183876075, 0.009426233042696757, 0.007750329815744594, 0.013035038633424104, 0.009261900121916314, 0.00856072410028808, 0.019505391408195148, 0.009721390938633407, 0.008225407734114145, 0.0078044207552828045, 0.10589263430435439, 0.007237131225078234, 0.007516045467833019, 0.007749405877227534, 0.06262496653088957, 0.011474195592181415, 0.009479027835424153, 0.007100784694966005, 0.007416883654588339, 0.007857164061552256, 0.040021618286489834, 0.007974841365856784, 0.008165250040533744, 0.007662462284408358, 0.007763833325470285, 0.007874862735197708, 0.008130281999688216, 0.0817208777143791, 0.007621675161929915, 0.007513553733291219, 0.007940186144860119, 0.04399731904220748, 0.007560378612893425, 0.008006541509827485, 0.008776010080639805, 0.007644018265228643]
[12.518420696466023, 136.8479536994476, 120.36325848187906, 116.77892456070336, 127.048469598676, 137.1696769668617, 10.217753817137062, 112.1471082849769, 131.17437516409893, 131.96530276288215, 10.532110599788426, 122.73448451073398, 124.78624116084194, 136.24152376127478, 14.05473002584344, 132.0354748905936, 136.20488967219936, 10.675780977435236, 133.9306055094092, 131.85135279617444, 10.533702272695994, 129.82395857708283, 107.94296549707254, 121.20199990218877, 11.405914629394688, 131.10116706044337, 129.8827501485353, 10.10876415437228, 128.16396717424533, 129.67570258223557, 12.354341176369065, 116.5839254004152, 113.04666221388362, 7.390165550015966, 124.51861264913866, 137.9640103026621, 8.805596914660452, 132.31180912990587, 92.82128498727123, 7.783753768428848, 111.06486655566685, 138.45052071635365, 131.95740433216375, 101.75346006106139, 138.00241935548812, 133.0203287138722, 94.30513972456919, 115.5952964393314, 134.9061949654451, 10.067509135181112, 127.4772756686273, 99.75972685481628, 10.142143731987838, 130.13982943606368, 130.20741989199874, 132.59077271203586, 114.06615948059344, 125.15610928542812, 117.91021857426887, 124.51195791024428, 10.100499598031154, 131.85758292824573, 128.43656673257777, 10.854748269055293, 106.08691674292717, 129.02676708912773, 76.7163050392368, 107.96920576089057, 116.81254859812019, 51.26787661281445, 102.8659382502496, 121.57452035509307, 128.13250737706588, 9.443527461275739, 138.17629788648608, 133.04868953757327, 129.04215056519467, 15.968072406182493, 87.15207893801339, 105.49605058262323, 140.8295058867135, 134.82751605269763, 127.27238379727058, 24.986495869347987, 125.39434380241921, 122.47022381872243, 130.5063519901179, 128.80235292009263, 126.98634041332198, 122.99696370167094, 12.23677508084383, 131.20475207274328, 133.09281273509472, 125.94163181518421, 22.72865760390265, 132.26850812664406, 124.89787241751861, 113.94699764600728, 130.82124679749032]
Elapsed: 1.262572898863429~1.7249663530903356
Time per graph: 0.025766793854355697~0.03520339496102726
Speed: 99.2527760490124~47.223043348136414
Total Time: 0.3750
best val loss: 0.1502758413553238 test_score: 0.8980

Testing...
Test loss: 0.5521 score: 0.8571 time: 0.38s
test Score 0.8571
Epoch Time List: [9.759478076943196, 1.5073709689313546, 1.4608465661294758, 15.518461757921614, 1.5132925459183753, 1.5778491019736975, 13.222404149011709, 1.7941595610463992, 1.4317098270403221, 1.5524205897236243, 15.566443693009205, 9.42438658804167, 1.4763864419655874, 1.6608070739312097, 9.575630121049471, 1.5302704249043018, 1.5791342559969053, 8.509201943990774, 1.903606062871404, 1.5482106151757762, 8.229788397089578, 7.191978291957639, 1.6481086179846898, 1.5253445780836046, 5.373382931109518, 8.514264754834585, 1.646221678936854, 6.072275631013326, 6.330612709163688, 1.6919345310889184, 4.981394028058276, 7.8807920999825, 1.551743071875535, 11.581146836979315, 2.4647544069448486, 1.452458448940888, 7.439215880003758, 1.4919944379944354, 1.6408460759557784, 7.568347327876836, 2.171074070967734, 1.4757833479670808, 1.5904667071299627, 17.374158116057515, 1.52734468691051, 1.4812681999756023, 11.123533857869916, 1.6278758371481672, 1.5250620750011876, 17.62523155787494, 4.995695278048515, 1.6846679261652753, 9.7744298230391, 2.1486451431410387, 1.5485604680143297, 1.4865497690625489, 1.5222456710180268, 10.602809347910807, 1.4786442919867113, 1.566340591874905, 5.990858100936748, 6.719650533865206, 1.479297557962127, 5.748866343987174, 4.994952565175481, 1.5925733570475131, 1.6802950791316107, 8.443185098934919, 1.776391587103717, 5.319918653927743, 1.4710496669868007, 1.5576508169760928, 1.4528159220935777, 6.4828625719528645, 8.624162543099374, 1.4550498290918767, 1.4812764320522547, 9.612837339984253, 10.721768668852746, 1.8470979030244052, 1.4065199920441955, 1.582656827988103, 1.4331560620339587, 3.182353514013812, 6.459223063895479, 1.8829335230402648, 14.494893767987378, 1.7143822179641575, 1.694782793871127, 1.5902311431709677, 8.200136737897992, 2.0825223450083286, 1.4289655380416662, 1.574063181062229, 11.674054412986152, 1.6115749781019986, 1.4903089040890336, 1.5495475911302492, 3.949573906022124]
Total Epoch List: [99]
Total Time List: [0.3750356190139428]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba35e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 1.99s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 7.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.46s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.48s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.51s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 5.63s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 6.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.41s
Test loss: 0.6925 score: 0.5306 time: 0.57s
Epoch 7/1000, LR 0.000150
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.58s
Val loss: 0.6928 score: 0.5918 time: 0.37s
Test loss: 0.6924 score: 0.7347 time: 0.48s
Epoch 8/1000, LR 0.000180
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.60s
Val loss: 0.6926 score: 0.6735 time: 0.36s
Test loss: 0.6922 score: 0.7755 time: 0.46s
Epoch 9/1000, LR 0.000210
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.73s
Val loss: 0.6924 score: 0.5510 time: 4.64s
Test loss: 0.6920 score: 0.5918 time: 5.44s
Epoch 10/1000, LR 0.000240
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 2.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.41s
Test loss: 0.6917 score: 0.5306 time: 0.61s
Epoch 11/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4898 time: 0.47s
Epoch 12/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4898 time: 4.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 10.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4898 time: 0.63s
Epoch 14/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4898 time: 0.55s
Epoch 15/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 9.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4898 time: 0.47s
Epoch 16/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4898 time: 0.47s
Epoch 17/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5102 time: 0.37s
Test loss: 0.6885 score: 0.5306 time: 0.57s
Epoch 18/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 5.80s
Val loss: 0.6897 score: 0.5306 time: 0.39s
Test loss: 0.6878 score: 0.5306 time: 0.47s
Epoch 19/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.65s
Val loss: 0.6892 score: 0.5306 time: 0.35s
Test loss: 0.6870 score: 0.5510 time: 0.47s
Epoch 20/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.63s
Val loss: 0.6887 score: 0.5306 time: 2.89s
Test loss: 0.6862 score: 0.5714 time: 3.85s
Epoch 21/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 5.26s
Val loss: 0.6881 score: 0.5306 time: 0.41s
Test loss: 0.6852 score: 0.5714 time: 0.47s
Epoch 22/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.62s
Val loss: 0.6873 score: 0.5306 time: 0.37s
Test loss: 0.6841 score: 0.5714 time: 0.51s
Epoch 23/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.62s
Val loss: 0.6865 score: 0.5510 time: 0.46s
Test loss: 0.6829 score: 0.5918 time: 5.68s
Epoch 24/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 12.02s
Val loss: 0.6856 score: 0.5510 time: 0.39s
Test loss: 0.6814 score: 0.5918 time: 0.50s
Epoch 25/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.58s
Val loss: 0.6846 score: 0.5510 time: 0.46s
Test loss: 0.6798 score: 0.5918 time: 0.47s
Epoch 26/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.71s
Val loss: 0.6835 score: 0.5510 time: 0.51s
Test loss: 0.6782 score: 0.5918 time: 0.52s
Epoch 27/1000, LR 0.000270
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 12.94s
Val loss: 0.6823 score: 0.5510 time: 0.37s
Test loss: 0.6764 score: 0.5918 time: 0.51s
Epoch 28/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.67s
Val loss: 0.6810 score: 0.5510 time: 0.40s
Test loss: 0.6744 score: 0.5918 time: 0.56s
Epoch 29/1000, LR 0.000270
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 7.44s
Val loss: 0.6796 score: 0.5510 time: 0.49s
Test loss: 0.6723 score: 0.5918 time: 0.50s
Epoch 30/1000, LR 0.000270
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.63s
Val loss: 0.6780 score: 0.5510 time: 0.39s
Test loss: 0.6700 score: 0.5918 time: 0.66s
Epoch 31/1000, LR 0.000270
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.84s
Val loss: 0.6764 score: 0.5510 time: 0.41s
Test loss: 0.6676 score: 0.5918 time: 0.49s
Epoch 32/1000, LR 0.000270
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 11.88s
Val loss: 0.6746 score: 0.5510 time: 2.22s
Test loss: 0.6649 score: 0.5918 time: 0.73s
Epoch 33/1000, LR 0.000270
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.80s
Val loss: 0.6726 score: 0.5714 time: 0.36s
Test loss: 0.6621 score: 0.5918 time: 0.46s
Epoch 34/1000, LR 0.000270
Train loss: 0.6633;  Loss pred: 0.6633; Loss self: 0.0000; time: 0.58s
Val loss: 0.6704 score: 0.5918 time: 0.37s
Test loss: 0.6590 score: 0.6122 time: 0.69s
Epoch 35/1000, LR 0.000270
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.63s
Val loss: 0.6681 score: 0.6122 time: 0.38s
Test loss: 0.6556 score: 0.6122 time: 4.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 9.26s
Val loss: 0.6655 score: 0.6327 time: 0.36s
Test loss: 0.6520 score: 0.6122 time: 0.46s
Epoch 37/1000, LR 0.000270
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.66s
Val loss: 0.6627 score: 0.6327 time: 0.37s
Test loss: 0.6481 score: 0.6122 time: 0.47s
Epoch 38/1000, LR 0.000270
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.58s
Val loss: 0.6597 score: 0.6327 time: 0.37s
Test loss: 0.6438 score: 0.6122 time: 1.14s
Epoch 39/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 9.80s
Val loss: 0.6564 score: 0.6327 time: 3.70s
Test loss: 0.6393 score: 0.6122 time: 0.49s
Epoch 40/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.60s
Val loss: 0.6528 score: 0.6327 time: 0.37s
Test loss: 0.6343 score: 0.6122 time: 0.50s
Epoch 41/1000, LR 0.000269
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.75s
Val loss: 0.6490 score: 0.6531 time: 0.38s
Test loss: 0.6290 score: 0.6531 time: 0.48s
Epoch 42/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 7.34s
Val loss: 0.6448 score: 0.6735 time: 0.38s
Test loss: 0.6233 score: 0.6531 time: 0.48s
Epoch 43/1000, LR 0.000269
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.85s
Val loss: 0.6403 score: 0.6735 time: 0.36s
Test loss: 0.6173 score: 0.6735 time: 0.47s
Epoch 44/1000, LR 0.000269
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.57s
Val loss: 0.6355 score: 0.6531 time: 0.38s
Test loss: 0.6108 score: 0.7143 time: 0.47s
Epoch 45/1000, LR 0.000269
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.78s
Val loss: 0.6304 score: 0.6531 time: 0.37s
Test loss: 0.6039 score: 0.7347 time: 5.20s
Epoch 46/1000, LR 0.000269
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.71s
Val loss: 0.6249 score: 0.6531 time: 0.35s
Test loss: 0.5965 score: 0.7551 time: 0.50s
Epoch 47/1000, LR 0.000269
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.61s
Val loss: 0.6190 score: 0.6735 time: 0.47s
Test loss: 0.5887 score: 0.7755 time: 0.48s
Epoch 48/1000, LR 0.000269
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.60s
Val loss: 0.6127 score: 0.7143 time: 0.38s
Test loss: 0.5803 score: 0.7755 time: 5.02s
Epoch 49/1000, LR 0.000269
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 6.66s
Val loss: 0.6060 score: 0.7347 time: 0.45s
Test loss: 0.5715 score: 0.7755 time: 0.50s
Epoch 50/1000, LR 0.000269
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.77s
Val loss: 0.5988 score: 0.7347 time: 0.37s
Test loss: 0.5622 score: 0.7755 time: 0.48s
Epoch 51/1000, LR 0.000269
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.78s
Val loss: 0.5913 score: 0.7755 time: 3.83s
Test loss: 0.5525 score: 0.8163 time: 4.33s
Epoch 52/1000, LR 0.000269
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.68s
Val loss: 0.5835 score: 0.7755 time: 0.47s
Test loss: 0.5423 score: 0.8571 time: 0.52s
Epoch 53/1000, LR 0.000269
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.68s
Val loss: 0.5753 score: 0.7755 time: 0.35s
Test loss: 0.5316 score: 0.8776 time: 0.47s
Epoch 54/1000, LR 0.000269
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.62s
Val loss: 0.5667 score: 0.7755 time: 2.84s
Test loss: 0.5205 score: 0.9184 time: 5.00s
Epoch 55/1000, LR 0.000269
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.82s
Val loss: 0.5580 score: 0.7755 time: 0.38s
Test loss: 0.5090 score: 0.9184 time: 0.46s
Epoch 56/1000, LR 0.000269
Train loss: 0.4920;  Loss pred: 0.4920; Loss self: 0.0000; time: 0.56s
Val loss: 0.5490 score: 0.7755 time: 0.35s
Test loss: 0.4972 score: 0.9184 time: 0.51s
Epoch 57/1000, LR 0.000269
Train loss: 0.4826;  Loss pred: 0.4826; Loss self: 0.0000; time: 0.79s
Val loss: 0.5398 score: 0.8163 time: 0.35s
Test loss: 0.4853 score: 0.9184 time: 0.48s
Epoch 58/1000, LR 0.000269
Train loss: 0.4715;  Loss pred: 0.4715; Loss self: 0.0000; time: 0.70s
Val loss: 0.5304 score: 0.8163 time: 2.99s
Test loss: 0.4734 score: 0.9184 time: 2.35s
Epoch 59/1000, LR 0.000268
Train loss: 0.4622;  Loss pred: 0.4622; Loss self: 0.0000; time: 8.04s
Val loss: 0.5211 score: 0.8163 time: 0.40s
Test loss: 0.4618 score: 0.9184 time: 0.51s
Epoch 60/1000, LR 0.000268
Train loss: 0.4443;  Loss pred: 0.4443; Loss self: 0.0000; time: 0.59s
Val loss: 0.5117 score: 0.8163 time: 0.47s
Test loss: 0.4503 score: 0.9184 time: 0.58s
Epoch 61/1000, LR 0.000268
Train loss: 0.4341;  Loss pred: 0.4341; Loss self: 0.0000; time: 0.69s
Val loss: 0.5025 score: 0.8571 time: 0.37s
Test loss: 0.4392 score: 0.9184 time: 4.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.4201;  Loss pred: 0.4201; Loss self: 0.0000; time: 6.99s
Val loss: 0.4933 score: 0.8571 time: 0.39s
Test loss: 0.4285 score: 0.9184 time: 0.47s
Epoch 63/1000, LR 0.000268
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 0.71s
Val loss: 0.4841 score: 0.8571 time: 0.36s
Test loss: 0.4181 score: 0.9184 time: 0.47s
Epoch 64/1000, LR 0.000268
Train loss: 0.3976;  Loss pred: 0.3976; Loss self: 0.0000; time: 0.60s
Val loss: 0.4752 score: 0.8571 time: 0.38s
Test loss: 0.4083 score: 0.9184 time: 4.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.3844;  Loss pred: 0.3844; Loss self: 0.0000; time: 4.21s
Val loss: 0.4665 score: 0.8571 time: 0.37s
Test loss: 0.3989 score: 0.9592 time: 0.49s
Epoch 66/1000, LR 0.000268
Train loss: 0.3771;  Loss pred: 0.3771; Loss self: 0.0000; time: 0.65s
Val loss: 0.4583 score: 0.8776 time: 0.37s
Test loss: 0.3897 score: 0.9592 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.3621;  Loss pred: 0.3621; Loss self: 0.0000; time: 0.68s
Val loss: 0.4503 score: 0.8776 time: 0.37s
Test loss: 0.3808 score: 0.9592 time: 0.54s
Epoch 68/1000, LR 0.000268
Train loss: 0.3463;  Loss pred: 0.3463; Loss self: 0.0000; time: 9.79s
Val loss: 0.4425 score: 0.8571 time: 2.50s
Test loss: 0.3722 score: 0.9388 time: 0.49s
Epoch 69/1000, LR 0.000268
Train loss: 0.3424;  Loss pred: 0.3424; Loss self: 0.0000; time: 0.73s
Val loss: 0.4349 score: 0.8571 time: 0.37s
Test loss: 0.3640 score: 0.9388 time: 0.49s
Epoch 70/1000, LR 0.000268
Train loss: 0.3224;  Loss pred: 0.3224; Loss self: 0.0000; time: 0.64s
Val loss: 0.4276 score: 0.8571 time: 0.38s
Test loss: 0.3562 score: 0.9388 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.3122;  Loss pred: 0.3122; Loss self: 0.0000; time: 0.61s
Val loss: 0.4204 score: 0.8571 time: 4.82s
Test loss: 0.3485 score: 0.9388 time: 3.30s
Epoch 72/1000, LR 0.000267
Train loss: 0.3135;  Loss pred: 0.3135; Loss self: 0.0000; time: 0.82s
Val loss: 0.4135 score: 0.8571 time: 0.79s
Test loss: 0.3410 score: 0.9388 time: 0.57s
Epoch 73/1000, LR 0.000267
Train loss: 0.2983;  Loss pred: 0.2983; Loss self: 0.0000; time: 0.65s
Val loss: 0.4070 score: 0.8571 time: 0.48s
Test loss: 0.3341 score: 0.9592 time: 0.48s
Epoch 74/1000, LR 0.000267
Train loss: 0.2884;  Loss pred: 0.2884; Loss self: 0.0000; time: 12.07s
Val loss: 0.4007 score: 0.8571 time: 0.36s
Test loss: 0.3276 score: 0.9592 time: 0.51s
Epoch 75/1000, LR 0.000267
Train loss: 0.2843;  Loss pred: 0.2843; Loss self: 0.0000; time: 0.84s
Val loss: 0.3949 score: 0.8571 time: 0.40s
Test loss: 0.3214 score: 0.9388 time: 0.78s
Epoch 76/1000, LR 0.000267
Train loss: 0.2622;  Loss pred: 0.2622; Loss self: 0.0000; time: 0.72s
Val loss: 0.3892 score: 0.8571 time: 0.41s
Test loss: 0.3152 score: 0.9388 time: 0.52s
Epoch 77/1000, LR 0.000267
Train loss: 0.2566;  Loss pred: 0.2566; Loss self: 0.0000; time: 4.11s
Val loss: 0.3837 score: 0.8571 time: 0.39s
Test loss: 0.3095 score: 0.9388 time: 0.47s
Epoch 78/1000, LR 0.000267
Train loss: 0.2444;  Loss pred: 0.2444; Loss self: 0.0000; time: 0.59s
Val loss: 0.3781 score: 0.8571 time: 0.38s
Test loss: 0.3035 score: 0.9388 time: 0.56s
Epoch 79/1000, LR 0.000267
Train loss: 0.2394;  Loss pred: 0.2394; Loss self: 0.0000; time: 0.60s
Val loss: 0.3724 score: 0.8571 time: 0.38s
Test loss: 0.2975 score: 0.9388 time: 2.80s
Epoch 80/1000, LR 0.000267
Train loss: 0.2362;  Loss pred: 0.2362; Loss self: 0.0000; time: 13.62s
Val loss: 0.3667 score: 0.8571 time: 0.36s
Test loss: 0.2915 score: 0.9388 time: 0.51s
Epoch 81/1000, LR 0.000267
Train loss: 0.2235;  Loss pred: 0.2235; Loss self: 0.0000; time: 0.73s
Val loss: 0.3611 score: 0.8571 time: 0.37s
Test loss: 0.2859 score: 0.9388 time: 0.55s
Epoch 82/1000, LR 0.000267
Train loss: 0.2080;  Loss pred: 0.2080; Loss self: 0.0000; time: 0.64s
Val loss: 0.3558 score: 0.8571 time: 5.25s
Test loss: 0.2805 score: 0.9388 time: 2.59s
Epoch 83/1000, LR 0.000266
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.70s
Val loss: 0.3506 score: 0.8571 time: 0.38s
Test loss: 0.2753 score: 0.9388 time: 0.47s
Epoch 84/1000, LR 0.000266
Train loss: 0.1958;  Loss pred: 0.1958; Loss self: 0.0000; time: 0.65s
Val loss: 0.3457 score: 0.8571 time: 0.36s
Test loss: 0.2703 score: 0.9388 time: 0.46s
Epoch 85/1000, LR 0.000266
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.68s
Val loss: 0.3410 score: 0.8571 time: 0.37s
Test loss: 0.2656 score: 0.9388 time: 0.49s
Epoch 86/1000, LR 0.000266
Train loss: 0.1795;  Loss pred: 0.1795; Loss self: 0.0000; time: 10.97s
Val loss: 0.3368 score: 0.8571 time: 0.61s
Test loss: 0.2613 score: 0.9388 time: 0.47s
Epoch 87/1000, LR 0.000266
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.70s
Val loss: 0.3328 score: 0.8571 time: 0.38s
Test loss: 0.2575 score: 0.9388 time: 0.48s
Epoch 88/1000, LR 0.000266
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.61s
Val loss: 0.3292 score: 0.8776 time: 0.42s
Test loss: 0.2541 score: 0.9388 time: 5.43s
Epoch 89/1000, LR 0.000266
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 1.95s
Val loss: 0.3259 score: 0.8776 time: 0.37s
Test loss: 0.2511 score: 0.9388 time: 0.48s
Epoch 90/1000, LR 0.000266
Train loss: 0.1397;  Loss pred: 0.1397; Loss self: 0.0000; time: 0.63s
Val loss: 0.3231 score: 0.8776 time: 0.36s
Test loss: 0.2487 score: 0.9388 time: 0.48s
Epoch 91/1000, LR 0.000266
Train loss: 0.1313;  Loss pred: 0.1313; Loss self: 0.0000; time: 0.69s
Val loss: 0.3209 score: 0.8776 time: 0.43s
Test loss: 0.2469 score: 0.9388 time: 0.47s
Epoch 92/1000, LR 0.000266
Train loss: 0.1238;  Loss pred: 0.1238; Loss self: 0.0000; time: 0.61s
Val loss: 0.3195 score: 0.8571 time: 0.69s
Test loss: 0.2458 score: 0.9388 time: 4.90s
Epoch 93/1000, LR 0.000265
Train loss: 0.1141;  Loss pred: 0.1141; Loss self: 0.0000; time: 13.20s
Val loss: 0.3187 score: 0.8571 time: 0.43s
Test loss: 0.2454 score: 0.9388 time: 0.44s
Epoch 94/1000, LR 0.000265
Train loss: 0.1084;  Loss pred: 0.1084; Loss self: 0.0000; time: 0.75s
Val loss: 0.3184 score: 0.8571 time: 0.37s
Test loss: 0.2454 score: 0.9592 time: 0.59s
Epoch 95/1000, LR 0.000265
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.66s
Val loss: 0.3188 score: 0.8571 time: 0.45s
Test loss: 0.2461 score: 0.9592 time: 0.57s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.1004;  Loss pred: 0.1004; Loss self: 0.0000; time: 2.59s
Val loss: 0.3198 score: 0.8571 time: 4.88s
Test loss: 0.2473 score: 0.9592 time: 2.10s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.1084,   Val_Loss: 0.3184,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3184,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2454


[3.914231769973412, 0.35806162003427744, 0.40710097597911954, 0.4195962600642815, 0.3856795768951997, 0.35722180793527514, 4.795574533985928, 0.43692611204460263, 0.3735485679935664, 0.3713097228901461, 4.652438799967058, 0.39923579909373075, 0.3926714960252866, 0.3596554020186886, 3.4863707741023973, 0.3711123850662261, 0.35975213604979217, 4.58982814499177, 0.36586111003998667, 0.3716306200949475, 4.651735802995972, 0.3774341850075871, 0.453943429980427, 0.40428375802002847, 4.296016723965295, 0.3737571609672159, 0.377263339003548, 4.847278980072588, 0.38232274702750146, 0.3778656990034506, 3.9662171620875597, 0.4202980799600482, 0.433449330041185, 6.630433333106339, 0.39351546694524586, 0.3551650890149176, 5.5646426329622045, 0.37033731397241354, 0.5278961609583348, 6.2951631639152765, 0.44118362106382847, 0.35391705099027604, 0.3713319479720667, 0.4815561060095206, 0.3550662389025092, 0.3683647490106523, 0.5195899199461564, 0.42389267997350544, 0.36321534391026944, 4.867142342962325, 0.38438223395496607, 0.4911801740527153, 4.831325732986443, 0.3765180899063125, 0.37632263998966664, 0.3695581449428573, 0.4295752589823678, 0.391511051915586, 0.41557042801287025, 0.3935364990029484, 4.85124518093653, 0.3716130609391257, 0.38151128799654543, 4.514153510099277, 0.4618854190921411, 0.3797661609714851, 0.6387168930377811, 0.45383310597389936, 0.4194754809141159, 0.9557641790015623, 0.4763481559930369, 0.4030449789715931, 0.38241661700885743, 5.188739080913365, 0.35461943002883345, 0.36828622792381793, 0.37972088798414916, 3.0686233600135893, 0.5622355840168893, 0.4644723639357835, 0.34793845005333424, 0.3634272990748286, 0.38500103901606053, 1.9610592960380018, 0.3907672269269824, 0.4000972519861534, 0.3754606519360095, 0.38042783294804394, 0.3858682740246877, 0.39838381798472255, 4.004323008004576, 0.37346208293456584, 0.3681641329312697, 0.38906912109814584, 2.1558686330681667, 0.3704585520317778, 0.39232053398154676, 0.43002449395135045, 0.3745568949962035, 1.995982224936597, 0.46833548496942967, 0.4838660330278799, 0.5185114539926872, 5.635687520029023, 0.5701116259442642, 0.4837657749885693, 0.4631859390065074, 5.44654701591935, 0.6120161059079692, 0.4756288279313594, 4.285789323039353, 0.6348216709448025, 0.5504947259323671, 0.47252372396178544, 0.47543852496892214, 0.5809611449949443, 0.4771252010250464, 0.4727590960683301, 3.8550926400348544, 0.4793026300612837, 0.5144352529896423, 5.68560601095669, 0.5074402340687811, 0.47771679295692593, 0.5223618400050327, 0.5193625059910119, 0.5608508220175281, 0.5017018769867718, 0.6643551129382104, 0.49850919691380113, 0.7323175260098651, 0.46356927300803363, 0.6907452089944854, 4.059719659970142, 0.46703317610081285, 0.4752469100058079, 1.1426213359227404, 0.49476772104389966, 0.5080398930003867, 0.48760858294554055, 0.480938975000754, 0.4756807229714468, 0.4727928179781884, 5.207791077089496, 0.5002657329896465, 0.48896437394432724, 5.02350910694804, 0.502014169935137, 0.48170674801804125, 4.330215687979944, 0.5235855950741097, 0.4766875860514119, 5.005719608045183, 0.4645940790651366, 0.5169758589472622, 0.48706208704970777, 2.3500333230476826, 0.5116658700862899, 0.583354715956375, 4.258069160976447, 0.4758151510031894, 0.4791343400720507, 4.251929934020154, 0.4919753809226677, 0.47979003202635795, 0.5478066739160568, 0.49408891797065735, 0.4925750009715557, 0.47042171796783805, 3.302910421974957, 0.5762023099232465, 0.48568384896498173, 0.511218519997783, 0.7838541999226436, 0.5299326429376379, 0.4780430309474468, 0.5691546950256452, 2.805535760940984, 0.5103353479644284, 0.5528807879891247, 2.597219813033007, 0.4772143190493807, 0.4649324310012162, 0.4985488790553063, 0.47805286501534283, 0.48480444808956236, 5.436277852044441, 0.4822841129498556, 0.4882308500818908, 0.47358545497991145, 4.907314229058102, 0.44365376397036016, 0.5898112630238757, 0.5716493159998208, 2.1037636429537088]
[0.07988228101986555, 0.00730738000069954, 0.008308183183247338, 0.008563188980903705, 0.007871011773371423, 0.007290240978270921, 0.09786886804052913, 0.008916859429481686, 0.0076234401631340085, 0.0075777494467376754, 0.0949477306115726, 0.008147669369259812, 0.008013704000516054, 0.007339906163646707, 0.07115042396127341, 0.007573722144208697, 0.007341880327546779, 0.09366996214268919, 0.007466553266122177, 0.007584298369284643, 0.09493338373461169, 0.007702738469542593, 0.009264151632253612, 0.008250688939184254, 0.08767381069316928, 0.007627697162596243, 0.00769925181639894, 0.09892406081780791, 0.0078025050413775806, 0.007711544877621441, 0.08094320738954204, 0.00857751183591935, 0.008845904694718063, 0.13531496598176201, 0.008030927896841752, 0.00724826712275342, 0.1135641353665756, 0.007557904366783949, 0.010773391039966017, 0.12847271763092402, 0.00900374736864956, 0.0072227969589852255, 0.007578203019838096, 0.009827675632847359, 0.007246249773520596, 0.007517647938992904, 0.0106038759172685, 0.008650871019867458, 0.007412558038985091, 0.09932943557065968, 0.007844535386836042, 0.010024085184749293, 0.0985984843466621, 0.0076840426511492355, 0.0076800538773401355, 0.007542002958017497, 0.008766842020048323, 0.00799002146766502, 0.008481029143119802, 0.008031357122509152, 0.09900500369258225, 0.007583940019165831, 0.0077859446529907235, 0.09212558183876075, 0.009426233042696757, 0.007750329815744594, 0.013035038633424104, 0.009261900121916314, 0.00856072410028808, 0.019505391408195148, 0.009721390938633407, 0.008225407734114145, 0.0078044207552828045, 0.10589263430435439, 0.007237131225078234, 0.007516045467833019, 0.007749405877227534, 0.06262496653088957, 0.011474195592181415, 0.009479027835424153, 0.007100784694966005, 0.007416883654588339, 0.007857164061552256, 0.040021618286489834, 0.007974841365856784, 0.008165250040533744, 0.007662462284408358, 0.007763833325470285, 0.007874862735197708, 0.008130281999688216, 0.0817208777143791, 0.007621675161929915, 0.007513553733291219, 0.007940186144860119, 0.04399731904220748, 0.007560378612893425, 0.008006541509827485, 0.008776010080639805, 0.007644018265228643, 0.04073433112115504, 0.009557867040192443, 0.009874817000568978, 0.010581866408014024, 0.11501403102100048, 0.011634931141719679, 0.009872770918134068, 0.009452774265438927, 0.11115402073304796, 0.012490124610366717, 0.009706710774109376, 0.08746508822529292, 0.012955544304995969, 0.011234586243517697, 0.00964334130534256, 0.009702827040182084, 0.011856349897856007, 0.009737249000511152, 0.009648144817721022, 0.07867536000071132, 0.0097816863277813, 0.01049867863244168, 0.11603277573380999, 0.010355923144260839, 0.009749322305243386, 0.010660445714388423, 0.010599234816143098, 0.01144593514321486, 0.010238813816056567, 0.013558267610983886, 0.010173657079873492, 0.01494525563285439, 0.009460597408327217, 0.014096840999887459, 0.08285142163204372, 0.009531289308179855, 0.009698916530730774, 0.023318802773933476, 0.01009730042946734, 0.010368161081640544, 0.009951195570317154, 0.009815081122464367, 0.009707769856560139, 0.009648833019963029, 0.10628145055284685, 0.010209504754890745, 0.009978864774374025, 0.10252059401934777, 0.01024518714153341, 0.009830749959551863, 0.08837174873428458, 0.010685420307634893, 0.009728318082681877, 0.10215754302133027, 0.009481511817655849, 0.010550527733617596, 0.00994004259285118, 0.04795986373566699, 0.010442160614005918, 0.01190519828482398, 0.08689937063217239, 0.009710513285779375, 0.009778251838205116, 0.0867740802861256, 0.010040313896380973, 0.009791633306660366, 0.0111797280391032, 0.01008344730552362, 0.01005255104023583, 0.00960044322383343, 0.06740633514234606, 0.011759230814760133, 0.009911915284999627, 0.010433031020362919, 0.015997024488217215, 0.010814951896686487, 0.009755980223417282, 0.011615401939298881, 0.057255831855938445, 0.010415007101314865, 0.011283281387533156, 0.05300448598026545, 0.009739067735701648, 0.009488416959208493, 0.010174466919496047, 0.009756180918680466, 0.009893968328358415, 0.11094444596009063, 0.009842532917343992, 0.009963894899630425, 0.009665009285304315, 0.10014926998077758, 0.009054158448374696, 0.012036964551507667, 0.011666312571424914, 0.04293395189701447]
[12.518420696466023, 136.8479536994476, 120.36325848187906, 116.77892456070336, 127.048469598676, 137.1696769668617, 10.217753817137062, 112.1471082849769, 131.17437516409893, 131.96530276288215, 10.532110599788426, 122.73448451073398, 124.78624116084194, 136.24152376127478, 14.05473002584344, 132.0354748905936, 136.20488967219936, 10.675780977435236, 133.9306055094092, 131.85135279617444, 10.533702272695994, 129.82395857708283, 107.94296549707254, 121.20199990218877, 11.405914629394688, 131.10116706044337, 129.8827501485353, 10.10876415437228, 128.16396717424533, 129.67570258223557, 12.354341176369065, 116.5839254004152, 113.04666221388362, 7.390165550015966, 124.51861264913866, 137.9640103026621, 8.805596914660452, 132.31180912990587, 92.82128498727123, 7.783753768428848, 111.06486655566685, 138.45052071635365, 131.95740433216375, 101.75346006106139, 138.00241935548812, 133.0203287138722, 94.30513972456919, 115.5952964393314, 134.9061949654451, 10.067509135181112, 127.4772756686273, 99.75972685481628, 10.142143731987838, 130.13982943606368, 130.20741989199874, 132.59077271203586, 114.06615948059344, 125.15610928542812, 117.91021857426887, 124.51195791024428, 10.100499598031154, 131.85758292824573, 128.43656673257777, 10.854748269055293, 106.08691674292717, 129.02676708912773, 76.7163050392368, 107.96920576089057, 116.81254859812019, 51.26787661281445, 102.8659382502496, 121.57452035509307, 128.13250737706588, 9.443527461275739, 138.17629788648608, 133.04868953757327, 129.04215056519467, 15.968072406182493, 87.15207893801339, 105.49605058262323, 140.8295058867135, 134.82751605269763, 127.27238379727058, 24.986495869347987, 125.39434380241921, 122.47022381872243, 130.5063519901179, 128.80235292009263, 126.98634041332198, 122.99696370167094, 12.23677508084383, 131.20475207274328, 133.09281273509472, 125.94163181518421, 22.72865760390265, 132.26850812664406, 124.89787241751861, 113.94699764600728, 130.82124679749032, 24.549316816464383, 104.6258538432091, 101.26769943608889, 94.5012875273746, 8.694591356574655, 85.94808063919466, 101.28868666072502, 105.78904900502947, 8.996525662365745, 80.06325246507204, 103.021509888529, 11.433133153929898, 77.18703100836728, 89.01084368612109, 103.69849705993343, 103.0627461314856, 84.34298992650604, 102.69841101398409, 103.64686879111427, 12.710459793141828, 102.23186130594537, 95.25008193982882, 8.618254572260629, 96.56309592778227, 102.57123199857486, 93.80470824501252, 94.34643324223332, 87.36726073385093, 97.66756364216666, 73.75573551814801, 98.29307122787696, 66.91086620169175, 105.70157008476019, 70.9378789196802, 12.069798928027552, 104.91760009233896, 103.10430003511476, 42.88384827019648, 99.03637184862417, 96.4491188095788, 100.49043785078872, 101.88402800983883, 103.01027061578291, 103.63947618650278, 9.40897959896365, 97.94794400002229, 100.21179990012742, 9.7541377863191, 97.60680660932552, 101.72163915412871, 11.315833559057335, 93.58546235991157, 102.79269155273373, 9.788802377433864, 105.46841255187444, 94.78198865954899, 100.6031906462045, 20.850768165471575, 95.76562140393767, 83.99692101514503, 11.507563204718702, 102.9811679949459, 102.26776897817747, 11.524178610739954, 99.59847972088299, 102.12800752248249, 89.44761415504135, 99.17243277030954, 99.47723677278043, 104.16185760231004, 14.835400825282061, 85.03957578116457, 100.88867501857766, 95.84942267000127, 62.51162525484417, 92.46458140108626, 102.5012327925491, 86.09258682789596, 17.46546976238338, 96.01529699137247, 88.6267004831498, 18.86632766087607, 102.67923246227997, 105.39165851364717, 98.28524756258494, 102.49912423059607, 101.07167991773001, 9.013520157284159, 101.59986340892523, 100.36235930560562, 103.4660154461004, 9.985095250239343, 110.44648773288355, 83.07742335876091, 85.71688730930863, 23.291589891345126]
Elapsed: 1.2683842659533884~1.6472573686915386
Time per graph: 0.02588539318272221~0.03361749732023548
Speed: 88.97424058263134~42.92649420892374
Total Time: 2.1044
best val loss: 0.318368136882782 test_score: 0.9592

Testing...
Test loss: 0.3897 score: 0.9592 time: 0.59s
test Score 0.9592
Epoch Time List: [9.759478076943196, 1.5073709689313546, 1.4608465661294758, 15.518461757921614, 1.5132925459183753, 1.5778491019736975, 13.222404149011709, 1.7941595610463992, 1.4317098270403221, 1.5524205897236243, 15.566443693009205, 9.42438658804167, 1.4763864419655874, 1.6608070739312097, 9.575630121049471, 1.5302704249043018, 1.5791342559969053, 8.509201943990774, 1.903606062871404, 1.5482106151757762, 8.229788397089578, 7.191978291957639, 1.6481086179846898, 1.5253445780836046, 5.373382931109518, 8.514264754834585, 1.646221678936854, 6.072275631013326, 6.330612709163688, 1.6919345310889184, 4.981394028058276, 7.8807920999825, 1.551743071875535, 11.581146836979315, 2.4647544069448486, 1.452458448940888, 7.439215880003758, 1.4919944379944354, 1.6408460759557784, 7.568347327876836, 2.171074070967734, 1.4757833479670808, 1.5904667071299627, 17.374158116057515, 1.52734468691051, 1.4812681999756023, 11.123533857869916, 1.6278758371481672, 1.5250620750011876, 17.62523155787494, 4.995695278048515, 1.6846679261652753, 9.7744298230391, 2.1486451431410387, 1.5485604680143297, 1.4865497690625489, 1.5222456710180268, 10.602809347910807, 1.4786442919867113, 1.566340591874905, 5.990858100936748, 6.719650533865206, 1.479297557962127, 5.748866343987174, 4.994952565175481, 1.5925733570475131, 1.6802950791316107, 8.443185098934919, 1.776391587103717, 5.319918653927743, 1.4710496669868007, 1.5576508169760928, 1.4528159220935777, 6.4828625719528645, 8.624162543099374, 1.4550498290918767, 1.4812764320522547, 9.612837339984253, 10.721768668852746, 1.8470979030244052, 1.4065199920441955, 1.582656827988103, 1.4331560620339587, 3.182353514013812, 6.459223063895479, 1.8829335230402648, 14.494893767987378, 1.7143822179641575, 1.694782793871127, 1.5902311431709677, 8.200136737897992, 2.0825223450083286, 1.4289655380416662, 1.574063181062229, 11.674054412986152, 1.6115749781019986, 1.4903089040890336, 1.5495475911302492, 3.949573906022124, 3.0972224870929495, 8.027029191958718, 1.533279633964412, 1.4793239100836217, 6.737287227180786, 7.4721206579124555, 1.4271110189147294, 1.4173507549567148, 10.814090140978806, 3.652353659970686, 1.6651762200053781, 5.199168502003886, 11.571412367047742, 1.9617919919546694, 10.394847569870763, 1.4036526140989736, 1.624565229867585, 6.661810475052334, 1.4710370389511809, 7.363827517954633, 6.138973772991449, 1.5000695240451023, 6.760716508957557, 12.92098091589287, 1.51465323404409, 1.7383892920333892, 13.829951928928494, 1.6228115749545395, 8.422608449007384, 1.6771080970065668, 1.7398631909163669, 14.826143136946484, 1.616435697185807, 1.6366655288729817, 5.066755145904608, 10.082021709880792, 1.4963103159097955, 2.087786488002166, 13.992784449947067, 1.471878970041871, 1.6138055339688435, 8.189900416997261, 1.6876441378844902, 1.4217249201610684, 6.361383924027905, 1.5642622450832278, 1.5627762309741229, 5.99915478611365, 7.6108938830439, 1.6169654299737886, 8.932879029074684, 1.6723862019134685, 1.5010300979483873, 8.46320956305135, 1.661564843961969, 1.4297696839785203, 1.6286328569985926, 6.043205944006331, 8.94114268803969, 1.640474476851523, 5.309907230082899, 7.8467482960550115, 1.5476361878681928, 5.222198897972703, 5.066049889079295, 1.4992295710835606, 1.5869967358885333, 12.779029434896074, 1.585316312033683, 1.4779764040140435, 8.728760443045758, 2.1826854201499373, 1.61476159398444, 12.935872383997776, 2.0194303429452702, 1.6464442070573568, 4.971892399014905, 1.5385893719503656, 3.7751245428808033, 14.484450881020166, 1.6432764439377934, 8.48335403506644, 1.5460427469806746, 1.465317631023936, 1.5357999169500545, 12.051919821999036, 1.568724764045328, 6.45937350904569, 2.7974265549564734, 1.4824548219330609, 1.591858017956838, 6.194921575020999, 14.065470701898448, 1.6989787109196186, 1.6800785929663107, 9.569016782101244]
Total Epoch List: [99, 96]
Total Time List: [0.3750356190139428, 2.1044147319626063]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb61060>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 12.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 3.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.43s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.54s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.42s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 4.87s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 7.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.45s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.55s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.44s
Epoch 8/1000, LR 0.000180
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 5.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 4.90s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.42s
Epoch 10/1000, LR 0.000240
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.66s
Epoch 11/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 4.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 4.80s
Epoch 12/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.56s
Epoch 13/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.46s
Epoch 14/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 2.55s
Epoch 15/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 11.57s
Val loss: 0.6918 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.42s
Epoch 16/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.59s
Val loss: 0.6915 score: 0.5102 time: 0.50s
Test loss: 0.6910 score: 0.5417 time: 0.43s
Epoch 17/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.71s
Val loss: 0.6912 score: 0.5918 time: 0.42s
Test loss: 0.6906 score: 0.5625 time: 0.43s
Epoch 18/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.63s
Val loss: 0.6907 score: 0.6122 time: 4.23s
Test loss: 0.6901 score: 0.5833 time: 5.11s
Epoch 19/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.80s
Val loss: 0.6903 score: 0.6122 time: 0.41s
Test loss: 0.6896 score: 0.6667 time: 0.44s
Epoch 20/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.62s
Val loss: 0.6898 score: 0.6327 time: 0.41s
Test loss: 0.6890 score: 0.6458 time: 0.52s
Epoch 21/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.58s
Val loss: 0.6893 score: 0.6531 time: 0.52s
Test loss: 0.6884 score: 0.7083 time: 5.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 4.49s
Val loss: 0.6887 score: 0.6327 time: 0.49s
Test loss: 0.6877 score: 0.7500 time: 0.44s
Epoch 23/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.72s
Val loss: 0.6879 score: 0.6735 time: 0.42s
Test loss: 0.6869 score: 0.7292 time: 0.46s
Epoch 24/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.64s
Val loss: 0.6871 score: 0.6735 time: 5.31s
Test loss: 0.6859 score: 0.7083 time: 4.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 1.14s
Val loss: 0.6862 score: 0.6531 time: 0.42s
Test loss: 0.6849 score: 0.6875 time: 0.43s
Epoch 26/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.72s
Val loss: 0.6853 score: 0.6735 time: 0.51s
Test loss: 0.6837 score: 0.6875 time: 0.55s
Epoch 27/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 8.34s
Val loss: 0.6842 score: 0.6122 time: 0.43s
Test loss: 0.6825 score: 0.6875 time: 0.43s
Epoch 28/1000, LR 0.000270
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.72s
Val loss: 0.6831 score: 0.6531 time: 0.41s
Test loss: 0.6812 score: 0.7083 time: 0.51s
Epoch 29/1000, LR 0.000270
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.69s
Val loss: 0.6820 score: 0.6531 time: 0.42s
Test loss: 0.6798 score: 0.7083 time: 5.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 3.87s
Val loss: 0.6808 score: 0.6531 time: 0.57s
Test loss: 0.6784 score: 0.7292 time: 0.52s
Epoch 31/1000, LR 0.000270
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.58s
Val loss: 0.6794 score: 0.6327 time: 0.40s
Test loss: 0.6768 score: 0.7292 time: 0.56s
Epoch 32/1000, LR 0.000270
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.59s
Val loss: 0.6780 score: 0.6735 time: 0.41s
Test loss: 0.6751 score: 0.7500 time: 0.55s
Epoch 33/1000, LR 0.000270
Train loss: 0.6710;  Loss pred: 0.6710; Loss self: 0.0000; time: 2.89s
Val loss: 0.6765 score: 0.6735 time: 1.54s
Test loss: 0.6732 score: 0.7292 time: 0.44s
Epoch 34/1000, LR 0.000270
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.60s
Val loss: 0.6748 score: 0.6735 time: 0.42s
Test loss: 0.6712 score: 0.7292 time: 0.56s
Epoch 35/1000, LR 0.000270
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.59s
Val loss: 0.6730 score: 0.6735 time: 0.42s
Test loss: 0.6689 score: 0.7500 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 3.08s
Val loss: 0.6709 score: 0.6531 time: 3.09s
Test loss: 0.6665 score: 0.7500 time: 0.62s
Epoch 37/1000, LR 0.000270
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.61s
Val loss: 0.6687 score: 0.6531 time: 0.41s
Test loss: 0.6638 score: 0.7500 time: 0.44s
Epoch 38/1000, LR 0.000270
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 0.59s
Val loss: 0.6663 score: 0.6531 time: 0.51s
Test loss: 0.6609 score: 0.7500 time: 0.43s
Epoch 39/1000, LR 0.000269
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.62s
Val loss: 0.6637 score: 0.6531 time: 0.43s
Test loss: 0.6577 score: 0.7292 time: 0.41s
Epoch 40/1000, LR 0.000269
Train loss: 0.6502;  Loss pred: 0.6502; Loss self: 0.0000; time: 0.58s
Val loss: 0.6609 score: 0.6531 time: 0.48s
Test loss: 0.6543 score: 0.7292 time: 0.50s
Epoch 41/1000, LR 0.000269
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 2.04s
Val loss: 0.6577 score: 0.6531 time: 5.78s
Test loss: 0.6505 score: 0.7292 time: 2.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6437;  Loss pred: 0.6437; Loss self: 0.0000; time: 0.64s
Val loss: 0.6544 score: 0.6531 time: 0.53s
Test loss: 0.6465 score: 0.7292 time: 0.44s
Epoch 43/1000, LR 0.000269
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.62s
Val loss: 0.6507 score: 0.6735 time: 0.44s
Test loss: 0.6420 score: 0.7292 time: 0.43s
Epoch 44/1000, LR 0.000269
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.64s
Val loss: 0.6467 score: 0.7143 time: 0.42s
Test loss: 0.6373 score: 0.7292 time: 4.33s
Epoch 45/1000, LR 0.000269
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 10.47s
Val loss: 0.6425 score: 0.7143 time: 0.43s
Test loss: 0.6322 score: 0.7292 time: 0.42s
Epoch 46/1000, LR 0.000269
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.77s
Val loss: 0.6379 score: 0.7143 time: 0.49s
Test loss: 0.6267 score: 0.7292 time: 0.43s
Epoch 47/1000, LR 0.000269
Train loss: 0.6164;  Loss pred: 0.6164; Loss self: 0.0000; time: 0.62s
Val loss: 0.6331 score: 0.7143 time: 0.42s
Test loss: 0.6208 score: 0.7292 time: 0.48s
Epoch 48/1000, LR 0.000269
Train loss: 0.6082;  Loss pred: 0.6082; Loss self: 0.0000; time: 13.71s
Val loss: 0.6279 score: 0.7143 time: 0.51s
Test loss: 0.6145 score: 0.7500 time: 0.46s
Epoch 49/1000, LR 0.000269
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.65s
Val loss: 0.6223 score: 0.7143 time: 0.41s
Test loss: 0.6078 score: 0.7917 time: 0.45s
Epoch 50/1000, LR 0.000269
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.61s
Val loss: 0.6164 score: 0.7143 time: 0.44s
Test loss: 0.6006 score: 0.7917 time: 0.54s
Epoch 51/1000, LR 0.000269
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.61s
Val loss: 0.6100 score: 0.7143 time: 0.45s
Test loss: 0.5929 score: 0.7917 time: 0.49s
Epoch 52/1000, LR 0.000269
Train loss: 0.5743;  Loss pred: 0.5743; Loss self: 0.0000; time: 12.68s
Val loss: 0.6033 score: 0.7143 time: 0.41s
Test loss: 0.5848 score: 0.7917 time: 0.50s
Epoch 53/1000, LR 0.000269
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.76s
Val loss: 0.5960 score: 0.7143 time: 0.40s
Test loss: 0.5761 score: 0.7917 time: 0.46s
Epoch 54/1000, LR 0.000269
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 0.61s
Val loss: 0.5883 score: 0.7551 time: 0.50s
Test loss: 0.5669 score: 0.7917 time: 0.55s
Epoch 55/1000, LR 0.000269
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 7.57s
Val loss: 0.5802 score: 0.7959 time: 0.41s
Test loss: 0.5571 score: 0.8125 time: 0.45s
Epoch 56/1000, LR 0.000269
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.63s
Val loss: 0.5716 score: 0.7959 time: 0.47s
Test loss: 0.5469 score: 0.8333 time: 0.59s
Epoch 57/1000, LR 0.000269
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.67s
Val loss: 0.5625 score: 0.7959 time: 5.43s
Test loss: 0.5362 score: 0.8542 time: 5.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.60s
Val loss: 0.5529 score: 0.7959 time: 0.41s
Test loss: 0.5250 score: 0.8750 time: 0.54s
Epoch 59/1000, LR 0.000268
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.58s
Val loss: 0.5429 score: 0.8367 time: 0.41s
Test loss: 0.5134 score: 0.8958 time: 0.43s
Epoch 60/1000, LR 0.000268
Train loss: 0.4878;  Loss pred: 0.4878; Loss self: 0.0000; time: 0.60s
Val loss: 0.5326 score: 0.8571 time: 0.44s
Test loss: 0.5015 score: 0.9167 time: 0.55s
Epoch 61/1000, LR 0.000268
Train loss: 0.4736;  Loss pred: 0.4736; Loss self: 0.0000; time: 5.91s
Val loss: 0.5220 score: 0.8571 time: 0.48s
Test loss: 0.4893 score: 0.9167 time: 0.46s
Epoch 62/1000, LR 0.000268
Train loss: 0.4607;  Loss pred: 0.4607; Loss self: 0.0000; time: 0.78s
Val loss: 0.5111 score: 0.8571 time: 0.61s
Test loss: 0.4769 score: 0.9167 time: 0.42s
Epoch 63/1000, LR 0.000268
Train loss: 0.4436;  Loss pred: 0.4436; Loss self: 0.0000; time: 0.58s
Val loss: 0.4999 score: 0.8571 time: 0.45s
Test loss: 0.4644 score: 0.9167 time: 0.46s
Epoch 64/1000, LR 0.000268
Train loss: 0.4267;  Loss pred: 0.4267; Loss self: 0.0000; time: 0.74s
Val loss: 0.4886 score: 0.8571 time: 3.10s
Test loss: 0.4517 score: 0.9167 time: 0.43s
Epoch 65/1000, LR 0.000268
Train loss: 0.4233;  Loss pred: 0.4233; Loss self: 0.0000; time: 0.58s
Val loss: 0.4773 score: 0.8571 time: 0.40s
Test loss: 0.4393 score: 0.9167 time: 0.59s
Epoch 66/1000, LR 0.000268
Train loss: 0.4013;  Loss pred: 0.4013; Loss self: 0.0000; time: 0.59s
Val loss: 0.4660 score: 0.8571 time: 0.50s
Test loss: 0.4271 score: 0.9375 time: 0.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.3834;  Loss pred: 0.3834; Loss self: 0.0000; time: 3.14s
Val loss: 0.4549 score: 0.8571 time: 5.70s
Test loss: 0.4151 score: 0.9375 time: 5.55s
Epoch 68/1000, LR 0.000268
Train loss: 0.3676;  Loss pred: 0.3676; Loss self: 0.0000; time: 1.38s
Val loss: 0.4442 score: 0.8776 time: 0.53s
Test loss: 0.4034 score: 0.9375 time: 0.53s
Epoch 69/1000, LR 0.000268
Train loss: 0.3523;  Loss pred: 0.3523; Loss self: 0.0000; time: 0.62s
Val loss: 0.4336 score: 0.8776 time: 0.41s
Test loss: 0.3918 score: 0.9375 time: 0.43s
Epoch 70/1000, LR 0.000268
Train loss: 0.3371;  Loss pred: 0.3371; Loss self: 0.0000; time: 0.62s
Val loss: 0.4234 score: 0.8776 time: 5.35s
Test loss: 0.3805 score: 0.9375 time: 3.55s
Epoch 71/1000, LR 0.000268
Train loss: 0.3290;  Loss pred: 0.3290; Loss self: 0.0000; time: 3.28s
Val loss: 0.4135 score: 0.8980 time: 0.40s
Test loss: 0.3696 score: 0.9375 time: 0.42s
Epoch 72/1000, LR 0.000267
Train loss: 0.3151;  Loss pred: 0.3151; Loss self: 0.0000; time: 0.71s
Val loss: 0.4040 score: 0.9184 time: 0.49s
Test loss: 0.3589 score: 0.9375 time: 0.42s
Epoch 73/1000, LR 0.000267
Train loss: 0.2984;  Loss pred: 0.2984; Loss self: 0.0000; time: 0.59s
Val loss: 0.3947 score: 0.9184 time: 0.41s
Test loss: 0.3485 score: 0.9375 time: 0.49s
Epoch 74/1000, LR 0.000267
Train loss: 0.2930;  Loss pred: 0.2930; Loss self: 0.0000; time: 14.69s
Val loss: 0.3854 score: 0.9184 time: 1.40s
Test loss: 0.3381 score: 0.9375 time: 0.51s
Epoch 75/1000, LR 0.000267
Train loss: 0.2719;  Loss pred: 0.2719; Loss self: 0.0000; time: 0.66s
Val loss: 0.3764 score: 0.9184 time: 0.41s
Test loss: 0.3278 score: 0.9375 time: 0.46s
Epoch 76/1000, LR 0.000267
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.58s
Val loss: 0.3674 score: 0.9184 time: 0.41s
Test loss: 0.3175 score: 0.9167 time: 0.53s
Epoch 77/1000, LR 0.000267
Train loss: 0.2435;  Loss pred: 0.2435; Loss self: 0.0000; time: 11.47s
Val loss: 0.3587 score: 0.9184 time: 1.38s
Test loss: 0.3073 score: 0.9167 time: 0.44s
Epoch 78/1000, LR 0.000267
Train loss: 0.2383;  Loss pred: 0.2383; Loss self: 0.0000; time: 0.64s
Val loss: 0.3509 score: 0.9184 time: 0.42s
Test loss: 0.2984 score: 0.9167 time: 0.54s
Epoch 79/1000, LR 0.000267
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 0.60s
Val loss: 0.3434 score: 0.9184 time: 0.41s
Test loss: 0.2901 score: 0.9167 time: 3.84s
Epoch 80/1000, LR 0.000267
Train loss: 0.2085;  Loss pred: 0.2085; Loss self: 0.0000; time: 10.69s
Val loss: 0.3364 score: 0.9184 time: 0.41s
Test loss: 0.2824 score: 0.8958 time: 0.51s
Epoch 81/1000, LR 0.000267
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 0.65s
Val loss: 0.3297 score: 0.9184 time: 0.42s
Test loss: 0.2748 score: 0.8958 time: 0.44s
Epoch 82/1000, LR 0.000267
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.72s
Val loss: 0.3234 score: 0.9184 time: 0.44s
Test loss: 0.2676 score: 0.8958 time: 0.56s
Epoch 83/1000, LR 0.000266
Train loss: 0.1708;  Loss pred: 0.1708; Loss self: 0.0000; time: 5.80s
Val loss: 0.3175 score: 0.9184 time: 0.41s
Test loss: 0.2607 score: 0.8958 time: 0.45s
Epoch 84/1000, LR 0.000266
Train loss: 0.1623;  Loss pred: 0.1623; Loss self: 0.0000; time: 0.59s
Val loss: 0.3123 score: 0.9184 time: 0.43s
Test loss: 0.2547 score: 0.8958 time: 0.58s
Epoch 85/1000, LR 0.000266
Train loss: 0.1505;  Loss pred: 0.1505; Loss self: 0.0000; time: 0.73s
Val loss: 0.3076 score: 0.9184 time: 0.60s
Test loss: 0.2493 score: 0.8958 time: 0.52s
Epoch 86/1000, LR 0.000266
Train loss: 0.1412;  Loss pred: 0.1412; Loss self: 0.0000; time: 10.18s
Val loss: 0.3038 score: 0.9184 time: 4.23s
Test loss: 0.2452 score: 0.8958 time: 3.65s
Epoch 87/1000, LR 0.000266
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 1.36s
Val loss: 0.3003 score: 0.9184 time: 0.41s
Test loss: 0.2413 score: 0.8958 time: 0.43s
Epoch 88/1000, LR 0.000266
Train loss: 0.1247;  Loss pred: 0.1247; Loss self: 0.0000; time: 0.63s
Val loss: 0.2979 score: 0.9184 time: 0.50s
Test loss: 0.2385 score: 0.8958 time: 0.47s
Epoch 89/1000, LR 0.000266
Train loss: 0.1167;  Loss pred: 0.1167; Loss self: 0.0000; time: 0.57s
Val loss: 0.2967 score: 0.9184 time: 0.41s
Test loss: 0.2371 score: 0.8958 time: 0.44s
Epoch 90/1000, LR 0.000266
Train loss: 0.1066;  Loss pred: 0.1066; Loss self: 0.0000; time: 0.65s
Val loss: 0.2960 score: 0.9184 time: 0.60s
Test loss: 0.2357 score: 0.8958 time: 3.75s
Epoch 91/1000, LR 0.000266
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 11.78s
Val loss: 0.2960 score: 0.8980 time: 0.41s
Test loss: 0.2349 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.0916;  Loss pred: 0.0916; Loss self: 0.0000; time: 0.62s
Val loss: 0.2974 score: 0.8980 time: 0.42s
Test loss: 0.2353 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.1066,   Val_Loss: 0.2960,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2960,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2357


[3.914231769973412, 0.35806162003427744, 0.40710097597911954, 0.4195962600642815, 0.3856795768951997, 0.35722180793527514, 4.795574533985928, 0.43692611204460263, 0.3735485679935664, 0.3713097228901461, 4.652438799967058, 0.39923579909373075, 0.3926714960252866, 0.3596554020186886, 3.4863707741023973, 0.3711123850662261, 0.35975213604979217, 4.58982814499177, 0.36586111003998667, 0.3716306200949475, 4.651735802995972, 0.3774341850075871, 0.453943429980427, 0.40428375802002847, 4.296016723965295, 0.3737571609672159, 0.377263339003548, 4.847278980072588, 0.38232274702750146, 0.3778656990034506, 3.9662171620875597, 0.4202980799600482, 0.433449330041185, 6.630433333106339, 0.39351546694524586, 0.3551650890149176, 5.5646426329622045, 0.37033731397241354, 0.5278961609583348, 6.2951631639152765, 0.44118362106382847, 0.35391705099027604, 0.3713319479720667, 0.4815561060095206, 0.3550662389025092, 0.3683647490106523, 0.5195899199461564, 0.42389267997350544, 0.36321534391026944, 4.867142342962325, 0.38438223395496607, 0.4911801740527153, 4.831325732986443, 0.3765180899063125, 0.37632263998966664, 0.3695581449428573, 0.4295752589823678, 0.391511051915586, 0.41557042801287025, 0.3935364990029484, 4.85124518093653, 0.3716130609391257, 0.38151128799654543, 4.514153510099277, 0.4618854190921411, 0.3797661609714851, 0.6387168930377811, 0.45383310597389936, 0.4194754809141159, 0.9557641790015623, 0.4763481559930369, 0.4030449789715931, 0.38241661700885743, 5.188739080913365, 0.35461943002883345, 0.36828622792381793, 0.37972088798414916, 3.0686233600135893, 0.5622355840168893, 0.4644723639357835, 0.34793845005333424, 0.3634272990748286, 0.38500103901606053, 1.9610592960380018, 0.3907672269269824, 0.4000972519861534, 0.3754606519360095, 0.38042783294804394, 0.3858682740246877, 0.39838381798472255, 4.004323008004576, 0.37346208293456584, 0.3681641329312697, 0.38906912109814584, 2.1558686330681667, 0.3704585520317778, 0.39232053398154676, 0.43002449395135045, 0.3745568949962035, 1.995982224936597, 0.46833548496942967, 0.4838660330278799, 0.5185114539926872, 5.635687520029023, 0.5701116259442642, 0.4837657749885693, 0.4631859390065074, 5.44654701591935, 0.6120161059079692, 0.4756288279313594, 4.285789323039353, 0.6348216709448025, 0.5504947259323671, 0.47252372396178544, 0.47543852496892214, 0.5809611449949443, 0.4771252010250464, 0.4727590960683301, 3.8550926400348544, 0.4793026300612837, 0.5144352529896423, 5.68560601095669, 0.5074402340687811, 0.47771679295692593, 0.5223618400050327, 0.5193625059910119, 0.5608508220175281, 0.5017018769867718, 0.6643551129382104, 0.49850919691380113, 0.7323175260098651, 0.46356927300803363, 0.6907452089944854, 4.059719659970142, 0.46703317610081285, 0.4752469100058079, 1.1426213359227404, 0.49476772104389966, 0.5080398930003867, 0.48760858294554055, 0.480938975000754, 0.4756807229714468, 0.4727928179781884, 5.207791077089496, 0.5002657329896465, 0.48896437394432724, 5.02350910694804, 0.502014169935137, 0.48170674801804125, 4.330215687979944, 0.5235855950741097, 0.4766875860514119, 5.005719608045183, 0.4645940790651366, 0.5169758589472622, 0.48706208704970777, 2.3500333230476826, 0.5116658700862899, 0.583354715956375, 4.258069160976447, 0.4758151510031894, 0.4791343400720507, 4.251929934020154, 0.4919753809226677, 0.47979003202635795, 0.5478066739160568, 0.49408891797065735, 0.4925750009715557, 0.47042171796783805, 3.302910421974957, 0.5762023099232465, 0.48568384896498173, 0.511218519997783, 0.7838541999226436, 0.5299326429376379, 0.4780430309474468, 0.5691546950256452, 2.805535760940984, 0.5103353479644284, 0.5528807879891247, 2.597219813033007, 0.4772143190493807, 0.4649324310012162, 0.4985488790553063, 0.47805286501534283, 0.48480444808956236, 5.436277852044441, 0.4822841129498556, 0.4882308500818908, 0.47358545497991145, 4.907314229058102, 0.44365376397036016, 0.5898112630238757, 0.5716493159998208, 2.1037636429537088, 0.43568846699781716, 0.5400249179219827, 0.42678045597858727, 4.870718166930601, 0.4528874900424853, 0.5529666380025446, 0.44918765290640295, 4.907478998997249, 0.42703943292144686, 0.6623225859366357, 4.801097589079291, 0.5619237050414085, 0.4645998269552365, 2.5527894910192117, 0.42813200410455465, 0.4372553989524022, 0.4362954570678994, 5.114034111960791, 0.4438534020446241, 0.5200859280303121, 5.073540103971027, 0.44434733397793025, 0.4611980899935588, 4.222103954991326, 0.43846499803476036, 0.5535768879344687, 0.43408455699682236, 0.5095074690179899, 5.26779502700083, 0.5257014859234914, 0.565906596952118, 0.5571634740335867, 0.4448770919116214, 0.5667243680218235, 0.4367826560046524, 0.623635220923461, 0.439568129950203, 0.432314062025398, 0.41950161498971283, 0.5085575470002368, 2.4465928020654246, 0.44340351806022227, 0.4344951470848173, 4.339265436981805, 0.4243634300073609, 0.4329161240020767, 0.49749819899443537, 0.4614148080581799, 0.45589356299024075, 0.5444324840791523, 0.4971093749627471, 0.5072134789079428, 0.46699488698504865, 0.5582281369715929, 0.4565726009896025, 0.5986039299750701, 5.247038598987274, 0.5399197669466957, 0.438596835010685, 0.5512022050097585, 0.4661565769929439, 0.42327738110907376, 0.46938990300986916, 0.43545667396392673, 0.5900928879855201, 0.44524329295381904, 5.554915867978707, 0.5379155731061473, 0.43274042604025453, 3.556335485074669, 0.4238760240841657, 0.4290994779439643, 0.4988329299958423, 0.5171615069266409, 0.46640792104881257, 0.5321242000209168, 0.449705335078761, 0.5445011130068451, 3.851493765017949, 0.5189106269972399, 0.4442544530611485, 0.5604993520537391, 0.45338633307255805, 0.5869001880055293, 0.520426296046935, 3.6555149300256744, 0.43764756596647203, 0.4735862329835072, 0.44145466294139624, 3.757663342054002, 0.4700221240054816, 0.5147475940175354]
[0.07988228101986555, 0.00730738000069954, 0.008308183183247338, 0.008563188980903705, 0.007871011773371423, 0.007290240978270921, 0.09786886804052913, 0.008916859429481686, 0.0076234401631340085, 0.0075777494467376754, 0.0949477306115726, 0.008147669369259812, 0.008013704000516054, 0.007339906163646707, 0.07115042396127341, 0.007573722144208697, 0.007341880327546779, 0.09366996214268919, 0.007466553266122177, 0.007584298369284643, 0.09493338373461169, 0.007702738469542593, 0.009264151632253612, 0.008250688939184254, 0.08767381069316928, 0.007627697162596243, 0.00769925181639894, 0.09892406081780791, 0.0078025050413775806, 0.007711544877621441, 0.08094320738954204, 0.00857751183591935, 0.008845904694718063, 0.13531496598176201, 0.008030927896841752, 0.00724826712275342, 0.1135641353665756, 0.007557904366783949, 0.010773391039966017, 0.12847271763092402, 0.00900374736864956, 0.0072227969589852255, 0.007578203019838096, 0.009827675632847359, 0.007246249773520596, 0.007517647938992904, 0.0106038759172685, 0.008650871019867458, 0.007412558038985091, 0.09932943557065968, 0.007844535386836042, 0.010024085184749293, 0.0985984843466621, 0.0076840426511492355, 0.0076800538773401355, 0.007542002958017497, 0.008766842020048323, 0.00799002146766502, 0.008481029143119802, 0.008031357122509152, 0.09900500369258225, 0.007583940019165831, 0.0077859446529907235, 0.09212558183876075, 0.009426233042696757, 0.007750329815744594, 0.013035038633424104, 0.009261900121916314, 0.00856072410028808, 0.019505391408195148, 0.009721390938633407, 0.008225407734114145, 0.0078044207552828045, 0.10589263430435439, 0.007237131225078234, 0.007516045467833019, 0.007749405877227534, 0.06262496653088957, 0.011474195592181415, 0.009479027835424153, 0.007100784694966005, 0.007416883654588339, 0.007857164061552256, 0.040021618286489834, 0.007974841365856784, 0.008165250040533744, 0.007662462284408358, 0.007763833325470285, 0.007874862735197708, 0.008130281999688216, 0.0817208777143791, 0.007621675161929915, 0.007513553733291219, 0.007940186144860119, 0.04399731904220748, 0.007560378612893425, 0.008006541509827485, 0.008776010080639805, 0.007644018265228643, 0.04073433112115504, 0.009557867040192443, 0.009874817000568978, 0.010581866408014024, 0.11501403102100048, 0.011634931141719679, 0.009872770918134068, 0.009452774265438927, 0.11115402073304796, 0.012490124610366717, 0.009706710774109376, 0.08746508822529292, 0.012955544304995969, 0.011234586243517697, 0.00964334130534256, 0.009702827040182084, 0.011856349897856007, 0.009737249000511152, 0.009648144817721022, 0.07867536000071132, 0.0097816863277813, 0.01049867863244168, 0.11603277573380999, 0.010355923144260839, 0.009749322305243386, 0.010660445714388423, 0.010599234816143098, 0.01144593514321486, 0.010238813816056567, 0.013558267610983886, 0.010173657079873492, 0.01494525563285439, 0.009460597408327217, 0.014096840999887459, 0.08285142163204372, 0.009531289308179855, 0.009698916530730774, 0.023318802773933476, 0.01009730042946734, 0.010368161081640544, 0.009951195570317154, 0.009815081122464367, 0.009707769856560139, 0.009648833019963029, 0.10628145055284685, 0.010209504754890745, 0.009978864774374025, 0.10252059401934777, 0.01024518714153341, 0.009830749959551863, 0.08837174873428458, 0.010685420307634893, 0.009728318082681877, 0.10215754302133027, 0.009481511817655849, 0.010550527733617596, 0.00994004259285118, 0.04795986373566699, 0.010442160614005918, 0.01190519828482398, 0.08689937063217239, 0.009710513285779375, 0.009778251838205116, 0.0867740802861256, 0.010040313896380973, 0.009791633306660366, 0.0111797280391032, 0.01008344730552362, 0.01005255104023583, 0.00960044322383343, 0.06740633514234606, 0.011759230814760133, 0.009911915284999627, 0.010433031020362919, 0.015997024488217215, 0.010814951896686487, 0.009755980223417282, 0.011615401939298881, 0.057255831855938445, 0.010415007101314865, 0.011283281387533156, 0.05300448598026545, 0.009739067735701648, 0.009488416959208493, 0.010174466919496047, 0.009756180918680466, 0.009893968328358415, 0.11094444596009063, 0.009842532917343992, 0.009963894899630425, 0.009665009285304315, 0.10014926998077758, 0.009054158448374696, 0.012036964551507667, 0.011666312571424914, 0.04293395189701447, 0.009076843062454524, 0.011250519123374639, 0.008891259499553902, 0.10147329514438752, 0.009435156042551776, 0.01152013829171968, 0.009358076102216728, 0.10223914581244269, 0.008896654852530142, 0.013798387207013244, 0.10002286643915188, 0.011706743855029345, 0.009679163061567428, 0.05318311439623358, 0.008919416752178222, 0.009109487478175046, 0.009089488688914571, 0.10654237733251648, 0.009246945875929669, 0.010835123500631502, 0.10569875216606306, 0.009257236124540213, 0.009608293541532476, 0.08796049906231929, 0.009134687459057508, 0.011532851831968097, 0.009043428270767132, 0.01061473893787479, 0.10974572972918395, 0.010952114290072737, 0.01178972076983579, 0.011607572375699723, 0.009268272748158779, 0.011806757667121323, 0.00909963866676359, 0.012992400435905438, 0.009157669373962563, 0.00900654295886246, 0.00873961697895235, 0.010594948895838266, 0.05097068337636301, 0.009237573292921297, 0.009051982230933694, 0.09040136327045427, 0.008840904791820018, 0.009019085916709932, 0.01036454581238407, 0.009612808501212081, 0.009497782562296683, 0.011342343418315673, 0.010356445311723897, 0.010566947477248808, 0.009729060145521847, 0.011629752853574852, 0.009511929187283386, 0.01247091520781396, 0.10931330414556821, 0.01124832847805616, 0.009137434062722605, 0.011483379271036634, 0.009711595354019664, 0.008818278773105703, 0.009778956312705608, 0.00907201404091514, 0.012293601833031667, 0.009275901936537897, 0.11572741391622306, 0.011206574439711403, 0.009015425542505303, 0.07409032260572228, 0.008830750501753451, 0.008939572457165923, 0.010392352708246714, 0.010774198060971685, 0.009716831688516928, 0.0110859208337691, 0.009368861147474187, 0.011343773187642606, 0.08023945343787393, 0.010810638062442498, 0.009255301105440594, 0.011677069834452899, 0.009445548605678292, 0.012227087250115195, 0.010842214500977812, 0.07615656104220155, 0.0091176576243015, 0.009866379853823068, 0.009196972144612422, 0.07828465295945837, 0.009792127583447533, 0.010723908208698655]
[12.518420696466023, 136.8479536994476, 120.36325848187906, 116.77892456070336, 127.048469598676, 137.1696769668617, 10.217753817137062, 112.1471082849769, 131.17437516409893, 131.96530276288215, 10.532110599788426, 122.73448451073398, 124.78624116084194, 136.24152376127478, 14.05473002584344, 132.0354748905936, 136.20488967219936, 10.675780977435236, 133.9306055094092, 131.85135279617444, 10.533702272695994, 129.82395857708283, 107.94296549707254, 121.20199990218877, 11.405914629394688, 131.10116706044337, 129.8827501485353, 10.10876415437228, 128.16396717424533, 129.67570258223557, 12.354341176369065, 116.5839254004152, 113.04666221388362, 7.390165550015966, 124.51861264913866, 137.9640103026621, 8.805596914660452, 132.31180912990587, 92.82128498727123, 7.783753768428848, 111.06486655566685, 138.45052071635365, 131.95740433216375, 101.75346006106139, 138.00241935548812, 133.0203287138722, 94.30513972456919, 115.5952964393314, 134.9061949654451, 10.067509135181112, 127.4772756686273, 99.75972685481628, 10.142143731987838, 130.13982943606368, 130.20741989199874, 132.59077271203586, 114.06615948059344, 125.15610928542812, 117.91021857426887, 124.51195791024428, 10.100499598031154, 131.85758292824573, 128.43656673257777, 10.854748269055293, 106.08691674292717, 129.02676708912773, 76.7163050392368, 107.96920576089057, 116.81254859812019, 51.26787661281445, 102.8659382502496, 121.57452035509307, 128.13250737706588, 9.443527461275739, 138.17629788648608, 133.04868953757327, 129.04215056519467, 15.968072406182493, 87.15207893801339, 105.49605058262323, 140.8295058867135, 134.82751605269763, 127.27238379727058, 24.986495869347987, 125.39434380241921, 122.47022381872243, 130.5063519901179, 128.80235292009263, 126.98634041332198, 122.99696370167094, 12.23677508084383, 131.20475207274328, 133.09281273509472, 125.94163181518421, 22.72865760390265, 132.26850812664406, 124.89787241751861, 113.94699764600728, 130.82124679749032, 24.549316816464383, 104.6258538432091, 101.26769943608889, 94.5012875273746, 8.694591356574655, 85.94808063919466, 101.28868666072502, 105.78904900502947, 8.996525662365745, 80.06325246507204, 103.021509888529, 11.433133153929898, 77.18703100836728, 89.01084368612109, 103.69849705993343, 103.0627461314856, 84.34298992650604, 102.69841101398409, 103.64686879111427, 12.710459793141828, 102.23186130594537, 95.25008193982882, 8.618254572260629, 96.56309592778227, 102.57123199857486, 93.80470824501252, 94.34643324223332, 87.36726073385093, 97.66756364216666, 73.75573551814801, 98.29307122787696, 66.91086620169175, 105.70157008476019, 70.9378789196802, 12.069798928027552, 104.91760009233896, 103.10430003511476, 42.88384827019648, 99.03637184862417, 96.4491188095788, 100.49043785078872, 101.88402800983883, 103.01027061578291, 103.63947618650278, 9.40897959896365, 97.94794400002229, 100.21179990012742, 9.7541377863191, 97.60680660932552, 101.72163915412871, 11.315833559057335, 93.58546235991157, 102.79269155273373, 9.788802377433864, 105.46841255187444, 94.78198865954899, 100.6031906462045, 20.850768165471575, 95.76562140393767, 83.99692101514503, 11.507563204718702, 102.9811679949459, 102.26776897817747, 11.524178610739954, 99.59847972088299, 102.12800752248249, 89.44761415504135, 99.17243277030954, 99.47723677278043, 104.16185760231004, 14.835400825282061, 85.03957578116457, 100.88867501857766, 95.84942267000127, 62.51162525484417, 92.46458140108626, 102.5012327925491, 86.09258682789596, 17.46546976238338, 96.01529699137247, 88.6267004831498, 18.86632766087607, 102.67923246227997, 105.39165851364717, 98.28524756258494, 102.49912423059607, 101.07167991773001, 9.013520157284159, 101.59986340892523, 100.36235930560562, 103.4660154461004, 9.985095250239343, 110.44648773288355, 83.07742335876091, 85.71688730930863, 23.291589891345126, 110.17046269494318, 88.88478736259825, 112.4700049582596, 9.85480956912938, 105.98658840299856, 86.80451351167973, 106.85957124917176, 9.780989385753438, 112.40179781905414, 72.47223787804234, 9.997713878839315, 85.42084907498759, 103.31471777458223, 18.8029605139262, 112.1149541258725, 109.7756599804159, 110.0171895499015, 9.385936610734914, 108.14381455427974, 92.29244133135327, 9.460849626955882, 108.02360300058434, 104.07675365843423, 11.368739498527724, 109.47282044208848, 86.70882229043157, 110.57753432207765, 94.20862876164273, 9.111971850455305, 91.30657090626093, 84.81965090797719, 86.1506581766817, 107.89496890870615, 84.69725797665296, 109.89447346436927, 76.96807106071236, 109.1980895098963, 111.03039252324864, 114.42149037060817, 94.38459872069826, 19.619120909486117, 108.25353892090844, 110.47304054383368, 11.061780086305717, 113.11059484830587, 110.87598113987029, 96.4827613386735, 104.02787071789787, 105.28773357790868, 88.16520212085933, 96.55822725853254, 94.63470904469361, 102.78485126441389, 85.98635006182539, 105.1311443042398, 80.18657679377255, 9.148017323383982, 88.90209793844954, 109.43991421832908, 87.08238022950255, 102.96969380896789, 113.40081502637852, 102.26040162391558, 110.22910629215967, 81.34312576425738, 107.80622809960799, 8.640994956682572, 89.2333340022638, 110.92099815868585, 13.497039354540025, 113.24065828849291, 111.862172916156, 96.2246016925949, 92.81433238380748, 102.91420414143543, 90.20450488459878, 106.73655893273609, 88.1540897775843, 12.462697054314537, 92.5014781018453, 108.04618765046601, 85.63792237068972, 105.86997555640541, 81.78562723436677, 92.23208043982291, 13.130845016043438, 109.67729226141122, 101.35429760617981, 108.73143729002149, 12.773895804556666, 102.12285241160308, 93.24958592883647]
Elapsed: 1.2318708697599212~1.6041974929838376
Time per graph: 0.025297567359935144~0.03292608317875955
Speed: 87.55381807417864~40.56343214914981
Total Time: 0.5156
best val loss: 0.29597342014312744 test_score: 0.8958

Testing...
Test loss: 0.3589 score: 0.9375 time: 0.42s
test Score 0.9375
Epoch Time List: [9.759478076943196, 1.5073709689313546, 1.4608465661294758, 15.518461757921614, 1.5132925459183753, 1.5778491019736975, 13.222404149011709, 1.7941595610463992, 1.4317098270403221, 1.5524205897236243, 15.566443693009205, 9.42438658804167, 1.4763864419655874, 1.6608070739312097, 9.575630121049471, 1.5302704249043018, 1.5791342559969053, 8.509201943990774, 1.903606062871404, 1.5482106151757762, 8.229788397089578, 7.191978291957639, 1.6481086179846898, 1.5253445780836046, 5.373382931109518, 8.514264754834585, 1.646221678936854, 6.072275631013326, 6.330612709163688, 1.6919345310889184, 4.981394028058276, 7.8807920999825, 1.551743071875535, 11.581146836979315, 2.4647544069448486, 1.452458448940888, 7.439215880003758, 1.4919944379944354, 1.6408460759557784, 7.568347327876836, 2.171074070967734, 1.4757833479670808, 1.5904667071299627, 17.374158116057515, 1.52734468691051, 1.4812681999756023, 11.123533857869916, 1.6278758371481672, 1.5250620750011876, 17.62523155787494, 4.995695278048515, 1.6846679261652753, 9.7744298230391, 2.1486451431410387, 1.5485604680143297, 1.4865497690625489, 1.5222456710180268, 10.602809347910807, 1.4786442919867113, 1.566340591874905, 5.990858100936748, 6.719650533865206, 1.479297557962127, 5.748866343987174, 4.994952565175481, 1.5925733570475131, 1.6802950791316107, 8.443185098934919, 1.776391587103717, 5.319918653927743, 1.4710496669868007, 1.5576508169760928, 1.4528159220935777, 6.4828625719528645, 8.624162543099374, 1.4550498290918767, 1.4812764320522547, 9.612837339984253, 10.721768668852746, 1.8470979030244052, 1.4065199920441955, 1.582656827988103, 1.4331560620339587, 3.182353514013812, 6.459223063895479, 1.8829335230402648, 14.494893767987378, 1.7143822179641575, 1.694782793871127, 1.5902311431709677, 8.200136737897992, 2.0825223450083286, 1.4289655380416662, 1.574063181062229, 11.674054412986152, 1.6115749781019986, 1.4903089040890336, 1.5495475911302492, 3.949573906022124, 3.0972224870929495, 8.027029191958718, 1.533279633964412, 1.4793239100836217, 6.737287227180786, 7.4721206579124555, 1.4271110189147294, 1.4173507549567148, 10.814090140978806, 3.652353659970686, 1.6651762200053781, 5.199168502003886, 11.571412367047742, 1.9617919919546694, 10.394847569870763, 1.4036526140989736, 1.624565229867585, 6.661810475052334, 1.4710370389511809, 7.363827517954633, 6.138973772991449, 1.5000695240451023, 6.760716508957557, 12.92098091589287, 1.51465323404409, 1.7383892920333892, 13.829951928928494, 1.6228115749545395, 8.422608449007384, 1.6771080970065668, 1.7398631909163669, 14.826143136946484, 1.616435697185807, 1.6366655288729817, 5.066755145904608, 10.082021709880792, 1.4963103159097955, 2.087786488002166, 13.992784449947067, 1.471878970041871, 1.6138055339688435, 8.189900416997261, 1.6876441378844902, 1.4217249201610684, 6.361383924027905, 1.5642622450832278, 1.5627762309741229, 5.99915478611365, 7.6108938830439, 1.6169654299737886, 8.932879029074684, 1.6723862019134685, 1.5010300979483873, 8.46320956305135, 1.661564843961969, 1.4297696839785203, 1.6286328569985926, 6.043205944006331, 8.94114268803969, 1.640474476851523, 5.309907230082899, 7.8467482960550115, 1.5476361878681928, 5.222198897972703, 5.066049889079295, 1.4992295710835606, 1.5869967358885333, 12.779029434896074, 1.585316312033683, 1.4779764040140435, 8.728760443045758, 2.1826854201499373, 1.61476159398444, 12.935872383997776, 2.0194303429452702, 1.6464442070573568, 4.971892399014905, 1.5385893719503656, 3.7751245428808033, 14.484450881020166, 1.6432764439377934, 8.48335403506644, 1.5460427469806746, 1.465317631023936, 1.5357999169500545, 12.051919821999036, 1.568724764045328, 6.45937350904569, 2.7974265549564734, 1.4824548219330609, 1.591858017956838, 6.194921575020999, 14.065470701898448, 1.6989787109196186, 1.6800785929663107, 9.569016782101244, 15.49672835203819, 1.587071937858127, 1.435811756993644, 6.145237439894117, 8.189436514046974, 1.5951808240497485, 1.496143694035709, 11.170325996936299, 1.4595535319531336, 1.7778138520661741, 10.145189120084979, 2.9186682569561526, 1.7934117959812284, 3.6648823919240385, 12.430059850099497, 1.5176461999071762, 1.5677335020154715, 9.966613164986484, 1.6486230249283835, 1.5499101770110428, 6.164198981015943, 5.4259003199404106, 1.6009678558912128, 10.170860284008086, 1.9901057500392199, 1.7865145610412583, 9.194059333065525, 1.6382869589142501, 6.38048583001364, 4.961721520987339, 1.5467749980743974, 1.552598663023673, 4.869747517048381, 1.5817630040692165, 1.4382900609634817, 6.786936033051461, 1.4548955869395286, 1.528047667001374, 1.4644572078250349, 1.5663181020645425, 10.26151333306916, 1.6110508679412305, 1.484491914859973, 5.388843135093339, 11.319682275992818, 1.697149537038058, 1.5246030209818855, 14.675041410140693, 1.5104934581322595, 1.5829680861206725, 1.5592579210642725, 13.595653178053908, 1.6269903238862753, 1.6689346159109846, 8.430078388890252, 1.6915201001102105, 11.334066338953562, 1.5473985369317234, 1.4267710299463943, 1.5857966768089682, 6.85168795613572, 1.8074615799123421, 1.4878269080072641, 4.268993749981746, 1.5683323269477114, 1.5283959549851716, 14.39784967689775, 2.437505012960173, 1.4518550919601694, 9.517782090930268, 4.10310920199845, 1.6322244429029524, 1.4943486760603264, 16.60456345102284, 1.5324488810729235, 1.51844004902523, 13.294984571053647, 1.6034805410308763, 4.858007718110457, 11.610882675973698, 1.519225327996537, 1.7189706600038335, 6.654624328948557, 1.601171963964589, 1.8456944689387456, 18.056553184054792, 2.2069750480586663, 1.5963321070885286, 1.4157142790500075, 5.007615176960826, 12.661889500101097, 1.5515024249907583]
Total Epoch List: [99, 96, 92]
Total Time List: [0.3750356190139428, 2.1044147319626063, 0.5155655789421871]
========================training times:6========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3ca0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.44s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 3.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 4.55s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 6.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.45s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.76s
Val loss: 0.6930 score: 0.6122 time: 0.47s
Test loss: 0.6931 score: 0.5510 time: 0.37s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 5.01s
Val loss: 0.6930 score: 0.5102 time: 4.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 3.62s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.40s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.46s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 2.55s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 4.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.45s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.38s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.47s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 10.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.37s
Epoch 13/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.69s
Val loss: 0.6914 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.37s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.69s
Val loss: 0.6910 score: 0.5306 time: 0.46s
Test loss: 0.6919 score: 0.5510 time: 0.37s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.61s
Val loss: 0.6906 score: 0.5306 time: 1.91s
Test loss: 0.6916 score: 0.5510 time: 6.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 7.97s
Val loss: 0.6902 score: 0.5306 time: 0.48s
Test loss: 0.6914 score: 0.5510 time: 0.36s
Epoch 17/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.69s
Val loss: 0.6897 score: 0.5510 time: 0.55s
Test loss: 0.6911 score: 0.5510 time: 0.36s
Epoch 18/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.58s
Val loss: 0.6891 score: 0.5510 time: 0.48s
Test loss: 0.6908 score: 0.5306 time: 0.40s
Epoch 19/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.63s
Val loss: 0.6885 score: 0.5714 time: 0.47s
Test loss: 0.6904 score: 0.5510 time: 0.46s
Epoch 20/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 10.71s
Val loss: 0.6878 score: 0.5918 time: 4.89s
Test loss: 0.6900 score: 0.5510 time: 0.36s
Epoch 21/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.60s
Val loss: 0.6870 score: 0.5918 time: 0.57s
Test loss: 0.6895 score: 0.5510 time: 0.37s
Epoch 22/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.65s
Val loss: 0.6862 score: 0.5918 time: 0.51s
Test loss: 0.6890 score: 0.5510 time: 3.44s
Epoch 23/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 7.57s
Val loss: 0.6852 score: 0.5918 time: 0.57s
Test loss: 0.6885 score: 0.5510 time: 0.36s
Epoch 24/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.58s
Val loss: 0.6842 score: 0.5918 time: 0.46s
Test loss: 0.6879 score: 0.5510 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.58s
Val loss: 0.6830 score: 0.5918 time: 0.54s
Test loss: 0.6872 score: 0.5510 time: 0.36s
Epoch 26/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.68s
Val loss: 0.6818 score: 0.5918 time: 2.47s
Test loss: 0.6864 score: 0.5510 time: 2.64s
Epoch 27/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 10.50s
Val loss: 0.6803 score: 0.5918 time: 0.45s
Test loss: 0.6855 score: 0.5510 time: 0.49s
Epoch 28/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.63s
Val loss: 0.6789 score: 0.5918 time: 0.49s
Test loss: 0.6846 score: 0.5510 time: 0.39s
Epoch 29/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.60s
Val loss: 0.6772 score: 0.5918 time: 0.57s
Test loss: 0.6835 score: 0.5510 time: 0.49s
Epoch 30/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.58s
Val loss: 0.6754 score: 0.5918 time: 0.51s
Test loss: 0.6825 score: 0.5510 time: 4.55s
Epoch 31/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 10.44s
Val loss: 0.6734 score: 0.5918 time: 0.54s
Test loss: 0.6813 score: 0.5714 time: 0.35s
Epoch 32/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.57s
Val loss: 0.6713 score: 0.6122 time: 0.47s
Test loss: 0.6800 score: 0.5918 time: 0.47s
Epoch 33/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.59s
Val loss: 0.6690 score: 0.6122 time: 0.58s
Test loss: 0.6786 score: 0.5918 time: 0.39s
Epoch 34/1000, LR 0.000270
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.62s
Val loss: 0.6665 score: 0.6122 time: 4.14s
Test loss: 0.6771 score: 0.6122 time: 4.52s
Epoch 35/1000, LR 0.000270
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 1.49s
Val loss: 0.6638 score: 0.6122 time: 0.46s
Test loss: 0.6753 score: 0.6327 time: 0.45s
Epoch 36/1000, LR 0.000270
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.60s
Val loss: 0.6609 score: 0.6531 time: 0.47s
Test loss: 0.6735 score: 0.6327 time: 0.38s
Epoch 37/1000, LR 0.000270
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.63s
Val loss: 0.6577 score: 0.6531 time: 0.56s
Test loss: 0.6714 score: 0.6122 time: 0.40s
Epoch 38/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 5.77s
Val loss: 0.6542 score: 0.6531 time: 0.53s
Test loss: 0.6692 score: 0.6122 time: 0.34s
Epoch 39/1000, LR 0.000269
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.56s
Val loss: 0.6504 score: 0.6735 time: 0.55s
Test loss: 0.6668 score: 0.6122 time: 0.35s
Epoch 40/1000, LR 0.000269
Train loss: 0.6576;  Loss pred: 0.6576; Loss self: 0.0000; time: 0.57s
Val loss: 0.6464 score: 0.6939 time: 0.47s
Test loss: 0.6642 score: 0.6327 time: 0.36s
Epoch 41/1000, LR 0.000269
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 4.62s
Val loss: 0.6419 score: 0.7347 time: 2.51s
Test loss: 0.6615 score: 0.6327 time: 3.02s
Epoch 42/1000, LR 0.000269
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 0.59s
Val loss: 0.6371 score: 0.7347 time: 0.45s
Test loss: 0.6584 score: 0.6122 time: 0.37s
Epoch 43/1000, LR 0.000269
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.58s
Val loss: 0.6318 score: 0.7551 time: 0.48s
Test loss: 0.6551 score: 0.6122 time: 0.47s
Epoch 44/1000, LR 0.000269
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.59s
Val loss: 0.6262 score: 0.7551 time: 0.45s
Test loss: 0.6515 score: 0.6122 time: 0.42s
Epoch 45/1000, LR 0.000269
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.65s
Val loss: 0.6202 score: 0.7551 time: 3.40s
Test loss: 0.6477 score: 0.6327 time: 3.79s
Epoch 46/1000, LR 0.000269
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 7.16s
Val loss: 0.6138 score: 0.7959 time: 0.45s
Test loss: 0.6436 score: 0.6939 time: 0.41s
Epoch 47/1000, LR 0.000269
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.66s
Val loss: 0.6069 score: 0.8163 time: 0.53s
Test loss: 0.6391 score: 0.6939 time: 0.37s
Epoch 48/1000, LR 0.000269
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.62s
Val loss: 0.5996 score: 0.8571 time: 0.48s
Test loss: 0.6343 score: 0.6939 time: 0.38s
Epoch 49/1000, LR 0.000269
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.67s
Val loss: 0.5916 score: 0.8571 time: 0.55s
Test loss: 0.6291 score: 0.6939 time: 0.37s
Epoch 50/1000, LR 0.000269
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.76s
Val loss: 0.5832 score: 0.8571 time: 0.46s
Test loss: 0.6236 score: 0.6939 time: 0.41s
Epoch 51/1000, LR 0.000269
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 7.95s
Val loss: 0.5742 score: 0.8571 time: 0.47s
Test loss: 0.6177 score: 0.6939 time: 0.47s
Epoch 52/1000, LR 0.000269
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.60s
Val loss: 0.5648 score: 0.8571 time: 0.48s
Test loss: 0.6116 score: 0.6939 time: 0.37s
Epoch 53/1000, LR 0.000269
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.61s
Val loss: 0.5549 score: 0.8980 time: 0.58s
Test loss: 0.6051 score: 0.7143 time: 4.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 6.81s
Val loss: 0.5445 score: 0.8980 time: 0.56s
Test loss: 0.5982 score: 0.7347 time: 0.37s
Epoch 55/1000, LR 0.000269
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.63s
Val loss: 0.5337 score: 0.9184 time: 0.55s
Test loss: 0.5911 score: 0.7347 time: 0.38s
Epoch 56/1000, LR 0.000269
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.60s
Val loss: 0.5224 score: 0.9184 time: 0.48s
Test loss: 0.5836 score: 0.7347 time: 0.38s
Epoch 57/1000, LR 0.000269
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.67s
Val loss: 0.5107 score: 0.9184 time: 3.78s
Test loss: 0.5758 score: 0.7347 time: 4.01s
Epoch 58/1000, LR 0.000269
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.86s
Val loss: 0.4986 score: 0.9388 time: 0.47s
Test loss: 0.5677 score: 0.7347 time: 0.37s
Epoch 59/1000, LR 0.000268
Train loss: 0.5121;  Loss pred: 0.5121; Loss self: 0.0000; time: 0.60s
Val loss: 0.4862 score: 0.9388 time: 0.46s
Test loss: 0.5593 score: 0.7551 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.4973;  Loss pred: 0.4973; Loss self: 0.0000; time: 0.60s
Val loss: 0.4736 score: 0.9592 time: 0.46s
Test loss: 0.5506 score: 0.7551 time: 0.37s
Epoch 61/1000, LR 0.000268
Train loss: 0.4900;  Loss pred: 0.4900; Loss self: 0.0000; time: 0.59s
Val loss: 0.4608 score: 0.9592 time: 0.56s
Test loss: 0.5416 score: 0.7551 time: 0.37s
Epoch 62/1000, LR 0.000268
Train loss: 0.4841;  Loss pred: 0.4841; Loss self: 0.0000; time: 0.59s
Val loss: 0.4479 score: 0.9592 time: 3.06s
Test loss: 0.5324 score: 0.7551 time: 4.36s
Epoch 63/1000, LR 0.000268
Train loss: 0.4660;  Loss pred: 0.4660; Loss self: 0.0000; time: 0.68s
Val loss: 0.4352 score: 0.9592 time: 0.56s
Test loss: 0.5230 score: 0.7755 time: 0.38s
Epoch 64/1000, LR 0.000268
Train loss: 0.4549;  Loss pred: 0.4549; Loss self: 0.0000; time: 0.59s
Val loss: 0.4226 score: 0.9592 time: 0.47s
Test loss: 0.5135 score: 0.7755 time: 0.38s
Epoch 65/1000, LR 0.000268
Train loss: 0.4365;  Loss pred: 0.4365; Loss self: 0.0000; time: 0.55s
Val loss: 0.4102 score: 0.9592 time: 0.59s
Test loss: 0.5039 score: 0.7959 time: 0.34s
Epoch 66/1000, LR 0.000268
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.80s
Val loss: 0.3978 score: 0.9592 time: 6.87s
Test loss: 0.4944 score: 0.8163 time: 5.03s
Epoch 67/1000, LR 0.000268
Train loss: 0.4131;  Loss pred: 0.4131; Loss self: 0.0000; time: 0.58s
Val loss: 0.3856 score: 0.9592 time: 0.46s
Test loss: 0.4851 score: 0.8163 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3957;  Loss pred: 0.3957; Loss self: 0.0000; time: 0.63s
Val loss: 0.3736 score: 0.9592 time: 0.52s
Test loss: 0.4759 score: 0.8367 time: 0.36s
Epoch 69/1000, LR 0.000268
Train loss: 0.3815;  Loss pred: 0.3815; Loss self: 0.0000; time: 0.58s
Val loss: 0.3619 score: 0.9592 time: 0.56s
Test loss: 0.4670 score: 0.8367 time: 3.66s
Epoch 70/1000, LR 0.000268
Train loss: 0.3723;  Loss pred: 0.3723; Loss self: 0.0000; time: 3.02s
Val loss: 0.3507 score: 0.9592 time: 0.45s
Test loss: 0.4583 score: 0.8367 time: 0.48s
Epoch 71/1000, LR 0.000268
Train loss: 0.3597;  Loss pred: 0.3597; Loss self: 0.0000; time: 0.59s
Val loss: 0.3400 score: 0.9592 time: 0.54s
Test loss: 0.4498 score: 0.8367 time: 0.35s
Epoch 72/1000, LR 0.000267
Train loss: 0.3468;  Loss pred: 0.3468; Loss self: 0.0000; time: 0.56s
Val loss: 0.3298 score: 0.9592 time: 0.44s
Test loss: 0.4414 score: 0.8571 time: 0.37s
Epoch 73/1000, LR 0.000267
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.60s
Val loss: 0.3200 score: 0.9796 time: 0.55s
Test loss: 0.4334 score: 0.8571 time: 0.37s
Epoch 74/1000, LR 0.000267
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.71s
Val loss: 0.3106 score: 0.9796 time: 0.47s
Test loss: 0.4257 score: 0.8571 time: 0.35s
Epoch 75/1000, LR 0.000267
Train loss: 0.3121;  Loss pred: 0.3121; Loss self: 0.0000; time: 0.58s
Val loss: 0.3016 score: 0.9796 time: 0.51s
Test loss: 0.4183 score: 0.8571 time: 0.87s
Epoch 76/1000, LR 0.000267
Train loss: 0.2967;  Loss pred: 0.2967; Loss self: 0.0000; time: 11.90s
Val loss: 0.2930 score: 0.9796 time: 0.48s
Test loss: 0.4114 score: 0.8571 time: 0.37s
Epoch 77/1000, LR 0.000267
Train loss: 0.2881;  Loss pred: 0.2881; Loss self: 0.0000; time: 0.80s
Val loss: 0.2846 score: 0.9796 time: 0.55s
Test loss: 0.4048 score: 0.8571 time: 0.35s
Epoch 78/1000, LR 0.000267
Train loss: 0.2784;  Loss pred: 0.2784; Loss self: 0.0000; time: 0.64s
Val loss: 0.2766 score: 0.9796 time: 0.46s
Test loss: 0.3985 score: 0.8776 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 0.2662;  Loss pred: 0.2662; Loss self: 0.0000; time: 8.61s
Val loss: 0.2688 score: 0.9796 time: 4.00s
Test loss: 0.3924 score: 0.8776 time: 1.83s
Epoch 80/1000, LR 0.000267
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.58s
Val loss: 0.2613 score: 0.9796 time: 0.47s
Test loss: 0.3868 score: 0.8776 time: 0.43s
Epoch 81/1000, LR 0.000267
Train loss: 0.2477;  Loss pred: 0.2477; Loss self: 0.0000; time: 0.61s
Val loss: 0.2538 score: 0.9796 time: 0.55s
Test loss: 0.3816 score: 0.8776 time: 0.38s
Epoch 82/1000, LR 0.000267
Train loss: 0.2501;  Loss pred: 0.2501; Loss self: 0.0000; time: 0.67s
Val loss: 0.2464 score: 0.9796 time: 3.28s
Test loss: 0.3758 score: 0.8776 time: 3.95s
Epoch 83/1000, LR 0.000266
Train loss: 0.2416;  Loss pred: 0.2416; Loss self: 0.0000; time: 8.20s
Val loss: 0.2390 score: 0.9796 time: 0.48s
Test loss: 0.3694 score: 0.8776 time: 0.46s
Epoch 84/1000, LR 0.000266
Train loss: 0.2129;  Loss pred: 0.2129; Loss self: 0.0000; time: 0.62s
Val loss: 0.2317 score: 0.9796 time: 0.47s
Test loss: 0.3635 score: 0.8776 time: 0.38s
Epoch 85/1000, LR 0.000266
Train loss: 0.2121;  Loss pred: 0.2121; Loss self: 0.0000; time: 0.70s
Val loss: 0.2246 score: 0.9796 time: 0.54s
Test loss: 0.3574 score: 0.8776 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.2027;  Loss pred: 0.2027; Loss self: 0.0000; time: 4.91s
Val loss: 0.2176 score: 0.9592 time: 2.88s
Test loss: 0.3516 score: 0.8776 time: 2.05s
Epoch 87/1000, LR 0.000266
Train loss: 0.1909;  Loss pred: 0.1909; Loss self: 0.0000; time: 2.60s
Val loss: 0.2110 score: 0.9592 time: 0.54s
Test loss: 0.3465 score: 0.8776 time: 0.42s
Epoch 88/1000, LR 0.000266
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.71s
Val loss: 0.2046 score: 0.9592 time: 0.45s
Test loss: 0.3421 score: 0.8776 time: 0.37s
Epoch 89/1000, LR 0.000266
Train loss: 0.1773;  Loss pred: 0.1773; Loss self: 0.0000; time: 0.59s
Val loss: 0.1985 score: 0.9592 time: 0.56s
Test loss: 0.3384 score: 0.8776 time: 0.36s
Epoch 90/1000, LR 0.000266
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 4.32s
Val loss: 0.1927 score: 0.9592 time: 4.43s
Test loss: 0.3354 score: 0.8776 time: 2.90s
Epoch 91/1000, LR 0.000266
Train loss: 0.1492;  Loss pred: 0.1492; Loss self: 0.0000; time: 4.61s
Val loss: 0.1872 score: 0.9592 time: 0.47s
Test loss: 0.3329 score: 0.8776 time: 0.48s
Epoch 92/1000, LR 0.000266
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.59s
Val loss: 0.1821 score: 0.9592 time: 0.47s
Test loss: 0.3306 score: 0.8776 time: 0.34s
Epoch 93/1000, LR 0.000265
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.64s
Val loss: 0.1775 score: 0.9592 time: 4.55s
Test loss: 0.3291 score: 0.8776 time: 2.14s
Epoch 94/1000, LR 0.000265
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 6.41s
Val loss: 0.1732 score: 0.9592 time: 0.49s
Test loss: 0.3280 score: 0.8776 time: 0.37s
Epoch 95/1000, LR 0.000265
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.59s
Val loss: 0.1694 score: 0.9592 time: 0.56s
Test loss: 0.3278 score: 0.8776 time: 0.36s
Epoch 96/1000, LR 0.000265
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.60s
Val loss: 0.1660 score: 0.9592 time: 0.50s
Test loss: 0.3279 score: 0.8776 time: 4.44s
Epoch 97/1000, LR 0.000265
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 11.04s
Val loss: 0.1630 score: 0.9592 time: 4.12s
Test loss: 0.3285 score: 0.8776 time: 0.36s
Epoch 98/1000, LR 0.000265
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.57s
Val loss: 0.1605 score: 0.9592 time: 0.48s
Test loss: 0.3298 score: 0.8776 time: 0.37s
Epoch 99/1000, LR 0.000265
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.62s
Val loss: 0.1584 score: 0.9592 time: 0.47s
Test loss: 0.3315 score: 0.8776 time: 0.46s
Epoch 100/1000, LR 0.000265
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.60s
Val loss: 0.1568 score: 0.9592 time: 0.47s
Test loss: 0.3332 score: 0.8776 time: 0.39s
Epoch 101/1000, LR 0.000265
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 10.60s
Val loss: 0.1555 score: 0.9592 time: 2.21s
Test loss: 0.3350 score: 0.8776 time: 0.37s
Epoch 102/1000, LR 0.000264
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 0.66s
Val loss: 0.1548 score: 0.9592 time: 0.47s
Test loss: 0.3360 score: 0.8776 time: 0.39s
Epoch 103/1000, LR 0.000264
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 0.59s
Val loss: 0.1544 score: 0.9592 time: 0.55s
Test loss: 0.3381 score: 0.8980 time: 0.42s
Epoch 104/1000, LR 0.000264
Train loss: 0.0730;  Loss pred: 0.0730; Loss self: 0.0000; time: 11.06s
Val loss: 0.1543 score: 0.9592 time: 0.48s
Test loss: 0.3404 score: 0.8980 time: 0.37s
Epoch 105/1000, LR 0.000264
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.65s
Val loss: 0.1546 score: 0.9592 time: 0.54s
Test loss: 0.3435 score: 0.8980 time: 0.41s
     INFO: Early stopping counter 1 of 2
Epoch 106/1000, LR 0.000264
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.61s
Val loss: 0.1552 score: 0.9592 time: 2.98s
Test loss: 0.3469 score: 0.8980 time: 3.75s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 103,   Train_Loss: 0.0730,   Val_Loss: 0.1543,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1543,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3404


[0.4435802800580859, 4.551122078089975, 0.4576469419989735, 0.3754494460299611, 3.626675458974205, 0.40348730597179383, 0.4629972199909389, 2.5590932270279154, 0.4493953241035342, 0.3801690438995138, 0.4760875770589337, 0.37338174995966256, 0.3729933761060238, 0.36962133296765387, 6.506644036970101, 0.36563153797760606, 0.3603579089976847, 0.4029770550550893, 0.4601977460552007, 0.3652433300158009, 0.37709669198375195, 3.4448492290684953, 0.3672673850087449, 0.4580735800554976, 0.3595658600097522, 2.6463547020684928, 0.4906719740247354, 0.3978246410842985, 0.49129294499289244, 4.554239718010649, 0.3534883940592408, 0.4719445559894666, 0.3927007169695571, 4.523859304958023, 0.4518898680107668, 0.38163037598133087, 0.4062572290422395, 0.34865663608070463, 0.3519276090664789, 0.3673571450635791, 3.0235011499607936, 0.3783585879718885, 0.4702433709753677, 0.4254812920698896, 3.7896698450203985, 0.41332442907150835, 0.37521107599604875, 0.38887714594602585, 0.3770813839510083, 0.4122032399754971, 0.4777176899369806, 0.37352726398967206, 4.081755780032836, 0.37888861494138837, 0.38508199399802834, 0.3864831259706989, 4.011834317003377, 0.3714592409087345, 0.44997662398964167, 0.3781173990573734, 0.37739273300394416, 4.360086142085493, 0.3855460670311004, 0.38391760701779276, 0.3477050910005346, 5.038651328999549, 0.4596331869252026, 0.3624830699991435, 3.6667593780439347, 0.4831357180373743, 0.3559741899371147, 0.37138498097192496, 0.3717262849677354, 0.35259851603768766, 0.8736362350173295, 0.37558121606707573, 0.3563393659424037, 0.3832240530755371, 1.8378295360598713, 0.4374832910252735, 0.3817098040599376, 3.957931222044863, 0.46898721798788756, 0.3815993530442938, 0.3883537979563698, 2.0519517899956554, 0.42359994701109827, 0.3753528910456225, 0.3647870810236782, 2.907217377098277, 0.4856897610006854, 0.3483068080386147, 2.1446776599623263, 0.3701723190024495, 0.36315767897758633, 4.442414731020108, 0.36290659999940544, 0.37786815990693867, 0.4690808559535071, 0.3989610029384494, 0.3697837529471144, 0.3984077100176364, 0.4221927149919793, 0.3774319199146703, 0.41746534197591245, 3.7595237749628723]
[0.009052658776695631, 0.09288004240999949, 0.009339733510183133, 0.007662233592448186, 0.07401378487702459, 0.008234434815750894, 0.009448922856957937, 0.052226392388324806, 0.009171333144970087, 0.0077585519163166075, 0.009716073001202728, 0.007620035713462502, 0.0076121097164494654, 0.0075432925095439566, 0.13278865381571636, 0.007461868121991961, 0.0073542430407690755, 0.008224021531736516, 0.009391790735820423, 0.007453945510526549, 0.007695850856811264, 0.07030304549119379, 0.007495252755280508, 0.00934844040929587, 0.007338078775709229, 0.054007238817724346, 0.010013713755606845, 0.008118870226210174, 0.01002638663250801, 0.09294366771450305, 0.007214048858351854, 0.009631521550805442, 0.008014300346317492, 0.09232365928485761, 0.009222242204301363, 0.007788375020027161, 0.008290963858004888, 0.0071154415526674415, 0.0071821961033975285, 0.007497084593134267, 0.061704105101240685, 0.007721603836160989, 0.009596803489293218, 0.008683291674895706, 0.07734020091878364, 0.008435192430030783, 0.007657368897878546, 0.007936268284612772, 0.007695538447979762, 0.008412311019908105, 0.009749340610958787, 0.007623005387544328, 0.08330113836801706, 0.007732420713089558, 0.007858816204041394, 0.007887410734095896, 0.08187416973476279, 0.007580800834872133, 0.009183196407951871, 0.007716681613415784, 0.007701892510284574, 0.08898134983847944, 0.007868287082267354, 0.007835053204444751, 0.007096022265317033, 0.10282961895917447, 0.009380269120922503, 0.007397613673451908, 0.07483182404171296, 0.00985991261300764, 0.007264779386471729, 0.007579285325957652, 0.007586250713627253, 0.007195888082401789, 0.01782931091872101, 0.007664922776879097, 0.007272231958008238, 0.0078208990423579, 0.03750672522571166, 0.008928230429087214, 0.007789996001223216, 0.08077410657234414, 0.009571167714038521, 0.007787741898863139, 0.007925587713395302, 0.04187656714276848, 0.008644896877777516, 0.0076602630825637254, 0.007444634306605677, 0.059330966879556676, 0.009912035938789497, 0.0071083022048696876, 0.04376893183596584, 0.007554537122498969, 0.007411381203624211, 0.09066152512285934, 0.007406257142845009, 0.007711595100141606, 0.009573078692928717, 0.00814206128445815, 0.007546607203002335, 0.008130769592196661, 0.008616177856979169, 0.007702692243156536, 0.008519700856651274, 0.07672497499924229]
[110.46478439840371, 10.766575617889035, 107.06943607220674, 130.51024716677773, 13.510996656386649, 121.44124306955369, 105.83216892956497, 19.147407168478857, 109.03540239931625, 128.89003138548986, 102.92224027919639, 131.23298073698996, 131.36962514334758, 132.5681058681969, 7.530763896347625, 134.01469761342392, 135.97592498050273, 121.59501238426941, 106.47596695122112, 134.1571384695244, 129.94014808836158, 14.224134858073265, 133.41778224830193, 106.96971432856581, 136.27545173134905, 18.516036403472185, 99.863050253467, 123.1698465596478, 99.73682809694914, 10.759205275519365, 138.6184124387069, 103.8257553310852, 124.77695578997013, 10.831459755235397, 108.43349999348186, 128.39648802588255, 120.61323835521301, 140.5394159446253, 139.2331796018423, 133.38518294375217, 16.206377166628627, 129.50677362090335, 104.20136258032804, 115.1637002924943, 12.929886244414057, 118.55094098859233, 130.5931597832576, 126.00380482837875, 129.94542315132227, 118.87339847914039, 102.57103940711085, 131.18185665117832, 12.004637866797072, 129.3256067025925, 127.24562759029155, 126.78431917805105, 12.213864314466592, 131.9121847127207, 108.89454560005758, 129.58938182203303, 129.8382181606234, 11.23831007076447, 127.0924649220903, 127.63155193798934, 140.9240223058013, 9.724824521590623, 106.60674945556946, 135.17872710611223, 13.36329847368918, 101.42077716599184, 137.65042911862847, 131.9385610903425, 131.81742045562646, 138.96825361216952, 56.08741720634794, 130.46445856133818, 137.5093651817297, 127.86253787243787, 26.66188514145401, 112.00427766090216, 128.3697706446802, 12.380205024048948, 104.48045942536864, 128.40692629348447, 126.17360833820139, 23.879703333626445, 115.67517972025668, 130.54381934690963, 134.3249324030185, 16.854604814211516, 100.88744695594036, 140.68056916811017, 22.847256216983553, 132.37078377996102, 134.92761639503766, 11.030037258306177, 135.02096682749854, 129.67485805649176, 104.45960302600076, 122.81902150611845, 132.50987802865384, 122.98958772115857, 116.06074254722962, 129.8247376932982, 117.37501314019806, 13.033565667631375]
Elapsed: 1.089991635601212~1.4252936059263213
Time per graph: 0.022244727257167588~0.029087624610741252
Speed: 100.48414027872613~45.75005809357876
Total Time: 3.7601
best val loss: 0.15434403717517853 test_score: 0.8980

Testing...
Test loss: 0.4334 score: 0.8571 time: 3.72s
test Score 0.8571
Epoch Time List: [1.8813385849352926, 8.49357967497781, 7.717399749089964, 1.6007732298457995, 13.049987877951935, 1.5104781170375645, 1.558591723907739, 3.755699184141122, 5.027086477144621, 1.4792417108546942, 1.6509268539957702, 11.538081565056928, 1.6111235129646957, 1.519467237056233, 9.024763936875388, 8.807763322023675, 1.5920812080148607, 1.457161070080474, 1.5501198739511892, 15.958313024952076, 1.5339648810913786, 4.604160696035251, 8.507032567053102, 1.4915480819763616, 1.4687214189907536, 5.790521082933992, 11.437618295894936, 1.5082647640956566, 1.6568819229723886, 5.638171296915971, 11.33021611708682, 1.5088091499637812, 1.5597970798844472, 9.277279934147373, 2.406080229091458, 1.4440430050017312, 1.5835587558103725, 6.637765489052981, 1.4555649599060416, 1.4083128590136766, 10.149847115040757, 1.4100076919421554, 1.5298861380433664, 1.4612722530728206, 7.835309653892182, 8.023101286962628, 1.5674914820119739, 1.4800229619722813, 1.5979695859132335, 1.6269284079317003, 8.89813007588964, 1.4486775650875643, 5.270478735095821, 7.744967365986668, 1.5621171781094745, 1.464288822025992, 8.455258563975804, 1.6920138499699533, 1.5054120689164847, 1.4382804959313944, 1.5191279030404985, 8.007179253152572, 1.6204417400294915, 1.440046028001234, 1.4887892259284854, 12.69772507098969, 1.4950107811018825, 1.5103925400180742, 4.801245086826384, 3.951403641141951, 1.4821460959501565, 1.366814156062901, 1.5207180159632117, 1.5290972280781716, 1.954230366856791, 12.746766753029078, 1.7050792860100046, 1.4808608629973605, 14.441290398011915, 1.4837391949258745, 1.535826510982588, 7.901042829966173, 9.146282617002726, 1.4636473978171125, 1.6197860719403252, 9.833450815174729, 3.559310813085176, 1.5249522379599512, 1.5150061291642487, 11.64627356082201, 5.560740241897292, 1.4091687520267442, 7.322594702942297, 7.263032285147347, 1.5052163981599733, 5.543712810962461, 15.513742491952144, 1.4248059660894796, 1.5611297860741615, 1.4601827278966084, 13.17440832790453, 1.5264933709986508, 1.5600484929746017, 11.914933681022376, 1.6058888878906146, 7.341572490986437]
Total Epoch List: [106]
Total Time List: [3.7601034099934623]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.4898 time: 4.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 3.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.4898 time: 0.48s
Epoch 3/1000, LR 0.000030
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4898 time: 0.70s
Epoch 4/1000, LR 0.000060
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4898 time: 3.62s
Epoch 5/1000, LR 0.000090
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 8.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.4898 time: 0.55s
Epoch 6/1000, LR 0.000120
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4898 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5102 time: 5.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4898 time: 4.52s
Epoch 8/1000, LR 0.000180
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 3.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4898 time: 0.58s
Epoch 9/1000, LR 0.000210
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4898 time: 0.57s
Epoch 10/1000, LR 0.000240
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 3.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 1.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4898 time: 2.79s
Epoch 11/1000, LR 0.000270
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4898 time: 0.58s
Epoch 12/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4898 time: 0.48s
Epoch 13/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 5.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 0.55s
Epoch 14/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.45s
Epoch 15/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 4.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 9.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 1.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.46s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.47s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 1.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4898 time: 3.71s
Epoch 20/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 7.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4898 time: 0.51s
Epoch 21/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4898 time: 0.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5102 time: 3.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4898 time: 2.67s
Epoch 23/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 1.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4898 time: 0.47s
Epoch 24/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4898 time: 0.47s
Epoch 25/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4898 time: 0.55s
Epoch 26/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5102 time: 4.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4898 time: 3.10s
Epoch 27/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.4898 time: 0.48s
Epoch 28/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4898 time: 0.47s
Epoch 29/1000, LR 0.000270
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.4898 time: 2.57s
Epoch 30/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 7.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.4898 time: 0.47s
Epoch 31/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.4898 time: 0.47s
Epoch 32/1000, LR 0.000270
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.5102 time: 4.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6740 score: 0.4898 time: 4.69s
Epoch 33/1000, LR 0.000270
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 5.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.5102 time: 0.37s
Test loss: 0.6713 score: 0.5306 time: 0.45s
Epoch 34/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.68s
Val loss: 0.6757 score: 0.5510 time: 0.35s
Test loss: 0.6683 score: 0.5714 time: 0.54s
Epoch 35/1000, LR 0.000270
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.63s
Val loss: 0.6735 score: 0.5918 time: 3.98s
Test loss: 0.6650 score: 0.5714 time: 4.19s
Epoch 36/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 4.89s
Val loss: 0.6710 score: 0.5510 time: 0.37s
Test loss: 0.6614 score: 0.5918 time: 0.60s
Epoch 37/1000, LR 0.000270
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.72s
Val loss: 0.6682 score: 0.5918 time: 0.40s
Test loss: 0.6574 score: 0.5918 time: 0.45s
Epoch 38/1000, LR 0.000270
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.67s
Val loss: 0.6652 score: 0.6122 time: 2.72s
Test loss: 0.6532 score: 0.6122 time: 4.58s
Epoch 39/1000, LR 0.000269
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 7.13s
Val loss: 0.6619 score: 0.6327 time: 0.39s
Test loss: 0.6487 score: 0.6122 time: 0.48s
Epoch 40/1000, LR 0.000269
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.69s
Val loss: 0.6584 score: 0.6735 time: 0.36s
Test loss: 0.6438 score: 0.6531 time: 0.49s
Epoch 41/1000, LR 0.000269
Train loss: 0.6425;  Loss pred: 0.6425; Loss self: 0.0000; time: 0.65s
Val loss: 0.6546 score: 0.6735 time: 4.82s
Test loss: 0.6385 score: 0.6939 time: 3.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 8.11s
Val loss: 0.6504 score: 0.6531 time: 0.56s
Test loss: 0.6329 score: 0.7347 time: 0.49s
Epoch 43/1000, LR 0.000269
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.61s
Val loss: 0.6459 score: 0.6531 time: 0.37s
Test loss: 0.6267 score: 0.7347 time: 0.45s
Epoch 44/1000, LR 0.000269
Train loss: 0.6243;  Loss pred: 0.6243; Loss self: 0.0000; time: 0.57s
Val loss: 0.6409 score: 0.6531 time: 0.45s
Test loss: 0.6200 score: 0.7551 time: 0.55s
Epoch 45/1000, LR 0.000269
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 12.92s
Val loss: 0.6356 score: 0.6939 time: 0.39s
Test loss: 0.6128 score: 0.7755 time: 0.50s
Epoch 46/1000, LR 0.000269
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.92s
Val loss: 0.6298 score: 0.7143 time: 0.49s
Test loss: 0.6050 score: 0.7959 time: 0.49s
Epoch 47/1000, LR 0.000269
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.66s
Val loss: 0.6235 score: 0.7551 time: 0.52s
Test loss: 0.5966 score: 0.8163 time: 0.61s
Epoch 48/1000, LR 0.000269
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 6.80s
Val loss: 0.6168 score: 0.7755 time: 0.38s
Test loss: 0.5876 score: 0.8367 time: 0.62s
Epoch 49/1000, LR 0.000269
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.66s
Val loss: 0.6096 score: 0.7755 time: 0.36s
Test loss: 0.5781 score: 0.8776 time: 0.56s
Epoch 50/1000, LR 0.000269
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.68s
Val loss: 0.6020 score: 0.7755 time: 1.75s
Test loss: 0.5680 score: 0.9184 time: 3.83s
Epoch 51/1000, LR 0.000269
Train loss: 0.5588;  Loss pred: 0.5588; Loss self: 0.0000; time: 2.55s
Val loss: 0.5939 score: 0.7755 time: 0.37s
Test loss: 0.5575 score: 0.9184 time: 0.48s
Epoch 52/1000, LR 0.000269
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.73s
Val loss: 0.5855 score: 0.7755 time: 0.35s
Test loss: 0.5465 score: 0.9184 time: 0.47s
Epoch 53/1000, LR 0.000269
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.58s
Val loss: 0.5768 score: 0.7959 time: 0.37s
Test loss: 0.5350 score: 0.9184 time: 0.63s
Epoch 54/1000, LR 0.000269
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 14.18s
Val loss: 0.5676 score: 0.7959 time: 0.41s
Test loss: 0.5231 score: 0.9184 time: 0.47s
Epoch 55/1000, LR 0.000269
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.59s
Val loss: 0.5580 score: 0.8163 time: 0.36s
Test loss: 0.5108 score: 0.9184 time: 0.46s
Epoch 56/1000, LR 0.000269
Train loss: 0.4969;  Loss pred: 0.4969; Loss self: 0.0000; time: 0.72s
Val loss: 0.5481 score: 0.8163 time: 0.36s
Test loss: 0.4982 score: 0.9184 time: 3.69s
Epoch 57/1000, LR 0.000269
Train loss: 0.4823;  Loss pred: 0.4823; Loss self: 0.0000; time: 0.57s
Val loss: 0.5380 score: 0.8367 time: 0.37s
Test loss: 0.4853 score: 0.9388 time: 0.56s
Epoch 58/1000, LR 0.000269
Train loss: 0.4679;  Loss pred: 0.4679; Loss self: 0.0000; time: 0.80s
Val loss: 0.5277 score: 0.8367 time: 0.35s
Test loss: 0.4722 score: 0.9388 time: 0.47s
Epoch 59/1000, LR 0.000268
Train loss: 0.4538;  Loss pred: 0.4538; Loss self: 0.0000; time: 0.63s
Val loss: 0.5170 score: 0.8571 time: 2.31s
Test loss: 0.4589 score: 0.9388 time: 4.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.4368;  Loss pred: 0.4368; Loss self: 0.0000; time: 4.44s
Val loss: 0.5060 score: 0.8571 time: 0.35s
Test loss: 0.4453 score: 0.9388 time: 0.46s
Epoch 61/1000, LR 0.000268
Train loss: 0.4273;  Loss pred: 0.4273; Loss self: 0.0000; time: 0.58s
Val loss: 0.4949 score: 0.8571 time: 0.36s
Test loss: 0.4318 score: 0.9592 time: 0.49s
Epoch 62/1000, LR 0.000268
Train loss: 0.4050;  Loss pred: 0.4050; Loss self: 0.0000; time: 0.68s
Val loss: 0.4836 score: 0.8571 time: 3.01s
Test loss: 0.4182 score: 0.9592 time: 4.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.4031;  Loss pred: 0.4031; Loss self: 0.0000; time: 1.83s
Val loss: 0.4724 score: 0.8571 time: 0.39s
Test loss: 0.4046 score: 0.9592 time: 0.47s
Epoch 64/1000, LR 0.000268
Train loss: 0.3803;  Loss pred: 0.3803; Loss self: 0.0000; time: 0.75s
Val loss: 0.4612 score: 0.8571 time: 0.40s
Test loss: 0.3913 score: 0.9388 time: 0.46s
Epoch 65/1000, LR 0.000268
Train loss: 0.3687;  Loss pred: 0.3687; Loss self: 0.0000; time: 0.61s
Val loss: 0.4501 score: 0.8571 time: 0.39s
Test loss: 0.3782 score: 0.9592 time: 3.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.3499;  Loss pred: 0.3499; Loss self: 0.0000; time: 6.05s
Val loss: 0.4393 score: 0.8571 time: 0.45s
Test loss: 0.3654 score: 0.9592 time: 0.46s
Epoch 67/1000, LR 0.000268
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.58s
Val loss: 0.4286 score: 0.8571 time: 0.37s
Test loss: 0.3530 score: 0.9592 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.60s
Val loss: 0.4178 score: 0.8571 time: 0.51s
Test loss: 0.3406 score: 0.9592 time: 0.48s
Epoch 69/1000, LR 0.000268
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 7.24s
Val loss: 0.4075 score: 0.8571 time: 0.35s
Test loss: 0.3286 score: 0.9592 time: 0.45s
Epoch 70/1000, LR 0.000268
Train loss: 0.2875;  Loss pred: 0.2875; Loss self: 0.0000; time: 0.66s
Val loss: 0.3974 score: 0.8571 time: 0.35s
Test loss: 0.3171 score: 0.9592 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.2646;  Loss pred: 0.2646; Loss self: 0.0000; time: 0.58s
Val loss: 0.3876 score: 0.8571 time: 2.84s
Test loss: 0.3060 score: 0.9592 time: 3.92s
Epoch 72/1000, LR 0.000267
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 9.00s
Val loss: 0.3784 score: 0.8571 time: 0.36s
Test loss: 0.2957 score: 0.9592 time: 0.47s
Epoch 73/1000, LR 0.000267
Train loss: 0.2480;  Loss pred: 0.2480; Loss self: 0.0000; time: 0.87s
Val loss: 0.3699 score: 0.8571 time: 0.36s
Test loss: 0.2861 score: 0.9388 time: 0.59s
Epoch 74/1000, LR 0.000267
Train loss: 0.2316;  Loss pred: 0.2316; Loss self: 0.0000; time: 7.96s
Val loss: 0.3618 score: 0.8776 time: 4.61s
Test loss: 0.2771 score: 0.9388 time: 0.59s
Epoch 75/1000, LR 0.000267
Train loss: 0.2011;  Loss pred: 0.2011; Loss self: 0.0000; time: 0.60s
Val loss: 0.3541 score: 0.8776 time: 0.36s
Test loss: 0.2684 score: 0.9388 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.68s
Val loss: 0.3469 score: 0.8776 time: 0.35s
Test loss: 0.2605 score: 0.9388 time: 0.47s
Epoch 77/1000, LR 0.000267
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.64s
Val loss: 0.3402 score: 0.8776 time: 3.51s
Test loss: 0.2532 score: 0.9388 time: 4.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 4.00s
Val loss: 0.3341 score: 0.8776 time: 0.37s
Test loss: 0.2465 score: 0.9388 time: 0.46s
Epoch 79/1000, LR 0.000267
Train loss: 0.1597;  Loss pred: 0.1597; Loss self: 0.0000; time: 0.61s
Val loss: 0.3290 score: 0.8776 time: 0.38s
Test loss: 0.2412 score: 0.9388 time: 0.46s
Epoch 80/1000, LR 0.000267
Train loss: 0.1566;  Loss pred: 0.1566; Loss self: 0.0000; time: 0.70s
Val loss: 0.3254 score: 0.8776 time: 0.37s
Test loss: 0.2381 score: 0.9388 time: 0.48s
Epoch 81/1000, LR 0.000267
Train loss: 0.1420;  Loss pred: 0.1420; Loss self: 0.0000; time: 3.70s
Val loss: 0.3220 score: 0.8776 time: 0.40s
Test loss: 0.2345 score: 0.9388 time: 0.48s
Epoch 82/1000, LR 0.000267
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.67s
Val loss: 0.3193 score: 0.8776 time: 0.37s
Test loss: 0.2316 score: 0.9388 time: 0.47s
Epoch 83/1000, LR 0.000266
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.59s
Val loss: 0.3171 score: 0.8776 time: 0.39s
Test loss: 0.2288 score: 0.9388 time: 3.52s
Epoch 84/1000, LR 0.000266
Train loss: 0.1029;  Loss pred: 0.1029; Loss self: 0.0000; time: 6.23s
Val loss: 0.3155 score: 0.8776 time: 0.36s
Test loss: 0.2264 score: 0.9388 time: 0.47s
Epoch 85/1000, LR 0.000266
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.61s
Val loss: 0.3146 score: 0.8776 time: 0.36s
Test loss: 0.2246 score: 0.9388 time: 0.50s
Epoch 86/1000, LR 0.000266
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 6.32s
Val loss: 0.3149 score: 0.8776 time: 0.36s
Test loss: 0.2241 score: 0.9388 time: 0.63s
     INFO: Early stopping counter 1 of 2
Epoch 87/1000, LR 0.000266
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.63s
Val loss: 0.3163 score: 0.8776 time: 0.38s
Test loss: 0.2254 score: 0.9388 time: 0.57s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.0970,   Val_Loss: 0.3146,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3146,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2246


[0.4435802800580859, 4.551122078089975, 0.4576469419989735, 0.3754494460299611, 3.626675458974205, 0.40348730597179383, 0.4629972199909389, 2.5590932270279154, 0.4493953241035342, 0.3801690438995138, 0.4760875770589337, 0.37338174995966256, 0.3729933761060238, 0.36962133296765387, 6.506644036970101, 0.36563153797760606, 0.3603579089976847, 0.4029770550550893, 0.4601977460552007, 0.3652433300158009, 0.37709669198375195, 3.4448492290684953, 0.3672673850087449, 0.4580735800554976, 0.3595658600097522, 2.6463547020684928, 0.4906719740247354, 0.3978246410842985, 0.49129294499289244, 4.554239718010649, 0.3534883940592408, 0.4719445559894666, 0.3927007169695571, 4.523859304958023, 0.4518898680107668, 0.38163037598133087, 0.4062572290422395, 0.34865663608070463, 0.3519276090664789, 0.3673571450635791, 3.0235011499607936, 0.3783585879718885, 0.4702433709753677, 0.4254812920698896, 3.7896698450203985, 0.41332442907150835, 0.37521107599604875, 0.38887714594602585, 0.3770813839510083, 0.4122032399754971, 0.4777176899369806, 0.37352726398967206, 4.081755780032836, 0.37888861494138837, 0.38508199399802834, 0.3864831259706989, 4.011834317003377, 0.3714592409087345, 0.44997662398964167, 0.3781173990573734, 0.37739273300394416, 4.360086142085493, 0.3855460670311004, 0.38391760701779276, 0.3477050910005346, 5.038651328999549, 0.4596331869252026, 0.3624830699991435, 3.6667593780439347, 0.4831357180373743, 0.3559741899371147, 0.37138498097192496, 0.3717262849677354, 0.35259851603768766, 0.8736362350173295, 0.37558121606707573, 0.3563393659424037, 0.3832240530755371, 1.8378295360598713, 0.4374832910252735, 0.3817098040599376, 3.957931222044863, 0.46898721798788756, 0.3815993530442938, 0.3883537979563698, 2.0519517899956554, 0.42359994701109827, 0.3753528910456225, 0.3647870810236782, 2.907217377098277, 0.4856897610006854, 0.3483068080386147, 2.1446776599623263, 0.3701723190024495, 0.36315767897758633, 4.442414731020108, 0.36290659999940544, 0.37786815990693867, 0.4690808559535071, 0.3989610029384494, 0.3697837529471144, 0.3984077100176364, 0.4221927149919793, 0.3774319199146703, 0.41746534197591245, 3.7595237749628723, 4.225840575993061, 0.47957511502318084, 0.7010523810749874, 3.621445572003722, 0.5508374370401725, 0.4619803329696879, 4.5204147200565785, 0.5858709809836, 0.5744806979782879, 2.794109210022725, 0.579839578946121, 0.4861949310870841, 0.5567227349383757, 0.4539338960312307, 4.202859974000603, 0.46176943299360573, 0.4634208040079102, 0.47482201911043376, 3.7185363830067217, 0.5187543999636546, 0.46726293803658336, 2.6699049830203876, 0.4746380050200969, 0.471718191052787, 0.5565014029853046, 3.1078128500375897, 0.48804002604447305, 0.4722104601096362, 2.578281335067004, 0.475961270974949, 0.47803018102422357, 4.6924588709371164, 0.4518580069998279, 0.5419006400043145, 4.1897394669940695, 0.6006032929290086, 0.45508084003813565, 4.580973110976629, 0.48054605408106, 0.49017839203588665, 3.2409194270148873, 0.4929110480006784, 0.45226673199795187, 0.5520312449662015, 0.5035801320336759, 0.4974994409130886, 0.61854900396429, 0.6246068630134687, 0.564935531001538, 3.8391890710918233, 0.49069759994745255, 0.472272319952026, 0.6306147479917854, 0.4743257329100743, 0.4673180090030655, 3.6921879090368748, 0.5686922420281917, 0.4747353499988094, 4.054767710040323, 0.46192762395367026, 0.4952418969478458, 4.09319442207925, 0.4696924629388377, 0.4613591629313305, 3.4889669050462544, 0.4671358389314264, 0.4684681269573048, 0.488010025001131, 0.45859468705020845, 0.47256085800472647, 3.9214308250229806, 0.4699987900676206, 0.5986171810654923, 0.5955904220463708, 0.4781657459679991, 0.4771062929648906, 4.225311993970536, 0.4668075799709186, 0.4626039389986545, 0.4875067980028689, 0.481148503953591, 0.4713813300477341, 3.529947553994134, 0.4713771849637851, 0.5037895120913163, 0.6373581690713763, 0.5771085509331897]
[0.009052658776695631, 0.09288004240999949, 0.009339733510183133, 0.007662233592448186, 0.07401378487702459, 0.008234434815750894, 0.009448922856957937, 0.052226392388324806, 0.009171333144970087, 0.0077585519163166075, 0.009716073001202728, 0.007620035713462502, 0.0076121097164494654, 0.0075432925095439566, 0.13278865381571636, 0.007461868121991961, 0.0073542430407690755, 0.008224021531736516, 0.009391790735820423, 0.007453945510526549, 0.007695850856811264, 0.07030304549119379, 0.007495252755280508, 0.00934844040929587, 0.007338078775709229, 0.054007238817724346, 0.010013713755606845, 0.008118870226210174, 0.01002638663250801, 0.09294366771450305, 0.007214048858351854, 0.009631521550805442, 0.008014300346317492, 0.09232365928485761, 0.009222242204301363, 0.007788375020027161, 0.008290963858004888, 0.0071154415526674415, 0.0071821961033975285, 0.007497084593134267, 0.061704105101240685, 0.007721603836160989, 0.009596803489293218, 0.008683291674895706, 0.07734020091878364, 0.008435192430030783, 0.007657368897878546, 0.007936268284612772, 0.007695538447979762, 0.008412311019908105, 0.009749340610958787, 0.007623005387544328, 0.08330113836801706, 0.007732420713089558, 0.007858816204041394, 0.007887410734095896, 0.08187416973476279, 0.007580800834872133, 0.009183196407951871, 0.007716681613415784, 0.007701892510284574, 0.08898134983847944, 0.007868287082267354, 0.007835053204444751, 0.007096022265317033, 0.10282961895917447, 0.009380269120922503, 0.007397613673451908, 0.07483182404171296, 0.00985991261300764, 0.007264779386471729, 0.007579285325957652, 0.007586250713627253, 0.007195888082401789, 0.01782931091872101, 0.007664922776879097, 0.007272231958008238, 0.0078208990423579, 0.03750672522571166, 0.008928230429087214, 0.007789996001223216, 0.08077410657234414, 0.009571167714038521, 0.007787741898863139, 0.007925587713395302, 0.04187656714276848, 0.008644896877777516, 0.0076602630825637254, 0.007444634306605677, 0.059330966879556676, 0.009912035938789497, 0.0071083022048696876, 0.04376893183596584, 0.007554537122498969, 0.007411381203624211, 0.09066152512285934, 0.007406257142845009, 0.007711595100141606, 0.009573078692928717, 0.00814206128445815, 0.007546607203002335, 0.008130769592196661, 0.008616177856979169, 0.007702692243156536, 0.008519700856651274, 0.07672497499924229, 0.08624164440802165, 0.009787247245371038, 0.014307191450509946, 0.07390705248987188, 0.011241580347758623, 0.009428170060605876, 0.09225336163380772, 0.011956550632318368, 0.011724095877107918, 0.05702263693923929, 0.011833460794818796, 0.009922345532389472, 0.011361688468130115, 0.00926395706186185, 0.08577265253062455, 0.009423865979461342, 0.009457567428732862, 0.009690245287968037, 0.07588849761238207, 0.010586824489054176, 0.009535978327277211, 0.054487856796334444, 0.009686489898369325, 0.009626901858220143, 0.011357171489496012, 0.06342475204158346, 0.009960000531519858, 0.00963694816550278, 0.052617986429938854, 0.009713495326019367, 0.009755717980086195, 0.09576446675381871, 0.009221591979588325, 0.011059196734781928, 0.08550488708151162, 0.012257210059775685, 0.009287364082410932, 0.09348924716278835, 0.009807062328184898, 0.010003640653793605, 0.06614121279622219, 0.010059409142870988, 0.00922993330608065, 0.01126594377482044, 0.010277145551707672, 0.010153049814552829, 0.012623449060495714, 0.012747078837009564, 0.011529296551051796, 0.07835079736922089, 0.01001423673362148, 0.009638210611265838, 0.012869688734526232, 0.00968011699816478, 0.009537102224552358, 0.07535077365381378, 0.01160596412302432, 0.009688476530587947, 0.08275036142939436, 0.009427094366401434, 0.010106977488731548, 0.08353458004243368, 0.009585560468139544, 0.009415493121047561, 0.07120340622543377, 0.009533384467988295, 0.009560574019536833, 0.009959388265329204, 0.009359075245922622, 0.009644099142953602, 0.08002920051067307, 0.009591812042196338, 0.012216677164601885, 0.012154906572374915, 0.009758484611591818, 0.009736863121732461, 0.08623085701980684, 0.009526685305528951, 0.009440896714258254, 0.009949118326589162, 0.009819357223542673, 0.009620027143831308, 0.07203974599988029, 0.009619942550281329, 0.010281418614108495, 0.01300730957288523, 0.011777725529248769]
[110.46478439840371, 10.766575617889035, 107.06943607220674, 130.51024716677773, 13.510996656386649, 121.44124306955369, 105.83216892956497, 19.147407168478857, 109.03540239931625, 128.89003138548986, 102.92224027919639, 131.23298073698996, 131.36962514334758, 132.5681058681969, 7.530763896347625, 134.01469761342392, 135.97592498050273, 121.59501238426941, 106.47596695122112, 134.1571384695244, 129.94014808836158, 14.224134858073265, 133.41778224830193, 106.96971432856581, 136.27545173134905, 18.516036403472185, 99.863050253467, 123.1698465596478, 99.73682809694914, 10.759205275519365, 138.6184124387069, 103.8257553310852, 124.77695578997013, 10.831459755235397, 108.43349999348186, 128.39648802588255, 120.61323835521301, 140.5394159446253, 139.2331796018423, 133.38518294375217, 16.206377166628627, 129.50677362090335, 104.20136258032804, 115.1637002924943, 12.929886244414057, 118.55094098859233, 130.5931597832576, 126.00380482837875, 129.94542315132227, 118.87339847914039, 102.57103940711085, 131.18185665117832, 12.004637866797072, 129.3256067025925, 127.24562759029155, 126.78431917805105, 12.213864314466592, 131.9121847127207, 108.89454560005758, 129.58938182203303, 129.8382181606234, 11.23831007076447, 127.0924649220903, 127.63155193798934, 140.9240223058013, 9.724824521590623, 106.60674945556946, 135.17872710611223, 13.36329847368918, 101.42077716599184, 137.65042911862847, 131.9385610903425, 131.81742045562646, 138.96825361216952, 56.08741720634794, 130.46445856133818, 137.5093651817297, 127.86253787243787, 26.66188514145401, 112.00427766090216, 128.3697706446802, 12.380205024048948, 104.48045942536864, 128.40692629348447, 126.17360833820139, 23.879703333626445, 115.67517972025668, 130.54381934690963, 134.3249324030185, 16.854604814211516, 100.88744695594036, 140.68056916811017, 22.847256216983553, 132.37078377996102, 134.92761639503766, 11.030037258306177, 135.02096682749854, 129.67485805649176, 104.45960302600076, 122.81902150611845, 132.50987802865384, 122.98958772115857, 116.06074254722962, 129.8247376932982, 117.37501314019806, 13.033565667631375, 11.595326212344187, 102.17377521273498, 69.89491987012988, 13.530508473965169, 88.95546436221333, 106.06512118171719, 10.839713396780263, 83.63616152780851, 85.29442359410989, 17.53689505915965, 84.50613200475075, 100.78262208625009, 88.01508708895079, 107.94523261736946, 11.658727700451571, 106.11356339101492, 105.73543435301539, 103.19656213879925, 13.177227530682313, 94.45703015421765, 104.86601014387246, 18.352713040959117, 103.2365707797151, 103.87557853268525, 88.05009248340373, 15.766715167358722, 100.40160106772643, 103.76729051834927, 19.004908166364473, 102.94955280632274, 102.50398812688563, 10.442286517092981, 108.44114576024025, 90.42248040085333, 11.695237946419391, 81.58463427837351, 107.6731773543659, 10.696417292341096, 101.96733400236084, 99.96360671160029, 15.119166367283748, 99.40941717324331, 108.3431447268642, 88.76309166703066, 97.30328280052801, 98.49257299680086, 79.21765241873845, 78.44934614326098, 86.73556062783135, 12.763111973035738, 99.85783506022298, 103.75369872402744, 77.70195694921853, 103.30453652467077, 104.85365223679743, 13.271263870419284, 86.16259617899084, 103.21540201319091, 12.084539363048423, 106.07722391790652, 98.94154816461382, 11.971090289698262, 104.32358163341588, 106.20792635540056, 14.044271938816339, 104.8945422643819, 104.59623009628093, 100.40777338516035, 106.84816327720631, 103.6903483857944, 12.495439084970432, 104.25558753661936, 81.85531847379286, 82.27130287226986, 102.47492718409673, 102.70248102471753, 11.596776775282477, 104.96830407735159, 105.92214174843531, 100.51141891915043, 101.83965989162938, 103.94981064489349, 13.88122606653363, 103.95072473387647, 97.26284256413484, 76.87984931830786, 84.90603703716842]
Elapsed: 1.181801955270523~1.4281241718337394
Time per graph: 0.024118407250418836~0.029145391261913053
Speed: 89.9721166532427~43.59130179788223
Total Time: 0.5777
best val loss: 0.3146343529224396 test_score: 0.9388

Testing...
Test loss: 0.2771 score: 0.9388 time: 0.60s
test Score 0.9388
Epoch Time List: [1.8813385849352926, 8.49357967497781, 7.717399749089964, 1.6007732298457995, 13.049987877951935, 1.5104781170375645, 1.558591723907739, 3.755699184141122, 5.027086477144621, 1.4792417108546942, 1.6509268539957702, 11.538081565056928, 1.6111235129646957, 1.519467237056233, 9.024763936875388, 8.807763322023675, 1.5920812080148607, 1.457161070080474, 1.5501198739511892, 15.958313024952076, 1.5339648810913786, 4.604160696035251, 8.507032567053102, 1.4915480819763616, 1.4687214189907536, 5.790521082933992, 11.437618295894936, 1.5082647640956566, 1.6568819229723886, 5.638171296915971, 11.33021611708682, 1.5088091499637812, 1.5597970798844472, 9.277279934147373, 2.406080229091458, 1.4440430050017312, 1.5835587558103725, 6.637765489052981, 1.4555649599060416, 1.4083128590136766, 10.149847115040757, 1.4100076919421554, 1.5298861380433664, 1.4612722530728206, 7.835309653892182, 8.023101286962628, 1.5674914820119739, 1.4800229619722813, 1.5979695859132335, 1.6269284079317003, 8.89813007588964, 1.4486775650875643, 5.270478735095821, 7.744967365986668, 1.5621171781094745, 1.464288822025992, 8.455258563975804, 1.6920138499699533, 1.5054120689164847, 1.4382804959313944, 1.5191279030404985, 8.007179253152572, 1.6204417400294915, 1.440046028001234, 1.4887892259284854, 12.69772507098969, 1.4950107811018825, 1.5103925400180742, 4.801245086826384, 3.951403641141951, 1.4821460959501565, 1.366814156062901, 1.5207180159632117, 1.5290972280781716, 1.954230366856791, 12.746766753029078, 1.7050792860100046, 1.4808608629973605, 14.441290398011915, 1.4837391949258745, 1.535826510982588, 7.901042829966173, 9.146282617002726, 1.4636473978171125, 1.6197860719403252, 9.833450815174729, 3.559310813085176, 1.5249522379599512, 1.5150061291642487, 11.64627356082201, 5.560740241897292, 1.4091687520267442, 7.322594702942297, 7.263032285147347, 1.5052163981599733, 5.543712810962461, 15.513742491952144, 1.4248059660894796, 1.5611297860741615, 1.4601827278966084, 13.17440832790453, 1.5264933709986508, 1.5600484929746017, 11.914933681022376, 1.6058888878906146, 7.341572490986437, 5.496903002844192, 4.615807801950723, 1.6815815991722047, 4.623368648113683, 9.3943837370025, 1.5919439468998462, 10.104463055031374, 4.620230028987862, 1.5427911259466782, 8.485337057034485, 1.5939589131157845, 1.461819155025296, 6.107839887030423, 1.5471356848720461, 5.139938196982257, 11.58852217788808, 1.4360214379848912, 1.5390114680631086, 6.386476896004751, 8.311171577894129, 1.4477983069373295, 7.09156307997182, 2.9614952058764175, 1.6166118669789284, 1.5136608820175752, 7.787328423000872, 1.9098348130937666, 1.4945948680397123, 3.604609655099921, 8.212939065997489, 1.59696924907621, 10.209345137001947, 6.119421831914224, 1.5585051300004125, 8.793701850809157, 5.852664112113416, 1.5710942149162292, 7.968460127944127, 8.000403916928917, 1.5325094850268215, 8.706784020992927, 9.164426844916306, 1.4314256010111421, 1.5680351259652525, 13.811214971006848, 1.8988316919421777, 1.7906992320204154, 7.792909653973766, 1.5722028180025518, 6.268739135935903, 3.405476756975986, 1.550063027185388, 1.5777383509557694, 15.055914688040502, 1.4159451919840649, 4.771195495035499, 1.4999581220326945, 1.615647662896663, 6.984806337044574, 5.254899669089355, 1.4243231390137225, 7.777151288930327, 2.6855854149907827, 1.6083561261184514, 4.48448153201025, 6.96388291392941, 1.4102920820005238, 1.585681983968243, 8.040699512930587, 1.479267354006879, 7.332484332029708, 9.833024838007987, 1.8209269230719656, 13.162613885942847, 1.4271073141135275, 1.504712555091828, 8.363308369996957, 4.833901448058896, 1.443941831937991, 1.5608427380211651, 4.574654714902863, 1.5058941399911419, 4.501524406950921, 7.051531965844333, 1.470265806070529, 7.314850716036744, 1.588022938114591]
Total Epoch List: [106, 87]
Total Time List: [3.7601034099934623, 0.5777355550089851]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba37c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 2.43s
Epoch 2/1000, LR 0.000000
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 8.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.43s
Epoch 3/1000, LR 0.000030
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.46s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 7.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.48s
Epoch 5/1000, LR 0.000090
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.49s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 2.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 5.11s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 5.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.49s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.57s
Epoch 9/1000, LR 0.000210
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 4.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 5.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.52s
Epoch 11/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.54s
Epoch 12/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 11.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.51s
Epoch 13/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.40s
Epoch 14/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.54s
Epoch 15/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 8.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.45s
Epoch 16/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.50s
Epoch 17/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.59s
Val loss: 0.6909 score: 0.5102 time: 0.41s
Test loss: 0.6901 score: 0.5417 time: 0.40s
Epoch 18/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 1.73s
Val loss: 0.6904 score: 0.6122 time: 3.32s
Test loss: 0.6896 score: 0.5833 time: 3.54s
Epoch 19/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 4.06s
Val loss: 0.6898 score: 0.6327 time: 0.42s
Test loss: 0.6890 score: 0.6458 time: 0.43s
Epoch 20/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.64s
Val loss: 0.6892 score: 0.6327 time: 0.50s
Test loss: 0.6883 score: 0.7292 time: 0.43s
Epoch 21/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.80s
Val loss: 0.6885 score: 0.6531 time: 4.12s
Test loss: 0.6876 score: 0.6875 time: 2.90s
Epoch 22/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 3.20s
Val loss: 0.6878 score: 0.5918 time: 0.60s
Test loss: 0.6869 score: 0.7083 time: 0.43s
Epoch 23/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.67s
Val loss: 0.6871 score: 0.6327 time: 0.40s
Test loss: 0.6860 score: 0.7083 time: 0.47s
Epoch 24/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 2.35s
Val loss: 0.6862 score: 0.6122 time: 3.33s
Test loss: 0.6851 score: 0.6250 time: 4.68s
Epoch 25/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 4.43s
Val loss: 0.6853 score: 0.5714 time: 0.42s
Test loss: 0.6840 score: 0.6250 time: 0.42s
Epoch 26/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.60s
Val loss: 0.6842 score: 0.5714 time: 0.49s
Test loss: 0.6829 score: 0.6042 time: 0.47s
Epoch 27/1000, LR 0.000270
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 0.59s
Val loss: 0.6830 score: 0.5714 time: 0.42s
Test loss: 0.6815 score: 0.6042 time: 7.93s
Epoch 28/1000, LR 0.000270
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 6.58s
Val loss: 0.6817 score: 0.5714 time: 0.49s
Test loss: 0.6800 score: 0.6042 time: 0.43s
Epoch 29/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.59s
Val loss: 0.6802 score: 0.5510 time: 0.42s
Test loss: 0.6783 score: 0.5833 time: 0.46s
Epoch 30/1000, LR 0.000270
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 0.65s
Val loss: 0.6786 score: 0.5510 time: 0.42s
Test loss: 0.6764 score: 0.5625 time: 0.60s
Epoch 31/1000, LR 0.000270
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.64s
Val loss: 0.6768 score: 0.5510 time: 2.77s
Test loss: 0.6745 score: 0.5625 time: 4.59s
Epoch 32/1000, LR 0.000270
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 8.49s
Val loss: 0.6750 score: 0.5510 time: 0.43s
Test loss: 0.6723 score: 0.5625 time: 0.51s
Epoch 33/1000, LR 0.000270
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.58s
Val loss: 0.6730 score: 0.5510 time: 0.43s
Test loss: 0.6699 score: 0.5625 time: 0.61s
Epoch 34/1000, LR 0.000270
Train loss: 0.6653;  Loss pred: 0.6653; Loss self: 0.0000; time: 0.69s
Val loss: 0.6709 score: 0.5510 time: 1.29s
Test loss: 0.6674 score: 0.5625 time: 5.20s
Epoch 35/1000, LR 0.000270
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 11.59s
Val loss: 0.6686 score: 0.5510 time: 0.42s
Test loss: 0.6647 score: 0.5417 time: 0.45s
Epoch 36/1000, LR 0.000270
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.61s
Val loss: 0.6662 score: 0.5510 time: 0.40s
Test loss: 0.6617 score: 0.5417 time: 0.51s
Epoch 37/1000, LR 0.000270
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 0.54s
Val loss: 0.6636 score: 0.5510 time: 0.45s
Test loss: 0.6586 score: 0.5208 time: 4.73s
Epoch 38/1000, LR 0.000270
Train loss: 0.6507;  Loss pred: 0.6507; Loss self: 0.0000; time: 11.26s
Val loss: 0.6609 score: 0.5510 time: 0.57s
Test loss: 0.6553 score: 0.5208 time: 0.52s
Epoch 39/1000, LR 0.000269
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.58s
Val loss: 0.6580 score: 0.5510 time: 0.47s
Test loss: 0.6518 score: 0.5208 time: 0.47s
Epoch 40/1000, LR 0.000269
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.58s
Val loss: 0.6550 score: 0.5510 time: 0.46s
Test loss: 0.6481 score: 0.5208 time: 3.26s
Epoch 41/1000, LR 0.000269
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 5.52s
Val loss: 0.6518 score: 0.5510 time: 0.41s
Test loss: 0.6441 score: 0.5208 time: 0.45s
Epoch 42/1000, LR 0.000269
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.57s
Val loss: 0.6484 score: 0.5510 time: 0.49s
Test loss: 0.6398 score: 0.5208 time: 0.41s
Epoch 43/1000, LR 0.000269
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.60s
Val loss: 0.6448 score: 0.5510 time: 0.67s
Test loss: 0.6353 score: 0.5208 time: 4.94s
Epoch 44/1000, LR 0.000269
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 9.80s
Val loss: 0.6410 score: 0.5510 time: 1.84s
Test loss: 0.6306 score: 0.5417 time: 0.44s
Epoch 45/1000, LR 0.000269
Train loss: 0.6195;  Loss pred: 0.6195; Loss self: 0.0000; time: 0.61s
Val loss: 0.6370 score: 0.5510 time: 0.40s
Test loss: 0.6256 score: 0.5417 time: 0.44s
Epoch 46/1000, LR 0.000269
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.59s
Val loss: 0.6327 score: 0.5510 time: 0.52s
Test loss: 0.6203 score: 0.5625 time: 0.44s
Epoch 47/1000, LR 0.000269
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 12.86s
Val loss: 0.6283 score: 0.5510 time: 3.17s
Test loss: 0.6147 score: 0.5625 time: 3.11s
Epoch 48/1000, LR 0.000269
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.73s
Val loss: 0.6235 score: 0.5510 time: 0.39s
Test loss: 0.6088 score: 0.5625 time: 0.49s
Epoch 49/1000, LR 0.000269
Train loss: 0.5919;  Loss pred: 0.5919; Loss self: 0.0000; time: 0.57s
Val loss: 0.6184 score: 0.5510 time: 0.39s
Test loss: 0.6025 score: 0.5625 time: 0.41s
Epoch 50/1000, LR 0.000269
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.64s
Val loss: 0.6129 score: 0.5510 time: 3.07s
Test loss: 0.5958 score: 0.5833 time: 4.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 2.52s
Val loss: 0.6070 score: 0.5714 time: 0.45s
Test loss: 0.5886 score: 0.6042 time: 0.48s
Epoch 52/1000, LR 0.000269
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.64s
Val loss: 0.6008 score: 0.5714 time: 0.57s
Test loss: 0.5811 score: 0.6042 time: 0.44s
Epoch 53/1000, LR 0.000269
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.60s
Val loss: 0.5941 score: 0.5714 time: 4.30s
Test loss: 0.5731 score: 0.6042 time: 2.83s
Epoch 54/1000, LR 0.000269
Train loss: 0.5558;  Loss pred: 0.5558; Loss self: 0.0000; time: 0.62s
Val loss: 0.5869 score: 0.5714 time: 0.40s
Test loss: 0.5646 score: 0.6250 time: 0.52s
Epoch 55/1000, LR 0.000269
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.63s
Val loss: 0.5794 score: 0.5918 time: 0.39s
Test loss: 0.5556 score: 0.6458 time: 0.43s
Epoch 56/1000, LR 0.000269
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 4.54s
Val loss: 0.5713 score: 0.6735 time: 4.72s
Test loss: 0.5462 score: 0.7083 time: 4.03s
Epoch 57/1000, LR 0.000269
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.59s
Val loss: 0.5628 score: 0.6735 time: 0.43s
Test loss: 0.5363 score: 0.7500 time: 0.43s
Epoch 58/1000, LR 0.000269
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.57s
Val loss: 0.5539 score: 0.6939 time: 0.39s
Test loss: 0.5260 score: 0.7708 time: 0.51s
Epoch 59/1000, LR 0.000268
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 0.61s
Val loss: 0.5445 score: 0.6735 time: 5.26s
Test loss: 0.5154 score: 0.7917 time: 3.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 2.54s
Val loss: 0.5349 score: 0.7143 time: 0.41s
Test loss: 0.5045 score: 0.8125 time: 0.52s
Epoch 61/1000, LR 0.000268
Train loss: 0.4731;  Loss pred: 0.4731; Loss self: 0.0000; time: 0.58s
Val loss: 0.5251 score: 0.7551 time: 0.42s
Test loss: 0.4935 score: 0.8542 time: 0.42s
Epoch 62/1000, LR 0.000268
Train loss: 0.4699;  Loss pred: 0.4699; Loss self: 0.0000; time: 0.72s
Val loss: 0.5152 score: 0.7959 time: 0.43s
Test loss: 0.4825 score: 0.8542 time: 0.53s
Epoch 63/1000, LR 0.000268
Train loss: 0.4494;  Loss pred: 0.4494; Loss self: 0.0000; time: 8.45s
Val loss: 0.5051 score: 0.8367 time: 4.30s
Test loss: 0.4717 score: 0.8750 time: 3.19s
Epoch 64/1000, LR 0.000268
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 0.96s
Val loss: 0.4951 score: 0.8571 time: 0.42s
Test loss: 0.4611 score: 0.8958 time: 0.50s
Epoch 65/1000, LR 0.000268
Train loss: 0.4255;  Loss pred: 0.4255; Loss self: 0.0000; time: 0.62s
Val loss: 0.4851 score: 0.8571 time: 0.47s
Test loss: 0.4509 score: 0.9167 time: 0.42s
Epoch 66/1000, LR 0.000268
Train loss: 0.4157;  Loss pred: 0.4157; Loss self: 0.0000; time: 0.68s
Val loss: 0.4756 score: 0.8571 time: 1.46s
Test loss: 0.4412 score: 0.9167 time: 3.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4042;  Loss pred: 0.4042; Loss self: 0.0000; time: 5.09s
Val loss: 0.4665 score: 0.8571 time: 0.57s
Test loss: 0.4320 score: 0.9167 time: 0.43s
Epoch 68/1000, LR 0.000268
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.63s
Val loss: 0.4581 score: 0.8571 time: 0.49s
Test loss: 0.4233 score: 0.9167 time: 0.43s
Epoch 69/1000, LR 0.000268
Train loss: 0.3759;  Loss pred: 0.3759; Loss self: 0.0000; time: 0.63s
Val loss: 0.4501 score: 0.8571 time: 0.87s
Test loss: 0.4148 score: 0.9167 time: 4.60s
Epoch 70/1000, LR 0.000268
Train loss: 0.3647;  Loss pred: 0.3647; Loss self: 0.0000; time: 10.10s
Val loss: 0.4424 score: 0.8571 time: 1.94s
Test loss: 0.4060 score: 0.9375 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 0.68s
Val loss: 0.4350 score: 0.8571 time: 0.39s
Test loss: 0.3971 score: 0.9375 time: 0.42s
Epoch 72/1000, LR 0.000267
Train loss: 0.3525;  Loss pred: 0.3525; Loss self: 0.0000; time: 0.59s
Val loss: 0.4282 score: 0.8571 time: 0.42s
Test loss: 0.3890 score: 0.9375 time: 0.52s
Epoch 73/1000, LR 0.000267
Train loss: 0.3405;  Loss pred: 0.3405; Loss self: 0.0000; time: 0.58s
Val loss: 0.4219 score: 0.8571 time: 0.44s
Test loss: 0.3812 score: 0.9375 time: 3.17s
Epoch 74/1000, LR 0.000267
Train loss: 0.3269;  Loss pred: 0.3269; Loss self: 0.0000; time: 5.93s
Val loss: 0.4160 score: 0.8571 time: 0.49s
Test loss: 0.3739 score: 0.9375 time: 0.44s
Epoch 75/1000, LR 0.000267
Train loss: 0.3125;  Loss pred: 0.3125; Loss self: 0.0000; time: 0.57s
Val loss: 0.4106 score: 0.8776 time: 0.40s
Test loss: 0.3673 score: 0.9375 time: 0.43s
Epoch 76/1000, LR 0.000267
Train loss: 0.3174;  Loss pred: 0.3174; Loss self: 0.0000; time: 0.66s
Val loss: 0.4056 score: 0.8776 time: 0.56s
Test loss: 0.3621 score: 0.9375 time: 3.32s
Epoch 77/1000, LR 0.000267
Train loss: 0.2997;  Loss pred: 0.2997; Loss self: 0.0000; time: 3.97s
Val loss: 0.4009 score: 0.8776 time: 0.40s
Test loss: 0.3570 score: 0.9375 time: 0.43s
Epoch 78/1000, LR 0.000267
Train loss: 0.2954;  Loss pred: 0.2954; Loss self: 0.0000; time: 0.60s
Val loss: 0.3964 score: 0.8980 time: 0.42s
Test loss: 0.3525 score: 0.9375 time: 0.52s
Epoch 79/1000, LR 0.000267
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.58s
Val loss: 0.3922 score: 0.8980 time: 0.44s
Test loss: 0.3482 score: 0.9375 time: 0.47s
Epoch 80/1000, LR 0.000267
Train loss: 0.2754;  Loss pred: 0.2754; Loss self: 0.0000; time: 10.97s
Val loss: 0.3883 score: 0.8980 time: 1.14s
Test loss: 0.3443 score: 0.9375 time: 0.52s
Epoch 81/1000, LR 0.000267
Train loss: 0.2719;  Loss pred: 0.2719; Loss self: 0.0000; time: 0.64s
Val loss: 0.3845 score: 0.8980 time: 0.40s
Test loss: 0.3405 score: 0.9375 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.2729;  Loss pred: 0.2729; Loss self: 0.0000; time: 0.64s
Val loss: 0.3811 score: 0.8980 time: 0.43s
Test loss: 0.3372 score: 0.9375 time: 0.55s
Epoch 83/1000, LR 0.000266
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 11.24s
Val loss: 0.3777 score: 0.8980 time: 1.72s
Test loss: 0.3338 score: 0.9167 time: 0.43s
Epoch 84/1000, LR 0.000266
Train loss: 0.2578;  Loss pred: 0.2578; Loss self: 0.0000; time: 0.60s
Val loss: 0.3744 score: 0.8980 time: 0.42s
Test loss: 0.3307 score: 0.9167 time: 0.60s
Epoch 85/1000, LR 0.000266
Train loss: 0.2485;  Loss pred: 0.2485; Loss self: 0.0000; time: 0.60s
Val loss: 0.3713 score: 0.8980 time: 0.41s
Test loss: 0.3274 score: 0.9167 time: 0.43s
Epoch 86/1000, LR 0.000266
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 11.75s
Val loss: 0.3682 score: 0.8980 time: 0.45s
Test loss: 0.3241 score: 0.9167 time: 0.62s
Epoch 87/1000, LR 0.000266
Train loss: 0.2363;  Loss pred: 0.2363; Loss self: 0.0000; time: 0.59s
Val loss: 0.3650 score: 0.8980 time: 0.41s
Test loss: 0.3203 score: 0.9167 time: 0.42s
Epoch 88/1000, LR 0.000266
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.59s
Val loss: 0.3620 score: 0.8980 time: 0.40s
Test loss: 0.3166 score: 0.8958 time: 4.86s
Epoch 89/1000, LR 0.000266
Train loss: 0.2266;  Loss pred: 0.2266; Loss self: 0.0000; time: 9.77s
Val loss: 0.3589 score: 0.8980 time: 0.47s
Test loss: 0.3126 score: 0.8958 time: 0.47s
Epoch 90/1000, LR 0.000266
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 0.72s
Val loss: 0.3558 score: 0.8980 time: 0.53s
Test loss: 0.3085 score: 0.8958 time: 0.48s
Epoch 91/1000, LR 0.000266
Train loss: 0.2156;  Loss pred: 0.2156; Loss self: 0.0000; time: 0.60s
Val loss: 0.3528 score: 0.8980 time: 0.44s
Test loss: 0.3043 score: 0.8958 time: 2.61s
Epoch 92/1000, LR 0.000266
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 13.31s
Val loss: 0.3500 score: 0.8980 time: 3.95s
Test loss: 0.3014 score: 0.8958 time: 0.89s
Epoch 93/1000, LR 0.000265
Train loss: 0.2115;  Loss pred: 0.2115; Loss self: 0.0000; time: 0.66s
Val loss: 0.3473 score: 0.8980 time: 0.50s
Test loss: 0.2997 score: 0.8958 time: 0.42s
Epoch 94/1000, LR 0.000265
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.63s
Val loss: 0.3448 score: 0.8980 time: 0.50s
Test loss: 0.2980 score: 0.8958 time: 0.42s
Epoch 95/1000, LR 0.000265
Train loss: 0.1966;  Loss pred: 0.1966; Loss self: 0.0000; time: 7.37s
Val loss: 0.3426 score: 0.9184 time: 6.49s
Test loss: 0.2966 score: 0.8958 time: 5.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.1902;  Loss pred: 0.1902; Loss self: 0.0000; time: 2.10s
Val loss: 0.3407 score: 0.9184 time: 0.44s
Test loss: 0.2949 score: 0.8958 time: 0.52s
Epoch 97/1000, LR 0.000265
Train loss: 0.1853;  Loss pred: 0.1853; Loss self: 0.0000; time: 0.60s
Val loss: 0.3390 score: 0.9184 time: 0.41s
Test loss: 0.2927 score: 0.8958 time: 0.55s
Epoch 98/1000, LR 0.000265
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 0.59s
Val loss: 0.3380 score: 0.9184 time: 0.53s
Test loss: 0.2905 score: 0.8958 time: 0.50s
Epoch 99/1000, LR 0.000265
Train loss: 0.1731;  Loss pred: 0.1731; Loss self: 0.0000; time: 11.18s
Val loss: 0.3373 score: 0.8980 time: 0.45s
Test loss: 0.2878 score: 0.8958 time: 0.42s
Epoch 100/1000, LR 0.000265
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.87s
Val loss: 0.3369 score: 0.8980 time: 0.51s
Test loss: 0.2850 score: 0.8958 time: 0.41s
Epoch 101/1000, LR 0.000265
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 6.41s
Val loss: 0.3367 score: 0.8980 time: 3.20s
Test loss: 0.2822 score: 0.8958 time: 0.58s
Epoch 102/1000, LR 0.000264
Train loss: 0.1552;  Loss pred: 0.1552; Loss self: 0.0000; time: 0.68s
Val loss: 0.3365 score: 0.8980 time: 0.41s
Test loss: 0.2794 score: 0.8958 time: 0.51s
Epoch 103/1000, LR 0.000264
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 0.74s
Val loss: 0.3362 score: 0.8980 time: 0.41s
Test loss: 0.2767 score: 0.8958 time: 0.46s
Epoch 104/1000, LR 0.000264
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 11.07s
Val loss: 0.3361 score: 0.8980 time: 3.78s
Test loss: 0.2743 score: 0.8958 time: 0.51s
Epoch 105/1000, LR 0.000264
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.59s
Val loss: 0.3361 score: 0.8980 time: 0.40s
Test loss: 0.2720 score: 0.8958 time: 0.41s
Epoch 106/1000, LR 0.000264
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.57s
Val loss: 0.3368 score: 0.8980 time: 0.40s
Test loss: 0.2703 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 1 of 2
Epoch 107/1000, LR 0.000264
Train loss: 0.1235;  Loss pred: 0.1235; Loss self: 0.0000; time: 0.78s
Val loss: 0.3379 score: 0.8980 time: 4.64s
Test loss: 0.2691 score: 0.8958 time: 3.45s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 104,   Train_Loss: 0.1349,   Val_Loss: 0.3361,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3361,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2720


[0.4435802800580859, 4.551122078089975, 0.4576469419989735, 0.3754494460299611, 3.626675458974205, 0.40348730597179383, 0.4629972199909389, 2.5590932270279154, 0.4493953241035342, 0.3801690438995138, 0.4760875770589337, 0.37338174995966256, 0.3729933761060238, 0.36962133296765387, 6.506644036970101, 0.36563153797760606, 0.3603579089976847, 0.4029770550550893, 0.4601977460552007, 0.3652433300158009, 0.37709669198375195, 3.4448492290684953, 0.3672673850087449, 0.4580735800554976, 0.3595658600097522, 2.6463547020684928, 0.4906719740247354, 0.3978246410842985, 0.49129294499289244, 4.554239718010649, 0.3534883940592408, 0.4719445559894666, 0.3927007169695571, 4.523859304958023, 0.4518898680107668, 0.38163037598133087, 0.4062572290422395, 0.34865663608070463, 0.3519276090664789, 0.3673571450635791, 3.0235011499607936, 0.3783585879718885, 0.4702433709753677, 0.4254812920698896, 3.7896698450203985, 0.41332442907150835, 0.37521107599604875, 0.38887714594602585, 0.3770813839510083, 0.4122032399754971, 0.4777176899369806, 0.37352726398967206, 4.081755780032836, 0.37888861494138837, 0.38508199399802834, 0.3864831259706989, 4.011834317003377, 0.3714592409087345, 0.44997662398964167, 0.3781173990573734, 0.37739273300394416, 4.360086142085493, 0.3855460670311004, 0.38391760701779276, 0.3477050910005346, 5.038651328999549, 0.4596331869252026, 0.3624830699991435, 3.6667593780439347, 0.4831357180373743, 0.3559741899371147, 0.37138498097192496, 0.3717262849677354, 0.35259851603768766, 0.8736362350173295, 0.37558121606707573, 0.3563393659424037, 0.3832240530755371, 1.8378295360598713, 0.4374832910252735, 0.3817098040599376, 3.957931222044863, 0.46898721798788756, 0.3815993530442938, 0.3883537979563698, 2.0519517899956554, 0.42359994701109827, 0.3753528910456225, 0.3647870810236782, 2.907217377098277, 0.4856897610006854, 0.3483068080386147, 2.1446776599623263, 0.3701723190024495, 0.36315767897758633, 4.442414731020108, 0.36290659999940544, 0.37786815990693867, 0.4690808559535071, 0.3989610029384494, 0.3697837529471144, 0.3984077100176364, 0.4221927149919793, 0.3774319199146703, 0.41746534197591245, 3.7595237749628723, 4.225840575993061, 0.47957511502318084, 0.7010523810749874, 3.621445572003722, 0.5508374370401725, 0.4619803329696879, 4.5204147200565785, 0.5858709809836, 0.5744806979782879, 2.794109210022725, 0.579839578946121, 0.4861949310870841, 0.5567227349383757, 0.4539338960312307, 4.202859974000603, 0.46176943299360573, 0.4634208040079102, 0.47482201911043376, 3.7185363830067217, 0.5187543999636546, 0.46726293803658336, 2.6699049830203876, 0.4746380050200969, 0.471718191052787, 0.5565014029853046, 3.1078128500375897, 0.48804002604447305, 0.4722104601096362, 2.578281335067004, 0.475961270974949, 0.47803018102422357, 4.6924588709371164, 0.4518580069998279, 0.5419006400043145, 4.1897394669940695, 0.6006032929290086, 0.45508084003813565, 4.580973110976629, 0.48054605408106, 0.49017839203588665, 3.2409194270148873, 0.4929110480006784, 0.45226673199795187, 0.5520312449662015, 0.5035801320336759, 0.4974994409130886, 0.61854900396429, 0.6246068630134687, 0.564935531001538, 3.8391890710918233, 0.49069759994745255, 0.472272319952026, 0.6306147479917854, 0.4743257329100743, 0.4673180090030655, 3.6921879090368748, 0.5686922420281917, 0.4747353499988094, 4.054767710040323, 0.46192762395367026, 0.4952418969478458, 4.09319442207925, 0.4696924629388377, 0.4613591629313305, 3.4889669050462544, 0.4671358389314264, 0.4684681269573048, 0.488010025001131, 0.45859468705020845, 0.47256085800472647, 3.9214308250229806, 0.4699987900676206, 0.5986171810654923, 0.5955904220463708, 0.4781657459679991, 0.4771062929648906, 4.225311993970536, 0.4668075799709186, 0.4626039389986545, 0.4875067980028689, 0.481148503953591, 0.4713813300477341, 3.529947553994134, 0.4713771849637851, 0.5037895120913163, 0.6373581690713763, 0.5771085509331897, 2.434312078054063, 0.4346636519767344, 0.4638839060207829, 0.48446532106027007, 0.4938828080194071, 5.117517079110257, 0.48993213195353746, 0.5705777360126376, 5.206362974946387, 0.5200467030517757, 0.5518790640635416, 0.5190000549191609, 0.4071130690863356, 0.5422735990723595, 0.44950587896164507, 0.5051423109835014, 0.40659402194432914, 3.5474458740791306, 0.4303653760580346, 0.43124841700773686, 2.9097418970195577, 0.4383850280428305, 0.47186815994791687, 4.682230520993471, 0.4229272640077397, 0.4779839339898899, 7.936252164072357, 0.43664178589824587, 0.46836370695382357, 0.6072526180651039, 4.5937411359045655, 0.517364006023854, 0.6103119830368087, 5.2070746660465375, 0.45184029696974903, 0.5175840680021793, 4.739840839058161, 0.5237485599936917, 0.47264090401586145, 3.2631184939527884, 0.4548454750329256, 0.4187286819797009, 4.946104219998233, 0.44855895393993706, 0.44547487795352936, 0.4454670019913465, 3.11167059908621, 0.4965197949204594, 0.4183662279974669, 4.247119721956551, 0.48077574907802045, 0.44268264796119183, 2.835791828110814, 0.5232198649318889, 0.43422532407566905, 4.03581775596831, 0.4320626149419695, 0.5162212389986962, 3.0572900669649243, 0.5270146909169853, 0.42253106995485723, 0.5379715969320387, 3.1911760489456356, 0.506492331973277, 0.4232883859658614, 3.448602937045507, 0.4339938350021839, 0.4337190210353583, 4.604138242080808, 0.47204065101686865, 0.42516244493890554, 0.5247069700853899, 3.17273945605848, 0.44054308510385454, 0.4349423550302163, 3.3258482849923894, 0.43815762700978667, 0.5263108150102198, 0.4790498170768842, 0.5275879979599267, 0.4282766350079328, 0.5508432110073045, 0.43718652706593275, 0.6068968019681051, 0.4318053589668125, 0.6243575609987602, 0.427939033950679, 4.8689854020485654, 0.47264343791175634, 0.47968051594216377, 2.6170805520378053, 0.8920191589277238, 0.42440497607458383, 0.426528392941691, 5.099168102024123, 0.5283970789751038, 0.5550004249671474, 0.5049856329569593, 0.4292452191002667, 0.41512197197880596, 0.5794794060057029, 0.512759282020852, 0.46179724601097405, 0.5151355430716649, 0.4156300019240007, 0.5183179040905088, 3.454832580056973]
[0.009052658776695631, 0.09288004240999949, 0.009339733510183133, 0.007662233592448186, 0.07401378487702459, 0.008234434815750894, 0.009448922856957937, 0.052226392388324806, 0.009171333144970087, 0.0077585519163166075, 0.009716073001202728, 0.007620035713462502, 0.0076121097164494654, 0.0075432925095439566, 0.13278865381571636, 0.007461868121991961, 0.0073542430407690755, 0.008224021531736516, 0.009391790735820423, 0.007453945510526549, 0.007695850856811264, 0.07030304549119379, 0.007495252755280508, 0.00934844040929587, 0.007338078775709229, 0.054007238817724346, 0.010013713755606845, 0.008118870226210174, 0.01002638663250801, 0.09294366771450305, 0.007214048858351854, 0.009631521550805442, 0.008014300346317492, 0.09232365928485761, 0.009222242204301363, 0.007788375020027161, 0.008290963858004888, 0.0071154415526674415, 0.0071821961033975285, 0.007497084593134267, 0.061704105101240685, 0.007721603836160989, 0.009596803489293218, 0.008683291674895706, 0.07734020091878364, 0.008435192430030783, 0.007657368897878546, 0.007936268284612772, 0.007695538447979762, 0.008412311019908105, 0.009749340610958787, 0.007623005387544328, 0.08330113836801706, 0.007732420713089558, 0.007858816204041394, 0.007887410734095896, 0.08187416973476279, 0.007580800834872133, 0.009183196407951871, 0.007716681613415784, 0.007701892510284574, 0.08898134983847944, 0.007868287082267354, 0.007835053204444751, 0.007096022265317033, 0.10282961895917447, 0.009380269120922503, 0.007397613673451908, 0.07483182404171296, 0.00985991261300764, 0.007264779386471729, 0.007579285325957652, 0.007586250713627253, 0.007195888082401789, 0.01782931091872101, 0.007664922776879097, 0.007272231958008238, 0.0078208990423579, 0.03750672522571166, 0.008928230429087214, 0.007789996001223216, 0.08077410657234414, 0.009571167714038521, 0.007787741898863139, 0.007925587713395302, 0.04187656714276848, 0.008644896877777516, 0.0076602630825637254, 0.007444634306605677, 0.059330966879556676, 0.009912035938789497, 0.0071083022048696876, 0.04376893183596584, 0.007554537122498969, 0.007411381203624211, 0.09066152512285934, 0.007406257142845009, 0.007711595100141606, 0.009573078692928717, 0.00814206128445815, 0.007546607203002335, 0.008130769592196661, 0.008616177856979169, 0.007702692243156536, 0.008519700856651274, 0.07672497499924229, 0.08624164440802165, 0.009787247245371038, 0.014307191450509946, 0.07390705248987188, 0.011241580347758623, 0.009428170060605876, 0.09225336163380772, 0.011956550632318368, 0.011724095877107918, 0.05702263693923929, 0.011833460794818796, 0.009922345532389472, 0.011361688468130115, 0.00926395706186185, 0.08577265253062455, 0.009423865979461342, 0.009457567428732862, 0.009690245287968037, 0.07588849761238207, 0.010586824489054176, 0.009535978327277211, 0.054487856796334444, 0.009686489898369325, 0.009626901858220143, 0.011357171489496012, 0.06342475204158346, 0.009960000531519858, 0.00963694816550278, 0.052617986429938854, 0.009713495326019367, 0.009755717980086195, 0.09576446675381871, 0.009221591979588325, 0.011059196734781928, 0.08550488708151162, 0.012257210059775685, 0.009287364082410932, 0.09348924716278835, 0.009807062328184898, 0.010003640653793605, 0.06614121279622219, 0.010059409142870988, 0.00922993330608065, 0.01126594377482044, 0.010277145551707672, 0.010153049814552829, 0.012623449060495714, 0.012747078837009564, 0.011529296551051796, 0.07835079736922089, 0.01001423673362148, 0.009638210611265838, 0.012869688734526232, 0.00968011699816478, 0.009537102224552358, 0.07535077365381378, 0.01160596412302432, 0.009688476530587947, 0.08275036142939436, 0.009427094366401434, 0.010106977488731548, 0.08353458004243368, 0.009585560468139544, 0.009415493121047561, 0.07120340622543377, 0.009533384467988295, 0.009560574019536833, 0.009959388265329204, 0.009359075245922622, 0.009644099142953602, 0.08002920051067307, 0.009591812042196338, 0.012216677164601885, 0.012154906572374915, 0.009758484611591818, 0.009736863121732461, 0.08623085701980684, 0.009526685305528951, 0.009440896714258254, 0.009949118326589162, 0.009819357223542673, 0.009620027143831308, 0.07203974599988029, 0.009619942550281329, 0.010281418614108495, 0.01300730957288523, 0.011777725529248769, 0.050714834959459644, 0.0090554927495153, 0.009664248042099643, 0.01009302752208896, 0.010289225167070981, 0.10661493914813036, 0.010206919415698698, 0.011887036166929951, 0.10846589531138306, 0.01083430631357866, 0.011497480501323784, 0.010812501144149186, 0.008481522272631992, 0.011297366647340823, 0.009364705811700938, 0.010523798145489613, 0.008470708790506857, 0.07390512237664855, 0.008965945334542388, 0.008984342020994518, 0.06061962285457412, 0.00913302141755897, 0.0098305866655816, 0.09754646918736398, 0.008810984666827911, 0.009957998624789374, 0.16533858675150745, 0.009096703872880122, 0.009757577228204658, 0.012651096209689664, 0.09570294033134512, 0.010778416792163625, 0.012714832979933513, 0.10848072220930287, 0.009413339520203104, 0.010783001416712068, 0.09874668414704502, 0.010911428333201911, 0.009846685500330446, 0.06798163529068309, 0.009475947396519283, 0.008723514207910435, 0.10304383791662985, 0.009344978207082022, 0.009280726624031862, 0.009280562541486384, 0.06482647081429604, 0.010344162394176237, 0.008715963083280561, 0.08848166087409481, 0.010016161439125426, 0.009222555165858163, 0.059078996418975294, 0.010900413852747684, 0.009046360918243105, 0.08407953658267313, 0.009001304477957698, 0.010754609145806171, 0.06369354306176926, 0.010979472727437193, 0.008802730624059526, 0.011207741602750806, 0.06648283435303408, 0.010551923582776604, 0.008818508040955445, 0.0718458945217814, 0.009041538229212165, 0.009035812938236631, 0.09591954671001683, 0.009834180229518097, 0.008857550936227199, 0.01093139521011229, 0.066098738667885, 0.009177980939663636, 0.009061299063129505, 0.06928850593734144, 0.009128283896037223, 0.010964808646046246, 0.009980204522435088, 0.01099141662416514, 0.008922429895998599, 0.011475900229318844, 0.009108052647206932, 0.012643683374335524, 0.00899594497847526, 0.01300744918747417, 0.008915396540639145, 0.10143719587601178, 0.009846738289828258, 0.009993344082128411, 0.05452251150078761, 0.018583732477660913, 0.008841770334887164, 0.00888600818628523, 0.10623266879216924, 0.011008272478647996, 0.011562508853482237, 0.010520534019936653, 0.008942608731255556, 0.008648374416225124, 0.01207248762511881, 0.010682485042101083, 0.009620775958561959, 0.010731990480659684, 0.00865895837341668, 0.010798289668552266, 0.07197567875118693]
[110.46478439840371, 10.766575617889035, 107.06943607220674, 130.51024716677773, 13.510996656386649, 121.44124306955369, 105.83216892956497, 19.147407168478857, 109.03540239931625, 128.89003138548986, 102.92224027919639, 131.23298073698996, 131.36962514334758, 132.5681058681969, 7.530763896347625, 134.01469761342392, 135.97592498050273, 121.59501238426941, 106.47596695122112, 134.1571384695244, 129.94014808836158, 14.224134858073265, 133.41778224830193, 106.96971432856581, 136.27545173134905, 18.516036403472185, 99.863050253467, 123.1698465596478, 99.73682809694914, 10.759205275519365, 138.6184124387069, 103.8257553310852, 124.77695578997013, 10.831459755235397, 108.43349999348186, 128.39648802588255, 120.61323835521301, 140.5394159446253, 139.2331796018423, 133.38518294375217, 16.206377166628627, 129.50677362090335, 104.20136258032804, 115.1637002924943, 12.929886244414057, 118.55094098859233, 130.5931597832576, 126.00380482837875, 129.94542315132227, 118.87339847914039, 102.57103940711085, 131.18185665117832, 12.004637866797072, 129.3256067025925, 127.24562759029155, 126.78431917805105, 12.213864314466592, 131.9121847127207, 108.89454560005758, 129.58938182203303, 129.8382181606234, 11.23831007076447, 127.0924649220903, 127.63155193798934, 140.9240223058013, 9.724824521590623, 106.60674945556946, 135.17872710611223, 13.36329847368918, 101.42077716599184, 137.65042911862847, 131.9385610903425, 131.81742045562646, 138.96825361216952, 56.08741720634794, 130.46445856133818, 137.5093651817297, 127.86253787243787, 26.66188514145401, 112.00427766090216, 128.3697706446802, 12.380205024048948, 104.48045942536864, 128.40692629348447, 126.17360833820139, 23.879703333626445, 115.67517972025668, 130.54381934690963, 134.3249324030185, 16.854604814211516, 100.88744695594036, 140.68056916811017, 22.847256216983553, 132.37078377996102, 134.92761639503766, 11.030037258306177, 135.02096682749854, 129.67485805649176, 104.45960302600076, 122.81902150611845, 132.50987802865384, 122.98958772115857, 116.06074254722962, 129.8247376932982, 117.37501314019806, 13.033565667631375, 11.595326212344187, 102.17377521273498, 69.89491987012988, 13.530508473965169, 88.95546436221333, 106.06512118171719, 10.839713396780263, 83.63616152780851, 85.29442359410989, 17.53689505915965, 84.50613200475075, 100.78262208625009, 88.01508708895079, 107.94523261736946, 11.658727700451571, 106.11356339101492, 105.73543435301539, 103.19656213879925, 13.177227530682313, 94.45703015421765, 104.86601014387246, 18.352713040959117, 103.2365707797151, 103.87557853268525, 88.05009248340373, 15.766715167358722, 100.40160106772643, 103.76729051834927, 19.004908166364473, 102.94955280632274, 102.50398812688563, 10.442286517092981, 108.44114576024025, 90.42248040085333, 11.695237946419391, 81.58463427837351, 107.6731773543659, 10.696417292341096, 101.96733400236084, 99.96360671160029, 15.119166367283748, 99.40941717324331, 108.3431447268642, 88.76309166703066, 97.30328280052801, 98.49257299680086, 79.21765241873845, 78.44934614326098, 86.73556062783135, 12.763111973035738, 99.85783506022298, 103.75369872402744, 77.70195694921853, 103.30453652467077, 104.85365223679743, 13.271263870419284, 86.16259617899084, 103.21540201319091, 12.084539363048423, 106.07722391790652, 98.94154816461382, 11.971090289698262, 104.32358163341588, 106.20792635540056, 14.044271938816339, 104.8945422643819, 104.59623009628093, 100.40777338516035, 106.84816327720631, 103.6903483857944, 12.495439084970432, 104.25558753661936, 81.85531847379286, 82.27130287226986, 102.47492718409673, 102.70248102471753, 11.596776775282477, 104.96830407735159, 105.92214174843531, 100.51141891915043, 101.83965989162938, 103.94981064489349, 13.88122606653363, 103.95072473387647, 97.26284256413484, 76.87984931830786, 84.90603703716842, 19.718096308493926, 110.4302137565651, 103.47416536121327, 99.07829913387866, 97.18904813166496, 9.379548569742221, 97.97275350895349, 84.12525931249596, 9.219487813466229, 92.29940256965946, 86.97557694356283, 92.48553934638109, 117.90336308221228, 88.51620304235945, 106.783920403621, 95.02272717275459, 118.053875387701, 13.530861838014697, 111.53313595916977, 111.30475639320167, 16.496308503914488, 109.49279042282978, 101.72332883256644, 10.25152430765314, 113.49469302390868, 100.42178530840593, 6.048194917154647, 109.9299278039913, 102.48445660357788, 79.04453364555754, 10.448999754216276, 92.77800434726585, 78.64830010572652, 9.218227714880054, 106.23222479692559, 92.73855778690216, 10.126922322889207, 91.64703001871382, 101.55701631441777, 14.709855032525974, 105.53034521564791, 114.63270147404647, 9.70460747792677, 107.00934532326222, 107.75018384988978, 107.75208889867994, 15.42579732382211, 96.67288291635869, 114.73201417273725, 11.301777002388691, 99.83864637941741, 108.42982037147289, 16.926489287465536, 91.7396360825262, 110.54168731908278, 11.89350037647659, 111.09500877887085, 92.98338846558235, 15.70017857273557, 91.07905496236138, 113.6011134166495, 89.22404133180248, 15.041476641772615, 94.76945053243674, 113.39786677698085, 13.918679788959015, 110.60064943032764, 110.67072844860745, 10.425403729473322, 101.68615753028587, 112.89802420554206, 91.47963098753684, 15.128881732895518, 108.95642588212316, 110.35945210869461, 14.432408181875271, 109.54961648751096, 91.20086198317613, 100.19834741382716, 90.98008329531005, 112.07709241273673, 87.139133315675, 109.7929534154218, 79.0908764790667, 111.16119567124029, 76.87902413356906, 112.16551001872878, 9.858316679241756, 101.55647185555915, 100.06660350946478, 18.341047990526892, 53.8105034175496, 113.09952216857279, 112.53647071172092, 9.413300177522354, 90.84077469372534, 86.48642026326611, 95.05220914689095, 111.82419247583456, 115.62866636808769, 82.83296956290387, 93.61117718011006, 103.94171990982237, 93.17935957939193, 115.48733194861366, 92.60725825056244, 13.893582073145913]
Elapsed: 1.2433612657454796~1.5100777722771077
Time per graph: 0.025580106050299516~0.03110640572675755
Speed: 86.16246522413486~42.36324794486564
Total Time: 3.4559
best val loss: 0.3361043334007263 test_score: 0.8958

Testing...
Test loss: 0.2966 score: 0.8958 time: 2.26s
test Score 0.8958
Epoch Time List: [1.8813385849352926, 8.49357967497781, 7.717399749089964, 1.6007732298457995, 13.049987877951935, 1.5104781170375645, 1.558591723907739, 3.755699184141122, 5.027086477144621, 1.4792417108546942, 1.6509268539957702, 11.538081565056928, 1.6111235129646957, 1.519467237056233, 9.024763936875388, 8.807763322023675, 1.5920812080148607, 1.457161070080474, 1.5501198739511892, 15.958313024952076, 1.5339648810913786, 4.604160696035251, 8.507032567053102, 1.4915480819763616, 1.4687214189907536, 5.790521082933992, 11.437618295894936, 1.5082647640956566, 1.6568819229723886, 5.638171296915971, 11.33021611708682, 1.5088091499637812, 1.5597970798844472, 9.277279934147373, 2.406080229091458, 1.4440430050017312, 1.5835587558103725, 6.637765489052981, 1.4555649599060416, 1.4083128590136766, 10.149847115040757, 1.4100076919421554, 1.5298861380433664, 1.4612722530728206, 7.835309653892182, 8.023101286962628, 1.5674914820119739, 1.4800229619722813, 1.5979695859132335, 1.6269284079317003, 8.89813007588964, 1.4486775650875643, 5.270478735095821, 7.744967365986668, 1.5621171781094745, 1.464288822025992, 8.455258563975804, 1.6920138499699533, 1.5054120689164847, 1.4382804959313944, 1.5191279030404985, 8.007179253152572, 1.6204417400294915, 1.440046028001234, 1.4887892259284854, 12.69772507098969, 1.4950107811018825, 1.5103925400180742, 4.801245086826384, 3.951403641141951, 1.4821460959501565, 1.366814156062901, 1.5207180159632117, 1.5290972280781716, 1.954230366856791, 12.746766753029078, 1.7050792860100046, 1.4808608629973605, 14.441290398011915, 1.4837391949258745, 1.535826510982588, 7.901042829966173, 9.146282617002726, 1.4636473978171125, 1.6197860719403252, 9.833450815174729, 3.559310813085176, 1.5249522379599512, 1.5150061291642487, 11.64627356082201, 5.560740241897292, 1.4091687520267442, 7.322594702942297, 7.263032285147347, 1.5052163981599733, 5.543712810962461, 15.513742491952144, 1.4248059660894796, 1.5611297860741615, 1.4601827278966084, 13.17440832790453, 1.5264933709986508, 1.5600484929746017, 11.914933681022376, 1.6058888878906146, 7.341572490986437, 5.496903002844192, 4.615807801950723, 1.6815815991722047, 4.623368648113683, 9.3943837370025, 1.5919439468998462, 10.104463055031374, 4.620230028987862, 1.5427911259466782, 8.485337057034485, 1.5939589131157845, 1.461819155025296, 6.107839887030423, 1.5471356848720461, 5.139938196982257, 11.58852217788808, 1.4360214379848912, 1.5390114680631086, 6.386476896004751, 8.311171577894129, 1.4477983069373295, 7.09156307997182, 2.9614952058764175, 1.6166118669789284, 1.5136608820175752, 7.787328423000872, 1.9098348130937666, 1.4945948680397123, 3.604609655099921, 8.212939065997489, 1.59696924907621, 10.209345137001947, 6.119421831914224, 1.5585051300004125, 8.793701850809157, 5.852664112113416, 1.5710942149162292, 7.968460127944127, 8.000403916928917, 1.5325094850268215, 8.706784020992927, 9.164426844916306, 1.4314256010111421, 1.5680351259652525, 13.811214971006848, 1.8988316919421777, 1.7906992320204154, 7.792909653973766, 1.5722028180025518, 6.268739135935903, 3.405476756975986, 1.550063027185388, 1.5777383509557694, 15.055914688040502, 1.4159451919840649, 4.771195495035499, 1.4999581220326945, 1.615647662896663, 6.984806337044574, 5.254899669089355, 1.4243231390137225, 7.777151288930327, 2.6855854149907827, 1.6083561261184514, 4.48448153201025, 6.96388291392941, 1.4102920820005238, 1.585681983968243, 8.040699512930587, 1.479267354006879, 7.332484332029708, 9.833024838007987, 1.8209269230719656, 13.162613885942847, 1.4271073141135275, 1.504712555091828, 8.363308369996957, 4.833901448058896, 1.443941831937991, 1.5608427380211651, 4.574654714902863, 1.5058941399911419, 4.501524406950921, 7.051531965844333, 1.470265806070529, 7.314850716036744, 1.588022938114591, 3.6926523610018194, 9.419769044034183, 1.470670104958117, 8.314501966000535, 1.5177139601437375, 8.575162591063417, 6.780808110139333, 1.6896593909477815, 10.439488196163438, 2.199178285780363, 1.5523748248815536, 12.958850330091082, 1.438566914992407, 1.5786303471541032, 9.108247632044367, 1.4919052720069885, 1.405434587970376, 8.598355456022546, 4.905129284015857, 1.562058581970632, 7.820911738090217, 4.231719763018191, 1.5390503720846027, 10.35863619716838, 5.268463808926754, 1.5698015978559852, 8.946530964924023, 7.503165606874973, 1.4741306218784302, 1.6712627270026132, 8.003862978890538, 9.428910108050331, 1.6102903771679848, 7.185976971988566, 12.459407004993409, 1.5246737150009722, 5.721450885874219, 12.345547135919333, 1.515417059068568, 4.293702029041015, 6.376886876882054, 1.480742808082141, 6.205855588894337, 12.09177112299949, 1.4554240619763732, 1.5498049159068614, 19.142538622021675, 1.6106491830432788, 1.369262073887512, 7.952555999043398, 3.4471845380030572, 1.6459460448240861, 7.732329730992205, 1.5449814470484853, 1.4497253210283816, 13.291730055003427, 1.4466120690340176, 1.475090165855363, 8.922928260988556, 3.4700530021218583, 1.4231535160215572, 1.6787395399296656, 15.941460522008128, 1.8807727618841454, 1.505132601945661, 5.585212451056577, 6.085884119034745, 1.5512945810332894, 6.098489524098113, 12.50298410304822, 1.485958603094332, 1.5237508750287816, 4.195392163004726, 6.857742108171806, 1.4017896549776196, 4.545720768976025, 4.8049877270823345, 1.5372241539880633, 1.5025064719375223, 12.634382840013131, 1.4720346059184521, 1.612440938130021, 13.391373949008994, 1.622783878003247, 1.4380536740645766, 12.82790499611292, 1.4208846361143515, 5.848134191939607, 10.708134508924559, 1.723620257806033, 3.6507155239814892, 18.149038372095674, 1.583780184853822, 1.547022480983287, 18.95667444798164, 3.063459082157351, 1.5588788000168279, 1.6165721649304032, 12.049339943099767, 1.7993519080337137, 10.184253686922602, 1.5945279541192576, 1.615606672829017, 15.35781059297733, 1.3941618038807064, 1.4793547409353778, 8.871454015024938]
Total Epoch List: [106, 87, 107]
Total Time List: [3.7601034099934623, 0.5777355550089851, 3.4558767169946805]
========================training times:7========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3190>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 3.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 4.93s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 6.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.40s
Epoch 3/1000, LR 0.000030
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.37s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.42s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 11.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.36s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.45s
Epoch 7/1000, LR 0.000150
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 3.11s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 7.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.39s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.35s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 5.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 4.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.39s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.54s
Epoch 13/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 3.66s
Val loss: 0.6911 score: 0.5918 time: 3.28s
Test loss: 0.6922 score: 0.5714 time: 3.42s
Epoch 14/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.64s
Val loss: 0.6909 score: 0.8571 time: 0.48s
Test loss: 0.6919 score: 0.6531 time: 0.50s
Epoch 15/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.62s
Val loss: 0.6905 score: 0.7755 time: 0.48s
Test loss: 0.6916 score: 0.6327 time: 0.40s
Epoch 16/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 9.29s
Val loss: 0.6901 score: 0.5918 time: 2.70s
Test loss: 0.6913 score: 0.5510 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.65s
Val loss: 0.6897 score: 0.5714 time: 0.49s
Test loss: 0.6910 score: 0.5510 time: 0.41s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.61s
Val loss: 0.6892 score: 0.5102 time: 0.56s
Test loss: 0.6906 score: 0.5306 time: 0.41s
Epoch 19/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 10.95s
Val loss: 0.6886 score: 0.5102 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5102 time: 0.37s
Epoch 20/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.59s
Val loss: 0.6880 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5102 time: 0.36s
Epoch 21/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.59s
Val loss: 0.6873 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5102 time: 0.43s
Epoch 22/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 9.23s
Val loss: 0.6864 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5102 time: 0.37s
Epoch 23/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.64s
Val loss: 0.6855 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5102 time: 0.37s
Epoch 24/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.61s
Val loss: 0.6845 score: 0.5102 time: 0.47s
Test loss: 0.6875 score: 0.5306 time: 0.46s
Epoch 25/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 10.44s
Val loss: 0.6833 score: 0.5102 time: 0.54s
Test loss: 0.6868 score: 0.5306 time: 0.37s
Epoch 26/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.62s
Val loss: 0.6819 score: 0.5306 time: 0.56s
Test loss: 0.6859 score: 0.5510 time: 0.42s
Epoch 27/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.63s
Val loss: 0.6804 score: 0.5510 time: 4.88s
Test loss: 0.6850 score: 0.5510 time: 2.64s
Epoch 28/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 2.39s
Val loss: 0.6787 score: 0.5714 time: 0.55s
Test loss: 0.6839 score: 0.5510 time: 0.37s
Epoch 29/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.67s
Val loss: 0.6768 score: 0.5714 time: 0.48s
Test loss: 0.6827 score: 0.5714 time: 0.35s
Epoch 30/1000, LR 0.000270
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.60s
Val loss: 0.6747 score: 0.5918 time: 0.56s
Test loss: 0.6814 score: 0.5714 time: 2.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 11.03s
Val loss: 0.6724 score: 0.5918 time: 0.51s
Test loss: 0.6800 score: 0.5918 time: 0.46s
Epoch 32/1000, LR 0.000270
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.57s
Val loss: 0.6699 score: 0.5918 time: 0.46s
Test loss: 0.6784 score: 0.5714 time: 0.45s
Epoch 33/1000, LR 0.000270
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.60s
Val loss: 0.6671 score: 0.5918 time: 0.48s
Test loss: 0.6766 score: 0.5918 time: 0.37s
Epoch 34/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 4.66s
Val loss: 0.6640 score: 0.6122 time: 3.91s
Test loss: 0.6746 score: 0.5918 time: 3.19s
Epoch 35/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 1.02s
Val loss: 0.6605 score: 0.6122 time: 0.47s
Test loss: 0.6725 score: 0.6122 time: 0.38s
Epoch 36/1000, LR 0.000270
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.61s
Val loss: 0.6568 score: 0.6735 time: 0.64s
Test loss: 0.6701 score: 0.6122 time: 0.38s
Epoch 37/1000, LR 0.000270
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.58s
Val loss: 0.6526 score: 0.6735 time: 0.50s
Test loss: 0.6675 score: 0.6531 time: 2.37s
Epoch 38/1000, LR 0.000270
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 10.58s
Val loss: 0.6481 score: 0.7347 time: 0.67s
Test loss: 0.6646 score: 0.6327 time: 0.38s
Epoch 39/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.61s
Val loss: 0.6432 score: 0.7347 time: 0.49s
Test loss: 0.6614 score: 0.6122 time: 0.35s
Epoch 40/1000, LR 0.000269
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.58s
Val loss: 0.6377 score: 0.7551 time: 0.46s
Test loss: 0.6579 score: 0.6531 time: 2.33s
Epoch 41/1000, LR 0.000269
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 9.49s
Val loss: 0.6318 score: 0.7959 time: 0.86s
Test loss: 0.6540 score: 0.6939 time: 0.35s
Epoch 42/1000, LR 0.000269
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.73s
Val loss: 0.6254 score: 0.8367 time: 0.55s
Test loss: 0.6499 score: 0.6939 time: 0.35s
Epoch 43/1000, LR 0.000269
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.71s
Val loss: 0.6184 score: 0.8571 time: 3.96s
Test loss: 0.6453 score: 0.6939 time: 2.90s
Epoch 44/1000, LR 0.000269
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 4.52s
Val loss: 0.6108 score: 0.8571 time: 0.56s
Test loss: 0.6404 score: 0.6939 time: 0.37s
Epoch 45/1000, LR 0.000269
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.76s
Val loss: 0.6026 score: 0.8776 time: 0.49s
Test loss: 0.6350 score: 0.7143 time: 0.47s
Epoch 46/1000, LR 0.000269
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 7.34s
Val loss: 0.5937 score: 0.8980 time: 1.98s
Test loss: 0.6292 score: 0.7347 time: 0.35s
Epoch 47/1000, LR 0.000269
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.59s
Val loss: 0.5841 score: 0.9184 time: 0.48s
Test loss: 0.6228 score: 0.7347 time: 0.37s
Epoch 48/1000, LR 0.000269
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.64s
Val loss: 0.5738 score: 0.9184 time: 3.26s
Test loss: 0.6160 score: 0.7347 time: 3.03s
Epoch 49/1000, LR 0.000269
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 5.12s
Val loss: 0.5628 score: 0.9388 time: 0.49s
Test loss: 0.6087 score: 0.7347 time: 0.50s
Epoch 50/1000, LR 0.000269
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.66s
Val loss: 0.5510 score: 0.9388 time: 0.57s
Test loss: 0.6008 score: 0.7551 time: 0.35s
Epoch 51/1000, LR 0.000269
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 11.46s
Val loss: 0.5386 score: 0.9592 time: 2.50s
Test loss: 0.5924 score: 0.7551 time: 0.44s
Epoch 52/1000, LR 0.000269
Train loss: 0.5522;  Loss pred: 0.5522; Loss self: 0.0000; time: 0.64s
Val loss: 0.5255 score: 0.9592 time: 0.58s
Test loss: 0.5834 score: 0.7551 time: 0.38s
Epoch 53/1000, LR 0.000269
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.63s
Val loss: 0.5119 score: 0.9592 time: 0.46s
Test loss: 0.5740 score: 0.7755 time: 0.36s
Epoch 54/1000, LR 0.000269
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.64s
Val loss: 0.4977 score: 0.9592 time: 0.54s
Test loss: 0.5642 score: 0.8163 time: 0.37s
Epoch 55/1000, LR 0.000269
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.61s
Val loss: 0.4831 score: 0.9796 time: 0.46s
Test loss: 0.5541 score: 0.8571 time: 0.37s
Epoch 56/1000, LR 0.000269
Train loss: 0.4985;  Loss pred: 0.4985; Loss self: 0.0000; time: 0.61s
Val loss: 0.4682 score: 0.9796 time: 0.47s
Test loss: 0.5436 score: 0.8571 time: 0.44s
Epoch 57/1000, LR 0.000269
Train loss: 0.4861;  Loss pred: 0.4861; Loss self: 0.0000; time: 0.57s
Val loss: 0.4534 score: 0.9796 time: 0.45s
Test loss: 0.5329 score: 0.8776 time: 0.49s
Epoch 58/1000, LR 0.000269
Train loss: 0.4695;  Loss pred: 0.4695; Loss self: 0.0000; time: 0.73s
Val loss: 0.4387 score: 0.9796 time: 0.64s
Test loss: 0.5220 score: 0.8776 time: 0.45s
Epoch 59/1000, LR 0.000268
Train loss: 0.4557;  Loss pred: 0.4557; Loss self: 0.0000; time: 0.59s
Val loss: 0.4242 score: 0.9796 time: 0.51s
Test loss: 0.5110 score: 0.8776 time: 0.42s
Epoch 60/1000, LR 0.000268
Train loss: 0.4415;  Loss pred: 0.4415; Loss self: 0.0000; time: 0.67s
Val loss: 0.4097 score: 0.9796 time: 0.55s
Test loss: 0.5000 score: 0.8776 time: 0.36s
Epoch 61/1000, LR 0.000268
Train loss: 0.4222;  Loss pred: 0.4222; Loss self: 0.0000; time: 0.55s
Val loss: 0.3954 score: 0.9796 time: 0.43s
Test loss: 0.4891 score: 0.8776 time: 0.36s
Epoch 62/1000, LR 0.000268
Train loss: 0.4087;  Loss pred: 0.4087; Loss self: 0.0000; time: 0.73s
Val loss: 0.3812 score: 0.9796 time: 0.54s
Test loss: 0.4782 score: 0.8776 time: 0.35s
Epoch 63/1000, LR 0.000268
Train loss: 0.3941;  Loss pred: 0.3941; Loss self: 0.0000; time: 0.56s
Val loss: 0.3673 score: 0.9796 time: 0.47s
Test loss: 0.4678 score: 0.8776 time: 0.37s
Epoch 64/1000, LR 0.000268
Train loss: 0.3806;  Loss pred: 0.3806; Loss self: 0.0000; time: 0.58s
Val loss: 0.3542 score: 0.9796 time: 0.46s
Test loss: 0.4579 score: 0.8776 time: 0.46s
Epoch 65/1000, LR 0.000268
Train loss: 0.3688;  Loss pred: 0.3688; Loss self: 0.0000; time: 0.72s
Val loss: 0.3420 score: 0.9796 time: 0.48s
Test loss: 0.4483 score: 0.8571 time: 0.40s
Epoch 66/1000, LR 0.000268
Train loss: 0.3481;  Loss pred: 0.3481; Loss self: 0.0000; time: 0.64s
Val loss: 0.3304 score: 0.9796 time: 0.54s
Test loss: 0.4391 score: 0.8571 time: 0.37s
Epoch 67/1000, LR 0.000268
Train loss: 0.3413;  Loss pred: 0.3413; Loss self: 0.0000; time: 0.56s
Val loss: 0.3194 score: 0.9796 time: 0.47s
Test loss: 0.4301 score: 0.8571 time: 0.40s
Epoch 68/1000, LR 0.000268
Train loss: 0.3221;  Loss pred: 0.3221; Loss self: 0.0000; time: 0.64s
Val loss: 0.3091 score: 0.9796 time: 0.55s
Test loss: 0.4211 score: 0.8776 time: 0.37s
Epoch 69/1000, LR 0.000268
Train loss: 0.3118;  Loss pred: 0.3118; Loss self: 0.0000; time: 0.59s
Val loss: 0.2991 score: 0.9796 time: 0.47s
Test loss: 0.4127 score: 0.8776 time: 0.37s
Epoch 70/1000, LR 0.000268
Train loss: 0.2950;  Loss pred: 0.2950; Loss self: 0.0000; time: 0.59s
Val loss: 0.2894 score: 0.9796 time: 0.53s
Test loss: 0.4046 score: 0.8776 time: 0.36s
Epoch 71/1000, LR 0.000268
Train loss: 0.2901;  Loss pred: 0.2901; Loss self: 0.0000; time: 0.63s
Val loss: 0.2802 score: 0.9796 time: 1.04s
Test loss: 0.3969 score: 0.8776 time: 2.96s
Epoch 72/1000, LR 0.000267
Train loss: 0.2770;  Loss pred: 0.2770; Loss self: 0.0000; time: 6.41s
Val loss: 0.2714 score: 0.9796 time: 0.60s
Test loss: 0.3894 score: 0.8776 time: 0.46s
Epoch 73/1000, LR 0.000267
Train loss: 0.2713;  Loss pred: 0.2713; Loss self: 0.0000; time: 0.63s
Val loss: 0.2630 score: 0.9796 time: 0.46s
Test loss: 0.3821 score: 0.8776 time: 0.37s
Epoch 74/1000, LR 0.000267
Train loss: 0.2505;  Loss pred: 0.2505; Loss self: 0.0000; time: 0.82s
Val loss: 0.2548 score: 0.9796 time: 0.54s
Test loss: 0.3755 score: 0.8776 time: 0.40s
Epoch 75/1000, LR 0.000267
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 11.35s
Val loss: 0.2469 score: 0.9796 time: 1.74s
Test loss: 0.3695 score: 0.8776 time: 2.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 4.06s
Val loss: 0.2393 score: 0.9796 time: 0.59s
Test loss: 0.3633 score: 0.8776 time: 0.37s
Epoch 77/1000, LR 0.000267
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.61s
Val loss: 0.2321 score: 0.9796 time: 0.47s
Test loss: 0.3575 score: 0.8776 time: 0.36s
Epoch 78/1000, LR 0.000267
Train loss: 0.2101;  Loss pred: 0.2101; Loss self: 0.0000; time: 0.63s
Val loss: 0.2251 score: 0.9592 time: 0.58s
Test loss: 0.3522 score: 0.8776 time: 5.41s
Epoch 79/1000, LR 0.000267
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 8.06s
Val loss: 0.2185 score: 0.9592 time: 0.47s
Test loss: 0.3472 score: 0.8776 time: 0.37s
Epoch 80/1000, LR 0.000267
Train loss: 0.1976;  Loss pred: 0.1976; Loss self: 0.0000; time: 0.70s
Val loss: 0.2121 score: 0.9592 time: 0.63s
Test loss: 0.3425 score: 0.8776 time: 0.46s
Epoch 81/1000, LR 0.000267
Train loss: 0.1787;  Loss pred: 0.1787; Loss self: 0.0000; time: 0.62s
Val loss: 0.2061 score: 0.9592 time: 0.51s
Test loss: 0.3378 score: 0.8980 time: 1.14s
Epoch 82/1000, LR 0.000267
Train loss: 0.1782;  Loss pred: 0.1782; Loss self: 0.0000; time: 9.09s
Val loss: 0.2004 score: 0.9592 time: 2.48s
Test loss: 0.3335 score: 0.8980 time: 0.63s
Epoch 83/1000, LR 0.000266
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.64s
Val loss: 0.1951 score: 0.9592 time: 0.49s
Test loss: 0.3293 score: 0.8980 time: 0.37s
Epoch 84/1000, LR 0.000266
Train loss: 0.1609;  Loss pred: 0.1609; Loss self: 0.0000; time: 0.64s
Val loss: 0.1904 score: 0.9592 time: 0.58s
Test loss: 0.3254 score: 0.8980 time: 3.69s
Epoch 85/1000, LR 0.000266
Train loss: 0.1470;  Loss pred: 0.1470; Loss self: 0.0000; time: 8.15s
Val loss: 0.1861 score: 0.9592 time: 2.10s
Test loss: 0.3221 score: 0.8980 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.1453;  Loss pred: 0.1453; Loss self: 0.0000; time: 0.61s
Val loss: 0.1821 score: 0.9592 time: 0.55s
Test loss: 0.3194 score: 0.8980 time: 0.40s
Epoch 87/1000, LR 0.000266
Train loss: 0.1371;  Loss pred: 0.1371; Loss self: 0.0000; time: 0.57s
Val loss: 0.1786 score: 0.9592 time: 0.46s
Test loss: 0.3174 score: 0.8980 time: 0.37s
Epoch 88/1000, LR 0.000266
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.58s
Val loss: 0.1755 score: 0.9592 time: 0.47s
Test loss: 0.3161 score: 0.8776 time: 0.47s
Epoch 89/1000, LR 0.000266
Train loss: 0.1193;  Loss pred: 0.1193; Loss self: 0.0000; time: 0.65s
Val loss: 0.1729 score: 0.9592 time: 2.48s
Test loss: 0.3159 score: 0.8776 time: 3.00s
Epoch 90/1000, LR 0.000266
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 8.03s
Val loss: 0.1707 score: 0.9592 time: 0.54s
Test loss: 0.3163 score: 0.8776 time: 0.37s
Epoch 91/1000, LR 0.000266
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.58s
Val loss: 0.1689 score: 0.9592 time: 0.47s
Test loss: 0.3175 score: 0.8776 time: 0.37s
Epoch 92/1000, LR 0.000266
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 0.61s
Val loss: 0.1674 score: 0.9592 time: 0.59s
Test loss: 0.3184 score: 0.8776 time: 0.41s
Epoch 93/1000, LR 0.000265
Train loss: 0.1027;  Loss pred: 0.1027; Loss self: 0.0000; time: 10.47s
Val loss: 0.1663 score: 0.9592 time: 0.49s
Test loss: 0.3188 score: 0.8776 time: 0.38s
Epoch 94/1000, LR 0.000265
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.72s
Val loss: 0.1655 score: 0.9592 time: 0.57s
Test loss: 0.3198 score: 0.8776 time: 0.37s
Epoch 95/1000, LR 0.000265
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 9.92s
Val loss: 0.1651 score: 0.9592 time: 0.50s
Test loss: 0.3211 score: 0.8776 time: 0.36s
Epoch 96/1000, LR 0.000265
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.65s
Val loss: 0.1650 score: 0.9592 time: 0.48s
Test loss: 0.3227 score: 0.8776 time: 0.45s
Epoch 97/1000, LR 0.000265
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 0.69s
Val loss: 0.1653 score: 0.9592 time: 4.50s
Test loss: 0.3247 score: 0.8776 time: 2.73s
     INFO: Early stopping counter 1 of 2
Epoch 98/1000, LR 0.000265
Train loss: 0.0717;  Loss pred: 0.0717; Loss self: 0.0000; time: 0.63s
Val loss: 0.1658 score: 0.9592 time: 0.68s
Test loss: 0.3270 score: 0.8776 time: 0.38s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 095,   Train_Loss: 0.0806,   Val_Loss: 0.1650,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1650,   Test_Precision: 0.9130,   Test_Recall: 0.8400,   Test_accuracy: 0.8750,   Test_Score: 0.8776,   Test_loss: 0.3227


[4.938047067029402, 0.4017314480151981, 0.3784795890096575, 0.4220256449189037, 0.36643017700407654, 0.45049568405374885, 3.111043203040026, 0.3916942370124161, 0.354084423976019, 5.082324043032713, 0.39170245290733874, 0.5429728500312194, 3.422456788015552, 0.5090833640424535, 0.4086882770061493, 0.5393458240432665, 0.4095404639374465, 0.41994707693811506, 0.3770470699528232, 0.3650650220224634, 0.43923347804229707, 0.3743851010221988, 0.37040561402682215, 0.46500753692816943, 0.37843320693355054, 0.4286267750430852, 2.6459110270952806, 0.37522926798556, 0.3528707689838484, 2.060861893929541, 0.4684408470056951, 0.45520198007579893, 0.37268904596567154, 3.1912024749908596, 0.38661649893037975, 0.38749880401883274, 2.3768540610326454, 0.38707919197622687, 0.35305902699474245, 2.3329953890061006, 0.35779799497686327, 0.35021345200948417, 2.9063334519742057, 0.3709618670400232, 0.47810623492114246, 0.35294202005025, 0.37709338299464434, 3.037049644975923, 0.5001879420597106, 0.35366689902730286, 0.4477209460455924, 0.38517261401284486, 0.3662499310448766, 0.3697732560103759, 0.37588649801909924, 0.44919727300293744, 0.49347098998259753, 0.45683002995792776, 0.42563074396457523, 0.36486962996423244, 0.35962470597587526, 0.355716299964115, 0.37677501898724586, 0.46666197502054274, 0.4039790009846911, 0.3713947570649907, 0.40033689502160996, 0.36957437894307077, 0.3700328799895942, 0.35974500502925366, 2.961001966963522, 0.46833598997909576, 0.3788541309768334, 0.4082262869924307, 2.2205037449020892, 0.3749764480162412, 0.36018149100709707, 5.417494022054598, 0.3715697380248457, 0.46290857903659344, 1.139950628974475, 0.6360835010418668, 0.37394307903014123, 3.6941445430275053, 0.381391791976057, 0.4083539100829512, 0.37577746505849063, 0.4710084410617128, 3.0069744429783896, 0.37407005194108933, 0.37134358496405184, 0.4102795639773831, 0.38180868001654744, 0.3727364609949291, 0.36566075403243303, 0.45926622103434056, 2.7323762429878116, 0.3796613769372925]
[0.10077647075570208, 0.008198600979902002, 0.007724073245095051, 0.008612768263651096, 0.007478166877634215, 0.009193789470484671, 0.06349067761306176, 0.007993759939028901, 0.00722621273420447, 0.1037208988374023, 0.007993927610353852, 0.011081078572065703, 0.06984605689827657, 0.010389456409029663, 0.008340577081758149, 0.01100705763353605, 0.008357968651784621, 0.008570348508941124, 0.007694838162302515, 0.007450306571887008, 0.00896394853147545, 0.007640512265759159, 0.00755929824544535, 0.009489949733227948, 0.007723126672113276, 0.008747485204960923, 0.0539981842264343, 0.007657740162970612, 0.007201444264976498, 0.042058405998562066, 0.009560017285830513, 0.00928983632807753, 0.007605898897258603, 0.06512658112226244, 0.00789013263123224, 0.007908138857527199, 0.04850722573536011, 0.00789957534645361, 0.0072052862651988256, 0.04761215079604287, 0.007301999897487006, 0.007147213306316003, 0.05931292759131032, 0.007570650347755576, 0.00975727010043148, 0.007202898368372449, 0.007695783326421313, 0.06198060499950863, 0.010207917184892053, 0.007217691816883732, 0.009137162164195764, 0.007860665592098874, 0.007474488388670951, 0.0075463929798035905, 0.007671153020797943, 0.009167291285774233, 0.010070836530257093, 0.009323061835876077, 0.00868634171356276, 0.007446318978861887, 0.007339279713793372, 0.007259516325798266, 0.007689286101780528, 0.009523713775929444, 0.008244469407850839, 0.007579484838061035, 0.008170140714726734, 0.007542334264144301, 0.007551691428359065, 0.0073417347965153805, 0.060428611570684125, 0.009557877346512159, 0.0077317169587108854, 0.008331148714131239, 0.045316402957185496, 0.007652580571760025, 0.007350642673614226, 0.11056110249091015, 0.007583055878058076, 0.009447113857889662, 0.02326429855049949, 0.012981295939629935, 0.007631491408778393, 0.07539070495974501, 0.0077835059586950405, 0.008333753266999004, 0.007668927858336543, 0.009612417164524751, 0.06136682536690591, 0.0076340826926752925, 0.0075784405094704455, 0.008373052326069042, 0.007792013877888723, 0.00760686655091692, 0.007462464368008837, 0.00937278002110899, 0.05576278046913901, 0.0077481913660671945]
[9.922951185938595, 121.97202942933723, 129.46536992447878, 116.10668827818704, 133.72261095039363, 108.76907756158167, 15.750343792114023, 125.09757706352666, 138.38507621933297, 9.641258523681389, 125.09495316229601, 90.24392287235473, 14.317200489304568, 96.25142650686537, 119.89578061536305, 90.85080075834462, 119.64629704449497, 116.68136936983805, 129.95724912046384, 134.22266457777735, 111.5579810045386, 130.8812767020197, 132.2874118113441, 105.37463612674544, 129.48123764573324, 114.31857003117587, 18.51914123272426, 130.58682832247956, 138.8610344265802, 23.77645981243771, 104.60232132447723, 107.64452296943142, 131.47689885286675, 15.354713586495432, 126.74058177952647, 126.45200318506934, 20.615485318737456, 126.58908310165486, 138.786990994369, 21.003041939519182, 136.94878307847574, 139.91467123505302, 16.859730932359266, 132.08904837303228, 102.4876824877256, 138.8330015026934, 129.94128831132505, 16.134079362534905, 97.96317719740345, 138.548448087072, 109.4431708696743, 127.21569036153213, 133.78842109323438, 132.51363965225502, 130.35849986159985, 109.08347611379995, 99.29661721700805, 107.26089964907234, 115.123262815992, 134.2945424227371, 136.25315276110945, 137.75022399857178, 130.0510849464218, 105.00105563099069, 121.2934332739163, 131.93508811818108, 122.39691272360747, 132.5849485024717, 132.4206648916657, 136.20758958422627, 16.548452363997924, 104.62574102448782, 129.33737814514237, 120.03146676565822, 22.067064787661774, 130.6748737400107, 136.0425264024311, 9.044772324717146, 131.87295677110154, 105.85243440935774, 42.984317701619666, 77.03391130211823, 131.03598581657508, 13.264234636537113, 128.4768079200722, 119.99395325992191, 130.39632377203097, 104.03210585684575, 16.295449439026758, 130.99150746159384, 131.95326911260221, 119.43075966294329, 128.33652707391659, 131.46017395026624, 134.00398992683213, 106.6919310757151, 17.933108636027097, 129.0623776252415]
Elapsed: 0.9050682199292113~1.1490330087971508
Time per graph: 0.018470779998555335~0.02344965324075818
Speed: 103.41195462891628~42.57263480237508
Total Time: 0.3801
best val loss: 0.16499225795269012 test_score: 0.8776

Testing...
Test loss: 0.5541 score: 0.8571 time: 0.38s
test Score 0.8571
Epoch Time List: [8.667025815928355, 7.828540118061937, 1.541525087901391, 1.6023802760755643, 12.312216233927757, 1.56362211110536, 4.219232761999592, 8.588461442966945, 1.481844321009703, 6.4227633100235835, 5.3252551219193265, 1.7121444890508428, 10.352617898955941, 2.6312766250921413, 1.51083515281789, 12.51680204400327, 1.5417888090014458, 1.5906344579998404, 12.425437713158317, 1.5079472438665107, 1.481722931843251, 10.169427181011997, 1.4695215708343312, 1.5467675849795341, 11.34536928019952, 1.609004182042554, 8.155053514172323, 3.311926015885547, 1.5034517891472206, 3.214562300941907, 12.003778851008974, 1.483293769066222, 1.4445417299866676, 11.759089034982026, 1.8632634751265869, 1.6290349300252274, 3.4608568670228124, 11.624923562048934, 1.4420588420471177, 3.3629151819041, 10.699814243242145, 1.6236603941069916, 7.575301059056073, 5.445086161023937, 1.7229212310630828, 9.673697177087888, 1.440616738051176, 6.929385305964388, 6.1044525830075145, 1.5766736050136387, 14.39897893811576, 1.6066781389527023, 1.4530581639846787, 1.5459035550011322, 1.449287572875619, 1.5290959250414744, 1.5107941849855706, 1.8149585030041635, 1.5255228739697486, 1.5812528759706765, 1.3333771398756653, 1.6202378900488839, 1.399839312885888, 1.5041243018349633, 1.60298468102701, 1.544830510043539, 1.4223287729546428, 1.5560098929563537, 1.4325262360507622, 1.4793786760419607, 4.633811862091534, 7.47930092504248, 1.464562664856203, 1.7675832770764828, 15.304666578071192, 5.020688264048658, 1.4307615170255303, 6.614678013138473, 8.892258609994315, 1.7932006750488654, 2.2604337020311505, 12.206094717839733, 1.4995965940179303, 4.908534814952873, 10.624994999030605, 1.5630930450279266, 1.3997956699458882, 1.5141913659172133, 6.138256532140076, 8.938605118193664, 1.4217113559134305, 1.613686871016398, 11.33338535297662, 1.6569737539393827, 10.771903496934101, 1.5863762310473248, 7.916624281089753, 1.6899625360965729]
Total Epoch List: [98]
Total Time List: [0.38012989598792046]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb612a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7022 score: 0.4898 time: 0.48s
Epoch 2/1000, LR 0.000000
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7021 score: 0.4898 time: 0.47s
Epoch 3/1000, LR 0.000030
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7020 score: 0.4898 time: 2.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 12.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5102 time: 1.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4898 time: 1.01s
Epoch 5/1000, LR 0.000090
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.4898 time: 0.47s
Epoch 6/1000, LR 0.000120
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.4898 time: 0.48s
Epoch 7/1000, LR 0.000150
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 10.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5102 time: 3.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.4898 time: 0.47s
Epoch 8/1000, LR 0.000180
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.4898 time: 0.47s
Epoch 9/1000, LR 0.000210
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.4898 time: 0.63s
Epoch 10/1000, LR 0.000240
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 11.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5102 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6996 score: 0.4898 time: 4.01s
Epoch 11/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.4898 time: 0.47s
Epoch 12/1000, LR 0.000270
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.4898 time: 0.46s
Epoch 13/1000, LR 0.000270
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4898 time: 0.67s
Epoch 14/1000, LR 0.000270
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 8.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5102 time: 1.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4898 time: 2.93s
Epoch 15/1000, LR 0.000270
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 4.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4898 time: 0.46s
Epoch 16/1000, LR 0.000270
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4898 time: 0.48s
Epoch 17/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4898 time: 0.53s
Epoch 18/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 11.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 3.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4898 time: 3.88s
Epoch 19/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 3.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4898 time: 0.51s
Epoch 20/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 0.50s
Epoch 21/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 3.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 4.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 4.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.48s
Epoch 23/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.49s
Epoch 24/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.48s
Epoch 25/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 10.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.47s
Epoch 26/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4898 time: 0.51s
Epoch 27/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4898 time: 0.96s
Epoch 28/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 10.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4898 time: 0.47s
Epoch 29/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4898 time: 0.46s
Epoch 30/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5102 time: 3.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4898 time: 4.39s
Epoch 31/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 5.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4898 time: 0.55s
Epoch 32/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.4898 time: 0.55s
Epoch 33/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 3.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5102 time: 1.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.4898 time: 4.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 2.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4898 time: 0.47s
Epoch 35/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.4898 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5102 time: 5.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6781 score: 0.4898 time: 2.87s
Epoch 37/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6798 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.4898 time: 0.55s
Epoch 38/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.4898 time: 3.39s
Epoch 39/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 4.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6757 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6702 score: 0.4898 time: 0.47s
Epoch 40/1000, LR 0.000269
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6733 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.4898 time: 0.47s
Epoch 41/1000, LR 0.000269
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6707 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6634 score: 0.4898 time: 4.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 6.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6678 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6595 score: 0.4898 time: 0.56s
Epoch 43/1000, LR 0.000269
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.59s
Val loss: 0.6645 score: 0.5306 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6551 score: 0.4898 time: 0.50s
Epoch 44/1000, LR 0.000269
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.77s
Val loss: 0.6608 score: 0.5714 time: 4.49s
Test loss: 0.6501 score: 0.5306 time: 4.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.6465;  Loss pred: 0.6465; Loss self: 0.0000; time: 0.62s
Val loss: 0.6568 score: 0.6122 time: 0.38s
Test loss: 0.6447 score: 0.5714 time: 0.48s
Epoch 46/1000, LR 0.000269
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.65s
Val loss: 0.6523 score: 0.7143 time: 0.41s
Test loss: 0.6387 score: 0.6327 time: 0.46s
Epoch 47/1000, LR 0.000269
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 12.01s
Val loss: 0.6474 score: 0.7143 time: 0.38s
Test loss: 0.6321 score: 0.6939 time: 0.51s
Epoch 48/1000, LR 0.000269
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.73s
Val loss: 0.6421 score: 0.7143 time: 0.39s
Test loss: 0.6249 score: 0.7143 time: 0.48s
Epoch 49/1000, LR 0.000269
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 0.64s
Val loss: 0.6364 score: 0.7551 time: 0.38s
Test loss: 0.6173 score: 0.7959 time: 0.52s
Epoch 50/1000, LR 0.000269
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 9.29s
Val loss: 0.6302 score: 0.7755 time: 3.81s
Test loss: 0.6091 score: 0.8163 time: 2.60s
Epoch 51/1000, LR 0.000269
Train loss: 0.6002;  Loss pred: 0.6002; Loss self: 0.0000; time: 0.66s
Val loss: 0.6236 score: 0.7755 time: 0.38s
Test loss: 0.6003 score: 0.8367 time: 0.47s
Epoch 52/1000, LR 0.000269
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.80s
Val loss: 0.6166 score: 0.7755 time: 0.56s
Test loss: 0.5909 score: 0.8367 time: 5.00s
Epoch 53/1000, LR 0.000269
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 6.82s
Val loss: 0.6092 score: 0.7755 time: 0.39s
Test loss: 0.5810 score: 0.8571 time: 0.47s
Epoch 54/1000, LR 0.000269
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.68s
Val loss: 0.6013 score: 0.7755 time: 0.37s
Test loss: 0.5706 score: 0.8776 time: 0.49s
Epoch 55/1000, LR 0.000269
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.60s
Val loss: 0.5931 score: 0.7755 time: 0.38s
Test loss: 0.5597 score: 0.8980 time: 3.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 9.91s
Val loss: 0.5845 score: 0.7755 time: 0.38s
Test loss: 0.5483 score: 0.8980 time: 0.48s
Epoch 57/1000, LR 0.000269
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.62s
Val loss: 0.5756 score: 0.7959 time: 0.35s
Test loss: 0.5366 score: 0.9184 time: 0.55s
Epoch 58/1000, LR 0.000269
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.59s
Val loss: 0.5663 score: 0.7959 time: 1.85s
Test loss: 0.5246 score: 0.9184 time: 4.61s
Epoch 59/1000, LR 0.000268
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 2.89s
Val loss: 0.5566 score: 0.7959 time: 0.37s
Test loss: 0.5121 score: 0.9184 time: 0.51s
Epoch 60/1000, LR 0.000268
Train loss: 0.4914;  Loss pred: 0.4914; Loss self: 0.0000; time: 0.70s
Val loss: 0.5466 score: 0.8163 time: 0.38s
Test loss: 0.4995 score: 0.9184 time: 0.46s
Epoch 61/1000, LR 0.000268
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.55s
Val loss: 0.5363 score: 0.8367 time: 0.36s
Test loss: 0.4866 score: 0.9184 time: 0.61s
Epoch 62/1000, LR 0.000268
Train loss: 0.4670;  Loss pred: 0.4670; Loss self: 0.0000; time: 8.49s
Val loss: 0.5259 score: 0.8367 time: 4.41s
Test loss: 0.4737 score: 0.9184 time: 2.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.4547;  Loss pred: 0.4547; Loss self: 0.0000; time: 5.31s
Val loss: 0.5154 score: 0.8571 time: 0.36s
Test loss: 0.4607 score: 0.9184 time: 0.49s
Epoch 64/1000, LR 0.000268
Train loss: 0.4412;  Loss pred: 0.4412; Loss self: 0.0000; time: 0.68s
Val loss: 0.5049 score: 0.8571 time: 0.40s
Test loss: 0.4479 score: 0.9184 time: 0.47s
Epoch 65/1000, LR 0.000268
Train loss: 0.4231;  Loss pred: 0.4231; Loss self: 0.0000; time: 0.65s
Val loss: 0.4944 score: 0.8571 time: 0.44s
Test loss: 0.4353 score: 0.9184 time: 4.44s
Epoch 66/1000, LR 0.000268
Train loss: 0.4144;  Loss pred: 0.4144; Loss self: 0.0000; time: 6.84s
Val loss: 0.4841 score: 0.8367 time: 0.37s
Test loss: 0.4230 score: 0.9184 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.3992;  Loss pred: 0.3992; Loss self: 0.0000; time: 0.63s
Val loss: 0.4738 score: 0.8571 time: 0.41s
Test loss: 0.4109 score: 0.9592 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3851;  Loss pred: 0.3851; Loss self: 0.0000; time: 0.78s
Val loss: 0.4637 score: 0.8571 time: 2.41s
Test loss: 0.3991 score: 0.9388 time: 3.56s
Epoch 69/1000, LR 0.000268
Train loss: 0.3738;  Loss pred: 0.3738; Loss self: 0.0000; time: 0.62s
Val loss: 0.4535 score: 0.8571 time: 0.37s
Test loss: 0.3872 score: 0.9592 time: 0.46s
Epoch 70/1000, LR 0.000268
Train loss: 0.3577;  Loss pred: 0.3577; Loss self: 0.0000; time: 0.73s
Val loss: 0.4431 score: 0.8571 time: 0.35s
Test loss: 0.3752 score: 0.9592 time: 0.46s
Epoch 71/1000, LR 0.000268
Train loss: 0.3362;  Loss pred: 0.3362; Loss self: 0.0000; time: 0.64s
Val loss: 0.4328 score: 0.8571 time: 0.43s
Test loss: 0.3634 score: 0.9592 time: 3.50s
Epoch 72/1000, LR 0.000267
Train loss: 0.3242;  Loss pred: 0.3242; Loss self: 0.0000; time: 4.29s
Val loss: 0.4227 score: 0.8571 time: 0.36s
Test loss: 0.3519 score: 0.9592 time: 0.47s
Epoch 73/1000, LR 0.000267
Train loss: 0.3115;  Loss pred: 0.3115; Loss self: 0.0000; time: 0.65s
Val loss: 0.4128 score: 0.8571 time: 0.37s
Test loss: 0.3407 score: 0.9592 time: 0.48s
Epoch 74/1000, LR 0.000267
Train loss: 0.3066;  Loss pred: 0.3066; Loss self: 0.0000; time: 0.60s
Val loss: 0.4033 score: 0.8571 time: 0.48s
Test loss: 0.3300 score: 0.9592 time: 4.56s
Epoch 75/1000, LR 0.000267
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 11.12s
Val loss: 0.3940 score: 0.8571 time: 2.19s
Test loss: 0.3198 score: 0.9592 time: 0.48s
Epoch 76/1000, LR 0.000267
Train loss: 0.2705;  Loss pred: 0.2705; Loss self: 0.0000; time: 0.72s
Val loss: 0.3851 score: 0.8571 time: 0.46s
Test loss: 0.3103 score: 0.9592 time: 0.50s
Epoch 77/1000, LR 0.000267
Train loss: 0.2636;  Loss pred: 0.2636; Loss self: 0.0000; time: 0.74s
Val loss: 0.3763 score: 0.8571 time: 5.04s
Test loss: 0.3008 score: 0.9592 time: 3.02s
Epoch 78/1000, LR 0.000267
Train loss: 0.2416;  Loss pred: 0.2416; Loss self: 0.0000; time: 1.91s
Val loss: 0.3679 score: 0.8571 time: 0.42s
Test loss: 0.2918 score: 0.9388 time: 0.51s
Epoch 79/1000, LR 0.000267
Train loss: 0.2345;  Loss pred: 0.2345; Loss self: 0.0000; time: 0.62s
Val loss: 0.3601 score: 0.8571 time: 0.40s
Test loss: 0.2841 score: 0.9388 time: 0.56s
Epoch 80/1000, LR 0.000267
Train loss: 0.2095;  Loss pred: 0.2095; Loss self: 0.0000; time: 0.80s
Val loss: 0.3529 score: 0.8776 time: 4.75s
Test loss: 0.2773 score: 0.9388 time: 4.64s
Epoch 81/1000, LR 0.000267
Train loss: 0.1988;  Loss pred: 0.1988; Loss self: 0.0000; time: 2.77s
Val loss: 0.3463 score: 0.8776 time: 0.36s
Test loss: 0.2717 score: 0.9388 time: 0.57s
Epoch 82/1000, LR 0.000267
Train loss: 0.1920;  Loss pred: 0.1920; Loss self: 0.0000; time: 0.78s
Val loss: 0.3404 score: 0.8776 time: 0.37s
Test loss: 0.2665 score: 0.9388 time: 0.47s
Epoch 83/1000, LR 0.000266
Train loss: 0.1769;  Loss pred: 0.1769; Loss self: 0.0000; time: 0.60s
Val loss: 0.3357 score: 0.8776 time: 3.92s
Test loss: 0.2626 score: 0.9592 time: 4.42s
Epoch 84/1000, LR 0.000266
Train loss: 0.1668;  Loss pred: 0.1668; Loss self: 0.0000; time: 2.59s
Val loss: 0.3316 score: 0.8776 time: 0.36s
Test loss: 0.2590 score: 0.9592 time: 0.47s
Epoch 85/1000, LR 0.000266
Train loss: 0.1497;  Loss pred: 0.1497; Loss self: 0.0000; time: 0.57s
Val loss: 0.3292 score: 0.8776 time: 0.38s
Test loss: 0.2569 score: 0.9592 time: 0.58s
Epoch 86/1000, LR 0.000266
Train loss: 0.1438;  Loss pred: 0.1438; Loss self: 0.0000; time: 0.58s
Val loss: 0.3269 score: 0.8571 time: 0.38s
Test loss: 0.2546 score: 0.9592 time: 0.50s
Epoch 87/1000, LR 0.000266
Train loss: 0.1454;  Loss pred: 0.1454; Loss self: 0.0000; time: 0.62s
Val loss: 0.3244 score: 0.8571 time: 2.82s
Test loss: 0.2524 score: 0.9592 time: 4.40s
Epoch 88/1000, LR 0.000266
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 1.04s
Val loss: 0.3214 score: 0.8571 time: 0.38s
Test loss: 0.2497 score: 0.9592 time: 0.47s
Epoch 89/1000, LR 0.000266
Train loss: 0.1181;  Loss pred: 0.1181; Loss self: 0.0000; time: 0.57s
Val loss: 0.3181 score: 0.8776 time: 0.44s
Test loss: 0.2468 score: 0.9592 time: 0.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.79s
Val loss: 0.3158 score: 0.8776 time: 2.10s
Test loss: 0.2447 score: 0.9592 time: 4.28s
Epoch 91/1000, LR 0.000266
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 2.94s
Val loss: 0.3144 score: 0.8776 time: 0.40s
Test loss: 0.2434 score: 0.9592 time: 0.57s
Epoch 92/1000, LR 0.000266
Train loss: 0.0877;  Loss pred: 0.0877; Loss self: 0.0000; time: 0.78s
Val loss: 0.3146 score: 0.8776 time: 0.41s
Test loss: 0.2439 score: 0.9592 time: 0.51s
     INFO: Early stopping counter 1 of 2
Epoch 93/1000, LR 0.000265
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 0.62s
Val loss: 0.3157 score: 0.8776 time: 4.30s
Test loss: 0.2452 score: 0.9592 time: 4.69s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1230,   Val_Loss: 0.3144,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3144,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2434


[4.938047067029402, 0.4017314480151981, 0.3784795890096575, 0.4220256449189037, 0.36643017700407654, 0.45049568405374885, 3.111043203040026, 0.3916942370124161, 0.354084423976019, 5.082324043032713, 0.39170245290733874, 0.5429728500312194, 3.422456788015552, 0.5090833640424535, 0.4086882770061493, 0.5393458240432665, 0.4095404639374465, 0.41994707693811506, 0.3770470699528232, 0.3650650220224634, 0.43923347804229707, 0.3743851010221988, 0.37040561402682215, 0.46500753692816943, 0.37843320693355054, 0.4286267750430852, 2.6459110270952806, 0.37522926798556, 0.3528707689838484, 2.060861893929541, 0.4684408470056951, 0.45520198007579893, 0.37268904596567154, 3.1912024749908596, 0.38661649893037975, 0.38749880401883274, 2.3768540610326454, 0.38707919197622687, 0.35305902699474245, 2.3329953890061006, 0.35779799497686327, 0.35021345200948417, 2.9063334519742057, 0.3709618670400232, 0.47810623492114246, 0.35294202005025, 0.37709338299464434, 3.037049644975923, 0.5001879420597106, 0.35366689902730286, 0.4477209460455924, 0.38517261401284486, 0.3662499310448766, 0.3697732560103759, 0.37588649801909924, 0.44919727300293744, 0.49347098998259753, 0.45683002995792776, 0.42563074396457523, 0.36486962996423244, 0.35962470597587526, 0.355716299964115, 0.37677501898724586, 0.46666197502054274, 0.4039790009846911, 0.3713947570649907, 0.40033689502160996, 0.36957437894307077, 0.3700328799895942, 0.35974500502925366, 2.961001966963522, 0.46833598997909576, 0.3788541309768334, 0.4082262869924307, 2.2205037449020892, 0.3749764480162412, 0.36018149100709707, 5.417494022054598, 0.3715697380248457, 0.46290857903659344, 1.139950628974475, 0.6360835010418668, 0.37394307903014123, 3.6941445430275053, 0.381391791976057, 0.4083539100829512, 0.37577746505849063, 0.4710084410617128, 3.0069744429783896, 0.37407005194108933, 0.37134358496405184, 0.4102795639773831, 0.38180868001654744, 0.3727364609949291, 0.36566075403243303, 0.45926622103434056, 2.7323762429878116, 0.3796613769372925, 0.4862067779758945, 0.47160934703424573, 2.2434486250858754, 1.0192485300358385, 0.4705201550386846, 0.48360957799013704, 0.47554487199522555, 0.47748585289809853, 0.6330724350409582, 4.018560849945061, 0.4745268350234255, 0.46344176307320595, 0.6719027000945061, 2.932229476980865, 0.46716409898363054, 0.48060345998965204, 0.5383200300857425, 3.883510477025993, 0.5100902360863984, 0.504760800045915, 4.256885011098348, 0.481784095056355, 0.49675301497336477, 0.4894285110058263, 0.47592485696077347, 0.5158317020395771, 0.9611282320693135, 0.47479145391844213, 0.46703092206735164, 4.395618582959287, 0.5563451610505581, 0.5552310569910333, 4.214828134980053, 0.4711389630101621, 0.4700433020479977, 2.873931455076672, 0.5569107899209484, 3.4002639880636707, 0.47279610100667924, 0.47125837800558656, 4.079632769920863, 0.5603255219757557, 0.5021658859914169, 4.093322823988274, 0.4800087970215827, 0.46775698696728796, 0.5105640159454197, 0.48280361690558493, 0.5299824149115011, 2.6052337059518322, 0.47983314096927643, 5.008944759028964, 0.4782529219519347, 0.4932315479964018, 3.21678898204118, 0.4800564710749313, 0.5538470309693366, 4.618345053982921, 0.5169406239874661, 0.46230188198387623, 0.6141695169499144, 2.222887646057643, 0.4927082289941609, 0.47349554405082017, 4.44463826098945, 0.4883109260117635, 0.46464320505037904, 3.560225871973671, 0.46440580405760556, 0.4667680070269853, 3.508224464021623, 0.47346236009616405, 0.4803475959924981, 4.566378662944771, 0.47993923898320645, 0.5004839969333261, 3.0273226159624755, 0.5135488790692762, 0.5603646390372887, 4.648560479050502, 0.5716517880791798, 0.47302405210211873, 4.429388693999499, 0.472483090008609, 0.5796068670460954, 0.5001900289207697, 4.405992559040897, 0.4710135080385953, 0.46887652901932597, 4.282311675953679, 0.5793755679624155, 0.5165414059301838, 4.690635689068586]
[0.10077647075570208, 0.008198600979902002, 0.007724073245095051, 0.008612768263651096, 0.007478166877634215, 0.009193789470484671, 0.06349067761306176, 0.007993759939028901, 0.00722621273420447, 0.1037208988374023, 0.007993927610353852, 0.011081078572065703, 0.06984605689827657, 0.010389456409029663, 0.008340577081758149, 0.01100705763353605, 0.008357968651784621, 0.008570348508941124, 0.007694838162302515, 0.007450306571887008, 0.00896394853147545, 0.007640512265759159, 0.00755929824544535, 0.009489949733227948, 0.007723126672113276, 0.008747485204960923, 0.0539981842264343, 0.007657740162970612, 0.007201444264976498, 0.042058405998562066, 0.009560017285830513, 0.00928983632807753, 0.007605898897258603, 0.06512658112226244, 0.00789013263123224, 0.007908138857527199, 0.04850722573536011, 0.00789957534645361, 0.0072052862651988256, 0.04761215079604287, 0.007301999897487006, 0.007147213306316003, 0.05931292759131032, 0.007570650347755576, 0.00975727010043148, 0.007202898368372449, 0.007695783326421313, 0.06198060499950863, 0.010207917184892053, 0.007217691816883732, 0.009137162164195764, 0.007860665592098874, 0.007474488388670951, 0.0075463929798035905, 0.007671153020797943, 0.009167291285774233, 0.010070836530257093, 0.009323061835876077, 0.00868634171356276, 0.007446318978861887, 0.007339279713793372, 0.007259516325798266, 0.007689286101780528, 0.009523713775929444, 0.008244469407850839, 0.007579484838061035, 0.008170140714726734, 0.007542334264144301, 0.007551691428359065, 0.0073417347965153805, 0.060428611570684125, 0.009557877346512159, 0.0077317169587108854, 0.008331148714131239, 0.045316402957185496, 0.007652580571760025, 0.007350642673614226, 0.11056110249091015, 0.007583055878058076, 0.009447113857889662, 0.02326429855049949, 0.012981295939629935, 0.007631491408778393, 0.07539070495974501, 0.0077835059586950405, 0.008333753266999004, 0.007668927858336543, 0.009612417164524751, 0.06136682536690591, 0.0076340826926752925, 0.0075784405094704455, 0.008373052326069042, 0.007792013877888723, 0.00760686655091692, 0.007462464368008837, 0.00937278002110899, 0.05576278046913901, 0.0077481913660671945, 0.0099225873056305, 0.0096246805517193, 0.04578466581807909, 0.020800990408894663, 0.009602452143646625, 0.00986958322428851, 0.009704997387657665, 0.009744609242818338, 0.01291984561308078, 0.08201144591724614, 0.009684221122927052, 0.009457995164759305, 0.013712300001928697, 0.05984141789756867, 0.009533961203747563, 0.009808233877339837, 0.010986123062974336, 0.07925531585767333, 0.010410004818089763, 0.01030124081726357, 0.08687520430812955, 0.009832328470537856, 0.010137816632109485, 0.009988336959302577, 0.009712752182872928, 0.01052717759264443, 0.01961486187896558, 0.009689621508539635, 0.009531243307496972, 0.08970650169304667, 0.011353982878582818, 0.011331246061041494, 0.08601690071387863, 0.009615080877758411, 0.009592720449959137, 0.05865166234850351, 0.011365526324917316, 0.0693931426135443, 0.009648900020544474, 0.009617517918481358, 0.08325781163103803, 0.011435214734199096, 0.010248283387579936, 0.08353720048955661, 0.009796097898399648, 0.009546060958516081, 0.010419673794804483, 0.009853135038889488, 0.010815967651255125, 0.05316803481534352, 0.00979251308100564, 0.10222336242916252, 0.00976026371330479, 0.01006594995911024, 0.06564875473553429, 0.009797070838263904, 0.011303000632027278, 0.09425193987720247, 0.01054980865280543, 0.009434732285385229, 0.01253407177448805, 0.04536505400117639, 0.010055269979472672, 0.009663174368384085, 0.09070690328549898, 0.009965529102280888, 0.009482514388783246, 0.07265767085660553, 0.00947766947056338, 0.00952587769442827, 0.07159641763309435, 0.009662497144819674, 0.009803012163112206, 0.09319140128458717, 0.00979467834659605, 0.010213959121088289, 0.061782094203315825, 0.010480589368760737, 0.011436013041577322, 0.0948685812051123, 0.011666363022024078, 0.009653552083716708, 0.09039568763264284, 0.009642512040992021, 0.011828711572369295, 0.01020795977389326, 0.08991821549063055, 0.00961252057221623, 0.009568908755496448, 0.08739411583578936, 0.011823991182906439, 0.010541661345513955, 0.09572725896058339]
[9.922951185938595, 121.97202942933723, 129.46536992447878, 116.10668827818704, 133.72261095039363, 108.76907756158167, 15.750343792114023, 125.09757706352666, 138.38507621933297, 9.641258523681389, 125.09495316229601, 90.24392287235473, 14.317200489304568, 96.25142650686537, 119.89578061536305, 90.85080075834462, 119.64629704449497, 116.68136936983805, 129.95724912046384, 134.22266457777735, 111.5579810045386, 130.8812767020197, 132.2874118113441, 105.37463612674544, 129.48123764573324, 114.31857003117587, 18.51914123272426, 130.58682832247956, 138.8610344265802, 23.77645981243771, 104.60232132447723, 107.64452296943142, 131.47689885286675, 15.354713586495432, 126.74058177952647, 126.45200318506934, 20.615485318737456, 126.58908310165486, 138.786990994369, 21.003041939519182, 136.94878307847574, 139.91467123505302, 16.859730932359266, 132.08904837303228, 102.4876824877256, 138.8330015026934, 129.94128831132505, 16.134079362534905, 97.96317719740345, 138.548448087072, 109.4431708696743, 127.21569036153213, 133.78842109323438, 132.51363965225502, 130.35849986159985, 109.08347611379995, 99.29661721700805, 107.26089964907234, 115.123262815992, 134.2945424227371, 136.25315276110945, 137.75022399857178, 130.0510849464218, 105.00105563099069, 121.2934332739163, 131.93508811818108, 122.39691272360747, 132.5849485024717, 132.4206648916657, 136.20758958422627, 16.548452363997924, 104.62574102448782, 129.33737814514237, 120.03146676565822, 22.067064787661774, 130.6748737400107, 136.0425264024311, 9.044772324717146, 131.87295677110154, 105.85243440935774, 42.984317701619666, 77.03391130211823, 131.03598581657508, 13.264234636537113, 128.4768079200722, 119.99395325992191, 130.39632377203097, 104.03210585684575, 16.295449439026758, 130.99150746159384, 131.95326911260221, 119.43075966294329, 128.33652707391659, 131.46017395026624, 134.00398992683213, 106.6919310757151, 17.933108636027097, 129.0623776252415, 100.78016642217472, 103.89955226320373, 21.841373790373453, 48.07463396417857, 104.14006599987493, 101.32140104346595, 103.0396980087548, 102.6208414397928, 77.40030569618754, 12.193419940541624, 103.26075657571835, 105.73065248817441, 72.92722591099565, 16.7108339864492, 104.88819690255556, 101.95515446571073, 91.02392120203179, 12.617450188398715, 96.06143488639616, 97.07568415681799, 11.510764296486641, 101.70530846243149, 98.64056890047726, 100.11676659232606, 102.95742969365153, 94.99222286310834, 50.98175078522329, 103.20320552445544, 104.91810645662899, 11.147464020186087, 88.07482014847093, 88.25154750086563, 11.625622310275268, 104.00328532994438, 104.24571478096809, 17.049815128138732, 87.98536657362193, 14.410645812210527, 103.638756528806, 103.97693131180604, 12.010884989766003, 87.4491667401153, 97.57731731071337, 11.970714773055088, 102.0814624732738, 104.75524976696235, 95.97229430528101, 101.49054042729394, 92.45589782102911, 18.808293431816193, 102.11883218616084, 9.782499579711708, 102.45624804551552, 99.34482130968124, 15.232581395161194, 102.07132483868061, 88.47208210946039, 10.60986120076536, 94.78844905249325, 105.99134874754627, 79.78253340110977, 22.04339930850889, 99.4503381849965, 103.4856623587166, 11.024519234798602, 100.3459013301283, 105.45726154477427, 13.763171709337652, 105.51117055789848, 104.97720336939707, 13.96717926760857, 103.492915445375, 102.00946233270055, 10.73060374901122, 102.09625723416742, 97.90522833945423, 16.185919446322853, 95.41448145852159, 87.44306222495128, 10.54089760062852, 85.71651663094768, 103.58881283571948, 11.062474617858756, 103.70741522010276, 84.54006117926669, 97.96276848165964, 11.12121714764457, 104.03098672062902, 104.50512441406582, 11.442417952702522, 84.57381137476385, 94.86170796271631, 10.446345281982424]
Elapsed: 1.1673967671088132~1.3831322409469933
Time per graph: 0.02382442381854721~0.028227188590754966
Speed: 88.81673881889331~43.35385960780747
Total Time: 4.6983
best val loss: 0.31436464190483093 test_score: 0.9592

Testing...
Test loss: 0.2773 score: 0.9388 time: 5.19s
test Score 0.9388
Epoch Time List: [8.667025815928355, 7.828540118061937, 1.541525087901391, 1.6023802760755643, 12.312216233927757, 1.56362211110536, 4.219232761999592, 8.588461442966945, 1.481844321009703, 6.4227633100235835, 5.3252551219193265, 1.7121444890508428, 10.352617898955941, 2.6312766250921413, 1.51083515281789, 12.51680204400327, 1.5417888090014458, 1.5906344579998404, 12.425437713158317, 1.5079472438665107, 1.481722931843251, 10.169427181011997, 1.4695215708343312, 1.5467675849795341, 11.34536928019952, 1.609004182042554, 8.155053514172323, 3.311926015885547, 1.5034517891472206, 3.214562300941907, 12.003778851008974, 1.483293769066222, 1.4445417299866676, 11.759089034982026, 1.8632634751265869, 1.6290349300252274, 3.4608568670228124, 11.624923562048934, 1.4420588420471177, 3.3629151819041, 10.699814243242145, 1.6236603941069916, 7.575301059056073, 5.445086161023937, 1.7229212310630828, 9.673697177087888, 1.440616738051176, 6.929385305964388, 6.1044525830075145, 1.5766736050136387, 14.39897893811576, 1.6066781389527023, 1.4530581639846787, 1.5459035550011322, 1.449287572875619, 1.5290959250414744, 1.5107941849855706, 1.8149585030041635, 1.5255228739697486, 1.5812528759706765, 1.3333771398756653, 1.6202378900488839, 1.399839312885888, 1.5041243018349633, 1.60298468102701, 1.544830510043539, 1.4223287729546428, 1.5560098929563537, 1.4325262360507622, 1.4793786760419607, 4.633811862091534, 7.47930092504248, 1.464562664856203, 1.7675832770764828, 15.304666578071192, 5.020688264048658, 1.4307615170255303, 6.614678013138473, 8.892258609994315, 1.7932006750488654, 2.2604337020311505, 12.206094717839733, 1.4995965940179303, 4.908534814952873, 10.624994999030605, 1.5630930450279266, 1.3997956699458882, 1.5141913659172133, 6.138256532140076, 8.938605118193664, 1.4217113559134305, 1.613686871016398, 11.33338535297662, 1.6569737539393827, 10.771903496934101, 1.5863762310473248, 7.916624281089753, 1.6899625360965729, 1.5075779120670632, 1.6250720110256225, 3.2231244968716055, 15.121146508026868, 1.44605356908869, 1.6146361589198932, 14.759912437060848, 1.583059353986755, 1.6027469469700009, 18.024860262987204, 1.5464906889246777, 1.4451589260715991, 1.6114849841687828, 12.562888402142562, 4.99259044311475, 1.5062697309767827, 1.470780486939475, 19.209869703976437, 4.14343832700979, 1.6292097030673176, 8.64933189493604, 4.930525396135636, 1.4239059360697865, 1.562270064954646, 11.338859395124018, 1.5953018077416345, 1.895741076907143, 11.14143952797167, 1.4835860969033092, 8.94131684792228, 6.248305753921159, 1.6931995819322765, 9.62481481814757, 3.540892111021094, 1.4372547109378502, 8.725648688036017, 1.7742177940672264, 4.370485604857095, 5.45943105709739, 1.6018190879840404, 5.124267776031047, 7.157911850139499, 1.4822109040105715, 9.35747047199402, 1.4765958309872076, 1.5256464349804446, 12.890990790096112, 1.5980078999418765, 1.5444891740335152, 15.698989218915813, 1.5116312189493328, 6.364514529122971, 7.675388130010106, 1.539817083044909, 4.184690609923564, 10.768454452045262, 1.519982947036624, 7.051969639956951, 3.767107236897573, 1.5394100550329313, 1.5119657180039212, 15.109421149012633, 6.157783722039312, 1.550980325206183, 5.529798747971654, 7.688968570088036, 1.4934573752107099, 6.742668555001728, 1.4522260800004005, 1.5445809479570016, 4.569018309935927, 5.117817286984064, 1.5009536789730191, 5.63743257895112, 13.786327652051114, 1.6797005720436573, 8.801394759910181, 2.8344013040186837, 1.5764415001031011, 10.191710280952975, 3.704143450828269, 1.6236092980252579, 8.942131952033378, 3.4179766461020336, 1.5305328300455585, 1.4509463680442423, 7.838967549963854, 1.8888780809938908, 1.4663146090460941, 7.171383103122935, 3.922976473928429, 1.6945075718685985, 9.612309644930065]
Total Epoch List: [98, 93]
Total Time List: [0.38012989598792046, 4.698304936056957]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb61240>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 0.52s
Epoch 2/1000, LR 0.000000
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 10.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 1.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 1.65s
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.42s
Epoch 4/1000, LR 0.000060
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.52s
Epoch 5/1000, LR 0.000090
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4898 time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 4.33s
Epoch 6/1000, LR 0.000120
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 7.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.44s
Epoch 7/1000, LR 0.000150
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.43s
Epoch 8/1000, LR 0.000180
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 3.94s
Epoch 9/1000, LR 0.000210
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 9.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 2.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 2.76s
Epoch 10/1000, LR 0.000240
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.42s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.42s
Epoch 12/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 3.15s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 6.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.43s
Epoch 14/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.49s
Epoch 15/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 4.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 4.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 11.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 3.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.57s
Epoch 17/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.48s
Epoch 18/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.61s
Epoch 19/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 10.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.45s
Epoch 20/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.57s
Val loss: 0.6903 score: 0.5102 time: 0.49s
Test loss: 0.6892 score: 0.5208 time: 0.42s
Epoch 21/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.64s
Val loss: 0.6895 score: 0.5918 time: 0.43s
Test loss: 0.6885 score: 0.5625 time: 2.55s
Epoch 22/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 11.12s
Val loss: 0.6888 score: 0.6327 time: 0.52s
Test loss: 0.6877 score: 0.6667 time: 0.42s
Epoch 23/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.59s
Val loss: 0.6879 score: 0.6531 time: 0.40s
Test loss: 0.6868 score: 0.7083 time: 0.46s
Epoch 24/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.60s
Val loss: 0.6871 score: 0.6939 time: 0.50s
Test loss: 0.6859 score: 0.7083 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 9.29s
Val loss: 0.6861 score: 0.6531 time: 0.40s
Test loss: 0.6848 score: 0.6875 time: 0.49s
Epoch 26/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.61s
Val loss: 0.6851 score: 0.6531 time: 0.39s
Test loss: 0.6837 score: 0.7083 time: 0.59s
Epoch 27/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.74s
Val loss: 0.6840 score: 0.6531 time: 0.45s
Test loss: 0.6825 score: 0.7292 time: 2.37s
Epoch 28/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 9.56s
Val loss: 0.6828 score: 0.6122 time: 3.68s
Test loss: 0.6811 score: 0.6250 time: 0.43s
Epoch 29/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.77s
Val loss: 0.6815 score: 0.5510 time: 0.42s
Test loss: 0.6797 score: 0.6250 time: 0.42s
Epoch 30/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.73s
Val loss: 0.6801 score: 0.5714 time: 0.49s
Test loss: 0.6780 score: 0.6042 time: 0.47s
Epoch 31/1000, LR 0.000270
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 6.95s
Val loss: 0.6785 score: 0.5714 time: 1.46s
Test loss: 0.6762 score: 0.6042 time: 0.48s
Epoch 32/1000, LR 0.000270
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.57s
Val loss: 0.6768 score: 0.5714 time: 0.40s
Test loss: 0.6743 score: 0.6042 time: 0.51s
Epoch 33/1000, LR 0.000270
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.58s
Val loss: 0.6749 score: 0.5510 time: 0.43s
Test loss: 0.6722 score: 0.5833 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.60s
Val loss: 0.6729 score: 0.5510 time: 3.66s
Test loss: 0.6698 score: 0.5625 time: 3.97s
Epoch 35/1000, LR 0.000270
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 1.85s
Val loss: 0.6707 score: 0.5510 time: 0.41s
Test loss: 0.6672 score: 0.5625 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.59s
Val loss: 0.6684 score: 0.5510 time: 0.41s
Test loss: 0.6644 score: 0.5625 time: 0.53s
Epoch 37/1000, LR 0.000270
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.59s
Val loss: 0.6659 score: 0.5510 time: 0.43s
Test loss: 0.6614 score: 0.5625 time: 2.70s
Epoch 38/1000, LR 0.000270
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 8.07s
Val loss: 0.6631 score: 0.5510 time: 0.41s
Test loss: 0.6581 score: 0.5625 time: 0.51s
Epoch 39/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.63s
Val loss: 0.6602 score: 0.5510 time: 0.42s
Test loss: 0.6546 score: 0.5625 time: 0.45s
Epoch 40/1000, LR 0.000269
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.62s
Val loss: 0.6571 score: 0.5510 time: 4.84s
Test loss: 0.6508 score: 0.5625 time: 2.50s
Epoch 41/1000, LR 0.000269
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.57s
Val loss: 0.6538 score: 0.5510 time: 0.40s
Test loss: 0.6467 score: 0.5625 time: 0.42s
Epoch 42/1000, LR 0.000269
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.64s
Val loss: 0.6503 score: 0.5510 time: 0.41s
Test loss: 0.6424 score: 0.5625 time: 0.57s
Epoch 43/1000, LR 0.000269
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.71s
Val loss: 0.6467 score: 0.5510 time: 4.47s
Test loss: 0.6379 score: 0.5625 time: 4.14s
Epoch 44/1000, LR 0.000269
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 2.97s
Val loss: 0.6429 score: 0.5510 time: 0.50s
Test loss: 0.6332 score: 0.5625 time: 0.45s
Epoch 45/1000, LR 0.000269
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.61s
Val loss: 0.6389 score: 0.5510 time: 0.40s
Test loss: 0.6281 score: 0.5625 time: 0.42s
Epoch 46/1000, LR 0.000269
Train loss: 0.6145;  Loss pred: 0.6145; Loss self: 0.0000; time: 0.64s
Val loss: 0.6347 score: 0.5510 time: 0.55s
Test loss: 0.6228 score: 0.5625 time: 0.42s
Epoch 47/1000, LR 0.000269
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 4.36s
Val loss: 0.6302 score: 0.5510 time: 2.23s
Test loss: 0.6172 score: 0.5625 time: 3.73s
Epoch 48/1000, LR 0.000269
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 2.28s
Val loss: 0.6255 score: 0.5510 time: 0.49s
Test loss: 0.6113 score: 0.5625 time: 0.42s
Epoch 49/1000, LR 0.000269
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.62s
Val loss: 0.6206 score: 0.5510 time: 0.40s
Test loss: 0.6051 score: 0.6042 time: 0.45s
Epoch 50/1000, LR 0.000269
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.62s
Val loss: 0.6152 score: 0.5714 time: 4.12s
Test loss: 0.5984 score: 0.6042 time: 4.10s
Epoch 51/1000, LR 0.000269
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 3.91s
Val loss: 0.6095 score: 0.5714 time: 0.51s
Test loss: 0.5913 score: 0.6042 time: 0.42s
Epoch 52/1000, LR 0.000269
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.59s
Val loss: 0.6034 score: 0.5714 time: 0.50s
Test loss: 0.5838 score: 0.6042 time: 0.43s
Epoch 53/1000, LR 0.000269
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 3.98s
Val loss: 0.5969 score: 0.5714 time: 3.17s
Test loss: 0.5758 score: 0.6250 time: 0.42s
Epoch 54/1000, LR 0.000269
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.58s
Val loss: 0.5899 score: 0.5714 time: 0.51s
Test loss: 0.5674 score: 0.6250 time: 0.42s
Epoch 55/1000, LR 0.000269
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.67s
Val loss: 0.5823 score: 0.6531 time: 0.41s
Test loss: 0.5583 score: 0.6875 time: 0.56s
Epoch 56/1000, LR 0.000269
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 10.11s
Val loss: 0.5743 score: 0.6939 time: 3.80s
Test loss: 0.5488 score: 0.7500 time: 0.52s
Epoch 57/1000, LR 0.000269
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.58s
Val loss: 0.5657 score: 0.6735 time: 0.40s
Test loss: 0.5387 score: 0.7708 time: 0.44s
Epoch 58/1000, LR 0.000269
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.59s
Val loss: 0.5567 score: 0.6735 time: 0.42s
Test loss: 0.5281 score: 0.7917 time: 0.52s
Epoch 59/1000, LR 0.000268
Train loss: 0.5045;  Loss pred: 0.5045; Loss self: 0.0000; time: 0.59s
Val loss: 0.5472 score: 0.6939 time: 0.40s
Test loss: 0.5170 score: 0.7917 time: 0.48s
Epoch 60/1000, LR 0.000268
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 7.72s
Val loss: 0.5372 score: 0.7143 time: 0.43s
Test loss: 0.5056 score: 0.8125 time: 0.52s
Epoch 61/1000, LR 0.000268
Train loss: 0.4796;  Loss pred: 0.4796; Loss self: 0.0000; time: 0.60s
Val loss: 0.5269 score: 0.7959 time: 0.41s
Test loss: 0.4939 score: 0.8333 time: 0.42s
Epoch 62/1000, LR 0.000268
Train loss: 0.4625;  Loss pred: 0.4625; Loss self: 0.0000; time: 0.61s
Val loss: 0.5164 score: 0.7959 time: 0.42s
Test loss: 0.4821 score: 0.8333 time: 2.03s
Epoch 63/1000, LR 0.000268
Train loss: 0.4562;  Loss pred: 0.4562; Loss self: 0.0000; time: 9.04s
Val loss: 0.5057 score: 0.7959 time: 0.53s
Test loss: 0.4704 score: 0.8750 time: 0.43s
Epoch 64/1000, LR 0.000268
Train loss: 0.4445;  Loss pred: 0.4445; Loss self: 0.0000; time: 0.63s
Val loss: 0.4950 score: 0.8571 time: 0.40s
Test loss: 0.4590 score: 0.9167 time: 0.55s
Epoch 65/1000, LR 0.000268
Train loss: 0.4337;  Loss pred: 0.4337; Loss self: 0.0000; time: 0.65s
Val loss: 0.4844 score: 0.8571 time: 3.88s
Test loss: 0.4482 score: 0.9167 time: 5.15s
Epoch 66/1000, LR 0.000268
Train loss: 0.4136;  Loss pred: 0.4136; Loss self: 0.0000; time: 2.54s
Val loss: 0.4742 score: 0.8571 time: 0.43s
Test loss: 0.4374 score: 0.9167 time: 0.54s
Epoch 67/1000, LR 0.000268
Train loss: 0.4039;  Loss pred: 0.4039; Loss self: 0.0000; time: 0.60s
Val loss: 0.4643 score: 0.8571 time: 0.42s
Test loss: 0.4269 score: 0.9167 time: 0.44s
Epoch 68/1000, LR 0.000268
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.69s
Val loss: 0.4550 score: 0.8571 time: 4.15s
Test loss: 0.4167 score: 0.9167 time: 3.71s
Epoch 69/1000, LR 0.000268
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 7.65s
Val loss: 0.4460 score: 0.8571 time: 0.42s
Test loss: 0.4068 score: 0.9167 time: 0.49s
Epoch 70/1000, LR 0.000268
Train loss: 0.3676;  Loss pred: 0.3676; Loss self: 0.0000; time: 0.75s
Val loss: 0.4374 score: 0.8571 time: 0.51s
Test loss: 0.3973 score: 0.9375 time: 0.42s
Epoch 71/1000, LR 0.000268
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 9.28s
Val loss: 0.4293 score: 0.8571 time: 0.49s
Test loss: 0.3886 score: 0.9375 time: 0.53s
Epoch 72/1000, LR 0.000267
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.70s
Val loss: 0.4215 score: 0.8571 time: 0.55s
Test loss: 0.3802 score: 0.9375 time: 0.43s
Epoch 73/1000, LR 0.000267
Train loss: 0.3289;  Loss pred: 0.3289; Loss self: 0.0000; time: 0.57s
Val loss: 0.4143 score: 0.8776 time: 0.44s
Test loss: 0.3726 score: 0.9375 time: 0.42s
Epoch 74/1000, LR 0.000267
Train loss: 0.3145;  Loss pred: 0.3145; Loss self: 0.0000; time: 0.66s
Val loss: 0.4073 score: 0.8980 time: 3.64s
Test loss: 0.3653 score: 0.9375 time: 5.42s
Epoch 75/1000, LR 0.000267
Train loss: 0.3044;  Loss pred: 0.3044; Loss self: 0.0000; time: 6.92s
Val loss: 0.4006 score: 0.8980 time: 0.44s
Test loss: 0.3580 score: 0.9375 time: 0.41s
Epoch 76/1000, LR 0.000267
Train loss: 0.2945;  Loss pred: 0.2945; Loss self: 0.0000; time: 0.58s
Val loss: 0.3940 score: 0.8980 time: 0.48s
Test loss: 0.3505 score: 0.9375 time: 0.42s
Epoch 77/1000, LR 0.000267
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.66s
Val loss: 0.3876 score: 0.8980 time: 0.46s
Test loss: 0.3428 score: 0.9375 time: 1.12s
Epoch 78/1000, LR 0.000267
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 8.37s
Val loss: 0.3815 score: 0.8980 time: 0.57s
Test loss: 0.3353 score: 0.9375 time: 0.43s
Epoch 79/1000, LR 0.000267
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.76s
Val loss: 0.3758 score: 0.8980 time: 0.46s
Test loss: 0.3277 score: 0.9375 time: 0.42s
Epoch 80/1000, LR 0.000267
Train loss: 0.2553;  Loss pred: 0.2553; Loss self: 0.0000; time: 0.97s
Val loss: 0.3706 score: 0.8980 time: 3.31s
Test loss: 0.3202 score: 0.9375 time: 2.96s
Epoch 81/1000, LR 0.000267
Train loss: 0.2453;  Loss pred: 0.2453; Loss self: 0.0000; time: 7.51s
Val loss: 0.3660 score: 0.8980 time: 0.42s
Test loss: 0.3138 score: 0.9375 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.2442;  Loss pred: 0.2442; Loss self: 0.0000; time: 0.61s
Val loss: 0.3615 score: 0.8980 time: 0.39s
Test loss: 0.3081 score: 0.9375 time: 0.51s
Epoch 83/1000, LR 0.000266
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 0.57s
Val loss: 0.3572 score: 0.8980 time: 0.41s
Test loss: 0.3029 score: 0.9375 time: 2.35s
Epoch 84/1000, LR 0.000266
Train loss: 0.2274;  Loss pred: 0.2274; Loss self: 0.0000; time: 7.88s
Val loss: 0.3530 score: 0.8980 time: 0.42s
Test loss: 0.2981 score: 0.9375 time: 0.53s
Epoch 85/1000, LR 0.000266
Train loss: 0.2164;  Loss pred: 0.2164; Loss self: 0.0000; time: 0.63s
Val loss: 0.3489 score: 0.8980 time: 0.42s
Test loss: 0.2940 score: 0.9375 time: 0.48s
Epoch 86/1000, LR 0.000266
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.79s
Val loss: 0.3450 score: 0.8980 time: 2.04s
Test loss: 0.2902 score: 0.9375 time: 3.82s
Epoch 87/1000, LR 0.000266
Train loss: 0.2060;  Loss pred: 0.2060; Loss self: 0.0000; time: 5.23s
Val loss: 0.3413 score: 0.8980 time: 0.42s
Test loss: 0.2871 score: 0.8958 time: 0.43s
Epoch 88/1000, LR 0.000266
Train loss: 0.2029;  Loss pred: 0.2029; Loss self: 0.0000; time: 0.64s
Val loss: 0.3378 score: 0.9184 time: 0.43s
Test loss: 0.2843 score: 0.8958 time: 0.52s
Epoch 89/1000, LR 0.000266
Train loss: 0.1971;  Loss pred: 0.1971; Loss self: 0.0000; time: 0.59s
Val loss: 0.3346 score: 0.9184 time: 4.05s
Test loss: 0.2819 score: 0.8958 time: 3.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1872;  Loss pred: 0.1872; Loss self: 0.0000; time: 5.52s
Val loss: 0.3319 score: 0.9184 time: 0.42s
Test loss: 0.2800 score: 0.8958 time: 0.55s
Epoch 91/1000, LR 0.000266
Train loss: 0.1806;  Loss pred: 0.1806; Loss self: 0.0000; time: 0.74s
Val loss: 0.3295 score: 0.9184 time: 0.41s
Test loss: 0.2779 score: 0.8958 time: 0.51s
Epoch 92/1000, LR 0.000266
Train loss: 0.1721;  Loss pred: 0.1721; Loss self: 0.0000; time: 0.78s
Val loss: 0.3277 score: 0.9184 time: 2.64s
Test loss: 0.2757 score: 0.8958 time: 3.91s
Epoch 93/1000, LR 0.000265
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.65s
Val loss: 0.3264 score: 0.9184 time: 0.41s
Test loss: 0.2733 score: 0.8958 time: 0.42s
Epoch 94/1000, LR 0.000265
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.61s
Val loss: 0.3260 score: 0.8980 time: 0.50s
Test loss: 0.2711 score: 0.8958 time: 0.45s
Epoch 95/1000, LR 0.000265
Train loss: 0.1522;  Loss pred: 0.1522; Loss self: 0.0000; time: 0.58s
Val loss: 0.3265 score: 0.8980 time: 0.42s
Test loss: 0.2692 score: 0.8958 time: 3.62s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.1481;  Loss pred: 0.1481; Loss self: 0.0000; time: 10.36s
Val loss: 0.3287 score: 0.8980 time: 0.50s
Test loss: 0.2680 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.1572,   Val_Loss: 0.3260,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3260,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2711


[4.938047067029402, 0.4017314480151981, 0.3784795890096575, 0.4220256449189037, 0.36643017700407654, 0.45049568405374885, 3.111043203040026, 0.3916942370124161, 0.354084423976019, 5.082324043032713, 0.39170245290733874, 0.5429728500312194, 3.422456788015552, 0.5090833640424535, 0.4086882770061493, 0.5393458240432665, 0.4095404639374465, 0.41994707693811506, 0.3770470699528232, 0.3650650220224634, 0.43923347804229707, 0.3743851010221988, 0.37040561402682215, 0.46500753692816943, 0.37843320693355054, 0.4286267750430852, 2.6459110270952806, 0.37522926798556, 0.3528707689838484, 2.060861893929541, 0.4684408470056951, 0.45520198007579893, 0.37268904596567154, 3.1912024749908596, 0.38661649893037975, 0.38749880401883274, 2.3768540610326454, 0.38707919197622687, 0.35305902699474245, 2.3329953890061006, 0.35779799497686327, 0.35021345200948417, 2.9063334519742057, 0.3709618670400232, 0.47810623492114246, 0.35294202005025, 0.37709338299464434, 3.037049644975923, 0.5001879420597106, 0.35366689902730286, 0.4477209460455924, 0.38517261401284486, 0.3662499310448766, 0.3697732560103759, 0.37588649801909924, 0.44919727300293744, 0.49347098998259753, 0.45683002995792776, 0.42563074396457523, 0.36486962996423244, 0.35962470597587526, 0.355716299964115, 0.37677501898724586, 0.46666197502054274, 0.4039790009846911, 0.3713947570649907, 0.40033689502160996, 0.36957437894307077, 0.3700328799895942, 0.35974500502925366, 2.961001966963522, 0.46833598997909576, 0.3788541309768334, 0.4082262869924307, 2.2205037449020892, 0.3749764480162412, 0.36018149100709707, 5.417494022054598, 0.3715697380248457, 0.46290857903659344, 1.139950628974475, 0.6360835010418668, 0.37394307903014123, 3.6941445430275053, 0.381391791976057, 0.4083539100829512, 0.37577746505849063, 0.4710084410617128, 3.0069744429783896, 0.37407005194108933, 0.37134358496405184, 0.4102795639773831, 0.38180868001654744, 0.3727364609949291, 0.36566075403243303, 0.45926622103434056, 2.7323762429878116, 0.3796613769372925, 0.4862067779758945, 0.47160934703424573, 2.2434486250858754, 1.0192485300358385, 0.4705201550386846, 0.48360957799013704, 0.47554487199522555, 0.47748585289809853, 0.6330724350409582, 4.018560849945061, 0.4745268350234255, 0.46344176307320595, 0.6719027000945061, 2.932229476980865, 0.46716409898363054, 0.48060345998965204, 0.5383200300857425, 3.883510477025993, 0.5100902360863984, 0.504760800045915, 4.256885011098348, 0.481784095056355, 0.49675301497336477, 0.4894285110058263, 0.47592485696077347, 0.5158317020395771, 0.9611282320693135, 0.47479145391844213, 0.46703092206735164, 4.395618582959287, 0.5563451610505581, 0.5552310569910333, 4.214828134980053, 0.4711389630101621, 0.4700433020479977, 2.873931455076672, 0.5569107899209484, 3.4002639880636707, 0.47279610100667924, 0.47125837800558656, 4.079632769920863, 0.5603255219757557, 0.5021658859914169, 4.093322823988274, 0.4800087970215827, 0.46775698696728796, 0.5105640159454197, 0.48280361690558493, 0.5299824149115011, 2.6052337059518322, 0.47983314096927643, 5.008944759028964, 0.4782529219519347, 0.4932315479964018, 3.21678898204118, 0.4800564710749313, 0.5538470309693366, 4.618345053982921, 0.5169406239874661, 0.46230188198387623, 0.6141695169499144, 2.222887646057643, 0.4927082289941609, 0.47349554405082017, 4.44463826098945, 0.4883109260117635, 0.46464320505037904, 3.560225871973671, 0.46440580405760556, 0.4667680070269853, 3.508224464021623, 0.47346236009616405, 0.4803475959924981, 4.566378662944771, 0.47993923898320645, 0.5004839969333261, 3.0273226159624755, 0.5135488790692762, 0.5603646390372887, 4.648560479050502, 0.5716517880791798, 0.47302405210211873, 4.429388693999499, 0.472483090008609, 0.5796068670460954, 0.5001900289207697, 4.405992559040897, 0.4710135080385953, 0.46887652901932597, 4.282311675953679, 0.5793755679624155, 0.5165414059301838, 4.690635689068586, 0.5296930319163948, 1.655577655066736, 0.42870792699977756, 0.528519892017357, 4.330434917937964, 0.441907593049109, 0.43913457298185676, 3.9481493190396577, 2.7624375929590315, 0.42533080105204135, 0.4255834800424054, 3.1495825099991634, 0.43807631195522845, 0.4986573440255597, 4.255375453969464, 0.5742393720429391, 0.48722669505514205, 0.6176389359170571, 0.4514080190565437, 0.4238491510041058, 2.552941727102734, 0.422631342895329, 0.4657584991073236, 0.45580138999503106, 0.4919602379668504, 0.5964803240494803, 2.3798871119506657, 0.43203446199186146, 0.4285874899942428, 0.4761003879830241, 0.4820664660073817, 0.5147375479573384, 0.43321829091291875, 3.9729467360302806, 0.4354558470658958, 0.5293511949712411, 2.7053323220461607, 0.5149485799483955, 0.4529116820776835, 2.5011077959788963, 0.42735615698620677, 0.5759259629994631, 4.142411908949725, 0.45539957098662853, 0.42369832494296134, 0.422512149089016, 3.737242169911042, 0.42808606196194887, 0.4583741370588541, 4.109135129023343, 0.4228613270679489, 0.4320526869269088, 0.4249180780025199, 0.42450490104965866, 0.5683587860548869, 0.5296646320493892, 0.4459707720670849, 0.5273192879976705, 0.4817544730613008, 0.528329603956081, 0.4275075059849769, 2.03881667193491, 0.4363769320771098, 0.5502320629311725, 5.149625440943055, 0.5443408590508625, 0.4456749790115282, 3.719184348010458, 0.49483646999578923, 0.4220571939367801, 0.5305684409104288, 0.43357638409361243, 0.42652565496973693, 5.42787378304638, 0.4149997679051012, 0.4249963479815051, 1.1205912709701806, 0.42954410798847675, 0.4281712840311229, 2.9609339389717206, 0.42545187403447926, 0.510910693090409, 2.356250355951488, 0.5385399969527498, 0.48831581603735685, 3.8265429560560733, 0.43225518602412194, 0.5220251450082287, 3.4683628069469705, 0.5552453909767792, 0.5182979439850897, 3.919244643067941, 0.42513552808668464, 0.4568583950167522, 3.6250743550481275, 0.4426378010539338]
[0.10077647075570208, 0.008198600979902002, 0.007724073245095051, 0.008612768263651096, 0.007478166877634215, 0.009193789470484671, 0.06349067761306176, 0.007993759939028901, 0.00722621273420447, 0.1037208988374023, 0.007993927610353852, 0.011081078572065703, 0.06984605689827657, 0.010389456409029663, 0.008340577081758149, 0.01100705763353605, 0.008357968651784621, 0.008570348508941124, 0.007694838162302515, 0.007450306571887008, 0.00896394853147545, 0.007640512265759159, 0.00755929824544535, 0.009489949733227948, 0.007723126672113276, 0.008747485204960923, 0.0539981842264343, 0.007657740162970612, 0.007201444264976498, 0.042058405998562066, 0.009560017285830513, 0.00928983632807753, 0.007605898897258603, 0.06512658112226244, 0.00789013263123224, 0.007908138857527199, 0.04850722573536011, 0.00789957534645361, 0.0072052862651988256, 0.04761215079604287, 0.007301999897487006, 0.007147213306316003, 0.05931292759131032, 0.007570650347755576, 0.00975727010043148, 0.007202898368372449, 0.007695783326421313, 0.06198060499950863, 0.010207917184892053, 0.007217691816883732, 0.009137162164195764, 0.007860665592098874, 0.007474488388670951, 0.0075463929798035905, 0.007671153020797943, 0.009167291285774233, 0.010070836530257093, 0.009323061835876077, 0.00868634171356276, 0.007446318978861887, 0.007339279713793372, 0.007259516325798266, 0.007689286101780528, 0.009523713775929444, 0.008244469407850839, 0.007579484838061035, 0.008170140714726734, 0.007542334264144301, 0.007551691428359065, 0.0073417347965153805, 0.060428611570684125, 0.009557877346512159, 0.0077317169587108854, 0.008331148714131239, 0.045316402957185496, 0.007652580571760025, 0.007350642673614226, 0.11056110249091015, 0.007583055878058076, 0.009447113857889662, 0.02326429855049949, 0.012981295939629935, 0.007631491408778393, 0.07539070495974501, 0.0077835059586950405, 0.008333753266999004, 0.007668927858336543, 0.009612417164524751, 0.06136682536690591, 0.0076340826926752925, 0.0075784405094704455, 0.008373052326069042, 0.007792013877888723, 0.00760686655091692, 0.007462464368008837, 0.00937278002110899, 0.05576278046913901, 0.0077481913660671945, 0.0099225873056305, 0.0096246805517193, 0.04578466581807909, 0.020800990408894663, 0.009602452143646625, 0.00986958322428851, 0.009704997387657665, 0.009744609242818338, 0.01291984561308078, 0.08201144591724614, 0.009684221122927052, 0.009457995164759305, 0.013712300001928697, 0.05984141789756867, 0.009533961203747563, 0.009808233877339837, 0.010986123062974336, 0.07925531585767333, 0.010410004818089763, 0.01030124081726357, 0.08687520430812955, 0.009832328470537856, 0.010137816632109485, 0.009988336959302577, 0.009712752182872928, 0.01052717759264443, 0.01961486187896558, 0.009689621508539635, 0.009531243307496972, 0.08970650169304667, 0.011353982878582818, 0.011331246061041494, 0.08601690071387863, 0.009615080877758411, 0.009592720449959137, 0.05865166234850351, 0.011365526324917316, 0.0693931426135443, 0.009648900020544474, 0.009617517918481358, 0.08325781163103803, 0.011435214734199096, 0.010248283387579936, 0.08353720048955661, 0.009796097898399648, 0.009546060958516081, 0.010419673794804483, 0.009853135038889488, 0.010815967651255125, 0.05316803481534352, 0.00979251308100564, 0.10222336242916252, 0.00976026371330479, 0.01006594995911024, 0.06564875473553429, 0.009797070838263904, 0.011303000632027278, 0.09425193987720247, 0.01054980865280543, 0.009434732285385229, 0.01253407177448805, 0.04536505400117639, 0.010055269979472672, 0.009663174368384085, 0.09070690328549898, 0.009965529102280888, 0.009482514388783246, 0.07265767085660553, 0.00947766947056338, 0.00952587769442827, 0.07159641763309435, 0.009662497144819674, 0.009803012163112206, 0.09319140128458717, 0.00979467834659605, 0.010213959121088289, 0.061782094203315825, 0.010480589368760737, 0.011436013041577322, 0.0948685812051123, 0.011666363022024078, 0.009653552083716708, 0.09039568763264284, 0.009642512040992021, 0.011828711572369295, 0.01020795977389326, 0.08991821549063055, 0.00961252057221623, 0.009568908755496448, 0.08739411583578936, 0.011823991182906439, 0.010541661345513955, 0.09572725896058339, 0.011035271498258226, 0.03449120114722367, 0.0089314151458287, 0.011010831083694939, 0.09021739412370759, 0.009206408188523104, 0.009148636937122015, 0.0822531108133262, 0.05755078318664649, 0.008861058355250861, 0.008866322500883447, 0.06561630229164923, 0.009126589832400592, 0.010388694667199161, 0.08865365529103049, 0.011963320250894563, 0.010150556146982126, 0.012867477831605356, 0.00940433373034466, 0.00883019064591887, 0.05318628598130696, 0.008804819643652687, 0.009703302064735908, 0.009495862291563148, 0.010249171624309383, 0.012426673417697506, 0.0495809814989722, 0.009000717958163781, 0.008928906041546725, 0.00991875808297967, 0.010043051375153786, 0.010723698915777883, 0.009025381060685808, 0.08276972366729751, 0.009071996813872829, 0.011028149895234188, 0.05636109004262835, 0.010728095415591573, 0.009435660043285074, 0.05210641241622701, 0.008903253270545974, 0.011998457562488815, 0.08630024810311927, 0.009487491062221428, 0.008827048436311694, 0.0088023364393545, 0.0778592118731467, 0.008918459624207268, 0.009549461188726127, 0.08560698185465299, 0.008809610980582269, 0.0090010976443106, 0.00885245995838583, 0.008843852105201222, 0.011840808042810144, 0.011034679834362274, 0.009291057751397602, 0.010985818499951469, 0.010036551522110434, 0.01100686674908502, 0.008906406374687018, 0.042475347331977296, 0.009091186084939787, 0.011463167977732761, 0.1072838633529803, 0.011340434563559635, 0.009284895396073503, 0.07748300725021788, 0.010309093124912275, 0.008792858207016252, 0.011053509185633933, 0.009032841335283592, 0.008885951145202853, 0.11308070381346624, 0.008645828498022942, 0.008854090582948023, 0.02334565147854543, 0.008948835583093265, 0.008920235083981728, 0.06168612372857751, 0.008863580709051652, 0.010643972772716856, 0.049088549082322665, 0.011219583269848954, 0.010173246167444935, 0.07971964491783486, 0.00900531637550254, 0.010875523854338098, 0.07225755847806188, 0.011567612312016232, 0.0107978738330227, 0.08165093006391544, 0.008856990168472597, 0.00951788322951567, 0.075522382396836, 0.009221620855290288]
[9.922951185938595, 121.97202942933723, 129.46536992447878, 116.10668827818704, 133.72261095039363, 108.76907756158167, 15.750343792114023, 125.09757706352666, 138.38507621933297, 9.641258523681389, 125.09495316229601, 90.24392287235473, 14.317200489304568, 96.25142650686537, 119.89578061536305, 90.85080075834462, 119.64629704449497, 116.68136936983805, 129.95724912046384, 134.22266457777735, 111.5579810045386, 130.8812767020197, 132.2874118113441, 105.37463612674544, 129.48123764573324, 114.31857003117587, 18.51914123272426, 130.58682832247956, 138.8610344265802, 23.77645981243771, 104.60232132447723, 107.64452296943142, 131.47689885286675, 15.354713586495432, 126.74058177952647, 126.45200318506934, 20.615485318737456, 126.58908310165486, 138.786990994369, 21.003041939519182, 136.94878307847574, 139.91467123505302, 16.859730932359266, 132.08904837303228, 102.4876824877256, 138.8330015026934, 129.94128831132505, 16.134079362534905, 97.96317719740345, 138.548448087072, 109.4431708696743, 127.21569036153213, 133.78842109323438, 132.51363965225502, 130.35849986159985, 109.08347611379995, 99.29661721700805, 107.26089964907234, 115.123262815992, 134.2945424227371, 136.25315276110945, 137.75022399857178, 130.0510849464218, 105.00105563099069, 121.2934332739163, 131.93508811818108, 122.39691272360747, 132.5849485024717, 132.4206648916657, 136.20758958422627, 16.548452363997924, 104.62574102448782, 129.33737814514237, 120.03146676565822, 22.067064787661774, 130.6748737400107, 136.0425264024311, 9.044772324717146, 131.87295677110154, 105.85243440935774, 42.984317701619666, 77.03391130211823, 131.03598581657508, 13.264234636537113, 128.4768079200722, 119.99395325992191, 130.39632377203097, 104.03210585684575, 16.295449439026758, 130.99150746159384, 131.95326911260221, 119.43075966294329, 128.33652707391659, 131.46017395026624, 134.00398992683213, 106.6919310757151, 17.933108636027097, 129.0623776252415, 100.78016642217472, 103.89955226320373, 21.841373790373453, 48.07463396417857, 104.14006599987493, 101.32140104346595, 103.0396980087548, 102.6208414397928, 77.40030569618754, 12.193419940541624, 103.26075657571835, 105.73065248817441, 72.92722591099565, 16.7108339864492, 104.88819690255556, 101.95515446571073, 91.02392120203179, 12.617450188398715, 96.06143488639616, 97.07568415681799, 11.510764296486641, 101.70530846243149, 98.64056890047726, 100.11676659232606, 102.95742969365153, 94.99222286310834, 50.98175078522329, 103.20320552445544, 104.91810645662899, 11.147464020186087, 88.07482014847093, 88.25154750086563, 11.625622310275268, 104.00328532994438, 104.24571478096809, 17.049815128138732, 87.98536657362193, 14.410645812210527, 103.638756528806, 103.97693131180604, 12.010884989766003, 87.4491667401153, 97.57731731071337, 11.970714773055088, 102.0814624732738, 104.75524976696235, 95.97229430528101, 101.49054042729394, 92.45589782102911, 18.808293431816193, 102.11883218616084, 9.782499579711708, 102.45624804551552, 99.34482130968124, 15.232581395161194, 102.07132483868061, 88.47208210946039, 10.60986120076536, 94.78844905249325, 105.99134874754627, 79.78253340110977, 22.04339930850889, 99.4503381849965, 103.4856623587166, 11.024519234798602, 100.3459013301283, 105.45726154477427, 13.763171709337652, 105.51117055789848, 104.97720336939707, 13.96717926760857, 103.492915445375, 102.00946233270055, 10.73060374901122, 102.09625723416742, 97.90522833945423, 16.185919446322853, 95.41448145852159, 87.44306222495128, 10.54089760062852, 85.71651663094768, 103.58881283571948, 11.062474617858756, 103.70741522010276, 84.54006117926669, 97.96276848165964, 11.12121714764457, 104.03098672062902, 104.50512441406582, 11.442417952702522, 84.57381137476385, 94.86170796271631, 10.446345281982424, 90.61852263062462, 28.992901573115958, 111.9643397683778, 90.81966587252622, 11.084337003003915, 108.61999376115219, 109.30590063557388, 12.157594893517212, 17.37595814737809, 112.85333646487304, 112.78633276652855, 15.24011510973648, 109.5699509196599, 96.25848405742052, 11.279850748592592, 83.58883479067815, 98.5167694774351, 77.71530777723821, 106.33395503323455, 113.24783802512566, 18.801839262690077, 113.57416057021574, 103.05770070110836, 105.30902505699527, 97.5688608460962, 80.47205928626444, 20.169023883093757, 111.10224813710396, 111.99580277213593, 100.8190734801743, 99.57133172433736, 93.2514058678662, 110.7986458716917, 12.081712438954318, 110.22931560897459, 90.67704098147502, 17.742737041523796, 93.21319034380136, 105.98092718608002, 19.191495895207314, 112.3184941068937, 83.34404608191757, 11.587452202977566, 105.40194382706035, 113.28815143761025, 113.60620068202401, 12.843695382240256, 112.12698628872097, 104.71795007456303, 11.681290221139212, 113.51239029784098, 111.09756159928767, 112.96295094254697, 113.0728994678555, 84.45369575999587, 90.6233814673965, 107.6303717786681, 91.0264446845192, 99.63581592711489, 90.85237632072979, 112.27873038020243, 23.543068222238087, 109.99664847434735, 87.23591959417354, 9.321066269862479, 88.18004234276108, 107.70180571155284, 12.906055604820217, 97.00174281901343, 113.72866210921622, 90.46900700998049, 110.70713664523858, 112.53719311070684, 8.84324174042605, 115.66271528849687, 112.94214698072854, 42.83452963045372, 111.74638205324348, 112.10466883274445, 16.21110129078717, 112.82122122256771, 93.9498833145504, 20.371349707708333, 89.12987015189404, 98.29704143010582, 12.543959535076658, 111.04551559347021, 91.94959372932767, 13.839382634324823, 86.44826374075639, 92.61082463676695, 12.247257921216711, 112.90517218361683, 105.0653780768107, 13.241107712220357, 108.44080619800336]
Elapsed: 1.185825891923792~1.3782352752761247
Time per graph: 0.024374387778221974~0.028327268492718816
Speed: 85.93929369780739~42.19829978397326
Total Time: 0.4435
best val loss: 0.32599562406539917 test_score: 0.8958

Testing...
Test loss: 0.2843 score: 0.8958 time: 0.50s
test Score 0.8958
Epoch Time List: [8.667025815928355, 7.828540118061937, 1.541525087901391, 1.6023802760755643, 12.312216233927757, 1.56362211110536, 4.219232761999592, 8.588461442966945, 1.481844321009703, 6.4227633100235835, 5.3252551219193265, 1.7121444890508428, 10.352617898955941, 2.6312766250921413, 1.51083515281789, 12.51680204400327, 1.5417888090014458, 1.5906344579998404, 12.425437713158317, 1.5079472438665107, 1.481722931843251, 10.169427181011997, 1.4695215708343312, 1.5467675849795341, 11.34536928019952, 1.609004182042554, 8.155053514172323, 3.311926015885547, 1.5034517891472206, 3.214562300941907, 12.003778851008974, 1.483293769066222, 1.4445417299866676, 11.759089034982026, 1.8632634751265869, 1.6290349300252274, 3.4608568670228124, 11.624923562048934, 1.4420588420471177, 3.3629151819041, 10.699814243242145, 1.6236603941069916, 7.575301059056073, 5.445086161023937, 1.7229212310630828, 9.673697177087888, 1.440616738051176, 6.929385305964388, 6.1044525830075145, 1.5766736050136387, 14.39897893811576, 1.6066781389527023, 1.4530581639846787, 1.5459035550011322, 1.449287572875619, 1.5290959250414744, 1.5107941849855706, 1.8149585030041635, 1.5255228739697486, 1.5812528759706765, 1.3333771398756653, 1.6202378900488839, 1.399839312885888, 1.5041243018349633, 1.60298468102701, 1.544830510043539, 1.4223287729546428, 1.5560098929563537, 1.4325262360507622, 1.4793786760419607, 4.633811862091534, 7.47930092504248, 1.464562664856203, 1.7675832770764828, 15.304666578071192, 5.020688264048658, 1.4307615170255303, 6.614678013138473, 8.892258609994315, 1.7932006750488654, 2.2604337020311505, 12.206094717839733, 1.4995965940179303, 4.908534814952873, 10.624994999030605, 1.5630930450279266, 1.3997956699458882, 1.5141913659172133, 6.138256532140076, 8.938605118193664, 1.4217113559134305, 1.613686871016398, 11.33338535297662, 1.6569737539393827, 10.771903496934101, 1.5863762310473248, 7.916624281089753, 1.6899625360965729, 1.5075779120670632, 1.6250720110256225, 3.2231244968716055, 15.121146508026868, 1.44605356908869, 1.6146361589198932, 14.759912437060848, 1.583059353986755, 1.6027469469700009, 18.024860262987204, 1.5464906889246777, 1.4451589260715991, 1.6114849841687828, 12.562888402142562, 4.99259044311475, 1.5062697309767827, 1.470780486939475, 19.209869703976437, 4.14343832700979, 1.6292097030673176, 8.64933189493604, 4.930525396135636, 1.4239059360697865, 1.562270064954646, 11.338859395124018, 1.5953018077416345, 1.895741076907143, 11.14143952797167, 1.4835860969033092, 8.94131684792228, 6.248305753921159, 1.6931995819322765, 9.62481481814757, 3.540892111021094, 1.4372547109378502, 8.725648688036017, 1.7742177940672264, 4.370485604857095, 5.45943105709739, 1.6018190879840404, 5.124267776031047, 7.157911850139499, 1.4822109040105715, 9.35747047199402, 1.4765958309872076, 1.5256464349804446, 12.890990790096112, 1.5980078999418765, 1.5444891740335152, 15.698989218915813, 1.5116312189493328, 6.364514529122971, 7.675388130010106, 1.539817083044909, 4.184690609923564, 10.768454452045262, 1.519982947036624, 7.051969639956951, 3.767107236897573, 1.5394100550329313, 1.5119657180039212, 15.109421149012633, 6.157783722039312, 1.550980325206183, 5.529798747971654, 7.688968570088036, 1.4934573752107099, 6.742668555001728, 1.4522260800004005, 1.5445809479570016, 4.569018309935927, 5.117817286984064, 1.5009536789730191, 5.63743257895112, 13.786327652051114, 1.6797005720436573, 8.801394759910181, 2.8344013040186837, 1.5764415001031011, 10.191710280952975, 3.704143450828269, 1.6236092980252579, 8.942131952033378, 3.4179766461020336, 1.5305328300455585, 1.4509463680442423, 7.838967549963854, 1.8888780809938908, 1.4663146090460941, 7.171383103122935, 3.922976473928429, 1.6945075718685985, 9.612309644930065, 1.5048935199156404, 13.394421088858508, 1.4266674360260367, 1.559118114062585, 6.367063401034102, 8.393001509015448, 1.5198373850435019, 4.962575134006329, 14.962217338033952, 1.796940112952143, 1.5425552939996123, 4.1886296678567305, 7.6739126078318805, 1.5350423259660602, 9.837973856017925, 16.26694215706084, 1.5266493030358106, 1.6211966129485518, 11.670933141955175, 1.4771285239839926, 3.619693633983843, 12.062414284911938, 1.448012090055272, 1.55326736101415, 10.17950526101049, 1.5971154511207715, 3.5593307019444183, 13.6662348479731, 1.6120495179202408, 1.6876946890261024, 8.887802437064238, 1.477930399007164, 1.4397948479745537, 8.221754725906067, 2.683820287929848, 1.5195212848484516, 3.728004591888748, 8.987366936984472, 1.4985627988353372, 7.950968606048264, 1.386334762093611, 1.6193733820691705, 9.317591229919344, 3.9216833740938455, 1.4366882998729125, 1.6143205770058557, 10.317998482030816, 3.198468708083965, 1.481515871011652, 8.840938570909202, 4.845690131885931, 1.5142619839170948, 7.570231654914096, 1.509092645952478, 1.6425475000869483, 14.433191456948407, 1.4237426799954847, 1.5288292040349916, 1.4741398219484836, 8.677976620849222, 1.4267605389468372, 3.0591927269706503, 10.002157418988645, 1.5764710599323735, 9.675057105836459, 3.510921068955213, 1.461095968959853, 8.557505917036906, 8.563537448993884, 1.676155427005142, 10.296013994142413, 1.6724307190161198, 1.4382567389402539, 9.725556778954342, 7.771794390981086, 1.4840501309372485, 2.2369169739540666, 9.370701541076414, 1.6363637329777703, 7.2405899710720405, 8.3558686618926, 1.5066549499752, 3.3323946699965745, 8.829717718879692, 1.539948598947376, 6.646542126080021, 6.072849292890169, 1.589449767023325, 8.103200274053961, 6.487926217028871, 1.6664913119748235, 7.340245716972277, 1.4799676049733534, 1.560707652126439, 4.6224567189347, 11.300106844981201]
Total Epoch List: [98, 93, 96]
Total Time List: [0.38012989598792046, 4.698304936056957, 0.44346380210481584]
========================training times:8========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba0d90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.37s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.37s
Epoch 3/1000, LR 0.000030
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 2.78s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 8.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.41s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.40s
Epoch 6/1000, LR 0.000120
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 4.62s
Epoch 7/1000, LR 0.000150
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 6.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 3.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.36s
Epoch 8/1000, LR 0.000180
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.37s
Epoch 9/1000, LR 0.000210
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.39s
Epoch 10/1000, LR 0.000240
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 10.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 3.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.39s
Epoch 11/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.36s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.43s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 10.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 4.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 3.00s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.37s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.37s
Epoch 16/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 3.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 4.59s
Epoch 17/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 7.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.36s
Epoch 18/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.36s
Epoch 19/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 3.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 5.15s
Epoch 20/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 11.18s
Val loss: 0.6887 score: 0.5102 time: 0.48s
Test loss: 0.6903 score: 0.5306 time: 0.37s
Epoch 21/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.61s
Val loss: 0.6880 score: 0.5306 time: 0.46s
Test loss: 0.6899 score: 0.5510 time: 0.39s
Epoch 22/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.57s
Val loss: 0.6871 score: 0.5714 time: 0.55s
Test loss: 0.6895 score: 0.5510 time: 0.35s
Epoch 23/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 2.43s
Val loss: 0.6863 score: 0.5918 time: 3.58s
Test loss: 0.6889 score: 0.5510 time: 4.30s
Epoch 24/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 9.01s
Val loss: 0.6853 score: 0.5918 time: 0.60s
Test loss: 0.6883 score: 0.5510 time: 0.35s
Epoch 25/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.56s
Val loss: 0.6842 score: 0.5918 time: 0.46s
Test loss: 0.6877 score: 0.5510 time: 0.40s
Epoch 26/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.88s
Val loss: 0.6829 score: 0.5918 time: 0.62s
Test loss: 0.6869 score: 0.5510 time: 4.59s
Epoch 27/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 3.27s
Val loss: 0.6815 score: 0.5918 time: 0.47s
Test loss: 0.6861 score: 0.5510 time: 0.36s
Epoch 28/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.82s
Val loss: 0.6800 score: 0.5918 time: 0.45s
Test loss: 0.6851 score: 0.5510 time: 0.36s
Epoch 29/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.61s
Val loss: 0.6783 score: 0.5918 time: 4.48s
Test loss: 0.6840 score: 0.5510 time: 4.01s
Epoch 30/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.82s
Val loss: 0.6764 score: 0.5918 time: 0.55s
Test loss: 0.6829 score: 0.5714 time: 0.37s
Epoch 31/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.55s
Val loss: 0.6743 score: 0.5918 time: 0.45s
Test loss: 0.6816 score: 0.5714 time: 0.38s
Epoch 32/1000, LR 0.000270
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.59s
Val loss: 0.6720 score: 0.5918 time: 0.56s
Test loss: 0.6802 score: 0.5714 time: 0.37s
Epoch 33/1000, LR 0.000270
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.67s
Val loss: 0.6694 score: 0.5918 time: 4.36s
Test loss: 0.6786 score: 0.5714 time: 3.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 3.79s
Val loss: 0.6666 score: 0.5918 time: 0.46s
Test loss: 0.6768 score: 0.5714 time: 0.36s
Epoch 35/1000, LR 0.000270
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.66s
Val loss: 0.6635 score: 0.6122 time: 0.44s
Test loss: 0.6749 score: 0.5918 time: 0.37s
Epoch 36/1000, LR 0.000270
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.72s
Val loss: 0.6600 score: 0.6122 time: 1.74s
Test loss: 0.6727 score: 0.5918 time: 2.21s
Epoch 37/1000, LR 0.000270
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 3.67s
Val loss: 0.6562 score: 0.6122 time: 0.47s
Test loss: 0.6703 score: 0.5918 time: 0.50s
Epoch 38/1000, LR 0.000270
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.58s
Val loss: 0.6520 score: 0.6122 time: 0.56s
Test loss: 0.6677 score: 0.5918 time: 0.38s
Epoch 39/1000, LR 0.000269
Train loss: 0.6576;  Loss pred: 0.6576; Loss self: 0.0000; time: 6.59s
Val loss: 0.6473 score: 0.6122 time: 3.38s
Test loss: 0.6648 score: 0.6327 time: 2.12s
Epoch 40/1000, LR 0.000269
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 0.58s
Val loss: 0.6422 score: 0.6531 time: 0.58s
Test loss: 0.6616 score: 0.6327 time: 0.35s
Epoch 41/1000, LR 0.000269
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.58s
Val loss: 0.6366 score: 0.6531 time: 0.47s
Test loss: 0.6582 score: 0.6327 time: 0.39s
Epoch 42/1000, LR 0.000269
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 1.77s
Val loss: 0.6306 score: 0.6735 time: 1.17s
Test loss: 0.6544 score: 0.6327 time: 3.83s
Epoch 43/1000, LR 0.000269
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 2.32s
Val loss: 0.6241 score: 0.6735 time: 0.47s
Test loss: 0.6503 score: 0.6327 time: 0.35s
Epoch 44/1000, LR 0.000269
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.65s
Val loss: 0.6173 score: 0.6939 time: 0.45s
Test loss: 0.6458 score: 0.6327 time: 0.35s
Epoch 45/1000, LR 0.000269
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.56s
Val loss: 0.6100 score: 0.7143 time: 0.49s
Test loss: 0.6411 score: 0.6327 time: 0.37s
Epoch 46/1000, LR 0.000269
Train loss: 0.6190;  Loss pred: 0.6190; Loss self: 0.0000; time: 3.85s
Val loss: 0.6021 score: 0.7143 time: 4.02s
Test loss: 0.6360 score: 0.6122 time: 4.84s
Epoch 47/1000, LR 0.000269
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.97s
Val loss: 0.5937 score: 0.7347 time: 0.45s
Test loss: 0.6305 score: 0.6122 time: 0.38s
Epoch 48/1000, LR 0.000269
Train loss: 0.6064;  Loss pred: 0.6064; Loss self: 0.0000; time: 0.60s
Val loss: 0.5848 score: 0.7551 time: 0.55s
Test loss: 0.6247 score: 0.6531 time: 0.36s
Epoch 49/1000, LR 0.000269
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.63s
Val loss: 0.5752 score: 0.7959 time: 4.39s
Test loss: 0.6185 score: 0.6735 time: 2.60s
Epoch 50/1000, LR 0.000269
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.70s
Val loss: 0.5651 score: 0.8367 time: 0.47s
Test loss: 0.6119 score: 0.6939 time: 0.36s
Epoch 51/1000, LR 0.000269
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.58s
Val loss: 0.5544 score: 0.8571 time: 0.46s
Test loss: 0.6048 score: 0.6939 time: 0.36s
Epoch 52/1000, LR 0.000269
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.67s
Val loss: 0.5433 score: 0.8776 time: 2.82s
Test loss: 0.5973 score: 0.6939 time: 2.69s
Epoch 53/1000, LR 0.000269
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.61s
Val loss: 0.5317 score: 0.8980 time: 0.46s
Test loss: 0.5895 score: 0.7347 time: 0.38s
Epoch 54/1000, LR 0.000269
Train loss: 0.5518;  Loss pred: 0.5518; Loss self: 0.0000; time: 0.56s
Val loss: 0.5197 score: 0.9184 time: 0.56s
Test loss: 0.5812 score: 0.7347 time: 0.36s
Epoch 55/1000, LR 0.000269
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 8.02s
Val loss: 0.5074 score: 0.9184 time: 3.39s
Test loss: 0.5727 score: 0.7347 time: 0.39s
Epoch 56/1000, LR 0.000269
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.85s
Val loss: 0.4947 score: 0.9388 time: 0.63s
Test loss: 0.5638 score: 0.7347 time: 0.39s
Epoch 57/1000, LR 0.000269
Train loss: 0.5129;  Loss pred: 0.5129; Loss self: 0.0000; time: 0.62s
Val loss: 0.4818 score: 0.9592 time: 0.49s
Test loss: 0.5547 score: 0.7551 time: 3.54s
Epoch 58/1000, LR 0.000269
Train loss: 0.4985;  Loss pred: 0.4985; Loss self: 0.0000; time: 6.08s
Val loss: 0.4689 score: 0.9592 time: 0.46s
Test loss: 0.5453 score: 0.7551 time: 0.44s
Epoch 59/1000, LR 0.000268
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.58s
Val loss: 0.4562 score: 0.9592 time: 0.45s
Test loss: 0.5357 score: 0.7551 time: 0.34s
Epoch 60/1000, LR 0.000268
Train loss: 0.4643;  Loss pred: 0.4643; Loss self: 0.0000; time: 0.72s
Val loss: 0.4433 score: 0.9592 time: 0.47s
Test loss: 0.5260 score: 0.7755 time: 2.13s
Epoch 61/1000, LR 0.000268
Train loss: 0.4607;  Loss pred: 0.4607; Loss self: 0.0000; time: 11.76s
Val loss: 0.4306 score: 0.9592 time: 3.33s
Test loss: 0.5161 score: 0.8163 time: 0.99s
Epoch 62/1000, LR 0.000268
Train loss: 0.4449;  Loss pred: 0.4449; Loss self: 0.0000; time: 0.62s
Val loss: 0.4179 score: 0.9592 time: 0.55s
Test loss: 0.5062 score: 0.8163 time: 0.37s
Epoch 63/1000, LR 0.000268
Train loss: 0.4336;  Loss pred: 0.4336; Loss self: 0.0000; time: 0.64s
Val loss: 0.4052 score: 0.9592 time: 0.50s
Test loss: 0.4966 score: 0.8163 time: 0.37s
Epoch 64/1000, LR 0.000268
Train loss: 0.4113;  Loss pred: 0.4113; Loss self: 0.0000; time: 3.78s
Val loss: 0.3928 score: 0.9592 time: 3.54s
Test loss: 0.4872 score: 0.8163 time: 3.86s
Epoch 65/1000, LR 0.000268
Train loss: 0.4055;  Loss pred: 0.4055; Loss self: 0.0000; time: 4.39s
Val loss: 0.3809 score: 0.9592 time: 0.46s
Test loss: 0.4782 score: 0.8163 time: 0.40s
Epoch 66/1000, LR 0.000268
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.94s
Val loss: 0.3696 score: 0.9592 time: 0.44s
Test loss: 0.4696 score: 0.8163 time: 0.39s
Epoch 67/1000, LR 0.000268
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.57s
Val loss: 0.3589 score: 0.9592 time: 0.53s
Test loss: 0.4615 score: 0.8163 time: 3.14s
Epoch 68/1000, LR 0.000268
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 10.76s
Val loss: 0.3488 score: 0.9592 time: 0.99s
Test loss: 0.4534 score: 0.8163 time: 0.40s
Epoch 69/1000, LR 0.000268
Train loss: 0.3591;  Loss pred: 0.3591; Loss self: 0.0000; time: 0.57s
Val loss: 0.3392 score: 0.9592 time: 0.45s
Test loss: 0.4453 score: 0.8163 time: 0.37s
Epoch 70/1000, LR 0.000268
Train loss: 0.3425;  Loss pred: 0.3425; Loss self: 0.0000; time: 0.57s
Val loss: 0.3302 score: 0.9796 time: 0.56s
Test loss: 0.4371 score: 0.8163 time: 0.37s
Epoch 71/1000, LR 0.000268
Train loss: 0.3368;  Loss pred: 0.3368; Loss self: 0.0000; time: 0.61s
Val loss: 0.3216 score: 0.9796 time: 4.62s
Test loss: 0.4290 score: 0.8571 time: 3.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.3247;  Loss pred: 0.3247; Loss self: 0.0000; time: 6.83s
Val loss: 0.3134 score: 0.9796 time: 0.55s
Test loss: 0.4208 score: 0.8571 time: 0.36s
Epoch 73/1000, LR 0.000267
Train loss: 0.3052;  Loss pred: 0.3052; Loss self: 0.0000; time: 0.66s
Val loss: 0.3054 score: 0.9796 time: 0.45s
Test loss: 0.4133 score: 0.8571 time: 0.39s
Epoch 74/1000, LR 0.000267
Train loss: 0.3041;  Loss pred: 0.3041; Loss self: 0.0000; time: 0.67s
Val loss: 0.2977 score: 0.9796 time: 0.48s
Test loss: 0.4061 score: 0.8571 time: 3.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.2910;  Loss pred: 0.2910; Loss self: 0.0000; time: 9.45s
Val loss: 0.2902 score: 0.9796 time: 0.45s
Test loss: 0.3999 score: 0.8571 time: 0.38s
Epoch 76/1000, LR 0.000267
Train loss: 0.2825;  Loss pred: 0.2825; Loss self: 0.0000; time: 0.66s
Val loss: 0.2829 score: 0.9796 time: 0.47s
Test loss: 0.3938 score: 0.8776 time: 0.37s
Epoch 77/1000, LR 0.000267
Train loss: 0.2668;  Loss pred: 0.2668; Loss self: 0.0000; time: 0.67s
Val loss: 0.2758 score: 0.9796 time: 0.47s
Test loss: 0.3880 score: 0.8776 time: 5.41s
Epoch 78/1000, LR 0.000267
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 5.09s
Val loss: 0.2688 score: 0.9796 time: 0.60s
Test loss: 0.3827 score: 0.8776 time: 0.37s
Epoch 79/1000, LR 0.000267
Train loss: 0.2589;  Loss pred: 0.2589; Loss self: 0.0000; time: 0.60s
Val loss: 0.2620 score: 0.9796 time: 0.56s
Test loss: 0.3772 score: 0.8776 time: 0.37s
Epoch 80/1000, LR 0.000267
Train loss: 0.2423;  Loss pred: 0.2423; Loss self: 0.0000; time: 0.63s
Val loss: 0.2554 score: 0.9796 time: 3.01s
Test loss: 0.3720 score: 0.8776 time: 3.54s
Epoch 81/1000, LR 0.000267
Train loss: 0.2317;  Loss pred: 0.2317; Loss self: 0.0000; time: 5.04s
Val loss: 0.2488 score: 0.9796 time: 0.50s
Test loss: 0.3669 score: 0.8776 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.2264;  Loss pred: 0.2264; Loss self: 0.0000; time: 0.66s
Val loss: 0.2425 score: 0.9796 time: 0.46s
Test loss: 0.3624 score: 0.8776 time: 0.36s
Epoch 83/1000, LR 0.000266
Train loss: 0.2178;  Loss pred: 0.2178; Loss self: 0.0000; time: 0.59s
Val loss: 0.2363 score: 0.9592 time: 0.47s
Test loss: 0.3579 score: 0.8776 time: 1.14s
Epoch 84/1000, LR 0.000266
Train loss: 0.2043;  Loss pred: 0.2043; Loss self: 0.0000; time: 6.97s
Val loss: 0.2302 score: 0.9592 time: 0.47s
Test loss: 0.3536 score: 0.8776 time: 0.36s
Epoch 85/1000, LR 0.000266
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.73s
Val loss: 0.2243 score: 0.9592 time: 0.52s
Test loss: 0.3492 score: 0.8776 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.1933;  Loss pred: 0.1933; Loss self: 0.0000; time: 0.64s
Val loss: 0.2185 score: 0.9592 time: 0.54s
Test loss: 0.3447 score: 0.8776 time: 1.54s
Epoch 87/1000, LR 0.000266
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 9.22s
Val loss: 0.2129 score: 0.9592 time: 2.03s
Test loss: 0.3404 score: 0.8776 time: 3.39s
Epoch 88/1000, LR 0.000266
Train loss: 0.1778;  Loss pred: 0.1778; Loss self: 0.0000; time: 3.89s
Val loss: 0.2075 score: 0.9592 time: 0.63s
Test loss: 0.3360 score: 0.8776 time: 0.36s
Epoch 89/1000, LR 0.000266
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.62s
Val loss: 0.2024 score: 0.9592 time: 0.46s
Test loss: 0.3320 score: 0.8980 time: 0.36s
Epoch 90/1000, LR 0.000266
Train loss: 0.1568;  Loss pred: 0.1568; Loss self: 0.0000; time: 0.72s
Val loss: 0.1977 score: 0.9592 time: 4.84s
Test loss: 0.3286 score: 0.8980 time: 1.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.1531;  Loss pred: 0.1531; Loss self: 0.0000; time: 0.76s
Val loss: 0.1933 score: 0.9592 time: 0.51s
Test loss: 0.3259 score: 0.8980 time: 0.36s
Epoch 92/1000, LR 0.000266
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.72s
Val loss: 0.1893 score: 0.9592 time: 0.45s
Test loss: 0.3236 score: 0.8980 time: 0.40s
Epoch 93/1000, LR 0.000265
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.63s
Val loss: 0.1856 score: 0.9592 time: 3.28s
Test loss: 0.3215 score: 0.8980 time: 4.62s
Epoch 94/1000, LR 0.000265
Train loss: 0.1307;  Loss pred: 0.1307; Loss self: 0.0000; time: 8.85s
Val loss: 0.1822 score: 0.9592 time: 0.58s
Test loss: 0.3198 score: 0.8980 time: 0.58s
Epoch 95/1000, LR 0.000265
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.55s
Val loss: 0.1793 score: 0.9592 time: 0.47s
Test loss: 0.3184 score: 0.8980 time: 0.41s
Epoch 96/1000, LR 0.000265
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 0.66s
Val loss: 0.1767 score: 0.9592 time: 0.79s
Test loss: 0.3172 score: 0.8980 time: 1.87s
Epoch 97/1000, LR 0.000265
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 6.06s
Val loss: 0.1744 score: 0.9592 time: 0.45s
Test loss: 0.3166 score: 0.8980 time: 0.38s
Epoch 98/1000, LR 0.000265
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.68s
Val loss: 0.1725 score: 0.9592 time: 0.49s
Test loss: 0.3163 score: 0.8980 time: 0.36s
Epoch 99/1000, LR 0.000265
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.61s
Val loss: 0.1710 score: 0.9592 time: 0.47s
Test loss: 0.3166 score: 0.8980 time: 0.35s
Epoch 100/1000, LR 0.000265
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 9.54s
Val loss: 0.1699 score: 0.9592 time: 4.21s
Test loss: 0.3173 score: 0.8980 time: 0.39s
Epoch 101/1000, LR 0.000265
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.58s
Val loss: 0.1692 score: 0.9592 time: 0.47s
Test loss: 0.3186 score: 0.8980 time: 0.39s
Epoch 102/1000, LR 0.000264
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 0.61s
Val loss: 0.1688 score: 0.9592 time: 0.54s
Test loss: 0.3202 score: 0.8980 time: 0.35s
Epoch 103/1000, LR 0.000264
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 5.04s
Val loss: 0.1685 score: 0.9592 time: 2.52s
Test loss: 0.3215 score: 0.8980 time: 4.18s
Epoch 104/1000, LR 0.000264
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.66s
Val loss: 0.1685 score: 0.9592 time: 0.57s
Test loss: 0.3232 score: 0.8980 time: 0.36s
     INFO: Early stopping counter 1 of 2
Epoch 105/1000, LR 0.000264
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.58s
Val loss: 0.1688 score: 0.9592 time: 0.46s
Test loss: 0.3251 score: 0.8980 time: 0.40s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 102,   Train_Loss: 0.0844,   Val_Loss: 0.1685,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1685,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3215


[0.3777761060046032, 0.3790616230107844, 2.786415035952814, 0.41136884095612913, 0.4038271229946986, 4.627412057016045, 0.3620967019814998, 0.37637127796187997, 0.39077343803364784, 0.39419224904850125, 0.36492536508012563, 0.4309730059467256, 3.0074319769628346, 0.370179157005623, 0.37112932198215276, 4.595855289022438, 0.3693024090025574, 0.36723589594475925, 5.154684701003134, 0.3763011069968343, 0.39813246799167246, 0.3578430518973619, 4.303114828071557, 0.35605985298752785, 0.4013199550099671, 4.599332981975749, 0.3686455700080842, 0.3621312740724534, 4.012088172021322, 0.3722999739693478, 0.3889803399797529, 0.3733879759674892, 3.269251944962889, 0.3593425879953429, 0.37461338902357966, 2.2108459708979353, 0.5017428619321436, 0.3794344230554998, 2.126986271003261, 0.35201983200386167, 0.3932023770175874, 3.836485236068256, 0.35173511097673327, 0.35361694591119885, 0.37787731701973826, 4.845903299981728, 0.37951324589084834, 0.36902304110117257, 2.602608363959007, 0.3676262029912323, 0.36793721304275095, 2.691094732028432, 0.3862986390013248, 0.36696887004654855, 0.3928933870047331, 0.3922393099637702, 3.5486341340001673, 0.4439318709773943, 0.34465736406855285, 2.1390638659941033, 0.9971110880142078, 0.3783165679778904, 0.3722632459830493, 3.864240511902608, 0.4035761100240052, 0.3958305970299989, 3.146391554037109, 0.40013766090851277, 0.37553239392582327, 0.3719928580103442, 3.205471045919694, 0.36205872998107225, 0.393173013930209, 3.252548294956796, 0.3847858349326998, 0.3729683639248833, 5.411424473975785, 0.3697744640521705, 0.3705024280352518, 3.5431408260483295, 0.4204563299426809, 0.3683441929752007, 1.148615877958946, 0.3686072949785739, 0.37964703794568777, 1.549738356960006, 3.3918030289933085, 0.3682931129587814, 0.36520313599612564, 1.2562500489875674, 0.36508860101457685, 0.40720906504429877, 4.629580554901622, 0.5844746680231765, 0.4097218499518931, 1.8720070999115705, 0.3846245059976354, 0.3634586801053956, 0.3545837310375646, 0.3986363069852814, 0.3951238470617682, 0.35645965801086277, 4.183872628957033, 0.3634236470097676, 0.40389715996570885]
[0.007709716449073535, 0.007735951490016008, 0.056865612978628854, 0.008395282468492431, 0.008241369857034666, 0.09443698075542949, 0.007389728611867342, 0.0076810464890179585, 0.00797496812313567, 0.008044739776500026, 0.007447456430206645, 0.008795367468300522, 0.06137616279515989, 0.007554676673584142, 0.007574067795554138, 0.09379296508209058, 0.007536783857195049, 0.007494610121321617, 0.10519764695924763, 0.007679614428506822, 0.008125152407993316, 0.007302919426476773, 0.08781866996064402, 0.007266527611990364, 0.008190203163468716, 0.09386393840766835, 0.00752337897975682, 0.007390434164743946, 0.08187935044941473, 0.007597958652435669, 0.00793837428530108, 0.007620162774846718, 0.06671942744822222, 0.00733352220398659, 0.00764517120456285, 0.045119305528529295, 0.010239650243513134, 0.007743559654193873, 0.04340788308169921, 0.007184078204160442, 0.008024538306481376, 0.07829561706261748, 0.00717826757095374, 0.007216672365534671, 0.007711781979994658, 0.09889598571391282, 0.0077451682834867005, 0.007531082471452502, 0.05311445640732667, 0.007502575571249638, 0.007508922715158183, 0.05492030065364147, 0.007883645693904586, 0.007489160613194868, 0.008018232387851695, 0.008004883876811636, 0.07242110477551361, 0.009059834101579475, 0.007033823756501079, 0.043654364612124553, 0.020349205877840976, 0.007720746285263069, 0.007597209101694883, 0.07886205126331854, 0.008236247143347045, 0.008078175449591815, 0.06421207253136957, 0.008166074712418628, 0.007663926406649454, 0.007591690979802943, 0.06541777644734069, 0.007388953673083107, 0.008023939059800183, 0.06637853663177135, 0.00785277214148367, 0.0076115992637731285, 0.11043723416277113, 0.0075464176337177655, 0.007561274041535751, 0.07230899644996591, 0.008580741427401652, 0.007517228428065321, 0.0234411403665091, 0.00752259785670559, 0.0077478987335854645, 0.03162731340734706, 0.06922046997945527, 0.007516185978750641, 0.0074531252244107275, 0.02563775610178709, 0.007450787775807691, 0.00831038908253671, 0.09448123581431882, 0.011928054449452581, 0.008361670407181492, 0.03820422652880756, 0.007849479714237457, 0.007417524083783584, 0.007236402674236012, 0.008135434836434315, 0.008063751980852412, 0.007274686898180872, 0.08538515569300067, 0.0074168091226483185, 0.00824279918297365]
[129.70645634057897, 129.26658101341465, 17.585319978451977, 119.11451505687971, 121.33905131638042, 10.58907211984863, 135.3229668534885, 130.19059335596504, 125.39235073541722, 124.30482871815944, 134.27403159339502, 113.69621605965989, 16.29297033992584, 132.36833860761018, 132.0294492989599, 10.661780434436295, 132.6825896758791, 133.42922230938646, 9.505916043800756, 130.21487072163256, 123.07461445476713, 136.9315395120607, 11.387100265218665, 137.6173123391038, 122.09709332490841, 10.653718744005989, 132.91899858968998, 135.31004778724616, 12.213091512222029, 131.61429875370948, 125.97037681274723, 131.23079251021844, 14.988138211708314, 136.3601243964855, 130.80151814038805, 22.163461699730554, 97.65958565171742, 129.13957464748205, 23.037290211040045, 139.1967029842298, 124.61776139722649, 12.772107015904133, 139.30937933358967, 138.5680199056552, 129.67171564161526, 10.111633882620968, 129.1127530607795, 132.78303667376153, 18.8272660145696, 133.2875611186198, 133.1748957784997, 18.208203307308285, 126.84486832953083, 133.5263124465695, 124.71576672123959, 124.9237359828263, 13.808129592882311, 110.37729706614184, 142.17018148567348, 22.907216927451515, 49.14196681694287, 129.52115806586525, 131.6272839952381, 12.680370139764987, 121.41452078787673, 123.79032941783029, 15.573395478108408, 122.45785585077265, 130.48141995888292, 131.7229590430401, 15.286364873697126, 135.33715925745426, 124.62706814536831, 15.065110662915114, 127.3435650472171, 131.37843511539415, 9.05491709912004, 132.51320673427227, 132.25284449509155, 13.829537804358111, 116.54004592267637, 133.0277521255751, 42.66004061085369, 132.9328004830947, 129.06725221707097, 31.618240446806297, 14.446593620309155, 133.0462022663019, 134.171903716949, 39.0049736033761, 134.21399590080213, 120.33130940901198, 10.584112193084248, 83.83596874391309, 119.59332900052381, 26.175114401176497, 127.39697870499505, 134.81587504194698, 138.19020928179287, 122.91905965757704, 124.01175065583936, 137.46296081142168, 11.711637601218001, 134.82887094213504, 121.31801076333424]
Elapsed: 1.239967213514527~1.4750716750164987
Time per graph: 0.025305453337031165~0.03010350357176528
Speed: 95.91480761633724~51.16846224492063
Total Time: 0.4044
best val loss: 0.16851018369197845 test_score: 0.8980

Testing...
Test loss: 0.4371 score: 0.8163 time: 0.48s
test Score 0.8163
Epoch Time List: [1.5254303100518882, 1.6452161410124972, 3.8313912278972566, 8.940008548786864, 1.4739857909735292, 5.802249365951866, 9.745023442083038, 1.5057844369439408, 1.4823550340952352, 14.116506763035432, 1.3791411500424147, 1.6482419380918145, 18.28088282479439, 1.5068352241069078, 1.4399947610218078, 8.861998098902404, 8.757078099064529, 1.469383744057268, 8.750164972851053, 12.028789927018806, 1.468567990930751, 1.4752585899550468, 10.306545775965787, 9.96542911499273, 1.4194220198551193, 6.0941390208899975, 4.1067292569205165, 1.6228205768857151, 9.092725037131459, 1.742652386892587, 1.3906361958943307, 1.524723618873395, 8.292830168851651, 4.603940226137638, 1.4726729389512911, 4.666632142965682, 4.631235647015274, 1.5146250571124256, 12.09829104389064, 1.506565086892806, 1.4392736901063472, 6.76649587310385, 3.1390180110465735, 1.4510899900924414, 1.4187746449606493, 12.71111766283866, 1.7924457649933174, 1.5123513179132715, 7.61004760407377, 1.5313597009517252, 1.4056466419715434, 6.174983064061962, 1.4592807448934764, 1.4831951769301668, 11.804629980935715, 1.8600998299662024, 4.64996678812895, 6.985381135949865, 1.3731486740289256, 3.3252886830596253, 16.08337309502531, 1.5413428889587522, 1.507501607062295, 11.182367717963643, 5.249350955011323, 1.769834850099869, 4.23800336685963, 12.147797760087997, 1.389602443901822, 1.4961517330957577, 8.432140898890793, 7.742725523887202, 1.4918990089790896, 4.398861582973041, 10.288134982110932, 1.5000079739838839, 6.552208048873581, 6.057737553841434, 1.5312836680095643, 7.17919100006111, 5.950089815072715, 1.4884320079581812, 2.2003402538830414, 7.804656345047988, 1.6215006868587807, 2.7239643511129543, 14.640630019945092, 4.888931100955233, 1.4344048540806398, 6.8163068409776315, 1.6341969849308953, 1.5690942970104516, 8.53140636102762, 10.011419997899793, 1.4333432308631018, 3.3137192690046504, 6.888070740038529, 1.5279888729564846, 1.4332552410196513, 14.148237054934725, 1.442706924979575, 1.5082274369196966, 11.738000817131251, 1.5885900999419391, 1.4379052519798279]
Total Epoch List: [105]
Total Time List: [0.40440532693173736]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3e50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 3.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 3.75s
Epoch 2/1000, LR 0.000000
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 5.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.61s
Epoch 3/1000, LR 0.000030
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5102 time: 0.56s
Epoch 4/1000, LR 0.000060
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 4.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 4.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.47s
Epoch 6/1000, LR 0.000120
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.56s
Epoch 7/1000, LR 0.000150
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4898 time: 2.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 4.15s
Epoch 8/1000, LR 0.000180
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 5.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.46s
Epoch 9/1000, LR 0.000210
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.47s
Epoch 10/1000, LR 0.000240
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 3.80s
Epoch 11/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 9.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.49s
Epoch 12/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.59s
Epoch 13/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 2.98s
Epoch 14/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 7.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.56s
Epoch 15/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.48s
Epoch 16/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.56s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 9.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.48s
Epoch 18/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5102 time: 0.48s
Epoch 19/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5102 time: 0.48s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 2.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5102 time: 3.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 8.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5102 time: 0.47s
Epoch 22/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5102 time: 0.57s
Epoch 23/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 3.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5102 time: 3.98s
Epoch 24/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 9.31s
Val loss: 0.6907 score: 0.5306 time: 0.38s
Test loss: 0.6881 score: 0.5510 time: 0.56s
Epoch 25/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.70s
Val loss: 0.6902 score: 0.5714 time: 0.37s
Test loss: 0.6874 score: 0.6327 time: 0.57s
Epoch 26/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.64s
Val loss: 0.6897 score: 0.6327 time: 0.82s
Test loss: 0.6867 score: 0.7755 time: 3.40s
Epoch 27/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.69s
Val loss: 0.6891 score: 0.6939 time: 0.38s
Test loss: 0.6859 score: 0.7959 time: 0.56s
Epoch 28/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.69s
Val loss: 0.6884 score: 0.6531 time: 0.40s
Test loss: 0.6850 score: 0.8163 time: 4.30s
Epoch 29/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 4.06s
Val loss: 0.6877 score: 0.6327 time: 0.36s
Test loss: 0.6840 score: 0.7959 time: 0.47s
Epoch 30/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.59s
Val loss: 0.6869 score: 0.6327 time: 0.36s
Test loss: 0.6829 score: 0.7959 time: 0.62s
Epoch 31/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.61s
Val loss: 0.6860 score: 0.6531 time: 0.37s
Test loss: 0.6816 score: 0.7755 time: 0.57s
Epoch 32/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 7.64s
Val loss: 0.6851 score: 0.6531 time: 0.37s
Test loss: 0.6803 score: 0.6531 time: 1.02s
Epoch 33/1000, LR 0.000270
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.68s
Val loss: 0.6840 score: 0.6531 time: 0.41s
Test loss: 0.6789 score: 0.6122 time: 0.46s
Epoch 34/1000, LR 0.000270
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.57s
Val loss: 0.6829 score: 0.6122 time: 0.36s
Test loss: 0.6773 score: 0.6122 time: 0.52s
Epoch 35/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.83s
Val loss: 0.6817 score: 0.6122 time: 4.41s
Test loss: 0.6756 score: 0.6122 time: 3.66s
Epoch 36/1000, LR 0.000270
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.59s
Val loss: 0.6803 score: 0.5714 time: 0.38s
Test loss: 0.6736 score: 0.5918 time: 0.47s
Epoch 37/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.78s
Val loss: 0.6788 score: 0.5510 time: 0.37s
Test loss: 0.6715 score: 0.5918 time: 0.49s
Epoch 38/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.61s
Val loss: 0.6772 score: 0.5510 time: 0.35s
Test loss: 0.6692 score: 0.5918 time: 0.54s
Epoch 39/1000, LR 0.000269
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 12.11s
Val loss: 0.6753 score: 0.5510 time: 0.67s
Test loss: 0.6666 score: 0.5918 time: 0.51s
Epoch 40/1000, LR 0.000269
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.64s
Val loss: 0.6733 score: 0.5510 time: 0.49s
Test loss: 0.6637 score: 0.5918 time: 0.46s
Epoch 41/1000, LR 0.000269
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.65s
Val loss: 0.6711 score: 0.5510 time: 0.35s
Test loss: 0.6605 score: 0.5918 time: 0.49s
Epoch 42/1000, LR 0.000269
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 10.62s
Val loss: 0.6687 score: 0.5510 time: 0.41s
Test loss: 0.6569 score: 0.5918 time: 0.55s
Epoch 43/1000, LR 0.000269
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.60s
Val loss: 0.6660 score: 0.5510 time: 0.37s
Test loss: 0.6530 score: 0.5918 time: 0.44s
Epoch 44/1000, LR 0.000269
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.55s
Val loss: 0.6630 score: 0.5510 time: 0.39s
Test loss: 0.6488 score: 0.5918 time: 4.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 11.96s
Val loss: 0.6597 score: 0.5510 time: 0.37s
Test loss: 0.6442 score: 0.5918 time: 0.49s
Epoch 46/1000, LR 0.000269
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.76s
Val loss: 0.6562 score: 0.5510 time: 0.36s
Test loss: 0.6392 score: 0.5918 time: 0.56s
Epoch 47/1000, LR 0.000269
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 3.42s
Val loss: 0.6525 score: 0.5510 time: 3.62s
Test loss: 0.6338 score: 0.5918 time: 4.11s
Epoch 48/1000, LR 0.000269
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.81s
Val loss: 0.6486 score: 0.5510 time: 0.37s
Test loss: 0.6281 score: 0.5918 time: 0.57s
Epoch 49/1000, LR 0.000269
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.72s
Val loss: 0.6444 score: 0.5510 time: 5.50s
Test loss: 0.6222 score: 0.5918 time: 4.47s
Epoch 50/1000, LR 0.000269
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 10.56s
Val loss: 0.6400 score: 0.5510 time: 1.30s
Test loss: 0.6159 score: 0.5918 time: 0.49s
Epoch 51/1000, LR 0.000269
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.66s
Val loss: 0.6352 score: 0.5510 time: 0.36s
Test loss: 0.6093 score: 0.5918 time: 0.46s
Epoch 52/1000, LR 0.000269
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.56s
Val loss: 0.6303 score: 0.5510 time: 0.37s
Test loss: 0.6024 score: 0.5918 time: 0.51s
Epoch 53/1000, LR 0.000269
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 5.27s
Val loss: 0.6250 score: 0.5714 time: 0.38s
Test loss: 0.5952 score: 0.5918 time: 0.45s
Epoch 54/1000, LR 0.000269
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.56s
Val loss: 0.6195 score: 0.5714 time: 0.37s
Test loss: 0.5877 score: 0.5918 time: 0.50s
Epoch 55/1000, LR 0.000269
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 0.70s
Val loss: 0.6137 score: 0.5714 time: 0.41s
Test loss: 0.5799 score: 0.5918 time: 4.87s
Epoch 56/1000, LR 0.000269
Train loss: 0.5762;  Loss pred: 0.5762; Loss self: 0.0000; time: 12.40s
Val loss: 0.6075 score: 0.6122 time: 1.53s
Test loss: 0.5718 score: 0.6122 time: 0.47s
Epoch 57/1000, LR 0.000269
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.67s
Val loss: 0.6010 score: 0.6122 time: 0.34s
Test loss: 0.5633 score: 0.6122 time: 0.50s
Epoch 58/1000, LR 0.000269
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.58s
Val loss: 0.5940 score: 0.6122 time: 0.38s
Test loss: 0.5546 score: 0.6122 time: 0.52s
Epoch 59/1000, LR 0.000268
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 9.17s
Val loss: 0.5865 score: 0.6122 time: 0.54s
Test loss: 0.5454 score: 0.6122 time: 0.47s
Epoch 60/1000, LR 0.000268
Train loss: 0.5402;  Loss pred: 0.5402; Loss self: 0.0000; time: 0.71s
Val loss: 0.5785 score: 0.6327 time: 0.37s
Test loss: 0.5358 score: 0.6122 time: 0.46s
Epoch 61/1000, LR 0.000268
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 6.06s
Val loss: 0.5702 score: 0.7143 time: 2.47s
Test loss: 0.5258 score: 0.7347 time: 1.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.70s
Val loss: 0.5615 score: 0.7143 time: 0.36s
Test loss: 0.5153 score: 0.7347 time: 0.48s
Epoch 63/1000, LR 0.000268
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.67s
Val loss: 0.5526 score: 0.7959 time: 0.41s
Test loss: 0.5045 score: 0.8163 time: 0.54s
Epoch 64/1000, LR 0.000268
Train loss: 0.4888;  Loss pred: 0.4888; Loss self: 0.0000; time: 0.63s
Val loss: 0.5434 score: 0.7959 time: 3.04s
Test loss: 0.4933 score: 0.8367 time: 4.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.4789;  Loss pred: 0.4789; Loss self: 0.0000; time: 0.68s
Val loss: 0.5339 score: 0.7755 time: 0.39s
Test loss: 0.4820 score: 0.8571 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.4619;  Loss pred: 0.4619; Loss self: 0.0000; time: 0.73s
Val loss: 0.5241 score: 0.7755 time: 0.37s
Test loss: 0.4706 score: 0.8776 time: 0.56s
Epoch 67/1000, LR 0.000268
Train loss: 0.4520;  Loss pred: 0.4520; Loss self: 0.0000; time: 0.70s
Val loss: 0.5143 score: 0.7959 time: 4.75s
Test loss: 0.4592 score: 0.9184 time: 2.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.4319;  Loss pred: 0.4319; Loss self: 0.0000; time: 0.61s
Val loss: 0.5045 score: 0.8367 time: 0.37s
Test loss: 0.4479 score: 0.9184 time: 0.50s
Epoch 69/1000, LR 0.000268
Train loss: 0.4298;  Loss pred: 0.4298; Loss self: 0.0000; time: 0.68s
Val loss: 0.4952 score: 0.8571 time: 0.36s
Test loss: 0.4368 score: 0.9184 time: 0.57s
Epoch 70/1000, LR 0.000268
Train loss: 0.4146;  Loss pred: 0.4146; Loss self: 0.0000; time: 9.08s
Val loss: 0.4862 score: 0.8571 time: 0.37s
Test loss: 0.4258 score: 0.9388 time: 0.57s
Epoch 71/1000, LR 0.000268
Train loss: 0.4087;  Loss pred: 0.4087; Loss self: 0.0000; time: 0.59s
Val loss: 0.4775 score: 0.8571 time: 0.37s
Test loss: 0.4153 score: 0.9388 time: 0.48s
Epoch 72/1000, LR 0.000267
Train loss: 0.3910;  Loss pred: 0.3910; Loss self: 0.0000; time: 0.59s
Val loss: 0.4691 score: 0.8571 time: 0.37s
Test loss: 0.4054 score: 0.9388 time: 0.55s
Epoch 73/1000, LR 0.000267
Train loss: 0.3802;  Loss pred: 0.3802; Loss self: 0.0000; time: 0.77s
Val loss: 0.4609 score: 0.8571 time: 3.12s
Test loss: 0.3961 score: 0.9388 time: 4.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 6.29s
Val loss: 0.4528 score: 0.8776 time: 0.37s
Test loss: 0.3870 score: 0.9184 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 0.64s
Val loss: 0.4451 score: 0.8776 time: 0.35s
Test loss: 0.3785 score: 0.9184 time: 0.46s
Epoch 76/1000, LR 0.000267
Train loss: 0.3499;  Loss pred: 0.3499; Loss self: 0.0000; time: 0.62s
Val loss: 0.4376 score: 0.8776 time: 0.36s
Test loss: 0.3703 score: 0.9184 time: 0.48s
Epoch 77/1000, LR 0.000267
Train loss: 0.3409;  Loss pred: 0.3409; Loss self: 0.0000; time: 7.83s
Val loss: 0.4307 score: 0.8776 time: 0.38s
Test loss: 0.3628 score: 0.9184 time: 0.51s
Epoch 78/1000, LR 0.000267
Train loss: 0.3207;  Loss pred: 0.3207; Loss self: 0.0000; time: 0.59s
Val loss: 0.4242 score: 0.8776 time: 0.37s
Test loss: 0.3558 score: 0.9184 time: 0.47s
Epoch 79/1000, LR 0.000267
Train loss: 0.3146;  Loss pred: 0.3146; Loss self: 0.0000; time: 0.68s
Val loss: 0.4182 score: 0.8776 time: 0.41s
Test loss: 0.3493 score: 0.9184 time: 3.11s
Epoch 80/1000, LR 0.000267
Train loss: 0.3094;  Loss pred: 0.3094; Loss self: 0.0000; time: 6.15s
Val loss: 0.4128 score: 0.8776 time: 0.44s
Test loss: 0.3434 score: 0.9184 time: 0.52s
Epoch 81/1000, LR 0.000267
Train loss: 0.3043;  Loss pred: 0.3043; Loss self: 0.0000; time: 0.75s
Val loss: 0.4078 score: 0.8776 time: 0.38s
Test loss: 0.3380 score: 0.9184 time: 0.48s
Epoch 82/1000, LR 0.000267
Train loss: 0.2997;  Loss pred: 0.2997; Loss self: 0.0000; time: 0.63s
Val loss: 0.4032 score: 0.8776 time: 4.30s
Test loss: 0.3330 score: 0.9184 time: 4.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2851;  Loss pred: 0.2851; Loss self: 0.0000; time: 4.23s
Val loss: 0.3988 score: 0.8776 time: 0.45s
Test loss: 0.3283 score: 0.9184 time: 0.47s
Epoch 84/1000, LR 0.000266
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.64s
Val loss: 0.3947 score: 0.8776 time: 0.36s
Test loss: 0.3236 score: 0.9184 time: 0.47s
Epoch 85/1000, LR 0.000266
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.62s
Val loss: 0.3908 score: 0.8571 time: 0.92s
Test loss: 0.3192 score: 0.9388 time: 1.88s
Epoch 86/1000, LR 0.000266
Train loss: 0.2608;  Loss pred: 0.2608; Loss self: 0.0000; time: 9.53s
Val loss: 0.3870 score: 0.8571 time: 0.38s
Test loss: 0.3152 score: 0.9592 time: 0.47s
Epoch 87/1000, LR 0.000266
Train loss: 0.2650;  Loss pred: 0.2650; Loss self: 0.0000; time: 0.85s
Val loss: 0.3833 score: 0.8571 time: 0.38s
Test loss: 0.3113 score: 0.9388 time: 0.46s
Epoch 88/1000, LR 0.000266
Train loss: 0.2470;  Loss pred: 0.2470; Loss self: 0.0000; time: 8.84s
Val loss: 0.3798 score: 0.8571 time: 0.37s
Test loss: 0.3075 score: 0.9388 time: 0.46s
Epoch 89/1000, LR 0.000266
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.71s
Val loss: 0.3764 score: 0.8571 time: 0.40s
Test loss: 0.3033 score: 0.9388 time: 0.47s
Epoch 90/1000, LR 0.000266
Train loss: 0.2457;  Loss pred: 0.2457; Loss self: 0.0000; time: 0.55s
Val loss: 0.3729 score: 0.8571 time: 0.37s
Test loss: 0.2988 score: 0.9388 time: 0.61s
Epoch 91/1000, LR 0.000266
Train loss: 0.2369;  Loss pred: 0.2369; Loss self: 0.0000; time: 9.69s
Val loss: 0.3694 score: 0.8571 time: 0.88s
Test loss: 0.2940 score: 0.9388 time: 0.73s
Epoch 92/1000, LR 0.000266
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 0.66s
Val loss: 0.3658 score: 0.8776 time: 0.38s
Test loss: 0.2893 score: 0.9388 time: 0.47s
Epoch 93/1000, LR 0.000265
Train loss: 0.2261;  Loss pred: 0.2261; Loss self: 0.0000; time: 0.65s
Val loss: 0.3620 score: 0.8776 time: 0.36s
Test loss: 0.2849 score: 0.9388 time: 0.53s
Epoch 94/1000, LR 0.000265
Train loss: 0.2110;  Loss pred: 0.2110; Loss self: 0.0000; time: 8.76s
Val loss: 0.3582 score: 0.8776 time: 0.36s
Test loss: 0.2806 score: 0.9388 time: 0.56s
Epoch 95/1000, LR 0.000265
Train loss: 0.1992;  Loss pred: 0.1992; Loss self: 0.0000; time: 0.60s
Val loss: 0.3543 score: 0.8776 time: 0.37s
Test loss: 0.2765 score: 0.9388 time: 0.51s
Epoch 96/1000, LR 0.000265
Train loss: 0.1971;  Loss pred: 0.1971; Loss self: 0.0000; time: 0.63s
Val loss: 0.3505 score: 0.8776 time: 0.36s
Test loss: 0.2726 score: 0.9388 time: 0.46s
Epoch 97/1000, LR 0.000265
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 0.72s
Val loss: 0.3468 score: 0.8776 time: 1.94s
Test loss: 0.2689 score: 0.9388 time: 4.65s
Epoch 98/1000, LR 0.000265
Train loss: 0.1900;  Loss pred: 0.1900; Loss self: 0.0000; time: 6.93s
Val loss: 0.3432 score: 0.8776 time: 0.36s
Test loss: 0.2651 score: 0.9388 time: 0.45s
Epoch 99/1000, LR 0.000265
Train loss: 0.1844;  Loss pred: 0.1844; Loss self: 0.0000; time: 0.76s
Val loss: 0.3398 score: 0.8776 time: 0.34s
Test loss: 0.2615 score: 0.9388 time: 0.47s
Epoch 100/1000, LR 0.000265
Train loss: 0.1695;  Loss pred: 0.1695; Loss self: 0.0000; time: 0.57s
Val loss: 0.3365 score: 0.8776 time: 0.46s
Test loss: 0.2582 score: 0.9388 time: 0.52s
Epoch 101/1000, LR 0.000265
Train loss: 0.1684;  Loss pred: 0.1684; Loss self: 0.0000; time: 9.03s
Val loss: 0.3333 score: 0.8776 time: 4.58s
Test loss: 0.2554 score: 0.9388 time: 3.65s
Epoch 102/1000, LR 0.000264
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.60s
Val loss: 0.3303 score: 0.8776 time: 0.38s
Test loss: 0.2529 score: 0.9388 time: 0.52s
Epoch 103/1000, LR 0.000264
Train loss: 0.1421;  Loss pred: 0.1421; Loss self: 0.0000; time: 0.71s
Val loss: 0.3276 score: 0.8776 time: 0.38s
Test loss: 0.2508 score: 0.9388 time: 0.50s
Epoch 104/1000, LR 0.000264
Train loss: 0.1357;  Loss pred: 0.1357; Loss self: 0.0000; time: 10.00s
Val loss: 0.3250 score: 0.8776 time: 0.39s
Test loss: 0.2488 score: 0.9388 time: 0.53s
Epoch 105/1000, LR 0.000264
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.91s
Val loss: 0.3228 score: 0.8776 time: 0.37s
Test loss: 0.2472 score: 0.9388 time: 0.49s
Epoch 106/1000, LR 0.000264
Train loss: 0.1215;  Loss pred: 0.1215; Loss self: 0.0000; time: 6.36s
Val loss: 0.3213 score: 0.8776 time: 2.72s
Test loss: 0.2463 score: 0.9388 time: 0.70s
Epoch 107/1000, LR 0.000264
Train loss: 0.1168;  Loss pred: 0.1168; Loss self: 0.0000; time: 0.61s
Val loss: 0.3203 score: 0.8571 time: 0.45s
Test loss: 0.2460 score: 0.9388 time: 0.48s
Epoch 108/1000, LR 0.000264
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.57s
Val loss: 0.3200 score: 0.8571 time: 0.37s
Test loss: 0.2465 score: 0.9388 time: 0.50s
Epoch 109/1000, LR 0.000264
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 9.19s
Val loss: 0.3196 score: 0.8571 time: 1.99s
Test loss: 0.2476 score: 0.9388 time: 0.49s
Epoch 110/1000, LR 0.000263
Train loss: 0.0955;  Loss pred: 0.0955; Loss self: 0.0000; time: 0.60s
Val loss: 0.3205 score: 0.8571 time: 0.35s
Test loss: 0.2494 score: 0.9388 time: 0.48s
     INFO: Early stopping counter 1 of 2
Epoch 111/1000, LR 0.000263
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.70s
Val loss: 0.3228 score: 0.8571 time: 0.36s
Test loss: 0.2520 score: 0.9592 time: 0.47s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.1059,   Val_Loss: 0.3196,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3196,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2476


[0.3777761060046032, 0.3790616230107844, 2.786415035952814, 0.41136884095612913, 0.4038271229946986, 4.627412057016045, 0.3620967019814998, 0.37637127796187997, 0.39077343803364784, 0.39419224904850125, 0.36492536508012563, 0.4309730059467256, 3.0074319769628346, 0.370179157005623, 0.37112932198215276, 4.595855289022438, 0.3693024090025574, 0.36723589594475925, 5.154684701003134, 0.3763011069968343, 0.39813246799167246, 0.3578430518973619, 4.303114828071557, 0.35605985298752785, 0.4013199550099671, 4.599332981975749, 0.3686455700080842, 0.3621312740724534, 4.012088172021322, 0.3722999739693478, 0.3889803399797529, 0.3733879759674892, 3.269251944962889, 0.3593425879953429, 0.37461338902357966, 2.2108459708979353, 0.5017428619321436, 0.3794344230554998, 2.126986271003261, 0.35201983200386167, 0.3932023770175874, 3.836485236068256, 0.35173511097673327, 0.35361694591119885, 0.37787731701973826, 4.845903299981728, 0.37951324589084834, 0.36902304110117257, 2.602608363959007, 0.3676262029912323, 0.36793721304275095, 2.691094732028432, 0.3862986390013248, 0.36696887004654855, 0.3928933870047331, 0.3922393099637702, 3.5486341340001673, 0.4439318709773943, 0.34465736406855285, 2.1390638659941033, 0.9971110880142078, 0.3783165679778904, 0.3722632459830493, 3.864240511902608, 0.4035761100240052, 0.3958305970299989, 3.146391554037109, 0.40013766090851277, 0.37553239392582327, 0.3719928580103442, 3.205471045919694, 0.36205872998107225, 0.393173013930209, 3.252548294956796, 0.3847858349326998, 0.3729683639248833, 5.411424473975785, 0.3697744640521705, 0.3705024280352518, 3.5431408260483295, 0.4204563299426809, 0.3683441929752007, 1.148615877958946, 0.3686072949785739, 0.37964703794568777, 1.549738356960006, 3.3918030289933085, 0.3682931129587814, 0.36520313599612564, 1.2562500489875674, 0.36508860101457685, 0.40720906504429877, 4.629580554901622, 0.5844746680231765, 0.4097218499518931, 1.8720070999115705, 0.3846245059976354, 0.3634586801053956, 0.3545837310375646, 0.3986363069852814, 0.3951238470617682, 0.35645965801086277, 4.183872628957033, 0.3634236470097676, 0.40389715996570885, 3.7545361389638856, 0.6113258369732648, 0.5627237090375274, 4.097201204975136, 0.473263118066825, 0.5641126229893416, 4.152734710020013, 0.4615614659851417, 0.4760454329662025, 3.803730932995677, 0.49061490898020566, 0.5950429160147905, 2.9876599450362846, 0.5600727580022067, 0.4878741109278053, 0.5694202949525788, 0.4808080740040168, 0.4875251139746979, 0.4801292399642989, 3.2477885209955275, 0.4793422990478575, 0.5763029590016231, 3.9862126719672233, 0.5676628580549732, 0.5773157129297033, 3.4026318170363083, 0.5665845810435712, 4.306491803028621, 0.47038686997257173, 0.6202325940830633, 0.5749479559017345, 1.0237082300009206, 0.46766636602114886, 0.5228239250136539, 3.666696937987581, 0.47749616706278175, 0.4904723430518061, 0.5435318569652736, 0.5158112730132416, 0.46015978406649083, 0.49094421695917845, 0.5531350480159745, 0.44802563497796655, 4.060522946063429, 0.49801144003868103, 0.5617907070554793, 4.111833535018377, 0.5720123689388856, 4.477530878968537, 0.49473247199784964, 0.4631636970443651, 0.51411188300699, 0.4549887489993125, 0.5027904460439458, 4.875286253984086, 0.4695235879626125, 0.5043511510593817, 0.529717039084062, 0.47803630598355085, 0.46407729003112763, 1.0875958499964327, 0.4833403299562633, 0.543771211989224, 4.044057604973204, 0.48541095294058323, 0.5687005169456825, 2.2133491080021486, 0.49968248896766454, 0.5766095950966701, 0.571397188003175, 0.4868855520617217, 0.5535417259670794, 4.264622948947363, 0.4774314110400155, 0.4642693739151582, 0.48034105403348804, 0.5102781229652464, 0.4786498199682683, 3.1173096890561283, 0.5248166730161756, 0.4868512190878391, 4.238642144948244, 0.47180149203632027, 0.47809490899089724, 1.8852308940840885, 0.4730623729992658, 0.46705394505988806, 0.46762691996991634, 0.4720029599266127, 0.613970318925567, 0.7367473680060357, 0.4742600219324231, 0.5307398380246013, 0.5673495159717277, 0.5116743190446869, 0.46090186201035976, 4.658071928890422, 0.45320023293606937, 0.4738401580834761, 0.5221104060765356, 3.649934957968071, 0.5199309180025011, 0.509675545967184, 0.5387777700088918, 0.4983004729729146, 0.7069041879149154, 0.4871378350071609, 0.5033549179788679, 0.49035457393620163, 0.4836082149995491, 0.4783208209555596]
[0.007709716449073535, 0.007735951490016008, 0.056865612978628854, 0.008395282468492431, 0.008241369857034666, 0.09443698075542949, 0.007389728611867342, 0.0076810464890179585, 0.00797496812313567, 0.008044739776500026, 0.007447456430206645, 0.008795367468300522, 0.06137616279515989, 0.007554676673584142, 0.007574067795554138, 0.09379296508209058, 0.007536783857195049, 0.007494610121321617, 0.10519764695924763, 0.007679614428506822, 0.008125152407993316, 0.007302919426476773, 0.08781866996064402, 0.007266527611990364, 0.008190203163468716, 0.09386393840766835, 0.00752337897975682, 0.007390434164743946, 0.08187935044941473, 0.007597958652435669, 0.00793837428530108, 0.007620162774846718, 0.06671942744822222, 0.00733352220398659, 0.00764517120456285, 0.045119305528529295, 0.010239650243513134, 0.007743559654193873, 0.04340788308169921, 0.007184078204160442, 0.008024538306481376, 0.07829561706261748, 0.00717826757095374, 0.007216672365534671, 0.007711781979994658, 0.09889598571391282, 0.0077451682834867005, 0.007531082471452502, 0.05311445640732667, 0.007502575571249638, 0.007508922715158183, 0.05492030065364147, 0.007883645693904586, 0.007489160613194868, 0.008018232387851695, 0.008004883876811636, 0.07242110477551361, 0.009059834101579475, 0.007033823756501079, 0.043654364612124553, 0.020349205877840976, 0.007720746285263069, 0.007597209101694883, 0.07886205126331854, 0.008236247143347045, 0.008078175449591815, 0.06421207253136957, 0.008166074712418628, 0.007663926406649454, 0.007591690979802943, 0.06541777644734069, 0.007388953673083107, 0.008023939059800183, 0.06637853663177135, 0.00785277214148367, 0.0076115992637731285, 0.11043723416277113, 0.0075464176337177655, 0.007561274041535751, 0.07230899644996591, 0.008580741427401652, 0.007517228428065321, 0.0234411403665091, 0.00752259785670559, 0.0077478987335854645, 0.03162731340734706, 0.06922046997945527, 0.007516185978750641, 0.0074531252244107275, 0.02563775610178709, 0.007450787775807691, 0.00831038908253671, 0.09448123581431882, 0.011928054449452581, 0.008361670407181492, 0.03820422652880756, 0.007849479714237457, 0.007417524083783584, 0.007236402674236012, 0.008135434836434315, 0.008063751980852412, 0.007274686898180872, 0.08538515569300067, 0.0074168091226483185, 0.00824279918297365, 0.07662318650946705, 0.012476037489250302, 0.011484157327296478, 0.08361635112194155, 0.009658430980955611, 0.011512502509986564, 0.0847496879595921, 0.00941962175479881, 0.009715212917677601, 0.07762716189787096, 0.01001254916286134, 0.012143732979893684, 0.06097265193951601, 0.01143005628575932, 0.00995661450873072, 0.011620822345970996, 0.009812409673551363, 0.00994949212193261, 0.009798555917638754, 0.06628139838766382, 0.009782495898935867, 0.011761284877584145, 0.08135127901973925, 0.011584956286836187, 0.011781953325095986, 0.06944146565380221, 0.01156295063354227, 0.08788758781691063, 0.009599732040256565, 0.012657808042511496, 0.011733631753096623, 0.02089200469389634, 0.009544211551452018, 0.010669876020686815, 0.0748305497548486, 0.009744819735975139, 0.010009639654118491, 0.01109248687684232, 0.010526760673739624, 0.009391016001356957, 0.010019269733860785, 0.011288470367672948, 0.009143380305672787, 0.08286781522578426, 0.010163498776299613, 0.011465116470519985, 0.08391497010241586, 0.011673721815079299, 0.09137818120343953, 0.010096581061180604, 0.009452320347844186, 0.01049207924504061, 0.009285484673455358, 0.010261029511100936, 0.09949563783640993, 0.009582114040053316, 0.010292880633864932, 0.010810551818042082, 0.00975584297925614, 0.009470965102676074, 0.022195833673396587, 0.009864088366454353, 0.011097371673249468, 0.08253178785659601, 0.00990634597837925, 0.011606132998891478, 0.04517038995922752, 0.010197601815666623, 0.0117675427570749, 0.011661167102105612, 0.00993643983799432, 0.011296769917695498, 0.08703312140708903, 0.009743498184490112, 0.009474885181942004, 0.009802878653744653, 0.010413839244188703, 0.009768363672821802, 0.06361856508277813, 0.01071054434726889, 0.009935739165057942, 0.08650290091731111, 0.00962860187829225, 0.009757038958997903, 0.038474099879267115, 0.009654334142842159, 0.009531713164487511, 0.009543406529998293, 0.009632713467890054, 0.01253000650868504, 0.015035660571551748, 0.009678775957804553, 0.01083142526580819, 0.011578561550443421, 0.010442333041728303, 0.009406160449191016, 0.09506269242633514, 0.009248984345634068, 0.009670207307826043, 0.010655314409725216, 0.07448846852996063, 0.010610835061275534, 0.010401541754432326, 0.010995464694059017, 0.010169397407610501, 0.014426616079896232, 0.009941588469533896, 0.010272549346507507, 0.010007236202779626, 0.009869555408154063, 0.009761649407256318]
[129.70645634057897, 129.26658101341465, 17.585319978451977, 119.11451505687971, 121.33905131638042, 10.58907211984863, 135.3229668534885, 130.19059335596504, 125.39235073541722, 124.30482871815944, 134.27403159339502, 113.69621605965989, 16.29297033992584, 132.36833860761018, 132.0294492989599, 10.661780434436295, 132.6825896758791, 133.42922230938646, 9.505916043800756, 130.21487072163256, 123.07461445476713, 136.9315395120607, 11.387100265218665, 137.6173123391038, 122.09709332490841, 10.653718744005989, 132.91899858968998, 135.31004778724616, 12.213091512222029, 131.61429875370948, 125.97037681274723, 131.23079251021844, 14.988138211708314, 136.3601243964855, 130.80151814038805, 22.163461699730554, 97.65958565171742, 129.13957464748205, 23.037290211040045, 139.1967029842298, 124.61776139722649, 12.772107015904133, 139.30937933358967, 138.5680199056552, 129.67171564161526, 10.111633882620968, 129.1127530607795, 132.78303667376153, 18.8272660145696, 133.2875611186198, 133.1748957784997, 18.208203307308285, 126.84486832953083, 133.5263124465695, 124.71576672123959, 124.9237359828263, 13.808129592882311, 110.37729706614184, 142.17018148567348, 22.907216927451515, 49.14196681694287, 129.52115806586525, 131.6272839952381, 12.680370139764987, 121.41452078787673, 123.79032941783029, 15.573395478108408, 122.45785585077265, 130.48141995888292, 131.7229590430401, 15.286364873697126, 135.33715925745426, 124.62706814536831, 15.065110662915114, 127.3435650472171, 131.37843511539415, 9.05491709912004, 132.51320673427227, 132.25284449509155, 13.829537804358111, 116.54004592267637, 133.0277521255751, 42.66004061085369, 132.9328004830947, 129.06725221707097, 31.618240446806297, 14.446593620309155, 133.0462022663019, 134.171903716949, 39.0049736033761, 134.21399590080213, 120.33130940901198, 10.584112193084248, 83.83596874391309, 119.59332900052381, 26.175114401176497, 127.39697870499505, 134.81587504194698, 138.19020928179287, 122.91905965757704, 124.01175065583936, 137.46296081142168, 11.711637601218001, 134.82887094213504, 121.31801076333424, 13.050879838786743, 80.15365462484604, 87.07648036335402, 11.959383381148196, 103.53648558153897, 86.86208746817178, 11.799453473820353, 106.1613752686568, 102.93135193984483, 12.882088891973604, 99.87466565549673, 82.34700167203073, 16.40079557293958, 87.48863303900765, 100.43574541558515, 86.05242987357995, 101.9117661480673, 100.50764277662023, 102.05585480201853, 15.08718923145288, 102.22340089187047, 85.02472394881804, 12.292369733453826, 86.31884102456951, 84.87556964514228, 14.400617708523924, 86.48311591845423, 11.378170966339663, 104.16957429712524, 79.00262009358022, 85.2251051543432, 47.86520081015265, 104.77554846820888, 93.72180127127949, 13.363526036840398, 102.61862477643182, 99.90369629225837, 90.15110958460457, 94.9959850891861, 106.48475094233733, 99.80767326988253, 88.58596137734683, 109.36874181855637, 12.067411180991902, 98.39131405534404, 87.22109387822437, 11.916824838041748, 85.66248329716662, 10.943531451711138, 99.04342806148568, 105.79412918734525, 95.30999305716065, 107.69497071690016, 97.45610797806847, 10.050691887057335, 104.36110401316365, 97.15453191110255, 92.50221605996721, 102.50267476898728, 105.5858604861129, 45.05350034220958, 101.37784282233169, 90.11142723195697, 12.116543527901767, 100.94539421321598, 86.16134246398104, 22.138396434094044, 98.06227170624523, 84.97950852133327, 85.75470973393683, 100.63966735613538, 88.52087873663595, 11.489878609806448, 102.63254337049287, 105.54217605780386, 102.01085164080907, 96.02606460033806, 102.37129098523096, 15.71868209694508, 93.36593618185172, 100.64676451217693, 11.560305948073452, 103.85723832392603, 102.49011039130923, 25.991511254013226, 103.58042151891047, 104.91293461554417, 104.78438667122136, 103.81290830807195, 79.80841824039442, 66.50855113689198, 103.31884985865827, 92.32395326187802, 86.3665141514667, 95.76404008605444, 106.31330449886232, 10.51937384137219, 108.11997973291462, 103.41039940174868, 93.8498819975964, 13.424896762346263, 94.24328945131954, 96.13959388028971, 90.94658823654012, 98.33424340872224, 69.31632438694471, 100.58754725811782, 97.34681881474563, 99.92769029697115, 101.32168660543884, 102.44170408913173]
Elapsed: 1.2041823490365426~1.405464313419778
Time per graph: 0.024575149980337604~0.028682945171832206
Speed: 87.1449722141546~44.05089550621497
Total Time: 0.4790
best val loss: 0.3195776343345642 test_score: 0.9388

Testing...
Test loss: 0.3870 score: 0.9184 time: 0.48s
test Score 0.9184
Epoch Time List: [1.5254303100518882, 1.6452161410124972, 3.8313912278972566, 8.940008548786864, 1.4739857909735292, 5.802249365951866, 9.745023442083038, 1.5057844369439408, 1.4823550340952352, 14.116506763035432, 1.3791411500424147, 1.6482419380918145, 18.28088282479439, 1.5068352241069078, 1.4399947610218078, 8.861998098902404, 8.757078099064529, 1.469383744057268, 8.750164972851053, 12.028789927018806, 1.468567990930751, 1.4752585899550468, 10.306545775965787, 9.96542911499273, 1.4194220198551193, 6.0941390208899975, 4.1067292569205165, 1.6228205768857151, 9.092725037131459, 1.742652386892587, 1.3906361958943307, 1.524723618873395, 8.292830168851651, 4.603940226137638, 1.4726729389512911, 4.666632142965682, 4.631235647015274, 1.5146250571124256, 12.09829104389064, 1.506565086892806, 1.4392736901063472, 6.76649587310385, 3.1390180110465735, 1.4510899900924414, 1.4187746449606493, 12.71111766283866, 1.7924457649933174, 1.5123513179132715, 7.61004760407377, 1.5313597009517252, 1.4056466419715434, 6.174983064061962, 1.4592807448934764, 1.4831951769301668, 11.804629980935715, 1.8600998299662024, 4.64996678812895, 6.985381135949865, 1.3731486740289256, 3.3252886830596253, 16.08337309502531, 1.5413428889587522, 1.507501607062295, 11.182367717963643, 5.249350955011323, 1.769834850099869, 4.23800336685963, 12.147797760087997, 1.389602443901822, 1.4961517330957577, 8.432140898890793, 7.742725523887202, 1.4918990089790896, 4.398861582973041, 10.288134982110932, 1.5000079739838839, 6.552208048873581, 6.057737553841434, 1.5312836680095643, 7.17919100006111, 5.950089815072715, 1.4884320079581812, 2.2003402538830414, 7.804656345047988, 1.6215006868587807, 2.7239643511129543, 14.640630019945092, 4.888931100955233, 1.4344048540806398, 6.8163068409776315, 1.6341969849308953, 1.5690942970104516, 8.53140636102762, 10.011419997899793, 1.4333432308631018, 3.3137192690046504, 6.888070740038529, 1.5279888729564846, 1.4332552410196513, 14.148237054934725, 1.442706924979575, 1.5082274369196966, 11.738000817131251, 1.5885900999419391, 1.4379052519798279, 7.811394697870128, 6.321875995141454, 1.5285032809479162, 5.281766023952514, 5.6391253439942375, 1.589860902982764, 7.606702130986378, 6.340873457025737, 1.4237860190914944, 4.777604103088379, 9.910676470026374, 1.658219775883481, 4.057403387851082, 8.545552224037237, 1.440485060098581, 1.5447869540657848, 10.769115512957796, 1.6719725261209533, 1.4382960530929267, 7.8538108200300485, 9.688694001175463, 1.5392634050222114, 8.113021235913038, 10.248319830861874, 1.63823283102829, 4.86717198707629, 1.630552786984481, 5.38526588503737, 4.889825096121058, 1.5691232479875907, 1.5479457471519709, 9.028319863136858, 1.5519310898380354, 1.4484785880194977, 8.904907518881373, 1.4419797020964324, 1.6304030440514907, 1.4983604040462524, 13.289437215891667, 1.586415136931464, 1.492007158929482, 11.584452339913696, 1.4101301908958703, 5.002479778137058, 12.819062803988345, 1.6698864481877536, 11.148671100032516, 1.7454386811004952, 10.685491576907225, 12.348722919006832, 1.4798599248751998, 1.4359881491400301, 6.094888056162745, 1.4331249609822407, 5.983243820024654, 14.39485719113145, 1.519515363033861, 1.484580494929105, 10.18344877101481, 1.5438596439780667, 9.611940188799053, 1.5385236708680168, 1.6169625098118559, 7.710626647109166, 1.5527552472194657, 1.6637780430028215, 7.65183314983733, 1.476578967878595, 1.6155887380009517, 10.019765510922298, 1.4382161089451984, 1.5108899280894548, 8.148132972884923, 7.132788182003424, 1.450414114864543, 1.4618789941305295, 8.714084984036162, 1.4397605119738728, 4.19868014100939, 7.102808495052159, 1.6120865530101582, 9.15729914989788, 5.144495054031722, 1.4743123319931328, 3.4213438000297174, 10.380595745053142, 1.6877442591357976, 9.671946015907452, 1.5784707890124992, 1.5252138811629266, 11.300455593154766, 1.5070246087852865, 1.5366083019180223, 9.691652027890086, 1.4747626029420644, 1.4424127989914268, 7.318764396826737, 7.741440181038342, 1.5720585800008848, 1.5514833240304142, 17.26133695698809, 1.4867297220043838, 1.5855045161442831, 10.919145227991976, 1.7722139989491552, 9.77812906098552, 1.5441915368428454, 1.4358210588106886, 11.668196903076023, 1.431575497961603, 1.5402973669115454]
Total Epoch List: [105, 111]
Total Time List: [0.40440532693173736, 0.47902564494870603]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3df0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.55s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.51s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 4.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 4.12s
Epoch 4/1000, LR 0.000060
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.52s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.59s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 8.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.45s
Epoch 7/1000, LR 0.000150
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.52s
Epoch 8/1000, LR 0.000180
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 2.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 3.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.53s
Epoch 10/1000, LR 0.000240
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.45s
Epoch 11/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 2.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 4.27s
Epoch 12/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 5.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.46s
Epoch 13/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.41s
Epoch 14/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 3.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 1.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 10.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.42s
Epoch 16/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.44s
Epoch 17/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.60s
Val loss: 0.6910 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 4.91s
Epoch 18/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 2.98s
Val loss: 0.6905 score: 0.5102 time: 0.47s
Test loss: 0.6897 score: 0.5417 time: 0.40s
Epoch 19/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.58s
Val loss: 0.6900 score: 0.5918 time: 0.49s
Test loss: 0.6890 score: 0.5625 time: 0.42s
Epoch 20/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.62s
Val loss: 0.6893 score: 0.6122 time: 0.40s
Test loss: 0.6883 score: 0.6250 time: 0.46s
Epoch 21/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 10.52s
Val loss: 0.6886 score: 0.6327 time: 2.64s
Test loss: 0.6875 score: 0.6667 time: 0.42s
Epoch 22/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.60s
Val loss: 0.6879 score: 0.6735 time: 0.41s
Test loss: 0.6867 score: 0.6667 time: 0.43s
Epoch 23/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.59s
Val loss: 0.6871 score: 0.6735 time: 0.45s
Test loss: 0.6857 score: 0.7708 time: 0.70s
Epoch 24/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 10.28s
Val loss: 0.6863 score: 0.6735 time: 0.42s
Test loss: 0.6847 score: 0.8125 time: 0.47s
Epoch 25/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.63s
Val loss: 0.6854 score: 0.7143 time: 0.40s
Test loss: 0.6836 score: 0.7708 time: 0.52s
Epoch 26/1000, LR 0.000270
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.57s
Val loss: 0.6844 score: 0.7143 time: 0.40s
Test loss: 0.6824 score: 0.7292 time: 0.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 7.09s
Val loss: 0.6834 score: 0.6939 time: 4.54s
Test loss: 0.6812 score: 0.6875 time: 2.80s
Epoch 28/1000, LR 0.000270
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.64s
Val loss: 0.6822 score: 0.6735 time: 0.40s
Test loss: 0.6799 score: 0.6875 time: 0.42s
Epoch 29/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.59s
Val loss: 0.6810 score: 0.6327 time: 0.41s
Test loss: 0.6784 score: 0.7083 time: 0.53s
Epoch 30/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.60s
Val loss: 0.6797 score: 0.6735 time: 0.47s
Test loss: 0.6767 score: 0.7083 time: 2.67s
Epoch 31/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 8.62s
Val loss: 0.6782 score: 0.6531 time: 0.52s
Test loss: 0.6749 score: 0.6875 time: 0.52s
Epoch 32/1000, LR 0.000270
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.60s
Val loss: 0.6766 score: 0.6531 time: 0.39s
Test loss: 0.6730 score: 0.6875 time: 0.42s
Epoch 33/1000, LR 0.000270
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.71s
Val loss: 0.6748 score: 0.6531 time: 0.40s
Test loss: 0.6708 score: 0.7083 time: 0.52s
Epoch 34/1000, LR 0.000270
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 4.67s
Val loss: 0.6728 score: 0.6735 time: 5.20s
Test loss: 0.6684 score: 0.7292 time: 4.73s
Epoch 35/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 4.21s
Val loss: 0.6706 score: 0.6735 time: 0.55s
Test loss: 0.6658 score: 0.7292 time: 0.42s
Epoch 36/1000, LR 0.000270
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.62s
Val loss: 0.6682 score: 0.6939 time: 0.40s
Test loss: 0.6630 score: 0.7292 time: 0.41s
Epoch 37/1000, LR 0.000270
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.69s
Val loss: 0.6657 score: 0.6939 time: 4.53s
Test loss: 0.6599 score: 0.7292 time: 4.49s
Epoch 38/1000, LR 0.000270
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 1.20s
Val loss: 0.6629 score: 0.6939 time: 0.40s
Test loss: 0.6566 score: 0.7500 time: 0.43s
Epoch 39/1000, LR 0.000269
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.58s
Val loss: 0.6600 score: 0.7143 time: 0.51s
Test loss: 0.6530 score: 0.7500 time: 0.43s
Epoch 40/1000, LR 0.000269
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.58s
Val loss: 0.6568 score: 0.7143 time: 0.40s
Test loss: 0.6492 score: 0.7500 time: 4.31s
Epoch 41/1000, LR 0.000269
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 8.06s
Val loss: 0.6534 score: 0.7143 time: 0.55s
Test loss: 0.6451 score: 0.7500 time: 0.53s
Epoch 42/1000, LR 0.000269
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.71s
Val loss: 0.6497 score: 0.7143 time: 0.40s
Test loss: 0.6406 score: 0.7500 time: 0.43s
Epoch 43/1000, LR 0.000269
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.58s
Val loss: 0.6458 score: 0.7143 time: 0.53s
Test loss: 0.6359 score: 0.7500 time: 3.03s
Epoch 44/1000, LR 0.000269
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 6.22s
Val loss: 0.6416 score: 0.7143 time: 0.41s
Test loss: 0.6308 score: 0.7708 time: 0.43s
Epoch 45/1000, LR 0.000269
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.66s
Val loss: 0.6371 score: 0.7143 time: 0.50s
Test loss: 0.6253 score: 0.7708 time: 0.41s
Epoch 46/1000, LR 0.000269
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.56s
Val loss: 0.6323 score: 0.7143 time: 0.40s
Test loss: 0.6194 score: 0.7708 time: 3.81s
Epoch 47/1000, LR 0.000269
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 8.91s
Val loss: 0.6271 score: 0.7143 time: 2.03s
Test loss: 0.6131 score: 0.7708 time: 2.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 1.84s
Val loss: 0.6216 score: 0.7347 time: 0.42s
Test loss: 0.6064 score: 0.7708 time: 0.43s
Epoch 49/1000, LR 0.000269
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.66s
Val loss: 0.6157 score: 0.7347 time: 0.40s
Test loss: 0.5993 score: 0.8125 time: 0.51s
Epoch 50/1000, LR 0.000269
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.59s
Val loss: 0.6094 score: 0.7551 time: 0.39s
Test loss: 0.5918 score: 0.8125 time: 0.47s
Epoch 51/1000, LR 0.000269
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 10.23s
Val loss: 0.6028 score: 0.7551 time: 0.45s
Test loss: 0.5838 score: 0.8125 time: 0.64s
Epoch 52/1000, LR 0.000269
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 0.67s
Val loss: 0.5957 score: 0.7755 time: 0.44s
Test loss: 0.5753 score: 0.8125 time: 0.42s
Epoch 53/1000, LR 0.000269
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.73s
Val loss: 0.5883 score: 0.7755 time: 3.93s
Test loss: 0.5664 score: 0.8125 time: 3.30s
Epoch 54/1000, LR 0.000269
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 4.63s
Val loss: 0.5804 score: 0.7755 time: 0.41s
Test loss: 0.5570 score: 0.8125 time: 0.44s
Epoch 55/1000, LR 0.000269
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.64s
Val loss: 0.5722 score: 0.7959 time: 0.41s
Test loss: 0.5472 score: 0.8125 time: 0.51s
Epoch 56/1000, LR 0.000269
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 7.53s
Val loss: 0.5635 score: 0.7959 time: 2.99s
Test loss: 0.5369 score: 0.8333 time: 0.45s
Epoch 57/1000, LR 0.000269
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.67s
Val loss: 0.5543 score: 0.7959 time: 0.43s
Test loss: 0.5260 score: 0.8542 time: 0.59s
Epoch 58/1000, LR 0.000269
Train loss: 0.5055;  Loss pred: 0.5055; Loss self: 0.0000; time: 0.61s
Val loss: 0.5448 score: 0.7959 time: 0.43s
Test loss: 0.5148 score: 0.8542 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.4850;  Loss pred: 0.4850; Loss self: 0.0000; time: 0.64s
Val loss: 0.5349 score: 0.7959 time: 0.57s
Test loss: 0.5032 score: 0.8750 time: 3.62s
Epoch 60/1000, LR 0.000268
Train loss: 0.4716;  Loss pred: 0.4716; Loss self: 0.0000; time: 2.51s
Val loss: 0.5247 score: 0.7959 time: 0.46s
Test loss: 0.4911 score: 0.8958 time: 0.48s
Epoch 61/1000, LR 0.000268
Train loss: 0.4655;  Loss pred: 0.4655; Loss self: 0.0000; time: 0.56s
Val loss: 0.5141 score: 0.8163 time: 0.49s
Test loss: 0.4788 score: 0.9167 time: 0.44s
Epoch 62/1000, LR 0.000268
Train loss: 0.4462;  Loss pred: 0.4462; Loss self: 0.0000; time: 0.63s
Val loss: 0.5032 score: 0.8163 time: 3.72s
Test loss: 0.4662 score: 0.9375 time: 3.11s
Epoch 63/1000, LR 0.000268
Train loss: 0.4329;  Loss pred: 0.4329; Loss self: 0.0000; time: 0.61s
Val loss: 0.4920 score: 0.8571 time: 0.53s
Test loss: 0.4533 score: 0.9375 time: 0.43s
Epoch 64/1000, LR 0.000268
Train loss: 0.4202;  Loss pred: 0.4202; Loss self: 0.0000; time: 0.60s
Val loss: 0.4806 score: 0.8571 time: 0.40s
Test loss: 0.4403 score: 0.9375 time: 0.42s
Epoch 65/1000, LR 0.000268
Train loss: 0.4019;  Loss pred: 0.4019; Loss self: 0.0000; time: 0.59s
Val loss: 0.4690 score: 0.8571 time: 0.62s
Test loss: 0.4274 score: 0.9375 time: 3.44s
Epoch 66/1000, LR 0.000268
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 7.96s
Val loss: 0.4575 score: 0.8571 time: 0.49s
Test loss: 0.4145 score: 0.9375 time: 0.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.3737;  Loss pred: 0.3737; Loss self: 0.0000; time: 0.68s
Val loss: 0.4460 score: 0.8571 time: 0.51s
Test loss: 0.4019 score: 0.9375 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3650;  Loss pred: 0.3650; Loss self: 0.0000; time: 3.59s
Val loss: 0.4348 score: 0.8776 time: 4.49s
Test loss: 0.3896 score: 0.9375 time: 4.12s
Epoch 69/1000, LR 0.000268
Train loss: 0.3367;  Loss pred: 0.3367; Loss self: 0.0000; time: 6.61s
Val loss: 0.4237 score: 0.8980 time: 0.50s
Test loss: 0.3774 score: 0.9583 time: 0.44s
Epoch 70/1000, LR 0.000268
Train loss: 0.3277;  Loss pred: 0.3277; Loss self: 0.0000; time: 0.60s
Val loss: 0.4129 score: 0.8980 time: 0.40s
Test loss: 0.3656 score: 0.9583 time: 0.43s
Epoch 71/1000, LR 0.000268
Train loss: 0.3133;  Loss pred: 0.3133; Loss self: 0.0000; time: 0.57s
Val loss: 0.4022 score: 0.9184 time: 0.40s
Test loss: 0.3538 score: 0.9583 time: 0.51s
Epoch 72/1000, LR 0.000267
Train loss: 0.2962;  Loss pred: 0.2962; Loss self: 0.0000; time: 0.62s
Val loss: 0.3918 score: 0.9184 time: 0.46s
Test loss: 0.3421 score: 0.9583 time: 3.13s
Epoch 73/1000, LR 0.000267
Train loss: 0.2824;  Loss pred: 0.2824; Loss self: 0.0000; time: 12.18s
Val loss: 0.3817 score: 0.9184 time: 1.98s
Test loss: 0.3305 score: 0.9583 time: 1.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.2629;  Loss pred: 0.2629; Loss self: 0.0000; time: 0.61s
Val loss: 0.3722 score: 0.9184 time: 0.45s
Test loss: 0.3195 score: 0.9583 time: 0.42s
Epoch 75/1000, LR 0.000267
Train loss: 0.2522;  Loss pred: 0.2522; Loss self: 0.0000; time: 0.57s
Val loss: 0.3631 score: 0.9184 time: 0.40s
Test loss: 0.3090 score: 0.9583 time: 0.51s
Epoch 76/1000, LR 0.000267
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.57s
Val loss: 0.3545 score: 0.9184 time: 0.41s
Test loss: 0.2992 score: 0.9583 time: 1.04s
Epoch 77/1000, LR 0.000267
Train loss: 0.2201;  Loss pred: 0.2201; Loss self: 0.0000; time: 10.73s
Val loss: 0.3464 score: 0.9184 time: 1.40s
Test loss: 0.2897 score: 0.9583 time: 0.55s
Epoch 78/1000, LR 0.000267
Train loss: 0.2126;  Loss pred: 0.2126; Loss self: 0.0000; time: 0.60s
Val loss: 0.3387 score: 0.9184 time: 0.48s
Test loss: 0.2806 score: 0.9583 time: 0.46s
Epoch 79/1000, LR 0.000267
Train loss: 0.1980;  Loss pred: 0.1980; Loss self: 0.0000; time: 0.71s
Val loss: 0.3315 score: 0.9184 time: 3.53s
Test loss: 0.2723 score: 0.9375 time: 4.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.1831;  Loss pred: 0.1831; Loss self: 0.0000; time: 0.65s
Val loss: 0.3249 score: 0.9184 time: 0.39s
Test loss: 0.2647 score: 0.9167 time: 0.42s
Epoch 81/1000, LR 0.000267
Train loss: 0.1644;  Loss pred: 0.1644; Loss self: 0.0000; time: 0.60s
Val loss: 0.3186 score: 0.9184 time: 0.40s
Test loss: 0.2571 score: 0.9167 time: 0.54s
Epoch 82/1000, LR 0.000267
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.59s
Val loss: 0.3128 score: 0.9184 time: 0.41s
Test loss: 0.2499 score: 0.9167 time: 0.77s
Epoch 83/1000, LR 0.000266
Train loss: 0.1461;  Loss pred: 0.1461; Loss self: 0.0000; time: 9.93s
Val loss: 0.3077 score: 0.9184 time: 0.87s
Test loss: 0.2434 score: 0.9167 time: 0.41s
Epoch 84/1000, LR 0.000266
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.69s
Val loss: 0.3032 score: 0.9184 time: 0.51s
Test loss: 0.2376 score: 0.9167 time: 0.42s
Epoch 85/1000, LR 0.000266
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.70s
Val loss: 0.2992 score: 0.9184 time: 0.49s
Test loss: 0.2320 score: 0.9167 time: 4.34s
Epoch 86/1000, LR 0.000266
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 12.29s
Val loss: 0.2960 score: 0.9184 time: 0.43s
Test loss: 0.2274 score: 0.9167 time: 0.44s
Epoch 87/1000, LR 0.000266
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.60s
Val loss: 0.2933 score: 0.9184 time: 0.50s
Test loss: 0.2230 score: 0.9167 time: 0.45s
Epoch 88/1000, LR 0.000266
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.67s
Val loss: 0.2914 score: 0.9184 time: 0.47s
Test loss: 0.2196 score: 0.9167 time: 2.54s
Epoch 89/1000, LR 0.000266
Train loss: 0.0877;  Loss pred: 0.0877; Loss self: 0.0000; time: 8.30s
Val loss: 0.2901 score: 0.9184 time: 0.41s
Test loss: 0.2166 score: 0.9167 time: 0.53s
Epoch 90/1000, LR 0.000266
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.63s
Val loss: 0.2902 score: 0.9184 time: 0.38s
Test loss: 0.2156 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 1 of 2
Epoch 91/1000, LR 0.000266
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.60s
Val loss: 0.2913 score: 0.9184 time: 5.27s
Test loss: 0.2158 score: 0.9167 time: 3.57s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.0877,   Val_Loss: 0.2901,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2901,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2166


[0.3777761060046032, 0.3790616230107844, 2.786415035952814, 0.41136884095612913, 0.4038271229946986, 4.627412057016045, 0.3620967019814998, 0.37637127796187997, 0.39077343803364784, 0.39419224904850125, 0.36492536508012563, 0.4309730059467256, 3.0074319769628346, 0.370179157005623, 0.37112932198215276, 4.595855289022438, 0.3693024090025574, 0.36723589594475925, 5.154684701003134, 0.3763011069968343, 0.39813246799167246, 0.3578430518973619, 4.303114828071557, 0.35605985298752785, 0.4013199550099671, 4.599332981975749, 0.3686455700080842, 0.3621312740724534, 4.012088172021322, 0.3722999739693478, 0.3889803399797529, 0.3733879759674892, 3.269251944962889, 0.3593425879953429, 0.37461338902357966, 2.2108459708979353, 0.5017428619321436, 0.3794344230554998, 2.126986271003261, 0.35201983200386167, 0.3932023770175874, 3.836485236068256, 0.35173511097673327, 0.35361694591119885, 0.37787731701973826, 4.845903299981728, 0.37951324589084834, 0.36902304110117257, 2.602608363959007, 0.3676262029912323, 0.36793721304275095, 2.691094732028432, 0.3862986390013248, 0.36696887004654855, 0.3928933870047331, 0.3922393099637702, 3.5486341340001673, 0.4439318709773943, 0.34465736406855285, 2.1390638659941033, 0.9971110880142078, 0.3783165679778904, 0.3722632459830493, 3.864240511902608, 0.4035761100240052, 0.3958305970299989, 3.146391554037109, 0.40013766090851277, 0.37553239392582327, 0.3719928580103442, 3.205471045919694, 0.36205872998107225, 0.393173013930209, 3.252548294956796, 0.3847858349326998, 0.3729683639248833, 5.411424473975785, 0.3697744640521705, 0.3705024280352518, 3.5431408260483295, 0.4204563299426809, 0.3683441929752007, 1.148615877958946, 0.3686072949785739, 0.37964703794568777, 1.549738356960006, 3.3918030289933085, 0.3682931129587814, 0.36520313599612564, 1.2562500489875674, 0.36508860101457685, 0.40720906504429877, 4.629580554901622, 0.5844746680231765, 0.4097218499518931, 1.8720070999115705, 0.3846245059976354, 0.3634586801053956, 0.3545837310375646, 0.3986363069852814, 0.3951238470617682, 0.35645965801086277, 4.183872628957033, 0.3634236470097676, 0.40389715996570885, 3.7545361389638856, 0.6113258369732648, 0.5627237090375274, 4.097201204975136, 0.473263118066825, 0.5641126229893416, 4.152734710020013, 0.4615614659851417, 0.4760454329662025, 3.803730932995677, 0.49061490898020566, 0.5950429160147905, 2.9876599450362846, 0.5600727580022067, 0.4878741109278053, 0.5694202949525788, 0.4808080740040168, 0.4875251139746979, 0.4801292399642989, 3.2477885209955275, 0.4793422990478575, 0.5763029590016231, 3.9862126719672233, 0.5676628580549732, 0.5773157129297033, 3.4026318170363083, 0.5665845810435712, 4.306491803028621, 0.47038686997257173, 0.6202325940830633, 0.5749479559017345, 1.0237082300009206, 0.46766636602114886, 0.5228239250136539, 3.666696937987581, 0.47749616706278175, 0.4904723430518061, 0.5435318569652736, 0.5158112730132416, 0.46015978406649083, 0.49094421695917845, 0.5531350480159745, 0.44802563497796655, 4.060522946063429, 0.49801144003868103, 0.5617907070554793, 4.111833535018377, 0.5720123689388856, 4.477530878968537, 0.49473247199784964, 0.4631636970443651, 0.51411188300699, 0.4549887489993125, 0.5027904460439458, 4.875286253984086, 0.4695235879626125, 0.5043511510593817, 0.529717039084062, 0.47803630598355085, 0.46407729003112763, 1.0875958499964327, 0.4833403299562633, 0.543771211989224, 4.044057604973204, 0.48541095294058323, 0.5687005169456825, 2.2133491080021486, 0.49968248896766454, 0.5766095950966701, 0.571397188003175, 0.4868855520617217, 0.5535417259670794, 4.264622948947363, 0.4774314110400155, 0.4642693739151582, 0.48034105403348804, 0.5102781229652464, 0.4786498199682683, 3.1173096890561283, 0.5248166730161756, 0.4868512190878391, 4.238642144948244, 0.47180149203632027, 0.47809490899089724, 1.8852308940840885, 0.4730623729992658, 0.46705394505988806, 0.46762691996991634, 0.4720029599266127, 0.613970318925567, 0.7367473680060357, 0.4742600219324231, 0.5307398380246013, 0.5673495159717277, 0.5116743190446869, 0.46090186201035976, 4.658071928890422, 0.45320023293606937, 0.4738401580834761, 0.5221104060765356, 3.649934957968071, 0.5199309180025011, 0.509675545967184, 0.5387777700088918, 0.4983004729729146, 0.7069041879149154, 0.4871378350071609, 0.5033549179788679, 0.49035457393620163, 0.4836082149995491, 0.4783208209555596, 0.5528676459798589, 0.5144994440488517, 4.1204930120147765, 0.5254142979392782, 0.5993129030102864, 0.45781337399967015, 0.5226494229864329, 3.2515355800278485, 0.5314485561102629, 0.45356539299245924, 4.270501331076957, 0.4632932849926874, 0.41882138000801206, 1.0695503749884665, 0.4280135299777612, 0.4436550539685413, 4.911586113972589, 0.40833886293694377, 0.4215012810891494, 0.4621631820918992, 0.4286393270594999, 0.4387347230222076, 0.7049554949626327, 0.4733806570293382, 0.5204392939340323, 0.43186294997576624, 2.804128305055201, 0.4263709890656173, 0.5340993159916252, 2.6782603400060907, 0.5215985879767686, 0.42748655297327787, 0.5253078640671447, 4.739425305975601, 0.4243975040735677, 0.4164716680534184, 4.496468110010028, 0.4322324029635638, 0.4315735069103539, 4.312621852033772, 0.534865138004534, 0.43462525797076523, 3.0377368920017034, 0.4345686980523169, 0.4123120909789577, 3.8189498650608584, 2.2413278659805655, 0.43907613295596093, 0.5122719570063055, 0.47314692405052483, 0.6432269349461421, 0.4281034809537232, 3.3107836300041527, 0.44691368809435517, 0.5191171619808301, 0.45755517901852727, 0.5958203389309347, 0.46437961503397673, 3.6276406039251015, 0.4878478419268504, 0.4428768189391121, 3.1180353180970997, 0.43355967092793435, 0.42617120302747935, 3.445408085011877, 0.44255690602585673, 0.4599034199491143, 4.126469331094995, 0.44230964698363096, 0.4313542830059305, 0.5108033240539953, 3.1315229180036113, 1.2108461160678416, 0.4209481030702591, 0.5166335150133818, 1.0403024679981172, 0.5588869929779321, 0.4617783509893343, 4.211026781937107, 0.42216567997820675, 0.5479223839938641, 0.7786590229952708, 0.4152848580852151, 0.419424450956285, 4.340823460952379, 0.4442503539612517, 0.45514673995785415, 2.5490488189971074, 0.5360354209551588, 0.4406680850079283, 3.5792798020411283]
[0.007709716449073535, 0.007735951490016008, 0.056865612978628854, 0.008395282468492431, 0.008241369857034666, 0.09443698075542949, 0.007389728611867342, 0.0076810464890179585, 0.00797496812313567, 0.008044739776500026, 0.007447456430206645, 0.008795367468300522, 0.06137616279515989, 0.007554676673584142, 0.007574067795554138, 0.09379296508209058, 0.007536783857195049, 0.007494610121321617, 0.10519764695924763, 0.007679614428506822, 0.008125152407993316, 0.007302919426476773, 0.08781866996064402, 0.007266527611990364, 0.008190203163468716, 0.09386393840766835, 0.00752337897975682, 0.007390434164743946, 0.08187935044941473, 0.007597958652435669, 0.00793837428530108, 0.007620162774846718, 0.06671942744822222, 0.00733352220398659, 0.00764517120456285, 0.045119305528529295, 0.010239650243513134, 0.007743559654193873, 0.04340788308169921, 0.007184078204160442, 0.008024538306481376, 0.07829561706261748, 0.00717826757095374, 0.007216672365534671, 0.007711781979994658, 0.09889598571391282, 0.0077451682834867005, 0.007531082471452502, 0.05311445640732667, 0.007502575571249638, 0.007508922715158183, 0.05492030065364147, 0.007883645693904586, 0.007489160613194868, 0.008018232387851695, 0.008004883876811636, 0.07242110477551361, 0.009059834101579475, 0.007033823756501079, 0.043654364612124553, 0.020349205877840976, 0.007720746285263069, 0.007597209101694883, 0.07886205126331854, 0.008236247143347045, 0.008078175449591815, 0.06421207253136957, 0.008166074712418628, 0.007663926406649454, 0.007591690979802943, 0.06541777644734069, 0.007388953673083107, 0.008023939059800183, 0.06637853663177135, 0.00785277214148367, 0.0076115992637731285, 0.11043723416277113, 0.0075464176337177655, 0.007561274041535751, 0.07230899644996591, 0.008580741427401652, 0.007517228428065321, 0.0234411403665091, 0.00752259785670559, 0.0077478987335854645, 0.03162731340734706, 0.06922046997945527, 0.007516185978750641, 0.0074531252244107275, 0.02563775610178709, 0.007450787775807691, 0.00831038908253671, 0.09448123581431882, 0.011928054449452581, 0.008361670407181492, 0.03820422652880756, 0.007849479714237457, 0.007417524083783584, 0.007236402674236012, 0.008135434836434315, 0.008063751980852412, 0.007274686898180872, 0.08538515569300067, 0.0074168091226483185, 0.00824279918297365, 0.07662318650946705, 0.012476037489250302, 0.011484157327296478, 0.08361635112194155, 0.009658430980955611, 0.011512502509986564, 0.0847496879595921, 0.00941962175479881, 0.009715212917677601, 0.07762716189787096, 0.01001254916286134, 0.012143732979893684, 0.06097265193951601, 0.01143005628575932, 0.00995661450873072, 0.011620822345970996, 0.009812409673551363, 0.00994949212193261, 0.009798555917638754, 0.06628139838766382, 0.009782495898935867, 0.011761284877584145, 0.08135127901973925, 0.011584956286836187, 0.011781953325095986, 0.06944146565380221, 0.01156295063354227, 0.08788758781691063, 0.009599732040256565, 0.012657808042511496, 0.011733631753096623, 0.02089200469389634, 0.009544211551452018, 0.010669876020686815, 0.0748305497548486, 0.009744819735975139, 0.010009639654118491, 0.01109248687684232, 0.010526760673739624, 0.009391016001356957, 0.010019269733860785, 0.011288470367672948, 0.009143380305672787, 0.08286781522578426, 0.010163498776299613, 0.011465116470519985, 0.08391497010241586, 0.011673721815079299, 0.09137818120343953, 0.010096581061180604, 0.009452320347844186, 0.01049207924504061, 0.009285484673455358, 0.010261029511100936, 0.09949563783640993, 0.009582114040053316, 0.010292880633864932, 0.010810551818042082, 0.00975584297925614, 0.009470965102676074, 0.022195833673396587, 0.009864088366454353, 0.011097371673249468, 0.08253178785659601, 0.00990634597837925, 0.011606132998891478, 0.04517038995922752, 0.010197601815666623, 0.0117675427570749, 0.011661167102105612, 0.00993643983799432, 0.011296769917695498, 0.08703312140708903, 0.009743498184490112, 0.009474885181942004, 0.009802878653744653, 0.010413839244188703, 0.009768363672821802, 0.06361856508277813, 0.01071054434726889, 0.009935739165057942, 0.08650290091731111, 0.00962860187829225, 0.009757038958997903, 0.038474099879267115, 0.009654334142842159, 0.009531713164487511, 0.009543406529998293, 0.009632713467890054, 0.01253000650868504, 0.015035660571551748, 0.009678775957804553, 0.01083142526580819, 0.011578561550443421, 0.010442333041728303, 0.009406160449191016, 0.09506269242633514, 0.009248984345634068, 0.009670207307826043, 0.010655314409725216, 0.07448846852996063, 0.010610835061275534, 0.010401541754432326, 0.010995464694059017, 0.010169397407610501, 0.014426616079896232, 0.009941588469533896, 0.010272549346507507, 0.010007236202779626, 0.009869555408154063, 0.009761649407256318, 0.011518075957913728, 0.010718738417684412, 0.08584360441697451, 0.010946131207068296, 0.012485685479380967, 0.009537778624993129, 0.010888529645550685, 0.0677403245839135, 0.01107184491896381, 0.009449279020676235, 0.08896877773076994, 0.009651943437347654, 0.008725445416833585, 0.022282299478926387, 0.008916948541203359, 0.009242813624344611, 0.10232471070776228, 0.008507059644519662, 0.008781276689357279, 0.009628399626914566, 0.008929985980406249, 0.009140306729629325, 0.014686572811721513, 0.009862097021444546, 0.01084248529029234, 0.008997144791161796, 0.05841933968865002, 0.008882728938867027, 0.011127069083158858, 0.05579709041679356, 0.010866637249516012, 0.008905969853609955, 0.01094391383473218, 0.09873802720782503, 0.008841614668199327, 0.008676493084446216, 0.09367641895854224, 0.009004841728407579, 0.00899111472729904, 0.08984628858403691, 0.01114302370842779, 0.009054692874390943, 0.06328618525003549, 0.009053514542756602, 0.008589835228728285, 0.07956145552210121, 0.046694330541261785, 0.009147419436582519, 0.010672332437631363, 0.009857227584385933, 0.013400561144711295, 0.008918822519869233, 0.06897465895841985, 0.009310701835299065, 0.010814940874600628, 0.009532399562885985, 0.012412923727727806, 0.009674575313207848, 0.07557584591510629, 0.010163496706809383, 0.009226600394564835, 0.06495906912702291, 0.009032493144331966, 0.008878566729739154, 0.0717793351044141, 0.009219935542205349, 0.009581321248939881, 0.08596811106447906, 0.009214784312158978, 0.008986547562623551, 0.01064173591779157, 0.0652400607917419, 0.025225960751413368, 0.008769752147297064, 0.010763198229445456, 0.02167296808329411, 0.011643479020373585, 0.009620382312277798, 0.08772972462368973, 0.008795118332879307, 0.011415049666538835, 0.01622206297906814, 0.008651767876775315, 0.008738009394922605, 0.09043382210317456, 0.009255215707526077, 0.009482223749121962, 0.05310518372910641, 0.011167404603232475, 0.00918058510433184, 0.07456832920919017]
[129.70645634057897, 129.26658101341465, 17.585319978451977, 119.11451505687971, 121.33905131638042, 10.58907211984863, 135.3229668534885, 130.19059335596504, 125.39235073541722, 124.30482871815944, 134.27403159339502, 113.69621605965989, 16.29297033992584, 132.36833860761018, 132.0294492989599, 10.661780434436295, 132.6825896758791, 133.42922230938646, 9.505916043800756, 130.21487072163256, 123.07461445476713, 136.9315395120607, 11.387100265218665, 137.6173123391038, 122.09709332490841, 10.653718744005989, 132.91899858968998, 135.31004778724616, 12.213091512222029, 131.61429875370948, 125.97037681274723, 131.23079251021844, 14.988138211708314, 136.3601243964855, 130.80151814038805, 22.163461699730554, 97.65958565171742, 129.13957464748205, 23.037290211040045, 139.1967029842298, 124.61776139722649, 12.772107015904133, 139.30937933358967, 138.5680199056552, 129.67171564161526, 10.111633882620968, 129.1127530607795, 132.78303667376153, 18.8272660145696, 133.2875611186198, 133.1748957784997, 18.208203307308285, 126.84486832953083, 133.5263124465695, 124.71576672123959, 124.9237359828263, 13.808129592882311, 110.37729706614184, 142.17018148567348, 22.907216927451515, 49.14196681694287, 129.52115806586525, 131.6272839952381, 12.680370139764987, 121.41452078787673, 123.79032941783029, 15.573395478108408, 122.45785585077265, 130.48141995888292, 131.7229590430401, 15.286364873697126, 135.33715925745426, 124.62706814536831, 15.065110662915114, 127.3435650472171, 131.37843511539415, 9.05491709912004, 132.51320673427227, 132.25284449509155, 13.829537804358111, 116.54004592267637, 133.0277521255751, 42.66004061085369, 132.9328004830947, 129.06725221707097, 31.618240446806297, 14.446593620309155, 133.0462022663019, 134.171903716949, 39.0049736033761, 134.21399590080213, 120.33130940901198, 10.584112193084248, 83.83596874391309, 119.59332900052381, 26.175114401176497, 127.39697870499505, 134.81587504194698, 138.19020928179287, 122.91905965757704, 124.01175065583936, 137.46296081142168, 11.711637601218001, 134.82887094213504, 121.31801076333424, 13.050879838786743, 80.15365462484604, 87.07648036335402, 11.959383381148196, 103.53648558153897, 86.86208746817178, 11.799453473820353, 106.1613752686568, 102.93135193984483, 12.882088891973604, 99.87466565549673, 82.34700167203073, 16.40079557293958, 87.48863303900765, 100.43574541558515, 86.05242987357995, 101.9117661480673, 100.50764277662023, 102.05585480201853, 15.08718923145288, 102.22340089187047, 85.02472394881804, 12.292369733453826, 86.31884102456951, 84.87556964514228, 14.400617708523924, 86.48311591845423, 11.378170966339663, 104.16957429712524, 79.00262009358022, 85.2251051543432, 47.86520081015265, 104.77554846820888, 93.72180127127949, 13.363526036840398, 102.61862477643182, 99.90369629225837, 90.15110958460457, 94.9959850891861, 106.48475094233733, 99.80767326988253, 88.58596137734683, 109.36874181855637, 12.067411180991902, 98.39131405534404, 87.22109387822437, 11.916824838041748, 85.66248329716662, 10.943531451711138, 99.04342806148568, 105.79412918734525, 95.30999305716065, 107.69497071690016, 97.45610797806847, 10.050691887057335, 104.36110401316365, 97.15453191110255, 92.50221605996721, 102.50267476898728, 105.5858604861129, 45.05350034220958, 101.37784282233169, 90.11142723195697, 12.116543527901767, 100.94539421321598, 86.16134246398104, 22.138396434094044, 98.06227170624523, 84.97950852133327, 85.75470973393683, 100.63966735613538, 88.52087873663595, 11.489878609806448, 102.63254337049287, 105.54217605780386, 102.01085164080907, 96.02606460033806, 102.37129098523096, 15.71868209694508, 93.36593618185172, 100.64676451217693, 11.560305948073452, 103.85723832392603, 102.49011039130923, 25.991511254013226, 103.58042151891047, 104.91293461554417, 104.78438667122136, 103.81290830807195, 79.80841824039442, 66.50855113689198, 103.31884985865827, 92.32395326187802, 86.3665141514667, 95.76404008605444, 106.31330449886232, 10.51937384137219, 108.11997973291462, 103.41039940174868, 93.8498819975964, 13.424896762346263, 94.24328945131954, 96.13959388028971, 90.94658823654012, 98.33424340872224, 69.31632438694471, 100.58754725811782, 97.34681881474563, 99.92769029697115, 101.32168660543884, 102.44170408913173, 86.82005602792798, 93.29456145231985, 11.649091470374726, 91.3564784747204, 80.0917179638566, 104.84621622267109, 91.83976464706821, 14.762255807635633, 90.31918413951085, 105.828179886727, 11.239898147484036, 103.60607752119185, 114.60732973823295, 44.878671563756505, 112.14598754037983, 108.19216319218035, 9.772810429496193, 117.54942856715606, 113.87865744077723, 103.859419919035, 111.98225867253907, 109.4055188277641, 68.08940471134893, 101.39831293745735, 92.22977695854799, 111.14637178922965, 17.11761901674292, 112.57801593206646, 89.87092580502883, 17.922081465721455, 92.02478899758422, 112.28423365869119, 91.37498842748079, 10.127810209284288, 113.10151341436584, 115.25393845961047, 10.675045130008261, 111.0513688258728, 111.22091423922951, 11.130120295004273, 89.74224825921092, 110.4399689610968, 15.801236810990108, 110.45434292697577, 116.41666846595017, 12.568900272597602, 21.41587615473666, 109.32044900016092, 93.700229621215, 101.44840336080117, 74.62374069272934, 112.12242398278623, 14.49807820873507, 107.4032889989844, 92.4646756366967, 104.9053801619332, 80.56119750145687, 103.36371030517358, 13.231740748536135, 98.39133408977402, 108.38228136433388, 15.394309269496613, 110.71140426245616, 112.63079170768134, 13.931586278214278, 108.46062810551997, 104.36973920592051, 11.632220222332968, 108.52125954597682, 111.27743919802478, 93.96963124485478, 15.328005336968971, 39.64170125587671, 114.02830812136591, 92.90918727709081, 46.14042692061255, 85.88498319533319, 103.94597299150703, 11.39864514894384, 113.69943668201044, 87.60364862286322, 61.6444407403875, 115.58331363516885, 114.44254117889483, 11.057809752407847, 108.0471845930969, 105.46049391553309, 18.830553437138573, 89.54632123837833, 108.92551930357381, 13.410519058227136]
Elapsed: 1.2220822078018854~1.4027155431427596
Time per graph: 0.025099823927756913~0.02880887548010398
Speed: 84.63687672037202~42.93677215583564
Total Time: 3.5809
best val loss: 0.29013580083847046 test_score: 0.9167

Testing...
Test loss: 0.3538 score: 0.9583 time: 1.64s
test Score 0.9583
Epoch Time List: [1.5254303100518882, 1.6452161410124972, 3.8313912278972566, 8.940008548786864, 1.4739857909735292, 5.802249365951866, 9.745023442083038, 1.5057844369439408, 1.4823550340952352, 14.116506763035432, 1.3791411500424147, 1.6482419380918145, 18.28088282479439, 1.5068352241069078, 1.4399947610218078, 8.861998098902404, 8.757078099064529, 1.469383744057268, 8.750164972851053, 12.028789927018806, 1.468567990930751, 1.4752585899550468, 10.306545775965787, 9.96542911499273, 1.4194220198551193, 6.0941390208899975, 4.1067292569205165, 1.6228205768857151, 9.092725037131459, 1.742652386892587, 1.3906361958943307, 1.524723618873395, 8.292830168851651, 4.603940226137638, 1.4726729389512911, 4.666632142965682, 4.631235647015274, 1.5146250571124256, 12.09829104389064, 1.506565086892806, 1.4392736901063472, 6.76649587310385, 3.1390180110465735, 1.4510899900924414, 1.4187746449606493, 12.71111766283866, 1.7924457649933174, 1.5123513179132715, 7.61004760407377, 1.5313597009517252, 1.4056466419715434, 6.174983064061962, 1.4592807448934764, 1.4831951769301668, 11.804629980935715, 1.8600998299662024, 4.64996678812895, 6.985381135949865, 1.3731486740289256, 3.3252886830596253, 16.08337309502531, 1.5413428889587522, 1.507501607062295, 11.182367717963643, 5.249350955011323, 1.769834850099869, 4.23800336685963, 12.147797760087997, 1.389602443901822, 1.4961517330957577, 8.432140898890793, 7.742725523887202, 1.4918990089790896, 4.398861582973041, 10.288134982110932, 1.5000079739838839, 6.552208048873581, 6.057737553841434, 1.5312836680095643, 7.17919100006111, 5.950089815072715, 1.4884320079581812, 2.2003402538830414, 7.804656345047988, 1.6215006868587807, 2.7239643511129543, 14.640630019945092, 4.888931100955233, 1.4344048540806398, 6.8163068409776315, 1.6341969849308953, 1.5690942970104516, 8.53140636102762, 10.011419997899793, 1.4333432308631018, 3.3137192690046504, 6.888070740038529, 1.5279888729564846, 1.4332552410196513, 14.148237054934725, 1.442706924979575, 1.5082274369196966, 11.738000817131251, 1.5885900999419391, 1.4379052519798279, 7.811394697870128, 6.321875995141454, 1.5285032809479162, 5.281766023952514, 5.6391253439942375, 1.589860902982764, 7.606702130986378, 6.340873457025737, 1.4237860190914944, 4.777604103088379, 9.910676470026374, 1.658219775883481, 4.057403387851082, 8.545552224037237, 1.440485060098581, 1.5447869540657848, 10.769115512957796, 1.6719725261209533, 1.4382960530929267, 7.8538108200300485, 9.688694001175463, 1.5392634050222114, 8.113021235913038, 10.248319830861874, 1.63823283102829, 4.86717198707629, 1.630552786984481, 5.38526588503737, 4.889825096121058, 1.5691232479875907, 1.5479457471519709, 9.028319863136858, 1.5519310898380354, 1.4484785880194977, 8.904907518881373, 1.4419797020964324, 1.6304030440514907, 1.4983604040462524, 13.289437215891667, 1.586415136931464, 1.492007158929482, 11.584452339913696, 1.4101301908958703, 5.002479778137058, 12.819062803988345, 1.6698864481877536, 11.148671100032516, 1.7454386811004952, 10.685491576907225, 12.348722919006832, 1.4798599248751998, 1.4359881491400301, 6.094888056162745, 1.4331249609822407, 5.983243820024654, 14.39485719113145, 1.519515363033861, 1.484580494929105, 10.18344877101481, 1.5438596439780667, 9.611940188799053, 1.5385236708680168, 1.6169625098118559, 7.710626647109166, 1.5527552472194657, 1.6637780430028215, 7.65183314983733, 1.476578967878595, 1.6155887380009517, 10.019765510922298, 1.4382161089451984, 1.5108899280894548, 8.148132972884923, 7.132788182003424, 1.450414114864543, 1.4618789941305295, 8.714084984036162, 1.4397605119738728, 4.19868014100939, 7.102808495052159, 1.6120865530101582, 9.15729914989788, 5.144495054031722, 1.4743123319931328, 3.4213438000297174, 10.380595745053142, 1.6877442591357976, 9.671946015907452, 1.5784707890124992, 1.5252138811629266, 11.300455593154766, 1.5070246087852865, 1.5366083019180223, 9.691652027890086, 1.4747626029420644, 1.4424127989914268, 7.318764396826737, 7.741440181038342, 1.5720585800008848, 1.5514833240304142, 17.26133695698809, 1.4867297220043838, 1.5855045161442831, 10.919145227991976, 1.7722139989491552, 9.77812906098552, 1.5441915368428454, 1.4358210588106886, 11.668196903076023, 1.431575497961603, 1.5402973669115454, 1.5741361579857767, 1.496524679940194, 10.070699519012123, 2.1076137389754876, 1.796831242972985, 9.415188271086663, 1.5344999028602615, 6.473982864874415, 2.0762588719371706, 1.5112901029642671, 7.564723687944934, 6.005527111934498, 1.5416163249174133, 5.555690837092698, 11.305280032916926, 1.4311999279307202, 6.019575463840738, 3.856983297970146, 1.4899006930645555, 1.4869110359577462, 13.579528959002346, 1.4395692229736596, 1.7376019610092044, 11.172230324009433, 1.5424862259533256, 1.3986102278577164, 14.432784763979726, 1.4633257540408522, 1.5308825341053307, 3.7488974980078638, 9.650103863095865, 1.4103715181117877, 1.6271954331314191, 14.606739675975405, 5.177634374820627, 1.434552778955549, 9.707830915111117, 2.0336274780565873, 1.517353618866764, 5.293753499048762, 9.138745567877777, 1.5383455540286377, 4.144696984090842, 7.05977353383787, 1.5698734150500968, 4.78009001002647, 13.181379572954029, 2.697404017089866, 1.5622275830246508, 1.4481066869338974, 11.323306087986566, 1.5286901359213516, 7.958609869936481, 5.4821320249466226, 1.568024223903194, 10.969182683038525, 1.6928168310550973, 1.4982862839242443, 4.829732675920241, 3.451788242906332, 1.4949429150437936, 7.454903922975063, 1.568844721885398, 1.4177213580114767, 4.651911719120108, 8.890406018006615, 1.6469366429373622, 12.205405981047079, 7.551561470027082, 1.4272080820519477, 1.4745382769033313, 4.200056879082695, 15.37041189998854, 1.4786942839855328, 1.480182294966653, 2.0142140509560704, 12.683699353830889, 1.528549806913361, 8.448878559982404, 1.4576581859728321, 1.5441734180785716, 1.7786283359164372, 11.208643010002561, 1.6135578339453787, 5.526761699933559, 13.160798719036393, 1.5490804689470679, 3.686927558039315, 9.235548228956759, 1.4505228179041296, 9.448109406977892]
Total Epoch List: [105, 111, 91]
Total Time List: [0.40440532693173736, 0.47902564494870603, 3.5809112570714206]
========================training times:9========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cb60070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 2.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 4.91s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 8.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.38s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.38s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 4.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 5.39s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 6.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.37s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.40s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 8.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.39s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.39s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 1.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 2.67s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 7.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.36s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.69s
Val loss: 0.6921 score: 0.7347 time: 0.46s
Test loss: 0.6926 score: 0.5918 time: 0.40s
Epoch 12/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.66s
Val loss: 0.6920 score: 0.6122 time: 0.46s
Test loss: 0.6925 score: 0.5714 time: 0.40s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 11.86s
Val loss: 0.6918 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.48s
Epoch 14/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.37s
Epoch 15/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.46s
Epoch 16/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 3.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 4.15s
Epoch 17/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 13.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 4.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.39s
Epoch 18/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.40s
Epoch 19/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5102 time: 0.40s
Epoch 20/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 12.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 2.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 2.39s
Epoch 21/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4898 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5102 time: 0.35s
Epoch 22/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.44s
Epoch 23/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4898 time: 3.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5102 time: 4.95s
Epoch 24/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 3.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5102 time: 0.37s
Epoch 25/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.63s
Val loss: 0.6850 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.5102 time: 0.38s
Epoch 26/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.64s
Val loss: 0.6839 score: 0.5102 time: 3.02s
Test loss: 0.6874 score: 0.5306 time: 4.11s
Epoch 27/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.65s
Val loss: 0.6826 score: 0.5306 time: 0.59s
Test loss: 0.6866 score: 0.5510 time: 0.41s
Epoch 28/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.60s
Val loss: 0.6812 score: 0.5306 time: 0.47s
Test loss: 0.6858 score: 0.5510 time: 0.51s
Epoch 29/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 6.72s
Val loss: 0.6796 score: 0.5510 time: 0.58s
Test loss: 0.6848 score: 0.5510 time: 0.39s
Epoch 30/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.58s
Val loss: 0.6779 score: 0.5918 time: 0.46s
Test loss: 0.6838 score: 0.5510 time: 0.44s
Epoch 31/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.60s
Val loss: 0.6760 score: 0.5918 time: 0.46s
Test loss: 0.6826 score: 0.5510 time: 4.93s
Epoch 32/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 6.43s
Val loss: 0.6740 score: 0.5918 time: 0.47s
Test loss: 0.6814 score: 0.5510 time: 0.39s
Epoch 33/1000, LR 0.000270
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.60s
Val loss: 0.6718 score: 0.5918 time: 0.55s
Test loss: 0.6801 score: 0.5510 time: 0.35s
Epoch 34/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.67s
Val loss: 0.6695 score: 0.5918 time: 0.45s
Test loss: 0.6786 score: 0.5714 time: 0.37s
Epoch 35/1000, LR 0.000270
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 1.11s
Val loss: 0.6669 score: 0.6122 time: 3.83s
Test loss: 0.6770 score: 0.5714 time: 3.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 3.53s
Val loss: 0.6641 score: 0.6122 time: 0.48s
Test loss: 0.6753 score: 0.6122 time: 0.38s
Epoch 37/1000, LR 0.000270
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.75s
Val loss: 0.6611 score: 0.6122 time: 0.57s
Test loss: 0.6733 score: 0.6122 time: 0.37s
Epoch 38/1000, LR 0.000270
Train loss: 0.6669;  Loss pred: 0.6669; Loss self: 0.0000; time: 0.73s
Val loss: 0.6578 score: 0.6531 time: 3.56s
Test loss: 0.6713 score: 0.6327 time: 4.50s
Epoch 39/1000, LR 0.000269
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 4.00s
Val loss: 0.6543 score: 0.6531 time: 0.47s
Test loss: 0.6690 score: 0.6122 time: 0.47s
Epoch 40/1000, LR 0.000269
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 0.61s
Val loss: 0.6505 score: 0.6939 time: 0.47s
Test loss: 0.6665 score: 0.6327 time: 0.35s
Epoch 41/1000, LR 0.000269
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.59s
Val loss: 0.6463 score: 0.7347 time: 1.61s
Test loss: 0.6638 score: 0.6327 time: 4.38s
Epoch 42/1000, LR 0.000269
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.61s
Val loss: 0.6418 score: 0.7755 time: 0.47s
Test loss: 0.6608 score: 0.6122 time: 0.36s
Epoch 43/1000, LR 0.000269
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.65s
Val loss: 0.6369 score: 0.7959 time: 0.55s
Test loss: 0.6576 score: 0.6735 time: 0.39s
Epoch 44/1000, LR 0.000269
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.63s
Val loss: 0.6315 score: 0.7959 time: 0.48s
Test loss: 0.6540 score: 0.6939 time: 2.30s
Epoch 45/1000, LR 0.000269
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 7.89s
Val loss: 0.6258 score: 0.8367 time: 0.57s
Test loss: 0.6502 score: 0.6939 time: 0.34s
Epoch 46/1000, LR 0.000269
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.62s
Val loss: 0.6196 score: 0.8571 time: 0.45s
Test loss: 0.6460 score: 0.7143 time: 0.35s
Epoch 47/1000, LR 0.000269
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.63s
Val loss: 0.6129 score: 0.8980 time: 0.48s
Test loss: 0.6415 score: 0.7143 time: 0.44s
Epoch 48/1000, LR 0.000269
Train loss: 0.6247;  Loss pred: 0.6247; Loss self: 0.0000; time: 7.10s
Val loss: 0.6057 score: 0.8980 time: 0.50s
Test loss: 0.6367 score: 0.7347 time: 0.37s
Epoch 49/1000, LR 0.000269
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.72s
Val loss: 0.5980 score: 0.9388 time: 0.55s
Test loss: 0.6315 score: 0.7347 time: 0.37s
Epoch 50/1000, LR 0.000269
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 0.62s
Val loss: 0.5897 score: 0.9388 time: 0.50s
Test loss: 0.6259 score: 0.7347 time: 3.43s
Epoch 51/1000, LR 0.000269
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 5.77s
Val loss: 0.5808 score: 0.9592 time: 0.56s
Test loss: 0.6199 score: 0.7959 time: 0.36s
Epoch 52/1000, LR 0.000269
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.70s
Val loss: 0.5713 score: 0.9796 time: 0.46s
Test loss: 0.6134 score: 0.8163 time: 0.36s
Epoch 53/1000, LR 0.000269
Train loss: 0.5796;  Loss pred: 0.5796; Loss self: 0.0000; time: 0.59s
Val loss: 0.5611 score: 0.9796 time: 0.59s
Test loss: 0.6064 score: 0.8367 time: 2.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 8.29s
Val loss: 0.5504 score: 0.9796 time: 0.47s
Test loss: 0.5990 score: 0.8571 time: 0.38s
Epoch 55/1000, LR 0.000269
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.61s
Val loss: 0.5391 score: 0.9796 time: 0.47s
Test loss: 0.5911 score: 0.8776 time: 0.46s
Epoch 56/1000, LR 0.000269
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 0.59s
Val loss: 0.5272 score: 0.9796 time: 0.48s
Test loss: 0.5826 score: 0.8776 time: 0.40s
Epoch 57/1000, LR 0.000269
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 10.27s
Val loss: 0.5148 score: 0.9796 time: 3.61s
Test loss: 0.5737 score: 0.8776 time: 0.46s
Epoch 58/1000, LR 0.000269
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.61s
Val loss: 0.5017 score: 0.9796 time: 0.47s
Test loss: 0.5643 score: 0.8776 time: 0.37s
Epoch 59/1000, LR 0.000268
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.75s
Val loss: 0.4878 score: 0.9796 time: 0.58s
Test loss: 0.5545 score: 0.8776 time: 0.42s
Epoch 60/1000, LR 0.000268
Train loss: 0.4994;  Loss pred: 0.4994; Loss self: 0.0000; time: 0.62s
Val loss: 0.4732 score: 0.9796 time: 3.61s
Test loss: 0.5444 score: 0.8776 time: 4.02s
Epoch 61/1000, LR 0.000268
Train loss: 0.4875;  Loss pred: 0.4875; Loss self: 0.0000; time: 4.38s
Val loss: 0.4582 score: 0.9796 time: 0.65s
Test loss: 0.5339 score: 0.8776 time: 0.41s
Epoch 62/1000, LR 0.000268
Train loss: 0.4708;  Loss pred: 0.4708; Loss self: 0.0000; time: 0.58s
Val loss: 0.4429 score: 0.9796 time: 0.45s
Test loss: 0.5230 score: 0.8776 time: 0.38s
Epoch 63/1000, LR 0.000268
Train loss: 0.4591;  Loss pred: 0.4591; Loss self: 0.0000; time: 0.67s
Val loss: 0.4275 score: 0.9796 time: 2.58s
Test loss: 0.5118 score: 0.8776 time: 3.60s
Epoch 64/1000, LR 0.000268
Train loss: 0.4396;  Loss pred: 0.4396; Loss self: 0.0000; time: 0.59s
Val loss: 0.4120 score: 0.9796 time: 0.47s
Test loss: 0.5004 score: 0.8776 time: 0.38s
Epoch 65/1000, LR 0.000268
Train loss: 0.4216;  Loss pred: 0.4216; Loss self: 0.0000; time: 0.61s
Val loss: 0.3966 score: 0.9796 time: 0.57s
Test loss: 0.4889 score: 0.8776 time: 0.37s
Epoch 66/1000, LR 0.000268
Train loss: 0.4071;  Loss pred: 0.4071; Loss self: 0.0000; time: 0.64s
Val loss: 0.3811 score: 0.9796 time: 0.48s
Test loss: 0.4773 score: 0.8776 time: 0.38s
Epoch 67/1000, LR 0.000268
Train loss: 0.3967;  Loss pred: 0.3967; Loss self: 0.0000; time: 0.65s
Val loss: 0.3660 score: 0.9796 time: 2.81s
Test loss: 0.4656 score: 0.8776 time: 5.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3839;  Loss pred: 0.3839; Loss self: 0.0000; time: 3.63s
Val loss: 0.3513 score: 0.9796 time: 0.48s
Test loss: 0.4542 score: 0.8776 time: 0.36s
Epoch 69/1000, LR 0.000268
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.62s
Val loss: 0.3367 score: 0.9796 time: 0.53s
Test loss: 0.4429 score: 0.8776 time: 0.36s
Epoch 70/1000, LR 0.000268
Train loss: 0.3488;  Loss pred: 0.3488; Loss self: 0.0000; time: 0.58s
Val loss: 0.3227 score: 0.9796 time: 0.46s
Test loss: 0.4317 score: 0.8776 time: 0.40s
Epoch 71/1000, LR 0.000268
Train loss: 0.3300;  Loss pred: 0.3300; Loss self: 0.0000; time: 0.61s
Val loss: 0.3090 score: 0.9796 time: 0.44s
Test loss: 0.4206 score: 0.8776 time: 3.74s
Epoch 72/1000, LR 0.000267
Train loss: 0.3129;  Loss pred: 0.3129; Loss self: 0.0000; time: 5.64s
Val loss: 0.2956 score: 0.9796 time: 0.54s
Test loss: 0.4098 score: 0.8776 time: 0.38s
Epoch 73/1000, LR 0.000267
Train loss: 0.2980;  Loss pred: 0.2980; Loss self: 0.0000; time: 0.70s
Val loss: 0.2827 score: 0.9796 time: 0.55s
Test loss: 0.3994 score: 0.8776 time: 0.37s
Epoch 74/1000, LR 0.000267
Train loss: 0.2877;  Loss pred: 0.2877; Loss self: 0.0000; time: 0.60s
Val loss: 0.2702 score: 0.9796 time: 0.47s
Test loss: 0.3893 score: 0.8776 time: 0.35s
Epoch 75/1000, LR 0.000267
Train loss: 0.2644;  Loss pred: 0.2644; Loss self: 0.0000; time: 9.89s
Val loss: 0.2580 score: 0.9796 time: 1.78s
Test loss: 0.3797 score: 0.8776 time: 0.39s
Epoch 76/1000, LR 0.000267
Train loss: 0.2496;  Loss pred: 0.2496; Loss self: 0.0000; time: 0.58s
Val loss: 0.2464 score: 0.9796 time: 0.47s
Test loss: 0.3705 score: 0.8776 time: 0.37s
Epoch 77/1000, LR 0.000267
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 0.62s
Val loss: 0.2353 score: 0.9592 time: 0.59s
Test loss: 0.3619 score: 0.8776 time: 0.51s
Epoch 78/1000, LR 0.000267
Train loss: 0.2292;  Loss pred: 0.2292; Loss self: 0.0000; time: 7.92s
Val loss: 0.2246 score: 0.9592 time: 0.50s
Test loss: 0.3539 score: 0.8776 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.70s
Val loss: 0.2145 score: 0.9592 time: 0.54s
Test loss: 0.3464 score: 0.8776 time: 0.48s
Epoch 80/1000, LR 0.000267
Train loss: 0.2109;  Loss pred: 0.2109; Loss self: 0.0000; time: 5.71s
Val loss: 0.2053 score: 0.9592 time: 0.49s
Test loss: 0.3392 score: 0.8980 time: 0.39s
Epoch 81/1000, LR 0.000267
Train loss: 0.1936;  Loss pred: 0.1936; Loss self: 0.0000; time: 0.62s
Val loss: 0.1968 score: 0.9592 time: 0.56s
Test loss: 0.3327 score: 0.8980 time: 0.38s
Epoch 82/1000, LR 0.000267
Train loss: 0.1841;  Loss pred: 0.1841; Loss self: 0.0000; time: 0.64s
Val loss: 0.1890 score: 0.9592 time: 0.48s
Test loss: 0.3271 score: 0.8980 time: 1.43s
Epoch 83/1000, LR 0.000266
Train loss: 0.1724;  Loss pred: 0.1724; Loss self: 0.0000; time: 11.54s
Val loss: 0.1820 score: 0.9592 time: 0.55s
Test loss: 0.3223 score: 0.8980 time: 0.35s
Epoch 84/1000, LR 0.000266
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.60s
Val loss: 0.1757 score: 0.9592 time: 0.45s
Test loss: 0.3185 score: 0.8980 time: 0.37s
Epoch 85/1000, LR 0.000266
Train loss: 0.1513;  Loss pred: 0.1513; Loss self: 0.0000; time: 0.62s
Val loss: 0.1700 score: 0.9592 time: 0.55s
Test loss: 0.3155 score: 0.8980 time: 0.36s
Epoch 86/1000, LR 0.000266
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.73s
Val loss: 0.1648 score: 0.9592 time: 0.46s
Test loss: 0.3131 score: 0.8980 time: 0.44s
Epoch 87/1000, LR 0.000266
Train loss: 0.1357;  Loss pred: 0.1357; Loss self: 0.0000; time: 0.66s
Val loss: 0.1600 score: 0.9592 time: 0.46s
Test loss: 0.3112 score: 0.8980 time: 0.52s
Epoch 88/1000, LR 0.000266
Train loss: 0.1351;  Loss pred: 0.1351; Loss self: 0.0000; time: 0.63s
Val loss: 0.1561 score: 0.9592 time: 0.46s
Test loss: 0.3102 score: 0.8980 time: 0.35s
Epoch 89/1000, LR 0.000266
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.58s
Val loss: 0.1527 score: 0.9592 time: 0.53s
Test loss: 0.3097 score: 0.8980 time: 0.36s
Epoch 90/1000, LR 0.000266
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.61s
Val loss: 0.1498 score: 0.9592 time: 0.47s
Test loss: 0.3099 score: 0.8980 time: 0.37s
Epoch 91/1000, LR 0.000266
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.59s
Val loss: 0.1474 score: 0.9592 time: 0.54s
Test loss: 0.3104 score: 0.8980 time: 0.36s
Epoch 92/1000, LR 0.000266
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.59s
Val loss: 0.1453 score: 0.9592 time: 0.45s
Test loss: 0.3113 score: 0.8980 time: 0.36s
Epoch 93/1000, LR 0.000265
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.65s
Val loss: 0.1439 score: 0.9592 time: 0.57s
Test loss: 0.3128 score: 0.8980 time: 0.36s
Epoch 94/1000, LR 0.000265
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.57s
Val loss: 0.1432 score: 0.9592 time: 0.46s
Test loss: 0.3150 score: 0.8980 time: 0.36s
Epoch 95/1000, LR 0.000265
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.65s
Val loss: 0.1428 score: 0.9592 time: 0.48s
Test loss: 0.3175 score: 0.8980 time: 0.44s
Epoch 96/1000, LR 0.000265
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.60s
Val loss: 0.1428 score: 0.9592 time: 0.45s
Test loss: 0.3204 score: 0.8980 time: 0.36s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.62s
Val loss: 0.1432 score: 0.9592 time: 0.55s
Test loss: 0.3235 score: 0.8980 time: 0.38s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.0780,   Val_Loss: 0.1428,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1428,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3175


[4.915992407943122, 0.3807145800674334, 0.38255758001469076, 5.392568312003277, 0.3775552390143275, 0.40737279306631535, 0.3934049910167232, 0.3926306270295754, 2.6701717450050637, 0.3676138080190867, 0.4082417479949072, 0.4073310950770974, 0.4850885859923437, 0.37428306695073843, 0.46603351493831724, 4.153200707049109, 0.393920412985608, 0.4012454339535907, 0.40142505906987935, 2.397826666943729, 0.35944156302139163, 0.4468554869526997, 4.95929003204219, 0.3770726240472868, 0.38584203203208745, 4.111084116972052, 0.41155415691901, 0.512544246041216, 0.38932502502575517, 0.4403065850492567, 4.93725185492076, 0.39163147401995957, 0.3549945449922234, 0.3761095970403403, 3.045841192943044, 0.3862283700145781, 0.37835483299568295, 4.510075359954499, 0.4704971049213782, 0.35424237104598433, 4.3844813940813765, 0.36596133408602327, 0.39062804193235934, 2.3044597279513255, 0.3472868410171941, 0.355802837992087, 0.44409423402976245, 0.37002318899612874, 0.3791715479455888, 3.4380417469656095, 0.36403298401273787, 0.36771918705198914, 2.058683636947535, 0.3803649169858545, 0.46730588108766824, 0.3999058639165014, 0.460240351036191, 0.37768920802045614, 0.4224639959866181, 4.0243046139366925, 0.41083162499126047, 0.38100612396374345, 3.6053621839964762, 0.386216914979741, 0.3714907420799136, 0.38095290097407997, 5.4679728880291805, 0.3686901709297672, 0.36452194198500365, 0.4080893429927528, 3.7413997859694064, 0.38404192600864917, 0.37901805399451405, 0.35183862200938165, 0.3905426920391619, 0.37275992205832154, 0.5137855939101428, 0.3839378439588472, 0.4854809589451179, 0.391843774006702, 0.38284757698420435, 1.4331600699806586, 0.35487410600762814, 0.3769912450807169, 0.3670760130044073, 0.4418421620503068, 0.525930730975233, 0.34940152696799487, 0.3611778090707958, 0.37209409300703555, 0.3684391799615696, 0.3608634959673509, 0.3639933749800548, 0.3653270569629967, 0.442938684951514, 0.362762329983525, 0.3827929219696671]
[0.10032637567230862, 0.007769685307498641, 0.00780729755132022, 0.11005241453067913, 0.007705208959476072, 0.00831373047074113, 0.008028673286055575, 0.00801286993937909, 0.05449330091847069, 0.007502322612634423, 0.008331464244794024, 0.008312879491369335, 0.00989976706106824, 0.007638429937770172, 0.009510888059965658, 0.08475919810304304, 0.008039192101747101, 0.008188682325583483, 0.008192348144283252, 0.048935238100892425, 0.00733554210247738, 0.009119499733728565, 0.10121000065392226, 0.007695359674434425, 0.007874327184328315, 0.08389967585657249, 0.008399064426918571, 0.010460086653902367, 0.007945408673995003, 0.008985848674474627, 0.10076024193715837, 0.007992479061631828, 0.007244786632494355, 0.007675706062047761, 0.06216002434577641, 0.007882211632950574, 0.007721527203993529, 0.09204235428478569, 0.00960198173308935, 0.007229436143795598, 0.08947921212410972, 0.007468598654816801, 0.007972000855762435, 0.04702979036635358, 0.007087486551371308, 0.007261282408001776, 0.009063147633260459, 0.007551493652982219, 0.007738194856032425, 0.07016411728501244, 0.007429244571688528, 0.007504473205142636, 0.04201395177443949, 0.007762549326241929, 0.009536854716074862, 0.008161344161561253, 0.009392660225228387, 0.007707943020825636, 0.008621714203808533, 0.08212866559054474, 0.008384318877372662, 0.007775635182933539, 0.07357882008156073, 0.007881977856729408, 0.007581443715916604, 0.00777454899947102, 0.11159128342916695, 0.0075242892026483104, 0.007439223305816401, 0.008328353938627608, 0.07635509767284503, 0.007837590326707125, 0.007735062326418654, 0.007180380041007788, 0.007970259021207387, 0.007607345348129011, 0.010485420283880465, 0.00783546620324178, 0.009907774672349344, 0.00799681171442249, 0.007813215856820497, 0.029248164693482827, 0.007242328694033228, 0.007693698879198304, 0.007491347204171577, 0.009017186980618506, 0.010733280223984348, 0.007130643407510099, 0.007370975695322363, 0.007593757000143583, 0.007519166937991217, 0.007364561142190835, 0.007428436224082751, 0.007455654223734627, 0.00903956499901049, 0.007403312856806632, 0.0078121004483605525]
[9.967468607320708, 128.7053413907105, 128.08529371740153, 9.086579374605467, 129.78233364718466, 120.28294680941885, 124.55357994661807, 124.79923018412165, 18.35087952363419, 133.2920552251288, 120.02692091307446, 120.29526002851696, 101.01247775137999, 130.91695651422344, 105.1426526834352, 11.798129552668547, 124.39060882531679, 122.11976973093105, 122.06512496637677, 20.435171847702996, 136.32257657716627, 109.65513780339172, 9.880446532348149, 129.94844195810714, 126.99497704263867, 11.918997180746112, 119.06087978025877, 95.6015024624034, 125.85885019016818, 111.28609397135956, 9.924549413286194, 125.11762524352858, 138.03029001776184, 130.28117438530668, 16.087509786632623, 126.86794602413724, 129.5080589087099, 10.864563469399402, 104.14516792444043, 138.32337406537766, 11.17578011988949, 133.8939265875613, 125.43902316281937, 21.263118381140558, 141.0937421541233, 137.71672052005877, 110.33694257943485, 132.42413301970825, 129.2291055736901, 14.252299304755983, 134.60318749106932, 133.25385708815963, 23.801617266776162, 128.8236580499938, 104.85637348699967, 122.52883596183273, 106.46611034795357, 129.7362989448883, 115.98621531183007, 12.176016653984542, 119.2702728302437, 128.60685673562256, 13.590867574276386, 126.87170887523219, 131.90099900109843, 128.6248244197882, 8.961273401203906, 132.9029192083728, 134.4226351181237, 120.0717461540528, 13.09670251860133, 127.59023606942449, 129.28144051077112, 139.2683944706145, 125.46643682961681, 131.45190000424327, 95.37052144083614, 127.62482461940407, 100.93083796009248, 125.04983682390196, 127.98827247644212, 34.19017946869067, 138.07713544179165, 129.9764931928557, 133.48733849141945, 110.89933059493994, 93.16816286650398, 140.23979925104447, 135.6672496742327, 131.68712140526645, 132.99345635583867, 135.7854162240707, 134.61783474131906, 134.12639186197237, 110.62479224492157, 135.0746644565475, 128.00654658887035]
Elapsed: 1.055903905822617~1.4316013309805524
Time per graph: 0.021549059302502394~0.029216353693480662
Speed: 103.36916830810164~44.96166948628615
Total Time: 0.3833
best val loss: 0.14277882874011993 test_score: 0.8980

Testing...
Test loss: 0.6134 score: 0.8163 time: 0.37s
test Score 0.8163
Epoch Time List: [7.923659118940122, 9.207748327055015, 1.4872148950817063, 10.74356619711034, 7.560045945108868, 1.6494764690287411, 9.429147329065017, 1.7364736699964851, 5.2602265710011125, 8.180418497067876, 1.5457166510168463, 1.531742964987643, 12.797365090926178, 1.3900724129052833, 1.5375783210620284, 8.35525265603792, 18.265497262007557, 1.7849884229945019, 1.5618441288825125, 16.967584008118138, 1.5336890189209953, 1.5118753620190546, 8.725671448977664, 4.700059161172248, 1.5765038869576529, 7.772470562835224, 1.6527807540260255, 1.5821148490067571, 7.683307546074502, 1.4798189599532634, 5.988346641999669, 7.2782104189973325, 1.5038141058757901, 1.4959390969015658, 7.979308106936514, 4.38928855501581, 1.6920231770491228, 8.79599118605256, 4.938298386870883, 1.427105645998381, 6.586969259078614, 1.4352359669283032, 1.5876566120423377, 3.4097868141252548, 8.796874755993485, 1.4163004419533536, 1.5533243250101805, 7.96936046204064, 1.6421714950120077, 4.554225887055509, 6.6834568521007895, 1.520565114915371, 3.2268955130130053, 9.132425990072079, 1.547913048765622, 1.464365872903727, 14.335941906902008, 1.4498268159804866, 1.748888718080707, 8.251580965821631, 5.435493079130538, 1.4061997719109058, 6.845060609979555, 1.434191903914325, 1.5461277860449627, 1.4989638219121844, 8.914823223953135, 4.472729481989518, 1.5184299108805135, 1.446767644956708, 4.7895467159105465, 6.55743250309024, 1.6284682389814407, 1.416277106036432, 12.054401285131462, 1.4132568029453978, 1.7132551869144663, 8.799951805034652, 1.7227576959412545, 6.57991077308543, 1.556645117001608, 2.5463930839905515, 12.44134214008227, 1.413465806050226, 1.5361707038246095, 1.6356829339638352, 1.6406312569743022, 1.4364570169709623, 1.4706430219812319, 1.4420645500067621, 1.488934675930068, 1.3990223510190845, 1.5798063160618767, 1.397037714952603, 1.5636816860642284, 1.4000145850004628, 1.5436726240441203]
Total Epoch List: [97]
Total Time List: [0.38325287494808435]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3dc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.46s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.45s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.45s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.46s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.48s
Epoch 6/1000, LR 0.000120
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.46s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.48s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.54s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.57s
Val loss: 0.6928 score: 0.5918 time: 0.36s
Test loss: 0.6926 score: 0.6122 time: 0.45s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.56s
Val loss: 0.6927 score: 0.6327 time: 0.34s
Test loss: 0.6924 score: 0.7347 time: 0.46s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.65s
Val loss: 0.6926 score: 0.5918 time: 0.37s
Test loss: 0.6922 score: 0.7755 time: 0.45s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.59s
Val loss: 0.6925 score: 0.5714 time: 0.37s
Test loss: 0.6919 score: 0.7755 time: 0.55s
Epoch 14/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.55s
Val loss: 0.6924 score: 0.5714 time: 0.36s
Test loss: 0.6917 score: 0.7755 time: 0.46s
Epoch 15/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.64s
Val loss: 0.6922 score: 0.6122 time: 0.35s
Test loss: 0.6914 score: 0.7755 time: 0.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.67s
Val loss: 0.6920 score: 0.6122 time: 0.38s
Test loss: 0.6911 score: 0.7959 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.56s
Val loss: 0.6918 score: 0.6327 time: 0.36s
Test loss: 0.6907 score: 0.7143 time: 0.48s
Epoch 18/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.69s
Val loss: 0.6915 score: 0.6327 time: 0.36s
Test loss: 0.6903 score: 0.6735 time: 0.47s
Epoch 19/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.59s
Val loss: 0.6912 score: 0.6327 time: 0.37s
Test loss: 0.6898 score: 0.6531 time: 0.46s
Epoch 20/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.64s
Val loss: 0.6909 score: 0.6122 time: 0.35s
Test loss: 0.6892 score: 0.6122 time: 0.46s
Epoch 21/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.62s
Val loss: 0.6905 score: 0.6122 time: 0.37s
Test loss: 0.6886 score: 0.6122 time: 0.47s
Epoch 22/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.68s
Val loss: 0.6901 score: 0.6122 time: 0.47s
Test loss: 0.6880 score: 0.6122 time: 0.46s
Epoch 23/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.58s
Val loss: 0.6896 score: 0.6122 time: 0.37s
Test loss: 0.6872 score: 0.6122 time: 0.45s
Epoch 24/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.63s
Val loss: 0.6891 score: 0.6122 time: 0.34s
Test loss: 0.6864 score: 0.6122 time: 0.51s
Epoch 25/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.57s
Val loss: 0.6885 score: 0.6122 time: 0.36s
Test loss: 0.6855 score: 0.6122 time: 0.46s
Epoch 26/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.59s
Val loss: 0.6879 score: 0.6122 time: 0.45s
Test loss: 0.6845 score: 0.6122 time: 0.46s
Epoch 27/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.65s
Val loss: 0.6872 score: 0.6122 time: 0.35s
Test loss: 0.6835 score: 0.6122 time: 0.49s
Epoch 28/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.71s
Val loss: 0.6865 score: 0.6122 time: 0.44s
Test loss: 0.6823 score: 0.6122 time: 0.48s
Epoch 29/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.71s
Val loss: 0.6856 score: 0.6122 time: 0.39s
Test loss: 0.6809 score: 0.6122 time: 0.54s
Epoch 30/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.69s
Val loss: 0.6847 score: 0.6122 time: 0.36s
Test loss: 0.6795 score: 0.6122 time: 0.48s
Epoch 31/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.62s
Val loss: 0.6836 score: 0.6122 time: 0.36s
Test loss: 0.6778 score: 0.6122 time: 0.48s
Epoch 32/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.67s
Val loss: 0.6824 score: 0.6122 time: 0.38s
Test loss: 0.6760 score: 0.6122 time: 0.46s
Epoch 33/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.59s
Val loss: 0.6811 score: 0.6122 time: 0.36s
Test loss: 0.6741 score: 0.6122 time: 0.57s
Epoch 34/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.58s
Val loss: 0.6797 score: 0.6122 time: 0.35s
Test loss: 0.6719 score: 0.6122 time: 0.48s
Epoch 35/1000, LR 0.000270
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.59s
Val loss: 0.6781 score: 0.6122 time: 0.38s
Test loss: 0.6696 score: 0.6122 time: 0.48s
Epoch 36/1000, LR 0.000270
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.71s
Val loss: 0.6764 score: 0.6122 time: 0.38s
Test loss: 0.6671 score: 0.6122 time: 0.48s
Epoch 37/1000, LR 0.000270
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.60s
Val loss: 0.6745 score: 0.6122 time: 0.34s
Test loss: 0.6644 score: 0.6122 time: 0.54s
Epoch 38/1000, LR 0.000270
Train loss: 0.6651;  Loss pred: 0.6651; Loss self: 0.0000; time: 0.56s
Val loss: 0.6725 score: 0.6327 time: 0.36s
Test loss: 0.6614 score: 0.6122 time: 0.48s
Epoch 39/1000, LR 0.000269
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.58s
Val loss: 0.6703 score: 0.6327 time: 0.36s
Test loss: 0.6581 score: 0.6122 time: 0.54s
Epoch 40/1000, LR 0.000269
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.67s
Val loss: 0.6678 score: 0.6327 time: 0.47s
Test loss: 0.6546 score: 0.6122 time: 0.45s
Epoch 41/1000, LR 0.000269
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.54s
Val loss: 0.6651 score: 0.6327 time: 0.36s
Test loss: 0.6507 score: 0.6122 time: 0.48s
Epoch 42/1000, LR 0.000269
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.66s
Val loss: 0.6621 score: 0.6327 time: 0.37s
Test loss: 0.6465 score: 0.6122 time: 0.46s
Epoch 43/1000, LR 0.000269
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.68s
Val loss: 0.6589 score: 0.6327 time: 0.37s
Test loss: 0.6419 score: 0.6122 time: 0.46s
Epoch 44/1000, LR 0.000269
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.66s
Val loss: 0.6555 score: 0.6327 time: 0.35s
Test loss: 0.6370 score: 0.6122 time: 0.44s
Epoch 45/1000, LR 0.000269
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.59s
Val loss: 0.6518 score: 0.6327 time: 0.36s
Test loss: 0.6318 score: 0.6122 time: 0.47s
Epoch 46/1000, LR 0.000269
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.67s
Val loss: 0.6478 score: 0.6327 time: 0.36s
Test loss: 0.6262 score: 0.6122 time: 0.46s
Epoch 47/1000, LR 0.000269
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.59s
Val loss: 0.6435 score: 0.6531 time: 0.37s
Test loss: 0.6203 score: 0.6122 time: 0.46s
Epoch 48/1000, LR 0.000269
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.67s
Val loss: 0.6390 score: 0.6531 time: 0.37s
Test loss: 0.6140 score: 0.6531 time: 0.47s
Epoch 49/1000, LR 0.000269
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.56s
Val loss: 0.6341 score: 0.6531 time: 0.36s
Test loss: 0.6073 score: 0.6531 time: 0.47s
Epoch 50/1000, LR 0.000269
Train loss: 0.6038;  Loss pred: 0.6038; Loss self: 0.0000; time: 0.58s
Val loss: 0.6289 score: 0.6735 time: 0.45s
Test loss: 0.6003 score: 0.6735 time: 0.46s
Epoch 51/1000, LR 0.000269
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.61s
Val loss: 0.6233 score: 0.6735 time: 0.36s
Test loss: 0.5928 score: 0.7143 time: 0.46s
Epoch 52/1000, LR 0.000269
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.56s
Val loss: 0.6173 score: 0.6531 time: 0.44s
Test loss: 0.5850 score: 0.7347 time: 0.49s
Epoch 53/1000, LR 0.000269
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.58s
Val loss: 0.6109 score: 0.6531 time: 0.37s
Test loss: 0.5767 score: 0.7551 time: 0.46s
Epoch 54/1000, LR 0.000269
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.68s
Val loss: 0.6041 score: 0.6327 time: 0.36s
Test loss: 0.5680 score: 0.7551 time: 0.48s
Epoch 55/1000, LR 0.000269
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.58s
Val loss: 0.5969 score: 0.7143 time: 0.36s
Test loss: 0.5588 score: 0.7755 time: 0.48s
Epoch 56/1000, LR 0.000269
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.66s
Val loss: 0.5892 score: 0.7347 time: 0.36s
Test loss: 0.5490 score: 0.7755 time: 0.47s
Epoch 57/1000, LR 0.000269
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.58s
Val loss: 0.5809 score: 0.7755 time: 0.36s
Test loss: 0.5386 score: 0.8163 time: 0.55s
Epoch 58/1000, LR 0.000269
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.67s
Val loss: 0.5723 score: 0.7755 time: 0.40s
Test loss: 0.5278 score: 0.8367 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.5102;  Loss pred: 0.5102; Loss self: 0.0000; time: 0.59s
Val loss: 0.5632 score: 0.7755 time: 0.35s
Test loss: 0.5164 score: 0.8571 time: 0.46s
Epoch 60/1000, LR 0.000268
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.64s
Val loss: 0.5539 score: 0.7755 time: 0.36s
Test loss: 0.5046 score: 0.8980 time: 0.47s
Epoch 61/1000, LR 0.000268
Train loss: 0.4896;  Loss pred: 0.4896; Loss self: 0.0000; time: 0.58s
Val loss: 0.5444 score: 0.7755 time: 0.36s
Test loss: 0.4926 score: 0.9184 time: 0.55s
Epoch 62/1000, LR 0.000268
Train loss: 0.4833;  Loss pred: 0.4833; Loss self: 0.0000; time: 0.61s
Val loss: 0.5347 score: 0.8163 time: 0.38s
Test loss: 0.4805 score: 0.9184 time: 0.44s
Epoch 63/1000, LR 0.000268
Train loss: 0.4596;  Loss pred: 0.4596; Loss self: 0.0000; time: 0.58s
Val loss: 0.5249 score: 0.8571 time: 0.37s
Test loss: 0.4685 score: 0.9184 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.4457;  Loss pred: 0.4457; Loss self: 0.0000; time: 0.66s
Val loss: 0.5151 score: 0.8571 time: 0.38s
Test loss: 0.4564 score: 0.9184 time: 0.46s
Epoch 65/1000, LR 0.000268
Train loss: 0.4326;  Loss pred: 0.4326; Loss self: 0.0000; time: 0.67s
Val loss: 0.5052 score: 0.8571 time: 0.37s
Test loss: 0.4442 score: 0.9592 time: 0.46s
Epoch 66/1000, LR 0.000268
Train loss: 0.4236;  Loss pred: 0.4236; Loss self: 0.0000; time: 0.65s
Val loss: 0.4953 score: 0.8571 time: 0.35s
Test loss: 0.4320 score: 0.9592 time: 0.47s
Epoch 67/1000, LR 0.000268
Train loss: 0.4120;  Loss pred: 0.4120; Loss self: 0.0000; time: 0.59s
Val loss: 0.4853 score: 0.8571 time: 0.36s
Test loss: 0.4200 score: 0.9592 time: 0.47s
Epoch 68/1000, LR 0.000268
Train loss: 0.3909;  Loss pred: 0.3909; Loss self: 0.0000; time: 0.69s
Val loss: 0.4752 score: 0.8571 time: 0.37s
Test loss: 0.4083 score: 0.9592 time: 0.47s
Epoch 69/1000, LR 0.000268
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 0.60s
Val loss: 0.4656 score: 0.8571 time: 0.36s
Test loss: 0.3971 score: 0.9592 time: 0.53s
Epoch 70/1000, LR 0.000268
Train loss: 0.3657;  Loss pred: 0.3657; Loss self: 0.0000; time: 0.75s
Val loss: 0.4561 score: 0.8571 time: 0.36s
Test loss: 0.3862 score: 0.9592 time: 0.50s
Epoch 71/1000, LR 0.000268
Train loss: 0.3604;  Loss pred: 0.3604; Loss self: 0.0000; time: 0.60s
Val loss: 0.4470 score: 0.8776 time: 0.38s
Test loss: 0.3760 score: 0.9592 time: 0.50s
Epoch 72/1000, LR 0.000267
Train loss: 0.3516;  Loss pred: 0.3516; Loss self: 0.0000; time: 0.86s
Val loss: 0.4383 score: 0.8776 time: 0.39s
Test loss: 0.3663 score: 0.9388 time: 0.49s
Epoch 73/1000, LR 0.000267
Train loss: 0.3348;  Loss pred: 0.3348; Loss self: 0.0000; time: 0.61s
Val loss: 0.4300 score: 0.8571 time: 0.37s
Test loss: 0.3572 score: 0.9388 time: 0.45s
Epoch 74/1000, LR 0.000267
Train loss: 0.3221;  Loss pred: 0.3221; Loss self: 0.0000; time: 0.56s
Val loss: 0.4220 score: 0.8571 time: 0.46s
Test loss: 0.3486 score: 0.9388 time: 0.44s
Epoch 75/1000, LR 0.000267
Train loss: 0.3102;  Loss pred: 0.3102; Loss self: 0.0000; time: 0.57s
Val loss: 0.4143 score: 0.8571 time: 0.36s
Test loss: 0.3402 score: 0.9388 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.2986;  Loss pred: 0.2986; Loss self: 0.0000; time: 0.59s
Val loss: 0.4067 score: 0.8571 time: 0.46s
Test loss: 0.3319 score: 0.9388 time: 0.49s
Epoch 77/1000, LR 0.000267
Train loss: 0.2908;  Loss pred: 0.2908; Loss self: 0.0000; time: 0.59s
Val loss: 0.3993 score: 0.8571 time: 0.37s
Test loss: 0.3240 score: 0.9388 time: 0.50s
Epoch 78/1000, LR 0.000267
Train loss: 0.2726;  Loss pred: 0.2726; Loss self: 0.0000; time: 0.66s
Val loss: 0.3922 score: 0.8571 time: 0.37s
Test loss: 0.3164 score: 0.9388 time: 0.45s
Epoch 79/1000, LR 0.000267
Train loss: 0.2699;  Loss pred: 0.2699; Loss self: 0.0000; time: 0.58s
Val loss: 0.3854 score: 0.8571 time: 0.37s
Test loss: 0.3092 score: 0.9592 time: 0.47s
Epoch 80/1000, LR 0.000267
Train loss: 0.2637;  Loss pred: 0.2637; Loss self: 0.0000; time: 0.67s
Val loss: 0.3788 score: 0.8571 time: 0.36s
Test loss: 0.3024 score: 0.9592 time: 0.47s
Epoch 81/1000, LR 0.000267
Train loss: 0.2505;  Loss pred: 0.2505; Loss self: 0.0000; time: 0.58s
Val loss: 0.3726 score: 0.8571 time: 0.35s
Test loss: 0.2959 score: 0.9592 time: 0.54s
Epoch 82/1000, LR 0.000267
Train loss: 0.2306;  Loss pred: 0.2306; Loss self: 0.0000; time: 0.56s
Val loss: 0.3665 score: 0.8571 time: 0.36s
Test loss: 0.2897 score: 0.9388 time: 0.47s
Epoch 83/1000, LR 0.000266
Train loss: 0.2331;  Loss pred: 0.2331; Loss self: 0.0000; time: 0.58s
Val loss: 0.3607 score: 0.8571 time: 0.37s
Test loss: 0.2838 score: 0.9388 time: 0.46s
Epoch 84/1000, LR 0.000266
Train loss: 0.2152;  Loss pred: 0.2152; Loss self: 0.0000; time: 0.67s
Val loss: 0.3552 score: 0.8571 time: 0.35s
Test loss: 0.2785 score: 0.9388 time: 0.45s
Epoch 85/1000, LR 0.000266
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.57s
Val loss: 0.3499 score: 0.8571 time: 0.36s
Test loss: 0.2735 score: 0.9388 time: 0.55s
Epoch 86/1000, LR 0.000266
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 0.58s
Val loss: 0.3450 score: 0.8776 time: 0.37s
Test loss: 0.2688 score: 0.9388 time: 0.46s
Epoch 87/1000, LR 0.000266
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.59s
Val loss: 0.3405 score: 0.8776 time: 0.37s
Test loss: 0.2642 score: 0.9388 time: 0.46s
Epoch 88/1000, LR 0.000266
Train loss: 0.1761;  Loss pred: 0.1761; Loss self: 0.0000; time: 0.67s
Val loss: 0.3364 score: 0.8776 time: 0.36s
Test loss: 0.2596 score: 0.9388 time: 0.47s
Epoch 89/1000, LR 0.000266
Train loss: 0.1695;  Loss pred: 0.1695; Loss self: 0.0000; time: 0.56s
Val loss: 0.3323 score: 0.8571 time: 0.37s
Test loss: 0.2553 score: 0.9388 time: 0.47s
Epoch 90/1000, LR 0.000266
Train loss: 0.1535;  Loss pred: 0.1535; Loss self: 0.0000; time: 0.69s
Val loss: 0.3285 score: 0.8571 time: 0.38s
Test loss: 0.2514 score: 0.9388 time: 0.47s
Epoch 91/1000, LR 0.000266
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.59s
Val loss: 0.3246 score: 0.8571 time: 0.34s
Test loss: 0.2480 score: 0.9388 time: 0.48s
Epoch 92/1000, LR 0.000266
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.66s
Val loss: 0.3212 score: 0.8571 time: 0.37s
Test loss: 0.2451 score: 0.9388 time: 0.46s
Epoch 93/1000, LR 0.000265
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.58s
Val loss: 0.3185 score: 0.8571 time: 0.37s
Test loss: 0.2430 score: 0.9388 time: 0.54s
Epoch 94/1000, LR 0.000265
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.69s
Val loss: 0.3165 score: 0.8571 time: 0.38s
Test loss: 0.2415 score: 0.9388 time: 0.45s
Epoch 95/1000, LR 0.000265
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.55s
Val loss: 0.3148 score: 0.8571 time: 0.36s
Test loss: 0.2407 score: 0.9388 time: 0.45s
Epoch 96/1000, LR 0.000265
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.70s
Val loss: 0.3136 score: 0.8571 time: 0.36s
Test loss: 0.2406 score: 0.9388 time: 0.46s
Epoch 97/1000, LR 0.000265
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 0.60s
Val loss: 0.3135 score: 0.8571 time: 0.35s
Test loss: 0.2414 score: 0.9388 time: 0.46s
Epoch 98/1000, LR 0.000265
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.57s
Val loss: 0.3146 score: 0.8571 time: 0.44s
Test loss: 0.2431 score: 0.9388 time: 0.45s
     INFO: Early stopping counter 1 of 2
Epoch 99/1000, LR 0.000265
Train loss: 0.0986;  Loss pred: 0.0986; Loss self: 0.0000; time: 0.68s
Val loss: 0.3163 score: 0.8571 time: 0.37s
Test loss: 0.2456 score: 0.9388 time: 0.44s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 096,   Train_Loss: 0.0914,   Val_Loss: 0.3135,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3135,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2414


[4.915992407943122, 0.3807145800674334, 0.38255758001469076, 5.392568312003277, 0.3775552390143275, 0.40737279306631535, 0.3934049910167232, 0.3926306270295754, 2.6701717450050637, 0.3676138080190867, 0.4082417479949072, 0.4073310950770974, 0.4850885859923437, 0.37428306695073843, 0.46603351493831724, 4.153200707049109, 0.393920412985608, 0.4012454339535907, 0.40142505906987935, 2.397826666943729, 0.35944156302139163, 0.4468554869526997, 4.95929003204219, 0.3770726240472868, 0.38584203203208745, 4.111084116972052, 0.41155415691901, 0.512544246041216, 0.38932502502575517, 0.4403065850492567, 4.93725185492076, 0.39163147401995957, 0.3549945449922234, 0.3761095970403403, 3.045841192943044, 0.3862283700145781, 0.37835483299568295, 4.510075359954499, 0.4704971049213782, 0.35424237104598433, 4.3844813940813765, 0.36596133408602327, 0.39062804193235934, 2.3044597279513255, 0.3472868410171941, 0.355802837992087, 0.44409423402976245, 0.37002318899612874, 0.3791715479455888, 3.4380417469656095, 0.36403298401273787, 0.36771918705198914, 2.058683636947535, 0.3803649169858545, 0.46730588108766824, 0.3999058639165014, 0.460240351036191, 0.37768920802045614, 0.4224639959866181, 4.0243046139366925, 0.41083162499126047, 0.38100612396374345, 3.6053621839964762, 0.386216914979741, 0.3714907420799136, 0.38095290097407997, 5.4679728880291805, 0.3686901709297672, 0.36452194198500365, 0.4080893429927528, 3.7413997859694064, 0.38404192600864917, 0.37901805399451405, 0.35183862200938165, 0.3905426920391619, 0.37275992205832154, 0.5137855939101428, 0.3839378439588472, 0.4854809589451179, 0.391843774006702, 0.38284757698420435, 1.4331600699806586, 0.35487410600762814, 0.3769912450807169, 0.3670760130044073, 0.4418421620503068, 0.525930730975233, 0.34940152696799487, 0.3611778090707958, 0.37209409300703555, 0.3684391799615696, 0.3608634959673509, 0.3639933749800548, 0.3653270569629967, 0.442938684951514, 0.362762329983525, 0.3827929219696671, 0.46075183304492384, 0.45644203305710107, 0.45925686694681644, 0.4598510379437357, 0.4802599009126425, 0.4635600340552628, 0.4660474599804729, 0.48424630507361144, 0.5485110230511054, 0.4544175430200994, 0.4592052069492638, 0.45904159697238356, 0.5494884270010516, 0.4691297580720857, 0.5013732610968873, 0.46153183199930936, 0.4852614681003615, 0.47919648909009993, 0.46791925106663257, 0.4621530759613961, 0.47349005297292024, 0.4662772750016302, 0.45212786400225013, 0.510248898062855, 0.45977575902361423, 0.46662421396467835, 0.4927701649721712, 0.4875709320185706, 0.5474966539768502, 0.4822819040855393, 0.48043445695657283, 0.46290362696163356, 0.5712084849365056, 0.48394914297387004, 0.48352937097661197, 0.4854079809738323, 0.5443117680260912, 0.4794570020167157, 0.5423025690251961, 0.4507052880944684, 0.48579229798633605, 0.46317271096631885, 0.4653396949870512, 0.44690115097910166, 0.47327215189579874, 0.463816842995584, 0.46189983503427356, 0.4709057229338214, 0.47426316991914064, 0.46286939608398825, 0.46529428008943796, 0.49388032604474574, 0.4668387370184064, 0.48669444397091866, 0.48691269499249756, 0.4762177369557321, 0.5562550249742344, 0.4635255780303851, 0.45949803304392844, 0.4757321070646867, 0.554402697016485, 0.4472514180233702, 0.48243899003136903, 0.46358107402920723, 0.4626587820239365, 0.4720385579857975, 0.4704232580261305, 0.47523791599087417, 0.5325399219291285, 0.5058744329726323, 0.5091750419232994, 0.497089519049041, 0.4573724700603634, 0.4470574710285291, 0.47581348300445825, 0.4940992599586025, 0.5039960610447451, 0.45344234199728817, 0.47367697407025844, 0.47322746098507196, 0.5490392630454153, 0.47802345897071064, 0.4625050399918109, 0.45873227901756763, 0.5517444669967517, 0.4607766600092873, 0.46669634303543717, 0.4720222020987421, 0.46996769797988236, 0.4736497129779309, 0.4802260610740632, 0.46817418094724417, 0.540467272978276, 0.4518571459921077, 0.4591544249560684, 0.4592459269333631, 0.46872745896689594, 0.4584158608922735, 0.44841110694687814]
[0.10032637567230862, 0.007769685307498641, 0.00780729755132022, 0.11005241453067913, 0.007705208959476072, 0.00831373047074113, 0.008028673286055575, 0.00801286993937909, 0.05449330091847069, 0.007502322612634423, 0.008331464244794024, 0.008312879491369335, 0.00989976706106824, 0.007638429937770172, 0.009510888059965658, 0.08475919810304304, 0.008039192101747101, 0.008188682325583483, 0.008192348144283252, 0.048935238100892425, 0.00733554210247738, 0.009119499733728565, 0.10121000065392226, 0.007695359674434425, 0.007874327184328315, 0.08389967585657249, 0.008399064426918571, 0.010460086653902367, 0.007945408673995003, 0.008985848674474627, 0.10076024193715837, 0.007992479061631828, 0.007244786632494355, 0.007675706062047761, 0.06216002434577641, 0.007882211632950574, 0.007721527203993529, 0.09204235428478569, 0.00960198173308935, 0.007229436143795598, 0.08947921212410972, 0.007468598654816801, 0.007972000855762435, 0.04702979036635358, 0.007087486551371308, 0.007261282408001776, 0.009063147633260459, 0.007551493652982219, 0.007738194856032425, 0.07016411728501244, 0.007429244571688528, 0.007504473205142636, 0.04201395177443949, 0.007762549326241929, 0.009536854716074862, 0.008161344161561253, 0.009392660225228387, 0.007707943020825636, 0.008621714203808533, 0.08212866559054474, 0.008384318877372662, 0.007775635182933539, 0.07357882008156073, 0.007881977856729408, 0.007581443715916604, 0.00777454899947102, 0.11159128342916695, 0.0075242892026483104, 0.007439223305816401, 0.008328353938627608, 0.07635509767284503, 0.007837590326707125, 0.007735062326418654, 0.007180380041007788, 0.007970259021207387, 0.007607345348129011, 0.010485420283880465, 0.00783546620324178, 0.009907774672349344, 0.00799681171442249, 0.007813215856820497, 0.029248164693482827, 0.007242328694033228, 0.007693698879198304, 0.007491347204171577, 0.009017186980618506, 0.010733280223984348, 0.007130643407510099, 0.007370975695322363, 0.007593757000143583, 0.007519166937991217, 0.007364561142190835, 0.007428436224082751, 0.007455654223734627, 0.00903956499901049, 0.007403312856806632, 0.0078121004483605525, 0.009403098633569874, 0.009315143531777573, 0.0093725891213636, 0.00938471506007624, 0.009801222467604948, 0.00946040885827067, 0.009511172652662712, 0.009882577654563499, 0.011194102511247049, 0.009273827408573456, 0.00937153483569926, 0.009368195856579256, 0.011214049530633706, 0.009574076695348687, 0.01023210736932423, 0.009419016979577742, 0.009903295267354317, 0.009779520185512244, 0.009549372470747603, 0.009431695427783594, 0.0096630623055698, 0.00951586275513531, 0.009227099265352043, 0.010413242817609285, 0.009383178755583964, 0.009522943142136293, 0.010056533979023901, 0.009950427184052462, 0.011173401101568371, 0.009842487838480393, 0.009804784835848426, 0.009447012795135379, 0.011657316019112358, 0.009876513121915715, 0.00986794634646147, 0.009906285325996578, 0.011108403429103901, 0.009784836775851341, 0.011067399367861146, 0.009198067103968742, 0.00991412853033339, 0.009452504305435079, 0.009496728469123493, 0.009120431652634728, 0.00965861534481222, 0.009465649857052736, 0.00942652724559742, 0.009610320876200437, 0.00967884020243144, 0.009446314205795678, 0.009495801634478326, 0.01007919032744379, 0.009527321163640946, 0.009932539672875891, 0.009936993775357093, 0.009718729325627186, 0.01135214336682111, 0.009459705674089491, 0.00937751087844752, 0.009708818511524218, 0.01131434075543847, 0.009127579959660617, 0.009845693674109573, 0.009460838245494025, 0.009442015959672173, 0.009633439958893828, 0.0096004746535945, 0.009698732979405595, 0.01086816167202303, 0.01032396801984964, 0.010391327386189784, 0.010144684062225327, 0.009334132042048233, 0.009123621857725084, 0.009710479244988943, 0.010083658366502091, 0.01028563389887235, 0.009253925346883431, 0.009666877021842008, 0.009657703285409632, 0.011204882919294189, 0.009755580795320625, 0.009438878367179814, 0.009361883245256483, 0.011260091163199015, 0.009403605306311985, 0.009524415163988513, 0.009633106165280451, 0.009591177509793518, 0.009666320673018997, 0.00980053185865435, 0.00955457512137233, 0.01102994434649543, 0.009221574408002198, 0.009370498468491192, 0.00937236585578292, 0.009565866509528488, 0.009355425732495377, 0.009151247080548533]
[9.967468607320708, 128.7053413907105, 128.08529371740153, 9.086579374605467, 129.78233364718466, 120.28294680941885, 124.55357994661807, 124.79923018412165, 18.35087952363419, 133.2920552251288, 120.02692091307446, 120.29526002851696, 101.01247775137999, 130.91695651422344, 105.1426526834352, 11.798129552668547, 124.39060882531679, 122.11976973093105, 122.06512496637677, 20.435171847702996, 136.32257657716627, 109.65513780339172, 9.880446532348149, 129.94844195810714, 126.99497704263867, 11.918997180746112, 119.06087978025877, 95.6015024624034, 125.85885019016818, 111.28609397135956, 9.924549413286194, 125.11762524352858, 138.03029001776184, 130.28117438530668, 16.087509786632623, 126.86794602413724, 129.5080589087099, 10.864563469399402, 104.14516792444043, 138.32337406537766, 11.17578011988949, 133.8939265875613, 125.43902316281937, 21.263118381140558, 141.0937421541233, 137.71672052005877, 110.33694257943485, 132.42413301970825, 129.2291055736901, 14.252299304755983, 134.60318749106932, 133.25385708815963, 23.801617266776162, 128.8236580499938, 104.85637348699967, 122.52883596183273, 106.46611034795357, 129.7362989448883, 115.98621531183007, 12.176016653984542, 119.2702728302437, 128.60685673562256, 13.590867574276386, 126.87170887523219, 131.90099900109843, 128.6248244197882, 8.961273401203906, 132.9029192083728, 134.4226351181237, 120.0717461540528, 13.09670251860133, 127.59023606942449, 129.28144051077112, 139.2683944706145, 125.46643682961681, 131.45190000424327, 95.37052144083614, 127.62482461940407, 100.93083796009248, 125.04983682390196, 127.98827247644212, 34.19017946869067, 138.07713544179165, 129.9764931928557, 133.48733849141945, 110.89933059493994, 93.16816286650398, 140.23979925104447, 135.6672496742327, 131.68712140526645, 132.99345635583867, 135.7854162240707, 134.61783474131906, 134.12639186197237, 110.62479224492157, 135.0746644565475, 128.00654658887035, 106.34792199561893, 107.35207638922702, 106.69410416387824, 106.55624529871196, 102.0280891802227, 105.70367676295088, 105.13950661173664, 101.18817528726707, 89.33275347400742, 107.83034403632756, 106.70610711392447, 106.74413892592808, 89.17385260946764, 104.44871414972303, 97.73157805184799, 106.16819166673064, 100.97649045125884, 102.25450543897219, 104.71892295156351, 106.02547629498629, 103.4868624849494, 105.08768629101363, 108.37642158625317, 96.03156456785514, 106.57369171453718, 105.00955272695967, 99.43783833334804, 100.49819786658998, 89.49826385984062, 101.6003287390795, 101.9910193586077, 105.85356680314199, 85.78303945440646, 101.25030844955059, 101.33820806176033, 100.94601226311836, 90.02193757025519, 102.19894546099813, 90.35546353409104, 108.71849364618402, 100.86615247526676, 105.79207030088438, 105.29942003199083, 109.64393332316877, 103.53450927488424, 105.6451501060873, 106.08360575916667, 104.05479826136282, 103.31816406564786, 105.86139505993368, 105.30969774780235, 99.21431856259159, 104.96129844098186, 100.67918507598144, 100.63405720147634, 102.89410955844953, 88.08909187340765, 105.71153421179261, 106.63810609895593, 102.99914441834662, 88.38340842079813, 109.55806516289145, 101.56724686952424, 105.6988793224825, 105.90958586292412, 103.80507941784343, 104.16151660018096, 103.10625131379653, 92.0118811421635, 96.8619815634187, 96.23409626464215, 98.57379430115452, 107.13368907737942, 109.60559475108973, 102.9815290029116, 99.17035699285485, 97.2229820574922, 108.06225061419836, 103.44602478551563, 103.54428692282866, 89.24680491556545, 102.50542955675805, 105.94479143593244, 106.81611528392902, 88.80922769686511, 106.34219189620492, 104.99332324161652, 103.80867633372405, 104.2624848699655, 103.45197866146093, 102.03527874019927, 104.66190147619766, 90.66228881904853, 108.44135239338641, 106.71790869637876, 106.69664579760102, 104.53836032563359, 106.88984430997877, 109.27472411116007]
Elapsed: 0.7653138383068334~1.0475899324679008
Time per graph: 0.01561864976136395~0.021379386376895935
Speed: 102.82834270596237~31.89088397230985
Total Time: 0.4490
best val loss: 0.3135344982147217 test_score: 0.9388

Testing...
Test loss: 0.3760 score: 0.9592 time: 0.56s
test Score 0.9592
Epoch Time List: [7.923659118940122, 9.207748327055015, 1.4872148950817063, 10.74356619711034, 7.560045945108868, 1.6494764690287411, 9.429147329065017, 1.7364736699964851, 5.2602265710011125, 8.180418497067876, 1.5457166510168463, 1.531742964987643, 12.797365090926178, 1.3900724129052833, 1.5375783210620284, 8.35525265603792, 18.265497262007557, 1.7849884229945019, 1.5618441288825125, 16.967584008118138, 1.5336890189209953, 1.5118753620190546, 8.725671448977664, 4.700059161172248, 1.5765038869576529, 7.772470562835224, 1.6527807540260255, 1.5821148490067571, 7.683307546074502, 1.4798189599532634, 5.988346641999669, 7.2782104189973325, 1.5038141058757901, 1.4959390969015658, 7.979308106936514, 4.38928855501581, 1.6920231770491228, 8.79599118605256, 4.938298386870883, 1.427105645998381, 6.586969259078614, 1.4352359669283032, 1.5876566120423377, 3.4097868141252548, 8.796874755993485, 1.4163004419533536, 1.5533243250101805, 7.96936046204064, 1.6421714950120077, 4.554225887055509, 6.6834568521007895, 1.520565114915371, 3.2268955130130053, 9.132425990072079, 1.547913048765622, 1.464365872903727, 14.335941906902008, 1.4498268159804866, 1.748888718080707, 8.251580965821631, 5.435493079130538, 1.4061997719109058, 6.845060609979555, 1.434191903914325, 1.5461277860449627, 1.4989638219121844, 8.914823223953135, 4.472729481989518, 1.5184299108805135, 1.446767644956708, 4.7895467159105465, 6.55743250309024, 1.6284682389814407, 1.416277106036432, 12.054401285131462, 1.4132568029453978, 1.7132551869144663, 8.799951805034652, 1.7227576959412545, 6.57991077308543, 1.556645117001608, 2.5463930839905515, 12.44134214008227, 1.413465806050226, 1.5361707038246095, 1.6356829339638352, 1.6406312569743022, 1.4364570169709623, 1.4706430219812319, 1.4420645500067621, 1.488934675930068, 1.3990223510190845, 1.5798063160618767, 1.397037714952603, 1.5636816860642284, 1.4000145850004628, 1.5436726240441203, 1.439770636963658, 1.4836718711303547, 1.3869103530887514, 1.5149906650185585, 1.3734645859804004, 1.506850284175016, 1.4606848231051117, 1.4669037479907274, 1.461744910106063, 1.3832474971422926, 1.3587632070994005, 1.4738155539380386, 1.4994700100505725, 1.3686345630558208, 1.4896116128657013, 1.5074229788733646, 1.407978605129756, 1.5217002930585295, 1.4236640458693728, 1.4445297691272572, 1.46064715704415, 1.6126477929065004, 1.4001271689776331, 1.475591677124612, 1.391963695990853, 1.501733117038384, 1.4917481262236834, 1.6352967689745128, 1.6403585079824552, 1.526567690889351, 1.4608117749448866, 1.5125458549009636, 1.5143629640806466, 1.4158951959107071, 1.4415670790476725, 1.569815056049265, 1.4804265419952571, 1.3979803059482947, 1.4810811439529061, 1.5860001990804449, 1.3818741249851882, 1.4901134959654883, 1.509428198914975, 1.4604357690550387, 1.4192559729563072, 1.4903773190453649, 1.4195914899464697, 1.5000987630337477, 1.3904316000407562, 1.4909243901493028, 1.4231649750145152, 1.4919537469977513, 1.4173253230983391, 1.5303194739390165, 1.4257863389793783, 1.4987874799408019, 1.490487452945672, 1.530893139890395, 1.3954885830171406, 1.467967708944343, 1.496247423114255, 1.4346488610608503, 1.4252954800613225, 1.49925210501533, 1.4962798540946096, 1.463600264978595, 1.4207520670024678, 1.526222402928397, 1.492097630049102, 1.612390099093318, 1.4814493488520384, 1.7476383401080966, 1.435110822902061, 1.461283787037246, 1.403561029001139, 1.5356185629498214, 1.4641577310394496, 1.4814007020322606, 1.411448203958571, 1.4960418069968, 1.4733001559507102, 1.3907451620325446, 1.4043862831313163, 1.4770810491172597, 1.4827914090128615, 1.4038887660717592, 1.4231864380417392, 1.4919741589110345, 1.3991872171172872, 1.5315719748614356, 1.4079329658998176, 1.4949637049576268, 1.4835311949718744, 1.5220025669550523, 1.3678106530569494, 1.5200861990451813, 1.4094363249605522, 1.46480989002157, 1.4900362610351294]
Total Epoch List: [97, 99]
Total Time List: [0.38325287494808435, 0.44903752696700394]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x73442cba3460>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.43s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.64s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.42s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.42s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.40s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.62s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.43s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.51s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.40s
Epoch 10/1000, LR 0.000240
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.51s
Epoch 11/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.46s
Epoch 12/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.50s
Epoch 13/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.45s
Epoch 14/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.43s
Epoch 15/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.42s
Epoch 16/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.59s
Val loss: 0.6916 score: 0.5102 time: 0.49s
Test loss: 0.6910 score: 0.5208 time: 0.43s
Epoch 17/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.57s
Val loss: 0.6912 score: 0.5918 time: 0.40s
Test loss: 0.6906 score: 0.5625 time: 0.42s
Epoch 18/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.61s
Val loss: 0.6908 score: 0.6122 time: 0.49s
Test loss: 0.6901 score: 0.6458 time: 0.41s
Epoch 19/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.58s
Val loss: 0.6903 score: 0.6327 time: 0.40s
Test loss: 0.6896 score: 0.6458 time: 0.42s
Epoch 20/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.59s
Val loss: 0.6898 score: 0.6531 time: 0.40s
Test loss: 0.6890 score: 0.7500 time: 0.52s
Epoch 21/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.64s
Val loss: 0.6892 score: 0.6735 time: 0.39s
Test loss: 0.6884 score: 0.7917 time: 0.41s
Epoch 22/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.58s
Val loss: 0.6886 score: 0.7143 time: 0.49s
Test loss: 0.6877 score: 0.7083 time: 0.42s
Epoch 23/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.67s
Val loss: 0.6879 score: 0.6735 time: 0.40s
Test loss: 0.6869 score: 0.6875 time: 0.42s
Epoch 24/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.58s
Val loss: 0.6871 score: 0.6327 time: 0.49s
Test loss: 0.6860 score: 0.7083 time: 0.41s
Epoch 25/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.66s
Val loss: 0.6863 score: 0.6531 time: 0.40s
Test loss: 0.6850 score: 0.7083 time: 0.43s
Epoch 26/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.58s
Val loss: 0.6853 score: 0.6531 time: 0.42s
Test loss: 0.6839 score: 0.7292 time: 0.52s
Epoch 27/1000, LR 0.000270
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.62s
Val loss: 0.6843 score: 0.6327 time: 0.41s
Test loss: 0.6827 score: 0.7292 time: 0.44s
Epoch 28/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.59s
Val loss: 0.6832 score: 0.6531 time: 0.39s
Test loss: 0.6814 score: 0.7500 time: 0.49s
Epoch 29/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.55s
Val loss: 0.6821 score: 0.6735 time: 0.39s
Test loss: 0.6800 score: 0.7500 time: 0.41s
Epoch 30/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.65s
Val loss: 0.6808 score: 0.6735 time: 0.42s
Test loss: 0.6784 score: 0.7292 time: 0.50s
Epoch 31/1000, LR 0.000270
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.61s
Val loss: 0.6794 score: 0.6735 time: 0.39s
Test loss: 0.6767 score: 0.7292 time: 0.42s
Epoch 32/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.57s
Val loss: 0.6778 score: 0.6531 time: 0.40s
Test loss: 0.6748 score: 0.7083 time: 0.52s
Epoch 33/1000, LR 0.000270
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.57s
Val loss: 0.6760 score: 0.6531 time: 0.41s
Test loss: 0.6728 score: 0.7083 time: 0.42s
Epoch 34/1000, LR 0.000270
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.59s
Val loss: 0.6741 score: 0.6531 time: 0.41s
Test loss: 0.6705 score: 0.7083 time: 0.52s
Epoch 35/1000, LR 0.000270
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.56s
Val loss: 0.6720 score: 0.6531 time: 0.40s
Test loss: 0.6679 score: 0.7083 time: 0.42s
Epoch 36/1000, LR 0.000270
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.58s
Val loss: 0.6696 score: 0.6531 time: 0.41s
Test loss: 0.6651 score: 0.7083 time: 0.49s
Epoch 37/1000, LR 0.000270
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 0.59s
Val loss: 0.6671 score: 0.6531 time: 0.41s
Test loss: 0.6621 score: 0.7083 time: 0.41s
Epoch 38/1000, LR 0.000270
Train loss: 0.6576;  Loss pred: 0.6576; Loss self: 0.0000; time: 0.55s
Val loss: 0.6642 score: 0.6531 time: 0.48s
Test loss: 0.6587 score: 0.7083 time: 0.42s
Epoch 39/1000, LR 0.000269
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.57s
Val loss: 0.6611 score: 0.6531 time: 0.41s
Test loss: 0.6549 score: 0.7083 time: 0.42s
Epoch 40/1000, LR 0.000269
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.60s
Val loss: 0.6577 score: 0.6531 time: 0.48s
Test loss: 0.6508 score: 0.7083 time: 0.43s
Epoch 41/1000, LR 0.000269
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.57s
Val loss: 0.6540 score: 0.6531 time: 0.39s
Test loss: 0.6463 score: 0.7292 time: 0.43s
Epoch 42/1000, LR 0.000269
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.57s
Val loss: 0.6500 score: 0.6531 time: 0.50s
Test loss: 0.6416 score: 0.7292 time: 0.43s
Epoch 43/1000, LR 0.000269
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.57s
Val loss: 0.6458 score: 0.6735 time: 0.40s
Test loss: 0.6365 score: 0.7500 time: 0.44s
Epoch 44/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.58s
Val loss: 0.6413 score: 0.6735 time: 0.39s
Test loss: 0.6310 score: 0.7500 time: 0.49s
Epoch 45/1000, LR 0.000269
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.56s
Val loss: 0.6365 score: 0.6531 time: 0.39s
Test loss: 0.6251 score: 0.7500 time: 0.43s
Epoch 46/1000, LR 0.000269
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.59s
Val loss: 0.6313 score: 0.6531 time: 0.50s
Test loss: 0.6187 score: 0.7500 time: 0.42s
Epoch 47/1000, LR 0.000269
Train loss: 0.6085;  Loss pred: 0.6085; Loss self: 0.0000; time: 0.59s
Val loss: 0.6257 score: 0.6531 time: 0.44s
Test loss: 0.6120 score: 0.7500 time: 0.41s
Epoch 48/1000, LR 0.000269
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.57s
Val loss: 0.6198 score: 0.6531 time: 0.49s
Test loss: 0.6048 score: 0.7500 time: 0.41s
Epoch 49/1000, LR 0.000269
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.61s
Val loss: 0.6136 score: 0.6939 time: 0.42s
Test loss: 0.5973 score: 0.7708 time: 0.43s
Epoch 50/1000, LR 0.000269
Train loss: 0.5795;  Loss pred: 0.5795; Loss self: 0.0000; time: 0.59s
Val loss: 0.6070 score: 0.6939 time: 0.42s
Test loss: 0.5894 score: 0.7917 time: 0.54s
Epoch 51/1000, LR 0.000269
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.58s
Val loss: 0.6001 score: 0.6939 time: 0.39s
Test loss: 0.5810 score: 0.7708 time: 0.41s
Epoch 52/1000, LR 0.000269
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.60s
Val loss: 0.5926 score: 0.6939 time: 0.40s
Test loss: 0.5720 score: 0.7708 time: 0.51s
Epoch 53/1000, LR 0.000269
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.57s
Val loss: 0.5847 score: 0.7143 time: 0.41s
Test loss: 0.5626 score: 0.7917 time: 0.42s
Epoch 54/1000, LR 0.000269
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.58s
Val loss: 0.5763 score: 0.7143 time: 0.40s
Test loss: 0.5526 score: 0.7917 time: 0.54s
Epoch 55/1000, LR 0.000269
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.63s
Val loss: 0.5674 score: 0.7755 time: 0.40s
Test loss: 0.5421 score: 0.7917 time: 0.42s
Epoch 56/1000, LR 0.000269
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.59s
Val loss: 0.5580 score: 0.7959 time: 0.40s
Test loss: 0.5311 score: 0.8333 time: 0.51s
Epoch 57/1000, LR 0.000269
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.58s
Val loss: 0.5480 score: 0.7959 time: 0.40s
Test loss: 0.5195 score: 0.8542 time: 0.47s
Epoch 58/1000, LR 0.000269
Train loss: 0.5040;  Loss pred: 0.5040; Loss self: 0.0000; time: 0.65s
Val loss: 0.5376 score: 0.7959 time: 0.39s
Test loss: 0.5077 score: 0.8750 time: 0.48s
Epoch 59/1000, LR 0.000268
Train loss: 0.4827;  Loss pred: 0.4827; Loss self: 0.0000; time: 0.56s
Val loss: 0.5268 score: 0.8367 time: 0.39s
Test loss: 0.4954 score: 0.8958 time: 0.43s
Epoch 60/1000, LR 0.000268
Train loss: 0.4683;  Loss pred: 0.4683; Loss self: 0.0000; time: 0.57s
Val loss: 0.5157 score: 0.8571 time: 0.40s
Test loss: 0.4829 score: 0.9167 time: 0.51s
Epoch 61/1000, LR 0.000268
Train loss: 0.4542;  Loss pred: 0.4542; Loss self: 0.0000; time: 0.61s
Val loss: 0.5044 score: 0.8571 time: 0.42s
Test loss: 0.4704 score: 0.9167 time: 0.43s
Epoch 62/1000, LR 0.000268
Train loss: 0.4406;  Loss pred: 0.4406; Loss self: 0.0000; time: 0.55s
Val loss: 0.4929 score: 0.8571 time: 0.47s
Test loss: 0.4577 score: 0.9167 time: 0.41s
Epoch 63/1000, LR 0.000268
Train loss: 0.4301;  Loss pred: 0.4301; Loss self: 0.0000; time: 0.57s
Val loss: 0.4815 score: 0.8571 time: 0.41s
Test loss: 0.4450 score: 0.9167 time: 0.41s
Epoch 64/1000, LR 0.000268
Train loss: 0.4085;  Loss pred: 0.4085; Loss self: 0.0000; time: 0.68s
Val loss: 0.4700 score: 0.8571 time: 0.50s
Test loss: 0.4323 score: 0.9375 time: 0.43s
Epoch 65/1000, LR 0.000268
Train loss: 0.3971;  Loss pred: 0.3971; Loss self: 0.0000; time: 0.58s
Val loss: 0.4587 score: 0.8571 time: 0.39s
Test loss: 0.4199 score: 0.9375 time: 0.41s
Epoch 66/1000, LR 0.000268
Train loss: 0.3787;  Loss pred: 0.3787; Loss self: 0.0000; time: 0.59s
Val loss: 0.4475 score: 0.8571 time: 0.49s
Test loss: 0.4079 score: 0.9375 time: 0.43s
Epoch 67/1000, LR 0.000268
Train loss: 0.3612;  Loss pred: 0.3612; Loss self: 0.0000; time: 0.67s
Val loss: 0.4366 score: 0.8776 time: 0.40s
Test loss: 0.3962 score: 0.9375 time: 0.42s
Epoch 68/1000, LR 0.000268
Train loss: 0.3478;  Loss pred: 0.3478; Loss self: 0.0000; time: 0.58s
Val loss: 0.4260 score: 0.8980 time: 0.39s
Test loss: 0.3847 score: 0.9375 time: 0.50s
Epoch 69/1000, LR 0.000268
Train loss: 0.3357;  Loss pred: 0.3357; Loss self: 0.0000; time: 0.58s
Val loss: 0.4156 score: 0.8980 time: 0.39s
Test loss: 0.3734 score: 0.9375 time: 0.41s
Epoch 70/1000, LR 0.000268
Train loss: 0.3173;  Loss pred: 0.3173; Loss self: 0.0000; time: 0.62s
Val loss: 0.4055 score: 0.8980 time: 0.48s
Test loss: 0.3622 score: 0.9375 time: 0.44s
Epoch 71/1000, LR 0.000268
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.56s
Val loss: 0.3956 score: 0.8980 time: 0.41s
Test loss: 0.3509 score: 0.9375 time: 0.42s
Epoch 72/1000, LR 0.000267
Train loss: 0.2979;  Loss pred: 0.2979; Loss self: 0.0000; time: 0.61s
Val loss: 0.3863 score: 0.9184 time: 0.48s
Test loss: 0.3402 score: 0.9375 time: 0.42s
Epoch 73/1000, LR 0.000267
Train loss: 0.2742;  Loss pred: 0.2742; Loss self: 0.0000; time: 0.58s
Val loss: 0.3773 score: 0.9184 time: 0.39s
Test loss: 0.3298 score: 0.9375 time: 0.45s
Epoch 74/1000, LR 0.000267
Train loss: 0.2698;  Loss pred: 0.2698; Loss self: 0.0000; time: 0.58s
Val loss: 0.3687 score: 0.9184 time: 0.41s
Test loss: 0.3199 score: 0.9375 time: 0.52s
Epoch 75/1000, LR 0.000267
Train loss: 0.2478;  Loss pred: 0.2478; Loss self: 0.0000; time: 0.59s
Val loss: 0.3604 score: 0.9184 time: 0.41s
Test loss: 0.3101 score: 0.9375 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.2391;  Loss pred: 0.2391; Loss self: 0.0000; time: 0.63s
Val loss: 0.3523 score: 0.9184 time: 0.40s
Test loss: 0.3006 score: 0.9375 time: 0.51s
Epoch 77/1000, LR 0.000267
Train loss: 0.2267;  Loss pred: 0.2267; Loss self: 0.0000; time: 0.70s
Val loss: 0.3445 score: 0.9184 time: 0.39s
Test loss: 0.2910 score: 0.9375 time: 0.41s
Epoch 78/1000, LR 0.000267
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.59s
Val loss: 0.3371 score: 0.9184 time: 0.41s
Test loss: 0.2820 score: 0.9375 time: 0.52s
Epoch 79/1000, LR 0.000267
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.58s
Val loss: 0.3302 score: 0.9184 time: 0.42s
Test loss: 0.2741 score: 0.9167 time: 0.45s
Epoch 80/1000, LR 0.000267
Train loss: 0.1966;  Loss pred: 0.1966; Loss self: 0.0000; time: 0.65s
Val loss: 0.3237 score: 0.9184 time: 0.41s
Test loss: 0.2671 score: 0.9167 time: 0.52s
Epoch 81/1000, LR 0.000267
Train loss: 0.1846;  Loss pred: 0.1846; Loss self: 0.0000; time: 0.59s
Val loss: 0.3179 score: 0.9184 time: 0.41s
Test loss: 0.2619 score: 0.8958 time: 0.47s
Epoch 82/1000, LR 0.000267
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.58s
Val loss: 0.3129 score: 0.9184 time: 0.40s
Test loss: 0.2580 score: 0.8958 time: 0.50s
Epoch 83/1000, LR 0.000266
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.56s
Val loss: 0.3088 score: 0.9184 time: 0.40s
Test loss: 0.2551 score: 0.8958 time: 0.43s
Epoch 84/1000, LR 0.000266
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 0.61s
Val loss: 0.3051 score: 0.9184 time: 0.40s
Test loss: 0.2519 score: 0.8958 time: 0.57s
Epoch 85/1000, LR 0.000266
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.63s
Val loss: 0.3024 score: 0.9184 time: 0.41s
Test loss: 0.2495 score: 0.8958 time: 0.41s
Epoch 86/1000, LR 0.000266
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.60s
Val loss: 0.3011 score: 0.9184 time: 0.48s
Test loss: 0.2483 score: 0.8958 time: 0.42s
Epoch 87/1000, LR 0.000266
Train loss: 0.1234;  Loss pred: 0.1234; Loss self: 0.0000; time: 0.59s
Val loss: 0.3009 score: 0.9184 time: 0.41s
Test loss: 0.2482 score: 0.8958 time: 0.45s
Epoch 88/1000, LR 0.000266
Train loss: 0.1155;  Loss pred: 0.1155; Loss self: 0.0000; time: 0.68s
Val loss: 0.3011 score: 0.8980 time: 0.50s
Test loss: 0.2477 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1091;  Loss pred: 0.1091; Loss self: 0.0000; time: 0.61s
Val loss: 0.3018 score: 0.8980 time: 0.41s
Test loss: 0.2474 score: 0.8958 time: 0.45s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1234,   Val_Loss: 0.3009,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.3009,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2482


[4.915992407943122, 0.3807145800674334, 0.38255758001469076, 5.392568312003277, 0.3775552390143275, 0.40737279306631535, 0.3934049910167232, 0.3926306270295754, 2.6701717450050637, 0.3676138080190867, 0.4082417479949072, 0.4073310950770974, 0.4850885859923437, 0.37428306695073843, 0.46603351493831724, 4.153200707049109, 0.393920412985608, 0.4012454339535907, 0.40142505906987935, 2.397826666943729, 0.35944156302139163, 0.4468554869526997, 4.95929003204219, 0.3770726240472868, 0.38584203203208745, 4.111084116972052, 0.41155415691901, 0.512544246041216, 0.38932502502575517, 0.4403065850492567, 4.93725185492076, 0.39163147401995957, 0.3549945449922234, 0.3761095970403403, 3.045841192943044, 0.3862283700145781, 0.37835483299568295, 4.510075359954499, 0.4704971049213782, 0.35424237104598433, 4.3844813940813765, 0.36596133408602327, 0.39062804193235934, 2.3044597279513255, 0.3472868410171941, 0.355802837992087, 0.44409423402976245, 0.37002318899612874, 0.3791715479455888, 3.4380417469656095, 0.36403298401273787, 0.36771918705198914, 2.058683636947535, 0.3803649169858545, 0.46730588108766824, 0.3999058639165014, 0.460240351036191, 0.37768920802045614, 0.4224639959866181, 4.0243046139366925, 0.41083162499126047, 0.38100612396374345, 3.6053621839964762, 0.386216914979741, 0.3714907420799136, 0.38095290097407997, 5.4679728880291805, 0.3686901709297672, 0.36452194198500365, 0.4080893429927528, 3.7413997859694064, 0.38404192600864917, 0.37901805399451405, 0.35183862200938165, 0.3905426920391619, 0.37275992205832154, 0.5137855939101428, 0.3839378439588472, 0.4854809589451179, 0.391843774006702, 0.38284757698420435, 1.4331600699806586, 0.35487410600762814, 0.3769912450807169, 0.3670760130044073, 0.4418421620503068, 0.525930730975233, 0.34940152696799487, 0.3611778090707958, 0.37209409300703555, 0.3684391799615696, 0.3608634959673509, 0.3639933749800548, 0.3653270569629967, 0.442938684951514, 0.362762329983525, 0.3827929219696671, 0.46075183304492384, 0.45644203305710107, 0.45925686694681644, 0.4598510379437357, 0.4802599009126425, 0.4635600340552628, 0.4660474599804729, 0.48424630507361144, 0.5485110230511054, 0.4544175430200994, 0.4592052069492638, 0.45904159697238356, 0.5494884270010516, 0.4691297580720857, 0.5013732610968873, 0.46153183199930936, 0.4852614681003615, 0.47919648909009993, 0.46791925106663257, 0.4621530759613961, 0.47349005297292024, 0.4662772750016302, 0.45212786400225013, 0.510248898062855, 0.45977575902361423, 0.46662421396467835, 0.4927701649721712, 0.4875709320185706, 0.5474966539768502, 0.4822819040855393, 0.48043445695657283, 0.46290362696163356, 0.5712084849365056, 0.48394914297387004, 0.48352937097661197, 0.4854079809738323, 0.5443117680260912, 0.4794570020167157, 0.5423025690251961, 0.4507052880944684, 0.48579229798633605, 0.46317271096631885, 0.4653396949870512, 0.44690115097910166, 0.47327215189579874, 0.463816842995584, 0.46189983503427356, 0.4709057229338214, 0.47426316991914064, 0.46286939608398825, 0.46529428008943796, 0.49388032604474574, 0.4668387370184064, 0.48669444397091866, 0.48691269499249756, 0.4762177369557321, 0.5562550249742344, 0.4635255780303851, 0.45949803304392844, 0.4757321070646867, 0.554402697016485, 0.4472514180233702, 0.48243899003136903, 0.46358107402920723, 0.4626587820239365, 0.4720385579857975, 0.4704232580261305, 0.47523791599087417, 0.5325399219291285, 0.5058744329726323, 0.5091750419232994, 0.497089519049041, 0.4573724700603634, 0.4470574710285291, 0.47581348300445825, 0.4940992599586025, 0.5039960610447451, 0.45344234199728817, 0.47367697407025844, 0.47322746098507196, 0.5490392630454153, 0.47802345897071064, 0.4625050399918109, 0.45873227901756763, 0.5517444669967517, 0.4607766600092873, 0.46669634303543717, 0.4720222020987421, 0.46996769797988236, 0.4736497129779309, 0.4802260610740632, 0.46817418094724417, 0.540467272978276, 0.4518571459921077, 0.4591544249560684, 0.4592459269333631, 0.46872745896689594, 0.4584158608922735, 0.44841110694687814, 0.43028384889476, 0.6490593060152605, 0.427501403959468, 0.4197618479374796, 0.39934392599388957, 0.6263940019998699, 0.43379183602519333, 0.5176920980447903, 0.40105044608935714, 0.5118093710625544, 0.46628552803304046, 0.507260211976245, 0.4568709450541064, 0.43106018006801605, 0.42285673203878105, 0.4363867830252275, 0.42607322009280324, 0.4164452280383557, 0.4273644069908187, 0.5212638200027868, 0.4097940609790385, 0.4203084269538522, 0.4215806720312685, 0.417745991027914, 0.4328257660381496, 0.5267809049692005, 0.4437236749799922, 0.49030344805214554, 0.41834163304883987, 0.49943688709754497, 0.4278423829236999, 0.5200983330141753, 0.41955939098261297, 0.5287724039517343, 0.4272298979340121, 0.49850976606830955, 0.41411210503429174, 0.42668225103989244, 0.4226873699808493, 0.43438332399819046, 0.43169469595886767, 0.4341499200090766, 0.4394461699994281, 0.49889172799885273, 0.43135796510614455, 0.4210768169723451, 0.4145903470925987, 0.4132881569676101, 0.43301947601139545, 0.5400322979548946, 0.4141476840013638, 0.5111560920486227, 0.4209879069821909, 0.5435267509892583, 0.4210961420321837, 0.5126932420535013, 0.47899770794901997, 0.4886890919879079, 0.4318421729840338, 0.5169761619763449, 0.432591475895606, 0.4132656609872356, 0.4187717429595068, 0.43183343298733234, 0.41603664797730744, 0.4322531189536676, 0.42641158890910447, 0.499590881052427, 0.41555915703065693, 0.44730169395916164, 0.4230140830622986, 0.42248909501358867, 0.4512385129928589, 0.5196280130185187, 0.4700001240707934, 0.5100650649983436, 0.4138674431014806, 0.519734280067496, 0.4567303129006177, 0.5229793631006032, 0.4782778370426968, 0.5071005380013958, 0.4355029440484941, 0.5751022159820423, 0.4167288140160963, 0.4290183660341427, 0.45078782399650663, 0.4486286999890581, 0.45976417092606425]
[0.10032637567230862, 0.007769685307498641, 0.00780729755132022, 0.11005241453067913, 0.007705208959476072, 0.00831373047074113, 0.008028673286055575, 0.00801286993937909, 0.05449330091847069, 0.007502322612634423, 0.008331464244794024, 0.008312879491369335, 0.00989976706106824, 0.007638429937770172, 0.009510888059965658, 0.08475919810304304, 0.008039192101747101, 0.008188682325583483, 0.008192348144283252, 0.048935238100892425, 0.00733554210247738, 0.009119499733728565, 0.10121000065392226, 0.007695359674434425, 0.007874327184328315, 0.08389967585657249, 0.008399064426918571, 0.010460086653902367, 0.007945408673995003, 0.008985848674474627, 0.10076024193715837, 0.007992479061631828, 0.007244786632494355, 0.007675706062047761, 0.06216002434577641, 0.007882211632950574, 0.007721527203993529, 0.09204235428478569, 0.00960198173308935, 0.007229436143795598, 0.08947921212410972, 0.007468598654816801, 0.007972000855762435, 0.04702979036635358, 0.007087486551371308, 0.007261282408001776, 0.009063147633260459, 0.007551493652982219, 0.007738194856032425, 0.07016411728501244, 0.007429244571688528, 0.007504473205142636, 0.04201395177443949, 0.007762549326241929, 0.009536854716074862, 0.008161344161561253, 0.009392660225228387, 0.007707943020825636, 0.008621714203808533, 0.08212866559054474, 0.008384318877372662, 0.007775635182933539, 0.07357882008156073, 0.007881977856729408, 0.007581443715916604, 0.00777454899947102, 0.11159128342916695, 0.0075242892026483104, 0.007439223305816401, 0.008328353938627608, 0.07635509767284503, 0.007837590326707125, 0.007735062326418654, 0.007180380041007788, 0.007970259021207387, 0.007607345348129011, 0.010485420283880465, 0.00783546620324178, 0.009907774672349344, 0.00799681171442249, 0.007813215856820497, 0.029248164693482827, 0.007242328694033228, 0.007693698879198304, 0.007491347204171577, 0.009017186980618506, 0.010733280223984348, 0.007130643407510099, 0.007370975695322363, 0.007593757000143583, 0.007519166937991217, 0.007364561142190835, 0.007428436224082751, 0.007455654223734627, 0.00903956499901049, 0.007403312856806632, 0.0078121004483605525, 0.009403098633569874, 0.009315143531777573, 0.0093725891213636, 0.00938471506007624, 0.009801222467604948, 0.00946040885827067, 0.009511172652662712, 0.009882577654563499, 0.011194102511247049, 0.009273827408573456, 0.00937153483569926, 0.009368195856579256, 0.011214049530633706, 0.009574076695348687, 0.01023210736932423, 0.009419016979577742, 0.009903295267354317, 0.009779520185512244, 0.009549372470747603, 0.009431695427783594, 0.0096630623055698, 0.00951586275513531, 0.009227099265352043, 0.010413242817609285, 0.009383178755583964, 0.009522943142136293, 0.010056533979023901, 0.009950427184052462, 0.011173401101568371, 0.009842487838480393, 0.009804784835848426, 0.009447012795135379, 0.011657316019112358, 0.009876513121915715, 0.00986794634646147, 0.009906285325996578, 0.011108403429103901, 0.009784836775851341, 0.011067399367861146, 0.009198067103968742, 0.00991412853033339, 0.009452504305435079, 0.009496728469123493, 0.009120431652634728, 0.00965861534481222, 0.009465649857052736, 0.00942652724559742, 0.009610320876200437, 0.00967884020243144, 0.009446314205795678, 0.009495801634478326, 0.01007919032744379, 0.009527321163640946, 0.009932539672875891, 0.009936993775357093, 0.009718729325627186, 0.01135214336682111, 0.009459705674089491, 0.00937751087844752, 0.009708818511524218, 0.01131434075543847, 0.009127579959660617, 0.009845693674109573, 0.009460838245494025, 0.009442015959672173, 0.009633439958893828, 0.0096004746535945, 0.009698732979405595, 0.01086816167202303, 0.01032396801984964, 0.010391327386189784, 0.010144684062225327, 0.009334132042048233, 0.009123621857725084, 0.009710479244988943, 0.010083658366502091, 0.01028563389887235, 0.009253925346883431, 0.009666877021842008, 0.009657703285409632, 0.011204882919294189, 0.009755580795320625, 0.009438878367179814, 0.009361883245256483, 0.011260091163199015, 0.009403605306311985, 0.009524415163988513, 0.009633106165280451, 0.009591177509793518, 0.009666320673018997, 0.00980053185865435, 0.00955457512137233, 0.01102994434649543, 0.009221574408002198, 0.009370498468491192, 0.00937236585578292, 0.009565866509528488, 0.009355425732495377, 0.009151247080548533, 0.008964246851974167, 0.013522068875317927, 0.008906279249155583, 0.008745038498697491, 0.0083196651248727, 0.013049875041663958, 0.009037329917191528, 0.010785252042599799, 0.008355217626861608, 0.010662695230469884, 0.009714281834021676, 0.010567921082838438, 0.009518144688627217, 0.008980420418083668, 0.008809515250807939, 0.009091391313025573, 0.008876525418600067, 0.008675942250799077, 0.008903425145642055, 0.010859662916724725, 0.008537376270396635, 0.008756425561538586, 0.008782930667318093, 0.008703041479748208, 0.009017203459128117, 0.010974602186858343, 0.009244243228749838, 0.010214655167753032, 0.008715450688517498, 0.01040493514786552, 0.008913382977577081, 0.010835381937795319, 0.008740820645471103, 0.011016091748994464, 0.008900622873625252, 0.010385620126423115, 0.008627335521547744, 0.008889213563331092, 0.008805986874601027, 0.009049652583295634, 0.008993639499143077, 0.009044790000189096, 0.009155128541654753, 0.010393577666642765, 0.008986624273044677, 0.008772433686923856, 0.008637298897762472, 0.00861016993682521, 0.00902123908357074, 0.011250672874060305, 0.008628076750028413, 0.010649085251012972, 0.00877058139546231, 0.01132347397894288, 0.00877283629233716, 0.010681109209447944, 0.009979118915604582, 0.010181022749748081, 0.00899671193716737, 0.010770336707840519, 0.009012322414491791, 0.008609701270567408, 0.008724411311656391, 0.008996529853902757, 0.008667430166193904, 0.009005273311534742, 0.008883574768939676, 0.010408143355258895, 0.008657482438138686, 0.009318785290815867, 0.008812793397131221, 0.00880185614611643, 0.009400802354017893, 0.010825583604552472, 0.009791669251474863, 0.010626355520798825, 0.008622238397947513, 0.010827797501406167, 0.009515214852096202, 0.010895403397929234, 0.009964121605056183, 0.010564594541695746, 0.009072978001010293, 0.011981296166292546, 0.008681850292002006, 0.008937882625711305, 0.009391412999927221, 0.009346431249772044, 0.009578420227626339]
[9.967468607320708, 128.7053413907105, 128.08529371740153, 9.086579374605467, 129.78233364718466, 120.28294680941885, 124.55357994661807, 124.79923018412165, 18.35087952363419, 133.2920552251288, 120.02692091307446, 120.29526002851696, 101.01247775137999, 130.91695651422344, 105.1426526834352, 11.798129552668547, 124.39060882531679, 122.11976973093105, 122.06512496637677, 20.435171847702996, 136.32257657716627, 109.65513780339172, 9.880446532348149, 129.94844195810714, 126.99497704263867, 11.918997180746112, 119.06087978025877, 95.6015024624034, 125.85885019016818, 111.28609397135956, 9.924549413286194, 125.11762524352858, 138.03029001776184, 130.28117438530668, 16.087509786632623, 126.86794602413724, 129.5080589087099, 10.864563469399402, 104.14516792444043, 138.32337406537766, 11.17578011988949, 133.8939265875613, 125.43902316281937, 21.263118381140558, 141.0937421541233, 137.71672052005877, 110.33694257943485, 132.42413301970825, 129.2291055736901, 14.252299304755983, 134.60318749106932, 133.25385708815963, 23.801617266776162, 128.8236580499938, 104.85637348699967, 122.52883596183273, 106.46611034795357, 129.7362989448883, 115.98621531183007, 12.176016653984542, 119.2702728302437, 128.60685673562256, 13.590867574276386, 126.87170887523219, 131.90099900109843, 128.6248244197882, 8.961273401203906, 132.9029192083728, 134.4226351181237, 120.0717461540528, 13.09670251860133, 127.59023606942449, 129.28144051077112, 139.2683944706145, 125.46643682961681, 131.45190000424327, 95.37052144083614, 127.62482461940407, 100.93083796009248, 125.04983682390196, 127.98827247644212, 34.19017946869067, 138.07713544179165, 129.9764931928557, 133.48733849141945, 110.89933059493994, 93.16816286650398, 140.23979925104447, 135.6672496742327, 131.68712140526645, 132.99345635583867, 135.7854162240707, 134.61783474131906, 134.12639186197237, 110.62479224492157, 135.0746644565475, 128.00654658887035, 106.34792199561893, 107.35207638922702, 106.69410416387824, 106.55624529871196, 102.0280891802227, 105.70367676295088, 105.13950661173664, 101.18817528726707, 89.33275347400742, 107.83034403632756, 106.70610711392447, 106.74413892592808, 89.17385260946764, 104.44871414972303, 97.73157805184799, 106.16819166673064, 100.97649045125884, 102.25450543897219, 104.71892295156351, 106.02547629498629, 103.4868624849494, 105.08768629101363, 108.37642158625317, 96.03156456785514, 106.57369171453718, 105.00955272695967, 99.43783833334804, 100.49819786658998, 89.49826385984062, 101.6003287390795, 101.9910193586077, 105.85356680314199, 85.78303945440646, 101.25030844955059, 101.33820806176033, 100.94601226311836, 90.02193757025519, 102.19894546099813, 90.35546353409104, 108.71849364618402, 100.86615247526676, 105.79207030088438, 105.29942003199083, 109.64393332316877, 103.53450927488424, 105.6451501060873, 106.08360575916667, 104.05479826136282, 103.31816406564786, 105.86139505993368, 105.30969774780235, 99.21431856259159, 104.96129844098186, 100.67918507598144, 100.63405720147634, 102.89410955844953, 88.08909187340765, 105.71153421179261, 106.63810609895593, 102.99914441834662, 88.38340842079813, 109.55806516289145, 101.56724686952424, 105.6988793224825, 105.90958586292412, 103.80507941784343, 104.16151660018096, 103.10625131379653, 92.0118811421635, 96.8619815634187, 96.23409626464215, 98.57379430115452, 107.13368907737942, 109.60559475108973, 102.9815290029116, 99.17035699285485, 97.2229820574922, 108.06225061419836, 103.44602478551563, 103.54428692282866, 89.24680491556545, 102.50542955675805, 105.94479143593244, 106.81611528392902, 88.80922769686511, 106.34219189620492, 104.99332324161652, 103.80867633372405, 104.2624848699655, 103.45197866146093, 102.03527874019927, 104.66190147619766, 90.66228881904853, 108.44135239338641, 106.71790869637876, 106.69664579760102, 104.53836032563359, 106.88984430997877, 109.27472411116007, 111.55426847485501, 73.95318048004604, 112.28033301277988, 114.35055433420247, 120.19714555702159, 76.62908624085128, 110.65215159377112, 92.71920545298158, 119.68569158331077, 93.78491820176802, 102.94121758931959, 94.62599050100117, 105.06249197859462, 111.3533613622724, 113.51362379539418, 109.99416542188246, 112.65669311379179, 115.26125590657284, 112.31632586808104, 92.0838894971521, 117.13200500105653, 114.20185016959026, 113.85721211725726, 114.90235940239728, 110.89912793169815, 91.11947594760754, 108.17543148258734, 97.89855688490901, 114.73875944447579, 96.10823957947889, 112.19084858304038, 92.29024004330304, 114.40573380465503, 90.77629551254226, 112.35168753899768, 96.286980250298, 115.91064210988283, 112.49589098917608, 113.55910634891866, 110.50147956462517, 111.18969134745517, 110.5608864306516, 109.22839536879447, 96.21326092644772, 111.27648932641966, 113.99345218084632, 115.77693580328152, 116.1417262768597, 110.84951753702833, 88.88357267107222, 115.90068435549173, 93.90477927715689, 114.0175268788197, 88.31212063184839, 113.98822076202123, 93.62323522686697, 100.20924777600119, 98.22195908802424, 111.15171931522924, 92.84760793708767, 110.95919054027657, 116.1480484135423, 114.62091415428038, 111.15396894572557, 115.37445134549334, 111.04604662238431, 112.56729706338227, 96.0786151638402, 115.50702033130486, 107.31012345413198, 113.4713994685867, 113.61239986195658, 106.37389898667543, 92.37377277096368, 102.1276326147736, 94.10564120904041, 115.97916386051745, 92.35488564226785, 105.09484184476402, 91.78182426821009, 100.36007584377143, 94.65578598906531, 110.2173949819616, 83.46342383333612, 115.18282006327976, 111.8833220211836, 106.48024956497488, 106.99270911818773, 104.40134972526816]
Elapsed: 0.669377964104346~0.8807811882913268
Time per graph: 0.013721598276060766~0.01796113457427251
Speed: 103.78155066618007~27.093201202972104
Total Time: 0.4621
best val loss: 0.30094605684280396 test_score: 0.8958

Testing...
Test loss: 0.3402 score: 0.9375 time: 0.48s
test Score 0.9375
Epoch Time List: [7.923659118940122, 9.207748327055015, 1.4872148950817063, 10.74356619711034, 7.560045945108868, 1.6494764690287411, 9.429147329065017, 1.7364736699964851, 5.2602265710011125, 8.180418497067876, 1.5457166510168463, 1.531742964987643, 12.797365090926178, 1.3900724129052833, 1.5375783210620284, 8.35525265603792, 18.265497262007557, 1.7849884229945019, 1.5618441288825125, 16.967584008118138, 1.5336890189209953, 1.5118753620190546, 8.725671448977664, 4.700059161172248, 1.5765038869576529, 7.772470562835224, 1.6527807540260255, 1.5821148490067571, 7.683307546074502, 1.4798189599532634, 5.988346641999669, 7.2782104189973325, 1.5038141058757901, 1.4959390969015658, 7.979308106936514, 4.38928855501581, 1.6920231770491228, 8.79599118605256, 4.938298386870883, 1.427105645998381, 6.586969259078614, 1.4352359669283032, 1.5876566120423377, 3.4097868141252548, 8.796874755993485, 1.4163004419533536, 1.5533243250101805, 7.96936046204064, 1.6421714950120077, 4.554225887055509, 6.6834568521007895, 1.520565114915371, 3.2268955130130053, 9.132425990072079, 1.547913048765622, 1.464365872903727, 14.335941906902008, 1.4498268159804866, 1.748888718080707, 8.251580965821631, 5.435493079130538, 1.4061997719109058, 6.845060609979555, 1.434191903914325, 1.5461277860449627, 1.4989638219121844, 8.914823223953135, 4.472729481989518, 1.5184299108805135, 1.446767644956708, 4.7895467159105465, 6.55743250309024, 1.6284682389814407, 1.416277106036432, 12.054401285131462, 1.4132568029453978, 1.7132551869144663, 8.799951805034652, 1.7227576959412545, 6.57991077308543, 1.556645117001608, 2.5463930839905515, 12.44134214008227, 1.413465806050226, 1.5361707038246095, 1.6356829339638352, 1.6406312569743022, 1.4364570169709623, 1.4706430219812319, 1.4420645500067621, 1.488934675930068, 1.3990223510190845, 1.5798063160618767, 1.397037714952603, 1.5636816860642284, 1.4000145850004628, 1.5436726240441203, 1.439770636963658, 1.4836718711303547, 1.3869103530887514, 1.5149906650185585, 1.3734645859804004, 1.506850284175016, 1.4606848231051117, 1.4669037479907274, 1.461744910106063, 1.3832474971422926, 1.3587632070994005, 1.4738155539380386, 1.4994700100505725, 1.3686345630558208, 1.4896116128657013, 1.5074229788733646, 1.407978605129756, 1.5217002930585295, 1.4236640458693728, 1.4445297691272572, 1.46064715704415, 1.6126477929065004, 1.4001271689776331, 1.475591677124612, 1.391963695990853, 1.501733117038384, 1.4917481262236834, 1.6352967689745128, 1.6403585079824552, 1.526567690889351, 1.4608117749448866, 1.5125458549009636, 1.5143629640806466, 1.4158951959107071, 1.4415670790476725, 1.569815056049265, 1.4804265419952571, 1.3979803059482947, 1.4810811439529061, 1.5860001990804449, 1.3818741249851882, 1.4901134959654883, 1.509428198914975, 1.4604357690550387, 1.4192559729563072, 1.4903773190453649, 1.4195914899464697, 1.5000987630337477, 1.3904316000407562, 1.4909243901493028, 1.4231649750145152, 1.4919537469977513, 1.4173253230983391, 1.5303194739390165, 1.4257863389793783, 1.4987874799408019, 1.490487452945672, 1.530893139890395, 1.3954885830171406, 1.467967708944343, 1.496247423114255, 1.4346488610608503, 1.4252954800613225, 1.49925210501533, 1.4962798540946096, 1.463600264978595, 1.4207520670024678, 1.526222402928397, 1.492097630049102, 1.612390099093318, 1.4814493488520384, 1.7476383401080966, 1.435110822902061, 1.461283787037246, 1.403561029001139, 1.5356185629498214, 1.4641577310394496, 1.4814007020322606, 1.411448203958571, 1.4960418069968, 1.4733001559507102, 1.3907451620325446, 1.4043862831313163, 1.4770810491172597, 1.4827914090128615, 1.4038887660717592, 1.4231864380417392, 1.4919741589110345, 1.3991872171172872, 1.5315719748614356, 1.4079329658998176, 1.4949637049576268, 1.4835311949718744, 1.5220025669550523, 1.3678106530569494, 1.5200861990451813, 1.4094363249605522, 1.46480989002157, 1.4900362610351294, 1.3765852599171922, 1.617988054989837, 1.5981203761184588, 1.5102371220709756, 1.497159105958417, 1.5990244508720934, 1.4995463091181591, 1.556354729924351, 1.4102089409716427, 1.4921229179017246, 1.4268903200281784, 1.464572803932242, 1.4276899999240413, 1.510238127084449, 1.4121075508883223, 1.5116823100252077, 1.3946201329817995, 1.5088588240323588, 1.3991601738380268, 1.5089251439785585, 1.4296665219590068, 1.4931287789950147, 1.4867093639913946, 1.4794276000466198, 1.488974750856869, 1.5213272400433198, 1.4627251640195027, 1.4677709989482537, 1.356330121983774, 1.5581379809882492, 1.4273626140784472, 1.480958531028591, 1.397248320048675, 1.5293033319758251, 1.382375956978649, 1.4874161339830607, 1.4100263819564134, 1.4498694849899039, 1.3978371450211853, 1.50505514012184, 1.39094390894752, 1.5003532589180395, 1.4022897359682247, 1.4638474938692525, 1.38333226100076, 1.5098606599494815, 1.4440562358358875, 1.469868474174291, 1.4624945007963106, 1.541005447972566, 1.373933722032234, 1.5024756779894233, 1.4005323860328645, 1.523475673981011, 1.4414736931212246, 1.5058068110374734, 1.4573068448808044, 1.5308506059227511, 1.3740770230069757, 1.4838199000805616, 1.4598190149990842, 1.4315365151269361, 1.3939034888753667, 1.606040446087718, 1.3774034130619839, 1.5036327739944682, 1.4922208180651069, 1.473590026027523, 1.3814077340066433, 1.5441246461123228, 1.3944518038770184, 1.508100142935291, 1.4199963530991226, 1.4972144979983568, 1.4566848508547992, 1.5341896699974313, 1.5009176600724459, 1.5087603400461376, 1.4443660110700876, 1.5809120229678228, 1.4710032609291375, 1.4884525189409032, 1.3933264011284336, 1.579706314834766, 1.4464058759622276, 1.5013397200964391, 1.447500155074522, 1.6224743031198159, 1.4698501790408045]
Total Epoch List: [97, 99, 89]
Total Time List: [0.38325287494808435, 0.44903752696700394, 0.46208947501145303]
T-times Epoch Time: 5.706759246842484 ~ 1.7379560657116013
T-times Total Epoch: 97.33333333333334 ~ 2.573367875415836
T-times Total Time: 1.9968932629252474 ~ 1.4691897312126303
T-times Inference Elapsed: 1.4435117557138244 ~ 0.41368001011591554
T-times Time Per Graph: 0.029626306693510757 ~ 0.008467688927507098
T-times Speed: 82.36233470687239 ~ 9.832027621911298
T-times cross validation test micro f1 score:0.9102943931756539 ~ 0.003350221431648338
T-times cross validation test precision:0.949120082815735 ~ 0.007286707086454866
T-times cross validation test recall:0.8753333333333334 ~ 0.003999999999999992
T-times cross validation test f1_score:0.9102943931756539 ~ 0.0033975641035846995
