Namespace(seed=35, model='SAMamba', dataset='ico_wallets/averVolume', num_heads=8, num_layers=2, dim_hidden=128, dropout=0.6, epochs=1000, lr=0.0005, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed35/khopgnn_gat_1_0.6_0.0005_0.0001_2_8_128_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead
  warnings.warn(out)
Data(edge_index=[2, 309], edge_attr=[309, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d72ec80>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7001;  Loss pred: 0.6876; Loss self: 1.2584; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6934 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6805; Loss self: 1.2496; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6957 score: 0.4898 time: 0.06s
Epoch 3/1000, LR 0.000050
Train loss: 0.6860;  Loss pred: 0.6730; Loss self: 1.2986; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5102 time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6956 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6686;  Loss pred: 0.6566; Loss self: 1.2049; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6956 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6277;  Loss pred: 0.6146; Loss self: 1.3129; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6956 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5798;  Loss pred: 0.5669; Loss self: 1.2966; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.4898 time: 0.06s
Epoch 7/1000, LR 0.000250
Train loss: 0.5443;  Loss pred: 0.5309; Loss self: 1.3369; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6962 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000300
Train loss: 0.4717;  Loss pred: 0.4586; Loss self: 1.3106; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6967 score: 0.4898 time: 0.14s
Epoch 9/1000, LR 0.000350
Train loss: 0.4218;  Loss pred: 0.4075; Loss self: 1.4283; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6975 score: 0.4898 time: 0.06s
Epoch 10/1000, LR 0.000400
Train loss: 0.3691;  Loss pred: 0.3545; Loss self: 1.4680; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6982 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000450
Train loss: 0.3125;  Loss pred: 0.2967; Loss self: 1.5760; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6912 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6987 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2719;  Loss pred: 0.2552; Loss self: 1.6694; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6902 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6993 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2432;  Loss pred: 0.2260; Loss self: 1.7262; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6874 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6988 score: 0.4898 time: 0.06s
Epoch 14/1000, LR 0.000450
Train loss: 0.1974;  Loss pred: 0.1795; Loss self: 1.7908; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6828 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6971 score: 0.4898 time: 0.06s
Epoch 15/1000, LR 0.000450
Train loss: 0.1701;  Loss pred: 0.1514; Loss self: 1.8708; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6762 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.4898 time: 0.06s
Epoch 16/1000, LR 0.000450
Train loss: 0.1486;  Loss pred: 0.1298; Loss self: 1.8801; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6665 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6889 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1300;  Loss pred: 0.1103; Loss self: 1.9641; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6522 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6801 score: 0.4898 time: 0.06s
Epoch 18/1000, LR 0.000450
Train loss: 0.1167;  Loss pred: 0.0963; Loss self: 2.0362; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6331 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6679 score: 0.4898 time: 0.06s
Epoch 19/1000, LR 0.000450
Train loss: 0.1063;  Loss pred: 0.0859; Loss self: 2.0373; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6079 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6509 score: 0.4898 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0964;  Loss pred: 0.0753; Loss self: 2.1020; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.5738 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6275 score: 0.4898 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0867;  Loss pred: 0.0654; Loss self: 2.1228; time: 0.13s
Val loss: 0.5323 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.5999 score: 0.4898 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0792;  Loss pred: 0.0574; Loss self: 2.1718; time: 0.13s
Val loss: 0.4804 score: 0.6122 time: 0.07s
Test loss: 0.5652 score: 0.6122 time: 0.06s
Epoch 23/1000, LR 0.000450
Train loss: 0.0722;  Loss pred: 0.0499; Loss self: 2.2275; time: 0.14s
Val loss: 0.4250 score: 0.7551 time: 0.07s
Test loss: 0.5303 score: 0.7347 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0657;  Loss pred: 0.0431; Loss self: 2.2586; time: 0.15s
Val loss: 0.3694 score: 0.8163 time: 0.07s
Test loss: 0.4984 score: 0.7551 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0590;  Loss pred: 0.0364; Loss self: 2.2589; time: 0.14s
Val loss: 0.3166 score: 0.9184 time: 0.07s
Test loss: 0.4721 score: 0.7551 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0539;  Loss pred: 0.0308; Loss self: 2.3064; time: 0.15s
Val loss: 0.2693 score: 0.9184 time: 0.07s
Test loss: 0.4521 score: 0.7959 time: 0.06s
Epoch 27/1000, LR 0.000450
Train loss: 0.0493;  Loss pred: 0.0264; Loss self: 2.2949; time: 0.13s
Val loss: 0.2296 score: 0.9184 time: 0.07s
Test loss: 0.4410 score: 0.8367 time: 0.06s
Epoch 28/1000, LR 0.000450
Train loss: 0.0458;  Loss pred: 0.0226; Loss self: 2.3206; time: 0.13s
Val loss: 0.1967 score: 0.9184 time: 0.07s
Test loss: 0.4363 score: 0.8367 time: 0.06s
Epoch 29/1000, LR 0.000450
Train loss: 0.0420;  Loss pred: 0.0187; Loss self: 2.3336; time: 0.13s
Val loss: 0.1696 score: 0.9184 time: 0.07s
Test loss: 0.4368 score: 0.8367 time: 0.06s
Epoch 30/1000, LR 0.000450
Train loss: 0.0397;  Loss pred: 0.0160; Loss self: 2.3689; time: 0.15s
Val loss: 0.1494 score: 0.9184 time: 0.07s
Test loss: 0.4445 score: 0.8776 time: 0.07s
Epoch 31/1000, LR 0.000450
Train loss: 0.0371;  Loss pred: 0.0134; Loss self: 2.3660; time: 0.13s
Val loss: 0.1335 score: 0.9388 time: 0.07s
Test loss: 0.4542 score: 0.8776 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0354;  Loss pred: 0.0118; Loss self: 2.3660; time: 0.13s
Val loss: 0.1215 score: 0.9388 time: 0.07s
Test loss: 0.4669 score: 0.8776 time: 0.06s
Epoch 33/1000, LR 0.000449
Train loss: 0.0341;  Loss pred: 0.0100; Loss self: 2.4022; time: 0.13s
Val loss: 0.1131 score: 0.9388 time: 0.07s
Test loss: 0.4807 score: 0.8776 time: 0.06s
Epoch 34/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0085; Loss self: 2.4115; time: 0.13s
Val loss: 0.1065 score: 0.9592 time: 0.07s
Test loss: 0.4947 score: 0.8776 time: 0.07s
Epoch 35/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0075; Loss self: 2.4224; time: 0.13s
Val loss: 0.0994 score: 0.9592 time: 0.07s
Test loss: 0.5006 score: 0.8776 time: 0.06s
Epoch 36/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0067; Loss self: 2.4209; time: 0.13s
Val loss: 0.0945 score: 0.9592 time: 0.07s
Test loss: 0.5077 score: 0.8776 time: 0.06s
Epoch 37/1000, LR 0.000449
Train loss: 0.0310;  Loss pred: 0.0066; Loss self: 2.4459; time: 0.13s
Val loss: 0.0907 score: 0.9592 time: 0.07s
Test loss: 0.5088 score: 0.8776 time: 0.06s
Epoch 38/1000, LR 0.000449
Train loss: 0.0295;  Loss pred: 0.0051; Loss self: 2.4437; time: 0.13s
Val loss: 0.0871 score: 0.9592 time: 0.07s
Test loss: 0.5062 score: 0.8776 time: 0.07s
Epoch 39/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0043; Loss self: 2.4574; time: 0.13s
Val loss: 0.0841 score: 0.9592 time: 0.07s
Test loss: 0.5033 score: 0.8776 time: 0.06s
Epoch 40/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0040; Loss self: 2.4314; time: 0.13s
Val loss: 0.0822 score: 0.9592 time: 0.07s
Test loss: 0.5016 score: 0.8776 time: 0.06s
Epoch 41/1000, LR 0.000449
Train loss: 0.0279;  Loss pred: 0.0035; Loss self: 2.4415; time: 0.13s
Val loss: 0.0807 score: 0.9592 time: 0.07s
Test loss: 0.5005 score: 0.8776 time: 0.06s
Epoch 42/1000, LR 0.000449
Train loss: 0.0274;  Loss pred: 0.0033; Loss self: 2.4106; time: 0.13s
Val loss: 0.0805 score: 0.9592 time: 0.07s
Test loss: 0.5016 score: 0.8776 time: 0.06s
Epoch 43/1000, LR 0.000449
Train loss: 0.0272;  Loss pred: 0.0031; Loss self: 2.4077; time: 0.13s
Val loss: 0.0807 score: 0.9592 time: 0.07s
Test loss: 0.5035 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0267;  Loss pred: 0.0027; Loss self: 2.4075; time: 0.14s
Val loss: 0.0810 score: 0.9592 time: 0.07s
Test loss: 0.5046 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0267;  Loss pred: 0.0027; Loss self: 2.4053; time: 0.13s
Val loss: 0.0821 score: 0.9592 time: 0.07s
Test loss: 0.5071 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0262;  Loss pred: 0.0023; Loss self: 2.3883; time: 0.13s
Val loss: 0.0832 score: 0.9592 time: 0.07s
Test loss: 0.5092 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0262;  Loss pred: 0.0023; Loss self: 2.3911; time: 0.13s
Val loss: 0.0851 score: 0.9592 time: 0.07s
Test loss: 0.5133 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0020; Loss self: 2.3653; time: 0.13s
Val loss: 0.0876 score: 0.9592 time: 0.07s
Test loss: 0.5192 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0027; Loss self: 2.3749; time: 0.13s
Val loss: 0.0900 score: 0.9592 time: 0.07s
Test loss: 0.5235 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0255;  Loss pred: 0.0019; Loss self: 2.3550; time: 0.13s
Val loss: 0.0920 score: 0.9592 time: 0.07s
Test loss: 0.5264 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0252;  Loss pred: 0.0018; Loss self: 2.3383; time: 0.13s
Val loss: 0.0953 score: 0.9388 time: 0.07s
Test loss: 0.5324 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0252;  Loss pred: 0.0018; Loss self: 2.3431; time: 0.15s
Val loss: 0.0977 score: 0.9388 time: 0.07s
Test loss: 0.5358 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0250;  Loss pred: 0.0017; Loss self: 2.3375; time: 0.13s
Val loss: 0.0997 score: 0.9388 time: 0.07s
Test loss: 0.5379 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0248;  Loss pred: 0.0016; Loss self: 2.3186; time: 0.13s
Val loss: 0.1017 score: 0.9388 time: 0.07s
Test loss: 0.5408 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0248;  Loss pred: 0.0016; Loss self: 2.3157; time: 0.13s
Val loss: 0.1034 score: 0.9388 time: 0.07s
Test loss: 0.5433 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0245;  Loss pred: 0.0016; Loss self: 2.2896; time: 0.13s
Val loss: 0.1057 score: 0.9388 time: 0.07s
Test loss: 0.5474 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0241;  Loss pred: 0.0015; Loss self: 2.2611; time: 0.14s
Val loss: 0.1074 score: 0.9388 time: 0.07s
Test loss: 0.5500 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000448
Train loss: 0.0242;  Loss pred: 0.0015; Loss self: 2.2720; time: 0.14s
Val loss: 0.1091 score: 0.9388 time: 0.07s
Test loss: 0.5526 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000447
Train loss: 0.0238;  Loss pred: 0.0015; Loss self: 2.2388; time: 0.15s
Val loss: 0.1107 score: 0.9388 time: 0.07s
Test loss: 0.5544 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000447
Train loss: 0.0239;  Loss pred: 0.0014; Loss self: 2.2454; time: 0.15s
Val loss: 0.1124 score: 0.9388 time: 0.07s
Test loss: 0.5578 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000447
Train loss: 0.0237;  Loss pred: 0.0015; Loss self: 2.2176; time: 0.15s
Val loss: 0.1147 score: 0.9388 time: 0.07s
Test loss: 0.5620 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000447
Train loss: 0.0237;  Loss pred: 0.0014; Loss self: 2.2274; time: 0.15s
Val loss: 0.1162 score: 0.9388 time: 0.07s
Test loss: 0.5641 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0274,   Val_Loss: 0.0805,   Val_Precision: 1.0000,   Val_Recall: 0.9167,   Val_accuracy: 0.9565,   Val_Score: 0.9592,   Val_Loss: 0.0805,   Test_Precision: 1.0000,   Test_Recall: 0.7600,   Test_accuracy: 0.8636,   Test_Score: 0.8776,   Test_loss: 0.5016


[0.06828890298493207, 0.06659876205958426, 0.07127433503046632, 0.07317946292459965, 0.07194466097280383, 0.06711245304904878, 0.06573899788782, 0.14557718788273633, 0.06385656516067684, 0.06851792009547353, 0.07136060111224651, 0.07143767992965877, 0.06713745603337884, 0.06727784499526024, 0.06817495613358915, 0.07189378491602838, 0.06828459701500833, 0.06780188088305295, 0.07925928197801113, 0.07272779010236263, 0.07389485696330667, 0.06784835690632463, 0.07278219889849424, 0.07414324907585979, 0.0747198830358684, 0.06786377006210387, 0.06775543512776494, 0.06768793310038745, 0.07094177091494203, 0.07478091004304588, 0.0746957459487021, 0.06782425101846457, 0.06745740794576705, 0.07399326888844371, 0.06755911512300372, 0.068730239989236, 0.06842740206047893, 0.07146139512769878, 0.07135666091926396, 0.06882636994123459, 0.06806009518913925, 0.06850203708745539, 0.07486293790861964, 0.06870868289843202, 0.06871188106015325, 0.07626523193903267, 0.07451410102657974, 0.06826157006435096, 0.0749645410105586, 0.07491347193717957, 0.07481265184469521, 0.0756433520000428, 0.07576676015742123, 0.07579500903375447, 0.06904984195716679, 0.06804451299831271, 0.06924062315374613, 0.07683976204134524, 0.0762723870575428, 0.07571453391574323, 0.06899794703349471, 0.06925304606556892]
[0.0013936510813251442, 0.0013591584093792706, 0.001454578265927884, 0.0014934584270326458, 0.0014682583872000783, 0.0013696418989601793, 0.001341612201792245, 0.002970963018015027, 0.0013031952073607519, 0.001398324899907623, 0.0014563387982091125, 0.0014579118352991585, 0.001370152163946507, 0.0013730172448012294, 0.0013913256353793703, 0.0014672201003271099, 0.0013935632043879252, 0.0013837118547561826, 0.0016175363668981865, 0.0014842406143339313, 0.0015080583053736054, 0.0013846603450270332, 0.001485350997928454, 0.0015131275321604038, 0.0015248955721605796, 0.0013849748992266096, 0.0013827639821992845, 0.0013813863898038256, 0.0014477912431620822, 0.0015261410212866506, 0.0015244029785449408, 0.0013841683881319299, 0.0013766817948115723, 0.0015100667120090552, 0.001378757451489872, 0.0014026579589639999, 0.0013964775930709985, 0.0014583958189326282, 0.0014562583861074277, 0.0014046197947190733, 0.0013889815344722296, 0.0013980007568868448, 0.0015278150593595846, 0.0014022180183353473, 0.001402283286941903, 0.0015564333048782178, 0.0015206959393179538, 0.0013930932666194073, 0.0015298885920522164, 0.001528846366064889, 0.001526788813157045, 0.0015437418775518937, 0.0015462604113759435, 0.0015468369190562137, 0.0014091804481054448, 0.0013886635305778105, 0.0014130739419131863, 0.0015681584090070457, 0.0015565793277049552, 0.0015451945697090455, 0.0014081213680305043, 0.0014133274707258964]
[717.5397152127607, 735.7494116206082, 687.4844918448538, 669.5867671301042, 681.0790312643594, 730.1178510669042, 745.3718732314085, 336.5911975128268, 767.3447495446313, 715.1413809952627, 686.6534086915208, 685.9125331092489, 729.8459443509237, 728.3229717517271, 718.7390029849711, 681.5610008185239, 717.5849626707213, 722.6938155966044, 618.2241218586114, 673.7452070389279, 663.1043351816962, 722.1987714109605, 673.2415445202183, 660.8828262956964, 655.7826111221042, 722.034746303644, 723.1892158555513, 723.9104188235217, 690.7073134493663, 655.2474417841973, 655.9945198706649, 722.4554530894882, 726.3842696030355, 662.2223985518882, 725.2907310995199, 712.9321825105515, 716.0873937124167, 685.684905989295, 686.6913245203659, 711.9364284624809, 719.9519757330463, 715.3071949881218, 654.5294823963646, 713.15586230104, 713.1226688016786, 642.4946040834332, 657.593654421481, 717.827028499449, 653.6423666370271, 654.0879595206866, 654.9694308620404, 647.7766876324079, 646.7215953036964, 646.4805615126768, 709.6323266083047, 720.1168447074498, 707.677050958906, 637.690678605102, 642.4343316150906, 647.1676898193451, 710.1660572047629, 707.5501047796035]
Elapsed: 0.07215145676802363~0.010008821661497656
Time per graph: 0.0014724787095515026~0.0002042616665611767
Speed: 686.5381359265142~55.69623982398512
Total Time: 0.0697
best val loss: 0.08051453530788422 test_score: 0.8776

Testing...
Test loss: 0.4947 score: 0.8776 time: 0.06s
test Score 0.8776
Epoch Time List: [0.5523008189629763, 0.28034329088404775, 0.38013797998428345, 0.28366734995506704, 0.2807061441708356, 0.2753755869343877, 0.27178946626372635, 0.3481444981880486, 0.2789499789942056, 0.2804980722721666, 0.28219108399935067, 0.27482295385561883, 0.28811906394548714, 0.2694305127952248, 0.265966763952747, 0.2716147140599787, 0.2747285128571093, 0.2645625078584999, 0.2760090581141412, 0.2855536649003625, 0.2667768942192197, 0.26558957411907613, 0.27437451272271574, 0.2850755378603935, 0.2821221458725631, 0.28135149483568966, 0.26460521202534437, 0.2681340689305216, 0.2612410911824554, 0.285267619183287, 0.26780739915557206, 0.26497378409840167, 0.26446221210062504, 0.26843456691130996, 0.26427810406312346, 0.266078864922747, 0.2680864797439426, 0.2656542288605124, 0.2621540119871497, 0.2661384646780789, 0.26739229797385633, 0.26758886384777725, 0.272417546948418, 0.2760032380465418, 0.2641326910816133, 0.2726678899489343, 0.2693086629733443, 0.2668420821428299, 0.2698659331072122, 0.26925457804463804, 0.2684701180551201, 0.28511630883440375, 0.2709937528707087, 0.2714713551104069, 0.2691407350357622, 0.2646327700931579, 0.2752387400250882, 0.2867561839520931, 0.289987237425521, 0.28934478387236595, 0.28692773915827274, 0.2881008400581777]
Total Epoch List: [62]
Total Time List: [0.06973856897093356]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d72df90>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.6969;  Loss pred: 0.6835; Loss self: 1.3378; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6959 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7107;  Loss pred: 0.6992; Loss self: 1.1445; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.6839;  Loss pred: 0.6711; Loss self: 1.2753; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6652;  Loss pred: 0.6531; Loss self: 1.2098; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6165;  Loss pred: 0.6048; Loss self: 1.1708; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.5683;  Loss pred: 0.5562; Loss self: 1.2118; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5165;  Loss pred: 0.5043; Loss self: 1.2198; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6916 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4547;  Loss pred: 0.4417; Loss self: 1.2996; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6909 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6913 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4061;  Loss pred: 0.3918; Loss self: 1.4344; time: 0.16s
Val loss: 0.6899 score: 0.5306 time: 0.07s
Test loss: 0.6896 score: 0.5306 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3490;  Loss pred: 0.3334; Loss self: 1.5590; time: 0.16s
Val loss: 0.6883 score: 0.8776 time: 0.07s
Test loss: 0.6872 score: 0.9592 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.2985;  Loss pred: 0.2811; Loss self: 1.7320; time: 0.16s
Val loss: 0.6858 score: 0.7551 time: 0.07s
Test loss: 0.6836 score: 1.0000 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2634;  Loss pred: 0.2448; Loss self: 1.8604; time: 0.16s
Val loss: 0.6822 score: 0.7143 time: 0.07s
Test loss: 0.6785 score: 0.9184 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2313;  Loss pred: 0.2117; Loss self: 1.9620; time: 0.16s
Val loss: 0.6772 score: 0.7143 time: 0.07s
Test loss: 0.6716 score: 0.9184 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2068;  Loss pred: 0.1860; Loss self: 2.0825; time: 0.16s
Val loss: 0.6698 score: 0.7755 time: 0.07s
Test loss: 0.6618 score: 0.9388 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1871;  Loss pred: 0.1650; Loss self: 2.2118; time: 0.16s
Val loss: 0.6588 score: 0.7959 time: 0.07s
Test loss: 0.6471 score: 0.9796 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1649;  Loss pred: 0.1420; Loss self: 2.2839; time: 0.16s
Val loss: 0.6430 score: 0.8163 time: 0.07s
Test loss: 0.6263 score: 1.0000 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1500;  Loss pred: 0.1266; Loss self: 2.3468; time: 0.16s
Val loss: 0.6214 score: 0.8163 time: 0.07s
Test loss: 0.5979 score: 1.0000 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1340;  Loss pred: 0.1096; Loss self: 2.4371; time: 0.16s
Val loss: 0.5934 score: 0.8367 time: 0.07s
Test loss: 0.5604 score: 1.0000 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1205;  Loss pred: 0.0958; Loss self: 2.4694; time: 0.16s
Val loss: 0.5586 score: 0.8367 time: 0.07s
Test loss: 0.5137 score: 1.0000 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1079;  Loss pred: 0.0826; Loss self: 2.5275; time: 0.16s
Val loss: 0.5174 score: 0.8367 time: 0.07s
Test loss: 0.4559 score: 1.0000 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0967;  Loss pred: 0.0709; Loss self: 2.5787; time: 0.16s
Val loss: 0.4733 score: 0.8367 time: 0.07s
Test loss: 0.3897 score: 1.0000 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0853;  Loss pred: 0.0593; Loss self: 2.5992; time: 0.16s
Val loss: 0.4307 score: 0.8367 time: 0.07s
Test loss: 0.3204 score: 1.0000 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0770;  Loss pred: 0.0504; Loss self: 2.6616; time: 0.16s
Val loss: 0.3975 score: 0.8367 time: 0.07s
Test loss: 0.2553 score: 1.0000 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0695;  Loss pred: 0.0424; Loss self: 2.7103; time: 0.16s
Val loss: 0.3770 score: 0.8367 time: 0.07s
Test loss: 0.1987 score: 1.0000 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0632;  Loss pred: 0.0359; Loss self: 2.7298; time: 0.16s
Val loss: 0.3733 score: 0.8367 time: 0.07s
Test loss: 0.1532 score: 1.0000 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0576;  Loss pred: 0.0301; Loss self: 2.7494; time: 0.16s
Val loss: 0.3831 score: 0.8367 time: 0.07s
Test loss: 0.1190 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000450
Train loss: 0.0534;  Loss pred: 0.0257; Loss self: 2.7702; time: 0.16s
Val loss: 0.4071 score: 0.8163 time: 0.07s
Test loss: 0.0943 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0490;  Loss pred: 0.0211; Loss self: 2.7899; time: 0.16s
Val loss: 0.4409 score: 0.8163 time: 0.07s
Test loss: 0.0765 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0456;  Loss pred: 0.0175; Loss self: 2.8101; time: 0.16s
Val loss: 0.4828 score: 0.8163 time: 0.07s
Test loss: 0.0640 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0432;  Loss pred: 0.0149; Loss self: 2.8344; time: 0.16s
Val loss: 0.5292 score: 0.8163 time: 0.07s
Test loss: 0.0555 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0414;  Loss pred: 0.0127; Loss self: 2.8721; time: 0.17s
Val loss: 0.5785 score: 0.8163 time: 0.07s
Test loss: 0.0493 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0391;  Loss pred: 0.0105; Loss self: 2.8559; time: 0.16s
Val loss: 0.6281 score: 0.8163 time: 0.07s
Test loss: 0.0448 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0381;  Loss pred: 0.0093; Loss self: 2.8814; time: 0.16s
Val loss: 0.6745 score: 0.8163 time: 0.07s
Test loss: 0.0417 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0365;  Loss pred: 0.0078; Loss self: 2.8723; time: 0.16s
Val loss: 0.7200 score: 0.8163 time: 0.07s
Test loss: 0.0394 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0354;  Loss pred: 0.0067; Loss self: 2.8691; time: 0.16s
Val loss: 0.7626 score: 0.8163 time: 0.07s
Test loss: 0.0379 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0345;  Loss pred: 0.0057; Loss self: 2.8745; time: 0.16s
Val loss: 0.8008 score: 0.8163 time: 0.07s
Test loss: 0.0368 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0339;  Loss pred: 0.0052; Loss self: 2.8711; time: 0.16s
Val loss: 0.8341 score: 0.8163 time: 0.07s
Test loss: 0.0360 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0334;  Loss pred: 0.0045; Loss self: 2.8852; time: 0.16s
Val loss: 0.8626 score: 0.7959 time: 0.07s
Test loss: 0.0356 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0328;  Loss pred: 0.0039; Loss self: 2.8849; time: 0.16s
Val loss: 0.8895 score: 0.7959 time: 0.07s
Test loss: 0.0354 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0324;  Loss pred: 0.0036; Loss self: 2.8805; time: 0.16s
Val loss: 0.9115 score: 0.7959 time: 0.06s
Test loss: 0.0354 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0323;  Loss pred: 0.0033; Loss self: 2.8919; time: 0.16s
Val loss: 0.9374 score: 0.7959 time: 0.07s
Test loss: 0.0362 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0031; Loss self: 2.8669; time: 0.16s
Val loss: 0.9570 score: 0.7959 time: 0.06s
Test loss: 0.0367 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0314;  Loss pred: 0.0026; Loss self: 2.8800; time: 0.16s
Val loss: 0.9743 score: 0.7959 time: 0.06s
Test loss: 0.0373 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0311;  Loss pred: 0.0024; Loss self: 2.8701; time: 0.16s
Val loss: 0.9893 score: 0.7959 time: 0.07s
Test loss: 0.0378 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0023; Loss self: 2.8531; time: 0.14s
Val loss: 1.0032 score: 0.7959 time: 0.07s
Test loss: 0.0385 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.0632,   Val_Loss: 0.3733,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.3733,   Test_Precision: 1.0000,   Test_Recall: 1.0000,   Test_accuracy: 1.0000,   Test_Score: 1.0000,   Test_loss: 0.1532


[0.06828890298493207, 0.06659876205958426, 0.07127433503046632, 0.07317946292459965, 0.07194466097280383, 0.06711245304904878, 0.06573899788782, 0.14557718788273633, 0.06385656516067684, 0.06851792009547353, 0.07136060111224651, 0.07143767992965877, 0.06713745603337884, 0.06727784499526024, 0.06817495613358915, 0.07189378491602838, 0.06828459701500833, 0.06780188088305295, 0.07925928197801113, 0.07272779010236263, 0.07389485696330667, 0.06784835690632463, 0.07278219889849424, 0.07414324907585979, 0.0747198830358684, 0.06786377006210387, 0.06775543512776494, 0.06768793310038745, 0.07094177091494203, 0.07478091004304588, 0.0746957459487021, 0.06782425101846457, 0.06745740794576705, 0.07399326888844371, 0.06755911512300372, 0.068730239989236, 0.06842740206047893, 0.07146139512769878, 0.07135666091926396, 0.06882636994123459, 0.06806009518913925, 0.06850203708745539, 0.07486293790861964, 0.06870868289843202, 0.06871188106015325, 0.07626523193903267, 0.07451410102657974, 0.06826157006435096, 0.0749645410105586, 0.07491347193717957, 0.07481265184469521, 0.0756433520000428, 0.07576676015742123, 0.07579500903375447, 0.06904984195716679, 0.06804451299831271, 0.06924062315374613, 0.07683976204134524, 0.0762723870575428, 0.07571453391574323, 0.06899794703349471, 0.06925304606556892, 0.08322629588656127, 0.08213873789645731, 0.08230934594757855, 0.08265574905090034, 0.08289043209515512, 0.08243244607001543, 0.08227706584148109, 0.08249178691767156, 0.08172065112739801, 0.08162517985329032, 0.08600170211866498, 0.08233708702027798, 0.08627743204124272, 0.08221267093904316, 0.08314996701665223, 0.08238691790029407, 0.08223645109683275, 0.08573622815310955, 0.0859384520445019, 0.08224346092902124, 0.08661332004703581, 0.08268341491930187, 0.08236654591746628, 0.08655086578801274, 0.08242237195372581, 0.08234424679540098, 0.08631139900535345, 0.08209287002682686, 0.08258347189985216, 0.08736896584741771, 0.08500287705101073, 0.08191154897212982, 0.08508770982734859, 0.0861004360485822, 0.08583309105597436, 0.08560976712033153, 0.08244025893509388, 0.08197735506109893, 0.082269525853917, 0.0813488969579339, 0.07837304309941828, 0.07829603110440075, 0.08189262403175235, 0.0785412669647485, 0.07777439686469734]
[0.0013936510813251442, 0.0013591584093792706, 0.001454578265927884, 0.0014934584270326458, 0.0014682583872000783, 0.0013696418989601793, 0.001341612201792245, 0.002970963018015027, 0.0013031952073607519, 0.001398324899907623, 0.0014563387982091125, 0.0014579118352991585, 0.001370152163946507, 0.0013730172448012294, 0.0013913256353793703, 0.0014672201003271099, 0.0013935632043879252, 0.0013837118547561826, 0.0016175363668981865, 0.0014842406143339313, 0.0015080583053736054, 0.0013846603450270332, 0.001485350997928454, 0.0015131275321604038, 0.0015248955721605796, 0.0013849748992266096, 0.0013827639821992845, 0.0013813863898038256, 0.0014477912431620822, 0.0015261410212866506, 0.0015244029785449408, 0.0013841683881319299, 0.0013766817948115723, 0.0015100667120090552, 0.001378757451489872, 0.0014026579589639999, 0.0013964775930709985, 0.0014583958189326282, 0.0014562583861074277, 0.0014046197947190733, 0.0013889815344722296, 0.0013980007568868448, 0.0015278150593595846, 0.0014022180183353473, 0.001402283286941903, 0.0015564333048782178, 0.0015206959393179538, 0.0013930932666194073, 0.0015298885920522164, 0.001528846366064889, 0.001526788813157045, 0.0015437418775518937, 0.0015462604113759435, 0.0015468369190562137, 0.0014091804481054448, 0.0013886635305778105, 0.0014130739419131863, 0.0015681584090070457, 0.0015565793277049552, 0.0015451945697090455, 0.0014081213680305043, 0.0014133274707258964, 0.0016984958344196178, 0.001676300773397088, 0.0016797825703587458, 0.0016868520214469458, 0.0016916414713296964, 0.001682294817755417, 0.0016791237926832875, 0.001683505855462685, 0.0016677683903550615, 0.0016658199970059249, 0.0017551367779319383, 0.0016803487146995505, 0.001760763919209035, 0.001677809611000881, 0.0016969381023806576, 0.0016813656714345729, 0.0016782949203435257, 0.001749718941900195, 0.0017538459600918755, 0.0016784379781432906, 0.0017676187764701186, 0.0016874166310061605, 0.0016809499166829853, 0.0017663441997553622, 0.0016820892235454247, 0.0016804948325592037, 0.0017614571225582337, 0.001675364694425038, 0.0016853769775480032, 0.0017830401193350554, 0.0017347525928777699, 0.0016716642647373434, 0.0017364838740275223, 0.0017571517560935142, 0.0017516957358362116, 0.0017471381044965617, 0.0016824542639815078, 0.0016730072461448762, 0.0016789699153860612, 0.0016601815705700796, 0.0015994498591718016, 0.001597878185804097, 0.0016712780414643337, 0.0016028829992805816, 0.0015872325890754558]
[717.5397152127607, 735.7494116206082, 687.4844918448538, 669.5867671301042, 681.0790312643594, 730.1178510669042, 745.3718732314085, 336.5911975128268, 767.3447495446313, 715.1413809952627, 686.6534086915208, 685.9125331092489, 729.8459443509237, 728.3229717517271, 718.7390029849711, 681.5610008185239, 717.5849626707213, 722.6938155966044, 618.2241218586114, 673.7452070389279, 663.1043351816962, 722.1987714109605, 673.2415445202183, 660.8828262956964, 655.7826111221042, 722.034746303644, 723.1892158555513, 723.9104188235217, 690.7073134493663, 655.2474417841973, 655.9945198706649, 722.4554530894882, 726.3842696030355, 662.2223985518882, 725.2907310995199, 712.9321825105515, 716.0873937124167, 685.684905989295, 686.6913245203659, 711.9364284624809, 719.9519757330463, 715.3071949881218, 654.5294823963646, 713.15586230104, 713.1226688016786, 642.4946040834332, 657.593654421481, 717.827028499449, 653.6423666370271, 654.0879595206866, 654.9694308620404, 647.7766876324079, 646.7215953036964, 646.4805615126768, 709.6323266083047, 720.1168447074498, 707.677050958906, 637.690678605102, 642.4343316150906, 647.1676898193451, 710.1660572047629, 707.5501047796035, 588.7562275604306, 596.5516546135461, 595.3151423558545, 592.8202280258237, 591.1418092711814, 594.4261311666161, 595.5487048408572, 593.998527985616, 599.6036414787211, 600.3049559960609, 569.7561652022874, 595.1145683345864, 567.9353086978389, 596.0151816054146, 589.2966859528266, 594.7546194081513, 595.8428330315822, 571.5203602436857, 570.17550158602, 595.7920477384649, 565.7328454028814, 592.6218703935182, 594.9017219818766, 566.1410726960802, 594.498785202517, 595.0628235357993, 567.711803593414, 596.8849667941619, 593.3390649816906, 560.8398763191753, 576.4510767158482, 598.2062433793325, 575.8763527591219, 569.1028088679102, 570.8753977885477, 572.3645986692908, 594.3697973896253, 597.7260423135093, 595.6032867748322, 602.3437542778023, 625.214972676794, 625.8299342742275, 598.3444855912928, 623.8758539761341, 630.0273865864159]
Elapsed: 0.07672406243656492~0.009435663728605215
Time per graph: 0.0015657971925829575~0.00019256456588990237
Speed: 646.2054350044976~64.42173894793214
Total Time: 0.0784
best val loss: 0.37334176898002625 test_score: 1.0000

Testing...
Test loss: 0.6872 score: 0.9592 time: 0.07s
test Score 0.9592
Epoch Time List: [0.5523008189629763, 0.28034329088404775, 0.38013797998428345, 0.28366734995506704, 0.2807061441708356, 0.2753755869343877, 0.27178946626372635, 0.3481444981880486, 0.2789499789942056, 0.2804980722721666, 0.28219108399935067, 0.27482295385561883, 0.28811906394548714, 0.2694305127952248, 0.265966763952747, 0.2716147140599787, 0.2747285128571093, 0.2645625078584999, 0.2760090581141412, 0.2855536649003625, 0.2667768942192197, 0.26558957411907613, 0.27437451272271574, 0.2850755378603935, 0.2821221458725631, 0.28135149483568966, 0.26460521202534437, 0.2681340689305216, 0.2612410911824554, 0.285267619183287, 0.26780739915557206, 0.26497378409840167, 0.26446221210062504, 0.26843456691130996, 0.26427810406312346, 0.266078864922747, 0.2680864797439426, 0.2656542288605124, 0.2621540119871497, 0.2661384646780789, 0.26739229797385633, 0.26758886384777725, 0.272417546948418, 0.2760032380465418, 0.2641326910816133, 0.2726678899489343, 0.2693086629733443, 0.2668420821428299, 0.2698659331072122, 0.26925457804463804, 0.2684701180551201, 0.28511630883440375, 0.2709937528707087, 0.2714713551104069, 0.2691407350357622, 0.2646327700931579, 0.2752387400250882, 0.2867561839520931, 0.289987237425521, 0.28934478387236595, 0.28692773915827274, 0.2881008400581777, 0.31274113710969687, 0.3064010478556156, 0.3065177630633116, 0.3079992651473731, 0.3078282210044563, 0.3075412637554109, 0.31025993707589805, 0.3062668649945408, 0.3045671519357711, 0.3059573301579803, 0.3110867962241173, 0.3045087961945683, 0.3088553079869598, 0.3051741309463978, 0.3082023069728166, 0.3051136319991201, 0.30577073385939, 0.30883357813581824, 0.31001919601112604, 0.3049350897781551, 0.3111337381415069, 0.30647216783836484, 0.30722707509994507, 0.3115105021279305, 0.3055394981056452, 0.3077218569815159, 0.31105027976445854, 0.30510427989065647, 0.3066576460842043, 0.31150217703543603, 0.3167955121025443, 0.302530765067786, 0.3072625850327313, 0.30808631074614823, 0.3062274360563606, 0.30830011586658657, 0.3031201930716634, 0.3069489279296249, 0.31116104195825756, 0.2985889178235084, 0.29456420917995274, 0.29739511106163263, 0.2999728829599917, 0.2963559930212796, 0.2761164119001478]
Total Epoch List: [62, 45]
Total Time List: [0.06973856897093356, 0.07841397589072585]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d72cd00>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7120;  Loss pred: 0.6988; Loss self: 1.3232; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7118;  Loss pred: 0.6991; Loss self: 1.2644; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000050
Train loss: 0.6996;  Loss pred: 0.6858; Loss self: 1.3730; time: 0.14s
Val loss: 0.6929 score: 0.5102 time: 0.07s
Test loss: 0.6928 score: 0.5208 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6522;  Loss pred: 0.6394; Loss self: 1.2845; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
Test loss: 0.6927 score: 0.5208 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6199;  Loss pred: 0.6069; Loss self: 1.2961; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5669;  Loss pred: 0.5531; Loss self: 1.3822; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5070;  Loss pred: 0.4931; Loss self: 1.3922; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6920 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.4468;  Loss pred: 0.4319; Loss self: 1.4969; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6915 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.3896;  Loss pred: 0.3745; Loss self: 1.5056; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6907 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3452;  Loss pred: 0.3283; Loss self: 1.6869; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6892 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.2864;  Loss pred: 0.2682; Loss self: 1.8202; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6892 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6867 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2406;  Loss pred: 0.2211; Loss self: 1.9523; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6865 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6831 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2106;  Loss pred: 0.1903; Loss self: 2.0219; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6821 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6775 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.1865;  Loss pred: 0.1646; Loss self: 2.1818; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6753 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6691 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1623;  Loss pred: 0.1397; Loss self: 2.2612; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6653 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6568 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1461;  Loss pred: 0.1224; Loss self: 2.3653; time: 0.14s
Val loss: 0.6510 score: 0.5714 time: 0.07s
Test loss: 0.6391 score: 0.6250 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1305;  Loss pred: 0.1065; Loss self: 2.4059; time: 0.14s
Val loss: 0.6306 score: 0.7143 time: 0.07s
Test loss: 0.6140 score: 0.8333 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1161;  Loss pred: 0.0915; Loss self: 2.4658; time: 0.14s
Val loss: 0.6028 score: 0.8163 time: 0.07s
Test loss: 0.5798 score: 0.8750 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1046;  Loss pred: 0.0796; Loss self: 2.5035; time: 0.16s
Val loss: 0.5679 score: 0.8571 time: 0.07s
Test loss: 0.5362 score: 0.8958 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0955;  Loss pred: 0.0698; Loss self: 2.5606; time: 0.14s
Val loss: 0.5271 score: 0.8571 time: 0.07s
Test loss: 0.4838 score: 0.8958 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0852;  Loss pred: 0.0592; Loss self: 2.6003; time: 0.14s
Val loss: 0.4828 score: 0.8367 time: 0.07s
Test loss: 0.4249 score: 0.9167 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0752;  Loss pred: 0.0489; Loss self: 2.6325; time: 0.14s
Val loss: 0.4406 score: 0.8367 time: 0.07s
Test loss: 0.3647 score: 0.9375 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0677;  Loss pred: 0.0410; Loss self: 2.6708; time: 0.13s
Val loss: 0.4074 score: 0.8367 time: 0.07s
Test loss: 0.3112 score: 0.9375 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0616;  Loss pred: 0.0344; Loss self: 2.7185; time: 0.13s
Val loss: 0.3870 score: 0.8367 time: 0.07s
Test loss: 0.2676 score: 0.9375 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0551;  Loss pred: 0.0280; Loss self: 2.7079; time: 0.13s
Val loss: 0.3812 score: 0.8367 time: 0.07s
Test loss: 0.2359 score: 0.9375 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0514;  Loss pred: 0.0238; Loss self: 2.7580; time: 0.14s
Val loss: 0.3890 score: 0.8367 time: 0.07s
Test loss: 0.2153 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000450
Train loss: 0.0471;  Loss pred: 0.0192; Loss self: 2.7887; time: 0.14s
Val loss: 0.4081 score: 0.8571 time: 0.07s
Test loss: 0.2041 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0442;  Loss pred: 0.0161; Loss self: 2.8051; time: 0.13s
Val loss: 0.4361 score: 0.8571 time: 0.08s
Test loss: 0.2006 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0412;  Loss pred: 0.0132; Loss self: 2.8035; time: 0.14s
Val loss: 0.4681 score: 0.8571 time: 0.07s
Test loss: 0.2017 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0398;  Loss pred: 0.0111; Loss self: 2.8611; time: 0.13s
Val loss: 0.5051 score: 0.8571 time: 0.07s
Test loss: 0.2075 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0376;  Loss pred: 0.0091; Loss self: 2.8544; time: 0.14s
Val loss: 0.5408 score: 0.8571 time: 0.07s
Test loss: 0.2146 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0365;  Loss pred: 0.0078; Loss self: 2.8675; time: 0.14s
Val loss: 0.5786 score: 0.8571 time: 0.07s
Test loss: 0.2245 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0351;  Loss pred: 0.0064; Loss self: 2.8687; time: 0.13s
Val loss: 0.6189 score: 0.8571 time: 0.07s
Test loss: 0.2372 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0345;  Loss pred: 0.0056; Loss self: 2.8922; time: 0.13s
Val loss: 0.6557 score: 0.8571 time: 0.07s
Test loss: 0.2501 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0049; Loss self: 2.8773; time: 0.13s
Val loss: 0.6882 score: 0.8571 time: 0.07s
Test loss: 0.2626 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0331;  Loss pred: 0.0043; Loss self: 2.8815; time: 0.13s
Val loss: 0.7176 score: 0.8571 time: 0.07s
Test loss: 0.2746 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0037; Loss self: 2.8758; time: 0.13s
Val loss: 0.7415 score: 0.8571 time: 0.07s
Test loss: 0.2848 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0319;  Loss pred: 0.0032; Loss self: 2.8651; time: 0.13s
Val loss: 0.7628 score: 0.8367 time: 0.07s
Test loss: 0.2942 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0029; Loss self: 2.8752; time: 0.14s
Val loss: 0.7774 score: 0.8367 time: 0.07s
Test loss: 0.3011 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0311;  Loss pred: 0.0026; Loss self: 2.8568; time: 0.14s
Val loss: 0.7930 score: 0.8367 time: 0.13s
Test loss: 0.3086 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0023; Loss self: 2.8582; time: 0.14s
Val loss: 0.8045 score: 0.8367 time: 0.07s
Test loss: 0.3147 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0021; Loss self: 2.8600; time: 0.14s
Val loss: 0.8129 score: 0.8367 time: 0.07s
Test loss: 0.3192 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0019; Loss self: 2.8705; time: 0.14s
Val loss: 0.8234 score: 0.8367 time: 0.07s
Test loss: 0.3254 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0018; Loss self: 2.8580; time: 0.14s
Val loss: 0.8301 score: 0.8367 time: 0.07s
Test loss: 0.3302 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0017; Loss self: 2.8280; time: 0.14s
Val loss: 0.8343 score: 0.8367 time: 0.07s
Test loss: 0.3336 score: 0.8958 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.0551,   Val_Loss: 0.3812,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.3812,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.2359


[0.06828890298493207, 0.06659876205958426, 0.07127433503046632, 0.07317946292459965, 0.07194466097280383, 0.06711245304904878, 0.06573899788782, 0.14557718788273633, 0.06385656516067684, 0.06851792009547353, 0.07136060111224651, 0.07143767992965877, 0.06713745603337884, 0.06727784499526024, 0.06817495613358915, 0.07189378491602838, 0.06828459701500833, 0.06780188088305295, 0.07925928197801113, 0.07272779010236263, 0.07389485696330667, 0.06784835690632463, 0.07278219889849424, 0.07414324907585979, 0.0747198830358684, 0.06786377006210387, 0.06775543512776494, 0.06768793310038745, 0.07094177091494203, 0.07478091004304588, 0.0746957459487021, 0.06782425101846457, 0.06745740794576705, 0.07399326888844371, 0.06755911512300372, 0.068730239989236, 0.06842740206047893, 0.07146139512769878, 0.07135666091926396, 0.06882636994123459, 0.06806009518913925, 0.06850203708745539, 0.07486293790861964, 0.06870868289843202, 0.06871188106015325, 0.07626523193903267, 0.07451410102657974, 0.06826157006435096, 0.0749645410105586, 0.07491347193717957, 0.07481265184469521, 0.0756433520000428, 0.07576676015742123, 0.07579500903375447, 0.06904984195716679, 0.06804451299831271, 0.06924062315374613, 0.07683976204134524, 0.0762723870575428, 0.07571453391574323, 0.06899794703349471, 0.06925304606556892, 0.08322629588656127, 0.08213873789645731, 0.08230934594757855, 0.08265574905090034, 0.08289043209515512, 0.08243244607001543, 0.08227706584148109, 0.08249178691767156, 0.08172065112739801, 0.08162517985329032, 0.08600170211866498, 0.08233708702027798, 0.08627743204124272, 0.08221267093904316, 0.08314996701665223, 0.08238691790029407, 0.08223645109683275, 0.08573622815310955, 0.0859384520445019, 0.08224346092902124, 0.08661332004703581, 0.08268341491930187, 0.08236654591746628, 0.08655086578801274, 0.08242237195372581, 0.08234424679540098, 0.08631139900535345, 0.08209287002682686, 0.08258347189985216, 0.08736896584741771, 0.08500287705101073, 0.08191154897212982, 0.08508770982734859, 0.0861004360485822, 0.08583309105597436, 0.08560976712033153, 0.08244025893509388, 0.08197735506109893, 0.082269525853917, 0.0813488969579339, 0.07837304309941828, 0.07829603110440075, 0.08189262403175235, 0.0785412669647485, 0.07777439686469734, 0.07295321696437895, 0.07331265904940665, 0.07319769100286067, 0.07294450793415308, 0.07355657103471458, 0.07365528889931738, 0.07329984311945736, 0.0730232079513371, 0.07373265479691327, 0.07306284899823368, 0.0728482489939779, 0.07374758902005851, 0.0734372038859874, 0.07294190395623446, 0.07339896005578339, 0.07409525010734797, 0.07396564981900156, 0.07406565290875733, 0.07374472497031093, 0.07267897203564644, 0.07301434502005577, 0.07271833997219801, 0.07290900684893131, 0.07256479212082922, 0.07239380478858948, 0.07321320404298604, 0.07740674912929535, 0.07539352704770863, 0.07341206585988402, 0.07237369893118739, 0.07249403605237603, 0.07227335497736931, 0.0732321001123637, 0.07264992408454418, 0.072564393049106, 0.07300589396618307, 0.07263427996076643, 0.07326420303434134, 0.07294911006465554, 0.07402459112927318, 0.07381613994948566, 0.0741045109461993, 0.075249332934618, 0.07414010190404952, 0.1553272979799658]
[0.0013936510813251442, 0.0013591584093792706, 0.001454578265927884, 0.0014934584270326458, 0.0014682583872000783, 0.0013696418989601793, 0.001341612201792245, 0.002970963018015027, 0.0013031952073607519, 0.001398324899907623, 0.0014563387982091125, 0.0014579118352991585, 0.001370152163946507, 0.0013730172448012294, 0.0013913256353793703, 0.0014672201003271099, 0.0013935632043879252, 0.0013837118547561826, 0.0016175363668981865, 0.0014842406143339313, 0.0015080583053736054, 0.0013846603450270332, 0.001485350997928454, 0.0015131275321604038, 0.0015248955721605796, 0.0013849748992266096, 0.0013827639821992845, 0.0013813863898038256, 0.0014477912431620822, 0.0015261410212866506, 0.0015244029785449408, 0.0013841683881319299, 0.0013766817948115723, 0.0015100667120090552, 0.001378757451489872, 0.0014026579589639999, 0.0013964775930709985, 0.0014583958189326282, 0.0014562583861074277, 0.0014046197947190733, 0.0013889815344722296, 0.0013980007568868448, 0.0015278150593595846, 0.0014022180183353473, 0.001402283286941903, 0.0015564333048782178, 0.0015206959393179538, 0.0013930932666194073, 0.0015298885920522164, 0.001528846366064889, 0.001526788813157045, 0.0015437418775518937, 0.0015462604113759435, 0.0015468369190562137, 0.0014091804481054448, 0.0013886635305778105, 0.0014130739419131863, 0.0015681584090070457, 0.0015565793277049552, 0.0015451945697090455, 0.0014081213680305043, 0.0014133274707258964, 0.0016984958344196178, 0.001676300773397088, 0.0016797825703587458, 0.0016868520214469458, 0.0016916414713296964, 0.001682294817755417, 0.0016791237926832875, 0.001683505855462685, 0.0016677683903550615, 0.0016658199970059249, 0.0017551367779319383, 0.0016803487146995505, 0.001760763919209035, 0.001677809611000881, 0.0016969381023806576, 0.0016813656714345729, 0.0016782949203435257, 0.001749718941900195, 0.0017538459600918755, 0.0016784379781432906, 0.0017676187764701186, 0.0016874166310061605, 0.0016809499166829853, 0.0017663441997553622, 0.0016820892235454247, 0.0016804948325592037, 0.0017614571225582337, 0.001675364694425038, 0.0016853769775480032, 0.0017830401193350554, 0.0017347525928777699, 0.0016716642647373434, 0.0017364838740275223, 0.0017571517560935142, 0.0017516957358362116, 0.0017471381044965617, 0.0016824542639815078, 0.0016730072461448762, 0.0016789699153860612, 0.0016601815705700796, 0.0015994498591718016, 0.001597878185804097, 0.0016712780414643337, 0.0016028829992805816, 0.0015872325890754558, 0.001519858686757895, 0.0015273470635293052, 0.0015249518958929305, 0.001519677248628189, 0.0015324285632232204, 0.0015344851854024455, 0.001527080064988695, 0.001521316832319523, 0.001536096974935693, 0.0015221426874632016, 0.0015176718540412064, 0.0015364081045845523, 0.0015299417476247374, 0.0015196229990882177, 0.001529145001162154, 0.0015436510439030826, 0.0015409510378958657, 0.001543034435599111, 0.0015363484368814777, 0.001514145250742634, 0.0015211321879178286, 0.0015149654160874586, 0.001518937642686069, 0.0015117665025172755, 0.0015082042664289474, 0.0015252750842288758, 0.0016126406068603198, 0.0015706984801605965, 0.0015294180387475838, 0.0015077853943997372, 0.001510292417757834, 0.0015056948953618605, 0.0015256687523409103, 0.0015135400850946705, 0.0015117581885230418, 0.0015209561242954805, 0.0015132141658493008, 0.0015263375632154446, 0.0015197731263469905, 0.001542178981859858, 0.0015378362489476178, 0.0015438439780458186, 0.0015676944361378748, 0.0015445854563343648, 0.003235985374582621]
[717.5397152127607, 735.7494116206082, 687.4844918448538, 669.5867671301042, 681.0790312643594, 730.1178510669042, 745.3718732314085, 336.5911975128268, 767.3447495446313, 715.1413809952627, 686.6534086915208, 685.9125331092489, 729.8459443509237, 728.3229717517271, 718.7390029849711, 681.5610008185239, 717.5849626707213, 722.6938155966044, 618.2241218586114, 673.7452070389279, 663.1043351816962, 722.1987714109605, 673.2415445202183, 660.8828262956964, 655.7826111221042, 722.034746303644, 723.1892158555513, 723.9104188235217, 690.7073134493663, 655.2474417841973, 655.9945198706649, 722.4554530894882, 726.3842696030355, 662.2223985518882, 725.2907310995199, 712.9321825105515, 716.0873937124167, 685.684905989295, 686.6913245203659, 711.9364284624809, 719.9519757330463, 715.3071949881218, 654.5294823963646, 713.15586230104, 713.1226688016786, 642.4946040834332, 657.593654421481, 717.827028499449, 653.6423666370271, 654.0879595206866, 654.9694308620404, 647.7766876324079, 646.7215953036964, 646.4805615126768, 709.6323266083047, 720.1168447074498, 707.677050958906, 637.690678605102, 642.4343316150906, 647.1676898193451, 710.1660572047629, 707.5501047796035, 588.7562275604306, 596.5516546135461, 595.3151423558545, 592.8202280258237, 591.1418092711814, 594.4261311666161, 595.5487048408572, 593.998527985616, 599.6036414787211, 600.3049559960609, 569.7561652022874, 595.1145683345864, 567.9353086978389, 596.0151816054146, 589.2966859528266, 594.7546194081513, 595.8428330315822, 571.5203602436857, 570.17550158602, 595.7920477384649, 565.7328454028814, 592.6218703935182, 594.9017219818766, 566.1410726960802, 594.498785202517, 595.0628235357993, 567.711803593414, 596.8849667941619, 593.3390649816906, 560.8398763191753, 576.4510767158482, 598.2062433793325, 575.8763527591219, 569.1028088679102, 570.8753977885477, 572.3645986692908, 594.3697973896253, 597.7260423135093, 595.6032867748322, 602.3437542778023, 625.214972676794, 625.8299342742275, 598.3444855912928, 623.8758539761341, 630.0273865864159, 657.9559065015197, 654.7300373821114, 655.7583899487225, 658.0344615297089, 652.5589668575863, 651.6843626207656, 654.8445120376861, 657.32527160389, 651.0005659257703, 656.9686325968539, 658.9039635526173, 650.8687353419044, 653.6196567957691, 658.0579529264861, 653.9602191028304, 647.8148050038079, 648.9498857572254, 648.0736767301838, 650.894013359253, 660.43862007924, 657.4050617973116, 660.0810747103353, 658.3548737600665, 661.47781309804, 663.040161242715, 655.619442250028, 620.1009671627448, 636.6594305851463, 653.8434716115185, 663.2243578656688, 662.1234326823878, 664.1451751482972, 655.4502728496273, 660.7026862704141, 661.4814509303109, 657.4811620310272, 660.8449898027117, 655.1630675283648, 657.9929481998768, 648.4331661646734, 650.2642922380887, 647.7338476040754, 637.8794087345044, 647.4229029536612, 309.0248824529933]
Elapsed: 0.07627806664554794~0.010323135039166719
Time per graph: 0.0015661630798405853~0.00021201218074421067
Speed: 646.2656218474327~60.87273086000909
Total Time: 0.1560
best val loss: 0.3812062442302704 test_score: 0.9375

Testing...
Test loss: 0.5362 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.5523008189629763, 0.28034329088404775, 0.38013797998428345, 0.28366734995506704, 0.2807061441708356, 0.2753755869343877, 0.27178946626372635, 0.3481444981880486, 0.2789499789942056, 0.2804980722721666, 0.28219108399935067, 0.27482295385561883, 0.28811906394548714, 0.2694305127952248, 0.265966763952747, 0.2716147140599787, 0.2747285128571093, 0.2645625078584999, 0.2760090581141412, 0.2855536649003625, 0.2667768942192197, 0.26558957411907613, 0.27437451272271574, 0.2850755378603935, 0.2821221458725631, 0.28135149483568966, 0.26460521202534437, 0.2681340689305216, 0.2612410911824554, 0.285267619183287, 0.26780739915557206, 0.26497378409840167, 0.26446221210062504, 0.26843456691130996, 0.26427810406312346, 0.266078864922747, 0.2680864797439426, 0.2656542288605124, 0.2621540119871497, 0.2661384646780789, 0.26739229797385633, 0.26758886384777725, 0.272417546948418, 0.2760032380465418, 0.2641326910816133, 0.2726678899489343, 0.2693086629733443, 0.2668420821428299, 0.2698659331072122, 0.26925457804463804, 0.2684701180551201, 0.28511630883440375, 0.2709937528707087, 0.2714713551104069, 0.2691407350357622, 0.2646327700931579, 0.2752387400250882, 0.2867561839520931, 0.289987237425521, 0.28934478387236595, 0.28692773915827274, 0.2881008400581777, 0.31274113710969687, 0.3064010478556156, 0.3065177630633116, 0.3079992651473731, 0.3078282210044563, 0.3075412637554109, 0.31025993707589805, 0.3062668649945408, 0.3045671519357711, 0.3059573301579803, 0.3110867962241173, 0.3045087961945683, 0.3088553079869598, 0.3051741309463978, 0.3082023069728166, 0.3051136319991201, 0.30577073385939, 0.30883357813581824, 0.31001919601112604, 0.3049350897781551, 0.3111337381415069, 0.30647216783836484, 0.30722707509994507, 0.3115105021279305, 0.3055394981056452, 0.3077218569815159, 0.31105027976445854, 0.30510427989065647, 0.3066576460842043, 0.31150217703543603, 0.3167955121025443, 0.302530765067786, 0.3072625850327313, 0.30808631074614823, 0.3062274360563606, 0.30830011586658657, 0.3031201930716634, 0.3069489279296249, 0.31116104195825756, 0.2985889178235084, 0.29456420917995274, 0.29739511106163263, 0.2999728829599917, 0.2963559930212796, 0.2761164119001478, 0.27582478802651167, 0.2730597690679133, 0.2743538331706077, 0.2739433730021119, 0.2745545906946063, 0.27404933073557913, 0.2738073910586536, 0.27195858233608305, 0.2742146549280733, 0.2725616400130093, 0.2719336471054703, 0.27911442215554416, 0.2742265211418271, 0.27108832309022546, 0.2731257958803326, 0.2767981148790568, 0.2759427069686353, 0.2756279658060521, 0.2950700812507421, 0.2711835019290447, 0.27198109426535666, 0.27008712431415915, 0.26994549203664064, 0.2692516192328185, 0.2693683188408613, 0.2732719839550555, 0.2742610340937972, 0.2801464539952576, 0.2722134820651263, 0.26883709197863936, 0.27856635209172964, 0.2701956359669566, 0.27162705082446337, 0.2700946608092636, 0.2701850140001625, 0.2691033852752298, 0.2706235048826784, 0.2717421690467745, 0.2728297400753945, 0.3447671360336244, 0.2775046781171113, 0.2773620281368494, 0.28079056297428906, 0.2765339990146458, 0.35788120282813907]
Total Epoch List: [62, 45, 45]
Total Time List: [0.06973856897093356, 0.07841397589072585, 0.15597291197627783]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d8948b0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7177;  Loss pred: 0.7056; Loss self: 1.2098; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7149;  Loss pred: 0.7029; Loss self: 1.2007; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000050
Train loss: 0.7080;  Loss pred: 0.6948; Loss self: 1.3189; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6796;  Loss pred: 0.6674; Loss self: 1.2246; time: 0.24s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6287;  Loss pred: 0.6166; Loss self: 1.2069; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6952 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5745;  Loss pred: 0.5626; Loss self: 1.1851; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5255;  Loss pred: 0.5133; Loss self: 1.2195; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6954 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4753;  Loss pred: 0.4625; Loss self: 1.2743; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6952 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4087;  Loss pred: 0.3954; Loss self: 1.3306; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3571;  Loss pred: 0.3431; Loss self: 1.4008; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6911 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000450
Train loss: 0.3023;  Loss pred: 0.2869; Loss self: 1.5474; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6897 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.4898 time: 0.06s
Epoch 12/1000, LR 0.000450
Train loss: 0.2650;  Loss pred: 0.2476; Loss self: 1.7458; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6873 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6888 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2334;  Loss pred: 0.2152; Loss self: 1.8127; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6838 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6843 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.2019;  Loss pred: 0.1831; Loss self: 1.8824; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6784 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6770 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1807;  Loss pred: 0.1608; Loss self: 1.9948; time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6705 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6666 score: 0.4898 time: 0.06s
Epoch 16/1000, LR 0.000450
Train loss: 0.1615;  Loss pred: 0.1405; Loss self: 2.1001; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6590 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6514 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1438;  Loss pred: 0.1225; Loss self: 2.1283; time: 0.13s
Val loss: 0.6429 score: 0.5918 time: 0.07s
Test loss: 0.6297 score: 0.6735 time: 0.06s
Epoch 18/1000, LR 0.000450
Train loss: 0.1307;  Loss pred: 0.1085; Loss self: 2.2147; time: 0.13s
Val loss: 0.6207 score: 0.7143 time: 0.07s
Test loss: 0.6002 score: 0.8163 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1182;  Loss pred: 0.0957; Loss self: 2.2551; time: 0.14s
Val loss: 0.5911 score: 0.7959 time: 0.07s
Test loss: 0.5612 score: 0.8367 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.1046;  Loss pred: 0.0813; Loss self: 2.3283; time: 0.13s
Val loss: 0.5536 score: 0.8367 time: 0.16s
Test loss: 0.5128 score: 0.8980 time: 0.06s
Epoch 21/1000, LR 0.000450
Train loss: 0.0939;  Loss pred: 0.0707; Loss self: 2.3229; time: 0.13s
Val loss: 0.5087 score: 0.8367 time: 0.06s
Test loss: 0.4562 score: 0.9184 time: 0.06s
Epoch 22/1000, LR 0.000450
Train loss: 0.0860;  Loss pred: 0.0624; Loss self: 2.3635; time: 0.13s
Val loss: 0.4602 score: 0.8776 time: 0.06s
Test loss: 0.3967 score: 0.9184 time: 0.06s
Epoch 23/1000, LR 0.000450
Train loss: 0.0765;  Loss pred: 0.0524; Loss self: 2.4114; time: 0.13s
Val loss: 0.4140 score: 0.8776 time: 0.07s
Test loss: 0.3416 score: 0.9184 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0698;  Loss pred: 0.0455; Loss self: 2.4341; time: 0.14s
Val loss: 0.3760 score: 0.8776 time: 0.07s
Test loss: 0.2981 score: 0.9184 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0635;  Loss pred: 0.0390; Loss self: 2.4525; time: 0.14s
Val loss: 0.3488 score: 0.8776 time: 0.07s
Test loss: 0.2685 score: 0.9184 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0585;  Loss pred: 0.0337; Loss self: 2.4780; time: 0.23s
Val loss: 0.3346 score: 0.8776 time: 0.07s
Test loss: 0.2536 score: 0.9184 time: 0.06s
Epoch 27/1000, LR 0.000450
Train loss: 0.0536;  Loss pred: 0.0285; Loss self: 2.5014; time: 0.14s
Val loss: 0.3325 score: 0.8980 time: 0.07s
Test loss: 0.2508 score: 0.9184 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0494;  Loss pred: 0.0244; Loss self: 2.5043; time: 0.14s
Val loss: 0.3404 score: 0.8980 time: 0.07s
Test loss: 0.2570 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0455;  Loss pred: 0.0204; Loss self: 2.5079; time: 0.14s
Val loss: 0.3545 score: 0.8776 time: 0.07s
Test loss: 0.2685 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0430;  Loss pred: 0.0174; Loss self: 2.5576; time: 0.15s
Val loss: 0.3739 score: 0.8776 time: 0.07s
Test loss: 0.2843 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0405;  Loss pred: 0.0148; Loss self: 2.5674; time: 0.15s
Val loss: 0.3945 score: 0.8776 time: 0.08s
Test loss: 0.3017 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0389;  Loss pred: 0.0130; Loss self: 2.5901; time: 0.13s
Val loss: 0.4155 score: 0.8776 time: 0.07s
Test loss: 0.3202 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0368;  Loss pred: 0.0110; Loss self: 2.5860; time: 0.13s
Val loss: 0.4358 score: 0.8776 time: 0.07s
Test loss: 0.3395 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0355;  Loss pred: 0.0098; Loss self: 2.5707; time: 0.13s
Val loss: 0.4535 score: 0.8776 time: 0.07s
Test loss: 0.3572 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0084; Loss self: 2.5993; time: 0.15s
Val loss: 0.4667 score: 0.8776 time: 0.07s
Test loss: 0.3709 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0075; Loss self: 2.5921; time: 0.15s
Val loss: 0.4769 score: 0.8776 time: 0.07s
Test loss: 0.3804 score: 0.9184 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0062; Loss self: 2.6258; time: 0.13s
Val loss: 0.4839 score: 0.8776 time: 0.07s
Test loss: 0.3863 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0055; Loss self: 2.6103; time: 0.13s
Val loss: 0.4888 score: 0.8776 time: 0.07s
Test loss: 0.3895 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0312;  Loss pred: 0.0049; Loss self: 2.6359; time: 0.15s
Val loss: 0.4953 score: 0.8776 time: 0.07s
Test loss: 0.3925 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0045; Loss self: 2.6283; time: 0.15s
Val loss: 0.5023 score: 0.8776 time: 0.07s
Test loss: 0.3948 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0038; Loss self: 2.6183; time: 0.15s
Val loss: 0.5081 score: 0.8776 time: 0.07s
Test loss: 0.3960 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0035; Loss self: 2.6099; time: 0.15s
Val loss: 0.5152 score: 0.8776 time: 0.07s
Test loss: 0.3975 score: 0.9184 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0032; Loss self: 2.6083; time: 0.13s
Val loss: 0.5222 score: 0.8776 time: 0.07s
Test loss: 0.3986 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0285;  Loss pred: 0.0029; Loss self: 2.5650; time: 0.13s
Val loss: 0.5315 score: 0.8776 time: 0.06s
Test loss: 0.4011 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0027; Loss self: 2.5910; time: 0.13s
Val loss: 0.5413 score: 0.8776 time: 0.07s
Test loss: 0.4041 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0024; Loss self: 2.5858; time: 0.13s
Val loss: 0.5490 score: 0.8776 time: 0.07s
Test loss: 0.4056 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0280;  Loss pred: 0.0023; Loss self: 2.5719; time: 0.13s
Val loss: 0.5580 score: 0.8776 time: 0.07s
Test loss: 0.4083 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 026,   Train_Loss: 0.0536,   Val_Loss: 0.3325,   Val_Precision: 1.0000,   Val_Recall: 0.7917,   Val_accuracy: 0.8837,   Val_Score: 0.8980,   Val_Loss: 0.3325,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2508


[0.07546765194274485, 0.07605456095188856, 0.0760096749290824, 0.07510193902999163, 0.07011739700101316, 0.07139949384145439, 0.07503522909246385, 0.07422560011036694, 0.07192501309327781, 0.06944633112289011, 0.07002369011752307, 0.07157985493540764, 0.07127621793188155, 0.07294294401071966, 0.06957876705564559, 0.07020246889442205, 0.06997944507747889, 0.07166679995134473, 0.07080392492935061, 0.0698250278364867, 0.06918445904739201, 0.06925725284963846, 0.07020581606775522, 0.07138505484908819, 0.07102529099211097, 0.06979767116717994, 0.07065669214352965, 0.070675915107131, 0.07201880915090442, 0.07158777606673539, 0.07051803497597575, 0.07042908598668873, 0.07057409198023379, 0.07226642593741417, 0.07194403186440468, 0.15845922892913222, 0.07067344198003411, 0.07051627291366458, 0.07063705893233418, 0.0730077379848808, 0.07163617014884949, 0.1617016140371561, 0.07001528702676296, 0.06987381400540471, 0.07017200021073222, 0.07179631106555462, 0.07251943298615515]
[0.0015401561620968338, 0.0015521338969773175, 0.001551217855695559, 0.0015326926332651352, 0.0014309672857349624, 0.0014571325273766201, 0.00153133120596865, 0.0015148081655176927, 0.0014678574100668942, 0.0014172720637324514, 0.0014290549003576137, 0.0014608133660287273, 0.0014546166924873786, 0.00148863151042285, 0.0014199748378703181, 0.00143270344682494, 0.0014281519403567119, 0.0014625877541090762, 0.0014449780597826656, 0.0014250005680915652, 0.0014119277356610615, 0.0014134133234620094, 0.0014327717564848004, 0.0014568378540630244, 0.0014494957345328769, 0.001424442268717958, 0.0014419733090516255, 0.001442365614431245, 0.0014697716153245798, 0.00146097502177011, 0.0014391435709382807, 0.0014373282854426273, 0.0014402875914333426, 0.0014748250191309014, 0.0014682455482531566, 0.0032338618148802494, 0.0014423151424496758, 0.0014391076104829507, 0.0014415726312721263, 0.0014899538364261389, 0.0014619626560989692, 0.003300032939533798, 0.0014288834087094482, 0.0014259962041919328, 0.001432081636953719, 0.0014652308380725433, 0.0014799884282888807]
[649.2848093004789, 644.2743128975127, 644.6547764573046, 652.4465364393863, 698.8279955585341, 686.2793748763344, 653.0265928770424, 660.1495970007828, 681.2650827946751, 705.5808306603136, 699.763178972169, 684.5501439506507, 687.4663305905084, 671.7579152385046, 704.2378310729805, 697.981150402152, 700.2056096007742, 683.7196586601685, 692.0520302920079, 701.7541062030957, 708.2515448510559, 707.5071271796163, 697.9478730467342, 686.4181880029175, 689.8950967401543, 702.0291534173799, 693.4941123547509, 693.3054906431064, 680.3778148751111, 684.4743990136157, 694.8577057868024, 695.7352820006938, 694.3057802815769, 678.0465391001363, 681.0849869013726, 309.2278078793018, 693.3297519858017, 694.8750689077447, 693.6868655154356, 671.1617337075616, 684.0119997786755, 303.0272783099165, 699.8471631098222, 701.2641387546108, 698.2842138295762, 682.4863182073501, 675.6809586384204]
Elapsed: 0.0752169534098357~0.017979489256770845
Time per graph: 0.0015350398655068508~0.0003669283521789968
Speed: 670.0828139715875~78.45426516595224
Total Time: 0.0729
best val loss: 0.3324723243713379 test_score: 0.9184

Testing...
Test loss: 0.2508 score: 0.9184 time: 0.07s
test Score 0.9184
Epoch Time List: [0.2903297378215939, 0.2724794130772352, 0.2857345568481833, 0.37884562998078763, 0.2603118787519634, 0.26445262786000967, 0.2689348461572081, 0.27441333001479506, 0.35422517475672066, 0.2601067419163883, 0.2600257107988, 0.2636140719987452, 0.26792451716028154, 0.2661338117904961, 0.3512656937818974, 0.25968759204261005, 0.2605229739565402, 0.2672611989546567, 0.2668922198936343, 0.35864680213853717, 0.25973453070037067, 0.25881675095297396, 0.26165567291900516, 0.271086088148877, 0.2676614220254123, 0.35594405396841466, 0.2673302448820323, 0.26887950114905834, 0.27147994097322226, 0.2828991829883307, 0.2916812147013843, 0.2631109480280429, 0.26332018803805113, 0.26531066792085767, 0.2851134098600596, 0.37016159179620445, 0.26323096500709653, 0.2617155851330608, 0.27971915481612086, 0.28604077687487006, 0.28398579615168273, 0.3711271609645337, 0.26208157511428, 0.2603619499132037, 0.26177747710607946, 0.2659403670113534, 0.26865623309277]
Total Epoch List: [47]
Total Time List: [0.07289576204493642]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d72d0f0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7177;  Loss pred: 0.7038; Loss self: 1.3897; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7271;  Loss pred: 0.7131; Loss self: 1.4027; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000050
Train loss: 0.7133;  Loss pred: 0.6987; Loss self: 1.4609; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6863;  Loss pred: 0.6728; Loss self: 1.3430; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6483;  Loss pred: 0.6347; Loss self: 1.3568; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.6160;  Loss pred: 0.6017; Loss self: 1.4250; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5755;  Loss pred: 0.5615; Loss self: 1.3945; time: 0.14s
Val loss: 0.6918 score: 0.7959 time: 0.08s
Test loss: 0.6919 score: 0.8571 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.5327;  Loss pred: 0.5187; Loss self: 1.4032; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6914 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6911 score: 0.5102 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4824;  Loss pred: 0.4674; Loss self: 1.4997; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6908 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6900 score: 0.5102 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.4225;  Loss pred: 0.4069; Loss self: 1.5658; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6897 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6885 score: 0.5102 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.3662;  Loss pred: 0.3497; Loss self: 1.6494; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6882 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6866 score: 0.5102 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.3240;  Loss pred: 0.3070; Loss self: 1.7051; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6857 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6836 score: 0.5102 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2991;  Loss pred: 0.2813; Loss self: 1.7807; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6817 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6794 score: 0.5102 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.2694;  Loss pred: 0.2505; Loss self: 1.8882; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6761 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6735 score: 0.5102 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.2465;  Loss pred: 0.2270; Loss self: 1.9486; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6680 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6655 score: 0.5102 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.2124;  Loss pred: 0.1924; Loss self: 2.0056; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6561 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6538 score: 0.5102 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1981;  Loss pred: 0.1774; Loss self: 2.0698; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6398 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6379 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1677;  Loss pred: 0.1472; Loss self: 2.0536; time: 0.14s
Val loss: 0.6185 score: 0.5306 time: 0.08s
Test loss: 0.6173 score: 0.5306 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1462;  Loss pred: 0.1255; Loss self: 2.0727; time: 0.14s
Val loss: 0.5908 score: 0.6735 time: 0.08s
Test loss: 0.5906 score: 0.6939 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.1272;  Loss pred: 0.1066; Loss self: 2.0582; time: 0.15s
Val loss: 0.5573 score: 0.7959 time: 0.07s
Test loss: 0.5585 score: 0.7959 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.1152;  Loss pred: 0.0944; Loss self: 2.0822; time: 0.14s
Val loss: 0.5183 score: 0.8776 time: 0.07s
Test loss: 0.5212 score: 0.8776 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.1046;  Loss pred: 0.0837; Loss self: 2.0894; time: 0.14s
Val loss: 0.4739 score: 0.8980 time: 0.07s
Test loss: 0.4786 score: 0.8980 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0926;  Loss pred: 0.0711; Loss self: 2.1491; time: 0.14s
Val loss: 0.4265 score: 0.9184 time: 0.07s
Test loss: 0.4333 score: 0.9184 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0849;  Loss pred: 0.0634; Loss self: 2.1486; time: 0.14s
Val loss: 0.3779 score: 0.9592 time: 0.07s
Test loss: 0.3875 score: 0.9388 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0769;  Loss pred: 0.0548; Loss self: 2.2067; time: 0.14s
Val loss: 0.3300 score: 0.9388 time: 0.07s
Test loss: 0.3428 score: 0.9388 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0702;  Loss pred: 0.0480; Loss self: 2.2261; time: 0.14s
Val loss: 0.2851 score: 0.9388 time: 0.07s
Test loss: 0.3026 score: 0.9184 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0647;  Loss pred: 0.0421; Loss self: 2.2625; time: 0.14s
Val loss: 0.2452 score: 0.9388 time: 0.07s
Test loss: 0.2690 score: 0.9184 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0578;  Loss pred: 0.0346; Loss self: 2.3153; time: 0.14s
Val loss: 0.2126 score: 0.9388 time: 0.07s
Test loss: 0.2430 score: 0.9184 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0526;  Loss pred: 0.0293; Loss self: 2.3285; time: 0.14s
Val loss: 0.1869 score: 0.9184 time: 0.07s
Test loss: 0.2246 score: 0.9184 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0478;  Loss pred: 0.0243; Loss self: 2.3464; time: 0.14s
Val loss: 0.1680 score: 0.9184 time: 0.07s
Test loss: 0.2131 score: 0.9184 time: 0.07s
Epoch 31/1000, LR 0.000450
Train loss: 0.0437;  Loss pred: 0.0199; Loss self: 2.3787; time: 0.14s
Val loss: 0.1545 score: 0.9184 time: 0.07s
Test loss: 0.2075 score: 0.9184 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0407;  Loss pred: 0.0168; Loss self: 2.3888; time: 0.14s
Val loss: 0.1457 score: 0.9184 time: 0.07s
Test loss: 0.2062 score: 0.9184 time: 0.07s
Epoch 33/1000, LR 0.000449
Train loss: 0.0374;  Loss pred: 0.0133; Loss self: 2.4158; time: 0.14s
Val loss: 0.1401 score: 0.9184 time: 0.07s
Test loss: 0.2086 score: 0.9184 time: 0.07s
Epoch 34/1000, LR 0.000449
Train loss: 0.0365;  Loss pred: 0.0119; Loss self: 2.4578; time: 0.15s
Val loss: 0.1373 score: 0.9388 time: 0.07s
Test loss: 0.2128 score: 0.9184 time: 0.07s
Epoch 35/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0100; Loss self: 2.4375; time: 0.14s
Val loss: 0.1360 score: 0.9388 time: 0.07s
Test loss: 0.2185 score: 0.9184 time: 0.07s
Epoch 36/1000, LR 0.000449
Train loss: 0.0332;  Loss pred: 0.0085; Loss self: 2.4646; time: 0.14s
Val loss: 0.1358 score: 0.9388 time: 0.07s
Test loss: 0.2237 score: 0.9388 time: 0.07s
Epoch 37/1000, LR 0.000449
Train loss: 0.0319;  Loss pred: 0.0070; Loss self: 2.4879; time: 0.14s
Val loss: 0.1363 score: 0.9388 time: 0.07s
Test loss: 0.2294 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0067; Loss self: 2.4885; time: 0.14s
Val loss: 0.1376 score: 0.9388 time: 0.07s
Test loss: 0.2347 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0060; Loss self: 2.4742; time: 0.14s
Val loss: 0.1389 score: 0.9388 time: 0.07s
Test loss: 0.2393 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0049; Loss self: 2.4889; time: 0.14s
Val loss: 0.1406 score: 0.9388 time: 0.07s
Test loss: 0.2433 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0044; Loss self: 2.4528; time: 0.13s
Val loss: 0.1423 score: 0.9388 time: 0.07s
Test loss: 0.2470 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0285;  Loss pred: 0.0039; Loss self: 2.4639; time: 0.15s
Val loss: 0.1438 score: 0.9388 time: 0.07s
Test loss: 0.2497 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0280;  Loss pred: 0.0034; Loss self: 2.4685; time: 0.12s
Val loss: 0.1451 score: 0.9388 time: 0.07s
Test loss: 0.2530 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0278;  Loss pred: 0.0031; Loss self: 2.4711; time: 0.12s
Val loss: 0.1467 score: 0.9388 time: 0.07s
Test loss: 0.2554 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0274;  Loss pred: 0.0028; Loss self: 2.4598; time: 0.13s
Val loss: 0.1486 score: 0.9388 time: 0.07s
Test loss: 0.2577 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0269;  Loss pred: 0.0026; Loss self: 2.4323; time: 0.12s
Val loss: 0.1504 score: 0.9388 time: 0.07s
Test loss: 0.2602 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0267;  Loss pred: 0.0024; Loss self: 2.4365; time: 0.13s
Val loss: 0.1519 score: 0.9388 time: 0.07s
Test loss: 0.2626 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0021; Loss self: 2.4327; time: 0.12s
Val loss: 0.1531 score: 0.9388 time: 0.07s
Test loss: 0.2646 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0263;  Loss pred: 0.0021; Loss self: 2.4245; time: 0.12s
Val loss: 0.1540 score: 0.9388 time: 0.07s
Test loss: 0.2666 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0259;  Loss pred: 0.0020; Loss self: 2.3902; time: 0.14s
Val loss: 0.1549 score: 0.9388 time: 0.07s
Test loss: 0.2687 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0255;  Loss pred: 0.0018; Loss self: 2.3690; time: 0.14s
Val loss: 0.1556 score: 0.9388 time: 0.07s
Test loss: 0.2705 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0256;  Loss pred: 0.0018; Loss self: 2.3798; time: 0.14s
Val loss: 0.1561 score: 0.9388 time: 0.07s
Test loss: 0.2729 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0253;  Loss pred: 0.0018; Loss self: 2.3522; time: 0.14s
Val loss: 0.1566 score: 0.9388 time: 0.07s
Test loss: 0.2749 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0250;  Loss pred: 0.0017; Loss self: 2.3333; time: 0.14s
Val loss: 0.1570 score: 0.9388 time: 0.07s
Test loss: 0.2767 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0248;  Loss pred: 0.0016; Loss self: 2.3167; time: 0.14s
Val loss: 0.1572 score: 0.9388 time: 0.07s
Test loss: 0.2781 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0246;  Loss pred: 0.0015; Loss self: 2.3013; time: 0.14s
Val loss: 0.1574 score: 0.9388 time: 0.07s
Test loss: 0.2797 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 035,   Train_Loss: 0.0332,   Val_Loss: 0.1358,   Val_Precision: 0.9231,   Val_Recall: 0.9600,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.1358,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2237


[0.07546765194274485, 0.07605456095188856, 0.0760096749290824, 0.07510193902999163, 0.07011739700101316, 0.07139949384145439, 0.07503522909246385, 0.07422560011036694, 0.07192501309327781, 0.06944633112289011, 0.07002369011752307, 0.07157985493540764, 0.07127621793188155, 0.07294294401071966, 0.06957876705564559, 0.07020246889442205, 0.06997944507747889, 0.07166679995134473, 0.07080392492935061, 0.0698250278364867, 0.06918445904739201, 0.06925725284963846, 0.07020581606775522, 0.07138505484908819, 0.07102529099211097, 0.06979767116717994, 0.07065669214352965, 0.070675915107131, 0.07201880915090442, 0.07158777606673539, 0.07051803497597575, 0.07042908598668873, 0.07057409198023379, 0.07226642593741417, 0.07194403186440468, 0.15845922892913222, 0.07067344198003411, 0.07051627291366458, 0.07063705893233418, 0.0730077379848808, 0.07163617014884949, 0.1617016140371561, 0.07001528702676296, 0.06987381400540471, 0.07017200021073222, 0.07179631106555462, 0.07251943298615515, 0.07702614506706595, 0.07729381904937327, 0.08012030413374305, 0.07801705598831177, 0.07802700088359416, 0.07817764487117529, 0.07816984807141125, 0.07884047902189195, 0.07817930891178548, 0.0786815769970417, 0.07846269593574107, 0.07858217298053205, 0.07911408017389476, 0.07880670996382833, 0.07873677019961178, 0.07875716895796359, 0.0785860491450876, 0.0786129750777036, 0.07802844699472189, 0.07583019905723631, 0.0754148568958044, 0.07531544519588351, 0.07590169203467667, 0.07592660305090249, 0.07576582510955632, 0.07633329299278557, 0.07560579595156014, 0.07544433791190386, 0.07570934016257524, 0.07527110096998513, 0.07502574217505753, 0.07495409017428756, 0.07525210990570486, 0.075069400947541, 0.0750416989903897, 0.07497497508302331, 0.07529529882594943, 0.07533829403109848, 0.07512498996220529, 0.07534903404302895, 0.07513234089128673, 0.07501841196790338, 0.07537745521403849, 0.07519854488782585, 0.07518889498896897, 0.07546506100334227, 0.07521056197583675, 0.07540983497165143, 0.07545477105304599, 0.07514035888016224, 0.07531304704025388, 0.07504924107342958, 0.07492424082010984, 0.07485929690301418, 0.07509810593910515, 0.07472838996909559]
[0.0015401561620968338, 0.0015521338969773175, 0.001551217855695559, 0.0015326926332651352, 0.0014309672857349624, 0.0014571325273766201, 0.00153133120596865, 0.0015148081655176927, 0.0014678574100668942, 0.0014172720637324514, 0.0014290549003576137, 0.0014608133660287273, 0.0014546166924873786, 0.00148863151042285, 0.0014199748378703181, 0.00143270344682494, 0.0014281519403567119, 0.0014625877541090762, 0.0014449780597826656, 0.0014250005680915652, 0.0014119277356610615, 0.0014134133234620094, 0.0014327717564848004, 0.0014568378540630244, 0.0014494957345328769, 0.001424442268717958, 0.0014419733090516255, 0.001442365614431245, 0.0014697716153245798, 0.00146097502177011, 0.0014391435709382807, 0.0014373282854426273, 0.0014402875914333426, 0.0014748250191309014, 0.0014682455482531566, 0.0032338618148802494, 0.0014423151424496758, 0.0014391076104829507, 0.0014415726312721263, 0.0014899538364261389, 0.0014619626560989692, 0.003300032939533798, 0.0014288834087094482, 0.0014259962041919328, 0.001432081636953719, 0.0014652308380725433, 0.0014799884282888807, 0.0015719621442258358, 0.0015774248785586382, 0.0016351082476274092, 0.0015921848160879953, 0.0015923877731345746, 0.0015954621402280672, 0.0015953030218655358, 0.0016089893677937134, 0.0015954961002405202, 0.0016057464693273818, 0.001601279508892675, 0.0016037178159292256, 0.0016145730647733625, 0.0016083002033434352, 0.001606872861216567, 0.0016072891624074202, 0.0016037969213283183, 0.0016043464301572163, 0.0015924172856065693, 0.001547555082800741, 0.0015390787121592735, 0.0015370499019568063, 0.0015490141231566668, 0.0015495225112429078, 0.0015462413287664555, 0.0015578223059752158, 0.00154297542758286, 0.0015396803655490583, 0.0015450885747464336, 0.0015361449177547985, 0.0015311375954093374, 0.0015296753096793378, 0.001535757345014385, 0.0015320285907661428, 0.0015314632447018307, 0.0015301015323065982, 0.0015366387515499884, 0.0015375162047162956, 0.001533163060453169, 0.0015377353886332438, 0.001533313079414015, 0.001530987999344967, 0.0015383154125313979, 0.0015346641813842009, 0.001534467244672836, 0.0015401032857824952, 0.0015349094280783012, 0.0015389762239112537, 0.001539893286796857, 0.0015334767118400457, 0.0015370009600051812, 0.001531617164763869, 0.0015290661391859151, 0.0015277407531227385, 0.0015326144069205134, 0.0015250691830427671]
[649.2848093004789, 644.2743128975127, 644.6547764573046, 652.4465364393863, 698.8279955585341, 686.2793748763344, 653.0265928770424, 660.1495970007828, 681.2650827946751, 705.5808306603136, 699.763178972169, 684.5501439506507, 687.4663305905084, 671.7579152385046, 704.2378310729805, 697.981150402152, 700.2056096007742, 683.7196586601685, 692.0520302920079, 701.7541062030957, 708.2515448510559, 707.5071271796163, 697.9478730467342, 686.4181880029175, 689.8950967401543, 702.0291534173799, 693.4941123547509, 693.3054906431064, 680.3778148751111, 684.4743990136157, 694.8577057868024, 695.7352820006938, 694.3057802815769, 678.0465391001363, 681.0849869013726, 309.2278078793018, 693.3297519858017, 694.8750689077447, 693.6868655154356, 671.1617337075616, 684.0119997786755, 303.0272783099165, 699.8471631098222, 701.2641387546108, 698.2842138295762, 682.4863182073501, 675.6809586384204, 636.1476347717538, 633.944610353644, 611.5803045156367, 628.0677908089867, 627.9877407194139, 626.7776431580211, 626.8401590756139, 621.5081466767087, 626.7643022438291, 622.7633185573075, 624.5005912125392, 623.5510948792324, 619.3587777586082, 621.7744659368552, 622.3267715423968, 622.1655837597922, 623.5203389539909, 623.3067753963875, 627.9761021427804, 646.1805535155593, 649.7393486763488, 650.5969641759241, 645.5719060599297, 645.3600981878456, 646.7295766811314, 641.9217366219364, 648.0984610147323, 649.4854531988492, 647.2120863129877, 650.9802483098937, 653.1091673264401, 653.7335038830087, 651.1445335074295, 652.7293328774733, 652.9702906416769, 653.5514009273094, 650.7710410083778, 650.3996490785092, 652.2463433892166, 650.3069431788332, 652.1825277732381, 653.1729839997765, 650.0617440700507, 651.6083532346752, 651.691982003311, 649.3071011740101, 651.5042397335425, 649.7826181216336, 649.3956487596015, 652.1129354485485, 650.6176808091448, 652.9046703091572, 653.9939472679753, 654.5613173936587, 652.4798380365633, 655.7079581169121]
Elapsed: 0.0758439780945338~0.012212368855441809
Time per graph: 0.001547836287643547~0.0002492320174579961
Speed: 654.744452368479~55.63644234870202
Total Time: 0.0753
best val loss: 0.135848268866539 test_score: 0.9388

Testing...
Test loss: 0.3875 score: 0.9388 time: 0.07s
test Score 0.9388
Epoch Time List: [0.2903297378215939, 0.2724794130772352, 0.2857345568481833, 0.37884562998078763, 0.2603118787519634, 0.26445262786000967, 0.2689348461572081, 0.27441333001479506, 0.35422517475672066, 0.2601067419163883, 0.2600257107988, 0.2636140719987452, 0.26792451716028154, 0.2661338117904961, 0.3512656937818974, 0.25968759204261005, 0.2605229739565402, 0.2672611989546567, 0.2668922198936343, 0.35864680213853717, 0.25973453070037067, 0.25881675095297396, 0.26165567291900516, 0.271086088148877, 0.2676614220254123, 0.35594405396841466, 0.2673302448820323, 0.26887950114905834, 0.27147994097322226, 0.2828991829883307, 0.2916812147013843, 0.2631109480280429, 0.26332018803805113, 0.26531066792085767, 0.2851134098600596, 0.37016159179620445, 0.26323096500709653, 0.2617155851330608, 0.27971915481612086, 0.28604077687487006, 0.28398579615168273, 0.3711271609645337, 0.26208157511428, 0.2603619499132037, 0.26177747710607946, 0.2659403670113534, 0.26865623309277, 0.29451163578778505, 0.2863802940119058, 0.29176111170090735, 0.2978410960640758, 0.2925001201219857, 0.2934917928650975, 0.29319295194000006, 0.29471452604047954, 0.29340406227856874, 0.2963820556178689, 0.2947969469241798, 0.29608828108757734, 0.2953812840860337, 0.2971685591619462, 0.28401938802562654, 0.2948705740272999, 0.294560540933162, 0.29484850401058793, 0.2937284605577588, 0.2912608340848237, 0.2867832942865789, 0.28806957323104143, 0.28804590879008174, 0.28762283199466765, 0.28834651526995003, 0.2843555563595146, 0.28572112903930247, 0.2864133499097079, 0.2834974767174572, 0.28766610007733107, 0.2853085140231997, 0.28588874195702374, 0.28751304815523326, 0.2888744599185884, 0.28742541931569576, 0.28503147792071104, 0.2855210618581623, 0.28740216535516083, 0.2849180439952761, 0.28760783304460347, 0.2720713303424418, 0.29362551239319146, 0.2669460428878665, 0.2660779261495918, 0.268961995607242, 0.26605976466089487, 0.2706018779426813, 0.2663670089095831, 0.26623937487602234, 0.2805013719480485, 0.285395345184952, 0.2848436329513788, 0.2849322040565312, 0.284429999999702, 0.28610587283037603, 0.2852968748193234]
Total Epoch List: [47, 56]
Total Time List: [0.07289576204493642, 0.0753304110839963]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d6876d0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7129;  Loss pred: 0.6980; Loss self: 1.4988; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6957 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6944 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7059;  Loss pred: 0.6921; Loss self: 1.3771; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6880;  Loss pred: 0.6743; Loss self: 1.3714; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6737;  Loss pred: 0.6599; Loss self: 1.3752; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6395;  Loss pred: 0.6263; Loss self: 1.3179; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5966;  Loss pred: 0.5827; Loss self: 1.3936; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6960 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5538;  Loss pred: 0.5398; Loss self: 1.3967; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6960 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.5066;  Loss pred: 0.4918; Loss self: 1.4765; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6962 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.4609;  Loss pred: 0.4451; Loss self: 1.5731; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6962 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.4047;  Loss pred: 0.3883; Loss self: 1.6374; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6959 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000450
Train loss: 0.3370;  Loss pred: 0.3200; Loss self: 1.6999; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6949 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6938 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2997;  Loss pred: 0.2822; Loss self: 1.7501; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2623;  Loss pred: 0.2444; Loss self: 1.7904; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6904 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6896 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.2267;  Loss pred: 0.2079; Loss self: 1.8800; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6856 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6851 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1964;  Loss pred: 0.1768; Loss self: 1.9521; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6779 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6777 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1762;  Loss pred: 0.1559; Loss self: 2.0295; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6665 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6669 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1586;  Loss pred: 0.1379; Loss self: 2.0731; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6498 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6510 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1411;  Loss pred: 0.1196; Loss self: 2.1471; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6255 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6276 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1253;  Loss pred: 0.1033; Loss self: 2.2019; time: 0.16s
Val loss: 0.5926 score: 0.5918 time: 0.07s
Test loss: 0.5955 score: 0.5625 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.1127;  Loss pred: 0.0901; Loss self: 2.2608; time: 0.16s
Val loss: 0.5499 score: 0.6939 time: 0.07s
Test loss: 0.5534 score: 0.7292 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.1023;  Loss pred: 0.0791; Loss self: 2.3191; time: 0.16s
Val loss: 0.4988 score: 0.8163 time: 0.07s
Test loss: 0.5030 score: 0.8542 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0906;  Loss pred: 0.0670; Loss self: 2.3524; time: 0.16s
Val loss: 0.4435 score: 0.8980 time: 0.07s
Test loss: 0.4477 score: 0.8750 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0840;  Loss pred: 0.0599; Loss self: 2.4087; time: 0.16s
Val loss: 0.3895 score: 0.8980 time: 0.07s
Test loss: 0.3916 score: 0.8750 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0744;  Loss pred: 0.0495; Loss self: 2.4856; time: 0.16s
Val loss: 0.3414 score: 0.9184 time: 0.07s
Test loss: 0.3394 score: 0.8750 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0668;  Loss pred: 0.0416; Loss self: 2.5178; time: 0.16s
Val loss: 0.3022 score: 0.9184 time: 0.07s
Test loss: 0.2944 score: 0.8750 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0609;  Loss pred: 0.0355; Loss self: 2.5464; time: 0.16s
Val loss: 0.2757 score: 0.9184 time: 0.07s
Test loss: 0.2603 score: 0.8750 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0552;  Loss pred: 0.0296; Loss self: 2.5576; time: 0.16s
Val loss: 0.2598 score: 0.9184 time: 0.07s
Test loss: 0.2358 score: 0.8750 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0505;  Loss pred: 0.0243; Loss self: 2.6214; time: 0.16s
Val loss: 0.2529 score: 0.9184 time: 0.07s
Test loss: 0.2194 score: 0.8958 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0467;  Loss pred: 0.0203; Loss self: 2.6432; time: 0.16s
Val loss: 0.2535 score: 0.9184 time: 0.07s
Test loss: 0.2096 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0442;  Loss pred: 0.0175; Loss self: 2.6682; time: 0.16s
Val loss: 0.2595 score: 0.9388 time: 0.07s
Test loss: 0.2043 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0412;  Loss pred: 0.0145; Loss self: 2.6735; time: 0.16s
Val loss: 0.2668 score: 0.9388 time: 0.07s
Test loss: 0.2005 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0390;  Loss pred: 0.0120; Loss self: 2.7014; time: 0.16s
Val loss: 0.2768 score: 0.9388 time: 0.07s
Test loss: 0.1998 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0375;  Loss pred: 0.0105; Loss self: 2.7050; time: 0.16s
Val loss: 0.2850 score: 0.9388 time: 0.07s
Test loss: 0.1989 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0361;  Loss pred: 0.0090; Loss self: 2.7135; time: 0.16s
Val loss: 0.2944 score: 0.9388 time: 0.07s
Test loss: 0.1997 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0353;  Loss pred: 0.0077; Loss self: 2.7513; time: 0.16s
Val loss: 0.3018 score: 0.9388 time: 0.07s
Test loss: 0.2004 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0338;  Loss pred: 0.0064; Loss self: 2.7376; time: 0.16s
Val loss: 0.3085 score: 0.9388 time: 0.07s
Test loss: 0.2017 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0329;  Loss pred: 0.0056; Loss self: 2.7265; time: 0.16s
Val loss: 0.3133 score: 0.9388 time: 0.07s
Test loss: 0.2028 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0322;  Loss pred: 0.0049; Loss self: 2.7249; time: 0.16s
Val loss: 0.3176 score: 0.9388 time: 0.07s
Test loss: 0.2048 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0044; Loss self: 2.7364; time: 0.16s
Val loss: 0.3237 score: 0.9388 time: 0.07s
Test loss: 0.2087 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0310;  Loss pred: 0.0038; Loss self: 2.7125; time: 0.14s
Val loss: 0.3270 score: 0.9388 time: 0.07s
Test loss: 0.2112 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0037; Loss self: 2.7168; time: 0.14s
Val loss: 0.3310 score: 0.9388 time: 0.07s
Test loss: 0.2143 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0031; Loss self: 2.7301; time: 0.14s
Val loss: 0.3344 score: 0.9388 time: 0.07s
Test loss: 0.2176 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0028; Loss self: 2.7000; time: 0.16s
Val loss: 0.3363 score: 0.9388 time: 0.07s
Test loss: 0.2199 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0299;  Loss pred: 0.0026; Loss self: 2.7273; time: 0.16s
Val loss: 0.3391 score: 0.9388 time: 0.07s
Test loss: 0.2228 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0024; Loss self: 2.6929; time: 0.16s
Val loss: 0.3396 score: 0.9388 time: 0.07s
Test loss: 0.2244 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0292;  Loss pred: 0.0022; Loss self: 2.6969; time: 0.16s
Val loss: 0.3429 score: 0.9388 time: 0.07s
Test loss: 0.2285 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0021; Loss self: 2.7028; time: 0.16s
Val loss: 0.3454 score: 0.9388 time: 0.07s
Test loss: 0.2323 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0288;  Loss pred: 0.0020; Loss self: 2.6766; time: 0.16s
Val loss: 0.3476 score: 0.9388 time: 0.07s
Test loss: 0.2359 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 027,   Train_Loss: 0.0505,   Val_Loss: 0.2529,   Val_Precision: 1.0000,   Val_Recall: 0.8400,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.2529,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.2194


[0.07546765194274485, 0.07605456095188856, 0.0760096749290824, 0.07510193902999163, 0.07011739700101316, 0.07139949384145439, 0.07503522909246385, 0.07422560011036694, 0.07192501309327781, 0.06944633112289011, 0.07002369011752307, 0.07157985493540764, 0.07127621793188155, 0.07294294401071966, 0.06957876705564559, 0.07020246889442205, 0.06997944507747889, 0.07166679995134473, 0.07080392492935061, 0.0698250278364867, 0.06918445904739201, 0.06925725284963846, 0.07020581606775522, 0.07138505484908819, 0.07102529099211097, 0.06979767116717994, 0.07065669214352965, 0.070675915107131, 0.07201880915090442, 0.07158777606673539, 0.07051803497597575, 0.07042908598668873, 0.07057409198023379, 0.07226642593741417, 0.07194403186440468, 0.15845922892913222, 0.07067344198003411, 0.07051627291366458, 0.07063705893233418, 0.0730077379848808, 0.07163617014884949, 0.1617016140371561, 0.07001528702676296, 0.06987381400540471, 0.07017200021073222, 0.07179631106555462, 0.07251943298615515, 0.07702614506706595, 0.07729381904937327, 0.08012030413374305, 0.07801705598831177, 0.07802700088359416, 0.07817764487117529, 0.07816984807141125, 0.07884047902189195, 0.07817930891178548, 0.0786815769970417, 0.07846269593574107, 0.07858217298053205, 0.07911408017389476, 0.07880670996382833, 0.07873677019961178, 0.07875716895796359, 0.0785860491450876, 0.0786129750777036, 0.07802844699472189, 0.07583019905723631, 0.0754148568958044, 0.07531544519588351, 0.07590169203467667, 0.07592660305090249, 0.07576582510955632, 0.07633329299278557, 0.07560579595156014, 0.07544433791190386, 0.07570934016257524, 0.07527110096998513, 0.07502574217505753, 0.07495409017428756, 0.07525210990570486, 0.075069400947541, 0.0750416989903897, 0.07497497508302331, 0.07529529882594943, 0.07533829403109848, 0.07512498996220529, 0.07534903404302895, 0.07513234089128673, 0.07501841196790338, 0.07537745521403849, 0.07519854488782585, 0.07518889498896897, 0.07546506100334227, 0.07521056197583675, 0.07540983497165143, 0.07545477105304599, 0.07514035888016224, 0.07531304704025388, 0.07504924107342958, 0.07492424082010984, 0.07485929690301418, 0.07509810593910515, 0.07472838996909559, 0.07247395999729633, 0.0728635648265481, 0.07334688794799149, 0.07364121312275529, 0.0735632129944861, 0.07331800414249301, 0.07365575502626598, 0.07352073607034981, 0.07352406997233629, 0.07367181312292814, 0.07353270589374006, 0.07349677802994847, 0.07380740297958255, 0.07394345803186297, 0.07387153897434473, 0.07364735496230423, 0.07749033090658486, 0.07320757699199021, 0.07330871792510152, 0.07326652994379401, 0.07345331297256052, 0.07345765992067754, 0.07333741616457701, 0.07325136289000511, 0.07305568107403815, 0.07343401899561286, 0.0729677639901638, 0.07316654385067523, 0.07348861801438034, 0.07343023200519383, 0.07315526297315955, 0.07357690297067165, 0.07323667616583407, 0.07339714304544032, 0.0735291400924325, 0.07381832413375378, 0.07345917401835322, 0.07340427697636187, 0.07361593702808022, 0.07362949103116989, 0.07373095490038395, 0.07378010195679963, 0.07384700002148747, 0.07373830094002187, 0.0739124498795718, 0.0746811858844012, 0.07439751480706036, 0.07783698104321957]
[0.0015401561620968338, 0.0015521338969773175, 0.001551217855695559, 0.0015326926332651352, 0.0014309672857349624, 0.0014571325273766201, 0.00153133120596865, 0.0015148081655176927, 0.0014678574100668942, 0.0014172720637324514, 0.0014290549003576137, 0.0014608133660287273, 0.0014546166924873786, 0.00148863151042285, 0.0014199748378703181, 0.00143270344682494, 0.0014281519403567119, 0.0014625877541090762, 0.0014449780597826656, 0.0014250005680915652, 0.0014119277356610615, 0.0014134133234620094, 0.0014327717564848004, 0.0014568378540630244, 0.0014494957345328769, 0.001424442268717958, 0.0014419733090516255, 0.001442365614431245, 0.0014697716153245798, 0.00146097502177011, 0.0014391435709382807, 0.0014373282854426273, 0.0014402875914333426, 0.0014748250191309014, 0.0014682455482531566, 0.0032338618148802494, 0.0014423151424496758, 0.0014391076104829507, 0.0014415726312721263, 0.0014899538364261389, 0.0014619626560989692, 0.003300032939533798, 0.0014288834087094482, 0.0014259962041919328, 0.001432081636953719, 0.0014652308380725433, 0.0014799884282888807, 0.0015719621442258358, 0.0015774248785586382, 0.0016351082476274092, 0.0015921848160879953, 0.0015923877731345746, 0.0015954621402280672, 0.0015953030218655358, 0.0016089893677937134, 0.0015954961002405202, 0.0016057464693273818, 0.001601279508892675, 0.0016037178159292256, 0.0016145730647733625, 0.0016083002033434352, 0.001606872861216567, 0.0016072891624074202, 0.0016037969213283183, 0.0016043464301572163, 0.0015924172856065693, 0.001547555082800741, 0.0015390787121592735, 0.0015370499019568063, 0.0015490141231566668, 0.0015495225112429078, 0.0015462413287664555, 0.0015578223059752158, 0.00154297542758286, 0.0015396803655490583, 0.0015450885747464336, 0.0015361449177547985, 0.0015311375954093374, 0.0015296753096793378, 0.001535757345014385, 0.0015320285907661428, 0.0015314632447018307, 0.0015301015323065982, 0.0015366387515499884, 0.0015375162047162956, 0.001533163060453169, 0.0015377353886332438, 0.001533313079414015, 0.001530987999344967, 0.0015383154125313979, 0.0015346641813842009, 0.001534467244672836, 0.0015401032857824952, 0.0015349094280783012, 0.0015389762239112537, 0.001539893286796857, 0.0015334767118400457, 0.0015370009600051812, 0.001531617164763869, 0.0015290661391859151, 0.0015277407531227385, 0.0015326144069205134, 0.0015250691830427671, 0.0015098741666103403, 0.0015179909338864188, 0.001528060165583156, 0.0015341919400574018, 0.001532566937385127, 0.001527458419635271, 0.0015344948963805412, 0.0015316820014656212, 0.0015317514577570062, 0.001534829440061003, 0.0015319313727862511, 0.0015311828756239265, 0.001537654228741303, 0.0015404887089971453, 0.0015389903952988486, 0.0015343198950480048, 0.0016143818938871846, 0.001525157853999796, 0.0015272649567729484, 0.0015263860404957086, 0.001530277353595011, 0.0015303679150141154, 0.001527862836762021, 0.0015260700602084398, 0.001521993355709128, 0.0015298753957419346, 0.0015201617497950792, 0.0015243029968890671, 0.0015310128752995904, 0.0015297965001082048, 0.0015240679786074907, 0.001532852145222326, 0.0015257640867882098, 0.0015291071467800066, 0.0015318570852590103, 0.001537881752786537, 0.0015303994587156922, 0.0015292557703408722, 0.0015336653547516714, 0.0015339477298160393, 0.0015360615604246657, 0.0015370854574333255, 0.0015384791671143223, 0.0015362146029171224, 0.0015398427058244124, 0.0015558580392583583, 0.0015499482251470909, 0.001621603771733741]
[649.2848093004789, 644.2743128975127, 644.6547764573046, 652.4465364393863, 698.8279955585341, 686.2793748763344, 653.0265928770424, 660.1495970007828, 681.2650827946751, 705.5808306603136, 699.763178972169, 684.5501439506507, 687.4663305905084, 671.7579152385046, 704.2378310729805, 697.981150402152, 700.2056096007742, 683.7196586601685, 692.0520302920079, 701.7541062030957, 708.2515448510559, 707.5071271796163, 697.9478730467342, 686.4181880029175, 689.8950967401543, 702.0291534173799, 693.4941123547509, 693.3054906431064, 680.3778148751111, 684.4743990136157, 694.8577057868024, 695.7352820006938, 694.3057802815769, 678.0465391001363, 681.0849869013726, 309.2278078793018, 693.3297519858017, 694.8750689077447, 693.6868655154356, 671.1617337075616, 684.0119997786755, 303.0272783099165, 699.8471631098222, 701.2641387546108, 698.2842138295762, 682.4863182073501, 675.6809586384204, 636.1476347717538, 633.944610353644, 611.5803045156367, 628.0677908089867, 627.9877407194139, 626.7776431580211, 626.8401590756139, 621.5081466767087, 626.7643022438291, 622.7633185573075, 624.5005912125392, 623.5510948792324, 619.3587777586082, 621.7744659368552, 622.3267715423968, 622.1655837597922, 623.5203389539909, 623.3067753963875, 627.9761021427804, 646.1805535155593, 649.7393486763488, 650.5969641759241, 645.5719060599297, 645.3600981878456, 646.7295766811314, 641.9217366219364, 648.0984610147323, 649.4854531988492, 647.2120863129877, 650.9802483098937, 653.1091673264401, 653.7335038830087, 651.1445335074295, 652.7293328774733, 652.9702906416769, 653.5514009273094, 650.7710410083778, 650.3996490785092, 652.2463433892166, 650.3069431788332, 652.1825277732381, 653.1729839997765, 650.0617440700507, 651.6083532346752, 651.691982003311, 649.3071011740101, 651.5042397335425, 649.7826181216336, 649.3956487596015, 652.1129354485485, 650.6176808091448, 652.9046703091572, 653.9939472679753, 654.5613173936587, 652.4798380365633, 655.7079581169121, 662.3068478911689, 658.7654627421006, 654.4244935659117, 651.8089255263491, 652.5000478649271, 654.682305681867, 651.6802384672179, 652.877032597582, 652.84742830559, 651.5381930387355, 652.7707557690503, 653.0898535503278, 650.341267437337, 649.1446475131894, 649.7766347695856, 651.7545677583179, 619.4321206069483, 655.6698359959622, 654.7652360943063, 655.1422598671305, 653.476311108405, 653.4376408373515, 654.5090147747075, 655.2779102837611, 657.0330916681824, 653.6480047873677, 657.8247348578544, 656.0375476797518, 653.1623712206332, 653.681715136143, 656.1387116824535, 652.3786414213872, 655.4093182944404, 653.9764084588838, 652.8024119370883, 650.2450517980775, 653.4241725615858, 653.9128505476224, 652.0327246760545, 651.912695956026, 651.0155750030852, 650.5819147296025, 649.9925519795429, 650.9507188000277, 649.4169802003332, 642.7321611402779, 645.1828414494939, 616.6734546570818]
Elapsed: 0.07515808465791904~0.010148913190631242
Time per graph: 0.001543797412488754~0.00020619906928916225
Speed: 653.7143329579084~46.17682823289151
Total Time: 0.0784
best val loss: 0.25286391377449036 test_score: 0.8958

Testing...
Test loss: 0.2043 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.2903297378215939, 0.2724794130772352, 0.2857345568481833, 0.37884562998078763, 0.2603118787519634, 0.26445262786000967, 0.2689348461572081, 0.27441333001479506, 0.35422517475672066, 0.2601067419163883, 0.2600257107988, 0.2636140719987452, 0.26792451716028154, 0.2661338117904961, 0.3512656937818974, 0.25968759204261005, 0.2605229739565402, 0.2672611989546567, 0.2668922198936343, 0.35864680213853717, 0.25973453070037067, 0.25881675095297396, 0.26165567291900516, 0.271086088148877, 0.2676614220254123, 0.35594405396841466, 0.2673302448820323, 0.26887950114905834, 0.27147994097322226, 0.2828991829883307, 0.2916812147013843, 0.2631109480280429, 0.26332018803805113, 0.26531066792085767, 0.2851134098600596, 0.37016159179620445, 0.26323096500709653, 0.2617155851330608, 0.27971915481612086, 0.28604077687487006, 0.28398579615168273, 0.3711271609645337, 0.26208157511428, 0.2603619499132037, 0.26177747710607946, 0.2659403670113534, 0.26865623309277, 0.29451163578778505, 0.2863802940119058, 0.29176111170090735, 0.2978410960640758, 0.2925001201219857, 0.2934917928650975, 0.29319295194000006, 0.29471452604047954, 0.29340406227856874, 0.2963820556178689, 0.2947969469241798, 0.29608828108757734, 0.2953812840860337, 0.2971685591619462, 0.28401938802562654, 0.2948705740272999, 0.294560540933162, 0.29484850401058793, 0.2937284605577588, 0.2912608340848237, 0.2867832942865789, 0.28806957323104143, 0.28804590879008174, 0.28762283199466765, 0.28834651526995003, 0.2843555563595146, 0.28572112903930247, 0.2864133499097079, 0.2834974767174572, 0.28766610007733107, 0.2853085140231997, 0.28588874195702374, 0.28751304815523326, 0.2888744599185884, 0.28742541931569576, 0.28503147792071104, 0.2855210618581623, 0.28740216535516083, 0.2849180439952761, 0.28760783304460347, 0.2720713303424418, 0.29362551239319146, 0.2669460428878665, 0.2660779261495918, 0.268961995607242, 0.26605976466089487, 0.2706018779426813, 0.2663670089095831, 0.26623937487602234, 0.2805013719480485, 0.285395345184952, 0.2848436329513788, 0.2849322040565312, 0.284429999999702, 0.28610587283037603, 0.2852968748193234, 0.27299629896879196, 0.27238110988400877, 0.2738990990910679, 0.2926476248539984, 0.275608945870772, 0.2762172322254628, 0.288541940972209, 0.2793359011411667, 0.2761095156893134, 0.27675021323375404, 0.27752199536189437, 0.2923143121879548, 0.2948202909901738, 0.2957921838387847, 0.2981826257891953, 0.2983425920829177, 0.30282775894738734, 0.30401062685996294, 0.29753371491096914, 0.29748459230177104, 0.2971509660128504, 0.2980257640592754, 0.29746673512272537, 0.2976323172915727, 0.29727943474426866, 0.2966725642327219, 0.2978112339042127, 0.2978922517504543, 0.29893748299218714, 0.3007445000112057, 0.29954727506265044, 0.2992376780603081, 0.2971898000687361, 0.29747793124988675, 0.2985286549665034, 0.2982347190845758, 0.29878375004045665, 0.2974301890935749, 0.29766761185601354, 0.277694026241079, 0.27689772518351674, 0.27648039697669446, 0.2979143401607871, 0.29683746001683176, 0.29675024608150125, 0.300449812784791, 0.29798338911496103, 0.30246151285246015]
Total Epoch List: [47, 56, 48]
Total Time List: [0.07289576204493642, 0.0753304110839963, 0.07844658102840185]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d6845e0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.6992;  Loss pred: 0.6858; Loss self: 1.3452; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6956 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7000;  Loss pred: 0.6871; Loss self: 1.2940; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5102 time: 0.07s
Epoch 3/1000, LR 0.000050
Train loss: 0.6794;  Loss pred: 0.6672; Loss self: 1.2226; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6941 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6484;  Loss pred: 0.6365; Loss self: 1.1948; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6094;  Loss pred: 0.5973; Loss self: 1.2114; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5804;  Loss pred: 0.5683; Loss self: 1.2154; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5102 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5265;  Loss pred: 0.5139; Loss self: 1.2569; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5102 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.4720;  Loss pred: 0.4585; Loss self: 1.3444; time: 0.12s
Val loss: 0.6914 score: 0.9184 time: 0.07s
Test loss: 0.6918 score: 0.8776 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4186;  Loss pred: 0.4045; Loss self: 1.4044; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6905 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6913 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3561;  Loss pred: 0.3404; Loss self: 1.5648; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6891 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6906 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.3000;  Loss pred: 0.2837; Loss self: 1.6270; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6872 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6895 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2569;  Loss pred: 0.2393; Loss self: 1.7572; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6840 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6874 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2269;  Loss pred: 0.2081; Loss self: 1.8835; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6796 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6844 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.1907;  Loss pred: 0.1712; Loss self: 1.9477; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6732 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6798 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1668;  Loss pred: 0.1462; Loss self: 2.0553; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6639 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6730 score: 0.4898 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1467;  Loss pred: 0.1255; Loss self: 2.1222; time: 0.12s
Val loss: 0.6502 score: 0.5510 time: 0.07s
Test loss: 0.6625 score: 0.5306 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1336;  Loss pred: 0.1118; Loss self: 2.1824; time: 0.12s
Val loss: 0.6309 score: 0.7347 time: 0.07s
Test loss: 0.6475 score: 0.6531 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1170;  Loss pred: 0.0950; Loss self: 2.1973; time: 0.13s
Val loss: 0.6036 score: 0.9184 time: 0.07s
Test loss: 0.6261 score: 0.7551 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1040;  Loss pred: 0.0811; Loss self: 2.2941; time: 0.12s
Val loss: 0.5675 score: 0.9796 time: 0.07s
Test loss: 0.5977 score: 0.8776 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0934;  Loss pred: 0.0700; Loss self: 2.3463; time: 0.13s
Val loss: 0.5220 score: 0.9796 time: 0.09s
Test loss: 0.5610 score: 0.8980 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0838;  Loss pred: 0.0596; Loss self: 2.4192; time: 0.12s
Val loss: 0.4658 score: 0.9796 time: 0.07s
Test loss: 0.5156 score: 0.9388 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0758;  Loss pred: 0.0509; Loss self: 2.4906; time: 0.12s
Val loss: 0.4017 score: 0.9796 time: 0.07s
Test loss: 0.4640 score: 0.9184 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0679;  Loss pred: 0.0430; Loss self: 2.4885; time: 0.12s
Val loss: 0.3331 score: 1.0000 time: 0.07s
Test loss: 0.4095 score: 0.9184 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0625;  Loss pred: 0.0371; Loss self: 2.5393; time: 0.12s
Val loss: 0.2674 score: 1.0000 time: 0.07s
Test loss: 0.3580 score: 0.9184 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0555;  Loss pred: 0.0297; Loss self: 2.5797; time: 0.12s
Val loss: 0.2102 score: 1.0000 time: 0.07s
Test loss: 0.3150 score: 0.9184 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0497;  Loss pred: 0.0236; Loss self: 2.6056; time: 0.13s
Val loss: 0.1647 score: 1.0000 time: 0.10s
Test loss: 0.2821 score: 0.9184 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0457;  Loss pred: 0.0194; Loss self: 2.6319; time: 0.12s
Val loss: 0.1299 score: 1.0000 time: 0.07s
Test loss: 0.2603 score: 0.9184 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0431;  Loss pred: 0.0164; Loss self: 2.6758; time: 0.13s
Val loss: 0.1041 score: 1.0000 time: 0.07s
Test loss: 0.2486 score: 0.9184 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0399;  Loss pred: 0.0131; Loss self: 2.6846; time: 0.13s
Val loss: 0.0854 score: 1.0000 time: 0.07s
Test loss: 0.2442 score: 0.9184 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0384;  Loss pred: 0.0112; Loss self: 2.7212; time: 0.12s
Val loss: 0.0726 score: 1.0000 time: 0.07s
Test loss: 0.2452 score: 0.9184 time: 0.07s
Epoch 31/1000, LR 0.000450
Train loss: 0.0367;  Loss pred: 0.0095; Loss self: 2.7236; time: 0.12s
Val loss: 0.0634 score: 1.0000 time: 0.07s
Test loss: 0.2494 score: 0.8980 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0363;  Loss pred: 0.0091; Loss self: 2.7189; time: 0.13s
Val loss: 0.0571 score: 1.0000 time: 0.09s
Test loss: 0.2552 score: 0.8980 time: 0.07s
Epoch 33/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0069; Loss self: 2.7480; time: 0.12s
Val loss: 0.0518 score: 1.0000 time: 0.07s
Test loss: 0.2624 score: 0.8980 time: 0.07s
Epoch 34/1000, LR 0.000449
Train loss: 0.0333;  Loss pred: 0.0060; Loss self: 2.7297; time: 0.12s
Val loss: 0.0484 score: 1.0000 time: 0.07s
Test loss: 0.2683 score: 0.8980 time: 0.07s
Epoch 35/1000, LR 0.000449
Train loss: 0.0322;  Loss pred: 0.0050; Loss self: 2.7264; time: 0.12s
Val loss: 0.0456 score: 1.0000 time: 0.07s
Test loss: 0.2739 score: 0.8980 time: 0.07s
Epoch 36/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0044; Loss self: 2.7291; time: 0.12s
Val loss: 0.0436 score: 1.0000 time: 0.07s
Test loss: 0.2790 score: 0.8980 time: 0.07s
Epoch 37/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0037; Loss self: 2.7176; time: 0.12s
Val loss: 0.0420 score: 1.0000 time: 0.07s
Test loss: 0.2832 score: 0.8980 time: 0.07s
Epoch 38/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0031; Loss self: 2.7121; time: 0.13s
Val loss: 0.0409 score: 1.0000 time: 0.09s
Test loss: 0.2869 score: 0.8980 time: 0.07s
Epoch 39/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0031; Loss self: 2.7100; time: 0.13s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2900 score: 0.8980 time: 0.07s
Epoch 40/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0027; Loss self: 2.7151; time: 0.14s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2921 score: 0.8980 time: 0.07s
Epoch 41/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0025; Loss self: 2.6804; time: 0.13s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2937 score: 0.8980 time: 0.07s
Epoch 42/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0022; Loss self: 2.6733; time: 0.14s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2949 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0287;  Loss pred: 0.0019; Loss self: 2.6745; time: 0.14s
Val loss: 0.0398 score: 1.0000 time: 0.07s
Test loss: 0.2961 score: 0.9184 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0284;  Loss pred: 0.0019; Loss self: 2.6489; time: 0.13s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2972 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0279;  Loss pred: 0.0016; Loss self: 2.6369; time: 0.14s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2977 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0279;  Loss pred: 0.0015; Loss self: 2.6335; time: 0.13s
Val loss: 0.0403 score: 0.9796 time: 0.07s
Test loss: 0.2986 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0276;  Loss pred: 0.0016; Loss self: 2.6051; time: 0.12s
Val loss: 0.0406 score: 0.9796 time: 0.07s
Test loss: 0.2989 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0274;  Loss pred: 0.0014; Loss self: 2.5955; time: 0.14s
Val loss: 0.0410 score: 0.9796 time: 0.07s
Test loss: 0.2994 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0273;  Loss pred: 0.0014; Loss self: 2.5934; time: 0.21s
Val loss: 0.0414 score: 0.9796 time: 0.07s
Test loss: 0.2996 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0268;  Loss pred: 0.0013; Loss self: 2.5493; time: 0.13s
Val loss: 0.0419 score: 0.9796 time: 0.07s
Test loss: 0.2993 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0267;  Loss pred: 0.0013; Loss self: 2.5363; time: 0.14s
Val loss: 0.0423 score: 0.9796 time: 0.07s
Test loss: 0.2994 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0012; Loss self: 2.5223; time: 0.14s
Val loss: 0.0428 score: 0.9796 time: 0.07s
Test loss: 0.2994 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0012; Loss self: 2.5229; time: 0.12s
Val loss: 0.0433 score: 0.9796 time: 0.07s
Test loss: 0.2996 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0013; Loss self: 2.5053; time: 0.14s
Val loss: 0.0437 score: 0.9796 time: 0.16s
Test loss: 0.2990 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0262;  Loss pred: 0.0012; Loss self: 2.4967; time: 0.12s
Val loss: 0.0442 score: 0.9796 time: 0.07s
Test loss: 0.2986 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0256;  Loss pred: 0.0013; Loss self: 2.4380; time: 0.12s
Val loss: 0.0447 score: 0.9796 time: 0.07s
Test loss: 0.2986 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0255;  Loss pred: 0.0011; Loss self: 2.4309; time: 0.12s
Val loss: 0.0452 score: 0.9796 time: 0.07s
Test loss: 0.2982 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 58/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0012; Loss self: 2.4453; time: 0.13s
Val loss: 0.0456 score: 0.9796 time: 0.07s
Test loss: 0.2981 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 59/1000, LR 0.000447
Train loss: 0.0252;  Loss pred: 0.0013; Loss self: 2.3952; time: 0.13s
Val loss: 0.0461 score: 0.9796 time: 0.07s
Test loss: 0.2974 score: 0.9184 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 60/1000, LR 0.000447
Train loss: 0.0250;  Loss pred: 0.0013; Loss self: 2.3687; time: 0.12s
Val loss: 0.0466 score: 0.9796 time: 0.07s
Test loss: 0.2972 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 61/1000, LR 0.000447
Train loss: 0.0249;  Loss pred: 0.0012; Loss self: 2.3680; time: 0.14s
Val loss: 0.0471 score: 0.9796 time: 0.07s
Test loss: 0.2971 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 040,   Train_Loss: 0.0293,   Val_Loss: 0.0396,   Val_Precision: 1.0000,   Val_Recall: 1.0000,   Val_accuracy: 1.0000,   Val_Score: 1.0000,   Val_Loss: 0.0396,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.2937


[0.07653751294128597, 0.07697066408582032, 0.07702512200921774, 0.07709258189424872, 0.07686479180119932, 0.07729266188107431, 0.07698917505331337, 0.07696645008400083, 0.07721949205733836, 0.07661820109933615, 0.07674231193959713, 0.07636552886106074, 0.07682616892270744, 0.07684865710325539, 0.07708050985820591, 0.07663812302052975, 0.07716256892308593, 0.07701448001898825, 0.07611878495663404, 0.07555329194292426, 0.07510562893003225, 0.07521314406767488, 0.07654601987451315, 0.07647759816609323, 0.07590521196834743, 0.07527540507726371, 0.07473676884546876, 0.07537511107511818, 0.07516981707885861, 0.07652934407815337, 0.07532217120751739, 0.07537160301581025, 0.07467137998901308, 0.07525896886363626, 0.0770738699939102, 0.07597143482416868, 0.07613689010031521, 0.07497339020483196, 0.07449777913279831, 0.07515608193352818, 0.07728854799643159, 0.07587432605214417, 0.16691234102472663, 0.07452917820774019, 0.07388782408088446, 0.07484774687327445, 0.07702478603459895, 0.07674724003300071, 0.07515149493701756, 0.07566750096157193, 0.07679657801054418, 0.07718751206994057, 0.07608358794823289, 0.07558295712806284, 0.0748729130718857, 0.07550211087800562, 0.07866709888912737, 0.07624876382760704, 0.16801708890125155, 0.07542437990196049, 0.0756057808175683]
[0.0015619900600262443, 0.0015708298793024554, 0.0015719412654942395, 0.0015733179978418108, 0.0015686692204326391, 0.0015774012628790674, 0.0015712076541492526, 0.001570743879265323, 0.0015759080011701705, 0.001563636757129309, 0.0015661696314203497, 0.0015584801808379743, 0.0015678809984226007, 0.0015683399408827632, 0.0015730716297593043, 0.0015640433269495867, 0.001574746304552774, 0.0015717240820201685, 0.0015534445909517153, 0.001541903917202536, 0.001532767937347597, 0.0015349621238300996, 0.0015621636709084316, 0.0015607673095121067, 0.0015490859585377027, 0.0015362327566788513, 0.0015252401805197705, 0.0015382675729615955, 0.0015340778995685431, 0.0015618233485337422, 0.0015371871675003547, 0.0015381959799144948, 0.0015239057140614912, 0.0015358973237476787, 0.0015729361223246977, 0.0015504374453911977, 0.0015538140836799024, 0.0015300691878537135, 0.0015203628394448636, 0.001533797590480167, 0.0015773173060496242, 0.001548455633717228, 0.003406374306627074, 0.0015210036368926568, 0.0015079147771609072, 0.0015275050382300907, 0.0015719344088693662, 0.0015662702047551165, 0.0015337039783064807, 0.001544234713501468, 0.0015672771022560037, 0.001575255348366134, 0.001552726284657814, 0.0015425093291441397, 0.0015280186341201164, 0.0015408594056735842, 0.0016054509977372934, 0.0015560972209715722, 0.003428920181658195, 0.0015392730592236835, 0.0015429751187258838]
[640.20893960311, 636.6061743389176, 636.156084168696, 635.5994156119385, 637.4830250855562, 633.9541013012779, 636.4531113116686, 636.6410292604327, 634.5548085658951, 639.5347227804407, 638.5004407811855, 641.6507648254552, 637.8035074129164, 637.6168673209555, 635.6989606080493, 639.3684770551325, 635.0229221741205, 636.2439892851168, 643.7307167726863, 648.548842014937, 652.4144820842653, 651.4818733798849, 640.137790055301, 640.7104979105427, 645.5419691131758, 650.9430264733312, 655.6344454938372, 650.0819607571393, 651.8573797857647, 640.2772765170987, 650.5388680977062, 650.1122178563929, 656.20857693014, 651.0851894447878, 635.7537256643739, 644.9792624478865, 643.5776393734939, 653.5652164871957, 657.7377281630576, 651.9765099428422, 633.9878451625502, 645.804747792093, 293.5672682988795, 657.4606238568606, 663.1674516001453, 654.6623251460387, 636.1588590196093, 638.4594413939888, 652.0163044137123, 647.5699524540247, 638.0492629928418, 634.8177144976572, 644.028512868498, 648.2942962522303, 654.4422807879126, 648.9884776754512, 622.8779336207646, 642.6333692541622, 291.63700145286236, 649.6573132413165, 648.0985907444528]
Elapsed: 0.0790920402381386~0.016297541004279114
Time per graph: 0.0016141232701660936~0.00033260287763834924
Speed: 632.4979034226683~63.073130428043335
Total Time: 0.0760
best val loss: 0.03955620154738426 test_score: 0.8980

Testing...
Test loss: 0.4095 score: 0.9184 time: 0.07s
test Score 0.9184
Epoch Time List: [0.26782772480510175, 0.28646805603057146, 0.26702319900505245, 0.26614724891260266, 0.2669380821753293, 0.2678878151345998, 0.2674695572350174, 0.2677551850210875, 0.26658264407888055, 0.26678583095781505, 0.2826147477608174, 0.2673338009044528, 0.2670375183224678, 0.2674575010314584, 0.26769756176508963, 0.2672335540410131, 0.2680296800099313, 0.2750725301448256, 0.26398561685346067, 0.29652356496080756, 0.2608420879114419, 0.2601204758975655, 0.26237693009898067, 0.2674788611475378, 0.26424697902984917, 0.2977700000628829, 0.2589214858599007, 0.273525019409135, 0.2768610189668834, 0.2693258738145232, 0.26532938005402684, 0.2963942722417414, 0.25810846500098705, 0.259263392072171, 0.26317284000106156, 0.26818542601540685, 0.26685132388956845, 0.2896444520447403, 0.2725651101209223, 0.2775194710120559, 0.27756143314763904, 0.28237523790448904, 0.3722560121677816, 0.2660499468911439, 0.27458157297223806, 0.2746767569333315, 0.2672580659855157, 0.2826385307125747, 0.35563680808991194, 0.2687094109132886, 0.28164872503839433, 0.28382601775228977, 0.26830789423547685, 0.36555219790898263, 0.25965318991802633, 0.26067184400744736, 0.26516942121088505, 0.2781940447166562, 0.36307374807074666, 0.25849485117942095, 0.2832915158942342]
Total Epoch List: [61]
Total Time List: [0.07597679691389203]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d8af1c0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7110;  Loss pred: 0.6987; Loss self: 1.2284; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7013;  Loss pred: 0.6884; Loss self: 1.2939; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000050
Train loss: 0.6882;  Loss pred: 0.6753; Loss self: 1.2883; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6631;  Loss pred: 0.6502; Loss self: 1.2872; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6210;  Loss pred: 0.6080; Loss self: 1.2993; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5667;  Loss pred: 0.5532; Loss self: 1.3439; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5223;  Loss pred: 0.5093; Loss self: 1.3030; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.4627;  Loss pred: 0.4488; Loss self: 1.3909; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4082;  Loss pred: 0.3934; Loss self: 1.4853; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6909 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3470;  Loss pred: 0.3299; Loss self: 1.7044; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6898 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6907 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.2955;  Loss pred: 0.2775; Loss self: 1.8037; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6880 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6888 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2563;  Loss pred: 0.2369; Loss self: 1.9416; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6854 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6861 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2141;  Loss pred: 0.1939; Loss self: 2.0191; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6815 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6818 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.2001;  Loss pred: 0.1790; Loss self: 2.1156; time: 0.13s
Val loss: 0.6760 score: 0.5306 time: 0.07s
Test loss: 0.6757 score: 0.5306 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1730;  Loss pred: 0.1511; Loss self: 2.1887; time: 0.13s
Val loss: 0.6679 score: 0.5306 time: 0.07s
Test loss: 0.6668 score: 0.5306 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1533;  Loss pred: 0.1314; Loss self: 2.1855; time: 0.13s
Val loss: 0.6568 score: 0.5510 time: 0.07s
Test loss: 0.6541 score: 0.5510 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1339;  Loss pred: 0.1110; Loss self: 2.2874; time: 0.13s
Val loss: 0.6417 score: 0.6122 time: 0.07s
Test loss: 0.6365 score: 0.5918 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1177;  Loss pred: 0.0949; Loss self: 2.2812; time: 0.14s
Val loss: 0.6224 score: 0.6939 time: 0.07s
Test loss: 0.6137 score: 0.6735 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1034;  Loss pred: 0.0803; Loss self: 2.3096; time: 0.13s
Val loss: 0.5976 score: 0.6939 time: 0.07s
Test loss: 0.5833 score: 0.6939 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0911;  Loss pred: 0.0675; Loss self: 2.3638; time: 0.13s
Val loss: 0.5667 score: 0.7755 time: 0.07s
Test loss: 0.5444 score: 0.8163 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0840;  Loss pred: 0.0599; Loss self: 2.4073; time: 0.13s
Val loss: 0.5267 score: 0.7959 time: 0.07s
Test loss: 0.4943 score: 0.8980 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0755;  Loss pred: 0.0510; Loss self: 2.4480; time: 0.13s
Val loss: 0.4790 score: 0.7959 time: 0.07s
Test loss: 0.4336 score: 0.9184 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0668;  Loss pred: 0.0423; Loss self: 2.4461; time: 0.13s
Val loss: 0.4288 score: 0.8163 time: 0.07s
Test loss: 0.3681 score: 0.9184 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0622;  Loss pred: 0.0375; Loss self: 2.4749; time: 0.13s
Val loss: 0.3787 score: 0.8367 time: 0.07s
Test loss: 0.3024 score: 0.9388 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0561;  Loss pred: 0.0305; Loss self: 2.5589; time: 0.13s
Val loss: 0.3329 score: 0.9184 time: 0.07s
Test loss: 0.2415 score: 0.9388 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0500;  Loss pred: 0.0247; Loss self: 2.5289; time: 0.13s
Val loss: 0.2952 score: 0.9184 time: 0.07s
Test loss: 0.1899 score: 0.9592 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0470;  Loss pred: 0.0212; Loss self: 2.5809; time: 0.13s
Val loss: 0.2669 score: 0.9184 time: 0.07s
Test loss: 0.1478 score: 0.9592 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0438;  Loss pred: 0.0179; Loss self: 2.5948; time: 0.13s
Val loss: 0.2488 score: 0.9388 time: 0.07s
Test loss: 0.1167 score: 0.9592 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0413;  Loss pred: 0.0148; Loss self: 2.6484; time: 0.13s
Val loss: 0.2383 score: 0.9388 time: 0.07s
Test loss: 0.0930 score: 0.9592 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0397;  Loss pred: 0.0133; Loss self: 2.6399; time: 0.13s
Val loss: 0.2333 score: 0.9592 time: 0.07s
Test loss: 0.0753 score: 0.9796 time: 0.07s
Epoch 31/1000, LR 0.000450
Train loss: 0.0374;  Loss pred: 0.0108; Loss self: 2.6609; time: 0.13s
Val loss: 0.2332 score: 0.9592 time: 0.07s
Test loss: 0.0627 score: 0.9796 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0360;  Loss pred: 0.0092; Loss self: 2.6829; time: 0.13s
Val loss: 0.2366 score: 0.9592 time: 0.07s
Test loss: 0.0528 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0342;  Loss pred: 0.0075; Loss self: 2.6697; time: 0.15s
Val loss: 0.2420 score: 0.9592 time: 0.07s
Test loss: 0.0453 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0331;  Loss pred: 0.0063; Loss self: 2.6775; time: 0.15s
Val loss: 0.2491 score: 0.9388 time: 0.07s
Test loss: 0.0394 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0057; Loss self: 2.6835; time: 0.15s
Val loss: 0.2567 score: 0.9388 time: 0.07s
Test loss: 0.0350 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0314;  Loss pred: 0.0048; Loss self: 2.6600; time: 0.15s
Val loss: 0.2635 score: 0.9388 time: 0.07s
Test loss: 0.0318 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0313;  Loss pred: 0.0044; Loss self: 2.6905; time: 0.15s
Val loss: 0.2694 score: 0.9388 time: 0.07s
Test loss: 0.0295 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0305;  Loss pred: 0.0040; Loss self: 2.6524; time: 0.13s
Val loss: 0.2752 score: 0.9388 time: 0.07s
Test loss: 0.0278 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0036; Loss self: 2.6554; time: 0.14s
Val loss: 0.2800 score: 0.9388 time: 0.07s
Test loss: 0.0265 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0297;  Loss pred: 0.0033; Loss self: 2.6384; time: 0.13s
Val loss: 0.2843 score: 0.9592 time: 0.07s
Test loss: 0.0255 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0033; Loss self: 2.6327; time: 0.13s
Val loss: 0.2891 score: 0.9592 time: 0.07s
Test loss: 0.0247 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0288;  Loss pred: 0.0027; Loss self: 2.6054; time: 0.13s
Val loss: 0.2935 score: 0.9592 time: 0.07s
Test loss: 0.0240 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0285;  Loss pred: 0.0025; Loss self: 2.5995; time: 0.14s
Val loss: 0.2967 score: 0.9592 time: 0.07s
Test loss: 0.0237 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0023; Loss self: 2.6249; time: 0.13s
Val loss: 0.2978 score: 0.9592 time: 0.07s
Test loss: 0.0238 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0282;  Loss pred: 0.0023; Loss self: 2.5861; time: 0.13s
Val loss: 0.2989 score: 0.9388 time: 0.07s
Test loss: 0.0240 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0022; Loss self: 2.6076; time: 0.13s
Val loss: 0.2993 score: 0.9388 time: 0.07s
Test loss: 0.0241 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0276;  Loss pred: 0.0019; Loss self: 2.5659; time: 0.13s
Val loss: 0.3004 score: 0.9388 time: 0.07s
Test loss: 0.0240 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0273;  Loss pred: 0.0018; Loss self: 2.5529; time: 0.13s
Val loss: 0.3000 score: 0.9388 time: 0.07s
Test loss: 0.0242 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0271;  Loss pred: 0.0018; Loss self: 2.5316; time: 0.13s
Val loss: 0.3005 score: 0.9388 time: 0.07s
Test loss: 0.0242 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0271;  Loss pred: 0.0018; Loss self: 2.5247; time: 0.15s
Val loss: 0.3010 score: 0.9388 time: 0.07s
Test loss: 0.0242 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0015; Loss self: 2.4968; time: 0.15s
Val loss: 0.3020 score: 0.9388 time: 0.07s
Test loss: 0.0241 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.0374,   Val_Loss: 0.2332,   Val_Precision: 0.9600,   Val_Recall: 0.9600,   Val_accuracy: 0.9600,   Val_Score: 0.9592,   Val_Loss: 0.2332,   Test_Precision: 0.9600,   Test_Recall: 1.0000,   Test_accuracy: 0.9796,   Test_Score: 0.9796,   Test_loss: 0.0627


[0.07653751294128597, 0.07697066408582032, 0.07702512200921774, 0.07709258189424872, 0.07686479180119932, 0.07729266188107431, 0.07698917505331337, 0.07696645008400083, 0.07721949205733836, 0.07661820109933615, 0.07674231193959713, 0.07636552886106074, 0.07682616892270744, 0.07684865710325539, 0.07708050985820591, 0.07663812302052975, 0.07716256892308593, 0.07701448001898825, 0.07611878495663404, 0.07555329194292426, 0.07510562893003225, 0.07521314406767488, 0.07654601987451315, 0.07647759816609323, 0.07590521196834743, 0.07527540507726371, 0.07473676884546876, 0.07537511107511818, 0.07516981707885861, 0.07652934407815337, 0.07532217120751739, 0.07537160301581025, 0.07467137998901308, 0.07525896886363626, 0.0770738699939102, 0.07597143482416868, 0.07613689010031521, 0.07497339020483196, 0.07449777913279831, 0.07515608193352818, 0.07728854799643159, 0.07587432605214417, 0.16691234102472663, 0.07452917820774019, 0.07388782408088446, 0.07484774687327445, 0.07702478603459895, 0.07674724003300071, 0.07515149493701756, 0.07566750096157193, 0.07679657801054418, 0.07718751206994057, 0.07608358794823289, 0.07558295712806284, 0.0748729130718857, 0.07550211087800562, 0.07866709888912737, 0.07624876382760704, 0.16801708890125155, 0.07542437990196049, 0.0756057808175683, 0.07500237599015236, 0.0750873691868037, 0.07509441091679037, 0.07488051312975585, 0.0758380291517824, 0.07720147795043886, 0.07641500094905496, 0.07447450794279575, 0.07457897905260324, 0.07476979796774685, 0.07572567998431623, 0.0760462461039424, 0.07647050893865526, 0.07610485097393394, 0.07637277920730412, 0.07622583792544901, 0.07621224201284349, 0.0765900039114058, 0.07640910614281893, 0.07673933682963252, 0.07680922490544617, 0.07661751401610672, 0.07647622190415859, 0.07661028299480677, 0.07694655586965382, 0.07605505594983697, 0.0763991167768836, 0.07600916991941631, 0.07649313914589584, 0.0763007199857384, 0.07608069805428386, 0.07666899403557181, 0.07643439993262291, 0.07651047315448523, 0.07679987489245832, 0.07650065491907299, 0.0852354250382632, 0.07644646405242383, 0.07643454009667039, 0.0763576659373939, 0.07589109078980982, 0.0766772881615907, 0.07624562014825642, 0.0766100031323731, 0.07662449195049703, 0.07668841700069606, 0.0765310728456825, 0.07712392997927964, 0.07713195704855025, 0.07668489497154951, 0.07657111785374582]
[0.0015619900600262443, 0.0015708298793024554, 0.0015719412654942395, 0.0015733179978418108, 0.0015686692204326391, 0.0015774012628790674, 0.0015712076541492526, 0.001570743879265323, 0.0015759080011701705, 0.001563636757129309, 0.0015661696314203497, 0.0015584801808379743, 0.0015678809984226007, 0.0015683399408827632, 0.0015730716297593043, 0.0015640433269495867, 0.001574746304552774, 0.0015717240820201685, 0.0015534445909517153, 0.001541903917202536, 0.001532767937347597, 0.0015349621238300996, 0.0015621636709084316, 0.0015607673095121067, 0.0015490859585377027, 0.0015362327566788513, 0.0015252401805197705, 0.0015382675729615955, 0.0015340778995685431, 0.0015618233485337422, 0.0015371871675003547, 0.0015381959799144948, 0.0015239057140614912, 0.0015358973237476787, 0.0015729361223246977, 0.0015504374453911977, 0.0015538140836799024, 0.0015300691878537135, 0.0015203628394448636, 0.001533797590480167, 0.0015773173060496242, 0.001548455633717228, 0.003406374306627074, 0.0015210036368926568, 0.0015079147771609072, 0.0015275050382300907, 0.0015719344088693662, 0.0015662702047551165, 0.0015337039783064807, 0.001544234713501468, 0.0015672771022560037, 0.001575255348366134, 0.001552726284657814, 0.0015425093291441397, 0.0015280186341201164, 0.0015408594056735842, 0.0016054509977372934, 0.0015560972209715722, 0.003428920181658195, 0.0015392730592236835, 0.0015429751187258838, 0.0015306607344929054, 0.001532395289526606, 0.0015325389983018441, 0.0015281737373419562, 0.0015477148806486204, 0.001575540366335487, 0.001559489815286836, 0.0015198879171999134, 0.0015220199806653724, 0.0015259142442397317, 0.0015454220404962497, 0.001551964206202906, 0.0015606226314011278, 0.0015531602239578354, 0.0015586281470878391, 0.001555629345417327, 0.0015553518778131325, 0.0015630613043144041, 0.0015593695131187536, 0.0015661089148904597, 0.0015675352021519626, 0.0015636227350225862, 0.0015607392225338488, 0.0015634751631593217, 0.0015703378748908943, 0.0015521439989762647, 0.0015591656485078285, 0.001551207549375843, 0.0015610844723652213, 0.0015571575507293551, 0.0015526673072302829, 0.001564673347664731, 0.0015598857129106717, 0.0015614382276425557, 0.0015673443855603738, 0.0015612378554912855, 0.0017394984701686368, 0.0015601319194372212, 0.0015598885734014365, 0.0015583197130080387, 0.0015487977712206086, 0.0015648426155426673, 0.0015560330642501311, 0.0015634694516810837, 0.0015637651418468782, 0.0015650697347080829, 0.0015618586295037245, 0.0015739577546791763, 0.0015741215724193929, 0.0015649978565622348, 0.0015626758745662412]
[640.20893960311, 636.6061743389176, 636.156084168696, 635.5994156119385, 637.4830250855562, 633.9541013012779, 636.4531113116686, 636.6410292604327, 634.5548085658951, 639.5347227804407, 638.5004407811855, 641.6507648254552, 637.8035074129164, 637.6168673209555, 635.6989606080493, 639.3684770551325, 635.0229221741205, 636.2439892851168, 643.7307167726863, 648.548842014937, 652.4144820842653, 651.4818733798849, 640.137790055301, 640.7104979105427, 645.5419691131758, 650.9430264733312, 655.6344454938372, 650.0819607571393, 651.8573797857647, 640.2772765170987, 650.5388680977062, 650.1122178563929, 656.20857693014, 651.0851894447878, 635.7537256643739, 644.9792624478865, 643.5776393734939, 653.5652164871957, 657.7377281630576, 651.9765099428422, 633.9878451625502, 645.804747792093, 293.5672682988795, 657.4606238568606, 663.1674516001453, 654.6623251460387, 636.1588590196093, 638.4594413939888, 652.0163044137123, 647.5699524540247, 638.0492629928418, 634.8177144976572, 644.028512868498, 648.2942962522303, 654.4422807879126, 648.9884776754512, 622.8779336207646, 642.6333692541622, 291.63700145286236, 649.6573132413165, 648.0985907444528, 653.312636474791, 652.5731362101251, 652.5119433228564, 654.3758576425738, 646.113836923838, 634.7028748783359, 641.2353515858458, 657.9432527118829, 657.0215980757598, 655.34482280047, 647.0724331580586, 644.3447574391149, 640.7698952194481, 643.848576968932, 641.5898505800841, 642.8266495138218, 642.9413268244017, 639.7701723149137, 641.2848215815061, 638.5251948265325, 637.9442060549376, 639.5404579388871, 640.722028101855, 639.6008222985105, 636.8056301701818, 644.2701196922206, 641.3686710947179, 644.6590595838503, 640.5803258582707, 642.1957749436538, 644.0529760260391, 639.1110332980978, 641.0726066168323, 640.4351976893703, 638.0218726738025, 640.5173923260547, 574.8783440453698, 640.9714380824445, 641.0714310314078, 641.7168387542829, 645.6620861559603, 639.0419011264037, 642.6598656384662, 639.6031588111771, 639.4822171435246, 638.9491648987258, 640.2628132340932, 635.3410674632958, 635.2749479590832, 638.9785109333363, 639.9279698853574]
Elapsed: 0.07787338914515983~0.012138103918743539
Time per graph: 0.0015892528396971395~0.00024771640650497013
Speed: 636.5642948871988~47.33242503462386
Total Time: 0.0771
best val loss: 0.2331857532262802 test_score: 0.9796

Testing...
Test loss: 0.0753 score: 0.9796 time: 0.07s
test Score 0.9796
Epoch Time List: [0.26782772480510175, 0.28646805603057146, 0.26702319900505245, 0.26614724891260266, 0.2669380821753293, 0.2678878151345998, 0.2674695572350174, 0.2677551850210875, 0.26658264407888055, 0.26678583095781505, 0.2826147477608174, 0.2673338009044528, 0.2670375183224678, 0.2674575010314584, 0.26769756176508963, 0.2672335540410131, 0.2680296800099313, 0.2750725301448256, 0.26398561685346067, 0.29652356496080756, 0.2608420879114419, 0.2601204758975655, 0.26237693009898067, 0.2674788611475378, 0.26424697902984917, 0.2977700000628829, 0.2589214858599007, 0.273525019409135, 0.2768610189668834, 0.2693258738145232, 0.26532938005402684, 0.2963942722417414, 0.25810846500098705, 0.259263392072171, 0.26317284000106156, 0.26818542601540685, 0.26685132388956845, 0.2896444520447403, 0.2725651101209223, 0.2775194710120559, 0.27756143314763904, 0.28237523790448904, 0.3722560121677816, 0.2660499468911439, 0.27458157297223806, 0.2746767569333315, 0.2672580659855157, 0.2826385307125747, 0.35563680808991194, 0.2687094109132886, 0.28164872503839433, 0.28382601775228977, 0.26830789423547685, 0.36555219790898263, 0.25965318991802633, 0.26067184400744736, 0.26516942121088505, 0.2781940447166562, 0.36307374807074666, 0.25849485117942095, 0.2832915158942342, 0.2876845500431955, 0.2845229080412537, 0.2667798539623618, 0.2664239821024239, 0.2684588853735477, 0.27527849609032273, 0.272732469253242, 0.3605542059522122, 0.2666752659715712, 0.26555693708360195, 0.26810736185871065, 0.27144926809705794, 0.27187041589058936, 0.2719110418111086, 0.27246470795944333, 0.27153935423120856, 0.2729558339342475, 0.2780148561578244, 0.2729688009712845, 0.27377888816408813, 0.27334864693693817, 0.2721777558326721, 0.2721748207695782, 0.2721353331580758, 0.27298885909840465, 0.27246715989895165, 0.2724686381407082, 0.2724222356919199, 0.2732764952816069, 0.27336559095419943, 0.27252460992895067, 0.27295964816585183, 0.2907853862270713, 0.29081179411150515, 0.29376730788499117, 0.2938295758794993, 0.30039316485635936, 0.27357570780441165, 0.2801356699783355, 0.2729070133063942, 0.2718188699800521, 0.27180269197560847, 0.2765730631072074, 0.27228097314946353, 0.27276927907951176, 0.2721938348840922, 0.27273288811556995, 0.27458154782652855, 0.2735217169392854, 0.29060692992061377, 0.29136808263137937]
Total Epoch List: [61, 51]
Total Time List: [0.07597679691389203, 0.0771002268884331]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b7c0d696b30>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7006;  Loss pred: 0.6879; Loss self: 1.2696; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7044;  Loss pred: 0.6923; Loss self: 1.2142; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000050
Train loss: 0.6904;  Loss pred: 0.6775; Loss self: 1.2905; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6547;  Loss pred: 0.6426; Loss self: 1.2063; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6084;  Loss pred: 0.5962; Loss self: 1.2242; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5630;  Loss pred: 0.5510; Loss self: 1.2032; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5087;  Loss pred: 0.4959; Loss self: 1.2859; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6919 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.4581;  Loss pred: 0.4447; Loss self: 1.3385; time: 0.15s
Val loss: 0.6913 score: 0.5306 time: 0.08s
Test loss: 0.6911 score: 0.5417 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.3884;  Loss pred: 0.3736; Loss self: 1.4755; time: 0.15s
Val loss: 0.6904 score: 0.8776 time: 0.08s
Test loss: 0.6900 score: 0.8750 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3427;  Loss pred: 0.3266; Loss self: 1.6149; time: 0.15s
Val loss: 0.6887 score: 0.5714 time: 0.08s
Test loss: 0.6881 score: 0.6042 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.2946;  Loss pred: 0.2768; Loss self: 1.7865; time: 0.15s
Val loss: 0.6860 score: 0.5102 time: 0.08s
Test loss: 0.6851 score: 0.5417 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2639;  Loss pred: 0.2445; Loss self: 1.9366; time: 0.16s
Val loss: 0.6816 score: 0.5306 time: 0.08s
Test loss: 0.6804 score: 0.5417 time: 0.06s
Epoch 13/1000, LR 0.000450
Train loss: 0.2293;  Loss pred: 0.2086; Loss self: 2.0697; time: 0.13s
Val loss: 0.6755 score: 0.6122 time: 0.08s
Test loss: 0.6739 score: 0.6458 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.2066;  Loss pred: 0.1850; Loss self: 2.1595; time: 0.13s
Val loss: 0.6668 score: 0.7551 time: 0.08s
Test loss: 0.6648 score: 0.7708 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1849;  Loss pred: 0.1623; Loss self: 2.2668; time: 0.13s
Val loss: 0.6549 score: 0.8776 time: 0.08s
Test loss: 0.6522 score: 0.8333 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1585;  Loss pred: 0.1350; Loss self: 2.3466; time: 0.14s
Val loss: 0.6380 score: 0.9388 time: 0.08s
Test loss: 0.6343 score: 0.9375 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1396;  Loss pred: 0.1157; Loss self: 2.3885; time: 0.15s
Val loss: 0.6149 score: 0.9388 time: 0.08s
Test loss: 0.6103 score: 0.9583 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1231;  Loss pred: 0.0991; Loss self: 2.4008; time: 0.15s
Val loss: 0.5843 score: 0.9592 time: 0.08s
Test loss: 0.5791 score: 0.9583 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1079;  Loss pred: 0.0836; Loss self: 2.4290; time: 0.15s
Val loss: 0.5457 score: 0.9592 time: 0.08s
Test loss: 0.5395 score: 0.9583 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0961;  Loss pred: 0.0712; Loss self: 2.4861; time: 0.15s
Val loss: 0.5002 score: 0.9592 time: 0.08s
Test loss: 0.4923 score: 0.9792 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0862;  Loss pred: 0.0610; Loss self: 2.5171; time: 0.15s
Val loss: 0.4478 score: 0.9592 time: 0.08s
Test loss: 0.4385 score: 0.9792 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0786;  Loss pred: 0.0530; Loss self: 2.5548; time: 0.15s
Val loss: 0.3909 score: 0.9592 time: 0.08s
Test loss: 0.3803 score: 0.9792 time: 0.06s
Epoch 23/1000, LR 0.000450
Train loss: 0.0685;  Loss pred: 0.0421; Loss self: 2.6359; time: 0.15s
Val loss: 0.3340 score: 0.9592 time: 0.08s
Test loss: 0.3214 score: 0.9792 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0617;  Loss pred: 0.0352; Loss self: 2.6476; time: 0.16s
Val loss: 0.2824 score: 0.9592 time: 0.08s
Test loss: 0.2679 score: 0.9792 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0543;  Loss pred: 0.0276; Loss self: 2.6695; time: 0.15s
Val loss: 0.2396 score: 0.9592 time: 0.08s
Test loss: 0.2228 score: 0.9583 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0497;  Loss pred: 0.0227; Loss self: 2.6918; time: 0.15s
Val loss: 0.2076 score: 0.9592 time: 0.08s
Test loss: 0.1872 score: 0.9583 time: 0.06s
Epoch 27/1000, LR 0.000450
Train loss: 0.0457;  Loss pred: 0.0183; Loss self: 2.7401; time: 0.15s
Val loss: 0.1869 score: 0.9592 time: 0.08s
Test loss: 0.1612 score: 0.9583 time: 0.06s
Epoch 28/1000, LR 0.000450
Train loss: 0.0419;  Loss pred: 0.0143; Loss self: 2.7597; time: 0.15s
Val loss: 0.1761 score: 0.9592 time: 0.08s
Test loss: 0.1443 score: 0.9583 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0398;  Loss pred: 0.0119; Loss self: 2.7928; time: 0.15s
Val loss: 0.1751 score: 0.9388 time: 0.08s
Test loss: 0.1364 score: 0.9583 time: 0.06s
Epoch 30/1000, LR 0.000450
Train loss: 0.0376;  Loss pred: 0.0098; Loss self: 2.7799; time: 0.15s
Val loss: 0.1800 score: 0.9388 time: 0.08s
Test loss: 0.1350 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0358;  Loss pred: 0.0079; Loss self: 2.7942; time: 0.15s
Val loss: 0.1903 score: 0.9388 time: 0.08s
Test loss: 0.1389 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0348;  Loss pred: 0.0068; Loss self: 2.8034; time: 0.15s
Val loss: 0.2040 score: 0.9388 time: 0.08s
Test loss: 0.1465 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0339;  Loss pred: 0.0058; Loss self: 2.8087; time: 0.13s
Val loss: 0.2201 score: 0.9388 time: 0.08s
Test loss: 0.1567 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0329;  Loss pred: 0.0048; Loss self: 2.8068; time: 0.13s
Val loss: 0.2375 score: 0.9388 time: 0.08s
Test loss: 0.1681 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0042; Loss self: 2.8317; time: 0.13s
Val loss: 0.2543 score: 0.9388 time: 0.08s
Test loss: 0.1790 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0036; Loss self: 2.8124; time: 0.13s
Val loss: 0.2736 score: 0.9388 time: 0.09s
Test loss: 0.1912 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0315;  Loss pred: 0.0034; Loss self: 2.8190; time: 0.13s
Val loss: 0.2912 score: 0.9388 time: 0.08s
Test loss: 0.2019 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0029; Loss self: 2.7987; time: 0.13s
Val loss: 0.3067 score: 0.9388 time: 0.09s
Test loss: 0.2107 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0027; Loss self: 2.8158; time: 0.14s
Val loss: 0.3226 score: 0.9388 time: 0.08s
Test loss: 0.2196 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0024; Loss self: 2.8266; time: 0.15s
Val loss: 0.3373 score: 0.9388 time: 0.08s
Test loss: 0.2277 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0022; Loss self: 2.7769; time: 0.15s
Val loss: 0.3502 score: 0.9184 time: 0.08s
Test loss: 0.2345 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0023; Loss self: 2.7931; time: 0.15s
Val loss: 0.3626 score: 0.9184 time: 0.08s
Test loss: 0.2412 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0019; Loss self: 2.7727; time: 0.15s
Val loss: 0.3732 score: 0.9184 time: 0.08s
Test loss: 0.2461 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0021; Loss self: 2.7730; time: 0.13s
Val loss: 0.3832 score: 0.9184 time: 0.08s
Test loss: 0.2495 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0017; Loss self: 2.7395; time: 0.13s
Val loss: 0.3926 score: 0.9184 time: 0.08s
Test loss: 0.2525 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0016; Loss self: 2.7425; time: 0.13s
Val loss: 0.4013 score: 0.9184 time: 0.08s
Test loss: 0.2553 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0015; Loss self: 2.7602; time: 0.13s
Val loss: 0.4081 score: 0.9184 time: 0.08s
Test loss: 0.2575 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0288;  Loss pred: 0.0013; Loss self: 2.7424; time: 0.13s
Val loss: 0.4137 score: 0.9184 time: 0.08s
Test loss: 0.2591 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0287;  Loss pred: 0.0013; Loss self: 2.7371; time: 0.13s
Val loss: 0.4196 score: 0.9184 time: 0.08s
Test loss: 0.2612 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0398,   Val_Loss: 0.1751,   Val_Precision: 1.0000,   Val_Recall: 0.8800,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.1751,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9583,   Test_loss: 0.1364


[0.07653751294128597, 0.07697066408582032, 0.07702512200921774, 0.07709258189424872, 0.07686479180119932, 0.07729266188107431, 0.07698917505331337, 0.07696645008400083, 0.07721949205733836, 0.07661820109933615, 0.07674231193959713, 0.07636552886106074, 0.07682616892270744, 0.07684865710325539, 0.07708050985820591, 0.07663812302052975, 0.07716256892308593, 0.07701448001898825, 0.07611878495663404, 0.07555329194292426, 0.07510562893003225, 0.07521314406767488, 0.07654601987451315, 0.07647759816609323, 0.07590521196834743, 0.07527540507726371, 0.07473676884546876, 0.07537511107511818, 0.07516981707885861, 0.07652934407815337, 0.07532217120751739, 0.07537160301581025, 0.07467137998901308, 0.07525896886363626, 0.0770738699939102, 0.07597143482416868, 0.07613689010031521, 0.07497339020483196, 0.07449777913279831, 0.07515608193352818, 0.07728854799643159, 0.07587432605214417, 0.16691234102472663, 0.07452917820774019, 0.07388782408088446, 0.07484774687327445, 0.07702478603459895, 0.07674724003300071, 0.07515149493701756, 0.07566750096157193, 0.07679657801054418, 0.07718751206994057, 0.07608358794823289, 0.07558295712806284, 0.0748729130718857, 0.07550211087800562, 0.07866709888912737, 0.07624876382760704, 0.16801708890125155, 0.07542437990196049, 0.0756057808175683, 0.07500237599015236, 0.0750873691868037, 0.07509441091679037, 0.07488051312975585, 0.0758380291517824, 0.07720147795043886, 0.07641500094905496, 0.07447450794279575, 0.07457897905260324, 0.07476979796774685, 0.07572567998431623, 0.0760462461039424, 0.07647050893865526, 0.07610485097393394, 0.07637277920730412, 0.07622583792544901, 0.07621224201284349, 0.0765900039114058, 0.07640910614281893, 0.07673933682963252, 0.07680922490544617, 0.07661751401610672, 0.07647622190415859, 0.07661028299480677, 0.07694655586965382, 0.07605505594983697, 0.0763991167768836, 0.07600916991941631, 0.07649313914589584, 0.0763007199857384, 0.07608069805428386, 0.07666899403557181, 0.07643439993262291, 0.07651047315448523, 0.07679987489245832, 0.07650065491907299, 0.0852354250382632, 0.07644646405242383, 0.07643454009667039, 0.0763576659373939, 0.07589109078980982, 0.0766772881615907, 0.07624562014825642, 0.0766100031323731, 0.07662449195049703, 0.07668841700069606, 0.0765310728456825, 0.07712392997927964, 0.07713195704855025, 0.07668489497154951, 0.07657111785374582, 0.0691249028313905, 0.06953016202896833, 0.06991660594940186, 0.07079509296454489, 0.07048211200162768, 0.07031678315252066, 0.07072121696546674, 0.070581333944574, 0.07026271498762071, 0.0704149198718369, 0.07064813701435924, 0.07000059681013227, 0.07022183993831277, 0.07032044092193246, 0.0701363009866327, 0.0700655491091311, 0.07019787188619375, 0.07028426905162632, 0.07035343418829143, 0.07032908708788455, 0.07026758277788758, 0.07001172681339085, 0.0702736540697515, 0.07105272612534463, 0.07058795099146664, 0.06994258402846754, 0.07000258099287748, 0.07024701591581106, 0.06976420618593693, 0.07023763214237988, 0.07011345122009516, 0.07028959901072085, 0.07029440114274621, 0.07037744182161987, 0.0700120881665498, 0.07259643799625337, 0.07363075017929077, 0.07718909601680934, 0.07066079694777727, 0.07140745990909636, 0.0700439908541739, 0.07308254414238036, 0.07096510613337159, 0.07199019100517035, 0.07172075402922928, 0.07044398691505194, 0.0694536049850285, 0.06955774687230587, 0.06989443209022284]
[0.0015619900600262443, 0.0015708298793024554, 0.0015719412654942395, 0.0015733179978418108, 0.0015686692204326391, 0.0015774012628790674, 0.0015712076541492526, 0.001570743879265323, 0.0015759080011701705, 0.001563636757129309, 0.0015661696314203497, 0.0015584801808379743, 0.0015678809984226007, 0.0015683399408827632, 0.0015730716297593043, 0.0015640433269495867, 0.001574746304552774, 0.0015717240820201685, 0.0015534445909517153, 0.001541903917202536, 0.001532767937347597, 0.0015349621238300996, 0.0015621636709084316, 0.0015607673095121067, 0.0015490859585377027, 0.0015362327566788513, 0.0015252401805197705, 0.0015382675729615955, 0.0015340778995685431, 0.0015618233485337422, 0.0015371871675003547, 0.0015381959799144948, 0.0015239057140614912, 0.0015358973237476787, 0.0015729361223246977, 0.0015504374453911977, 0.0015538140836799024, 0.0015300691878537135, 0.0015203628394448636, 0.001533797590480167, 0.0015773173060496242, 0.001548455633717228, 0.003406374306627074, 0.0015210036368926568, 0.0015079147771609072, 0.0015275050382300907, 0.0015719344088693662, 0.0015662702047551165, 0.0015337039783064807, 0.001544234713501468, 0.0015672771022560037, 0.001575255348366134, 0.001552726284657814, 0.0015425093291441397, 0.0015280186341201164, 0.0015408594056735842, 0.0016054509977372934, 0.0015560972209715722, 0.003428920181658195, 0.0015392730592236835, 0.0015429751187258838, 0.0015306607344929054, 0.001532395289526606, 0.0015325389983018441, 0.0015281737373419562, 0.0015477148806486204, 0.001575540366335487, 0.001559489815286836, 0.0015198879171999134, 0.0015220199806653724, 0.0015259142442397317, 0.0015454220404962497, 0.001551964206202906, 0.0015606226314011278, 0.0015531602239578354, 0.0015586281470878391, 0.001555629345417327, 0.0015553518778131325, 0.0015630613043144041, 0.0015593695131187536, 0.0015661089148904597, 0.0015675352021519626, 0.0015636227350225862, 0.0015607392225338488, 0.0015634751631593217, 0.0015703378748908943, 0.0015521439989762647, 0.0015591656485078285, 0.001551207549375843, 0.0015610844723652213, 0.0015571575507293551, 0.0015526673072302829, 0.001564673347664731, 0.0015598857129106717, 0.0015614382276425557, 0.0015673443855603738, 0.0015612378554912855, 0.0017394984701686368, 0.0015601319194372212, 0.0015598885734014365, 0.0015583197130080387, 0.0015487977712206086, 0.0015648426155426673, 0.0015560330642501311, 0.0015634694516810837, 0.0015637651418468782, 0.0015650697347080829, 0.0015618586295037245, 0.0015739577546791763, 0.0015741215724193929, 0.0015649978565622348, 0.0015626758745662412, 0.0014401021423206355, 0.0014485450422701736, 0.0014565959572792053, 0.0014748977700946853, 0.0014683773333672434, 0.0014649329823441803, 0.001473358686780557, 0.001470444457178625, 0.0014638065622420982, 0.0014669774973299354, 0.0014718361877991508, 0.0014583457668777555, 0.0014629549987148494, 0.001465009185873593, 0.0014611729372215148, 0.0014596989397735645, 0.0014624556642957032, 0.001464255605242215, 0.0014656965455894049, 0.001465189314330928, 0.0014639079745393246, 0.0014585776419456427, 0.0014640344597864896, 0.0014802651276113465, 0.0014705823123222217, 0.0014571371672597404, 0.001458387104018281, 0.0014634794982460637, 0.0014534209622070193, 0.0014632840029662475, 0.0014606969004186492, 0.0014643666460566844, 0.0014644666904738794, 0.0014661967046170805, 0.0014585851701364543, 0.001512425791588612, 0.0015339739620685577, 0.0016081061670168613, 0.0014720999364120264, 0.001487655414772841, 0.0014592498094619562, 0.0015225530029662575, 0.001478439711111908, 0.0014997956459410489, 0.0014941823756089434, 0.0014675830607302487, 0.0014469501038547605, 0.0014491197265063722, 0.0014561340018796425]
[640.20893960311, 636.6061743389176, 636.156084168696, 635.5994156119385, 637.4830250855562, 633.9541013012779, 636.4531113116686, 636.6410292604327, 634.5548085658951, 639.5347227804407, 638.5004407811855, 641.6507648254552, 637.8035074129164, 637.6168673209555, 635.6989606080493, 639.3684770551325, 635.0229221741205, 636.2439892851168, 643.7307167726863, 648.548842014937, 652.4144820842653, 651.4818733798849, 640.137790055301, 640.7104979105427, 645.5419691131758, 650.9430264733312, 655.6344454938372, 650.0819607571393, 651.8573797857647, 640.2772765170987, 650.5388680977062, 650.1122178563929, 656.20857693014, 651.0851894447878, 635.7537256643739, 644.9792624478865, 643.5776393734939, 653.5652164871957, 657.7377281630576, 651.9765099428422, 633.9878451625502, 645.804747792093, 293.5672682988795, 657.4606238568606, 663.1674516001453, 654.6623251460387, 636.1588590196093, 638.4594413939888, 652.0163044137123, 647.5699524540247, 638.0492629928418, 634.8177144976572, 644.028512868498, 648.2942962522303, 654.4422807879126, 648.9884776754512, 622.8779336207646, 642.6333692541622, 291.63700145286236, 649.6573132413165, 648.0985907444528, 653.312636474791, 652.5731362101251, 652.5119433228564, 654.3758576425738, 646.113836923838, 634.7028748783359, 641.2353515858458, 657.9432527118829, 657.0215980757598, 655.34482280047, 647.0724331580586, 644.3447574391149, 640.7698952194481, 643.848576968932, 641.5898505800841, 642.8266495138218, 642.9413268244017, 639.7701723149137, 641.2848215815061, 638.5251948265325, 637.9442060549376, 639.5404579388871, 640.722028101855, 639.6008222985105, 636.8056301701818, 644.2701196922206, 641.3686710947179, 644.6590595838503, 640.5803258582707, 642.1957749436538, 644.0529760260391, 639.1110332980978, 641.0726066168323, 640.4351976893703, 638.0218726738025, 640.5173923260547, 574.8783440453698, 640.9714380824445, 641.0714310314078, 641.7168387542829, 645.6620861559603, 639.0419011264037, 642.6598656384662, 639.6031588111771, 639.4822171435246, 638.9491648987258, 640.2628132340932, 635.3410674632958, 635.2749479590832, 638.9785109333363, 639.9279698853574, 694.3951894888247, 690.3478806794923, 686.5321814210669, 678.0130937046587, 681.0238603362441, 682.6250839132612, 678.7213520864391, 680.0664895012237, 683.1503736861993, 681.6737147094027, 679.4234360382922, 685.7084394607936, 683.5480249757936, 682.5895766678723, 684.3816871543929, 685.0727727151221, 683.7814126020601, 682.9408720853634, 682.2694663566033, 682.5056599983774, 683.103048410327, 685.5994300488992, 683.04403172712, 675.554656626726, 680.0027387932355, 686.2771895940158, 685.6890034509418, 683.3030467447409, 688.0319095449815, 683.3943362825557, 684.6047251235974, 682.8890856622877, 682.842434385732, 682.0367259392832, 685.5958914668298, 661.1894650048426, 651.9015476973964, 621.8494901086433, 679.301707217865, 672.1986758961222, 685.2836255423001, 656.7915849574938, 676.3887580156501, 666.7575030680587, 669.2623446267442, 681.3924381918214, 691.108834600406, 690.074106168482, 686.7499822881381]
Elapsed: 0.07566853723870533~0.010681486434699943
Time per graph: 0.0015533952092890112~0.0002141003655081342
Speed: 649.7278876529998~44.66792484025605
Total Time: 0.0705
best val loss: 0.1750761866569519 test_score: 0.9583

Testing...
Test loss: 0.5791 score: 0.9583 time: 0.06s
test Score 0.9583
Epoch Time List: [0.26782772480510175, 0.28646805603057146, 0.26702319900505245, 0.26614724891260266, 0.2669380821753293, 0.2678878151345998, 0.2674695572350174, 0.2677551850210875, 0.26658264407888055, 0.26678583095781505, 0.2826147477608174, 0.2673338009044528, 0.2670375183224678, 0.2674575010314584, 0.26769756176508963, 0.2672335540410131, 0.2680296800099313, 0.2750725301448256, 0.26398561685346067, 0.29652356496080756, 0.2608420879114419, 0.2601204758975655, 0.26237693009898067, 0.2674788611475378, 0.26424697902984917, 0.2977700000628829, 0.2589214858599007, 0.273525019409135, 0.2768610189668834, 0.2693258738145232, 0.26532938005402684, 0.2963942722417414, 0.25810846500098705, 0.259263392072171, 0.26317284000106156, 0.26818542601540685, 0.26685132388956845, 0.2896444520447403, 0.2725651101209223, 0.2775194710120559, 0.27756143314763904, 0.28237523790448904, 0.3722560121677816, 0.2660499468911439, 0.27458157297223806, 0.2746767569333315, 0.2672580659855157, 0.2826385307125747, 0.35563680808991194, 0.2687094109132886, 0.28164872503839433, 0.28382601775228977, 0.26830789423547685, 0.36555219790898263, 0.25965318991802633, 0.26067184400744736, 0.26516942121088505, 0.2781940447166562, 0.36307374807074666, 0.25849485117942095, 0.2832915158942342, 0.2876845500431955, 0.2845229080412537, 0.2667798539623618, 0.2664239821024239, 0.2684588853735477, 0.27527849609032273, 0.272732469253242, 0.3605542059522122, 0.2666752659715712, 0.26555693708360195, 0.26810736185871065, 0.27144926809705794, 0.27187041589058936, 0.2719110418111086, 0.27246470795944333, 0.27153935423120856, 0.2729558339342475, 0.2780148561578244, 0.2729688009712845, 0.27377888816408813, 0.27334864693693817, 0.2721777558326721, 0.2721748207695782, 0.2721353331580758, 0.27298885909840465, 0.27246715989895165, 0.2724686381407082, 0.2724222356919199, 0.2732764952816069, 0.27336559095419943, 0.27252460992895067, 0.27295964816585183, 0.2907853862270713, 0.29081179411150515, 0.29376730788499117, 0.2938295758794993, 0.30039316485635936, 0.27357570780441165, 0.2801356699783355, 0.2729070133063942, 0.2718188699800521, 0.27180269197560847, 0.2765730631072074, 0.27228097314946353, 0.27276927907951176, 0.2721938348840922, 0.27273288811556995, 0.27458154782652855, 0.2735217169392854, 0.29060692992061377, 0.29136808263137937, 0.28526740218512714, 0.2851748620159924, 0.2884376822039485, 0.29091108683496714, 0.2925513740628958, 0.29178344714455307, 0.29191349423490465, 0.2926950368564576, 0.29645865107886493, 0.2947882036678493, 0.29559823917225003, 0.2989111638162285, 0.270886252168566, 0.27132425480522215, 0.27146457601338625, 0.2812694201711565, 0.2920057319570333, 0.29262796603143215, 0.2915786986704916, 0.29191465582698584, 0.29205177212134004, 0.2928810869343579, 0.2925661485642195, 0.30443190410733223, 0.292499192757532, 0.2909180731512606, 0.2927374120336026, 0.2923356231767684, 0.29216577974148095, 0.29280956997536123, 0.29175211605615914, 0.29535908810794353, 0.27229098812676966, 0.2719360622577369, 0.27307568094693124, 0.28784618712961674, 0.28371778526343405, 0.29159746086224914, 0.28476950293406844, 0.2917922269552946, 0.2931912709027529, 0.3003643269184977, 0.297463744180277, 0.2802169208880514, 0.2793943118304014, 0.2727230309974402, 0.26914484379813075, 0.27083944086916745, 0.27137203118763864]
Total Epoch List: [61, 51, 49]
Total Time List: [0.07597679691389203, 0.0771002268884331, 0.07054372504353523]
T-times Epoch Time: 0.2849456042824868 ~ 0.0032237726217995487
T-times Total Epoch: 51.55555555555555 ~ 1.4989708403591144
T-times Total Time: 0.0838243288712369 ~ 0.012417253974596717
T-times Inference Elapsed: 0.07570156284739077 ~ 0.00045782670030997466
T-times Time Per Graph: 0.0015544519005394502 ~ 9.161266881905875e-06
T-times Speed: 649.9026141527803 ~ 3.0434324168746243
T-times cross validation test micro f1 score:0.9297333629081641 ~ 0.011738443660320341
T-times cross validation test precision:0.9766344605475039 ~ 0.017247587129812798
T-times cross validation test recall:0.8911111111111111 ~ 0.029755519597245435
T-times cross validation test f1_score:0.9297333629081641 ~ 0.013395871313695353
