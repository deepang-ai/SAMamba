Namespace(seed=15, model='FAGNN', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a05aeaa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 2.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 1.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4961 time: 1.44s
Epoch 2/1000, LR 0.000015
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 1.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4961 time: 1.23s
Epoch 3/1000, LR 0.000045
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4961 time: 1.32s
Epoch 4/1000, LR 0.000075
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 1.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4961 time: 1.32s
Epoch 5/1000, LR 0.000105
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 1.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4961 time: 1.48s
Epoch 6/1000, LR 0.000135
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 1.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 1.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4961 time: 1.57s
Epoch 7/1000, LR 0.000165
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 1.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 5.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 1.20s
Epoch 8/1000, LR 0.000195
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 1.29s
Epoch 9/1000, LR 0.000225
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 1.33s
Epoch 10/1000, LR 0.000255
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4961 time: 1.35s
Epoch 11/1000, LR 0.000285
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 1.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 1.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5039 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4961 time: 1.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.35s
Val loss: 0.6875 score: 0.5349 time: 1.13s
Test loss: 0.6870 score: 0.5426 time: 1.26s
Epoch 14/1000, LR 0.000285
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 1.25s
Val loss: 0.6858 score: 0.6512 time: 1.22s
Test loss: 0.6850 score: 0.6202 time: 1.13s
Epoch 15/1000, LR 0.000285
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.33s
Val loss: 0.6837 score: 0.7132 time: 1.12s
Test loss: 0.6824 score: 0.7054 time: 1.34s
Epoch 16/1000, LR 0.000285
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 1.35s
Val loss: 0.6809 score: 0.8062 time: 1.36s
Test loss: 0.6791 score: 0.8217 time: 1.43s
Epoch 17/1000, LR 0.000285
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 1.42s
Val loss: 0.6775 score: 0.7752 time: 1.20s
Test loss: 0.6751 score: 0.7984 time: 1.22s
Epoch 18/1000, LR 0.000285
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 1.60s
Val loss: 0.6733 score: 0.7287 time: 1.52s
Test loss: 0.6701 score: 0.7442 time: 1.38s
Epoch 19/1000, LR 0.000285
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 1.33s
Val loss: 0.6678 score: 0.6279 time: 1.37s
Test loss: 0.6639 score: 0.6977 time: 1.26s
Epoch 20/1000, LR 0.000285
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 1.79s
Val loss: 0.6614 score: 0.6047 time: 1.59s
Test loss: 0.6564 score: 0.6977 time: 1.35s
Epoch 21/1000, LR 0.000285
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 1.58s
Val loss: 0.6539 score: 0.5814 time: 1.33s
Test loss: 0.6477 score: 0.6744 time: 1.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 1.59s
Val loss: 0.6457 score: 0.5504 time: 1.44s
Test loss: 0.6379 score: 0.6667 time: 1.51s
Epoch 23/1000, LR 0.000285
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 1.33s
Val loss: 0.6367 score: 0.5504 time: 1.51s
Test loss: 0.6272 score: 0.6512 time: 1.55s
Epoch 24/1000, LR 0.000285
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 1.85s
Val loss: 0.6269 score: 0.5504 time: 1.40s
Test loss: 0.6157 score: 0.6434 time: 1.24s
Epoch 25/1000, LR 0.000285
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 1.44s
Val loss: 0.6163 score: 0.5504 time: 1.45s
Test loss: 0.6034 score: 0.6512 time: 1.59s
Epoch 26/1000, LR 0.000285
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 1.61s
Val loss: 0.6047 score: 0.5426 time: 1.39s
Test loss: 0.5900 score: 0.6667 time: 1.53s
Epoch 27/1000, LR 0.000285
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 1.80s
Val loss: 0.5919 score: 0.5659 time: 1.35s
Test loss: 0.5755 score: 0.6667 time: 1.74s
Epoch 28/1000, LR 0.000285
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 1.83s
Val loss: 0.5782 score: 0.6047 time: 1.50s
Test loss: 0.5605 score: 0.6899 time: 1.19s
Epoch 29/1000, LR 0.000285
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 1.41s
Val loss: 0.5630 score: 0.6977 time: 1.24s
Test loss: 0.5443 score: 0.7597 time: 1.33s
Epoch 30/1000, LR 0.000285
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 1.45s
Val loss: 0.5464 score: 0.7209 time: 1.53s
Test loss: 0.5268 score: 0.8217 time: 1.74s
Epoch 31/1000, LR 0.000285
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 1.58s
Val loss: 0.5286 score: 0.8062 time: 1.75s
Test loss: 0.5083 score: 0.8605 time: 1.48s
Epoch 32/1000, LR 0.000285
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 1.57s
Val loss: 0.5101 score: 0.8837 time: 1.49s
Test loss: 0.4893 score: 0.8992 time: 1.68s
Epoch 33/1000, LR 0.000285
Train loss: 0.4929;  Loss pred: 0.4929; Loss self: 0.0000; time: 1.49s
Val loss: 0.4924 score: 0.8837 time: 1.64s
Test loss: 0.4715 score: 0.9070 time: 1.36s
Epoch 34/1000, LR 0.000285
Train loss: 0.4729;  Loss pred: 0.4729; Loss self: 0.0000; time: 1.32s
Val loss: 0.4756 score: 0.8915 time: 1.50s
Test loss: 0.4537 score: 0.9070 time: 1.30s
Epoch 35/1000, LR 0.000285
Train loss: 0.4522;  Loss pred: 0.4522; Loss self: 0.0000; time: 1.45s
Val loss: 0.4597 score: 0.8992 time: 1.26s
Test loss: 0.4359 score: 0.9147 time: 1.48s
Epoch 36/1000, LR 0.000285
Train loss: 0.4291;  Loss pred: 0.4291; Loss self: 0.0000; time: 1.46s
Val loss: 0.4446 score: 0.8992 time: 1.36s
Test loss: 0.4173 score: 0.9070 time: 1.27s
Epoch 37/1000, LR 0.000285
Train loss: 0.4175;  Loss pred: 0.4175; Loss self: 0.0000; time: 1.48s
Val loss: 0.4310 score: 0.8915 time: 1.26s
Test loss: 0.3999 score: 0.9070 time: 1.24s
Epoch 38/1000, LR 0.000284
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 1.62s
Val loss: 0.4187 score: 0.8915 time: 1.22s
Test loss: 0.3835 score: 0.9070 time: 1.39s
Epoch 39/1000, LR 0.000284
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 1.29s
Val loss: 0.4074 score: 0.8915 time: 5.65s
Test loss: 0.3685 score: 0.9070 time: 1.22s
Epoch 40/1000, LR 0.000284
Train loss: 0.3575;  Loss pred: 0.3575; Loss self: 0.0000; time: 1.46s
Val loss: 0.3969 score: 0.8915 time: 1.22s
Test loss: 0.3542 score: 0.9225 time: 1.36s
Epoch 41/1000, LR 0.000284
Train loss: 0.3454;  Loss pred: 0.3454; Loss self: 0.0000; time: 1.28s
Val loss: 0.3871 score: 0.8915 time: 1.35s
Test loss: 0.3408 score: 0.9225 time: 1.21s
Epoch 42/1000, LR 0.000284
Train loss: 0.3209;  Loss pred: 0.3209; Loss self: 0.0000; time: 1.50s
Val loss: 0.3784 score: 0.8837 time: 1.18s
Test loss: 0.3280 score: 0.9225 time: 1.25s
Epoch 43/1000, LR 0.000284
Train loss: 0.3099;  Loss pred: 0.3099; Loss self: 0.0000; time: 1.44s
Val loss: 0.3704 score: 0.8837 time: 1.22s
Test loss: 0.3157 score: 0.9225 time: 1.35s
Epoch 44/1000, LR 0.000284
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 1.29s
Val loss: 0.3634 score: 0.8760 time: 1.35s
Test loss: 0.3042 score: 0.9302 time: 1.32s
Epoch 45/1000, LR 0.000284
Train loss: 0.2777;  Loss pred: 0.2777; Loss self: 0.0000; time: 1.44s
Val loss: 0.3571 score: 0.8605 time: 1.29s
Test loss: 0.2940 score: 0.9302 time: 1.38s
Epoch 46/1000, LR 0.000284
Train loss: 0.2677;  Loss pred: 0.2677; Loss self: 0.0000; time: 1.40s
Val loss: 0.3522 score: 0.8605 time: 1.33s
Test loss: 0.2840 score: 0.9302 time: 1.30s
Epoch 47/1000, LR 0.000284
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 1.31s
Val loss: 0.3487 score: 0.8605 time: 1.38s
Test loss: 0.2750 score: 0.9302 time: 1.24s
Epoch 48/1000, LR 0.000284
Train loss: 0.2343;  Loss pred: 0.2343; Loss self: 0.0000; time: 1.63s
Val loss: 0.3467 score: 0.8605 time: 1.52s
Test loss: 0.2669 score: 0.9302 time: 1.42s
Epoch 49/1000, LR 0.000284
Train loss: 0.2292;  Loss pred: 0.2292; Loss self: 0.0000; time: 1.46s
Val loss: 0.3463 score: 0.8605 time: 1.49s
Test loss: 0.2608 score: 0.9302 time: 1.56s
Epoch 50/1000, LR 0.000284
Train loss: 0.2152;  Loss pred: 0.2152; Loss self: 0.0000; time: 1.46s
Val loss: 0.3479 score: 0.8605 time: 1.23s
Test loss: 0.2568 score: 0.9302 time: 1.44s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1991;  Loss pred: 0.1991; Loss self: 0.0000; time: 1.51s
Val loss: 0.3515 score: 0.8605 time: 1.43s
Test loss: 0.2543 score: 0.9302 time: 1.23s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 1.56s
Val loss: 0.3559 score: 0.8605 time: 1.41s
Test loss: 0.2518 score: 0.9302 time: 1.59s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 1.84s
Val loss: 0.3606 score: 0.8682 time: 1.21s
Test loss: 0.2501 score: 0.9225 time: 1.31s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1658;  Loss pred: 0.1658; Loss self: 0.0000; time: 1.27s
Val loss: 0.3649 score: 0.8682 time: 1.23s
Test loss: 0.2490 score: 0.9225 time: 1.18s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 1.46s
Val loss: 0.3696 score: 0.8682 time: 1.18s
Test loss: 0.2489 score: 0.9225 time: 1.58s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 1.54s
Val loss: 0.3768 score: 0.8682 time: 1.25s
Test loss: 0.2510 score: 0.9302 time: 1.59s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 1.30s
Val loss: 0.3839 score: 0.8682 time: 1.68s
Test loss: 0.2546 score: 0.9302 time: 1.38s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1460;  Loss pred: 0.1460; Loss self: 0.0000; time: 1.43s
Val loss: 0.3915 score: 0.8682 time: 1.21s
Test loss: 0.2592 score: 0.9225 time: 1.21s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1377;  Loss pred: 0.1377; Loss self: 0.0000; time: 1.45s
Val loss: 0.3961 score: 0.8682 time: 1.19s
Test loss: 0.2616 score: 0.9225 time: 1.36s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 1.29s
Val loss: 0.4013 score: 0.8682 time: 1.41s
Test loss: 0.2646 score: 0.9225 time: 1.20s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 1.46s
Val loss: 0.4054 score: 0.8915 time: 1.27s
Test loss: 0.2665 score: 0.9225 time: 1.46s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1242;  Loss pred: 0.1242; Loss self: 0.0000; time: 1.37s
Val loss: 0.4092 score: 0.8915 time: 1.36s
Test loss: 0.2692 score: 0.9147 time: 1.32s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 1.78s
Val loss: 0.4132 score: 0.8915 time: 1.32s
Test loss: 0.2727 score: 0.9225 time: 1.21s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 1.48s
Val loss: 0.4198 score: 0.8915 time: 1.17s
Test loss: 0.2802 score: 0.9302 time: 1.35s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1100;  Loss pred: 0.1100; Loss self: 0.0000; time: 1.62s
Val loss: 0.4251 score: 0.8915 time: 1.36s
Test loss: 0.2857 score: 0.9302 time: 1.21s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 1.43s
Val loss: 0.4322 score: 0.8837 time: 1.19s
Test loss: 0.2922 score: 0.9302 time: 1.39s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0977;  Loss pred: 0.0977; Loss self: 0.0000; time: 1.34s
Val loss: 0.4384 score: 0.8760 time: 1.38s
Test loss: 0.2967 score: 0.9302 time: 1.57s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 1.56s
Val loss: 0.4441 score: 0.8915 time: 1.20s
Test loss: 0.2996 score: 0.9302 time: 1.34s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0968;  Loss pred: 0.0968; Loss self: 0.0000; time: 1.29s
Val loss: 0.4404 score: 0.8915 time: 1.21s
Test loss: 0.2920 score: 0.9147 time: 1.31s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.2292,   Val_Loss: 0.3463,   Val_Precision: 0.9600,   Val_Recall: 0.7500,   Val_accuracy: 0.8421,   Val_Score: 0.8605,   Val_Loss: 0.3463,   Test_Precision: 0.9828,   Test_Recall: 0.8769,   Test_accuracy: 0.9268,   Test_Score: 0.9302,   Test_loss: 0.2608


[1.44611028698273, 1.2402159590274096, 1.3296262258663774, 1.3241270540747792, 1.4912268400657922, 1.5824488799553365, 1.205544576048851, 1.2975220610387623, 1.3349636031780392, 1.3536454278510064, 1.1616179589182138, 1.1818027279805392, 1.2681487977970392, 1.1346176587976515, 1.3411852070130408, 1.4393572290427983, 1.2239101100713015, 1.3899476630613208, 1.268316422821954, 1.3512980230152607, 1.184850461082533, 1.5179957810323685, 1.5563397039659321, 1.242630867054686, 1.5991378859616816, 1.538215802051127, 1.7482750050257891, 1.1997576209250838, 1.3397998248692602, 1.7470801379531622, 1.4865795460063964, 1.6837548408657312, 1.362330748932436, 1.302479563979432, 1.4848536842036992, 1.2747622330207378, 1.2470481200143695, 1.3955301519017667, 1.2271418650634587, 1.363150330958888, 1.2123550809919834, 1.2534846980124712, 1.3546441940125078, 1.321333914063871, 1.3893555998802185, 1.3044607199262828, 1.2459686789661646, 1.421663640998304, 1.563937318045646, 1.4461701659020036, 1.2315050200559199, 1.5962572691496462, 1.3136053150519729, 1.1834247908554971, 1.5872141649015248, 1.5978830088861287, 1.3895936040207744, 1.2155874138697982, 1.3635227070190012, 1.2073028369341046, 1.4698795250151306, 1.331852009985596, 1.2134974880609661, 1.3590291088912636, 1.2112045628018677, 1.3999920340720564, 1.576962191844359, 1.3491092720068991, 1.3121223791968077]
[0.011210157263432016, 0.009614077201762865, 0.010307180045475794, 0.010264550806781235, 0.01155989798500614, 0.01226704558104912, 0.009345306791076365, 0.01005831055068808, 0.010348555063395652, 0.010493375409697725, 0.00900479037921096, 0.009161261457213483, 0.009830610835635963, 0.008795485727113578, 0.010396784550488689, 0.011157807977075955, 0.009487675271870555, 0.010774788085746672, 0.009831910254433751, 0.010475178472986516, 0.009184887295213435, 0.011767409155289678, 0.012064648867952962, 0.009632797419028572, 0.01239641772063319, 0.011924153504272303, 0.013552519418804566, 0.00930044667383786, 0.010386045154025273, 0.013543256883357847, 0.011523872449661987, 0.013052363107486288, 0.010560703480096405, 0.010096740806042108, 0.011510493675997668, 0.009881877775354556, 0.009667039690033873, 0.01081806319303695, 0.009512727636150842, 0.010567056829138666, 0.009398101403038631, 0.009716935643507528, 0.010501117783042696, 0.010242898558634658, 0.010770198448683864, 0.010112098604079712, 0.009658671929970268, 0.011020648379831813, 0.012123545101129039, 0.011210621441100804, 0.009546550543069146, 0.012374087357749195, 0.010182986938387387, 0.00917383558802711, 0.012303985774430424, 0.012386689991365338, 0.010772043442021506, 0.009423158247052699, 0.010569943465263576, 0.009358936720419415, 0.011394414922597912, 0.010324434185934853, 0.009406957271790434, 0.010535109371250106, 0.00938918265737882, 0.010852651426915166, 0.012224513115072552, 0.010458211410906195, 0.010171491311603161]
[89.20481457133882, 104.01414290875854, 97.0197469713297, 97.42267526596041, 86.5059537114478, 81.51922102130777, 107.00558284024221, 99.42027490208989, 96.6318480091144, 95.29822015857968, 111.05200208865101, 109.15527350358617, 101.7230787302657, 113.69468736869531, 96.18358398635819, 89.6233383881968, 105.39989737684644, 92.80924989353996, 101.70963466118344, 95.46376728366097, 108.87449871280782, 84.98047333983288, 82.88678857917499, 103.8120035644688, 80.66846588555596, 83.86339538833597, 73.78701842053567, 107.52171751201998, 96.28303990306017, 73.83748300815402, 86.7763856609964, 76.61447906137717, 94.69066164812644, 99.0418610529824, 86.87724681046969, 101.19534189078963, 103.44428408946526, 92.43798840476826, 105.12232014293562, 94.63372972902913, 106.40447012804906, 102.91310313124903, 95.22795769559022, 97.62861501318007, 92.84879984009966, 98.89144075360889, 103.53390271979951, 90.73876286898295, 82.48412421106697, 89.20112103096821, 104.74987750690809, 80.81404075216555, 98.203013128716, 109.00565967250523, 81.27447628216166, 80.73181783810622, 92.83289706194665, 106.12153311898079, 94.60788539563524, 106.84974478117698, 87.76229466743004, 96.85760807718805, 106.30429915938898, 94.92070416743468, 106.50554329285679, 92.14338143395499, 81.80284896312332, 95.61864459511351, 98.31400031372458]
Elapsed: 1.3665255884187755~0.14517646227607173
Time per graph: 0.010593221615649422~0.001125398932372649
Speed: 95.41302527604567~9.599557033857716
Total Time: 1.3126
best val loss: 0.3462573222873747 test_score: 0.9302

Testing...
Test loss: 0.4359 score: 0.9147 time: 1.19s
test Score 0.9147
Epoch Time List: [5.833448173711076, 3.984576453221962, 4.066681918222457, 4.445195639971644, 4.604470815043896, 4.893421276006848, 8.638216018909588, 3.9618758109863847, 3.896271134959534, 3.87333961087279, 3.742987170116976, 3.7719827382825315, 3.742806263966486, 3.6065464448183775, 3.785075943917036, 4.138352988054976, 3.835627819877118, 4.509542111074552, 3.9580085752531886, 4.724141204962507, 4.09422349371016, 4.550104948924854, 4.391774018760771, 4.486060467781499, 4.487080877413973, 4.52967137680389, 4.889423510292545, 4.52594668394886, 3.9845819268375635, 4.719668237026781, 4.811915077269077, 4.740033771144226, 4.488925870275125, 4.116564202588052, 4.18974828789942, 4.092173795914277, 3.9870903128758073, 4.235221538692713, 8.16445699473843, 4.038144581019878, 3.8347212730441242, 3.9211358288303018, 4.014673670055345, 3.9532157161738724, 4.11133376811631, 4.029833317035809, 3.9223433919250965, 4.573753071948886, 4.500060840975493, 4.137092796852812, 4.158973869169131, 4.559569382108748, 4.359155646292493, 3.6799578808713704, 4.221883582882583, 4.383619510801509, 4.3610912340227515, 3.855075262952596, 4.0005756102036685, 3.9024628959596157, 4.200511948904023, 4.054004296660423, 4.307426766958088, 4.0047980500385165, 4.186591337900609, 4.020484821638092, 4.285186409018934, 4.0998677760362625, 3.8015135729219764]
Total Epoch List: [69]
Total Time List: [1.312614191090688]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a05aee60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 1.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5039 time: 6.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4961 time: 1.38s
Epoch 2/1000, LR 0.000015
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5039 time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4961 time: 1.12s
Epoch 3/1000, LR 0.000045
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 1.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4961 time: 1.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4961 time: 1.24s
Epoch 5/1000, LR 0.000105
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4961 time: 1.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4961 time: 1.26s
Epoch 7/1000, LR 0.000165
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4961 time: 1.26s
Epoch 8/1000, LR 0.000195
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 1.23s
Epoch 9/1000, LR 0.000225
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 1.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 1.36s
Epoch 10/1000, LR 0.000255
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 1.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 1.12s
Epoch 11/1000, LR 0.000285
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 1.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5039 time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 1.28s
Epoch 12/1000, LR 0.000285
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5039 time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4961 time: 1.15s
Epoch 13/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.5039 time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4961 time: 1.43s
Epoch 14/1000, LR 0.000285
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5039 time: 1.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4961 time: 1.21s
Epoch 15/1000, LR 0.000285
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.5039 time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.4961 time: 1.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5039 time: 1.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.4961 time: 1.25s
Epoch 17/1000, LR 0.000285
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5039 time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6776 score: 0.4961 time: 1.30s
Epoch 18/1000, LR 0.000285
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6656 score: 0.5039 time: 1.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6736 score: 0.4961 time: 1.35s
Epoch 19/1000, LR 0.000285
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6595 score: 0.5039 time: 1.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6687 score: 0.4961 time: 1.20s
Epoch 20/1000, LR 0.000285
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6525 score: 0.5039 time: 1.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6631 score: 0.4961 time: 1.35s
Epoch 21/1000, LR 0.000285
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6443 score: 0.5039 time: 1.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6566 score: 0.4961 time: 1.54s
Epoch 22/1000, LR 0.000285
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6348 score: 0.5039 time: 1.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6490 score: 0.4961 time: 1.38s
Epoch 23/1000, LR 0.000285
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 1.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6241 score: 0.5039 time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6403 score: 0.4961 time: 1.56s
Epoch 24/1000, LR 0.000285
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 1.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6120 score: 0.5039 time: 1.46s
Test loss: 0.6304 score: 0.5116 time: 1.13s
Epoch 25/1000, LR 0.000285
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 1.51s
Val loss: 0.5988 score: 0.5504 time: 1.77s
Test loss: 0.6195 score: 0.5116 time: 1.41s
Epoch 26/1000, LR 0.000285
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 1.32s
Val loss: 0.5842 score: 0.5891 time: 1.33s
Test loss: 0.6069 score: 0.5271 time: 1.27s
Epoch 27/1000, LR 0.000285
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 1.39s
Val loss: 0.5683 score: 0.6047 time: 1.41s
Test loss: 0.5928 score: 0.5194 time: 1.23s
Epoch 28/1000, LR 0.000285
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 1.65s
Val loss: 0.5514 score: 0.6279 time: 1.42s
Test loss: 0.5777 score: 0.5349 time: 1.28s
Epoch 29/1000, LR 0.000285
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 1.52s
Val loss: 0.5326 score: 0.6899 time: 1.95s
Test loss: 0.5605 score: 0.6279 time: 1.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 1.49s
Val loss: 0.5130 score: 0.7674 time: 1.40s
Test loss: 0.5424 score: 0.7054 time: 1.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 1.32s
Val loss: 0.4929 score: 0.8217 time: 1.66s
Test loss: 0.5237 score: 0.7984 time: 1.58s
Epoch 32/1000, LR 0.000285
Train loss: 0.5003;  Loss pred: 0.5003; Loss self: 0.0000; time: 1.28s
Val loss: 0.4733 score: 0.8605 time: 1.43s
Test loss: 0.5051 score: 0.8217 time: 1.38s
Epoch 33/1000, LR 0.000285
Train loss: 0.4837;  Loss pred: 0.4837; Loss self: 0.0000; time: 1.64s
Val loss: 0.4544 score: 0.8992 time: 1.59s
Test loss: 0.4871 score: 0.8372 time: 1.27s
Epoch 34/1000, LR 0.000285
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 1.21s
Val loss: 0.4356 score: 0.9225 time: 5.54s
Test loss: 0.4692 score: 0.8605 time: 2.05s
Epoch 35/1000, LR 0.000285
Train loss: 0.4494;  Loss pred: 0.4494; Loss self: 0.0000; time: 1.37s
Val loss: 0.4175 score: 0.9225 time: 1.38s
Test loss: 0.4518 score: 0.8760 time: 1.24s
Epoch 36/1000, LR 0.000285
Train loss: 0.4255;  Loss pred: 0.4255; Loss self: 0.0000; time: 1.24s
Val loss: 0.4001 score: 0.9225 time: 1.46s
Test loss: 0.4352 score: 0.8760 time: 1.10s
Epoch 37/1000, LR 0.000285
Train loss: 0.4050;  Loss pred: 0.4050; Loss self: 0.0000; time: 1.35s
Val loss: 0.3832 score: 0.9225 time: 1.38s
Test loss: 0.4190 score: 0.8760 time: 1.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.3913;  Loss pred: 0.3913; Loss self: 0.0000; time: 1.35s
Val loss: 0.3663 score: 0.9225 time: 1.31s
Test loss: 0.4028 score: 0.8837 time: 1.23s
Epoch 39/1000, LR 0.000284
Train loss: 0.3798;  Loss pred: 0.3798; Loss self: 0.0000; time: 1.21s
Val loss: 0.3485 score: 0.9380 time: 1.42s
Test loss: 0.3858 score: 0.8837 time: 1.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.3537;  Loss pred: 0.3537; Loss self: 0.0000; time: 1.35s
Val loss: 0.3317 score: 0.9380 time: 1.30s
Test loss: 0.3703 score: 0.8915 time: 1.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.3346;  Loss pred: 0.3346; Loss self: 0.0000; time: 1.23s
Val loss: 0.3156 score: 0.9302 time: 1.43s
Test loss: 0.3558 score: 0.8915 time: 1.13s
Epoch 42/1000, LR 0.000284
Train loss: 0.3144;  Loss pred: 0.3144; Loss self: 0.0000; time: 1.38s
Val loss: 0.2998 score: 0.9302 time: 1.32s
Test loss: 0.3418 score: 0.8915 time: 1.27s
Epoch 43/1000, LR 0.000284
Train loss: 0.3019;  Loss pred: 0.3019; Loss self: 0.0000; time: 1.20s
Val loss: 0.2844 score: 0.9302 time: 1.54s
Test loss: 0.3286 score: 0.8915 time: 1.14s
Epoch 44/1000, LR 0.000284
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 1.28s
Val loss: 0.2700 score: 0.9302 time: 1.45s
Test loss: 0.3167 score: 0.8915 time: 1.26s
Epoch 45/1000, LR 0.000284
Train loss: 0.2583;  Loss pred: 0.2583; Loss self: 0.0000; time: 1.37s
Val loss: 0.2578 score: 0.9302 time: 1.35s
Test loss: 0.3072 score: 0.8915 time: 1.23s
Epoch 46/1000, LR 0.000284
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 1.33s
Val loss: 0.2446 score: 0.9302 time: 1.52s
Test loss: 0.2970 score: 0.8837 time: 1.12s
Epoch 47/1000, LR 0.000284
Train loss: 0.2291;  Loss pred: 0.2291; Loss self: 0.0000; time: 1.35s
Val loss: 0.2355 score: 0.9302 time: 1.48s
Test loss: 0.2913 score: 0.8837 time: 1.33s
Epoch 48/1000, LR 0.000284
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 1.27s
Val loss: 0.2278 score: 0.9302 time: 1.68s
Test loss: 0.2875 score: 0.8837 time: 1.36s
Epoch 49/1000, LR 0.000284
Train loss: 0.2065;  Loss pred: 0.2065; Loss self: 0.0000; time: 1.66s
Val loss: 0.2182 score: 0.9380 time: 1.58s
Test loss: 0.2822 score: 0.8837 time: 1.29s
Epoch 50/1000, LR 0.000284
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 1.59s
Val loss: 0.2122 score: 0.9380 time: 1.79s
Test loss: 0.2807 score: 0.8837 time: 1.15s
Epoch 51/1000, LR 0.000284
Train loss: 0.1924;  Loss pred: 0.1924; Loss self: 0.0000; time: 1.28s
Val loss: 0.2098 score: 0.9380 time: 1.44s
Test loss: 0.2826 score: 0.8837 time: 1.15s
Epoch 52/1000, LR 0.000284
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 1.38s
Val loss: 0.2097 score: 0.9302 time: 1.36s
Test loss: 0.2866 score: 0.8837 time: 1.23s
Epoch 53/1000, LR 0.000284
Train loss: 0.1721;  Loss pred: 0.1721; Loss self: 0.0000; time: 1.24s
Val loss: 0.2133 score: 0.9225 time: 1.42s
Test loss: 0.2927 score: 0.8837 time: 1.12s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1675;  Loss pred: 0.1675; Loss self: 0.0000; time: 1.55s
Val loss: 0.2211 score: 0.9225 time: 1.31s
Test loss: 0.3009 score: 0.8992 time: 1.30s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1792;  Loss pred: 0.1792; Loss self: 0.0000; time: 1.45s
Val loss: 0.2215 score: 0.9225 time: 1.75s
Test loss: 0.3027 score: 0.8992 time: 1.31s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 1.35s
Val loss: 0.2185 score: 0.9225 time: 1.30s
Test loss: 0.3013 score: 0.8915 time: 1.44s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 1.32s
Val loss: 0.2117 score: 0.9225 time: 1.49s
Test loss: 0.2984 score: 0.8915 time: 1.18s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 1.46s
Val loss: 0.2137 score: 0.9225 time: 1.81s
Test loss: 0.2995 score: 0.8915 time: 1.12s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 1.39s
Val loss: 0.2088 score: 0.9225 time: 1.37s
Test loss: 0.2972 score: 0.8915 time: 1.23s
Epoch 60/1000, LR 0.000283
Train loss: 0.1552;  Loss pred: 0.1552; Loss self: 0.0000; time: 1.28s
Val loss: 0.2245 score: 0.9302 time: 1.53s
Test loss: 0.3036 score: 0.8915 time: 1.12s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 1.43s
Val loss: 0.2335 score: 0.9302 time: 1.32s
Test loss: 0.3080 score: 0.9070 time: 1.30s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 1.54s
Val loss: 0.2558 score: 0.9225 time: 1.84s
Test loss: 0.3214 score: 0.9147 time: 1.45s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1390;  Loss pred: 0.1390; Loss self: 0.0000; time: 1.29s
Val loss: 0.2731 score: 0.9147 time: 1.49s
Test loss: 0.3334 score: 0.9147 time: 1.18s
     INFO: Early stopping counter 4 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1371;  Loss pred: 0.1371; Loss self: 0.0000; time: 1.43s
Val loss: 0.2433 score: 0.9302 time: 1.32s
Test loss: 0.3141 score: 0.9147 time: 1.29s
     INFO: Early stopping counter 5 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 1.23s
Val loss: 0.2106 score: 0.9302 time: 1.46s
Test loss: 0.2998 score: 0.9147 time: 1.10s
     INFO: Early stopping counter 6 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1217;  Loss pred: 0.1217; Loss self: 0.0000; time: 1.42s
Val loss: 0.2035 score: 0.9302 time: 1.41s
Test loss: 0.2990 score: 0.9147 time: 1.31s
Epoch 67/1000, LR 0.000283
Train loss: 0.1222;  Loss pred: 0.1222; Loss self: 0.0000; time: 1.25s
Val loss: 0.2099 score: 0.9302 time: 1.58s
Test loss: 0.3029 score: 0.9147 time: 1.13s
     INFO: Early stopping counter 1 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 1.52s
Val loss: 0.2096 score: 0.9302 time: 1.39s
Test loss: 0.3041 score: 0.9070 time: 1.55s
     INFO: Early stopping counter 2 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 1.23s
Val loss: 0.2222 score: 0.9302 time: 1.44s
Test loss: 0.3103 score: 0.9070 time: 1.11s
     INFO: Early stopping counter 3 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.1155;  Loss pred: 0.1155; Loss self: 0.0000; time: 1.35s
Val loss: 0.2380 score: 0.9225 time: 5.49s
Test loss: 0.3189 score: 0.9070 time: 1.70s
     INFO: Early stopping counter 4 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.1077;  Loss pred: 0.1077; Loss self: 0.0000; time: 1.23s
Val loss: 0.2323 score: 0.9225 time: 1.45s
Test loss: 0.3180 score: 0.9070 time: 1.10s
     INFO: Early stopping counter 5 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 1.24s
Val loss: 0.2196 score: 0.9225 time: 1.41s
Test loss: 0.3149 score: 0.8992 time: 1.13s
     INFO: Early stopping counter 6 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 1.34s
Val loss: 0.2067 score: 0.9380 time: 1.30s
Test loss: 0.3133 score: 0.8992 time: 1.26s
     INFO: Early stopping counter 7 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.0903;  Loss pred: 0.0903; Loss self: 0.0000; time: 1.26s
Val loss: 0.2040 score: 0.9457 time: 1.47s
Test loss: 0.3152 score: 0.8992 time: 1.14s
     INFO: Early stopping counter 8 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 1.38s
Val loss: 0.2039 score: 0.9457 time: 1.38s
Test loss: 0.3177 score: 0.8915 time: 1.59s
     INFO: Early stopping counter 9 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 1.45s
Val loss: 0.2073 score: 0.9457 time: 1.52s
Test loss: 0.3209 score: 0.8992 time: 1.20s
     INFO: Early stopping counter 10 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.0709;  Loss pred: 0.0709; Loss self: 0.0000; time: 1.37s
Val loss: 0.2221 score: 0.9302 time: 1.46s
Test loss: 0.3266 score: 0.8992 time: 1.40s
     INFO: Early stopping counter 11 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.0790;  Loss pred: 0.0790; Loss self: 0.0000; time: 1.67s
Val loss: 0.2344 score: 0.9225 time: 1.37s
Test loss: 0.3326 score: 0.8992 time: 1.27s
     INFO: Early stopping counter 12 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 1.30s
Val loss: 0.2431 score: 0.9225 time: 1.44s
Test loss: 0.3375 score: 0.8915 time: 1.15s
     INFO: Early stopping counter 13 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 1.38s
Val loss: 0.2592 score: 0.9147 time: 1.33s
Test loss: 0.3459 score: 0.8915 time: 1.24s
     INFO: Early stopping counter 14 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.0763;  Loss pred: 0.0763; Loss self: 0.0000; time: 1.22s
Val loss: 0.2770 score: 0.9147 time: 1.46s
Test loss: 0.3556 score: 0.8915 time: 1.26s
     INFO: Early stopping counter 15 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.0854;  Loss pred: 0.0854; Loss self: 0.0000; time: 1.22s
Val loss: 0.2742 score: 0.9147 time: 1.43s
Test loss: 0.3568 score: 0.8915 time: 1.11s
     INFO: Early stopping counter 16 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 1.38s
Val loss: 0.2614 score: 0.9147 time: 1.32s
Test loss: 0.3545 score: 0.8915 time: 1.29s
     INFO: Early stopping counter 17 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 1.24s
Val loss: 0.2386 score: 0.9380 time: 1.43s
Test loss: 0.3505 score: 0.8992 time: 1.12s
     INFO: Early stopping counter 18 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 1.50s
Val loss: 0.2305 score: 0.9380 time: 1.61s
Test loss: 0.3513 score: 0.8992 time: 1.27s
     INFO: Early stopping counter 19 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 1.24s
Val loss: 0.2278 score: 0.9457 time: 1.47s
Test loss: 0.3537 score: 0.8992 time: 1.13s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 065,   Train_Loss: 0.1217,   Val_Loss: 0.2035,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.2035,   Test_Precision: 0.9344,   Test_Recall: 0.8906,   Test_accuracy: 0.9120,   Test_Score: 0.9147,   Test_loss: 0.2990


[1.44611028698273, 1.2402159590274096, 1.3296262258663774, 1.3241270540747792, 1.4912268400657922, 1.5824488799553365, 1.205544576048851, 1.2975220610387623, 1.3349636031780392, 1.3536454278510064, 1.1616179589182138, 1.1818027279805392, 1.2681487977970392, 1.1346176587976515, 1.3411852070130408, 1.4393572290427983, 1.2239101100713015, 1.3899476630613208, 1.268316422821954, 1.3512980230152607, 1.184850461082533, 1.5179957810323685, 1.5563397039659321, 1.242630867054686, 1.5991378859616816, 1.538215802051127, 1.7482750050257891, 1.1997576209250838, 1.3397998248692602, 1.7470801379531622, 1.4865795460063964, 1.6837548408657312, 1.362330748932436, 1.302479563979432, 1.4848536842036992, 1.2747622330207378, 1.2470481200143695, 1.3955301519017667, 1.2271418650634587, 1.363150330958888, 1.2123550809919834, 1.2534846980124712, 1.3546441940125078, 1.321333914063871, 1.3893555998802185, 1.3044607199262828, 1.2459686789661646, 1.421663640998304, 1.563937318045646, 1.4461701659020036, 1.2315050200559199, 1.5962572691496462, 1.3136053150519729, 1.1834247908554971, 1.5872141649015248, 1.5978830088861287, 1.3895936040207744, 1.2155874138697982, 1.3635227070190012, 1.2073028369341046, 1.4698795250151306, 1.331852009985596, 1.2134974880609661, 1.3590291088912636, 1.2112045628018677, 1.3999920340720564, 1.576962191844359, 1.3491092720068991, 1.3121223791968077, 1.382732065860182, 1.124884343938902, 1.1796095839235932, 1.2440761511679739, 1.1765136260073632, 1.2698128810152411, 1.2633620121050626, 1.238640618044883, 1.364172762958333, 1.1273891560267657, 1.2803331969771534, 1.1545468717813492, 1.4326218839269131, 1.2149654228705913, 1.1628007169347256, 1.2548773509915918, 1.3060854321811348, 1.353934507118538, 1.2049151500687003, 1.355244108941406, 1.550969598814845, 1.3894209160935134, 1.567415569908917, 1.130240865983069, 1.4186052060686052, 1.2718530939891934, 1.2365405480377376, 1.282256058882922, 1.1751494209747761, 1.1819032048806548, 1.5806870060041547, 1.3882714400533587, 1.2764596389606595, 2.053684012964368, 1.240833843825385, 1.1084861929994076, 1.167099699145183, 1.2352833040058613, 1.1273760988842696, 1.2121289989445359, 1.132518001133576, 1.2704993388615549, 1.144443558063358, 1.2682391521520913, 1.2383759480435401, 1.1240420548710972, 1.3330486039631069, 1.3650473488960415, 1.3010590800549835, 1.1565167859662324, 1.1557756010442972, 1.2354600538965315, 1.1288516740314662, 1.3081380289513618, 1.310437825974077, 1.4441987690515816, 1.1905428909230977, 1.1200245271902531, 1.2366712871007621, 1.1287995919119567, 1.302287011174485, 1.4508612360805273, 1.1851724258158356, 1.2936687010806054, 1.1030205148272216, 1.3103646310046315, 1.1336034031119198, 1.5556638119742274, 1.1173391100019217, 1.7003468698821962, 1.1042537298053503, 1.1335651651024818, 1.2628299980424345, 1.1411811718717217, 1.595570051111281, 1.201415941119194, 1.4027500981464982, 1.2715566218830645, 1.1525425019208342, 1.2463895459659398, 1.265286416048184, 1.115096366032958, 1.2980331310536712, 1.128055701032281, 1.2779271369799972, 1.1320521167945117]
[0.011210157263432016, 0.009614077201762865, 0.010307180045475794, 0.010264550806781235, 0.01155989798500614, 0.01226704558104912, 0.009345306791076365, 0.01005831055068808, 0.010348555063395652, 0.010493375409697725, 0.00900479037921096, 0.009161261457213483, 0.009830610835635963, 0.008795485727113578, 0.010396784550488689, 0.011157807977075955, 0.009487675271870555, 0.010774788085746672, 0.009831910254433751, 0.010475178472986516, 0.009184887295213435, 0.011767409155289678, 0.012064648867952962, 0.009632797419028572, 0.01239641772063319, 0.011924153504272303, 0.013552519418804566, 0.00930044667383786, 0.010386045154025273, 0.013543256883357847, 0.011523872449661987, 0.013052363107486288, 0.010560703480096405, 0.010096740806042108, 0.011510493675997668, 0.009881877775354556, 0.009667039690033873, 0.01081806319303695, 0.009512727636150842, 0.010567056829138666, 0.009398101403038631, 0.009716935643507528, 0.010501117783042696, 0.010242898558634658, 0.010770198448683864, 0.010112098604079712, 0.009658671929970268, 0.011020648379831813, 0.012123545101129039, 0.011210621441100804, 0.009546550543069146, 0.012374087357749195, 0.010182986938387387, 0.00917383558802711, 0.012303985774430424, 0.012386689991365338, 0.010772043442021506, 0.009423158247052699, 0.010569943465263576, 0.009358936720419415, 0.011394414922597912, 0.010324434185934853, 0.009406957271790434, 0.010535109371250106, 0.00938918265737882, 0.010852651426915166, 0.012224513115072552, 0.010458211410906195, 0.010171491311603161, 0.010718853223722341, 0.008720033673944977, 0.00914426034049297, 0.009644001171844758, 0.009120260666723745, 0.009843510705544505, 0.009793503969806687, 0.009601865256161884, 0.010574982658591728, 0.008739450821912912, 0.009925063542458553, 0.008949975750243018, 0.011105595999433435, 0.009418336611399932, 0.009013959046005625, 0.009727731403035596, 0.010124693272721976, 0.010495616334252233, 0.009340427519912405, 0.01050576828636749, 0.012023020145851512, 0.010770704775918708, 0.012150508293867573, 0.008761557100643946, 0.010996939581927172, 0.009859326309993748, 0.009585585643703392, 0.009939969448704821, 0.009109685433912993, 0.009162040347912053, 0.01225338764344306, 0.010761794108940764, 0.009895035960935345, 0.015920031108250913, 0.009618867006398333, 0.00859291622480161, 0.009047284489497542, 0.00957583956593691, 0.008739349603754028, 0.00939634882902741, 0.008779209311112993, 0.0098488320841981, 0.00887165548886324, 0.009831311256992955, 0.009599813550725117, 0.008713504301326336, 0.010333710108241138, 0.010581762394542957, 0.01008572930275181, 0.008965246402839012, 0.008959500783289125, 0.009577209720128151, 0.008750788170786561, 0.010140604875591952, 0.010158432759488968, 0.0111953392949735, 0.009229014658318588, 0.008682360675893435, 0.00958659912481211, 0.00875038443342602, 0.01009524814863942, 0.011246986326205638, 0.009187383145859191, 0.010028439543260506, 0.00855054662656761, 0.010157865356625051, 0.008787623279937363, 0.012059409395149049, 0.00866154348838699, 0.013180983487458886, 0.008560106432599615, 0.00878732686125955, 0.009789379829786313, 0.0088463656734242, 0.01236876008613396, 0.009313301869141038, 0.010874031768577505, 0.009857028076612904, 0.008934437999386311, 0.009661934464852246, 0.009808421829830884, 0.008644157876224481, 0.01006227233374939, 0.008744617837459542, 0.009906411914573622, 0.008775597804608617]
[89.20481457133882, 104.01414290875854, 97.0197469713297, 97.42267526596041, 86.5059537114478, 81.51922102130777, 107.00558284024221, 99.42027490208989, 96.6318480091144, 95.29822015857968, 111.05200208865101, 109.15527350358617, 101.7230787302657, 113.69468736869531, 96.18358398635819, 89.6233383881968, 105.39989737684644, 92.80924989353996, 101.70963466118344, 95.46376728366097, 108.87449871280782, 84.98047333983288, 82.88678857917499, 103.8120035644688, 80.66846588555596, 83.86339538833597, 73.78701842053567, 107.52171751201998, 96.28303990306017, 73.83748300815402, 86.7763856609964, 76.61447906137717, 94.69066164812644, 99.0418610529824, 86.87724681046969, 101.19534189078963, 103.44428408946526, 92.43798840476826, 105.12232014293562, 94.63372972902913, 106.40447012804906, 102.91310313124903, 95.22795769559022, 97.62861501318007, 92.84879984009966, 98.89144075360889, 103.53390271979951, 90.73876286898295, 82.48412421106697, 89.20112103096821, 104.74987750690809, 80.81404075216555, 98.203013128716, 109.00565967250523, 81.27447628216166, 80.73181783810622, 92.83289706194665, 106.12153311898079, 94.60788539563524, 106.84974478117698, 87.76229466743004, 96.85760807718805, 106.30429915938898, 94.92070416743468, 106.50554329285679, 92.14338143395499, 81.80284896312332, 95.61864459511351, 98.31400031372458, 93.29356220559661, 114.67845622981363, 109.35821627603504, 103.69140175132459, 109.64598891878254, 101.58977116129259, 102.10849998968642, 104.14643127368007, 94.56280282290034, 114.42366578602905, 100.75502244616243, 111.73214631032349, 90.04469458919775, 106.1758611164526, 110.93904408664162, 102.79889098170867, 98.76842419455882, 95.27787298555494, 107.06148063010482, 95.18580390713753, 83.17377729297455, 92.84443504902427, 82.30108369249913, 114.13496351310697, 90.93439065932867, 101.42680833946697, 104.32330763816012, 100.60393094370124, 109.77327452792822, 109.14599390821184, 81.61008441899024, 92.92130939108121, 101.06077471046132, 62.81394761105256, 103.96234809513577, 116.37492718871324, 110.53040292486004, 104.42948559384715, 114.42499102798742, 106.42431631643748, 113.90547423606466, 101.53488164393056, 112.7185339033193, 101.71583157726857, 104.16868981007089, 114.76438932242037, 96.77066508789515, 94.50221642811623, 99.14999401452883, 111.54183109605682, 111.61336152402117, 104.41454549108686, 114.2754207373432, 98.61344685729367, 98.44038186558869, 89.32288460868546, 108.3539291053838, 115.17604915636583, 104.31227873206801, 114.28069333503237, 99.05650512759058, 88.91270701289872, 108.84492179372165, 99.71641108132701, 116.95158726962272, 98.44588059515783, 113.79641208369232, 82.92280054794843, 115.45286372351018, 75.8668729804157, 116.82097738781391, 113.80025072342228, 102.15151698959345, 113.04077142143795, 80.8488476642904, 107.37330476889497, 91.96221063926649, 101.45045669217801, 111.92645805686804, 103.49894253969072, 101.95320076453542, 115.6850689585937, 99.381130507266, 114.35605518588525, 100.94472232967316, 113.95235085578297]
Elapsed: 1.3125804625361437~0.15762530895221552
Time per graph: 0.010175042345241423~0.0012219016197846163
Speed: 99.55896813410789~10.79694373884798
Total Time: 1.1328
best val loss: 0.203546237518159 test_score: 0.9147

Testing...
Test loss: 0.3152 score: 0.8992 time: 1.18s
test Score 0.8992
Epoch Time List: [5.833448173711076, 3.984576453221962, 4.066681918222457, 4.445195639971644, 4.604470815043896, 4.893421276006848, 8.638216018909588, 3.9618758109863847, 3.896271134959534, 3.87333961087279, 3.742987170116976, 3.7719827382825315, 3.742806263966486, 3.6065464448183775, 3.785075943917036, 4.138352988054976, 3.835627819877118, 4.509542111074552, 3.9580085752531886, 4.724141204962507, 4.09422349371016, 4.550104948924854, 4.391774018760771, 4.486060467781499, 4.487080877413973, 4.52967137680389, 4.889423510292545, 4.52594668394886, 3.9845819268375635, 4.719668237026781, 4.811915077269077, 4.740033771144226, 4.488925870275125, 4.116564202588052, 4.18974828789942, 4.092173795914277, 3.9870903128758073, 4.235221538692713, 8.16445699473843, 4.038144581019878, 3.8347212730441242, 3.9211358288303018, 4.014673670055345, 3.9532157161738724, 4.11133376811631, 4.029833317035809, 3.9223433919250965, 4.573753071948886, 4.500060840975493, 4.137092796852812, 4.158973869169131, 4.559569382108748, 4.359155646292493, 3.6799578808713704, 4.221883582882583, 4.383619510801509, 4.3610912340227515, 3.855075262952596, 4.0005756102036685, 3.9024628959596157, 4.200511948904023, 4.054004296660423, 4.307426766958088, 4.0047980500385165, 4.186591337900609, 4.020484821638092, 4.285186409018934, 4.0998677760362625, 3.8015135729219764, 8.982987635070458, 3.884798824088648, 3.9713306489866227, 3.9399593551643193, 3.842902211006731, 3.987280906178057, 3.9981539307627827, 3.961822328856215, 4.899161732988432, 3.9312721479218453, 3.948146545095369, 3.8170804022811353, 4.1224473242182285, 4.574128988431767, 3.868603458162397, 4.219321062089875, 3.998730313964188, 3.991292526014149, 4.215740439016372, 4.4426161490846425, 4.501155967125669, 4.738612061133608, 4.89793567196466, 4.118828496197239, 4.692795681999996, 3.9180654159281403, 4.030500850873068, 4.346858602715656, 4.642613753909245, 4.070245401002467, 4.560760854044929, 4.099436928983778, 4.49853958026506, 8.797999254893512, 3.9845318829175085, 3.7993693097960204, 3.8979078643023968, 3.883349322946742, 3.752375311218202, 3.858248701086268, 3.7893161738757044, 3.9651487530209124, 3.8821986878756434, 3.993018431123346, 3.9502831059508026, 3.962895848089829, 4.150701103033498, 4.302397115854546, 4.531574614113197, 4.529585235984996, 3.8708030020352453, 3.9660071569960564, 3.7839029899332672, 4.1605135491117835, 4.505986889125779, 4.081027275882661, 3.9973035003058612, 4.37943481025286, 3.986074737040326, 3.9337587277404964, 4.050911539932713, 4.830813016043976, 3.960869668284431, 4.037560117896646, 3.7877979748882353, 4.13753645401448, 3.955022383015603, 4.452583944890648, 3.779862000141293, 8.533538688905537, 3.7824012879282236, 3.7791576681192964, 3.896714575123042, 3.8660861228127033, 4.3465168280527, 4.165792458923534, 4.221031506778672, 4.301379862008616, 3.8853571719955653, 3.9480488528497517, 3.942826973972842, 3.757164233131334, 3.99447251111269, 3.792555662803352, 4.3821495489683, 3.83444986701943]
Total Epoch List: [69, 86]
Total Time List: [1.312614191090688, 1.1327760219573975]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a073bca0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 1.46s
Epoch 2/1000, LR 0.000020
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 1.56s
Epoch 3/1000, LR 0.000050
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 1.41s
Epoch 4/1000, LR 0.000080
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 1.28s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 1.35s
Epoch 6/1000, LR 0.000140
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 1.51s
Epoch 7/1000, LR 0.000170
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 1.26s
Epoch 8/1000, LR 0.000200
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 1.56s
Epoch 9/1000, LR 0.000230
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 1.28s
Epoch 10/1000, LR 0.000260
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 1.67s
Epoch 11/1000, LR 0.000290
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 1.26s
Epoch 12/1000, LR 0.000290
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 1.37s
Epoch 13/1000, LR 0.000290
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 1.26s
Test loss: 0.6917 score: 0.5078 time: 1.23s
Epoch 14/1000, LR 0.000290
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 1.51s
Val loss: 0.6912 score: 0.5581 time: 1.10s
Test loss: 0.6913 score: 0.6094 time: 1.73s
Epoch 15/1000, LR 0.000290
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 1.76s
Val loss: 0.6909 score: 0.5581 time: 1.39s
Test loss: 0.6910 score: 0.6094 time: 1.42s
Epoch 16/1000, LR 0.000290
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 1.36s
Val loss: 0.6904 score: 0.5581 time: 1.28s
Test loss: 0.6905 score: 0.6172 time: 1.22s
Epoch 17/1000, LR 0.000290
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.51s
Val loss: 0.6897 score: 0.6202 time: 1.13s
Test loss: 0.6897 score: 0.6484 time: 1.37s
Epoch 18/1000, LR 0.000290
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 1.54s
Val loss: 0.6887 score: 0.6589 time: 1.55s
Test loss: 0.6887 score: 0.6641 time: 1.48s
Epoch 19/1000, LR 0.000290
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 1.47s
Val loss: 0.6871 score: 0.7674 time: 1.14s
Test loss: 0.6872 score: 0.7031 time: 1.34s
Epoch 20/1000, LR 0.000290
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 1.83s
Val loss: 0.6850 score: 0.8372 time: 1.22s
Test loss: 0.6851 score: 0.7812 time: 5.73s
Epoch 21/1000, LR 0.000290
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.34s
Val loss: 0.6824 score: 0.8372 time: 1.22s
Test loss: 0.6826 score: 0.7812 time: 1.22s
Epoch 22/1000, LR 0.000290
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 1.47s
Val loss: 0.6796 score: 0.7829 time: 1.09s
Test loss: 0.6797 score: 0.7344 time: 1.31s
Epoch 23/1000, LR 0.000290
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 1.31s
Val loss: 0.6753 score: 0.8217 time: 1.10s
Test loss: 0.6757 score: 0.7734 time: 1.32s
Epoch 24/1000, LR 0.000290
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 1.54s
Val loss: 0.6692 score: 0.8915 time: 1.10s
Test loss: 0.6705 score: 0.8594 time: 1.27s
Epoch 25/1000, LR 0.000290
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 1.45s
Val loss: 0.6640 score: 0.8915 time: 1.13s
Test loss: 0.6657 score: 0.8672 time: 1.36s
Epoch 26/1000, LR 0.000290
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 1.34s
Val loss: 0.6591 score: 0.8605 time: 1.20s
Test loss: 0.6609 score: 0.7969 time: 1.23s
Epoch 27/1000, LR 0.000290
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 1.66s
Val loss: 0.6522 score: 0.8837 time: 1.14s
Test loss: 0.6539 score: 0.8516 time: 1.24s
Epoch 28/1000, LR 0.000290
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 1.48s
Val loss: 0.6437 score: 0.8992 time: 1.14s
Test loss: 0.6452 score: 0.9062 time: 1.60s
Epoch 29/1000, LR 0.000290
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 1.69s
Val loss: 0.6355 score: 0.9070 time: 1.28s
Test loss: 0.6368 score: 0.8516 time: 1.37s
Epoch 30/1000, LR 0.000290
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 1.46s
Val loss: 0.6268 score: 0.8760 time: 1.13s
Test loss: 0.6276 score: 0.8281 time: 1.35s
Epoch 31/1000, LR 0.000290
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 1.38s
Val loss: 0.6128 score: 0.8682 time: 1.14s
Test loss: 0.6138 score: 0.8281 time: 1.36s
Epoch 32/1000, LR 0.000290
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 1.35s
Val loss: 0.5953 score: 0.8760 time: 1.30s
Test loss: 0.5967 score: 0.8359 time: 1.68s
Epoch 33/1000, LR 0.000290
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 1.67s
Val loss: 0.5745 score: 0.8760 time: 1.42s
Test loss: 0.5768 score: 0.8438 time: 1.72s
Epoch 34/1000, LR 0.000290
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 1.85s
Val loss: 0.5511 score: 0.9147 time: 1.21s
Test loss: 0.5547 score: 0.8594 time: 1.56s
Epoch 35/1000, LR 0.000290
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 1.81s
Val loss: 0.5293 score: 0.9070 time: 1.25s
Test loss: 0.5344 score: 0.8828 time: 1.60s
Epoch 36/1000, LR 0.000290
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 1.35s
Val loss: 0.5070 score: 0.9070 time: 1.23s
Test loss: 0.5144 score: 0.8828 time: 1.25s
Epoch 37/1000, LR 0.000290
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 1.72s
Val loss: 0.4822 score: 0.8992 time: 1.39s
Test loss: 0.4930 score: 0.8984 time: 1.45s
Epoch 38/1000, LR 0.000289
Train loss: 0.4852;  Loss pred: 0.4852; Loss self: 0.0000; time: 1.47s
Val loss: 0.4562 score: 0.9070 time: 1.11s
Test loss: 0.4714 score: 0.8906 time: 1.46s
Epoch 39/1000, LR 0.000289
Train loss: 0.4569;  Loss pred: 0.4569; Loss self: 0.0000; time: 1.53s
Val loss: 0.4287 score: 0.9070 time: 1.24s
Test loss: 0.4497 score: 0.8984 time: 1.49s
Epoch 40/1000, LR 0.000289
Train loss: 0.4262;  Loss pred: 0.4262; Loss self: 0.0000; time: 1.87s
Val loss: 0.4010 score: 0.9070 time: 1.28s
Test loss: 0.4277 score: 0.8984 time: 1.39s
Epoch 41/1000, LR 0.000289
Train loss: 0.4023;  Loss pred: 0.4023; Loss self: 0.0000; time: 1.32s
Val loss: 0.3727 score: 0.9070 time: 1.28s
Test loss: 0.4039 score: 0.8906 time: 1.21s
Epoch 42/1000, LR 0.000289
Train loss: 0.3701;  Loss pred: 0.3701; Loss self: 0.0000; time: 1.67s
Val loss: 0.3448 score: 0.9070 time: 1.15s
Test loss: 0.3809 score: 0.8906 time: 1.53s
Epoch 43/1000, LR 0.000289
Train loss: 0.3392;  Loss pred: 0.3392; Loss self: 0.0000; time: 1.85s
Val loss: 0.3179 score: 0.9147 time: 1.39s
Test loss: 0.3595 score: 0.8984 time: 1.72s
Epoch 44/1000, LR 0.000289
Train loss: 0.3114;  Loss pred: 0.3114; Loss self: 0.0000; time: 1.70s
Val loss: 0.2912 score: 0.9147 time: 1.54s
Test loss: 0.3398 score: 0.8906 time: 1.55s
Epoch 45/1000, LR 0.000289
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 1.73s
Val loss: 0.2690 score: 0.9225 time: 1.12s
Test loss: 0.3265 score: 0.8750 time: 1.42s
Epoch 46/1000, LR 0.000289
Train loss: 0.2639;  Loss pred: 0.2639; Loss self: 0.0000; time: 1.35s
Val loss: 0.2526 score: 0.9147 time: 1.26s
Test loss: 0.3211 score: 0.8672 time: 1.22s
Epoch 47/1000, LR 0.000289
Train loss: 0.2532;  Loss pred: 0.2532; Loss self: 0.0000; time: 1.54s
Val loss: 0.2427 score: 0.9070 time: 1.07s
Test loss: 0.3229 score: 0.8594 time: 1.44s
Epoch 48/1000, LR 0.000289
Train loss: 0.2385;  Loss pred: 0.2385; Loss self: 0.0000; time: 1.35s
Val loss: 0.2303 score: 0.9147 time: 1.27s
Test loss: 0.3196 score: 0.8672 time: 1.26s
Epoch 49/1000, LR 0.000289
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 1.56s
Val loss: 0.2195 score: 0.9070 time: 1.09s
Test loss: 0.3143 score: 0.8828 time: 1.32s
Epoch 50/1000, LR 0.000289
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 1.35s
Val loss: 0.2150 score: 0.9070 time: 1.21s
Test loss: 0.3115 score: 0.8828 time: 1.33s
Epoch 51/1000, LR 0.000289
Train loss: 0.1917;  Loss pred: 0.1917; Loss self: 0.0000; time: 1.46s
Val loss: 0.2148 score: 0.9070 time: 1.16s
Test loss: 0.3145 score: 0.8906 time: 1.27s
Epoch 52/1000, LR 0.000289
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 1.79s
Val loss: 0.2137 score: 0.8992 time: 1.20s
Test loss: 0.3202 score: 0.8906 time: 1.32s
Epoch 53/1000, LR 0.000289
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 1.50s
Val loss: 0.2088 score: 0.9070 time: 1.37s
Test loss: 0.3321 score: 0.8828 time: 1.23s
Epoch 54/1000, LR 0.000289
Train loss: 0.1924;  Loss pred: 0.1924; Loss self: 0.0000; time: 1.71s
Val loss: 0.2203 score: 0.9147 time: 1.40s
Test loss: 0.3683 score: 0.8750 time: 1.73s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.2072;  Loss pred: 0.2072; Loss self: 0.0000; time: 1.87s
Val loss: 0.2388 score: 0.8992 time: 1.60s
Test loss: 0.3960 score: 0.8594 time: 1.49s
     INFO: Early stopping counter 2 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.2261;  Loss pred: 0.2261; Loss self: 0.0000; time: 1.47s
Val loss: 0.2405 score: 0.8992 time: 1.10s
Test loss: 0.4026 score: 0.8594 time: 1.25s
     INFO: Early stopping counter 3 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.2337;  Loss pred: 0.2337; Loss self: 0.0000; time: 1.44s
Val loss: 0.2248 score: 0.9147 time: 5.60s
Test loss: 0.3896 score: 0.8750 time: 1.65s
     INFO: Early stopping counter 4 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 1.31s
Val loss: 0.2074 score: 0.9070 time: 1.22s
Test loss: 0.3670 score: 0.8750 time: 1.23s
Epoch 59/1000, LR 0.000288
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 1.44s
Val loss: 0.2047 score: 0.8992 time: 1.14s
Test loss: 0.3524 score: 0.8750 time: 1.34s
Epoch 60/1000, LR 0.000288
Train loss: 0.1584;  Loss pred: 0.1584; Loss self: 0.0000; time: 1.46s
Val loss: 0.2142 score: 0.9225 time: 1.09s
Test loss: 0.3478 score: 0.8906 time: 1.20s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 1.43s
Val loss: 0.2281 score: 0.9225 time: 1.23s
Test loss: 0.3498 score: 0.8984 time: 1.34s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 1.36s
Val loss: 0.2366 score: 0.9225 time: 1.20s
Test loss: 0.3543 score: 0.8906 time: 1.26s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 1.44s
Val loss: 0.2291 score: 0.9225 time: 1.08s
Test loss: 0.3554 score: 0.8984 time: 1.38s
     INFO: Early stopping counter 4 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 1.58s
Val loss: 0.2067 score: 0.9225 time: 1.10s
Test loss: 0.3633 score: 0.8750 time: 1.32s
     INFO: Early stopping counter 5 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 1.34s
Val loss: 0.2043 score: 0.9147 time: 1.20s
Test loss: 0.3828 score: 0.8750 time: 1.33s
Epoch 66/1000, LR 0.000288
Train loss: 0.1603;  Loss pred: 0.1603; Loss self: 0.0000; time: 1.48s
Val loss: 0.2078 score: 0.9070 time: 1.21s
Test loss: 0.3921 score: 0.8672 time: 1.49s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 1.57s
Val loss: 0.2074 score: 0.9070 time: 1.57s
Test loss: 0.3935 score: 0.8672 time: 1.50s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 1.85s
Val loss: 0.2050 score: 0.9147 time: 1.47s
Test loss: 0.3897 score: 0.8750 time: 1.48s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 1.61s
Val loss: 0.2055 score: 0.9070 time: 1.49s
Test loss: 0.3834 score: 0.8750 time: 1.52s
     INFO: Early stopping counter 4 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.1422;  Loss pred: 0.1422; Loss self: 0.0000; time: 1.37s
Val loss: 0.2133 score: 0.9225 time: 1.24s
Test loss: 0.3800 score: 0.8828 time: 1.49s
     INFO: Early stopping counter 5 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 1.84s
Val loss: 0.2295 score: 0.9225 time: 1.27s
Test loss: 0.3802 score: 0.8906 time: 1.52s
     INFO: Early stopping counter 6 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 1.64s
Val loss: 0.2393 score: 0.9225 time: 1.55s
Test loss: 0.3837 score: 0.8906 time: 1.57s
     INFO: Early stopping counter 7 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.1372;  Loss pred: 0.1372; Loss self: 0.0000; time: 1.70s
Val loss: 0.2493 score: 0.9302 time: 1.46s
Test loss: 0.3896 score: 0.8906 time: 1.67s
     INFO: Early stopping counter 8 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 1.55s
Val loss: 0.2565 score: 0.9302 time: 1.24s
Test loss: 0.3969 score: 0.8828 time: 1.26s
     INFO: Early stopping counter 9 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 1.49s
Val loss: 0.2473 score: 0.9302 time: 1.41s
Test loss: 0.3974 score: 0.8906 time: 1.56s
     INFO: Early stopping counter 10 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 1.60s
Val loss: 0.2283 score: 0.9225 time: 1.23s
Test loss: 0.3995 score: 0.8828 time: 1.33s
     INFO: Early stopping counter 11 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.1253;  Loss pred: 0.1253; Loss self: 0.0000; time: 1.33s
Val loss: 0.2149 score: 0.9147 time: 1.20s
Test loss: 0.4090 score: 0.8828 time: 1.25s
     INFO: Early stopping counter 12 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 1.45s
Val loss: 0.2169 score: 0.9147 time: 1.10s
Test loss: 0.4136 score: 0.8828 time: 1.31s
     INFO: Early stopping counter 13 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.1106;  Loss pred: 0.1106; Loss self: 0.0000; time: 1.34s
Val loss: 0.2203 score: 0.9147 time: 1.22s
Test loss: 0.4168 score: 0.8828 time: 1.21s
     INFO: Early stopping counter 14 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.1081;  Loss pred: 0.1081; Loss self: 0.0000; time: 1.46s
Val loss: 0.2285 score: 0.9225 time: 1.10s
Test loss: 0.4199 score: 0.8906 time: 1.38s
     INFO: Early stopping counter 15 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.1036;  Loss pred: 0.1036; Loss self: 0.0000; time: 1.61s
Val loss: 0.2484 score: 0.9302 time: 1.12s
Test loss: 0.4279 score: 0.8750 time: 1.34s
     INFO: Early stopping counter 16 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 1.50s
Val loss: 0.2647 score: 0.9302 time: 1.23s
Test loss: 0.4400 score: 0.8906 time: 1.44s
     INFO: Early stopping counter 17 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.1360;  Loss pred: 0.1360; Loss self: 0.0000; time: 1.82s
Val loss: 0.2620 score: 0.9302 time: 1.18s
Test loss: 0.4426 score: 0.8828 time: 1.22s
     INFO: Early stopping counter 18 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 1.77s
Val loss: 0.2384 score: 0.9225 time: 1.44s
Test loss: 0.4394 score: 0.8906 time: 1.48s
     INFO: Early stopping counter 19 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 1.65s
Val loss: 0.2207 score: 0.9147 time: 1.48s
Test loss: 0.4523 score: 0.8828 time: 1.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 064,   Train_Loss: 0.1381,   Val_Loss: 0.2043,   Val_Precision: 0.9500,   Val_Recall: 0.8769,   Val_accuracy: 0.9120,   Val_Score: 0.9147,   Val_Loss: 0.2043,   Test_Precision: 0.9286,   Test_Recall: 0.8125,   Test_accuracy: 0.8667,   Test_Score: 0.8750,   Test_loss: 0.3828


[1.44611028698273, 1.2402159590274096, 1.3296262258663774, 1.3241270540747792, 1.4912268400657922, 1.5824488799553365, 1.205544576048851, 1.2975220610387623, 1.3349636031780392, 1.3536454278510064, 1.1616179589182138, 1.1818027279805392, 1.2681487977970392, 1.1346176587976515, 1.3411852070130408, 1.4393572290427983, 1.2239101100713015, 1.3899476630613208, 1.268316422821954, 1.3512980230152607, 1.184850461082533, 1.5179957810323685, 1.5563397039659321, 1.242630867054686, 1.5991378859616816, 1.538215802051127, 1.7482750050257891, 1.1997576209250838, 1.3397998248692602, 1.7470801379531622, 1.4865795460063964, 1.6837548408657312, 1.362330748932436, 1.302479563979432, 1.4848536842036992, 1.2747622330207378, 1.2470481200143695, 1.3955301519017667, 1.2271418650634587, 1.363150330958888, 1.2123550809919834, 1.2534846980124712, 1.3546441940125078, 1.321333914063871, 1.3893555998802185, 1.3044607199262828, 1.2459686789661646, 1.421663640998304, 1.563937318045646, 1.4461701659020036, 1.2315050200559199, 1.5962572691496462, 1.3136053150519729, 1.1834247908554971, 1.5872141649015248, 1.5978830088861287, 1.3895936040207744, 1.2155874138697982, 1.3635227070190012, 1.2073028369341046, 1.4698795250151306, 1.331852009985596, 1.2134974880609661, 1.3590291088912636, 1.2112045628018677, 1.3999920340720564, 1.576962191844359, 1.3491092720068991, 1.3121223791968077, 1.382732065860182, 1.124884343938902, 1.1796095839235932, 1.2440761511679739, 1.1765136260073632, 1.2698128810152411, 1.2633620121050626, 1.238640618044883, 1.364172762958333, 1.1273891560267657, 1.2803331969771534, 1.1545468717813492, 1.4326218839269131, 1.2149654228705913, 1.1628007169347256, 1.2548773509915918, 1.3060854321811348, 1.353934507118538, 1.2049151500687003, 1.355244108941406, 1.550969598814845, 1.3894209160935134, 1.567415569908917, 1.130240865983069, 1.4186052060686052, 1.2718530939891934, 1.2365405480377376, 1.282256058882922, 1.1751494209747761, 1.1819032048806548, 1.5806870060041547, 1.3882714400533587, 1.2764596389606595, 2.053684012964368, 1.240833843825385, 1.1084861929994076, 1.167099699145183, 1.2352833040058613, 1.1273760988842696, 1.2121289989445359, 1.132518001133576, 1.2704993388615549, 1.144443558063358, 1.2682391521520913, 1.2383759480435401, 1.1240420548710972, 1.3330486039631069, 1.3650473488960415, 1.3010590800549835, 1.1565167859662324, 1.1557756010442972, 1.2354600538965315, 1.1288516740314662, 1.3081380289513618, 1.310437825974077, 1.4441987690515816, 1.1905428909230977, 1.1200245271902531, 1.2366712871007621, 1.1287995919119567, 1.302287011174485, 1.4508612360805273, 1.1851724258158356, 1.2936687010806054, 1.1030205148272216, 1.3103646310046315, 1.1336034031119198, 1.5556638119742274, 1.1173391100019217, 1.7003468698821962, 1.1042537298053503, 1.1335651651024818, 1.2628299980424345, 1.1411811718717217, 1.595570051111281, 1.201415941119194, 1.4027500981464982, 1.2715566218830645, 1.1525425019208342, 1.2463895459659398, 1.265286416048184, 1.115096366032958, 1.2980331310536712, 1.128055701032281, 1.2779271369799972, 1.1320521167945117, 1.4645435439888388, 1.5684748119674623, 1.4150134010706097, 1.2848986610770226, 1.3580503950361162, 1.5190128518734127, 1.2617300529964268, 1.5695931389927864, 1.2875389128457755, 1.679629211081192, 1.2677745539695024, 1.3755476889200509, 1.232769237132743, 1.7421656851656735, 1.4243714241310954, 1.2294345828704536, 1.3777614219579846, 1.481423120945692, 1.3466746949125081, 5.733032983960584, 1.2278686501085758, 1.3120889491401613, 1.3240605951286852, 1.271833020960912, 1.3653782960027456, 1.2312618840951473, 1.2479443971533328, 1.6080945311114192, 1.375652004033327, 1.3527463688515127, 1.3612709869630635, 1.684819569112733, 1.726660112151876, 1.5716883460991085, 1.6004556769039482, 1.2552256670314819, 1.4526237461250275, 1.4634415130130947, 1.496895574964583, 1.3980145130772144, 1.2191275411751121, 1.5399997308850288, 1.7246877851430327, 1.5526527820620686, 1.4224777589552104, 1.223032323177904, 1.44096125010401, 1.2651829030364752, 1.3285628999583423, 1.332305563846603, 1.278330302098766, 1.3276238508988172, 1.231901080114767, 1.738948345882818, 1.4933397539425641, 1.2563285389915109, 1.6546868421137333, 1.2352069881744683, 1.3402079979423434, 1.2094663958996534, 1.3407038669101894, 1.2621229379437864, 1.388022965984419, 1.3280105891171843, 1.3370619060005993, 1.492956514004618, 1.5140455469954759, 1.4911350118927658, 1.5227399480063468, 1.4947020919062197, 1.5297458679415286, 1.58181191701442, 1.6776518251281232, 1.2696914079133421, 1.5689114518463612, 1.339868419105187, 1.2527802889235318, 1.3191859819926322, 1.2133647769223899, 1.388755006948486, 1.350105470046401, 1.4509250822011381, 1.229747265111655, 1.48792361211963, 1.2561744570266455]
[0.011210157263432016, 0.009614077201762865, 0.010307180045475794, 0.010264550806781235, 0.01155989798500614, 0.01226704558104912, 0.009345306791076365, 0.01005831055068808, 0.010348555063395652, 0.010493375409697725, 0.00900479037921096, 0.009161261457213483, 0.009830610835635963, 0.008795485727113578, 0.010396784550488689, 0.011157807977075955, 0.009487675271870555, 0.010774788085746672, 0.009831910254433751, 0.010475178472986516, 0.009184887295213435, 0.011767409155289678, 0.012064648867952962, 0.009632797419028572, 0.01239641772063319, 0.011924153504272303, 0.013552519418804566, 0.00930044667383786, 0.010386045154025273, 0.013543256883357847, 0.011523872449661987, 0.013052363107486288, 0.010560703480096405, 0.010096740806042108, 0.011510493675997668, 0.009881877775354556, 0.009667039690033873, 0.01081806319303695, 0.009512727636150842, 0.010567056829138666, 0.009398101403038631, 0.009716935643507528, 0.010501117783042696, 0.010242898558634658, 0.010770198448683864, 0.010112098604079712, 0.009658671929970268, 0.011020648379831813, 0.012123545101129039, 0.011210621441100804, 0.009546550543069146, 0.012374087357749195, 0.010182986938387387, 0.00917383558802711, 0.012303985774430424, 0.012386689991365338, 0.010772043442021506, 0.009423158247052699, 0.010569943465263576, 0.009358936720419415, 0.011394414922597912, 0.010324434185934853, 0.009406957271790434, 0.010535109371250106, 0.00938918265737882, 0.010852651426915166, 0.012224513115072552, 0.010458211410906195, 0.010171491311603161, 0.010718853223722341, 0.008720033673944977, 0.00914426034049297, 0.009644001171844758, 0.009120260666723745, 0.009843510705544505, 0.009793503969806687, 0.009601865256161884, 0.010574982658591728, 0.008739450821912912, 0.009925063542458553, 0.008949975750243018, 0.011105595999433435, 0.009418336611399932, 0.009013959046005625, 0.009727731403035596, 0.010124693272721976, 0.010495616334252233, 0.009340427519912405, 0.01050576828636749, 0.012023020145851512, 0.010770704775918708, 0.012150508293867573, 0.008761557100643946, 0.010996939581927172, 0.009859326309993748, 0.009585585643703392, 0.009939969448704821, 0.009109685433912993, 0.009162040347912053, 0.01225338764344306, 0.010761794108940764, 0.009895035960935345, 0.015920031108250913, 0.009618867006398333, 0.00859291622480161, 0.009047284489497542, 0.00957583956593691, 0.008739349603754028, 0.00939634882902741, 0.008779209311112993, 0.0098488320841981, 0.00887165548886324, 0.009831311256992955, 0.009599813550725117, 0.008713504301326336, 0.010333710108241138, 0.010581762394542957, 0.01008572930275181, 0.008965246402839012, 0.008959500783289125, 0.009577209720128151, 0.008750788170786561, 0.010140604875591952, 0.010158432759488968, 0.0111953392949735, 0.009229014658318588, 0.008682360675893435, 0.00958659912481211, 0.00875038443342602, 0.01009524814863942, 0.011246986326205638, 0.009187383145859191, 0.010028439543260506, 0.00855054662656761, 0.010157865356625051, 0.008787623279937363, 0.012059409395149049, 0.00866154348838699, 0.013180983487458886, 0.008560106432599615, 0.00878732686125955, 0.009789379829786313, 0.0088463656734242, 0.01236876008613396, 0.009313301869141038, 0.010874031768577505, 0.009857028076612904, 0.008934437999386311, 0.009661934464852246, 0.009808421829830884, 0.008644157876224481, 0.01006227233374939, 0.008744617837459542, 0.009906411914573622, 0.008775597804608617, 0.011441746437412803, 0.0122537094684958, 0.011054792195864138, 0.010038270789664239, 0.010609768711219658, 0.011867287905261037, 0.009857266039034585, 0.012262446398381144, 0.010058897756607621, 0.013122103211571812, 0.009904488702886738, 0.010746466319687897, 0.009631009665099555, 0.013610669415356824, 0.011127901751024183, 0.009604957678675419, 0.010763761109046754, 0.01157361813238822, 0.01052089605400397, 0.04478932018719206, 0.009592723828973249, 0.01025069491515751, 0.010344223399442853, 0.009936195476257126, 0.01066701793752145, 0.009619233469493338, 0.009749565602760413, 0.012563238524307963, 0.010747281281510368, 0.010568331006652443, 0.010634929585648933, 0.013162652883693227, 0.013489532126186532, 0.012278815203899285, 0.012503559975812095, 0.009806450523683452, 0.011348623016601778, 0.011433136820414802, 0.011694496679410804, 0.010921988383415737, 0.009524433915430564, 0.012031247897539288, 0.013474123321429943, 0.01213009985985991, 0.011113107491837582, 0.009554940024827374, 0.011257509766437579, 0.009884241429972462, 0.01037939765592455, 0.010408637217551586, 0.00998695548514661, 0.01037206133514701, 0.009624227188396617, 0.013585533952209516, 0.011666716827676282, 0.009815066710871179, 0.012927240954013541, 0.009650054595113033, 0.010470374983924557, 0.009448956217966042, 0.010474248960235855, 0.009860335452685831, 0.010843929421753273, 0.010375082727478002, 0.010445796140629682, 0.011663722765661078, 0.011828480835902155, 0.011649492280412233, 0.011896405843799585, 0.011677360093017342, 0.011951139593293192, 0.012357905601675157, 0.013106654883813462, 0.009919464124322985, 0.012257120717549697, 0.010467722024259274, 0.009787346007215092, 0.010306140484317439, 0.00947941231970617, 0.010849648491785047, 0.010547698984737508, 0.011335352204696392, 0.009607400508684805, 0.01162440321968461, 0.009813862945520668]
[89.20481457133882, 104.01414290875854, 97.0197469713297, 97.42267526596041, 86.5059537114478, 81.51922102130777, 107.00558284024221, 99.42027490208989, 96.6318480091144, 95.29822015857968, 111.05200208865101, 109.15527350358617, 101.7230787302657, 113.69468736869531, 96.18358398635819, 89.6233383881968, 105.39989737684644, 92.80924989353996, 101.70963466118344, 95.46376728366097, 108.87449871280782, 84.98047333983288, 82.88678857917499, 103.8120035644688, 80.66846588555596, 83.86339538833597, 73.78701842053567, 107.52171751201998, 96.28303990306017, 73.83748300815402, 86.7763856609964, 76.61447906137717, 94.69066164812644, 99.0418610529824, 86.87724681046969, 101.19534189078963, 103.44428408946526, 92.43798840476826, 105.12232014293562, 94.63372972902913, 106.40447012804906, 102.91310313124903, 95.22795769559022, 97.62861501318007, 92.84879984009966, 98.89144075360889, 103.53390271979951, 90.73876286898295, 82.48412421106697, 89.20112103096821, 104.74987750690809, 80.81404075216555, 98.203013128716, 109.00565967250523, 81.27447628216166, 80.73181783810622, 92.83289706194665, 106.12153311898079, 94.60788539563524, 106.84974478117698, 87.76229466743004, 96.85760807718805, 106.30429915938898, 94.92070416743468, 106.50554329285679, 92.14338143395499, 81.80284896312332, 95.61864459511351, 98.31400031372458, 93.29356220559661, 114.67845622981363, 109.35821627603504, 103.69140175132459, 109.64598891878254, 101.58977116129259, 102.10849998968642, 104.14643127368007, 94.56280282290034, 114.42366578602905, 100.75502244616243, 111.73214631032349, 90.04469458919775, 106.1758611164526, 110.93904408664162, 102.79889098170867, 98.76842419455882, 95.27787298555494, 107.06148063010482, 95.18580390713753, 83.17377729297455, 92.84443504902427, 82.30108369249913, 114.13496351310697, 90.93439065932867, 101.42680833946697, 104.32330763816012, 100.60393094370124, 109.77327452792822, 109.14599390821184, 81.61008441899024, 92.92130939108121, 101.06077471046132, 62.81394761105256, 103.96234809513577, 116.37492718871324, 110.53040292486004, 104.42948559384715, 114.42499102798742, 106.42431631643748, 113.90547423606466, 101.53488164393056, 112.7185339033193, 101.71583157726857, 104.16868981007089, 114.76438932242037, 96.77066508789515, 94.50221642811623, 99.14999401452883, 111.54183109605682, 111.61336152402117, 104.41454549108686, 114.2754207373432, 98.61344685729367, 98.44038186558869, 89.32288460868546, 108.3539291053838, 115.17604915636583, 104.31227873206801, 114.28069333503237, 99.05650512759058, 88.91270701289872, 108.84492179372165, 99.71641108132701, 116.95158726962272, 98.44588059515783, 113.79641208369232, 82.92280054794843, 115.45286372351018, 75.8668729804157, 116.82097738781391, 113.80025072342228, 102.15151698959345, 113.04077142143795, 80.8488476642904, 107.37330476889497, 91.96221063926649, 101.45045669217801, 111.92645805686804, 103.49894253969072, 101.95320076453542, 115.6850689585937, 99.381130507266, 114.35605518588525, 100.94472232967316, 113.95235085578297, 87.39924499027083, 81.60794105417571, 90.4585072502877, 99.61875117272545, 94.25276150860067, 84.2652515033934, 101.4480075956172, 81.54979581660128, 99.4144710679763, 76.2072957266594, 100.96432334851798, 93.05384395687031, 103.83127364348493, 73.47177199614494, 89.86420103034811, 104.1129001765582, 92.90432868855825, 86.40340372053123, 95.04893830972001, 22.32675101610405, 104.24567806066344, 97.55436175564242, 96.67231278607736, 100.64214239640653, 93.74691276017076, 103.95838745066573, 102.5686723639121, 79.59731068268357, 93.0467877229938, 94.62232015353517, 94.02977160746109, 75.97252687859503, 74.13155553844241, 81.4410823352434, 79.97722264175015, 101.97369553692346, 88.11641716683256, 87.46506017617301, 85.51030689166713, 91.55842003260403, 104.9931165336658, 83.11689764155942, 74.21633127029112, 82.43955215151452, 89.9838322210494, 104.6579044349435, 88.82959204542163, 101.17114268047438, 96.34470449537127, 96.07405648779341, 100.13061553015623, 96.41285060775496, 103.90444660384193, 73.60770680915067, 85.71391718600364, 101.88417760751426, 77.35602697879074, 103.626356736512, 95.50756315178074, 105.83179527264836, 95.47223899263537, 101.41642794998548, 92.21749433318587, 96.38477362224197, 95.73229139619406, 85.73591983376689, 84.54171029002899, 85.84065089956124, 84.05900178003769, 85.63579370974142, 83.67402892366731, 80.9198607136509, 76.29711843828177, 100.81189744393083, 81.58522894925919, 95.5317687728494, 102.17274420080932, 97.02953317215805, 105.49177167039787, 92.16888461936468, 94.80740789502974, 88.21957906042711, 104.08642786319044, 86.02592159798908, 101.89667468878083]
Elapsed: 1.363752547139302~0.324730154721967
Time per graph: 0.01060297785644271~0.002542342627295434
Speed: 96.5843021106738~11.86503532729175
Total Time: 1.2570
best val loss: 0.2043018618150968 test_score: 0.8750

Testing...
Test loss: 0.3896 score: 0.8906 time: 1.57s
test Score 0.8906
Epoch Time List: [5.833448173711076, 3.984576453221962, 4.066681918222457, 4.445195639971644, 4.604470815043896, 4.893421276006848, 8.638216018909588, 3.9618758109863847, 3.896271134959534, 3.87333961087279, 3.742987170116976, 3.7719827382825315, 3.742806263966486, 3.6065464448183775, 3.785075943917036, 4.138352988054976, 3.835627819877118, 4.509542111074552, 3.9580085752531886, 4.724141204962507, 4.09422349371016, 4.550104948924854, 4.391774018760771, 4.486060467781499, 4.487080877413973, 4.52967137680389, 4.889423510292545, 4.52594668394886, 3.9845819268375635, 4.719668237026781, 4.811915077269077, 4.740033771144226, 4.488925870275125, 4.116564202588052, 4.18974828789942, 4.092173795914277, 3.9870903128758073, 4.235221538692713, 8.16445699473843, 4.038144581019878, 3.8347212730441242, 3.9211358288303018, 4.014673670055345, 3.9532157161738724, 4.11133376811631, 4.029833317035809, 3.9223433919250965, 4.573753071948886, 4.500060840975493, 4.137092796852812, 4.158973869169131, 4.559569382108748, 4.359155646292493, 3.6799578808713704, 4.221883582882583, 4.383619510801509, 4.3610912340227515, 3.855075262952596, 4.0005756102036685, 3.9024628959596157, 4.200511948904023, 4.054004296660423, 4.307426766958088, 4.0047980500385165, 4.186591337900609, 4.020484821638092, 4.285186409018934, 4.0998677760362625, 3.8015135729219764, 8.982987635070458, 3.884798824088648, 3.9713306489866227, 3.9399593551643193, 3.842902211006731, 3.987280906178057, 3.9981539307627827, 3.961822328856215, 4.899161732988432, 3.9312721479218453, 3.948146545095369, 3.8170804022811353, 4.1224473242182285, 4.574128988431767, 3.868603458162397, 4.219321062089875, 3.998730313964188, 3.991292526014149, 4.215740439016372, 4.4426161490846425, 4.501155967125669, 4.738612061133608, 4.89793567196466, 4.118828496197239, 4.692795681999996, 3.9180654159281403, 4.030500850873068, 4.346858602715656, 4.642613753909245, 4.070245401002467, 4.560760854044929, 4.099436928983778, 4.49853958026506, 8.797999254893512, 3.9845318829175085, 3.7993693097960204, 3.8979078643023968, 3.883349322946742, 3.752375311218202, 3.858248701086268, 3.7893161738757044, 3.9651487530209124, 3.8821986878756434, 3.993018431123346, 3.9502831059508026, 3.962895848089829, 4.150701103033498, 4.302397115854546, 4.531574614113197, 4.529585235984996, 3.8708030020352453, 3.9660071569960564, 3.7839029899332672, 4.1605135491117835, 4.505986889125779, 4.081027275882661, 3.9973035003058612, 4.37943481025286, 3.986074737040326, 3.9337587277404964, 4.050911539932713, 4.830813016043976, 3.960869668284431, 4.037560117896646, 3.7877979748882353, 4.13753645401448, 3.955022383015603, 4.452583944890648, 3.779862000141293, 8.533538688905537, 3.7824012879282236, 3.7791576681192964, 3.896714575123042, 3.8660861228127033, 4.3465168280527, 4.165792458923534, 4.221031506778672, 4.301379862008616, 3.8853571719955653, 3.9480488528497517, 3.942826973972842, 3.757164233131334, 3.99447251111269, 3.792555662803352, 4.3821495489683, 3.83444986701943, 4.153654061025009, 4.7629227850120515, 4.482732247794047, 3.9028376669157296, 4.003977908287197, 4.707121515180916, 4.320618191035464, 4.105876351008192, 3.8670518416911364, 4.885462194215506, 4.330331620993093, 3.9543014320079237, 3.874938137130812, 4.338618042180315, 4.567991344956681, 3.859508786117658, 4.009289175737649, 4.563475921982899, 3.9457363239489496, 8.77642909414135, 3.7766600269824266, 3.8648151860106736, 3.730900692055002, 3.906662874156609, 3.938841789960861, 3.765173827763647, 4.048158424906433, 4.221987671917304, 4.339661799836904, 3.938596189254895, 3.8749077199026942, 4.320946481078863, 4.805714100133628, 4.620266605168581, 4.6623623601626605, 3.8277360151987523, 4.564825884299353, 4.036515463609248, 4.254968094173819, 4.545588436769322, 3.814010466216132, 4.351104842033237, 4.950915240217, 4.783830208936706, 4.269129097927362, 3.8274107028264552, 4.042392296018079, 3.884085597936064, 3.971469479147345, 3.8940252161119133, 3.8825182132422924, 4.319861383875832, 4.0923089541029185, 4.8382955570705235, 4.961024549556896, 3.8176833549514413, 8.691214028047398, 3.765029480913654, 3.910599877126515, 3.7591680681798607, 3.994569865986705, 3.8137188551481813, 3.903733813902363, 3.9988044891506433, 3.8683134210295975, 4.17164929700084, 4.645859363954514, 4.803050181828439, 4.619734257925302, 4.098121372284368, 4.630298852920532, 4.76264242711477, 4.829704306321219, 4.059003407135606, 4.45603015483357, 4.166904123267159, 3.779026350006461, 3.8556762088555843, 3.759468977106735, 3.946824000682682, 4.0789618589915335, 4.173069827957079, 4.230668776901439, 4.695006453897804, 4.379228286910802]
Total Epoch List: [69, 86, 85]
Total Time List: [1.312614191090688, 1.1327760219573975, 1.2570252779405564]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a0655930>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.33s
Epoch 2/1000, LR 0.000015
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 1.42s
Epoch 3/1000, LR 0.000045
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 1.73s
Epoch 4/1000, LR 0.000075
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 1.26s
Epoch 5/1000, LR 0.000105
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 1.38s
Epoch 6/1000, LR 0.000135
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 1.42s
Epoch 7/1000, LR 0.000165
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 5.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 1.23s
Epoch 8/1000, LR 0.000195
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 1.40s
Epoch 9/1000, LR 0.000225
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5039 time: 1.21s
Epoch 10/1000, LR 0.000255
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 1.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5039 time: 1.42s
Epoch 11/1000, LR 0.000285
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5039 time: 1.28s
Epoch 12/1000, LR 0.000285
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5039 time: 1.39s
Epoch 13/1000, LR 0.000285
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.4961 time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.5039 time: 1.28s
Epoch 14/1000, LR 0.000285
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5039 time: 1.70s
Epoch 15/1000, LR 0.000285
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.4961 time: 1.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6828 score: 0.5039 time: 1.33s
Epoch 16/1000, LR 0.000285
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.4961 time: 1.44s
Test loss: 0.6802 score: 0.5116 time: 1.55s
Epoch 17/1000, LR 0.000285
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 1.59s
Val loss: 0.6805 score: 0.5039 time: 1.38s
Test loss: 0.6770 score: 0.5426 time: 1.69s
Epoch 18/1000, LR 0.000285
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 1.63s
Val loss: 0.6774 score: 0.5116 time: 1.56s
Test loss: 0.6733 score: 0.5504 time: 1.25s
Epoch 19/1000, LR 0.000285
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 1.41s
Val loss: 0.6738 score: 0.5194 time: 1.20s
Test loss: 0.6687 score: 0.5504 time: 1.59s
Epoch 20/1000, LR 0.000285
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 1.53s
Val loss: 0.6695 score: 0.5194 time: 1.55s
Test loss: 0.6634 score: 0.5736 time: 1.26s
Epoch 21/1000, LR 0.000285
Train loss: 0.6633;  Loss pred: 0.6633; Loss self: 0.0000; time: 1.69s
Val loss: 0.6644 score: 0.5426 time: 1.43s
Test loss: 0.6571 score: 0.5891 time: 1.60s
Epoch 22/1000, LR 0.000285
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 1.36s
Val loss: 0.6584 score: 0.5271 time: 1.35s
Test loss: 0.6499 score: 0.6047 time: 1.50s
Epoch 23/1000, LR 0.000285
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 1.36s
Val loss: 0.6514 score: 0.5504 time: 1.40s
Test loss: 0.6417 score: 0.6124 time: 1.55s
Epoch 24/1000, LR 0.000285
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 1.86s
Val loss: 0.6436 score: 0.5814 time: 1.55s
Test loss: 0.6324 score: 0.6279 time: 1.66s
Epoch 25/1000, LR 0.000285
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 1.52s
Val loss: 0.6346 score: 0.5891 time: 1.39s
Test loss: 0.6218 score: 0.6434 time: 1.26s
Epoch 26/1000, LR 0.000285
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 1.45s
Val loss: 0.6243 score: 0.6047 time: 1.29s
Test loss: 0.6098 score: 0.6434 time: 1.49s
Epoch 27/1000, LR 0.000285
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 1.53s
Val loss: 0.6121 score: 0.6202 time: 1.51s
Test loss: 0.5959 score: 0.6822 time: 1.28s
Epoch 28/1000, LR 0.000285
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 1.44s
Val loss: 0.5980 score: 0.6357 time: 1.17s
Test loss: 0.5802 score: 0.7132 time: 1.38s
Epoch 29/1000, LR 0.000285
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 1.32s
Val loss: 0.5817 score: 0.6667 time: 1.27s
Test loss: 0.5626 score: 0.7287 time: 1.30s
Epoch 30/1000, LR 0.000285
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 1.62s
Val loss: 0.5636 score: 0.7209 time: 1.46s
Test loss: 0.5433 score: 0.7519 time: 1.71s
Epoch 31/1000, LR 0.000285
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 1.63s
Val loss: 0.5439 score: 0.7907 time: 1.55s
Test loss: 0.5228 score: 0.7829 time: 1.52s
Epoch 32/1000, LR 0.000285
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 1.75s
Val loss: 0.5224 score: 0.8217 time: 1.39s
Test loss: 0.5011 score: 0.8372 time: 1.33s
Epoch 33/1000, LR 0.000285
Train loss: 0.4924;  Loss pred: 0.4924; Loss self: 0.0000; time: 1.83s
Val loss: 0.5007 score: 0.8295 time: 1.44s
Test loss: 0.4790 score: 0.8682 time: 1.69s
Epoch 34/1000, LR 0.000285
Train loss: 0.4732;  Loss pred: 0.4732; Loss self: 0.0000; time: 1.37s
Val loss: 0.4791 score: 0.8605 time: 1.40s
Test loss: 0.4566 score: 0.8915 time: 1.55s
Epoch 35/1000, LR 0.000285
Train loss: 0.4475;  Loss pred: 0.4475; Loss self: 0.0000; time: 1.80s
Val loss: 0.4591 score: 0.8682 time: 1.38s
Test loss: 0.4344 score: 0.8915 time: 1.39s
Epoch 36/1000, LR 0.000285
Train loss: 0.4266;  Loss pred: 0.4266; Loss self: 0.0000; time: 1.39s
Val loss: 0.4412 score: 0.8682 time: 1.25s
Test loss: 0.4131 score: 0.8992 time: 1.35s
Epoch 37/1000, LR 0.000285
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 1.76s
Val loss: 0.4247 score: 0.8682 time: 1.38s
Test loss: 0.3927 score: 0.8992 time: 1.43s
Epoch 38/1000, LR 0.000284
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 1.54s
Val loss: 0.4095 score: 0.8760 time: 1.22s
Test loss: 0.3732 score: 0.8992 time: 1.55s
Epoch 39/1000, LR 0.000284
Train loss: 0.3659;  Loss pred: 0.3659; Loss self: 0.0000; time: 1.40s
Val loss: 0.3979 score: 0.8760 time: 1.63s
Test loss: 0.3556 score: 0.9070 time: 1.60s
Epoch 40/1000, LR 0.000284
Train loss: 0.3480;  Loss pred: 0.3480; Loss self: 0.0000; time: 1.77s
Val loss: 0.3876 score: 0.8682 time: 5.43s
Test loss: 0.3390 score: 0.9070 time: 2.33s
Epoch 41/1000, LR 0.000284
Train loss: 0.3258;  Loss pred: 0.3258; Loss self: 0.0000; time: 1.31s
Val loss: 0.3763 score: 0.8760 time: 1.13s
Test loss: 0.3221 score: 0.9070 time: 1.33s
Epoch 42/1000, LR 0.000284
Train loss: 0.3044;  Loss pred: 0.3044; Loss self: 0.0000; time: 1.32s
Val loss: 0.3647 score: 0.8837 time: 1.23s
Test loss: 0.3052 score: 0.9070 time: 1.26s
Epoch 43/1000, LR 0.000284
Train loss: 0.2896;  Loss pred: 0.2896; Loss self: 0.0000; time: 1.39s
Val loss: 0.3515 score: 0.8915 time: 1.11s
Test loss: 0.2885 score: 0.9070 time: 1.34s
Epoch 44/1000, LR 0.000284
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 1.31s
Val loss: 0.3426 score: 0.8915 time: 1.25s
Test loss: 0.2737 score: 0.9147 time: 1.25s
Epoch 45/1000, LR 0.000284
Train loss: 0.2605;  Loss pred: 0.2605; Loss self: 0.0000; time: 1.41s
Val loss: 0.3369 score: 0.8992 time: 1.15s
Test loss: 0.2605 score: 0.9147 time: 1.35s
Epoch 46/1000, LR 0.000284
Train loss: 0.2414;  Loss pred: 0.2414; Loss self: 0.0000; time: 1.30s
Val loss: 0.3308 score: 0.8915 time: 1.37s
Test loss: 0.2485 score: 0.9147 time: 1.30s
Epoch 47/1000, LR 0.000284
Train loss: 0.2155;  Loss pred: 0.2155; Loss self: 0.0000; time: 1.45s
Val loss: 0.3285 score: 0.8992 time: 1.26s
Test loss: 0.2387 score: 0.9147 time: 1.43s
Epoch 48/1000, LR 0.000284
Train loss: 0.2026;  Loss pred: 0.2026; Loss self: 0.0000; time: 1.51s
Val loss: 0.3323 score: 0.8992 time: 1.21s
Test loss: 0.2306 score: 0.9147 time: 1.42s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.1992;  Loss pred: 0.1992; Loss self: 0.0000; time: 1.30s
Val loss: 0.3421 score: 0.8992 time: 1.45s
Test loss: 0.2245 score: 0.9070 time: 1.61s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.1916;  Loss pred: 0.1916; Loss self: 0.0000; time: 1.61s
Val loss: 0.3534 score: 0.8992 time: 1.14s
Test loss: 0.2199 score: 0.9070 time: 1.36s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1708;  Loss pred: 0.1708; Loss self: 0.0000; time: 1.29s
Val loss: 0.3604 score: 0.8915 time: 1.54s
Test loss: 0.2157 score: 0.9070 time: 1.57s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1674;  Loss pred: 0.1674; Loss self: 0.0000; time: 1.41s
Val loss: 0.3666 score: 0.8915 time: 1.23s
Test loss: 0.2127 score: 0.9070 time: 1.35s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1591;  Loss pred: 0.1591; Loss self: 0.0000; time: 1.32s
Val loss: 0.3740 score: 0.8915 time: 1.40s
Test loss: 0.2108 score: 0.9070 time: 1.41s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1491;  Loss pred: 0.1491; Loss self: 0.0000; time: 1.31s
Val loss: 0.3807 score: 0.8915 time: 1.25s
Test loss: 0.2102 score: 0.9070 time: 1.27s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 1.49s
Val loss: 0.3909 score: 0.8915 time: 1.17s
Test loss: 0.2095 score: 0.9070 time: 1.50s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 1.31s
Val loss: 0.3954 score: 0.8915 time: 1.51s
Test loss: 0.2113 score: 0.9070 time: 1.62s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1264;  Loss pred: 0.1264; Loss self: 0.0000; time: 1.43s
Val loss: 0.4036 score: 0.8915 time: 1.39s
Test loss: 0.2123 score: 0.9070 time: 1.23s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 1.40s
Val loss: 0.4090 score: 0.8915 time: 1.12s
Test loss: 0.2156 score: 0.8992 time: 1.36s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 1.27s
Val loss: 0.4131 score: 0.8915 time: 1.24s
Test loss: 0.2199 score: 0.8992 time: 1.23s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1224;  Loss pred: 0.1224; Loss self: 0.0000; time: 1.39s
Val loss: 0.4242 score: 0.8992 time: 1.16s
Test loss: 0.2194 score: 0.9147 time: 1.38s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 1.48s
Val loss: 0.4317 score: 0.8992 time: 1.45s
Test loss: 0.2216 score: 0.9147 time: 1.36s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 1.40s
Val loss: 0.4369 score: 0.8992 time: 1.29s
Test loss: 0.2248 score: 0.9147 time: 1.30s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 1.47s
Val loss: 0.4389 score: 0.8915 time: 1.19s
Test loss: 0.2296 score: 0.9147 time: 1.42s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 1.30s
Val loss: 0.4354 score: 0.8992 time: 1.27s
Test loss: 0.2384 score: 0.9147 time: 1.26s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 1.46s
Val loss: 0.4323 score: 0.8992 time: 1.18s
Test loss: 0.2480 score: 0.9147 time: 1.35s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0736;  Loss pred: 0.0736; Loss self: 0.0000; time: 1.63s
Val loss: 0.4348 score: 0.8992 time: 1.56s
Test loss: 0.2542 score: 0.9147 time: 1.62s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0822;  Loss pred: 0.0822; Loss self: 0.0000; time: 1.61s
Val loss: 0.4431 score: 0.8992 time: 1.17s
Test loss: 0.2568 score: 0.9225 time: 1.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.2155,   Val_Loss: 0.3285,   Val_Precision: 0.9322,   Val_Recall: 0.8594,   Val_accuracy: 0.8943,   Val_Score: 0.8992,   Val_Loss: 0.3285,   Test_Precision: 0.9655,   Test_Recall: 0.8615,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2387


[1.3330678700003773, 1.4234237431082875, 1.738369552185759, 1.2686544940806925, 1.3874888110440224, 1.426706582074985, 1.232859511859715, 1.4030895109754056, 1.2182617329526693, 1.429677948821336, 1.2929440848529339, 1.392578257014975, 1.2833041520789266, 1.7069565979763865, 1.3402405991218984, 1.5598475239239633, 1.6936344648711383, 1.2499800219666213, 1.599837358109653, 1.2682381020858884, 1.6047646459192038, 1.5043788771145046, 1.5567132190335542, 1.6709745349362493, 1.2626032789703459, 1.4984863551799208, 1.2894554079975933, 1.383400030899793, 1.3020977908745408, 1.714290644042194, 1.5264798200223595, 1.3417074820026755, 1.699239161098376, 1.5545109331142157, 1.3955135631840676, 1.3578996111173183, 1.4335980280302465, 1.5589276489336044, 1.608496607048437, 2.339774162042886, 1.335615849122405, 1.2613908550702035, 1.3457482599187642, 1.250245706178248, 1.3589172679930925, 1.30587408086285, 1.4393103660549968, 1.4268184041138738, 1.6136269879061729, 1.366180449957028, 1.5786538620013744, 1.3575013121590018, 1.4140511699952185, 1.2767994329333305, 1.5020016618072987, 1.627178329974413, 1.2332204428967088, 1.3680908069945872, 1.2346956271212548, 1.3838310989085585, 1.3663001370150596, 1.3045680280774832, 1.4202051779720932, 1.2621343820355833, 1.3554474138654768, 1.6234904029406607, 1.2596747938077897]
[0.010333859457367266, 0.011034292582234786, 0.013475732962680302, 0.009834530961865833, 0.010755727217395523, 0.011059740946317712, 0.009557050479532675, 0.010876662875778337, 0.009443889402733871, 0.011082774797064621, 0.010022822363201038, 0.010795180286937791, 0.009948094202162222, 0.013232221689739431, 0.010389462008696887, 0.012091841270728397, 0.013128949340086344, 0.009689767612144351, 0.012401839985346148, 0.009831303116944872, 0.012440036014877549, 0.011661851760577555, 0.012067544333593444, 0.012953290968498057, 0.00978762231759958, 0.011616173295968379, 0.00999577835657049, 0.010724031247285217, 0.010093781324608843, 0.013289074760017007, 0.011833176899398135, 0.01040083319381919, 0.01317239659766183, 0.012050472349722603, 0.010817934598326105, 0.010526353574552855, 0.011113163007986407, 0.012084710456849647, 0.012468965946111915, 0.018137784201882837, 0.010353611233507016, 0.009778223682714755, 0.010432157053633831, 0.00969182717967634, 0.010534242387543353, 0.01012305489040969, 0.011157444698100752, 0.01106060778382848, 0.012508736340357929, 0.010590546123697892, 0.012237626837219956, 0.010523265985728697, 0.01096163697670712, 0.009897670022738997, 0.0116434237349403, 0.01261378550367762, 0.009559848394548131, 0.010605355092981297, 0.009571283931172517, 0.010727372859756267, 0.0105914739303493, 0.010112930450213048, 0.011009342464899947, 0.00978398745764018, 0.010507344293530828, 0.012585196922020626, 0.009764920882230927]
[96.76926651901339, 90.62656192477624, 74.20746632256666, 101.68253106097063, 92.97372272352477, 90.41803102385913, 104.63479314476724, 91.939964621588, 105.88857591984446, 90.23011098852785, 99.77229604223227, 92.63393231236758, 100.52176624771504, 75.57309901899717, 96.2513746296885, 82.70039091736781, 76.16755721242073, 103.20164941279737, 80.63319645968558, 101.71591579517438, 80.3856193667011, 85.74967513996893, 82.86690086699872, 77.20045835702791, 102.169859803627, 86.08686996319776, 100.04223426409544, 93.24851606089356, 99.0708999769967, 75.24978360485348, 84.50815943188194, 96.1461434257268, 75.91632946866369, 82.98429895347792, 92.43908723156204, 94.99965899088457, 89.98338270403809, 82.74918986025001, 80.19911228579632, 55.1335261721877, 96.58465799485849, 102.26806344875587, 95.85745257273233, 103.17971848455878, 94.92851628157817, 98.7844095310965, 89.62625646445935, 90.41094481824791, 79.94412647212178, 94.42383691265498, 81.71518982410578, 95.02753245581427, 91.22725028432755, 101.0338794587606, 85.88539099535978, 79.2783419147602, 104.60416930568567, 94.29198657023824, 104.47919079519944, 93.21946883672696, 94.41556544217644, 98.88330636931585, 90.83194597572071, 102.20781704080312, 95.17152689244998, 79.45843090069378, 102.4073837423183]
Elapsed: 1.434687239229123~0.18114096074955585
Time per graph: 0.011121606505652118~0.0014041934941826035
Speed: 91.13161594048113~9.85393440852575
Total Time: 1.2602
best val loss: 0.32854131303092304 test_score: 0.9147

Testing...
Test loss: 0.2605 score: 0.9147 time: 1.46s
test Score 0.9147
Epoch Time List: [4.652922890847549, 4.138778166146949, 4.83580164401792, 3.912639813730493, 4.009068435290828, 3.8085116436704993, 7.9120609681122005, 3.9409864868503064, 3.8235652348957956, 4.072058917256072, 3.9113198057748377, 4.011085175909102, 3.841019446030259, 4.42344012320973, 4.517082290025428, 4.417703517014161, 4.656676267972216, 4.438157722121105, 4.209331799997017, 4.337904985994101, 4.72563558164984, 4.202679600799456, 4.3131542860064656, 5.0800639991648495, 4.166837419150397, 4.233520584879443, 4.322031663032249, 3.987868442898616, 3.887837915914133, 4.7912493431940675, 4.696160361170769, 4.470685878768563, 4.967087675118819, 4.318230634089559, 4.578838092973456, 3.997072590980679, 4.562198496889323, 4.31230599200353, 4.629744255915284, 9.533644078997895, 3.771136953961104, 3.8058154799509794, 3.843700709985569, 3.80289413430728, 3.9197561300825328, 3.968889348907396, 4.139833925059065, 4.147966081276536, 4.359186687273905, 4.115180616732687, 4.40298425918445, 3.9929984922055155, 4.129646398825571, 3.8360003859270364, 4.154960800195113, 4.4362591952085495, 4.045982276787981, 3.891302583971992, 3.737466603750363, 3.9263150221668184, 4.291641787160188, 3.9925013850443065, 4.0691952330525964, 3.8292030268348753, 3.9962141611613333, 4.807289110030979, 4.034951844019815]
Total Epoch List: [67]
Total Time List: [1.2601661880034953]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a05b84f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 1.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 1.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 1.34s
Epoch 3/1000, LR 0.000045
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 1.34s
Epoch 4/1000, LR 0.000075
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 1.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 1.28s
Epoch 6/1000, LR 0.000135
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 1.11s
Epoch 7/1000, LR 0.000165
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 5.87s
Epoch 8/1000, LR 0.000195
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 1.22s
Epoch 9/1000, LR 0.000225
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5039 time: 1.35s
Epoch 10/1000, LR 0.000255
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 1.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4961 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5039 time: 1.23s
Epoch 11/1000, LR 0.000285
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 1.62s
Val loss: 0.6900 score: 0.5581 time: 1.09s
Test loss: 0.6885 score: 0.6047 time: 1.41s
Epoch 12/1000, LR 0.000285
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 1.50s
Val loss: 0.6888 score: 0.6512 time: 1.09s
Test loss: 0.6871 score: 0.6977 time: 1.28s
Epoch 13/1000, LR 0.000285
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 1.44s
Val loss: 0.6875 score: 0.7829 time: 1.22s
Test loss: 0.6854 score: 0.7674 time: 1.13s
Epoch 14/1000, LR 0.000285
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 1.60s
Val loss: 0.6858 score: 0.6667 time: 1.09s
Test loss: 0.6833 score: 0.7829 time: 1.28s
Epoch 15/1000, LR 0.000285
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 1.51s
Val loss: 0.6840 score: 0.6124 time: 1.34s
Test loss: 0.6810 score: 0.6977 time: 1.21s
Epoch 16/1000, LR 0.000285
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 1.69s
Val loss: 0.6818 score: 0.5504 time: 1.52s
Test loss: 0.6782 score: 0.6124 time: 1.44s
Epoch 17/1000, LR 0.000285
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 1.78s
Val loss: 0.6792 score: 0.5194 time: 1.09s
Test loss: 0.6749 score: 0.5736 time: 1.29s
Epoch 18/1000, LR 0.000285
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 1.46s
Val loss: 0.6760 score: 0.5194 time: 1.27s
Test loss: 0.6709 score: 0.5504 time: 1.14s
Epoch 19/1000, LR 0.000285
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 1.67s
Val loss: 0.6723 score: 0.5271 time: 1.09s
Test loss: 0.6662 score: 0.5504 time: 1.31s
Epoch 20/1000, LR 0.000285
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 1.48s
Val loss: 0.6679 score: 0.5426 time: 1.25s
Test loss: 0.6606 score: 0.5581 time: 1.12s
Epoch 21/1000, LR 0.000285
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 1.57s
Val loss: 0.6626 score: 0.5504 time: 1.14s
Test loss: 0.6540 score: 0.5504 time: 1.30s
Epoch 22/1000, LR 0.000285
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 1.72s
Val loss: 0.6563 score: 0.5504 time: 1.09s
Test loss: 0.6461 score: 0.5581 time: 1.29s
Epoch 23/1000, LR 0.000285
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 1.45s
Val loss: 0.6490 score: 0.5504 time: 1.11s
Test loss: 0.6371 score: 0.5504 time: 1.26s
Epoch 24/1000, LR 0.000285
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 1.62s
Val loss: 0.6405 score: 0.5504 time: 1.06s
Test loss: 0.6270 score: 0.5581 time: 1.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 1.95s
Val loss: 0.6312 score: 0.5504 time: 1.43s
Test loss: 0.6160 score: 0.5736 time: 1.42s
Epoch 26/1000, LR 0.000285
Train loss: 0.5969;  Loss pred: 0.5969; Loss self: 0.0000; time: 1.74s
Val loss: 0.6217 score: 0.5504 time: 1.33s
Test loss: 0.6049 score: 0.5736 time: 1.39s
Epoch 27/1000, LR 0.000285
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 1.65s
Val loss: 0.6114 score: 0.5349 time: 1.08s
Test loss: 0.5936 score: 0.5736 time: 1.36s
Epoch 28/1000, LR 0.000285
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 2.04s
Val loss: 0.6001 score: 0.5504 time: 1.31s
Test loss: 0.5817 score: 0.5736 time: 1.47s
Epoch 29/1000, LR 0.000285
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 1.94s
Val loss: 0.5877 score: 0.5736 time: 1.30s
Test loss: 0.5692 score: 0.5659 time: 1.15s
Epoch 30/1000, LR 0.000285
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 1.84s
Val loss: 0.5739 score: 0.5814 time: 1.39s
Test loss: 0.5557 score: 0.6202 time: 1.56s
Epoch 31/1000, LR 0.000285
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 1.54s
Val loss: 0.5589 score: 0.6589 time: 1.12s
Test loss: 0.5414 score: 0.6977 time: 1.34s
Epoch 32/1000, LR 0.000285
Train loss: 0.5028;  Loss pred: 0.5028; Loss self: 0.0000; time: 1.98s
Val loss: 0.5425 score: 0.7364 time: 1.17s
Test loss: 0.5261 score: 0.7442 time: 1.14s
Epoch 33/1000, LR 0.000285
Train loss: 0.4858;  Loss pred: 0.4858; Loss self: 0.0000; time: 1.67s
Val loss: 0.5250 score: 0.7597 time: 1.45s
Test loss: 0.5098 score: 0.8295 time: 1.42s
Epoch 34/1000, LR 0.000285
Train loss: 0.4688;  Loss pred: 0.4688; Loss self: 0.0000; time: 1.46s
Val loss: 0.5068 score: 0.8372 time: 1.26s
Test loss: 0.4929 score: 0.8527 time: 1.15s
Epoch 35/1000, LR 0.000285
Train loss: 0.4471;  Loss pred: 0.4471; Loss self: 0.0000; time: 1.71s
Val loss: 0.4887 score: 0.8527 time: 1.40s
Test loss: 0.4763 score: 0.8760 time: 1.20s
Epoch 36/1000, LR 0.000285
Train loss: 0.4262;  Loss pred: 0.4262; Loss self: 0.0000; time: 1.96s
Val loss: 0.4717 score: 0.8605 time: 1.53s
Test loss: 0.4608 score: 0.8837 time: 1.44s
Epoch 37/1000, LR 0.000285
Train loss: 0.4106;  Loss pred: 0.4106; Loss self: 0.0000; time: 1.55s
Val loss: 0.4553 score: 0.8682 time: 1.24s
Test loss: 0.4461 score: 0.8992 time: 1.21s
Epoch 38/1000, LR 0.000284
Train loss: 0.3897;  Loss pred: 0.3897; Loss self: 0.0000; time: 1.73s
Val loss: 0.4395 score: 0.8837 time: 1.38s
Test loss: 0.4325 score: 0.8915 time: 1.60s
Epoch 39/1000, LR 0.000284
Train loss: 0.3743;  Loss pred: 0.3743; Loss self: 0.0000; time: 1.49s
Val loss: 0.4244 score: 0.8837 time: 1.43s
Test loss: 0.4199 score: 0.8915 time: 1.45s
Epoch 40/1000, LR 0.000284
Train loss: 0.3542;  Loss pred: 0.3542; Loss self: 0.0000; time: 1.68s
Val loss: 0.4103 score: 0.8837 time: 1.13s
Test loss: 0.4082 score: 0.8915 time: 1.30s
Epoch 41/1000, LR 0.000284
Train loss: 0.3421;  Loss pred: 0.3421; Loss self: 0.0000; time: 1.55s
Val loss: 0.3968 score: 0.8837 time: 4.91s
Test loss: 0.3978 score: 0.8915 time: 1.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.3269;  Loss pred: 0.3269; Loss self: 0.0000; time: 1.57s
Val loss: 0.3839 score: 0.8837 time: 1.24s
Test loss: 0.3892 score: 0.8915 time: 1.14s
Epoch 43/1000, LR 0.000284
Train loss: 0.3035;  Loss pred: 0.3035; Loss self: 0.0000; time: 1.57s
Val loss: 0.3721 score: 0.8837 time: 1.09s
Test loss: 0.3812 score: 0.8915 time: 1.19s
Epoch 44/1000, LR 0.000284
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 1.57s
Val loss: 0.3611 score: 0.8837 time: 1.20s
Test loss: 0.3735 score: 0.8915 time: 1.24s
Epoch 45/1000, LR 0.000284
Train loss: 0.2770;  Loss pred: 0.2770; Loss self: 0.0000; time: 1.64s
Val loss: 0.3505 score: 0.8837 time: 1.11s
Test loss: 0.3666 score: 0.8915 time: 1.14s
Epoch 46/1000, LR 0.000284
Train loss: 0.2703;  Loss pred: 0.2703; Loss self: 0.0000; time: 1.57s
Val loss: 0.3401 score: 0.8837 time: 1.11s
Test loss: 0.3613 score: 0.8837 time: 1.25s
Epoch 47/1000, LR 0.000284
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 1.46s
Val loss: 0.3303 score: 0.8837 time: 1.21s
Test loss: 0.3573 score: 0.8915 time: 1.14s
Epoch 48/1000, LR 0.000284
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 1.59s
Val loss: 0.3212 score: 0.8837 time: 1.15s
Test loss: 0.3519 score: 0.8837 time: 1.30s
Epoch 49/1000, LR 0.000284
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 1.96s
Val loss: 0.3124 score: 0.8837 time: 1.43s
Test loss: 0.3472 score: 0.8915 time: 1.14s
Epoch 50/1000, LR 0.000284
Train loss: 0.2168;  Loss pred: 0.2168; Loss self: 0.0000; time: 1.64s
Val loss: 0.3046 score: 0.8837 time: 1.10s
Test loss: 0.3436 score: 0.8915 time: 1.29s
Epoch 51/1000, LR 0.000284
Train loss: 0.2042;  Loss pred: 0.2042; Loss self: 0.0000; time: 1.45s
Val loss: 0.2985 score: 0.8760 time: 1.22s
Test loss: 0.3403 score: 0.8915 time: 1.13s
Epoch 52/1000, LR 0.000284
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 1.76s
Val loss: 0.2940 score: 0.8760 time: 1.32s
Test loss: 0.3385 score: 0.8915 time: 1.17s
Epoch 53/1000, LR 0.000284
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 1.79s
Val loss: 0.2908 score: 0.8760 time: 1.40s
Test loss: 0.3385 score: 0.8915 time: 1.58s
Epoch 54/1000, LR 0.000284
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 1.73s
Val loss: 0.2876 score: 0.8760 time: 1.24s
Test loss: 0.3411 score: 0.8915 time: 1.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.1637;  Loss pred: 0.1637; Loss self: 0.0000; time: 1.59s
Val loss: 0.2853 score: 0.8760 time: 1.09s
Test loss: 0.3447 score: 0.8915 time: 1.16s
Epoch 56/1000, LR 0.000284
Train loss: 0.1645;  Loss pred: 0.1645; Loss self: 0.0000; time: 1.61s
Val loss: 0.2829 score: 0.8760 time: 1.10s
Test loss: 0.3496 score: 0.8915 time: 1.39s
Epoch 57/1000, LR 0.000283
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 1.65s
Val loss: 0.2820 score: 0.8760 time: 1.20s
Test loss: 0.3545 score: 0.8915 time: 1.30s
Epoch 58/1000, LR 0.000283
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 1.58s
Val loss: 0.2804 score: 0.8760 time: 1.25s
Test loss: 0.3598 score: 0.8915 time: 1.62s
Epoch 59/1000, LR 0.000283
Train loss: 0.1483;  Loss pred: 0.1483; Loss self: 0.0000; time: 1.85s
Val loss: 0.2806 score: 0.8760 time: 1.35s
Test loss: 0.3642 score: 0.8915 time: 1.13s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1454;  Loss pred: 0.1454; Loss self: 0.0000; time: 1.54s
Val loss: 0.2809 score: 0.8760 time: 1.08s
Test loss: 0.3683 score: 0.8915 time: 1.25s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1421;  Loss pred: 0.1421; Loss self: 0.0000; time: 1.55s
Val loss: 0.2819 score: 0.8760 time: 1.19s
Test loss: 0.3717 score: 0.8915 time: 1.20s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1409;  Loss pred: 0.1409; Loss self: 0.0000; time: 1.81s
Val loss: 0.2810 score: 0.8760 time: 1.36s
Test loss: 0.3739 score: 0.8915 time: 1.15s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 1.59s
Val loss: 0.2807 score: 0.8760 time: 1.11s
Test loss: 0.3750 score: 0.8915 time: 1.23s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 1.52s
Val loss: 0.2792 score: 0.8760 time: 1.24s
Test loss: 0.3754 score: 0.8915 time: 1.27s
Epoch 65/1000, LR 0.000283
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 1.82s
Val loss: 0.2764 score: 0.8915 time: 1.45s
Test loss: 0.3753 score: 0.8992 time: 1.38s
Epoch 66/1000, LR 0.000283
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 1.65s
Val loss: 0.2733 score: 0.8992 time: 1.10s
Test loss: 0.3753 score: 0.9070 time: 1.37s
Epoch 67/1000, LR 0.000283
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 1.79s
Val loss: 0.2705 score: 0.8992 time: 1.23s
Test loss: 0.3762 score: 0.9070 time: 1.30s
Epoch 68/1000, LR 0.000283
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 1.93s
Val loss: 0.2690 score: 0.9070 time: 1.39s
Test loss: 0.3772 score: 0.9070 time: 1.47s
Epoch 69/1000, LR 0.000283
Train loss: 0.1180;  Loss pred: 0.1180; Loss self: 0.0000; time: 1.60s
Val loss: 0.2692 score: 0.9070 time: 1.65s
Test loss: 0.3809 score: 0.8992 time: 1.14s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 1.62s
Val loss: 0.2699 score: 0.9070 time: 1.09s
Test loss: 0.3809 score: 0.8992 time: 1.58s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.1070;  Loss pred: 0.1070; Loss self: 0.0000; time: 1.73s
Val loss: 0.2702 score: 0.9070 time: 1.39s
Test loss: 0.3776 score: 0.8992 time: 1.61s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.1088;  Loss pred: 0.1088; Loss self: 0.0000; time: 1.82s
Val loss: 0.2707 score: 0.9070 time: 1.54s
Test loss: 0.3734 score: 0.8992 time: 1.37s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 1.89s
Val loss: 0.2708 score: 0.8915 time: 1.36s
Test loss: 0.3631 score: 0.8992 time: 1.44s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 1.99s
Val loss: 0.2728 score: 0.8915 time: 1.49s
Test loss: 0.3576 score: 0.8992 time: 1.27s
     INFO: Early stopping counter 6 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 1.43s
Val loss: 0.2746 score: 0.8915 time: 1.18s
Test loss: 0.3554 score: 0.8992 time: 1.13s
     INFO: Early stopping counter 7 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 1.58s
Val loss: 0.2761 score: 0.8915 time: 1.26s
Test loss: 0.3537 score: 0.8992 time: 1.43s
     INFO: Early stopping counter 8 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 1.58s
Val loss: 0.2774 score: 0.8915 time: 1.41s
Test loss: 0.3530 score: 0.8992 time: 1.25s
     INFO: Early stopping counter 9 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 1.58s
Val loss: 0.2789 score: 0.8915 time: 1.18s
Test loss: 0.3526 score: 0.8992 time: 1.28s
     INFO: Early stopping counter 10 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 1.57s
Val loss: 0.2808 score: 0.8992 time: 5.39s
Test loss: 0.3506 score: 0.9070 time: 1.16s
     INFO: Early stopping counter 11 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 1.69s
Val loss: 0.2839 score: 0.8992 time: 1.10s
Test loss: 0.3474 score: 0.9147 time: 1.23s
     INFO: Early stopping counter 12 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 1.44s
Val loss: 0.2864 score: 0.9070 time: 1.18s
Test loss: 0.3469 score: 0.9147 time: 1.14s
     INFO: Early stopping counter 13 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 1.55s
Val loss: 0.2870 score: 0.9070 time: 1.10s
Test loss: 0.3502 score: 0.9070 time: 1.23s
     INFO: Early stopping counter 14 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 1.55s
Val loss: 0.2897 score: 0.9070 time: 1.13s
Test loss: 0.3532 score: 0.9147 time: 1.24s
     INFO: Early stopping counter 15 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 1.49s
Val loss: 0.2936 score: 0.9070 time: 1.11s
Test loss: 0.3573 score: 0.9147 time: 1.26s
     INFO: Early stopping counter 16 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 1.47s
Val loss: 0.2968 score: 0.9070 time: 1.19s
Test loss: 0.3512 score: 0.9147 time: 1.13s
     INFO: Early stopping counter 17 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 1.62s
Val loss: 0.3010 score: 0.9070 time: 1.10s
Test loss: 0.3477 score: 0.9147 time: 1.27s
     INFO: Early stopping counter 18 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 1.50s
Val loss: 0.3079 score: 0.9070 time: 1.29s
Test loss: 0.3440 score: 0.9225 time: 1.28s
     INFO: Early stopping counter 19 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 1.60s
Val loss: 0.3161 score: 0.9070 time: 1.25s
Test loss: 0.3434 score: 0.9225 time: 1.24s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 067,   Train_Loss: 0.1216,   Val_Loss: 0.2690,   Val_Precision: 0.9649,   Val_Recall: 0.8462,   Val_accuracy: 0.9016,   Val_Score: 0.9070,   Val_Loss: 0.2690,   Test_Precision: 0.9643,   Test_Recall: 0.8438,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.3772


[1.3330678700003773, 1.4234237431082875, 1.738369552185759, 1.2686544940806925, 1.3874888110440224, 1.426706582074985, 1.232859511859715, 1.4030895109754056, 1.2182617329526693, 1.429677948821336, 1.2929440848529339, 1.392578257014975, 1.2833041520789266, 1.7069565979763865, 1.3402405991218984, 1.5598475239239633, 1.6936344648711383, 1.2499800219666213, 1.599837358109653, 1.2682381020858884, 1.6047646459192038, 1.5043788771145046, 1.5567132190335542, 1.6709745349362493, 1.2626032789703459, 1.4984863551799208, 1.2894554079975933, 1.383400030899793, 1.3020977908745408, 1.714290644042194, 1.5264798200223595, 1.3417074820026755, 1.699239161098376, 1.5545109331142157, 1.3955135631840676, 1.3578996111173183, 1.4335980280302465, 1.5589276489336044, 1.608496607048437, 2.339774162042886, 1.335615849122405, 1.2613908550702035, 1.3457482599187642, 1.250245706178248, 1.3589172679930925, 1.30587408086285, 1.4393103660549968, 1.4268184041138738, 1.6136269879061729, 1.366180449957028, 1.5786538620013744, 1.3575013121590018, 1.4140511699952185, 1.2767994329333305, 1.5020016618072987, 1.627178329974413, 1.2332204428967088, 1.3680908069945872, 1.2346956271212548, 1.3838310989085585, 1.3663001370150596, 1.3045680280774832, 1.4202051779720932, 1.2621343820355833, 1.3554474138654768, 1.6234904029406607, 1.2596747938077897, 1.1862809709273279, 1.3407973980065435, 1.3434052250813693, 1.167899041902274, 1.281773435883224, 1.1147927579004318, 5.875606356887147, 1.2265815429855138, 1.3577595970127732, 1.23727314802818, 1.418698072899133, 1.288647140841931, 1.1311145429499447, 1.289330321131274, 1.214149926090613, 1.445964879123494, 1.2927197320386767, 1.1422988350968808, 1.3126052520237863, 1.1300605949945748, 1.3036211640574038, 1.2917848769575357, 1.2616093680262566, 1.177987352013588, 1.4311979610938579, 1.3967390339821577, 1.3630121061578393, 1.4758229032158852, 1.1513497789856046, 1.5616426430642605, 1.3456078758463264, 1.143105389084667, 1.423998409183696, 1.153513127937913, 1.207500632153824, 1.4444744128268212, 1.212273346958682, 1.607730871066451, 1.457870498066768, 1.3028347298968583, 1.175552834989503, 1.1474153471644968, 1.1940770221408457, 1.2476487341336906, 1.1423871060833335, 1.2586470439564437, 1.1496159438975155, 1.301816973136738, 1.1486500261817127, 1.2936315189581364, 1.1352696728426963, 1.1745916828513145, 1.5926580268424004, 1.1763755278661847, 1.1662297600414604, 1.3965950689744204, 1.3009039801545441, 1.632179050007835, 1.1370730211492628, 1.2540496240835637, 1.2063537540379912, 1.158664905000478, 1.2356050910893828, 1.2759155998937786, 1.3919252050109208, 1.380048513179645, 1.3010020388755947, 1.4764670331496745, 1.1480615071486682, 1.5893902438692749, 1.6206751931458712, 1.3729506840463728, 1.448534110095352, 1.2769841868430376, 1.138814123114571, 1.432737911120057, 1.259005593135953, 1.28501870110631, 1.167948953108862, 1.2327482181135565, 1.1490913957823068, 1.2366294409148395, 1.2442200740333647, 1.2620030648540705, 1.1371358658652753, 1.2707149400375783, 1.2845448509324342, 1.2416720180772245]
[0.010333859457367266, 0.011034292582234786, 0.013475732962680302, 0.009834530961865833, 0.010755727217395523, 0.011059740946317712, 0.009557050479532675, 0.010876662875778337, 0.009443889402733871, 0.011082774797064621, 0.010022822363201038, 0.010795180286937791, 0.009948094202162222, 0.013232221689739431, 0.010389462008696887, 0.012091841270728397, 0.013128949340086344, 0.009689767612144351, 0.012401839985346148, 0.009831303116944872, 0.012440036014877549, 0.011661851760577555, 0.012067544333593444, 0.012953290968498057, 0.00978762231759958, 0.011616173295968379, 0.00999577835657049, 0.010724031247285217, 0.010093781324608843, 0.013289074760017007, 0.011833176899398135, 0.01040083319381919, 0.01317239659766183, 0.012050472349722603, 0.010817934598326105, 0.010526353574552855, 0.011113163007986407, 0.012084710456849647, 0.012468965946111915, 0.018137784201882837, 0.010353611233507016, 0.009778223682714755, 0.010432157053633831, 0.00969182717967634, 0.010534242387543353, 0.01012305489040969, 0.011157444698100752, 0.01106060778382848, 0.012508736340357929, 0.010590546123697892, 0.012237626837219956, 0.010523265985728697, 0.01096163697670712, 0.009897670022738997, 0.0116434237349403, 0.01261378550367762, 0.009559848394548131, 0.010605355092981297, 0.009571283931172517, 0.010727372859756267, 0.0105914739303493, 0.010112930450213048, 0.011009342464899947, 0.00978398745764018, 0.010507344293530828, 0.012585196922020626, 0.009764920882230927, 0.009195976518816495, 0.010393778279120492, 0.010413993992878831, 0.009053480944978867, 0.009936228185141272, 0.008641804324809549, 0.04554733609990036, 0.00950838405415127, 0.010525268193897467, 0.009591264713396745, 0.01099765947983824, 0.009989512719704892, 0.00876832979030965, 0.00999480869094011, 0.009412014930934985, 0.01120903007072476, 0.010021083194098269, 0.008855029729433184, 0.010175234511812297, 0.008760159651120735, 0.010105590419049642, 0.010013836255484774, 0.009779917581598888, 0.00913168489933014, 0.011094557837936883, 0.01082743437195471, 0.010565985319053018, 0.011440487621828567, 0.008925192085159727, 0.012105756922978763, 0.010431068805010282, 0.00886128208592765, 0.011038747358013147, 0.008941962232076845, 0.00936047001669631, 0.01119747606842497, 0.009397467805881255, 0.012463030008267062, 0.011301321690440062, 0.010099494030208205, 0.009112812674337233, 0.008894692613678269, 0.009256411024347641, 0.009671695613439462, 0.008855714000645996, 0.009756953829119718, 0.008911751503081516, 0.010091604442920449, 0.008904263768850486, 0.010028151309752995, 0.008800540099555785, 0.00910536188256833, 0.012346186254592252, 0.009119190138497556, 0.009040540775515197, 0.010826318364142794, 0.010084526977942203, 0.012652550775254534, 0.008814519543792735, 0.009721314915376462, 0.009351579488666599, 0.008981898488375798, 0.00957833403945258, 0.00989081860382774, 0.010790117868301712, 0.01069805048976469, 0.010085287123066626, 0.01144548087712926, 0.008899701605803629, 0.01232085460363779, 0.01256337359027807, 0.010643028558499013, 0.011228946589886449, 0.009899102223589439, 0.008828016458252487, 0.011106495435039202, 0.009759733280123667, 0.009961385279893875, 0.00905386785355707, 0.00955618773731439, 0.008907685238622533, 0.00958627473577395, 0.009645116852971819, 0.009782969494992796, 0.008815006712133916, 0.009850503411144018, 0.009957712022732048, 0.009625364481218795]
[96.76926651901339, 90.62656192477624, 74.20746632256666, 101.68253106097063, 92.97372272352477, 90.41803102385913, 104.63479314476724, 91.939964621588, 105.88857591984446, 90.23011098852785, 99.77229604223227, 92.63393231236758, 100.52176624771504, 75.57309901899717, 96.2513746296885, 82.70039091736781, 76.16755721242073, 103.20164941279737, 80.63319645968558, 101.71591579517438, 80.3856193667011, 85.74967513996893, 82.86690086699872, 77.20045835702791, 102.169859803627, 86.08686996319776, 100.04223426409544, 93.24851606089356, 99.0708999769967, 75.24978360485348, 84.50815943188194, 96.1461434257268, 75.91632946866369, 82.98429895347792, 92.43908723156204, 94.99965899088457, 89.98338270403809, 82.74918986025001, 80.19911228579632, 55.1335261721877, 96.58465799485849, 102.26806344875587, 95.85745257273233, 103.17971848455878, 94.92851628157817, 98.7844095310965, 89.62625646445935, 90.41094481824791, 79.94412647212178, 94.42383691265498, 81.71518982410578, 95.02753245581427, 91.22725028432755, 101.0338794587606, 85.88539099535978, 79.2783419147602, 104.60416930568567, 94.29198657023824, 104.47919079519944, 93.21946883672696, 94.41556544217644, 98.88330636931585, 90.83194597572071, 102.20781704080312, 95.17152689244998, 79.45843090069378, 102.4073837423183, 108.74320937573448, 96.2114038942746, 96.02463768308371, 110.4547528268238, 100.64181109441603, 115.7165751982053, 21.955180821259656, 105.17034170106008, 95.00945549110078, 104.26153691736137, 90.92843816752804, 100.10498290146246, 114.04680525419488, 100.05194005428635, 106.24717526884125, 89.21378510811162, 99.78961162491211, 112.93016856579324, 98.2778331879342, 114.15317069844322, 98.95512864987485, 99.8618286226001, 102.25035044073583, 109.50881584551342, 90.13428156465923, 92.3579830315302, 94.6433266566024, 87.40886167229357, 112.04240653405544, 82.60532623960356, 95.86745315299589, 112.8504871307586, 90.58998884271857, 111.83227730629116, 106.83224220752759, 89.30583944893033, 106.41164414249613, 80.23730981444105, 88.48522565691704, 99.01486124046798, 109.73560367548421, 112.42659453595942, 108.03323203449433, 103.39448634119893, 112.92144257674234, 102.49100462231263, 112.21138736355236, 99.09227077380433, 112.30574766869209, 99.719277173993, 113.62938963831047, 109.82539880314259, 80.99667212035158, 109.65886057999842, 110.61285213251114, 92.36750355615263, 99.1618151438626, 79.03544650899713, 113.44917837344966, 102.8667426891267, 106.93380740782065, 111.33503694059567, 104.40228915394476, 101.10386612620725, 92.67739353781434, 93.47497480561952, 99.1543411503718, 87.37072830187792, 112.3633178159462, 81.16320110658114, 79.59645495011237, 93.95821823680514, 89.05554871021185, 101.01926188993303, 113.27572900764059, 90.0374025135923, 102.46181645522682, 100.38764407781782, 110.4500326462266, 104.64423967889034, 112.26261067961109, 104.31580854533738, 103.67940743941153, 102.2184522308721, 113.44290851457812, 101.51765430269131, 100.42467564006084, 103.89216968886946]
Elapsed: 1.3799078158823954~0.400302250357752
Time per graph: 0.010696959813041826~0.003103118219827535
Speed: 96.37097174138633~12.406443168921326
Total Time: 1.2423
best val loss: 0.2690312001825303 test_score: 0.9070

Testing...
Test loss: 0.3772 score: 0.9070 time: 1.14s
test Score 0.9070
Epoch Time List: [4.652922890847549, 4.138778166146949, 4.83580164401792, 3.912639813730493, 4.009068435290828, 3.8085116436704993, 7.9120609681122005, 3.9409864868503064, 3.8235652348957956, 4.072058917256072, 3.9113198057748377, 4.011085175909102, 3.841019446030259, 4.42344012320973, 4.517082290025428, 4.417703517014161, 4.656676267972216, 4.438157722121105, 4.209331799997017, 4.337904985994101, 4.72563558164984, 4.202679600799456, 4.3131542860064656, 5.0800639991648495, 4.166837419150397, 4.233520584879443, 4.322031663032249, 3.987868442898616, 3.887837915914133, 4.7912493431940675, 4.696160361170769, 4.470685878768563, 4.967087675118819, 4.318230634089559, 4.578838092973456, 3.997072590980679, 4.562198496889323, 4.31230599200353, 4.629744255915284, 9.533644078997895, 3.771136953961104, 3.8058154799509794, 3.843700709985569, 3.80289413430728, 3.9197561300825328, 3.968889348907396, 4.139833925059065, 4.147966081276536, 4.359186687273905, 4.115180616732687, 4.40298425918445, 3.9929984922055155, 4.129646398825571, 3.8360003859270364, 4.154960800195113, 4.4362591952085495, 4.045982276787981, 3.891302583971992, 3.737466603750363, 3.9263150221668184, 4.291641787160188, 3.9925013850443065, 4.0691952330525964, 3.8292030268348753, 3.9962141611613333, 4.807289110030979, 4.034951844019815, 4.550324131269008, 4.058791384100914, 4.129663116764277, 3.974584818119183, 4.046622463269159, 3.784770981175825, 8.84844662528485, 3.9616122010629624, 3.903841534163803, 3.876538683893159, 4.119760042056441, 3.874135131947696, 3.788800063310191, 3.9815352980513126, 4.053934939671308, 4.6487005380913615, 4.156515212962404, 3.8643463170155883, 4.0638162607792765, 3.8502266057766974, 4.011980260256678, 4.097684434382245, 3.81861932692118, 3.8542824459727854, 4.805456513073295, 4.4560190353076905, 4.083712598774582, 4.820282470900565, 4.377755165100098, 4.780718357069418, 4.0007810548413545, 4.284156847279519, 4.542887791292742, 3.8610806791111827, 4.313608421944082, 4.926487013930455, 3.991356454556808, 4.717637034598738, 4.378265774343163, 4.106164963915944, 7.630517017794773, 3.9508197652176023, 3.8498159961309284, 4.010812405729666, 3.8859476919751614, 3.932726122904569, 3.814139242982492, 4.037273271242157, 4.530481044901535, 4.029148834524676, 3.80371275707148, 4.246324151055887, 4.773896488361061, 4.142857528990135, 3.847112078918144, 4.09839550871402, 4.150416744174436, 4.454526741290465, 4.331959057133645, 3.8673964280169457, 3.9390274609904736, 4.321040540002286, 3.934481196803972, 4.028203779831529, 4.654609956080094, 4.12026389897801, 4.317440775223076, 4.797132689040154, 4.394130954286084, 4.292696401942521, 4.73105818987824, 4.733421717770398, 4.6963405860587955, 4.7482507231179625, 3.7441249289549887, 4.266045346157625, 4.236738533945754, 4.038095413008705, 8.117583862040192, 4.024642884964123, 3.7715060322079808, 3.8801628770306706, 3.9181853919290006, 3.8558021739590913, 3.8007624209858477, 3.989022580208257, 4.06567646516487, 4.093121469719335]
Total Epoch List: [67, 88]
Total Time List: [1.2601661880034953, 1.242315156152472]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a0417a90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4961 time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 1.64s
Epoch 2/1000, LR 0.000020
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.4961 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 1.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4961 time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 1.37s
Epoch 4/1000, LR 0.000080
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4961 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 1.31s
Epoch 5/1000, LR 0.000110
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 1.18s
Epoch 6/1000, LR 0.000140
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 1.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 1.37s
Epoch 7/1000, LR 0.000170
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 1.19s
Epoch 8/1000, LR 0.000200
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 1.46s
Epoch 9/1000, LR 0.000230
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 1.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 1.60s
Epoch 10/1000, LR 0.000260
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 1.49s
Epoch 11/1000, LR 0.000290
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4961 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 1.39s
Epoch 12/1000, LR 0.000290
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4961 time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 1.44s
Epoch 13/1000, LR 0.000290
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 1.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.4961 time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 1.31s
Epoch 14/1000, LR 0.000290
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4961 time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 1.47s
Epoch 15/1000, LR 0.000290
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 1.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.4961 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 1.39s
Epoch 16/1000, LR 0.000290
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 1.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.4961 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5000 time: 1.38s
Epoch 17/1000, LR 0.000290
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 1.38s
Val loss: 0.6831 score: 0.5581 time: 1.53s
Test loss: 0.6846 score: 0.5469 time: 1.42s
Epoch 18/1000, LR 0.000290
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 1.84s
Val loss: 0.6802 score: 0.6047 time: 1.56s
Test loss: 0.6822 score: 0.6016 time: 1.63s
Epoch 19/1000, LR 0.000290
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 1.32s
Val loss: 0.6776 score: 0.6279 time: 1.51s
Test loss: 0.6798 score: 0.6094 time: 1.47s
Epoch 20/1000, LR 0.000290
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 1.45s
Val loss: 0.6750 score: 0.6667 time: 1.40s
Test loss: 0.6773 score: 0.6172 time: 1.56s
Epoch 21/1000, LR 0.000290
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 1.63s
Val loss: 0.6724 score: 0.6667 time: 1.73s
Test loss: 0.6748 score: 0.6250 time: 1.34s
Epoch 22/1000, LR 0.000290
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 1.41s
Val loss: 0.6683 score: 0.6977 time: 1.41s
Test loss: 0.6710 score: 0.6719 time: 1.29s
Epoch 23/1000, LR 0.000290
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 1.45s
Val loss: 0.6621 score: 0.7442 time: 1.26s
Test loss: 0.6653 score: 0.7344 time: 1.30s
Epoch 24/1000, LR 0.000290
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 1.34s
Val loss: 0.6551 score: 0.8217 time: 1.33s
Test loss: 0.6589 score: 0.7969 time: 1.22s
Epoch 25/1000, LR 0.000290
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 3.62s
Val loss: 0.6471 score: 0.8837 time: 2.85s
Test loss: 0.6515 score: 0.8359 time: 1.31s
Epoch 26/1000, LR 0.000290
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 1.28s
Val loss: 0.6381 score: 0.8915 time: 1.23s
Test loss: 0.6427 score: 0.8359 time: 1.29s
Epoch 27/1000, LR 0.000290
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 1.30s
Val loss: 0.6289 score: 0.8915 time: 1.32s
Test loss: 0.6334 score: 0.8359 time: 1.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 1.39s
Val loss: 0.6202 score: 0.8527 time: 1.26s
Test loss: 0.6243 score: 0.8359 time: 1.29s
Epoch 29/1000, LR 0.000290
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 1.33s
Val loss: 0.6104 score: 0.8450 time: 1.33s
Test loss: 0.6144 score: 0.8281 time: 1.23s
Epoch 30/1000, LR 0.000290
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 1.41s
Val loss: 0.5946 score: 0.8682 time: 1.26s
Test loss: 0.5988 score: 0.8359 time: 1.34s
Epoch 31/1000, LR 0.000290
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 1.35s
Val loss: 0.5732 score: 0.8915 time: 1.37s
Test loss: 0.5779 score: 0.8594 time: 1.22s
Epoch 32/1000, LR 0.000290
Train loss: 0.5558;  Loss pred: 0.5558; Loss self: 0.0000; time: 1.34s
Val loss: 0.5532 score: 0.8837 time: 1.52s
Test loss: 0.5580 score: 0.8828 time: 1.46s
Epoch 33/1000, LR 0.000290
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 1.96s
Val loss: 0.5341 score: 0.8837 time: 1.22s
Test loss: 0.5380 score: 0.8984 time: 1.34s
Epoch 34/1000, LR 0.000290
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 1.31s
Val loss: 0.5151 score: 0.8837 time: 1.40s
Test loss: 0.5180 score: 0.8984 time: 1.20s
Epoch 35/1000, LR 0.000290
Train loss: 0.4902;  Loss pred: 0.4902; Loss self: 0.0000; time: 1.47s
Val loss: 0.4957 score: 0.8760 time: 1.20s
Test loss: 0.4977 score: 0.9062 time: 1.35s
Epoch 36/1000, LR 0.000290
Train loss: 0.4654;  Loss pred: 0.4654; Loss self: 0.0000; time: 1.30s
Val loss: 0.4759 score: 0.8837 time: 1.29s
Test loss: 0.4763 score: 0.9062 time: 1.31s
Epoch 37/1000, LR 0.000290
Train loss: 0.4398;  Loss pred: 0.4398; Loss self: 0.0000; time: 1.49s
Val loss: 0.4570 score: 0.8915 time: 1.36s
Test loss: 0.4553 score: 0.9062 time: 1.30s
Epoch 38/1000, LR 0.000289
Train loss: 0.4207;  Loss pred: 0.4207; Loss self: 0.0000; time: 1.44s
Val loss: 0.4387 score: 0.8915 time: 1.42s
Test loss: 0.4340 score: 0.9062 time: 1.35s
Epoch 39/1000, LR 0.000289
Train loss: 0.3948;  Loss pred: 0.3948; Loss self: 0.0000; time: 1.34s
Val loss: 0.4207 score: 0.8837 time: 1.42s
Test loss: 0.4124 score: 0.8984 time: 1.27s
Epoch 40/1000, LR 0.000289
Train loss: 0.3663;  Loss pred: 0.3663; Loss self: 0.0000; time: 1.40s
Val loss: 0.4034 score: 0.8915 time: 1.41s
Test loss: 0.3910 score: 0.8984 time: 1.23s
Epoch 41/1000, LR 0.000289
Train loss: 0.3454;  Loss pred: 0.3454; Loss self: 0.0000; time: 1.70s
Val loss: 0.3865 score: 0.8915 time: 1.56s
Test loss: 0.3691 score: 0.8906 time: 1.66s
Epoch 42/1000, LR 0.000289
Train loss: 0.3187;  Loss pred: 0.3187; Loss self: 0.0000; time: 1.63s
Val loss: 0.3702 score: 0.8915 time: 1.59s
Test loss: 0.3465 score: 0.8906 time: 1.25s
Epoch 43/1000, LR 0.000289
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 1.42s
Val loss: 0.3557 score: 0.8915 time: 1.32s
Test loss: 0.3242 score: 0.8906 time: 1.39s
Epoch 44/1000, LR 0.000289
Train loss: 0.2638;  Loss pred: 0.2638; Loss self: 0.0000; time: 1.34s
Val loss: 0.3439 score: 0.8992 time: 1.46s
Test loss: 0.3030 score: 0.8984 time: 1.18s
Epoch 45/1000, LR 0.000289
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 1.33s
Val loss: 0.3365 score: 0.8915 time: 1.38s
Test loss: 0.2837 score: 0.9062 time: 1.26s
Epoch 46/1000, LR 0.000289
Train loss: 0.2061;  Loss pred: 0.2061; Loss self: 0.0000; time: 1.46s
Val loss: 0.3378 score: 0.8760 time: 1.25s
Test loss: 0.2698 score: 0.9062 time: 1.37s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1812;  Loss pred: 0.1812; Loss self: 0.0000; time: 1.55s
Val loss: 0.3436 score: 0.8760 time: 1.38s
Test loss: 0.2611 score: 0.9141 time: 1.18s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 1.46s
Val loss: 0.3429 score: 0.8760 time: 1.49s
Test loss: 0.2505 score: 0.9141 time: 1.67s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 1.47s
Val loss: 0.3441 score: 0.8837 time: 1.39s
Test loss: 0.2445 score: 0.9062 time: 1.24s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 1.34s
Val loss: 0.3522 score: 0.8915 time: 1.33s
Test loss: 0.2470 score: 0.8984 time: 1.37s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 1.56s
Val loss: 0.3697 score: 0.8837 time: 1.34s
Test loss: 0.2613 score: 0.8828 time: 1.21s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1188;  Loss pred: 0.1188; Loss self: 0.0000; time: 1.35s
Val loss: 0.3809 score: 0.8915 time: 1.51s
Test loss: 0.2619 score: 0.8828 time: 1.18s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 1.43s
Val loss: 0.3882 score: 0.8915 time: 1.21s
Test loss: 0.2572 score: 0.8984 time: 1.29s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 1.47s
Val loss: 0.4042 score: 0.8915 time: 1.60s
Test loss: 0.2702 score: 0.8828 time: 1.39s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 1.52s
Val loss: 0.4184 score: 0.8915 time: 1.70s
Test loss: 0.2797 score: 0.8828 time: 1.45s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 1.44s
Val loss: 0.4296 score: 0.8915 time: 1.41s
Test loss: 0.2847 score: 0.8828 time: 1.33s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 1.40s
Val loss: 0.4380 score: 0.8915 time: 1.38s
Test loss: 0.2854 score: 0.8828 time: 1.25s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 1.57s
Val loss: 0.4445 score: 0.8915 time: 1.40s
Test loss: 0.2834 score: 0.8828 time: 1.59s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 1.47s
Val loss: 0.4519 score: 0.8915 time: 1.34s
Test loss: 0.2828 score: 0.8828 time: 1.22s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0686;  Loss pred: 0.0686; Loss self: 0.0000; time: 1.44s
Val loss: 0.4580 score: 0.8915 time: 1.23s
Test loss: 0.2816 score: 0.8984 time: 1.19s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0642;  Loss pred: 0.0642; Loss self: 0.0000; time: 5.01s
Val loss: 0.4639 score: 0.8760 time: 1.78s
Test loss: 0.2810 score: 0.8984 time: 1.27s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 1.31s
Val loss: 0.4653 score: 0.8760 time: 1.33s
Test loss: 0.2751 score: 0.9219 time: 1.18s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 1.53s
Val loss: 0.4703 score: 0.8760 time: 1.20s
Test loss: 0.2693 score: 0.9297 time: 1.32s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 1.30s
Val loss: 0.4807 score: 0.8760 time: 1.38s
Test loss: 0.2738 score: 0.9219 time: 1.19s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 1.32s
Val loss: 0.4867 score: 0.8760 time: 1.34s
Test loss: 0.2767 score: 0.9141 time: 1.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.2374,   Val_Loss: 0.3365,   Val_Precision: 0.9636,   Val_Recall: 0.8154,   Val_accuracy: 0.8833,   Val_Score: 0.8915,   Val_Loss: 0.3365,   Test_Precision: 0.9643,   Test_Recall: 0.8438,   Test_accuracy: 0.9000,   Test_Score: 0.9062,   Test_loss: 0.2837


[1.3330678700003773, 1.4234237431082875, 1.738369552185759, 1.2686544940806925, 1.3874888110440224, 1.426706582074985, 1.232859511859715, 1.4030895109754056, 1.2182617329526693, 1.429677948821336, 1.2929440848529339, 1.392578257014975, 1.2833041520789266, 1.7069565979763865, 1.3402405991218984, 1.5598475239239633, 1.6936344648711383, 1.2499800219666213, 1.599837358109653, 1.2682381020858884, 1.6047646459192038, 1.5043788771145046, 1.5567132190335542, 1.6709745349362493, 1.2626032789703459, 1.4984863551799208, 1.2894554079975933, 1.383400030899793, 1.3020977908745408, 1.714290644042194, 1.5264798200223595, 1.3417074820026755, 1.699239161098376, 1.5545109331142157, 1.3955135631840676, 1.3578996111173183, 1.4335980280302465, 1.5589276489336044, 1.608496607048437, 2.339774162042886, 1.335615849122405, 1.2613908550702035, 1.3457482599187642, 1.250245706178248, 1.3589172679930925, 1.30587408086285, 1.4393103660549968, 1.4268184041138738, 1.6136269879061729, 1.366180449957028, 1.5786538620013744, 1.3575013121590018, 1.4140511699952185, 1.2767994329333305, 1.5020016618072987, 1.627178329974413, 1.2332204428967088, 1.3680908069945872, 1.2346956271212548, 1.3838310989085585, 1.3663001370150596, 1.3045680280774832, 1.4202051779720932, 1.2621343820355833, 1.3554474138654768, 1.6234904029406607, 1.2596747938077897, 1.1862809709273279, 1.3407973980065435, 1.3434052250813693, 1.167899041902274, 1.281773435883224, 1.1147927579004318, 5.875606356887147, 1.2265815429855138, 1.3577595970127732, 1.23727314802818, 1.418698072899133, 1.288647140841931, 1.1311145429499447, 1.289330321131274, 1.214149926090613, 1.445964879123494, 1.2927197320386767, 1.1422988350968808, 1.3126052520237863, 1.1300605949945748, 1.3036211640574038, 1.2917848769575357, 1.2616093680262566, 1.177987352013588, 1.4311979610938579, 1.3967390339821577, 1.3630121061578393, 1.4758229032158852, 1.1513497789856046, 1.5616426430642605, 1.3456078758463264, 1.143105389084667, 1.423998409183696, 1.153513127937913, 1.207500632153824, 1.4444744128268212, 1.212273346958682, 1.607730871066451, 1.457870498066768, 1.3028347298968583, 1.175552834989503, 1.1474153471644968, 1.1940770221408457, 1.2476487341336906, 1.1423871060833335, 1.2586470439564437, 1.1496159438975155, 1.301816973136738, 1.1486500261817127, 1.2936315189581364, 1.1352696728426963, 1.1745916828513145, 1.5926580268424004, 1.1763755278661847, 1.1662297600414604, 1.3965950689744204, 1.3009039801545441, 1.632179050007835, 1.1370730211492628, 1.2540496240835637, 1.2063537540379912, 1.158664905000478, 1.2356050910893828, 1.2759155998937786, 1.3919252050109208, 1.380048513179645, 1.3010020388755947, 1.4764670331496745, 1.1480615071486682, 1.5893902438692749, 1.6206751931458712, 1.3729506840463728, 1.448534110095352, 1.2769841868430376, 1.138814123114571, 1.432737911120057, 1.259005593135953, 1.28501870110631, 1.167948953108862, 1.2327482181135565, 1.1490913957823068, 1.2366294409148395, 1.2442200740333647, 1.2620030648540705, 1.1371358658652753, 1.2707149400375783, 1.2845448509324342, 1.2416720180772245, 1.6456733669620007, 1.1717929989099503, 1.3790637981146574, 1.3159878300502896, 1.1848687308374792, 1.3801467700395733, 1.1981646541971713, 1.4650773350149393, 1.61112061701715, 1.4997433561366051, 1.3971885789651424, 1.449617464095354, 1.3191494431812316, 1.477538914885372, 1.3956784789916128, 1.3879356570541859, 1.4243899520952255, 1.6337413180153817, 1.471779365092516, 1.5647767630871385, 1.3500332059338689, 1.299932091962546, 1.3008271581493318, 1.2209584929514676, 1.3119238368235528, 1.2936039478518069, 1.171769596170634, 1.2958404149394482, 1.2349577900022268, 1.3463646741583943, 1.2205431980546564, 1.4722906639799476, 1.3515509809367359, 1.204740053974092, 1.357177295954898, 1.3160089531447738, 1.3067206831183285, 1.3563185599632561, 1.2761060609482229, 1.2359388039913028, 1.6726704561151564, 1.2534515659790486, 1.3945939929690212, 1.1850396418012679, 1.2656671069562435, 1.3772076880559325, 1.1871622649487108, 1.6771404228638858, 1.2453711389098316, 1.3741301170084625, 1.214802518952638, 1.1907259030267596, 1.2947862870059907, 1.3911537630483508, 1.4558260370977223, 1.335660858079791, 1.2546469951048493, 1.5949701881036162, 1.2203456789720803, 1.193452880019322, 1.2775722569786012, 1.1887235408648849, 1.3273516912013292, 1.1991815080400556, 1.1851353351958096]
[0.010333859457367266, 0.011034292582234786, 0.013475732962680302, 0.009834530961865833, 0.010755727217395523, 0.011059740946317712, 0.009557050479532675, 0.010876662875778337, 0.009443889402733871, 0.011082774797064621, 0.010022822363201038, 0.010795180286937791, 0.009948094202162222, 0.013232221689739431, 0.010389462008696887, 0.012091841270728397, 0.013128949340086344, 0.009689767612144351, 0.012401839985346148, 0.009831303116944872, 0.012440036014877549, 0.011661851760577555, 0.012067544333593444, 0.012953290968498057, 0.00978762231759958, 0.011616173295968379, 0.00999577835657049, 0.010724031247285217, 0.010093781324608843, 0.013289074760017007, 0.011833176899398135, 0.01040083319381919, 0.01317239659766183, 0.012050472349722603, 0.010817934598326105, 0.010526353574552855, 0.011113163007986407, 0.012084710456849647, 0.012468965946111915, 0.018137784201882837, 0.010353611233507016, 0.009778223682714755, 0.010432157053633831, 0.00969182717967634, 0.010534242387543353, 0.01012305489040969, 0.011157444698100752, 0.01106060778382848, 0.012508736340357929, 0.010590546123697892, 0.012237626837219956, 0.010523265985728697, 0.01096163697670712, 0.009897670022738997, 0.0116434237349403, 0.01261378550367762, 0.009559848394548131, 0.010605355092981297, 0.009571283931172517, 0.010727372859756267, 0.0105914739303493, 0.010112930450213048, 0.011009342464899947, 0.00978398745764018, 0.010507344293530828, 0.012585196922020626, 0.009764920882230927, 0.009195976518816495, 0.010393778279120492, 0.010413993992878831, 0.009053480944978867, 0.009936228185141272, 0.008641804324809549, 0.04554733609990036, 0.00950838405415127, 0.010525268193897467, 0.009591264713396745, 0.01099765947983824, 0.009989512719704892, 0.00876832979030965, 0.00999480869094011, 0.009412014930934985, 0.01120903007072476, 0.010021083194098269, 0.008855029729433184, 0.010175234511812297, 0.008760159651120735, 0.010105590419049642, 0.010013836255484774, 0.009779917581598888, 0.00913168489933014, 0.011094557837936883, 0.01082743437195471, 0.010565985319053018, 0.011440487621828567, 0.008925192085159727, 0.012105756922978763, 0.010431068805010282, 0.00886128208592765, 0.011038747358013147, 0.008941962232076845, 0.00936047001669631, 0.01119747606842497, 0.009397467805881255, 0.012463030008267062, 0.011301321690440062, 0.010099494030208205, 0.009112812674337233, 0.008894692613678269, 0.009256411024347641, 0.009671695613439462, 0.008855714000645996, 0.009756953829119718, 0.008911751503081516, 0.010091604442920449, 0.008904263768850486, 0.010028151309752995, 0.008800540099555785, 0.00910536188256833, 0.012346186254592252, 0.009119190138497556, 0.009040540775515197, 0.010826318364142794, 0.010084526977942203, 0.012652550775254534, 0.008814519543792735, 0.009721314915376462, 0.009351579488666599, 0.008981898488375798, 0.00957833403945258, 0.00989081860382774, 0.010790117868301712, 0.01069805048976469, 0.010085287123066626, 0.01144548087712926, 0.008899701605803629, 0.01232085460363779, 0.01256337359027807, 0.010643028558499013, 0.011228946589886449, 0.009899102223589439, 0.008828016458252487, 0.011106495435039202, 0.009759733280123667, 0.009961385279893875, 0.00905386785355707, 0.00955618773731439, 0.008907685238622533, 0.00958627473577395, 0.009645116852971819, 0.009782969494992796, 0.008815006712133916, 0.009850503411144018, 0.009957712022732048, 0.009625364481218795, 0.01285682317939063, 0.009154632803983986, 0.010773935922770761, 0.010281154922267888, 0.009256786959667807, 0.010782396640934166, 0.009360661360915401, 0.011445916679804213, 0.012586879820446484, 0.011716744969817228, 0.010915535773165175, 0.011325136438244954, 0.010305855024853372, 0.011543272772541968, 0.010903738117121975, 0.010843247320735827, 0.011128046500743949, 0.01276360404699517, 0.01149827628978528, 0.01222481846161827, 0.01054713442135835, 0.010155719468457391, 0.010162712173041655, 0.009538738226183341, 0.010249404975184007, 0.010106280842592241, 0.009154449970083078, 0.01012375324171444, 0.009648107734392397, 0.010518474016862456, 0.009535493734802003, 0.01150227081234334, 0.010558992038568249, 0.009412031671672594, 0.01060294762464764, 0.010281319946443546, 0.010208755336861941, 0.010596238749712938, 0.009969578601157991, 0.009655771906182053, 0.01306773793839966, 0.009792590359211317, 0.010895265570070478, 0.009258122201572405, 0.009888024273095652, 0.010759435062936973, 0.009274705194911803, 0.013102659553624108, 0.00972946202273306, 0.010735391539128614, 0.009490644679317484, 0.00930254611739656, 0.010115517867234303, 0.01086838877381524, 0.011373640914825955, 0.010434850453748368, 0.009801929649256635, 0.012460704594559502, 0.009533950616969378, 0.009323850625150953, 0.009981033257645322, 0.009286902663006913, 0.010369935087510385, 0.009368605531562935, 0.009258869806217263]
[96.76926651901339, 90.62656192477624, 74.20746632256666, 101.68253106097063, 92.97372272352477, 90.41803102385913, 104.63479314476724, 91.939964621588, 105.88857591984446, 90.23011098852785, 99.77229604223227, 92.63393231236758, 100.52176624771504, 75.57309901899717, 96.2513746296885, 82.70039091736781, 76.16755721242073, 103.20164941279737, 80.63319645968558, 101.71591579517438, 80.3856193667011, 85.74967513996893, 82.86690086699872, 77.20045835702791, 102.169859803627, 86.08686996319776, 100.04223426409544, 93.24851606089356, 99.0708999769967, 75.24978360485348, 84.50815943188194, 96.1461434257268, 75.91632946866369, 82.98429895347792, 92.43908723156204, 94.99965899088457, 89.98338270403809, 82.74918986025001, 80.19911228579632, 55.1335261721877, 96.58465799485849, 102.26806344875587, 95.85745257273233, 103.17971848455878, 94.92851628157817, 98.7844095310965, 89.62625646445935, 90.41094481824791, 79.94412647212178, 94.42383691265498, 81.71518982410578, 95.02753245581427, 91.22725028432755, 101.0338794587606, 85.88539099535978, 79.2783419147602, 104.60416930568567, 94.29198657023824, 104.47919079519944, 93.21946883672696, 94.41556544217644, 98.88330636931585, 90.83194597572071, 102.20781704080312, 95.17152689244998, 79.45843090069378, 102.4073837423183, 108.74320937573448, 96.2114038942746, 96.02463768308371, 110.4547528268238, 100.64181109441603, 115.7165751982053, 21.955180821259656, 105.17034170106008, 95.00945549110078, 104.26153691736137, 90.92843816752804, 100.10498290146246, 114.04680525419488, 100.05194005428635, 106.24717526884125, 89.21378510811162, 99.78961162491211, 112.93016856579324, 98.2778331879342, 114.15317069844322, 98.95512864987485, 99.8618286226001, 102.25035044073583, 109.50881584551342, 90.13428156465923, 92.3579830315302, 94.6433266566024, 87.40886167229357, 112.04240653405544, 82.60532623960356, 95.86745315299589, 112.8504871307586, 90.58998884271857, 111.83227730629116, 106.83224220752759, 89.30583944893033, 106.41164414249613, 80.23730981444105, 88.48522565691704, 99.01486124046798, 109.73560367548421, 112.42659453595942, 108.03323203449433, 103.39448634119893, 112.92144257674234, 102.49100462231263, 112.21138736355236, 99.09227077380433, 112.30574766869209, 99.719277173993, 113.62938963831047, 109.82539880314259, 80.99667212035158, 109.65886057999842, 110.61285213251114, 92.36750355615263, 99.1618151438626, 79.03544650899713, 113.44917837344966, 102.8667426891267, 106.93380740782065, 111.33503694059567, 104.40228915394476, 101.10386612620725, 92.67739353781434, 93.47497480561952, 99.1543411503718, 87.37072830187792, 112.3633178159462, 81.16320110658114, 79.59645495011237, 93.95821823680514, 89.05554871021185, 101.01926188993303, 113.27572900764059, 90.0374025135923, 102.46181645522682, 100.38764407781782, 110.4500326462266, 104.64423967889034, 112.26261067961109, 104.31580854533738, 103.67940743941153, 102.2184522308721, 113.44290851457812, 101.51765430269131, 100.42467564006084, 103.89216968886946, 77.77971167893098, 109.23431025707683, 92.81659062836039, 97.26533716889203, 108.02884460418504, 92.74375941649295, 106.83005841610829, 87.36740166599792, 79.44780710272387, 85.34793601602128, 91.61254388065922, 88.29915696406118, 97.03222077046709, 86.63054401509979, 91.71166706853636, 92.22329533032726, 89.86303210838906, 78.34777671870994, 86.96955741865149, 81.80080572481764, 94.81248271331043, 98.4666820608718, 98.39892963343706, 104.83566864798239, 97.56663947040956, 98.94836840329701, 109.23649189935163, 98.77759523805341, 103.64726716673385, 95.07082476002435, 104.871339420031, 86.93935452527147, 94.70600947016108, 106.24698629199278, 94.31339618007706, 97.26377597517663, 97.9551342943036, 94.37310951747769, 100.30514227390188, 103.56499819136735, 76.5243383907701, 102.11802631561714, 91.78298533144716, 108.01326426974137, 101.13243782389392, 92.94168273246056, 107.82013864425686, 76.32038334716609, 102.780605717303, 93.14983960810147, 105.36692013970907, 107.49745149125509, 98.85801331429128, 92.00995849626382, 87.92259290483345, 95.83271024653581, 102.02072814059001, 80.25228368198476, 104.88831337348346, 107.25182547460749, 100.1900278444634, 107.67852709206926, 96.43261906281423, 106.73947116579826, 108.00454277135503]
Elapsed: 1.3688160143401051~0.34410908791129313
Time per graph: 0.010634996249424775~0.002666846156576768
Speed: 96.33037663810651~11.490709996038213
Total Time: 1.1861
best val loss: 0.33653102098971377 test_score: 0.9062

Testing...
Test loss: 0.3030 score: 0.8984 time: 1.29s
test Score 0.8984
Epoch Time List: [4.652922890847549, 4.138778166146949, 4.83580164401792, 3.912639813730493, 4.009068435290828, 3.8085116436704993, 7.9120609681122005, 3.9409864868503064, 3.8235652348957956, 4.072058917256072, 3.9113198057748377, 4.011085175909102, 3.841019446030259, 4.42344012320973, 4.517082290025428, 4.417703517014161, 4.656676267972216, 4.438157722121105, 4.209331799997017, 4.337904985994101, 4.72563558164984, 4.202679600799456, 4.3131542860064656, 5.0800639991648495, 4.166837419150397, 4.233520584879443, 4.322031663032249, 3.987868442898616, 3.887837915914133, 4.7912493431940675, 4.696160361170769, 4.470685878768563, 4.967087675118819, 4.318230634089559, 4.578838092973456, 3.997072590980679, 4.562198496889323, 4.31230599200353, 4.629744255915284, 9.533644078997895, 3.771136953961104, 3.8058154799509794, 3.843700709985569, 3.80289413430728, 3.9197561300825328, 3.968889348907396, 4.139833925059065, 4.147966081276536, 4.359186687273905, 4.115180616732687, 4.40298425918445, 3.9929984922055155, 4.129646398825571, 3.8360003859270364, 4.154960800195113, 4.4362591952085495, 4.045982276787981, 3.891302583971992, 3.737466603750363, 3.9263150221668184, 4.291641787160188, 3.9925013850443065, 4.0691952330525964, 3.8292030268348753, 3.9962141611613333, 4.807289110030979, 4.034951844019815, 4.550324131269008, 4.058791384100914, 4.129663116764277, 3.974584818119183, 4.046622463269159, 3.784770981175825, 8.84844662528485, 3.9616122010629624, 3.903841534163803, 3.876538683893159, 4.119760042056441, 3.874135131947696, 3.788800063310191, 3.9815352980513126, 4.053934939671308, 4.6487005380913615, 4.156515212962404, 3.8643463170155883, 4.0638162607792765, 3.8502266057766974, 4.011980260256678, 4.097684434382245, 3.81861932692118, 3.8542824459727854, 4.805456513073295, 4.4560190353076905, 4.083712598774582, 4.820282470900565, 4.377755165100098, 4.780718357069418, 4.0007810548413545, 4.284156847279519, 4.542887791292742, 3.8610806791111827, 4.313608421944082, 4.926487013930455, 3.991356454556808, 4.717637034598738, 4.378265774343163, 4.106164963915944, 7.630517017794773, 3.9508197652176023, 3.8498159961309284, 4.010812405729666, 3.8859476919751614, 3.932726122904569, 3.814139242982492, 4.037273271242157, 4.530481044901535, 4.029148834524676, 3.80371275707148, 4.246324151055887, 4.773896488361061, 4.142857528990135, 3.847112078918144, 4.09839550871402, 4.150416744174436, 4.454526741290465, 4.331959057133645, 3.8673964280169457, 3.9390274609904736, 4.321040540002286, 3.934481196803972, 4.028203779831529, 4.654609956080094, 4.12026389897801, 4.317440775223076, 4.797132689040154, 4.394130954286084, 4.292696401942521, 4.73105818987824, 4.733421717770398, 4.6963405860587955, 4.7482507231179625, 3.7441249289549887, 4.266045346157625, 4.236738533945754, 4.038095413008705, 8.117583862040192, 4.024642884964123, 3.7715060322079808, 3.8801628770306706, 3.9181853919290006, 3.8558021739590913, 3.8007624209858477, 3.989022580208257, 4.06567646516487, 4.093121469719335, 4.489199357805774, 3.9600507917348295, 4.0775601849891245, 4.055791802704334, 4.042763530043885, 4.100790811236948, 3.9031799738295376, 4.5807194211520255, 4.516657062340528, 4.463831427739933, 4.267902565654367, 4.530516900820658, 4.512263346929103, 4.071017985232174, 4.2521790659520775, 4.353567122016102, 4.325959412846714, 5.036627155030146, 4.300148104084656, 4.404170833993703, 4.70256954501383, 4.112917379010469, 4.005656425142661, 3.8876783640589565, 7.785591037943959, 3.796296581858769, 3.7897232898976654, 3.9444279549643397, 3.891161099774763, 4.006073076277971, 3.9283044850453734, 4.317486915038899, 4.525341983884573, 3.9071095581166446, 4.027263873955235, 3.903600732097402, 4.14649117202498, 4.206598052289337, 4.033868349157274, 4.046080203028396, 4.919406777014956, 4.477564206812531, 4.127161276061088, 3.983940879115835, 3.9720707098022103, 4.084882092196494, 4.113208438036963, 4.62457630969584, 4.091436405200511, 4.032056514872238, 4.109785626176745, 4.04459177213721, 3.9363459893502295, 4.455020877765492, 4.667817623121664, 4.184384585591033, 4.027767553925514, 4.560040378943086, 4.026829230133444, 3.8505373960360885, 8.066805265843868, 3.830336892977357, 4.0471556219272316, 3.8705995939671993, 3.833805002970621]
Total Epoch List: [67, 88, 65]
Total Time List: [1.2601661880034953, 1.242315156152472, 1.1860747220925987]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a060b8e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 1.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 1.59s
Epoch 3/1000, LR 0.000045
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 1.16s
Epoch 4/1000, LR 0.000075
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 1.61s
Epoch 5/1000, LR 0.000105
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.35s
Epoch 6/1000, LR 0.000135
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 1.39s
Epoch 7/1000, LR 0.000165
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 1.41s
Epoch 8/1000, LR 0.000195
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 1.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 1.24s
Epoch 9/1000, LR 0.000225
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 1.32s
Epoch 10/1000, LR 0.000255
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 1.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5039 time: 1.21s
Epoch 11/1000, LR 0.000285
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 1.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5039 time: 1.35s
Epoch 12/1000, LR 0.000285
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 1.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5039 time: 1.48s
Epoch 13/1000, LR 0.000285
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5039 time: 1.23s
Epoch 14/1000, LR 0.000285
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.4961 time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5039 time: 1.30s
Epoch 15/1000, LR 0.000285
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.4961 time: 1.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5039 time: 1.26s
Epoch 16/1000, LR 0.000285
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.4961 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5039 time: 1.29s
Epoch 17/1000, LR 0.000285
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.4961 time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6812 score: 0.5039 time: 1.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.4961 time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6783 score: 0.5039 time: 1.43s
Epoch 19/1000, LR 0.000285
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6711 score: 0.4961 time: 1.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6746 score: 0.5039 time: 1.32s
Epoch 20/1000, LR 0.000285
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6658 score: 0.4961 time: 1.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6701 score: 0.5039 time: 1.20s
Epoch 21/1000, LR 0.000285
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 1.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6594 score: 0.4961 time: 1.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6648 score: 0.5039 time: 1.39s
Epoch 22/1000, LR 0.000285
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6518 score: 0.4961 time: 1.31s
Test loss: 0.6584 score: 0.5116 time: 1.41s
Epoch 23/1000, LR 0.000285
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 1.23s
Val loss: 0.6431 score: 0.5116 time: 1.42s
Test loss: 0.6511 score: 0.5349 time: 1.21s
Epoch 24/1000, LR 0.000285
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 1.34s
Val loss: 0.6331 score: 0.5194 time: 1.33s
Test loss: 0.6426 score: 0.5426 time: 1.30s
Epoch 25/1000, LR 0.000285
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 1.24s
Val loss: 0.6214 score: 0.5271 time: 1.42s
Test loss: 0.6326 score: 0.5426 time: 1.23s
Epoch 26/1000, LR 0.000285
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 1.24s
Val loss: 0.6079 score: 0.5814 time: 1.44s
Test loss: 0.6210 score: 0.5581 time: 1.20s
Epoch 27/1000, LR 0.000285
Train loss: 0.6132;  Loss pred: 0.6132; Loss self: 0.0000; time: 1.40s
Val loss: 0.5924 score: 0.5891 time: 1.41s
Test loss: 0.6076 score: 0.5736 time: 1.61s
Epoch 28/1000, LR 0.000285
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 1.49s
Val loss: 0.5747 score: 0.6202 time: 1.68s
Test loss: 0.5922 score: 0.6279 time: 1.30s
Epoch 29/1000, LR 0.000285
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 1.19s
Val loss: 0.5551 score: 0.7364 time: 1.41s
Test loss: 0.5750 score: 0.7132 time: 1.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 2.79s
Val loss: 0.5338 score: 0.8217 time: 4.46s
Test loss: 0.5562 score: 0.7519 time: 1.28s
Epoch 31/1000, LR 0.000285
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 1.18s
Val loss: 0.5110 score: 0.8682 time: 1.39s
Test loss: 0.5359 score: 0.8295 time: 1.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 1.29s
Val loss: 0.4880 score: 0.8760 time: 1.29s
Test loss: 0.5149 score: 0.8605 time: 1.39s
Epoch 33/1000, LR 0.000285
Train loss: 0.4923;  Loss pred: 0.4923; Loss self: 0.0000; time: 1.26s
Val loss: 0.4659 score: 0.8837 time: 1.43s
Test loss: 0.4941 score: 0.8682 time: 1.23s
Epoch 34/1000, LR 0.000285
Train loss: 0.4665;  Loss pred: 0.4665; Loss self: 0.0000; time: 1.37s
Val loss: 0.4442 score: 0.9070 time: 1.27s
Test loss: 0.4733 score: 0.8682 time: 1.20s
Epoch 35/1000, LR 0.000285
Train loss: 0.4433;  Loss pred: 0.4433; Loss self: 0.0000; time: 1.28s
Val loss: 0.4249 score: 0.9070 time: 1.38s
Test loss: 0.4536 score: 0.8605 time: 1.27s
Epoch 36/1000, LR 0.000285
Train loss: 0.4172;  Loss pred: 0.4172; Loss self: 0.0000; time: 1.21s
Val loss: 0.4060 score: 0.9302 time: 1.37s
Test loss: 0.4341 score: 0.8605 time: 1.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.3945;  Loss pred: 0.3945; Loss self: 0.0000; time: 1.28s
Val loss: 0.3881 score: 0.9225 time: 1.30s
Test loss: 0.4150 score: 0.8605 time: 1.36s
Epoch 38/1000, LR 0.000284
Train loss: 0.3659;  Loss pred: 0.3659; Loss self: 0.0000; time: 1.27s
Val loss: 0.3720 score: 0.9225 time: 1.74s
Test loss: 0.3968 score: 0.8605 time: 1.46s
Epoch 39/1000, LR 0.000284
Train loss: 0.3479;  Loss pred: 0.3479; Loss self: 0.0000; time: 1.64s
Val loss: 0.3564 score: 0.9225 time: 1.41s
Test loss: 0.3792 score: 0.8682 time: 1.37s
Epoch 40/1000, LR 0.000284
Train loss: 0.3322;  Loss pred: 0.3322; Loss self: 0.0000; time: 1.22s
Val loss: 0.3425 score: 0.9147 time: 1.50s
Test loss: 0.3628 score: 0.8682 time: 1.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.3086;  Loss pred: 0.3086; Loss self: 0.0000; time: 1.35s
Val loss: 0.3281 score: 0.9147 time: 1.31s
Test loss: 0.3465 score: 0.8760 time: 1.22s
Epoch 42/1000, LR 0.000284
Train loss: 0.2920;  Loss pred: 0.2920; Loss self: 0.0000; time: 1.37s
Val loss: 0.3145 score: 0.9147 time: 1.31s
Test loss: 0.3315 score: 0.8837 time: 1.32s
Epoch 43/1000, LR 0.000284
Train loss: 0.2724;  Loss pred: 0.2724; Loss self: 0.0000; time: 1.25s
Val loss: 0.3048 score: 0.9147 time: 1.44s
Test loss: 0.3191 score: 0.8837 time: 1.34s
Epoch 44/1000, LR 0.000284
Train loss: 0.2489;  Loss pred: 0.2489; Loss self: 0.0000; time: 1.44s
Val loss: 0.2980 score: 0.9147 time: 1.27s
Test loss: 0.3088 score: 0.8837 time: 1.52s
Epoch 45/1000, LR 0.000284
Train loss: 0.2363;  Loss pred: 0.2363; Loss self: 0.0000; time: 1.23s
Val loss: 0.2934 score: 0.9147 time: 1.45s
Test loss: 0.3006 score: 0.8760 time: 1.20s
Epoch 46/1000, LR 0.000284
Train loss: 0.2250;  Loss pred: 0.2250; Loss self: 0.0000; time: 1.35s
Val loss: 0.2894 score: 0.9147 time: 1.28s
Test loss: 0.2934 score: 0.8760 time: 1.63s
Epoch 47/1000, LR 0.000284
Train loss: 0.2173;  Loss pred: 0.2173; Loss self: 0.0000; time: 1.51s
Val loss: 0.2853 score: 0.9070 time: 1.76s
Test loss: 0.2867 score: 0.8837 time: 1.48s
Epoch 48/1000, LR 0.000284
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 1.66s
Val loss: 0.2854 score: 0.9070 time: 1.57s
Test loss: 0.2832 score: 0.8837 time: 1.18s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.2085;  Loss pred: 0.2085; Loss self: 0.0000; time: 1.35s
Val loss: 0.2852 score: 0.9070 time: 1.36s
Test loss: 0.2799 score: 0.8837 time: 1.58s
Epoch 50/1000, LR 0.000284
Train loss: 0.1889;  Loss pred: 0.1889; Loss self: 0.0000; time: 1.24s
Val loss: 0.2879 score: 0.9070 time: 1.52s
Test loss: 0.2787 score: 0.8837 time: 1.53s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1924;  Loss pred: 0.1924; Loss self: 0.0000; time: 1.69s
Val loss: 0.2937 score: 0.9070 time: 1.59s
Test loss: 0.2798 score: 0.8837 time: 1.41s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1762;  Loss pred: 0.1762; Loss self: 0.0000; time: 1.22s
Val loss: 0.3010 score: 0.9070 time: 1.46s
Test loss: 0.2825 score: 0.8760 time: 1.42s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1868;  Loss pred: 0.1868; Loss self: 0.0000; time: 1.51s
Val loss: 0.3085 score: 0.9070 time: 1.38s
Test loss: 0.2859 score: 0.8837 time: 1.23s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1792;  Loss pred: 0.1792; Loss self: 0.0000; time: 1.37s
Val loss: 0.3132 score: 0.9070 time: 1.31s
Test loss: 0.2862 score: 0.8837 time: 1.33s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 1.38s
Val loss: 0.3149 score: 0.9070 time: 1.77s
Test loss: 0.2835 score: 0.8837 time: 1.33s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1721;  Loss pred: 0.1721; Loss self: 0.0000; time: 1.49s
Val loss: 0.3179 score: 0.9070 time: 1.60s
Test loss: 0.2820 score: 0.8760 time: 1.33s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1675;  Loss pred: 0.1675; Loss self: 0.0000; time: 1.23s
Val loss: 0.3234 score: 0.9070 time: 1.47s
Test loss: 0.2829 score: 0.8760 time: 1.19s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1689;  Loss pred: 0.1689; Loss self: 0.0000; time: 1.32s
Val loss: 0.3307 score: 0.9147 time: 1.47s
Test loss: 0.2851 score: 0.8837 time: 1.21s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1673;  Loss pred: 0.1673; Loss self: 0.0000; time: 1.32s
Val loss: 0.3365 score: 0.9147 time: 1.28s
Test loss: 0.2857 score: 0.8837 time: 1.61s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 1.28s
Val loss: 0.3369 score: 0.9147 time: 1.70s
Test loss: 0.2818 score: 0.8837 time: 1.19s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 1.51s
Val loss: 0.3412 score: 0.9147 time: 1.28s
Test loss: 0.2813 score: 0.8915 time: 1.35s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 1.26s
Val loss: 0.3476 score: 0.9147 time: 1.43s
Test loss: 0.2826 score: 0.8915 time: 1.18s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 1.27s
Val loss: 0.3474 score: 0.9147 time: 1.65s
Test loss: 0.2790 score: 0.8915 time: 1.55s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1516;  Loss pred: 0.1516; Loss self: 0.0000; time: 1.63s
Val loss: 0.3501 score: 0.9147 time: 1.70s
Test loss: 0.2776 score: 0.8915 time: 1.64s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1429;  Loss pred: 0.1429; Loss self: 0.0000; time: 1.46s
Val loss: 0.3569 score: 0.9225 time: 1.78s
Test loss: 0.2788 score: 0.8992 time: 1.54s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 1.57s
Val loss: 0.3660 score: 0.9302 time: 1.43s
Test loss: 0.2820 score: 0.8992 time: 1.53s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1333;  Loss pred: 0.1333; Loss self: 0.0000; time: 1.51s
Val loss: 0.3709 score: 0.9302 time: 1.58s
Test loss: 0.2828 score: 0.8992 time: 5.56s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1229;  Loss pred: 0.1229; Loss self: 0.0000; time: 1.34s
Val loss: 0.3716 score: 0.9302 time: 1.24s
Test loss: 0.2813 score: 0.8992 time: 1.32s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 1.20s
Val loss: 0.3693 score: 0.9302 time: 1.31s
Test loss: 0.2784 score: 0.8992 time: 1.33s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.2085,   Val_Loss: 0.2852,   Val_Precision: 0.9333,   Val_Recall: 0.8750,   Val_accuracy: 0.9032,   Val_Score: 0.9070,   Val_Loss: 0.2852,   Test_Precision: 0.9167,   Test_Recall: 0.8462,   Test_accuracy: 0.8800,   Test_Score: 0.8837,   Test_loss: 0.2799


[1.1839197729714215, 1.5982667850330472, 1.165575867984444, 1.6206485060974956, 1.3517932968679816, 1.3988368359860033, 1.4156071478500962, 1.2422216369304806, 1.3297108199913055, 1.2125387880951166, 1.353949174983427, 1.4919962659478188, 1.2324072821065784, 1.30850166012533, 1.2624288590159267, 1.2914791940711439, 1.1854721039999276, 1.4394726199097931, 1.3301333899144083, 1.2002927190624177, 1.399288160027936, 1.4105318698566407, 1.2190679491031915, 1.3095926740206778, 1.2363076058682054, 1.2089056039694697, 1.612848880002275, 1.3012240501120687, 1.1797539091203362, 1.2848882649559528, 1.1529305779840797, 1.4026351920329034, 1.2397842050995678, 1.2032111519947648, 1.2766174760181457, 1.1698085248935968, 1.3612820960115641, 1.4680215159896761, 1.3776488029398024, 1.2108107439707965, 1.2296306481584907, 1.3202206071000546, 1.3515591889154166, 1.524498637067154, 1.2026863400824368, 1.633078402839601, 1.4899093490093946, 1.1857657060027122, 1.5887425320688635, 1.5384087851271033, 1.4148597619496286, 1.422523746965453, 1.2373529837932438, 1.3362804960925132, 1.330696902005002, 1.3343762590084225, 1.193378210067749, 1.2141608640085906, 1.6180610021110624, 1.19053024193272, 1.359632310923189, 1.1862025309819728, 1.5560418139211833, 1.647277822950855, 1.5498370840214193, 1.5340723190456629, 5.564302900107577, 1.324467923026532, 1.3409048828762025]
[0.00917767265869319, 0.01238966500025618, 0.00903547184484065, 0.012563166713934074, 0.010479017805178152, 0.010843696402992273, 0.010973698820543382, 0.009629625092484346, 0.010307835813886088, 0.00939952548910943, 0.010495730038631216, 0.011565862526727278, 0.009553544822531615, 0.010143423721901784, 0.009786270224929665, 0.010011466620706542, 0.009189706232557578, 0.011158702479920877, 0.010311111549724096, 0.009304594721414091, 0.010847195038976248, 0.010934355580284036, 0.009450139140334818, 0.010151881193958743, 0.009583779890451205, 0.009371361271081161, 0.012502704496141668, 0.010087008140403633, 0.009145379140467723, 0.00996037414694537, 0.008937446340961858, 0.01087314102351088, 0.009610730272089673, 0.009327218232517557, 0.009896259504016633, 0.009068283138710052, 0.010552574387686543, 0.011380011751857955, 0.01067944808480467, 0.009386129798223229, 0.009532020528360392, 0.010234268272093446, 0.010477203014848191, 0.011817818891993442, 0.009323149923119665, 0.012659522502632567, 0.011549684876041818, 0.00919198221707529, 0.012315833581929175, 0.011925649497109328, 0.01096790513139247, 0.011027315867949248, 0.009591883595296464, 0.010358763535600878, 0.010315479860503891, 0.01034400200781723, 0.009250993876494178, 0.009412099720996827, 0.012543108543496608, 0.009228916604129611, 0.010539785355993713, 0.009195368457224596, 0.01206233964279987, 0.012769595526750813, 0.012014240961406351, 0.01189203348097413, 0.043134131008585874, 0.010267193201756062, 0.01039461149516436]
[108.96008576344127, 80.71243249751491, 110.67490632168928, 79.59776565655845, 95.42879099850897, 92.21947598275197, 91.12697699775974, 103.84620277485902, 97.01357472660371, 106.38834919471516, 95.27684080281598, 86.46134239354166, 104.67318870389805, 98.58604228873826, 102.1839758167098, 99.88546512573068, 108.8174066388726, 89.61615401068482, 96.98275449524722, 107.47378364568063, 92.18973166858252, 91.4548637693035, 105.81854776421525, 98.50391084118355, 104.3429639902675, 106.70808339081687, 79.98269496880454, 99.13742371184256, 109.34483793843641, 100.3978349856143, 111.88878364693811, 91.96974433033752, 104.05036575670839, 107.21310202796498, 101.04827986716862, 110.27445710547678, 94.76360585212907, 87.87337146965031, 93.63779776436736, 106.54018445273339, 104.90955165536246, 97.71094263053233, 95.44532052903907, 84.61798316079279, 107.25988622366646, 78.99192088738327, 86.58244884883032, 108.79046286037972, 81.19629039704499, 83.85287528720269, 91.17511393655184, 90.68389914416863, 104.25480981549508, 96.53661815555608, 96.94168507165811, 96.67438185378099, 108.09649356064291, 106.24621812805134, 79.7250535249879, 108.35508033007207, 94.8785925162437, 108.75040023157771, 82.90265650054961, 78.31101603062655, 83.23455499288929, 84.08990788664391, 23.183497073372116, 97.39760228033538, 96.20369173635844]
Elapsed: 1.4066648149721162~0.5229456906157623
Time per graph: 0.01090437841063656~0.004053842562912887
Speed: 95.79804428099507~12.971470912930373
Total Time: 1.3417
best val loss: 0.28520548597786777 test_score: 0.8837

Testing...
Test loss: 0.4341 score: 0.8605 time: 1.23s
test Score 0.8605
Epoch Time List: [3.7930302037857473, 4.41567610995844, 3.7871032061520964, 4.653760149143636, 3.9914206170942634, 4.11238209111616, 4.231614408781752, 3.8790774680674076, 4.117794113699347, 4.005853175185621, 4.067787413252518, 4.466356138931587, 3.965818125056103, 3.9779428211040795, 4.344314276240766, 3.8484785321634263, 3.841660025063902, 4.022199902450666, 4.468978880904615, 4.4147861499805, 4.331436178879812, 3.92236438812688, 3.8623894788324833, 3.9821339738555253, 3.89097563480027, 3.8782091378234327, 4.412222251761705, 4.467189346672967, 3.7806163509376347, 8.531422794796526, 3.7210451727733016, 3.975593271199614, 3.9237871549557894, 3.8376648887060583, 3.9303262799512595, 3.7451506808865815, 3.9365781978704035, 4.475616718875244, 4.4233812619932, 3.9340548431500793, 3.878435217309743, 3.994853446725756, 4.032368524000049, 4.223747457843274, 3.8861184930428863, 4.259393736021593, 4.75359738082625, 4.409643871244043, 4.286538890795782, 4.293566727079451, 4.689657811075449, 4.08633211790584, 4.117895420873538, 4.003019927069545, 4.470634600147605, 4.414519644808024, 3.8893583798781037, 4.004652299918234, 4.2178150753024966, 4.163179727969691, 4.142677313880995, 3.876961291069165, 4.468080376973376, 4.96623676083982, 4.784716246882454, 4.5306052409578115, 8.6428880286403, 3.903046804945916, 3.8543147940654308]
Total Epoch List: [69]
Total Time List: [1.3416893610265106]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a03743a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 1.29s
Epoch 2/1000, LR 0.000015
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5039 time: 1.22s
Epoch 3/1000, LR 0.000045
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 1.37s
Epoch 4/1000, LR 0.000075
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 1.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 1.29s
Epoch 5/1000, LR 0.000105
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 1.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 1.28s
Epoch 6/1000, LR 0.000135
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 1.52s
Epoch 7/1000, LR 0.000165
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 1.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 1.49s
Epoch 8/1000, LR 0.000195
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 1.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 1.34s
Epoch 9/1000, LR 0.000225
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5039 time: 1.20s
Epoch 10/1000, LR 0.000255
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4961 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5039 time: 1.41s
Epoch 11/1000, LR 0.000285
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 1.36s
Val loss: 0.6882 score: 0.5039 time: 1.25s
Test loss: 0.6888 score: 0.5194 time: 1.37s
Epoch 12/1000, LR 0.000285
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 1.55s
Val loss: 0.6870 score: 0.6279 time: 1.38s
Test loss: 0.6878 score: 0.5814 time: 1.22s
Epoch 13/1000, LR 0.000285
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 1.39s
Val loss: 0.6855 score: 0.6744 time: 1.38s
Test loss: 0.6867 score: 0.6589 time: 1.34s
Epoch 14/1000, LR 0.000285
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 1.25s
Val loss: 0.6838 score: 0.7209 time: 1.40s
Test loss: 0.6853 score: 0.7287 time: 1.23s
Epoch 15/1000, LR 0.000285
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 1.35s
Val loss: 0.6817 score: 0.7829 time: 1.71s
Test loss: 0.6836 score: 0.7829 time: 1.20s
Epoch 16/1000, LR 0.000285
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 1.41s
Val loss: 0.6793 score: 0.7829 time: 1.25s
Test loss: 0.6816 score: 0.7597 time: 1.34s
Epoch 17/1000, LR 0.000285
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 1.24s
Val loss: 0.6762 score: 0.7752 time: 1.38s
Test loss: 0.6791 score: 0.7674 time: 1.23s
Epoch 18/1000, LR 0.000285
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 1.41s
Val loss: 0.6725 score: 0.7364 time: 1.42s
Test loss: 0.6762 score: 0.6977 time: 1.40s
Epoch 19/1000, LR 0.000285
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 1.29s
Val loss: 0.6682 score: 0.7287 time: 1.55s
Test loss: 0.6727 score: 0.6822 time: 1.55s
Epoch 20/1000, LR 0.000285
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 1.68s
Val loss: 0.6632 score: 0.7054 time: 1.64s
Test loss: 0.6687 score: 0.6124 time: 1.52s
Epoch 21/1000, LR 0.000285
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 1.37s
Val loss: 0.6574 score: 0.6589 time: 1.27s
Test loss: 0.6641 score: 0.5891 time: 1.71s
Epoch 22/1000, LR 0.000285
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 1.58s
Val loss: 0.6506 score: 0.6822 time: 1.75s
Test loss: 0.6586 score: 0.5814 time: 1.21s
Epoch 23/1000, LR 0.000285
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 1.56s
Val loss: 0.6425 score: 0.6822 time: 1.59s
Test loss: 0.6521 score: 0.5814 time: 1.64s
Epoch 24/1000, LR 0.000285
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 1.24s
Val loss: 0.6334 score: 0.6822 time: 1.39s
Test loss: 0.6449 score: 0.5814 time: 1.22s
Epoch 25/1000, LR 0.000285
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 1.45s
Val loss: 0.6230 score: 0.6822 time: 1.34s
Test loss: 0.6365 score: 0.5814 time: 1.33s
Epoch 26/1000, LR 0.000285
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 1.31s
Val loss: 0.6114 score: 0.6822 time: 1.54s
Test loss: 0.6271 score: 0.5814 time: 1.73s
Epoch 27/1000, LR 0.000285
Train loss: 0.6127;  Loss pred: 0.6127; Loss self: 0.0000; time: 1.54s
Val loss: 0.5986 score: 0.6822 time: 1.66s
Test loss: 0.6167 score: 0.5891 time: 1.26s
Epoch 28/1000, LR 0.000285
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 1.35s
Val loss: 0.5845 score: 0.6822 time: 1.25s
Test loss: 0.6050 score: 0.6124 time: 1.32s
Epoch 29/1000, LR 0.000285
Train loss: 0.5890;  Loss pred: 0.5890; Loss self: 0.0000; time: 1.31s
Val loss: 0.5693 score: 0.7054 time: 1.49s
Test loss: 0.5923 score: 0.6124 time: 1.39s
Epoch 30/1000, LR 0.000285
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 1.24s
Val loss: 0.5531 score: 0.7364 time: 1.38s
Test loss: 0.5786 score: 0.6512 time: 1.21s
Epoch 31/1000, LR 0.000285
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 1.34s
Val loss: 0.5359 score: 0.7519 time: 1.22s
Test loss: 0.5636 score: 0.6822 time: 1.28s
Epoch 32/1000, LR 0.000285
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 1.24s
Val loss: 0.5178 score: 0.7984 time: 1.64s
Test loss: 0.5476 score: 0.7364 time: 5.58s
Epoch 33/1000, LR 0.000285
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 1.39s
Val loss: 0.4990 score: 0.8527 time: 1.23s
Test loss: 0.5306 score: 0.8140 time: 1.34s
Epoch 34/1000, LR 0.000285
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 1.24s
Val loss: 0.4793 score: 0.8760 time: 1.27s
Test loss: 0.5128 score: 0.8372 time: 1.33s
Epoch 35/1000, LR 0.000285
Train loss: 0.4782;  Loss pred: 0.4782; Loss self: 0.0000; time: 1.35s
Val loss: 0.4590 score: 0.8760 time: 1.24s
Test loss: 0.4946 score: 0.8682 time: 1.22s
Epoch 36/1000, LR 0.000285
Train loss: 0.4594;  Loss pred: 0.4594; Loss self: 0.0000; time: 1.34s
Val loss: 0.4384 score: 0.8915 time: 1.27s
Test loss: 0.4764 score: 0.8837 time: 1.31s
Epoch 37/1000, LR 0.000285
Train loss: 0.4369;  Loss pred: 0.4369; Loss self: 0.0000; time: 1.27s
Val loss: 0.4189 score: 0.8992 time: 1.39s
Test loss: 0.4596 score: 0.8837 time: 1.23s
Epoch 38/1000, LR 0.000284
Train loss: 0.4201;  Loss pred: 0.4201; Loss self: 0.0000; time: 1.34s
Val loss: 0.4002 score: 0.9147 time: 1.26s
Test loss: 0.4442 score: 0.8760 time: 1.43s
Epoch 39/1000, LR 0.000284
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 1.24s
Val loss: 0.3825 score: 0.9147 time: 1.42s
Test loss: 0.4306 score: 0.8760 time: 1.36s
Epoch 40/1000, LR 0.000284
Train loss: 0.3815;  Loss pred: 0.3815; Loss self: 0.0000; time: 1.25s
Val loss: 0.3654 score: 0.9147 time: 1.40s
Test loss: 0.4183 score: 0.8682 time: 1.23s
Epoch 41/1000, LR 0.000284
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 1.69s
Val loss: 0.3488 score: 0.9147 time: 1.54s
Test loss: 0.4070 score: 0.8605 time: 1.40s
Epoch 42/1000, LR 0.000284
Train loss: 0.3443;  Loss pred: 0.3443; Loss self: 0.0000; time: 1.24s
Val loss: 0.3332 score: 0.9147 time: 1.39s
Test loss: 0.3970 score: 0.8760 time: 1.20s
Epoch 43/1000, LR 0.000284
Train loss: 0.3264;  Loss pred: 0.3264; Loss self: 0.0000; time: 1.37s
Val loss: 0.3183 score: 0.9225 time: 1.26s
Test loss: 0.3881 score: 0.8760 time: 1.38s
Epoch 44/1000, LR 0.000284
Train loss: 0.3087;  Loss pred: 0.3087; Loss self: 0.0000; time: 1.56s
Val loss: 0.3036 score: 0.9225 time: 1.61s
Test loss: 0.3800 score: 0.8760 time: 1.21s
Epoch 45/1000, LR 0.000284
Train loss: 0.2878;  Loss pred: 0.2878; Loss self: 0.0000; time: 1.29s
Val loss: 0.2896 score: 0.9302 time: 1.38s
Test loss: 0.3729 score: 0.8760 time: 1.20s
Epoch 46/1000, LR 0.000284
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 1.40s
Val loss: 0.2758 score: 0.9225 time: 1.48s
Test loss: 0.3664 score: 0.8760 time: 1.55s
Epoch 47/1000, LR 0.000284
Train loss: 0.2662;  Loss pred: 0.2662; Loss self: 0.0000; time: 1.28s
Val loss: 0.2630 score: 0.9302 time: 1.40s
Test loss: 0.3611 score: 0.8760 time: 1.24s
Epoch 48/1000, LR 0.000284
Train loss: 0.2565;  Loss pred: 0.2565; Loss self: 0.0000; time: 1.39s
Val loss: 0.2512 score: 0.9147 time: 1.30s
Test loss: 0.3570 score: 0.8760 time: 1.34s
Epoch 49/1000, LR 0.000284
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 1.27s
Val loss: 0.2406 score: 0.9147 time: 1.56s
Test loss: 0.3540 score: 0.8682 time: 1.32s
Epoch 50/1000, LR 0.000284
Train loss: 0.2238;  Loss pred: 0.2238; Loss self: 0.0000; time: 1.22s
Val loss: 0.2300 score: 0.9225 time: 1.39s
Test loss: 0.3513 score: 0.8760 time: 1.21s
Epoch 51/1000, LR 0.000284
Train loss: 0.2173;  Loss pred: 0.2173; Loss self: 0.0000; time: 1.60s
Val loss: 0.2204 score: 0.9302 time: 1.26s
Test loss: 0.3511 score: 0.8837 time: 1.34s
Epoch 52/1000, LR 0.000284
Train loss: 0.2040;  Loss pred: 0.2040; Loss self: 0.0000; time: 1.23s
Val loss: 0.2120 score: 0.9302 time: 1.63s
Test loss: 0.3529 score: 0.8837 time: 1.55s
Epoch 53/1000, LR 0.000284
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 1.61s
Val loss: 0.2043 score: 0.9302 time: 1.59s
Test loss: 0.3562 score: 0.8837 time: 1.63s
Epoch 54/1000, LR 0.000284
Train loss: 0.1870;  Loss pred: 0.1870; Loss self: 0.0000; time: 1.26s
Val loss: 0.1977 score: 0.9225 time: 1.45s
Test loss: 0.3609 score: 0.8837 time: 1.19s
Epoch 55/1000, LR 0.000284
Train loss: 0.1869;  Loss pred: 0.1869; Loss self: 0.0000; time: 1.42s
Val loss: 0.1933 score: 0.9225 time: 1.35s
Test loss: 0.3630 score: 0.8837 time: 1.65s
Epoch 56/1000, LR 0.000284
Train loss: 0.1657;  Loss pred: 0.1657; Loss self: 0.0000; time: 1.58s
Val loss: 0.1906 score: 0.9225 time: 1.50s
Test loss: 0.3686 score: 0.8837 time: 1.22s
Epoch 57/1000, LR 0.000283
Train loss: 0.1639;  Loss pred: 0.1639; Loss self: 0.0000; time: 1.24s
Val loss: 0.1876 score: 0.9225 time: 1.40s
Test loss: 0.3760 score: 0.8837 time: 1.33s
Epoch 58/1000, LR 0.000283
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 1.22s
Val loss: 0.1835 score: 0.9225 time: 1.36s
Test loss: 0.3844 score: 0.8837 time: 1.19s
Epoch 59/1000, LR 0.000283
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 1.22s
Val loss: 0.1788 score: 0.9147 time: 1.42s
Test loss: 0.3950 score: 0.8915 time: 1.50s
Epoch 60/1000, LR 0.000283
Train loss: 0.1468;  Loss pred: 0.1468; Loss self: 0.0000; time: 1.46s
Val loss: 0.1773 score: 0.9457 time: 1.57s
Test loss: 0.4101 score: 0.8837 time: 1.39s
Epoch 61/1000, LR 0.000283
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 1.23s
Val loss: 0.1783 score: 0.9302 time: 1.58s
Test loss: 0.4237 score: 0.8837 time: 1.51s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1330;  Loss pred: 0.1330; Loss self: 0.0000; time: 1.41s
Val loss: 0.1767 score: 0.9302 time: 1.25s
Test loss: 0.4314 score: 0.8837 time: 1.34s
Epoch 63/1000, LR 0.000283
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 1.30s
Val loss: 0.1750 score: 0.9457 time: 1.70s
Test loss: 0.4367 score: 0.8915 time: 1.51s
Epoch 64/1000, LR 0.000283
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 1.29s
Val loss: 0.1777 score: 0.9302 time: 1.43s
Test loss: 0.4412 score: 0.8915 time: 1.19s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 1.39s
Val loss: 0.1833 score: 0.9302 time: 1.24s
Test loss: 0.4494 score: 0.8992 time: 1.34s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1108;  Loss pred: 0.1108; Loss self: 0.0000; time: 1.29s
Val loss: 0.1841 score: 0.9302 time: 1.40s
Test loss: 0.4580 score: 0.8992 time: 1.20s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1071;  Loss pred: 0.1071; Loss self: 0.0000; time: 1.36s
Val loss: 0.1842 score: 0.9302 time: 1.44s
Test loss: 0.4667 score: 0.8992 time: 1.23s
     INFO: Early stopping counter 4 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 1.35s
Val loss: 0.1814 score: 0.9457 time: 1.27s
Test loss: 0.4774 score: 0.8992 time: 1.57s
     INFO: Early stopping counter 5 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 4.79s
Val loss: 0.1805 score: 0.9380 time: 1.68s
Test loss: 0.4890 score: 0.8992 time: 1.20s
     INFO: Early stopping counter 6 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.1120;  Loss pred: 0.1120; Loss self: 0.0000; time: 1.35s
Val loss: 0.1819 score: 0.9380 time: 1.23s
Test loss: 0.4958 score: 0.9070 time: 1.34s
     INFO: Early stopping counter 7 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.1011;  Loss pred: 0.1011; Loss self: 0.0000; time: 1.21s
Val loss: 0.1836 score: 0.9380 time: 1.33s
Test loss: 0.5016 score: 0.8992 time: 1.18s
     INFO: Early stopping counter 8 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 1.21s
Val loss: 0.1854 score: 0.9380 time: 1.36s
Test loss: 0.5073 score: 0.9070 time: 1.19s
     INFO: Early stopping counter 9 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 1.37s
Val loss: 0.1873 score: 0.9380 time: 1.24s
Test loss: 0.5133 score: 0.9070 time: 1.31s
     INFO: Early stopping counter 10 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 1.29s
Val loss: 0.1898 score: 0.9380 time: 1.39s
Test loss: 0.5185 score: 0.8992 time: 1.22s
     INFO: Early stopping counter 11 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0859;  Loss pred: 0.0859; Loss self: 0.0000; time: 1.41s
Val loss: 0.1911 score: 0.9380 time: 1.30s
Test loss: 0.5253 score: 0.8992 time: 1.32s
     INFO: Early stopping counter 12 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 1.27s
Val loss: 0.1936 score: 0.9302 time: 1.42s
Test loss: 0.5313 score: 0.8992 time: 1.23s
     INFO: Early stopping counter 13 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 1.42s
Val loss: 0.1941 score: 0.9302 time: 1.25s
Test loss: 0.5375 score: 0.8992 time: 1.23s
     INFO: Early stopping counter 14 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 1.51s
Val loss: 0.1956 score: 0.9302 time: 1.49s
Test loss: 0.5437 score: 0.8915 time: 1.25s
     INFO: Early stopping counter 15 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 1.33s
Val loss: 0.1956 score: 0.9302 time: 1.52s
Test loss: 0.5493 score: 0.8915 time: 1.32s
     INFO: Early stopping counter 16 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 1.57s
Val loss: 0.1948 score: 0.9302 time: 1.24s
Test loss: 0.5554 score: 0.8915 time: 1.34s
     INFO: Early stopping counter 17 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 1.21s
Val loss: 0.1919 score: 0.9380 time: 1.38s
Test loss: 0.5627 score: 0.8992 time: 1.35s
     INFO: Early stopping counter 18 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 1.56s
Val loss: 0.1901 score: 0.9380 time: 1.25s
Test loss: 0.5701 score: 0.9070 time: 1.34s
     INFO: Early stopping counter 19 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 1.51s
Val loss: 0.1881 score: 0.9380 time: 1.56s
Test loss: 0.5832 score: 0.9070 time: 1.41s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 062,   Train_Loss: 0.1335,   Val_Loss: 0.1750,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.1750,   Test_Precision: 0.9167,   Test_Recall: 0.8594,   Test_accuracy: 0.8871,   Test_Score: 0.8915,   Test_loss: 0.4367


[1.1839197729714215, 1.5982667850330472, 1.165575867984444, 1.6206485060974956, 1.3517932968679816, 1.3988368359860033, 1.4156071478500962, 1.2422216369304806, 1.3297108199913055, 1.2125387880951166, 1.353949174983427, 1.4919962659478188, 1.2324072821065784, 1.30850166012533, 1.2624288590159267, 1.2914791940711439, 1.1854721039999276, 1.4394726199097931, 1.3301333899144083, 1.2002927190624177, 1.399288160027936, 1.4105318698566407, 1.2190679491031915, 1.3095926740206778, 1.2363076058682054, 1.2089056039694697, 1.612848880002275, 1.3012240501120687, 1.1797539091203362, 1.2848882649559528, 1.1529305779840797, 1.4026351920329034, 1.2397842050995678, 1.2032111519947648, 1.2766174760181457, 1.1698085248935968, 1.3612820960115641, 1.4680215159896761, 1.3776488029398024, 1.2108107439707965, 1.2296306481584907, 1.3202206071000546, 1.3515591889154166, 1.524498637067154, 1.2026863400824368, 1.633078402839601, 1.4899093490093946, 1.1857657060027122, 1.5887425320688635, 1.5384087851271033, 1.4148597619496286, 1.422523746965453, 1.2373529837932438, 1.3362804960925132, 1.330696902005002, 1.3343762590084225, 1.193378210067749, 1.2141608640085906, 1.6180610021110624, 1.19053024193272, 1.359632310923189, 1.1862025309819728, 1.5560418139211833, 1.647277822950855, 1.5498370840214193, 1.5340723190456629, 5.564302900107577, 1.324467923026532, 1.3409048828762025, 1.2939557021018118, 1.2269423420075327, 1.3722399030812085, 1.2957358618732542, 1.2887712009251118, 1.5203156410716474, 1.4932508210185915, 1.3503857220057398, 1.2076540009584278, 1.415711808949709, 1.3781412469688803, 1.2293628419283777, 1.3452189648523927, 1.2375914750155061, 1.2080138260498643, 1.3419437280390412, 1.2331611157860607, 1.4029994159936905, 1.5596722720656544, 1.5204232251271605, 1.7154951598495245, 1.2190001669805497, 1.6430079119745642, 1.2263320090714842, 1.3398558059707284, 1.7331789478193969, 1.2667200369760394, 1.3260013349354267, 1.3955374381039292, 1.2135207338724285, 1.2852641029749066, 5.584082592977211, 1.3480600691400468, 1.3394758789800107, 1.2244055150076747, 1.3177198299672455, 1.2338705910369754, 1.4322611929383129, 1.3667606429662555, 1.232851053122431, 1.407081374200061, 1.2053984839003533, 1.3882243200205266, 1.2129831509664655, 1.2044009228702635, 1.5596670841332525, 1.2487986779306084, 1.3405400218907744, 1.326972981914878, 1.2197561780922115, 1.3449388819281012, 1.5623022539075464, 1.635525605874136, 1.1976617409382015, 1.6611556359566748, 1.2238170488271862, 1.339724998921156, 1.1919766410719603, 1.507482165005058, 1.3918980970047414, 1.5145056250039488, 1.3484979530330747, 1.5123495790176094, 1.196882663993165, 1.3491057849023491, 1.2059402691666037, 1.2377279859501868, 1.575334883062169, 1.205059350002557, 1.3498807868454605, 1.1811844760086387, 1.1933767299633473, 1.316313642077148, 1.2231167159043252, 1.3294048358220607, 1.2303870618343353, 1.2374783239793032, 1.2575692948885262, 1.3319304250180721, 1.3452170479577035, 1.3599801599048078, 1.34375160606578, 1.4176769840996712]
[0.00917767265869319, 0.01238966500025618, 0.00903547184484065, 0.012563166713934074, 0.010479017805178152, 0.010843696402992273, 0.010973698820543382, 0.009629625092484346, 0.010307835813886088, 0.00939952548910943, 0.010495730038631216, 0.011565862526727278, 0.009553544822531615, 0.010143423721901784, 0.009786270224929665, 0.010011466620706542, 0.009189706232557578, 0.011158702479920877, 0.010311111549724096, 0.009304594721414091, 0.010847195038976248, 0.010934355580284036, 0.009450139140334818, 0.010151881193958743, 0.009583779890451205, 0.009371361271081161, 0.012502704496141668, 0.010087008140403633, 0.009145379140467723, 0.00996037414694537, 0.008937446340961858, 0.01087314102351088, 0.009610730272089673, 0.009327218232517557, 0.009896259504016633, 0.009068283138710052, 0.010552574387686543, 0.011380011751857955, 0.01067944808480467, 0.009386129798223229, 0.009532020528360392, 0.010234268272093446, 0.010477203014848191, 0.011817818891993442, 0.009323149923119665, 0.012659522502632567, 0.011549684876041818, 0.00919198221707529, 0.012315833581929175, 0.011925649497109328, 0.01096790513139247, 0.011027315867949248, 0.009591883595296464, 0.010358763535600878, 0.010315479860503891, 0.01034400200781723, 0.009250993876494178, 0.009412099720996827, 0.012543108543496608, 0.009228916604129611, 0.010539785355993713, 0.009195368457224596, 0.01206233964279987, 0.012769595526750813, 0.012014240961406351, 0.01189203348097413, 0.043134131008585874, 0.010267193201756062, 0.01039461149516436, 0.010030664357378385, 0.009511180945794827, 0.0106375186285365, 0.010044464045529103, 0.00999047442577606, 0.01178539256644688, 0.011575587759834043, 0.010468106372137518, 0.009361658922158355, 0.010974510146896968, 0.010683265480378916, 0.009529944511072697, 0.010428053991103818, 0.009593732364461288, 0.009364448263952437, 0.01040266455844218, 0.009559388494465587, 0.010875964465067368, 0.012090482729191119, 0.011786226551373338, 0.01329841209185678, 0.00944961369752364, 0.012736495441663288, 0.009506449682724684, 0.010386479116052158, 0.013435495719530208, 0.009819535170356894, 0.0102790801157785, 0.010818119675224258, 0.00940713747187929, 0.009963287619960516, 0.04328746196106365, 0.0104500780553492, 0.010383533945581478, 0.009491515620214533, 0.010214882402846863, 0.009564888302612212, 0.01110279994525824, 0.010595043743924462, 0.009556984907925822, 0.010907607551938456, 0.009344174293801188, 0.010761428837368423, 0.00940297016253074, 0.009336441262560181, 0.012090442512660872, 0.009680609906438825, 0.01039178311543236, 0.01028661226290603, 0.009455474248776833, 0.010425882805644195, 0.012110870185329817, 0.012678493068791753, 0.0092841995421566, 0.012877175472532363, 0.009486953866877413, 0.010385465107915937, 0.009240129000557831, 0.011685908255853163, 0.010789907728718925, 0.011740353682201153, 0.010453472504132363, 0.01172364014742333, 0.009278160185993527, 0.010458184379087977, 0.009348374179586075, 0.009594790588761138, 0.012211898318311386, 0.009341545348857032, 0.010464192146088841, 0.009156468806268518, 0.009250982402816646, 0.010203981721528279, 0.009481524929490894, 0.010305463843581867, 0.009537884200266165, 0.009592855224645761, 0.009748599185182373, 0.01032504205440366, 0.010428039131455066, 0.010542481859727192, 0.010416679116788992, 0.010989744062788149]
[108.96008576344127, 80.71243249751491, 110.67490632168928, 79.59776565655845, 95.42879099850897, 92.21947598275197, 91.12697699775974, 103.84620277485902, 97.01357472660371, 106.38834919471516, 95.27684080281598, 86.46134239354166, 104.67318870389805, 98.58604228873826, 102.1839758167098, 99.88546512573068, 108.8174066388726, 89.61615401068482, 96.98275449524722, 107.47378364568063, 92.18973166858252, 91.4548637693035, 105.81854776421525, 98.50391084118355, 104.3429639902675, 106.70808339081687, 79.98269496880454, 99.13742371184256, 109.34483793843641, 100.3978349856143, 111.88878364693811, 91.96974433033752, 104.05036575670839, 107.21310202796498, 101.04827986716862, 110.27445710547678, 94.76360585212907, 87.87337146965031, 93.63779776436736, 106.54018445273339, 104.90955165536246, 97.71094263053233, 95.44532052903907, 84.61798316079279, 107.25988622366646, 78.99192088738327, 86.58244884883032, 108.79046286037972, 81.19629039704499, 83.85287528720269, 91.17511393655184, 90.68389914416863, 104.25480981549508, 96.53661815555608, 96.94168507165811, 96.67438185378099, 108.09649356064291, 106.24621812805134, 79.7250535249879, 108.35508033007207, 94.8785925162437, 108.75040023157771, 82.90265650054961, 78.31101603062655, 83.23455499288929, 84.08990788664391, 23.183497073372116, 97.39760228033538, 96.20369173635844, 99.69429385446608, 105.13941493691479, 94.0068859026364, 99.55732784419799, 100.0953465653179, 84.85080105409548, 86.38870187394586, 95.52826122035351, 106.81867480058196, 91.12024013962473, 93.60433865812082, 104.93240530814376, 95.8951690174505, 104.23471929489796, 106.78685725131356, 96.12921712336286, 104.6092017893143, 91.94586863647001, 82.70968350879917, 84.84479707234878, 75.1969478079526, 105.82443177143413, 78.51453365490372, 105.1917417516258, 96.27901705925677, 74.42970627026232, 101.83781438237413, 97.28496993276558, 92.43750577932761, 106.30226282854854, 100.36847656556591, 23.101377505095662, 95.69306513343396, 96.3063254996659, 105.35725167751423, 97.8963790832582, 104.54905152702052, 90.06737083712635, 94.38375377859408, 104.63551105649205, 91.67913268224285, 107.01855172622238, 92.92446338794336, 106.34937500757286, 107.1071912603439, 82.70995862664412, 103.2992765605475, 96.22987594063102, 97.21373513863678, 105.75884124790046, 95.91513914376979, 82.57044990964593, 78.87372691487373, 107.70987799856279, 77.65678134409586, 105.40791217414716, 96.28841747663154, 108.22359730471614, 85.57315170595545, 92.6791984827037, 85.17630959585486, 95.66199170703229, 85.29773921965561, 107.7799886996581, 95.61889174564415, 106.97047216869936, 104.22322308642676, 81.88735067508128, 107.04866942837818, 95.56399443350875, 109.21240722355763, 108.09662762903214, 98.0009595558377, 105.4682667014508, 97.03590398046852, 104.84505567514584, 104.244250182242, 102.57884040611444, 96.85190575795255, 95.89530566524313, 94.85432494032074, 95.99988525980979, 90.99392982099124]
Elapsed: 1.4003798869303672~0.50064005765378
Time per graph: 0.010855658038219902~0.0038809306794866665
Speed: 95.88887962325742~12.416580907408438
Total Time: 1.4184
best val loss: 0.1750201091978901 test_score: 0.8915

Testing...
Test loss: 0.4101 score: 0.8837 time: 1.37s
test Score 0.8837
Epoch Time List: [3.7930302037857473, 4.41567610995844, 3.7871032061520964, 4.653760149143636, 3.9914206170942634, 4.11238209111616, 4.231614408781752, 3.8790774680674076, 4.117794113699347, 4.005853175185621, 4.067787413252518, 4.466356138931587, 3.965818125056103, 3.9779428211040795, 4.344314276240766, 3.8484785321634263, 3.841660025063902, 4.022199902450666, 4.468978880904615, 4.4147861499805, 4.331436178879812, 3.92236438812688, 3.8623894788324833, 3.9821339738555253, 3.89097563480027, 3.8782091378234327, 4.412222251761705, 4.467189346672967, 3.7806163509376347, 8.531422794796526, 3.7210451727733016, 3.975593271199614, 3.9237871549557894, 3.8376648887060583, 3.9303262799512595, 3.7451506808865815, 3.9365781978704035, 4.475616718875244, 4.4233812619932, 3.9340548431500793, 3.878435217309743, 3.994853446725756, 4.032368524000049, 4.223747457843274, 3.8861184930428863, 4.259393736021593, 4.75359738082625, 4.409643871244043, 4.286538890795782, 4.293566727079451, 4.689657811075449, 4.08633211790584, 4.117895420873538, 4.003019927069545, 4.470634600147605, 4.414519644808024, 3.8893583798781037, 4.004652299918234, 4.2178150753024966, 4.163179727969691, 4.142677313880995, 3.876961291069165, 4.468080376973376, 4.96623676083982, 4.784716246882454, 4.5306052409578115, 8.6428880286403, 3.903046804945916, 3.8543147940654308, 3.867794012185186, 3.782903050770983, 3.940450380090624, 3.9763391942251474, 3.9437375988345593, 4.8006052013952285, 4.62974749901332, 4.388459932990372, 3.9205029776785523, 4.073690715711564, 3.981518047163263, 4.159338983008638, 4.106620569014922, 3.88208102225326, 4.270935249980539, 3.9952584030106664, 3.8468469022773206, 4.234213653020561, 4.3901630602777, 4.830596229061484, 4.354233440943062, 4.547312226612121, 4.793182265013456, 3.8493134737946093, 4.124835449038073, 4.573114115744829, 4.461747939931229, 3.9201928600668907, 4.18772486387752, 3.8284781130496413, 3.84266225900501, 8.455193375935778, 3.962577662896365, 3.836087937699631, 3.8157080430537462, 3.925678625004366, 3.884958698414266, 4.026706979144365, 4.028252818156034, 3.8770093007478863, 4.635878290049732, 3.8317732410505414, 4.004928278271109, 4.386118552880362, 3.8705443339422345, 4.428533564088866, 3.9268535412847996, 4.028774220962077, 4.147589748026803, 3.8280991008505225, 4.199251327896491, 4.412016152171418, 4.824607967864722, 3.904097619932145, 4.422838156344369, 4.299470412079245, 3.976373135112226, 3.7700511761941016, 4.141724485903978, 4.423055458813906, 4.319368814816698, 4.010042169131339, 4.511584201129153, 3.9161260582040995, 3.9752209996804595, 3.8944067070260644, 4.032871270785108, 4.18356410600245, 7.672913500806317, 3.9227864369750023, 3.7074194757733494, 3.7616964601911604, 3.9231820618733764, 3.8993802932091057, 4.037334032123908, 3.9145672358572483, 3.899053743807599, 4.255705846007913, 4.180508938152343, 4.140832863748074, 3.947906536050141, 4.153836212819442, 4.4826560751535]
Total Epoch List: [69, 83]
Total Time List: [1.3416893610265106, 1.4183835270814598]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x76c8a0417a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 1.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 1.27s
Epoch 2/1000, LR 0.000020
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 1.47s
Epoch 3/1000, LR 0.000050
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 1.34s
Epoch 4/1000, LR 0.000080
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 1.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 1.69s
Epoch 5/1000, LR 0.000110
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 1.20s
Epoch 6/1000, LR 0.000140
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 1.37s
Epoch 7/1000, LR 0.000170
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 1.40s
Epoch 8/1000, LR 0.000200
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 1.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 1.48s
Epoch 9/1000, LR 0.000230
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 1.65s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 1.47s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 1.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 1.47s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 1.16s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 1.45s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 1.30s
Epoch 15/1000, LR 0.000290
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 1.38s
Epoch 16/1000, LR 0.000290
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5000 time: 1.57s
Epoch 17/1000, LR 0.000290
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.59s
Val loss: 0.6873 score: 0.5271 time: 1.45s
Test loss: 0.6867 score: 0.5391 time: 1.19s
Epoch 18/1000, LR 0.000290
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 1.58s
Val loss: 0.6850 score: 0.6357 time: 1.55s
Test loss: 0.6844 score: 0.6094 time: 1.46s
Epoch 19/1000, LR 0.000290
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 1.78s
Val loss: 0.6828 score: 0.6667 time: 1.51s
Test loss: 0.6821 score: 0.6250 time: 1.62s
Epoch 20/1000, LR 0.000290
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 1.41s
Val loss: 0.6804 score: 0.6744 time: 1.31s
Test loss: 0.6797 score: 0.6641 time: 1.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 1.47s
Val loss: 0.6777 score: 0.6744 time: 5.10s
Test loss: 0.6769 score: 0.6953 time: 3.21s
Epoch 22/1000, LR 0.000290
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 1.34s
Val loss: 0.6738 score: 0.6977 time: 1.39s
Test loss: 0.6730 score: 0.7109 time: 1.22s
Epoch 23/1000, LR 0.000290
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 1.31s
Val loss: 0.6673 score: 0.7752 time: 1.37s
Test loss: 0.6663 score: 0.7891 time: 1.20s
Epoch 24/1000, LR 0.000290
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 1.45s
Val loss: 0.6591 score: 0.8450 time: 1.24s
Test loss: 0.6576 score: 0.8672 time: 1.29s
Epoch 25/1000, LR 0.000290
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 1.32s
Val loss: 0.6493 score: 0.8760 time: 1.37s
Test loss: 0.6474 score: 0.8984 time: 1.26s
Epoch 26/1000, LR 0.000290
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 1.39s
Val loss: 0.6373 score: 0.7519 time: 1.23s
Test loss: 0.6350 score: 0.8047 time: 1.28s
Epoch 27/1000, LR 0.000290
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 1.29s
Val loss: 0.6229 score: 0.6744 time: 1.20s
Test loss: 0.6201 score: 0.6875 time: 1.27s
Epoch 28/1000, LR 0.000290
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 1.44s
Val loss: 0.6072 score: 0.7054 time: 1.61s
Test loss: 0.6042 score: 0.7500 time: 1.41s
Epoch 29/1000, LR 0.000290
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 1.43s
Val loss: 0.5890 score: 0.7519 time: 1.23s
Test loss: 0.5857 score: 0.8125 time: 1.42s
Epoch 30/1000, LR 0.000290
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 1.46s
Val loss: 0.5684 score: 0.7674 time: 1.72s
Test loss: 0.5643 score: 0.8125 time: 1.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 1.51s
Val loss: 0.5485 score: 0.8140 time: 1.22s
Test loss: 0.5438 score: 0.8438 time: 1.18s
Epoch 32/1000, LR 0.000290
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 1.64s
Val loss: 0.5310 score: 0.8605 time: 1.65s
Test loss: 0.5260 score: 0.8906 time: 1.70s
Epoch 33/1000, LR 0.000290
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 1.36s
Val loss: 0.5125 score: 0.8605 time: 1.45s
Test loss: 0.5066 score: 0.9219 time: 1.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 1.74s
Val loss: 0.4914 score: 0.8682 time: 1.48s
Test loss: 0.4835 score: 0.9062 time: 1.60s
Epoch 35/1000, LR 0.000290
Train loss: 0.5024;  Loss pred: 0.5024; Loss self: 0.0000; time: 1.38s
Val loss: 0.4735 score: 0.8527 time: 1.32s
Test loss: 0.4626 score: 0.8750 time: 1.19s
Epoch 36/1000, LR 0.000290
Train loss: 0.4797;  Loss pred: 0.4797; Loss self: 0.0000; time: 1.41s
Val loss: 0.4634 score: 0.8450 time: 1.36s
Test loss: 0.4488 score: 0.8359 time: 1.38s
Epoch 37/1000, LR 0.000290
Train loss: 0.4680;  Loss pred: 0.4680; Loss self: 0.0000; time: 1.61s
Val loss: 0.4565 score: 0.7984 time: 1.51s
Test loss: 0.4392 score: 0.8359 time: 1.37s
Epoch 38/1000, LR 0.000289
Train loss: 0.4563;  Loss pred: 0.4563; Loss self: 0.0000; time: 1.43s
Val loss: 0.4420 score: 0.8372 time: 1.47s
Test loss: 0.4244 score: 0.8672 time: 1.49s
Epoch 39/1000, LR 0.000289
Train loss: 0.4369;  Loss pred: 0.4369; Loss self: 0.0000; time: 1.51s
Val loss: 0.4271 score: 0.8605 time: 1.24s
Test loss: 0.4095 score: 0.8906 time: 1.37s
Epoch 40/1000, LR 0.000289
Train loss: 0.4280;  Loss pred: 0.4280; Loss self: 0.0000; time: 1.31s
Val loss: 0.4151 score: 0.8682 time: 1.67s
Test loss: 0.3972 score: 0.8906 time: 1.55s
Epoch 41/1000, LR 0.000289
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 1.32s
Val loss: 0.4095 score: 0.8682 time: 1.64s
Test loss: 0.3895 score: 0.8906 time: 1.49s
Epoch 42/1000, LR 0.000289
Train loss: 0.3986;  Loss pred: 0.3986; Loss self: 0.0000; time: 1.78s
Val loss: 0.3997 score: 0.8682 time: 1.51s
Test loss: 0.3787 score: 0.8906 time: 1.66s
Epoch 43/1000, LR 0.000289
Train loss: 0.3801;  Loss pred: 0.3801; Loss self: 0.0000; time: 1.36s
Val loss: 0.3845 score: 0.8760 time: 1.32s
Test loss: 0.3646 score: 0.8984 time: 1.18s
Epoch 44/1000, LR 0.000289
Train loss: 0.3714;  Loss pred: 0.3714; Loss self: 0.0000; time: 1.49s
Val loss: 0.3685 score: 0.9070 time: 1.65s
Test loss: 0.3515 score: 0.9297 time: 1.50s
Epoch 45/1000, LR 0.000289
Train loss: 0.3573;  Loss pred: 0.3573; Loss self: 0.0000; time: 1.32s
Val loss: 0.3550 score: 0.9070 time: 1.29s
Test loss: 0.3405 score: 0.9219 time: 1.34s
Epoch 46/1000, LR 0.000289
Train loss: 0.3433;  Loss pred: 0.3433; Loss self: 0.0000; time: 1.34s
Val loss: 0.3423 score: 0.8992 time: 1.34s
Test loss: 0.3299 score: 0.9141 time: 1.22s
Epoch 47/1000, LR 0.000289
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 1.43s
Val loss: 0.3284 score: 0.8992 time: 1.40s
Test loss: 0.3169 score: 0.9141 time: 1.46s
Epoch 48/1000, LR 0.000289
Train loss: 0.3155;  Loss pred: 0.3155; Loss self: 0.0000; time: 1.76s
Val loss: 0.3170 score: 0.9147 time: 1.25s
Test loss: 0.3039 score: 0.9219 time: 1.56s
Epoch 49/1000, LR 0.000289
Train loss: 0.3054;  Loss pred: 0.3054; Loss self: 0.0000; time: 1.73s
Val loss: 0.3094 score: 0.9225 time: 1.24s
Test loss: 0.2939 score: 0.9375 time: 1.33s
Epoch 50/1000, LR 0.000289
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 1.34s
Val loss: 0.3065 score: 0.9070 time: 1.39s
Test loss: 0.2885 score: 0.9219 time: 1.23s
Epoch 51/1000, LR 0.000289
Train loss: 0.2863;  Loss pred: 0.2863; Loss self: 0.0000; time: 1.48s
Val loss: 0.3002 score: 0.9070 time: 1.35s
Test loss: 0.2815 score: 0.9219 time: 1.56s
Epoch 52/1000, LR 0.000289
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 1.65s
Val loss: 0.2888 score: 0.9070 time: 1.26s
Test loss: 0.2716 score: 0.9141 time: 1.35s
Epoch 53/1000, LR 0.000289
Train loss: 0.2553;  Loss pred: 0.2553; Loss self: 0.0000; time: 1.29s
Val loss: 0.2779 score: 0.9147 time: 1.57s
Test loss: 0.2623 score: 0.9219 time: 1.50s
Epoch 54/1000, LR 0.000289
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 1.49s
Val loss: 0.2688 score: 0.9147 time: 1.23s
Test loss: 0.2542 score: 0.9219 time: 1.65s
Epoch 55/1000, LR 0.000289
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 1.62s
Val loss: 0.2552 score: 0.9225 time: 1.49s
Test loss: 0.2440 score: 0.9297 time: 1.18s
Epoch 56/1000, LR 0.000289
Train loss: 0.2257;  Loss pred: 0.2257; Loss self: 0.0000; time: 1.47s
Val loss: 0.2436 score: 0.9147 time: 1.21s
Test loss: 0.2404 score: 0.9297 time: 1.18s
Epoch 57/1000, LR 0.000288
Train loss: 0.2306;  Loss pred: 0.2306; Loss self: 0.0000; time: 1.74s
Val loss: 0.2418 score: 0.8915 time: 1.54s
Test loss: 0.2443 score: 0.9141 time: 1.39s
Epoch 58/1000, LR 0.000288
Train loss: 0.2242;  Loss pred: 0.2242; Loss self: 0.0000; time: 1.64s
Val loss: 0.2429 score: 0.8915 time: 1.69s
Test loss: 0.2499 score: 0.9062 time: 5.51s
     INFO: Early stopping counter 1 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.2385;  Loss pred: 0.2385; Loss self: 0.0000; time: 2.05s
Val loss: 0.2433 score: 0.8915 time: 1.27s
Test loss: 0.2536 score: 0.9062 time: 1.22s
     INFO: Early stopping counter 2 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.2325;  Loss pred: 0.2325; Loss self: 0.0000; time: 1.44s
Val loss: 0.2414 score: 0.8915 time: 1.20s
Test loss: 0.2539 score: 0.9062 time: 1.31s
Epoch 61/1000, LR 0.000288
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 1.29s
Val loss: 0.2370 score: 0.8915 time: 1.35s
Test loss: 0.2508 score: 0.9062 time: 1.16s
Epoch 62/1000, LR 0.000288
Train loss: 0.2227;  Loss pred: 0.2227; Loss self: 0.0000; time: 1.53s
Val loss: 0.2300 score: 0.8915 time: 1.21s
Test loss: 0.2436 score: 0.9062 time: 1.30s
Epoch 63/1000, LR 0.000288
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 1.30s
Val loss: 0.2210 score: 0.9070 time: 1.53s
Test loss: 0.2314 score: 0.9141 time: 1.62s
Epoch 64/1000, LR 0.000288
Train loss: 0.2050;  Loss pred: 0.2050; Loss self: 0.0000; time: 1.60s
Val loss: 0.2143 score: 0.9225 time: 1.54s
Test loss: 0.2193 score: 0.9297 time: 1.23s
Epoch 65/1000, LR 0.000288
Train loss: 0.1898;  Loss pred: 0.1898; Loss self: 0.0000; time: 1.44s
Val loss: 0.2125 score: 0.9302 time: 1.34s
Test loss: 0.2146 score: 0.9219 time: 1.62s
Epoch 66/1000, LR 0.000288
Train loss: 0.1744;  Loss pred: 0.1744; Loss self: 0.0000; time: 1.63s
Val loss: 0.2129 score: 0.9302 time: 1.71s
Test loss: 0.2133 score: 0.9297 time: 1.52s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1771;  Loss pred: 0.1771; Loss self: 0.0000; time: 1.71s
Val loss: 0.2147 score: 0.9380 time: 1.51s
Test loss: 0.2147 score: 0.9297 time: 1.62s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 1.31s
Val loss: 0.2148 score: 0.9535 time: 1.33s
Test loss: 0.2158 score: 0.9297 time: 1.18s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 1.28s
Val loss: 0.2137 score: 0.9457 time: 1.34s
Test loss: 0.2164 score: 0.9297 time: 1.19s
     INFO: Early stopping counter 4 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 1.48s
Val loss: 0.2093 score: 0.9535 time: 1.23s
Test loss: 0.2136 score: 0.9297 time: 1.31s
Epoch 71/1000, LR 0.000287
Train loss: 0.1483;  Loss pred: 0.1483; Loss self: 0.0000; time: 1.41s
Val loss: 0.2031 score: 0.9457 time: 1.60s
Test loss: 0.2088 score: 0.9297 time: 1.28s
Epoch 72/1000, LR 0.000287
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 1.46s
Val loss: 0.2028 score: 0.9535 time: 1.36s
Test loss: 0.2097 score: 0.9297 time: 1.30s
Epoch 73/1000, LR 0.000287
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 1.34s
Val loss: 0.2221 score: 0.9225 time: 1.23s
Test loss: 0.2315 score: 0.9219 time: 1.40s
     INFO: Early stopping counter 1 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 1.63s
Val loss: 0.2497 score: 0.9147 time: 1.41s
Test loss: 0.2576 score: 0.9062 time: 1.39s
     INFO: Early stopping counter 2 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.1657;  Loss pred: 0.1657; Loss self: 0.0000; time: 1.33s
Val loss: 0.2361 score: 0.9225 time: 1.23s
Test loss: 0.2487 score: 0.9062 time: 1.37s
     INFO: Early stopping counter 3 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.1500;  Loss pred: 0.1500; Loss self: 0.0000; time: 1.44s
Val loss: 0.2001 score: 0.9380 time: 1.58s
Test loss: 0.2177 score: 0.9219 time: 1.50s
Epoch 77/1000, LR 0.000287
Train loss: 0.1389;  Loss pred: 0.1389; Loss self: 0.0000; time: 1.74s
Val loss: 0.1911 score: 0.9380 time: 1.56s
Test loss: 0.2102 score: 0.9375 time: 1.69s
Epoch 78/1000, LR 0.000287
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 1.53s
Val loss: 0.1891 score: 0.9380 time: 1.67s
Test loss: 0.2113 score: 0.9453 time: 1.40s
Epoch 79/1000, LR 0.000287
Train loss: 0.1285;  Loss pred: 0.1285; Loss self: 0.0000; time: 1.45s
Val loss: 0.1878 score: 0.9380 time: 1.31s
Test loss: 0.2127 score: 0.9453 time: 1.33s
Epoch 80/1000, LR 0.000287
Train loss: 0.1229;  Loss pred: 0.1229; Loss self: 0.0000; time: 1.30s
Val loss: 0.1871 score: 0.9380 time: 1.28s
Test loss: 0.2146 score: 0.9453 time: 1.30s
Epoch 81/1000, LR 0.000286
Train loss: 0.1279;  Loss pred: 0.1279; Loss self: 0.0000; time: 1.36s
Val loss: 0.1871 score: 0.9457 time: 1.40s
Test loss: 0.2170 score: 0.9375 time: 1.24s
Epoch 82/1000, LR 0.000286
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 1.38s
Val loss: 0.1866 score: 0.9380 time: 1.57s
Test loss: 0.2180 score: 0.9375 time: 1.48s
Epoch 83/1000, LR 0.000286
Train loss: 0.1187;  Loss pred: 0.1187; Loss self: 0.0000; time: 1.26s
Val loss: 0.1864 score: 0.9380 time: 1.34s
Test loss: 0.2193 score: 0.9375 time: 1.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 1.39s
Val loss: 0.1863 score: 0.9380 time: 1.20s
Test loss: 0.2207 score: 0.9375 time: 1.15s
Epoch 85/1000, LR 0.000286
Train loss: 0.1147;  Loss pred: 0.1147; Loss self: 0.0000; time: 1.42s
Val loss: 0.1862 score: 0.9380 time: 1.48s
Test loss: 0.2222 score: 0.9375 time: 1.27s
Epoch 86/1000, LR 0.000286
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 1.44s
Val loss: 0.1859 score: 0.9380 time: 1.50s
Test loss: 0.2234 score: 0.9375 time: 1.52s
Epoch 87/1000, LR 0.000286
Train loss: 0.1084;  Loss pred: 0.1084; Loss self: 0.0000; time: 1.71s
Val loss: 0.1858 score: 0.9380 time: 1.20s
Test loss: 0.2257 score: 0.9375 time: 1.34s
Epoch 88/1000, LR 0.000286
Train loss: 0.1067;  Loss pred: 0.1067; Loss self: 0.0000; time: 1.29s
Val loss: 0.1865 score: 0.9457 time: 1.37s
Test loss: 0.2306 score: 0.9297 time: 1.18s
     INFO: Early stopping counter 1 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0998;  Loss pred: 0.0998; Loss self: 0.0000; time: 1.44s
Val loss: 0.1878 score: 0.9457 time: 1.21s
Test loss: 0.2361 score: 0.9297 time: 1.34s
     INFO: Early stopping counter 2 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0887;  Loss pred: 0.0887; Loss self: 0.0000; time: 1.33s
Val loss: 0.1895 score: 0.9380 time: 1.36s
Test loss: 0.2417 score: 0.9297 time: 1.17s
     INFO: Early stopping counter 3 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.1105;  Loss pred: 0.1105; Loss self: 0.0000; time: 1.33s
Val loss: 0.1909 score: 0.9380 time: 1.35s
Test loss: 0.2464 score: 0.9297 time: 1.58s
     INFO: Early stopping counter 4 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 1.72s
Val loss: 0.1887 score: 0.9380 time: 1.30s
Test loss: 0.2447 score: 0.9297 time: 1.28s
     INFO: Early stopping counter 5 of 20
Epoch 93/1000, LR 0.000285
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 1.34s
Val loss: 0.1866 score: 0.9457 time: 1.35s
Test loss: 0.2396 score: 0.9297 time: 1.23s
     INFO: Early stopping counter 6 of 20
Epoch 94/1000, LR 0.000285
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 1.43s
Val loss: 0.1870 score: 0.9380 time: 1.60s
Test loss: 0.2395 score: 0.9297 time: 1.67s
     INFO: Early stopping counter 7 of 20
Epoch 95/1000, LR 0.000285
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 1.65s
Val loss: 0.1878 score: 0.9380 time: 1.72s
Test loss: 0.2408 score: 0.9219 time: 1.51s
     INFO: Early stopping counter 8 of 20
Epoch 96/1000, LR 0.000285
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 1.71s
Val loss: 0.1881 score: 0.9302 time: 1.23s
Test loss: 0.2427 score: 0.9219 time: 1.28s
     INFO: Early stopping counter 9 of 20
Epoch 97/1000, LR 0.000285
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 1.32s
Val loss: 0.1903 score: 0.9302 time: 5.04s
Test loss: 0.2447 score: 0.9297 time: 2.11s
     INFO: Early stopping counter 10 of 20
Epoch 98/1000, LR 0.000285
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 1.26s
Val loss: 0.1995 score: 0.9302 time: 1.32s
Test loss: 0.2530 score: 0.9453 time: 1.20s
     INFO: Early stopping counter 11 of 20
Epoch 99/1000, LR 0.000284
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 1.49s
Val loss: 0.2073 score: 0.9147 time: 1.26s
Test loss: 0.2619 score: 0.9453 time: 1.37s
     INFO: Early stopping counter 12 of 20
Epoch 100/1000, LR 0.000284
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 1.29s
Val loss: 0.2113 score: 0.9225 time: 1.38s
Test loss: 0.2666 score: 0.9375 time: 1.24s
     INFO: Early stopping counter 13 of 20
Epoch 101/1000, LR 0.000284
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 1.30s
Val loss: 0.2121 score: 0.9225 time: 1.33s
Test loss: 0.2680 score: 0.9375 time: 1.19s
     INFO: Early stopping counter 14 of 20
Epoch 102/1000, LR 0.000284
Train loss: 0.0955;  Loss pred: 0.0955; Loss self: 0.0000; time: 1.40s
Val loss: 0.2103 score: 0.9225 time: 1.25s
Test loss: 0.2665 score: 0.9375 time: 1.32s
     INFO: Early stopping counter 15 of 20
Epoch 103/1000, LR 0.000284
Train loss: 0.0871;  Loss pred: 0.0871; Loss self: 0.0000; time: 1.26s
Val loss: 0.2063 score: 0.9147 time: 1.55s
Test loss: 0.2631 score: 0.9453 time: 1.48s
     INFO: Early stopping counter 16 of 20
Epoch 104/1000, LR 0.000284
Train loss: 0.0776;  Loss pred: 0.0776; Loss self: 0.0000; time: 1.74s
Val loss: 0.2020 score: 0.9225 time: 1.54s
Test loss: 0.2605 score: 0.9453 time: 1.62s
     INFO: Early stopping counter 17 of 20
Epoch 105/1000, LR 0.000284
Train loss: 0.0662;  Loss pred: 0.0662; Loss self: 0.0000; time: 1.39s
Val loss: 0.1980 score: 0.9302 time: 1.35s
Test loss: 0.2599 score: 0.9375 time: 1.22s
     INFO: Early stopping counter 18 of 20
Epoch 106/1000, LR 0.000283
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 1.33s
Val loss: 0.1956 score: 0.9302 time: 1.38s
Test loss: 0.2623 score: 0.9297 time: 1.30s
     INFO: Early stopping counter 19 of 20
Epoch 107/1000, LR 0.000283
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 1.31s
Val loss: 0.1942 score: 0.9302 time: 1.33s
Test loss: 0.2668 score: 0.9297 time: 1.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1084,   Val_Loss: 0.1858,   Val_Precision: 0.9672,   Val_Recall: 0.9077,   Val_accuracy: 0.9365,   Val_Score: 0.9380,   Val_Loss: 0.1858,   Test_Precision: 0.9667,   Test_Recall: 0.9062,   Test_accuracy: 0.9355,   Test_Score: 0.9375,   Test_loss: 0.2257


[1.1839197729714215, 1.5982667850330472, 1.165575867984444, 1.6206485060974956, 1.3517932968679816, 1.3988368359860033, 1.4156071478500962, 1.2422216369304806, 1.3297108199913055, 1.2125387880951166, 1.353949174983427, 1.4919962659478188, 1.2324072821065784, 1.30850166012533, 1.2624288590159267, 1.2914791940711439, 1.1854721039999276, 1.4394726199097931, 1.3301333899144083, 1.2002927190624177, 1.399288160027936, 1.4105318698566407, 1.2190679491031915, 1.3095926740206778, 1.2363076058682054, 1.2089056039694697, 1.612848880002275, 1.3012240501120687, 1.1797539091203362, 1.2848882649559528, 1.1529305779840797, 1.4026351920329034, 1.2397842050995678, 1.2032111519947648, 1.2766174760181457, 1.1698085248935968, 1.3612820960115641, 1.4680215159896761, 1.3776488029398024, 1.2108107439707965, 1.2296306481584907, 1.3202206071000546, 1.3515591889154166, 1.524498637067154, 1.2026863400824368, 1.633078402839601, 1.4899093490093946, 1.1857657060027122, 1.5887425320688635, 1.5384087851271033, 1.4148597619496286, 1.422523746965453, 1.2373529837932438, 1.3362804960925132, 1.330696902005002, 1.3343762590084225, 1.193378210067749, 1.2141608640085906, 1.6180610021110624, 1.19053024193272, 1.359632310923189, 1.1862025309819728, 1.5560418139211833, 1.647277822950855, 1.5498370840214193, 1.5340723190456629, 5.564302900107577, 1.324467923026532, 1.3409048828762025, 1.2939557021018118, 1.2269423420075327, 1.3722399030812085, 1.2957358618732542, 1.2887712009251118, 1.5203156410716474, 1.4932508210185915, 1.3503857220057398, 1.2076540009584278, 1.415711808949709, 1.3781412469688803, 1.2293628419283777, 1.3452189648523927, 1.2375914750155061, 1.2080138260498643, 1.3419437280390412, 1.2331611157860607, 1.4029994159936905, 1.5596722720656544, 1.5204232251271605, 1.7154951598495245, 1.2190001669805497, 1.6430079119745642, 1.2263320090714842, 1.3398558059707284, 1.7331789478193969, 1.2667200369760394, 1.3260013349354267, 1.3955374381039292, 1.2135207338724285, 1.2852641029749066, 5.584082592977211, 1.3480600691400468, 1.3394758789800107, 1.2244055150076747, 1.3177198299672455, 1.2338705910369754, 1.4322611929383129, 1.3667606429662555, 1.232851053122431, 1.407081374200061, 1.2053984839003533, 1.3882243200205266, 1.2129831509664655, 1.2044009228702635, 1.5596670841332525, 1.2487986779306084, 1.3405400218907744, 1.326972981914878, 1.2197561780922115, 1.3449388819281012, 1.5623022539075464, 1.635525605874136, 1.1976617409382015, 1.6611556359566748, 1.2238170488271862, 1.339724998921156, 1.1919766410719603, 1.507482165005058, 1.3918980970047414, 1.5145056250039488, 1.3484979530330747, 1.5123495790176094, 1.196882663993165, 1.3491057849023491, 1.2059402691666037, 1.2377279859501868, 1.575334883062169, 1.205059350002557, 1.3498807868454605, 1.1811844760086387, 1.1933767299633473, 1.316313642077148, 1.2231167159043252, 1.3294048358220607, 1.2303870618343353, 1.2374783239793032, 1.2575692948885262, 1.3319304250180721, 1.3452170479577035, 1.3599801599048078, 1.34375160606578, 1.4176769840996712, 1.2796492998022586, 1.4809395289048553, 1.3476833628956228, 1.701727295992896, 1.2074835740495473, 1.3798782730009407, 1.4091346771456301, 1.4908776560332626, 1.6589860739186406, 1.480542209930718, 1.4779837459791452, 1.1669588899239898, 1.4528417140245438, 1.3049036969896406, 1.3862737829331309, 1.5822020808700472, 1.1976492889225483, 1.462394034024328, 1.6267620199359953, 1.1697904160246253, 3.2145062871277332, 1.2249660708475858, 1.205022448906675, 1.29802551609464, 1.2603240159805864, 1.2832869528792799, 1.2709334529936314, 1.4142785659059882, 1.4278290560469031, 1.1615572720766068, 1.187415950000286, 1.7105750769842416, 1.1690651380922645, 1.6024720289278775, 1.1983283578883857, 1.3850017879158258, 1.3756921452004462, 1.49497559084557, 1.3730466100387275, 1.5580362470354885, 1.5019988280255347, 1.662719997111708, 1.1878427639603615, 1.5049190209247172, 1.3426741471048445, 1.225321470061317, 1.4711427469737828, 1.570375693961978, 1.34092623507604, 1.2301887068897486, 1.5695408820174634, 1.3509295829571784, 1.5064032080117613, 1.657666663872078, 1.1804149199742824, 1.1829420230351388, 1.3961176089942455, 5.512998779071495, 1.223591144895181, 1.3122796670068055, 1.166110099060461, 1.3088835240341723, 1.6267074078787118, 1.232933757128194, 1.6246908970642835, 1.5242223429959267, 1.6266877769958228, 1.1836194491479546, 1.1917643200140446, 1.31627445993945, 1.2881819778122008, 1.3073146359529346, 1.40990280197002, 1.3994197440333664, 1.3741199581418186, 1.5064761729445308, 1.6956884500104934, 1.4082819460891187, 1.3357796799391508, 1.3019675461109728, 1.2496357711497694, 1.48217330314219, 1.1685310760512948, 1.1563605391420424, 1.2801910960115492, 1.531512884190306, 1.3501586199272424, 1.1830569189041853, 1.3437007260508835, 1.175336780026555, 1.5851889248006046, 1.2818399430252612, 1.2305469689890742, 1.6784393759444356, 1.5197035940364003, 1.2846917221322656, 2.1112873521633446, 1.2086506949272007, 1.3794016549363732, 1.248205692972988, 1.1944333519786596, 1.3252652809023857, 1.4917064828332514, 1.622022707015276, 1.2207664691377431, 1.3054741160012782, 1.2240461909677833]
[0.00917767265869319, 0.01238966500025618, 0.00903547184484065, 0.012563166713934074, 0.010479017805178152, 0.010843696402992273, 0.010973698820543382, 0.009629625092484346, 0.010307835813886088, 0.00939952548910943, 0.010495730038631216, 0.011565862526727278, 0.009553544822531615, 0.010143423721901784, 0.009786270224929665, 0.010011466620706542, 0.009189706232557578, 0.011158702479920877, 0.010311111549724096, 0.009304594721414091, 0.010847195038976248, 0.010934355580284036, 0.009450139140334818, 0.010151881193958743, 0.009583779890451205, 0.009371361271081161, 0.012502704496141668, 0.010087008140403633, 0.009145379140467723, 0.00996037414694537, 0.008937446340961858, 0.01087314102351088, 0.009610730272089673, 0.009327218232517557, 0.009896259504016633, 0.009068283138710052, 0.010552574387686543, 0.011380011751857955, 0.01067944808480467, 0.009386129798223229, 0.009532020528360392, 0.010234268272093446, 0.010477203014848191, 0.011817818891993442, 0.009323149923119665, 0.012659522502632567, 0.011549684876041818, 0.00919198221707529, 0.012315833581929175, 0.011925649497109328, 0.01096790513139247, 0.011027315867949248, 0.009591883595296464, 0.010358763535600878, 0.010315479860503891, 0.01034400200781723, 0.009250993876494178, 0.009412099720996827, 0.012543108543496608, 0.009228916604129611, 0.010539785355993713, 0.009195368457224596, 0.01206233964279987, 0.012769595526750813, 0.012014240961406351, 0.01189203348097413, 0.043134131008585874, 0.010267193201756062, 0.01039461149516436, 0.010030664357378385, 0.009511180945794827, 0.0106375186285365, 0.010044464045529103, 0.00999047442577606, 0.01178539256644688, 0.011575587759834043, 0.010468106372137518, 0.009361658922158355, 0.010974510146896968, 0.010683265480378916, 0.009529944511072697, 0.010428053991103818, 0.009593732364461288, 0.009364448263952437, 0.01040266455844218, 0.009559388494465587, 0.010875964465067368, 0.012090482729191119, 0.011786226551373338, 0.01329841209185678, 0.00944961369752364, 0.012736495441663288, 0.009506449682724684, 0.010386479116052158, 0.013435495719530208, 0.009819535170356894, 0.0102790801157785, 0.010818119675224258, 0.00940713747187929, 0.009963287619960516, 0.04328746196106365, 0.0104500780553492, 0.010383533945581478, 0.009491515620214533, 0.010214882402846863, 0.009564888302612212, 0.01110279994525824, 0.010595043743924462, 0.009556984907925822, 0.010907607551938456, 0.009344174293801188, 0.010761428837368423, 0.00940297016253074, 0.009336441262560181, 0.012090442512660872, 0.009680609906438825, 0.01039178311543236, 0.01028661226290603, 0.009455474248776833, 0.010425882805644195, 0.012110870185329817, 0.012678493068791753, 0.0092841995421566, 0.012877175472532363, 0.009486953866877413, 0.010385465107915937, 0.009240129000557831, 0.011685908255853163, 0.010789907728718925, 0.011740353682201153, 0.010453472504132363, 0.01172364014742333, 0.009278160185993527, 0.010458184379087977, 0.009348374179586075, 0.009594790588761138, 0.012211898318311386, 0.009341545348857032, 0.010464192146088841, 0.009156468806268518, 0.009250982402816646, 0.010203981721528279, 0.009481524929490894, 0.010305463843581867, 0.009537884200266165, 0.009592855224645761, 0.009748599185182373, 0.01032504205440366, 0.010428039131455066, 0.010542481859727192, 0.010416679116788992, 0.010989744062788149, 0.009997260154705145, 0.011569840069569182, 0.010528776272622054, 0.0132947444999445, 0.009433465422262088, 0.010780299007819849, 0.011008864665200235, 0.011647481687759864, 0.01296082870248938, 0.011566736015083734, 0.011546748015462072, 0.00911686632753117, 0.011350325890816748, 0.010194560132731567, 0.010830263929165085, 0.012360953756797244, 0.009356635069707409, 0.011424953390815062, 0.012709078280749964, 0.009138987625192385, 0.025113330368185416, 0.009570047428496764, 0.009414237882083398, 0.010140824344489374, 0.009846281374848331, 0.010025679319369374, 0.009929167601512745, 0.011049051296140533, 0.01115491450036643, 0.00907466618809849, 0.009276687109377235, 0.013363867788939388, 0.009133321391345817, 0.012519312725999043, 0.009361940296003013, 0.01082032646809239, 0.010747594884378486, 0.011679496803481015, 0.010726926640927559, 0.012172158179964754, 0.01173436584394949, 0.012989999977435218, 0.009280021593440324, 0.011757179850974353, 0.010489641774256597, 0.00957282398485404, 0.011493302710732678, 0.012268560109077953, 0.010475986211531563, 0.00961084927257616, 0.012262038140761433, 0.010554137366852956, 0.011768775062591885, 0.01295052081150061, 0.009221991562299081, 0.009241734554962022, 0.010907168820267543, 0.043070302961496054, 0.009559305819493602, 0.010252184898490668, 0.009110235148909851, 0.010225652531516971, 0.012708651624052436, 0.009632294977564015, 0.012692897633314715, 0.011907987054655678, 0.012708498257779866, 0.009247026946468395, 0.009310658750109724, 0.010283394218276953, 0.010063921701657819, 0.010213395593382302, 0.01101486564039078, 0.010932966750260675, 0.010735312172982958, 0.011769345101129147, 0.01324756601570698, 0.01100220270382124, 0.010435778749524616, 0.010171621453991975, 0.009762779462107574, 0.01157947893079836, 0.00912914903165074, 0.009034066712047206, 0.010001492937590228, 0.011964944407736766, 0.010548114218181581, 0.009242632178938948, 0.010497661922272528, 0.009182318593957461, 0.012384288475004723, 0.010014374554884853, 0.009613648195227142, 0.013112807624565903, 0.011872684328409377, 0.010036654079158325, 0.01649443243877613, 0.009442583554118755, 0.010776575429190416, 0.009751606976351468, 0.009331510562333278, 0.010353635007049888, 0.011653956897134776, 0.012672052398556843, 0.009537238040138618, 0.010199016531259986, 0.009562860866935807]
[108.96008576344127, 80.71243249751491, 110.67490632168928, 79.59776565655845, 95.42879099850897, 92.21947598275197, 91.12697699775974, 103.84620277485902, 97.01357472660371, 106.38834919471516, 95.27684080281598, 86.46134239354166, 104.67318870389805, 98.58604228873826, 102.1839758167098, 99.88546512573068, 108.8174066388726, 89.61615401068482, 96.98275449524722, 107.47378364568063, 92.18973166858252, 91.4548637693035, 105.81854776421525, 98.50391084118355, 104.3429639902675, 106.70808339081687, 79.98269496880454, 99.13742371184256, 109.34483793843641, 100.3978349856143, 111.88878364693811, 91.96974433033752, 104.05036575670839, 107.21310202796498, 101.04827986716862, 110.27445710547678, 94.76360585212907, 87.87337146965031, 93.63779776436736, 106.54018445273339, 104.90955165536246, 97.71094263053233, 95.44532052903907, 84.61798316079279, 107.25988622366646, 78.99192088738327, 86.58244884883032, 108.79046286037972, 81.19629039704499, 83.85287528720269, 91.17511393655184, 90.68389914416863, 104.25480981549508, 96.53661815555608, 96.94168507165811, 96.67438185378099, 108.09649356064291, 106.24621812805134, 79.7250535249879, 108.35508033007207, 94.8785925162437, 108.75040023157771, 82.90265650054961, 78.31101603062655, 83.23455499288929, 84.08990788664391, 23.183497073372116, 97.39760228033538, 96.20369173635844, 99.69429385446608, 105.13941493691479, 94.0068859026364, 99.55732784419799, 100.0953465653179, 84.85080105409548, 86.38870187394586, 95.52826122035351, 106.81867480058196, 91.12024013962473, 93.60433865812082, 104.93240530814376, 95.8951690174505, 104.23471929489796, 106.78685725131356, 96.12921712336286, 104.6092017893143, 91.94586863647001, 82.70968350879917, 84.84479707234878, 75.1969478079526, 105.82443177143413, 78.51453365490372, 105.1917417516258, 96.27901705925677, 74.42970627026232, 101.83781438237413, 97.28496993276558, 92.43750577932761, 106.30226282854854, 100.36847656556591, 23.101377505095662, 95.69306513343396, 96.3063254996659, 105.35725167751423, 97.8963790832582, 104.54905152702052, 90.06737083712635, 94.38375377859408, 104.63551105649205, 91.67913268224285, 107.01855172622238, 92.92446338794336, 106.34937500757286, 107.1071912603439, 82.70995862664412, 103.2992765605475, 96.22987594063102, 97.21373513863678, 105.75884124790046, 95.91513914376979, 82.57044990964593, 78.87372691487373, 107.70987799856279, 77.65678134409586, 105.40791217414716, 96.28841747663154, 108.22359730471614, 85.57315170595545, 92.6791984827037, 85.17630959585486, 95.66199170703229, 85.29773921965561, 107.7799886996581, 95.61889174564415, 106.97047216869936, 104.22322308642676, 81.88735067508128, 107.04866942837818, 95.56399443350875, 109.21240722355763, 108.09662762903214, 98.0009595558377, 105.4682667014508, 97.03590398046852, 104.84505567514584, 104.244250182242, 102.57884040611444, 96.85190575795255, 95.89530566524313, 94.85432494032074, 95.99988525980979, 90.99392982099124, 100.02740596175808, 86.43161824079012, 94.9777993289018, 75.21769222448575, 106.00558280948317, 92.76180551899503, 90.8358882057173, 85.8554687448775, 77.15556026197079, 86.45481306878091, 86.60447068394629, 109.68681168223272, 88.1031971786003, 98.09152989242865, 92.33385322282639, 80.89990624308449, 106.87602888751661, 87.52770937375873, 78.68391223261786, 109.42131021639817, 39.81948970284087, 104.49269008033298, 106.22208749400085, 98.61131265363156, 101.56118456602644, 99.74386454472206, 100.71337700530368, 90.50550795698659, 89.64658581355785, 110.19689091280398, 107.7971034496961, 74.82863612491367, 109.48919425386033, 79.87658922548401, 106.81546435699232, 92.41865325864782, 93.04407272118986, 85.62012703337999, 93.22334658135873, 82.1546997019797, 85.2197735521961, 76.98229420608843, 107.75836994892985, 85.05441038372199, 95.33214017414528, 104.46238242572757, 87.00719237701621, 81.50915764434848, 95.45640666262413, 104.04907741643865, 81.55251097089665, 94.74957215742408, 84.97061033807931, 77.21697177706999, 108.43644707810769, 108.20479576132009, 91.68282039806833, 23.217853863112573, 104.61010651639288, 97.5401838633656, 109.76665076747904, 97.79327010358041, 78.68655382034368, 103.81741862445513, 78.7842168816777, 83.97724950574488, 78.6875034103909, 108.14286643578106, 107.40378600904208, 97.2441568196105, 99.36484301495223, 97.91063029497683, 90.78640018386302, 91.46648140827438, 93.15052826471612, 84.96649485654561, 75.48556457951217, 90.89089038985658, 95.8241856215621, 98.31274241999421, 102.42984632412474, 86.3596717931982, 109.53923487643833, 110.69212037879565, 99.98507285262765, 83.57748819571452, 94.80367573914958, 108.19428715108728, 95.2593070156255, 108.90495573286495, 80.74747305978099, 99.85646078238764, 104.0187845126751, 76.2613186001884, 84.22695090167308, 99.63479782336587, 60.62651768781921, 105.90321962931539, 92.79385706254222, 102.54720093058414, 107.16378589727032, 96.58443622158696, 85.80776545053624, 78.913815106532, 104.85215906233852, 98.04866939228896, 104.57121711950896]
Elapsed: 1.4151741014712609~0.4869240440288752
Time per graph: 0.011006275104345051~0.003787575521792808
Speed: 94.6549132987624~13.081062360681601
Total Time: 1.2250
best val loss: 0.18583579575137574 test_score: 0.9375

Testing...
Test loss: 0.2158 score: 0.9297 time: 1.36s
test Score 0.9297
Epoch Time List: [3.7930302037857473, 4.41567610995844, 3.7871032061520964, 4.653760149143636, 3.9914206170942634, 4.11238209111616, 4.231614408781752, 3.8790774680674076, 4.117794113699347, 4.005853175185621, 4.067787413252518, 4.466356138931587, 3.965818125056103, 3.9779428211040795, 4.344314276240766, 3.8484785321634263, 3.841660025063902, 4.022199902450666, 4.468978880904615, 4.4147861499805, 4.331436178879812, 3.92236438812688, 3.8623894788324833, 3.9821339738555253, 3.89097563480027, 3.8782091378234327, 4.412222251761705, 4.467189346672967, 3.7806163509376347, 8.531422794796526, 3.7210451727733016, 3.975593271199614, 3.9237871549557894, 3.8376648887060583, 3.9303262799512595, 3.7451506808865815, 3.9365781978704035, 4.475616718875244, 4.4233812619932, 3.9340548431500793, 3.878435217309743, 3.994853446725756, 4.032368524000049, 4.223747457843274, 3.8861184930428863, 4.259393736021593, 4.75359738082625, 4.409643871244043, 4.286538890795782, 4.293566727079451, 4.689657811075449, 4.08633211790584, 4.117895420873538, 4.003019927069545, 4.470634600147605, 4.414519644808024, 3.8893583798781037, 4.004652299918234, 4.2178150753024966, 4.163179727969691, 4.142677313880995, 3.876961291069165, 4.468080376973376, 4.96623676083982, 4.784716246882454, 4.5306052409578115, 8.6428880286403, 3.903046804945916, 3.8543147940654308, 3.867794012185186, 3.782903050770983, 3.940450380090624, 3.9763391942251474, 3.9437375988345593, 4.8006052013952285, 4.62974749901332, 4.388459932990372, 3.9205029776785523, 4.073690715711564, 3.981518047163263, 4.159338983008638, 4.106620569014922, 3.88208102225326, 4.270935249980539, 3.9952584030106664, 3.8468469022773206, 4.234213653020561, 4.3901630602777, 4.830596229061484, 4.354233440943062, 4.547312226612121, 4.793182265013456, 3.8493134737946093, 4.124835449038073, 4.573114115744829, 4.461747939931229, 3.9201928600668907, 4.18772486387752, 3.8284781130496413, 3.84266225900501, 8.455193375935778, 3.962577662896365, 3.836087937699631, 3.8157080430537462, 3.925678625004366, 3.884958698414266, 4.026706979144365, 4.028252818156034, 3.8770093007478863, 4.635878290049732, 3.8317732410505414, 4.004928278271109, 4.386118552880362, 3.8705443339422345, 4.428533564088866, 3.9268535412847996, 4.028774220962077, 4.147589748026803, 3.8280991008505225, 4.199251327896491, 4.412016152171418, 4.824607967864722, 3.904097619932145, 4.422838156344369, 4.299470412079245, 3.976373135112226, 3.7700511761941016, 4.141724485903978, 4.423055458813906, 4.319368814816698, 4.010042169131339, 4.511584201129153, 3.9161260582040995, 3.9752209996804595, 3.8944067070260644, 4.032871270785108, 4.18356410600245, 7.672913500806317, 3.9227864369750023, 3.7074194757733494, 3.7616964601911604, 3.9231820618733764, 3.8993802932091057, 4.037334032123908, 3.9145672358572483, 3.899053743807599, 4.255705846007913, 4.180508938152343, 4.140832863748074, 3.947906536050141, 4.153836212819442, 4.4826560751535, 4.548109289025888, 4.214211476035416, 4.246935243019834, 4.714697001967579, 4.088010699953884, 4.436295758234337, 4.008428135421127, 4.566452894592658, 4.650313131976873, 4.744764030911028, 4.579511852003634, 3.8182637998834252, 4.295452726073563, 4.561299124034122, 4.112989517860115, 4.29275027802214, 4.234604289056733, 4.58470703382045, 4.904378276783973, 3.8818048441316932, 9.780931100947782, 3.950032789958641, 3.875804577721283, 3.9870713117998093, 3.951698963996023, 3.904400809900835, 3.7628923773299903, 4.460387908155099, 4.081593581940979, 4.345150710316375, 3.9085871349088848, 4.991300663910806, 3.9805485201068223, 4.812068320112303, 3.8888822079170495, 4.155210057971999, 4.483914889162406, 4.391400799853727, 4.120309314224869, 4.532320213271305, 4.463307838886976, 4.948554830858484, 3.869308166904375, 4.635699681006372, 3.943997262744233, 3.8994125130120665, 4.294678243808448, 4.577292568283156, 4.312096028821543, 3.9560821729246527, 4.390094698872417, 4.250984020996839, 4.360793628264219, 4.37120979395695, 4.285153860691935, 3.8493192011956125, 4.669691305840388, 8.842056724941358, 4.541553047951311, 3.948430218268186, 3.797039482044056, 4.045825036009774, 4.443517848616466, 4.365861215163022, 4.401978638255969, 4.8590311801526695, 4.848911261884496, 3.8161219370085746, 3.812643186887726, 4.021433243295178, 4.287111490033567, 4.12900195713155, 3.971182603854686, 4.435724267968908, 3.9202663430478424, 4.522271532099694, 4.987455992028117, 4.610949357971549, 4.09097977518104, 3.8805545328650624, 4.005960648180917, 4.433788646943867, 3.76249256497249, 3.7438765310216695, 4.180141364922747, 4.464939296944067, 4.261545615037903, 3.840385531075299, 3.9853640273213387, 3.865135641070083, 4.261553277960047, 4.288727738894522, 3.917686494300142, 4.697583020897582, 4.878785473993048, 4.220743410056457, 8.470652423100546, 3.79111451189965, 4.1189549611881375, 3.908779770601541, 3.819521723780781, 3.973711950937286, 4.292781406082213, 4.898619979154319, 3.9589598427992314, 4.014582997886464, 3.8553751497529447]
Total Epoch List: [69, 83, 107]
Total Time List: [1.3416893610265106, 1.4183835270814598, 1.2249888230580837]
T-times Epoch Time: 4.302669313811739 ~ 0.007604101710271626
T-times Total Epoch: 79.88888888888887 ~ 5.307809294442749
T-times Total Time: 1.2640036964892511 ~ 0.045541538074114755
T-times Inference Elapsed: 1.3825808876502226 ~ 0.023139401683583503
T-times Time Per Graph: 0.010748083070070845 ~ 0.00018303667992013333
T-times Speed: 95.85653068251423 ~ 0.8559722694517612
T-times cross validation test micro f1 score:0.9020717428679662 ~ 0.002077629252304298
T-times cross validation test precision:0.9488716609688909 ~ 0.012805445840565801
T-times cross validation test recall:0.8600961538461539 ~ 0.008538072919854194
T-times cross validation test f1_score:0.9020717428679662 ~ 0.0011002327336026745
