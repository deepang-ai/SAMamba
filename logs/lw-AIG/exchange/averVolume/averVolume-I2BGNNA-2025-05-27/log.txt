Namespace(seed=15, model='I2BGNNA', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf3561d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.19s
Epoch 3/1000, LR 0.000045
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.20s
Epoch 6/1000, LR 0.000135
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.21s
Epoch 10/1000, LR 0.000255
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4961 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4961 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.27s
Val loss: 0.6862 score: 0.5271 time: 0.19s
Test loss: 0.6857 score: 0.5116 time: 0.27s
Epoch 19/1000, LR 0.000285
Train loss: 0.5005;  Loss pred: 0.5005; Loss self: 0.0000; time: 0.27s
Val loss: 0.6835 score: 0.6047 time: 0.19s
Test loss: 0.6827 score: 0.6202 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4810;  Loss pred: 0.4810; Loss self: 0.0000; time: 0.27s
Val loss: 0.6800 score: 0.7442 time: 0.18s
Test loss: 0.6788 score: 0.7364 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4493;  Loss pred: 0.4493; Loss self: 0.0000; time: 0.27s
Val loss: 0.6758 score: 0.7752 time: 0.18s
Test loss: 0.6741 score: 0.7984 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4161;  Loss pred: 0.4161; Loss self: 0.0000; time: 0.27s
Val loss: 0.6707 score: 0.7984 time: 0.18s
Test loss: 0.6684 score: 0.8450 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3857;  Loss pred: 0.3857; Loss self: 0.0000; time: 0.27s
Val loss: 0.6645 score: 0.8062 time: 0.18s
Test loss: 0.6615 score: 0.8682 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3528;  Loss pred: 0.3528; Loss self: 0.0000; time: 0.27s
Val loss: 0.6568 score: 0.8140 time: 0.16s
Test loss: 0.6530 score: 0.8760 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3211;  Loss pred: 0.3211; Loss self: 0.0000; time: 0.26s
Val loss: 0.6473 score: 0.8372 time: 0.17s
Test loss: 0.6424 score: 0.8760 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2864;  Loss pred: 0.2864; Loss self: 0.0000; time: 0.26s
Val loss: 0.6358 score: 0.8372 time: 0.17s
Test loss: 0.6298 score: 0.8760 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.2528;  Loss pred: 0.2528; Loss self: 0.0000; time: 0.27s
Val loss: 0.6222 score: 0.8295 time: 0.17s
Test loss: 0.6150 score: 0.8760 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2229;  Loss pred: 0.2229; Loss self: 0.0000; time: 0.26s
Val loss: 0.6062 score: 0.8372 time: 0.17s
Test loss: 0.5975 score: 0.8760 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1991;  Loss pred: 0.1991; Loss self: 0.0000; time: 0.27s
Val loss: 0.5877 score: 0.8372 time: 0.17s
Test loss: 0.5773 score: 0.8760 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.27s
Val loss: 0.5667 score: 0.8372 time: 0.17s
Test loss: 0.5547 score: 0.8760 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 0.26s
Val loss: 0.5434 score: 0.8372 time: 0.17s
Test loss: 0.5299 score: 0.8760 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.27s
Val loss: 0.5176 score: 0.8527 time: 0.17s
Test loss: 0.5027 score: 0.8760 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1080;  Loss pred: 0.1080; Loss self: 0.0000; time: 0.26s
Val loss: 0.4902 score: 0.8527 time: 0.17s
Test loss: 0.4740 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.0858;  Loss pred: 0.0858; Loss self: 0.0000; time: 0.26s
Val loss: 0.4615 score: 0.8527 time: 0.17s
Test loss: 0.4440 score: 0.8837 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.27s
Val loss: 0.4325 score: 0.8605 time: 0.17s
Test loss: 0.4134 score: 0.8837 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.26s
Val loss: 0.4052 score: 0.8605 time: 0.17s
Test loss: 0.3842 score: 0.8837 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.27s
Val loss: 0.3801 score: 0.8605 time: 0.17s
Test loss: 0.3567 score: 0.8837 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0566;  Loss pred: 0.0566; Loss self: 0.0000; time: 0.27s
Val loss: 0.3573 score: 0.8605 time: 0.17s
Test loss: 0.3317 score: 0.8837 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.27s
Val loss: 0.3377 score: 0.8605 time: 0.17s
Test loss: 0.3087 score: 0.8837 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.26s
Val loss: 0.3217 score: 0.8605 time: 0.17s
Test loss: 0.2883 score: 0.8915 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.26s
Val loss: 0.3091 score: 0.8605 time: 0.17s
Test loss: 0.2719 score: 0.8992 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.27s
Val loss: 0.2985 score: 0.8605 time: 0.17s
Test loss: 0.2573 score: 0.8992 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.26s
Val loss: 0.2882 score: 0.8605 time: 0.17s
Test loss: 0.2432 score: 0.8992 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.27s
Val loss: 0.2815 score: 0.8605 time: 0.18s
Test loss: 0.2323 score: 0.8992 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.27s
Val loss: 0.2772 score: 0.8682 time: 0.17s
Test loss: 0.2242 score: 0.8992 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.27s
Val loss: 0.2742 score: 0.8682 time: 0.17s
Test loss: 0.2178 score: 0.9070 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.27s
Val loss: 0.2732 score: 0.8682 time: 0.17s
Test loss: 0.2131 score: 0.9070 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.26s
Val loss: 0.2744 score: 0.8682 time: 0.17s
Test loss: 0.2105 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.27s
Val loss: 0.2718 score: 0.8605 time: 0.17s
Test loss: 0.2050 score: 0.9070 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.26s
Val loss: 0.2756 score: 0.8605 time: 0.17s
Test loss: 0.2036 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.26s
Val loss: 0.2789 score: 0.8605 time: 0.17s
Test loss: 0.2009 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.27s
Val loss: 0.2831 score: 0.8605 time: 0.17s
Test loss: 0.1991 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.27s
Val loss: 0.2889 score: 0.8605 time: 0.17s
Test loss: 0.1976 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.27s
Val loss: 0.2936 score: 0.8605 time: 0.17s
Test loss: 0.1960 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.26s
Val loss: 0.3007 score: 0.8605 time: 0.17s
Test loss: 0.1964 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.27s
Val loss: 0.3080 score: 0.8682 time: 0.17s
Test loss: 0.1970 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.27s
Val loss: 0.3174 score: 0.8682 time: 0.17s
Test loss: 0.1998 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.27s
Val loss: 0.3265 score: 0.8682 time: 0.18s
Test loss: 0.2034 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.27s
Val loss: 0.3348 score: 0.8682 time: 0.17s
Test loss: 0.2068 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.28s
Val loss: 0.3436 score: 0.8682 time: 0.17s
Test loss: 0.2113 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.26s
Val loss: 0.3474 score: 0.8682 time: 0.17s
Test loss: 0.2147 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.27s
Val loss: 0.3518 score: 0.8605 time: 0.18s
Test loss: 0.2181 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.27s
Val loss: 0.3546 score: 0.8605 time: 0.17s
Test loss: 0.2194 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.27s
Val loss: 0.3580 score: 0.8605 time: 0.17s
Test loss: 0.2230 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.26s
Val loss: 0.3608 score: 0.8605 time: 0.17s
Test loss: 0.2256 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.27s
Val loss: 0.3618 score: 0.8605 time: 0.17s
Test loss: 0.2266 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.27s
Val loss: 0.3637 score: 0.8682 time: 0.17s
Test loss: 0.2283 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.27s
Val loss: 0.3664 score: 0.8682 time: 0.18s
Test loss: 0.2297 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.27s
Val loss: 0.3709 score: 0.8682 time: 0.17s
Test loss: 0.2319 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0332,   Val_Loss: 0.2718,   Val_Precision: 0.9600,   Val_Recall: 0.7500,   Val_accuracy: 0.8421,   Val_Score: 0.8605,   Val_Loss: 0.2718,   Test_Precision: 0.9818,   Test_Recall: 0.8308,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.2050


[0.18618092802353203, 0.19657805701717734, 0.18775695911608636, 0.1721046648453921, 0.20311037194915116, 0.18642077990807593, 0.18082049395889044, 0.17799630900844932, 0.21218924690037966, 0.18525908910669386, 0.18009238503873348, 0.181969370925799, 0.161266882205382, 0.17403713194653392, 0.171165935928002, 0.18297730386257172, 0.1897941050119698, 0.2731350250542164, 0.18563697719946504, 0.18879745109006763, 0.17997717997059226, 0.177901869174093, 0.18228526413440704, 0.17087721405550838, 0.1704679299145937, 0.16625817585736513, 0.17559016100130975, 0.17090958007611334, 0.16740149492397904, 0.17145002516917884, 0.17380339303053916, 0.17875967710278928, 0.17543348483741283, 0.17286804411560297, 0.17644482804462314, 0.1819818359799683, 0.16941162501461804, 0.17315985402092338, 0.17632513400167227, 0.17499897605739534, 0.16817640094086528, 0.17281204601749778, 0.17489536805078387, 0.1682910448871553, 0.17402966390363872, 0.17742549302056432, 0.1718050620984286, 0.1739712068811059, 0.1737079150043428, 0.17239457904361188, 0.1708531049080193, 0.17271101102232933, 0.17648081691004336, 0.1708798899780959, 0.17304537608288229, 0.17210328415967524, 0.17812334117479622, 0.17373536503873765, 0.17537904507480562, 0.1738894428126514, 0.17954506492242217, 0.17489478597417474, 0.17151029384694993, 0.1778975510969758, 0.1735234057996422, 0.175909809069708, 0.17985042999498546, 0.17087742290459573, 0.17416779207997024]
[0.001443263007934357, 0.0015238609071099019, 0.0014554803032254756, 0.0013341446887239698, 0.001574499007357761, 0.0014451223248688055, 0.0014017092554952748, 0.0013798163489027078, 0.0016448778829486795, 0.0014361169698193323, 0.0013960650003002595, 0.001410615278494566, 0.0012501308698091628, 0.0013491250538491002, 0.0013268677203721086, 0.001418428712112959, 0.001471272131875735, 0.0021173257756140805, 0.001439046334879574, 0.0014635461324811444, 0.001395171937756529, 0.0013790842571635117, 0.001413064063057419, 0.0013246295663217704, 0.001321456821043362, 0.0012888230686617453, 0.001361164038769843, 0.001324880465706305, 0.0012976860071626283, 0.001329069962551774, 0.0013473131242677453, 0.0013857339310293744, 0.0013599494948636653, 0.0013400623574852943, 0.001367789364687001, 0.0014107119068214598, 0.0013132684109660314, 0.0013423244497745997, 0.0013668615038889323, 0.0013565812097472508, 0.0013036930305493433, 0.001339628263701533, 0.0013557780469053012, 0.0013045817433112814, 0.001349067162043711, 0.0013753914187640646, 0.001331822186809524, 0.0013486140068302782, 0.0013465729845297892, 0.0013363920856093944, 0.001324442673705576, 0.0013388450466847235, 0.001368068348139871, 0.00132465030990772, 0.0013414370238983124, 0.0013341339857339165, 0.0013808010943782653, 0.0013467857754940903, 0.0013595274812000435, 0.0013479801768422589, 0.001391822208700947, 0.001355773534683525, 0.0013295371616042632, 0.0013790507836974868, 0.0013451426806173814, 0.0013636419307729302, 0.0013941893798060888, 0.0013246311853069436, 0.0013501379231005445]
[692.8744064681816, 656.2278718052837, 687.058421734674, 749.5438901431603, 635.1226614478124, 691.982943444448, 713.4147085635557, 724.7341291435234, 607.9478667482333, 696.3221109529838, 716.2990260374154, 708.9105124873013, 799.9162520901942, 741.2211322790023, 753.654629354883, 705.0054694044879, 679.6839132167162, 472.293877265993, 694.9046571760906, 683.2719364333966, 716.7575357113509, 725.1188568106717, 707.6819983917217, 754.9280383170056, 756.74057909092, 775.9016922612614, 734.66530962995, 754.7850737363627, 770.6024373234059, 752.4058388017665, 742.2179610574881, 721.639253833644, 735.3214246388244, 746.2339303945218, 731.1067228752986, 708.8619548502616, 761.4589612068768, 744.9763730131846, 731.6030169514946, 737.147170265106, 767.0517342404027, 746.4757403944997, 737.5838562090601, 766.5291999731701, 741.2529399093023, 727.0657547788152, 750.8509843912211, 741.5020123885225, 742.6259188982548, 748.2833898585984, 755.0345665034803, 746.9124246127072, 730.9576318753924, 754.91621639349, 745.4692111403995, 749.5499033029226, 724.2172707360658, 742.5085846582643, 735.5496772432348, 741.8506719754383, 718.4825718030085, 737.5863110009944, 752.1414435632377, 725.1364574978287, 743.4155606013699, 733.3303394632247, 717.2626721192535, 754.927115632024, 740.6650705014899]
Elapsed: 0.17835480039534363~0.01404044865451134
Time per graph: 0.0013825953519018882~0.00010884068724427394
Speed: 726.6620257539727~43.729184568574865
Total Time: 0.1746
best val loss: 0.2718168351683737 test_score: 0.9070

Testing...
Test loss: 0.2242 score: 0.8992 time: 0.17s
test Score 0.8992
Epoch Time List: [0.8017544918693602, 0.6555673787370324, 0.657769767800346, 0.5959120411425829, 0.6552093029022217, 0.6451585362665355, 0.624674468068406, 0.6164452037774026, 0.6983574617188424, 0.624930018093437, 0.7200210921000689, 0.6297661752905697, 0.634046294959262, 0.5923942590598017, 0.592369272839278, 0.7132060450967401, 0.6411763329524547, 0.7295834210235626, 0.6399023733101785, 0.6388592899311334, 0.6258268079254776, 0.6251674320083112, 0.6318882459308952, 0.5988149878103286, 0.5941635898780078, 0.5929048070684075, 0.609873304143548, 0.5993357268162072, 0.5975260229315609, 0.6072246138937771, 0.599108018912375, 0.6085274750366807, 0.6101081757806242, 0.5993651119060814, 0.6089603640139103, 0.6105151379015297, 0.6032386650331318, 0.607920465990901, 0.6048359950073063, 0.6076062412466854, 0.6002956479787827, 0.610068985959515, 0.600623567122966, 0.6052840750198811, 0.616429687011987, 0.6115204519592226, 0.6049161050468683, 0.604613175150007, 0.6046114980708808, 0.6025941281113774, 0.603105021873489, 0.6043417220935225, 0.6159117717761546, 0.5999778304249048, 0.6020580739714205, 0.6029467820189893, 0.6139386943541467, 0.6160465432330966, 0.6152490789536387, 0.6150313238613307, 0.6127949878573418, 0.6133281011134386, 0.5990058239549398, 0.611004987033084, 0.6052573712076992, 0.6076052419375628, 0.6163857378996909, 0.6093320427462459, 0.6141979810781777]
Total Epoch List: [69]
Total Time List: [0.17463409900665283]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf355f30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5039 time: 0.18s
Test loss: 0.6884 score: 0.5039 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.25s
Val loss: 0.6846 score: 0.5581 time: 0.18s
Test loss: 0.6866 score: 0.5194 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.26s
Val loss: 0.6819 score: 0.5891 time: 0.18s
Test loss: 0.6844 score: 0.5271 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.25s
Val loss: 0.6783 score: 0.6202 time: 0.18s
Test loss: 0.6815 score: 0.5581 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5015;  Loss pred: 0.5015; Loss self: 0.0000; time: 0.25s
Val loss: 0.6740 score: 0.6279 time: 0.18s
Test loss: 0.6779 score: 0.5891 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4815;  Loss pred: 0.4815; Loss self: 0.0000; time: 0.25s
Val loss: 0.6688 score: 0.6822 time: 0.18s
Test loss: 0.6735 score: 0.6589 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4573;  Loss pred: 0.4573; Loss self: 0.0000; time: 0.25s
Val loss: 0.6624 score: 0.7132 time: 0.18s
Test loss: 0.6681 score: 0.7132 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.4361;  Loss pred: 0.4361; Loss self: 0.0000; time: 0.25s
Val loss: 0.6547 score: 0.7209 time: 0.18s
Test loss: 0.6616 score: 0.7442 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 0.25s
Val loss: 0.6457 score: 0.7597 time: 0.18s
Test loss: 0.6538 score: 0.7597 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3855;  Loss pred: 0.3855; Loss self: 0.0000; time: 0.25s
Val loss: 0.6351 score: 0.7674 time: 0.18s
Test loss: 0.6446 score: 0.7674 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3636;  Loss pred: 0.3636; Loss self: 0.0000; time: 0.25s
Val loss: 0.6228 score: 0.7984 time: 0.18s
Test loss: 0.6337 score: 0.7829 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.3322;  Loss pred: 0.3322; Loss self: 0.0000; time: 0.25s
Val loss: 0.6092 score: 0.8295 time: 0.18s
Test loss: 0.6214 score: 0.8140 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.3095;  Loss pred: 0.3095; Loss self: 0.0000; time: 0.25s
Val loss: 0.5940 score: 0.8372 time: 0.19s
Test loss: 0.6074 score: 0.8217 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.3154;  Loss pred: 0.3154; Loss self: 0.0000; time: 0.25s
Val loss: 0.5764 score: 0.8527 time: 0.19s
Test loss: 0.5911 score: 0.8605 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2634;  Loss pred: 0.2634; Loss self: 0.0000; time: 0.25s
Val loss: 0.5595 score: 0.8605 time: 0.19s
Test loss: 0.5751 score: 0.8605 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 0.25s
Val loss: 0.5406 score: 0.8682 time: 0.27s
Test loss: 0.5568 score: 0.8760 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.2242;  Loss pred: 0.2242; Loss self: 0.0000; time: 0.25s
Val loss: 0.5213 score: 0.8837 time: 0.18s
Test loss: 0.5378 score: 0.8837 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1926;  Loss pred: 0.1926; Loss self: 0.0000; time: 0.25s
Val loss: 0.4996 score: 0.8837 time: 0.18s
Test loss: 0.5163 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1792;  Loss pred: 0.1792; Loss self: 0.0000; time: 0.35s
Val loss: 0.4770 score: 0.8837 time: 0.19s
Test loss: 0.4937 score: 0.8837 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1565;  Loss pred: 0.1565; Loss self: 0.0000; time: 0.25s
Val loss: 0.4535 score: 0.8992 time: 0.19s
Test loss: 0.4703 score: 0.8915 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 0.25s
Val loss: 0.4283 score: 0.9147 time: 0.27s
Test loss: 0.4452 score: 0.8915 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.25s
Val loss: 0.4004 score: 0.8992 time: 0.18s
Test loss: 0.4176 score: 0.9070 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.25s
Val loss: 0.3729 score: 0.8992 time: 0.18s
Test loss: 0.3901 score: 0.9225 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.30s
Val loss: 0.3459 score: 0.9147 time: 0.19s
Test loss: 0.3628 score: 0.9147 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0855;  Loss pred: 0.0855; Loss self: 0.0000; time: 0.26s
Val loss: 0.3194 score: 0.9225 time: 0.19s
Test loss: 0.3358 score: 0.9070 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0910;  Loss pred: 0.0910; Loss self: 0.0000; time: 0.26s
Val loss: 0.2964 score: 0.9225 time: 0.18s
Test loss: 0.3118 score: 0.9225 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 0.25s
Val loss: 0.2786 score: 0.9225 time: 0.18s
Test loss: 0.2925 score: 0.9225 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 0.25s
Val loss: 0.2611 score: 0.9225 time: 0.18s
Test loss: 0.2736 score: 0.9225 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.25s
Val loss: 0.2467 score: 0.9225 time: 0.18s
Test loss: 0.2584 score: 0.9225 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.25s
Val loss: 0.2348 score: 0.9225 time: 0.18s
Test loss: 0.2466 score: 0.9147 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.25s
Val loss: 0.2257 score: 0.9302 time: 0.19s
Test loss: 0.2388 score: 0.9070 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.25s
Val loss: 0.2162 score: 0.9302 time: 0.18s
Test loss: 0.2311 score: 0.9070 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.25s
Val loss: 0.2051 score: 0.9302 time: 0.18s
Test loss: 0.2221 score: 0.9225 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.25s
Val loss: 0.1954 score: 0.9380 time: 0.18s
Test loss: 0.2169 score: 0.9225 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.25s
Val loss: 0.1900 score: 0.9380 time: 0.18s
Test loss: 0.2170 score: 0.9302 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.25s
Val loss: 0.1887 score: 0.9380 time: 0.18s
Test loss: 0.2208 score: 0.9302 time: 0.31s
Epoch 52/1000, LR 0.000284
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.26s
Val loss: 0.1908 score: 0.9380 time: 0.18s
Test loss: 0.2268 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.26s
Val loss: 0.1939 score: 0.9380 time: 0.18s
Test loss: 0.2338 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.25s
Val loss: 0.1977 score: 0.9380 time: 0.18s
Test loss: 0.2429 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.25s
Val loss: 0.2013 score: 0.9380 time: 0.18s
Test loss: 0.2509 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.25s
Val loss: 0.2042 score: 0.9380 time: 0.19s
Test loss: 0.2584 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.25s
Val loss: 0.2083 score: 0.9380 time: 0.18s
Test loss: 0.2661 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.25s
Val loss: 0.2103 score: 0.9380 time: 0.18s
Test loss: 0.2709 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.25s
Val loss: 0.2090 score: 0.9380 time: 0.18s
Test loss: 0.2728 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.25s
Val loss: 0.2081 score: 0.9380 time: 0.18s
Test loss: 0.2747 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.25s
Val loss: 0.2074 score: 0.9380 time: 0.18s
Test loss: 0.2771 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.25s
Val loss: 0.2082 score: 0.9380 time: 0.18s
Test loss: 0.2818 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.25s
Val loss: 0.2092 score: 0.9380 time: 0.18s
Test loss: 0.2874 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.25s
Val loss: 0.2111 score: 0.9380 time: 0.18s
Test loss: 0.2941 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.25s
Val loss: 0.2134 score: 0.9380 time: 0.18s
Test loss: 0.2994 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.26s
Val loss: 0.2158 score: 0.9380 time: 0.20s
Test loss: 0.3041 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.26s
Val loss: 0.2187 score: 0.9380 time: 0.19s
Test loss: 0.3090 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.26s
Val loss: 0.2206 score: 0.9380 time: 0.19s
Test loss: 0.3127 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.26s
Val loss: 0.2220 score: 0.9380 time: 0.20s
Test loss: 0.3150 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.26s
Val loss: 0.2235 score: 0.9380 time: 0.18s
Test loss: 0.3163 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.25s
Val loss: 0.2254 score: 0.9380 time: 0.18s
Test loss: 0.3168 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0377,   Val_Loss: 0.1887,   Val_Precision: 0.9524,   Val_Recall: 0.9231,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1887,   Test_Precision: 0.9661,   Test_Recall: 0.8906,   Test_accuracy: 0.9268,   Test_Score: 0.9302,   Test_loss: 0.2208


[0.18618092802353203, 0.19657805701717734, 0.18775695911608636, 0.1721046648453921, 0.20311037194915116, 0.18642077990807593, 0.18082049395889044, 0.17799630900844932, 0.21218924690037966, 0.18525908910669386, 0.18009238503873348, 0.181969370925799, 0.161266882205382, 0.17403713194653392, 0.171165935928002, 0.18297730386257172, 0.1897941050119698, 0.2731350250542164, 0.18563697719946504, 0.18879745109006763, 0.17997717997059226, 0.177901869174093, 0.18228526413440704, 0.17087721405550838, 0.1704679299145937, 0.16625817585736513, 0.17559016100130975, 0.17090958007611334, 0.16740149492397904, 0.17145002516917884, 0.17380339303053916, 0.17875967710278928, 0.17543348483741283, 0.17286804411560297, 0.17644482804462314, 0.1819818359799683, 0.16941162501461804, 0.17315985402092338, 0.17632513400167227, 0.17499897605739534, 0.16817640094086528, 0.17281204601749778, 0.17489536805078387, 0.1682910448871553, 0.17402966390363872, 0.17742549302056432, 0.1718050620984286, 0.1739712068811059, 0.1737079150043428, 0.17239457904361188, 0.1708531049080193, 0.17271101102232933, 0.17648081691004336, 0.1708798899780959, 0.17304537608288229, 0.17210328415967524, 0.17812334117479622, 0.17373536503873765, 0.17537904507480562, 0.1738894428126514, 0.17954506492242217, 0.17489478597417474, 0.17151029384694993, 0.1778975510969758, 0.1735234057996422, 0.175909809069708, 0.17985042999498546, 0.17087742290459573, 0.17416779207997024, 0.17393215699121356, 0.17178101115860045, 0.17355912900529802, 0.17649681400507689, 0.17689946107566357, 0.17489602300338447, 0.1725397079717368, 0.1765042250044644, 0.17812266992405057, 0.17593822604976594, 0.1750652629416436, 0.1776518700644374, 0.18319995095953345, 0.1794312009587884, 0.1778971019666642, 0.1833613810595125, 0.17564253602176905, 0.1780928468797356, 0.1759950399864465, 0.17273017088882625, 0.17518748785369098, 0.18129990994930267, 0.17729454999789596, 0.17320860316976905, 0.174223291920498, 0.1768441318999976, 0.18033200409263372, 0.17919662594795227, 0.17992597399279475, 0.17376316501758993, 0.17843657289631665, 0.17480141087435186, 0.17364548798650503, 0.18881526216864586, 0.1787511669099331, 0.1747583549004048, 0.1787474669981748, 0.1783160602208227, 0.1847867809701711, 0.18169436999596655, 0.1774260241072625, 0.17389351595193148, 0.1750178940128535, 0.17774309893138707, 0.1804970179218799, 0.17549270391464233, 0.1748055969364941, 0.17734900419600308, 0.17844452010467649, 0.17728300602175295, 0.3119803830049932, 0.17812381009571254, 0.17937432089820504, 0.18112702714279294, 0.17895425390452147, 0.17681316589005291, 0.17572384607046843, 0.17795499507337809, 0.1743879651185125, 0.17465495015494525, 0.17822141596116126, 0.18027245299890637, 0.17496055201627314, 0.17664111708290875, 0.1931469300761819, 0.18953466904349625, 0.19320324598811567, 0.19237883388996124, 0.17968741292133927, 0.18448290904052556, 0.18219564692117274]
[0.001443263007934357, 0.0015238609071099019, 0.0014554803032254756, 0.0013341446887239698, 0.001574499007357761, 0.0014451223248688055, 0.0014017092554952748, 0.0013798163489027078, 0.0016448778829486795, 0.0014361169698193323, 0.0013960650003002595, 0.001410615278494566, 0.0012501308698091628, 0.0013491250538491002, 0.0013268677203721086, 0.001418428712112959, 0.001471272131875735, 0.0021173257756140805, 0.001439046334879574, 0.0014635461324811444, 0.001395171937756529, 0.0013790842571635117, 0.001413064063057419, 0.0013246295663217704, 0.001321456821043362, 0.0012888230686617453, 0.001361164038769843, 0.001324880465706305, 0.0012976860071626283, 0.001329069962551774, 0.0013473131242677453, 0.0013857339310293744, 0.0013599494948636653, 0.0013400623574852943, 0.001367789364687001, 0.0014107119068214598, 0.0013132684109660314, 0.0013423244497745997, 0.0013668615038889323, 0.0013565812097472508, 0.0013036930305493433, 0.001339628263701533, 0.0013557780469053012, 0.0013045817433112814, 0.001349067162043711, 0.0013753914187640646, 0.001331822186809524, 0.0013486140068302782, 0.0013465729845297892, 0.0013363920856093944, 0.001324442673705576, 0.0013388450466847235, 0.001368068348139871, 0.00132465030990772, 0.0013414370238983124, 0.0013341339857339165, 0.0013808010943782653, 0.0013467857754940903, 0.0013595274812000435, 0.0013479801768422589, 0.001391822208700947, 0.001355773534683525, 0.0013295371616042632, 0.0013790507836974868, 0.0013451426806173814, 0.0013636419307729302, 0.0013941893798060888, 0.0013246311853069436, 0.0013501379231005445, 0.0013483112945055315, 0.0013316357454155074, 0.0013454196046922327, 0.001368192356628503, 0.00137131365174933, 0.001355783124057244, 0.001337517116059975, 0.0013682498062361581, 0.001380795890884113, 0.001363862217440046, 0.0013570950615631287, 0.001377146279569282, 0.0014201546586010345, 0.001390939542316189, 0.0013790473020671642, 0.001421406054724903, 0.0013615700466803801, 0.0013805647044940743, 0.0013643026355538489, 0.0013389935727815988, 0.0013580425415014804, 0.0014054256585217262, 0.0013743763565728369, 0.0013427023501532485, 0.0013505681544224652, 0.0013708847434108341, 0.0013979225123459979, 0.001389121131379475, 0.0013947749921922074, 0.0013470012792061235, 0.0013832292472582687, 0.0013550496967004021, 0.0013460890541589538, 0.0014636842028577198, 0.001385667960542117, 0.0013547159294605023, 0.0013856392790556186, 0.0013822950404714937, 0.0014324556664354348, 0.001408483488340826, 0.001375395535715213, 0.0013480117515653604, 0.0013567278605647558, 0.001377853480088272, 0.0013992016893168984, 0.0013604085574778475, 0.001355082146794528, 0.0013747984821395587, 0.001383290853524624, 0.001374286868385682, 0.0024184525814340556, 0.0013808047294241283, 0.0013904986116139925, 0.001404085481727077, 0.0013872422783296238, 0.0013706446968221157, 0.0013622003571354142, 0.0013794960858401401, 0.0013518446908411823, 0.0013539143422863973, 0.0013815613640400098, 0.0013974608759605145, 0.0013562833489633578, 0.0013693109851388276, 0.0014972630238463713, 0.0014692610003371803, 0.0014976995813032223, 0.0014913087898446608, 0.0013929256815607696, 0.0014301000700815934, 0.0014123693559780833]
[692.8744064681816, 656.2278718052837, 687.058421734674, 749.5438901431603, 635.1226614478124, 691.982943444448, 713.4147085635557, 724.7341291435234, 607.9478667482333, 696.3221109529838, 716.2990260374154, 708.9105124873013, 799.9162520901942, 741.2211322790023, 753.654629354883, 705.0054694044879, 679.6839132167162, 472.293877265993, 694.9046571760906, 683.2719364333966, 716.7575357113509, 725.1188568106717, 707.6819983917217, 754.9280383170056, 756.74057909092, 775.9016922612614, 734.66530962995, 754.7850737363627, 770.6024373234059, 752.4058388017665, 742.2179610574881, 721.639253833644, 735.3214246388244, 746.2339303945218, 731.1067228752986, 708.8619548502616, 761.4589612068768, 744.9763730131846, 731.6030169514946, 737.147170265106, 767.0517342404027, 746.4757403944997, 737.5838562090601, 766.5291999731701, 741.2529399093023, 727.0657547788152, 750.8509843912211, 741.5020123885225, 742.6259188982548, 748.2833898585984, 755.0345665034803, 746.9124246127072, 730.9576318753924, 754.91621639349, 745.4692111403995, 749.5499033029226, 724.2172707360658, 742.5085846582643, 735.5496772432348, 741.8506719754383, 718.4825718030085, 737.5863110009944, 752.1414435632377, 725.1364574978287, 743.4155606013699, 733.3303394632247, 717.2626721192535, 754.927115632024, 740.6650705014899, 741.6684886309818, 750.9561105150209, 743.2625453891404, 730.8913802619086, 729.2277727450172, 737.5810940967118, 747.6539836333275, 730.8606918431394, 724.2199999304081, 733.2118942901642, 736.8680561317351, 726.1392742626884, 704.1486601079629, 718.9385085241034, 725.1382882233409, 703.5287324659234, 734.4462390591525, 724.3412762507664, 732.9752020848678, 746.8295743366563, 736.3539575824914, 711.5282077970837, 727.6027379382554, 744.766701187248, 740.4291273457603, 729.4559260408332, 715.3472321736896, 719.879625621233, 716.9615211040397, 742.38979237597, 722.9459628489809, 737.9803135154662, 742.8928991810333, 683.2074829034736, 721.6736104721426, 738.1621329264481, 721.6885484666321, 723.434556821462, 698.1018843594858, 709.9834739120628, 727.0635784636269, 741.8332954729539, 737.067490884824, 725.7665742049259, 714.6932480393217, 735.0732943447275, 737.9626411325089, 727.3793308556242, 722.9137657145645, 727.6501165834894, 413.4875364837775, 724.2153641934983, 719.166485782586, 712.207349918584, 720.8546161122615, 729.5836786284094, 734.1063998125143, 724.9023830255961, 739.7299458843547, 738.5991630100239, 723.8187358365069, 715.5835395482336, 737.3090591758173, 730.2942946146123, 667.8853241370144, 680.6142678329516, 667.6906453628375, 670.5519385453116, 717.9133913874734, 699.2517663067772, 708.0300884236387]
Elapsed: 0.17930013554536606~0.01531341545449009
Time per graph: 0.0013899235313594265~0.0001187086469340317
Speed: 723.003946815002~42.39034694445195
Total Time: 0.1829
best val loss: 0.18869747775931692 test_score: 0.9302

Testing...
Test loss: 0.2169 score: 0.9225 time: 0.18s
test Score 0.9225
Epoch Time List: [0.8017544918693602, 0.6555673787370324, 0.657769767800346, 0.5959120411425829, 0.6552093029022217, 0.6451585362665355, 0.624674468068406, 0.6164452037774026, 0.6983574617188424, 0.624930018093437, 0.7200210921000689, 0.6297661752905697, 0.634046294959262, 0.5923942590598017, 0.592369272839278, 0.7132060450967401, 0.6411763329524547, 0.7295834210235626, 0.6399023733101785, 0.6388592899311334, 0.6258268079254776, 0.6251674320083112, 0.6318882459308952, 0.5988149878103286, 0.5941635898780078, 0.5929048070684075, 0.609873304143548, 0.5993357268162072, 0.5975260229315609, 0.6072246138937771, 0.599108018912375, 0.6085274750366807, 0.6101081757806242, 0.5993651119060814, 0.6089603640139103, 0.6105151379015297, 0.6032386650331318, 0.607920465990901, 0.6048359950073063, 0.6076062412466854, 0.6002956479787827, 0.610068985959515, 0.600623567122966, 0.6052840750198811, 0.616429687011987, 0.6115204519592226, 0.6049161050468683, 0.604613175150007, 0.6046114980708808, 0.6025941281113774, 0.603105021873489, 0.6043417220935225, 0.6159117717761546, 0.5999778304249048, 0.6020580739714205, 0.6029467820189893, 0.6139386943541467, 0.6160465432330966, 0.6152490789536387, 0.6150313238613307, 0.6127949878573418, 0.6133281011134386, 0.5990058239549398, 0.611004987033084, 0.6052573712076992, 0.6076052419375628, 0.6163857378996909, 0.6093320427462459, 0.6141979810781777, 0.5957623259164393, 0.5949045452289283, 0.5941195969935507, 0.5956432134844363, 0.5974706911947578, 0.5936045781709254, 0.5971223551314324, 0.5992109740618616, 0.6023502282332629, 0.6001167721115053, 0.5982098018284887, 0.6034212016966194, 0.6103187259286642, 0.6106116818264127, 0.6033515890594572, 0.6128208809532225, 0.6031666791532189, 0.6091248888988048, 0.5983420959673822, 0.5988281250465661, 0.5998559058643878, 0.6043687278870493, 0.605030510108918, 0.5975488421972841, 0.5979090880136937, 0.6022782272193581, 0.6047424972057343, 0.6109073900151998, 0.6117822369560599, 0.609216048149392, 0.7005714189726859, 0.5986325140111148, 0.6058120150119066, 0.7210351249668747, 0.619359286967665, 0.695295395096764, 0.6012419308535755, 0.6022339090704918, 0.6736878149677068, 0.6276397188194096, 0.6132973551284522, 0.6018253150396049, 0.6045965361408889, 0.5994148487225175, 0.6072070631198585, 0.6057411511428654, 0.60275909001939, 0.6039693509228528, 0.6048457317519933, 0.601720335194841, 0.740015584975481, 0.6135080889798701, 0.6147942498791963, 0.6100111929699779, 0.6086681326851249, 0.6123108169995248, 0.6010182648897171, 0.6052010955754668, 0.6016366970725358, 0.5998817370273173, 0.6041441429406404, 0.6051554237492383, 0.6027472619898617, 0.6010181419551373, 0.620797973126173, 0.6417993390932679, 0.6421453419607133, 0.641262955032289, 0.6347319921478629, 0.6197640390601009, 0.6133946699555963]
Total Epoch List: [69, 71]
Total Time List: [0.17463409900665283, 0.18285132595337927]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf311630>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.17s
Epoch 15/1000, LR 0.000290
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.18s
Epoch 16/1000, LR 0.000290
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.5000 time: 0.16s
Epoch 20/1000, LR 0.000290
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6816 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5000 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5000 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6691 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.5000 time: 0.19s
Epoch 23/1000, LR 0.000290
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6612 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6664 score: 0.5000 time: 0.18s
Epoch 24/1000, LR 0.000290
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.28s
Val loss: 0.6500 score: 0.5194 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6565 score: 0.5000 time: 0.17s
Epoch 25/1000, LR 0.000290
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.29s
Val loss: 0.6351 score: 0.5504 time: 0.17s
Test loss: 0.6429 score: 0.5312 time: 0.18s
Epoch 26/1000, LR 0.000290
Train loss: 0.5235;  Loss pred: 0.5235; Loss self: 0.0000; time: 0.28s
Val loss: 0.6181 score: 0.5814 time: 0.17s
Test loss: 0.6278 score: 0.5703 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.28s
Val loss: 0.5985 score: 0.6434 time: 0.15s
Test loss: 0.6095 score: 0.5781 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.28s
Val loss: 0.5745 score: 0.8372 time: 0.15s
Test loss: 0.5872 score: 0.7969 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.4743;  Loss pred: 0.4743; Loss self: 0.0000; time: 0.28s
Val loss: 0.5452 score: 0.9380 time: 0.15s
Test loss: 0.5602 score: 0.8750 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.4526;  Loss pred: 0.4526; Loss self: 0.0000; time: 0.28s
Val loss: 0.5137 score: 0.9380 time: 0.15s
Test loss: 0.5310 score: 0.9062 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.4296;  Loss pred: 0.4296; Loss self: 0.0000; time: 0.27s
Val loss: 0.4793 score: 0.9225 time: 0.15s
Test loss: 0.4989 score: 0.9141 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 0.4126;  Loss pred: 0.4126; Loss self: 0.0000; time: 0.28s
Val loss: 0.4507 score: 0.9147 time: 0.15s
Test loss: 0.4722 score: 0.9219 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3987;  Loss pred: 0.3987; Loss self: 0.0000; time: 0.28s
Val loss: 0.4225 score: 0.9147 time: 0.15s
Test loss: 0.4461 score: 0.9062 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.3768;  Loss pred: 0.3768; Loss self: 0.0000; time: 0.27s
Val loss: 0.4033 score: 0.9302 time: 0.15s
Test loss: 0.4296 score: 0.9141 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.3571;  Loss pred: 0.3571; Loss self: 0.0000; time: 0.28s
Val loss: 0.3699 score: 0.9380 time: 0.15s
Test loss: 0.3997 score: 0.9141 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.3359;  Loss pred: 0.3359; Loss self: 0.0000; time: 0.27s
Val loss: 0.3486 score: 0.9302 time: 0.16s
Test loss: 0.3818 score: 0.9062 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.3097;  Loss pred: 0.3097; Loss self: 0.0000; time: 0.27s
Val loss: 0.3308 score: 0.9225 time: 0.15s
Test loss: 0.3678 score: 0.9062 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.2920;  Loss pred: 0.2920; Loss self: 0.0000; time: 0.28s
Val loss: 0.3081 score: 0.9302 time: 0.16s
Test loss: 0.3490 score: 0.8984 time: 0.17s
Epoch 39/1000, LR 0.000289
Train loss: 0.2619;  Loss pred: 0.2619; Loss self: 0.0000; time: 0.28s
Val loss: 0.2848 score: 0.9225 time: 0.16s
Test loss: 0.3285 score: 0.8906 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.2503;  Loss pred: 0.2503; Loss self: 0.0000; time: 0.28s
Val loss: 0.2620 score: 0.9302 time: 0.16s
Test loss: 0.3079 score: 0.8906 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 0.2319;  Loss pred: 0.2319; Loss self: 0.0000; time: 0.28s
Val loss: 0.2594 score: 0.9225 time: 0.16s
Test loss: 0.3049 score: 0.9297 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 0.2156;  Loss pred: 0.2156; Loss self: 0.0000; time: 0.28s
Val loss: 0.2417 score: 0.9225 time: 0.16s
Test loss: 0.2891 score: 0.9297 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.28s
Val loss: 0.2300 score: 0.9225 time: 0.16s
Test loss: 0.2784 score: 0.8984 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 0.1900;  Loss pred: 0.1900; Loss self: 0.0000; time: 0.28s
Val loss: 0.2219 score: 0.9225 time: 0.16s
Test loss: 0.2705 score: 0.8984 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 0.28s
Val loss: 0.2192 score: 0.9225 time: 0.16s
Test loss: 0.2744 score: 0.8906 time: 0.17s
Epoch 46/1000, LR 0.000289
Train loss: 0.1826;  Loss pred: 0.1826; Loss self: 0.0000; time: 0.27s
Val loss: 0.2126 score: 0.9302 time: 0.15s
Test loss: 0.2676 score: 0.8906 time: 0.16s
Epoch 47/1000, LR 0.000289
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.27s
Val loss: 0.2142 score: 0.9380 time: 0.15s
Test loss: 0.2770 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1753;  Loss pred: 0.1753; Loss self: 0.0000; time: 0.27s
Val loss: 0.2062 score: 0.9302 time: 0.24s
Test loss: 0.2670 score: 0.8906 time: 0.16s
Epoch 49/1000, LR 0.000289
Train loss: 0.1564;  Loss pred: 0.1564; Loss self: 0.0000; time: 0.27s
Val loss: 0.2137 score: 0.9147 time: 0.15s
Test loss: 0.2787 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.28s
Val loss: 0.2074 score: 0.9225 time: 0.15s
Test loss: 0.2712 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 0.27s
Val loss: 0.2058 score: 0.9225 time: 0.26s
Test loss: 0.2690 score: 0.8906 time: 0.17s
Epoch 52/1000, LR 0.000289
Train loss: 0.1359;  Loss pred: 0.1359; Loss self: 0.0000; time: 0.28s
Val loss: 0.2046 score: 0.9302 time: 0.17s
Test loss: 0.2632 score: 0.8984 time: 0.17s
Epoch 53/1000, LR 0.000289
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.29s
Val loss: 0.2117 score: 0.9225 time: 0.16s
Test loss: 0.2819 score: 0.8828 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.28s
Val loss: 0.2023 score: 0.9147 time: 0.17s
Test loss: 0.2728 score: 0.8828 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 0.1221;  Loss pred: 0.1221; Loss self: 0.0000; time: 0.29s
Val loss: 0.2068 score: 0.9225 time: 0.17s
Test loss: 0.2878 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.36s
Val loss: 0.1910 score: 0.9225 time: 0.16s
Test loss: 0.2647 score: 0.8906 time: 0.17s
Epoch 57/1000, LR 0.000288
Train loss: 0.1076;  Loss pred: 0.1076; Loss self: 0.0000; time: 0.29s
Val loss: 0.1972 score: 0.9225 time: 0.17s
Test loss: 0.2819 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.1003;  Loss pred: 0.1003; Loss self: 0.0000; time: 0.29s
Val loss: 0.1808 score: 0.9302 time: 0.17s
Test loss: 0.2619 score: 0.8984 time: 0.16s
Epoch 59/1000, LR 0.000288
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.28s
Val loss: 0.1718 score: 0.9225 time: 0.15s
Test loss: 0.2551 score: 0.8984 time: 0.16s
Epoch 60/1000, LR 0.000288
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.28s
Val loss: 0.1837 score: 0.9147 time: 0.16s
Test loss: 0.2778 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.28s
Val loss: 0.1848 score: 0.9147 time: 0.16s
Test loss: 0.2812 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.28s
Val loss: 0.1954 score: 0.9147 time: 0.16s
Test loss: 0.2985 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0723;  Loss pred: 0.0723; Loss self: 0.0000; time: 0.28s
Val loss: 0.1988 score: 0.9225 time: 0.15s
Test loss: 0.3049 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.28s
Val loss: 0.1762 score: 0.9380 time: 0.16s
Test loss: 0.2616 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.28s
Val loss: 0.1907 score: 0.9302 time: 0.16s
Test loss: 0.2639 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.28s
Val loss: 0.1883 score: 0.9225 time: 0.15s
Test loss: 0.2657 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.28s
Val loss: 0.1843 score: 0.9225 time: 0.15s
Test loss: 0.2631 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.28s
Val loss: 0.1718 score: 0.9380 time: 0.16s
Test loss: 0.2665 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.28s
Val loss: 0.1716 score: 0.9302 time: 0.16s
Test loss: 0.2661 score: 0.8984 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.28s
Val loss: 0.1688 score: 0.9380 time: 0.16s
Test loss: 0.2690 score: 0.8984 time: 0.17s
Epoch 71/1000, LR 0.000287
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.28s
Val loss: 0.1667 score: 0.9302 time: 0.16s
Test loss: 0.2766 score: 0.9062 time: 0.16s
Epoch 72/1000, LR 0.000287
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.28s
Val loss: 0.1703 score: 0.9457 time: 0.16s
Test loss: 0.2945 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.28s
Val loss: 0.1758 score: 0.9380 time: 0.15s
Test loss: 0.3061 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.28s
Val loss: 0.1679 score: 0.9457 time: 0.16s
Test loss: 0.2860 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.28s
Val loss: 0.1750 score: 0.9380 time: 0.16s
Test loss: 0.2978 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0384;  Loss pred: 0.0384; Loss self: 0.0000; time: 0.28s
Val loss: 0.1797 score: 0.9302 time: 0.16s
Test loss: 0.3038 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.28s
Val loss: 0.1718 score: 0.9457 time: 0.16s
Test loss: 0.2883 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.28s
Val loss: 0.1705 score: 0.9380 time: 0.15s
Test loss: 0.2719 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.30s
Val loss: 0.1714 score: 0.9380 time: 0.17s
Test loss: 0.2828 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.28s
Val loss: 0.1719 score: 0.9380 time: 0.16s
Test loss: 0.2731 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.28s
Val loss: 0.1729 score: 0.9380 time: 0.15s
Test loss: 0.2634 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.28s
Val loss: 0.1924 score: 0.9147 time: 0.15s
Test loss: 0.2557 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.28s
Val loss: 0.1854 score: 0.9225 time: 0.16s
Test loss: 0.2451 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.28s
Val loss: 0.1766 score: 0.9302 time: 0.16s
Test loss: 0.2564 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.28s
Val loss: 0.1764 score: 0.9302 time: 0.15s
Test loss: 0.2569 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.28s
Val loss: 0.1794 score: 0.9225 time: 0.16s
Test loss: 0.2728 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.29s
Val loss: 0.1847 score: 0.9380 time: 0.17s
Test loss: 0.2962 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.29s
Val loss: 0.1879 score: 0.9380 time: 0.17s
Test loss: 0.3076 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.29s
Val loss: 0.2116 score: 0.9225 time: 0.17s
Test loss: 0.3623 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.29s
Val loss: 0.1988 score: 0.9225 time: 0.17s
Test loss: 0.3359 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.29s
Val loss: 0.2035 score: 0.9380 time: 0.17s
Test loss: 0.3487 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 070,   Train_Loss: 0.0521,   Val_Loss: 0.1667,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1667,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9062,   Test_loss: 0.2766


[0.18618092802353203, 0.19657805701717734, 0.18775695911608636, 0.1721046648453921, 0.20311037194915116, 0.18642077990807593, 0.18082049395889044, 0.17799630900844932, 0.21218924690037966, 0.18525908910669386, 0.18009238503873348, 0.181969370925799, 0.161266882205382, 0.17403713194653392, 0.171165935928002, 0.18297730386257172, 0.1897941050119698, 0.2731350250542164, 0.18563697719946504, 0.18879745109006763, 0.17997717997059226, 0.177901869174093, 0.18228526413440704, 0.17087721405550838, 0.1704679299145937, 0.16625817585736513, 0.17559016100130975, 0.17090958007611334, 0.16740149492397904, 0.17145002516917884, 0.17380339303053916, 0.17875967710278928, 0.17543348483741283, 0.17286804411560297, 0.17644482804462314, 0.1819818359799683, 0.16941162501461804, 0.17315985402092338, 0.17632513400167227, 0.17499897605739534, 0.16817640094086528, 0.17281204601749778, 0.17489536805078387, 0.1682910448871553, 0.17402966390363872, 0.17742549302056432, 0.1718050620984286, 0.1739712068811059, 0.1737079150043428, 0.17239457904361188, 0.1708531049080193, 0.17271101102232933, 0.17648081691004336, 0.1708798899780959, 0.17304537608288229, 0.17210328415967524, 0.17812334117479622, 0.17373536503873765, 0.17537904507480562, 0.1738894428126514, 0.17954506492242217, 0.17489478597417474, 0.17151029384694993, 0.1778975510969758, 0.1735234057996422, 0.175909809069708, 0.17985042999498546, 0.17087742290459573, 0.17416779207997024, 0.17393215699121356, 0.17178101115860045, 0.17355912900529802, 0.17649681400507689, 0.17689946107566357, 0.17489602300338447, 0.1725397079717368, 0.1765042250044644, 0.17812266992405057, 0.17593822604976594, 0.1750652629416436, 0.1776518700644374, 0.18319995095953345, 0.1794312009587884, 0.1778971019666642, 0.1833613810595125, 0.17564253602176905, 0.1780928468797356, 0.1759950399864465, 0.17273017088882625, 0.17518748785369098, 0.18129990994930267, 0.17729454999789596, 0.17320860316976905, 0.174223291920498, 0.1768441318999976, 0.18033200409263372, 0.17919662594795227, 0.17992597399279475, 0.17376316501758993, 0.17843657289631665, 0.17480141087435186, 0.17364548798650503, 0.18881526216864586, 0.1787511669099331, 0.1747583549004048, 0.1787474669981748, 0.1783160602208227, 0.1847867809701711, 0.18169436999596655, 0.1774260241072625, 0.17389351595193148, 0.1750178940128535, 0.17774309893138707, 0.1804970179218799, 0.17549270391464233, 0.1748055969364941, 0.17734900419600308, 0.17844452010467649, 0.17728300602175295, 0.3119803830049932, 0.17812381009571254, 0.17937432089820504, 0.18112702714279294, 0.17895425390452147, 0.17681316589005291, 0.17572384607046843, 0.17795499507337809, 0.1743879651185125, 0.17465495015494525, 0.17822141596116126, 0.18027245299890637, 0.17496055201627314, 0.17664111708290875, 0.1931469300761819, 0.18953466904349625, 0.19320324598811567, 0.19237883388996124, 0.17968741292133927, 0.18448290904052556, 0.18219564692117274, 0.17324159410782158, 0.17243697890080512, 0.16978709702380002, 0.17206038301810622, 0.17303863889537752, 0.17138274502940476, 0.16904573095962405, 0.17295554699376225, 0.17612037691287696, 0.17046276503242552, 0.17464806814678013, 0.17159145209006965, 0.17162395012564957, 0.17891887202858925, 0.182102938182652, 0.17953361407853663, 0.17344714095816016, 0.1712570539675653, 0.16722672898322344, 0.1878643799573183, 0.17169494391418993, 0.1966580469161272, 0.18271613609977067, 0.17239931086078286, 0.17997802491299808, 0.17521587410010397, 0.16352314106188715, 0.16912350920028985, 0.1679177179466933, 0.16606586799025536, 0.1660921280272305, 0.16406352305784822, 0.16815085289999843, 0.16772932885214686, 0.16500518401153386, 0.16639098315499723, 0.16578568681143224, 0.17345223505981266, 0.17409649305045605, 0.175337536027655, 0.17294555390253663, 0.17323229811154306, 0.1760041390080005, 0.17458509490825236, 0.17204639804549515, 0.1632398939691484, 0.16676894016563892, 0.16488444595597684, 0.16391099896281958, 0.16373340995050967, 0.17317758104763925, 0.17746779904700816, 0.2638829939533025, 0.17472180305048823, 0.177908357931301, 0.17890512105077505, 0.1820363961160183, 0.1675953329540789, 0.16721092304214835, 0.16913590393960476, 0.16652821702882648, 0.16668572695925832, 0.16951815714128315, 0.16594127588905394, 0.16648536594584584, 0.167470321059227, 0.17120367009192705, 0.1702222169842571, 0.16727365693077445, 0.17012911313213408, 0.1662484100088477, 0.16628397605381906, 0.16703754104673862, 0.17140628304332495, 0.16996696684509516, 0.1661873620469123, 0.16851380793377757, 0.16796589503064752, 0.1792783960700035, 0.16650839592330158, 0.16885564103722572, 0.17010211595334113, 0.16867252998054028, 0.1664904069621116, 0.16955772717483342, 0.17014159820973873, 0.18064116686582565, 0.1785741311032325, 0.18142247200012207, 0.18124354095198214, 0.18002348812296987]
[0.001443263007934357, 0.0015238609071099019, 0.0014554803032254756, 0.0013341446887239698, 0.001574499007357761, 0.0014451223248688055, 0.0014017092554952748, 0.0013798163489027078, 0.0016448778829486795, 0.0014361169698193323, 0.0013960650003002595, 0.001410615278494566, 0.0012501308698091628, 0.0013491250538491002, 0.0013268677203721086, 0.001418428712112959, 0.001471272131875735, 0.0021173257756140805, 0.001439046334879574, 0.0014635461324811444, 0.001395171937756529, 0.0013790842571635117, 0.001413064063057419, 0.0013246295663217704, 0.001321456821043362, 0.0012888230686617453, 0.001361164038769843, 0.001324880465706305, 0.0012976860071626283, 0.001329069962551774, 0.0013473131242677453, 0.0013857339310293744, 0.0013599494948636653, 0.0013400623574852943, 0.001367789364687001, 0.0014107119068214598, 0.0013132684109660314, 0.0013423244497745997, 0.0013668615038889323, 0.0013565812097472508, 0.0013036930305493433, 0.001339628263701533, 0.0013557780469053012, 0.0013045817433112814, 0.001349067162043711, 0.0013753914187640646, 0.001331822186809524, 0.0013486140068302782, 0.0013465729845297892, 0.0013363920856093944, 0.001324442673705576, 0.0013388450466847235, 0.001368068348139871, 0.00132465030990772, 0.0013414370238983124, 0.0013341339857339165, 0.0013808010943782653, 0.0013467857754940903, 0.0013595274812000435, 0.0013479801768422589, 0.001391822208700947, 0.001355773534683525, 0.0013295371616042632, 0.0013790507836974868, 0.0013451426806173814, 0.0013636419307729302, 0.0013941893798060888, 0.0013246311853069436, 0.0013501379231005445, 0.0013483112945055315, 0.0013316357454155074, 0.0013454196046922327, 0.001368192356628503, 0.00137131365174933, 0.001355783124057244, 0.001337517116059975, 0.0013682498062361581, 0.001380795890884113, 0.001363862217440046, 0.0013570950615631287, 0.001377146279569282, 0.0014201546586010345, 0.001390939542316189, 0.0013790473020671642, 0.001421406054724903, 0.0013615700466803801, 0.0013805647044940743, 0.0013643026355538489, 0.0013389935727815988, 0.0013580425415014804, 0.0014054256585217262, 0.0013743763565728369, 0.0013427023501532485, 0.0013505681544224652, 0.0013708847434108341, 0.0013979225123459979, 0.001389121131379475, 0.0013947749921922074, 0.0013470012792061235, 0.0013832292472582687, 0.0013550496967004021, 0.0013460890541589538, 0.0014636842028577198, 0.001385667960542117, 0.0013547159294605023, 0.0013856392790556186, 0.0013822950404714937, 0.0014324556664354348, 0.001408483488340826, 0.001375395535715213, 0.0013480117515653604, 0.0013567278605647558, 0.001377853480088272, 0.0013992016893168984, 0.0013604085574778475, 0.001355082146794528, 0.0013747984821395587, 0.001383290853524624, 0.001374286868385682, 0.0024184525814340556, 0.0013808047294241283, 0.0013904986116139925, 0.001404085481727077, 0.0013872422783296238, 0.0013706446968221157, 0.0013622003571354142, 0.0013794960858401401, 0.0013518446908411823, 0.0013539143422863973, 0.0013815613640400098, 0.0013974608759605145, 0.0013562833489633578, 0.0013693109851388276, 0.0014972630238463713, 0.0014692610003371803, 0.0014976995813032223, 0.0014913087898446608, 0.0013929256815607696, 0.0014301000700815934, 0.0014123693559780833, 0.0013534499539673561, 0.00134716389766254, 0.0013264616954984376, 0.0013442217423289549, 0.0013518643663701368, 0.0013389276955422247, 0.001320669773122063, 0.0013512152108887676, 0.0013759404446318513, 0.0013317403518158244, 0.0013644380323967198, 0.0013405582194536692, 0.0013408121103566373, 0.0013978036877233535, 0.0014226792045519687, 0.0014026063599885674, 0.0013550557887356263, 0.0013379457341216039, 0.0013064588201814331, 0.0014676904684165493, 0.0013413667493296089, 0.0015363909915322438, 0.0014274698132794583, 0.001346869616099866, 0.0014060783196327975, 0.0013688740164070623, 0.0012775245395459933, 0.0013212774156272644, 0.0013118571714585414, 0.00129738959367387, 0.0012975947502127383, 0.0012817462738894392, 0.0013136785382812377, 0.0013103853816573974, 0.0012891030000901083, 0.0012999295558984159, 0.0012952006782143144, 0.0013550955864047864, 0.0013601288519566879, 0.0013698245002160547, 0.0013511371398635674, 0.0013533773289964302, 0.0013750323360000039, 0.0013639460539707216, 0.0013441124847304309, 0.0012753116716339719, 0.001302882345044054, 0.001288159734031069, 0.001280554679397028, 0.0012791672652383568, 0.0013529498519346816, 0.0013864671800547512, 0.002061585890260176, 0.0013650140863319393, 0.001389909046338289, 0.00139769625820918, 0.001422159344656393, 0.0013093385387037415, 0.001306335336266784, 0.0013213742495281622, 0.0013010016955377068, 0.0013022322418692056, 0.0013243606026662746, 0.001296416217883234, 0.0013006669214519206, 0.0013083618832752109, 0.00133752867259318, 0.0013298610701895086, 0.0013068254447716754, 0.0013291336963447975, 0.0012988157031941228, 0.0012990935629204614, 0.0013049807894276455, 0.0013391115862759762, 0.001327866928477306, 0.0012983387659915024, 0.0013165141244826373, 0.0013122335549269337, 0.0014006124692969024, 0.0013008468431507936, 0.001319184695603326, 0.0013289227808854776, 0.001317754140472971, 0.0013007063043914968, 0.001324669743553386, 0.0013292312360135838, 0.001411259116139263, 0.001395110399244004, 0.0014173630625009537, 0.0014159651636873605, 0.001406433500960702]
[692.8744064681816, 656.2278718052837, 687.058421734674, 749.5438901431603, 635.1226614478124, 691.982943444448, 713.4147085635557, 724.7341291435234, 607.9478667482333, 696.3221109529838, 716.2990260374154, 708.9105124873013, 799.9162520901942, 741.2211322790023, 753.654629354883, 705.0054694044879, 679.6839132167162, 472.293877265993, 694.9046571760906, 683.2719364333966, 716.7575357113509, 725.1188568106717, 707.6819983917217, 754.9280383170056, 756.74057909092, 775.9016922612614, 734.66530962995, 754.7850737363627, 770.6024373234059, 752.4058388017665, 742.2179610574881, 721.639253833644, 735.3214246388244, 746.2339303945218, 731.1067228752986, 708.8619548502616, 761.4589612068768, 744.9763730131846, 731.6030169514946, 737.147170265106, 767.0517342404027, 746.4757403944997, 737.5838562090601, 766.5291999731701, 741.2529399093023, 727.0657547788152, 750.8509843912211, 741.5020123885225, 742.6259188982548, 748.2833898585984, 755.0345665034803, 746.9124246127072, 730.9576318753924, 754.91621639349, 745.4692111403995, 749.5499033029226, 724.2172707360658, 742.5085846582643, 735.5496772432348, 741.8506719754383, 718.4825718030085, 737.5863110009944, 752.1414435632377, 725.1364574978287, 743.4155606013699, 733.3303394632247, 717.2626721192535, 754.927115632024, 740.6650705014899, 741.6684886309818, 750.9561105150209, 743.2625453891404, 730.8913802619086, 729.2277727450172, 737.5810940967118, 747.6539836333275, 730.8606918431394, 724.2199999304081, 733.2118942901642, 736.8680561317351, 726.1392742626884, 704.1486601079629, 718.9385085241034, 725.1382882233409, 703.5287324659234, 734.4462390591525, 724.3412762507664, 732.9752020848678, 746.8295743366563, 736.3539575824914, 711.5282077970837, 727.6027379382554, 744.766701187248, 740.4291273457603, 729.4559260408332, 715.3472321736896, 719.879625621233, 716.9615211040397, 742.38979237597, 722.9459628489809, 737.9803135154662, 742.8928991810333, 683.2074829034736, 721.6736104721426, 738.1621329264481, 721.6885484666321, 723.434556821462, 698.1018843594858, 709.9834739120628, 727.0635784636269, 741.8332954729539, 737.067490884824, 725.7665742049259, 714.6932480393217, 735.0732943447275, 737.9626411325089, 727.3793308556242, 722.9137657145645, 727.6501165834894, 413.4875364837775, 724.2153641934983, 719.166485782586, 712.207349918584, 720.8546161122615, 729.5836786284094, 734.1063998125143, 724.9023830255961, 739.7299458843547, 738.5991630100239, 723.8187358365069, 715.5835395482336, 737.3090591758173, 730.2942946146123, 667.8853241370144, 680.6142678329516, 667.6906453628375, 670.5519385453116, 717.9133913874734, 699.2517663067772, 708.0300884236387, 738.8525871006228, 742.3001772353736, 753.8853201669236, 743.9248812234152, 739.7191795838804, 746.8663194654665, 757.1915556422559, 740.0745580285806, 726.7756419992157, 750.8971239299783, 732.9024669910718, 745.9579043180532, 745.816652665834, 715.4080424760728, 702.8991474679781, 712.9584098051223, 737.9769957169651, 747.4144686865989, 765.4278761431807, 681.3425729192562, 745.5082664750577, 650.875985026897, 700.5402080640904, 742.462364616779, 711.1979368696572, 730.5274174351994, 782.7638288305444, 756.843330683329, 762.2781059985256, 770.7784962019466, 770.6566320771965, 780.1856111237332, 761.2212355302373, 763.134276372332, 775.733203576518, 769.2724543899407, 772.0811275197093, 737.9553221430725, 735.2244594777878, 730.0205244119051, 740.1173208079945, 738.8922354281862, 727.2556243360935, 733.1668265682493, 743.9853519406565, 784.1220481568765, 767.52901273383, 776.3012408955495, 780.9115972079129, 781.758591839561, 739.1256952872474, 721.2576066607724, 485.0634672678121, 732.5931724903999, 719.4715385402353, 715.4630300586663, 703.1560870850301, 763.7444178417066, 765.5002297172621, 756.7878671444378, 768.6385063369943, 767.9121802149624, 755.0813562308829, 771.3572124489331, 768.8363434996184, 764.3145316162137, 747.6475237433343, 751.9582476818405, 765.2131384499611, 752.3697599045632, 769.9321755509593, 769.7674967705357, 766.2948053347147, 746.7637575901841, 753.0875109200301, 770.215005662509, 759.5816720864876, 762.0594643730786, 713.9733665958242, 768.7300048158555, 758.043967105495, 752.4891697120939, 758.8669003468872, 768.8130645817275, 754.9051413505758, 752.3145506262995, 708.5870968441774, 716.7891519853123, 705.535530349922, 706.2320639272423, 711.0183306334236]
Elapsed: 0.17671919703177172~0.014224945412331734
Time per graph: 0.001374037649484204~0.00010944951855985858
Speed: 731.0040571938948~41.44924038181226
Total Time: 0.1809
best val loss: 0.16667365652415178 test_score: 0.9062

Testing...
Test loss: 0.2945 score: 0.9062 time: 0.18s
test Score 0.9062
Epoch Time List: [0.8017544918693602, 0.6555673787370324, 0.657769767800346, 0.5959120411425829, 0.6552093029022217, 0.6451585362665355, 0.624674468068406, 0.6164452037774026, 0.6983574617188424, 0.624930018093437, 0.7200210921000689, 0.6297661752905697, 0.634046294959262, 0.5923942590598017, 0.592369272839278, 0.7132060450967401, 0.6411763329524547, 0.7295834210235626, 0.6399023733101785, 0.6388592899311334, 0.6258268079254776, 0.6251674320083112, 0.6318882459308952, 0.5988149878103286, 0.5941635898780078, 0.5929048070684075, 0.609873304143548, 0.5993357268162072, 0.5975260229315609, 0.6072246138937771, 0.599108018912375, 0.6085274750366807, 0.6101081757806242, 0.5993651119060814, 0.6089603640139103, 0.6105151379015297, 0.6032386650331318, 0.607920465990901, 0.6048359950073063, 0.6076062412466854, 0.6002956479787827, 0.610068985959515, 0.600623567122966, 0.6052840750198811, 0.616429687011987, 0.6115204519592226, 0.6049161050468683, 0.604613175150007, 0.6046114980708808, 0.6025941281113774, 0.603105021873489, 0.6043417220935225, 0.6159117717761546, 0.5999778304249048, 0.6020580739714205, 0.6029467820189893, 0.6139386943541467, 0.6160465432330966, 0.6152490789536387, 0.6150313238613307, 0.6127949878573418, 0.6133281011134386, 0.5990058239549398, 0.611004987033084, 0.6052573712076992, 0.6076052419375628, 0.6163857378996909, 0.6093320427462459, 0.6141979810781777, 0.5957623259164393, 0.5949045452289283, 0.5941195969935507, 0.5956432134844363, 0.5974706911947578, 0.5936045781709254, 0.5971223551314324, 0.5992109740618616, 0.6023502282332629, 0.6001167721115053, 0.5982098018284887, 0.6034212016966194, 0.6103187259286642, 0.6106116818264127, 0.6033515890594572, 0.6128208809532225, 0.6031666791532189, 0.6091248888988048, 0.5983420959673822, 0.5988281250465661, 0.5998559058643878, 0.6043687278870493, 0.605030510108918, 0.5975488421972841, 0.5979090880136937, 0.6022782272193581, 0.6047424972057343, 0.6109073900151998, 0.6117822369560599, 0.609216048149392, 0.7005714189726859, 0.5986325140111148, 0.6058120150119066, 0.7210351249668747, 0.619359286967665, 0.695295395096764, 0.6012419308535755, 0.6022339090704918, 0.6736878149677068, 0.6276397188194096, 0.6132973551284522, 0.6018253150396049, 0.6045965361408889, 0.5994148487225175, 0.6072070631198585, 0.6057411511428654, 0.60275909001939, 0.6039693509228528, 0.6048457317519933, 0.601720335194841, 0.740015584975481, 0.6135080889798701, 0.6147942498791963, 0.6100111929699779, 0.6086681326851249, 0.6123108169995248, 0.6010182648897171, 0.6052010955754668, 0.6016366970725358, 0.5998817370273173, 0.6041441429406404, 0.6051554237492383, 0.6027472619898617, 0.6010181419551373, 0.620797973126173, 0.6417993390932679, 0.6421453419607133, 0.641262955032289, 0.6347319921478629, 0.6197640390601009, 0.6133946699555963, 0.6126046259887516, 0.6119444689247757, 0.610761261312291, 0.6091310738120228, 0.6111057591624558, 0.6084210518747568, 0.6057995762676001, 0.6172023159451783, 0.6222627139650285, 0.6164583151694387, 0.6300398567691445, 0.6106599750928581, 0.6099323357921094, 0.6207859232090414, 0.6338492929935455, 0.6243256272282451, 0.6184993339702487, 0.6166804912500083, 0.6325088937301189, 0.6647509858012199, 0.6831946528982371, 0.7023794970009476, 0.632831504335627, 0.6155492467805743, 0.6357128990348428, 0.6246891571208835, 0.5924053518101573, 0.5975536922924221, 0.5951124171260744, 0.591293822042644, 0.5909937953110784, 0.5903136858250946, 0.5918633702676743, 0.5914200460538268, 0.5903593541588634, 0.589856845093891, 0.582330155884847, 0.6100344229489565, 0.6120556690730155, 0.6126526251900941, 0.6099068729672581, 0.6145669468678534, 0.6140704660210758, 0.6113395518623292, 0.612709985813126, 0.5842944919131696, 0.584860113915056, 0.6705696587450802, 0.5850041469093412, 0.5862481789663434, 0.6966458191163838, 0.6247837680857629, 0.709501267876476, 0.6193096751812845, 0.6292452309280634, 0.6985409569460899, 0.6321244679857045, 0.6186576441396028, 0.5929048259276897, 0.5963682001456618, 0.5966314629185945, 0.5986802619881928, 0.6016673338599503, 0.5929719898849726, 0.5973277969751507, 0.5961603419855237, 0.6026425091549754, 0.6019988318439573, 0.601494655944407, 0.6020109138917178, 0.5954852860886604, 0.5965806499589235, 0.5960004518274218, 0.6001473197247833, 0.6004529087804258, 0.5983947201166302, 0.6010009499732405, 0.6005694454070181, 0.6451352699659765, 0.5940122201573104, 0.6000321770552546, 0.5991787137463689, 0.6012875998858362, 0.5977261499501765, 0.601817135931924, 0.5998200429603457, 0.6342570951674134, 0.6349764731712639, 0.6326207891106606, 0.6338365920819342, 0.6378722910303622]
Total Epoch List: [69, 71, 91]
Total Time List: [0.17463409900665283, 0.18285132595337927, 0.18085452704690397]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddd1d55ed0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.19s
Epoch 3/1000, LR 0.000045
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.20s
Epoch 4/1000, LR 0.000075
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6524;  Loss pred: 0.6524; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.26s
Val loss: 0.6918 score: 0.5426 time: 0.17s
Test loss: 0.6916 score: 0.5969 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.26s
Val loss: 0.6912 score: 0.7597 time: 0.17s
Test loss: 0.6910 score: 0.7442 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 0.26s
Val loss: 0.6904 score: 0.8915 time: 0.17s
Test loss: 0.6902 score: 0.8992 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.26s
Val loss: 0.6894 score: 0.8915 time: 0.17s
Test loss: 0.6891 score: 0.8837 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5335;  Loss pred: 0.5335; Loss self: 0.0000; time: 0.26s
Val loss: 0.6882 score: 0.8837 time: 0.17s
Test loss: 0.6878 score: 0.8837 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.26s
Val loss: 0.6867 score: 0.8837 time: 0.17s
Test loss: 0.6861 score: 0.8837 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.26s
Val loss: 0.6846 score: 0.8837 time: 0.17s
Test loss: 0.6838 score: 0.8915 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4540;  Loss pred: 0.4540; Loss self: 0.0000; time: 0.26s
Val loss: 0.6818 score: 0.8837 time: 0.17s
Test loss: 0.6807 score: 0.8915 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4341;  Loss pred: 0.4341; Loss self: 0.0000; time: 0.26s
Val loss: 0.6783 score: 0.8837 time: 0.17s
Test loss: 0.6768 score: 0.9070 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.4056;  Loss pred: 0.4056; Loss self: 0.0000; time: 0.26s
Val loss: 0.6738 score: 0.8837 time: 0.17s
Test loss: 0.6720 score: 0.9070 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3734;  Loss pred: 0.3734; Loss self: 0.0000; time: 0.27s
Val loss: 0.6683 score: 0.8682 time: 0.18s
Test loss: 0.6661 score: 0.8992 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3437;  Loss pred: 0.3437; Loss self: 0.0000; time: 0.27s
Val loss: 0.6618 score: 0.8682 time: 0.18s
Test loss: 0.6592 score: 0.8915 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3172;  Loss pred: 0.3172; Loss self: 0.0000; time: 0.25s
Val loss: 0.6541 score: 0.8760 time: 0.17s
Test loss: 0.6511 score: 0.9070 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2881;  Loss pred: 0.2881; Loss self: 0.0000; time: 0.26s
Val loss: 0.6446 score: 0.8760 time: 0.17s
Test loss: 0.6412 score: 0.8992 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2593;  Loss pred: 0.2593; Loss self: 0.0000; time: 0.26s
Val loss: 0.6333 score: 0.8760 time: 0.17s
Test loss: 0.6295 score: 0.8992 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.26s
Val loss: 0.6204 score: 0.9070 time: 0.18s
Test loss: 0.6161 score: 0.9147 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.27s
Val loss: 0.6054 score: 0.9147 time: 0.18s
Test loss: 0.6005 score: 0.9302 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.27s
Val loss: 0.5879 score: 0.9225 time: 0.18s
Test loss: 0.5821 score: 0.9302 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 0.26s
Val loss: 0.5687 score: 0.9225 time: 0.18s
Test loss: 0.5614 score: 0.9302 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1499;  Loss pred: 0.1499; Loss self: 0.0000; time: 0.27s
Val loss: 0.5464 score: 0.9225 time: 0.18s
Test loss: 0.5371 score: 0.9302 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.27s
Val loss: 0.5225 score: 0.9380 time: 0.18s
Test loss: 0.5108 score: 0.9380 time: 0.19s
Epoch 35/1000, LR 0.000285
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.27s
Val loss: 0.4977 score: 0.9302 time: 0.18s
Test loss: 0.4828 score: 0.9225 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.0861;  Loss pred: 0.0861; Loss self: 0.0000; time: 0.27s
Val loss: 0.4704 score: 0.9225 time: 0.18s
Test loss: 0.4529 score: 0.9225 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.27s
Val loss: 0.4430 score: 0.9380 time: 0.18s
Test loss: 0.4235 score: 0.9225 time: 0.19s
Epoch 38/1000, LR 0.000284
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.26s
Val loss: 0.4145 score: 0.9457 time: 0.17s
Test loss: 0.3930 score: 0.9225 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.26s
Val loss: 0.3867 score: 0.9302 time: 0.17s
Test loss: 0.3628 score: 0.9225 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0418;  Loss pred: 0.0418; Loss self: 0.0000; time: 0.26s
Val loss: 0.3593 score: 0.9380 time: 0.17s
Test loss: 0.3329 score: 0.9225 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.26s
Val loss: 0.3326 score: 0.9380 time: 0.17s
Test loss: 0.3039 score: 0.9225 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.26s
Val loss: 0.3090 score: 0.9380 time: 0.17s
Test loss: 0.2790 score: 0.9225 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.26s
Val loss: 0.2879 score: 0.9380 time: 0.17s
Test loss: 0.2554 score: 0.9225 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.26s
Val loss: 0.2682 score: 0.9380 time: 0.17s
Test loss: 0.2336 score: 0.9225 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.27s
Val loss: 0.2544 score: 0.9380 time: 0.17s
Test loss: 0.2164 score: 0.9225 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.28s
Val loss: 0.2428 score: 0.9380 time: 0.19s
Test loss: 0.2010 score: 0.9225 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.26s
Val loss: 0.2354 score: 0.9380 time: 0.18s
Test loss: 0.1933 score: 0.9225 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.27s
Val loss: 0.2308 score: 0.9457 time: 0.17s
Test loss: 0.1882 score: 0.9225 time: 0.28s
Epoch 49/1000, LR 0.000284
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.26s
Val loss: 0.2279 score: 0.9457 time: 0.18s
Test loss: 0.1863 score: 0.9225 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.27s
Val loss: 0.2266 score: 0.9457 time: 0.18s
Test loss: 0.1849 score: 0.9225 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.36s
Val loss: 0.2260 score: 0.9457 time: 0.18s
Test loss: 0.1834 score: 0.9225 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.27s
Val loss: 0.2267 score: 0.9457 time: 0.18s
Test loss: 0.1831 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.27s
Val loss: 0.2299 score: 0.9380 time: 0.18s
Test loss: 0.1865 score: 0.9225 time: 0.27s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.27s
Val loss: 0.2344 score: 0.9380 time: 0.18s
Test loss: 0.1880 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.27s
Val loss: 0.2406 score: 0.9380 time: 0.18s
Test loss: 0.1906 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.26s
Val loss: 0.2468 score: 0.9380 time: 0.18s
Test loss: 0.1940 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.26s
Val loss: 0.2534 score: 0.9380 time: 0.18s
Test loss: 0.1987 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.26s
Val loss: 0.2595 score: 0.9380 time: 0.18s
Test loss: 0.2025 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.26s
Val loss: 0.2652 score: 0.9380 time: 0.18s
Test loss: 0.2063 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.27s
Val loss: 0.2680 score: 0.9380 time: 0.18s
Test loss: 0.2125 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.26s
Val loss: 0.2725 score: 0.9380 time: 0.18s
Test loss: 0.2170 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.26s
Val loss: 0.2769 score: 0.9380 time: 0.18s
Test loss: 0.2198 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.27s
Val loss: 0.2807 score: 0.9535 time: 0.18s
Test loss: 0.2215 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.27s
Val loss: 0.2835 score: 0.9535 time: 0.18s
Test loss: 0.2221 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.26s
Val loss: 0.2857 score: 0.9457 time: 0.18s
Test loss: 0.2217 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.27s
Val loss: 0.2889 score: 0.9457 time: 0.18s
Test loss: 0.2195 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.26s
Val loss: 0.2919 score: 0.9457 time: 0.18s
Test loss: 0.2191 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.26s
Val loss: 0.2963 score: 0.9380 time: 0.18s
Test loss: 0.2196 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.26s
Val loss: 0.2977 score: 0.9380 time: 0.18s
Test loss: 0.2309 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.27s
Val loss: 0.3003 score: 0.9225 time: 0.18s
Test loss: 0.2399 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.27s
Val loss: 0.3024 score: 0.9225 time: 0.18s
Test loss: 0.2460 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0134,   Val_Loss: 0.2260,   Val_Precision: 0.9524,   Val_Recall: 0.9375,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.2260,   Test_Precision: 0.9508,   Test_Recall: 0.8923,   Test_accuracy: 0.9206,   Test_Score: 0.9225,   Test_loss: 0.1834


[0.18006708496250212, 0.1972219990566373, 0.2021802831441164, 0.17644813493825495, 0.17424821900203824, 0.17892567301169038, 0.17876605596393347, 0.18011537287384272, 0.1807163490448147, 0.17744221701286733, 0.1788838761858642, 0.17390496795997024, 0.17537656496278942, 0.17699436098337173, 0.1768266330473125, 0.1757854458410293, 0.17745078797452152, 0.17877721600234509, 0.17584418505430222, 0.17815070203505456, 0.1777893991675228, 0.17620705906301737, 0.1769334301352501, 0.1893428028561175, 0.1892134938389063, 0.1764975201804191, 0.17670195712707937, 0.17967911111190915, 0.18737640301696956, 0.18567933212034404, 0.1882117420900613, 0.1858699219301343, 0.1863530189730227, 0.1908104019239545, 0.1874292311258614, 0.18709116685204208, 0.19223792408593, 0.17707601306028664, 0.17698918608948588, 0.17571120406500995, 0.1772332100663334, 0.18158254586160183, 0.18088357988744974, 0.17740772408433259, 0.17735032783821225, 0.1813961360603571, 0.18034660816192627, 0.2796741190832108, 0.18803717102855444, 0.18409862997941673, 0.18755954084917903, 0.18957949616014957, 0.27246426208876073, 0.18829286098480225, 0.18764129979535937, 0.1837691911496222, 0.1865398921072483, 0.18879051390103996, 0.1868262190837413, 0.18742640409618616, 0.18756970996037126, 0.18470647605136037, 0.18465404794551432, 0.18740891804918647, 0.18892310792580247, 0.18423046986572444, 0.18735331296920776, 0.18713833997026086, 0.1850194949656725, 0.18486366886645555, 0.18742635287344456]
[0.001395868875678311, 0.0015288527058654052, 0.0015672890166210573, 0.0013678149995213563, 0.0013507613876126996, 0.0013870207210208557, 0.0013857833795653757, 0.0013962432005724243, 0.001400901930579959, 0.0013755210621152507, 0.0013866967146191024, 0.0013481005268214748, 0.0013595082555254994, 0.0013720493099486182, 0.0013707490933900193, 0.0013626778747366612, 0.0013755875036784613, 0.001385869891491047, 0.001363133217475211, 0.0013810131940701904, 0.0013782123966474635, 0.0013659461942869564, 0.0013715769777926364, 0.0014677736655512984, 0.001466771270069041, 0.001368197830855962, 0.0013697826133882121, 0.001392861326448908, 0.0014525302559455005, 0.0014393746675995661, 0.0014590057526361342, 0.0014408521079855372, 0.0014445970463025015, 0.001479150402511275, 0.0014529397761694682, 0.0014503191228840472, 0.001490216465782403, 0.0013726822717851678, 0.0013720091944921385, 0.0013621023570931003, 0.0013739008532273908, 0.0014076166345860608, 0.0014021982937011607, 0.0013752536750723456, 0.0013748087429318778, 0.0014061715973671092, 0.001398035722185475, 0.002168016427001634, 0.0014576524885934453, 0.0014271211626311375, 0.0014539499290634034, 0.0014696084973655006, 0.0021121260627035717, 0.0014596345812775368, 0.001454583719343871, 0.0014245673732528853, 0.001446045675249987, 0.0014634923558220151, 0.001448265264215049, 0.0014529178612107455, 0.001454028759382723, 0.001431833147684964, 0.001431426728259801, 0.0014527823104588098, 0.001464520216479089, 0.0014281431772536779, 0.001452351263327192, 0.0014506848059710144, 0.001434259650896686, 0.0014330516966391904, 0.0014529174641352291]
[716.3996686394044, 654.0852471683668, 638.0444126099444, 731.0930208763123, 740.3232052460235, 720.9697626319482, 721.6135037740391, 716.2076059457446, 713.8258418888825, 726.9972285718538, 721.1382196680835, 741.7844441896189, 735.5600791210081, 728.8367792243918, 729.5281133667472, 733.8491499271249, 726.9621142427493, 721.5684575729598, 733.6040140318753, 724.1060435148707, 725.5775687640928, 732.0932582721638, 729.0877699109253, 681.3039526938223, 681.7695576713402, 730.8884559292038, 730.0428478402569, 717.9465615213068, 688.4538176790448, 694.746143940196, 685.3982571303768, 694.0337557600586, 692.2345594984679, 676.0637716774561, 688.2597726358631, 689.50342322691, 671.0434510431836, 728.5007030064605, 728.8580893003118, 734.1592170313303, 727.8545592652694, 710.4207036414223, 713.1658942191823, 727.1385767774042, 727.3739021090515, 711.1507598876142, 715.2893049376114, 461.2511176324432, 686.0345712200204, 700.7113524659196, 687.7815941324568, 680.4533328384083, 473.4565884386542, 685.1029790790211, 687.4819143796527, 701.9675016960274, 691.5410883042292, 683.2970435560078, 690.4812431181203, 688.2701539415863, 687.7443059823169, 698.4053984340519, 698.6036939632316, 688.3343724664334, 682.8174775245777, 700.2099060704837, 688.538665025911, 689.3296158366055, 697.2238251106131, 697.8115320928141, 688.2703420425854]
Elapsed: 0.1855425363321277~0.016451682410943066
Time per graph: 0.0014383142351327735~0.0001275324217902563
Speed: 699.2109177878091~45.0803999062265
Total Time: 0.1879
best val loss: 0.22595298885144005 test_score: 0.9225

Testing...
Test loss: 0.2215 score: 0.9225 time: 0.18s
test Score 0.9225
Epoch Time List: [0.6136276395991445, 0.6692980478983372, 0.6531465058214962, 0.611377993831411, 0.6030081149656326, 0.6031049836892635, 0.6112898623105139, 0.6042674500495195, 0.6152863048482686, 0.60693065286614, 0.6034295388963073, 0.5951653399970382, 0.5977543098852038, 0.6012596080545336, 0.6008684108965099, 0.6031121080741286, 0.6022111249621958, 0.6050134201068431, 0.606601171195507, 0.60755554609932, 0.6013346230611205, 0.6028070540633053, 0.602004696149379, 0.6346944577526301, 0.633837427245453, 0.6002698224037886, 0.6023258771747351, 0.6047240810003132, 0.6269756378605962, 0.6294747751671821, 0.6310365928802639, 0.6282272259704769, 0.6299553399439901, 0.6356388076674193, 0.6332864339929074, 0.6349816401489079, 0.6404945340473205, 0.6079205339774489, 0.6038604027125984, 0.6015166570432484, 0.6051413011737168, 0.6095512728206813, 0.6115652930457145, 0.6124972568359226, 0.6107501762453467, 0.6457080177497119, 0.6103830831125379, 0.7158503320533782, 0.6295808088034391, 0.6300149958115071, 0.7148100340273231, 0.6308916942216456, 0.7189135199878365, 0.6286989278160036, 0.6337480309884995, 0.626115390798077, 0.627493133302778, 0.6302218416240066, 0.6301170520018786, 0.6360564627684653, 0.6283840883988887, 0.6260100856889039, 0.6254133810289204, 0.6297254059463739, 0.6268937978893518, 0.6272714491933584, 0.6265493480022997, 0.6233120549004525, 0.6242760112509131, 0.6269282419234514, 0.6272533612791449]
Total Epoch List: [71]
Total Time List: [0.1879198548849672]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf319480>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5039 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5039 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5039 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.5039 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5591;  Loss pred: 0.5591; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.5039 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4953;  Loss pred: 0.4953; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.5039 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4716;  Loss pred: 0.4716; Loss self: 0.0000; time: 0.28s
Val loss: 0.6722 score: 0.5116 time: 0.18s
Test loss: 0.6671 score: 0.5349 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.4440;  Loss pred: 0.4440; Loss self: 0.0000; time: 0.28s
Val loss: 0.6658 score: 0.5581 time: 0.18s
Test loss: 0.6598 score: 0.5891 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.4228;  Loss pred: 0.4228; Loss self: 0.0000; time: 0.28s
Val loss: 0.6582 score: 0.6434 time: 0.18s
Test loss: 0.6509 score: 0.6589 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3938;  Loss pred: 0.3938; Loss self: 0.0000; time: 0.28s
Val loss: 0.6490 score: 0.7132 time: 0.18s
Test loss: 0.6404 score: 0.7519 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3640;  Loss pred: 0.3640; Loss self: 0.0000; time: 0.29s
Val loss: 0.6377 score: 0.7907 time: 0.18s
Test loss: 0.6275 score: 0.7984 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.3325;  Loss pred: 0.3325; Loss self: 0.0000; time: 0.29s
Val loss: 0.6236 score: 0.8140 time: 0.18s
Test loss: 0.6118 score: 0.8682 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.28s
Val loss: 0.6076 score: 0.8372 time: 0.18s
Test loss: 0.5944 score: 0.8837 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.28s
Val loss: 0.5897 score: 0.8527 time: 0.18s
Test loss: 0.5752 score: 0.8837 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2467;  Loss pred: 0.2467; Loss self: 0.0000; time: 0.29s
Val loss: 0.5692 score: 0.8605 time: 0.18s
Test loss: 0.5535 score: 0.8915 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.2212;  Loss pred: 0.2212; Loss self: 0.0000; time: 0.28s
Val loss: 0.5466 score: 0.8837 time: 0.18s
Test loss: 0.5296 score: 0.8915 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.28s
Val loss: 0.5231 score: 0.8915 time: 0.18s
Test loss: 0.5048 score: 0.9070 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1728;  Loss pred: 0.1728; Loss self: 0.0000; time: 0.28s
Val loss: 0.4978 score: 0.9147 time: 0.18s
Test loss: 0.4782 score: 0.9225 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1492;  Loss pred: 0.1492; Loss self: 0.0000; time: 0.28s
Val loss: 0.4713 score: 0.9147 time: 0.18s
Test loss: 0.4509 score: 0.9225 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1295;  Loss pred: 0.1295; Loss self: 0.0000; time: 0.28s
Val loss: 0.4445 score: 0.9147 time: 0.18s
Test loss: 0.4232 score: 0.9225 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1167;  Loss pred: 0.1167; Loss self: 0.0000; time: 0.28s
Val loss: 0.4165 score: 0.9225 time: 0.18s
Test loss: 0.3948 score: 0.9225 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 0.28s
Val loss: 0.3899 score: 0.9302 time: 0.18s
Test loss: 0.3678 score: 0.9225 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.28s
Val loss: 0.3620 score: 0.9147 time: 0.17s
Test loss: 0.3400 score: 0.9225 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.28s
Val loss: 0.3365 score: 0.9225 time: 0.17s
Test loss: 0.3144 score: 0.9225 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.28s
Val loss: 0.3142 score: 0.9147 time: 0.18s
Test loss: 0.2918 score: 0.9225 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0553;  Loss pred: 0.0553; Loss self: 0.0000; time: 0.28s
Val loss: 0.2933 score: 0.9147 time: 0.17s
Test loss: 0.2708 score: 0.9225 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.28s
Val loss: 0.2749 score: 0.9147 time: 0.17s
Test loss: 0.2523 score: 0.9302 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.28s
Val loss: 0.2578 score: 0.9225 time: 0.17s
Test loss: 0.2357 score: 0.9302 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0396;  Loss pred: 0.0396; Loss self: 0.0000; time: 0.29s
Val loss: 0.2453 score: 0.9147 time: 0.17s
Test loss: 0.2235 score: 0.9302 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.29s
Val loss: 0.2356 score: 0.9070 time: 0.20s
Test loss: 0.2140 score: 0.9302 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.32s
Val loss: 0.2300 score: 0.9070 time: 0.19s
Test loss: 0.2093 score: 0.9302 time: 0.19s
Epoch 47/1000, LR 0.000284
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.31s
Val loss: 0.2295 score: 0.9070 time: 0.19s
Test loss: 0.2089 score: 0.9302 time: 0.19s
Epoch 48/1000, LR 0.000284
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.30s
Val loss: 0.2311 score: 0.8992 time: 0.17s
Test loss: 0.2109 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.29s
Val loss: 0.2358 score: 0.8837 time: 0.18s
Test loss: 0.2150 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.30s
Val loss: 0.2420 score: 0.8837 time: 0.20s
Test loss: 0.2203 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.28s
Val loss: 0.2488 score: 0.8837 time: 0.18s
Test loss: 0.2256 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.28s
Val loss: 0.2551 score: 0.8760 time: 0.17s
Test loss: 0.2305 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.30s
Val loss: 0.2625 score: 0.8760 time: 0.18s
Test loss: 0.2376 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.28s
Val loss: 0.2714 score: 0.8760 time: 0.18s
Test loss: 0.2461 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.29s
Val loss: 0.2761 score: 0.8760 time: 0.18s
Test loss: 0.2509 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.29s
Val loss: 0.2817 score: 0.8760 time: 0.19s
Test loss: 0.2588 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.29s
Val loss: 0.2918 score: 0.8760 time: 0.17s
Test loss: 0.2698 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.29s
Val loss: 0.3062 score: 0.8760 time: 0.17s
Test loss: 0.2837 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.27s
Val loss: 0.3182 score: 0.8760 time: 0.17s
Test loss: 0.2956 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.28s
Val loss: 0.3312 score: 0.8760 time: 0.18s
Test loss: 0.3078 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.34s
Val loss: 0.3443 score: 0.8760 time: 0.16s
Test loss: 0.3200 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.28s
Val loss: 0.3537 score: 0.8760 time: 0.17s
Test loss: 0.3291 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.28s
Val loss: 0.3632 score: 0.8760 time: 0.23s
Test loss: 0.3374 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.27s
Val loss: 0.3684 score: 0.8760 time: 0.17s
Test loss: 0.3427 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.28s
Val loss: 0.3757 score: 0.8760 time: 0.17s
Test loss: 0.3487 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.37s
Val loss: 0.3812 score: 0.8760 time: 0.16s
Test loss: 0.3519 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.27s
Val loss: 0.3835 score: 0.8760 time: 0.18s
Test loss: 0.3487 score: 0.9147 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0267,   Val_Loss: 0.2295,   Val_Precision: 0.9649,   Val_Recall: 0.8462,   Val_accuracy: 0.9016,   Val_Score: 0.9070,   Val_Loss: 0.2295,   Test_Precision: 1.0000,   Test_Recall: 0.8594,   Test_accuracy: 0.9244,   Test_Score: 0.9302,   Test_loss: 0.2089


[0.18006708496250212, 0.1972219990566373, 0.2021802831441164, 0.17644813493825495, 0.17424821900203824, 0.17892567301169038, 0.17876605596393347, 0.18011537287384272, 0.1807163490448147, 0.17744221701286733, 0.1788838761858642, 0.17390496795997024, 0.17537656496278942, 0.17699436098337173, 0.1768266330473125, 0.1757854458410293, 0.17745078797452152, 0.17877721600234509, 0.17584418505430222, 0.17815070203505456, 0.1777893991675228, 0.17620705906301737, 0.1769334301352501, 0.1893428028561175, 0.1892134938389063, 0.1764975201804191, 0.17670195712707937, 0.17967911111190915, 0.18737640301696956, 0.18567933212034404, 0.1882117420900613, 0.1858699219301343, 0.1863530189730227, 0.1908104019239545, 0.1874292311258614, 0.18709116685204208, 0.19223792408593, 0.17707601306028664, 0.17698918608948588, 0.17571120406500995, 0.1772332100663334, 0.18158254586160183, 0.18088357988744974, 0.17740772408433259, 0.17735032783821225, 0.1813961360603571, 0.18034660816192627, 0.2796741190832108, 0.18803717102855444, 0.18409862997941673, 0.18755954084917903, 0.18957949616014957, 0.27246426208876073, 0.18829286098480225, 0.18764129979535937, 0.1837691911496222, 0.1865398921072483, 0.18879051390103996, 0.1868262190837413, 0.18742640409618616, 0.18756970996037126, 0.18470647605136037, 0.18465404794551432, 0.18740891804918647, 0.18892310792580247, 0.18423046986572444, 0.18735331296920776, 0.18713833997026086, 0.1850194949656725, 0.18486366886645555, 0.18742635287344456, 0.17119526490569115, 0.1724476011004299, 0.17360520688816905, 0.1704855130519718, 0.17032999289222062, 0.17383340396918356, 0.17379562021233141, 0.17074222001247108, 0.17332330998033285, 0.17246024101041257, 0.1716027499642223, 0.1734238041099161, 0.175106979906559, 0.1696885460987687, 0.17381824110634625, 0.1756721599958837, 0.1684651670511812, 0.1701194979250431, 0.17186662298627198, 0.17982544493861496, 0.1812021469231695, 0.18098868196830153, 0.17797516798600554, 0.17971465596929193, 0.18084205989725888, 0.18061502603814006, 0.18164752703160048, 0.18341725994832814, 0.1812017410993576, 0.18454914703033864, 0.1821614250075072, 0.18315849895589054, 0.1847925540059805, 0.18138671293854713, 0.18446193891577423, 0.18225730187259614, 0.1822274650912732, 0.17505107005126774, 0.172115613007918, 0.17196715297177434, 0.1715962658636272, 0.17388255801051855, 0.1727279219776392, 0.17280229297466576, 0.18800213490612805, 0.19026095001026988, 0.19190111896023154, 0.1721467049792409, 0.17846558312885463, 0.20018563303165138, 0.17053417791612446, 0.16747846198268235, 0.17562669585458934, 0.17609374807216227, 0.17234785808250308, 0.18172014295123518, 0.17203264986164868, 0.17752224998548627, 0.16964059718884528, 0.1871710370760411, 0.16838323487900198, 0.16858136118389666, 0.1710430469829589, 0.17008521710522473, 0.16908373101614416, 0.1667263249401003, 0.20711441501043737]
[0.001395868875678311, 0.0015288527058654052, 0.0015672890166210573, 0.0013678149995213563, 0.0013507613876126996, 0.0013870207210208557, 0.0013857833795653757, 0.0013962432005724243, 0.001400901930579959, 0.0013755210621152507, 0.0013866967146191024, 0.0013481005268214748, 0.0013595082555254994, 0.0013720493099486182, 0.0013707490933900193, 0.0013626778747366612, 0.0013755875036784613, 0.001385869891491047, 0.001363133217475211, 0.0013810131940701904, 0.0013782123966474635, 0.0013659461942869564, 0.0013715769777926364, 0.0014677736655512984, 0.001466771270069041, 0.001368197830855962, 0.0013697826133882121, 0.001392861326448908, 0.0014525302559455005, 0.0014393746675995661, 0.0014590057526361342, 0.0014408521079855372, 0.0014445970463025015, 0.001479150402511275, 0.0014529397761694682, 0.0014503191228840472, 0.001490216465782403, 0.0013726822717851678, 0.0013720091944921385, 0.0013621023570931003, 0.0013739008532273908, 0.0014076166345860608, 0.0014021982937011607, 0.0013752536750723456, 0.0013748087429318778, 0.0014061715973671092, 0.001398035722185475, 0.002168016427001634, 0.0014576524885934453, 0.0014271211626311375, 0.0014539499290634034, 0.0014696084973655006, 0.0021121260627035717, 0.0014596345812775368, 0.001454583719343871, 0.0014245673732528853, 0.001446045675249987, 0.0014634923558220151, 0.001448265264215049, 0.0014529178612107455, 0.001454028759382723, 0.001431833147684964, 0.001431426728259801, 0.0014527823104588098, 0.001464520216479089, 0.0014281431772536779, 0.001452351263327192, 0.0014506848059710144, 0.001434259650896686, 0.0014330516966391904, 0.0014529174641352291, 0.0013270950767883035, 0.0013368031093056582, 0.0013457767975827057, 0.0013215931244338899, 0.0013203875418001598, 0.0013475457672029734, 0.0013472528698630342, 0.001323583100871869, 0.0013435915502351384, 0.0013369010931039735, 0.001330253875691646, 0.0013443705744954737, 0.001357418448888054, 0.0013154150860369667, 0.0013474282256305912, 0.0013617996898905714, 0.0013059315275285365, 0.0013187557978685512, 0.0013322994029943563, 0.0013939956971985657, 0.001404667805605965, 0.0014030130385139653, 0.001379652465007795, 0.0013931368679790071, 0.0014018764333120844, 0.0014001164809158144, 0.0014081203645860503, 0.0014218392244056445, 0.0014046646596849427, 0.0014306135428708423, 0.0014121040698256372, 0.0014198333252394616, 0.0014325004186510116, 0.001406098549911218, 0.001429937510974994, 0.0014128473013379547, 0.001412616008459482, 0.0013569850391571142, 0.0013342295582009147, 0.001333078705207553, 0.0013302036113459472, 0.0013479268062830895, 0.0013389761393615442, 0.0013395526587183391, 0.0014573808907451787, 0.0014748910853509293, 0.0014876055733351282, 0.0013344705812344255, 0.0013834541327818189, 0.0015518266126484602, 0.0013219703714428253, 0.0012982826510285453, 0.0013614472546867392, 0.0013650678145128857, 0.001336029907616303, 0.0014086832786917454, 0.0013335864330360362, 0.0013761414727557074, 0.001315043389060816, 0.0014509382719072954, 0.001305296394410868, 0.001306832257239509, 0.0013259150928911544, 0.0013184900550792615, 0.0013107265970243732, 0.0012924521313186072, 0.0016055381008561037]
[716.3996686394044, 654.0852471683668, 638.0444126099444, 731.0930208763123, 740.3232052460235, 720.9697626319482, 721.6135037740391, 716.2076059457446, 713.8258418888825, 726.9972285718538, 721.1382196680835, 741.7844441896189, 735.5600791210081, 728.8367792243918, 729.5281133667472, 733.8491499271249, 726.9621142427493, 721.5684575729598, 733.6040140318753, 724.1060435148707, 725.5775687640928, 732.0932582721638, 729.0877699109253, 681.3039526938223, 681.7695576713402, 730.8884559292038, 730.0428478402569, 717.9465615213068, 688.4538176790448, 694.746143940196, 685.3982571303768, 694.0337557600586, 692.2345594984679, 676.0637716774561, 688.2597726358631, 689.50342322691, 671.0434510431836, 728.5007030064605, 728.8580893003118, 734.1592170313303, 727.8545592652694, 710.4207036414223, 713.1658942191823, 727.1385767774042, 727.3739021090515, 711.1507598876142, 715.2893049376114, 461.2511176324432, 686.0345712200204, 700.7113524659196, 687.7815941324568, 680.4533328384083, 473.4565884386542, 685.1029790790211, 687.4819143796527, 701.9675016960274, 691.5410883042292, 683.2970435560078, 690.4812431181203, 688.2701539415863, 687.7443059823169, 698.4053984340519, 698.6036939632316, 688.3343724664334, 682.8174775245777, 700.2099060704837, 688.538665025911, 689.3296158366055, 697.2238251106131, 697.8115320928141, 688.2703420425854, 753.5255141026483, 748.0533169311708, 743.0652704045777, 756.6625321453259, 757.3534044683903, 742.089823097916, 742.2511559405051, 755.5249076097159, 744.2738083794831, 747.9984908069995, 751.7362048504199, 743.8425230151205, 736.692506882577, 760.2163078521188, 742.1545585717591, 734.3223878104686, 765.7369310108402, 758.2905050474526, 750.5820371550793, 717.362329029884, 711.9120948092109, 712.7517510879113, 724.8202176729703, 717.804562483999, 713.3296317974275, 714.226290191157, 710.1665632781139, 703.3143992901298, 711.9136892247959, 699.0007923406617, 708.1631031085955, 704.3080213878944, 698.080075216803, 711.1877044913676, 699.3312591108657, 707.7905723095541, 707.9064614951819, 736.9278003397487, 749.4962121423899, 750.1432556784451, 751.7646106735227, 741.8800452210768, 746.8392980302277, 746.5178718369928, 686.1624207853353, 678.0161667070245, 672.2211975571294, 749.3608432154194, 722.8284453415323, 644.4018886190688, 756.4466054625564, 770.2482962456324, 734.5124804192976, 732.5643380998199, 748.4862384436921, 709.8827785680165, 749.857658437185, 726.6694738859309, 760.4311829697001, 689.2091961193321, 766.1095244588794, 765.2091494224002, 754.1961060413776, 758.4433391421255, 762.9356131707494, 773.7230461137182, 622.8441414543703]
Elapsed: 0.1813059197705023~0.013643649294438048
Time per graph: 0.0014054722462829637~0.00010576472321269805
Speed: 714.5655513909148~41.48790589460885
Total Time: 0.2081
best val loss: 0.2295073917092279 test_score: 0.9302

Testing...
Test loss: 0.3678 score: 0.9225 time: 0.17s
test Score 0.9225
Epoch Time List: [0.6136276395991445, 0.6692980478983372, 0.6531465058214962, 0.611377993831411, 0.6030081149656326, 0.6031049836892635, 0.6112898623105139, 0.6042674500495195, 0.6152863048482686, 0.60693065286614, 0.6034295388963073, 0.5951653399970382, 0.5977543098852038, 0.6012596080545336, 0.6008684108965099, 0.6031121080741286, 0.6022111249621958, 0.6050134201068431, 0.606601171195507, 0.60755554609932, 0.6013346230611205, 0.6028070540633053, 0.602004696149379, 0.6346944577526301, 0.633837427245453, 0.6002698224037886, 0.6023258771747351, 0.6047240810003132, 0.6269756378605962, 0.6294747751671821, 0.6310365928802639, 0.6282272259704769, 0.6299553399439901, 0.6356388076674193, 0.6332864339929074, 0.6349816401489079, 0.6404945340473205, 0.6079205339774489, 0.6038604027125984, 0.6015166570432484, 0.6051413011737168, 0.6095512728206813, 0.6115652930457145, 0.6124972568359226, 0.6107501762453467, 0.6457080177497119, 0.6103830831125379, 0.7158503320533782, 0.6295808088034391, 0.6300149958115071, 0.7148100340273231, 0.6308916942216456, 0.7189135199878365, 0.6286989278160036, 0.6337480309884995, 0.626115390798077, 0.627493133302778, 0.6302218416240066, 0.6301170520018786, 0.6360564627684653, 0.6283840883988887, 0.6260100856889039, 0.6254133810289204, 0.6297254059463739, 0.6268937978893518, 0.6272714491933584, 0.6265493480022997, 0.6233120549004525, 0.6242760112509131, 0.6269282419234514, 0.6272533612791449, 0.6140329102054238, 0.6117751658894122, 0.6172638251446187, 0.6126077929511666, 0.6113092850428075, 0.6209227405488491, 0.6168064530938864, 0.6122028462123126, 0.6177828102372587, 0.6123310851398855, 0.6150758538860828, 0.622350333025679, 0.6167943859472871, 0.6090818280354142, 0.6209616751875728, 0.6157061748672277, 0.6085516971070319, 0.6125378240831196, 0.6065603422466666, 0.6301457299850881, 0.6356244026683271, 0.6352735720574856, 0.6339612696319818, 0.6337616802193224, 0.6358491259161383, 0.6439262991771102, 0.6411601461004466, 0.6395270288921893, 0.6420844779349864, 0.6451505171135068, 0.6451147538609803, 0.6445618341676891, 0.64478276995942, 0.6420878872741014, 0.6428235510829836, 0.6436161268502474, 0.6460213661193848, 0.6175568331964314, 0.6155466160271317, 0.6236977039370686, 0.618185079889372, 0.6139247077517211, 0.6175391268916428, 0.6275646288413554, 0.6661357139237225, 0.7016102550551295, 0.6903741823043674, 0.6437805686146021, 0.6425399868749082, 0.6897988799028099, 0.6281185580883175, 0.6123463821131736, 0.6540840868838131, 0.623258750885725, 0.6402538733091205, 0.6482879011891782, 0.6361716748215258, 0.6364611363969743, 0.610083777923137, 0.6427686458919197, 0.6631354386918247, 0.6106380017008632, 0.6762926448136568, 0.6062148108612746, 0.6090146966744214, 0.6926272490527481, 0.648746064864099]
Total Epoch List: [71, 67]
Total Time List: [0.1879198548849672, 0.20808054320514202]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf310c70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
Epoch 5/1000, LR 0.000110
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.16s
Epoch 11/1000, LR 0.000290
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.18s
Epoch 14/1000, LR 0.000290
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.16s
Epoch 16/1000, LR 0.000290
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.19s
Epoch 17/1000, LR 0.000290
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.18s
Epoch 18/1000, LR 0.000290
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.18s
Epoch 20/1000, LR 0.000290
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5000 time: 0.17s
Epoch 21/1000, LR 0.000290
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.5000 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6733 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6651 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6668 score: 0.5000 time: 0.18s
Epoch 24/1000, LR 0.000290
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6556 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6572 score: 0.5000 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 0.4954;  Loss pred: 0.4954; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6420 score: 0.4961 time: 0.17s
Test loss: 0.6434 score: 0.5078 time: 0.17s
Epoch 26/1000, LR 0.000290
Train loss: 0.4680;  Loss pred: 0.4680; Loss self: 0.0000; time: 0.27s
Val loss: 0.6284 score: 0.5039 time: 0.17s
Test loss: 0.6291 score: 0.5469 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 0.4491;  Loss pred: 0.4491; Loss self: 0.0000; time: 0.28s
Val loss: 0.6097 score: 0.7209 time: 0.17s
Test loss: 0.6099 score: 0.7031 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.4305;  Loss pred: 0.4305; Loss self: 0.0000; time: 0.28s
Val loss: 0.5870 score: 0.7984 time: 0.17s
Test loss: 0.5863 score: 0.8047 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 0.4018;  Loss pred: 0.4018; Loss self: 0.0000; time: 0.27s
Val loss: 0.5664 score: 0.8217 time: 0.17s
Test loss: 0.5642 score: 0.8281 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.3820;  Loss pred: 0.3820; Loss self: 0.0000; time: 0.27s
Val loss: 0.5403 score: 0.8682 time: 0.17s
Test loss: 0.5367 score: 0.8828 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.3545;  Loss pred: 0.3545; Loss self: 0.0000; time: 0.29s
Val loss: 0.5132 score: 0.8760 time: 0.17s
Test loss: 0.5077 score: 0.8828 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.3375;  Loss pred: 0.3375; Loss self: 0.0000; time: 0.27s
Val loss: 0.4890 score: 0.8992 time: 0.17s
Test loss: 0.4806 score: 0.8906 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3163;  Loss pred: 0.3163; Loss self: 0.0000; time: 0.27s
Val loss: 0.4800 score: 0.8760 time: 0.17s
Test loss: 0.4668 score: 0.8828 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.2944;  Loss pred: 0.2944; Loss self: 0.0000; time: 0.28s
Val loss: 0.4751 score: 0.8760 time: 0.17s
Test loss: 0.4590 score: 0.8828 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.27s
Val loss: 0.4606 score: 0.8760 time: 0.17s
Test loss: 0.4422 score: 0.8828 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.2567;  Loss pred: 0.2567; Loss self: 0.0000; time: 0.27s
Val loss: 0.4218 score: 0.8915 time: 0.17s
Test loss: 0.4028 score: 0.8984 time: 0.17s
Epoch 37/1000, LR 0.000290
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.27s
Val loss: 0.3963 score: 0.9070 time: 0.17s
Test loss: 0.3769 score: 0.8984 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.26s
Val loss: 0.3979 score: 0.8992 time: 0.17s
Test loss: 0.3765 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.1980;  Loss pred: 0.1980; Loss self: 0.0000; time: 0.26s
Val loss: 0.3915 score: 0.8992 time: 0.17s
Test loss: 0.3689 score: 0.9062 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 0.1824;  Loss pred: 0.1824; Loss self: 0.0000; time: 0.27s
Val loss: 0.3816 score: 0.8992 time: 0.17s
Test loss: 0.3592 score: 0.8984 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.27s
Val loss: 0.3754 score: 0.8992 time: 0.17s
Test loss: 0.3538 score: 0.8984 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 0.1395;  Loss pred: 0.1395; Loss self: 0.0000; time: 0.27s
Val loss: 0.3469 score: 0.9070 time: 0.17s
Test loss: 0.3271 score: 0.8984 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 0.1225;  Loss pred: 0.1225; Loss self: 0.0000; time: 0.27s
Val loss: 0.3124 score: 0.9147 time: 0.17s
Test loss: 0.2936 score: 0.9219 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.29s
Val loss: 0.3004 score: 0.8992 time: 0.20s
Test loss: 0.2809 score: 0.9219 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.26s
Val loss: 0.3203 score: 0.9147 time: 0.17s
Test loss: 0.2992 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0963;  Loss pred: 0.0963; Loss self: 0.0000; time: 0.27s
Val loss: 0.3379 score: 0.9070 time: 0.17s
Test loss: 0.3174 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0854;  Loss pred: 0.0854; Loss self: 0.0000; time: 0.27s
Val loss: 0.3517 score: 0.8992 time: 0.17s
Test loss: 0.3309 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.27s
Val loss: 0.3399 score: 0.8992 time: 0.17s
Test loss: 0.3209 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.27s
Val loss: 0.3039 score: 0.9147 time: 0.17s
Test loss: 0.2827 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.27s
Val loss: 0.2942 score: 0.9070 time: 0.17s
Test loss: 0.2722 score: 0.9219 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 0.0622;  Loss pred: 0.0622; Loss self: 0.0000; time: 0.27s
Val loss: 0.2962 score: 0.9070 time: 0.17s
Test loss: 0.2731 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.26s
Val loss: 0.2988 score: 0.9070 time: 0.17s
Test loss: 0.2744 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0533;  Loss pred: 0.0533; Loss self: 0.0000; time: 0.26s
Val loss: 0.3045 score: 0.9070 time: 0.17s
Test loss: 0.2775 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.26s
Val loss: 0.3386 score: 0.9147 time: 0.17s
Test loss: 0.3139 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.26s
Val loss: 0.4021 score: 0.8992 time: 0.17s
Test loss: 0.3845 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.26s
Val loss: 0.4240 score: 0.8992 time: 0.17s
Test loss: 0.4075 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.26s
Val loss: 0.4095 score: 0.9070 time: 0.17s
Test loss: 0.3953 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.27s
Val loss: 0.3404 score: 0.9070 time: 0.17s
Test loss: 0.3235 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.26s
Val loss: 0.3319 score: 0.9147 time: 0.17s
Test loss: 0.3134 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0370;  Loss pred: 0.0370; Loss self: 0.0000; time: 0.26s
Val loss: 0.3571 score: 0.9147 time: 0.17s
Test loss: 0.3485 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.26s
Val loss: 0.3127 score: 0.9070 time: 0.17s
Test loss: 0.2830 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.27s
Val loss: 0.3095 score: 0.8992 time: 0.17s
Test loss: 0.2765 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.27s
Val loss: 0.3147 score: 0.9147 time: 0.17s
Test loss: 0.3017 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.26s
Val loss: 0.3055 score: 0.8992 time: 0.17s
Test loss: 0.2813 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.26s
Val loss: 0.3055 score: 0.8992 time: 0.17s
Test loss: 0.2802 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.26s
Val loss: 0.3353 score: 0.9147 time: 0.17s
Test loss: 0.3332 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.26s
Val loss: 0.3220 score: 0.9147 time: 0.17s
Test loss: 0.3091 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.26s
Val loss: 0.3239 score: 0.9070 time: 0.17s
Test loss: 0.2986 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.26s
Val loss: 0.3456 score: 0.9147 time: 0.17s
Test loss: 0.3221 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.26s
Val loss: 0.3484 score: 0.9070 time: 0.17s
Test loss: 0.3165 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0590,   Val_Loss: 0.2942,   Val_Precision: 0.9649,   Val_Recall: 0.8462,   Val_accuracy: 0.9016,   Val_Score: 0.9070,   Val_Loss: 0.2942,   Test_Precision: 0.9655,   Test_Recall: 0.8750,   Test_accuracy: 0.9180,   Test_Score: 0.9219,   Test_loss: 0.2722


[0.18006708496250212, 0.1972219990566373, 0.2021802831441164, 0.17644813493825495, 0.17424821900203824, 0.17892567301169038, 0.17876605596393347, 0.18011537287384272, 0.1807163490448147, 0.17744221701286733, 0.1788838761858642, 0.17390496795997024, 0.17537656496278942, 0.17699436098337173, 0.1768266330473125, 0.1757854458410293, 0.17745078797452152, 0.17877721600234509, 0.17584418505430222, 0.17815070203505456, 0.1777893991675228, 0.17620705906301737, 0.1769334301352501, 0.1893428028561175, 0.1892134938389063, 0.1764975201804191, 0.17670195712707937, 0.17967911111190915, 0.18737640301696956, 0.18567933212034404, 0.1882117420900613, 0.1858699219301343, 0.1863530189730227, 0.1908104019239545, 0.1874292311258614, 0.18709116685204208, 0.19223792408593, 0.17707601306028664, 0.17698918608948588, 0.17571120406500995, 0.1772332100663334, 0.18158254586160183, 0.18088357988744974, 0.17740772408433259, 0.17735032783821225, 0.1813961360603571, 0.18034660816192627, 0.2796741190832108, 0.18803717102855444, 0.18409862997941673, 0.18755954084917903, 0.18957949616014957, 0.27246426208876073, 0.18829286098480225, 0.18764129979535937, 0.1837691911496222, 0.1865398921072483, 0.18879051390103996, 0.1868262190837413, 0.18742640409618616, 0.18756970996037126, 0.18470647605136037, 0.18465404794551432, 0.18740891804918647, 0.18892310792580247, 0.18423046986572444, 0.18735331296920776, 0.18713833997026086, 0.1850194949656725, 0.18486366886645555, 0.18742635287344456, 0.17119526490569115, 0.1724476011004299, 0.17360520688816905, 0.1704855130519718, 0.17032999289222062, 0.17383340396918356, 0.17379562021233141, 0.17074222001247108, 0.17332330998033285, 0.17246024101041257, 0.1716027499642223, 0.1734238041099161, 0.175106979906559, 0.1696885460987687, 0.17381824110634625, 0.1756721599958837, 0.1684651670511812, 0.1701194979250431, 0.17186662298627198, 0.17982544493861496, 0.1812021469231695, 0.18098868196830153, 0.17797516798600554, 0.17971465596929193, 0.18084205989725888, 0.18061502603814006, 0.18164752703160048, 0.18341725994832814, 0.1812017410993576, 0.18454914703033864, 0.1821614250075072, 0.18315849895589054, 0.1847925540059805, 0.18138671293854713, 0.18446193891577423, 0.18225730187259614, 0.1822274650912732, 0.17505107005126774, 0.172115613007918, 0.17196715297177434, 0.1715962658636272, 0.17388255801051855, 0.1727279219776392, 0.17280229297466576, 0.18800213490612805, 0.19026095001026988, 0.19190111896023154, 0.1721467049792409, 0.17846558312885463, 0.20018563303165138, 0.17053417791612446, 0.16747846198268235, 0.17562669585458934, 0.17609374807216227, 0.17234785808250308, 0.18172014295123518, 0.17203264986164868, 0.17752224998548627, 0.16964059718884528, 0.1871710370760411, 0.16838323487900198, 0.16858136118389666, 0.1710430469829589, 0.17008521710522473, 0.16908373101614416, 0.1667263249401003, 0.20711441501043737, 0.17267640004865825, 0.17298891418613493, 0.17152525298297405, 0.17163717304356396, 0.1734415041282773, 0.1732736781705171, 0.17006333195604384, 0.1716322568245232, 0.17234328598715365, 0.16856465092860162, 0.17091635195538402, 0.1746629299595952, 0.18651805608533323, 0.16771674202755094, 0.16943832114338875, 0.19906685501337051, 0.1807901100255549, 0.1783656079787761, 0.18166283005848527, 0.17948554386384785, 0.16933227702975273, 0.16995977889746428, 0.18029715190641582, 0.18198530096560717, 0.17070254706777632, 0.1658099670894444, 0.16865175985731184, 0.17179525480605662, 0.16971876309253275, 0.16789797088131309, 0.17069328087382019, 0.16913407086394727, 0.16784641100093722, 0.17180815106257796, 0.16907974798232317, 0.17004846781492233, 0.16771959094330668, 0.17059806594625115, 0.16580963809974492, 0.16524247406050563, 0.17032444290816784, 0.16950904298573732, 0.1677857139147818, 0.1792591328267008, 0.16887882095761597, 0.16536151291802526, 0.16784930787980556, 0.16891528805717826, 0.16807462903670967, 0.1661553611047566, 0.16568460501730442, 0.1689395101275295, 0.1700915088877082, 0.16843483201228082, 0.16598670394159853, 0.17057238100096583, 0.16599302692338824, 0.16481698607094586, 0.16970362095162272, 0.16997198411263525, 0.166721518849954, 0.16795215709134936, 0.17124729696661234, 0.16958483611233532, 0.16588764800690114, 0.16561706410720944, 0.16588637698441744, 0.1675220369361341, 0.16812331811524928, 0.168346252059564]
[0.001395868875678311, 0.0015288527058654052, 0.0015672890166210573, 0.0013678149995213563, 0.0013507613876126996, 0.0013870207210208557, 0.0013857833795653757, 0.0013962432005724243, 0.001400901930579959, 0.0013755210621152507, 0.0013866967146191024, 0.0013481005268214748, 0.0013595082555254994, 0.0013720493099486182, 0.0013707490933900193, 0.0013626778747366612, 0.0013755875036784613, 0.001385869891491047, 0.001363133217475211, 0.0013810131940701904, 0.0013782123966474635, 0.0013659461942869564, 0.0013715769777926364, 0.0014677736655512984, 0.001466771270069041, 0.001368197830855962, 0.0013697826133882121, 0.001392861326448908, 0.0014525302559455005, 0.0014393746675995661, 0.0014590057526361342, 0.0014408521079855372, 0.0014445970463025015, 0.001479150402511275, 0.0014529397761694682, 0.0014503191228840472, 0.001490216465782403, 0.0013726822717851678, 0.0013720091944921385, 0.0013621023570931003, 0.0013739008532273908, 0.0014076166345860608, 0.0014021982937011607, 0.0013752536750723456, 0.0013748087429318778, 0.0014061715973671092, 0.001398035722185475, 0.002168016427001634, 0.0014576524885934453, 0.0014271211626311375, 0.0014539499290634034, 0.0014696084973655006, 0.0021121260627035717, 0.0014596345812775368, 0.001454583719343871, 0.0014245673732528853, 0.001446045675249987, 0.0014634923558220151, 0.001448265264215049, 0.0014529178612107455, 0.001454028759382723, 0.001431833147684964, 0.001431426728259801, 0.0014527823104588098, 0.001464520216479089, 0.0014281431772536779, 0.001452351263327192, 0.0014506848059710144, 0.001434259650896686, 0.0014330516966391904, 0.0014529174641352291, 0.0013270950767883035, 0.0013368031093056582, 0.0013457767975827057, 0.0013215931244338899, 0.0013203875418001598, 0.0013475457672029734, 0.0013472528698630342, 0.001323583100871869, 0.0013435915502351384, 0.0013369010931039735, 0.001330253875691646, 0.0013443705744954737, 0.001357418448888054, 0.0013154150860369667, 0.0013474282256305912, 0.0013617996898905714, 0.0013059315275285365, 0.0013187557978685512, 0.0013322994029943563, 0.0013939956971985657, 0.001404667805605965, 0.0014030130385139653, 0.001379652465007795, 0.0013931368679790071, 0.0014018764333120844, 0.0014001164809158144, 0.0014081203645860503, 0.0014218392244056445, 0.0014046646596849427, 0.0014306135428708423, 0.0014121040698256372, 0.0014198333252394616, 0.0014325004186510116, 0.001406098549911218, 0.001429937510974994, 0.0014128473013379547, 0.001412616008459482, 0.0013569850391571142, 0.0013342295582009147, 0.001333078705207553, 0.0013302036113459472, 0.0013479268062830895, 0.0013389761393615442, 0.0013395526587183391, 0.0014573808907451787, 0.0014748910853509293, 0.0014876055733351282, 0.0013344705812344255, 0.0013834541327818189, 0.0015518266126484602, 0.0013219703714428253, 0.0012982826510285453, 0.0013614472546867392, 0.0013650678145128857, 0.001336029907616303, 0.0014086832786917454, 0.0013335864330360362, 0.0013761414727557074, 0.001315043389060816, 0.0014509382719072954, 0.001305296394410868, 0.001306832257239509, 0.0013259150928911544, 0.0013184900550792615, 0.0013107265970243732, 0.0012924521313186072, 0.0016055381008561037, 0.0013490343753801426, 0.0013514758920791792, 0.0013400410389294848, 0.0013409154144028435, 0.0013550117510021664, 0.0013537006107071647, 0.0013286197809065925, 0.0013408770064415876, 0.0013464319217746379, 0.0013169113353797002, 0.0013352839996514376, 0.0013645541403093375, 0.0014571723131666658, 0.0013102870470902417, 0.0013237368839327246, 0.0015552098047919571, 0.0014124227345746476, 0.0013934813123341883, 0.0014192408598319162, 0.0014022308114363113, 0.0013229084142949432, 0.0013278107726364397, 0.0014085714992688736, 0.001421760163793806, 0.0013336136489670025, 0.0012953903678862844, 0.0013175918738852488, 0.0013421504281723173, 0.0013259278366604121, 0.0013117028975102585, 0.0013335412568267202, 0.001321359928624588, 0.001311300085944822, 0.0013422511801763903, 0.0013209355311118998, 0.0013285036548040807, 0.0013103093042445835, 0.0013327973902050871, 0.0012953877976542572, 0.0012909568285977002, 0.0013306597102200612, 0.0013242893983260728, 0.0013108258899592329, 0.0014004619752086, 0.0013193657887313748, 0.0012918868196720723, 0.001311322717810981, 0.0013196506879467051, 0.0013130830393492943, 0.0012980887586309109, 0.0012944109766976908, 0.0013198399228713242, 0.0013288399131852202, 0.001315897125095944, 0.0012967711245437386, 0.0013325967265700456, 0.0012968205228389706, 0.0012876327036792645, 0.0013258095386845525, 0.001327906125879963, 0.0013025118660152657, 0.001312126227276167, 0.001337869507551659, 0.0013248815321276197, 0.0012959972500539152, 0.0012938833133375738, 0.0012959873201907612, 0.0013087659135635477, 0.001313463422775385, 0.0013152050942153437]
[716.3996686394044, 654.0852471683668, 638.0444126099444, 731.0930208763123, 740.3232052460235, 720.9697626319482, 721.6135037740391, 716.2076059457446, 713.8258418888825, 726.9972285718538, 721.1382196680835, 741.7844441896189, 735.5600791210081, 728.8367792243918, 729.5281133667472, 733.8491499271249, 726.9621142427493, 721.5684575729598, 733.6040140318753, 724.1060435148707, 725.5775687640928, 732.0932582721638, 729.0877699109253, 681.3039526938223, 681.7695576713402, 730.8884559292038, 730.0428478402569, 717.9465615213068, 688.4538176790448, 694.746143940196, 685.3982571303768, 694.0337557600586, 692.2345594984679, 676.0637716774561, 688.2597726358631, 689.50342322691, 671.0434510431836, 728.5007030064605, 728.8580893003118, 734.1592170313303, 727.8545592652694, 710.4207036414223, 713.1658942191823, 727.1385767774042, 727.3739021090515, 711.1507598876142, 715.2893049376114, 461.2511176324432, 686.0345712200204, 700.7113524659196, 687.7815941324568, 680.4533328384083, 473.4565884386542, 685.1029790790211, 687.4819143796527, 701.9675016960274, 691.5410883042292, 683.2970435560078, 690.4812431181203, 688.2701539415863, 687.7443059823169, 698.4053984340519, 698.6036939632316, 688.3343724664334, 682.8174775245777, 700.2099060704837, 688.538665025911, 689.3296158366055, 697.2238251106131, 697.8115320928141, 688.2703420425854, 753.5255141026483, 748.0533169311708, 743.0652704045777, 756.6625321453259, 757.3534044683903, 742.089823097916, 742.2511559405051, 755.5249076097159, 744.2738083794831, 747.9984908069995, 751.7362048504199, 743.8425230151205, 736.692506882577, 760.2163078521188, 742.1545585717591, 734.3223878104686, 765.7369310108402, 758.2905050474526, 750.5820371550793, 717.362329029884, 711.9120948092109, 712.7517510879113, 724.8202176729703, 717.804562483999, 713.3296317974275, 714.226290191157, 710.1665632781139, 703.3143992901298, 711.9136892247959, 699.0007923406617, 708.1631031085955, 704.3080213878944, 698.080075216803, 711.1877044913676, 699.3312591108657, 707.7905723095541, 707.9064614951819, 736.9278003397487, 749.4962121423899, 750.1432556784451, 751.7646106735227, 741.8800452210768, 746.8392980302277, 746.5178718369928, 686.1624207853353, 678.0161667070245, 672.2211975571294, 749.3608432154194, 722.8284453415323, 644.4018886190688, 756.4466054625564, 770.2482962456324, 734.5124804192976, 732.5643380998199, 748.4862384436921, 709.8827785680165, 749.857658437185, 726.6694738859309, 760.4311829697001, 689.2091961193321, 766.1095244588794, 765.2091494224002, 754.1961060413776, 758.4433391421255, 762.9356131707494, 773.7230461137182, 622.8441414543703, 741.2709551735561, 739.9318077820458, 746.2458021426474, 745.7591949939176, 738.0009798884771, 738.7157781347282, 752.66077953291, 745.7805564537159, 742.7037222067416, 759.3525646976633, 748.9043531271549, 732.8401053939165, 686.26063710121, 763.1915481578658, 755.4371356859633, 643.000061418576, 708.0033303918398, 717.6271336749562, 704.6020364143351, 713.1493559007561, 755.9102271890528, 753.119360535422, 709.9391124405507, 703.353508886909, 749.8423555987038, 771.9680683064836, 758.960357770916, 745.0729657492692, 754.1888573051456, 762.3677601826593, 749.8830612707018, 756.7960692140152, 762.601948035012, 745.0170391122965, 757.0392168634061, 752.726570517018, 763.1785844461493, 750.3015892356473, 771.9695999999708, 774.619241982126, 751.5069347328646, 755.1219554154999, 762.8778220356178, 714.0500904003832, 757.939919725781, 774.0616165229058, 762.588786434907, 757.7762881751216, 761.5664585048297, 770.3633463821812, 772.552163109128, 757.6676403487558, 752.5360956407509, 759.9378256313832, 771.1461036363254, 750.4145703358342, 771.1167292531909, 776.618982371769, 754.2561512961993, 753.0652811299673, 767.7473242982953, 762.1217983546389, 747.4570534386645, 754.7844661960875, 771.6065755219763, 772.8672204763957, 771.6124875765037, 764.0785793978769, 761.3459062963261, 760.337687557851]
Elapsed: 0.17778997265290505~0.012586900558533268
Time per graph: 0.0013816991865165716~9.581281793727622e-05
Speed: 726.4589581012436~39.9600666727845
Total Time: 0.1691
best val loss: 0.2941524520400883 test_score: 0.9219

Testing...
Test loss: 0.2936 score: 0.9219 time: 0.16s
test Score 0.9219
Epoch Time List: [0.6136276395991445, 0.6692980478983372, 0.6531465058214962, 0.611377993831411, 0.6030081149656326, 0.6031049836892635, 0.6112898623105139, 0.6042674500495195, 0.6152863048482686, 0.60693065286614, 0.6034295388963073, 0.5951653399970382, 0.5977543098852038, 0.6012596080545336, 0.6008684108965099, 0.6031121080741286, 0.6022111249621958, 0.6050134201068431, 0.606601171195507, 0.60755554609932, 0.6013346230611205, 0.6028070540633053, 0.602004696149379, 0.6346944577526301, 0.633837427245453, 0.6002698224037886, 0.6023258771747351, 0.6047240810003132, 0.6269756378605962, 0.6294747751671821, 0.6310365928802639, 0.6282272259704769, 0.6299553399439901, 0.6356388076674193, 0.6332864339929074, 0.6349816401489079, 0.6404945340473205, 0.6079205339774489, 0.6038604027125984, 0.6015166570432484, 0.6051413011737168, 0.6095512728206813, 0.6115652930457145, 0.6124972568359226, 0.6107501762453467, 0.6457080177497119, 0.6103830831125379, 0.7158503320533782, 0.6295808088034391, 0.6300149958115071, 0.7148100340273231, 0.6308916942216456, 0.7189135199878365, 0.6286989278160036, 0.6337480309884995, 0.626115390798077, 0.627493133302778, 0.6302218416240066, 0.6301170520018786, 0.6360564627684653, 0.6283840883988887, 0.6260100856889039, 0.6254133810289204, 0.6297254059463739, 0.6268937978893518, 0.6272714491933584, 0.6265493480022997, 0.6233120549004525, 0.6242760112509131, 0.6269282419234514, 0.6272533612791449, 0.6140329102054238, 0.6117751658894122, 0.6172638251446187, 0.6126077929511666, 0.6113092850428075, 0.6209227405488491, 0.6168064530938864, 0.6122028462123126, 0.6177828102372587, 0.6123310851398855, 0.6150758538860828, 0.622350333025679, 0.6167943859472871, 0.6090818280354142, 0.6209616751875728, 0.6157061748672277, 0.6085516971070319, 0.6125378240831196, 0.6065603422466666, 0.6301457299850881, 0.6356244026683271, 0.6352735720574856, 0.6339612696319818, 0.6337616802193224, 0.6358491259161383, 0.6439262991771102, 0.6411601461004466, 0.6395270288921893, 0.6420844779349864, 0.6451505171135068, 0.6451147538609803, 0.6445618341676891, 0.64478276995942, 0.6420878872741014, 0.6428235510829836, 0.6436161268502474, 0.6460213661193848, 0.6175568331964314, 0.6155466160271317, 0.6236977039370686, 0.618185079889372, 0.6139247077517211, 0.6175391268916428, 0.6275646288413554, 0.6661357139237225, 0.7016102550551295, 0.6903741823043674, 0.6437805686146021, 0.6425399868749082, 0.6897988799028099, 0.6281185580883175, 0.6123463821131736, 0.6540840868838131, 0.623258750885725, 0.6402538733091205, 0.6482879011891782, 0.6361716748215258, 0.6364611363969743, 0.610083777923137, 0.6427686458919197, 0.6631354386918247, 0.6106380017008632, 0.6762926448136568, 0.6062148108612746, 0.6090146966744214, 0.6926272490527481, 0.648746064864099, 0.6123222359456122, 0.6129231357481331, 0.6138380381744355, 0.6145813909824938, 0.6182088118512183, 0.6128279489930719, 0.6141295810230076, 0.6107505129184574, 0.6089527250733227, 0.6079240529797971, 0.6134508813265711, 0.6111421117093414, 0.6229720341507345, 0.6223068980034441, 0.6101987850852311, 0.6616699129808694, 0.6312459933105856, 0.636438112007454, 0.641049712896347, 0.6453281107824296, 0.6066285429988056, 0.6073166211135685, 0.6407782579772174, 0.6470929798670113, 0.6024005068466067, 0.6064704998861998, 0.613278876291588, 0.6126718192826957, 0.6078413110226393, 0.6054607820697129, 0.621298372047022, 0.610082346946001, 0.6115406521130353, 0.6143513380084187, 0.6073248840402812, 0.6090665990486741, 0.60206688195467, 0.5995444739237428, 0.5961682251654565, 0.5970382809173316, 0.6029333400074393, 0.6013768138363957, 0.6005048200022429, 0.6624839711003006, 0.6007823140826076, 0.6006289392244071, 0.5971928602084517, 0.5996959391050041, 0.6015988159924746, 0.6006507072597742, 0.5956570201087743, 0.5970359498169273, 0.5954241279978305, 0.594630234874785, 0.5924548401962966, 0.5963557888753712, 0.5946134158875793, 0.5967328113038093, 0.5960202342830598, 0.5994147039018571, 0.5960529029835016, 0.6028469160664827, 0.6028633201494813, 0.5998827740550041, 0.5929173729382455, 0.5951945721171796, 0.5921039448585361, 0.5898616591002792, 0.5891124457120895, 0.5938332879450172]
Total Epoch List: [71, 67, 70]
Total Time List: [0.1879198548849672, 0.20808054320514202, 0.1691235729958862]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcf310cd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.19s
Epoch 5/1000, LR 0.000105
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.28s
Epoch 6/1000, LR 0.000135
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.19s
Epoch 7/1000, LR 0.000165
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.20s
Epoch 8/1000, LR 0.000195
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5039 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5039 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4643;  Loss pred: 0.4643; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4332;  Loss pred: 0.4332; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.5039 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4098;  Loss pred: 0.4098; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6742 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.5039 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.3717;  Loss pred: 0.3717; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6690 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.5039 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3396;  Loss pred: 0.3396; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6630 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6664 score: 0.5039 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3132;  Loss pred: 0.3132; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6569 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6610 score: 0.5039 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.2788;  Loss pred: 0.2788; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6499 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6548 score: 0.5039 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2489;  Loss pred: 0.2489; Loss self: 0.0000; time: 0.25s
Val loss: 0.6407 score: 0.5039 time: 0.18s
Test loss: 0.6464 score: 0.5271 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.28s
Val loss: 0.6281 score: 0.5116 time: 0.18s
Test loss: 0.6350 score: 0.5349 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2069;  Loss pred: 0.2069; Loss self: 0.0000; time: 0.25s
Val loss: 0.6141 score: 0.5271 time: 0.18s
Test loss: 0.6220 score: 0.5426 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1743;  Loss pred: 0.1743; Loss self: 0.0000; time: 0.25s
Val loss: 0.5975 score: 0.5736 time: 0.18s
Test loss: 0.6066 score: 0.5659 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1588;  Loss pred: 0.1588; Loss self: 0.0000; time: 0.25s
Val loss: 0.5778 score: 0.5891 time: 0.18s
Test loss: 0.5884 score: 0.5736 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1505;  Loss pred: 0.1505; Loss self: 0.0000; time: 0.25s
Val loss: 0.5513 score: 0.6047 time: 0.18s
Test loss: 0.5641 score: 0.5969 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1252;  Loss pred: 0.1252; Loss self: 0.0000; time: 0.25s
Val loss: 0.5244 score: 0.6124 time: 0.18s
Test loss: 0.5395 score: 0.6124 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.25s
Val loss: 0.4916 score: 0.6434 time: 0.18s
Test loss: 0.5098 score: 0.6202 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.25s
Val loss: 0.4609 score: 0.6744 time: 0.18s
Test loss: 0.4818 score: 0.6899 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.25s
Val loss: 0.4297 score: 0.7287 time: 0.18s
Test loss: 0.4531 score: 0.6977 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.25s
Val loss: 0.3989 score: 0.8062 time: 0.18s
Test loss: 0.4243 score: 0.7829 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.25s
Val loss: 0.3672 score: 0.8527 time: 0.18s
Test loss: 0.3946 score: 0.8295 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.25s
Val loss: 0.3365 score: 0.8605 time: 0.18s
Test loss: 0.3657 score: 0.8527 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.25s
Val loss: 0.3070 score: 0.8915 time: 0.18s
Test loss: 0.3379 score: 0.8992 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.25s
Val loss: 0.2830 score: 0.8837 time: 0.18s
Test loss: 0.3148 score: 0.8992 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.25s
Val loss: 0.2618 score: 0.8992 time: 0.18s
Test loss: 0.2944 score: 0.8837 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.25s
Val loss: 0.2436 score: 0.9380 time: 0.18s
Test loss: 0.2763 score: 0.8915 time: 0.32s
Epoch 43/1000, LR 0.000284
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.25s
Val loss: 0.2286 score: 0.9380 time: 0.18s
Test loss: 0.2618 score: 0.8915 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.25s
Val loss: 0.2162 score: 0.9380 time: 0.18s
Test loss: 0.2496 score: 0.8915 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.26s
Val loss: 0.2035 score: 0.9380 time: 0.18s
Test loss: 0.2391 score: 0.8915 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.25s
Val loss: 0.1947 score: 0.9380 time: 0.18s
Test loss: 0.2327 score: 0.8915 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.25s
Val loss: 0.1905 score: 0.9380 time: 0.18s
Test loss: 0.2304 score: 0.8915 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.27s
Val loss: 0.1873 score: 0.9380 time: 0.20s
Test loss: 0.2292 score: 0.8992 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.26s
Val loss: 0.1867 score: 0.9380 time: 0.18s
Test loss: 0.2303 score: 0.8915 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.26s
Val loss: 0.1862 score: 0.9380 time: 0.19s
Test loss: 0.2316 score: 0.8915 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.26s
Val loss: 0.1878 score: 0.9380 time: 0.19s
Test loss: 0.2349 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.25s
Val loss: 0.1902 score: 0.9380 time: 0.18s
Test loss: 0.2392 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.25s
Val loss: 0.1936 score: 0.9380 time: 0.17s
Test loss: 0.2438 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.25s
Val loss: 0.1955 score: 0.9380 time: 0.18s
Test loss: 0.2479 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.26s
Val loss: 0.1971 score: 0.9380 time: 0.20s
Test loss: 0.2512 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.25s
Val loss: 0.1980 score: 0.9380 time: 0.18s
Test loss: 0.2557 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.25s
Val loss: 0.2003 score: 0.9457 time: 0.18s
Test loss: 0.2598 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.25s
Val loss: 0.2043 score: 0.9457 time: 0.18s
Test loss: 0.2671 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.25s
Val loss: 0.2083 score: 0.9535 time: 0.18s
Test loss: 0.2746 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.25s
Val loss: 0.2129 score: 0.9535 time: 0.18s
Test loss: 0.2807 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.25s
Val loss: 0.2178 score: 0.9535 time: 0.18s
Test loss: 0.2891 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.29s
Val loss: 0.2230 score: 0.9535 time: 0.19s
Test loss: 0.2974 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.26s
Val loss: 0.2276 score: 0.9535 time: 0.18s
Test loss: 0.3002 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.26s
Val loss: 0.2315 score: 0.9535 time: 0.18s
Test loss: 0.2980 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.26s
Val loss: 0.2352 score: 0.9535 time: 0.18s
Test loss: 0.2920 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.26s
Val loss: 0.2388 score: 0.9535 time: 0.19s
Test loss: 0.2858 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.26s
Val loss: 0.2431 score: 0.9535 time: 0.18s
Test loss: 0.2798 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.26s
Val loss: 0.2473 score: 0.9457 time: 0.18s
Test loss: 0.2774 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.26s
Val loss: 0.2503 score: 0.9380 time: 0.19s
Test loss: 0.2788 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.26s
Val loss: 0.2525 score: 0.9380 time: 0.18s
Test loss: 0.2811 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0327,   Val_Loss: 0.1862,   Val_Precision: 0.9242,   Val_Recall: 0.9531,   Val_accuracy: 0.9385,   Val_Score: 0.9380,   Val_Loss: 0.1862,   Test_Precision: 0.8923,   Test_Recall: 0.8923,   Test_accuracy: 0.8923,   Test_Score: 0.8915,   Test_loss: 0.2316


[0.17433307506144047, 0.17559706908650696, 0.18575256504118443, 0.19231122615747154, 0.2813546450342983, 0.1938636521808803, 0.20909508294425905, 0.17838199599646032, 0.1745298400055617, 0.172144838841632, 0.1733769099228084, 0.17459620512090623, 0.1724799908697605, 0.17685766192153096, 0.17884454200975597, 0.1738443709909916, 0.1747369859367609, 0.1784436150919646, 0.17703058291226625, 0.1755871248897165, 0.17699425504542887, 0.17810841579921544, 0.17557773482985795, 0.17354627419263124, 0.1760989069007337, 0.1745842129457742, 0.17251894809305668, 0.17471867892891169, 0.17646947014145553, 0.17657351586967707, 0.17310591507703066, 0.1755204990040511, 0.17842158093117177, 0.1773179059382528, 0.17462206608615816, 0.17782264482229948, 0.17383897304534912, 0.17316552414558828, 0.17507814103737473, 0.17615654901601374, 0.1764462050050497, 0.3289476309437305, 0.18037587497383356, 0.1781242780853063, 0.17361017200164497, 0.17453389195725322, 0.1858573250938207, 0.17829288798384368, 0.18502789386548102, 0.18723574304021895, 0.17826996999792755, 0.17774093500338495, 0.17782550095580518, 0.17942778812721372, 0.18554431991651654, 0.17730963812209666, 0.1797086710575968, 0.17840768001042306, 0.1762810491491109, 0.17662740591913462, 0.20607962389476597, 0.17605039500631392, 0.17698935698717833, 0.18115342617966235, 0.18271446600556374, 0.18181313597597182, 0.1831009869929403, 0.18481867713853717, 0.18771484098397195, 0.17748450301587582]
[0.0013514191865227944, 0.0013612175898178834, 0.0014399423646603443, 0.0014907846988951282, 0.0021810437599558007, 0.001502819009154111, 0.0016208921158469695, 0.0013828061705151962, 0.001352944496167145, 0.0013344561150514108, 0.0013440070536651814, 0.0013534589544256298, 0.0013370541927888412, 0.0013709896272986895, 0.0013863917985252401, 0.001347630782875904, 0.001354550278579542, 0.0013832838379222061, 0.0013723301000950873, 0.0013611405030210582, 0.001372048488724255, 0.0013806853937923676, 0.0013610677118593639, 0.0013453199549816376, 0.0013651078054320444, 0.001353365991827707, 0.0013373561867678812, 0.0013544083637900132, 0.0013679803886934537, 0.0013687869447261789, 0.0013419063184265943, 0.0013606240232872178, 0.0013831130304741998, 0.0013745574103740527, 0.0013536594270244819, 0.0013784701149015463, 0.0013475889383360398, 0.0013423684042293666, 0.001357194891762595, 0.0013655546435349902, 0.001367800038798835, 0.0025499816352227167, 0.001398262596696384, 0.0013808083572504363, 0.0013458152868344572, 0.0013529759066453738, 0.0014407544580916332, 0.0013821154107274704, 0.0014343247586471397, 0.001451439868528829, 0.001381937751921919, 0.0013778367054525964, 0.001378492255471358, 0.0013909130862574707, 0.0014383280613683453, 0.001374493318775943, 0.0013930904733147039, 0.0013830052713986285, 0.001366519760845821, 0.0013692046970475553, 0.0015975164643005114, 0.0013647317442349917, 0.001372010519280452, 0.0014042901254237392, 0.0014163912093454554, 0.0014094041548524947, 0.0014193874960693048, 0.0014327029235545517, 0.001455153806077302, 0.0013758488605881845]
[739.9628553247072, 734.6364074929339, 694.4722403773997, 670.7876735930644, 458.49607346725827, 665.4161239036151, 616.9442063560579, 723.1671519280443, 739.1286211910192, 749.3689666680978, 744.0437141106847, 738.8476737548145, 747.9128410750426, 729.4001209697961, 721.2968232095283, 742.0430081494247, 738.2524043689664, 722.9174321172387, 728.6876531606434, 734.6780128726572, 728.8372154615399, 724.2779596974453, 734.7173041331597, 743.3175998743355, 732.5428775813856, 738.8984251403497, 747.743951756635, 738.3297583911256, 731.0046315467222, 730.5738879618308, 745.2085039531786, 734.9568895483976, 723.0067087555024, 727.5068996411536, 738.738252795335, 725.4419150548088, 742.0660496328861, 744.9519795380501, 736.8138548630225, 732.3031741969075, 731.1010174251588, 392.159687029534, 715.1732459715776, 724.2134614475183, 743.0440193261124, 739.1114616959017, 694.0807952276342, 723.5285796239363, 697.1921762984861, 688.9710153914913, 723.6215948288973, 725.7754101357872, 725.4302634134586, 718.9521831955004, 695.2516792647816, 727.5408227451782, 717.8284678242119, 723.0630429837071, 731.7859782584044, 730.3509856169212, 625.9716393207002, 732.7447347980873, 728.8573855282449, 712.1035617182409, 706.0196317245724, 709.5196906842225, 704.5292443179117, 697.9814053279022, 687.2125790576924, 726.8240201707135]
Elapsed: 0.1825273780750909~0.02241638849350296
Time per graph: 0.0014149409153107822~0.00017377045343800742
Speed: 713.4519660566751~55.4715448237741
Total Time: 0.1779
best val loss: 0.18621467202613057 test_score: 0.8915

Testing...
Test loss: 0.2746 score: 0.8992 time: 0.18s
test Score 0.8992
Epoch Time List: [0.5951955972705036, 0.6030883467756212, 0.6364163088146597, 0.6560653699561954, 0.7236993659753352, 0.6352020287886262, 0.6751503441482782, 0.7078485200181603, 0.5913547929376364, 0.5984682748094201, 0.6701170692685992, 0.5996852822136134, 0.6827889462001622, 0.5978107212577015, 0.6046172678470612, 0.6023350378964096, 0.5958906090818346, 0.6035980221349746, 0.6020589517429471, 0.6006080771330744, 0.6002606281545013, 0.6017543750349432, 0.5962904668413103, 0.5933174269739538, 0.5947386990301311, 0.5968377240933478, 0.6275651280302554, 0.5942206801846623, 0.5956726777367294, 0.6001913489308208, 0.5972390151582658, 0.5999269247986376, 0.6012729380745441, 0.6006568758748472, 0.5992916547693312, 0.5997384889051318, 0.5933755510486662, 0.59132793196477, 0.5965314097702503, 0.5977786020375788, 0.5983967510983348, 0.7489399467594922, 0.607982984976843, 0.6117924840655178, 0.6061643029097468, 0.5995941839646548, 0.6107522069942206, 0.6404708989430219, 0.6251309788785875, 0.6340952522587031, 0.61546571389772, 0.6066175862215459, 0.6015546696726233, 0.612024205038324, 0.6404014183208346, 0.6071852489840239, 0.6073016948066652, 0.6044499205891043, 0.6064286159817129, 0.6044736271724105, 0.6326234920416027, 0.6479995411355048, 0.6104367221705616, 0.6216021068394184, 0.6205655941739678, 0.6211723750457168, 0.6229336482938379, 0.6238523579668254, 0.627266061026603, 0.6174731319770217]
Total Epoch List: [70]
Total Time List: [0.1779202080797404]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddcc970340>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.7002;  Loss pred: 0.7002; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6502;  Loss pred: 0.6502; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4961 time: 0.19s
Epoch 15/1000, LR 0.000285
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.27s
Val loss: 0.6898 score: 0.8992 time: 0.19s
Test loss: 0.6904 score: 0.8605 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.27s
Val loss: 0.6884 score: 0.8915 time: 0.18s
Test loss: 0.6892 score: 0.8450 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.26s
Val loss: 0.6864 score: 0.8140 time: 0.18s
Test loss: 0.6875 score: 0.7364 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.26s
Val loss: 0.6838 score: 0.7597 time: 0.21s
Test loss: 0.6852 score: 0.6977 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.27s
Val loss: 0.6805 score: 0.7597 time: 0.19s
Test loss: 0.6824 score: 0.6977 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.27s
Val loss: 0.6765 score: 0.7597 time: 0.20s
Test loss: 0.6788 score: 0.7054 time: 0.20s
Epoch 21/1000, LR 0.000285
Train loss: 0.4915;  Loss pred: 0.4915; Loss self: 0.0000; time: 0.27s
Val loss: 0.6718 score: 0.7597 time: 0.21s
Test loss: 0.6748 score: 0.7209 time: 0.19s
Epoch 22/1000, LR 0.000285
Train loss: 0.4662;  Loss pred: 0.4662; Loss self: 0.0000; time: 0.29s
Val loss: 0.6662 score: 0.7597 time: 0.19s
Test loss: 0.6699 score: 0.7209 time: 0.19s
Epoch 23/1000, LR 0.000285
Train loss: 0.4373;  Loss pred: 0.4373; Loss self: 0.0000; time: 0.27s
Val loss: 0.6593 score: 0.8062 time: 0.28s
Test loss: 0.6639 score: 0.7364 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.4092;  Loss pred: 0.4092; Loss self: 0.0000; time: 0.26s
Val loss: 0.6507 score: 0.8062 time: 0.18s
Test loss: 0.6565 score: 0.7442 time: 0.19s
Epoch 25/1000, LR 0.000285
Train loss: 0.3750;  Loss pred: 0.3750; Loss self: 0.0000; time: 0.27s
Val loss: 0.6404 score: 0.8372 time: 0.19s
Test loss: 0.6475 score: 0.7829 time: 0.19s
Epoch 26/1000, LR 0.000285
Train loss: 0.3564;  Loss pred: 0.3564; Loss self: 0.0000; time: 0.35s
Val loss: 0.6276 score: 0.8760 time: 0.18s
Test loss: 0.6364 score: 0.8217 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.3177;  Loss pred: 0.3177; Loss self: 0.0000; time: 0.26s
Val loss: 0.6129 score: 0.8760 time: 0.20s
Test loss: 0.6235 score: 0.8372 time: 0.20s
Epoch 28/1000, LR 0.000285
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 0.25s
Val loss: 0.5957 score: 0.8837 time: 0.29s
Test loss: 0.6083 score: 0.8682 time: 0.20s
Epoch 29/1000, LR 0.000285
Train loss: 0.2575;  Loss pred: 0.2575; Loss self: 0.0000; time: 0.25s
Val loss: 0.5767 score: 0.8992 time: 0.18s
Test loss: 0.5914 score: 0.8837 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.26s
Val loss: 0.5552 score: 0.9070 time: 0.18s
Test loss: 0.5723 score: 0.8915 time: 0.27s
Epoch 31/1000, LR 0.000285
Train loss: 0.2186;  Loss pred: 0.2186; Loss self: 0.0000; time: 0.25s
Val loss: 0.5324 score: 0.9070 time: 0.18s
Test loss: 0.5520 score: 0.8837 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1791;  Loss pred: 0.1791; Loss self: 0.0000; time: 0.26s
Val loss: 0.5072 score: 0.9070 time: 0.18s
Test loss: 0.5294 score: 0.8915 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.25s
Val loss: 0.4809 score: 0.9147 time: 0.17s
Test loss: 0.5057 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.24s
Val loss: 0.4518 score: 0.9225 time: 0.18s
Test loss: 0.4796 score: 0.8837 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 0.25s
Val loss: 0.4228 score: 0.9225 time: 0.17s
Test loss: 0.4539 score: 0.8837 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.1078;  Loss pred: 0.1078; Loss self: 0.0000; time: 0.24s
Val loss: 0.3936 score: 0.9225 time: 0.17s
Test loss: 0.4280 score: 0.8837 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.1103;  Loss pred: 0.1103; Loss self: 0.0000; time: 0.24s
Val loss: 0.3664 score: 0.9225 time: 0.17s
Test loss: 0.4036 score: 0.8915 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.24s
Val loss: 0.3398 score: 0.9225 time: 0.18s
Test loss: 0.3801 score: 0.8915 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.24s
Val loss: 0.3144 score: 0.9302 time: 0.18s
Test loss: 0.3569 score: 0.8837 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0719;  Loss pred: 0.0719; Loss self: 0.0000; time: 0.24s
Val loss: 0.2918 score: 0.9302 time: 0.17s
Test loss: 0.3367 score: 0.8915 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.24s
Val loss: 0.2716 score: 0.9302 time: 0.17s
Test loss: 0.3186 score: 0.8992 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.24s
Val loss: 0.2532 score: 0.9302 time: 0.18s
Test loss: 0.3017 score: 0.9070 time: 0.19s
Epoch 43/1000, LR 0.000284
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.26s
Val loss: 0.2372 score: 0.9302 time: 0.18s
Test loss: 0.2860 score: 0.9070 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.24s
Val loss: 0.2226 score: 0.9302 time: 0.17s
Test loss: 0.2694 score: 0.9070 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.25s
Val loss: 0.2118 score: 0.9302 time: 0.17s
Test loss: 0.2583 score: 0.9147 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.24s
Val loss: 0.2022 score: 0.9302 time: 0.17s
Test loss: 0.2503 score: 0.9147 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.24s
Val loss: 0.1944 score: 0.9302 time: 0.18s
Test loss: 0.2470 score: 0.9147 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.24s
Val loss: 0.1900 score: 0.9302 time: 0.18s
Test loss: 0.2491 score: 0.9147 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.24s
Val loss: 0.1864 score: 0.9302 time: 0.17s
Test loss: 0.2526 score: 0.9147 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.24s
Val loss: 0.1844 score: 0.9302 time: 0.18s
Test loss: 0.2586 score: 0.9147 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.24s
Val loss: 0.1836 score: 0.9225 time: 0.17s
Test loss: 0.2661 score: 0.9147 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.24s
Val loss: 0.1836 score: 0.9225 time: 0.18s
Test loss: 0.2732 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.24s
Val loss: 0.1867 score: 0.9302 time: 0.17s
Test loss: 0.2801 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.24s
Val loss: 0.1893 score: 0.9302 time: 0.17s
Test loss: 0.2839 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.24s
Val loss: 0.1932 score: 0.9302 time: 0.17s
Test loss: 0.2885 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.24s
Val loss: 0.1952 score: 0.9225 time: 0.17s
Test loss: 0.2912 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.24s
Val loss: 0.1977 score: 0.9225 time: 0.17s
Test loss: 0.2935 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.24s
Val loss: 0.2001 score: 0.9225 time: 0.17s
Test loss: 0.2965 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.24s
Val loss: 0.2042 score: 0.9225 time: 0.17s
Test loss: 0.3012 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.24s
Val loss: 0.2093 score: 0.9225 time: 0.17s
Test loss: 0.3037 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.24s
Val loss: 0.2121 score: 0.9302 time: 0.17s
Test loss: 0.3031 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.24s
Val loss: 0.2146 score: 0.9302 time: 0.17s
Test loss: 0.3042 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.24s
Val loss: 0.2173 score: 0.9302 time: 0.17s
Test loss: 0.3077 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.24s
Val loss: 0.2173 score: 0.9380 time: 0.17s
Test loss: 0.3104 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.24s
Val loss: 0.2165 score: 0.9380 time: 0.17s
Test loss: 0.3133 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.24s
Val loss: 0.2154 score: 0.9380 time: 0.17s
Test loss: 0.3165 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.24s
Val loss: 0.2133 score: 0.9380 time: 0.17s
Test loss: 0.3194 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.24s
Val loss: 0.2114 score: 0.9380 time: 0.17s
Test loss: 0.3234 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.24s
Val loss: 0.2089 score: 0.9302 time: 0.17s
Test loss: 0.3275 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.24s
Val loss: 0.2103 score: 0.9302 time: 0.17s
Test loss: 0.3276 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.24s
Val loss: 0.2156 score: 0.9302 time: 0.17s
Test loss: 0.3302 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0300,   Val_Loss: 0.1836,   Val_Precision: 1.0000,   Val_Recall: 0.8462,   Val_accuracy: 0.9167,   Val_Score: 0.9225,   Val_Loss: 0.1836,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2661


[0.17433307506144047, 0.17559706908650696, 0.18575256504118443, 0.19231122615747154, 0.2813546450342983, 0.1938636521808803, 0.20909508294425905, 0.17838199599646032, 0.1745298400055617, 0.172144838841632, 0.1733769099228084, 0.17459620512090623, 0.1724799908697605, 0.17685766192153096, 0.17884454200975597, 0.1738443709909916, 0.1747369859367609, 0.1784436150919646, 0.17703058291226625, 0.1755871248897165, 0.17699425504542887, 0.17810841579921544, 0.17557773482985795, 0.17354627419263124, 0.1760989069007337, 0.1745842129457742, 0.17251894809305668, 0.17471867892891169, 0.17646947014145553, 0.17657351586967707, 0.17310591507703066, 0.1755204990040511, 0.17842158093117177, 0.1773179059382528, 0.17462206608615816, 0.17782264482229948, 0.17383897304534912, 0.17316552414558828, 0.17507814103737473, 0.17615654901601374, 0.1764462050050497, 0.3289476309437305, 0.18037587497383356, 0.1781242780853063, 0.17361017200164497, 0.17453389195725322, 0.1858573250938207, 0.17829288798384368, 0.18502789386548102, 0.18723574304021895, 0.17826996999792755, 0.17774093500338495, 0.17782550095580518, 0.17942778812721372, 0.18554431991651654, 0.17730963812209666, 0.1797086710575968, 0.17840768001042306, 0.1762810491491109, 0.17662740591913462, 0.20607962389476597, 0.17605039500631392, 0.17698935698717833, 0.18115342617966235, 0.18271446600556374, 0.18181313597597182, 0.1831009869929403, 0.18481867713853717, 0.18771484098397195, 0.17748450301587582, 0.18820239300839603, 0.18696797615848482, 0.18505833204835653, 0.18934997590258718, 0.1866021070163697, 0.18568456103093922, 0.19053286593407393, 0.1882495549507439, 0.18738433602266014, 0.1885606930591166, 0.1878147090319544, 0.1898932030890137, 0.18913875613361597, 0.1977182028349489, 0.18751627206802368, 0.18781333602964878, 0.1896802969276905, 0.19650132511742413, 0.18741967296227813, 0.20079536805860698, 0.19018540997058153, 0.1935741459019482, 0.18837087391875684, 0.19167678384110332, 0.19208978419192135, 0.18830070295371115, 0.20819928916171193, 0.2049684631638229, 0.1800604620948434, 0.2745111398398876, 0.18236878491006792, 0.1812871100846678, 0.17635019798763096, 0.17746420996263623, 0.17583487485535443, 0.1792367580346763, 0.17632002686150372, 0.17745540896430612, 0.1742698869202286, 0.1757660610601306, 0.17891208990477026, 0.1956551370676607, 0.17528310092166066, 0.17489725397899747, 0.17676636390388012, 0.18220750289037824, 0.17530947481282055, 0.17636347492225468, 0.1777296259533614, 0.18065748387016356, 0.17695528804324567, 0.17858419590629637, 0.17655908595770597, 0.17322555603459477, 0.17706685094162822, 0.1777767480816692, 0.1759859120938927, 0.17551656300202012, 0.17625308386050165, 0.17769794492051005, 0.17464249511249363, 0.17553338804282248, 0.17737254686653614, 0.17557371500879526, 0.17429627408273518, 0.17285736999474466, 0.17615363001823425, 0.17733249417506158, 0.175321961985901, 0.1753493098076433, 0.17498820601031184]
[0.0013514191865227944, 0.0013612175898178834, 0.0014399423646603443, 0.0014907846988951282, 0.0021810437599558007, 0.001502819009154111, 0.0016208921158469695, 0.0013828061705151962, 0.001352944496167145, 0.0013344561150514108, 0.0013440070536651814, 0.0013534589544256298, 0.0013370541927888412, 0.0013709896272986895, 0.0013863917985252401, 0.001347630782875904, 0.001354550278579542, 0.0013832838379222061, 0.0013723301000950873, 0.0013611405030210582, 0.001372048488724255, 0.0013806853937923676, 0.0013610677118593639, 0.0013453199549816376, 0.0013651078054320444, 0.001353365991827707, 0.0013373561867678812, 0.0013544083637900132, 0.0013679803886934537, 0.0013687869447261789, 0.0013419063184265943, 0.0013606240232872178, 0.0013831130304741998, 0.0013745574103740527, 0.0013536594270244819, 0.0013784701149015463, 0.0013475889383360398, 0.0013423684042293666, 0.001357194891762595, 0.0013655546435349902, 0.001367800038798835, 0.0025499816352227167, 0.001398262596696384, 0.0013808083572504363, 0.0013458152868344572, 0.0013529759066453738, 0.0014407544580916332, 0.0013821154107274704, 0.0014343247586471397, 0.001451439868528829, 0.001381937751921919, 0.0013778367054525964, 0.001378492255471358, 0.0013909130862574707, 0.0014383280613683453, 0.001374493318775943, 0.0013930904733147039, 0.0013830052713986285, 0.001366519760845821, 0.0013692046970475553, 0.0015975164643005114, 0.0013647317442349917, 0.001372010519280452, 0.0014042901254237392, 0.0014163912093454554, 0.0014094041548524947, 0.0014193874960693048, 0.0014327029235545517, 0.001455153806077302, 0.0013758488605881845, 0.001458933279134853, 0.0014493641562673242, 0.0014345607135531513, 0.0014678292705626913, 0.001446527961367207, 0.0014394152017902266, 0.0014769989607292552, 0.0014592988755871622, 0.0014525917521136444, 0.0014617107989078806, 0.0014559279769918946, 0.001472040334023362, 0.0014661919080125269, 0.0015326992467825497, 0.0014536145121552223, 0.0014559173335631688, 0.0014703898986642675, 0.0015232660861815824, 0.001452865681878125, 0.0015565532407643952, 0.001474305503647919, 0.001500574774433707, 0.0014602393327035415, 0.0014858665414039017, 0.0014890680945110182, 0.001459695371734195, 0.0016139479779977669, 0.0015889028152234334, 0.001395817535618941, 0.0021279933320921523, 0.001413711510930759, 0.0014053264347648666, 0.0013670557983537284, 0.0013756915500979553, 0.0013630610453903443, 0.0013894322328269482, 0.0013668219136550677, 0.0013756233253046986, 0.0013509293559707644, 0.0013625276051172914, 0.0013869154256183742, 0.0015167064888965945, 0.001358783728074889, 0.0013557926665038564, 0.001370281890727753, 0.0014124612627161104, 0.0013589881768435702, 0.0013671587203275557, 0.0013777490383981503, 0.0014004456113966167, 0.001371746418939889, 0.0013843736116767162, 0.0013686750849434572, 0.001342833767710037, 0.0013726112476095211, 0.00137811432621449, 0.0013642318766968425, 0.001360593511643567, 0.0013663029756628034, 0.001377503448996202, 0.0013538177915697182, 0.0013607239383164533, 0.0013749809834615204, 0.0013610365504557772, 0.0013511339076181022, 0.0013399796123623616, 0.0013655320156452267, 0.0013746704974810975, 0.0013590849766348914, 0.0013592969752530487, 0.0013564977210101692]
[739.9628553247072, 734.6364074929339, 694.4722403773997, 670.7876735930644, 458.49607346725827, 665.4161239036151, 616.9442063560579, 723.1671519280443, 739.1286211910192, 749.3689666680978, 744.0437141106847, 738.8476737548145, 747.9128410750426, 729.4001209697961, 721.2968232095283, 742.0430081494247, 738.2524043689664, 722.9174321172387, 728.6876531606434, 734.6780128726572, 728.8372154615399, 724.2779596974453, 734.7173041331597, 743.3175998743355, 732.5428775813856, 738.8984251403497, 747.743951756635, 738.3297583911256, 731.0046315467222, 730.5738879618308, 745.2085039531786, 734.9568895483976, 723.0067087555024, 727.5068996411536, 738.738252795335, 725.4419150548088, 742.0660496328861, 744.9519795380501, 736.8138548630225, 732.3031741969075, 731.1010174251588, 392.159687029534, 715.1732459715776, 724.2134614475183, 743.0440193261124, 739.1114616959017, 694.0807952276342, 723.5285796239363, 697.1921762984861, 688.9710153914913, 723.6215948288973, 725.7754101357872, 725.4302634134586, 718.9521831955004, 695.2516792647816, 727.5408227451782, 717.8284678242119, 723.0630429837071, 731.7859782584044, 730.3509856169212, 625.9716393207002, 732.7447347980873, 728.8573855282449, 712.1035617182409, 706.0196317245724, 709.5196906842225, 704.5292443179117, 697.9814053279022, 687.2125790576924, 726.8240201707135, 685.4323047541967, 689.9577277910532, 697.0775029264381, 681.2781432111997, 691.3105219582727, 694.7265797639778, 677.0485468089015, 685.2605841950237, 688.4246716566544, 684.1298571147942, 686.8471626365122, 679.3292119019678, 682.0389572027676, 652.4437211665663, 687.9402975396384, 686.8521838067754, 680.0917232282544, 656.4841225519111, 688.2948730038803, 642.4450984464354, 678.2854690060301, 666.4113092114213, 684.8192468206995, 673.0079533624621, 671.5609606344974, 685.0744472882361, 619.5986572259789, 629.3651130949622, 716.4260187894649, 469.9262845043047, 707.3578960544929, 711.578445592476, 731.499036984625, 726.9071325827331, 733.6428572893642, 719.7184406506774, 731.6242079598095, 726.943183940634, 740.231156855282, 733.9300842377548, 721.0244990635512, 659.3233478729962, 735.9522927293146, 737.5759027954261, 729.7768486664469, 707.9840179666501, 735.841574665228, 731.4439685250381, 725.8215916903539, 714.0584338742966, 728.9977113793514, 722.3483542053557, 730.6335966810648, 744.6938139672502, 728.538398429676, 725.6292028738113, 733.0132194398345, 734.9733711371459, 731.902087467015, 725.9509954248813, 738.6518379556269, 734.902923246315, 727.2827857462405, 734.7341257405063, 740.1190913511216, 746.280011109286, 732.3153090097932, 727.4470513714873, 735.7891649100635, 735.674409791016, 737.1925396640628]
Elapsed: 0.18329726883329134~0.018477900960584284
Time per graph: 0.0014209090607231886~0.0001432395423301107
Speed: 708.6016439465857~48.94178193189471
Total Time: 0.1756
best val loss: 0.18356070424928222 test_score: 0.9147

Testing...
Test loss: 0.3104 score: 0.9070 time: 0.17s
test Score 0.9070
Epoch Time List: [0.5951955972705036, 0.6030883467756212, 0.6364163088146597, 0.6560653699561954, 0.7236993659753352, 0.6352020287886262, 0.6751503441482782, 0.7078485200181603, 0.5913547929376364, 0.5984682748094201, 0.6701170692685992, 0.5996852822136134, 0.6827889462001622, 0.5978107212577015, 0.6046172678470612, 0.6023350378964096, 0.5958906090818346, 0.6035980221349746, 0.6020589517429471, 0.6006080771330744, 0.6002606281545013, 0.6017543750349432, 0.5962904668413103, 0.5933174269739538, 0.5947386990301311, 0.5968377240933478, 0.6275651280302554, 0.5942206801846623, 0.5956726777367294, 0.6001913489308208, 0.5972390151582658, 0.5999269247986376, 0.6012729380745441, 0.6006568758748472, 0.5992916547693312, 0.5997384889051318, 0.5933755510486662, 0.59132793196477, 0.5965314097702503, 0.5977786020375788, 0.5983967510983348, 0.7489399467594922, 0.607982984976843, 0.6117924840655178, 0.6061643029097468, 0.5995941839646548, 0.6107522069942206, 0.6404708989430219, 0.6251309788785875, 0.6340952522587031, 0.61546571389772, 0.6066175862215459, 0.6015546696726233, 0.612024205038324, 0.6404014183208346, 0.6071852489840239, 0.6073016948066652, 0.6044499205891043, 0.6064286159817129, 0.6044736271724105, 0.6326234920416027, 0.6479995411355048, 0.6104367221705616, 0.6216021068394184, 0.6205655941739678, 0.6211723750457168, 0.6229336482938379, 0.6238523579668254, 0.627266061026603, 0.6174731319770217, 0.6250697406940162, 0.6234034409280866, 0.6234678619075567, 0.6299505901988596, 0.6252193879336119, 0.6281904468778521, 0.628820079844445, 0.6282231730874628, 0.6260667839087546, 0.6262384778819978, 0.6275171262677759, 0.625542108900845, 0.626867167185992, 0.6363716330379248, 0.6427037187386304, 0.6347304980736226, 0.6283730028662831, 0.6606172157917172, 0.6409535843413323, 0.660975752864033, 0.6694606989622116, 0.6705979569815099, 0.7344273896887898, 0.6287880791351199, 0.6475581370759755, 0.7141498439013958, 0.6567419427447021, 0.7436038388404995, 0.6045988493133336, 0.7071227061096579, 0.6085692690685391, 0.6165371190290898, 0.5918028119485825, 0.5906970668584108, 0.5953304679132998, 0.5907733370549977, 0.5869637760333717, 0.5870265369303524, 0.5879928418435156, 0.5853672339580953, 0.5833898619748652, 0.610967607004568, 0.6060040278825909, 0.5846078931353986, 0.5902921939268708, 0.5875636669807136, 0.5928011210635304, 0.5947421493474394, 0.5838342430070043, 0.5940975828561932, 0.5887690982781351, 0.5894718409981579, 0.5788298940751702, 0.5823687871452421, 0.583727850113064, 0.5824614497832954, 0.5861007529310882, 0.5853447283152491, 0.5854837377555668, 0.585927426116541, 0.5834655880462378, 0.5848979002330452, 0.5838128088507801, 0.5867467287462205, 0.5820777700282633, 0.5815801199059933, 0.5837258049286902, 0.5841583961155266, 0.5852057649753988, 0.5854112149681896, 0.583212606376037]
Total Epoch List: [70, 71]
Total Time List: [0.1779202080797404, 0.17560440208762884]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7dddd1dd9ff0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 0.18s
Epoch 2/1000, LR 0.000020
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.18s
Epoch 5/1000, LR 0.000110
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.16s
Epoch 8/1000, LR 0.000200
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.16s
Epoch 11/1000, LR 0.000290
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.18s
Epoch 12/1000, LR 0.000290
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.29s
Val loss: 0.6902 score: 0.5426 time: 0.18s
Test loss: 0.6898 score: 0.5938 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.28s
Val loss: 0.6893 score: 0.8992 time: 0.18s
Test loss: 0.6887 score: 0.8984 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.29s
Val loss: 0.6879 score: 0.8140 time: 0.18s
Test loss: 0.6871 score: 0.8516 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.5890;  Loss pred: 0.5890; Loss self: 0.0000; time: 0.28s
Val loss: 0.6865 score: 0.5504 time: 0.19s
Test loss: 0.6854 score: 0.5781 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.29s
Val loss: 0.6840 score: 0.5426 time: 0.18s
Test loss: 0.6825 score: 0.5781 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.29s
Val loss: 0.6815 score: 0.5194 time: 0.19s
Test loss: 0.6795 score: 0.5625 time: 0.18s
Epoch 22/1000, LR 0.000290
Train loss: 0.5455;  Loss pred: 0.5455; Loss self: 0.0000; time: 0.30s
Val loss: 0.6767 score: 0.5426 time: 0.19s
Test loss: 0.6739 score: 0.5781 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.29s
Val loss: 0.6693 score: 0.6202 time: 0.18s
Test loss: 0.6653 score: 0.6484 time: 0.16s
Epoch 24/1000, LR 0.000290
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.29s
Val loss: 0.6646 score: 0.5581 time: 0.18s
Test loss: 0.6593 score: 0.5781 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 0.4912;  Loss pred: 0.4912; Loss self: 0.0000; time: 0.28s
Val loss: 0.6484 score: 0.7209 time: 0.18s
Test loss: 0.6411 score: 0.7422 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.4702;  Loss pred: 0.4702; Loss self: 0.0000; time: 0.28s
Val loss: 0.6272 score: 0.8062 time: 0.18s
Test loss: 0.6176 score: 0.8594 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 0.4476;  Loss pred: 0.4476; Loss self: 0.0000; time: 0.28s
Val loss: 0.6055 score: 0.8295 time: 0.18s
Test loss: 0.5931 score: 0.8828 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.4289;  Loss pred: 0.4289; Loss self: 0.0000; time: 0.28s
Val loss: 0.5721 score: 0.8837 time: 0.18s
Test loss: 0.5573 score: 0.9141 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 0.4071;  Loss pred: 0.4071; Loss self: 0.0000; time: 0.28s
Val loss: 0.5505 score: 0.8760 time: 0.19s
Test loss: 0.5332 score: 0.9062 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.3914;  Loss pred: 0.3914; Loss self: 0.0000; time: 0.29s
Val loss: 0.5167 score: 0.8992 time: 0.19s
Test loss: 0.4978 score: 0.9141 time: 0.18s
Epoch 31/1000, LR 0.000290
Train loss: 0.3719;  Loss pred: 0.3719; Loss self: 0.0000; time: 0.29s
Val loss: 0.4893 score: 0.8992 time: 0.19s
Test loss: 0.4693 score: 0.9141 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.28s
Val loss: 0.4427 score: 0.9225 time: 0.18s
Test loss: 0.4248 score: 0.9141 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3294;  Loss pred: 0.3294; Loss self: 0.0000; time: 0.28s
Val loss: 0.4122 score: 0.9302 time: 0.18s
Test loss: 0.3955 score: 0.9062 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.3105;  Loss pred: 0.3105; Loss self: 0.0000; time: 0.29s
Val loss: 0.3813 score: 0.9535 time: 0.18s
Test loss: 0.3660 score: 0.9141 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 0.3003;  Loss pred: 0.3003; Loss self: 0.0000; time: 0.28s
Val loss: 0.3499 score: 0.9535 time: 0.18s
Test loss: 0.3363 score: 0.9062 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.2773;  Loss pred: 0.2773; Loss self: 0.0000; time: 0.28s
Val loss: 0.3276 score: 0.9457 time: 0.18s
Test loss: 0.3160 score: 0.9141 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.2668;  Loss pred: 0.2668; Loss self: 0.0000; time: 0.28s
Val loss: 0.3002 score: 0.9457 time: 0.18s
Test loss: 0.2913 score: 0.9141 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.2501;  Loss pred: 0.2501; Loss self: 0.0000; time: 0.28s
Val loss: 0.2872 score: 0.9535 time: 0.18s
Test loss: 0.2796 score: 0.9062 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 0.2389;  Loss pred: 0.2389; Loss self: 0.0000; time: 0.28s
Val loss: 0.2787 score: 0.9457 time: 0.18s
Test loss: 0.2739 score: 0.9141 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 0.2265;  Loss pred: 0.2265; Loss self: 0.0000; time: 0.38s
Val loss: 0.2550 score: 0.9612 time: 0.18s
Test loss: 0.2542 score: 0.9062 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.28s
Val loss: 0.2385 score: 0.9535 time: 0.18s
Test loss: 0.2406 score: 0.9062 time: 0.16s
Epoch 42/1000, LR 0.000289
Train loss: 0.1920;  Loss pred: 0.1920; Loss self: 0.0000; time: 0.29s
Val loss: 0.2429 score: 0.9380 time: 0.18s
Test loss: 0.2490 score: 0.9141 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.1880;  Loss pred: 0.1880; Loss self: 0.0000; time: 0.28s
Val loss: 0.2362 score: 0.9380 time: 0.19s
Test loss: 0.2462 score: 0.9141 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 0.1660;  Loss pred: 0.1660; Loss self: 0.0000; time: 0.28s
Val loss: 0.2221 score: 0.9380 time: 0.18s
Test loss: 0.2366 score: 0.9062 time: 0.16s
Epoch 45/1000, LR 0.000289
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.32s
Val loss: 0.2132 score: 0.9380 time: 0.17s
Test loss: 0.2309 score: 0.9141 time: 0.16s
Epoch 46/1000, LR 0.000289
Train loss: 0.1440;  Loss pred: 0.1440; Loss self: 0.0000; time: 0.28s
Val loss: 0.2078 score: 0.9457 time: 0.18s
Test loss: 0.2278 score: 0.9141 time: 0.17s
Epoch 47/1000, LR 0.000289
Train loss: 0.1383;  Loss pred: 0.1383; Loss self: 0.0000; time: 0.27s
Val loss: 0.1896 score: 0.9535 time: 0.27s
Test loss: 0.2103 score: 0.9219 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.27s
Val loss: 0.1806 score: 0.9302 time: 0.17s
Test loss: 0.2023 score: 0.9141 time: 0.16s
Epoch 49/1000, LR 0.000289
Train loss: 0.1223;  Loss pred: 0.1223; Loss self: 0.0000; time: 0.27s
Val loss: 0.1734 score: 0.9302 time: 0.18s
Test loss: 0.1974 score: 0.9141 time: 0.16s
Epoch 50/1000, LR 0.000289
Train loss: 0.1156;  Loss pred: 0.1156; Loss self: 0.0000; time: 0.27s
Val loss: 0.2008 score: 0.9225 time: 0.18s
Test loss: 0.2122 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 0.27s
Val loss: 0.1794 score: 0.9302 time: 0.17s
Test loss: 0.2004 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.27s
Val loss: 0.1649 score: 0.9302 time: 0.17s
Test loss: 0.1948 score: 0.9297 time: 0.16s
Epoch 53/1000, LR 0.000289
Train loss: 0.1066;  Loss pred: 0.1066; Loss self: 0.0000; time: 0.27s
Val loss: 0.1585 score: 0.9535 time: 0.17s
Test loss: 0.2045 score: 0.9141 time: 0.16s
Epoch 54/1000, LR 0.000289
Train loss: 0.1120;  Loss pred: 0.1120; Loss self: 0.0000; time: 0.27s
Val loss: 0.1648 score: 0.9535 time: 0.18s
Test loss: 0.2156 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 0.27s
Val loss: 0.1574 score: 0.9535 time: 0.17s
Test loss: 0.2087 score: 0.9141 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.27s
Val loss: 0.1551 score: 0.9535 time: 0.17s
Test loss: 0.2077 score: 0.9141 time: 0.16s
Epoch 57/1000, LR 0.000288
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.27s
Val loss: 0.1668 score: 0.9535 time: 0.18s
Test loss: 0.2251 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.27s
Val loss: 0.1599 score: 0.9535 time: 0.18s
Test loss: 0.2185 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.27s
Val loss: 0.1590 score: 0.9535 time: 0.18s
Test loss: 0.2209 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.27s
Val loss: 0.1630 score: 0.9535 time: 0.17s
Test loss: 0.2292 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.27s
Val loss: 0.1697 score: 0.9535 time: 0.17s
Test loss: 0.2390 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.27s
Val loss: 0.2003 score: 0.9302 time: 0.18s
Test loss: 0.2734 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.27s
Val loss: 0.1859 score: 0.9457 time: 0.18s
Test loss: 0.2625 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.27s
Val loss: 0.2038 score: 0.9302 time: 0.17s
Test loss: 0.2814 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.27s
Val loss: 0.1777 score: 0.9457 time: 0.18s
Test loss: 0.2586 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.27s
Val loss: 0.1695 score: 0.9535 time: 0.18s
Test loss: 0.2569 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.27s
Val loss: 0.1859 score: 0.9457 time: 0.17s
Test loss: 0.2750 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0566;  Loss pred: 0.0566; Loss self: 0.0000; time: 0.27s
Val loss: 0.1507 score: 0.9535 time: 0.18s
Test loss: 0.2405 score: 0.9219 time: 0.16s
Epoch 69/1000, LR 0.000288
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.27s
Val loss: 0.1674 score: 0.9535 time: 0.17s
Test loss: 0.2586 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.27s
Val loss: 0.1774 score: 0.9535 time: 0.17s
Test loss: 0.2630 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.27s
Val loss: 0.1675 score: 0.9535 time: 0.17s
Test loss: 0.2531 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0533;  Loss pred: 0.0533; Loss self: 0.0000; time: 0.27s
Val loss: 0.1522 score: 0.9535 time: 0.17s
Test loss: 0.2365 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.27s
Val loss: 0.1629 score: 0.9457 time: 0.17s
Test loss: 0.2338 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.27s
Val loss: 0.1981 score: 0.8992 time: 0.17s
Test loss: 0.2593 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.27s
Val loss: 0.1564 score: 0.9535 time: 0.17s
Test loss: 0.2323 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.26s
Val loss: 0.1564 score: 0.9535 time: 0.18s
Test loss: 0.2311 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.27s
Val loss: 0.1576 score: 0.9535 time: 0.17s
Test loss: 0.2320 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.27s
Val loss: 0.1667 score: 0.9612 time: 0.17s
Test loss: 0.2500 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.27s
Val loss: 0.1768 score: 0.9612 time: 0.17s
Test loss: 0.2566 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.27s
Val loss: 0.1699 score: 0.9612 time: 0.18s
Test loss: 0.2481 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.27s
Val loss: 0.1630 score: 0.9535 time: 0.17s
Test loss: 0.2407 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.27s
Val loss: 0.1613 score: 0.9535 time: 0.17s
Test loss: 0.2476 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.27s
Val loss: 0.1679 score: 0.9612 time: 0.18s
Test loss: 0.2600 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.27s
Val loss: 0.1666 score: 0.9612 time: 0.18s
Test loss: 0.2582 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.28s
Val loss: 0.1814 score: 0.9612 time: 0.17s
Test loss: 0.2786 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.28s
Val loss: 0.1745 score: 0.9535 time: 0.17s
Test loss: 0.2614 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.27s
Val loss: 0.1752 score: 0.9535 time: 0.17s
Test loss: 0.2644 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.27s
Val loss: 0.1768 score: 0.9535 time: 0.18s
Test loss: 0.2704 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 067,   Train_Loss: 0.0566,   Val_Loss: 0.1507,   Val_Precision: 0.9836,   Val_Recall: 0.9231,   Val_accuracy: 0.9524,   Val_Score: 0.9535,   Val_Loss: 0.1507,   Test_Precision: 0.9655,   Test_Recall: 0.8750,   Test_accuracy: 0.9180,   Test_Score: 0.9219,   Test_loss: 0.2405


[0.17433307506144047, 0.17559706908650696, 0.18575256504118443, 0.19231122615747154, 0.2813546450342983, 0.1938636521808803, 0.20909508294425905, 0.17838199599646032, 0.1745298400055617, 0.172144838841632, 0.1733769099228084, 0.17459620512090623, 0.1724799908697605, 0.17685766192153096, 0.17884454200975597, 0.1738443709909916, 0.1747369859367609, 0.1784436150919646, 0.17703058291226625, 0.1755871248897165, 0.17699425504542887, 0.17810841579921544, 0.17557773482985795, 0.17354627419263124, 0.1760989069007337, 0.1745842129457742, 0.17251894809305668, 0.17471867892891169, 0.17646947014145553, 0.17657351586967707, 0.17310591507703066, 0.1755204990040511, 0.17842158093117177, 0.1773179059382528, 0.17462206608615816, 0.17782264482229948, 0.17383897304534912, 0.17316552414558828, 0.17507814103737473, 0.17615654901601374, 0.1764462050050497, 0.3289476309437305, 0.18037587497383356, 0.1781242780853063, 0.17361017200164497, 0.17453389195725322, 0.1858573250938207, 0.17829288798384368, 0.18502789386548102, 0.18723574304021895, 0.17826996999792755, 0.17774093500338495, 0.17782550095580518, 0.17942778812721372, 0.18554431991651654, 0.17730963812209666, 0.1797086710575968, 0.17840768001042306, 0.1762810491491109, 0.17662740591913462, 0.20607962389476597, 0.17605039500631392, 0.17698935698717833, 0.18115342617966235, 0.18271446600556374, 0.18181313597597182, 0.1831009869929403, 0.18481867713853717, 0.18771484098397195, 0.17748450301587582, 0.18820239300839603, 0.18696797615848482, 0.18505833204835653, 0.18934997590258718, 0.1866021070163697, 0.18568456103093922, 0.19053286593407393, 0.1882495549507439, 0.18738433602266014, 0.1885606930591166, 0.1878147090319544, 0.1898932030890137, 0.18913875613361597, 0.1977182028349489, 0.18751627206802368, 0.18781333602964878, 0.1896802969276905, 0.19650132511742413, 0.18741967296227813, 0.20079536805860698, 0.19018540997058153, 0.1935741459019482, 0.18837087391875684, 0.19167678384110332, 0.19208978419192135, 0.18830070295371115, 0.20819928916171193, 0.2049684631638229, 0.1800604620948434, 0.2745111398398876, 0.18236878491006792, 0.1812871100846678, 0.17635019798763096, 0.17746420996263623, 0.17583487485535443, 0.1792367580346763, 0.17632002686150372, 0.17745540896430612, 0.1742698869202286, 0.1757660610601306, 0.17891208990477026, 0.1956551370676607, 0.17528310092166066, 0.17489725397899747, 0.17676636390388012, 0.18220750289037824, 0.17530947481282055, 0.17636347492225468, 0.1777296259533614, 0.18065748387016356, 0.17695528804324567, 0.17858419590629637, 0.17655908595770597, 0.17322555603459477, 0.17706685094162822, 0.1777767480816692, 0.1759859120938927, 0.17551656300202012, 0.17625308386050165, 0.17769794492051005, 0.17464249511249363, 0.17553338804282248, 0.17737254686653614, 0.17557371500879526, 0.17429627408273518, 0.17285736999474466, 0.17615363001823425, 0.17733249417506158, 0.175321961985901, 0.1753493098076433, 0.17498820601031184, 0.18058607610873878, 0.1752569740638137, 0.1758577038999647, 0.18953624600544572, 0.17005704995244741, 0.17366255400702357, 0.16990922507829964, 0.1731401989236474, 0.17127958708442748, 0.16961016412824392, 0.182485616998747, 0.17670751409605145, 0.17191569600254297, 0.16928153182379901, 0.1748605479951948, 0.17173460498452187, 0.17019158508628607, 0.17307132808491588, 0.17694649496115744, 0.16720068687573075, 0.18298365199007094, 0.17448871885426342, 0.16590637993067503, 0.1679852951783687, 0.16735867806710303, 0.16535387700423598, 0.1698054790031165, 0.17216658010147512, 0.17833255790174007, 0.18739050906151533, 0.173840792151168, 0.16791387903504074, 0.16733624297194183, 0.16964435018599033, 0.16718162200413644, 0.16631773090921342, 0.1678193281404674, 0.167743458179757, 0.16559529188089073, 0.16517748916521668, 0.16801945609040558, 0.2689722238574177, 0.17261768784374, 0.16608961508609354, 0.16538927610963583, 0.17087283311411738, 0.16112763807177544, 0.16304366709664464, 0.16489256196655333, 0.16210201499052346, 0.16375714680179954, 0.1638608630746603, 0.16569309099577367, 0.16229394497349858, 0.16013223794288933, 0.16226087999530137, 0.16556974803097546, 0.16568919993005693, 0.1636746998410672, 0.16314816917292774, 0.1651780440006405, 0.16358365793712437, 0.1629808838479221, 0.16641674400307238, 0.16408757586032152, 0.163296259008348, 0.16726179188117385, 0.16420724010095, 0.16152796102687716, 0.16315155918709934, 0.16532801906578243, 0.16292299493215978, 0.1621960869524628, 0.1634292679373175, 0.16329152206890285, 0.1623471148777753, 0.1614868100732565, 0.163834871025756, 0.16337650013156235, 0.1610582061111927, 0.16175531689077616, 0.1640294878743589, 0.16255817096680403, 0.16069384198635817, 0.1616567769087851, 0.16265140403993428, 0.16392723191529512, 0.16304165008477867]
[0.0013514191865227944, 0.0013612175898178834, 0.0014399423646603443, 0.0014907846988951282, 0.0021810437599558007, 0.001502819009154111, 0.0016208921158469695, 0.0013828061705151962, 0.001352944496167145, 0.0013344561150514108, 0.0013440070536651814, 0.0013534589544256298, 0.0013370541927888412, 0.0013709896272986895, 0.0013863917985252401, 0.001347630782875904, 0.001354550278579542, 0.0013832838379222061, 0.0013723301000950873, 0.0013611405030210582, 0.001372048488724255, 0.0013806853937923676, 0.0013610677118593639, 0.0013453199549816376, 0.0013651078054320444, 0.001353365991827707, 0.0013373561867678812, 0.0013544083637900132, 0.0013679803886934537, 0.0013687869447261789, 0.0013419063184265943, 0.0013606240232872178, 0.0013831130304741998, 0.0013745574103740527, 0.0013536594270244819, 0.0013784701149015463, 0.0013475889383360398, 0.0013423684042293666, 0.001357194891762595, 0.0013655546435349902, 0.001367800038798835, 0.0025499816352227167, 0.001398262596696384, 0.0013808083572504363, 0.0013458152868344572, 0.0013529759066453738, 0.0014407544580916332, 0.0013821154107274704, 0.0014343247586471397, 0.001451439868528829, 0.001381937751921919, 0.0013778367054525964, 0.001378492255471358, 0.0013909130862574707, 0.0014383280613683453, 0.001374493318775943, 0.0013930904733147039, 0.0013830052713986285, 0.001366519760845821, 0.0013692046970475553, 0.0015975164643005114, 0.0013647317442349917, 0.001372010519280452, 0.0014042901254237392, 0.0014163912093454554, 0.0014094041548524947, 0.0014193874960693048, 0.0014327029235545517, 0.001455153806077302, 0.0013758488605881845, 0.001458933279134853, 0.0014493641562673242, 0.0014345607135531513, 0.0014678292705626913, 0.001446527961367207, 0.0014394152017902266, 0.0014769989607292552, 0.0014592988755871622, 0.0014525917521136444, 0.0014617107989078806, 0.0014559279769918946, 0.001472040334023362, 0.0014661919080125269, 0.0015326992467825497, 0.0014536145121552223, 0.0014559173335631688, 0.0014703898986642675, 0.0015232660861815824, 0.001452865681878125, 0.0015565532407643952, 0.001474305503647919, 0.001500574774433707, 0.0014602393327035415, 0.0014858665414039017, 0.0014890680945110182, 0.001459695371734195, 0.0016139479779977669, 0.0015889028152234334, 0.001395817535618941, 0.0021279933320921523, 0.001413711510930759, 0.0014053264347648666, 0.0013670557983537284, 0.0013756915500979553, 0.0013630610453903443, 0.0013894322328269482, 0.0013668219136550677, 0.0013756233253046986, 0.0013509293559707644, 0.0013625276051172914, 0.0013869154256183742, 0.0015167064888965945, 0.001358783728074889, 0.0013557926665038564, 0.001370281890727753, 0.0014124612627161104, 0.0013589881768435702, 0.0013671587203275557, 0.0013777490383981503, 0.0014004456113966167, 0.001371746418939889, 0.0013843736116767162, 0.0013686750849434572, 0.001342833767710037, 0.0013726112476095211, 0.00137811432621449, 0.0013642318766968425, 0.001360593511643567, 0.0013663029756628034, 0.001377503448996202, 0.0013538177915697182, 0.0013607239383164533, 0.0013749809834615204, 0.0013610365504557772, 0.0013511339076181022, 0.0013399796123623616, 0.0013655320156452267, 0.0013746704974810975, 0.0013590849766348914, 0.0013592969752530487, 0.0013564977210101692, 0.0014108287195995217, 0.0013691951098735444, 0.0013738883117184741, 0.0014807519219175447, 0.0013285707027534954, 0.0013567387031798717, 0.001327415820924216, 0.0013526578040909953, 0.0013381217740970897, 0.0013250794072519056, 0.0014256688828027109, 0.001380527453875402, 0.001343091375019867, 0.0013225119673734298, 0.0013660980312124593, 0.001341676601441577, 0.00132962175848661, 0.0013521197506634053, 0.0013823944918840425, 0.0013062553662166465, 0.0014295597811724292, 0.001363193116048933, 0.0012961435932083987, 0.0013123851185810054, 0.0013074896723992424, 0.0012918271640955936, 0.0013266053047118476, 0.0013450514070427744, 0.0013932231086073443, 0.0014639883520430885, 0.001358131188681, 0.0013118271799612558, 0.0013073143982182955, 0.0013253464858280495, 0.001306106421907316, 0.0012993572727282299, 0.0013110885010974016, 0.0013104957670293516, 0.0012937132178194588, 0.0012904491341032553, 0.0013126520007062936, 0.002101345498886076, 0.0013485756862792186, 0.0012975751178601058, 0.00129210371960653, 0.001334944008704042, 0.0012588096724357456, 0.0012737786491925362, 0.001288223140363698, 0.0012664219921134645, 0.0012793527093890589, 0.0012801629927707836, 0.0012944772734044818, 0.0012679214451054577, 0.001251033108928823, 0.001267663124963292, 0.0012935136564919958, 0.0012944468744535698, 0.0012787085925083375, 0.001274595071663498, 0.001290453468755004, 0.0012779973276337842, 0.0012732881550618913, 0.001300130812524003, 0.0012819341864087619, 0.0012757520235027187, 0.0013067327490716707, 0.001282869063288672, 0.0012619371955224779, 0.0012746215561492136, 0.0012916251489514252, 0.0012728358979074983, 0.0012671569293161156, 0.001276791155760293, 0.0012757150161633035, 0.0012683368349826196, 0.0012616157036973163, 0.0012799599298887188, 0.0012763789072778309, 0.001258267235243693, 0.0012637134132091887, 0.0012814803740184288, 0.0012699857106781565, 0.0012554206405184232, 0.0012629435695998836, 0.0012707140940619865, 0.0012806814993382432, 0.0012737628912873333]
[739.9628553247072, 734.6364074929339, 694.4722403773997, 670.7876735930644, 458.49607346725827, 665.4161239036151, 616.9442063560579, 723.1671519280443, 739.1286211910192, 749.3689666680978, 744.0437141106847, 738.8476737548145, 747.9128410750426, 729.4001209697961, 721.2968232095283, 742.0430081494247, 738.2524043689664, 722.9174321172387, 728.6876531606434, 734.6780128726572, 728.8372154615399, 724.2779596974453, 734.7173041331597, 743.3175998743355, 732.5428775813856, 738.8984251403497, 747.743951756635, 738.3297583911256, 731.0046315467222, 730.5738879618308, 745.2085039531786, 734.9568895483976, 723.0067087555024, 727.5068996411536, 738.738252795335, 725.4419150548088, 742.0660496328861, 744.9519795380501, 736.8138548630225, 732.3031741969075, 731.1010174251588, 392.159687029534, 715.1732459715776, 724.2134614475183, 743.0440193261124, 739.1114616959017, 694.0807952276342, 723.5285796239363, 697.1921762984861, 688.9710153914913, 723.6215948288973, 725.7754101357872, 725.4302634134586, 718.9521831955004, 695.2516792647816, 727.5408227451782, 717.8284678242119, 723.0630429837071, 731.7859782584044, 730.3509856169212, 625.9716393207002, 732.7447347980873, 728.8573855282449, 712.1035617182409, 706.0196317245724, 709.5196906842225, 704.5292443179117, 697.9814053279022, 687.2125790576924, 726.8240201707135, 685.4323047541967, 689.9577277910532, 697.0775029264381, 681.2781432111997, 691.3105219582727, 694.7265797639778, 677.0485468089015, 685.2605841950237, 688.4246716566544, 684.1298571147942, 686.8471626365122, 679.3292119019678, 682.0389572027676, 652.4437211665663, 687.9402975396384, 686.8521838067754, 680.0917232282544, 656.4841225519111, 688.2948730038803, 642.4450984464354, 678.2854690060301, 666.4113092114213, 684.8192468206995, 673.0079533624621, 671.5609606344974, 685.0744472882361, 619.5986572259789, 629.3651130949622, 716.4260187894649, 469.9262845043047, 707.3578960544929, 711.578445592476, 731.499036984625, 726.9071325827331, 733.6428572893642, 719.7184406506774, 731.6242079598095, 726.943183940634, 740.231156855282, 733.9300842377548, 721.0244990635512, 659.3233478729962, 735.9522927293146, 737.5759027954261, 729.7768486664469, 707.9840179666501, 735.841574665228, 731.4439685250381, 725.8215916903539, 714.0584338742966, 728.9977113793514, 722.3483542053557, 730.6335966810648, 744.6938139672502, 728.538398429676, 725.6292028738113, 733.0132194398345, 734.9733711371459, 731.902087467015, 725.9509954248813, 738.6518379556269, 734.902923246315, 727.2827857462405, 734.7341257405063, 740.1190913511216, 746.280011109286, 732.3153090097932, 727.4470513714873, 735.7891649100635, 735.674409791016, 737.1925396640628, 708.8032630097439, 730.3560995717825, 727.8612034694358, 675.3325693509954, 752.688583247753, 737.061600480799, 753.3434393630686, 739.2852774556783, 747.3161406963574, 754.6717536527937, 701.4251430066342, 724.3608210708238, 744.5509803718365, 756.1368249741032, 732.0118887167016, 745.3360958412337, 752.0935887347474, 739.5794636601966, 723.3825119175038, 765.5470942839725, 699.5160420502785, 733.5717795424256, 771.519456053984, 761.9714562759088, 764.8243967885428, 774.0973620879838, 753.8037097003847, 743.4660078893169, 717.7601303208305, 683.0655439330761, 736.3058946987202, 762.2955334936227, 764.9269382811616, 754.5196751891037, 765.6343948908033, 769.6112693472854, 762.7250175430448, 763.0699962250261, 772.9688359260102, 774.9239962835954, 761.8165358845557, 475.88556975999444, 741.5230825931952, 770.6682921364495, 773.9316781043855, 749.0950882432858, 794.4012680368436, 785.0657574091952, 776.2630313547037, 789.6262116635806, 781.6452747245435, 781.1505297740258, 772.5125968183241, 788.6923940440382, 799.3393562990783, 788.8531111362549, 773.0880883870961, 772.5307385999399, 782.0390086207071, 784.5628954887462, 774.9213933027543, 782.4742496539511, 785.3681792487832, 769.1533731583936, 780.0712474962709, 783.8513924158938, 765.2674203737682, 779.5027790571792, 792.4324630006421, 784.5465935952953, 774.2184338945598, 785.6472320147226, 789.168237070447, 783.2134452752599, 783.8741312362123, 788.4340913379712, 792.6343949820694, 781.2744576206703, 783.4664097769588, 794.7437332788265, 791.318656229587, 780.3474951896682, 787.4104343000935, 796.5457693821668, 791.8010147649055, 786.9590844022063, 780.8342671591043, 785.0754695713786]
Elapsed: 0.17776869934084763~0.017814704150976367
Time per graph: 0.0013819829411671719~0.00013642177302346223
Speed: 728.6104800599761~52.327552338692996
Total Time: 0.1638
best val loss: 0.15071660228246866 test_score: 0.9219

Testing...
Test loss: 0.2542 score: 0.9062 time: 0.16s
test Score 0.9062
Epoch Time List: [0.5951955972705036, 0.6030883467756212, 0.6364163088146597, 0.6560653699561954, 0.7236993659753352, 0.6352020287886262, 0.6751503441482782, 0.7078485200181603, 0.5913547929376364, 0.5984682748094201, 0.6701170692685992, 0.5996852822136134, 0.6827889462001622, 0.5978107212577015, 0.6046172678470612, 0.6023350378964096, 0.5958906090818346, 0.6035980221349746, 0.6020589517429471, 0.6006080771330744, 0.6002606281545013, 0.6017543750349432, 0.5962904668413103, 0.5933174269739538, 0.5947386990301311, 0.5968377240933478, 0.6275651280302554, 0.5942206801846623, 0.5956726777367294, 0.6001913489308208, 0.5972390151582658, 0.5999269247986376, 0.6012729380745441, 0.6006568758748472, 0.5992916547693312, 0.5997384889051318, 0.5933755510486662, 0.59132793196477, 0.5965314097702503, 0.5977786020375788, 0.5983967510983348, 0.7489399467594922, 0.607982984976843, 0.6117924840655178, 0.6061643029097468, 0.5995941839646548, 0.6107522069942206, 0.6404708989430219, 0.6251309788785875, 0.6340952522587031, 0.61546571389772, 0.6066175862215459, 0.6015546696726233, 0.612024205038324, 0.6404014183208346, 0.6071852489840239, 0.6073016948066652, 0.6044499205891043, 0.6064286159817129, 0.6044736271724105, 0.6326234920416027, 0.6479995411355048, 0.6104367221705616, 0.6216021068394184, 0.6205655941739678, 0.6211723750457168, 0.6229336482938379, 0.6238523579668254, 0.627266061026603, 0.6174731319770217, 0.6250697406940162, 0.6234034409280866, 0.6234678619075567, 0.6299505901988596, 0.6252193879336119, 0.6281904468778521, 0.628820079844445, 0.6282231730874628, 0.6260667839087546, 0.6262384778819978, 0.6275171262677759, 0.625542108900845, 0.626867167185992, 0.6363716330379248, 0.6427037187386304, 0.6347304980736226, 0.6283730028662831, 0.6606172157917172, 0.6409535843413323, 0.660975752864033, 0.6694606989622116, 0.6705979569815099, 0.7344273896887898, 0.6287880791351199, 0.6475581370759755, 0.7141498439013958, 0.6567419427447021, 0.7436038388404995, 0.6045988493133336, 0.7071227061096579, 0.6085692690685391, 0.6165371190290898, 0.5918028119485825, 0.5906970668584108, 0.5953304679132998, 0.5907733370549977, 0.5869637760333717, 0.5870265369303524, 0.5879928418435156, 0.5853672339580953, 0.5833898619748652, 0.610967607004568, 0.6060040278825909, 0.5846078931353986, 0.5902921939268708, 0.5875636669807136, 0.5928011210635304, 0.5947421493474394, 0.5838342430070043, 0.5940975828561932, 0.5887690982781351, 0.5894718409981579, 0.5788298940751702, 0.5823687871452421, 0.583727850113064, 0.5824614497832954, 0.5861007529310882, 0.5853447283152491, 0.5854837377555668, 0.585927426116541, 0.5834655880462378, 0.5848979002330452, 0.5838128088507801, 0.5867467287462205, 0.5820777700282633, 0.5815801199059933, 0.5837258049286902, 0.5841583961155266, 0.5852057649753988, 0.5854112149681896, 0.583212606376037, 0.6547279539518058, 0.649332795292139, 0.6751813632436097, 0.6607899637892842, 0.6577278198674321, 0.6408204033505172, 0.6381746619008482, 0.6317988738883287, 0.6307502391282469, 0.6485312629956752, 0.648321412038058, 0.670029787812382, 0.6443251599557698, 0.6353244527708739, 0.6531209058593959, 0.6399593802634627, 0.6314049779903144, 0.6410310242790729, 0.6428053779527545, 0.6293025889899582, 0.651970008155331, 0.6612193377222866, 0.6281704381108284, 0.6240812458563596, 0.621855310164392, 0.6195098769385368, 0.6240105228498578, 0.6256093648262322, 0.6454242982435971, 0.6645575768779963, 0.6490530529990792, 0.6247263327240944, 0.6222155599389225, 0.6303660608828068, 0.6239899839274585, 0.6216719350777566, 0.6220562478993088, 0.6214022729545832, 0.6224084140267223, 0.7119513938669115, 0.6198476599529386, 0.7283521527424455, 0.6410037223249674, 0.6239622577559203, 0.6550397670362145, 0.6175953962374479, 0.7004585671238601, 0.5969299001153558, 0.6092971528414637, 0.6059960641432554, 0.6027429918758571, 0.6034658828284591, 0.6048221059609205, 0.6011359519325197, 0.6023346108850092, 0.598540493985638, 0.6056649750098586, 0.6061300688888878, 0.6036310351919383, 0.6028780380729586, 0.6051724459975958, 0.60351357399486, 0.6053315349854529, 0.6065565147437155, 0.6042879838496447, 0.6029175729490817, 0.6111925758887082, 0.6061623429413885, 0.5984911788254976, 0.6029949877411127, 0.6047033900395036, 0.5986042781732976, 0.5992960690055043, 0.5966065330430865, 0.5977128210943192, 0.597567608114332, 0.599817963084206, 0.6018653500359505, 0.597121151862666, 0.5970612401142716, 0.6004447429440916, 0.6069494963157922, 0.6078483071178198, 0.6056860331445932, 0.6070895639713854, 0.6084741451777518, 0.6066100799944252, 0.608121613971889]
Total Epoch List: [70, 71, 88]
Total Time List: [0.1779202080797404, 0.17560440208762884, 0.16381010203622282]
T-times Epoch Time: 0.618921812439929 ~ 0.002108925107477172
T-times Total Epoch: 74.22222222222221 ~ 3.467663674294942
T-times Total Time: 0.1800887372551693 ~ 0.006519123854024275
T-times Inference Elapsed: 0.17742595634184147 ~ 0.0004998297578370551
T-times Time Per Graph: 0.0013792399257226493 ~ 3.6803883600558848e-06
T-times Speed: 728.6911651183715 ~ 1.856405846340514
T-times cross validation test micro f1 score:0.9124906280953665 ~ 0.006449200837500177
T-times cross validation test precision:0.9577233475969716 ~ 0.015840661806896972
T-times cross validation test recall:0.8721955128205128 ~ 0.008705872024287098
T-times cross validation test f1_score:0.9124906280953665 ~ 0.006113031286362962
