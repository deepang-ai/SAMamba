Namespace(seed=66, model='SAMamba', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=128, dropout=0.6, epochs=1000, lr=0.0001, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed66/khopgnn_gat_1_0.6_0.0001_0.0001_2_8_128_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead
  warnings.warn(out)
Data(edge_index=[2, 95], edge_attr=[95, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c6243ea7a0>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.6943;  Loss pred: 0.6768; Loss self: 1.7468; time: 0.57s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000005
Train loss: 0.6955;  Loss pred: 0.6772; Loss self: 1.8277; time: 0.31s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6945 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000015
Train loss: 0.6812;  Loss pred: 0.6637; Loss self: 1.7462; time: 0.31s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.6657;  Loss pred: 0.6478; Loss self: 1.7910; time: 0.31s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000035
Train loss: 0.6491;  Loss pred: 0.6311; Loss self: 1.8032; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000045
Train loss: 0.6342;  Loss pred: 0.6168; Loss self: 1.7385; time: 0.31s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6944 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000055
Train loss: 0.6054;  Loss pred: 0.5870; Loss self: 1.8429; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6916 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000065
Train loss: 0.5851;  Loss pred: 0.5667; Loss self: 1.8352; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6894 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000075
Train loss: 0.5620;  Loss pred: 0.5432; Loss self: 1.8797; time: 0.31s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6883 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6851 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000085
Train loss: 0.5333;  Loss pred: 0.5139; Loss self: 1.9344; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6813 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6767 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000095
Train loss: 0.5036;  Loss pred: 0.4839; Loss self: 1.9658; time: 0.32s
Val loss: 0.6680 score: 0.5039 time: 0.18s
Test loss: 0.6610 score: 0.5814 time: 0.17s
Epoch 12/1000, LR 0.000095
Train loss: 0.4722;  Loss pred: 0.4519; Loss self: 2.0323; time: 0.33s
Val loss: 0.6448 score: 0.7674 time: 0.17s
Test loss: 0.6336 score: 0.8372 time: 0.16s
Epoch 13/1000, LR 0.000095
Train loss: 0.4551;  Loss pred: 0.4344; Loss self: 2.0721; time: 0.35s
Val loss: 0.6088 score: 0.8992 time: 0.18s
Test loss: 0.5921 score: 0.9070 time: 0.17s
Epoch 14/1000, LR 0.000095
Train loss: 0.4214;  Loss pred: 0.4002; Loss self: 2.1245; time: 0.30s
Val loss: 0.5626 score: 0.9070 time: 0.18s
Test loss: 0.5392 score: 0.9225 time: 0.17s
Epoch 15/1000, LR 0.000095
Train loss: 0.3965;  Loss pred: 0.3744; Loss self: 2.2080; time: 0.32s
Val loss: 0.5136 score: 0.8760 time: 0.17s
Test loss: 0.4836 score: 0.9070 time: 0.17s
Epoch 16/1000, LR 0.000095
Train loss: 0.3806;  Loss pred: 0.3585; Loss self: 2.2062; time: 0.31s
Val loss: 0.4703 score: 0.8450 time: 0.18s
Test loss: 0.4348 score: 0.8992 time: 0.18s
Epoch 17/1000, LR 0.000095
Train loss: 0.3528;  Loss pred: 0.3301; Loss self: 2.2705; time: 0.30s
Val loss: 0.4377 score: 0.8450 time: 0.18s
Test loss: 0.3982 score: 0.8992 time: 0.17s
Epoch 18/1000, LR 0.000095
Train loss: 0.3245;  Loss pred: 0.3016; Loss self: 2.2886; time: 0.30s
Val loss: 0.4135 score: 0.8527 time: 0.18s
Test loss: 0.3711 score: 0.8992 time: 0.18s
Epoch 19/1000, LR 0.000095
Train loss: 0.3051;  Loss pred: 0.2820; Loss self: 2.3047; time: 0.30s
Val loss: 0.3953 score: 0.8450 time: 0.18s
Test loss: 0.3506 score: 0.9070 time: 0.18s
Epoch 20/1000, LR 0.000095
Train loss: 0.2819;  Loss pred: 0.2584; Loss self: 2.3490; time: 0.30s
Val loss: 0.3826 score: 0.8450 time: 0.18s
Test loss: 0.3359 score: 0.9147 time: 0.17s
Epoch 21/1000, LR 0.000095
Train loss: 0.2599;  Loss pred: 0.2365; Loss self: 2.3422; time: 0.31s
Val loss: 0.3710 score: 0.8605 time: 0.17s
Test loss: 0.3228 score: 0.9070 time: 0.18s
Epoch 22/1000, LR 0.000095
Train loss: 0.2451;  Loss pred: 0.2211; Loss self: 2.3936; time: 0.30s
Val loss: 0.3625 score: 0.8605 time: 0.18s
Test loss: 0.3119 score: 0.9070 time: 0.17s
Epoch 23/1000, LR 0.000095
Train loss: 0.2212;  Loss pred: 0.1972; Loss self: 2.4038; time: 0.30s
Val loss: 0.3535 score: 0.8837 time: 0.18s
Test loss: 0.3012 score: 0.9070 time: 0.17s
Epoch 24/1000, LR 0.000095
Train loss: 0.2063;  Loss pred: 0.1814; Loss self: 2.4863; time: 0.31s
Val loss: 0.3451 score: 0.8837 time: 0.19s
Test loss: 0.2902 score: 0.9147 time: 0.18s
Epoch 25/1000, LR 0.000095
Train loss: 0.1871;  Loss pred: 0.1620; Loss self: 2.5054; time: 0.30s
Val loss: 0.3363 score: 0.8837 time: 0.18s
Test loss: 0.2789 score: 0.9147 time: 0.17s
Epoch 26/1000, LR 0.000095
Train loss: 0.1765;  Loss pred: 0.1511; Loss self: 2.5403; time: 0.30s
Val loss: 0.3308 score: 0.8837 time: 0.18s
Test loss: 0.2706 score: 0.9147 time: 0.17s
Epoch 27/1000, LR 0.000095
Train loss: 0.1668;  Loss pred: 0.1412; Loss self: 2.5518; time: 0.31s
Val loss: 0.3290 score: 0.8760 time: 0.17s
Test loss: 0.2665 score: 0.9147 time: 0.17s
Epoch 28/1000, LR 0.000095
Train loss: 0.1448;  Loss pred: 0.1184; Loss self: 2.6338; time: 0.30s
Val loss: 0.3224 score: 0.8760 time: 0.18s
Test loss: 0.2605 score: 0.9147 time: 0.18s
Epoch 29/1000, LR 0.000095
Train loss: 0.1362;  Loss pred: 0.1100; Loss self: 2.6254; time: 0.30s
Val loss: 0.3160 score: 0.8760 time: 0.18s
Test loss: 0.2554 score: 0.9147 time: 0.17s
Epoch 30/1000, LR 0.000095
Train loss: 0.1301;  Loss pred: 0.1036; Loss self: 2.6524; time: 0.30s
Val loss: 0.3098 score: 0.8837 time: 0.18s
Test loss: 0.2493 score: 0.9147 time: 0.18s
Epoch 31/1000, LR 0.000095
Train loss: 0.1191;  Loss pred: 0.0921; Loss self: 2.6973; time: 0.30s
Val loss: 0.3120 score: 0.8837 time: 0.18s
Test loss: 0.2481 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000095
Train loss: 0.1091;  Loss pred: 0.0820; Loss self: 2.7098; time: 0.30s
Val loss: 0.3085 score: 0.8837 time: 0.18s
Test loss: 0.2424 score: 0.9147 time: 0.18s
Epoch 33/1000, LR 0.000095
Train loss: 0.1024;  Loss pred: 0.0746; Loss self: 2.7841; time: 0.30s
Val loss: 0.3091 score: 0.8837 time: 0.18s
Test loss: 0.2412 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000095
Train loss: 0.0946;  Loss pred: 0.0668; Loss self: 2.7765; time: 0.30s
Val loss: 0.3114 score: 0.8837 time: 0.18s
Test loss: 0.2416 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000095
Train loss: 0.0902;  Loss pred: 0.0623; Loss self: 2.7872; time: 0.31s
Val loss: 0.3100 score: 0.8837 time: 0.18s
Test loss: 0.2412 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000095
Train loss: 0.0854;  Loss pred: 0.0572; Loss self: 2.8225; time: 0.30s
Val loss: 0.3100 score: 0.8837 time: 0.18s
Test loss: 0.2430 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000095
Train loss: 0.0772;  Loss pred: 0.0490; Loss self: 2.8171; time: 0.30s
Val loss: 0.3120 score: 0.8837 time: 0.17s
Test loss: 0.2458 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000095
Train loss: 0.0756;  Loss pred: 0.0471; Loss self: 2.8459; time: 0.30s
Val loss: 0.3168 score: 0.8760 time: 0.18s
Test loss: 0.2515 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000095
Train loss: 0.0701;  Loss pred: 0.0418; Loss self: 2.8356; time: 0.30s
Val loss: 0.3128 score: 0.8760 time: 0.17s
Test loss: 0.2497 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000095
Train loss: 0.0663;  Loss pred: 0.0379; Loss self: 2.8373; time: 0.32s
Val loss: 0.3200 score: 0.8760 time: 0.18s
Test loss: 0.2558 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000095
Train loss: 0.0651;  Loss pred: 0.0365; Loss self: 2.8639; time: 0.30s
Val loss: 0.3252 score: 0.8760 time: 0.18s
Test loss: 0.2606 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000095
Train loss: 0.0643;  Loss pred: 0.0357; Loss self: 2.8586; time: 0.30s
Val loss: 0.3313 score: 0.8760 time: 0.19s
Test loss: 0.2659 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000095
Train loss: 0.0571;  Loss pred: 0.0283; Loss self: 2.8843; time: 0.30s
Val loss: 0.3278 score: 0.8682 time: 0.19s
Test loss: 0.2629 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000095
Train loss: 0.0550;  Loss pred: 0.0261; Loss self: 2.8922; time: 0.30s
Val loss: 0.3323 score: 0.8682 time: 0.18s
Test loss: 0.2668 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0547;  Loss pred: 0.0257; Loss self: 2.9058; time: 0.31s
Val loss: 0.3290 score: 0.8682 time: 0.18s
Test loss: 0.2667 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000095
Train loss: 0.0512;  Loss pred: 0.0222; Loss self: 2.9006; time: 0.30s
Val loss: 0.3282 score: 0.8682 time: 0.19s
Test loss: 0.2688 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000095
Train loss: 0.0556;  Loss pred: 0.0267; Loss self: 2.8971; time: 0.30s
Val loss: 0.3135 score: 0.8682 time: 0.18s
Test loss: 0.2611 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0479;  Loss pred: 0.0185; Loss self: 2.9355; time: 0.30s
Val loss: 0.3127 score: 0.8760 time: 0.18s
Test loss: 0.2637 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000095
Train loss: 0.0471;  Loss pred: 0.0177; Loss self: 2.9354; time: 0.30s
Val loss: 0.3112 score: 0.8760 time: 0.18s
Test loss: 0.2646 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000095
Train loss: 0.0489;  Loss pred: 0.0198; Loss self: 2.9147; time: 0.30s
Val loss: 0.3102 score: 0.8760 time: 0.18s
Test loss: 0.2656 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000095
Train loss: 0.0465;  Loss pred: 0.0171; Loss self: 2.9440; time: 0.30s
Val loss: 0.3218 score: 0.8682 time: 0.18s
Test loss: 0.2756 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000095
Train loss: 0.0438;  Loss pred: 0.0143; Loss self: 2.9537; time: 0.30s
Val loss: 0.3250 score: 0.8682 time: 0.17s
Test loss: 0.2782 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.1091,   Val_Loss: 0.3085,   Val_Precision: 0.9804,   Val_Recall: 0.7812,   Val_accuracy: 0.8696,   Val_Score: 0.8837,   Val_Loss: 0.3085,   Test_Precision: 0.9821,   Test_Recall: 0.8462,   Test_accuracy: 0.9091,   Test_Score: 0.9147,   Test_loss: 0.2424


[0.1801604249048978, 0.17471509706228971, 0.2039087489247322, 0.18058360484428704, 0.19133089599199593, 0.1955629598814994, 0.1668132310733199, 0.1710793161764741, 0.1810268999543041, 0.1767932150978595, 0.1737013270612806, 0.16774436295963824, 0.17797013092786074, 0.17842646595090628, 0.1747619858942926, 0.1837988649494946, 0.1794021250680089, 0.18269979488104582, 0.18301044194959104, 0.1752725390251726, 0.181513381190598, 0.1765964450314641, 0.17324616410769522, 0.18062758492305875, 0.17374191409908235, 0.17751012882217765, 0.17708307690918446, 0.1822461939882487, 0.1734749551396817, 0.18409662903286517, 0.17338501592166722, 0.18313255300745368, 0.17589036282151937, 0.17764687701128423, 0.18340323911979795, 0.18269349401816726, 0.18307376909069717, 0.17559218988753855, 0.17589031602256, 0.184486421989277, 0.18590358505025506, 0.19049839582294226, 0.1772241429425776, 0.1771777190733701, 0.17454691883176565, 0.18795587704516947, 0.18133669113740325, 0.17589161498472095, 0.17509256303310394, 0.17984050000086427, 0.18549924297258258, 0.17470172699540854]
[0.0013965924411232387, 0.0013543805973820907, 0.0015806879761607149, 0.0013998729057696669, 0.0014831852402480304, 0.0015159919370658869, 0.0012931258222737977, 0.0013261962494300317, 0.0014033093019713496, 0.0013704900395182908, 0.0013465219152037256, 0.0013003438989119243, 0.001379613418045432, 0.0013831508988442348, 0.0013547440766999426, 0.0014247974027092605, 0.0013907141478140224, 0.0014162774796980296, 0.0014186855965084577, 0.0013587018529083146, 0.001407080474345721, 0.0013689646901663883, 0.0013429935202146916, 0.0014002138366128586, 0.0013468365434037392, 0.001376047510249439, 0.0013727370303037556, 0.0014127611937073542, 0.0013447670941060597, 0.0014271056514175593, 0.0013440698908656374, 0.00141963219385623, 0.0013634911846629408, 0.0013771075737308855, 0.0014217305358123873, 0.0014162286357997462, 0.0014191765045790479, 0.0013611797665700662, 0.00136349082188031, 0.0014301273022424574, 0.001441113062405078, 0.0014767317505654438, 0.001373830565446338, 0.0013734706904912411, 0.001353076890168726, 0.0014570223026757324, 0.0014057107840108778, 0.001363500891354426, 0.0013573066901791002, 0.001394112403107475, 0.0014379786276944387, 0.0013542769534527794]
[716.0285066384356, 738.344895026494, 632.6359250412404, 714.3505641679578, 674.2246166316837, 659.6341151625392, 773.3199529196833, 754.0362147983578, 712.6012765647699, 729.6660108171869, 742.6540843553239, 769.0273325669924, 724.8407321354923, 722.9869140349062, 738.1467962834182, 701.8541710551228, 719.0550276430553, 706.0763263800637, 704.8778125760286, 735.9966411023067, 710.691405525324, 730.4790307472845, 744.6052307386707, 714.176630634516, 742.4805964002059, 726.7190940367516, 728.4716430930131, 707.8337120626945, 743.6231927319391, 700.7189684987158, 744.0089289969573, 704.4078067035388, 733.4114156720441, 726.1596835828747, 703.3681663372256, 706.1006780415075, 704.6339879313442, 734.6568209133935, 733.4116108100814, 699.2384513126822, 693.9080812515133, 677.1710567048469, 727.8917976869399, 728.0825189231639, 739.0562999529923, 686.3312923649563, 711.3838859133783, 733.40619455456, 736.7531650993684, 717.3022761801711, 695.4206277761825, 738.4014011686922]
Elapsed: 0.17961081005006024~0.006502355513428556
Time per graph: 0.0013923318608531805~5.040585669324463e-05
Speed: 719.1281455432422~25.146663130062628
Total Time: 0.1752
best val loss: 0.3084570638286744 test_score: 0.9147

Testing...
Test loss: 0.5392 score: 0.9225 time: 0.18s
test Score 0.9225
Epoch Time List: [0.9370028958655894, 0.6665459787473083, 0.7015706209931523, 0.6493629426695406, 0.6669998839497566, 0.6820329558104277, 0.6460130731575191, 0.6417662019375712, 0.6651840410195291, 0.6789818287361413, 0.6667691601905972, 0.6661559699568897, 0.6943828121293336, 0.6521374916192144, 0.6553532159887254, 0.6712749998550862, 0.6531474927905947, 0.657466035336256, 0.6560297210235149, 0.6487062920350581, 0.6615562371443957, 0.6541097748558968, 0.6550771202892065, 0.6682363855652511, 0.6526509632822126, 0.6522284459788352, 0.6516832690685987, 0.6584071319084615, 0.649697600863874, 0.6618956220336258, 0.6521545152645558, 0.6591966722626239, 0.650514607783407, 0.6508739429991692, 0.6693455490749329, 0.6585810519754887, 0.6543326110113412, 0.6556414349470288, 0.6487686531618237, 0.6795665889512748, 0.6630257600918412, 0.6739840311929584, 0.6608609538525343, 0.6559302259702235, 0.6589249249082059, 0.6673058711457998, 0.6543702341150492, 0.6513051197398454, 0.651276005199179, 0.6558386168908328, 0.6604898090008646, 0.6465840539894998]
Total Epoch List: [52]
Total Time List: [0.17520171985961497]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c6243e9750>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7020;  Loss pred: 0.6847; Loss self: 1.7305; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5039 time: 0.20s
Epoch 2/1000, LR 0.000005
Train loss: 0.7079;  Loss pred: 0.6898; Loss self: 1.8114; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5039 time: 0.19s
Epoch 3/1000, LR 0.000015
Train loss: 0.7050;  Loss pred: 0.6874; Loss self: 1.7600; time: 0.35s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.6854;  Loss pred: 0.6679; Loss self: 1.7514; time: 0.29s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000035
Train loss: 0.6720;  Loss pred: 0.6543; Loss self: 1.7647; time: 0.29s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000045
Train loss: 0.6514;  Loss pred: 0.6337; Loss self: 1.7675; time: 0.29s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5039 time: 0.19s
Epoch 7/1000, LR 0.000055
Train loss: 0.6408;  Loss pred: 0.6238; Loss self: 1.6928; time: 0.29s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5039 time: 0.22s
Epoch 8/1000, LR 0.000065
Train loss: 0.6145;  Loss pred: 0.5969; Loss self: 1.7537; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6911 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6912 score: 0.5039 time: 0.20s
Epoch 9/1000, LR 0.000075
Train loss: 0.5867;  Loss pred: 0.5688; Loss self: 1.7976; time: 0.30s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6875 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6884 score: 0.5039 time: 0.24s
Epoch 10/1000, LR 0.000085
Train loss: 0.5603;  Loss pred: 0.5419; Loss self: 1.8446; time: 0.31s
Val loss: 0.6804 score: 0.5116 time: 0.19s
Test loss: 0.6826 score: 0.5116 time: 0.18s
Epoch 11/1000, LR 0.000095
Train loss: 0.5226;  Loss pred: 0.5037; Loss self: 1.8946; time: 0.28s
Val loss: 0.6674 score: 0.7597 time: 0.20s
Test loss: 0.6721 score: 0.6977 time: 0.18s
Epoch 12/1000, LR 0.000095
Train loss: 0.4999;  Loss pred: 0.4807; Loss self: 1.9157; time: 0.39s
Val loss: 0.6444 score: 0.8760 time: 0.19s
Test loss: 0.6536 score: 0.8217 time: 0.18s
Epoch 13/1000, LR 0.000095
Train loss: 0.4801;  Loss pred: 0.4598; Loss self: 2.0278; time: 0.29s
Val loss: 0.6086 score: 0.8915 time: 0.20s
Test loss: 0.6244 score: 0.8605 time: 0.18s
Epoch 14/1000, LR 0.000095
Train loss: 0.4513;  Loss pred: 0.4307; Loss self: 2.0645; time: 0.29s
Val loss: 0.5608 score: 0.8992 time: 0.26s
Test loss: 0.5849 score: 0.8682 time: 0.20s
Epoch 15/1000, LR 0.000095
Train loss: 0.4281;  Loss pred: 0.4068; Loss self: 2.1253; time: 0.32s
Val loss: 0.5056 score: 0.8992 time: 0.22s
Test loss: 0.5382 score: 0.8837 time: 0.21s
Epoch 16/1000, LR 0.000095
Train loss: 0.3993;  Loss pred: 0.3772; Loss self: 2.2091; time: 0.32s
Val loss: 0.4539 score: 0.8992 time: 0.29s
Test loss: 0.4929 score: 0.8837 time: 0.19s
Epoch 17/1000, LR 0.000095
Train loss: 0.3897;  Loss pred: 0.3669; Loss self: 2.2789; time: 0.32s
Val loss: 0.4118 score: 0.8992 time: 0.21s
Test loss: 0.4542 score: 0.8837 time: 0.20s
Epoch 18/1000, LR 0.000095
Train loss: 0.3595;  Loss pred: 0.3364; Loss self: 2.3103; time: 0.33s
Val loss: 0.3809 score: 0.8915 time: 0.21s
Test loss: 0.4245 score: 0.8915 time: 0.19s
Epoch 19/1000, LR 0.000095
Train loss: 0.3343;  Loss pred: 0.3111; Loss self: 2.3237; time: 0.32s
Val loss: 0.3596 score: 0.8915 time: 0.21s
Test loss: 0.4023 score: 0.8992 time: 0.20s
Epoch 20/1000, LR 0.000095
Train loss: 0.3301;  Loss pred: 0.3064; Loss self: 2.3751; time: 0.33s
Val loss: 0.3407 score: 0.8915 time: 0.26s
Test loss: 0.3819 score: 0.9070 time: 0.22s
Epoch 21/1000, LR 0.000095
Train loss: 0.3024;  Loss pred: 0.2781; Loss self: 2.4280; time: 0.33s
Val loss: 0.3225 score: 0.8915 time: 0.21s
Test loss: 0.3616 score: 0.9070 time: 0.20s
Epoch 22/1000, LR 0.000095
Train loss: 0.2777;  Loss pred: 0.2533; Loss self: 2.4421; time: 0.33s
Val loss: 0.3061 score: 0.8915 time: 0.21s
Test loss: 0.3445 score: 0.9070 time: 0.19s
Epoch 23/1000, LR 0.000095
Train loss: 0.2739;  Loss pred: 0.2498; Loss self: 2.4135; time: 0.33s
Val loss: 0.2946 score: 0.8992 time: 0.20s
Test loss: 0.3328 score: 0.9070 time: 0.19s
Epoch 24/1000, LR 0.000095
Train loss: 0.2404;  Loss pred: 0.2156; Loss self: 2.4755; time: 0.33s
Val loss: 0.2802 score: 0.9070 time: 0.20s
Test loss: 0.3188 score: 0.9147 time: 0.19s
Epoch 25/1000, LR 0.000095
Train loss: 0.2353;  Loss pred: 0.2098; Loss self: 2.5435; time: 0.33s
Val loss: 0.2705 score: 0.9070 time: 0.20s
Test loss: 0.3103 score: 0.9147 time: 0.19s
Epoch 26/1000, LR 0.000095
Train loss: 0.2128;  Loss pred: 0.1870; Loss self: 2.5860; time: 0.33s
Val loss: 0.2610 score: 0.9070 time: 0.20s
Test loss: 0.3015 score: 0.9147 time: 0.19s
Epoch 27/1000, LR 0.000095
Train loss: 0.2007;  Loss pred: 0.1751; Loss self: 2.5602; time: 0.34s
Val loss: 0.2500 score: 0.9070 time: 0.20s
Test loss: 0.2902 score: 0.9147 time: 0.18s
Epoch 28/1000, LR 0.000095
Train loss: 0.1800;  Loss pred: 0.1536; Loss self: 2.6450; time: 0.33s
Val loss: 0.2446 score: 0.9070 time: 0.20s
Test loss: 0.2872 score: 0.9147 time: 0.19s
Epoch 29/1000, LR 0.000095
Train loss: 0.1674;  Loss pred: 0.1409; Loss self: 2.6505; time: 0.32s
Val loss: 0.2419 score: 0.9070 time: 0.20s
Test loss: 0.2867 score: 0.9147 time: 0.19s
Epoch 30/1000, LR 0.000095
Train loss: 0.1579;  Loss pred: 0.1306; Loss self: 2.7262; time: 0.33s
Val loss: 0.2380 score: 0.9070 time: 0.20s
Test loss: 0.2853 score: 0.9147 time: 0.19s
Epoch 31/1000, LR 0.000095
Train loss: 0.1463;  Loss pred: 0.1192; Loss self: 2.7134; time: 0.34s
Val loss: 0.2363 score: 0.9070 time: 0.21s
Test loss: 0.2859 score: 0.9147 time: 0.20s
Epoch 32/1000, LR 0.000095
Train loss: 0.1400;  Loss pred: 0.1123; Loss self: 2.7644; time: 0.34s
Val loss: 0.2353 score: 0.9070 time: 0.21s
Test loss: 0.2870 score: 0.9070 time: 0.20s
Epoch 33/1000, LR 0.000095
Train loss: 0.1276;  Loss pred: 0.0999; Loss self: 2.7709; time: 0.35s
Val loss: 0.2352 score: 0.9147 time: 0.21s
Test loss: 0.2875 score: 0.9070 time: 0.20s
Epoch 34/1000, LR 0.000095
Train loss: 0.1212;  Loss pred: 0.0934; Loss self: 2.7802; time: 0.35s
Val loss: 0.2324 score: 0.9147 time: 0.21s
Test loss: 0.2869 score: 0.9070 time: 0.20s
Epoch 35/1000, LR 0.000095
Train loss: 0.1182;  Loss pred: 0.0904; Loss self: 2.7763; time: 0.34s
Val loss: 0.2290 score: 0.9147 time: 0.20s
Test loss: 0.2883 score: 0.9070 time: 0.19s
Epoch 36/1000, LR 0.000095
Train loss: 0.1066;  Loss pred: 0.0783; Loss self: 2.8264; time: 0.34s
Val loss: 0.2278 score: 0.9147 time: 0.20s
Test loss: 0.2896 score: 0.9147 time: 0.19s
Epoch 37/1000, LR 0.000095
Train loss: 0.1011;  Loss pred: 0.0726; Loss self: 2.8532; time: 0.33s
Val loss: 0.2264 score: 0.9225 time: 0.20s
Test loss: 0.2917 score: 0.9147 time: 0.19s
Epoch 38/1000, LR 0.000095
Train loss: 0.0976;  Loss pred: 0.0686; Loss self: 2.9008; time: 0.34s
Val loss: 0.2263 score: 0.9225 time: 0.20s
Test loss: 0.2941 score: 0.9070 time: 0.19s
Epoch 39/1000, LR 0.000095
Train loss: 0.0881;  Loss pred: 0.0591; Loss self: 2.9061; time: 0.34s
Val loss: 0.2253 score: 0.9225 time: 0.20s
Test loss: 0.2987 score: 0.9070 time: 0.19s
Epoch 40/1000, LR 0.000095
Train loss: 0.0900;  Loss pred: 0.0608; Loss self: 2.9181; time: 0.33s
Val loss: 0.2290 score: 0.9225 time: 0.20s
Test loss: 0.3110 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000095
Train loss: 0.0879;  Loss pred: 0.0587; Loss self: 2.9171; time: 0.33s
Val loss: 0.2362 score: 0.9225 time: 0.20s
Test loss: 0.3316 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000095
Train loss: 0.0797;  Loss pred: 0.0503; Loss self: 2.9425; time: 0.33s
Val loss: 0.2430 score: 0.9225 time: 0.20s
Test loss: 0.3489 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000095
Train loss: 0.0745;  Loss pred: 0.0445; Loss self: 2.9954; time: 0.33s
Val loss: 0.2431 score: 0.9225 time: 0.20s
Test loss: 0.3541 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000095
Train loss: 0.0689;  Loss pred: 0.0389; Loss self: 2.9992; time: 0.33s
Val loss: 0.2440 score: 0.9147 time: 0.20s
Test loss: 0.3583 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0637;  Loss pred: 0.0338; Loss self: 2.9895; time: 0.33s
Val loss: 0.2462 score: 0.9147 time: 0.20s
Test loss: 0.3660 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000095
Train loss: 0.0614;  Loss pred: 0.0313; Loss self: 3.0099; time: 0.34s
Val loss: 0.2466 score: 0.9070 time: 0.20s
Test loss: 0.3676 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000095
Train loss: 0.0603;  Loss pred: 0.0298; Loss self: 3.0486; time: 0.32s
Val loss: 0.2461 score: 0.9070 time: 0.20s
Test loss: 0.3674 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0593;  Loss pred: 0.0290; Loss self: 3.0348; time: 0.29s
Val loss: 0.2474 score: 0.9070 time: 0.20s
Test loss: 0.3710 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000095
Train loss: 0.0587;  Loss pred: 0.0283; Loss self: 3.0397; time: 0.29s
Val loss: 0.2514 score: 0.9070 time: 0.20s
Test loss: 0.3815 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000095
Train loss: 0.0541;  Loss pred: 0.0237; Loss self: 3.0470; time: 0.29s
Val loss: 0.2542 score: 0.9070 time: 0.20s
Test loss: 0.3881 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000095
Train loss: 0.0531;  Loss pred: 0.0226; Loss self: 3.0487; time: 0.29s
Val loss: 0.2583 score: 0.9070 time: 0.20s
Test loss: 0.3981 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000095
Train loss: 0.0578;  Loss pred: 0.0275; Loss self: 3.0314; time: 0.29s
Val loss: 0.2643 score: 0.9070 time: 0.20s
Test loss: 0.4128 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000095
Train loss: 0.0508;  Loss pred: 0.0203; Loss self: 3.0521; time: 0.29s
Val loss: 0.2705 score: 0.9070 time: 0.20s
Test loss: 0.4265 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000095
Train loss: 0.0494;  Loss pred: 0.0187; Loss self: 3.0632; time: 0.29s
Val loss: 0.2770 score: 0.9070 time: 0.20s
Test loss: 0.4374 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000095
Train loss: 0.0472;  Loss pred: 0.0163; Loss self: 3.0843; time: 0.29s
Val loss: 0.2812 score: 0.8992 time: 0.20s
Test loss: 0.4423 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000095
Train loss: 0.0449;  Loss pred: 0.0142; Loss self: 3.0733; time: 0.29s
Val loss: 0.2859 score: 0.8992 time: 0.20s
Test loss: 0.4509 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000094
Train loss: 0.0598;  Loss pred: 0.0298; Loss self: 3.0017; time: 0.29s
Val loss: 0.2934 score: 0.9070 time: 0.20s
Test loss: 0.4651 score: 0.8760 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000094
Train loss: 0.0446;  Loss pred: 0.0137; Loss self: 3.0848; time: 0.29s
Val loss: 0.2970 score: 0.8992 time: 0.20s
Test loss: 0.4664 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000094
Train loss: 0.0431;  Loss pred: 0.0125; Loss self: 3.0557; time: 0.29s
Val loss: 0.3002 score: 0.8992 time: 0.20s
Test loss: 0.4697 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.0881,   Val_Loss: 0.2253,   Val_Precision: 0.9508,   Val_Recall: 0.8923,   Val_accuracy: 0.9206,   Val_Score: 0.9225,   Val_Loss: 0.2253,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9070,   Test_loss: 0.2987


[0.1801604249048978, 0.17471509706228971, 0.2039087489247322, 0.18058360484428704, 0.19133089599199593, 0.1955629598814994, 0.1668132310733199, 0.1710793161764741, 0.1810268999543041, 0.1767932150978595, 0.1737013270612806, 0.16774436295963824, 0.17797013092786074, 0.17842646595090628, 0.1747619858942926, 0.1837988649494946, 0.1794021250680089, 0.18269979488104582, 0.18301044194959104, 0.1752725390251726, 0.181513381190598, 0.1765964450314641, 0.17324616410769522, 0.18062758492305875, 0.17374191409908235, 0.17751012882217765, 0.17708307690918446, 0.1822461939882487, 0.1734749551396817, 0.18409662903286517, 0.17338501592166722, 0.18313255300745368, 0.17589036282151937, 0.17764687701128423, 0.18340323911979795, 0.18269349401816726, 0.18307376909069717, 0.17559218988753855, 0.17589031602256, 0.184486421989277, 0.18590358505025506, 0.19049839582294226, 0.1772241429425776, 0.1771777190733701, 0.17454691883176565, 0.18795587704516947, 0.18133669113740325, 0.17589161498472095, 0.17509256303310394, 0.17984050000086427, 0.18549924297258258, 0.17470172699540854, 0.20199364004656672, 0.19183387514203787, 0.19155017309822142, 0.19280552794225514, 0.19426140189170837, 0.19334578700363636, 0.2214783439412713, 0.2023519710637629, 0.24870557500980794, 0.18471399205736816, 0.18649703403934836, 0.18602438201196492, 0.1862627170048654, 0.2067931571509689, 0.21086152410134673, 0.1980543821118772, 0.20221239305101335, 0.19780499418266118, 0.2010245560668409, 0.22811752907000482, 0.20358651294372976, 0.19969907798804343, 0.19342246605083346, 0.19210978015325963, 0.1957296731416136, 0.19685019087046385, 0.18997015594504774, 0.1928701230790466, 0.1966800750233233, 0.19356498192064464, 0.20121005596593022, 0.2058076518587768, 0.20186729193665087, 0.2015769318677485, 0.19541047513484955, 0.1928158481605351, 0.19272912293672562, 0.19296352192759514, 0.1964556430466473, 0.19261689903214574, 0.19243902992457151, 0.1936387389432639, 0.19364882493391633, 0.19777449406683445, 0.1942867108155042, 0.19455603579990566, 0.19478958495892584, 0.19841166702099144, 0.19455290399491787, 0.19477125816047192, 0.19502873392775655, 0.1947286280337721, 0.1939497261773795, 0.19506524014286697, 0.19878566707484424, 0.1953331681434065, 0.1985197509638965, 0.19564880803227425, 0.19455702090635896]
[0.0013965924411232387, 0.0013543805973820907, 0.0015806879761607149, 0.0013998729057696669, 0.0014831852402480304, 0.0015159919370658869, 0.0012931258222737977, 0.0013261962494300317, 0.0014033093019713496, 0.0013704900395182908, 0.0013465219152037256, 0.0013003438989119243, 0.001379613418045432, 0.0013831508988442348, 0.0013547440766999426, 0.0014247974027092605, 0.0013907141478140224, 0.0014162774796980296, 0.0014186855965084577, 0.0013587018529083146, 0.001407080474345721, 0.0013689646901663883, 0.0013429935202146916, 0.0014002138366128586, 0.0013468365434037392, 0.001376047510249439, 0.0013727370303037556, 0.0014127611937073542, 0.0013447670941060597, 0.0014271056514175593, 0.0013440698908656374, 0.00141963219385623, 0.0013634911846629408, 0.0013771075737308855, 0.0014217305358123873, 0.0014162286357997462, 0.0014191765045790479, 0.0013611797665700662, 0.00136349082188031, 0.0014301273022424574, 0.001441113062405078, 0.0014767317505654438, 0.001373830565446338, 0.0013734706904912411, 0.001353076890168726, 0.0014570223026757324, 0.0014057107840108778, 0.001363500891354426, 0.0013573066901791002, 0.001394112403107475, 0.0014379786276944387, 0.0013542769534527794, 0.001565842170903618, 0.0014870843034266502, 0.0014848850627769102, 0.0014946164956763965, 0.0015059023402458013, 0.0014988045504157857, 0.0017168863871416381, 0.0015686199307268442, 0.0019279501938744802, 0.0014318914112974276, 0.0014457134421654912, 0.0014420494729609683, 0.0014438970310454683, 0.001603047729852472, 0.0016345854581499746, 0.0015353052876889706, 0.0015675379306280106, 0.001533372047927606, 0.0015583298919910147, 0.0017683529385271691, 0.0015781900228196105, 0.0015480548681243676, 0.001499398961634368, 0.001489223101963253, 0.0015172842879194854, 0.0015259704718640608, 0.0014726368677910678, 0.0014951172331709038, 0.0015246517443668474, 0.0015005037358189507, 0.0015597678757048854, 0.0015954081539440062, 0.001564862728191092, 0.001562611874943787, 0.0015148098847662756, 0.001494696497368489, 0.0014940242088118264, 0.0014958412552526755, 0.001522911961601917, 0.0014931542560631453, 0.0014917754257718723, 0.0015010754956842163, 0.0015011536816582662, 0.0015331356129211972, 0.001506098533453521, 0.0015081863240302765, 0.001509996782627332, 0.0015380749381472205, 0.0015081620464722316, 0.0015098547144222628, 0.0015118506506027639, 0.0015095242483238148, 0.0015034862494370503, 0.0015121336445183486, 0.0015409741633708857, 0.0015142106057628411, 0.0015389127981697405, 0.001516657426606777, 0.0015081939605144105]
[716.0285066384356, 738.344895026494, 632.6359250412404, 714.3505641679578, 674.2246166316837, 659.6341151625392, 773.3199529196833, 754.0362147983578, 712.6012765647699, 729.6660108171869, 742.6540843553239, 769.0273325669924, 724.8407321354923, 722.9869140349062, 738.1467962834182, 701.8541710551228, 719.0550276430553, 706.0763263800637, 704.8778125760286, 735.9966411023067, 710.691405525324, 730.4790307472845, 744.6052307386707, 714.176630634516, 742.4805964002059, 726.7190940367516, 728.4716430930131, 707.8337120626945, 743.6231927319391, 700.7189684987158, 744.0089289969573, 704.4078067035388, 733.4114156720441, 726.1596835828747, 703.3681663372256, 706.1006780415075, 704.6339879313442, 734.6568209133935, 733.4116108100814, 699.2384513126822, 693.9080812515133, 677.1710567048469, 727.8917976869399, 728.0825189231639, 739.0562999529923, 686.3312923649563, 711.3838859133783, 733.40619455456, 736.7531650993684, 717.3022761801711, 695.4206277761825, 738.4014011686922, 638.6339687242673, 672.4568322695127, 673.4527978413912, 669.0679534802302, 664.0536861353006, 667.1984013676689, 582.4497226428897, 637.5030562926959, 518.6855984024996, 698.3769803423197, 691.7000083378416, 693.4574844694436, 692.570161513484, 623.8117439535189, 611.7759062483041, 651.3362573675867, 637.9430956413062, 652.1574469493736, 641.7126470713725, 565.4979717074369, 633.6372588475688, 645.971935873052, 666.9339019082583, 671.491060460782, 659.0722700827605, 655.3206752280353, 679.0540301357418, 668.8438724494937, 655.8874862372436, 666.4428592403444, 641.1210383135267, 626.7988523989309, 639.033687738191, 639.9541793037852, 660.1488477574154, 669.0321424854915, 669.3331969468446, 668.5201364038334, 656.6367756072533, 669.7231688818293, 670.3421860449146, 666.1890110624867, 666.1543133247616, 652.2580204725828, 663.9671826165154, 663.0480492143259, 662.2530666986199, 650.1633796885146, 663.0587225949079, 662.3153807104177, 661.4409959087607, 662.4603752542606, 665.1208152880877, 661.3172080557234, 648.9401469343892, 660.4101148110847, 649.809398680237, 659.3446762973385, 663.0446919830675]
Elapsed: 0.1892331310774426~0.012353936386280129
Time per graph: 0.0014669234967243613~9.576694873085372e-05
Speed: 684.4423459544901~42.32858870746886
Total Time: 0.1951
best val loss: 0.2253414766155472 test_score: 0.9070

Testing...
Test loss: 0.2917 score: 0.9147 time: 0.19s
test Score 0.9147
Epoch Time List: [0.9370028958655894, 0.6665459787473083, 0.7015706209931523, 0.6493629426695406, 0.6669998839497566, 0.6820329558104277, 0.6460130731575191, 0.6417662019375712, 0.6651840410195291, 0.6789818287361413, 0.6667691601905972, 0.6661559699568897, 0.6943828121293336, 0.6521374916192144, 0.6553532159887254, 0.6712749998550862, 0.6531474927905947, 0.657466035336256, 0.6560297210235149, 0.6487062920350581, 0.6615562371443957, 0.6541097748558968, 0.6550771202892065, 0.6682363855652511, 0.6526509632822126, 0.6522284459788352, 0.6516832690685987, 0.6584071319084615, 0.649697600863874, 0.6618956220336258, 0.6521545152645558, 0.6591966722626239, 0.650514607783407, 0.6508739429991692, 0.6693455490749329, 0.6585810519754887, 0.6543326110113412, 0.6556414349470288, 0.6487686531618237, 0.6795665889512748, 0.6630257600918412, 0.6739840311929584, 0.6608609538525343, 0.6559302259702235, 0.6589249249082059, 0.6673058711457998, 0.6543702341150492, 0.6513051197398454, 0.651276005199179, 0.6558386168908328, 0.6604898090008646, 0.6465840539894998, 0.7475682678632438, 0.6945268767885864, 0.7400197039823979, 0.6725105796940625, 0.7563140639103949, 0.675181308761239, 0.7643276781309396, 0.7094830279238522, 0.7486334247514606, 0.680376417003572, 0.6632104469463229, 0.7638046208303422, 0.670210471143946, 0.7480096318759024, 0.741146408719942, 0.8007964652497321, 0.7283627637661994, 0.7287556168157607, 0.7253806809894741, 0.8140344172716141, 0.7356670550070703, 0.7395437338855118, 0.7192984919529408, 0.7184694870375097, 0.7166367017198354, 0.7241158117540181, 0.7221665608230978, 0.7155065999832004, 0.713587298989296, 0.717223693151027, 0.7461543979588896, 0.7483052308671176, 0.7540224748663604, 0.7530258742626756, 0.729540588799864, 0.7281021298840642, 0.7220089419279248, 0.7288161288015544, 0.7302747520152479, 0.7170877128373832, 0.7150291751604527, 0.7170833528507501, 0.7168983239680529, 0.7221597500611097, 0.7198559930548072, 0.7317429138347507, 0.7085767039097846, 0.69034703518264, 0.6864142001140863, 0.6848562632221729, 0.6855245511978865, 0.683563482016325, 0.6840708020608872, 0.6844023996964097, 0.6901330661494285, 0.6877975699026138, 0.6855190158821642, 0.6883126571774483, 0.6836961000226438]
Total Epoch List: [52, 59]
Total Time List: [0.17520171985961497, 0.19513560109771788]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c6243e8220>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7027;  Loss pred: 0.6872; Loss self: 1.5508; time: 0.39s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000007
Train loss: 0.6912;  Loss pred: 0.6758; Loss self: 1.5417; time: 0.39s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000017
Train loss: 0.6856;  Loss pred: 0.6703; Loss self: 1.5272; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5000 time: 0.18s
Epoch 4/1000, LR 0.000027
Train loss: 0.6601;  Loss pred: 0.6447; Loss self: 1.5353; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6920 score: 0.5000 time: 0.18s
Epoch 5/1000, LR 0.000037
Train loss: 0.6382;  Loss pred: 0.6228; Loss self: 1.5376; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6907 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6909 score: 0.5000 time: 0.18s
Epoch 6/1000, LR 0.000047
Train loss: 0.6126;  Loss pred: 0.5966; Loss self: 1.5998; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6865 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6873 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000057
Train loss: 0.5899;  Loss pred: 0.5739; Loss self: 1.6075; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6759 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6776 score: 0.5000 time: 0.17s
Epoch 8/1000, LR 0.000067
Train loss: 0.5540;  Loss pred: 0.5368; Loss self: 1.7146; time: 0.35s
Val loss: 0.6488 score: 0.8837 time: 0.18s
Test loss: 0.6525 score: 0.8438 time: 0.17s
Epoch 9/1000, LR 0.000077
Train loss: 0.5247;  Loss pred: 0.5071; Loss self: 1.7596; time: 0.34s
Val loss: 0.5937 score: 0.9070 time: 0.18s
Test loss: 0.6010 score: 0.8672 time: 0.17s
Epoch 10/1000, LR 0.000087
Train loss: 0.4690;  Loss pred: 0.4508; Loss self: 1.8269; time: 0.34s
Val loss: 0.5060 score: 0.9147 time: 0.31s
Test loss: 0.5226 score: 0.9062 time: 0.17s
Epoch 11/1000, LR 0.000097
Train loss: 0.4387;  Loss pred: 0.4194; Loss self: 1.9308; time: 0.35s
Val loss: 0.4042 score: 0.9070 time: 0.18s
Test loss: 0.4316 score: 0.9219 time: 0.17s
Epoch 12/1000, LR 0.000097
Train loss: 0.4151;  Loss pred: 0.3950; Loss self: 2.0112; time: 0.34s
Val loss: 0.3290 score: 0.9302 time: 0.17s
Test loss: 0.3653 score: 0.9219 time: 0.16s
Epoch 13/1000, LR 0.000097
Train loss: 0.3688;  Loss pred: 0.3482; Loss self: 2.0534; time: 0.34s
Val loss: 0.3038 score: 0.9070 time: 0.17s
Test loss: 0.3444 score: 0.9141 time: 0.16s
Epoch 14/1000, LR 0.000097
Train loss: 0.3283;  Loss pred: 0.3076; Loss self: 2.0717; time: 0.34s
Val loss: 0.3210 score: 0.8992 time: 0.18s
Test loss: 0.3567 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000097
Train loss: 0.3064;  Loss pred: 0.2852; Loss self: 2.1180; time: 0.36s
Val loss: 0.2677 score: 0.9380 time: 0.17s
Test loss: 0.3228 score: 0.9453 time: 0.16s
Epoch 16/1000, LR 0.000097
Train loss: 0.2752;  Loss pred: 0.2539; Loss self: 2.1297; time: 0.35s
Val loss: 0.2873 score: 0.8915 time: 0.17s
Test loss: 0.3280 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000097
Train loss: 0.2468;  Loss pred: 0.2252; Loss self: 2.1599; time: 0.34s
Val loss: 0.3236 score: 0.8837 time: 0.26s
Test loss: 0.3621 score: 0.8594 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000097
Train loss: 0.2310;  Loss pred: 0.2087; Loss self: 2.2256; time: 0.34s
Val loss: 0.2293 score: 0.9380 time: 0.18s
Test loss: 0.2856 score: 0.9297 time: 0.17s
Epoch 19/1000, LR 0.000097
Train loss: 0.2074;  Loss pred: 0.1849; Loss self: 2.2422; time: 0.33s
Val loss: 0.2158 score: 0.9457 time: 0.18s
Test loss: 0.2799 score: 0.9297 time: 0.25s
Epoch 20/1000, LR 0.000097
Train loss: 0.1882;  Loss pred: 0.1652; Loss self: 2.2983; time: 0.32s
Val loss: 0.2080 score: 0.9302 time: 0.17s
Test loss: 0.2853 score: 0.9297 time: 0.16s
Epoch 21/1000, LR 0.000097
Train loss: 0.1626;  Loss pred: 0.1392; Loss self: 2.3483; time: 0.34s
Val loss: 0.2423 score: 0.9225 time: 0.17s
Test loss: 0.2966 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000097
Train loss: 0.1530;  Loss pred: 0.1294; Loss self: 2.3673; time: 0.46s
Val loss: 0.1966 score: 0.9380 time: 0.19s
Test loss: 0.2642 score: 0.9219 time: 0.17s
Epoch 23/1000, LR 0.000097
Train loss: 0.1391;  Loss pred: 0.1147; Loss self: 2.4349; time: 0.35s
Val loss: 0.3707 score: 0.8760 time: 0.18s
Test loss: 0.4209 score: 0.8516 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000097
Train loss: 0.1300;  Loss pred: 0.1054; Loss self: 2.4647; time: 0.36s
Val loss: 0.3050 score: 0.8992 time: 0.19s
Test loss: 0.3567 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000097
Train loss: 0.1241;  Loss pred: 0.0992; Loss self: 2.4914; time: 0.39s
Val loss: 0.3796 score: 0.8605 time: 0.19s
Test loss: 0.4294 score: 0.8359 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000097
Train loss: 0.1406;  Loss pred: 0.1161; Loss self: 2.4498; time: 0.40s
Val loss: 0.2113 score: 0.9225 time: 0.18s
Test loss: 0.2616 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000097
Train loss: 0.0983;  Loss pred: 0.0731; Loss self: 2.5192; time: 0.37s
Val loss: 0.2027 score: 0.9225 time: 0.18s
Test loss: 0.2502 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000097
Train loss: 0.1010;  Loss pred: 0.0765; Loss self: 2.4507; time: 0.37s
Val loss: 0.2167 score: 0.9225 time: 0.25s
Test loss: 0.2890 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000097
Train loss: 0.0872;  Loss pred: 0.0618; Loss self: 2.5399; time: 0.39s
Val loss: 0.2624 score: 0.9070 time: 0.19s
Test loss: 0.3004 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000097
Train loss: 0.0796;  Loss pred: 0.0537; Loss self: 2.5928; time: 0.38s
Val loss: 0.3887 score: 0.8837 time: 0.25s
Test loss: 0.4252 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000097
Train loss: 0.0787;  Loss pred: 0.0526; Loss self: 2.6122; time: 0.37s
Val loss: 0.2053 score: 0.9225 time: 0.19s
Test loss: 0.2560 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000097
Train loss: 0.0720;  Loss pred: 0.0458; Loss self: 2.6186; time: 0.39s
Val loss: 0.2467 score: 0.9225 time: 0.27s
Test loss: 0.3352 score: 0.8594 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000097
Train loss: 0.0688;  Loss pred: 0.0425; Loss self: 2.6347; time: 0.38s
Val loss: 0.2130 score: 0.9225 time: 0.19s
Test loss: 0.2775 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000097
Train loss: 0.0694;  Loss pred: 0.0427; Loss self: 2.6658; time: 0.37s
Val loss: 0.3353 score: 0.8915 time: 0.27s
Test loss: 0.3532 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000097
Train loss: 0.0617;  Loss pred: 0.0352; Loss self: 2.6487; time: 0.38s
Val loss: 0.2507 score: 0.9147 time: 0.18s
Test loss: 0.2774 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000097
Train loss: 0.0642;  Loss pred: 0.0377; Loss self: 2.6567; time: 0.38s
Val loss: 0.2163 score: 0.9225 time: 0.27s
Test loss: 0.2681 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000097
Train loss: 0.0585;  Loss pred: 0.0318; Loss self: 2.6723; time: 0.38s
Val loss: 0.2127 score: 0.9225 time: 0.18s
Test loss: 0.2491 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000096
Train loss: 0.0547;  Loss pred: 0.0277; Loss self: 2.7002; time: 0.37s
Val loss: 0.3548 score: 0.8992 time: 0.18s
Test loss: 0.3619 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 39/1000, LR 0.000096
Train loss: 0.0540;  Loss pred: 0.0269; Loss self: 2.7076; time: 0.37s
Val loss: 0.2439 score: 0.9147 time: 0.18s
Test loss: 0.2740 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 40/1000, LR 0.000096
Train loss: 0.0507;  Loss pred: 0.0235; Loss self: 2.7236; time: 0.37s
Val loss: 0.3825 score: 0.8915 time: 0.18s
Test loss: 0.3883 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 41/1000, LR 0.000096
Train loss: 0.0499;  Loss pred: 0.0227; Loss self: 2.7208; time: 0.37s
Val loss: 0.4860 score: 0.8760 time: 0.18s
Test loss: 0.4836 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 42/1000, LR 0.000096
Train loss: 0.0490;  Loss pred: 0.0215; Loss self: 2.7487; time: 0.36s
Val loss: 0.4432 score: 0.8837 time: 0.18s
Test loss: 0.4516 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 021,   Train_Loss: 0.1530,   Val_Loss: 0.1966,   Val_Precision: 0.9524,   Val_Recall: 0.9231,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1966,   Test_Precision: 0.9219,   Test_Recall: 0.9219,   Test_accuracy: 0.9219,   Test_Score: 0.9219,   Test_loss: 0.2642


[0.1801604249048978, 0.17471509706228971, 0.2039087489247322, 0.18058360484428704, 0.19133089599199593, 0.1955629598814994, 0.1668132310733199, 0.1710793161764741, 0.1810268999543041, 0.1767932150978595, 0.1737013270612806, 0.16774436295963824, 0.17797013092786074, 0.17842646595090628, 0.1747619858942926, 0.1837988649494946, 0.1794021250680089, 0.18269979488104582, 0.18301044194959104, 0.1752725390251726, 0.181513381190598, 0.1765964450314641, 0.17324616410769522, 0.18062758492305875, 0.17374191409908235, 0.17751012882217765, 0.17708307690918446, 0.1822461939882487, 0.1734749551396817, 0.18409662903286517, 0.17338501592166722, 0.18313255300745368, 0.17589036282151937, 0.17764687701128423, 0.18340323911979795, 0.18269349401816726, 0.18307376909069717, 0.17559218988753855, 0.17589031602256, 0.184486421989277, 0.18590358505025506, 0.19049839582294226, 0.1772241429425776, 0.1771777190733701, 0.17454691883176565, 0.18795587704516947, 0.18133669113740325, 0.17589161498472095, 0.17509256303310394, 0.17984050000086427, 0.18549924297258258, 0.17470172699540854, 0.20199364004656672, 0.19183387514203787, 0.19155017309822142, 0.19280552794225514, 0.19426140189170837, 0.19334578700363636, 0.2214783439412713, 0.2023519710637629, 0.24870557500980794, 0.18471399205736816, 0.18649703403934836, 0.18602438201196492, 0.1862627170048654, 0.2067931571509689, 0.21086152410134673, 0.1980543821118772, 0.20221239305101335, 0.19780499418266118, 0.2010245560668409, 0.22811752907000482, 0.20358651294372976, 0.19969907798804343, 0.19342246605083346, 0.19210978015325963, 0.1957296731416136, 0.19685019087046385, 0.18997015594504774, 0.1928701230790466, 0.1966800750233233, 0.19356498192064464, 0.20121005596593022, 0.2058076518587768, 0.20186729193665087, 0.2015769318677485, 0.19541047513484955, 0.1928158481605351, 0.19272912293672562, 0.19296352192759514, 0.1964556430466473, 0.19261689903214574, 0.19243902992457151, 0.1936387389432639, 0.19364882493391633, 0.19777449406683445, 0.1942867108155042, 0.19455603579990566, 0.19478958495892584, 0.19841166702099144, 0.19455290399491787, 0.19477125816047192, 0.19502873392775655, 0.1947286280337721, 0.1939497261773795, 0.19506524014286697, 0.19878566707484424, 0.1953331681434065, 0.1985197509638965, 0.19564880803227425, 0.19455702090635896, 0.19105893396772444, 0.18224700400605798, 0.18397599807940423, 0.18124690698459744, 0.18202871293760836, 0.1782450620085001, 0.17413684609346092, 0.17582590086385608, 0.17912859586067498, 0.1790434007998556, 0.17062070593237877, 0.16797138890251517, 0.16866073990240693, 0.16966149304062128, 0.16909556882455945, 0.17143572587519884, 0.16896419203840196, 0.1720805079676211, 0.25489957607351243, 0.1667324430309236, 0.16558045102283359, 0.17759469314478338, 0.17766338912770152, 0.18140919017605484, 0.18031978397630155, 0.1790693779475987, 0.17928144591860473, 0.1794867659918964, 0.18150993483141065, 0.1808983930386603, 0.18151891883462667, 0.17914305604062974, 0.18156522000208497, 0.177158169914037, 0.18138386611826718, 0.17912365985102952, 0.18117169104516506, 0.17729040095582604, 0.17711024009622633, 0.17826662911102176, 0.1768485619686544, 0.17626874009147286]
[0.0013965924411232387, 0.0013543805973820907, 0.0015806879761607149, 0.0013998729057696669, 0.0014831852402480304, 0.0015159919370658869, 0.0012931258222737977, 0.0013261962494300317, 0.0014033093019713496, 0.0013704900395182908, 0.0013465219152037256, 0.0013003438989119243, 0.001379613418045432, 0.0013831508988442348, 0.0013547440766999426, 0.0014247974027092605, 0.0013907141478140224, 0.0014162774796980296, 0.0014186855965084577, 0.0013587018529083146, 0.001407080474345721, 0.0013689646901663883, 0.0013429935202146916, 0.0014002138366128586, 0.0013468365434037392, 0.001376047510249439, 0.0013727370303037556, 0.0014127611937073542, 0.0013447670941060597, 0.0014271056514175593, 0.0013440698908656374, 0.00141963219385623, 0.0013634911846629408, 0.0013771075737308855, 0.0014217305358123873, 0.0014162286357997462, 0.0014191765045790479, 0.0013611797665700662, 0.00136349082188031, 0.0014301273022424574, 0.001441113062405078, 0.0014767317505654438, 0.001373830565446338, 0.0013734706904912411, 0.001353076890168726, 0.0014570223026757324, 0.0014057107840108778, 0.001363500891354426, 0.0013573066901791002, 0.001394112403107475, 0.0014379786276944387, 0.0013542769534527794, 0.001565842170903618, 0.0014870843034266502, 0.0014848850627769102, 0.0014946164956763965, 0.0015059023402458013, 0.0014988045504157857, 0.0017168863871416381, 0.0015686199307268442, 0.0019279501938744802, 0.0014318914112974276, 0.0014457134421654912, 0.0014420494729609683, 0.0014438970310454683, 0.001603047729852472, 0.0016345854581499746, 0.0015353052876889706, 0.0015675379306280106, 0.001533372047927606, 0.0015583298919910147, 0.0017683529385271691, 0.0015781900228196105, 0.0015480548681243676, 0.001499398961634368, 0.001489223101963253, 0.0015172842879194854, 0.0015259704718640608, 0.0014726368677910678, 0.0014951172331709038, 0.0015246517443668474, 0.0015005037358189507, 0.0015597678757048854, 0.0015954081539440062, 0.001564862728191092, 0.001562611874943787, 0.0015148098847662756, 0.001494696497368489, 0.0014940242088118264, 0.0014958412552526755, 0.001522911961601917, 0.0014931542560631453, 0.0014917754257718723, 0.0015010754956842163, 0.0015011536816582662, 0.0015331356129211972, 0.001506098533453521, 0.0015081863240302765, 0.001509996782627332, 0.0015380749381472205, 0.0015081620464722316, 0.0015098547144222628, 0.0015118506506027639, 0.0015095242483238148, 0.0015034862494370503, 0.0015121336445183486, 0.0015409741633708857, 0.0015142106057628411, 0.0015389127981697405, 0.001516657426606777, 0.0015081939605144105, 0.0014926479216228472, 0.001423804718797328, 0.0014373124849953456, 0.0014159914608171675, 0.0014220993198250653, 0.001392539546941407, 0.0013604441101051634, 0.0013736398504988756, 0.0013994421551615233, 0.0013987765687488718, 0.0013329742650967091, 0.0013122764758008998, 0.0013176620304875541, 0.0013254804143798538, 0.0013210591314418707, 0.001339341608399991, 0.0013200327503000153, 0.0013443789684970398, 0.001991402938074316, 0.0013025972111790907, 0.0012935972736158874, 0.0013874585401936201, 0.0013879952275601681, 0.0014172592982504284, 0.0014087483123148559, 0.0013989795152156148, 0.0014006362962390995, 0.0014022403593116906, 0.0014180463658703957, 0.0014132686956145335, 0.001418116553395521, 0.0013995551253174199, 0.0014184782812662888, 0.001384048202453414, 0.0014170614540489623, 0.001399403592586168, 0.001415403836290352, 0.001385081257467391, 0.0013836737507517682, 0.0013927080399298575, 0.0013816293903801125, 0.0013770995319646318]
[716.0285066384356, 738.344895026494, 632.6359250412404, 714.3505641679578, 674.2246166316837, 659.6341151625392, 773.3199529196833, 754.0362147983578, 712.6012765647699, 729.6660108171869, 742.6540843553239, 769.0273325669924, 724.8407321354923, 722.9869140349062, 738.1467962834182, 701.8541710551228, 719.0550276430553, 706.0763263800637, 704.8778125760286, 735.9966411023067, 710.691405525324, 730.4790307472845, 744.6052307386707, 714.176630634516, 742.4805964002059, 726.7190940367516, 728.4716430930131, 707.8337120626945, 743.6231927319391, 700.7189684987158, 744.0089289969573, 704.4078067035388, 733.4114156720441, 726.1596835828747, 703.3681663372256, 706.1006780415075, 704.6339879313442, 734.6568209133935, 733.4116108100814, 699.2384513126822, 693.9080812515133, 677.1710567048469, 727.8917976869399, 728.0825189231639, 739.0562999529923, 686.3312923649563, 711.3838859133783, 733.40619455456, 736.7531650993684, 717.3022761801711, 695.4206277761825, 738.4014011686922, 638.6339687242673, 672.4568322695127, 673.4527978413912, 669.0679534802302, 664.0536861353006, 667.1984013676689, 582.4497226428897, 637.5030562926959, 518.6855984024996, 698.3769803423197, 691.7000083378416, 693.4574844694436, 692.570161513484, 623.8117439535189, 611.7759062483041, 651.3362573675867, 637.9430956413062, 652.1574469493736, 641.7126470713725, 565.4979717074369, 633.6372588475688, 645.971935873052, 666.9339019082583, 671.491060460782, 659.0722700827605, 655.3206752280353, 679.0540301357418, 668.8438724494937, 655.8874862372436, 666.4428592403444, 641.1210383135267, 626.7988523989309, 639.033687738191, 639.9541793037852, 660.1488477574154, 669.0321424854915, 669.3331969468446, 668.5201364038334, 656.6367756072533, 669.7231688818293, 670.3421860449146, 666.1890110624867, 666.1543133247616, 652.2580204725828, 663.9671826165154, 663.0480492143259, 662.2530666986199, 650.1633796885146, 663.0587225949079, 662.3153807104177, 661.4409959087607, 662.4603752542606, 665.1208152880877, 661.3172080557234, 648.9401469343892, 660.4101148110847, 649.809398680237, 659.3446762973385, 663.0446919830675, 669.9503516628174, 702.3435073629261, 695.7429302530816, 706.218948116326, 703.1857663239805, 718.1124602144432, 735.0540845979326, 727.9928575432797, 714.5704424521786, 714.9104598559616, 750.2020302900961, 762.0345395505826, 758.9199482586482, 754.4434373765268, 756.968387106601, 746.6355063773638, 757.5569619562252, 743.8378786287908, 502.15854404985396, 767.6970220862178, 773.0381165730051, 720.7422571779692, 720.4635723119956, 705.5871859401277, 709.8500074557672, 714.8067495798015, 713.9612208288027, 713.1445000562272, 705.1955592342009, 707.579530419846, 705.1606566509729, 714.5127633134132, 704.9808327747469, 722.5181884759243, 705.6856970759491, 714.5901334667506, 706.5121447041654, 721.9793023757259, 722.7137173460773, 718.0255813345948, 723.7830976691087, 726.163924094401]
Elapsed: 0.1864156851764111~0.013340289439190802
Time per graph: 0.0014480581762589101~0.00010206755617745053
Speed: 693.6773281298811~44.36756398972532
Total Time: 0.1770
best val loss: 0.19660471824481507 test_score: 0.9219

Testing...
Test loss: 0.2799 score: 0.9297 time: 0.17s
test Score 0.9297
Epoch Time List: [0.9370028958655894, 0.6665459787473083, 0.7015706209931523, 0.6493629426695406, 0.6669998839497566, 0.6820329558104277, 0.6460130731575191, 0.6417662019375712, 0.6651840410195291, 0.6789818287361413, 0.6667691601905972, 0.6661559699568897, 0.6943828121293336, 0.6521374916192144, 0.6553532159887254, 0.6712749998550862, 0.6531474927905947, 0.657466035336256, 0.6560297210235149, 0.6487062920350581, 0.6615562371443957, 0.6541097748558968, 0.6550771202892065, 0.6682363855652511, 0.6526509632822126, 0.6522284459788352, 0.6516832690685987, 0.6584071319084615, 0.649697600863874, 0.6618956220336258, 0.6521545152645558, 0.6591966722626239, 0.650514607783407, 0.6508739429991692, 0.6693455490749329, 0.6585810519754887, 0.6543326110113412, 0.6556414349470288, 0.6487686531618237, 0.6795665889512748, 0.6630257600918412, 0.6739840311929584, 0.6608609538525343, 0.6559302259702235, 0.6589249249082059, 0.6673058711457998, 0.6543702341150492, 0.6513051197398454, 0.651276005199179, 0.6558386168908328, 0.6604898090008646, 0.6465840539894998, 0.7475682678632438, 0.6945268767885864, 0.7400197039823979, 0.6725105796940625, 0.7563140639103949, 0.675181308761239, 0.7643276781309396, 0.7094830279238522, 0.7486334247514606, 0.680376417003572, 0.6632104469463229, 0.7638046208303422, 0.670210471143946, 0.7480096318759024, 0.741146408719942, 0.8007964652497321, 0.7283627637661994, 0.7287556168157607, 0.7253806809894741, 0.8140344172716141, 0.7356670550070703, 0.7395437338855118, 0.7192984919529408, 0.7184694870375097, 0.7166367017198354, 0.7241158117540181, 0.7221665608230978, 0.7155065999832004, 0.713587298989296, 0.717223693151027, 0.7461543979588896, 0.7483052308671176, 0.7540224748663604, 0.7530258742626756, 0.729540588799864, 0.7281021298840642, 0.7220089419279248, 0.7288161288015544, 0.7302747520152479, 0.7170877128373832, 0.7150291751604527, 0.7170833528507501, 0.7168983239680529, 0.7221597500611097, 0.7198559930548072, 0.7317429138347507, 0.7085767039097846, 0.69034703518264, 0.6864142001140863, 0.6848562632221729, 0.6855245511978865, 0.683563482016325, 0.6840708020608872, 0.6844023996964097, 0.6901330661494285, 0.6877975699026138, 0.6855190158821642, 0.6883126571774483, 0.6836961000226438, 0.7620505299419165, 0.7502508980687708, 0.7369207523297518, 0.7371577031444758, 0.738617671886459, 0.7328101948369294, 0.6882620688993484, 0.6924790209159255, 0.6969135247636586, 0.8245739780832082, 0.68919030809775, 0.6720838230103254, 0.6799277951940894, 0.6777810028288513, 0.6984280869364738, 0.6906031719408929, 0.7722656999249011, 0.6815208289772272, 0.7590144602581859, 0.6546521079726517, 0.6750247720628977, 0.8145939700771123, 0.7045555487275124, 0.7264385449234396, 0.7485745588783175, 0.7533196550793946, 0.724856389220804, 0.7948155291378498, 0.7525470783002675, 0.8007611972279847, 0.7301509028766304, 0.8317762340884656, 0.7386702885851264, 0.8092808099463582, 0.7378025436773896, 0.8202862828038633, 0.737039860105142, 0.7254704958759248, 0.7195990732870996, 0.7221571924164891, 0.7169276457279921, 0.708748617907986]
Total Epoch List: [52, 59, 42]
Total Time List: [0.17520171985961497, 0.19513560109771788, 0.1769961949903518]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c6243eb100>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.6896;  Loss pred: 0.6718; Loss self: 1.7800; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6937 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5039 time: 0.21s
Epoch 2/1000, LR 0.000005
Train loss: 0.6820;  Loss pred: 0.6646; Loss self: 1.7338; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000015
Train loss: 0.6858;  Loss pred: 0.6676; Loss self: 1.8144; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.6739;  Loss pred: 0.6564; Loss self: 1.7425; time: 0.35s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5039 time: 0.20s
Epoch 5/1000, LR 0.000035
Train loss: 0.6402;  Loss pred: 0.6223; Loss self: 1.7887; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5039 time: 0.20s
Epoch 6/1000, LR 0.000045
Train loss: 0.6260;  Loss pred: 0.6079; Loss self: 1.8119; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5039 time: 0.20s
Epoch 7/1000, LR 0.000055
Train loss: 0.6054;  Loss pred: 0.5875; Loss self: 1.7927; time: 0.35s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5039 time: 0.20s
Epoch 8/1000, LR 0.000065
Train loss: 0.5704;  Loss pred: 0.5519; Loss self: 1.8517; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6911 score: 0.5039 time: 0.20s
Epoch 9/1000, LR 0.000075
Train loss: 0.5398;  Loss pred: 0.5212; Loss self: 1.8559; time: 0.35s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6899 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6889 score: 0.5039 time: 0.20s
Epoch 10/1000, LR 0.000085
Train loss: 0.5141;  Loss pred: 0.4943; Loss self: 1.9863; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6857 score: 0.4961 time: 0.20s
Test loss: 0.6843 score: 0.5194 time: 0.20s
Epoch 11/1000, LR 0.000095
Train loss: 0.4863;  Loss pred: 0.4658; Loss self: 2.0462; time: 0.34s
Val loss: 0.6770 score: 0.5814 time: 0.20s
Test loss: 0.6748 score: 0.5969 time: 0.20s
Epoch 12/1000, LR 0.000095
Train loss: 0.4739;  Loss pred: 0.4523; Loss self: 2.1553; time: 0.35s
Val loss: 0.6589 score: 0.8062 time: 0.20s
Test loss: 0.6547 score: 0.8062 time: 0.20s
Epoch 13/1000, LR 0.000095
Train loss: 0.4480;  Loss pred: 0.4263; Loss self: 2.1682; time: 0.35s
Val loss: 0.6236 score: 0.8682 time: 0.20s
Test loss: 0.6160 score: 0.9147 time: 0.20s
Epoch 14/1000, LR 0.000095
Train loss: 0.4173;  Loss pred: 0.3956; Loss self: 2.1640; time: 0.34s
Val loss: 0.5728 score: 0.8760 time: 0.20s
Test loss: 0.5610 score: 0.9070 time: 0.20s
Epoch 15/1000, LR 0.000095
Train loss: 0.3972;  Loss pred: 0.3747; Loss self: 2.2582; time: 0.34s
Val loss: 0.5201 score: 0.8915 time: 0.20s
Test loss: 0.5033 score: 0.9147 time: 0.20s
Epoch 16/1000, LR 0.000095
Train loss: 0.3897;  Loss pred: 0.3665; Loss self: 2.3200; time: 0.35s
Val loss: 0.4764 score: 0.8915 time: 0.20s
Test loss: 0.4542 score: 0.9147 time: 0.20s
Epoch 17/1000, LR 0.000095
Train loss: 0.3588;  Loss pred: 0.3359; Loss self: 2.2907; time: 0.35s
Val loss: 0.4412 score: 0.8915 time: 0.20s
Test loss: 0.4129 score: 0.9070 time: 0.21s
Epoch 18/1000, LR 0.000095
Train loss: 0.3378;  Loss pred: 0.3145; Loss self: 2.3232; time: 0.34s
Val loss: 0.4172 score: 0.8915 time: 0.20s
Test loss: 0.3839 score: 0.9070 time: 0.21s
Epoch 19/1000, LR 0.000095
Train loss: 0.3169;  Loss pred: 0.2936; Loss self: 2.3321; time: 0.34s
Val loss: 0.3984 score: 0.8760 time: 0.20s
Test loss: 0.3626 score: 0.9070 time: 0.21s
Epoch 20/1000, LR 0.000095
Train loss: 0.2989;  Loss pred: 0.2754; Loss self: 2.3494; time: 0.34s
Val loss: 0.3823 score: 0.8760 time: 0.20s
Test loss: 0.3445 score: 0.9147 time: 0.20s
Epoch 21/1000, LR 0.000095
Train loss: 0.2846;  Loss pred: 0.2610; Loss self: 2.3621; time: 0.34s
Val loss: 0.3695 score: 0.8837 time: 0.20s
Test loss: 0.3296 score: 0.9225 time: 0.20s
Epoch 22/1000, LR 0.000095
Train loss: 0.2595;  Loss pred: 0.2353; Loss self: 2.4195; time: 0.36s
Val loss: 0.3566 score: 0.8837 time: 0.20s
Test loss: 0.3150 score: 0.9225 time: 0.20s
Epoch 23/1000, LR 0.000095
Train loss: 0.2444;  Loss pred: 0.2200; Loss self: 2.4345; time: 0.37s
Val loss: 0.3460 score: 0.8837 time: 0.20s
Test loss: 0.3019 score: 0.9225 time: 0.20s
Epoch 24/1000, LR 0.000095
Train loss: 0.2338;  Loss pred: 0.2091; Loss self: 2.4762; time: 0.38s
Val loss: 0.3368 score: 0.8992 time: 0.20s
Test loss: 0.2911 score: 0.9380 time: 0.20s
Epoch 25/1000, LR 0.000095
Train loss: 0.2237;  Loss pred: 0.1991; Loss self: 2.4643; time: 0.38s
Val loss: 0.3292 score: 0.8992 time: 0.20s
Test loss: 0.2805 score: 0.9380 time: 0.20s
Epoch 26/1000, LR 0.000095
Train loss: 0.2005;  Loss pred: 0.1755; Loss self: 2.5015; time: 0.38s
Val loss: 0.3226 score: 0.8992 time: 0.20s
Test loss: 0.2713 score: 0.9380 time: 0.20s
Epoch 27/1000, LR 0.000095
Train loss: 0.1846;  Loss pred: 0.1596; Loss self: 2.4969; time: 0.38s
Val loss: 0.3161 score: 0.9070 time: 0.20s
Test loss: 0.2628 score: 0.9380 time: 0.20s
Epoch 28/1000, LR 0.000095
Train loss: 0.1791;  Loss pred: 0.1538; Loss self: 2.5304; time: 0.38s
Val loss: 0.3102 score: 0.9070 time: 0.20s
Test loss: 0.2534 score: 0.9380 time: 0.20s
Epoch 29/1000, LR 0.000095
Train loss: 0.1672;  Loss pred: 0.1419; Loss self: 2.5335; time: 0.38s
Val loss: 0.3061 score: 0.9070 time: 0.20s
Test loss: 0.2453 score: 0.9380 time: 0.20s
Epoch 30/1000, LR 0.000095
Train loss: 0.1610;  Loss pred: 0.1354; Loss self: 2.5537; time: 0.38s
Val loss: 0.3024 score: 0.9070 time: 0.20s
Test loss: 0.2394 score: 0.9380 time: 0.20s
Epoch 31/1000, LR 0.000095
Train loss: 0.1587;  Loss pred: 0.1332; Loss self: 2.5512; time: 0.38s
Val loss: 0.2989 score: 0.9070 time: 0.20s
Test loss: 0.2318 score: 0.9380 time: 0.20s
Epoch 32/1000, LR 0.000095
Train loss: 0.1335;  Loss pred: 0.1073; Loss self: 2.6227; time: 0.34s
Val loss: 0.2970 score: 0.9070 time: 0.20s
Test loss: 0.2279 score: 0.9380 time: 0.20s
Epoch 33/1000, LR 0.000095
Train loss: 0.1341;  Loss pred: 0.1076; Loss self: 2.6468; time: 0.35s
Val loss: 0.2941 score: 0.9070 time: 0.20s
Test loss: 0.2245 score: 0.9380 time: 0.20s
Epoch 34/1000, LR 0.000095
Train loss: 0.1370;  Loss pred: 0.1108; Loss self: 2.6161; time: 0.34s
Val loss: 0.2955 score: 0.9070 time: 0.20s
Test loss: 0.2236 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000095
Train loss: 0.1150;  Loss pred: 0.0887; Loss self: 2.6289; time: 0.34s
Val loss: 0.2907 score: 0.9147 time: 0.20s
Test loss: 0.2194 score: 0.9380 time: 0.20s
Epoch 36/1000, LR 0.000095
Train loss: 0.1182;  Loss pred: 0.0916; Loss self: 2.6555; time: 0.34s
Val loss: 0.2836 score: 0.9225 time: 0.20s
Test loss: 0.2124 score: 0.9380 time: 0.20s
Epoch 37/1000, LR 0.000095
Train loss: 0.1010;  Loss pred: 0.0741; Loss self: 2.6941; time: 0.34s
Val loss: 0.2845 score: 0.9225 time: 0.20s
Test loss: 0.2122 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 38/1000, LR 0.000095
Train loss: 0.1000;  Loss pred: 0.0728; Loss self: 2.7175; time: 0.34s
Val loss: 0.2870 score: 0.9225 time: 0.20s
Test loss: 0.2145 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 39/1000, LR 0.000095
Train loss: 0.1010;  Loss pred: 0.0738; Loss self: 2.7146; time: 0.34s
Val loss: 0.2855 score: 0.9225 time: 0.20s
Test loss: 0.2142 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 40/1000, LR 0.000095
Train loss: 0.0934;  Loss pred: 0.0659; Loss self: 2.7457; time: 0.34s
Val loss: 0.2869 score: 0.9225 time: 0.20s
Test loss: 0.2165 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 41/1000, LR 0.000095
Train loss: 0.0888;  Loss pred: 0.0612; Loss self: 2.7630; time: 0.34s
Val loss: 0.2859 score: 0.9225 time: 0.20s
Test loss: 0.2158 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 42/1000, LR 0.000095
Train loss: 0.0817;  Loss pred: 0.0538; Loss self: 2.7953; time: 0.34s
Val loss: 0.2824 score: 0.9225 time: 0.20s
Test loss: 0.2104 score: 0.9380 time: 0.20s
Epoch 43/1000, LR 0.000095
Train loss: 0.0789;  Loss pred: 0.0507; Loss self: 2.8180; time: 0.34s
Val loss: 0.2860 score: 0.9225 time: 0.20s
Test loss: 0.2105 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000095
Train loss: 0.0793;  Loss pred: 0.0510; Loss self: 2.8301; time: 0.34s
Val loss: 0.2858 score: 0.9225 time: 0.20s
Test loss: 0.2115 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0780;  Loss pred: 0.0497; Loss self: 2.8251; time: 0.35s
Val loss: 0.2868 score: 0.9225 time: 0.20s
Test loss: 0.2164 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000095
Train loss: 0.0690;  Loss pred: 0.0407; Loss self: 2.8297; time: 0.36s
Val loss: 0.2836 score: 0.9225 time: 0.20s
Test loss: 0.2157 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000095
Train loss: 0.0700;  Loss pred: 0.0418; Loss self: 2.8165; time: 0.35s
Val loss: 0.2834 score: 0.9225 time: 0.20s
Test loss: 0.2157 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0675;  Loss pred: 0.0391; Loss self: 2.8418; time: 0.37s
Val loss: 0.2814 score: 0.9225 time: 0.20s
Test loss: 0.2139 score: 0.9380 time: 0.20s
Epoch 49/1000, LR 0.000095
Train loss: 0.0632;  Loss pred: 0.0347; Loss self: 2.8467; time: 0.34s
Val loss: 0.2816 score: 0.9225 time: 0.20s
Test loss: 0.2127 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000095
Train loss: 0.0638;  Loss pred: 0.0352; Loss self: 2.8555; time: 0.35s
Val loss: 0.2848 score: 0.9225 time: 0.26s
Test loss: 0.2138 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000095
Train loss: 0.0593;  Loss pred: 0.0305; Loss self: 2.8807; time: 0.37s
Val loss: 0.2827 score: 0.9225 time: 0.20s
Test loss: 0.2113 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000095
Train loss: 0.0633;  Loss pred: 0.0347; Loss self: 2.8643; time: 0.35s
Val loss: 0.2807 score: 0.9302 time: 0.29s
Test loss: 0.2080 score: 0.9380 time: 0.20s
Epoch 53/1000, LR 0.000095
Train loss: 0.0561;  Loss pred: 0.0272; Loss self: 2.8847; time: 0.35s
Val loss: 0.2791 score: 0.9302 time: 0.20s
Test loss: 0.2078 score: 0.9380 time: 0.21s
Epoch 54/1000, LR 0.000095
Train loss: 0.0578;  Loss pred: 0.0292; Loss self: 2.8621; time: 0.36s
Val loss: 0.2830 score: 0.9302 time: 0.20s
Test loss: 0.2121 score: 0.9380 time: 0.28s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000095
Train loss: 0.0535;  Loss pred: 0.0249; Loss self: 2.8631; time: 0.34s
Val loss: 0.2787 score: 0.9302 time: 0.20s
Test loss: 0.2055 score: 0.9380 time: 0.21s
Epoch 56/1000, LR 0.000095
Train loss: 0.0536;  Loss pred: 0.0247; Loss self: 2.8905; time: 0.35s
Val loss: 0.2813 score: 0.9302 time: 0.20s
Test loss: 0.2046 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000094
Train loss: 0.0581;  Loss pred: 0.0290; Loss self: 2.9069; time: 0.36s
Val loss: 0.2741 score: 0.9302 time: 0.20s
Test loss: 0.1990 score: 0.9380 time: 0.20s
Epoch 58/1000, LR 0.000094
Train loss: 0.0506;  Loss pred: 0.0217; Loss self: 2.8934; time: 0.35s
Val loss: 0.2762 score: 0.9380 time: 0.20s
Test loss: 0.2001 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 59/1000, LR 0.000094
Train loss: 0.0505;  Loss pred: 0.0215; Loss self: 2.8936; time: 0.39s
Val loss: 0.2808 score: 0.9380 time: 0.20s
Test loss: 0.2026 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 60/1000, LR 0.000094
Train loss: 0.0570;  Loss pred: 0.0282; Loss self: 2.8743; time: 0.36s
Val loss: 0.2842 score: 0.9302 time: 0.20s
Test loss: 0.2057 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 61/1000, LR 0.000094
Train loss: 0.0467;  Loss pred: 0.0175; Loss self: 2.9186; time: 0.44s
Val loss: 0.2797 score: 0.9302 time: 0.21s
Test loss: 0.1966 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 62/1000, LR 0.000094
Train loss: 0.0446;  Loss pred: 0.0154; Loss self: 2.9232; time: 0.37s
Val loss: 0.2823 score: 0.9225 time: 0.20s
Test loss: 0.1941 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 63/1000, LR 0.000094
Train loss: 0.0453;  Loss pred: 0.0159; Loss self: 2.9379; time: 0.42s
Val loss: 0.2839 score: 0.9225 time: 0.20s
Test loss: 0.1912 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 64/1000, LR 0.000094
Train loss: 0.0450;  Loss pred: 0.0158; Loss self: 2.9209; time: 0.35s
Val loss: 0.2793 score: 0.9225 time: 0.20s
Test loss: 0.1836 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 65/1000, LR 0.000094
Train loss: 0.0493;  Loss pred: 0.0198; Loss self: 2.9515; time: 0.40s
Val loss: 0.2786 score: 0.9225 time: 0.20s
Test loss: 0.1849 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 66/1000, LR 0.000094
Train loss: 0.0447;  Loss pred: 0.0154; Loss self: 2.9289; time: 0.37s
Val loss: 0.2755 score: 0.9225 time: 0.20s
Test loss: 0.1859 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 67/1000, LR 0.000094
Train loss: 0.0459;  Loss pred: 0.0165; Loss self: 2.9385; time: 0.43s
Val loss: 0.2716 score: 0.9302 time: 0.21s
Test loss: 0.1802 score: 0.9380 time: 0.21s
Epoch 68/1000, LR 0.000094
Train loss: 0.0453;  Loss pred: 0.0160; Loss self: 2.9353; time: 0.37s
Val loss: 0.2695 score: 0.9302 time: 0.20s
Test loss: 0.1774 score: 0.9302 time: 0.20s
Epoch 69/1000, LR 0.000094
Train loss: 0.0445;  Loss pred: 0.0151; Loss self: 2.9422; time: 0.37s
Val loss: 0.2702 score: 0.9302 time: 0.20s
Test loss: 0.1798 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000094
Train loss: 0.0402;  Loss pred: 0.0108; Loss self: 2.9427; time: 0.38s
Val loss: 0.2701 score: 0.9302 time: 0.21s
Test loss: 0.1797 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000094
Train loss: 0.0403;  Loss pred: 0.0109; Loss self: 2.9475; time: 0.38s
Val loss: 0.2706 score: 0.9302 time: 0.20s
Test loss: 0.1804 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000094
Train loss: 0.0398;  Loss pred: 0.0107; Loss self: 2.9102; time: 0.38s
Val loss: 0.2715 score: 0.9302 time: 0.20s
Test loss: 0.1823 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000094
Train loss: 0.0407;  Loss pred: 0.0115; Loss self: 2.9184; time: 0.38s
Val loss: 0.2712 score: 0.9302 time: 0.20s
Test loss: 0.1857 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000094
Train loss: 0.0402;  Loss pred: 0.0111; Loss self: 2.9142; time: 0.38s
Val loss: 0.2676 score: 0.9302 time: 0.20s
Test loss: 0.1826 score: 0.9380 time: 0.20s
Epoch 75/1000, LR 0.000094
Train loss: 0.0381;  Loss pred: 0.0088; Loss self: 2.9208; time: 0.38s
Val loss: 0.2673 score: 0.9302 time: 0.20s
Test loss: 0.1824 score: 0.9380 time: 0.20s
Epoch 76/1000, LR 0.000094
Train loss: 0.0403;  Loss pred: 0.0111; Loss self: 2.9147; time: 0.38s
Val loss: 0.2644 score: 0.9302 time: 0.20s
Test loss: 0.1785 score: 0.9380 time: 0.20s
Epoch 77/1000, LR 0.000094
Train loss: 0.0369;  Loss pred: 0.0078; Loss self: 2.9078; time: 0.38s
Val loss: 0.2669 score: 0.9225 time: 0.20s
Test loss: 0.1808 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000094
Train loss: 0.0372;  Loss pred: 0.0080; Loss self: 2.9242; time: 0.38s
Val loss: 0.2700 score: 0.9225 time: 0.20s
Test loss: 0.1824 score: 0.9380 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000094
Train loss: 0.0392;  Loss pred: 0.0098; Loss self: 2.9384; time: 0.38s
Val loss: 0.2644 score: 0.9225 time: 0.20s
Test loss: 0.1791 score: 0.9380 time: 0.20s
Epoch 80/1000, LR 0.000094
Train loss: 0.0401;  Loss pred: 0.0112; Loss self: 2.8955; time: 0.36s
Val loss: 0.2668 score: 0.9225 time: 0.20s
Test loss: 0.1821 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000094
Train loss: 0.0362;  Loss pred: 0.0070; Loss self: 2.9159; time: 0.34s
Val loss: 0.2666 score: 0.9302 time: 0.20s
Test loss: 0.1817 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000094
Train loss: 0.0372;  Loss pred: 0.0080; Loss self: 2.9186; time: 0.36s
Val loss: 0.2667 score: 0.9302 time: 0.20s
Test loss: 0.1833 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 83/1000, LR 0.000094
Train loss: 0.0362;  Loss pred: 0.0072; Loss self: 2.9008; time: 0.34s
Val loss: 0.2671 score: 0.9302 time: 0.20s
Test loss: 0.1837 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 84/1000, LR 0.000094
Train loss: 0.0379;  Loss pred: 0.0091; Loss self: 2.8775; time: 0.34s
Val loss: 0.2643 score: 0.9302 time: 0.20s
Test loss: 0.1791 score: 0.9302 time: 0.20s
Epoch 85/1000, LR 0.000094
Train loss: 0.0401;  Loss pred: 0.0116; Loss self: 2.8466; time: 0.37s
Val loss: 0.2658 score: 0.9302 time: 0.20s
Test loss: 0.1857 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000094
Train loss: 0.0358;  Loss pred: 0.0070; Loss self: 2.8784; time: 0.37s
Val loss: 0.2647 score: 0.9302 time: 0.20s
Test loss: 0.1844 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000094
Train loss: 0.0356;  Loss pred: 0.0068; Loss self: 2.8802; time: 0.37s
Val loss: 0.2662 score: 0.9302 time: 0.20s
Test loss: 0.1872 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000094
Train loss: 0.0356;  Loss pred: 0.0068; Loss self: 2.8788; time: 0.38s
Val loss: 0.2677 score: 0.9302 time: 0.20s
Test loss: 0.1909 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000094
Train loss: 0.0353;  Loss pred: 0.0066; Loss self: 2.8686; time: 0.38s
Val loss: 0.2655 score: 0.9302 time: 0.20s
Test loss: 0.1868 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 90/1000, LR 0.000094
Train loss: 0.0340;  Loss pred: 0.0055; Loss self: 2.8550; time: 0.38s
Val loss: 0.2645 score: 0.9302 time: 0.20s
Test loss: 0.1849 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 91/1000, LR 0.000093
Train loss: 0.0337;  Loss pred: 0.0053; Loss self: 2.8432; time: 0.36s
Val loss: 0.2650 score: 0.9302 time: 0.20s
Test loss: 0.1856 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 92/1000, LR 0.000093
Train loss: 0.0340;  Loss pred: 0.0053; Loss self: 2.8723; time: 0.34s
Val loss: 0.2644 score: 0.9302 time: 0.20s
Test loss: 0.1840 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 93/1000, LR 0.000093
Train loss: 0.0344;  Loss pred: 0.0058; Loss self: 2.8658; time: 0.35s
Val loss: 0.2643 score: 0.9302 time: 0.20s
Test loss: 0.1839 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 94/1000, LR 0.000093
Train loss: 0.0344;  Loss pred: 0.0057; Loss self: 2.8752; time: 0.34s
Val loss: 0.2644 score: 0.9302 time: 0.20s
Test loss: 0.1823 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 95/1000, LR 0.000093
Train loss: 0.0412;  Loss pred: 0.0133; Loss self: 2.7935; time: 0.34s
Val loss: 0.2645 score: 0.9302 time: 0.20s
Test loss: 0.1802 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 96/1000, LR 0.000093
Train loss: 0.0347;  Loss pred: 0.0065; Loss self: 2.8263; time: 0.34s
Val loss: 0.2670 score: 0.9302 time: 0.20s
Test loss: 0.1859 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 97/1000, LR 0.000093
Train loss: 0.0337;  Loss pred: 0.0053; Loss self: 2.8411; time: 0.34s
Val loss: 0.2651 score: 0.9302 time: 0.20s
Test loss: 0.1813 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 98/1000, LR 0.000093
Train loss: 0.0331;  Loss pred: 0.0048; Loss self: 2.8280; time: 0.34s
Val loss: 0.2627 score: 0.9302 time: 0.20s
Test loss: 0.1777 score: 0.9302 time: 0.20s
Epoch 99/1000, LR 0.000093
Train loss: 0.0340;  Loss pred: 0.0060; Loss self: 2.7938; time: 0.34s
Val loss: 0.2595 score: 0.9302 time: 0.20s
Test loss: 0.1715 score: 0.9380 time: 0.20s
Epoch 100/1000, LR 0.000093
Train loss: 0.0328;  Loss pred: 0.0047; Loss self: 2.8142; time: 0.34s
Val loss: 0.2602 score: 0.9302 time: 0.20s
Test loss: 0.1746 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000093
Train loss: 0.0325;  Loss pred: 0.0045; Loss self: 2.8034; time: 0.34s
Val loss: 0.2594 score: 0.9302 time: 0.20s
Test loss: 0.1758 score: 0.9302 time: 0.20s
Epoch 102/1000, LR 0.000093
Train loss: 0.0327;  Loss pred: 0.0046; Loss self: 2.8127; time: 0.35s
Val loss: 0.2596 score: 0.9302 time: 0.20s
Test loss: 0.1786 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000093
Train loss: 0.0323;  Loss pred: 0.0044; Loss self: 2.7939; time: 0.37s
Val loss: 0.2600 score: 0.9302 time: 0.20s
Test loss: 0.1818 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 104/1000, LR 0.000093
Train loss: 0.0322;  Loss pred: 0.0043; Loss self: 2.7905; time: 0.38s
Val loss: 0.2613 score: 0.9302 time: 0.20s
Test loss: 0.1849 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 105/1000, LR 0.000093
Train loss: 0.0320;  Loss pred: 0.0041; Loss self: 2.7903; time: 0.38s
Val loss: 0.2601 score: 0.9302 time: 0.20s
Test loss: 0.1849 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 106/1000, LR 0.000093
Train loss: 0.0323;  Loss pred: 0.0047; Loss self: 2.7602; time: 0.38s
Val loss: 0.2577 score: 0.9302 time: 0.20s
Test loss: 0.1822 score: 0.9302 time: 0.20s
Epoch 107/1000, LR 0.000093
Train loss: 0.0330;  Loss pred: 0.0053; Loss self: 2.7696; time: 0.38s
Val loss: 0.2581 score: 0.9302 time: 0.20s
Test loss: 0.1840 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000093
Train loss: 0.0317;  Loss pred: 0.0040; Loss self: 2.7657; time: 0.38s
Val loss: 0.2614 score: 0.9380 time: 0.20s
Test loss: 0.1847 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000093
Train loss: 0.0340;  Loss pred: 0.0067; Loss self: 2.7297; time: 0.38s
Val loss: 0.2675 score: 0.9380 time: 0.20s
Test loss: 0.1868 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000093
Train loss: 0.0319;  Loss pred: 0.0044; Loss self: 2.7490; time: 0.38s
Val loss: 0.2692 score: 0.9380 time: 0.20s
Test loss: 0.1873 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000093
Train loss: 0.0345;  Loss pred: 0.0077; Loss self: 2.6869; time: 0.34s
Val loss: 0.2747 score: 0.9302 time: 0.20s
Test loss: 0.2013 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000093
Train loss: 0.0313;  Loss pred: 0.0042; Loss self: 2.7146; time: 0.34s
Val loss: 0.2725 score: 0.9380 time: 0.20s
Test loss: 0.1984 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000093
Train loss: 0.0316;  Loss pred: 0.0044; Loss self: 2.7222; time: 0.35s
Val loss: 0.2696 score: 0.9380 time: 0.20s
Test loss: 0.1963 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000092
Train loss: 0.0307;  Loss pred: 0.0038; Loss self: 2.6895; time: 0.34s
Val loss: 0.2664 score: 0.9380 time: 0.20s
Test loss: 0.1930 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000092
Train loss: 0.0314;  Loss pred: 0.0043; Loss self: 2.7106; time: 0.35s
Val loss: 0.2604 score: 0.9380 time: 0.20s
Test loss: 0.1838 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000092
Train loss: 0.0308;  Loss pred: 0.0037; Loss self: 2.7088; time: 0.34s
Val loss: 0.2607 score: 0.9380 time: 0.20s
Test loss: 0.1880 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 117/1000, LR 0.000092
Train loss: 0.0304;  Loss pred: 0.0035; Loss self: 2.6955; time: 0.34s
Val loss: 0.2589 score: 0.9380 time: 0.20s
Test loss: 0.1877 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 118/1000, LR 0.000092
Train loss: 0.0315;  Loss pred: 0.0049; Loss self: 2.6629; time: 0.34s
Val loss: 0.2544 score: 0.9380 time: 0.20s
Test loss: 0.1808 score: 0.9302 time: 0.20s
Epoch 119/1000, LR 0.000092
Train loss: 0.0304;  Loss pred: 0.0035; Loss self: 2.6897; time: 0.34s
Val loss: 0.2543 score: 0.9380 time: 0.20s
Test loss: 0.1823 score: 0.9302 time: 0.20s
Epoch 120/1000, LR 0.000092
Train loss: 0.0300;  Loss pred: 0.0033; Loss self: 2.6697; time: 0.34s
Val loss: 0.2554 score: 0.9380 time: 0.20s
Test loss: 0.1863 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 121/1000, LR 0.000092
Train loss: 0.0300;  Loss pred: 0.0033; Loss self: 2.6752; time: 0.34s
Val loss: 0.2553 score: 0.9380 time: 0.20s
Test loss: 0.1872 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 122/1000, LR 0.000092
Train loss: 0.0310;  Loss pred: 0.0045; Loss self: 2.6474; time: 0.37s
Val loss: 0.2542 score: 0.9380 time: 0.25s
Test loss: 0.1849 score: 0.9302 time: 0.23s
Epoch 123/1000, LR 0.000092
Train loss: 0.0298;  Loss pred: 0.0032; Loss self: 2.6680; time: 0.34s
Val loss: 0.2567 score: 0.9302 time: 0.20s
Test loss: 0.1897 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000092
Train loss: 0.0306;  Loss pred: 0.0043; Loss self: 2.6374; time: 0.36s
Val loss: 0.2570 score: 0.9302 time: 0.20s
Test loss: 0.1922 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000092
Train loss: 0.0300;  Loss pred: 0.0035; Loss self: 2.6450; time: 0.43s
Val loss: 0.2544 score: 0.9302 time: 0.21s
Test loss: 0.1906 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000092
Train loss: 0.0303;  Loss pred: 0.0040; Loss self: 2.6289; time: 0.37s
Val loss: 0.2556 score: 0.9302 time: 0.22s
Test loss: 0.1944 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000092
Train loss: 0.0313;  Loss pred: 0.0054; Loss self: 2.5908; time: 0.43s
Val loss: 0.2520 score: 0.9380 time: 0.21s
Test loss: 0.1837 score: 0.9302 time: 0.21s
Epoch 128/1000, LR 0.000092
Train loss: 0.0302;  Loss pred: 0.0046; Loss self: 2.5583; time: 0.35s
Val loss: 0.2549 score: 0.9380 time: 0.21s
Test loss: 0.1886 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 129/1000, LR 0.000092
Train loss: 0.0296;  Loss pred: 0.0041; Loss self: 2.5537; time: 0.37s
Val loss: 0.2592 score: 0.9302 time: 0.21s
Test loss: 0.1942 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 130/1000, LR 0.000092
Train loss: 0.0290;  Loss pred: 0.0034; Loss self: 2.5633; time: 0.36s
Val loss: 0.2579 score: 0.9380 time: 0.21s
Test loss: 0.1916 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 131/1000, LR 0.000092
Train loss: 0.0290;  Loss pred: 0.0034; Loss self: 2.5581; time: 0.36s
Val loss: 0.2550 score: 0.9380 time: 0.23s
Test loss: 0.1871 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 132/1000, LR 0.000092
Train loss: 0.0295;  Loss pred: 0.0035; Loss self: 2.5971; time: 0.35s
Val loss: 0.2558 score: 0.9380 time: 0.21s
Test loss: 0.1909 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 133/1000, LR 0.000091
Train loss: 0.0289;  Loss pred: 0.0033; Loss self: 2.5655; time: 0.35s
Val loss: 0.2527 score: 0.9380 time: 0.23s
Test loss: 0.1895 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 134/1000, LR 0.000091
Train loss: 0.0291;  Loss pred: 0.0037; Loss self: 2.5409; time: 0.34s
Val loss: 0.2504 score: 0.9380 time: 0.20s
Test loss: 0.1889 score: 0.9302 time: 0.21s
Epoch 135/1000, LR 0.000091
Train loss: 0.0288;  Loss pred: 0.0034; Loss self: 2.5356; time: 0.36s
Val loss: 0.2477 score: 0.9380 time: 0.20s
Test loss: 0.1891 score: 0.9302 time: 0.20s
Epoch 136/1000, LR 0.000091
Train loss: 0.0288;  Loss pred: 0.0034; Loss self: 2.5355; time: 0.36s
Val loss: 0.2449 score: 0.9380 time: 0.20s
Test loss: 0.1883 score: 0.9302 time: 0.21s
Epoch 137/1000, LR 0.000091
Train loss: 0.0306;  Loss pred: 0.0053; Loss self: 2.5315; time: 0.35s
Val loss: 0.2499 score: 0.9380 time: 0.20s
Test loss: 0.1903 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 138/1000, LR 0.000091
Train loss: 0.0285;  Loss pred: 0.0033; Loss self: 2.5161; time: 0.44s
Val loss: 0.2551 score: 0.9302 time: 0.20s
Test loss: 0.1941 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 139/1000, LR 0.000091
Train loss: 0.0280;  Loss pred: 0.0031; Loss self: 2.4904; time: 0.35s
Val loss: 0.2601 score: 0.9147 time: 0.20s
Test loss: 0.1949 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 140/1000, LR 0.000091
Train loss: 0.0312;  Loss pred: 0.0064; Loss self: 2.4807; time: 0.45s
Val loss: 0.2479 score: 0.9302 time: 0.20s
Test loss: 0.1957 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 141/1000, LR 0.000091
Train loss: 0.0294;  Loss pred: 0.0046; Loss self: 2.4799; time: 0.35s
Val loss: 0.2450 score: 0.9302 time: 0.20s
Test loss: 0.1957 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 142/1000, LR 0.000091
Train loss: 0.0278;  Loss pred: 0.0028; Loss self: 2.5081; time: 0.37s
Val loss: 0.2474 score: 0.9302 time: 0.20s
Test loss: 0.1996 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 143/1000, LR 0.000091
Train loss: 0.0301;  Loss pred: 0.0055; Loss self: 2.4625; time: 0.35s
Val loss: 0.2530 score: 0.9380 time: 0.21s
Test loss: 0.2081 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 144/1000, LR 0.000091
Train loss: 0.0284;  Loss pred: 0.0034; Loss self: 2.4999; time: 0.35s
Val loss: 0.2499 score: 0.9380 time: 0.29s
Test loss: 0.2063 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 145/1000, LR 0.000091
Train loss: 0.0282;  Loss pred: 0.0035; Loss self: 2.4751; time: 0.35s
Val loss: 0.2489 score: 0.9380 time: 0.21s
Test loss: 0.2053 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 146/1000, LR 0.000091
Train loss: 0.0279;  Loss pred: 0.0035; Loss self: 2.4437; time: 0.35s
Val loss: 0.2483 score: 0.9302 time: 0.20s
Test loss: 0.1995 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 147/1000, LR 0.000091
Train loss: 0.0275;  Loss pred: 0.0032; Loss self: 2.4241; time: 0.35s
Val loss: 0.2497 score: 0.9302 time: 0.20s
Test loss: 0.1970 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 148/1000, LR 0.000091
Train loss: 0.0285;  Loss pred: 0.0043; Loss self: 2.4129; time: 0.35s
Val loss: 0.2540 score: 0.9380 time: 0.20s
Test loss: 0.2040 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 149/1000, LR 0.000091
Train loss: 0.0272;  Loss pred: 0.0031; Loss self: 2.4131; time: 0.35s
Val loss: 0.2527 score: 0.9380 time: 0.20s
Test loss: 0.1985 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 150/1000, LR 0.000090
Train loss: 0.0334;  Loss pred: 0.0101; Loss self: 2.3340; time: 0.35s
Val loss: 0.2607 score: 0.9380 time: 0.20s
Test loss: 0.2153 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 151/1000, LR 0.000090
Train loss: 0.0273;  Loss pred: 0.0034; Loss self: 2.3929; time: 0.37s
Val loss: 0.2587 score: 0.9380 time: 0.20s
Test loss: 0.2142 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 152/1000, LR 0.000090
Train loss: 0.0306;  Loss pred: 0.0070; Loss self: 2.3571; time: 0.38s
Val loss: 0.2541 score: 0.9380 time: 0.20s
Test loss: 0.2052 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 153/1000, LR 0.000090
Train loss: 0.0274;  Loss pred: 0.0033; Loss self: 2.4043; time: 0.38s
Val loss: 0.2516 score: 0.9380 time: 0.20s
Test loss: 0.1997 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 154/1000, LR 0.000090
Train loss: 0.0268;  Loss pred: 0.0026; Loss self: 2.4271; time: 0.38s
Val loss: 0.2511 score: 0.9380 time: 0.20s
Test loss: 0.1981 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 18 of 20
Epoch 155/1000, LR 0.000090
Train loss: 0.0265;  Loss pred: 0.0026; Loss self: 2.3878; time: 0.38s
Val loss: 0.2487 score: 0.9380 time: 0.20s
Test loss: 0.1963 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 156/1000, LR 0.000090
Train loss: 0.0266;  Loss pred: 0.0023; Loss self: 2.4371; time: 0.39s
Val loss: 0.2460 score: 0.9457 time: 0.20s
Test loss: 0.1917 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 135,   Train_Loss: 0.0288,   Val_Loss: 0.2449,   Val_Precision: 0.9516,   Val_Recall: 0.9219,   Val_accuracy: 0.9365,   Val_Score: 0.9380,   Val_Loss: 0.2449,   Test_Precision: 0.9828,   Test_Recall: 0.8769,   Test_accuracy: 0.9268,   Test_Score: 0.9302,   Test_loss: 0.1883


[0.21077407989650965, 0.21537906490266323, 0.21634501800872386, 0.20807799487374723, 0.20685311988927424, 0.20643880288116634, 0.2072238209657371, 0.20855602901428938, 0.2076018990483135, 0.2075978028587997, 0.20728521212004125, 0.20832980796694756, 0.20770305395126343, 0.20698213810101151, 0.20776816015131772, 0.2099443101324141, 0.21031241305172443, 0.21382166189141572, 0.21338531910441816, 0.20761129399761558, 0.20742053491994739, 0.20778083405457437, 0.20709742396138608, 0.20794816315174103, 0.20820470503531396, 0.20818779594264925, 0.2072365980129689, 0.20746631897054613, 0.20817499398253858, 0.2086920018773526, 0.20810139109380543, 0.2085354500450194, 0.20748926210217178, 0.2073981580324471, 0.20716743194498122, 0.20751606998965144, 0.20813109492883086, 0.20853000390343368, 0.20843680505640805, 0.20756950392387807, 0.20782600599341094, 0.20889487001113594, 0.2087252561468631, 0.20804683095775545, 0.20772464503534138, 0.21068357094191015, 0.21217137994244695, 0.20890227984637022, 0.20931675611063838, 0.207973618991673, 0.21195168513804674, 0.20984823093749583, 0.2113700001500547, 0.2816388390492648, 0.21085541904903948, 0.20585855306126177, 0.20858648605644703, 0.21059708506800234, 0.21083116997033358, 0.21275937487371266, 0.2164832390844822, 0.21142596285790205, 0.21121979597955942, 0.20733035309240222, 0.21267560217529535, 0.2098662790376693, 0.21283836290240288, 0.20780274597927928, 0.20820009405724704, 0.20829784497618675, 0.20788364298641682, 0.20829805405810475, 0.20861767884343863, 0.2085554921068251, 0.2074288879521191, 0.2081953170709312, 0.20858370419591665, 0.22131987195461988, 0.20982272690162063, 0.20825795084238052, 0.20861131604760885, 0.2077798480167985, 0.20876767695881426, 0.20836385595612228, 0.2086327641736716, 0.2084705219604075, 0.20794569700956345, 0.20753151015378535, 0.20743715995922685, 0.20805068709887564, 0.2078772340901196, 0.20763632911257446, 0.20753296604380012, 0.2081575330812484, 0.2076570219360292, 0.20781520009040833, 0.20824743388220668, 0.20811417698860168, 0.20829277206212282, 0.20870302198454738, 0.20802592998370528, 0.20897890906780958, 0.20793474605306983, 0.20805572299286723, 0.20804945891723037, 0.20845808391459286, 0.20807713200338185, 0.2091101291589439, 0.20840337802655995, 0.20895097288303077, 0.20828989194706082, 0.20802677189931273, 0.2072260188870132, 0.20783856604248285, 0.20761130098253489, 0.20816816482692957, 0.20798393595032394, 0.20921607199124992, 0.20740707498043776, 0.20847306190989912, 0.20918936794623733, 0.23556483490392566, 0.2117038480937481, 0.2117551148403436, 0.22030240390449762, 0.22611965984106064, 0.2150433100759983, 0.21287649497389793, 0.21656290208920836, 0.21469921409152448, 0.21317598200403154, 0.21463779918849468, 0.21509815589524806, 0.21521565620787442, 0.2079621721059084, 0.21124992612749338, 0.2134021590463817, 0.21223743283189833, 0.21396394283510745, 0.21335726091638207, 0.2147139240987599, 0.21387937595136464, 0.21436570794321597, 0.21848036791197956, 0.2166881780140102, 0.2126138391904533, 0.21119193406775594, 0.21084743598476052, 0.2112062219530344, 0.21065336605533957, 0.21116446377709508, 0.20989946299232543, 0.2103022609371692, 0.2100973930209875, 0.21024130191653967, 0.20979507197625935]
[0.001633907596096974, 0.001669605154284211, 0.0016770931628583244, 0.0016130077121995908, 0.0016035125572811957, 0.0016003007975284212, 0.0016063862090367218, 0.0016167134032115456, 0.001609317046886151, 0.0016092852934790674, 0.0016068621094576842, 0.001614959751681764, 0.0016101011934206468, 0.0016045126984574535, 0.0016106058926458739, 0.0016274752723442953, 0.001630328783346701, 0.0016575322627241527, 0.0016541497604993655, 0.0016093898759505084, 0.001607911123410445, 0.0016107041399579408, 0.0016054063872975666, 0.0016120012647421785, 0.0016139899615140616, 0.0016138588832763506, 0.0016064852559144877, 0.0016082660385313653, 0.001613759643275493, 0.001617767456413586, 0.0016131890782465537, 0.0016165538763179797, 0.0016084438922648976, 0.0016077376591662567, 0.0016059490848448156, 0.0016086517053461352, 0.0016134193405335725, 0.0016165116581661526, 0.0016157891864837833, 0.0016090659218905277, 0.0016110543100264414, 0.001619340077605705, 0.0016180252414485512, 0.0016127661314554685, 0.001610268566165437, 0.0016332059762938771, 0.0016447393793988135, 0.0016193975181889165, 0.0016226105124855688, 0.0016121985968346744, 0.0016430363188995871, 0.0016267304723836886, 0.0016385271329461605, 0.002183246814335386, 0.0016345381321630967, 0.001595802736909006, 0.0016169495043135429, 0.0016325355431628088, 0.0016343501548087874, 0.0016492974796411834, 0.0016781646440657534, 0.0016389609523868376, 0.0016373627595314684, 0.0016072120394759863, 0.0016486480788782585, 0.0016268703801369713, 0.0016499097899411075, 0.0016108739998393744, 0.001613954217498039, 0.0016147119765595873, 0.0016115011084218357, 0.0016147135973496492, 0.0016171913088638653, 0.0016167092411381791, 0.0016079758755978225, 0.0016139171865963658, 0.0016169279395032298, 0.0017156579221288364, 0.0016265327666792296, 0.0016144027197083763, 0.001617141984865185, 0.0016106964962542519, 0.0016183540849520485, 0.0016152236895823433, 0.001617308249408307, 0.0016160505578326162, 0.0016119821473609569, 0.0016087713965409717, 0.001608039999683929, 0.0016127960240222919, 0.0016114514270551908, 0.0016095839466091044, 0.0016087826825100784, 0.001613624287451538, 0.0016097443560932495, 0.0016109705433364986, 0.001614321192885323, 0.0016132881937100906, 0.001614672651644363, 0.0016178528836011425, 0.0016126041084008162, 0.0016199915431613146, 0.0016118972562253475, 0.001612835061960211, 0.001612786503234344, 0.001615954138872813, 0.00161300102328203, 0.0016210087531701076, 0.001615530062221395, 0.0016197749835893858, 0.001614650325171014, 0.0016126106348783933, 0.0016064032471861489, 0.001611151674747929, 0.0016093899300971696, 0.0016137067040847253, 0.0016122785732583251, 0.001621830015436046, 0.001607806782794091, 0.001616070247363559, 0.001621623007335173, 0.0018260839915032996, 0.0016411151015019233, 0.0016415125181421983, 0.0017077705729030823, 0.0017528655801632607, 0.0016670024036899093, 0.0016502053873945577, 0.0016787821867380493, 0.001664334992957554, 0.0016525269922793144, 0.0016638589084379432, 0.0016674275650794422, 0.0016683384202160808, 0.0016121098612861117, 0.0016375963265697162, 0.0016542803026851295, 0.0016452514173015374, 0.0016586352157760267, 0.0016539322551657526, 0.0016644490240213944, 0.0016579796585377104, 0.0016617496739784183, 0.0016936462628835624, 0.0016797533179380634, 0.0016481692960500255, 0.0016371467757190382, 0.00163447624794388, 0.0016372575345196465, 0.0016329718298863532, 0.0016369338277294192, 0.001627127620095546, 0.0016302500847842573, 0.0016286619614030039, 0.0016297775342367417, 0.0016263183874128632]
[612.0297147701424, 598.9440062723797, 596.2697971385605, 619.9598380322324, 623.6309129349946, 624.8825230509455, 622.5153044607221, 618.5388195666186, 621.3815990670629, 621.3938597786654, 622.3309356255216, 619.2104781302655, 621.0789757105317, 623.2421849707889, 620.8843544942074, 614.4486598309729, 613.3732104926855, 603.3065072027623, 604.5401836518802, 621.3534799387241, 621.9249219937974, 620.8464827228371, 622.8952419227214, 620.3469078294668, 619.5825400685353, 619.6328628001635, 622.4769236557684, 621.7876744529027, 619.6709678340153, 618.1358118161747, 619.8901377927403, 618.5998590270912, 621.7189202614152, 621.9920235734116, 622.6847472543819, 621.6386037304632, 619.8016689630675, 618.6160148912551, 618.892618149129, 621.478577350689, 620.7115388826262, 617.5355095753341, 618.0373299397612, 620.0527035482403, 621.0144202102379, 612.2926406804069, 607.9990620553639, 617.5136053798383, 616.2908426299831, 620.2709777587945, 608.6292728268743, 614.7299856839069, 610.3042054616122, 458.03341767587347, 611.7936194468778, 626.6438682370933, 618.4485027716054, 612.5440908089756, 611.8639858525274, 606.3187583464671, 595.8890884372715, 610.1426629741779, 610.7382094644379, 622.195438708908, 606.557586674532, 614.6771200762822, 606.0937428801452, 620.7810170750247, 619.5962618755107, 619.305495046037, 620.539442867224, 619.3048734099813, 618.3560315461599, 618.5404119394964, 621.8998774644018, 619.6104783473602, 618.4567509590012, 582.8667749566138, 614.8047063580682, 619.4241299225753, 618.3748918517914, 620.8494290051202, 617.9117470634553, 619.1092951704883, 618.3113209036376, 618.7925217767697, 620.3542648640009, 621.5923543581802, 621.8750778566184, 620.0412111049314, 620.5585742211459, 621.2785621443919, 621.5879937492648, 619.7229477621091, 621.2166523303977, 620.7438144268542, 619.4554122235558, 619.8520536496909, 619.3205780636788, 618.1031725048587, 620.115002058179, 617.2871730234845, 620.3869360394254, 620.0262032898875, 620.0448714039717, 618.8294432028476, 619.962408929701, 616.899815034534, 618.9918859355514, 617.369702663281, 619.3291416790728, 620.1124923595768, 622.5087018167119, 620.6740281956718, 621.3534590337739, 619.6912967323809, 620.2402094689231, 616.587429312769, 621.9652825833788, 618.7849826648254, 616.666139711047, 547.6199367898533, 609.3417817463354, 609.1942577031103, 585.5587488546981, 570.4944014628096, 599.8791590141084, 605.9851747174692, 595.6698896972727, 600.8405785081653, 605.1338372517049, 601.0124986732292, 599.7262015710747, 599.3987717854519, 620.3051194055835, 610.6511011139763, 604.4924783163164, 607.809839568564, 602.905322694556, 604.6196855262266, 600.7994150424316, 603.143708579616, 601.7753550123382, 590.4420668678614, 595.3255095977565, 606.7337878436293, 610.818782305452, 611.8167830569387, 610.777461038461, 612.3804352887062, 610.8982434476865, 614.5799429925966, 613.4028204220809, 614.0009552003992, 613.5806752719294, 614.8857491495214]
Elapsed: 0.2105058282106303~0.006818970768318124
Time per graph: 0.001631828125663801~5.286023851409398e-05
Speed: 613.3308733695778~16.203066478964434
Total Time: 0.2149
best val loss: 0.2448927688373112 test_score: 0.9302

Testing...
Test loss: 0.1917 score: 0.9302 time: 0.20s
test Score 0.9302
Epoch Time List: [0.7483848640695214, 0.7374442981090397, 0.7409708639606833, 0.760413448093459, 0.7418989648576826, 0.7403650030028075, 0.75159185170196, 0.7523283290211111, 0.7476979123894125, 0.7415372929535806, 0.7435277686454356, 0.7491247011348605, 0.7549312759656459, 0.7394155110232532, 0.7432230273261666, 0.7533226478844881, 0.7543988910038024, 0.7531934208236635, 0.7544673110824078, 0.7431574459187686, 0.7434383369982243, 0.7617481048218906, 0.7703023178037256, 0.7796354878228158, 0.7803856122773141, 0.7849571350961924, 0.7854468871373683, 0.7802303833886981, 0.7814423639792949, 0.7852837021928281, 0.7802291051484644, 0.7446288359351456, 0.7546312222257257, 0.7441122320014983, 0.7432553102262318, 0.7415627848822623, 0.7412496798206121, 0.7454490477684885, 0.7432418542448431, 0.7434837480541319, 0.743331837002188, 0.7450948290061206, 0.7437369357794523, 0.7448658160865307, 0.7510314339306206, 0.7633813330903649, 0.7546700318343937, 0.7689395558554679, 0.7493993530515581, 0.805955832125619, 0.773435469949618, 0.8404987859539688, 0.7589360771235079, 0.8329522400163114, 0.7463266018312424, 0.7447722288779914, 0.7587766807992011, 0.7611645301803946, 0.7950897919945419, 0.7656612761784345, 0.8549104160629213, 0.7825132999569178, 0.8297876899596304, 0.7555305717978626, 0.8105489530134946, 0.7790201487950981, 0.8526139531750232, 0.7674437069799751, 0.7747048179153353, 0.7875480130314827, 0.7811126499436796, 0.7845008359290659, 0.7830137850251049, 0.7807568931020796, 0.7829175260849297, 0.7846328699961305, 0.7858209277037531, 0.7965986451599747, 0.7865562997758389, 0.766752284951508, 0.7455445779487491, 0.7595435800030828, 0.7472046748735011, 0.7467085630632937, 0.7686700150370598, 0.770467430818826, 0.7756337558384985, 0.7784230499528348, 0.7844902048818767, 0.7838017579633743, 0.7665791288018227, 0.7435217211022973, 0.7554813297465444, 0.7432514331303537, 0.7442903011105955, 0.7425736498553306, 0.7444211528636515, 0.7450086921453476, 0.746801323024556, 0.7486347672529519, 0.745265529025346, 0.7551004490815103, 0.770586330909282, 0.7797730411402881, 0.7767910179682076, 0.7846995461732149, 0.7838088797871023, 0.7862324113957584, 0.7835488868877292, 0.7842967230826616, 0.7446508798748255, 0.7464256379753351, 0.7553770618978888, 0.744119827169925, 0.7542444900609553, 0.7450195141136646, 0.7457944059278816, 0.746885844040662, 0.7447919459082186, 0.7447269188705832, 0.7470489100087434, 0.8419382858555764, 0.7479037980083376, 0.7631480451673269, 0.8589736851863563, 0.8069767109118402, 0.8459795962553471, 0.7641300777904689, 0.7896191431209445, 0.7727833411190659, 0.7953427296597511, 0.761018788209185, 0.7920999031048268, 0.7575520111713558, 0.7615538889076561, 0.7731876119505614, 0.7573906818870455, 0.8482710788957775, 0.7588843149133027, 0.8583694270346314, 0.7614068696275353, 0.7829976899083704, 0.7655625340994447, 0.859243095619604, 0.7749459790065885, 0.7589751379564404, 0.7569540382828563, 0.7570979741867632, 0.7573706647381186, 0.7555772848427296, 0.7803064109757543, 0.7867769217118621, 0.7889990319963545, 0.7874351378995925, 0.7908840868622065, 0.7924023331142962]
Total Epoch List: [156]
Total Time List: [0.21488695894367993]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c6243ea8c0>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7336;  Loss pred: 0.7158; Loss self: 1.7786; time: 0.41s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6975 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6963 score: 0.5039 time: 0.20s
Epoch 2/1000, LR 0.000005
Train loss: 0.7371;  Loss pred: 0.7192; Loss self: 1.7956; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6984 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6970 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000015
Train loss: 0.7214;  Loss pred: 0.7035; Loss self: 1.7904; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6988 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6973 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.7043;  Loss pred: 0.6865; Loss self: 1.7758; time: 0.41s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6990 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6975 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000035
Train loss: 0.6822;  Loss pred: 0.6648; Loss self: 1.7395; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6991 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6976 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000045
Train loss: 0.6530;  Loss pred: 0.6351; Loss self: 1.7891; time: 0.41s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6992 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6977 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000055
Train loss: 0.6261;  Loss pred: 0.6078; Loss self: 1.8250; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6991 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6974 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000065
Train loss: 0.6002;  Loss pred: 0.5819; Loss self: 1.8356; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6988 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6968 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000075
Train loss: 0.5747;  Loss pred: 0.5561; Loss self: 1.8588; time: 0.41s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6973 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.5039 time: 0.20s
Epoch 10/1000, LR 0.000085
Train loss: 0.5474;  Loss pred: 0.5281; Loss self: 1.9243; time: 0.40s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6901 score: 0.5039 time: 0.20s
Epoch 11/1000, LR 0.000095
Train loss: 0.5163;  Loss pred: 0.4961; Loss self: 2.0188; time: 0.39s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6857 score: 0.4961 time: 0.28s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6802 score: 0.5039 time: 0.21s
Epoch 12/1000, LR 0.000095
Train loss: 0.4887;  Loss pred: 0.4681; Loss self: 2.0605; time: 0.37s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6698 score: 0.4961 time: 0.21s
Test loss: 0.6608 score: 0.5271 time: 0.20s
Epoch 13/1000, LR 0.000095
Train loss: 0.4624;  Loss pred: 0.4417; Loss self: 2.0726; time: 0.39s
Val loss: 0.6426 score: 0.5271 time: 0.29s
Test loss: 0.6290 score: 0.5426 time: 0.20s
Epoch 14/1000, LR 0.000095
Train loss: 0.4329;  Loss pred: 0.4113; Loss self: 2.1558; time: 0.54s
Val loss: 0.6057 score: 0.6202 time: 0.21s
Test loss: 0.5880 score: 0.6589 time: 0.22s
Epoch 15/1000, LR 0.000095
Train loss: 0.3966;  Loss pred: 0.3745; Loss self: 2.2041; time: 0.41s
Val loss: 0.5637 score: 0.7364 time: 0.21s
Test loss: 0.5432 score: 0.7287 time: 0.21s
Epoch 16/1000, LR 0.000095
Train loss: 0.3807;  Loss pred: 0.3580; Loss self: 2.2712; time: 0.40s
Val loss: 0.5231 score: 0.7597 time: 0.20s
Test loss: 0.5009 score: 0.7752 time: 0.20s
Epoch 17/1000, LR 0.000095
Train loss: 0.3489;  Loss pred: 0.3256; Loss self: 2.3271; time: 0.39s
Val loss: 0.4866 score: 0.7907 time: 0.22s
Test loss: 0.4646 score: 0.8217 time: 0.20s
Epoch 18/1000, LR 0.000095
Train loss: 0.3324;  Loss pred: 0.3085; Loss self: 2.3833; time: 0.40s
Val loss: 0.4582 score: 0.8140 time: 0.20s
Test loss: 0.4375 score: 0.8527 time: 0.20s
Epoch 19/1000, LR 0.000095
Train loss: 0.3023;  Loss pred: 0.2782; Loss self: 2.4061; time: 0.37s
Val loss: 0.4356 score: 0.8140 time: 0.21s
Test loss: 0.4167 score: 0.8527 time: 0.20s
Epoch 20/1000, LR 0.000095
Train loss: 0.2824;  Loss pred: 0.2578; Loss self: 2.4618; time: 0.40s
Val loss: 0.4183 score: 0.8140 time: 0.20s
Test loss: 0.4007 score: 0.8605 time: 0.20s
Epoch 21/1000, LR 0.000095
Train loss: 0.2630;  Loss pred: 0.2384; Loss self: 2.4587; time: 0.40s
Val loss: 0.4002 score: 0.8217 time: 0.20s
Test loss: 0.3841 score: 0.8682 time: 0.20s
Epoch 22/1000, LR 0.000095
Train loss: 0.2477;  Loss pred: 0.2227; Loss self: 2.4976; time: 0.41s
Val loss: 0.3857 score: 0.8295 time: 0.20s
Test loss: 0.3707 score: 0.8760 time: 0.20s
Epoch 23/1000, LR 0.000095
Train loss: 0.2326;  Loss pred: 0.2070; Loss self: 2.5627; time: 0.41s
Val loss: 0.3722 score: 0.8372 time: 0.20s
Test loss: 0.3580 score: 0.8760 time: 0.20s
Epoch 24/1000, LR 0.000095
Train loss: 0.2110;  Loss pred: 0.1851; Loss self: 2.5866; time: 0.41s
Val loss: 0.3592 score: 0.8450 time: 0.20s
Test loss: 0.3456 score: 0.8760 time: 0.20s
Epoch 25/1000, LR 0.000095
Train loss: 0.1950;  Loss pred: 0.1689; Loss self: 2.6175; time: 0.41s
Val loss: 0.3479 score: 0.8605 time: 0.20s
Test loss: 0.3350 score: 0.8837 time: 0.20s
Epoch 26/1000, LR 0.000095
Train loss: 0.1852;  Loss pred: 0.1590; Loss self: 2.6252; time: 0.41s
Val loss: 0.3338 score: 0.8605 time: 0.20s
Test loss: 0.3216 score: 0.8837 time: 0.20s
Epoch 27/1000, LR 0.000095
Train loss: 0.1674;  Loss pred: 0.1406; Loss self: 2.6835; time: 0.41s
Val loss: 0.3272 score: 0.8682 time: 0.20s
Test loss: 0.3156 score: 0.8915 time: 0.20s
Epoch 28/1000, LR 0.000095
Train loss: 0.1548;  Loss pred: 0.1280; Loss self: 2.6848; time: 0.41s
Val loss: 0.3247 score: 0.8682 time: 0.20s
Test loss: 0.3128 score: 0.8915 time: 0.20s
Epoch 29/1000, LR 0.000095
Train loss: 0.1415;  Loss pred: 0.1143; Loss self: 2.7151; time: 0.41s
Val loss: 0.3212 score: 0.8760 time: 0.20s
Test loss: 0.3088 score: 0.8915 time: 0.20s
Epoch 30/1000, LR 0.000095
Train loss: 0.1339;  Loss pred: 0.1065; Loss self: 2.7457; time: 0.41s
Val loss: 0.3206 score: 0.8760 time: 0.20s
Test loss: 0.3073 score: 0.8915 time: 0.20s
Epoch 31/1000, LR 0.000095
Train loss: 0.1249;  Loss pred: 0.0972; Loss self: 2.7665; time: 0.41s
Val loss: 0.3205 score: 0.8837 time: 0.20s
Test loss: 0.3060 score: 0.8915 time: 0.20s
Epoch 32/1000, LR 0.000095
Train loss: 0.1162;  Loss pred: 0.0883; Loss self: 2.7959; time: 0.41s
Val loss: 0.3208 score: 0.8837 time: 0.20s
Test loss: 0.3047 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000095
Train loss: 0.1151;  Loss pred: 0.0871; Loss self: 2.7990; time: 0.39s
Val loss: 0.3190 score: 0.8915 time: 0.21s
Test loss: 0.3014 score: 0.8992 time: 0.20s
Epoch 34/1000, LR 0.000095
Train loss: 0.1017;  Loss pred: 0.0732; Loss self: 2.8440; time: 0.39s
Val loss: 0.3229 score: 0.8837 time: 0.20s
Test loss: 0.3032 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000095
Train loss: 0.0995;  Loss pred: 0.0711; Loss self: 2.8475; time: 0.37s
Val loss: 0.3260 score: 0.8837 time: 0.21s
Test loss: 0.3041 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000095
Train loss: 0.0904;  Loss pred: 0.0616; Loss self: 2.8782; time: 0.38s
Val loss: 0.3240 score: 0.8837 time: 0.24s
Test loss: 0.3006 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000095
Train loss: 0.0839;  Loss pred: 0.0549; Loss self: 2.8957; time: 0.39s
Val loss: 0.3201 score: 0.8837 time: 0.21s
Test loss: 0.2951 score: 0.9070 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000095
Train loss: 0.0783;  Loss pred: 0.0491; Loss self: 2.9206; time: 0.38s
Val loss: 0.3156 score: 0.8915 time: 0.20s
Test loss: 0.2891 score: 0.9070 time: 0.20s
Epoch 39/1000, LR 0.000095
Train loss: 0.0739;  Loss pred: 0.0446; Loss self: 2.9225; time: 0.36s
Val loss: 0.3141 score: 0.8915 time: 0.21s
Test loss: 0.2864 score: 0.9147 time: 0.20s
Epoch 40/1000, LR 0.000095
Train loss: 0.0781;  Loss pred: 0.0487; Loss self: 2.9346; time: 0.37s
Val loss: 0.3196 score: 0.8915 time: 0.29s
Test loss: 0.2906 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000095
Train loss: 0.0668;  Loss pred: 0.0368; Loss self: 2.9936; time: 0.38s
Val loss: 0.3166 score: 0.8837 time: 0.22s
Test loss: 0.2871 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000095
Train loss: 0.0625;  Loss pred: 0.0327; Loss self: 2.9822; time: 0.38s
Val loss: 0.3191 score: 0.8837 time: 0.29s
Test loss: 0.2892 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000095
Train loss: 0.0653;  Loss pred: 0.0356; Loss self: 2.9788; time: 0.38s
Val loss: 0.3222 score: 0.8837 time: 0.22s
Test loss: 0.2914 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000095
Train loss: 0.0636;  Loss pred: 0.0337; Loss self: 2.9906; time: 0.38s
Val loss: 0.3142 score: 0.8837 time: 0.22s
Test loss: 0.2838 score: 0.9225 time: 0.29s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0588;  Loss pred: 0.0286; Loss self: 3.0129; time: 0.38s
Val loss: 0.3211 score: 0.8837 time: 0.22s
Test loss: 0.2898 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000095
Train loss: 0.0529;  Loss pred: 0.0227; Loss self: 3.0240; time: 0.38s
Val loss: 0.3250 score: 0.8837 time: 0.21s
Test loss: 0.2930 score: 0.9147 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000095
Train loss: 0.0518;  Loss pred: 0.0213; Loss self: 3.0490; time: 0.39s
Val loss: 0.3234 score: 0.8837 time: 0.20s
Test loss: 0.2915 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0501;  Loss pred: 0.0196; Loss self: 3.0502; time: 0.37s
Val loss: 0.3234 score: 0.8837 time: 0.21s
Test loss: 0.2916 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000095
Train loss: 0.0513;  Loss pred: 0.0207; Loss self: 3.0636; time: 0.37s
Val loss: 0.3198 score: 0.8837 time: 0.20s
Test loss: 0.2884 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000095
Train loss: 0.0470;  Loss pred: 0.0162; Loss self: 3.0793; time: 0.37s
Val loss: 0.3255 score: 0.8837 time: 0.21s
Test loss: 0.2942 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000095
Train loss: 0.0458;  Loss pred: 0.0151; Loss self: 3.0724; time: 0.44s
Val loss: 0.3252 score: 0.8837 time: 0.21s
Test loss: 0.2939 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000095
Train loss: 0.0447;  Loss pred: 0.0138; Loss self: 3.0971; time: 0.39s
Val loss: 0.3297 score: 0.8837 time: 0.20s
Test loss: 0.2979 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000095
Train loss: 0.0528;  Loss pred: 0.0223; Loss self: 3.0532; time: 0.38s
Val loss: 0.3477 score: 0.8760 time: 0.27s
Test loss: 0.3137 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000095
Train loss: 0.0450;  Loss pred: 0.0141; Loss self: 3.0853; time: 0.38s
Val loss: 0.3469 score: 0.8760 time: 0.21s
Test loss: 0.3126 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000095
Train loss: 0.0487;  Loss pred: 0.0182; Loss self: 3.0561; time: 0.39s
Val loss: 0.3403 score: 0.8837 time: 0.20s
Test loss: 0.3055 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000095
Train loss: 0.0410;  Loss pred: 0.0102; Loss self: 3.0836; time: 0.40s
Val loss: 0.3449 score: 0.8837 time: 0.20s
Test loss: 0.3098 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000094
Train loss: 0.0424;  Loss pred: 0.0116; Loss self: 3.0815; time: 0.41s
Val loss: 0.3451 score: 0.8837 time: 0.20s
Test loss: 0.3098 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000094
Train loss: 0.0453;  Loss pred: 0.0146; Loss self: 3.0735; time: 0.41s
Val loss: 0.3423 score: 0.8837 time: 0.20s
Test loss: 0.3066 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000094
Train loss: 0.0408;  Loss pred: 0.0098; Loss self: 3.0980; time: 0.41s
Val loss: 0.3504 score: 0.8837 time: 0.20s
Test loss: 0.3135 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.0739,   Val_Loss: 0.3141,   Val_Precision: 1.0000,   Val_Recall: 0.7846,   Val_accuracy: 0.8793,   Val_Score: 0.8915,   Val_Loss: 0.3141,   Test_Precision: 1.0000,   Test_Recall: 0.8281,   Test_accuracy: 0.9060,   Test_Score: 0.9147,   Test_loss: 0.2864


[0.21077407989650965, 0.21537906490266323, 0.21634501800872386, 0.20807799487374723, 0.20685311988927424, 0.20643880288116634, 0.2072238209657371, 0.20855602901428938, 0.2076018990483135, 0.2075978028587997, 0.20728521212004125, 0.20832980796694756, 0.20770305395126343, 0.20698213810101151, 0.20776816015131772, 0.2099443101324141, 0.21031241305172443, 0.21382166189141572, 0.21338531910441816, 0.20761129399761558, 0.20742053491994739, 0.20778083405457437, 0.20709742396138608, 0.20794816315174103, 0.20820470503531396, 0.20818779594264925, 0.2072365980129689, 0.20746631897054613, 0.20817499398253858, 0.2086920018773526, 0.20810139109380543, 0.2085354500450194, 0.20748926210217178, 0.2073981580324471, 0.20716743194498122, 0.20751606998965144, 0.20813109492883086, 0.20853000390343368, 0.20843680505640805, 0.20756950392387807, 0.20782600599341094, 0.20889487001113594, 0.2087252561468631, 0.20804683095775545, 0.20772464503534138, 0.21068357094191015, 0.21217137994244695, 0.20890227984637022, 0.20931675611063838, 0.207973618991673, 0.21195168513804674, 0.20984823093749583, 0.2113700001500547, 0.2816388390492648, 0.21085541904903948, 0.20585855306126177, 0.20858648605644703, 0.21059708506800234, 0.21083116997033358, 0.21275937487371266, 0.2164832390844822, 0.21142596285790205, 0.21121979597955942, 0.20733035309240222, 0.21267560217529535, 0.2098662790376693, 0.21283836290240288, 0.20780274597927928, 0.20820009405724704, 0.20829784497618675, 0.20788364298641682, 0.20829805405810475, 0.20861767884343863, 0.2085554921068251, 0.2074288879521191, 0.2081953170709312, 0.20858370419591665, 0.22131987195461988, 0.20982272690162063, 0.20825795084238052, 0.20861131604760885, 0.2077798480167985, 0.20876767695881426, 0.20836385595612228, 0.2086327641736716, 0.2084705219604075, 0.20794569700956345, 0.20753151015378535, 0.20743715995922685, 0.20805068709887564, 0.2078772340901196, 0.20763632911257446, 0.20753296604380012, 0.2081575330812484, 0.2076570219360292, 0.20781520009040833, 0.20824743388220668, 0.20811417698860168, 0.20829277206212282, 0.20870302198454738, 0.20802592998370528, 0.20897890906780958, 0.20793474605306983, 0.20805572299286723, 0.20804945891723037, 0.20845808391459286, 0.20807713200338185, 0.2091101291589439, 0.20840337802655995, 0.20895097288303077, 0.20828989194706082, 0.20802677189931273, 0.2072260188870132, 0.20783856604248285, 0.20761130098253489, 0.20816816482692957, 0.20798393595032394, 0.20921607199124992, 0.20740707498043776, 0.20847306190989912, 0.20918936794623733, 0.23556483490392566, 0.2117038480937481, 0.2117551148403436, 0.22030240390449762, 0.22611965984106064, 0.2150433100759983, 0.21287649497389793, 0.21656290208920836, 0.21469921409152448, 0.21317598200403154, 0.21463779918849468, 0.21509815589524806, 0.21521565620787442, 0.2079621721059084, 0.21124992612749338, 0.2134021590463817, 0.21223743283189833, 0.21396394283510745, 0.21335726091638207, 0.2147139240987599, 0.21387937595136464, 0.21436570794321597, 0.21848036791197956, 0.2166881780140102, 0.2126138391904533, 0.21119193406775594, 0.21084743598476052, 0.2112062219530344, 0.21065336605533957, 0.21116446377709508, 0.20989946299232543, 0.2103022609371692, 0.2100973930209875, 0.21024130191653967, 0.20979507197625935, 0.20890541188418865, 0.2110403380356729, 0.2070707289967686, 0.201418787939474, 0.2047637680079788, 0.20447802008129656, 0.20247753197327256, 0.21075445390306413, 0.20685764495283365, 0.2044489800464362, 0.21622290392406285, 0.20977668790146708, 0.20951851387508214, 0.22399798198603094, 0.21718662092462182, 0.20574976899661124, 0.20382328191772103, 0.20621936907991767, 0.20730362320318818, 0.20641568186692894, 0.20680088200606406, 0.20722736092284322, 0.20582603313960135, 0.20620910404250026, 0.20579220913350582, 0.20419152383692563, 0.20400966494344175, 0.2046591688413173, 0.2044632020406425, 0.20489376806654036, 0.20400683814659715, 0.205421247985214, 0.2048962397966534, 0.2080291579477489, 0.20700941001996398, 0.20675973407924175, 0.2090755410026759, 0.2051572899799794, 0.20824818592518568, 0.21688505401834846, 0.22241550404578447, 0.22038515890017152, 0.22280013584531844, 0.2966229319572449, 0.22070679487660527, 0.20523673994466662, 0.20562962698750198, 0.2061162230093032, 0.20602758903987706, 0.20711057912558317, 0.2048904651310295, 0.206690605962649, 0.2044826620258391, 0.20841682911850512, 0.2069350539240986, 0.20748949912376702, 0.20576416095718741, 0.20495332614518702, 0.2053583210799843]
[0.001633907596096974, 0.001669605154284211, 0.0016770931628583244, 0.0016130077121995908, 0.0016035125572811957, 0.0016003007975284212, 0.0016063862090367218, 0.0016167134032115456, 0.001609317046886151, 0.0016092852934790674, 0.0016068621094576842, 0.001614959751681764, 0.0016101011934206468, 0.0016045126984574535, 0.0016106058926458739, 0.0016274752723442953, 0.001630328783346701, 0.0016575322627241527, 0.0016541497604993655, 0.0016093898759505084, 0.001607911123410445, 0.0016107041399579408, 0.0016054063872975666, 0.0016120012647421785, 0.0016139899615140616, 0.0016138588832763506, 0.0016064852559144877, 0.0016082660385313653, 0.001613759643275493, 0.001617767456413586, 0.0016131890782465537, 0.0016165538763179797, 0.0016084438922648976, 0.0016077376591662567, 0.0016059490848448156, 0.0016086517053461352, 0.0016134193405335725, 0.0016165116581661526, 0.0016157891864837833, 0.0016090659218905277, 0.0016110543100264414, 0.001619340077605705, 0.0016180252414485512, 0.0016127661314554685, 0.001610268566165437, 0.0016332059762938771, 0.0016447393793988135, 0.0016193975181889165, 0.0016226105124855688, 0.0016121985968346744, 0.0016430363188995871, 0.0016267304723836886, 0.0016385271329461605, 0.002183246814335386, 0.0016345381321630967, 0.001595802736909006, 0.0016169495043135429, 0.0016325355431628088, 0.0016343501548087874, 0.0016492974796411834, 0.0016781646440657534, 0.0016389609523868376, 0.0016373627595314684, 0.0016072120394759863, 0.0016486480788782585, 0.0016268703801369713, 0.0016499097899411075, 0.0016108739998393744, 0.001613954217498039, 0.0016147119765595873, 0.0016115011084218357, 0.0016147135973496492, 0.0016171913088638653, 0.0016167092411381791, 0.0016079758755978225, 0.0016139171865963658, 0.0016169279395032298, 0.0017156579221288364, 0.0016265327666792296, 0.0016144027197083763, 0.001617141984865185, 0.0016106964962542519, 0.0016183540849520485, 0.0016152236895823433, 0.001617308249408307, 0.0016160505578326162, 0.0016119821473609569, 0.0016087713965409717, 0.001608039999683929, 0.0016127960240222919, 0.0016114514270551908, 0.0016095839466091044, 0.0016087826825100784, 0.001613624287451538, 0.0016097443560932495, 0.0016109705433364986, 0.001614321192885323, 0.0016132881937100906, 0.001614672651644363, 0.0016178528836011425, 0.0016126041084008162, 0.0016199915431613146, 0.0016118972562253475, 0.001612835061960211, 0.001612786503234344, 0.001615954138872813, 0.00161300102328203, 0.0016210087531701076, 0.001615530062221395, 0.0016197749835893858, 0.001614650325171014, 0.0016126106348783933, 0.0016064032471861489, 0.001611151674747929, 0.0016093899300971696, 0.0016137067040847253, 0.0016122785732583251, 0.001621830015436046, 0.001607806782794091, 0.001616070247363559, 0.001621623007335173, 0.0018260839915032996, 0.0016411151015019233, 0.0016415125181421983, 0.0017077705729030823, 0.0017528655801632607, 0.0016670024036899093, 0.0016502053873945577, 0.0016787821867380493, 0.001664334992957554, 0.0016525269922793144, 0.0016638589084379432, 0.0016674275650794422, 0.0016683384202160808, 0.0016121098612861117, 0.0016375963265697162, 0.0016542803026851295, 0.0016452514173015374, 0.0016586352157760267, 0.0016539322551657526, 0.0016644490240213944, 0.0016579796585377104, 0.0016617496739784183, 0.0016936462628835624, 0.0016797533179380634, 0.0016481692960500255, 0.0016371467757190382, 0.00163447624794388, 0.0016372575345196465, 0.0016329718298863532, 0.0016369338277294192, 0.001627127620095546, 0.0016302500847842573, 0.0016286619614030039, 0.0016297775342367417, 0.0016263183874128632, 0.00161942179755185, 0.0016359716126796348, 0.0016051994495873535, 0.001561385953019178, 0.0015873160310696031, 0.0015851009308627641, 0.00156959327111064, 0.0016337554566129002, 0.0016035476352932842, 0.0015848758143134588, 0.0016761465420469988, 0.0016261758752051712, 0.0016241745261634274, 0.0017364184650079917, 0.0016836172164699367, 0.0015949594495861337, 0.0015800254412226436, 0.0015985997603094393, 0.0016070048310324666, 0.0016001215648599142, 0.001603107612450109, 0.001606413650564676, 0.0015955506444930336, 0.001598520186375971, 0.001595288442895394, 0.0015828800297436095, 0.0015814702708793933, 0.0015865051848164132, 0.001584986062330562, 0.0015883237834615532, 0.0015814483577255593, 0.0015924127750791782, 0.0015883429441601038, 0.0016126291313778985, 0.0016047241086818913, 0.0016027886362731919, 0.00162074062792772, 0.0015903665889920884, 0.001614327022675858, 0.0016812794885143292, 0.001724151194153368, 0.0017084120844974536, 0.0017271328360102204, 0.002299402573311976, 0.001710905386640351, 0.0015909824801912142, 0.001594028116182186, 0.0015978001783666916, 0.0015971130933323803, 0.0016055083653145983, 0.0015882981793103061, 0.0016022527594003798, 0.0015851369149289852, 0.0016156343342519778, 0.0016041477048379738, 0.0016084457296416047, 0.0015950710151719954, 0.001588785473993698, 0.0015919249696122815]
[612.0297147701424, 598.9440062723797, 596.2697971385605, 619.9598380322324, 623.6309129349946, 624.8825230509455, 622.5153044607221, 618.5388195666186, 621.3815990670629, 621.3938597786654, 622.3309356255216, 619.2104781302655, 621.0789757105317, 623.2421849707889, 620.8843544942074, 614.4486598309729, 613.3732104926855, 603.3065072027623, 604.5401836518802, 621.3534799387241, 621.9249219937974, 620.8464827228371, 622.8952419227214, 620.3469078294668, 619.5825400685353, 619.6328628001635, 622.4769236557684, 621.7876744529027, 619.6709678340153, 618.1358118161747, 619.8901377927403, 618.5998590270912, 621.7189202614152, 621.9920235734116, 622.6847472543819, 621.6386037304632, 619.8016689630675, 618.6160148912551, 618.892618149129, 621.478577350689, 620.7115388826262, 617.5355095753341, 618.0373299397612, 620.0527035482403, 621.0144202102379, 612.2926406804069, 607.9990620553639, 617.5136053798383, 616.2908426299831, 620.2709777587945, 608.6292728268743, 614.7299856839069, 610.3042054616122, 458.03341767587347, 611.7936194468778, 626.6438682370933, 618.4485027716054, 612.5440908089756, 611.8639858525274, 606.3187583464671, 595.8890884372715, 610.1426629741779, 610.7382094644379, 622.195438708908, 606.557586674532, 614.6771200762822, 606.0937428801452, 620.7810170750247, 619.5962618755107, 619.305495046037, 620.539442867224, 619.3048734099813, 618.3560315461599, 618.5404119394964, 621.8998774644018, 619.6104783473602, 618.4567509590012, 582.8667749566138, 614.8047063580682, 619.4241299225753, 618.3748918517914, 620.8494290051202, 617.9117470634553, 619.1092951704883, 618.3113209036376, 618.7925217767697, 620.3542648640009, 621.5923543581802, 621.8750778566184, 620.0412111049314, 620.5585742211459, 621.2785621443919, 621.5879937492648, 619.7229477621091, 621.2166523303977, 620.7438144268542, 619.4554122235558, 619.8520536496909, 619.3205780636788, 618.1031725048587, 620.115002058179, 617.2871730234845, 620.3869360394254, 620.0262032898875, 620.0448714039717, 618.8294432028476, 619.962408929701, 616.899815034534, 618.9918859355514, 617.369702663281, 619.3291416790728, 620.1124923595768, 622.5087018167119, 620.6740281956718, 621.3534590337739, 619.6912967323809, 620.2402094689231, 616.587429312769, 621.9652825833788, 618.7849826648254, 616.666139711047, 547.6199367898533, 609.3417817463354, 609.1942577031103, 585.5587488546981, 570.4944014628096, 599.8791590141084, 605.9851747174692, 595.6698896972727, 600.8405785081653, 605.1338372517049, 601.0124986732292, 599.7262015710747, 599.3987717854519, 620.3051194055835, 610.6511011139763, 604.4924783163164, 607.809839568564, 602.905322694556, 604.6196855262266, 600.7994150424316, 603.143708579616, 601.7753550123382, 590.4420668678614, 595.3255095977565, 606.7337878436293, 610.818782305452, 611.8167830569387, 610.777461038461, 612.3804352887062, 610.8982434476865, 614.5799429925966, 613.4028204220809, 614.0009552003992, 613.5806752719294, 614.8857491495214, 617.5043472378495, 611.2575501001835, 622.9755437911898, 640.4566392225748, 629.9942673206394, 630.8746531715832, 637.1077261897299, 612.0867085415579, 623.6172708502688, 630.9642629212453, 596.6065465724417, 614.9396355260972, 615.6973797404444, 575.8980454031267, 593.9592386069286, 626.9751875256038, 632.9012014048251, 625.5474477278984, 622.2756650691094, 624.9525173342357, 623.7884420445426, 622.5046703559115, 626.742876167204, 625.5785873227632, 626.8458876220743, 631.759818311674, 632.3229835006252, 630.3162508200172, 630.9203744855655, 629.5945514463209, 632.3317452099423, 627.9778808922691, 629.5869564420722, 620.1053798064269, 623.1600775421718, 623.9125842102316, 617.0018711004984, 628.7858453023466, 619.4531751952162, 594.7851067187258, 579.995538321129, 585.3388705653882, 578.9942609799831, 434.8955731399554, 584.4858563240995, 628.5424336538349, 627.341506619766, 625.8604884011361, 626.1297363191092, 622.8556771200945, 629.604700821501, 624.1212531125462, 630.8603317365809, 618.9519365859416, 623.3839920002906, 621.7182100528943, 626.9313343971527, 629.4115954410889, 628.1703089584387]
Elapsed: 0.2102555028996755~0.008780555960105166
Time per graph: 0.0016298876193773294~6.806632527213306e-05
Speed: 614.378031390508~20.26272312830412
Total Time: 0.2062
best val loss: 0.3140817258593648 test_score: 0.9147

Testing...
Test loss: 0.3014 score: 0.8992 time: 0.20s
test Score 0.8992
Epoch Time List: [0.7483848640695214, 0.7374442981090397, 0.7409708639606833, 0.760413448093459, 0.7418989648576826, 0.7403650030028075, 0.75159185170196, 0.7523283290211111, 0.7476979123894125, 0.7415372929535806, 0.7435277686454356, 0.7491247011348605, 0.7549312759656459, 0.7394155110232532, 0.7432230273261666, 0.7533226478844881, 0.7543988910038024, 0.7531934208236635, 0.7544673110824078, 0.7431574459187686, 0.7434383369982243, 0.7617481048218906, 0.7703023178037256, 0.7796354878228158, 0.7803856122773141, 0.7849571350961924, 0.7854468871373683, 0.7802303833886981, 0.7814423639792949, 0.7852837021928281, 0.7802291051484644, 0.7446288359351456, 0.7546312222257257, 0.7441122320014983, 0.7432553102262318, 0.7415627848822623, 0.7412496798206121, 0.7454490477684885, 0.7432418542448431, 0.7434837480541319, 0.743331837002188, 0.7450948290061206, 0.7437369357794523, 0.7448658160865307, 0.7510314339306206, 0.7633813330903649, 0.7546700318343937, 0.7689395558554679, 0.7493993530515581, 0.805955832125619, 0.773435469949618, 0.8404987859539688, 0.7589360771235079, 0.8329522400163114, 0.7463266018312424, 0.7447722288779914, 0.7587766807992011, 0.7611645301803946, 0.7950897919945419, 0.7656612761784345, 0.8549104160629213, 0.7825132999569178, 0.8297876899596304, 0.7555305717978626, 0.8105489530134946, 0.7790201487950981, 0.8526139531750232, 0.7674437069799751, 0.7747048179153353, 0.7875480130314827, 0.7811126499436796, 0.7845008359290659, 0.7830137850251049, 0.7807568931020796, 0.7829175260849297, 0.7846328699961305, 0.7858209277037531, 0.7965986451599747, 0.7865562997758389, 0.766752284951508, 0.7455445779487491, 0.7595435800030828, 0.7472046748735011, 0.7467085630632937, 0.7686700150370598, 0.770467430818826, 0.7756337558384985, 0.7784230499528348, 0.7844902048818767, 0.7838017579633743, 0.7665791288018227, 0.7435217211022973, 0.7554813297465444, 0.7432514331303537, 0.7442903011105955, 0.7425736498553306, 0.7444211528636515, 0.7450086921453476, 0.746801323024556, 0.7486347672529519, 0.745265529025346, 0.7551004490815103, 0.770586330909282, 0.7797730411402881, 0.7767910179682076, 0.7846995461732149, 0.7838088797871023, 0.7862324113957584, 0.7835488868877292, 0.7842967230826616, 0.7446508798748255, 0.7464256379753351, 0.7553770618978888, 0.744119827169925, 0.7542444900609553, 0.7450195141136646, 0.7457944059278816, 0.746885844040662, 0.7447919459082186, 0.7447269188705832, 0.7470489100087434, 0.8419382858555764, 0.7479037980083376, 0.7631480451673269, 0.8589736851863563, 0.8069767109118402, 0.8459795962553471, 0.7641300777904689, 0.7896191431209445, 0.7727833411190659, 0.7953427296597511, 0.761018788209185, 0.7920999031048268, 0.7575520111713558, 0.7615538889076561, 0.7731876119505614, 0.7573906818870455, 0.8482710788957775, 0.7588843149133027, 0.8583694270346314, 0.7614068696275353, 0.7829976899083704, 0.7655625340994447, 0.859243095619604, 0.7749459790065885, 0.7589751379564404, 0.7569540382828563, 0.7570979741867632, 0.7573706647381186, 0.7555772848427296, 0.7803064109757543, 0.7867769217118621, 0.7889990319963545, 0.7874351378995925, 0.7908840868622065, 0.7924023331142962, 0.8239777679555118, 0.8096300610341132, 0.8145198130514473, 0.8054066570475698, 0.8049294308293611, 0.8063590149395168, 0.8009782510343939, 0.808291673893109, 0.813853009371087, 0.8014889848418534, 0.8758083709981292, 0.7819183322135359, 0.8825016838964075, 0.9733207209501415, 0.8357376509811729, 0.80691656190902, 0.810750151053071, 0.8009292229544371, 0.7807854178827256, 0.8019546000286937, 0.8053186840843409, 0.8124479800462723, 0.8134265311527997, 0.8160600878763944, 0.8164469581097364, 0.8127828119322658, 0.8090637987479568, 0.8095764240715653, 0.8112748120911419, 0.808784837834537, 0.8088994692079723, 0.8107914619613439, 0.7989037598017603, 0.7949369782581925, 0.7747763101942837, 0.820232461206615, 0.7991440643090755, 0.7876052767969668, 0.7743639131076634, 0.8719908848870546, 0.8114713358227164, 0.8815659009851515, 0.8149817066732794, 0.8928370780777186, 0.8111922740936279, 0.7812386269215494, 0.7944262530654669, 0.7784671189729124, 0.7745992108248174, 0.780079496325925, 0.8436277392320335, 0.7912150032352656, 0.8457292369566858, 0.7913924448657781, 0.7965572532266378, 0.8064145718235523, 0.8095762880984694, 0.8121963432058692, 0.8145318541210145]
Total Epoch List: [156, 59]
Total Time List: [0.21488695894367993, 0.20622854912653565]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c62433afe0>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7271;  Loss pred: 0.7123; Loss self: 1.4762; time: 0.45s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
Epoch 2/1000, LR 0.000007
Train loss: 0.7255;  Loss pred: 0.7104; Loss self: 1.5086; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5000 time: 0.20s
Epoch 3/1000, LR 0.000017
Train loss: 0.7078;  Loss pred: 0.6928; Loss self: 1.4974; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5000 time: 0.20s
Epoch 4/1000, LR 0.000027
Train loss: 0.6939;  Loss pred: 0.6792; Loss self: 1.4661; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5000 time: 0.20s
Epoch 5/1000, LR 0.000037
Train loss: 0.6655;  Loss pred: 0.6509; Loss self: 1.4596; time: 0.43s
Val loss: 0.6914 score: 0.7597 time: 0.21s
Test loss: 0.6914 score: 0.7969 time: 0.20s
Epoch 6/1000, LR 0.000047
Train loss: 0.6361;  Loss pred: 0.6214; Loss self: 1.4724; time: 0.44s
Val loss: 0.6897 score: 0.8527 time: 0.21s
Test loss: 0.6896 score: 0.8281 time: 0.20s
Epoch 7/1000, LR 0.000057
Train loss: 0.6020;  Loss pred: 0.5872; Loss self: 1.4884; time: 0.44s
Val loss: 0.6832 score: 0.8527 time: 0.21s
Test loss: 0.6833 score: 0.8281 time: 0.20s
Epoch 8/1000, LR 0.000067
Train loss: 0.5663;  Loss pred: 0.5509; Loss self: 1.5461; time: 0.44s
Val loss: 0.6683 score: 0.8760 time: 0.21s
Test loss: 0.6687 score: 0.8750 time: 0.20s
Epoch 9/1000, LR 0.000077
Train loss: 0.5345;  Loss pred: 0.5185; Loss self: 1.5975; time: 0.44s
Val loss: 0.6346 score: 0.8140 time: 0.21s
Test loss: 0.6367 score: 0.7656 time: 0.20s
Epoch 10/1000, LR 0.000087
Train loss: 0.4868;  Loss pred: 0.4694; Loss self: 1.7414; time: 0.44s
Val loss: 0.5703 score: 0.8372 time: 0.21s
Test loss: 0.5703 score: 0.8359 time: 0.20s
Epoch 11/1000, LR 0.000097
Train loss: 0.4406;  Loss pred: 0.4225; Loss self: 1.8048; time: 0.44s
Val loss: 0.5185 score: 0.8527 time: 0.21s
Test loss: 0.5132 score: 0.8438 time: 0.20s
Epoch 12/1000, LR 0.000097
Train loss: 0.4087;  Loss pred: 0.3894; Loss self: 1.9221; time: 0.44s
Val loss: 0.4896 score: 0.8372 time: 0.21s
Test loss: 0.4812 score: 0.8359 time: 0.20s
Epoch 13/1000, LR 0.000097
Train loss: 0.3620;  Loss pred: 0.3423; Loss self: 1.9671; time: 0.41s
Val loss: 0.4030 score: 0.8837 time: 0.21s
Test loss: 0.3965 score: 0.9141 time: 0.20s
Epoch 14/1000, LR 0.000097
Train loss: 0.3372;  Loss pred: 0.3165; Loss self: 2.0684; time: 0.43s
Val loss: 0.3647 score: 0.9070 time: 0.21s
Test loss: 0.3550 score: 0.9141 time: 0.20s
Epoch 15/1000, LR 0.000097
Train loss: 0.2953;  Loss pred: 0.2740; Loss self: 2.1330; time: 0.44s
Val loss: 0.3503 score: 0.8992 time: 0.21s
Test loss: 0.3429 score: 0.9141 time: 0.20s
Epoch 16/1000, LR 0.000097
Train loss: 0.2664;  Loss pred: 0.2443; Loss self: 2.2080; time: 0.44s
Val loss: 0.3323 score: 0.8760 time: 0.21s
Test loss: 0.3290 score: 0.9062 time: 0.20s
Epoch 17/1000, LR 0.000097
Train loss: 0.2422;  Loss pred: 0.2196; Loss self: 2.2628; time: 0.43s
Val loss: 0.4080 score: 0.8837 time: 0.21s
Test loss: 0.3830 score: 0.8906 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000097
Train loss: 0.2187;  Loss pred: 0.1955; Loss self: 2.3179; time: 0.42s
Val loss: 0.5169 score: 0.8527 time: 0.21s
Test loss: 0.4863 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000097
Train loss: 0.2101;  Loss pred: 0.1861; Loss self: 2.4033; time: 0.44s
Val loss: 0.3453 score: 0.8915 time: 0.21s
Test loss: 0.3244 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000097
Train loss: 0.1828;  Loss pred: 0.1592; Loss self: 2.3665; time: 0.44s
Val loss: 0.2793 score: 0.8992 time: 0.21s
Test loss: 0.2747 score: 0.9141 time: 0.20s
Epoch 21/1000, LR 0.000097
Train loss: 0.1708;  Loss pred: 0.1470; Loss self: 2.3800; time: 0.44s
Val loss: 0.2652 score: 0.8992 time: 0.21s
Test loss: 0.2625 score: 0.9141 time: 0.20s
Epoch 22/1000, LR 0.000097
Train loss: 0.1553;  Loss pred: 0.1309; Loss self: 2.4474; time: 0.43s
Val loss: 0.2587 score: 0.8837 time: 0.21s
Test loss: 0.2655 score: 0.9219 time: 0.20s
Epoch 23/1000, LR 0.000097
Train loss: 0.1390;  Loss pred: 0.1144; Loss self: 2.4595; time: 0.43s
Val loss: 0.2755 score: 0.9070 time: 0.21s
Test loss: 0.2731 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000097
Train loss: 0.1445;  Loss pred: 0.1198; Loss self: 2.4670; time: 0.40s
Val loss: 0.2700 score: 0.9070 time: 0.21s
Test loss: 0.2652 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000097
Train loss: 0.1244;  Loss pred: 0.0993; Loss self: 2.5026; time: 0.43s
Val loss: 0.3792 score: 0.8915 time: 0.21s
Test loss: 0.3709 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000097
Train loss: 0.1169;  Loss pred: 0.0918; Loss self: 2.5120; time: 0.43s
Val loss: 0.2688 score: 0.9070 time: 0.21s
Test loss: 0.2707 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000097
Train loss: 0.1083;  Loss pred: 0.0827; Loss self: 2.5528; time: 0.44s
Val loss: 0.4714 score: 0.8760 time: 0.21s
Test loss: 0.4717 score: 0.8594 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000097
Train loss: 0.1019;  Loss pred: 0.0758; Loss self: 2.6056; time: 0.44s
Val loss: 0.2898 score: 0.9147 time: 0.21s
Test loss: 0.2971 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000097
Train loss: 0.0983;  Loss pred: 0.0727; Loss self: 2.5624; time: 0.43s
Val loss: 0.2258 score: 0.9070 time: 0.21s
Test loss: 0.2498 score: 0.9219 time: 0.20s
Epoch 30/1000, LR 0.000097
Train loss: 0.0896;  Loss pred: 0.0635; Loss self: 2.6074; time: 0.43s
Val loss: 0.2272 score: 0.8992 time: 0.21s
Test loss: 0.2532 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000097
Train loss: 0.0826;  Loss pred: 0.0562; Loss self: 2.6412; time: 0.44s
Val loss: 0.3303 score: 0.8992 time: 0.21s
Test loss: 0.3467 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000097
Train loss: 0.0858;  Loss pred: 0.0593; Loss self: 2.6488; time: 0.43s
Val loss: 0.4808 score: 0.8837 time: 0.21s
Test loss: 0.4910 score: 0.8594 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000097
Train loss: 0.0768;  Loss pred: 0.0504; Loss self: 2.6418; time: 0.44s
Val loss: 0.5352 score: 0.8605 time: 0.21s
Test loss: 0.5496 score: 0.8516 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000097
Train loss: 0.0697;  Loss pred: 0.0429; Loss self: 2.6798; time: 0.44s
Val loss: 0.5735 score: 0.8450 time: 0.21s
Test loss: 0.5871 score: 0.8516 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000097
Train loss: 0.0693;  Loss pred: 0.0422; Loss self: 2.7135; time: 0.44s
Val loss: 1.3459 score: 0.6899 time: 0.21s
Test loss: 1.3608 score: 0.7031 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000097
Train loss: 0.0641;  Loss pred: 0.0368; Loss self: 2.7314; time: 0.44s
Val loss: 0.3851 score: 0.8992 time: 0.21s
Test loss: 0.4204 score: 0.8906 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000097
Train loss: 0.0616;  Loss pred: 0.0343; Loss self: 2.7322; time: 0.44s
Val loss: 0.3200 score: 0.9070 time: 0.21s
Test loss: 0.3693 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000096
Train loss: 0.0605;  Loss pred: 0.0329; Loss self: 2.7522; time: 0.44s
Val loss: 0.2590 score: 0.9147 time: 0.21s
Test loss: 0.3214 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000096
Train loss: 0.0563;  Loss pred: 0.0285; Loss self: 2.7878; time: 0.44s
Val loss: 0.6461 score: 0.8837 time: 0.22s
Test loss: 0.6952 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000096
Train loss: 0.0565;  Loss pred: 0.0288; Loss self: 2.7667; time: 0.44s
Val loss: 0.3011 score: 0.9147 time: 0.29s
Test loss: 0.3725 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000096
Train loss: 0.0534;  Loss pred: 0.0255; Loss self: 2.7862; time: 0.41s
Val loss: 0.4318 score: 0.8992 time: 0.21s
Test loss: 0.4796 score: 0.8906 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000096
Train loss: 0.0545;  Loss pred: 0.0269; Loss self: 2.7608; time: 0.40s
Val loss: 0.2283 score: 0.9147 time: 0.30s
Test loss: 0.2933 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000096
Train loss: 0.0514;  Loss pred: 0.0236; Loss self: 2.7785; time: 0.38s
Val loss: 0.2013 score: 0.9225 time: 0.21s
Test loss: 0.2650 score: 0.9219 time: 0.20s
Epoch 44/1000, LR 0.000096
Train loss: 0.0490;  Loss pred: 0.0213; Loss self: 2.7704; time: 0.40s
Val loss: 0.4564 score: 0.8992 time: 0.21s
Test loss: 0.5175 score: 0.8984 time: 0.29s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000096
Train loss: 0.0461;  Loss pred: 0.0184; Loss self: 2.7772; time: 0.40s
Val loss: 0.4479 score: 0.8915 time: 0.23s
Test loss: 0.4979 score: 0.8906 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000096
Train loss: 0.0461;  Loss pred: 0.0182; Loss self: 2.7853; time: 0.42s
Val loss: 0.5358 score: 0.8915 time: 0.22s
Test loss: 0.5816 score: 0.8828 time: 0.29s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000096
Train loss: 0.0516;  Loss pred: 0.0238; Loss self: 2.7747; time: 0.41s
Val loss: 0.7198 score: 0.8760 time: 0.21s
Test loss: 0.7782 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000096
Train loss: 0.0481;  Loss pred: 0.0201; Loss self: 2.7943; time: 0.42s
Val loss: 0.2995 score: 0.9225 time: 0.21s
Test loss: 0.3788 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 49/1000, LR 0.000096
Train loss: 0.0491;  Loss pred: 0.0212; Loss self: 2.7859; time: 0.47s
Val loss: 0.5125 score: 0.8915 time: 0.21s
Test loss: 0.5894 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 50/1000, LR 0.000096
Train loss: 0.0485;  Loss pred: 0.0210; Loss self: 2.7541; time: 0.39s
Val loss: 0.2486 score: 0.9147 time: 0.21s
Test loss: 0.3312 score: 0.9297 time: 0.28s
     INFO: Early stopping counter 7 of 20
Epoch 51/1000, LR 0.000096
Train loss: 0.0429;  Loss pred: 0.0151; Loss self: 2.7781; time: 0.39s
Val loss: 0.3900 score: 0.8992 time: 0.21s
Test loss: 0.4623 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 52/1000, LR 0.000096
Train loss: 0.0461;  Loss pred: 0.0183; Loss self: 2.7747; time: 0.41s
Val loss: 0.5429 score: 0.8992 time: 0.21s
Test loss: 0.6035 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 53/1000, LR 0.000096
Train loss: 0.0426;  Loss pred: 0.0151; Loss self: 2.7498; time: 0.41s
Val loss: 0.6805 score: 0.8760 time: 0.21s
Test loss: 0.7436 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 54/1000, LR 0.000096
Train loss: 0.0413;  Loss pred: 0.0135; Loss self: 2.7819; time: 0.41s
Val loss: 0.6801 score: 0.8760 time: 0.21s
Test loss: 0.7560 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 55/1000, LR 0.000096
Train loss: 0.0410;  Loss pred: 0.0134; Loss self: 2.7692; time: 0.47s
Val loss: 0.2405 score: 0.9070 time: 0.22s
Test loss: 0.3374 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 56/1000, LR 0.000096
Train loss: 0.0398;  Loss pred: 0.0119; Loss self: 2.7924; time: 0.43s
Val loss: 0.5135 score: 0.9070 time: 0.22s
Test loss: 0.6156 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 57/1000, LR 0.000096
Train loss: 0.0398;  Loss pred: 0.0119; Loss self: 2.7909; time: 0.41s
Val loss: 0.6908 score: 0.8915 time: 0.21s
Test loss: 0.7883 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 58/1000, LR 0.000096
Train loss: 0.0400;  Loss pred: 0.0120; Loss self: 2.8038; time: 0.40s
Val loss: 0.8524 score: 0.8760 time: 0.21s
Test loss: 0.9466 score: 0.8594 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 59/1000, LR 0.000096
Train loss: 0.0425;  Loss pred: 0.0149; Loss self: 2.7686; time: 0.43s
Val loss: 0.2836 score: 0.9070 time: 0.21s
Test loss: 0.3880 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 60/1000, LR 0.000096
Train loss: 0.0396;  Loss pred: 0.0118; Loss self: 2.7752; time: 0.39s
Val loss: 0.2240 score: 0.9225 time: 0.21s
Test loss: 0.3240 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 61/1000, LR 0.000096
Train loss: 0.0400;  Loss pred: 0.0125; Loss self: 2.7520; time: 0.45s
Val loss: 0.2120 score: 0.9225 time: 0.21s
Test loss: 0.3104 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 18 of 20
Epoch 62/1000, LR 0.000096
Train loss: 0.0386;  Loss pred: 0.0112; Loss self: 2.7387; time: 0.42s
Val loss: 1.2918 score: 0.7907 time: 0.21s
Test loss: 1.4033 score: 0.8047 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 63/1000, LR 0.000096
Train loss: 0.0393;  Loss pred: 0.0118; Loss self: 2.7453; time: 0.39s
Val loss: 0.4372 score: 0.8992 time: 0.21s
Test loss: 0.5485 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 042,   Train_Loss: 0.0514,   Val_Loss: 0.2013,   Val_Precision: 0.9508,   Val_Recall: 0.8923,   Val_accuracy: 0.9206,   Val_Score: 0.9225,   Val_Loss: 0.2013,   Test_Precision: 0.9655,   Test_Recall: 0.8750,   Test_accuracy: 0.9180,   Test_Score: 0.9219,   Test_loss: 0.2650


[0.21077407989650965, 0.21537906490266323, 0.21634501800872386, 0.20807799487374723, 0.20685311988927424, 0.20643880288116634, 0.2072238209657371, 0.20855602901428938, 0.2076018990483135, 0.2075978028587997, 0.20728521212004125, 0.20832980796694756, 0.20770305395126343, 0.20698213810101151, 0.20776816015131772, 0.2099443101324141, 0.21031241305172443, 0.21382166189141572, 0.21338531910441816, 0.20761129399761558, 0.20742053491994739, 0.20778083405457437, 0.20709742396138608, 0.20794816315174103, 0.20820470503531396, 0.20818779594264925, 0.2072365980129689, 0.20746631897054613, 0.20817499398253858, 0.2086920018773526, 0.20810139109380543, 0.2085354500450194, 0.20748926210217178, 0.2073981580324471, 0.20716743194498122, 0.20751606998965144, 0.20813109492883086, 0.20853000390343368, 0.20843680505640805, 0.20756950392387807, 0.20782600599341094, 0.20889487001113594, 0.2087252561468631, 0.20804683095775545, 0.20772464503534138, 0.21068357094191015, 0.21217137994244695, 0.20890227984637022, 0.20931675611063838, 0.207973618991673, 0.21195168513804674, 0.20984823093749583, 0.2113700001500547, 0.2816388390492648, 0.21085541904903948, 0.20585855306126177, 0.20858648605644703, 0.21059708506800234, 0.21083116997033358, 0.21275937487371266, 0.2164832390844822, 0.21142596285790205, 0.21121979597955942, 0.20733035309240222, 0.21267560217529535, 0.2098662790376693, 0.21283836290240288, 0.20780274597927928, 0.20820009405724704, 0.20829784497618675, 0.20788364298641682, 0.20829805405810475, 0.20861767884343863, 0.2085554921068251, 0.2074288879521191, 0.2081953170709312, 0.20858370419591665, 0.22131987195461988, 0.20982272690162063, 0.20825795084238052, 0.20861131604760885, 0.2077798480167985, 0.20876767695881426, 0.20836385595612228, 0.2086327641736716, 0.2084705219604075, 0.20794569700956345, 0.20753151015378535, 0.20743715995922685, 0.20805068709887564, 0.2078772340901196, 0.20763632911257446, 0.20753296604380012, 0.2081575330812484, 0.2076570219360292, 0.20781520009040833, 0.20824743388220668, 0.20811417698860168, 0.20829277206212282, 0.20870302198454738, 0.20802592998370528, 0.20897890906780958, 0.20793474605306983, 0.20805572299286723, 0.20804945891723037, 0.20845808391459286, 0.20807713200338185, 0.2091101291589439, 0.20840337802655995, 0.20895097288303077, 0.20828989194706082, 0.20802677189931273, 0.2072260188870132, 0.20783856604248285, 0.20761130098253489, 0.20816816482692957, 0.20798393595032394, 0.20921607199124992, 0.20740707498043776, 0.20847306190989912, 0.20918936794623733, 0.23556483490392566, 0.2117038480937481, 0.2117551148403436, 0.22030240390449762, 0.22611965984106064, 0.2150433100759983, 0.21287649497389793, 0.21656290208920836, 0.21469921409152448, 0.21317598200403154, 0.21463779918849468, 0.21509815589524806, 0.21521565620787442, 0.2079621721059084, 0.21124992612749338, 0.2134021590463817, 0.21223743283189833, 0.21396394283510745, 0.21335726091638207, 0.2147139240987599, 0.21387937595136464, 0.21436570794321597, 0.21848036791197956, 0.2166881780140102, 0.2126138391904533, 0.21119193406775594, 0.21084743598476052, 0.2112062219530344, 0.21065336605533957, 0.21116446377709508, 0.20989946299232543, 0.2103022609371692, 0.2100973930209875, 0.21024130191653967, 0.20979507197625935, 0.20890541188418865, 0.2110403380356729, 0.2070707289967686, 0.201418787939474, 0.2047637680079788, 0.20447802008129656, 0.20247753197327256, 0.21075445390306413, 0.20685764495283365, 0.2044489800464362, 0.21622290392406285, 0.20977668790146708, 0.20951851387508214, 0.22399798198603094, 0.21718662092462182, 0.20574976899661124, 0.20382328191772103, 0.20621936907991767, 0.20730362320318818, 0.20641568186692894, 0.20680088200606406, 0.20722736092284322, 0.20582603313960135, 0.20620910404250026, 0.20579220913350582, 0.20419152383692563, 0.20400966494344175, 0.2046591688413173, 0.2044632020406425, 0.20489376806654036, 0.20400683814659715, 0.205421247985214, 0.2048962397966534, 0.2080291579477489, 0.20700941001996398, 0.20675973407924175, 0.2090755410026759, 0.2051572899799794, 0.20824818592518568, 0.21688505401834846, 0.22241550404578447, 0.22038515890017152, 0.22280013584531844, 0.2966229319572449, 0.22070679487660527, 0.20523673994466662, 0.20562962698750198, 0.2061162230093032, 0.20602758903987706, 0.20711057912558317, 0.2048904651310295, 0.206690605962649, 0.2044826620258391, 0.20841682911850512, 0.2069350539240986, 0.20748949912376702, 0.20576416095718741, 0.20495332614518702, 0.2053583210799843, 0.20487021515145898, 0.20416437392123044, 0.20491716777905822, 0.20374929788522422, 0.20400342112407088, 0.20374316000379622, 0.20403552008792758, 0.20341109298169613, 0.20366835105232894, 0.20363435917533934, 0.20430586114525795, 0.20341668487526476, 0.20364905497990549, 0.20369472401216626, 0.20368634979240596, 0.2033447651192546, 0.20343037717975676, 0.2052937508560717, 0.20580904791131616, 0.2054296361748129, 0.2040377000812441, 0.20394197013229132, 0.20374700101092458, 0.20285896793939173, 0.2039021549280733, 0.20379939512349665, 0.2037910120561719, 0.20318017294630408, 0.2033712021075189, 0.20290446607396007, 0.2030680098105222, 0.2031543750781566, 0.2061497289687395, 0.20479531306773424, 0.20391957508400083, 0.2035625430289656, 0.20372697315178812, 0.20329788699746132, 0.20570827019400895, 0.20249354001134634, 0.2082323159556836, 0.20344583108089864, 0.20542931300587952, 0.2998478030785918, 0.22299999091774225, 0.29545802902430296, 0.20402190694585443, 0.20408352999947965, 0.2052063108421862, 0.28353070886805654, 0.2038101248908788, 0.20390543201938272, 0.20641697314567864, 0.20995354419574142, 0.2173797490540892, 0.21557976701296866, 0.20615107379853725, 0.20504106604494154, 0.20489685609936714, 0.20673536090180278, 0.2063896970357746, 0.20514552388340235, 0.20590090192854404]
[0.001633907596096974, 0.001669605154284211, 0.0016770931628583244, 0.0016130077121995908, 0.0016035125572811957, 0.0016003007975284212, 0.0016063862090367218, 0.0016167134032115456, 0.001609317046886151, 0.0016092852934790674, 0.0016068621094576842, 0.001614959751681764, 0.0016101011934206468, 0.0016045126984574535, 0.0016106058926458739, 0.0016274752723442953, 0.001630328783346701, 0.0016575322627241527, 0.0016541497604993655, 0.0016093898759505084, 0.001607911123410445, 0.0016107041399579408, 0.0016054063872975666, 0.0016120012647421785, 0.0016139899615140616, 0.0016138588832763506, 0.0016064852559144877, 0.0016082660385313653, 0.001613759643275493, 0.001617767456413586, 0.0016131890782465537, 0.0016165538763179797, 0.0016084438922648976, 0.0016077376591662567, 0.0016059490848448156, 0.0016086517053461352, 0.0016134193405335725, 0.0016165116581661526, 0.0016157891864837833, 0.0016090659218905277, 0.0016110543100264414, 0.001619340077605705, 0.0016180252414485512, 0.0016127661314554685, 0.001610268566165437, 0.0016332059762938771, 0.0016447393793988135, 0.0016193975181889165, 0.0016226105124855688, 0.0016121985968346744, 0.0016430363188995871, 0.0016267304723836886, 0.0016385271329461605, 0.002183246814335386, 0.0016345381321630967, 0.001595802736909006, 0.0016169495043135429, 0.0016325355431628088, 0.0016343501548087874, 0.0016492974796411834, 0.0016781646440657534, 0.0016389609523868376, 0.0016373627595314684, 0.0016072120394759863, 0.0016486480788782585, 0.0016268703801369713, 0.0016499097899411075, 0.0016108739998393744, 0.001613954217498039, 0.0016147119765595873, 0.0016115011084218357, 0.0016147135973496492, 0.0016171913088638653, 0.0016167092411381791, 0.0016079758755978225, 0.0016139171865963658, 0.0016169279395032298, 0.0017156579221288364, 0.0016265327666792296, 0.0016144027197083763, 0.001617141984865185, 0.0016106964962542519, 0.0016183540849520485, 0.0016152236895823433, 0.001617308249408307, 0.0016160505578326162, 0.0016119821473609569, 0.0016087713965409717, 0.001608039999683929, 0.0016127960240222919, 0.0016114514270551908, 0.0016095839466091044, 0.0016087826825100784, 0.001613624287451538, 0.0016097443560932495, 0.0016109705433364986, 0.001614321192885323, 0.0016132881937100906, 0.001614672651644363, 0.0016178528836011425, 0.0016126041084008162, 0.0016199915431613146, 0.0016118972562253475, 0.001612835061960211, 0.001612786503234344, 0.001615954138872813, 0.00161300102328203, 0.0016210087531701076, 0.001615530062221395, 0.0016197749835893858, 0.001614650325171014, 0.0016126106348783933, 0.0016064032471861489, 0.001611151674747929, 0.0016093899300971696, 0.0016137067040847253, 0.0016122785732583251, 0.001621830015436046, 0.001607806782794091, 0.001616070247363559, 0.001621623007335173, 0.0018260839915032996, 0.0016411151015019233, 0.0016415125181421983, 0.0017077705729030823, 0.0017528655801632607, 0.0016670024036899093, 0.0016502053873945577, 0.0016787821867380493, 0.001664334992957554, 0.0016525269922793144, 0.0016638589084379432, 0.0016674275650794422, 0.0016683384202160808, 0.0016121098612861117, 0.0016375963265697162, 0.0016542803026851295, 0.0016452514173015374, 0.0016586352157760267, 0.0016539322551657526, 0.0016644490240213944, 0.0016579796585377104, 0.0016617496739784183, 0.0016936462628835624, 0.0016797533179380634, 0.0016481692960500255, 0.0016371467757190382, 0.00163447624794388, 0.0016372575345196465, 0.0016329718298863532, 0.0016369338277294192, 0.001627127620095546, 0.0016302500847842573, 0.0016286619614030039, 0.0016297775342367417, 0.0016263183874128632, 0.00161942179755185, 0.0016359716126796348, 0.0016051994495873535, 0.001561385953019178, 0.0015873160310696031, 0.0015851009308627641, 0.00156959327111064, 0.0016337554566129002, 0.0016035476352932842, 0.0015848758143134588, 0.0016761465420469988, 0.0016261758752051712, 0.0016241745261634274, 0.0017364184650079917, 0.0016836172164699367, 0.0015949594495861337, 0.0015800254412226436, 0.0015985997603094393, 0.0016070048310324666, 0.0016001215648599142, 0.001603107612450109, 0.001606413650564676, 0.0015955506444930336, 0.001598520186375971, 0.001595288442895394, 0.0015828800297436095, 0.0015814702708793933, 0.0015865051848164132, 0.001584986062330562, 0.0015883237834615532, 0.0015814483577255593, 0.0015924127750791782, 0.0015883429441601038, 0.0016126291313778985, 0.0016047241086818913, 0.0016027886362731919, 0.00162074062792772, 0.0015903665889920884, 0.001614327022675858, 0.0016812794885143292, 0.001724151194153368, 0.0017084120844974536, 0.0017271328360102204, 0.002299402573311976, 0.001710905386640351, 0.0015909824801912142, 0.001594028116182186, 0.0015978001783666916, 0.0015971130933323803, 0.0016055083653145983, 0.0015882981793103061, 0.0016022527594003798, 0.0015851369149289852, 0.0016156343342519778, 0.0016041477048379738, 0.0016084457296416047, 0.0015950710151719954, 0.001588785473993698, 0.0015919249696122815, 0.0016005485558707733, 0.0015950341712596128, 0.0016009153732738923, 0.0015917913897283142, 0.0015937767275318038, 0.001591743437529658, 0.0015940275006869342, 0.001589149163919501, 0.0015911589925963199, 0.0015908934310573386, 0.0015961395401973277, 0.001589192850588006, 0.0015910082420305116, 0.001591365031345049, 0.0015912996077531716, 0.0015886309774941765, 0.0015892998217168497, 0.0016038574285630602, 0.0016078831868071575, 0.0016049190326157259, 0.0015940445318847196, 0.001593296641658526, 0.0015917734453978483, 0.001584835687026498, 0.0015929855853755726, 0.0015921827744023176, 0.001592117281688843, 0.0015873451011430006, 0.0015888375164649915, 0.001585191141202813, 0.0015864688266447047, 0.0015871435552980984, 0.0016105447575682774, 0.0015999633833416738, 0.0015931216803437565, 0.0015903323674137937, 0.0015916169777483447, 0.0015882647421676666, 0.001607095860890695, 0.0015819807813386433, 0.001626814968403778, 0.0015894205553195206, 0.0016049165078584338, 0.0023425609615514986, 0.0017421874290448613, 0.002308265851752367, 0.0015939211480144877, 0.0015944025781209348, 0.0016031743034545798, 0.0022150836630316917, 0.0015922666007099906, 0.0015930111876514275, 0.0016126326027006144, 0.0016402620640292298, 0.0016982792894850718, 0.0016842169297888177, 0.0016105552640510723, 0.0016018833284761058, 0.0016007566882763058, 0.0016151200070453342, 0.001612419508091989, 0.0016026994053390808, 0.0016086007963167503]
[612.0297147701424, 598.9440062723797, 596.2697971385605, 619.9598380322324, 623.6309129349946, 624.8825230509455, 622.5153044607221, 618.5388195666186, 621.3815990670629, 621.3938597786654, 622.3309356255216, 619.2104781302655, 621.0789757105317, 623.2421849707889, 620.8843544942074, 614.4486598309729, 613.3732104926855, 603.3065072027623, 604.5401836518802, 621.3534799387241, 621.9249219937974, 620.8464827228371, 622.8952419227214, 620.3469078294668, 619.5825400685353, 619.6328628001635, 622.4769236557684, 621.7876744529027, 619.6709678340153, 618.1358118161747, 619.8901377927403, 618.5998590270912, 621.7189202614152, 621.9920235734116, 622.6847472543819, 621.6386037304632, 619.8016689630675, 618.6160148912551, 618.892618149129, 621.478577350689, 620.7115388826262, 617.5355095753341, 618.0373299397612, 620.0527035482403, 621.0144202102379, 612.2926406804069, 607.9990620553639, 617.5136053798383, 616.2908426299831, 620.2709777587945, 608.6292728268743, 614.7299856839069, 610.3042054616122, 458.03341767587347, 611.7936194468778, 626.6438682370933, 618.4485027716054, 612.5440908089756, 611.8639858525274, 606.3187583464671, 595.8890884372715, 610.1426629741779, 610.7382094644379, 622.195438708908, 606.557586674532, 614.6771200762822, 606.0937428801452, 620.7810170750247, 619.5962618755107, 619.305495046037, 620.539442867224, 619.3048734099813, 618.3560315461599, 618.5404119394964, 621.8998774644018, 619.6104783473602, 618.4567509590012, 582.8667749566138, 614.8047063580682, 619.4241299225753, 618.3748918517914, 620.8494290051202, 617.9117470634553, 619.1092951704883, 618.3113209036376, 618.7925217767697, 620.3542648640009, 621.5923543581802, 621.8750778566184, 620.0412111049314, 620.5585742211459, 621.2785621443919, 621.5879937492648, 619.7229477621091, 621.2166523303977, 620.7438144268542, 619.4554122235558, 619.8520536496909, 619.3205780636788, 618.1031725048587, 620.115002058179, 617.2871730234845, 620.3869360394254, 620.0262032898875, 620.0448714039717, 618.8294432028476, 619.962408929701, 616.899815034534, 618.9918859355514, 617.369702663281, 619.3291416790728, 620.1124923595768, 622.5087018167119, 620.6740281956718, 621.3534590337739, 619.6912967323809, 620.2402094689231, 616.587429312769, 621.9652825833788, 618.7849826648254, 616.666139711047, 547.6199367898533, 609.3417817463354, 609.1942577031103, 585.5587488546981, 570.4944014628096, 599.8791590141084, 605.9851747174692, 595.6698896972727, 600.8405785081653, 605.1338372517049, 601.0124986732292, 599.7262015710747, 599.3987717854519, 620.3051194055835, 610.6511011139763, 604.4924783163164, 607.809839568564, 602.905322694556, 604.6196855262266, 600.7994150424316, 603.143708579616, 601.7753550123382, 590.4420668678614, 595.3255095977565, 606.7337878436293, 610.818782305452, 611.8167830569387, 610.777461038461, 612.3804352887062, 610.8982434476865, 614.5799429925966, 613.4028204220809, 614.0009552003992, 613.5806752719294, 614.8857491495214, 617.5043472378495, 611.2575501001835, 622.9755437911898, 640.4566392225748, 629.9942673206394, 630.8746531715832, 637.1077261897299, 612.0867085415579, 623.6172708502688, 630.9642629212453, 596.6065465724417, 614.9396355260972, 615.6973797404444, 575.8980454031267, 593.9592386069286, 626.9751875256038, 632.9012014048251, 625.5474477278984, 622.2756650691094, 624.9525173342357, 623.7884420445426, 622.5046703559115, 626.742876167204, 625.5785873227632, 626.8458876220743, 631.759818311674, 632.3229835006252, 630.3162508200172, 630.9203744855655, 629.5945514463209, 632.3317452099423, 627.9778808922691, 629.5869564420722, 620.1053798064269, 623.1600775421718, 623.9125842102316, 617.0018711004984, 628.7858453023466, 619.4531751952162, 594.7851067187258, 579.995538321129, 585.3388705653882, 578.9942609799831, 434.8955731399554, 584.4858563240995, 628.5424336538349, 627.341506619766, 625.8604884011361, 626.1297363191092, 622.8556771200945, 629.604700821501, 624.1212531125462, 630.8603317365809, 618.9519365859416, 623.3839920002906, 621.7182100528943, 626.9313343971527, 629.4115954410889, 628.1703089584387, 624.785793803021, 626.9458159697551, 624.6426367653571, 628.2230237284292, 627.4404580801265, 628.2419493131208, 627.3417488525496, 629.2675493932774, 628.4727074120253, 628.5776158717184, 626.5116393748204, 629.2502509245477, 628.5322562023676, 628.3913371872844, 628.4171724342631, 629.4728065654037, 629.2078979281232, 623.4968159831559, 621.9357277973302, 623.0843922201994, 627.3350461656485, 627.6295159695185, 628.230105792511, 630.980238636739, 627.7520708162803, 628.068596192033, 628.0944321760311, 629.9827298297827, 629.3909790252828, 630.8387512443571, 630.3306962009116, 630.0627291475089, 620.9079228011494, 625.0143037095049, 627.6984440913669, 628.7993758350054, 628.2918654302725, 629.6179556534122, 622.2404178464959, 632.1189307709657, 614.6980568916172, 629.1601028143054, 623.0853724187675, 426.8832343802448, 573.9910547674202, 433.22566126463715, 627.3836075552908, 627.1941689774102, 623.762492852563, 451.45021684252873, 628.0355309557461, 627.7419818214194, 620.1040449792087, 609.658677067459, 588.8312989456557, 593.7477425342049, 620.9038722984727, 624.2651897446951, 624.7045583653313, 619.1490388564863, 620.1859968708278, 623.9473207943391, 621.6582773611219]
Elapsed: 0.21004374964805922~0.011920076041552135
Time per graph: 0.0016311189457057765~9.280089506038202e-05
Speed: 614.570751623959~26.441533005459426
Total Time: 0.2070
best val loss: 0.201318972592437 test_score: 0.9219

Testing...
Test loss: 0.2650 score: 0.9219 time: 0.20s
test Score 0.9219
Epoch Time List: [0.7483848640695214, 0.7374442981090397, 0.7409708639606833, 0.760413448093459, 0.7418989648576826, 0.7403650030028075, 0.75159185170196, 0.7523283290211111, 0.7476979123894125, 0.7415372929535806, 0.7435277686454356, 0.7491247011348605, 0.7549312759656459, 0.7394155110232532, 0.7432230273261666, 0.7533226478844881, 0.7543988910038024, 0.7531934208236635, 0.7544673110824078, 0.7431574459187686, 0.7434383369982243, 0.7617481048218906, 0.7703023178037256, 0.7796354878228158, 0.7803856122773141, 0.7849571350961924, 0.7854468871373683, 0.7802303833886981, 0.7814423639792949, 0.7852837021928281, 0.7802291051484644, 0.7446288359351456, 0.7546312222257257, 0.7441122320014983, 0.7432553102262318, 0.7415627848822623, 0.7412496798206121, 0.7454490477684885, 0.7432418542448431, 0.7434837480541319, 0.743331837002188, 0.7450948290061206, 0.7437369357794523, 0.7448658160865307, 0.7510314339306206, 0.7633813330903649, 0.7546700318343937, 0.7689395558554679, 0.7493993530515581, 0.805955832125619, 0.773435469949618, 0.8404987859539688, 0.7589360771235079, 0.8329522400163114, 0.7463266018312424, 0.7447722288779914, 0.7587766807992011, 0.7611645301803946, 0.7950897919945419, 0.7656612761784345, 0.8549104160629213, 0.7825132999569178, 0.8297876899596304, 0.7555305717978626, 0.8105489530134946, 0.7790201487950981, 0.8526139531750232, 0.7674437069799751, 0.7747048179153353, 0.7875480130314827, 0.7811126499436796, 0.7845008359290659, 0.7830137850251049, 0.7807568931020796, 0.7829175260849297, 0.7846328699961305, 0.7858209277037531, 0.7965986451599747, 0.7865562997758389, 0.766752284951508, 0.7455445779487491, 0.7595435800030828, 0.7472046748735011, 0.7467085630632937, 0.7686700150370598, 0.770467430818826, 0.7756337558384985, 0.7784230499528348, 0.7844902048818767, 0.7838017579633743, 0.7665791288018227, 0.7435217211022973, 0.7554813297465444, 0.7432514331303537, 0.7442903011105955, 0.7425736498553306, 0.7444211528636515, 0.7450086921453476, 0.746801323024556, 0.7486347672529519, 0.745265529025346, 0.7551004490815103, 0.770586330909282, 0.7797730411402881, 0.7767910179682076, 0.7846995461732149, 0.7838088797871023, 0.7862324113957584, 0.7835488868877292, 0.7842967230826616, 0.7446508798748255, 0.7464256379753351, 0.7553770618978888, 0.744119827169925, 0.7542444900609553, 0.7450195141136646, 0.7457944059278816, 0.746885844040662, 0.7447919459082186, 0.7447269188705832, 0.7470489100087434, 0.8419382858555764, 0.7479037980083376, 0.7631480451673269, 0.8589736851863563, 0.8069767109118402, 0.8459795962553471, 0.7641300777904689, 0.7896191431209445, 0.7727833411190659, 0.7953427296597511, 0.761018788209185, 0.7920999031048268, 0.7575520111713558, 0.7615538889076561, 0.7731876119505614, 0.7573906818870455, 0.8482710788957775, 0.7588843149133027, 0.8583694270346314, 0.7614068696275353, 0.7829976899083704, 0.7655625340994447, 0.859243095619604, 0.7749459790065885, 0.7589751379564404, 0.7569540382828563, 0.7570979741867632, 0.7573706647381186, 0.7555772848427296, 0.7803064109757543, 0.7867769217118621, 0.7889990319963545, 0.7874351378995925, 0.7908840868622065, 0.7924023331142962, 0.8239777679555118, 0.8096300610341132, 0.8145198130514473, 0.8054066570475698, 0.8049294308293611, 0.8063590149395168, 0.8009782510343939, 0.808291673893109, 0.813853009371087, 0.8014889848418534, 0.8758083709981292, 0.7819183322135359, 0.8825016838964075, 0.9733207209501415, 0.8357376509811729, 0.80691656190902, 0.810750151053071, 0.8009292229544371, 0.7807854178827256, 0.8019546000286937, 0.8053186840843409, 0.8124479800462723, 0.8134265311527997, 0.8160600878763944, 0.8164469581097364, 0.8127828119322658, 0.8090637987479568, 0.8095764240715653, 0.8112748120911419, 0.808784837834537, 0.8088994692079723, 0.8107914619613439, 0.7989037598017603, 0.7949369782581925, 0.7747763101942837, 0.820232461206615, 0.7991440643090755, 0.7876052767969668, 0.7743639131076634, 0.8719908848870546, 0.8114713358227164, 0.8815659009851515, 0.8149817066732794, 0.8928370780777186, 0.8111922740936279, 0.7812386269215494, 0.7944262530654669, 0.7784671189729124, 0.7745992108248174, 0.780079496325925, 0.8436277392320335, 0.7912150032352656, 0.8457292369566858, 0.7913924448657781, 0.7965572532266378, 0.8064145718235523, 0.8095762880984694, 0.8121963432058692, 0.8145318541210145, 0.8576142929960042, 0.8447234441991895, 0.844541525002569, 0.8417506041005254, 0.8410667243879288, 0.8438759536948055, 0.8451197182293981, 0.8453904597554356, 0.8462887948844582, 0.8455382080283016, 0.8442411841824651, 0.8459814989473671, 0.8171502400655299, 0.8342041210271418, 0.841553466161713, 0.8447718690149486, 0.8397060020361096, 0.8309096181765199, 0.8470443640835583, 0.8456233912147582, 0.8438123783562332, 0.8396923600230366, 0.8386515588499606, 0.8057181779295206, 0.8320116971153766, 0.8399932149332017, 0.8411958329379559, 0.842086496995762, 0.8346203188411891, 0.8390529239550233, 0.8415032876655459, 0.8372961219865829, 0.8484838348813355, 0.8510103009175509, 0.8465627001132816, 0.8476610381621867, 0.848017190117389, 0.8446572371758521, 0.8594158000778407, 0.92980728414841, 0.825513314222917, 0.9041068661026657, 0.7944267739076167, 0.9054073877632618, 0.8387592460494488, 0.9315009478013963, 0.8156734579242766, 0.8230926762335002, 0.8758874894119799, 0.8798172860406339, 0.789926357800141, 0.8197159490082413, 0.8229719197843224, 0.8279416158329695, 0.9042058440390974, 0.8603675607591867, 0.8207142832688987, 0.8110855580307543, 0.8403437661472708, 0.7981593436561525, 0.861690920079127, 0.8302892269566655, 0.7936986610293388]
Total Epoch List: [156, 59, 63]
Total Time List: [0.21488695894367993, 0.20622854912653565, 0.206999332876876]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c62433a650>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7216;  Loss pred: 0.7042; Loss self: 1.7388; time: 0.36s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6942 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.5039 time: 0.21s
Epoch 2/1000, LR 0.000005
Train loss: 0.7188;  Loss pred: 0.7016; Loss self: 1.7249; time: 0.37s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6954 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000015
Train loss: 0.7028;  Loss pred: 0.6854; Loss self: 1.7395; time: 0.37s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6951 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.6808;  Loss pred: 0.6639; Loss self: 1.6849; time: 0.37s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6964 score: 0.4961 time: 0.35s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000035
Train loss: 0.6719;  Loss pred: 0.6547; Loss self: 1.7250; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6952 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000045
Train loss: 0.6338;  Loss pred: 0.6168; Loss self: 1.6978; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6960 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000055
Train loss: 0.6044;  Loss pred: 0.5871; Loss self: 1.7228; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6951 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000065
Train loss: 0.5846;  Loss pred: 0.5672; Loss self: 1.7396; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5039 time: 0.21s
Epoch 9/1000, LR 0.000075
Train loss: 0.5514;  Loss pred: 0.5335; Loss self: 1.7926; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6889 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6886 score: 0.5039 time: 0.21s
Epoch 10/1000, LR 0.000085
Train loss: 0.5082;  Loss pred: 0.4896; Loss self: 1.8588; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6802 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6811 score: 0.5039 time: 0.21s
Epoch 11/1000, LR 0.000095
Train loss: 0.4713;  Loss pred: 0.4520; Loss self: 1.9351; time: 0.38s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6629 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6660 score: 0.5039 time: 0.21s
Epoch 12/1000, LR 0.000095
Train loss: 0.4343;  Loss pred: 0.4141; Loss self: 2.0197; time: 0.38s
Val loss: 0.6326 score: 0.7597 time: 0.22s
Test loss: 0.6400 score: 0.7442 time: 0.21s
Epoch 13/1000, LR 0.000095
Train loss: 0.4134;  Loss pred: 0.3922; Loss self: 2.1127; time: 0.38s
Val loss: 0.5860 score: 0.8992 time: 0.22s
Test loss: 0.6009 score: 0.8682 time: 0.21s
Epoch 14/1000, LR 0.000095
Train loss: 0.3811;  Loss pred: 0.3599; Loss self: 2.1142; time: 0.38s
Val loss: 0.5226 score: 0.9302 time: 0.22s
Test loss: 0.5489 score: 0.8605 time: 0.21s
Epoch 15/1000, LR 0.000095
Train loss: 0.3503;  Loss pred: 0.3287; Loss self: 2.1591; time: 0.38s
Val loss: 0.4533 score: 0.9147 time: 0.22s
Test loss: 0.4936 score: 0.8682 time: 0.21s
Epoch 16/1000, LR 0.000095
Train loss: 0.3330;  Loss pred: 0.3112; Loss self: 2.1800; time: 0.38s
Val loss: 0.3908 score: 0.9147 time: 0.22s
Test loss: 0.4476 score: 0.8450 time: 0.21s
Epoch 17/1000, LR 0.000095
Train loss: 0.3051;  Loss pred: 0.2822; Loss self: 2.2895; time: 0.38s
Val loss: 0.3426 score: 0.9225 time: 0.21s
Test loss: 0.4164 score: 0.8450 time: 0.21s
Epoch 18/1000, LR 0.000095
Train loss: 0.2976;  Loss pred: 0.2745; Loss self: 2.3141; time: 0.38s
Val loss: 0.3061 score: 0.9147 time: 0.22s
Test loss: 0.3944 score: 0.8527 time: 0.21s
Epoch 19/1000, LR 0.000095
Train loss: 0.2643;  Loss pred: 0.2408; Loss self: 2.3438; time: 0.38s
Val loss: 0.2805 score: 0.9147 time: 0.22s
Test loss: 0.3793 score: 0.8527 time: 0.21s
Epoch 20/1000, LR 0.000095
Train loss: 0.2643;  Loss pred: 0.2410; Loss self: 2.3325; time: 0.38s
Val loss: 0.2630 score: 0.9070 time: 0.22s
Test loss: 0.3703 score: 0.8527 time: 0.21s
Epoch 21/1000, LR 0.000095
Train loss: 0.2447;  Loss pred: 0.2212; Loss self: 2.3539; time: 0.38s
Val loss: 0.2503 score: 0.9225 time: 0.22s
Test loss: 0.3577 score: 0.8682 time: 0.21s
Epoch 22/1000, LR 0.000095
Train loss: 0.2240;  Loss pred: 0.2000; Loss self: 2.4037; time: 0.38s
Val loss: 0.2437 score: 0.9225 time: 0.22s
Test loss: 0.3521 score: 0.8682 time: 0.21s
Epoch 23/1000, LR 0.000095
Train loss: 0.2073;  Loss pred: 0.1831; Loss self: 2.4207; time: 0.38s
Val loss: 0.2350 score: 0.9225 time: 0.22s
Test loss: 0.3436 score: 0.8605 time: 0.21s
Epoch 24/1000, LR 0.000095
Train loss: 0.1987;  Loss pred: 0.1744; Loss self: 2.4222; time: 0.38s
Val loss: 0.2283 score: 0.9225 time: 0.22s
Test loss: 0.3375 score: 0.8682 time: 0.21s
Epoch 25/1000, LR 0.000095
Train loss: 0.1818;  Loss pred: 0.1575; Loss self: 2.4316; time: 0.38s
Val loss: 0.2223 score: 0.9380 time: 0.22s
Test loss: 0.3322 score: 0.8760 time: 0.21s
Epoch 26/1000, LR 0.000095
Train loss: 0.1677;  Loss pred: 0.1425; Loss self: 2.5187; time: 0.38s
Val loss: 0.2168 score: 0.9380 time: 0.22s
Test loss: 0.3286 score: 0.8837 time: 0.21s
Epoch 27/1000, LR 0.000095
Train loss: 0.1887;  Loss pred: 0.1645; Loss self: 2.4151; time: 0.38s
Val loss: 0.2122 score: 0.9380 time: 0.22s
Test loss: 0.3201 score: 0.8915 time: 0.21s
Epoch 28/1000, LR 0.000095
Train loss: 0.1462;  Loss pred: 0.1211; Loss self: 2.5112; time: 0.38s
Val loss: 0.2085 score: 0.9380 time: 0.21s
Test loss: 0.3190 score: 0.8915 time: 0.21s
Epoch 29/1000, LR 0.000095
Train loss: 0.1466;  Loss pred: 0.1208; Loss self: 2.5738; time: 0.38s
Val loss: 0.2052 score: 0.9380 time: 0.21s
Test loss: 0.3187 score: 0.8915 time: 0.21s
Epoch 30/1000, LR 0.000095
Train loss: 0.1471;  Loss pred: 0.1213; Loss self: 2.5751; time: 0.38s
Val loss: 0.2052 score: 0.9380 time: 0.22s
Test loss: 0.3181 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000095
Train loss: 0.1236;  Loss pred: 0.0980; Loss self: 2.5642; time: 0.38s
Val loss: 0.2049 score: 0.9380 time: 0.21s
Test loss: 0.3192 score: 0.8837 time: 0.21s
Epoch 32/1000, LR 0.000095
Train loss: 0.1218;  Loss pred: 0.0961; Loss self: 2.5776; time: 0.38s
Val loss: 0.2048 score: 0.9380 time: 0.22s
Test loss: 0.3203 score: 0.8760 time: 0.21s
Epoch 33/1000, LR 0.000095
Train loss: 0.1151;  Loss pred: 0.0889; Loss self: 2.6160; time: 0.36s
Val loss: 0.2045 score: 0.9380 time: 0.22s
Test loss: 0.3195 score: 0.8760 time: 0.21s
Epoch 34/1000, LR 0.000095
Train loss: 0.1100;  Loss pred: 0.0838; Loss self: 2.6242; time: 0.34s
Val loss: 0.2045 score: 0.9380 time: 0.22s
Test loss: 0.3172 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000095
Train loss: 0.1084;  Loss pred: 0.0822; Loss self: 2.6139; time: 0.34s
Val loss: 0.2025 score: 0.9380 time: 0.22s
Test loss: 0.3129 score: 0.8837 time: 0.21s
Epoch 36/1000, LR 0.000095
Train loss: 0.1017;  Loss pred: 0.0754; Loss self: 2.6277; time: 0.34s
Val loss: 0.2005 score: 0.9457 time: 0.22s
Test loss: 0.3083 score: 0.8837 time: 0.21s
Epoch 37/1000, LR 0.000095
Train loss: 0.0930;  Loss pred: 0.0664; Loss self: 2.6628; time: 0.34s
Val loss: 0.1989 score: 0.9457 time: 0.21s
Test loss: 0.3065 score: 0.8837 time: 0.21s
Epoch 38/1000, LR 0.000095
Train loss: 0.0918;  Loss pred: 0.0651; Loss self: 2.6695; time: 0.34s
Val loss: 0.1958 score: 0.9457 time: 0.22s
Test loss: 0.2992 score: 0.8837 time: 0.21s
Epoch 39/1000, LR 0.000095
Train loss: 0.1069;  Loss pred: 0.0806; Loss self: 2.6278; time: 0.34s
Val loss: 0.1959 score: 0.9457 time: 0.22s
Test loss: 0.2910 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000095
Train loss: 0.0814;  Loss pred: 0.0546; Loss self: 2.6757; time: 0.34s
Val loss: 0.1954 score: 0.9457 time: 0.21s
Test loss: 0.2930 score: 0.8837 time: 0.21s
Epoch 41/1000, LR 0.000095
Train loss: 0.0778;  Loss pred: 0.0508; Loss self: 2.7074; time: 0.36s
Val loss: 0.1944 score: 0.9457 time: 0.22s
Test loss: 0.2950 score: 0.8837 time: 0.21s
Epoch 42/1000, LR 0.000095
Train loss: 0.0721;  Loss pred: 0.0454; Loss self: 2.6763; time: 0.37s
Val loss: 0.1922 score: 0.9535 time: 0.21s
Test loss: 0.2930 score: 0.8837 time: 0.21s
Epoch 43/1000, LR 0.000095
Train loss: 0.0698;  Loss pred: 0.0427; Loss self: 2.7131; time: 0.37s
Val loss: 0.1913 score: 0.9457 time: 0.22s
Test loss: 0.2948 score: 0.8837 time: 0.21s
Epoch 44/1000, LR 0.000095
Train loss: 0.0645;  Loss pred: 0.0371; Loss self: 2.7393; time: 0.46s
Val loss: 0.1915 score: 0.9457 time: 0.22s
Test loss: 0.2953 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0627;  Loss pred: 0.0352; Loss self: 2.7542; time: 0.34s
Val loss: 0.1914 score: 0.9457 time: 0.22s
Test loss: 0.2947 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000095
Train loss: 0.0671;  Loss pred: 0.0394; Loss self: 2.7646; time: 0.36s
Val loss: 0.1926 score: 0.9457 time: 0.30s
Test loss: 0.2937 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000095
Train loss: 0.0608;  Loss pred: 0.0333; Loss self: 2.7504; time: 0.33s
Val loss: 0.1950 score: 0.9380 time: 0.22s
Test loss: 0.2912 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0554;  Loss pred: 0.0279; Loss self: 2.7547; time: 0.35s
Val loss: 0.1977 score: 0.9380 time: 0.22s
Test loss: 0.2896 score: 0.8760 time: 0.29s
     INFO: Early stopping counter 5 of 20
Epoch 49/1000, LR 0.000095
Train loss: 0.0529;  Loss pred: 0.0259; Loss self: 2.7047; time: 0.34s
Val loss: 0.1992 score: 0.9302 time: 0.22s
Test loss: 0.2861 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 50/1000, LR 0.000095
Train loss: 0.0584;  Loss pred: 0.0311; Loss self: 2.7234; time: 0.35s
Val loss: 0.2009 score: 0.9302 time: 0.22s
Test loss: 0.2862 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 51/1000, LR 0.000095
Train loss: 0.0535;  Loss pred: 0.0260; Loss self: 2.7507; time: 0.35s
Val loss: 0.2013 score: 0.9457 time: 0.22s
Test loss: 0.2809 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 52/1000, LR 0.000095
Train loss: 0.0516;  Loss pred: 0.0241; Loss self: 2.7550; time: 0.34s
Val loss: 0.2023 score: 0.9457 time: 0.22s
Test loss: 0.2799 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 53/1000, LR 0.000095
Train loss: 0.0467;  Loss pred: 0.0190; Loss self: 2.7709; time: 0.34s
Val loss: 0.2032 score: 0.9380 time: 0.34s
Test loss: 0.2772 score: 0.8915 time: 0.23s
     INFO: Early stopping counter 10 of 20
Epoch 54/1000, LR 0.000095
Train loss: 0.0477;  Loss pred: 0.0202; Loss self: 2.7458; time: 0.35s
Val loss: 0.2040 score: 0.9380 time: 0.23s
Test loss: 0.2724 score: 0.8915 time: 0.23s
     INFO: Early stopping counter 11 of 20
Epoch 55/1000, LR 0.000095
Train loss: 0.0530;  Loss pred: 0.0258; Loss self: 2.7183; time: 0.35s
Val loss: 0.2070 score: 0.9380 time: 0.30s
Test loss: 0.2690 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 56/1000, LR 0.000095
Train loss: 0.0460;  Loss pred: 0.0182; Loss self: 2.7803; time: 0.36s
Val loss: 0.2098 score: 0.9380 time: 0.23s
Test loss: 0.2693 score: 0.8915 time: 0.23s
     INFO: Early stopping counter 13 of 20
Epoch 57/1000, LR 0.000094
Train loss: 0.0440;  Loss pred: 0.0161; Loss self: 2.7913; time: 0.37s
Val loss: 0.2123 score: 0.9380 time: 0.23s
Test loss: 0.2720 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 58/1000, LR 0.000094
Train loss: 0.0446;  Loss pred: 0.0166; Loss self: 2.7924; time: 0.34s
Val loss: 0.2136 score: 0.9380 time: 0.22s
Test loss: 0.2722 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 59/1000, LR 0.000094
Train loss: 0.0416;  Loss pred: 0.0135; Loss self: 2.8039; time: 0.35s
Val loss: 0.2154 score: 0.9380 time: 0.22s
Test loss: 0.2725 score: 0.8915 time: 0.30s
     INFO: Early stopping counter 16 of 20
Epoch 60/1000, LR 0.000094
Train loss: 0.0533;  Loss pred: 0.0257; Loss self: 2.7533; time: 0.34s
Val loss: 0.2187 score: 0.9302 time: 0.22s
Test loss: 0.2777 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 61/1000, LR 0.000094
Train loss: 0.0428;  Loss pred: 0.0150; Loss self: 2.7837; time: 0.35s
Val loss: 0.2221 score: 0.9302 time: 0.22s
Test loss: 0.2751 score: 0.8837 time: 0.27s
     INFO: Early stopping counter 18 of 20
Epoch 62/1000, LR 0.000094
Train loss: 0.0400;  Loss pred: 0.0123; Loss self: 2.7695; time: 0.34s
Val loss: 0.2252 score: 0.9302 time: 0.22s
Test loss: 0.2708 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 63/1000, LR 0.000094
Train loss: 0.0405;  Loss pred: 0.0128; Loss self: 2.7697; time: 0.35s
Val loss: 0.2291 score: 0.9302 time: 0.22s
Test loss: 0.2692 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 042,   Train_Loss: 0.0698,   Val_Loss: 0.1913,   Val_Precision: 0.9385,   Val_Recall: 0.9531,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1913,   Test_Precision: 0.9310,   Test_Recall: 0.8308,   Test_accuracy: 0.8780,   Test_Score: 0.8837,   Test_loss: 0.2948


[0.213627491844818, 0.213681201916188, 0.2153941229917109, 0.2171111151110381, 0.21321348496712744, 0.21267742291092873, 0.21448990097269416, 0.21426719520241022, 0.21348713105544448, 0.21388688497245312, 0.2135059661231935, 0.21347777708433568, 0.21354604698717594, 0.2132729620207101, 0.21323773497715592, 0.21338571701198816, 0.2129195670131594, 0.21790446690283716, 0.21425099507905543, 0.2157651069574058, 0.2136197059880942, 0.2135233930312097, 0.21385216410271823, 0.21451947605237365, 0.21339625492691994, 0.21368708601221442, 0.2140804030932486, 0.21379256411455572, 0.2138300638180226, 0.21470457408577204, 0.2134073032066226, 0.2128003619145602, 0.21548200398683548, 0.21363969892263412, 0.2142296361271292, 0.21356197097338736, 0.217106516007334, 0.21599516505375504, 0.21808111807331443, 0.21359476703219116, 0.21310974098742008, 0.21412140596657991, 0.21472347993403673, 0.21728904685005546, 0.21944403694942594, 0.21634388901293278, 0.21737774298526347, 0.2911536870524287, 0.21699251607060432, 0.21538386191241443, 0.21608945704065263, 0.21815499011427164, 0.23112457199022174, 0.23227475909516215, 0.22921004402451217, 0.23193705989979208, 0.2217349992133677, 0.22111501800827682, 0.3023038150276989, 0.21856707311235368, 0.27231076383031905, 0.21670517302118242, 0.21631299308501184]
[0.0016560270685644805, 0.0016564434257068837, 0.0016697218836566738, 0.0016830319000855666, 0.0016528177129234685, 0.0016486621931079747, 0.00166271241064104, 0.0016609860093210094, 0.0016549390004298023, 0.0016580378680035126, 0.0016550850087069264, 0.001654866489025858, 0.0016553957130788833, 0.001653278775354342, 0.0016530056974973328, 0.0016541528450541717, 0.0016505392791717783, 0.0016891819139754818, 0.001660860426969422, 0.001672597728351983, 0.0016559667130860017, 0.0016552201010171296, 0.00165776871397456, 0.0016629416748246018, 0.0016542345343172088, 0.0016564890388543754, 0.0016595380084747954, 0.0016573066985624474, 0.0016575973939381597, 0.0016643765433005584, 0.0016543201798962992, 0.0016496152086400014, 0.0016704031316808951, 0.0016561216970746831, 0.0016606948536986759, 0.0016555191548324603, 0.001682996248118868, 0.0016743811244477136, 0.0016905513028939102, 0.001655773387846443, 0.0016520134960265123, 0.001659855860206046, 0.0016645231002638506, 0.0016844112158919027, 0.0017011165654994258, 0.0016770844109529673, 0.0016850987828314998, 0.0022570053259878195, 0.0016821125276791033, 0.0016696423404063134, 0.0016751120700825785, 0.0016911239543741987, 0.0017916633487614088, 0.0018005795278694741, 0.0017768220467016447, 0.001797961704649551, 0.0017188759628943232, 0.0017140699070409056, 0.002343440426571309, 0.001694318396219796, 0.0021109361537234033, 0.0016798850621797087, 0.0016768449076357507]
[603.8548638379718, 603.7030812406117, 598.9021344141519, 594.1658027688953, 605.0273978678638, 606.5523939230089, 601.4269176077547, 602.0520307746527, 604.2518786132246, 603.1225337477524, 604.1985727254416, 604.278355161239, 604.0851695454087, 604.8586692741357, 604.9585924077637, 604.5390563453349, 605.8625884394514, 592.0025497114782, 602.097553630502, 597.8723891878676, 603.8768727038208, 604.1492605034834, 603.2204562495718, 601.3440008985731, 604.5092030512793, 603.6864576487618, 602.5773407377718, 603.3886189366174, 603.2828017569308, 600.8255788182042, 604.4779070897175, 606.2019765351422, 598.6578814622545, 603.8203604036864, 602.157583479478, 604.0401266762756, 594.1783893563208, 597.2355907498928, 591.5230128113743, 603.9473803239675, 605.3219313312145, 602.46195104909, 600.7726776765584, 593.6792575146181, 587.8491928661061, 596.272908786846, 593.4370199470936, 443.0649713076471, 594.4905489644924, 598.9306666460353, 596.9749832622856, 591.3227102090517, 558.1405684786196, 555.3767464985257, 562.802561942724, 556.1853722545857, 581.7755449416801, 583.4067769886677, 426.7230302342703, 590.2078394657734, 473.7234701467102, 595.2788214584548, 596.3580742896129]
Elapsed: 0.2195839149810906~0.01626889876542578
Time per graph: 0.0017022008913262837~0.00012611549430562616
Speed: 589.9916025028302~34.086133536045146
Total Time: 0.2169
best val loss: 0.19131813728243344 test_score: 0.8837

Testing...
Test loss: 0.2930 score: 0.8837 time: 0.27s
test Score 0.8837
Epoch Time List: [0.7836062798742205, 0.7895430130884051, 0.7968918918631971, 0.9409429058432579, 0.8019228440243751, 0.8014834681525826, 0.8091826736927032, 0.8077139193192124, 0.8014970351941884, 0.802229902241379, 0.801603107014671, 0.8018870078958571, 0.8012890820391476, 0.8025224599987268, 0.8030894645489752, 0.8068508389405906, 0.8045546794310212, 0.811675266828388, 0.8049095019232482, 0.8087499346584082, 0.8066361309029162, 0.8075729869306087, 0.809883524896577, 0.8110811670776457, 0.8066726888064295, 0.8060439210385084, 0.804615173721686, 0.8036910858936608, 0.8022439032793045, 0.8033933981787413, 0.8010715590789914, 0.8000960790086538, 0.7886132900603116, 0.7628793979529291, 0.7671946820337325, 0.7612465033307672, 0.7649664618074894, 0.7644003059249371, 0.7722017620690167, 0.7610762771219015, 0.7866803302895278, 0.7960154563188553, 0.7996333320625126, 0.8889045359101146, 0.7729579226579517, 0.8675369769334793, 0.7640296958852559, 0.8496666289865971, 0.764191335067153, 0.7758518178015947, 0.7817853258457035, 0.7692281790077686, 0.9045360099989921, 0.8120185809675604, 0.870988825103268, 0.8164247367531061, 0.8144082620274276, 0.7746303370222449, 0.8712595000397414, 0.7763180288020521, 0.8352576743345708, 0.7664909849409014, 0.7799155570100993]
Total Epoch List: [63]
Total Time List: [0.2169127380475402]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c62433ab30>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7132;  Loss pred: 0.6959; Loss self: 1.7278; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4961 time: 0.21s
Epoch 2/1000, LR 0.000005
Train loss: 0.7133;  Loss pred: 0.6959; Loss self: 1.7451; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000015
Train loss: 0.7094;  Loss pred: 0.6926; Loss self: 1.6832; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000025
Train loss: 0.6913;  Loss pred: 0.6740; Loss self: 1.7275; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000035
Train loss: 0.6703;  Loss pred: 0.6532; Loss self: 1.7111; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4961 time: 0.21s
Epoch 6/1000, LR 0.000045
Train loss: 0.6420;  Loss pred: 0.6254; Loss self: 1.6629; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.4961 time: 0.21s
Epoch 7/1000, LR 0.000055
Train loss: 0.6267;  Loss pred: 0.6096; Loss self: 1.7164; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.4961 time: 0.21s
Epoch 8/1000, LR 0.000065
Train loss: 0.6023;  Loss pred: 0.5843; Loss self: 1.8039; time: 0.33s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6902 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6917 score: 0.4961 time: 0.21s
Epoch 9/1000, LR 0.000075
Train loss: 0.5797;  Loss pred: 0.5617; Loss self: 1.8074; time: 0.34s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6873 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6894 score: 0.4961 time: 0.21s
Epoch 10/1000, LR 0.000085
Train loss: 0.5605;  Loss pred: 0.5418; Loss self: 1.8700; time: 0.33s
Val loss: 0.6816 score: 0.5426 time: 0.21s
Test loss: 0.6847 score: 0.5116 time: 0.21s
Epoch 11/1000, LR 0.000095
Train loss: 0.5281;  Loss pred: 0.5092; Loss self: 1.8894; time: 0.34s
Val loss: 0.6712 score: 0.6822 time: 0.21s
Test loss: 0.6762 score: 0.6822 time: 0.21s
Epoch 12/1000, LR 0.000095
Train loss: 0.4915;  Loss pred: 0.4720; Loss self: 1.9483; time: 0.36s
Val loss: 0.6525 score: 0.9225 time: 0.21s
Test loss: 0.6609 score: 0.9147 time: 0.21s
Epoch 13/1000, LR 0.000095
Train loss: 0.4721;  Loss pred: 0.4520; Loss self: 2.0041; time: 0.37s
Val loss: 0.6219 score: 0.9147 time: 0.21s
Test loss: 0.6353 score: 0.8760 time: 0.21s
Epoch 14/1000, LR 0.000095
Train loss: 0.4387;  Loss pred: 0.4185; Loss self: 2.0239; time: 0.37s
Val loss: 0.5775 score: 0.8992 time: 0.21s
Test loss: 0.5965 score: 0.8527 time: 0.21s
Epoch 15/1000, LR 0.000095
Train loss: 0.4112;  Loss pred: 0.3904; Loss self: 2.0811; time: 0.37s
Val loss: 0.5230 score: 0.8992 time: 0.21s
Test loss: 0.5461 score: 0.8527 time: 0.21s
Epoch 16/1000, LR 0.000095
Train loss: 0.3926;  Loss pred: 0.3714; Loss self: 2.1241; time: 0.37s
Val loss: 0.4704 score: 0.8992 time: 0.21s
Test loss: 0.4942 score: 0.8605 time: 0.21s
Epoch 17/1000, LR 0.000095
Train loss: 0.3664;  Loss pred: 0.3450; Loss self: 2.1379; time: 0.37s
Val loss: 0.4231 score: 0.8992 time: 0.21s
Test loss: 0.4451 score: 0.8605 time: 0.21s
Epoch 18/1000, LR 0.000095
Train loss: 0.3353;  Loss pred: 0.3128; Loss self: 2.2511; time: 0.38s
Val loss: 0.3883 score: 0.8992 time: 0.21s
Test loss: 0.4061 score: 0.8605 time: 0.21s
Epoch 19/1000, LR 0.000095
Train loss: 0.3128;  Loss pred: 0.2903; Loss self: 2.2510; time: 0.37s
Val loss: 0.3606 score: 0.8992 time: 0.21s
Test loss: 0.3736 score: 0.8760 time: 0.21s
Epoch 20/1000, LR 0.000095
Train loss: 0.2924;  Loss pred: 0.2696; Loss self: 2.2810; time: 0.37s
Val loss: 0.3396 score: 0.9070 time: 0.21s
Test loss: 0.3481 score: 0.8837 time: 0.21s
Epoch 21/1000, LR 0.000095
Train loss: 0.2691;  Loss pred: 0.2455; Loss self: 2.3537; time: 0.37s
Val loss: 0.3259 score: 0.8992 time: 0.21s
Test loss: 0.3301 score: 0.8915 time: 0.21s
Epoch 22/1000, LR 0.000095
Train loss: 0.2505;  Loss pred: 0.2273; Loss self: 2.3238; time: 0.37s
Val loss: 0.3127 score: 0.8992 time: 0.21s
Test loss: 0.3141 score: 0.9070 time: 0.21s
Epoch 23/1000, LR 0.000095
Train loss: 0.2294;  Loss pred: 0.2055; Loss self: 2.3838; time: 0.35s
Val loss: 0.3042 score: 0.9070 time: 0.21s
Test loss: 0.3019 score: 0.9070 time: 0.21s
Epoch 24/1000, LR 0.000095
Train loss: 0.2121;  Loss pred: 0.1877; Loss self: 2.4343; time: 0.37s
Val loss: 0.2948 score: 0.9070 time: 0.21s
Test loss: 0.2902 score: 0.9147 time: 0.21s
Epoch 25/1000, LR 0.000095
Train loss: 0.1941;  Loss pred: 0.1697; Loss self: 2.4447; time: 0.37s
Val loss: 0.2842 score: 0.9147 time: 0.21s
Test loss: 0.2804 score: 0.9225 time: 0.21s
Epoch 26/1000, LR 0.000095
Train loss: 0.1823;  Loss pred: 0.1576; Loss self: 2.4751; time: 0.37s
Val loss: 0.2737 score: 0.9147 time: 0.21s
Test loss: 0.2720 score: 0.9225 time: 0.21s
Epoch 27/1000, LR 0.000095
Train loss: 0.1816;  Loss pred: 0.1563; Loss self: 2.5309; time: 0.37s
Val loss: 0.2660 score: 0.9070 time: 0.21s
Test loss: 0.2674 score: 0.9225 time: 0.21s
Epoch 28/1000, LR 0.000095
Train loss: 0.1574;  Loss pred: 0.1317; Loss self: 2.5669; time: 0.37s
Val loss: 0.2600 score: 0.9147 time: 0.21s
Test loss: 0.2619 score: 0.9225 time: 0.21s
Epoch 29/1000, LR 0.000095
Train loss: 0.1455;  Loss pred: 0.1196; Loss self: 2.5938; time: 0.37s
Val loss: 0.2511 score: 0.9225 time: 0.21s
Test loss: 0.2556 score: 0.9302 time: 0.21s
Epoch 30/1000, LR 0.000095
Train loss: 0.1409;  Loss pred: 0.1148; Loss self: 2.6168; time: 0.37s
Val loss: 0.2463 score: 0.9225 time: 0.21s
Test loss: 0.2537 score: 0.9302 time: 0.21s
Epoch 31/1000, LR 0.000095
Train loss: 0.1285;  Loss pred: 0.1018; Loss self: 2.6675; time: 0.37s
Val loss: 0.2435 score: 0.9225 time: 0.21s
Test loss: 0.2509 score: 0.9302 time: 0.21s
Epoch 32/1000, LR 0.000095
Train loss: 0.1138;  Loss pred: 0.0872; Loss self: 2.6574; time: 0.35s
Val loss: 0.2395 score: 0.9302 time: 0.21s
Test loss: 0.2502 score: 0.9302 time: 0.21s
Epoch 33/1000, LR 0.000095
Train loss: 0.1128;  Loss pred: 0.0861; Loss self: 2.6679; time: 0.37s
Val loss: 0.2356 score: 0.9302 time: 0.21s
Test loss: 0.2492 score: 0.9302 time: 0.21s
Epoch 34/1000, LR 0.000095
Train loss: 0.1006;  Loss pred: 0.0735; Loss self: 2.7043; time: 0.37s
Val loss: 0.2307 score: 0.9302 time: 0.21s
Test loss: 0.2480 score: 0.9225 time: 0.21s
Epoch 35/1000, LR 0.000095
Train loss: 0.1040;  Loss pred: 0.0769; Loss self: 2.7122; time: 0.37s
Val loss: 0.2295 score: 0.9302 time: 0.21s
Test loss: 0.2476 score: 0.9302 time: 0.21s
Epoch 36/1000, LR 0.000095
Train loss: 0.0913;  Loss pred: 0.0641; Loss self: 2.7110; time: 0.37s
Val loss: 0.2280 score: 0.9302 time: 0.21s
Test loss: 0.2483 score: 0.9225 time: 0.21s
Epoch 37/1000, LR 0.000095
Train loss: 0.0853;  Loss pred: 0.0580; Loss self: 2.7285; time: 0.37s
Val loss: 0.2282 score: 0.9225 time: 0.21s
Test loss: 0.2499 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 38/1000, LR 0.000095
Train loss: 0.0808;  Loss pred: 0.0533; Loss self: 2.7501; time: 0.37s
Val loss: 0.2260 score: 0.9225 time: 0.21s
Test loss: 0.2495 score: 0.9225 time: 0.21s
Epoch 39/1000, LR 0.000095
Train loss: 0.0759;  Loss pred: 0.0481; Loss self: 2.7774; time: 0.37s
Val loss: 0.2252 score: 0.9225 time: 0.21s
Test loss: 0.2486 score: 0.9225 time: 0.21s
Epoch 40/1000, LR 0.000095
Train loss: 0.0736;  Loss pred: 0.0458; Loss self: 2.7757; time: 0.37s
Val loss: 0.2239 score: 0.9302 time: 0.21s
Test loss: 0.2476 score: 0.9302 time: 0.21s
Epoch 41/1000, LR 0.000095
Train loss: 0.0705;  Loss pred: 0.0428; Loss self: 2.7697; time: 0.37s
Val loss: 0.2243 score: 0.9302 time: 0.21s
Test loss: 0.2485 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000095
Train loss: 0.0690;  Loss pred: 0.0413; Loss self: 2.7738; time: 0.37s
Val loss: 0.2227 score: 0.9302 time: 0.21s
Test loss: 0.2487 score: 0.9302 time: 0.21s
Epoch 43/1000, LR 0.000095
Train loss: 0.0643;  Loss pred: 0.0364; Loss self: 2.7899; time: 0.37s
Val loss: 0.2230 score: 0.9225 time: 0.21s
Test loss: 0.2522 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000095
Train loss: 0.0657;  Loss pred: 0.0377; Loss self: 2.7930; time: 0.37s
Val loss: 0.2266 score: 0.9302 time: 0.21s
Test loss: 0.2599 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000095
Train loss: 0.0588;  Loss pred: 0.0309; Loss self: 2.7915; time: 0.38s
Val loss: 0.2189 score: 0.9302 time: 0.21s
Test loss: 0.2604 score: 0.9302 time: 0.21s
Epoch 46/1000, LR 0.000095
Train loss: 0.0567;  Loss pred: 0.0283; Loss self: 2.8364; time: 0.36s
Val loss: 0.2178 score: 0.9302 time: 0.21s
Test loss: 0.2640 score: 0.9302 time: 0.21s
Epoch 47/1000, LR 0.000095
Train loss: 0.0604;  Loss pred: 0.0323; Loss self: 2.8132; time: 0.37s
Val loss: 0.2186 score: 0.9302 time: 0.21s
Test loss: 0.2649 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000095
Train loss: 0.0610;  Loss pred: 0.0328; Loss self: 2.8207; time: 0.37s
Val loss: 0.2162 score: 0.9302 time: 0.21s
Test loss: 0.2592 score: 0.9302 time: 0.21s
Epoch 49/1000, LR 0.000095
Train loss: 0.0502;  Loss pred: 0.0220; Loss self: 2.8202; time: 0.36s
Val loss: 0.2055 score: 0.9225 time: 0.21s
Test loss: 0.2514 score: 0.9380 time: 0.29s
Epoch 50/1000, LR 0.000095
Train loss: 0.0488;  Loss pred: 0.0203; Loss self: 2.8490; time: 0.34s
Val loss: 0.2009 score: 0.9302 time: 0.21s
Test loss: 0.2509 score: 0.9380 time: 0.21s
Epoch 51/1000, LR 0.000095
Train loss: 0.0535;  Loss pred: 0.0253; Loss self: 2.8209; time: 0.37s
Val loss: 0.1901 score: 0.9302 time: 0.21s
Test loss: 0.2488 score: 0.9302 time: 0.21s
Epoch 52/1000, LR 0.000095
Train loss: 0.0489;  Loss pred: 0.0203; Loss self: 2.8544; time: 0.40s
Val loss: 0.1874 score: 0.9302 time: 0.21s
Test loss: 0.2533 score: 0.9225 time: 0.21s
Epoch 53/1000, LR 0.000095
Train loss: 0.0454;  Loss pred: 0.0167; Loss self: 2.8750; time: 0.33s
Val loss: 0.1843 score: 0.9225 time: 0.21s
Test loss: 0.2584 score: 0.9225 time: 0.21s
Epoch 54/1000, LR 0.000095
Train loss: 0.0429;  Loss pred: 0.0141; Loss self: 2.8840; time: 0.35s
Val loss: 0.1856 score: 0.9225 time: 0.21s
Test loss: 0.2652 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000095
Train loss: 0.0443;  Loss pred: 0.0156; Loss self: 2.8702; time: 0.33s
Val loss: 0.1888 score: 0.9225 time: 0.21s
Test loss: 0.2727 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 56/1000, LR 0.000095
Train loss: 0.0498;  Loss pred: 0.0213; Loss self: 2.8554; time: 0.37s
Val loss: 0.1955 score: 0.9225 time: 0.21s
Test loss: 0.2761 score: 0.9147 time: 0.30s
     INFO: Early stopping counter 3 of 20
Epoch 57/1000, LR 0.000094
Train loss: 0.0417;  Loss pred: 0.0127; Loss self: 2.9016; time: 0.33s
Val loss: 0.1937 score: 0.9225 time: 0.21s
Test loss: 0.2682 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 58/1000, LR 0.000094
Train loss: 0.0457;  Loss pred: 0.0171; Loss self: 2.8667; time: 0.35s
Val loss: 0.1871 score: 0.9225 time: 0.21s
Test loss: 0.2633 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 59/1000, LR 0.000094
Train loss: 0.0394;  Loss pred: 0.0103; Loss self: 2.9140; time: 0.35s
Val loss: 0.1882 score: 0.9302 time: 0.21s
Test loss: 0.2637 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 60/1000, LR 0.000094
Train loss: 0.0386;  Loss pred: 0.0097; Loss self: 2.8848; time: 0.33s
Val loss: 0.1879 score: 0.9302 time: 0.21s
Test loss: 0.2649 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 61/1000, LR 0.000094
Train loss: 0.0395;  Loss pred: 0.0109; Loss self: 2.8669; time: 0.33s
Val loss: 0.1856 score: 0.9225 time: 0.29s
Test loss: 0.2696 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 62/1000, LR 0.000094
Train loss: 0.0414;  Loss pred: 0.0129; Loss self: 2.8463; time: 0.33s
Val loss: 0.1845 score: 0.9225 time: 0.21s
Test loss: 0.2769 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 63/1000, LR 0.000094
Train loss: 0.0369;  Loss pred: 0.0083; Loss self: 2.8665; time: 0.35s
Val loss: 0.1894 score: 0.9225 time: 0.29s
Test loss: 0.2873 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 64/1000, LR 0.000094
Train loss: 0.0442;  Loss pred: 0.0156; Loss self: 2.8591; time: 0.34s
Val loss: 0.1983 score: 0.9225 time: 0.21s
Test loss: 0.3021 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 65/1000, LR 0.000094
Train loss: 0.0385;  Loss pred: 0.0098; Loss self: 2.8760; time: 0.33s
Val loss: 0.1985 score: 0.9225 time: 0.26s
Test loss: 0.3066 score: 0.9070 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 66/1000, LR 0.000094
Train loss: 0.0369;  Loss pred: 0.0082; Loss self: 2.8723; time: 0.36s
Val loss: 0.2026 score: 0.9225 time: 0.21s
Test loss: 0.3128 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 67/1000, LR 0.000094
Train loss: 0.0355;  Loss pred: 0.0067; Loss self: 2.8751; time: 0.33s
Val loss: 0.2034 score: 0.9225 time: 0.21s
Test loss: 0.3166 score: 0.9070 time: 0.30s
     INFO: Early stopping counter 14 of 20
Epoch 68/1000, LR 0.000094
Train loss: 0.0354;  Loss pred: 0.0065; Loss self: 2.8856; time: 0.34s
Val loss: 0.2012 score: 0.9225 time: 0.21s
Test loss: 0.3157 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 69/1000, LR 0.000094
Train loss: 0.0360;  Loss pred: 0.0073; Loss self: 2.8670; time: 0.35s
Val loss: 0.1957 score: 0.9302 time: 0.21s
Test loss: 0.3115 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 70/1000, LR 0.000094
Train loss: 0.0365;  Loss pred: 0.0080; Loss self: 2.8559; time: 0.35s
Val loss: 0.1923 score: 0.9302 time: 0.22s
Test loss: 0.3061 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 71/1000, LR 0.000094
Train loss: 0.0358;  Loss pred: 0.0071; Loss self: 2.8697; time: 0.36s
Val loss: 0.1973 score: 0.9225 time: 0.22s
Test loss: 0.3063 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 72/1000, LR 0.000094
Train loss: 0.0351;  Loss pred: 0.0063; Loss self: 2.8825; time: 0.36s
Val loss: 0.1982 score: 0.9225 time: 0.32s
Test loss: 0.3043 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 73/1000, LR 0.000094
Train loss: 0.0340;  Loss pred: 0.0054; Loss self: 2.8630; time: 0.36s
Val loss: 0.1972 score: 0.9225 time: 0.21s
Test loss: 0.3019 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 052,   Train_Loss: 0.0454,   Val_Loss: 0.1843,   Val_Precision: 0.9825,   Val_Recall: 0.8615,   Val_accuracy: 0.9180,   Val_Score: 0.9225,   Val_Loss: 0.1843,   Test_Precision: 0.9500,   Test_Recall: 0.8906,   Test_accuracy: 0.9194,   Test_Score: 0.9225,   Test_loss: 0.2584


[0.213627491844818, 0.213681201916188, 0.2153941229917109, 0.2171111151110381, 0.21321348496712744, 0.21267742291092873, 0.21448990097269416, 0.21426719520241022, 0.21348713105544448, 0.21388688497245312, 0.2135059661231935, 0.21347777708433568, 0.21354604698717594, 0.2132729620207101, 0.21323773497715592, 0.21338571701198816, 0.2129195670131594, 0.21790446690283716, 0.21425099507905543, 0.2157651069574058, 0.2136197059880942, 0.2135233930312097, 0.21385216410271823, 0.21451947605237365, 0.21339625492691994, 0.21368708601221442, 0.2140804030932486, 0.21379256411455572, 0.2138300638180226, 0.21470457408577204, 0.2134073032066226, 0.2128003619145602, 0.21548200398683548, 0.21363969892263412, 0.2142296361271292, 0.21356197097338736, 0.217106516007334, 0.21599516505375504, 0.21808111807331443, 0.21359476703219116, 0.21310974098742008, 0.21412140596657991, 0.21472347993403673, 0.21728904685005546, 0.21944403694942594, 0.21634388901293278, 0.21737774298526347, 0.2911536870524287, 0.21699251607060432, 0.21538386191241443, 0.21608945704065263, 0.21815499011427164, 0.23112457199022174, 0.23227475909516215, 0.22921004402451217, 0.23193705989979208, 0.2217349992133677, 0.22111501800827682, 0.3023038150276989, 0.21856707311235368, 0.27231076383031905, 0.21670517302118242, 0.21631299308501184, 0.21541528310626745, 0.2141808201558888, 0.2142822709865868, 0.21590262185782194, 0.21386175788939, 0.21425833203829825, 0.214252243982628, 0.21506656100973487, 0.21457446902059019, 0.2142409288790077, 0.21440549497492611, 0.2139789629727602, 0.21390146389603615, 0.2136501558125019, 0.21388821699656546, 0.21369948494248092, 0.21451743016950786, 0.21485074702650309, 0.21402235492132604, 0.2150865530129522, 0.2142716699745506, 0.21979243610985577, 0.2139745729509741, 0.21351595898158848, 0.2153271611314267, 0.21307719289325178, 0.21532742911949754, 0.21419778699055314, 0.21743974997662008, 0.2138818979728967, 0.21392006310634315, 0.21468117809854448, 0.21474867896176875, 0.21484040399082005, 0.21387927094474435, 0.21401905291713774, 0.21435344917699695, 0.21386361215263605, 0.21533303707838058, 0.2139726139139384, 0.21465820400044322, 0.21493832091800869, 0.21487455000169575, 0.21863186196424067, 0.2156661490444094, 0.21526266913861036, 0.21363790100440383, 0.21434743702411652, 0.2958213018719107, 0.21446604584343731, 0.21460523107089102, 0.2147315419279039, 0.2156505819875747, 0.21497708512470126, 0.21673488593660295, 0.3079635901376605, 0.21408218704164028, 0.2147402218542993, 0.2161391549743712, 0.2184690658468753, 0.2161043209489435, 0.2148614600300789, 0.227551877964288, 0.21912212297320366, 0.24798235204070807, 0.21993383788503706, 0.302144339075312, 0.2194330538623035, 0.21692926506511867, 0.2193220630288124, 0.22024959698319435, 0.21944244299083948, 0.21261778799816966]
[0.0016560270685644805, 0.0016564434257068837, 0.0016697218836566738, 0.0016830319000855666, 0.0016528177129234685, 0.0016486621931079747, 0.00166271241064104, 0.0016609860093210094, 0.0016549390004298023, 0.0016580378680035126, 0.0016550850087069264, 0.001654866489025858, 0.0016553957130788833, 0.001653278775354342, 0.0016530056974973328, 0.0016541528450541717, 0.0016505392791717783, 0.0016891819139754818, 0.001660860426969422, 0.001672597728351983, 0.0016559667130860017, 0.0016552201010171296, 0.00165776871397456, 0.0016629416748246018, 0.0016542345343172088, 0.0016564890388543754, 0.0016595380084747954, 0.0016573066985624474, 0.0016575973939381597, 0.0016643765433005584, 0.0016543201798962992, 0.0016496152086400014, 0.0016704031316808951, 0.0016561216970746831, 0.0016606948536986759, 0.0016555191548324603, 0.001682996248118868, 0.0016743811244477136, 0.0016905513028939102, 0.001655773387846443, 0.0016520134960265123, 0.001659855860206046, 0.0016645231002638506, 0.0016844112158919027, 0.0017011165654994258, 0.0016770844109529673, 0.0016850987828314998, 0.0022570053259878195, 0.0016821125276791033, 0.0016696423404063134, 0.0016751120700825785, 0.0016911239543741987, 0.0017916633487614088, 0.0018005795278694741, 0.0017768220467016447, 0.001797961704649551, 0.0017188759628943232, 0.0017140699070409056, 0.002343440426571309, 0.001694318396219796, 0.0021109361537234033, 0.0016798850621797087, 0.0016768449076357507, 0.001669885915552461, 0.0016603164353169674, 0.001661102875865014, 0.001673663735331953, 0.0016578430844138758, 0.001660917302622467, 0.001660870108392465, 0.0016671826434863169, 0.0016633679769037998, 0.0016607823944109123, 0.0016620581005808226, 0.0016587516509516294, 0.0016581508829150088, 0.001656202758236449, 0.0016580481937718253, 0.0016565851545928753, 0.0016629258152675028, 0.001665509666872117, 0.0016590880226459383, 0.0016673376202554434, 0.0016610206974771364, 0.0017038173341849285, 0.0016587176197749931, 0.0016551624727254922, 0.001669202799468424, 0.001651761185219006, 0.0016692048768953297, 0.0016604479611670787, 0.0016855794571831015, 0.0016579992090922225, 0.0016582950628398693, 0.0016641951790584842, 0.0016647184415640988, 0.0016654294883009307, 0.001657978844532902, 0.001659062425714246, 0.001661654644782922, 0.0016578574585475662, 0.0016692483494448107, 0.001658702433441383, 0.0016640170852747537, 0.0016661885342481293, 0.001665694186059657, 0.0016948206353817107, 0.0016718306127473596, 0.0016687028615396152, 0.0016561077597240608, 0.0016616080389466397, 0.0022931883866039587, 0.0016625274871584288, 0.001663606442410008, 0.0016645855963403403, 0.001671709937888176, 0.0016664890319744285, 0.0016801153948573873, 0.0023873146522299265, 0.001659551837532095, 0.0016646528825914675, 0.0016754973253827225, 0.0016935586499757776, 0.0016752272941778566, 0.0016655927134114643, 0.0017639680462347906, 0.0016986211083194081, 0.0019223438142690547, 0.0017049134719770314, 0.002342204178878388, 0.0017010314252891743, 0.0016816222098071216, 0.001700171031231104, 0.0017073612169239871, 0.0017011042092313139, 0.0016481999069625556]
[603.8548638379718, 603.7030812406117, 598.9021344141519, 594.1658027688953, 605.0273978678638, 606.5523939230089, 601.4269176077547, 602.0520307746527, 604.2518786132246, 603.1225337477524, 604.1985727254416, 604.278355161239, 604.0851695454087, 604.8586692741357, 604.9585924077637, 604.5390563453349, 605.8625884394514, 592.0025497114782, 602.097553630502, 597.8723891878676, 603.8768727038208, 604.1492605034834, 603.2204562495718, 601.3440008985731, 604.5092030512793, 603.6864576487618, 602.5773407377718, 603.3886189366174, 603.2828017569308, 600.8255788182042, 604.4779070897175, 606.2019765351422, 598.6578814622545, 603.8203604036864, 602.157583479478, 604.0401266762756, 594.1783893563208, 597.2355907498928, 591.5230128113743, 603.9473803239675, 605.3219313312145, 602.46195104909, 600.7726776765584, 593.6792575146181, 587.8491928661061, 596.272908786846, 593.4370199470936, 443.0649713076471, 594.4905489644924, 598.9306666460353, 596.9749832622856, 591.3227102090517, 558.1405684786196, 555.3767464985257, 562.802561942724, 556.1853722545857, 581.7755449416801, 583.4067769886677, 426.7230302342703, 590.2078394657734, 473.7234701467102, 595.2788214584548, 596.3580742896129, 598.8433046153111, 602.2948268949058, 602.0096735304568, 597.4915862066288, 603.1933959259758, 602.0769356915441, 602.0940439273046, 599.8143058332573, 601.1898833482439, 602.1258434370054, 601.6636841098035, 602.8630020812927, 603.0814266081819, 603.7908070294582, 603.1187777028009, 603.6514315171207, 601.3497360007832, 600.4168092749852, 602.7407746607591, 599.7585539075137, 602.0394577375606, 586.9173766085551, 602.8753707551808, 604.1702953507269, 599.0883793859326, 605.4143958270884, 599.0876337840383, 602.2471184806841, 593.267790336728, 603.1365965171442, 603.0289918897041, 600.8910568805687, 600.702181841899, 600.4457150690895, 603.1440047003298, 602.7500740784291, 601.8097702429855, 603.1881660538481, 599.072031631841, 602.8808904109799, 600.9553680964071, 600.1721770647353, 600.350297412988, 590.0329386624314, 598.1467215489467, 599.2678643083036, 603.8254419909362, 601.8266501851666, 436.0740730424357, 601.4938145228429, 601.1037072874852, 600.750121951398, 598.1898996564391, 600.0639553056144, 595.1972126800747, 418.8806863250837, 602.5723194564932, 600.7258392772183, 596.837717882705, 590.4726122206058, 596.9339226237746, 600.3868724616366, 566.9036931448453, 588.7128065830914, 520.198308220029, 586.5400305860636, 426.94826053929694, 587.8786159579624, 594.6638871490035, 588.1761197141996, 585.699142095788, 587.8534628115904, 606.722519383517]
Elapsed: 0.21950244501075597~0.017039323894561167
Time per graph: 0.0017015693411686508~0.00013208778212838116
Speed: 590.3952508655447~34.91499180769873
Total Time: 0.2134
best val loss: 0.184324142699989 test_score: 0.9225

Testing...
Test loss: 0.2502 score: 0.9302 time: 0.22s
test Score 0.9302
Epoch Time List: [0.7836062798742205, 0.7895430130884051, 0.7968918918631971, 0.9409429058432579, 0.8019228440243751, 0.8014834681525826, 0.8091826736927032, 0.8077139193192124, 0.8014970351941884, 0.802229902241379, 0.801603107014671, 0.8018870078958571, 0.8012890820391476, 0.8025224599987268, 0.8030894645489752, 0.8068508389405906, 0.8045546794310212, 0.811675266828388, 0.8049095019232482, 0.8087499346584082, 0.8066361309029162, 0.8075729869306087, 0.809883524896577, 0.8110811670776457, 0.8066726888064295, 0.8060439210385084, 0.804615173721686, 0.8036910858936608, 0.8022439032793045, 0.8033933981787413, 0.8010715590789914, 0.8000960790086538, 0.7886132900603116, 0.7628793979529291, 0.7671946820337325, 0.7612465033307672, 0.7649664618074894, 0.7644003059249371, 0.7722017620690167, 0.7610762771219015, 0.7866803302895278, 0.7960154563188553, 0.7996333320625126, 0.8889045359101146, 0.7729579226579517, 0.8675369769334793, 0.7640296958852559, 0.8496666289865971, 0.764191335067153, 0.7758518178015947, 0.7817853258457035, 0.7692281790077686, 0.9045360099989921, 0.8120185809675604, 0.870988825103268, 0.8164247367531061, 0.8144082620274276, 0.7746303370222449, 0.8712595000397414, 0.7763180288020521, 0.8352576743345708, 0.7664909849409014, 0.7799155570100993, 0.7583281239494681, 0.7530014379881322, 0.7499062798451632, 0.7537200183141977, 0.7503237959463149, 0.7497114778961986, 0.7501207487657666, 0.753840422956273, 0.7628747851122171, 0.7490951041691005, 0.7609890562016517, 0.7757567842490971, 0.787322802701965, 0.7869784198701382, 0.7899475009180605, 0.788428959203884, 0.7881494069006294, 0.8014502730220556, 0.7912486852146685, 0.797140927053988, 0.7921946309506893, 0.7963361348956823, 0.7724658942315727, 0.7879706679377705, 0.7973502620588988, 0.7890950217843056, 0.7927886948455125, 0.7906419502105564, 0.795202795881778, 0.7911246584262699, 0.7875861618667841, 0.7731434390880167, 0.7890759611036628, 0.7918426527176052, 0.7886985039804131, 0.7892510499805212, 0.7914073441643268, 0.7902728300541639, 0.7920378816779703, 0.7875738262664527, 0.7926856810227036, 0.7945409950334579, 0.7928899589460343, 0.7942746439948678, 0.7971311514265835, 0.778689915779978, 0.790830094832927, 0.7933793398551643, 0.8637820167932659, 0.7601930210366845, 0.7893421498592943, 0.8152156199794263, 0.7532379990443587, 0.770766681060195, 0.7546103089116514, 0.88329929090105, 0.7535377778112888, 0.7717913549859077, 0.768010837957263, 0.758114418014884, 0.8323718709871173, 0.7545163542963564, 0.8621626738458872, 0.767948972992599, 0.8337294298689812, 0.7850934707093984, 0.8399615972302854, 0.7637129491195083, 0.7721520469058305, 0.7850293610244989, 0.7880882369354367, 0.8852202061098069, 0.7717961128801107]
Total Epoch List: [63, 73]
Total Time List: [0.2169127380475402, 0.2133794380351901]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78c61c5670d0>
Training...
Epoch 1/1000, LR 0.000100
Train loss: 0.7113;  Loss pred: 0.6955; Loss self: 1.5809; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000007
Train loss: 0.7003;  Loss pred: 0.6842; Loss self: 1.6117; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000017
Train loss: 0.6900;  Loss pred: 0.6746; Loss self: 1.5416; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000027
Train loss: 0.6782;  Loss pred: 0.6623; Loss self: 1.5879; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000037
Train loss: 0.6597;  Loss pred: 0.6440; Loss self: 1.5667; time: 0.44s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6916 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6910 score: 0.5000 time: 0.18s
Epoch 6/1000, LR 0.000047
Train loss: 0.6310;  Loss pred: 0.6148; Loss self: 1.6157; time: 0.44s
Val loss: 0.6894 score: 0.7287 time: 0.21s
Test loss: 0.6888 score: 0.7734 time: 0.18s
Epoch 7/1000, LR 0.000057
Train loss: 0.6164;  Loss pred: 0.6008; Loss self: 1.5591; time: 0.43s
Val loss: 0.6827 score: 0.7984 time: 0.21s
Test loss: 0.6813 score: 0.8906 time: 0.18s
Epoch 8/1000, LR 0.000067
Train loss: 0.5947;  Loss pred: 0.5791; Loss self: 1.5607; time: 0.40s
Val loss: 0.6636 score: 0.8372 time: 0.21s
Test loss: 0.6602 score: 0.8750 time: 0.18s
Epoch 9/1000, LR 0.000077
Train loss: 0.5457;  Loss pred: 0.5297; Loss self: 1.6062; time: 0.42s
Val loss: 0.6268 score: 0.8682 time: 0.21s
Test loss: 0.6190 score: 0.9062 time: 0.18s
Epoch 10/1000, LR 0.000087
Train loss: 0.5113;  Loss pred: 0.4944; Loss self: 1.6863; time: 0.44s
Val loss: 0.5598 score: 0.8682 time: 0.21s
Test loss: 0.5477 score: 0.8906 time: 0.18s
Epoch 11/1000, LR 0.000097
Train loss: 0.4874;  Loss pred: 0.4704; Loss self: 1.6948; time: 0.42s
Val loss: 0.4922 score: 0.8372 time: 0.21s
Test loss: 0.4781 score: 0.8594 time: 0.18s
Epoch 12/1000, LR 0.000097
Train loss: 0.4458;  Loss pred: 0.4283; Loss self: 1.7539; time: 0.39s
Val loss: 0.4009 score: 0.8915 time: 0.21s
Test loss: 0.3861 score: 0.9062 time: 0.18s
Epoch 13/1000, LR 0.000097
Train loss: 0.4165;  Loss pred: 0.3986; Loss self: 1.7992; time: 0.41s
Val loss: 0.3653 score: 0.9225 time: 0.21s
Test loss: 0.3509 score: 0.9375 time: 0.18s
Epoch 14/1000, LR 0.000097
Train loss: 0.3693;  Loss pred: 0.3506; Loss self: 1.8727; time: 0.40s
Val loss: 0.3162 score: 0.9302 time: 0.21s
Test loss: 0.3078 score: 0.9375 time: 0.18s
Epoch 15/1000, LR 0.000097
Train loss: 0.3410;  Loss pred: 0.3218; Loss self: 1.9160; time: 0.39s
Val loss: 0.3724 score: 0.8605 time: 0.21s
Test loss: 0.3710 score: 0.8750 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000097
Train loss: 0.3068;  Loss pred: 0.2872; Loss self: 1.9593; time: 0.39s
Val loss: 0.2743 score: 0.9225 time: 0.21s
Test loss: 0.2845 score: 0.9375 time: 0.18s
Epoch 17/1000, LR 0.000097
Train loss: 0.2805;  Loss pred: 0.2606; Loss self: 1.9891; time: 0.39s
Val loss: 0.2585 score: 0.9535 time: 0.21s
Test loss: 0.2614 score: 0.9453 time: 0.18s
Epoch 18/1000, LR 0.000097
Train loss: 0.2500;  Loss pred: 0.2292; Loss self: 2.0750; time: 0.39s
Val loss: 0.2663 score: 0.8992 time: 0.21s
Test loss: 0.2709 score: 0.8828 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000097
Train loss: 0.2230;  Loss pred: 0.2014; Loss self: 2.1570; time: 0.43s
Val loss: 0.2445 score: 0.8992 time: 0.21s
Test loss: 0.2521 score: 0.8984 time: 0.18s
Epoch 20/1000, LR 0.000097
Train loss: 0.2096;  Loss pred: 0.1878; Loss self: 2.1786; time: 0.44s
Val loss: 0.2082 score: 0.9457 time: 0.21s
Test loss: 0.2281 score: 0.9297 time: 0.18s
Epoch 21/1000, LR 0.000097
Train loss: 0.1740;  Loss pred: 0.1514; Loss self: 2.2591; time: 0.45s
Val loss: 0.1960 score: 0.9457 time: 0.21s
Test loss: 0.2219 score: 0.9453 time: 0.18s
Epoch 22/1000, LR 0.000097
Train loss: 0.1605;  Loss pred: 0.1373; Loss self: 2.3193; time: 0.44s
Val loss: 0.1852 score: 0.9457 time: 0.21s
Test loss: 0.2146 score: 0.9297 time: 0.18s
Epoch 23/1000, LR 0.000097
Train loss: 0.1607;  Loss pred: 0.1371; Loss self: 2.3652; time: 0.41s
Val loss: 0.2318 score: 0.9147 time: 0.21s
Test loss: 0.2695 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000097
Train loss: 0.1389;  Loss pred: 0.1151; Loss self: 2.3817; time: 0.39s
Val loss: 0.1833 score: 0.9457 time: 0.21s
Test loss: 0.2210 score: 0.9375 time: 0.18s
Epoch 25/1000, LR 0.000097
Train loss: 0.1232;  Loss pred: 0.0989; Loss self: 2.4339; time: 0.40s
Val loss: 0.2616 score: 0.8992 time: 0.21s
Test loss: 0.3066 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000097
Train loss: 0.1174;  Loss pred: 0.0927; Loss self: 2.4697; time: 0.41s
Val loss: 0.3821 score: 0.8605 time: 0.21s
Test loss: 0.4100 score: 0.8828 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000097
Train loss: 0.1007;  Loss pred: 0.0756; Loss self: 2.5017; time: 0.39s
Val loss: 0.1656 score: 0.9380 time: 0.21s
Test loss: 0.2167 score: 0.9297 time: 0.18s
Epoch 28/1000, LR 0.000097
Train loss: 0.0925;  Loss pred: 0.0671; Loss self: 2.5322; time: 0.39s
Val loss: 0.1579 score: 0.9380 time: 0.21s
Test loss: 0.2168 score: 0.9531 time: 0.18s
Epoch 29/1000, LR 0.000097
Train loss: 0.1078;  Loss pred: 0.0830; Loss self: 2.4854; time: 0.39s
Val loss: 0.1456 score: 0.9457 time: 0.21s
Test loss: 0.1927 score: 0.9453 time: 0.18s
Epoch 30/1000, LR 0.000097
Train loss: 0.0779;  Loss pred: 0.0522; Loss self: 2.5733; time: 0.39s
Val loss: 0.3413 score: 0.8760 time: 0.21s
Test loss: 0.3889 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000097
Train loss: 0.0768;  Loss pred: 0.0505; Loss self: 2.6258; time: 0.39s
Val loss: 0.3591 score: 0.8760 time: 0.21s
Test loss: 0.4188 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000097
Train loss: 0.0790;  Loss pred: 0.0532; Loss self: 2.5809; time: 0.40s
Val loss: 0.1566 score: 0.9380 time: 0.21s
Test loss: 0.2333 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000097
Train loss: 0.0720;  Loss pred: 0.0458; Loss self: 2.6264; time: 0.39s
Val loss: 0.1521 score: 0.9380 time: 0.21s
Test loss: 0.2101 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000097
Train loss: 0.0633;  Loss pred: 0.0368; Loss self: 2.6496; time: 0.39s
Val loss: 0.1712 score: 0.9302 time: 0.21s
Test loss: 0.2591 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000097
Train loss: 0.0733;  Loss pred: 0.0470; Loss self: 2.6379; time: 0.39s
Val loss: 0.2473 score: 0.9070 time: 0.21s
Test loss: 0.3300 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000097
Train loss: 0.0575;  Loss pred: 0.0307; Loss self: 2.6783; time: 0.39s
Val loss: 0.3037 score: 0.8837 time: 0.21s
Test loss: 0.3798 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000097
Train loss: 0.0640;  Loss pred: 0.0371; Loss self: 2.6939; time: 0.39s
Val loss: 0.1216 score: 0.9457 time: 0.21s
Test loss: 0.2076 score: 0.9531 time: 0.18s
Epoch 38/1000, LR 0.000096
Train loss: 0.0533;  Loss pred: 0.0262; Loss self: 2.7075; time: 0.39s
Val loss: 0.1885 score: 0.9302 time: 0.21s
Test loss: 0.2790 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000096
Train loss: 0.0510;  Loss pred: 0.0238; Loss self: 2.7198; time: 0.40s
Val loss: 0.1139 score: 0.9690 time: 0.21s
Test loss: 0.1905 score: 0.9609 time: 0.18s
Epoch 40/1000, LR 0.000096
Train loss: 0.0871;  Loss pred: 0.0610; Loss self: 2.6075; time: 0.39s
Val loss: 0.1351 score: 0.9302 time: 0.21s
Test loss: 0.1813 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000096
Train loss: 0.0585;  Loss pred: 0.0315; Loss self: 2.6977; time: 0.40s
Val loss: 0.1881 score: 0.9302 time: 0.21s
Test loss: 0.2911 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000096
Train loss: 0.0539;  Loss pred: 0.0269; Loss self: 2.7060; time: 0.39s
Val loss: 0.1049 score: 0.9535 time: 0.21s
Test loss: 0.1741 score: 0.9531 time: 0.18s
Epoch 43/1000, LR 0.000096
Train loss: 0.0586;  Loss pred: 0.0318; Loss self: 2.6792; time: 0.39s
Val loss: 0.1285 score: 0.9302 time: 0.21s
Test loss: 0.1696 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000096
Train loss: 0.0530;  Loss pred: 0.0262; Loss self: 2.6778; time: 0.39s
Val loss: 0.1754 score: 0.9380 time: 0.21s
Test loss: 0.2792 score: 0.9375 time: 0.28s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000096
Train loss: 0.0485;  Loss pred: 0.0216; Loss self: 2.6934; time: 0.41s
Val loss: 0.1312 score: 0.9380 time: 0.22s
Test loss: 0.1805 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000096
Train loss: 0.0458;  Loss pred: 0.0189; Loss self: 2.6820; time: 0.43s
Val loss: 0.1221 score: 0.9457 time: 0.21s
Test loss: 0.1673 score: 0.9453 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000096
Train loss: 0.0611;  Loss pred: 0.0347; Loss self: 2.6436; time: 0.42s
Val loss: 0.1840 score: 0.9302 time: 0.21s
Test loss: 0.3094 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000096
Train loss: 0.0523;  Loss pred: 0.0256; Loss self: 2.6715; time: 0.42s
Val loss: 0.0992 score: 0.9612 time: 0.21s
Test loss: 0.1875 score: 0.9609 time: 0.18s
Epoch 49/1000, LR 0.000096
Train loss: 0.0424;  Loss pred: 0.0151; Loss self: 2.7327; time: 0.50s
Val loss: 0.1840 score: 0.9225 time: 0.22s
Test loss: 0.3205 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000096
Train loss: 0.0451;  Loss pred: 0.0180; Loss self: 2.7124; time: 0.42s
Val loss: 0.1007 score: 0.9612 time: 0.21s
Test loss: 0.1986 score: 0.9609 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000096
Train loss: 0.0441;  Loss pred: 0.0169; Loss self: 2.7233; time: 0.42s
Val loss: 0.2666 score: 0.9147 time: 0.21s
Test loss: 0.4792 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000096
Train loss: 0.0418;  Loss pred: 0.0146; Loss self: 2.7199; time: 0.42s
Val loss: 0.5719 score: 0.8450 time: 0.21s
Test loss: 0.8457 score: 0.8750 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000096
Train loss: 0.0434;  Loss pred: 0.0162; Loss self: 2.7205; time: 0.48s
Val loss: 0.1549 score: 0.9457 time: 0.21s
Test loss: 0.3079 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000096
Train loss: 0.0402;  Loss pred: 0.0129; Loss self: 2.7226; time: 0.43s
Val loss: 0.2921 score: 0.9070 time: 0.22s
Test loss: 0.4626 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000096
Train loss: 0.0390;  Loss pred: 0.0116; Loss self: 2.7421; time: 0.43s
Val loss: 0.0957 score: 0.9457 time: 0.21s
Test loss: 0.1691 score: 0.9609 time: 0.18s
Epoch 56/1000, LR 0.000096
Train loss: 0.0397;  Loss pred: 0.0125; Loss self: 2.7208; time: 0.43s
Val loss: 0.1266 score: 0.9380 time: 0.21s
Test loss: 0.1709 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000096
Train loss: 0.0446;  Loss pred: 0.0174; Loss self: 2.7145; time: 0.43s
Val loss: 0.3658 score: 0.8837 time: 0.25s
Test loss: 0.5633 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000096
Train loss: 0.0428;  Loss pred: 0.0155; Loss self: 2.7289; time: 0.40s
Val loss: 0.3427 score: 0.8837 time: 0.21s
Test loss: 0.5163 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000096
Train loss: 0.0387;  Loss pred: 0.0115; Loss self: 2.7247; time: 0.40s
Val loss: 0.3503 score: 0.8682 time: 0.24s
Test loss: 0.4721 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000096
Train loss: 0.0437;  Loss pred: 0.0167; Loss self: 2.7075; time: 0.41s
Val loss: 0.1152 score: 0.9612 time: 0.21s
Test loss: 0.2179 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000096
Train loss: 0.0376;  Loss pred: 0.0106; Loss self: 2.7067; time: 0.41s
Val loss: 0.2950 score: 0.9070 time: 0.21s
Test loss: 0.4571 score: 0.9219 time: 0.26s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000096
Train loss: 0.0398;  Loss pred: 0.0129; Loss self: 2.6935; time: 0.41s
Val loss: 0.1019 score: 0.9612 time: 0.21s
Test loss: 0.2132 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000096
Train loss: 0.0392;  Loss pred: 0.0126; Loss self: 2.6622; time: 0.41s
Val loss: 0.1783 score: 0.9380 time: 0.21s
Test loss: 0.3211 score: 0.9453 time: 0.28s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000096
Train loss: 0.0408;  Loss pred: 0.0136; Loss self: 2.7136; time: 0.43s
Val loss: 0.0874 score: 0.9612 time: 0.21s
Test loss: 0.1711 score: 0.9609 time: 0.18s
Epoch 65/1000, LR 0.000096
Train loss: 0.0371;  Loss pred: 0.0102; Loss self: 2.6935; time: 0.43s
Val loss: 0.1999 score: 0.9302 time: 0.21s
Test loss: 0.3728 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 66/1000, LR 0.000096
Train loss: 0.0407;  Loss pred: 0.0137; Loss self: 2.7010; time: 0.49s
Val loss: 0.3065 score: 0.8992 time: 0.21s
Test loss: 0.4697 score: 0.9219 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 67/1000, LR 0.000096
Train loss: 0.0380;  Loss pred: 0.0111; Loss self: 2.6927; time: 0.41s
Val loss: 0.2022 score: 0.9380 time: 0.21s
Test loss: 0.3668 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 68/1000, LR 0.000096
Train loss: 0.0376;  Loss pred: 0.0109; Loss self: 2.6740; time: 0.39s
Val loss: 0.0940 score: 0.9690 time: 0.21s
Test loss: 0.1832 score: 0.9688 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 69/1000, LR 0.000096
Train loss: 0.0423;  Loss pred: 0.0152; Loss self: 2.7109; time: 0.43s
Val loss: 0.1231 score: 0.9535 time: 0.21s
Test loss: 0.2379 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 70/1000, LR 0.000096
Train loss: 0.0403;  Loss pred: 0.0136; Loss self: 2.6696; time: 0.43s
Val loss: 0.1101 score: 0.9380 time: 0.21s
Test loss: 0.1541 score: 0.9609 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 71/1000, LR 0.000096
Train loss: 0.0382;  Loss pred: 0.0113; Loss self: 2.6945; time: 0.44s
Val loss: 0.3063 score: 0.9147 time: 0.21s
Test loss: 0.4949 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 72/1000, LR 0.000096
Train loss: 0.0358;  Loss pred: 0.0089; Loss self: 2.6828; time: 0.39s
Val loss: 0.1008 score: 0.9612 time: 0.21s
Test loss: 0.2275 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 73/1000, LR 0.000096
Train loss: 0.0391;  Loss pred: 0.0122; Loss self: 2.6925; time: 0.41s
Val loss: 0.1352 score: 0.9457 time: 0.21s
Test loss: 0.2811 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 74/1000, LR 0.000096
Train loss: 0.0355;  Loss pred: 0.0090; Loss self: 2.6567; time: 0.44s
Val loss: 0.3431 score: 0.9070 time: 0.21s
Test loss: 0.5170 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 75/1000, LR 0.000096
Train loss: 0.0410;  Loss pred: 0.0150; Loss self: 2.5970; time: 0.44s
Val loss: 0.1063 score: 0.9535 time: 0.21s
Test loss: 0.2152 score: 0.9609 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 76/1000, LR 0.000096
Train loss: 0.0355;  Loss pred: 0.0092; Loss self: 2.6312; time: 0.46s
Val loss: 0.1490 score: 0.9302 time: 0.22s
Test loss: 0.1701 score: 0.9531 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 77/1000, LR 0.000096
Train loss: 0.0349;  Loss pred: 0.0086; Loss self: 2.6303; time: 0.45s
Val loss: 0.1494 score: 0.9302 time: 0.21s
Test loss: 0.1611 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 78/1000, LR 0.000096
Train loss: 0.0354;  Loss pred: 0.0088; Loss self: 2.6561; time: 0.44s
Val loss: 0.1250 score: 0.9457 time: 0.21s
Test loss: 0.2017 score: 0.9531 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 79/1000, LR 0.000096
Train loss: 0.0362;  Loss pred: 0.0095; Loss self: 2.6672; time: 0.39s
Val loss: 0.1448 score: 0.9380 time: 0.21s
Test loss: 0.2503 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 80/1000, LR 0.000096
Train loss: 0.0354;  Loss pred: 0.0090; Loss self: 2.6368; time: 0.42s
Val loss: 0.2846 score: 0.9147 time: 0.21s
Test loss: 0.4296 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 81/1000, LR 0.000095
Train loss: 0.0353;  Loss pred: 0.0089; Loss self: 2.6407; time: 0.43s
Val loss: 0.1068 score: 0.9612 time: 0.30s
Test loss: 0.1996 score: 0.9609 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 82/1000, LR 0.000095
Train loss: 0.0346;  Loss pred: 0.0085; Loss self: 2.6113; time: 0.45s
Val loss: 0.2824 score: 0.9225 time: 0.22s
Test loss: 0.4345 score: 0.9297 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 83/1000, LR 0.000095
Train loss: 0.0356;  Loss pred: 0.0097; Loss self: 2.5926; time: 0.43s
Val loss: 0.1003 score: 0.9612 time: 0.30s
Test loss: 0.1938 score: 0.9531 time: 0.35s
     INFO: Early stopping counter 19 of 20
Epoch 84/1000, LR 0.000095
Train loss: 0.0344;  Loss pred: 0.0083; Loss self: 2.6048; time: 0.44s
Val loss: 0.3107 score: 0.9070 time: 0.23s
Test loss: 0.4651 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 063,   Train_Loss: 0.0408,   Val_Loss: 0.0874,   Val_Precision: 0.9688,   Val_Recall: 0.9538,   Val_accuracy: 0.9612,   Val_Score: 0.9612,   Val_Loss: 0.0874,   Test_Precision: 0.9836,   Test_Recall: 0.9375,   Test_accuracy: 0.9600,   Test_Score: 0.9609,   Test_loss: 0.1711


[0.213627491844818, 0.213681201916188, 0.2153941229917109, 0.2171111151110381, 0.21321348496712744, 0.21267742291092873, 0.21448990097269416, 0.21426719520241022, 0.21348713105544448, 0.21388688497245312, 0.2135059661231935, 0.21347777708433568, 0.21354604698717594, 0.2132729620207101, 0.21323773497715592, 0.21338571701198816, 0.2129195670131594, 0.21790446690283716, 0.21425099507905543, 0.2157651069574058, 0.2136197059880942, 0.2135233930312097, 0.21385216410271823, 0.21451947605237365, 0.21339625492691994, 0.21368708601221442, 0.2140804030932486, 0.21379256411455572, 0.2138300638180226, 0.21470457408577204, 0.2134073032066226, 0.2128003619145602, 0.21548200398683548, 0.21363969892263412, 0.2142296361271292, 0.21356197097338736, 0.217106516007334, 0.21599516505375504, 0.21808111807331443, 0.21359476703219116, 0.21310974098742008, 0.21412140596657991, 0.21472347993403673, 0.21728904685005546, 0.21944403694942594, 0.21634388901293278, 0.21737774298526347, 0.2911536870524287, 0.21699251607060432, 0.21538386191241443, 0.21608945704065263, 0.21815499011427164, 0.23112457199022174, 0.23227475909516215, 0.22921004402451217, 0.23193705989979208, 0.2217349992133677, 0.22111501800827682, 0.3023038150276989, 0.21856707311235368, 0.27231076383031905, 0.21670517302118242, 0.21631299308501184, 0.21541528310626745, 0.2141808201558888, 0.2142822709865868, 0.21590262185782194, 0.21386175788939, 0.21425833203829825, 0.214252243982628, 0.21506656100973487, 0.21457446902059019, 0.2142409288790077, 0.21440549497492611, 0.2139789629727602, 0.21390146389603615, 0.2136501558125019, 0.21388821699656546, 0.21369948494248092, 0.21451743016950786, 0.21485074702650309, 0.21402235492132604, 0.2150865530129522, 0.2142716699745506, 0.21979243610985577, 0.2139745729509741, 0.21351595898158848, 0.2153271611314267, 0.21307719289325178, 0.21532742911949754, 0.21419778699055314, 0.21743974997662008, 0.2138818979728967, 0.21392006310634315, 0.21468117809854448, 0.21474867896176875, 0.21484040399082005, 0.21387927094474435, 0.21401905291713774, 0.21435344917699695, 0.21386361215263605, 0.21533303707838058, 0.2139726139139384, 0.21465820400044322, 0.21493832091800869, 0.21487455000169575, 0.21863186196424067, 0.2156661490444094, 0.21526266913861036, 0.21363790100440383, 0.21434743702411652, 0.2958213018719107, 0.21446604584343731, 0.21460523107089102, 0.2147315419279039, 0.2156505819875747, 0.21497708512470126, 0.21673488593660295, 0.3079635901376605, 0.21408218704164028, 0.2147402218542993, 0.2161391549743712, 0.2184690658468753, 0.2161043209489435, 0.2148614600300789, 0.227551877964288, 0.21912212297320366, 0.24798235204070807, 0.21993383788503706, 0.302144339075312, 0.2194330538623035, 0.21692926506511867, 0.2193220630288124, 0.22024959698319435, 0.21944244299083948, 0.21261778799816966, 0.19156951899640262, 0.1875509498640895, 0.18727868399582803, 0.18671397189609706, 0.18685042904689908, 0.18692688597366214, 0.18681154609657824, 0.18706809007562697, 0.18675170606002212, 0.1870731229428202, 0.18645757902413607, 0.1868911769706756, 0.18649767083115876, 0.18699063709937036, 0.18637903896160424, 0.18699283804744482, 0.18689877912402153, 0.1875441218726337, 0.18656711420044303, 0.18653992493636906, 0.1875670209992677, 0.18718315311707556, 0.18723298492841423, 0.18688915413804352, 0.18741425382904708, 0.18754468811675906, 0.18655733903869987, 0.18592839199118316, 0.18643885804340243, 0.1871394868940115, 0.18692377698607743, 0.18623234378173947, 0.1871099779382348, 0.1872907360084355, 0.18632241105660796, 0.18621199391782284, 0.18626417289488018, 0.18695358699187636, 0.18634168594144285, 0.18648456898517907, 0.18677314301021397, 0.18633547914214432, 0.1879349430091679, 0.28361769299954176, 0.18812095606699586, 0.26197439595125616, 0.18951017200015485, 0.18782124086283147, 0.18725174805149436, 0.18921092594973743, 0.18793312110938132, 0.188176441937685, 0.18771877605468035, 0.18939693295396864, 0.18673931201919913, 0.18976465612649918, 0.18644929281435907, 0.1896986539941281, 0.18799049500375986, 0.18974642688408494, 0.2629007811192423, 0.1881105611100793, 0.2813402411993593, 0.18921083700843155, 0.1888315409887582, 0.19106069579720497, 0.1877625009510666, 0.18718573707155883, 0.18745287181809545, 0.18724643485620618, 0.1877136358525604, 0.1872098259627819, 0.18765535484999418, 0.1880302259232849, 0.1883615031838417, 0.19722301792353392, 0.18736141407862306, 0.18780049798078835, 0.1877370709553361, 0.18779961904510856, 0.2010682849213481, 0.19925233605317771, 0.35405143979005516, 0.20408209413290024]
[0.0016560270685644805, 0.0016564434257068837, 0.0016697218836566738, 0.0016830319000855666, 0.0016528177129234685, 0.0016486621931079747, 0.00166271241064104, 0.0016609860093210094, 0.0016549390004298023, 0.0016580378680035126, 0.0016550850087069264, 0.001654866489025858, 0.0016553957130788833, 0.001653278775354342, 0.0016530056974973328, 0.0016541528450541717, 0.0016505392791717783, 0.0016891819139754818, 0.001660860426969422, 0.001672597728351983, 0.0016559667130860017, 0.0016552201010171296, 0.00165776871397456, 0.0016629416748246018, 0.0016542345343172088, 0.0016564890388543754, 0.0016595380084747954, 0.0016573066985624474, 0.0016575973939381597, 0.0016643765433005584, 0.0016543201798962992, 0.0016496152086400014, 0.0016704031316808951, 0.0016561216970746831, 0.0016606948536986759, 0.0016555191548324603, 0.001682996248118868, 0.0016743811244477136, 0.0016905513028939102, 0.001655773387846443, 0.0016520134960265123, 0.001659855860206046, 0.0016645231002638506, 0.0016844112158919027, 0.0017011165654994258, 0.0016770844109529673, 0.0016850987828314998, 0.0022570053259878195, 0.0016821125276791033, 0.0016696423404063134, 0.0016751120700825785, 0.0016911239543741987, 0.0017916633487614088, 0.0018005795278694741, 0.0017768220467016447, 0.001797961704649551, 0.0017188759628943232, 0.0017140699070409056, 0.002343440426571309, 0.001694318396219796, 0.0021109361537234033, 0.0016798850621797087, 0.0016768449076357507, 0.001669885915552461, 0.0016603164353169674, 0.001661102875865014, 0.001673663735331953, 0.0016578430844138758, 0.001660917302622467, 0.001660870108392465, 0.0016671826434863169, 0.0016633679769037998, 0.0016607823944109123, 0.0016620581005808226, 0.0016587516509516294, 0.0016581508829150088, 0.001656202758236449, 0.0016580481937718253, 0.0016565851545928753, 0.0016629258152675028, 0.001665509666872117, 0.0016590880226459383, 0.0016673376202554434, 0.0016610206974771364, 0.0017038173341849285, 0.0016587176197749931, 0.0016551624727254922, 0.001669202799468424, 0.001651761185219006, 0.0016692048768953297, 0.0016604479611670787, 0.0016855794571831015, 0.0016579992090922225, 0.0016582950628398693, 0.0016641951790584842, 0.0016647184415640988, 0.0016654294883009307, 0.001657978844532902, 0.001659062425714246, 0.001661654644782922, 0.0016578574585475662, 0.0016692483494448107, 0.001658702433441383, 0.0016640170852747537, 0.0016661885342481293, 0.001665694186059657, 0.0016948206353817107, 0.0016718306127473596, 0.0016687028615396152, 0.0016561077597240608, 0.0016616080389466397, 0.0022931883866039587, 0.0016625274871584288, 0.001663606442410008, 0.0016645855963403403, 0.001671709937888176, 0.0016664890319744285, 0.0016801153948573873, 0.0023873146522299265, 0.001659551837532095, 0.0016646528825914675, 0.0016754973253827225, 0.0016935586499757776, 0.0016752272941778566, 0.0016655927134114643, 0.0017639680462347906, 0.0016986211083194081, 0.0019223438142690547, 0.0017049134719770314, 0.002342204178878388, 0.0017010314252891743, 0.0016816222098071216, 0.001700171031231104, 0.0017073612169239871, 0.0017011042092313139, 0.0016481999069625556, 0.0014966368671593955, 0.0014652417958131991, 0.0014631147187174065, 0.0014587029054382583, 0.001459768976928899, 0.0014603662966692355, 0.0014594652038795175, 0.0014614694537158357, 0.0014589977035939228, 0.0014615087729907827, 0.001456699836126063, 0.001460087320083403, 0.0014570130533684278, 0.001460864352338831, 0.001456086241887533, 0.0014608815472456627, 0.0014601467119064182, 0.0014651884521299507, 0.0014575555796909612, 0.0014573431635653833, 0.0014653673515567789, 0.0014623683837271528, 0.0014627576947532361, 0.001460071516703465, 0.0014641738580394303, 0.0014651928759121802, 0.0014574792112398427, 0.0014525655624311185, 0.0014565535784640815, 0.0014620272413594648, 0.00146034200770373, 0.0014549401857948396, 0.0014617967026424594, 0.0014632088750659022, 0.0014556438363797497, 0.001454781202482991, 0.0014551888507412514, 0.001460574898374034, 0.0014557944214175222, 0.0014569106951967115, 0.0014591651797672966, 0.0014557459307980025, 0.0014682417422591243, 0.00221576322655892, 0.0014696949692734051, 0.0020466749683691887, 0.0014805482187512098, 0.0014673534442408709, 0.0014629042816522997, 0.0014782103589823237, 0.0014682275086670415, 0.0014701284526381642, 0.0014665529379271902, 0.00147966353870288, 0.0014589008751499932, 0.0014825363759882748, 0.0014566351001121802, 0.0014820207343291258, 0.001468675742216874, 0.0014823939600319136, 0.0020539123524940806, 0.0014696137586724944, 0.0021979706343699945, 0.0014782096641283715, 0.0014752464139746735, 0.0014926616859156638, 0.001466894538680208, 0.0014623885708715534, 0.0014644755610788707, 0.0014628627723141108, 0.0014665127800981281, 0.0014625767653342336, 0.0014660574597655796, 0.0014689861400256632, 0.0014715742436237633, 0.0015408048275276087, 0.0014637610474892426, 0.001467191390474909, 0.0014666958668385632, 0.0014671845237899106, 0.001570845975948032, 0.001556658875415451, 0.002766026873359806, 0.0015943913604132831]
[603.8548638379718, 603.7030812406117, 598.9021344141519, 594.1658027688953, 605.0273978678638, 606.5523939230089, 601.4269176077547, 602.0520307746527, 604.2518786132246, 603.1225337477524, 604.1985727254416, 604.278355161239, 604.0851695454087, 604.8586692741357, 604.9585924077637, 604.5390563453349, 605.8625884394514, 592.0025497114782, 602.097553630502, 597.8723891878676, 603.8768727038208, 604.1492605034834, 603.2204562495718, 601.3440008985731, 604.5092030512793, 603.6864576487618, 602.5773407377718, 603.3886189366174, 603.2828017569308, 600.8255788182042, 604.4779070897175, 606.2019765351422, 598.6578814622545, 603.8203604036864, 602.157583479478, 604.0401266762756, 594.1783893563208, 597.2355907498928, 591.5230128113743, 603.9473803239675, 605.3219313312145, 602.46195104909, 600.7726776765584, 593.6792575146181, 587.8491928661061, 596.272908786846, 593.4370199470936, 443.0649713076471, 594.4905489644924, 598.9306666460353, 596.9749832622856, 591.3227102090517, 558.1405684786196, 555.3767464985257, 562.802561942724, 556.1853722545857, 581.7755449416801, 583.4067769886677, 426.7230302342703, 590.2078394657734, 473.7234701467102, 595.2788214584548, 596.3580742896129, 598.8433046153111, 602.2948268949058, 602.0096735304568, 597.4915862066288, 603.1933959259758, 602.0769356915441, 602.0940439273046, 599.8143058332573, 601.1898833482439, 602.1258434370054, 601.6636841098035, 602.8630020812927, 603.0814266081819, 603.7908070294582, 603.1187777028009, 603.6514315171207, 601.3497360007832, 600.4168092749852, 602.7407746607591, 599.7585539075137, 602.0394577375606, 586.9173766085551, 602.8753707551808, 604.1702953507269, 599.0883793859326, 605.4143958270884, 599.0876337840383, 602.2471184806841, 593.267790336728, 603.1365965171442, 603.0289918897041, 600.8910568805687, 600.702181841899, 600.4457150690895, 603.1440047003298, 602.7500740784291, 601.8097702429855, 603.1881660538481, 599.072031631841, 602.8808904109799, 600.9553680964071, 600.1721770647353, 600.350297412988, 590.0329386624314, 598.1467215489467, 599.2678643083036, 603.8254419909362, 601.8266501851666, 436.0740730424357, 601.4938145228429, 601.1037072874852, 600.750121951398, 598.1898996564391, 600.0639553056144, 595.1972126800747, 418.8806863250837, 602.5723194564932, 600.7258392772183, 596.837717882705, 590.4726122206058, 596.9339226237746, 600.3868724616366, 566.9036931448453, 588.7128065830914, 520.198308220029, 586.5400305860636, 426.94826053929694, 587.8786159579624, 594.6638871490035, 588.1761197141996, 585.699142095788, 587.8534628115904, 606.722519383517, 668.1647512118233, 682.4812142660774, 683.4734058834556, 685.5405554289728, 685.0399041249847, 684.759708766748, 685.1824883127207, 684.2428334423727, 685.4020383560015, 684.2244251148992, 686.4832240658396, 684.8905447263785, 686.3356492847665, 684.5262521458675, 686.7725078589399, 684.5181951167732, 684.8626866367184, 682.506061623879, 686.0801837910191, 686.1801839132415, 682.4227378463282, 683.8222236802536, 683.6402252997191, 684.8979577779793, 682.9790017826352, 682.5040009680864, 686.1161327641331, 688.4370839181454, 686.5521562581227, 683.9817834516892, 684.771097951513, 687.31347842571, 684.0896536381023, 683.429424903509, 686.9812347002712, 687.3885903208128, 687.1960292237087, 684.6619102609781, 686.9101744642554, 686.3838691670668, 685.3233711069483, 686.9330553112559, 681.0867524181273, 451.31175931329085, 680.4132972533646, 488.5973666824147, 675.425485867299, 681.4990648128037, 683.5717227312607, 676.4937032970406, 681.0933551489368, 680.212669991855, 681.8710556833965, 675.8293178438604, 685.4475290496947, 674.5197056857301, 686.5137328648656, 674.7543923214241, 680.8854883723774, 674.5845078716266, 486.8756930088535, 680.4508967739267, 454.9651320917809, 676.4940212927451, 677.852859378086, 669.9441738444277, 681.712266036329, 683.8127840427669, 682.8382982801746, 683.5911193625458, 681.8897275024684, 683.7247956495987, 682.1015051892294, 680.7416167878412, 679.5443752382429, 649.0114660431135, 683.1716158284702, 681.5743375350057, 681.8046076283573, 681.5775274243503, 636.5996509597213, 642.4015022129455, 361.52938701760746, 627.1985817464474]
Elapsed: 0.20981513737087054~0.024028101402109616
Time per graph: 0.001630962954232691~0.00018403063307191782
Speed: 619.5348862413235~58.016191826056065
Total Time: 0.2051
best val loss: 0.08738529171014942 test_score: 0.9609

Testing...
Test loss: 0.1905 score: 0.9609 time: 0.20s
test Score 0.9609
Epoch Time List: [0.7836062798742205, 0.7895430130884051, 0.7968918918631971, 0.9409429058432579, 0.8019228440243751, 0.8014834681525826, 0.8091826736927032, 0.8077139193192124, 0.8014970351941884, 0.802229902241379, 0.801603107014671, 0.8018870078958571, 0.8012890820391476, 0.8025224599987268, 0.8030894645489752, 0.8068508389405906, 0.8045546794310212, 0.811675266828388, 0.8049095019232482, 0.8087499346584082, 0.8066361309029162, 0.8075729869306087, 0.809883524896577, 0.8110811670776457, 0.8066726888064295, 0.8060439210385084, 0.804615173721686, 0.8036910858936608, 0.8022439032793045, 0.8033933981787413, 0.8010715590789914, 0.8000960790086538, 0.7886132900603116, 0.7628793979529291, 0.7671946820337325, 0.7612465033307672, 0.7649664618074894, 0.7644003059249371, 0.7722017620690167, 0.7610762771219015, 0.7866803302895278, 0.7960154563188553, 0.7996333320625126, 0.8889045359101146, 0.7729579226579517, 0.8675369769334793, 0.7640296958852559, 0.8496666289865971, 0.764191335067153, 0.7758518178015947, 0.7817853258457035, 0.7692281790077686, 0.9045360099989921, 0.8120185809675604, 0.870988825103268, 0.8164247367531061, 0.8144082620274276, 0.7746303370222449, 0.8712595000397414, 0.7763180288020521, 0.8352576743345708, 0.7664909849409014, 0.7799155570100993, 0.7583281239494681, 0.7530014379881322, 0.7499062798451632, 0.7537200183141977, 0.7503237959463149, 0.7497114778961986, 0.7501207487657666, 0.753840422956273, 0.7628747851122171, 0.7490951041691005, 0.7609890562016517, 0.7757567842490971, 0.787322802701965, 0.7869784198701382, 0.7899475009180605, 0.788428959203884, 0.7881494069006294, 0.8014502730220556, 0.7912486852146685, 0.797140927053988, 0.7921946309506893, 0.7963361348956823, 0.7724658942315727, 0.7879706679377705, 0.7973502620588988, 0.7890950217843056, 0.7927886948455125, 0.7906419502105564, 0.795202795881778, 0.7911246584262699, 0.7875861618667841, 0.7731434390880167, 0.7890759611036628, 0.7918426527176052, 0.7886985039804131, 0.7892510499805212, 0.7914073441643268, 0.7902728300541639, 0.7920378816779703, 0.7875738262664527, 0.7926856810227036, 0.7945409950334579, 0.7928899589460343, 0.7942746439948678, 0.7971311514265835, 0.778689915779978, 0.790830094832927, 0.7933793398551643, 0.8637820167932659, 0.7601930210366845, 0.7893421498592943, 0.8152156199794263, 0.7532379990443587, 0.770766681060195, 0.7546103089116514, 0.88329929090105, 0.7535377778112888, 0.7717913549859077, 0.768010837957263, 0.758114418014884, 0.8323718709871173, 0.7545163542963564, 0.8621626738458872, 0.767948972992599, 0.8337294298689812, 0.7850934707093984, 0.8399615972302854, 0.7637129491195083, 0.7721520469058305, 0.7850293610244989, 0.7880882369354367, 0.8852202061098069, 0.7717961128801107, 0.8398431329987943, 0.8388420720584691, 0.8305779071524739, 0.8334095550235361, 0.8298313850536942, 0.8294385380577296, 0.8239118580240756, 0.7905022429767996, 0.8141081999056041, 0.8353181192651391, 0.8095502788200974, 0.7819643253460526, 0.7991020956542343, 0.7932092959526926, 0.7792241470888257, 0.7846485245972872, 0.7838325349148363, 0.7854981750715524, 0.8226085938513279, 0.8281301502138376, 0.8455132790841162, 0.8298781961202621, 0.8043896572198719, 0.7849339451640844, 0.7920450731180608, 0.7973484711255878, 0.7839919498655945, 0.7808931528124958, 0.7840403108857572, 0.7838156239595264, 0.7815676140598953, 0.7873832730110735, 0.7842668571975082, 0.7848214150872082, 0.7850460999179631, 0.7837234211619943, 0.7825653909239918, 0.7846815600059927, 0.7918181638233364, 0.7832047308329493, 0.7881251808721572, 0.7824285998940468, 0.7809901221189648, 0.8817938240244985, 0.805386115796864, 0.8951378620695323, 0.8124890769831836, 0.8160194894298911, 0.9026396351400763, 0.8158849100582302, 0.8121481270063668, 0.8151593210641295, 0.8740167689975351, 0.831430776976049, 0.8173501961864531, 0.8265147448983043, 0.8632873932365328, 0.7928861777763814, 0.8218687300104648, 0.8029044969007373, 0.8760722957085818, 0.7994985389523208, 0.8997305792290717, 0.8225671020336449, 0.8215492779854685, 0.8878105392213911, 0.8048946599010378, 0.7807052761781961, 0.8235109888482839, 0.8261007950641215, 0.8333720811642706, 0.7848716389853507, 0.8049282790161669, 0.8361663720570505, 0.8393477872014046, 0.8682974767871201, 0.8443751509767026, 0.8287830860354006, 0.7835471963044256, 0.8109721129294485, 0.924443184863776, 0.8718782057985663, 1.085783656919375, 0.8606710790190846]
Total Epoch List: [63, 73, 84]
Total Time List: [0.2169127380475402, 0.2133794380351901, 0.2050678359810263]
T-times Epoch Time: 0.7682436101005989 ~ 0.044942503789558504
T-times Total Epoch: 72.33333333333333 ~ 17.025036030853265
T-times Total Time: 0.20120092988428143 ~ 0.013299391516593331
T-times Inference Elapsed: 0.20209152406511363 ~ 0.011084884890147754
T-times Time Per Graph: 0.0015700466920657923 ~ 8.62589302619712e-05
T-times Speed: 642.5943219983878 ~ 36.17794719344421
T-times cross validation test micro f1 score:0.9156504259679469 ~ 0.003678320441362066
T-times cross validation test precision:0.9628011801573843 ~ 0.01421171987283547
T-times cross validation test recall:0.8740384615384614 ~ 0.010801765504410508
T-times cross validation test f1_score:0.9156504259679469 ~ 0.0034971979695497695
