Namespace(seed=15, model='GPSPerformer', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c72d0f850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7180;  Loss pred: 0.7180; Loss self: 0.0000; time: 1.49s
Val loss: 0.7935 score: 0.3566 time: 0.25s
Test loss: 0.8012 score: 0.2558 time: 0.45s
Epoch 2/1000, LR 0.000015
Train loss: 0.7424;  Loss pred: 0.7424; Loss self: 0.0000; time: 0.35s
Val loss: 0.7579 score: 0.3333 time: 0.23s
Test loss: 0.7581 score: 0.3643 time: 0.25s
Epoch 3/1000, LR 0.000045
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.45s
Val loss: 0.7200 score: 0.4651 time: 0.23s
Test loss: 0.7088 score: 0.4884 time: 0.31s
Epoch 4/1000, LR 0.000075
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.35s
Val loss: 0.6853 score: 0.6279 time: 0.24s
Test loss: 0.6725 score: 0.6047 time: 0.24s
Epoch 5/1000, LR 0.000105
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.41s
Val loss: 0.6627 score: 0.6822 time: 0.20s
Test loss: 0.6451 score: 0.6589 time: 0.20s
Epoch 6/1000, LR 0.000135
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.32s
Val loss: 0.6479 score: 0.6899 time: 0.31s
Test loss: 0.6294 score: 0.6744 time: 0.29s
Epoch 7/1000, LR 0.000165
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.32s
Val loss: 0.6372 score: 0.7364 time: 0.22s
Test loss: 0.6187 score: 0.6899 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.32s
Val loss: 0.6255 score: 0.7364 time: 0.20s
Test loss: 0.6056 score: 0.7132 time: 0.20s
Epoch 9/1000, LR 0.000225
Train loss: 0.5175;  Loss pred: 0.5175; Loss self: 0.0000; time: 0.32s
Val loss: 0.6163 score: 0.7597 time: 0.20s
Test loss: 0.5920 score: 0.7364 time: 0.20s
Epoch 10/1000, LR 0.000255
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.48s
Val loss: 0.6084 score: 0.7442 time: 0.21s
Test loss: 0.5802 score: 0.6977 time: 0.20s
Epoch 11/1000, LR 0.000285
Train loss: 0.4718;  Loss pred: 0.4718; Loss self: 0.0000; time: 0.33s
Val loss: 0.6028 score: 0.7132 time: 0.24s
Test loss: 0.5703 score: 0.7132 time: 0.20s
Epoch 12/1000, LR 0.000285
Train loss: 0.4359;  Loss pred: 0.4359; Loss self: 0.0000; time: 0.33s
Val loss: 0.5964 score: 0.7209 time: 0.43s
Test loss: 0.5592 score: 0.7287 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.3774;  Loss pred: 0.3774; Loss self: 0.0000; time: 0.31s
Val loss: 0.5864 score: 0.7287 time: 0.21s
Test loss: 0.5491 score: 0.7209 time: 0.31s
Epoch 14/1000, LR 0.000285
Train loss: 0.3562;  Loss pred: 0.3562; Loss self: 0.0000; time: 0.32s
Val loss: 0.5765 score: 0.7364 time: 0.20s
Test loss: 0.5429 score: 0.7132 time: 0.19s
Epoch 15/1000, LR 0.000285
Train loss: 0.3198;  Loss pred: 0.3198; Loss self: 0.0000; time: 0.31s
Val loss: 0.5655 score: 0.7442 time: 0.19s
Test loss: 0.5353 score: 0.7132 time: 0.19s
Epoch 16/1000, LR 0.000285
Train loss: 0.3018;  Loss pred: 0.3018; Loss self: 0.0000; time: 0.32s
Val loss: 0.5599 score: 0.7364 time: 0.20s
Test loss: 0.5290 score: 0.7287 time: 0.19s
Epoch 17/1000, LR 0.000285
Train loss: 0.2872;  Loss pred: 0.2872; Loss self: 0.0000; time: 0.32s
Val loss: 0.5597 score: 0.7364 time: 0.34s
Test loss: 0.5259 score: 0.7519 time: 0.21s
Epoch 18/1000, LR 0.000285
Train loss: 0.2775;  Loss pred: 0.2775; Loss self: 0.0000; time: 0.34s
Val loss: 0.5835 score: 0.6977 time: 0.21s
Test loss: 0.5453 score: 0.7054 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.2578;  Loss pred: 0.2578; Loss self: 0.0000; time: 0.34s
Val loss: 0.6132 score: 0.6744 time: 0.21s
Test loss: 0.5714 score: 0.6744 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.2465;  Loss pred: 0.2465; Loss self: 0.0000; time: 0.34s
Val loss: 0.6340 score: 0.6744 time: 0.21s
Test loss: 0.5871 score: 0.6667 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.2315;  Loss pred: 0.2315; Loss self: 0.0000; time: 0.34s
Val loss: 0.6530 score: 0.6589 time: 0.22s
Test loss: 0.6007 score: 0.6667 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.2245;  Loss pred: 0.2245; Loss self: 0.0000; time: 0.34s
Val loss: 0.6921 score: 0.6589 time: 0.21s
Test loss: 0.6356 score: 0.6434 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.34s
Val loss: 0.6900 score: 0.6589 time: 0.20s
Test loss: 0.6307 score: 0.6512 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.1968;  Loss pred: 0.1968; Loss self: 0.0000; time: 0.33s
Val loss: 0.6316 score: 0.7287 time: 0.20s
Test loss: 0.5781 score: 0.7209 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.1903;  Loss pred: 0.1903; Loss self: 0.0000; time: 0.33s
Val loss: 0.5921 score: 0.7364 time: 0.20s
Test loss: 0.5409 score: 0.7287 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.33s
Val loss: 0.5988 score: 0.7364 time: 0.21s
Test loss: 0.5479 score: 0.7287 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.1631;  Loss pred: 0.1631; Loss self: 0.0000; time: 0.34s
Val loss: 0.5745 score: 0.7442 time: 0.21s
Test loss: 0.5175 score: 0.7519 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 0.32s
Val loss: 0.5508 score: 0.7597 time: 0.20s
Test loss: 0.4839 score: 0.7907 time: 0.20s
Epoch 29/1000, LR 0.000285
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.33s
Val loss: 0.5598 score: 0.7597 time: 0.21s
Test loss: 0.4861 score: 0.7907 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.31s
Val loss: 0.5156 score: 0.8295 time: 0.20s
Test loss: 0.4449 score: 0.8062 time: 0.20s
Epoch 31/1000, LR 0.000285
Train loss: 0.1271;  Loss pred: 0.1271; Loss self: 0.0000; time: 0.32s
Val loss: 0.5214 score: 0.8217 time: 0.20s
Test loss: 0.4489 score: 0.7984 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.34s
Val loss: 0.6113 score: 0.7674 time: 5.03s
Test loss: 0.5297 score: 0.7674 time: 1.71s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 0.34s
Val loss: 0.5980 score: 0.7752 time: 0.34s
Test loss: 0.5066 score: 0.7829 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.33s
Val loss: 0.5351 score: 0.8140 time: 0.20s
Test loss: 0.4528 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0872;  Loss pred: 0.0872; Loss self: 0.0000; time: 0.35s
Val loss: 0.6439 score: 0.7752 time: 0.25s
Test loss: 0.5379 score: 0.7752 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.33s
Val loss: 0.6385 score: 0.7907 time: 0.22s
Test loss: 0.5236 score: 0.8140 time: 0.37s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.42s
Val loss: 0.5597 score: 0.8295 time: 0.23s
Test loss: 0.4494 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.53s
Val loss: 0.5468 score: 0.8450 time: 0.21s
Test loss: 0.4280 score: 0.8527 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.34s
Val loss: 0.5135 score: 0.8605 time: 0.32s
Test loss: 0.3905 score: 0.8605 time: 0.19s
Epoch 40/1000, LR 0.000284
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.33s
Val loss: 0.4864 score: 0.8682 time: 0.21s
Test loss: 0.3643 score: 0.8837 time: 0.23s
Epoch 41/1000, LR 0.000284
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.37s
Val loss: 0.4884 score: 0.8682 time: 0.23s
Test loss: 0.3640 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.34s
Val loss: 0.4813 score: 0.8760 time: 0.32s
Test loss: 0.3509 score: 0.8915 time: 0.23s
Epoch 43/1000, LR 0.000284
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.34s
Val loss: 0.5520 score: 0.8682 time: 0.21s
Test loss: 0.4013 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0301;  Loss pred: 0.0301; Loss self: 0.0000; time: 0.33s
Val loss: 0.6950 score: 0.8450 time: 0.20s
Test loss: 0.5104 score: 0.8527 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.33s
Val loss: 0.8001 score: 0.8217 time: 0.21s
Test loss: 0.5923 score: 0.8372 time: 0.44s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.33s
Val loss: 0.7720 score: 0.8450 time: 0.21s
Test loss: 0.5674 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.34s
Val loss: 0.7339 score: 0.8450 time: 0.23s
Test loss: 0.5328 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.34s
Val loss: 0.7177 score: 0.8682 time: 0.22s
Test loss: 0.5099 score: 0.8760 time: 0.31s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.36s
Val loss: 0.7951 score: 0.8527 time: 0.21s
Test loss: 0.5741 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.34s
Val loss: 0.8279 score: 0.8527 time: 0.21s
Test loss: 0.6109 score: 0.8527 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.34s
Val loss: 0.8345 score: 0.8605 time: 0.20s
Test loss: 0.6222 score: 0.8527 time: 7.02s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 3.99s
Val loss: 0.7638 score: 0.8682 time: 2.88s
Test loss: 0.5648 score: 0.8682 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.34s
Val loss: 0.7118 score: 0.8760 time: 0.21s
Test loss: 0.5078 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.34s
Val loss: 0.7214 score: 0.8837 time: 0.21s
Test loss: 0.5118 score: 0.8682 time: 0.33s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.33s
Val loss: 0.7974 score: 0.8682 time: 0.20s
Test loss: 0.5772 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.33s
Val loss: 0.9015 score: 0.8682 time: 0.21s
Test loss: 0.6644 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.34s
Val loss: 0.9124 score: 0.8605 time: 0.22s
Test loss: 0.6758 score: 0.8682 time: 0.35s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.49s
Val loss: 0.8318 score: 0.8682 time: 0.25s
Test loss: 0.6110 score: 0.8760 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.36s
Val loss: 0.8146 score: 0.8760 time: 0.22s
Test loss: 0.5956 score: 0.8837 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.37s
Val loss: 0.8451 score: 0.8682 time: 0.24s
Test loss: 0.6177 score: 0.8837 time: 0.38s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.37s
Val loss: 0.8821 score: 0.8682 time: 0.25s
Test loss: 0.6490 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.33s
Val loss: 0.9133 score: 0.8682 time: 0.20s
Test loss: 0.6784 score: 0.8760 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0436,   Val_Loss: 0.4813,   Val_Precision: 0.9800,   Val_Recall: 0.7656,   Val_accuracy: 0.8596,   Val_Score: 0.8760,   Val_Loss: 0.4813,   Test_Precision: 0.9636,   Test_Recall: 0.8154,   Test_accuracy: 0.8833,   Test_Score: 0.8915,   Test_loss: 0.3509


[0.45366635895334184, 0.25173455802723765, 0.31835447787307203, 0.248685842147097, 0.20160443801432848, 0.2972783800214529, 0.19881387101486325, 0.20376998302526772, 0.20215437095612288, 0.2060258009005338, 0.20137024112045765, 0.19561477890238166, 0.31904982402920723, 0.19760222686454654, 0.19935904606245458, 0.19820336904376745, 0.2198354417923838, 0.21666751196607947, 0.2158999228850007, 0.21153824008069932, 0.22087100194767118, 0.2062506559304893, 0.20569300488568842, 0.20526038808748126, 0.210061154095456, 0.24240144877694547, 0.21110697509720922, 0.20547522604465485, 0.22182475891895592, 0.20463787112385035, 0.20195089001208544, 1.71437360602431, 0.2132786880247295, 0.21528322878293693, 0.22550070495344698, 0.37897144979797304, 0.22172701894305646, 0.21410013688728213, 0.19472973584197462, 0.2386408078018576, 0.23012369801290333, 0.23901833896525204, 0.22479737596586347, 0.21051145903766155, 0.44762017019093037, 0.21195886912755668, 0.2313241099473089, 0.31145303789526224, 0.20706873200833797, 0.21224574907682836, 7.032253762008622, 0.21931399800814688, 0.20678466907702386, 0.33111198293045163, 0.20399723504669964, 0.2065676001366228, 0.3566613560542464, 0.20284778415225446, 0.20885716192424297, 0.38974063703790307, 0.2352138760033995, 0.20110014802776277]
[0.003516793480258464, 0.0019514306823816873, 0.002467864169558698, 0.0019277972259464883, 0.0015628251008862673, 0.0023044835660577744, 0.0015411927985648315, 0.0015796122715137032, 0.0015670881469466889, 0.0015970992317870837, 0.001561009621088819, 0.0015163936349021833, 0.0024732544498388158, 0.0015318002082522987, 0.0015454189617244542, 0.0015364602251454842, 0.0017041507115688665, 0.00167959311601612, 0.001673642813062021, 0.0016398313184550336, 0.0017121783096718695, 0.0015988422940348007, 0.001594519417718515, 0.0015911657991277616, 0.001628381039499659, 0.0018790809982708952, 0.0016364881790481335, 0.0015928312096484872, 0.0017195717745655498, 0.001586340086231398, 0.001565510775287484, 0.013289717876157441, 0.0016533231629823992, 0.001668862238627418, 0.001748067480259279, 0.0029377631767284734, 0.001718814100333771, 0.0016596909836223421, 0.0015095328359842994, 0.0018499287426500589, 0.0017839046357589405, 0.0018528553408159072, 0.001742615317564833, 0.0016318717754857485, 0.0034699237999296926, 0.0016430920087407495, 0.0017932101546303016, 0.002414364634846994, 0.0016051839690568836, 0.001645315884316499, 0.054513595054330406, 0.0017001085116910611, 0.0016029819308296424, 0.0025667595576004004, 0.001581373915090695, 0.0016012992258652929, 0.0027648167135988096, 0.0015724634430407322, 0.0016190477668545967, 0.0030212452483558377, 0.0018233633798713138, 0.0015589158761842076]
[284.3499357052112, 512.4445408327379, 405.20868706433686, 518.7267553562492, 639.8668663773745, 433.9367026646566, 648.8480876183735, 633.0667455765741, 638.126197271288, 626.1351706249611, 640.6110420398886, 659.459375839774, 404.32556385986527, 652.8266510297358, 647.073722250794, 650.8466562519131, 586.8025598976426, 595.3822925709125, 597.49905547077, 609.8188202321625, 584.0513189257987, 625.4525563471451, 627.1482108576823, 628.4700189937313, 614.1068802344093, 532.1750371166471, 611.0646033396049, 627.8129119661613, 581.5401338816754, 630.3818510793976, 638.7691581467168, 75.2461421166856, 604.842430318413, 599.2106339600958, 572.0602958941132, 340.39503521642325, 581.7964838697874, 602.5218006652418, 662.4566065487038, 540.5613616054641, 560.5680819224746, 539.7075410968936, 573.850114778872, 612.793244556446, 288.19076661575735, 608.6086443609393, 557.6591217810529, 414.187644056248, 622.9815518202216, 607.7860242718209, 18.344047920584956, 588.1977492162083, 623.8373501081438, 389.5962896247575, 632.3615120100474, 624.4929016684129, 361.6876283630227, 635.9448319296135, 617.6470024369627, 330.98934968758334, 548.4370318277294, 641.4714323442018]
Elapsed: 0.3714506320369941~0.8748716233866065
Time per graph: 0.0028794622638526673~0.006781950568888423
Speed: 549.4315929357598~133.3220134676385
Total Time: 0.2016
best val loss: 0.4812815544852453 test_score: 0.8915

Testing...
Test loss: 0.5118 score: 0.8682 time: 0.19s
test Score 0.8682
Epoch Time List: [2.190362560097128, 0.8277718329336494, 0.9959230497479439, 0.8352038431912661, 0.8083715459797531, 0.9180753952823579, 0.7333630181383342, 0.7211382961831987, 0.719183289911598, 0.8908776296302676, 0.7754188100807369, 0.9483896209858358, 0.8350253158714622, 0.7172621320933104, 0.6919972721952945, 0.7151709068566561, 0.8771548387594521, 0.7656345712020993, 0.7649024929851294, 0.7577030952088535, 0.7726880260743201, 0.7472082569729537, 0.7402134756557643, 0.7341135973110795, 0.7384308811742812, 0.7796321238856763, 0.7469516820274293, 0.7232228149659932, 0.7554672309197485, 0.7143497259821743, 0.7235916599165648, 7.07531742984429, 0.8923592120409012, 0.7399845102336258, 0.8194852231536061, 0.9311576976906508, 0.8629599041305482, 0.9499473709147424, 0.8449818880762905, 0.7704760020133108, 0.8161804070696235, 0.8846712140366435, 0.7724515988957137, 0.7436412780079991, 0.9746753880754113, 0.7526762371417135, 0.8016985531430691, 0.8677595187909901, 0.7684910742100328, 0.7503109879326075, 7.564781199907884, 7.08489133277908, 0.7427216118667275, 0.8718355209566653, 0.7310087170917541, 0.7453365540131927, 0.9093361040577292, 0.9388082961086184, 0.7839764568489045, 0.9883124229963869, 0.8445991871412843, 0.7246466549113393]
Total Epoch List: [62]
Total Time List: [0.20163656305521727]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c72d0f580>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7472;  Loss pred: 0.7472; Loss self: 0.0000; time: 0.36s
Val loss: 0.7109 score: 0.5969 time: 0.48s
Test loss: 0.7419 score: 0.5349 time: 0.20s
Epoch 2/1000, LR 0.000015
Train loss: 0.7401;  Loss pred: 0.7401; Loss self: 0.0000; time: 0.35s
Val loss: 0.7811 score: 0.5039 time: 0.24s
Test loss: 0.7909 score: 0.5194 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.33s
Val loss: 0.7710 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7640 score: 0.5039 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.33s
Val loss: 0.7183 score: 0.5039 time: 0.22s
Test loss: 0.7253 score: 0.5194 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.32s
Val loss: 0.6524 score: 0.6202 time: 0.23s
Test loss: 0.6800 score: 0.5891 time: 0.22s
Epoch 6/1000, LR 0.000135
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 0.33s
Val loss: 0.6237 score: 0.6202 time: 0.23s
Test loss: 0.6633 score: 0.6202 time: 0.22s
Epoch 7/1000, LR 0.000165
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.31s
Val loss: 0.6114 score: 0.5736 time: 0.22s
Test loss: 0.6587 score: 0.5969 time: 0.20s
Epoch 8/1000, LR 0.000195
Train loss: 0.4971;  Loss pred: 0.4971; Loss self: 0.0000; time: 0.32s
Val loss: 0.6033 score: 0.5969 time: 0.21s
Test loss: 0.6513 score: 0.5736 time: 0.20s
Epoch 9/1000, LR 0.000225
Train loss: 0.4429;  Loss pred: 0.4429; Loss self: 0.0000; time: 0.31s
Val loss: 0.5979 score: 0.6822 time: 0.23s
Test loss: 0.6478 score: 0.5814 time: 0.24s
Epoch 10/1000, LR 0.000255
Train loss: 0.4475;  Loss pred: 0.4475; Loss self: 0.0000; time: 0.32s
Val loss: 0.5963 score: 0.7054 time: 0.23s
Test loss: 0.6561 score: 0.6434 time: 0.21s
Epoch 11/1000, LR 0.000285
Train loss: 0.4006;  Loss pred: 0.4006; Loss self: 0.0000; time: 0.32s
Val loss: 0.5936 score: 0.7209 time: 0.22s
Test loss: 0.6604 score: 0.6434 time: 0.20s
Epoch 12/1000, LR 0.000285
Train loss: 0.3890;  Loss pred: 0.3890; Loss self: 0.0000; time: 0.35s
Val loss: 0.5932 score: 0.7287 time: 0.22s
Test loss: 0.6669 score: 0.6512 time: 0.20s
Epoch 13/1000, LR 0.000285
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.32s
Val loss: 0.5961 score: 0.7209 time: 0.22s
Test loss: 0.6764 score: 0.6667 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.3587;  Loss pred: 0.3587; Loss self: 0.0000; time: 0.33s
Val loss: 0.5910 score: 0.7209 time: 0.22s
Test loss: 0.6711 score: 0.6512 time: 0.20s
Epoch 15/1000, LR 0.000285
Train loss: 0.3321;  Loss pred: 0.3321; Loss self: 0.0000; time: 0.34s
Val loss: 0.5902 score: 0.6977 time: 0.23s
Test loss: 0.6717 score: 0.6202 time: 0.34s
Epoch 16/1000, LR 0.000285
Train loss: 0.2871;  Loss pred: 0.2871; Loss self: 0.0000; time: 0.35s
Val loss: 0.5797 score: 0.6822 time: 0.25s
Test loss: 0.6553 score: 0.6124 time: 0.36s
Epoch 17/1000, LR 0.000285
Train loss: 0.2822;  Loss pred: 0.2822; Loss self: 0.0000; time: 0.34s
Val loss: 0.5632 score: 0.6899 time: 0.26s
Test loss: 0.6309 score: 0.6202 time: 0.25s
Epoch 18/1000, LR 0.000285
Train loss: 0.2561;  Loss pred: 0.2561; Loss self: 0.0000; time: 0.35s
Val loss: 0.5631 score: 0.6977 time: 0.41s
Test loss: 0.6328 score: 0.6124 time: 0.20s
Epoch 19/1000, LR 0.000285
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.32s
Val loss: 0.5604 score: 0.7054 time: 0.22s
Test loss: 0.6220 score: 0.6047 time: 0.21s
Epoch 20/1000, LR 0.000285
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 0.34s
Val loss: 0.5582 score: 0.7054 time: 0.25s
Test loss: 0.6230 score: 0.6047 time: 0.25s
Epoch 21/1000, LR 0.000285
Train loss: 0.2197;  Loss pred: 0.2197; Loss self: 0.0000; time: 0.32s
Val loss: 0.5353 score: 0.7132 time: 0.37s
Test loss: 0.5944 score: 0.5814 time: 0.27s
Epoch 22/1000, LR 0.000285
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.38s
Val loss: 0.5233 score: 0.7287 time: 0.27s
Test loss: 0.5490 score: 0.6434 time: 0.28s
Epoch 23/1000, LR 0.000285
Train loss: 0.1904;  Loss pred: 0.1904; Loss self: 0.0000; time: 0.35s
Val loss: 0.5222 score: 0.7442 time: 0.24s
Test loss: 0.5319 score: 0.6589 time: 0.22s
Epoch 24/1000, LR 0.000285
Train loss: 0.1930;  Loss pred: 0.1930; Loss self: 0.0000; time: 0.32s
Val loss: 0.5167 score: 0.7364 time: 0.38s
Test loss: 0.5237 score: 0.6512 time: 0.20s
Epoch 25/1000, LR 0.000285
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.33s
Val loss: 0.5156 score: 0.7442 time: 0.24s
Test loss: 0.5160 score: 0.6512 time: 0.21s
Epoch 26/1000, LR 0.000285
Train loss: 0.1408;  Loss pred: 0.1408; Loss self: 0.0000; time: 0.34s
Val loss: 0.4893 score: 0.7597 time: 0.24s
Test loss: 0.4939 score: 0.6822 time: 0.23s
Epoch 27/1000, LR 0.000285
Train loss: 0.1516;  Loss pred: 0.1516; Loss self: 0.0000; time: 0.36s
Val loss: 0.4855 score: 0.7597 time: 0.27s
Test loss: 0.4745 score: 0.6977 time: 0.21s
Epoch 28/1000, LR 0.000285
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.32s
Val loss: 0.4602 score: 0.7829 time: 0.23s
Test loss: 0.4442 score: 0.7519 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 0.32s
Val loss: 0.4232 score: 0.8140 time: 0.25s
Test loss: 0.4141 score: 0.7674 time: 0.37s
Epoch 30/1000, LR 0.000285
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.35s
Val loss: 0.3975 score: 0.8372 time: 0.30s
Test loss: 0.3938 score: 0.8062 time: 0.23s
Epoch 31/1000, LR 0.000285
Train loss: 0.1176;  Loss pred: 0.1176; Loss self: 0.0000; time: 0.33s
Val loss: 0.3821 score: 0.8605 time: 0.24s
Test loss: 0.3879 score: 0.8527 time: 0.22s
Epoch 32/1000, LR 0.000285
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.33s
Val loss: 0.3772 score: 0.8760 time: 0.26s
Test loss: 0.3872 score: 0.8682 time: 0.22s
Epoch 33/1000, LR 0.000285
Train loss: 0.0714;  Loss pred: 0.0714; Loss self: 0.0000; time: 0.32s
Val loss: 0.4018 score: 0.8450 time: 0.35s
Test loss: 0.3717 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.30s
Val loss: 0.4603 score: 0.7752 time: 0.23s
Test loss: 0.4070 score: 0.7907 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.30s
Val loss: 0.4457 score: 0.7829 time: 0.22s
Test loss: 0.3811 score: 0.8062 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.31s
Val loss: 0.4167 score: 0.8372 time: 0.21s
Test loss: 0.3480 score: 0.8527 time: 0.36s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.30s
Val loss: 0.4062 score: 0.8450 time: 0.21s
Test loss: 0.3398 score: 0.8605 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.33s
Val loss: 0.3943 score: 0.8450 time: 0.30s
Test loss: 0.3336 score: 0.8605 time: 0.30s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.43s
Val loss: 0.3770 score: 0.8682 time: 0.35s
Test loss: 0.3129 score: 0.8837 time: 0.28s
Epoch 40/1000, LR 0.000284
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.35s
Val loss: 0.3548 score: 0.8682 time: 0.27s
Test loss: 0.2921 score: 0.9070 time: 0.25s
Epoch 41/1000, LR 0.000284
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.34s
Val loss: 0.3465 score: 0.8915 time: 0.24s
Test loss: 0.2850 score: 0.8992 time: 0.22s
Epoch 42/1000, LR 0.000284
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.44s
Val loss: 0.3522 score: 0.8992 time: 0.22s
Test loss: 0.2908 score: 0.9070 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.33s
Val loss: 0.3699 score: 0.8760 time: 0.27s
Test loss: 0.2980 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.34s
Val loss: 0.3708 score: 0.8915 time: 0.27s
Test loss: 0.3083 score: 0.8992 time: 0.27s
     INFO: Early stopping counter 3 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.35s
Val loss: 0.3804 score: 0.8915 time: 0.28s
Test loss: 0.3288 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.36s
Val loss: 0.4010 score: 0.8915 time: 0.25s
Test loss: 0.3595 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.34s
Val loss: 0.4083 score: 0.8915 time: 0.24s
Test loss: 0.3642 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.44s
Val loss: 0.4189 score: 0.8992 time: 0.28s
Test loss: 0.3636 score: 0.9147 time: 0.24s
     INFO: Early stopping counter 7 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.34s
Val loss: 0.4342 score: 0.9070 time: 0.27s
Test loss: 0.3672 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.35s
Val loss: 0.4696 score: 0.8992 time: 0.24s
Test loss: 0.4012 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.39s
Val loss: 0.4943 score: 0.8992 time: 0.24s
Test loss: 0.4202 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 10 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.37s
Val loss: 0.4849 score: 0.9070 time: 0.24s
Test loss: 0.4007 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 11 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.34s
Val loss: 0.4913 score: 0.9070 time: 0.25s
Test loss: 0.3884 score: 0.9070 time: 0.25s
     INFO: Early stopping counter 12 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.34s
Val loss: 0.5061 score: 0.9070 time: 0.38s
Test loss: 0.3997 score: 0.8992 time: 0.25s
     INFO: Early stopping counter 13 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.36s
Val loss: 0.5123 score: 0.9070 time: 0.26s
Test loss: 0.4199 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.40s
Val loss: 0.5317 score: 0.8992 time: 0.26s
Test loss: 0.4650 score: 0.9147 time: 0.24s
     INFO: Early stopping counter 15 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.39s
Val loss: 0.5650 score: 0.8992 time: 0.26s
Test loss: 0.5093 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 16 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.33s
Val loss: 0.5684 score: 0.8992 time: 0.24s
Test loss: 0.5085 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 17 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.34s
Val loss: 0.5568 score: 0.9070 time: 0.24s
Test loss: 0.4866 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.37s
Val loss: 0.5456 score: 0.8915 time: 0.30s
Test loss: 0.4642 score: 0.9147 time: 0.27s
     INFO: Early stopping counter 19 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.32s
Val loss: 0.5519 score: 0.8992 time: 0.28s
Test loss: 0.4739 score: 0.9147 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 040,   Train_Loss: 0.0243,   Val_Loss: 0.3465,   Val_Precision: 0.9048,   Val_Recall: 0.8769,   Val_accuracy: 0.8906,   Val_Score: 0.8915,   Val_Loss: 0.3465,   Test_Precision: 0.9474,   Test_Recall: 0.8438,   Test_accuracy: 0.8926,   Test_Score: 0.8992,   Test_loss: 0.2850


[0.45366635895334184, 0.25173455802723765, 0.31835447787307203, 0.248685842147097, 0.20160443801432848, 0.2972783800214529, 0.19881387101486325, 0.20376998302526772, 0.20215437095612288, 0.2060258009005338, 0.20137024112045765, 0.19561477890238166, 0.31904982402920723, 0.19760222686454654, 0.19935904606245458, 0.19820336904376745, 0.2198354417923838, 0.21666751196607947, 0.2158999228850007, 0.21153824008069932, 0.22087100194767118, 0.2062506559304893, 0.20569300488568842, 0.20526038808748126, 0.210061154095456, 0.24240144877694547, 0.21110697509720922, 0.20547522604465485, 0.22182475891895592, 0.20463787112385035, 0.20195089001208544, 1.71437360602431, 0.2132786880247295, 0.21528322878293693, 0.22550070495344698, 0.37897144979797304, 0.22172701894305646, 0.21410013688728213, 0.19472973584197462, 0.2386408078018576, 0.23012369801290333, 0.23901833896525204, 0.22479737596586347, 0.21051145903766155, 0.44762017019093037, 0.21195886912755668, 0.2313241099473089, 0.31145303789526224, 0.20706873200833797, 0.21224574907682836, 7.032253762008622, 0.21931399800814688, 0.20678466907702386, 0.33111198293045163, 0.20399723504669964, 0.2065676001366228, 0.3566613560542464, 0.20284778415225446, 0.20885716192424297, 0.38974063703790307, 0.2352138760033995, 0.20110014802776277, 0.208246692083776, 0.23920494108460844, 0.2244901219382882, 0.2209848330821842, 0.22646416001953185, 0.22836652491241693, 0.20742149092257023, 0.20379464398138225, 0.2496224760543555, 0.22104125889018178, 0.20880411309190094, 0.20662094606086612, 0.21351027698256075, 0.20856478181667626, 0.34465988306328654, 0.36617890396155417, 0.25581654999405146, 0.20830676308833063, 0.21291360212489963, 0.2558503688778728, 0.27492882893420756, 0.286354789044708, 0.22953562019392848, 0.2072939919307828, 0.21839440893381834, 0.23511599306948483, 0.21755900303833187, 0.22090655704960227, 0.3799701030366123, 0.23390174191445112, 0.22652936610393226, 0.22545030200853944, 0.22676420398056507, 0.22391282906755805, 0.21832899400033057, 0.371579437982291, 0.21112651284784079, 0.3110726401209831, 0.2866329150274396, 0.2590787981171161, 0.22905175294727087, 0.20899934601038694, 0.24012851202860475, 0.274814440170303, 0.21698856190778315, 0.22451204690150917, 0.22430946119129658, 0.24474525498226285, 0.2285885598976165, 0.22223990899510682, 0.23661566013470292, 0.23862428101710975, 0.2525885908398777, 0.25440792785957456, 0.21695948694832623, 0.2470103669911623, 0.2330256060231477, 0.23987405700609088, 0.22816055896691978, 0.2728702079039067, 0.2062583970837295]
[0.003516793480258464, 0.0019514306823816873, 0.002467864169558698, 0.0019277972259464883, 0.0015628251008862673, 0.0023044835660577744, 0.0015411927985648315, 0.0015796122715137032, 0.0015670881469466889, 0.0015970992317870837, 0.001561009621088819, 0.0015163936349021833, 0.0024732544498388158, 0.0015318002082522987, 0.0015454189617244542, 0.0015364602251454842, 0.0017041507115688665, 0.00167959311601612, 0.001673642813062021, 0.0016398313184550336, 0.0017121783096718695, 0.0015988422940348007, 0.001594519417718515, 0.0015911657991277616, 0.001628381039499659, 0.0018790809982708952, 0.0016364881790481335, 0.0015928312096484872, 0.0017195717745655498, 0.001586340086231398, 0.001565510775287484, 0.013289717876157441, 0.0016533231629823992, 0.001668862238627418, 0.001748067480259279, 0.0029377631767284734, 0.001718814100333771, 0.0016596909836223421, 0.0015095328359842994, 0.0018499287426500589, 0.0017839046357589405, 0.0018528553408159072, 0.001742615317564833, 0.0016318717754857485, 0.0034699237999296926, 0.0016430920087407495, 0.0017932101546303016, 0.002414364634846994, 0.0016051839690568836, 0.001645315884316499, 0.054513595054330406, 0.0017001085116910611, 0.0016029819308296424, 0.0025667595576004004, 0.001581373915090695, 0.0016012992258652929, 0.0027648167135988096, 0.0015724634430407322, 0.0016190477668545967, 0.0030212452483558377, 0.0018233633798713138, 0.0015589158761842076, 0.0016143154425098914, 0.0018543018688729337, 0.001740233503397583, 0.0017130607215673193, 0.0017555361241824174, 0.0017702831388559453, 0.0016079185342834901, 0.0015798034417161414, 0.0019350579539097327, 0.0017134981309316418, 0.0016186365355961314, 0.001601712760161753, 0.0016551184262214012, 0.0016167812543928391, 0.0026717820392502833, 0.00283859615474073, 0.001983074030961639, 0.0016147811092118653, 0.0016504930397279042, 0.0019833361928517272, 0.0021312312320481207, 0.0022198045662380466, 0.0017793458929761898, 0.0016069301700060682, 0.001692979914215646, 0.001822604597437867, 0.0016865038995219524, 0.001712453930617072, 0.0029455046747024205, 0.0018131917977864428, 0.0017560415977049013, 0.0017476767597561197, 0.0017578620463609695, 0.0017357583648647912, 0.001692472821707989, 0.0028804607595526435, 0.001636639634479386, 0.002411415814891342, 0.002221960581608059, 0.002008362776101675, 0.0017755949840873712, 0.0016201499690727671, 0.0018614613335550755, 0.0021303444974442092, 0.001682081875254133, 0.0017404034643527843, 0.0017388330324906711, 0.0018972500386221925, 0.0017720043402916009, 0.0017227899922101304, 0.0018342299235248288, 0.0018498006280396105, 0.001958051091781998, 0.0019721544795315857, 0.001681856487971521, 0.0019148090464431186, 0.0018064000466910674, 0.0018594888140007046, 0.0017686865036195332, 0.002115272929487649, 0.0015989023029746472]
[284.3499357052112, 512.4445408327379, 405.20868706433686, 518.7267553562492, 639.8668663773745, 433.9367026646566, 648.8480876183735, 633.0667455765741, 638.126197271288, 626.1351706249611, 640.6110420398886, 659.459375839774, 404.32556385986527, 652.8266510297358, 647.073722250794, 650.8466562519131, 586.8025598976426, 595.3822925709125, 597.49905547077, 609.8188202321625, 584.0513189257987, 625.4525563471451, 627.1482108576823, 628.4700189937313, 614.1068802344093, 532.1750371166471, 611.0646033396049, 627.8129119661613, 581.5401338816754, 630.3818510793976, 638.7691581467168, 75.2461421166856, 604.842430318413, 599.2106339600958, 572.0602958941132, 340.39503521642325, 581.7964838697874, 602.5218006652418, 662.4566065487038, 540.5613616054641, 560.5680819224746, 539.7075410968936, 573.850114778872, 612.793244556446, 288.19076661575735, 608.6086443609393, 557.6591217810529, 414.187644056248, 622.9815518202216, 607.7860242718209, 18.344047920584956, 588.1977492162083, 623.8373501081438, 389.5962896247575, 632.3615120100474, 624.4929016684129, 361.6876283630227, 635.9448319296135, 617.6470024369627, 330.98934968758334, 548.4370318277294, 641.4714323442018, 619.4576187942727, 539.2865189786017, 574.6355291101039, 583.750469209916, 569.6265580782149, 564.8813899036825, 621.9220555509133, 632.9901388958233, 516.7803878842631, 583.601453627669, 617.8039220100192, 624.3316684940517, 604.1863737104165, 618.512861454184, 374.2820279908033, 352.2868155549014, 504.2676089682222, 619.2789810924128, 605.8795620033983, 504.20095372845304, 469.2123430637775, 450.49010854803436, 562.0042758113593, 622.3045771778768, 590.6744619963774, 548.6653558351348, 592.9425957944449, 583.9573153595182, 339.5003948180895, 551.5136353588225, 569.4625920632933, 572.1881889300545, 568.8728544257189, 576.1170565223784, 590.8514377151606, 347.16668042903916, 611.0080551227145, 414.69413687371866, 450.0529884631384, 497.9180115760989, 563.1914986029232, 617.2268117699704, 537.2123406346398, 469.4076480117219, 594.501382311677, 574.5794124650728, 575.0983454504652, 527.0786557612685, 564.3327035166517, 580.4538014045005, 545.1879217400979, 540.598800131117, 510.7119033803722, 507.05967021280907, 594.5810520409474, 522.2452869948388, 553.5872310409773, 537.7822079222366, 565.3913217257821, 472.75223261246725, 625.4290822769904]
Elapsed: 0.3068781507360499~0.6251722022659598
Time per graph: 0.0023789003933027126~0.004846296141596588
Speed: 547.8433173410546~106.97822551778354
Total Time: 0.2069
best val loss: 0.34648893577183865 test_score: 0.8992

Testing...
Test loss: 0.3672 score: 0.9147 time: 0.21s
test Score 0.9147
Epoch Time List: [2.190362560097128, 0.8277718329336494, 0.9959230497479439, 0.8352038431912661, 0.8083715459797531, 0.9180753952823579, 0.7333630181383342, 0.7211382961831987, 0.719183289911598, 0.8908776296302676, 0.7754188100807369, 0.9483896209858358, 0.8350253158714622, 0.7172621320933104, 0.6919972721952945, 0.7151709068566561, 0.8771548387594521, 0.7656345712020993, 0.7649024929851294, 0.7577030952088535, 0.7726880260743201, 0.7472082569729537, 0.7402134756557643, 0.7341135973110795, 0.7384308811742812, 0.7796321238856763, 0.7469516820274293, 0.7232228149659932, 0.7554672309197485, 0.7143497259821743, 0.7235916599165648, 7.07531742984429, 0.8923592120409012, 0.7399845102336258, 0.8194852231536061, 0.9311576976906508, 0.8629599041305482, 0.9499473709147424, 0.8449818880762905, 0.7704760020133108, 0.8161804070696235, 0.8846712140366435, 0.7724515988957137, 0.7436412780079991, 0.9746753880754113, 0.7526762371417135, 0.8016985531430691, 0.8677595187909901, 0.7684910742100328, 0.7503109879326075, 7.564781199907884, 7.08489133277908, 0.7427216118667275, 0.8718355209566653, 0.7310087170917541, 0.7453365540131927, 0.9093361040577292, 0.9388082961086184, 0.7839764568489045, 0.9883124229963869, 0.8445991871412843, 0.7246466549113393, 1.0443999702110887, 0.8269910020753741, 0.7893095628824085, 0.759718107059598, 0.7735355570912361, 0.7869965101126581, 0.7340571158565581, 0.7324064907152206, 0.7818813943304121, 0.7688171660993248, 0.7420533860567957, 0.771168006118387, 0.7441481400746852, 0.761083303950727, 0.9050499140284956, 0.9574207419063896, 0.8463287791237235, 0.9615001920610666, 0.7488808687776327, 0.8380481451749802, 0.9650101361330599, 0.9292671410366893, 0.8079963880591094, 0.9022342970129102, 0.7799761909991503, 0.8156159960199147, 0.8482574170920998, 0.7622842427808791, 0.9431531731970608, 0.8805694440379739, 0.7857148952316493, 0.8103345099370927, 0.8944694099482149, 0.7485730391927063, 0.7282313990872353, 0.889157489174977, 0.7133336400147527, 0.928022351115942, 1.0613085792865604, 0.8755582219455391, 0.7965074318926781, 0.8627374120987952, 0.8336192730348557, 0.8775385990738869, 0.8417307659983635, 0.8324214688036591, 0.8030315558426082, 0.954708623001352, 0.8366348368581384, 0.8004741459153593, 0.8593834559433162, 0.8385932750534266, 0.8435565556865185, 0.9700685301795602, 0.8275838808622211, 0.8938181740231812, 0.8783031220082194, 0.8033116240985692, 0.8067833709064871, 0.9375000521540642, 0.7994015377480537]
Total Epoch List: [62, 61]
Total Time List: [0.20163656305521727, 0.20692215813323855]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c757c8d90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.2361;  Loss pred: 1.2361; Loss self: 0.0000; time: 0.37s
Val loss: 1.3704 score: 0.2403 time: 0.20s
Test loss: 1.4648 score: 0.3516 time: 0.19s
Epoch 2/1000, LR 0.000020
Train loss: 1.2060;  Loss pred: 1.2060; Loss self: 0.0000; time: 0.37s
Val loss: 1.0351 score: 0.2868 time: 0.19s
Test loss: 1.1173 score: 0.3047 time: 0.20s
Epoch 3/1000, LR 0.000050
Train loss: 0.9730;  Loss pred: 0.9730; Loss self: 0.0000; time: 0.49s
Val loss: 0.9636 score: 0.3178 time: 0.19s
Test loss: 1.0356 score: 0.3750 time: 0.21s
Epoch 4/1000, LR 0.000080
Train loss: 0.7900;  Loss pred: 0.7900; Loss self: 0.0000; time: 0.38s
Val loss: 0.8786 score: 0.3256 time: 0.20s
Test loss: 0.9382 score: 0.3906 time: 0.21s
Epoch 5/1000, LR 0.000110
Train loss: 0.7043;  Loss pred: 0.7043; Loss self: 0.0000; time: 0.38s
Val loss: 0.8086 score: 0.3876 time: 0.22s
Test loss: 0.8511 score: 0.4141 time: 0.21s
Epoch 6/1000, LR 0.000140
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.38s
Val loss: 0.7088 score: 0.4341 time: 0.20s
Test loss: 0.7337 score: 0.4453 time: 0.21s
Epoch 7/1000, LR 0.000170
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.37s
Val loss: 0.6514 score: 0.5271 time: 0.19s
Test loss: 0.6714 score: 0.4844 time: 0.20s
Epoch 8/1000, LR 0.000200
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.37s
Val loss: 0.6140 score: 0.5426 time: 0.19s
Test loss: 0.6405 score: 0.5938 time: 0.20s
Epoch 9/1000, LR 0.000230
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.38s
Val loss: 0.5931 score: 0.5659 time: 0.18s
Test loss: 0.6390 score: 0.5703 time: 0.20s
Epoch 10/1000, LR 0.000260
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 0.39s
Val loss: 0.5812 score: 0.5581 time: 0.18s
Test loss: 0.6384 score: 0.5781 time: 0.19s
Epoch 11/1000, LR 0.000290
Train loss: 0.4704;  Loss pred: 0.4704; Loss self: 0.0000; time: 0.37s
Val loss: 0.5683 score: 0.6434 time: 0.18s
Test loss: 0.6327 score: 0.6172 time: 0.19s
Epoch 12/1000, LR 0.000290
Train loss: 0.4421;  Loss pred: 0.4421; Loss self: 0.0000; time: 0.38s
Val loss: 0.5590 score: 0.6589 time: 0.19s
Test loss: 0.6179 score: 0.6250 time: 0.20s
Epoch 13/1000, LR 0.000290
Train loss: 0.3901;  Loss pred: 0.3901; Loss self: 0.0000; time: 0.39s
Val loss: 0.5581 score: 0.6822 time: 0.56s
Test loss: 0.6126 score: 0.6719 time: 0.22s
Epoch 14/1000, LR 0.000290
Train loss: 0.3791;  Loss pred: 0.3791; Loss self: 0.0000; time: 0.37s
Val loss: 0.5460 score: 0.7054 time: 0.22s
Test loss: 0.5857 score: 0.7266 time: 0.23s
Epoch 15/1000, LR 0.000290
Train loss: 0.3491;  Loss pred: 0.3491; Loss self: 0.0000; time: 0.42s
Val loss: 0.5349 score: 0.7519 time: 0.21s
Test loss: 0.5788 score: 0.7812 time: 0.22s
Epoch 16/1000, LR 0.000290
Train loss: 0.3181;  Loss pred: 0.3181; Loss self: 0.0000; time: 0.37s
Val loss: 0.5160 score: 0.7984 time: 0.33s
Test loss: 0.5592 score: 0.7969 time: 0.20s
Epoch 17/1000, LR 0.000290
Train loss: 0.2794;  Loss pred: 0.2794; Loss self: 0.0000; time: 0.35s
Val loss: 0.4963 score: 0.7674 time: 0.17s
Test loss: 0.5619 score: 0.7344 time: 0.18s
Epoch 18/1000, LR 0.000290
Train loss: 0.2826;  Loss pred: 0.2826; Loss self: 0.0000; time: 0.35s
Val loss: 0.4966 score: 0.7519 time: 0.18s
Test loss: 0.5941 score: 0.7188 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000290
Train loss: 0.2687;  Loss pred: 0.2687; Loss self: 0.0000; time: 0.39s
Val loss: 0.4767 score: 0.7597 time: 0.29s
Test loss: 0.5804 score: 0.7031 time: 0.21s
Epoch 20/1000, LR 0.000290
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.37s
Val loss: 0.4544 score: 0.7752 time: 0.20s
Test loss: 0.5318 score: 0.7188 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 0.2263;  Loss pred: 0.2263; Loss self: 0.0000; time: 0.38s
Val loss: 0.4501 score: 0.7752 time: 0.20s
Test loss: 0.5238 score: 0.7188 time: 0.20s
Epoch 22/1000, LR 0.000290
Train loss: 0.2092;  Loss pred: 0.2092; Loss self: 0.0000; time: 0.37s
Val loss: 0.4166 score: 0.7984 time: 0.33s
Test loss: 0.4695 score: 0.7344 time: 0.20s
Epoch 23/1000, LR 0.000290
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.37s
Val loss: 0.3574 score: 0.9070 time: 0.19s
Test loss: 0.3859 score: 0.8828 time: 0.23s
Epoch 24/1000, LR 0.000290
Train loss: 0.1875;  Loss pred: 0.1875; Loss self: 0.0000; time: 0.39s
Val loss: 0.3478 score: 0.9225 time: 0.22s
Test loss: 0.3720 score: 0.8984 time: 0.20s
Epoch 25/1000, LR 0.000290
Train loss: 0.1522;  Loss pred: 0.1522; Loss self: 0.0000; time: 0.38s
Val loss: 0.3432 score: 0.8837 time: 0.35s
Test loss: 0.3985 score: 0.8516 time: 0.20s
Epoch 26/1000, LR 0.000290
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 0.42s
Val loss: 0.3254 score: 0.9147 time: 0.20s
Test loss: 0.3537 score: 0.8984 time: 0.20s
Epoch 27/1000, LR 0.000290
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 0.38s
Val loss: 0.5259 score: 0.8217 time: 0.20s
Test loss: 0.4714 score: 0.7734 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000290
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.39s
Val loss: 0.3484 score: 0.8760 time: 0.34s
Test loss: 0.4249 score: 0.8203 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.38s
Val loss: 0.4833 score: 0.7829 time: 6.48s
Test loss: 0.6756 score: 0.7031 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.36s
Val loss: 0.4663 score: 0.8140 time: 0.19s
Test loss: 0.6590 score: 0.7266 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000290
Train loss: 0.0864;  Loss pred: 0.0864; Loss self: 0.0000; time: 0.38s
Val loss: 0.3612 score: 0.8760 time: 0.20s
Test loss: 0.4962 score: 0.7891 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000290
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.35s
Val loss: 0.5963 score: 0.7054 time: 0.18s
Test loss: 0.8640 score: 0.6562 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.36s
Val loss: 0.4578 score: 0.7907 time: 0.18s
Test loss: 0.7086 score: 0.7109 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.36s
Val loss: 0.3913 score: 0.8837 time: 0.34s
Test loss: 0.6059 score: 0.7656 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.34s
Val loss: 0.3800 score: 0.8992 time: 0.18s
Test loss: 0.5686 score: 0.8047 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.35s
Val loss: 0.3930 score: 0.8992 time: 0.19s
Test loss: 0.5920 score: 0.8125 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.35s
Val loss: 0.3202 score: 0.8992 time: 0.18s
Test loss: 0.4521 score: 0.8516 time: 0.33s
Epoch 38/1000, LR 0.000289
Train loss: 0.0459;  Loss pred: 0.0459; Loss self: 0.0000; time: 0.36s
Val loss: 0.3160 score: 0.8992 time: 0.18s
Test loss: 0.4363 score: 0.8672 time: 0.19s
Epoch 39/1000, LR 0.000289
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.39s
Val loss: 0.3413 score: 0.8992 time: 0.21s
Test loss: 0.4388 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0302;  Loss pred: 0.0302; Loss self: 0.0000; time: 0.37s
Val loss: 0.3223 score: 0.8992 time: 0.18s
Test loss: 0.4399 score: 0.8594 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.42s
Val loss: 0.3364 score: 0.8992 time: 0.20s
Test loss: 0.4931 score: 0.8281 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.37s
Val loss: 0.3233 score: 0.9070 time: 0.19s
Test loss: 0.5026 score: 0.8281 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.35s
Val loss: 0.3759 score: 0.9147 time: 0.24s
Test loss: 0.5325 score: 0.8359 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.50s
Val loss: 0.3251 score: 0.8992 time: 0.19s
Test loss: 0.4162 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.36s
Val loss: 0.3163 score: 0.8992 time: 0.20s
Test loss: 0.4447 score: 0.8516 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.37s
Val loss: 0.3380 score: 0.8915 time: 0.19s
Test loss: 0.4534 score: 0.8672 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.44s
Val loss: 0.3469 score: 0.8992 time: 0.23s
Test loss: 0.4970 score: 0.8359 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.36s
Val loss: 0.3678 score: 0.8992 time: 0.19s
Test loss: 0.5176 score: 0.8438 time: 0.36s
     INFO: Early stopping counter 10 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.37s
Val loss: 0.4480 score: 0.8760 time: 0.19s
Test loss: 0.5538 score: 0.8672 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.37s
Val loss: 0.5015 score: 0.8682 time: 0.17s
Test loss: 0.5824 score: 0.8750 time: 0.29s
     INFO: Early stopping counter 12 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.37s
Val loss: 0.5373 score: 0.8605 time: 0.18s
Test loss: 0.5965 score: 0.8672 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.36s
Val loss: 0.5378 score: 0.8837 time: 0.18s
Test loss: 0.6604 score: 0.8594 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.36s
Val loss: 0.4778 score: 0.8992 time: 0.18s
Test loss: 0.6676 score: 0.8594 time: 0.19s
     INFO: Early stopping counter 15 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.37s
Val loss: 0.5193 score: 0.8837 time: 0.19s
Test loss: 0.6351 score: 0.8672 time: 0.32s
     INFO: Early stopping counter 16 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.38s
Val loss: 0.5342 score: 0.8682 time: 0.19s
Test loss: 0.6186 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.37s
Val loss: 0.5380 score: 0.8605 time: 0.19s
Test loss: 0.6372 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 18 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.38s
Val loss: 0.4670 score: 0.8915 time: 0.19s
Test loss: 0.6138 score: 0.8672 time: 0.33s
     INFO: Early stopping counter 19 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.36s
Val loss: 0.5268 score: 0.8682 time: 0.19s
Test loss: 0.6358 score: 0.8672 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 037,   Train_Loss: 0.0459,   Val_Loss: 0.3160,   Val_Precision: 0.9333,   Val_Recall: 0.8615,   Val_accuracy: 0.8960,   Val_Score: 0.8992,   Val_Loss: 0.3160,   Test_Precision: 0.9123,   Test_Recall: 0.8125,   Test_accuracy: 0.8595,   Test_Score: 0.8672,   Test_loss: 0.4363


[0.45366635895334184, 0.25173455802723765, 0.31835447787307203, 0.248685842147097, 0.20160443801432848, 0.2972783800214529, 0.19881387101486325, 0.20376998302526772, 0.20215437095612288, 0.2060258009005338, 0.20137024112045765, 0.19561477890238166, 0.31904982402920723, 0.19760222686454654, 0.19935904606245458, 0.19820336904376745, 0.2198354417923838, 0.21666751196607947, 0.2158999228850007, 0.21153824008069932, 0.22087100194767118, 0.2062506559304893, 0.20569300488568842, 0.20526038808748126, 0.210061154095456, 0.24240144877694547, 0.21110697509720922, 0.20547522604465485, 0.22182475891895592, 0.20463787112385035, 0.20195089001208544, 1.71437360602431, 0.2132786880247295, 0.21528322878293693, 0.22550070495344698, 0.37897144979797304, 0.22172701894305646, 0.21410013688728213, 0.19472973584197462, 0.2386408078018576, 0.23012369801290333, 0.23901833896525204, 0.22479737596586347, 0.21051145903766155, 0.44762017019093037, 0.21195886912755668, 0.2313241099473089, 0.31145303789526224, 0.20706873200833797, 0.21224574907682836, 7.032253762008622, 0.21931399800814688, 0.20678466907702386, 0.33111198293045163, 0.20399723504669964, 0.2065676001366228, 0.3566613560542464, 0.20284778415225446, 0.20885716192424297, 0.38974063703790307, 0.2352138760033995, 0.20110014802776277, 0.208246692083776, 0.23920494108460844, 0.2244901219382882, 0.2209848330821842, 0.22646416001953185, 0.22836652491241693, 0.20742149092257023, 0.20379464398138225, 0.2496224760543555, 0.22104125889018178, 0.20880411309190094, 0.20662094606086612, 0.21351027698256075, 0.20856478181667626, 0.34465988306328654, 0.36617890396155417, 0.25581654999405146, 0.20830676308833063, 0.21291360212489963, 0.2558503688778728, 0.27492882893420756, 0.286354789044708, 0.22953562019392848, 0.2072939919307828, 0.21839440893381834, 0.23511599306948483, 0.21755900303833187, 0.22090655704960227, 0.3799701030366123, 0.23390174191445112, 0.22652936610393226, 0.22545030200853944, 0.22676420398056507, 0.22391282906755805, 0.21832899400033057, 0.371579437982291, 0.21112651284784079, 0.3110726401209831, 0.2866329150274396, 0.2590787981171161, 0.22905175294727087, 0.20899934601038694, 0.24012851202860475, 0.274814440170303, 0.21698856190778315, 0.22451204690150917, 0.22430946119129658, 0.24474525498226285, 0.2285885598976165, 0.22223990899510682, 0.23661566013470292, 0.23862428101710975, 0.2525885908398777, 0.25440792785957456, 0.21695948694832623, 0.2470103669911623, 0.2330256060231477, 0.23987405700609088, 0.22816055896691978, 0.2728702079039067, 0.2062583970837295, 0.2003201839979738, 0.2052352719474584, 0.21706391498446465, 0.21645489311777055, 0.21190987806767225, 0.2119001590181142, 0.2055032269563526, 0.20540215214714408, 0.20292954798787832, 0.19323081197217107, 0.1967317759990692, 0.2091068511363119, 0.22304474306292832, 0.23761464608833194, 0.22672728495672345, 0.20306056318804622, 0.18943372298963368, 0.21065394883044064, 0.21819035103544593, 0.21194901480339468, 0.20605896995402873, 0.20768760703504086, 0.23446177295409143, 0.20704857702367008, 0.20597687386907637, 0.20716158114373684, 0.20833743503317237, 0.21163961105048656, 0.22717512608505785, 0.231052900897339, 0.19688051799312234, 0.19727665488608181, 0.20283982204273343, 0.19165233802050352, 0.1971057818736881, 0.1968812821432948, 0.33247471507638693, 0.1982498769648373, 0.19720486691221595, 0.19974337588064373, 0.19589291489683092, 0.19933210499584675, 0.19667443097569048, 0.20645155990496278, 0.2040870760101825, 0.19832422002218664, 0.19795245002023876, 0.36124031990766525, 0.20031593996100128, 0.2903948030434549, 0.1913903469685465, 0.19026139099150896, 0.19296579086221755, 0.321098105981946, 0.20574795990251005, 0.20876428484916687, 0.3406008069869131, 0.19930853694677353]
[0.003516793480258464, 0.0019514306823816873, 0.002467864169558698, 0.0019277972259464883, 0.0015628251008862673, 0.0023044835660577744, 0.0015411927985648315, 0.0015796122715137032, 0.0015670881469466889, 0.0015970992317870837, 0.001561009621088819, 0.0015163936349021833, 0.0024732544498388158, 0.0015318002082522987, 0.0015454189617244542, 0.0015364602251454842, 0.0017041507115688665, 0.00167959311601612, 0.001673642813062021, 0.0016398313184550336, 0.0017121783096718695, 0.0015988422940348007, 0.001594519417718515, 0.0015911657991277616, 0.001628381039499659, 0.0018790809982708952, 0.0016364881790481335, 0.0015928312096484872, 0.0017195717745655498, 0.001586340086231398, 0.001565510775287484, 0.013289717876157441, 0.0016533231629823992, 0.001668862238627418, 0.001748067480259279, 0.0029377631767284734, 0.001718814100333771, 0.0016596909836223421, 0.0015095328359842994, 0.0018499287426500589, 0.0017839046357589405, 0.0018528553408159072, 0.001742615317564833, 0.0016318717754857485, 0.0034699237999296926, 0.0016430920087407495, 0.0017932101546303016, 0.002414364634846994, 0.0016051839690568836, 0.001645315884316499, 0.054513595054330406, 0.0017001085116910611, 0.0016029819308296424, 0.0025667595576004004, 0.001581373915090695, 0.0016012992258652929, 0.0027648167135988096, 0.0015724634430407322, 0.0016190477668545967, 0.0030212452483558377, 0.0018233633798713138, 0.0015589158761842076, 0.0016143154425098914, 0.0018543018688729337, 0.001740233503397583, 0.0017130607215673193, 0.0017555361241824174, 0.0017702831388559453, 0.0016079185342834901, 0.0015798034417161414, 0.0019350579539097327, 0.0017134981309316418, 0.0016186365355961314, 0.001601712760161753, 0.0016551184262214012, 0.0016167812543928391, 0.0026717820392502833, 0.00283859615474073, 0.001983074030961639, 0.0016147811092118653, 0.0016504930397279042, 0.0019833361928517272, 0.0021312312320481207, 0.0022198045662380466, 0.0017793458929761898, 0.0016069301700060682, 0.001692979914215646, 0.001822604597437867, 0.0016865038995219524, 0.001712453930617072, 0.0029455046747024205, 0.0018131917977864428, 0.0017560415977049013, 0.0017476767597561197, 0.0017578620463609695, 0.0017357583648647912, 0.001692472821707989, 0.0028804607595526435, 0.001636639634479386, 0.002411415814891342, 0.002221960581608059, 0.002008362776101675, 0.0017755949840873712, 0.0016201499690727671, 0.0018614613335550755, 0.0021303444974442092, 0.001682081875254133, 0.0017404034643527843, 0.0017388330324906711, 0.0018972500386221925, 0.0017720043402916009, 0.0017227899922101304, 0.0018342299235248288, 0.0018498006280396105, 0.001958051091781998, 0.0019721544795315857, 0.001681856487971521, 0.0019148090464431186, 0.0018064000466910674, 0.0018594888140007046, 0.0017686865036195332, 0.002115272929487649, 0.0015989023029746472, 0.0015650014374841703, 0.0016034005620895186, 0.00169581183581613, 0.0016910538524825824, 0.0016555459224036895, 0.0016554699923290173, 0.0016054939605965046, 0.0016047043136495631, 0.0015853870936552994, 0.0015096157185325865, 0.0015369669999927282, 0.0016336472745024366, 0.0017425370551791275, 0.0018563644225650933, 0.001771306913724402, 0.001586410649906611, 0.0014799509608565131, 0.0016457339752378175, 0.0017046121174644213, 0.001655851678151521, 0.0016098357027658494, 0.0016225594299612567, 0.0018317326012038393, 0.0016175670079974225, 0.0016091943271021591, 0.001618449852685444, 0.0016276362111966591, 0.0016534344613319263, 0.0017748056725395145, 0.0018051007882604608, 0.0015381290468212683, 0.0015412238662975142, 0.001584686109708855, 0.0014972838907851838, 0.0015398889208881883, 0.0015381350167444907, 0.002597458711534273, 0.0015488271637877915, 0.001540663022751687, 0.001560495124067529, 0.0015304133976314915, 0.0015572820702800527, 0.001536518991997582, 0.0016129028117575217, 0.0015944302813295508, 0.001549407968923333, 0.0015465035157831153, 0.002822189999278635, 0.0015649682809453225, 0.0022687093987769913, 0.0014952370856917696, 0.0014864171171211638, 0.0015075452411110746, 0.002508578952983953, 0.0016074059367383597, 0.0016309709753841162, 0.0026609438045852585, 0.0015570979448966682]
[284.3499357052112, 512.4445408327379, 405.20868706433686, 518.7267553562492, 639.8668663773745, 433.9367026646566, 648.8480876183735, 633.0667455765741, 638.126197271288, 626.1351706249611, 640.6110420398886, 659.459375839774, 404.32556385986527, 652.8266510297358, 647.073722250794, 650.8466562519131, 586.8025598976426, 595.3822925709125, 597.49905547077, 609.8188202321625, 584.0513189257987, 625.4525563471451, 627.1482108576823, 628.4700189937313, 614.1068802344093, 532.1750371166471, 611.0646033396049, 627.8129119661613, 581.5401338816754, 630.3818510793976, 638.7691581467168, 75.2461421166856, 604.842430318413, 599.2106339600958, 572.0602958941132, 340.39503521642325, 581.7964838697874, 602.5218006652418, 662.4566065487038, 540.5613616054641, 560.5680819224746, 539.7075410968936, 573.850114778872, 612.793244556446, 288.19076661575735, 608.6086443609393, 557.6591217810529, 414.187644056248, 622.9815518202216, 607.7860242718209, 18.344047920584956, 588.1977492162083, 623.8373501081438, 389.5962896247575, 632.3615120100474, 624.4929016684129, 361.6876283630227, 635.9448319296135, 617.6470024369627, 330.98934968758334, 548.4370318277294, 641.4714323442018, 619.4576187942727, 539.2865189786017, 574.6355291101039, 583.750469209916, 569.6265580782149, 564.8813899036825, 621.9220555509133, 632.9901388958233, 516.7803878842631, 583.601453627669, 617.8039220100192, 624.3316684940517, 604.1863737104165, 618.512861454184, 374.2820279908033, 352.2868155549014, 504.2676089682222, 619.2789810924128, 605.8795620033983, 504.20095372845304, 469.2123430637775, 450.49010854803436, 562.0042758113593, 622.3045771778768, 590.6744619963774, 548.6653558351348, 592.9425957944449, 583.9573153595182, 339.5003948180895, 551.5136353588225, 569.4625920632933, 572.1881889300545, 568.8728544257189, 576.1170565223784, 590.8514377151606, 347.16668042903916, 611.0080551227145, 414.69413687371866, 450.0529884631384, 497.9180115760989, 563.1914986029232, 617.2268117699704, 537.2123406346398, 469.4076480117219, 594.501382311677, 574.5794124650728, 575.0983454504652, 527.0786557612685, 564.3327035166517, 580.4538014045005, 545.1879217400979, 540.598800131117, 510.7119033803722, 507.05967021280907, 594.5810520409474, 522.2452869948388, 553.5872310409773, 537.7822079222366, 565.3913217257821, 472.75223261246725, 625.4290822769904, 638.9770488693974, 623.6744726450767, 589.688064960779, 591.3472232312009, 604.0303602983713, 604.0580648599606, 622.8612654690152, 623.1677646118554, 630.760780128707, 662.4202356425146, 650.6320565143762, 612.127241668231, 573.8758880494528, 538.6873330712817, 564.5549013848597, 630.353811643201, 675.6980646313143, 607.6316191111596, 586.6437236686322, 603.9188250944864, 621.1814027244556, 616.3102451192668, 545.9312125267555, 618.2124110196945, 621.4289866412854, 617.87518367698, 614.387903833122, 604.801716298116, 563.4419674629115, 553.9856868400572, 650.1405080845605, 648.835008247246, 631.0397963819624, 667.8760161345185, 649.3974899327237, 650.1379847112058, 384.9916826625197, 645.6498332289143, 649.0712019646965, 640.822252230713, 653.4182212124035, 642.1444252679065, 650.8217634849603, 620.0001591604495, 627.1832714856168, 645.4078074058759, 646.619933155226, 354.33475430626737, 638.9905866947973, 440.77923798397313, 668.7902604671896, 672.758668129954, 663.330010091764, 398.6320617138642, 622.1203848662727, 613.1316958381103, 375.80650830612433, 642.2203582487946]
Elapsed: 0.27790159233639955~0.5175079733385151
Time per graph: 0.0021584764962237603~0.004011246699612358
Speed: 565.7008031825682~100.72037339632159
Total Time: 0.2001
best val loss: 0.3159876010676687 test_score: 0.8672

Testing...
Test loss: 0.3720 score: 0.8984 time: 0.20s
test Score 0.8984
Epoch Time List: [2.190362560097128, 0.8277718329336494, 0.9959230497479439, 0.8352038431912661, 0.8083715459797531, 0.9180753952823579, 0.7333630181383342, 0.7211382961831987, 0.719183289911598, 0.8908776296302676, 0.7754188100807369, 0.9483896209858358, 0.8350253158714622, 0.7172621320933104, 0.6919972721952945, 0.7151709068566561, 0.8771548387594521, 0.7656345712020993, 0.7649024929851294, 0.7577030952088535, 0.7726880260743201, 0.7472082569729537, 0.7402134756557643, 0.7341135973110795, 0.7384308811742812, 0.7796321238856763, 0.7469516820274293, 0.7232228149659932, 0.7554672309197485, 0.7143497259821743, 0.7235916599165648, 7.07531742984429, 0.8923592120409012, 0.7399845102336258, 0.8194852231536061, 0.9311576976906508, 0.8629599041305482, 0.9499473709147424, 0.8449818880762905, 0.7704760020133108, 0.8161804070696235, 0.8846712140366435, 0.7724515988957137, 0.7436412780079991, 0.9746753880754113, 0.7526762371417135, 0.8016985531430691, 0.8677595187909901, 0.7684910742100328, 0.7503109879326075, 7.564781199907884, 7.08489133277908, 0.7427216118667275, 0.8718355209566653, 0.7310087170917541, 0.7453365540131927, 0.9093361040577292, 0.9388082961086184, 0.7839764568489045, 0.9883124229963869, 0.8445991871412843, 0.7246466549113393, 1.0443999702110887, 0.8269910020753741, 0.7893095628824085, 0.759718107059598, 0.7735355570912361, 0.7869965101126581, 0.7340571158565581, 0.7324064907152206, 0.7818813943304121, 0.7688171660993248, 0.7420533860567957, 0.771168006118387, 0.7441481400746852, 0.761083303950727, 0.9050499140284956, 0.9574207419063896, 0.8463287791237235, 0.9615001920610666, 0.7488808687776327, 0.8380481451749802, 0.9650101361330599, 0.9292671410366893, 0.8079963880591094, 0.9022342970129102, 0.7799761909991503, 0.8156159960199147, 0.8482574170920998, 0.7622842427808791, 0.9431531731970608, 0.8805694440379739, 0.7857148952316493, 0.8103345099370927, 0.8944694099482149, 0.7485730391927063, 0.7282313990872353, 0.889157489174977, 0.7133336400147527, 0.928022351115942, 1.0613085792865604, 0.8755582219455391, 0.7965074318926781, 0.8627374120987952, 0.8336192730348557, 0.8775385990738869, 0.8417307659983635, 0.8324214688036591, 0.8030315558426082, 0.954708623001352, 0.8366348368581384, 0.8004741459153593, 0.8593834559433162, 0.8385932750534266, 0.8435565556865185, 0.9700685301795602, 0.8275838808622211, 0.8938181740231812, 0.8783031220082194, 0.8033116240985692, 0.8067833709064871, 0.9375000521540642, 0.7994015377480537, 0.7621423720847815, 0.7582155549898744, 0.8871288960799575, 0.7817155960947275, 0.802932801656425, 0.7900750099215657, 0.7571681011468172, 0.7546031337697059, 0.7639221290592104, 0.7535017258487642, 0.7382316419389099, 0.7735064921434969, 1.1674908800050616, 0.8192474010866135, 0.8544580589514226, 0.8962985728867352, 0.7098880973644555, 0.7430075921583921, 0.8916815111879259, 0.7769663149956614, 0.7751821402925998, 0.8966540140099823, 0.7971141519956291, 0.8079139341134578, 0.9341071851085871, 0.8148933011107147, 0.7859153419267386, 0.9310737589839846, 7.0862657856196165, 0.7682217280380428, 0.7719565103761852, 0.7304181468207389, 0.7360118236392736, 0.8884720792993903, 0.715415180195123, 0.7323481121566147, 0.8570711731445044, 0.7398738609626889, 0.7855889508500695, 0.7449466120451689, 0.819461195031181, 0.7513906948734075, 0.7776138728950173, 0.8855995370540768, 0.7588742482475936, 0.7598599982447922, 0.861968751065433, 0.9064822751097381, 0.7535311498213559, 0.8246923417318612, 0.7311395958531648, 0.7191768102347851, 0.7276874161325395, 0.872234420850873, 0.7659836076200008, 0.7674948619678617, 0.9025994909461588, 0.7490084930323064]
Total Epoch List: [62, 61, 58]
Total Time List: [0.20163656305521727, 0.20692215813323855, 0.20005418197251856]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c757c82e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7204;  Loss pred: 0.7204; Loss self: 0.0000; time: 2.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6596 score: 0.5039 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.32s
Val loss: 0.6678 score: 0.4806 time: 0.21s
Test loss: 0.6478 score: 0.4884 time: 0.22s
Epoch 3/1000, LR 0.000045
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6515 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6326 score: 0.5039 time: 0.20s
Epoch 4/1000, LR 0.000075
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6586 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6318 score: 0.5039 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6650 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6296 score: 0.5039 time: 7.22s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6614 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6252 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6557 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6200 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6484 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6119 score: 0.5039 time: 0.22s
Epoch 9/1000, LR 0.000225
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6394 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6030 score: 0.5039 time: 0.22s
Epoch 10/1000, LR 0.000255
Train loss: 0.4944;  Loss pred: 0.4944; Loss self: 0.0000; time: 0.34s
Val loss: 0.6241 score: 0.5039 time: 0.22s
Test loss: 0.5890 score: 0.5116 time: 0.23s
Epoch 11/1000, LR 0.000285
Train loss: 0.4747;  Loss pred: 0.4747; Loss self: 0.0000; time: 0.33s
Val loss: 0.6044 score: 0.5581 time: 0.21s
Test loss: 0.5712 score: 0.5271 time: 0.21s
Epoch 12/1000, LR 0.000285
Train loss: 0.4339;  Loss pred: 0.4339; Loss self: 0.0000; time: 0.33s
Val loss: 0.5868 score: 0.5969 time: 0.34s
Test loss: 0.5547 score: 0.5736 time: 0.20s
Epoch 13/1000, LR 0.000285
Train loss: 0.4022;  Loss pred: 0.4022; Loss self: 0.0000; time: 0.35s
Val loss: 0.5676 score: 0.6667 time: 0.21s
Test loss: 0.5427 score: 0.6357 time: 0.23s
Epoch 14/1000, LR 0.000285
Train loss: 0.3520;  Loss pred: 0.3520; Loss self: 0.0000; time: 0.38s
Val loss: 0.5579 score: 0.6977 time: 0.23s
Test loss: 0.5315 score: 0.7132 time: 0.26s
Epoch 15/1000, LR 0.000285
Train loss: 0.3125;  Loss pred: 0.3125; Loss self: 0.0000; time: 0.33s
Val loss: 0.5345 score: 0.7054 time: 0.32s
Test loss: 0.5107 score: 0.7287 time: 0.21s
Epoch 16/1000, LR 0.000285
Train loss: 0.2947;  Loss pred: 0.2947; Loss self: 0.0000; time: 0.34s
Val loss: 0.5173 score: 0.6822 time: 0.21s
Test loss: 0.4951 score: 0.7519 time: 0.21s
Epoch 17/1000, LR 0.000285
Train loss: 0.2957;  Loss pred: 0.2957; Loss self: 0.0000; time: 0.35s
Val loss: 0.4991 score: 0.6822 time: 0.21s
Test loss: 0.4771 score: 0.7674 time: 0.21s
Epoch 18/1000, LR 0.000285
Train loss: 0.2302;  Loss pred: 0.2302; Loss self: 0.0000; time: 0.33s
Val loss: 0.4825 score: 0.7364 time: 0.32s
Test loss: 0.4630 score: 0.7674 time: 0.21s
Epoch 19/1000, LR 0.000285
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 0.33s
Val loss: 0.4654 score: 0.7752 time: 0.20s
Test loss: 0.4470 score: 0.8140 time: 0.21s
Epoch 20/1000, LR 0.000285
Train loss: 0.2073;  Loss pred: 0.2073; Loss self: 0.0000; time: 0.36s
Val loss: 0.4531 score: 0.8062 time: 0.21s
Test loss: 0.4294 score: 0.8140 time: 0.22s
Epoch 21/1000, LR 0.000285
Train loss: 0.1979;  Loss pred: 0.1979; Loss self: 0.0000; time: 0.35s
Val loss: 0.4359 score: 0.8217 time: 0.22s
Test loss: 0.4115 score: 0.8450 time: 0.33s
Epoch 22/1000, LR 0.000285
Train loss: 0.1878;  Loss pred: 0.1878; Loss self: 0.0000; time: 0.34s
Val loss: 0.4137 score: 0.8217 time: 0.22s
Test loss: 0.3958 score: 0.8295 time: 0.20s
Epoch 23/1000, LR 0.000285
Train loss: 0.1784;  Loss pred: 0.1784; Loss self: 0.0000; time: 0.39s
Val loss: 0.4106 score: 0.8372 time: 0.20s
Test loss: 0.4095 score: 0.8217 time: 0.21s
Epoch 24/1000, LR 0.000285
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.34s
Val loss: 0.3979 score: 0.8527 time: 0.21s
Test loss: 0.3854 score: 0.8527 time: 0.39s
Epoch 25/1000, LR 0.000285
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 0.34s
Val loss: 0.3848 score: 0.8682 time: 0.22s
Test loss: 0.3566 score: 0.8992 time: 0.21s
Epoch 26/1000, LR 0.000285
Train loss: 0.1333;  Loss pred: 0.1333; Loss self: 0.0000; time: 0.36s
Val loss: 0.3934 score: 0.8605 time: 0.21s
Test loss: 0.3546 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.33s
Val loss: 0.3972 score: 0.8450 time: 0.20s
Test loss: 0.3666 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.1253;  Loss pred: 0.1253; Loss self: 0.0000; time: 0.46s
Val loss: 0.3771 score: 0.8605 time: 0.22s
Test loss: 0.3502 score: 0.9070 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.34s
Val loss: 0.3554 score: 0.8760 time: 0.24s
Test loss: 0.3338 score: 0.9147 time: 0.21s
Epoch 30/1000, LR 0.000285
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.33s
Val loss: 0.3450 score: 0.8837 time: 0.22s
Test loss: 0.3260 score: 0.9225 time: 0.21s
Epoch 31/1000, LR 0.000285
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.42s
Val loss: 0.3559 score: 0.8760 time: 0.23s
Test loss: 0.3303 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0891;  Loss pred: 0.0891; Loss self: 0.0000; time: 0.35s
Val loss: 0.3969 score: 0.8682 time: 0.22s
Test loss: 0.3724 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.35s
Val loss: 0.3300 score: 0.8682 time: 0.24s
Test loss: 0.2999 score: 0.9147 time: 0.20s
Epoch 34/1000, LR 0.000285
Train loss: 0.0739;  Loss pred: 0.0739; Loss self: 0.0000; time: 0.45s
Val loss: 0.3192 score: 0.8682 time: 0.21s
Test loss: 0.3312 score: 0.8837 time: 0.22s
Epoch 35/1000, LR 0.000285
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.34s
Val loss: 0.3686 score: 0.8295 time: 0.21s
Test loss: 0.4255 score: 0.8450 time: 0.37s
     INFO: Early stopping counter 1 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.34s
Val loss: 0.3519 score: 0.8372 time: 0.21s
Test loss: 0.3766 score: 0.8527 time: 0.32s
     INFO: Early stopping counter 2 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0628;  Loss pred: 0.0628; Loss self: 0.0000; time: 0.39s
Val loss: 0.3252 score: 0.8605 time: 0.23s
Test loss: 0.3082 score: 0.8837 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.33s
Val loss: 0.3217 score: 0.8605 time: 0.23s
Test loss: 0.3004 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.34s
Val loss: 0.3144 score: 0.8837 time: 0.22s
Test loss: 0.2798 score: 0.9070 time: 0.23s
Epoch 40/1000, LR 0.000284
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.45s
Val loss: 0.3241 score: 0.8915 time: 0.22s
Test loss: 0.2598 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.35s
Val loss: 0.3370 score: 0.8837 time: 0.22s
Test loss: 0.2641 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.36s
Val loss: 0.3426 score: 0.8915 time: 0.21s
Test loss: 0.2729 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.45s
Val loss: 0.3465 score: 0.8992 time: 0.22s
Test loss: 0.2978 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.34s
Val loss: 0.3624 score: 0.8915 time: 0.21s
Test loss: 0.3074 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.35s
Val loss: 0.3591 score: 0.9070 time: 0.21s
Test loss: 0.3191 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.39s
Val loss: 0.3595 score: 0.8992 time: 0.22s
Test loss: 0.3474 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.34s
Val loss: 0.3689 score: 0.8837 time: 0.22s
Test loss: 0.3818 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.37s
Val loss: 0.3773 score: 0.8837 time: 0.22s
Test loss: 0.3812 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.35s
Val loss: 0.4197 score: 0.8682 time: 0.33s
Test loss: 0.4653 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.34s
Val loss: 0.4233 score: 0.8760 time: 0.20s
Test loss: 0.4777 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.35s
Val loss: 0.4430 score: 0.8915 time: 0.23s
Test loss: 0.4426 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.35s
Val loss: 0.4568 score: 0.8915 time: 0.32s
Test loss: 0.4833 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.33s
Val loss: 0.4842 score: 0.8760 time: 0.22s
Test loss: 0.5943 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.36s
Val loss: 0.6411 score: 0.8605 time: 0.22s
Test loss: 0.7911 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.35s
Val loss: 0.7392 score: 0.8450 time: 0.23s
Test loss: 0.9286 score: 0.8217 time: 0.23s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.34s
Val loss: 0.6012 score: 0.8682 time: 0.21s
Test loss: 0.7271 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.34s
Val loss: 0.4740 score: 0.8992 time: 0.22s
Test loss: 0.5144 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.34s
Val loss: 0.5255 score: 0.8992 time: 0.22s
Test loss: 0.4843 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.33s
Val loss: 0.4808 score: 0.8992 time: 0.21s
Test loss: 0.4712 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.0473,   Val_Loss: 0.3144,   Val_Precision: 0.9623,   Val_Recall: 0.7969,   Val_accuracy: 0.8718,   Val_Score: 0.8837,   Val_Loss: 0.3144,   Test_Precision: 0.9818,   Test_Recall: 0.8308,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.2798


[0.2245012829080224, 0.22792299115099013, 0.20512031391263008, 0.2242936238180846, 7.2249776241369545, 0.21327387704513967, 0.2106182100251317, 0.22093760687857866, 0.2225421688053757, 0.23338529095053673, 0.21402886603027582, 0.20624919491820037, 0.233391071902588, 0.2650288420263678, 0.2153807000722736, 0.21314218686893582, 0.22042002296075225, 0.21792708593420684, 0.2130003219936043, 0.2227448031771928, 0.33516552997753024, 0.20696760807186365, 0.21072436682879925, 0.39488415513187647, 0.2187357449438423, 0.22380926390178502, 0.2245597168803215, 0.2287372089922428, 0.2158951999153942, 0.21707792486995459, 0.2289484478533268, 0.22409966215491295, 0.2077743890695274, 0.22351776203140616, 0.3764821318909526, 0.32813476608134806, 0.23536612512543797, 0.22907703882083297, 0.24188316706568003, 0.22371618705801666, 0.21007584687322378, 0.22351218201220036, 0.22010875190608203, 0.22188499313779175, 0.21536617213860154, 0.2281198250129819, 0.22857814491726458, 0.22416185308247805, 0.21244094404391944, 0.21991211688145995, 0.21742518316023052, 0.23805435211397707, 0.22466951282694936, 0.22814419399946928, 0.24097152892500162, 0.24409681814722717, 0.22617881000041962, 0.22247378784231842, 0.2251824550330639]
[0.0017403200225428093, 0.0017668448926433343, 0.001590079952811086, 0.0017387102621556946, 0.056007578481681815, 0.0016532858685669742, 0.0016326993025204007, 0.0017126946269657261, 0.0017251330915145403, 0.001809188301942145, 0.0016591384963587272, 0.0015988309683581425, 0.001809233115523938, 0.002054487147491223, 0.001669617830017625, 0.0016522650144878746, 0.0017086823485329631, 0.001689357255303929, 0.0016511652867721265, 0.0017267039005983938, 0.002598182402926591, 0.0016044000625725864, 0.001633522223479064, 0.0030611174816424533, 0.0016956259297972271, 0.0017349555341223644, 0.0017407729990722596, 0.0017731566588545954, 0.0016736062008945287, 0.0016827746113949969, 0.0017747941694056342, 0.0017372066833714183, 0.0016106541788335456, 0.001732695829700823, 0.002918466138689555, 0.0025436803572197525, 0.00182454360562355, 0.0017757909986111083, 0.0018750633105866668, 0.0017342340082016794, 0.0016284949370017347, 0.0017326525737379873, 0.001706269394620791, 0.0017200387064945097, 0.0016695052103767561, 0.0017683707365347434, 0.001771923604009803, 0.0017376887835851011, 0.0016468290235962747, 0.0017047450921043408, 0.001685466536125818, 0.001845382574526954, 0.001741624130441468, 0.0017685596434067385, 0.0018679963482558265, 0.001892223396490133, 0.0017533241085303847, 0.0017246030065296001, 0.0017456004266128984]
[574.6069613902873, 565.9806382347032, 628.8991935481674, 575.1389531457507, 17.854726576459043, 604.8560742049857, 612.4826527801526, 583.8752479603661, 579.665421131116, 552.7340625221323, 602.7224382983559, 625.4569868801774, 552.7203716423292, 486.7394771591152, 598.9394590913322, 605.2297853138006, 585.2462869172716, 591.9411047369563, 605.6328872773885, 579.1380905860277, 384.8844480178145, 623.2859392915649, 612.1741018436879, 326.6780860247959, 589.752717522777, 576.3836480719114, 574.4574396161632, 563.9659614994025, 597.5121264880044, 594.2566480552103, 563.4456193502668, 575.6367446499157, 620.8657408533293, 577.1353418520466, 342.64574350998583, 393.13115626406847, 548.082269405802, 563.1293326647818, 533.3153255967243, 576.6234517779718, 614.063929385698, 577.1497501328969, 586.0739242892208, 581.3822655410063, 598.97986168868, 565.4922801762575, 564.3584168849233, 575.477041370352, 607.2275783774097, 586.5979639018043, 593.307537448107, 541.8930544829385, 574.1767023786696, 565.43187770231, 535.3329523013862, 528.4788264720172, 570.3452060772655, 579.8435907938541, 572.8687875840892]
Elapsed: 0.3507762704107721~0.903357443769574
Time per graph: 0.0027191958946571485~0.007002770881934682
Speed: 555.6899018430845~93.18609270488369
Total Time: 0.2256
best val loss: 0.3144133157970369 test_score: 0.9070

Testing...
Test loss: 0.3191 score: 0.9225 time: 0.23s
test Score 0.9225
Epoch Time List: [3.412392775993794, 0.7508424730040133, 0.7450332019943744, 0.7593655700329691, 7.844196134014055, 0.8954572926741093, 0.7589008309878409, 0.7793925509322435, 0.9048856711015105, 0.7820957801304758, 0.7477481586392969, 0.8705314833205193, 0.7846274049952626, 0.863914682995528, 0.8645233667921275, 0.7522227680310607, 0.7697230761405081, 0.8701152172870934, 0.7373735047876835, 0.78839713614434, 0.8927203421480954, 0.7660412658005953, 0.7988094510510564, 0.9386240029707551, 0.764566752128303, 0.796651936834678, 0.7472426318563521, 0.9018570419866592, 0.7844643052667379, 0.7564555609133095, 0.8681054338812828, 0.788407688960433, 0.7898249628487974, 0.8840459862258285, 0.9286172578576952, 0.8686953668948263, 0.8485953831113875, 0.7906710270326585, 0.7954516331665218, 0.8842835309915245, 0.7739354530349374, 0.7906618833076209, 0.8846431351266801, 0.7719594999216497, 0.7658592369407415, 0.8355820700526237, 0.7841517196502537, 0.8079335778020322, 0.8857866732869297, 0.7532671960070729, 0.7835823623463511, 0.9018026208505034, 0.7690251977182925, 0.8014025806915015, 0.8172864550724626, 0.785912818973884, 0.7852424751035869, 0.7726647749077529, 0.7664156118407845]
Total Epoch List: [59]
Total Time List: [0.2256447810214013]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c72d10be0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7552;  Loss pred: 0.7552; Loss self: 0.0000; time: 0.35s
Val loss: 1.5174 score: 0.4574 time: 0.20s
Test loss: 1.7440 score: 0.4651 time: 0.20s
Epoch 2/1000, LR 0.000015
Train loss: 0.7109;  Loss pred: 0.7109; Loss self: 0.0000; time: 0.36s
Val loss: 1.1705 score: 0.5194 time: 0.21s
Test loss: 1.3205 score: 0.4574 time: 0.20s
Epoch 3/1000, LR 0.000045
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.36s
Val loss: 0.9884 score: 0.4729 time: 0.22s
Test loss: 1.0783 score: 0.3566 time: 0.23s
Epoch 4/1000, LR 0.000075
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.36s
Val loss: 0.8686 score: 0.3411 time: 0.34s
Test loss: 0.8867 score: 0.2481 time: 0.22s
Epoch 5/1000, LR 0.000105
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.40s
Val loss: 0.7970 score: 0.4031 time: 0.23s
Test loss: 0.7613 score: 0.3566 time: 0.22s
Epoch 6/1000, LR 0.000135
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7882 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7121 score: 0.4961 time: 0.22s
Epoch 7/1000, LR 0.000165
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7979 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.4961 time: 0.35s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.4621;  Loss pred: 0.4621; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8016 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.4107;  Loss pred: 0.4107; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8070 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.4065;  Loss pred: 0.4065; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7966 score: 0.5039 time: 0.37s
Test loss: 0.6911 score: 0.5116 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 0.40s
Val loss: 0.8008 score: 0.5271 time: 0.23s
Test loss: 0.6943 score: 0.5426 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.3081;  Loss pred: 0.3081; Loss self: 0.0000; time: 0.42s
Val loss: 0.7868 score: 0.5271 time: 0.23s
Test loss: 0.6885 score: 0.5426 time: 0.23s
Epoch 13/1000, LR 0.000285
Train loss: 0.3189;  Loss pred: 0.3189; Loss self: 0.0000; time: 0.41s
Val loss: 0.7610 score: 0.5271 time: 0.20s
Test loss: 0.6722 score: 0.5349 time: 0.21s
Epoch 14/1000, LR 0.000285
Train loss: 0.2866;  Loss pred: 0.2866; Loss self: 0.0000; time: 0.37s
Val loss: 0.7578 score: 0.5271 time: 0.21s
Test loss: 0.6700 score: 0.5426 time: 0.21s
Epoch 15/1000, LR 0.000285
Train loss: 0.2725;  Loss pred: 0.2725; Loss self: 0.0000; time: 0.37s
Val loss: 0.7667 score: 0.5271 time: 0.25s
Test loss: 0.6794 score: 0.5426 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.2585;  Loss pred: 0.2585; Loss self: 0.0000; time: 0.38s
Val loss: 0.7695 score: 0.5349 time: 0.31s
Test loss: 0.6836 score: 0.5504 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.2248;  Loss pred: 0.2248; Loss self: 0.0000; time: 0.37s
Val loss: 0.7718 score: 0.5349 time: 0.24s
Test loss: 0.6864 score: 0.5581 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.40s
Val loss: 0.7758 score: 0.5349 time: 0.22s
Test loss: 0.6869 score: 0.5736 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.2006;  Loss pred: 0.2006; Loss self: 0.0000; time: 0.36s
Val loss: 0.7643 score: 0.5349 time: 0.32s
Test loss: 0.6747 score: 0.5736 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 0.36s
Val loss: 0.7293 score: 0.5581 time: 0.25s
Test loss: 0.6480 score: 0.5736 time: 0.22s
Epoch 21/1000, LR 0.000285
Train loss: 0.1664;  Loss pred: 0.1664; Loss self: 0.0000; time: 0.38s
Val loss: 0.6956 score: 0.5581 time: 0.21s
Test loss: 0.6230 score: 0.5814 time: 0.21s
Epoch 22/1000, LR 0.000285
Train loss: 0.1410;  Loss pred: 0.1410; Loss self: 0.0000; time: 0.52s
Val loss: 0.6865 score: 0.5581 time: 0.36s
Test loss: 0.6128 score: 0.6047 time: 0.23s
Epoch 23/1000, LR 0.000285
Train loss: 0.1503;  Loss pred: 0.1503; Loss self: 0.0000; time: 0.36s
Val loss: 0.6976 score: 0.5581 time: 0.23s
Test loss: 0.6177 score: 0.6124 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.35s
Val loss: 0.6857 score: 0.5659 time: 0.20s
Test loss: 0.6115 score: 0.6279 time: 0.20s
Epoch 25/1000, LR 0.000285
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 0.36s
Val loss: 0.6586 score: 0.5891 time: 0.20s
Test loss: 0.5963 score: 0.6434 time: 0.20s
Epoch 26/1000, LR 0.000285
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 0.36s
Val loss: 0.6141 score: 0.6124 time: 0.22s
Test loss: 0.5708 score: 0.6589 time: 0.19s
Epoch 27/1000, LR 0.000285
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.36s
Val loss: 0.6581 score: 0.5814 time: 0.20s
Test loss: 0.5962 score: 0.6512 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 0.38s
Val loss: 0.7034 score: 0.5659 time: 0.22s
Test loss: 0.6197 score: 0.6512 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.37s
Val loss: 0.6369 score: 0.6279 time: 0.31s
Test loss: 0.5677 score: 0.7132 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.49s
Val loss: 0.6334 score: 0.6434 time: 0.20s
Test loss: 0.5537 score: 0.7209 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.34s
Val loss: 0.5694 score: 0.6977 time: 0.20s
Test loss: 0.5069 score: 0.7597 time: 0.20s
Epoch 32/1000, LR 0.000285
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.36s
Val loss: 0.5179 score: 0.7209 time: 0.32s
Test loss: 0.4768 score: 0.8062 time: 0.21s
Epoch 33/1000, LR 0.000285
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.37s
Val loss: 0.5211 score: 0.7209 time: 0.21s
Test loss: 0.4792 score: 0.8062 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.36s
Val loss: 0.4321 score: 0.7829 time: 0.21s
Test loss: 0.4307 score: 0.8295 time: 0.21s
Epoch 35/1000, LR 0.000285
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.36s
Val loss: 0.4198 score: 0.7984 time: 0.34s
Test loss: 0.4337 score: 0.8295 time: 0.20s
Epoch 36/1000, LR 0.000285
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.35s
Val loss: 0.4501 score: 0.8062 time: 0.20s
Test loss: 0.4622 score: 0.8295 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.36s
Val loss: 0.4044 score: 0.8062 time: 0.20s
Test loss: 0.4460 score: 0.8527 time: 0.22s
Epoch 38/1000, LR 0.000284
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.37s
Val loss: 0.3971 score: 0.8062 time: 0.20s
Test loss: 0.4493 score: 0.8527 time: 0.27s
Epoch 39/1000, LR 0.000284
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 2.13s
Val loss: 0.5257 score: 0.7674 time: 4.68s
Test loss: 0.5347 score: 0.8295 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.35s
Val loss: 0.4293 score: 0.7907 time: 0.22s
Test loss: 0.4916 score: 0.8450 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.43s
Val loss: 0.3330 score: 0.8450 time: 0.34s
Test loss: 0.4422 score: 0.8915 time: 0.23s
Epoch 42/1000, LR 0.000284
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.35s
Val loss: 0.4025 score: 0.8217 time: 0.21s
Test loss: 0.4751 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.37s
Val loss: 0.5663 score: 0.7752 time: 0.21s
Test loss: 0.5545 score: 0.8527 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.38s
Val loss: 0.4672 score: 0.8140 time: 0.35s
Test loss: 0.5155 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.36s
Val loss: 0.3524 score: 0.8527 time: 0.22s
Test loss: 0.4760 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.37s
Val loss: 0.3145 score: 0.8527 time: 0.21s
Test loss: 0.5193 score: 0.8837 time: 0.21s
Epoch 47/1000, LR 0.000284
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.36s
Val loss: 0.3174 score: 0.8527 time: 0.34s
Test loss: 0.5602 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.37s
Val loss: 0.3222 score: 0.8527 time: 0.22s
Test loss: 0.5845 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.37s
Val loss: 0.3329 score: 0.8450 time: 0.26s
Test loss: 0.6040 score: 0.8837 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.36s
Val loss: 0.3989 score: 0.8372 time: 0.29s
Test loss: 0.6573 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.37s
Val loss: 0.4133 score: 0.8372 time: 0.22s
Test loss: 0.6509 score: 0.8760 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.37s
Val loss: 0.3501 score: 0.8605 time: 0.21s
Test loss: 0.5853 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.37s
Val loss: 0.3365 score: 0.8605 time: 0.21s
Test loss: 0.5417 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.37s
Val loss: 0.3545 score: 0.8527 time: 0.22s
Test loss: 0.5424 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.36s
Val loss: 0.3353 score: 0.8527 time: 0.20s
Test loss: 0.5996 score: 0.8837 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.39s
Val loss: 0.3911 score: 0.8527 time: 0.22s
Test loss: 0.6821 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.36s
Val loss: 0.4030 score: 0.8450 time: 0.21s
Test loss: 0.7040 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.38s
Val loss: 0.3898 score: 0.8527 time: 0.21s
Test loss: 0.7167 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.40s
Val loss: 0.4006 score: 0.8527 time: 0.21s
Test loss: 0.7424 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.37s
Val loss: 0.4297 score: 0.8527 time: 0.21s
Test loss: 0.7805 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.37s
Val loss: 0.4623 score: 0.8372 time: 0.22s
Test loss: 0.8203 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.37s
Val loss: 0.5536 score: 0.8527 time: 0.21s
Test loss: 0.8634 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.38s
Val loss: 0.5439 score: 0.8527 time: 0.21s
Test loss: 0.8567 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.40s
Val loss: 0.5034 score: 0.8450 time: 0.27s
Test loss: 0.8374 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.37s
Val loss: 0.4793 score: 0.8527 time: 0.21s
Test loss: 0.8092 score: 0.8915 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.36s
Val loss: 0.4863 score: 0.8450 time: 0.21s
Test loss: 0.8187 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.0219,   Val_Loss: 0.3145,   Val_Precision: 0.9423,   Val_Recall: 0.7538,   Val_accuracy: 0.8376,   Val_Score: 0.8527,   Val_Loss: 0.3145,   Test_Precision: 0.9298,   Test_Recall: 0.8281,   Test_accuracy: 0.8760,   Test_Score: 0.8837,   Test_loss: 0.5193


[0.2245012829080224, 0.22792299115099013, 0.20512031391263008, 0.2242936238180846, 7.2249776241369545, 0.21327387704513967, 0.2106182100251317, 0.22093760687857866, 0.2225421688053757, 0.23338529095053673, 0.21402886603027582, 0.20624919491820037, 0.233391071902588, 0.2650288420263678, 0.2153807000722736, 0.21314218686893582, 0.22042002296075225, 0.21792708593420684, 0.2130003219936043, 0.2227448031771928, 0.33516552997753024, 0.20696760807186365, 0.21072436682879925, 0.39488415513187647, 0.2187357449438423, 0.22380926390178502, 0.2245597168803215, 0.2287372089922428, 0.2158951999153942, 0.21707792486995459, 0.2289484478533268, 0.22409966215491295, 0.2077743890695274, 0.22351776203140616, 0.3764821318909526, 0.32813476608134806, 0.23536612512543797, 0.22907703882083297, 0.24188316706568003, 0.22371618705801666, 0.21007584687322378, 0.22351218201220036, 0.22010875190608203, 0.22188499313779175, 0.21536617213860154, 0.2281198250129819, 0.22857814491726458, 0.22416185308247805, 0.21244094404391944, 0.21991211688145995, 0.21742518316023052, 0.23805435211397707, 0.22466951282694936, 0.22814419399946928, 0.24097152892500162, 0.24409681814722717, 0.22617881000041962, 0.22247378784231842, 0.2251824550330639, 0.2069713280070573, 0.2099206750281155, 0.2313416430260986, 0.2253890719730407, 0.22331489599309862, 0.22948856907896698, 0.35208185808733106, 0.2197996738832444, 0.21798768802545965, 0.22461513779126108, 0.22782821278087795, 0.23281795205548406, 0.21619872795417905, 0.21336618205532432, 0.21744590415619314, 0.21122178109362721, 0.23066761693917215, 0.23189513408578932, 0.23107773391529918, 0.22604509093798697, 0.2158712709788233, 0.2313206670805812, 0.2308459091000259, 0.20131087605841458, 0.2018290450796485, 0.2003507239278406, 0.21542075811885297, 0.24029498593881726, 0.2183850109577179, 0.20583532098680735, 0.20681854290887713, 0.21665232512168586, 0.21236221306025982, 0.21133778290823102, 0.20408970792777836, 0.19848841591738164, 0.22760923393070698, 0.278655719012022, 0.22775147901847959, 0.21783598815090954, 0.2416678979061544, 0.21288683684542775, 0.21464314893819392, 0.22933309897780418, 0.21841540304012597, 0.21634008409455419, 0.22122335992753506, 0.21226745401509106, 0.2401758141350001, 0.21854330482892692, 0.22054480598308146, 0.22014905093237758, 0.21876181988045573, 0.2141285629477352, 0.20330396993085742, 0.22598235378973186, 0.21575847500935197, 0.2250464460812509, 0.22468901495449245, 0.21451542805880308, 0.21801876602694392, 0.21911091799847782, 0.21722148614935577, 0.21274854009971023, 0.20020523015409708, 0.21276517305523157]
[0.0017403200225428093, 0.0017668448926433343, 0.001590079952811086, 0.0017387102621556946, 0.056007578481681815, 0.0016532858685669742, 0.0016326993025204007, 0.0017126946269657261, 0.0017251330915145403, 0.001809188301942145, 0.0016591384963587272, 0.0015988309683581425, 0.001809233115523938, 0.002054487147491223, 0.001669617830017625, 0.0016522650144878746, 0.0017086823485329631, 0.001689357255303929, 0.0016511652867721265, 0.0017267039005983938, 0.002598182402926591, 0.0016044000625725864, 0.001633522223479064, 0.0030611174816424533, 0.0016956259297972271, 0.0017349555341223644, 0.0017407729990722596, 0.0017731566588545954, 0.0016736062008945287, 0.0016827746113949969, 0.0017747941694056342, 0.0017372066833714183, 0.0016106541788335456, 0.001732695829700823, 0.002918466138689555, 0.0025436803572197525, 0.00182454360562355, 0.0017757909986111083, 0.0018750633105866668, 0.0017342340082016794, 0.0016284949370017347, 0.0017326525737379873, 0.001706269394620791, 0.0017200387064945097, 0.0016695052103767561, 0.0017683707365347434, 0.001771923604009803, 0.0017376887835851011, 0.0016468290235962747, 0.0017047450921043408, 0.001685466536125818, 0.001845382574526954, 0.001741624130441468, 0.0017685596434067385, 0.0018679963482558265, 0.001892223396490133, 0.0017533241085303847, 0.0017246030065296001, 0.0017456004266128984, 0.0016044288992795141, 0.0016272920544815157, 0.0017933460699697567, 0.001747202108318145, 0.0017311232247527026, 0.0017789811556509069, 0.0027293167293591554, 0.001703873440955383, 0.0016898270389570514, 0.0017412026185369075, 0.0017661101765959531, 0.0018047903260115044, 0.0016759591314277446, 0.0016540014112815839, 0.0016856271640014972, 0.001637378148012614, 0.0017881210615439702, 0.001797636698339452, 0.001791300262909296, 0.0017522875266510617, 0.0016734207052621962, 0.0017931834657409394, 0.00178950317131803, 0.0015605494268094153, 0.0015645662409275076, 0.001553106387037524, 0.0016699283575104882, 0.0018627518289830796, 0.0016929070616877357, 0.001595622643308584, 0.0016032445186734661, 0.0016794753885402004, 0.001646218705893487, 0.0016382773868855117, 0.0015820907591300648, 0.0015386698908324158, 0.0017644126661295115, 0.002160121852806372, 0.001765515341228524, 0.0016886510709372832, 0.0018733945574120495, 0.0016502855569413004, 0.0016639003793658444, 0.0017777759610682494, 0.0016931426592257826, 0.0016770549154616603, 0.0017149097668801166, 0.0016454841396518686, 0.0018618280165503885, 0.0016941341459606738, 0.001709649658783577, 0.0017065817901734696, 0.001695828061088804, 0.0016599113406801178, 0.0015759997669058715, 0.001751801192168464, 0.0016725463179019533, 0.0017445460936531079, 0.0017417753097247477, 0.0016629102950294812, 0.0016900679536972396, 0.0016985342480502156, 0.0016838874895298897, 0.0016492134891450405, 0.001551978528326334, 0.0016493424267847408]
[574.6069613902873, 565.9806382347032, 628.8991935481674, 575.1389531457507, 17.854726576459043, 604.8560742049857, 612.4826527801526, 583.8752479603661, 579.665421131116, 552.7340625221323, 602.7224382983559, 625.4569868801774, 552.7203716423292, 486.7394771591152, 598.9394590913322, 605.2297853138006, 585.2462869172716, 591.9411047369563, 605.6328872773885, 579.1380905860277, 384.8844480178145, 623.2859392915649, 612.1741018436879, 326.6780860247959, 589.752717522777, 576.3836480719114, 574.4574396161632, 563.9659614994025, 597.5121264880044, 594.2566480552103, 563.4456193502668, 575.6367446499157, 620.8657408533293, 577.1353418520466, 342.64574350998583, 393.13115626406847, 548.082269405802, 563.1293326647818, 533.3153255967243, 576.6234517779718, 614.063929385698, 577.1497501328969, 586.0739242892208, 581.3822655410063, 598.97986168868, 565.4922801762575, 564.3584168849233, 575.477041370352, 607.2275783774097, 586.5979639018043, 593.307537448107, 541.8930544829385, 574.1767023786696, 565.43187770231, 535.3329523013862, 528.4788264720172, 570.3452060772655, 579.8435907938541, 572.8687875840892, 623.274736854379, 614.5178410021906, 557.6168575298265, 572.3436317064653, 577.6596291363683, 562.1195012794346, 366.39206774466237, 586.8980500331572, 591.776540998653, 574.3156995940409, 566.2160907353052, 554.0809841384454, 596.6732608498055, 604.5944055302598, 593.2509996018977, 610.7324695970574, 559.2462509985428, 556.2859285882066, 558.2537002344178, 570.6825990544946, 597.5783596171755, 557.6674217140421, 558.8143212193728, 640.7999534141809, 639.1547854229419, 643.8708953527981, 598.828084751366, 536.8401654158749, 590.6998810691089, 626.7145958310432, 623.7351747364188, 595.4240275406476, 607.4527014059465, 610.3972428631729, 632.0749895220071, 649.9119830433564, 566.7608373009707, 462.93684715092667, 566.4068595995069, 592.1886511728865, 533.7903839015221, 605.955736444447, 600.9975190829189, 562.5005748188366, 590.6176863190409, 596.2833958390204, 583.1210593775261, 607.7238764583703, 537.1065378277038, 590.2720291567827, 584.9151578291801, 585.9666414806597, 589.6824229680168, 602.4418145069535, 634.5178603441547, 570.8410317737893, 597.8907664897453, 573.2150062633104, 574.1268660870097, 601.3553485049963, 591.6921848097127, 588.7429123951558, 593.8639049329724, 606.3496367098019, 644.3388112324002, 606.302235218322]
Elapsed: 0.28261429800838234~0.624138595576318
Time per graph: 0.0021908085116928862~0.004838283686638124
Speed: 571.4840370629255~72.23274294912267
Total Time: 0.2134
best val loss: 0.31449286157424133 test_score: 0.8837

Testing...
Test loss: 0.5853 score: 0.8837 time: 0.21s
test Score 0.8837
Epoch Time List: [3.412392775993794, 0.7508424730040133, 0.7450332019943744, 0.7593655700329691, 7.844196134014055, 0.8954572926741093, 0.7589008309878409, 0.7793925509322435, 0.9048856711015105, 0.7820957801304758, 0.7477481586392969, 0.8705314833205193, 0.7846274049952626, 0.863914682995528, 0.8645233667921275, 0.7522227680310607, 0.7697230761405081, 0.8701152172870934, 0.7373735047876835, 0.78839713614434, 0.8927203421480954, 0.7660412658005953, 0.7988094510510564, 0.9386240029707551, 0.764566752128303, 0.796651936834678, 0.7472426318563521, 0.9018570419866592, 0.7844643052667379, 0.7564555609133095, 0.8681054338812828, 0.788407688960433, 0.7898249628487974, 0.8840459862258285, 0.9286172578576952, 0.8686953668948263, 0.8485953831113875, 0.7906710270326585, 0.7954516331665218, 0.8842835309915245, 0.7739354530349374, 0.7906618833076209, 0.8846431351266801, 0.7719594999216497, 0.7658592369407415, 0.8355820700526237, 0.7841517196502537, 0.8079335778020322, 0.8857866732869297, 0.7532671960070729, 0.7835823623463511, 0.9018026208505034, 0.7690251977182925, 0.8014025806915015, 0.8172864550724626, 0.785912818973884, 0.7852424751035869, 0.7726647749077529, 0.7664156118407845, 0.7497097770683467, 0.7755272381473333, 0.8079256240744144, 0.9226731720846146, 0.8466604221612215, 0.8490250559989363, 0.9640092838089913, 0.8539954731240869, 0.8485749687533826, 0.9637054330669343, 0.8555922689847648, 0.8670871907379478, 0.8252947942819446, 0.7864811618346721, 0.8313865850213915, 0.8922625419218093, 0.8346918001770973, 0.8441243420820683, 0.904949571006, 0.8281701719388366, 0.798797091236338, 1.1031175169628114, 0.8222302468493581, 0.746981190983206, 0.7533673229627311, 0.7772801530081779, 0.7655749360565096, 0.8363558507990092, 0.8989254548214376, 0.8850318400654942, 0.7462576772086322, 0.8932186020538211, 0.7903160059358925, 0.7724112577270716, 0.8964198341127485, 0.7387860859744251, 0.7796477768570185, 0.8453684740234166, 7.037920695031062, 0.778451960766688, 1.0040929990354925, 0.7717378339730203, 0.7794884808827192, 0.9526469262782484, 0.79415238276124, 0.7950714307371527, 0.9184294128790498, 0.7925028237514198, 0.8625418969895691, 0.8676912190858275, 0.8046548676211387, 0.8038576426915824, 0.7926903252955526, 0.7967822179198265, 0.7608418979216367, 0.8239214988425374, 0.7783924129325897, 0.808261978905648, 0.829949646955356, 0.7912889642175287, 0.7969363580923527, 0.7971011931076646, 0.8010706701315939, 0.877561752917245, 0.7704054240603, 0.7684781302232295]
Total Epoch List: [59, 66]
Total Time List: [0.2256447810214013, 0.21342759998515248]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c7578f850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7316;  Loss pred: 0.7316; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8051 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9214 score: 0.5000 time: 0.21s
Epoch 2/1000, LR 0.000020
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.38s
Val loss: 0.8149 score: 0.5116 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9210 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.38s
Val loss: 0.8552 score: 0.5116 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0349 score: 0.5000 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.42s
Val loss: 0.8040 score: 0.5349 time: 0.21s
Test loss: 0.9627 score: 0.5000 time: 0.23s
Epoch 5/1000, LR 0.000110
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.40s
Val loss: 0.8502 score: 0.5349 time: 0.27s
Test loss: 1.0325 score: 0.5078 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.37s
Val loss: 0.8204 score: 0.5426 time: 0.21s
Test loss: 0.9857 score: 0.5234 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.37s
Val loss: 0.8800 score: 0.5271 time: 0.22s
Test loss: 1.0659 score: 0.5078 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.37s
Val loss: 0.9436 score: 0.5039 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1631 score: 0.5000 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.3894;  Loss pred: 0.3894; Loss self: 0.0000; time: 0.56s
Val loss: 1.0091 score: 0.5194 time: 0.26s
Test loss: 1.2654 score: 0.5156 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.4054;  Loss pred: 0.4054; Loss self: 0.0000; time: 0.38s
Val loss: 1.0610 score: 0.5426 time: 0.22s
Test loss: 1.3377 score: 0.5234 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.3585;  Loss pred: 0.3585; Loss self: 0.0000; time: 0.40s
Val loss: 1.0185 score: 0.5581 time: 0.22s
Test loss: 1.2712 score: 0.5234 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.3569;  Loss pred: 0.3569; Loss self: 0.0000; time: 0.38s
Val loss: 1.0212 score: 0.5504 time: 0.22s
Test loss: 1.2683 score: 0.5391 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.2927;  Loss pred: 0.2927; Loss self: 0.0000; time: 0.39s
Val loss: 1.1474 score: 0.5581 time: 0.22s
Test loss: 1.4359 score: 0.5391 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.2884;  Loss pred: 0.2884; Loss self: 0.0000; time: 0.39s
Val loss: 1.1528 score: 0.5426 time: 0.23s
Test loss: 1.4306 score: 0.5312 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.2331;  Loss pred: 0.2331; Loss self: 0.0000; time: 0.36s
Val loss: 1.1196 score: 0.5426 time: 0.21s
Test loss: 1.3551 score: 0.5156 time: 0.22s
     INFO: Early stopping counter 11 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.2172;  Loss pred: 0.2172; Loss self: 0.0000; time: 0.38s
Val loss: 0.9181 score: 0.5581 time: 0.22s
Test loss: 1.0521 score: 0.5391 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.2081;  Loss pred: 0.2081; Loss self: 0.0000; time: 0.43s
Val loss: 0.8184 score: 0.5581 time: 0.36s
Test loss: 0.9008 score: 0.5469 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.1478;  Loss pred: 0.1478; Loss self: 0.0000; time: 0.40s
Val loss: 0.7075 score: 0.6047 time: 0.21s
Test loss: 0.7209 score: 0.5547 time: 0.20s
Epoch 19/1000, LR 0.000290
Train loss: 0.1271;  Loss pred: 0.1271; Loss self: 0.0000; time: 0.37s
Val loss: 0.6233 score: 0.6744 time: 0.21s
Test loss: 0.5648 score: 0.6719 time: 0.22s
Epoch 20/1000, LR 0.000290
Train loss: 0.1183;  Loss pred: 0.1183; Loss self: 0.0000; time: 0.38s
Val loss: 0.5861 score: 0.7054 time: 0.34s
Test loss: 0.5219 score: 0.6719 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 0.39s
Val loss: 0.6663 score: 0.6512 time: 0.22s
Test loss: 0.6294 score: 0.6250 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000290
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.38s
Val loss: 0.6945 score: 0.6589 time: 0.22s
Test loss: 0.6685 score: 0.6250 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000290
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.50s
Val loss: 0.5616 score: 0.7209 time: 0.22s
Test loss: 0.5048 score: 0.7109 time: 0.22s
Epoch 24/1000, LR 0.000290
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.39s
Val loss: 0.5801 score: 0.7209 time: 0.22s
Test loss: 0.5293 score: 0.7031 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000290
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.39s
Val loss: 0.5357 score: 0.7209 time: 0.22s
Test loss: 0.4707 score: 0.7656 time: 0.21s
Epoch 26/1000, LR 0.000290
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.46s
Val loss: 0.5966 score: 0.7054 time: 0.22s
Test loss: 0.5353 score: 0.7109 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000290
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.38s
Val loss: 0.6267 score: 0.7132 time: 0.24s
Test loss: 0.5405 score: 0.7500 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000290
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.39s
Val loss: 0.6757 score: 0.7287 time: 0.22s
Test loss: 0.5748 score: 0.7500 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.43s
Val loss: 0.7656 score: 0.7209 time: 0.21s
Test loss: 0.6769 score: 0.7266 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.36s
Val loss: 0.5570 score: 0.7984 time: 0.21s
Test loss: 0.4480 score: 0.8438 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000290
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.39s
Val loss: 0.4278 score: 0.9147 time: 0.25s
Test loss: 0.2785 score: 0.8984 time: 0.24s
Epoch 32/1000, LR 0.000290
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.47s
Val loss: 1.0441 score: 0.7364 time: 0.21s
Test loss: 0.9876 score: 0.7422 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.39s
Val loss: 1.2358 score: 0.7442 time: 0.21s
Test loss: 1.2911 score: 0.6953 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 4.40s
Val loss: 0.6331 score: 0.8527 time: 2.51s
Test loss: 0.5264 score: 0.8516 time: 0.34s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.37s
Val loss: 0.4698 score: 0.8992 time: 0.21s
Test loss: 0.3334 score: 0.8906 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.37s
Val loss: 0.5156 score: 0.8837 time: 0.21s
Test loss: 0.3698 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.38s
Val loss: 0.4581 score: 0.8837 time: 0.21s
Test loss: 0.3348 score: 0.8984 time: 0.33s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 2.33s
Val loss: 0.3801 score: 0.9070 time: 4.24s
Test loss: 0.2870 score: 0.8984 time: 0.21s
Epoch 39/1000, LR 0.000289
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.36s
Val loss: 0.4253 score: 0.9147 time: 0.20s
Test loss: 0.3044 score: 0.8906 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.36s
Val loss: 0.4819 score: 0.8915 time: 0.20s
Test loss: 0.3315 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.39s
Val loss: 0.5620 score: 0.8992 time: 0.22s
Test loss: 0.3637 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.38s
Val loss: 0.6006 score: 0.8992 time: 0.22s
Test loss: 0.3860 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.37s
Val loss: 0.7052 score: 0.8837 time: 0.22s
Test loss: 0.4658 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.38s
Val loss: 0.6622 score: 0.8837 time: 0.22s
Test loss: 0.4464 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.38s
Val loss: 0.5561 score: 0.8992 time: 0.26s
Test loss: 0.3604 score: 0.8984 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.39s
Val loss: 0.5989 score: 0.9070 time: 0.21s
Test loss: 0.3877 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.36s
Val loss: 0.5762 score: 0.8760 time: 0.25s
Test loss: 0.4086 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.39s
Val loss: 0.6664 score: 0.8760 time: 0.25s
Test loss: 0.4973 score: 0.8672 time: 0.24s
     INFO: Early stopping counter 10 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.38s
Val loss: 0.6711 score: 0.8915 time: 0.21s
Test loss: 0.4798 score: 0.8750 time: 0.34s
     INFO: Early stopping counter 11 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.37s
Val loss: 0.6817 score: 0.8992 time: 0.20s
Test loss: 0.4664 score: 0.8984 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.36s
Val loss: 0.7864 score: 0.8760 time: 0.22s
Test loss: 0.5564 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.37s
Val loss: 0.9038 score: 0.8760 time: 0.21s
Test loss: 0.6392 score: 0.8594 time: 0.23s
     INFO: Early stopping counter 14 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.48s
Val loss: 1.0926 score: 0.8605 time: 0.22s
Test loss: 0.8196 score: 0.8438 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.36s
Val loss: 0.7497 score: 0.8605 time: 0.21s
Test loss: 0.5054 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.36s
Val loss: 0.5358 score: 0.8915 time: 0.21s
Test loss: 0.3778 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.47s
Val loss: 0.5459 score: 0.9147 time: 0.22s
Test loss: 0.3971 score: 0.9141 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.39s
Val loss: 0.6057 score: 0.9147 time: 0.22s
Test loss: 0.4390 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.37s
Val loss: 0.6476 score: 0.8915 time: 0.21s
Test loss: 0.4624 score: 0.8906 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 037,   Train_Loss: 0.0320,   Val_Loss: 0.3801,   Val_Precision: 0.9492,   Val_Recall: 0.8615,   Val_accuracy: 0.9032,   Val_Score: 0.9070,   Val_Loss: 0.3801,   Test_Precision: 0.9636,   Test_Recall: 0.8281,   Test_accuracy: 0.8908,   Test_Score: 0.8984,   Test_loss: 0.2870


[0.2245012829080224, 0.22792299115099013, 0.20512031391263008, 0.2242936238180846, 7.2249776241369545, 0.21327387704513967, 0.2106182100251317, 0.22093760687857866, 0.2225421688053757, 0.23338529095053673, 0.21402886603027582, 0.20624919491820037, 0.233391071902588, 0.2650288420263678, 0.2153807000722736, 0.21314218686893582, 0.22042002296075225, 0.21792708593420684, 0.2130003219936043, 0.2227448031771928, 0.33516552997753024, 0.20696760807186365, 0.21072436682879925, 0.39488415513187647, 0.2187357449438423, 0.22380926390178502, 0.2245597168803215, 0.2287372089922428, 0.2158951999153942, 0.21707792486995459, 0.2289484478533268, 0.22409966215491295, 0.2077743890695274, 0.22351776203140616, 0.3764821318909526, 0.32813476608134806, 0.23536612512543797, 0.22907703882083297, 0.24188316706568003, 0.22371618705801666, 0.21007584687322378, 0.22351218201220036, 0.22010875190608203, 0.22188499313779175, 0.21536617213860154, 0.2281198250129819, 0.22857814491726458, 0.22416185308247805, 0.21244094404391944, 0.21991211688145995, 0.21742518316023052, 0.23805435211397707, 0.22466951282694936, 0.22814419399946928, 0.24097152892500162, 0.24409681814722717, 0.22617881000041962, 0.22247378784231842, 0.2251824550330639, 0.2069713280070573, 0.2099206750281155, 0.2313416430260986, 0.2253890719730407, 0.22331489599309862, 0.22948856907896698, 0.35208185808733106, 0.2197996738832444, 0.21798768802545965, 0.22461513779126108, 0.22782821278087795, 0.23281795205548406, 0.21619872795417905, 0.21336618205532432, 0.21744590415619314, 0.21122178109362721, 0.23066761693917215, 0.23189513408578932, 0.23107773391529918, 0.22604509093798697, 0.2158712709788233, 0.2313206670805812, 0.2308459091000259, 0.20131087605841458, 0.2018290450796485, 0.2003507239278406, 0.21542075811885297, 0.24029498593881726, 0.2183850109577179, 0.20583532098680735, 0.20681854290887713, 0.21665232512168586, 0.21236221306025982, 0.21133778290823102, 0.20408970792777836, 0.19848841591738164, 0.22760923393070698, 0.278655719012022, 0.22775147901847959, 0.21783598815090954, 0.2416678979061544, 0.21288683684542775, 0.21464314893819392, 0.22933309897780418, 0.21841540304012597, 0.21634008409455419, 0.22122335992753506, 0.21226745401509106, 0.2401758141350001, 0.21854330482892692, 0.22054480598308146, 0.22014905093237758, 0.21876181988045573, 0.2141285629477352, 0.20330396993085742, 0.22598235378973186, 0.21575847500935197, 0.2250464460812509, 0.22468901495449245, 0.21451542805880308, 0.21801876602694392, 0.21911091799847782, 0.21722148614935577, 0.21274854009971023, 0.20020523015409708, 0.21276517305523157, 0.21735401800833642, 0.21436528395861387, 0.23015968198888004, 0.23603555094450712, 0.2143136220984161, 0.2329092570580542, 0.2174825870897621, 0.22117981291376054, 0.21508346707560122, 0.23847009800374508, 0.21616949792951345, 0.2198590619955212, 0.20430263690650463, 0.20965068181976676, 0.23158707283437252, 0.21767520904541016, 0.22923169401474297, 0.2073837521020323, 0.22351960302330554, 0.21422863891348243, 0.21121145994402468, 0.21795076178386807, 0.22087656799703836, 0.21883223904296756, 0.21431560185737908, 0.23506928118877113, 0.219104876043275, 0.21537589095532894, 0.20581917208619416, 0.22692825295962393, 0.2450237579178065, 0.2052226138766855, 0.21039159805513918, 0.34421799913980067, 0.2203815181273967, 0.2103305710479617, 0.33161876699887216, 0.21243343292735517, 0.20502706104889512, 0.21387887792661786, 0.2159297140315175, 0.21568868798203766, 0.2188019729219377, 0.21626477502286434, 0.22366240504197776, 0.2013309309259057, 0.2066312930546701, 0.24578359816223383, 0.3486402400303632, 0.22545655188150704, 0.20573061308823526, 0.23223448591306806, 0.2161946629639715, 0.20868665911257267, 0.2076358301565051, 0.21861349395476282, 0.1999183939769864, 0.2075012968853116]
[0.0017403200225428093, 0.0017668448926433343, 0.001590079952811086, 0.0017387102621556946, 0.056007578481681815, 0.0016532858685669742, 0.0016326993025204007, 0.0017126946269657261, 0.0017251330915145403, 0.001809188301942145, 0.0016591384963587272, 0.0015988309683581425, 0.001809233115523938, 0.002054487147491223, 0.001669617830017625, 0.0016522650144878746, 0.0017086823485329631, 0.001689357255303929, 0.0016511652867721265, 0.0017267039005983938, 0.002598182402926591, 0.0016044000625725864, 0.001633522223479064, 0.0030611174816424533, 0.0016956259297972271, 0.0017349555341223644, 0.0017407729990722596, 0.0017731566588545954, 0.0016736062008945287, 0.0016827746113949969, 0.0017747941694056342, 0.0017372066833714183, 0.0016106541788335456, 0.001732695829700823, 0.002918466138689555, 0.0025436803572197525, 0.00182454360562355, 0.0017757909986111083, 0.0018750633105866668, 0.0017342340082016794, 0.0016284949370017347, 0.0017326525737379873, 0.001706269394620791, 0.0017200387064945097, 0.0016695052103767561, 0.0017683707365347434, 0.001771923604009803, 0.0017376887835851011, 0.0016468290235962747, 0.0017047450921043408, 0.001685466536125818, 0.001845382574526954, 0.001741624130441468, 0.0017685596434067385, 0.0018679963482558265, 0.001892223396490133, 0.0017533241085303847, 0.0017246030065296001, 0.0017456004266128984, 0.0016044288992795141, 0.0016272920544815157, 0.0017933460699697567, 0.001747202108318145, 0.0017311232247527026, 0.0017789811556509069, 0.0027293167293591554, 0.001703873440955383, 0.0016898270389570514, 0.0017412026185369075, 0.0017661101765959531, 0.0018047903260115044, 0.0016759591314277446, 0.0016540014112815839, 0.0016856271640014972, 0.001637378148012614, 0.0017881210615439702, 0.001797636698339452, 0.001791300262909296, 0.0017522875266510617, 0.0016734207052621962, 0.0017931834657409394, 0.00178950317131803, 0.0015605494268094153, 0.0015645662409275076, 0.001553106387037524, 0.0016699283575104882, 0.0018627518289830796, 0.0016929070616877357, 0.001595622643308584, 0.0016032445186734661, 0.0016794753885402004, 0.001646218705893487, 0.0016382773868855117, 0.0015820907591300648, 0.0015386698908324158, 0.0017644126661295115, 0.002160121852806372, 0.001765515341228524, 0.0016886510709372832, 0.0018733945574120495, 0.0016502855569413004, 0.0016639003793658444, 0.0017777759610682494, 0.0016931426592257826, 0.0016770549154616603, 0.0017149097668801166, 0.0016454841396518686, 0.0018618280165503885, 0.0016941341459606738, 0.001709649658783577, 0.0017065817901734696, 0.001695828061088804, 0.0016599113406801178, 0.0015759997669058715, 0.001751801192168464, 0.0016725463179019533, 0.0017445460936531079, 0.0017417753097247477, 0.0016629102950294812, 0.0016900679536972396, 0.0016985342480502156, 0.0016838874895298897, 0.0016492134891450405, 0.001551978528326334, 0.0016493424267847408, 0.0016980782656901283, 0.0016747287809266709, 0.0017981225155381253, 0.0018440277417539619, 0.0016743251726438757, 0.0018196035707660485, 0.0016990827116387663, 0.0017279672883887542, 0.0016803395865281345, 0.0018630476406542584, 0.0016888242025743239, 0.0017176489218400093, 0.0015961143508320674, 0.0016378959517169278, 0.0018092740065185353, 0.0017005875706672668, 0.0017908726094901795, 0.0016201855632971274, 0.0017462468986195745, 0.0016736612415115815, 0.0016500895308126928, 0.0017027403264364693, 0.0017255981874768622, 0.001709626867523184, 0.001674340639510774, 0.0018364787592872744, 0.001711756844088086, 0.0016826241480885074, 0.0016079622819233919, 0.001772876976247062, 0.0019142481087328633, 0.0016033016709116055, 0.0016436843598057749, 0.0026892031182796927, 0.0017217306103702867, 0.0016432075863122009, 0.0025907716171786888, 0.0016596361947449623, 0.0016017739144444931, 0.001670928733801702, 0.0016869508908712305, 0.0016850678748596692, 0.0017093904134526383, 0.0016895685548661277, 0.0017473625393904513, 0.0015728978978586383, 0.0016143069769896101, 0.0019201843606424518, 0.0027237518752372125, 0.0017613793115742737, 0.001607270414751838, 0.0018143319211958442, 0.0016890208044060273, 0.001630364524316974, 0.0016221549230976962, 0.0017079179215215845, 0.0015618624529452063, 0.001621103881916497]
[574.6069613902873, 565.9806382347032, 628.8991935481674, 575.1389531457507, 17.854726576459043, 604.8560742049857, 612.4826527801526, 583.8752479603661, 579.665421131116, 552.7340625221323, 602.7224382983559, 625.4569868801774, 552.7203716423292, 486.7394771591152, 598.9394590913322, 605.2297853138006, 585.2462869172716, 591.9411047369563, 605.6328872773885, 579.1380905860277, 384.8844480178145, 623.2859392915649, 612.1741018436879, 326.6780860247959, 589.752717522777, 576.3836480719114, 574.4574396161632, 563.9659614994025, 597.5121264880044, 594.2566480552103, 563.4456193502668, 575.6367446499157, 620.8657408533293, 577.1353418520466, 342.64574350998583, 393.13115626406847, 548.082269405802, 563.1293326647818, 533.3153255967243, 576.6234517779718, 614.063929385698, 577.1497501328969, 586.0739242892208, 581.3822655410063, 598.97986168868, 565.4922801762575, 564.3584168849233, 575.477041370352, 607.2275783774097, 586.5979639018043, 593.307537448107, 541.8930544829385, 574.1767023786696, 565.43187770231, 535.3329523013862, 528.4788264720172, 570.3452060772655, 579.8435907938541, 572.8687875840892, 623.274736854379, 614.5178410021906, 557.6168575298265, 572.3436317064653, 577.6596291363683, 562.1195012794346, 366.39206774466237, 586.8980500331572, 591.776540998653, 574.3156995940409, 566.2160907353052, 554.0809841384454, 596.6732608498055, 604.5944055302598, 593.2509996018977, 610.7324695970574, 559.2462509985428, 556.2859285882066, 558.2537002344178, 570.6825990544946, 597.5783596171755, 557.6674217140421, 558.8143212193728, 640.7999534141809, 639.1547854229419, 643.8708953527981, 598.828084751366, 536.8401654158749, 590.6998810691089, 626.7145958310432, 623.7351747364188, 595.4240275406476, 607.4527014059465, 610.3972428631729, 632.0749895220071, 649.9119830433564, 566.7608373009707, 462.93684715092667, 566.4068595995069, 592.1886511728865, 533.7903839015221, 605.955736444447, 600.9975190829189, 562.5005748188366, 590.6176863190409, 596.2833958390204, 583.1210593775261, 607.7238764583703, 537.1065378277038, 590.2720291567827, 584.9151578291801, 585.9666414806597, 589.6824229680168, 602.4418145069535, 634.5178603441547, 570.8410317737893, 597.8907664897453, 573.2150062633104, 574.1268660870097, 601.3553485049963, 591.6921848097127, 588.7429123951558, 593.8639049329724, 606.3496367098019, 644.3388112324002, 606.302235218322, 588.9010066291513, 597.1116107807463, 556.1356311145069, 542.2911908303732, 597.2555488853641, 549.5702558876616, 588.5528662907171, 578.7146589635106, 595.1178011976549, 536.7549268084329, 592.1279423137535, 582.1911493582535, 626.5215267807672, 610.5393929032842, 552.7078797336137, 588.0320527143602, 558.3870090484421, 617.2132517740556, 572.656707817497, 597.4924764923405, 606.0277223306093, 587.2886102914007, 579.5091854275655, 584.9229554099992, 597.2500316854223, 544.5203190850373, 584.1951229543562, 594.3097875636807, 621.9051349909979, 564.0549307131639, 522.3983220555198, 623.7129407040535, 608.3893139423462, 371.8573703869974, 580.8109549640483, 608.5658369215959, 385.9853926796469, 602.5416914661052, 624.3078320742958, 598.4695695098839, 592.7854838047758, 593.4479049298112, 585.0038657817164, 591.8670758401008, 572.2910829648662, 635.7691757115397, 619.4608672662858, 520.7833271100186, 367.14063754904606, 567.7368829240005, 622.1728408746948, 551.1670650323399, 592.0590186878526, 613.3597640803308, 616.4639306401032, 585.5082304594004, 640.2612458698259, 616.8636144512731]
Elapsed: 0.2641336851628609~0.5168098536908802
Time per graph: 0.0020518533692299324~0.00400598298499951
Speed: 573.436855509972~67.14326341467833
Total Time: 0.2084
best val loss: 0.3801417750876772 test_score: 0.8984

Testing...
Test loss: 0.2785 score: 0.8984 time: 0.20s
test Score 0.8984
Epoch Time List: [3.412392775993794, 0.7508424730040133, 0.7450332019943744, 0.7593655700329691, 7.844196134014055, 0.8954572926741093, 0.7589008309878409, 0.7793925509322435, 0.9048856711015105, 0.7820957801304758, 0.7477481586392969, 0.8705314833205193, 0.7846274049952626, 0.863914682995528, 0.8645233667921275, 0.7522227680310607, 0.7697230761405081, 0.8701152172870934, 0.7373735047876835, 0.78839713614434, 0.8927203421480954, 0.7660412658005953, 0.7988094510510564, 0.9386240029707551, 0.764566752128303, 0.796651936834678, 0.7472426318563521, 0.9018570419866592, 0.7844643052667379, 0.7564555609133095, 0.8681054338812828, 0.788407688960433, 0.7898249628487974, 0.8840459862258285, 0.9286172578576952, 0.8686953668948263, 0.8485953831113875, 0.7906710270326585, 0.7954516331665218, 0.8842835309915245, 0.7739354530349374, 0.7906618833076209, 0.8846431351266801, 0.7719594999216497, 0.7658592369407415, 0.8355820700526237, 0.7841517196502537, 0.8079335778020322, 0.8857866732869297, 0.7532671960070729, 0.7835823623463511, 0.9018026208505034, 0.7690251977182925, 0.8014025806915015, 0.8172864550724626, 0.785912818973884, 0.7852424751035869, 0.7726647749077529, 0.7664156118407845, 0.7497097770683467, 0.7755272381473333, 0.8079256240744144, 0.9226731720846146, 0.8466604221612215, 0.8490250559989363, 0.9640092838089913, 0.8539954731240869, 0.8485749687533826, 0.9637054330669343, 0.8555922689847648, 0.8670871907379478, 0.8252947942819446, 0.7864811618346721, 0.8313865850213915, 0.8922625419218093, 0.8346918001770973, 0.8441243420820683, 0.904949571006, 0.8281701719388366, 0.798797091236338, 1.1031175169628114, 0.8222302468493581, 0.746981190983206, 0.7533673229627311, 0.7772801530081779, 0.7655749360565096, 0.8363558507990092, 0.8989254548214376, 0.8850318400654942, 0.7462576772086322, 0.8932186020538211, 0.7903160059358925, 0.7724112577270716, 0.8964198341127485, 0.7387860859744251, 0.7796477768570185, 0.8453684740234166, 7.037920695031062, 0.778451960766688, 1.0040929990354925, 0.7717378339730203, 0.7794884808827192, 0.9526469262782484, 0.79415238276124, 0.7950714307371527, 0.9184294128790498, 0.7925028237514198, 0.8625418969895691, 0.8676912190858275, 0.8046548676211387, 0.8038576426915824, 0.7926903252955526, 0.7967822179198265, 0.7608418979216367, 0.8239214988425374, 0.7783924129325897, 0.808261978905648, 0.829949646955356, 0.7912889642175287, 0.7969363580923527, 0.7971011931076646, 0.8010706701315939, 0.877561752917245, 0.7704054240603, 0.7684781302232295, 0.7999702605884522, 1.0248016736004502, 0.818846033886075, 0.8550652449484915, 0.8783340018708259, 0.8073930630926043, 0.8079322869889438, 0.9211209190543741, 1.0311703789047897, 0.8418712853454053, 0.8288452192209661, 0.8185182930901647, 0.8087929571047425, 0.8267243709415197, 0.7918987821321934, 0.8093554140068591, 1.0118305617943406, 0.8169777009170502, 0.7981188849080354, 0.9203243332449347, 0.8128720317035913, 0.8150725571904331, 0.9350486556068063, 0.822951695881784, 0.8193559348583221, 0.9024120990652591, 0.839561871252954, 0.8223691789899021, 0.8420919908676296, 0.7905199769884348, 0.8772653962951154, 0.8751699640415609, 0.8036091600079089, 7.257946158060804, 0.7926603897940367, 0.7853878489695489, 0.907815745100379, 6.77749487105757, 0.7570935559924692, 0.7746168277226388, 0.8198280383367091, 0.8042404891457409, 0.8023993079550564, 0.814555553952232, 0.8513989900238812, 0.797535581048578, 0.8187124792020768, 0.8676923960447311, 0.926312041701749, 0.7852985309436917, 0.7829612630885094, 0.8111903939861804, 0.9043603909667581, 0.7799785169772804, 0.7789292128290981, 0.9011309931520373, 0.8012712900526822, 0.7792571762111038]
Total Epoch List: [59, 66, 58]
Total Time List: [0.2256447810214013, 0.21342759998515248, 0.20838201884180307]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c75709330>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7163;  Loss pred: 0.7163; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0123 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0505 score: 0.5039 time: 0.24s
Epoch 2/1000, LR 0.000015
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4961 time: 0.22s
Test loss: 0.7340 score: 0.5039 time: 0.34s
Epoch 3/1000, LR 0.000045
Train loss: 0.7299;  Loss pred: 0.7299; Loss self: 0.0000; time: 0.35s
Val loss: 0.6465 score: 0.5349 time: 0.23s
Test loss: 0.6672 score: 0.5504 time: 0.23s
Epoch 4/1000, LR 0.000075
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.32s
Val loss: 0.6583 score: 0.5039 time: 0.21s
Test loss: 0.6644 score: 0.5426 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.33s
Val loss: 0.6604 score: 0.5271 time: 0.22s
Test loss: 0.6660 score: 0.5504 time: 0.35s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.34s
Val loss: 0.6569 score: 0.5504 time: 0.23s
Test loss: 0.6616 score: 0.5271 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.31s
Val loss: 0.6530 score: 0.5581 time: 0.22s
Test loss: 0.6647 score: 0.4806 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.5101;  Loss pred: 0.5101; Loss self: 0.0000; time: 0.34s
Val loss: 0.6524 score: 0.5504 time: 0.21s
Test loss: 0.6615 score: 0.4806 time: 0.34s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.4818;  Loss pred: 0.4818; Loss self: 0.0000; time: 0.31s
Val loss: 0.6456 score: 0.5504 time: 0.23s
Test loss: 0.6558 score: 0.4961 time: 0.21s
Epoch 10/1000, LR 0.000255
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.35s
Val loss: 0.6355 score: 0.5504 time: 0.22s
Test loss: 0.6487 score: 0.5039 time: 0.21s
Epoch 11/1000, LR 0.000285
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 0.33s
Val loss: 0.6313 score: 0.5426 time: 0.37s
Test loss: 0.6442 score: 0.5271 time: 0.35s
Epoch 12/1000, LR 0.000285
Train loss: 0.3293;  Loss pred: 0.3293; Loss self: 0.0000; time: 0.33s
Val loss: 0.6404 score: 0.5116 time: 0.22s
Test loss: 0.6504 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.3023;  Loss pred: 0.3023; Loss self: 0.0000; time: 0.39s
Val loss: 0.6580 score: 0.5116 time: 0.29s
Test loss: 0.6589 score: 0.5116 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.2660;  Loss pred: 0.2660; Loss self: 0.0000; time: 0.40s
Val loss: 0.6812 score: 0.5116 time: 0.23s
Test loss: 0.6734 score: 0.5039 time: 0.36s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.2593;  Loss pred: 0.2593; Loss self: 0.0000; time: 0.35s
Val loss: 0.6885 score: 0.5116 time: 0.22s
Test loss: 0.6775 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.2327;  Loss pred: 0.2327; Loss self: 0.0000; time: 0.33s
Val loss: 0.6915 score: 0.5116 time: 0.22s
Test loss: 0.6791 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.2173;  Loss pred: 0.2173; Loss self: 0.0000; time: 0.84s
Val loss: 0.7311 score: 0.5194 time: 5.71s
Test loss: 0.7154 score: 0.5116 time: 0.37s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.2143;  Loss pred: 0.2143; Loss self: 0.0000; time: 0.32s
Val loss: 0.7699 score: 0.5194 time: 0.21s
Test loss: 0.7515 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.32s
Val loss: 0.7791 score: 0.5194 time: 0.22s
Test loss: 0.7623 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.2041;  Loss pred: 0.2041; Loss self: 0.0000; time: 0.33s
Val loss: 0.7652 score: 0.5194 time: 0.22s
Test loss: 0.7529 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.2049;  Loss pred: 0.2049; Loss self: 0.0000; time: 0.39s
Val loss: 0.7218 score: 0.5426 time: 0.21s
Test loss: 0.7150 score: 0.5504 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.1925;  Loss pred: 0.1925; Loss self: 0.0000; time: 0.30s
Val loss: 0.6713 score: 0.5581 time: 0.21s
Test loss: 0.6718 score: 0.5659 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.1442;  Loss pred: 0.1442; Loss self: 0.0000; time: 0.32s
Val loss: 0.6328 score: 0.5659 time: 0.22s
Test loss: 0.6413 score: 0.5891 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.1596;  Loss pred: 0.1596; Loss self: 0.0000; time: 0.42s
Val loss: 0.6023 score: 0.5736 time: 0.27s
Test loss: 0.6172 score: 0.5891 time: 0.21s
Epoch 25/1000, LR 0.000285
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.30s
Val loss: 0.5566 score: 0.6667 time: 0.22s
Test loss: 0.5752 score: 0.6124 time: 0.21s
Epoch 26/1000, LR 0.000285
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 0.32s
Val loss: 0.5270 score: 0.6744 time: 0.23s
Test loss: 0.5498 score: 0.6357 time: 0.22s
Epoch 27/1000, LR 0.000285
Train loss: 0.1133;  Loss pred: 0.1133; Loss self: 0.0000; time: 0.34s
Val loss: 0.4802 score: 0.7209 time: 0.25s
Test loss: 0.5153 score: 0.6512 time: 0.21s
Epoch 28/1000, LR 0.000285
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.32s
Val loss: 0.4484 score: 0.7597 time: 0.23s
Test loss: 0.4988 score: 0.7752 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.33s
Val loss: 0.4352 score: 0.7829 time: 0.22s
Test loss: 0.4818 score: 0.7907 time: 0.22s
Epoch 30/1000, LR 0.000285
Train loss: 0.0975;  Loss pred: 0.0975; Loss self: 0.0000; time: 0.36s
Val loss: 0.4680 score: 0.7829 time: 0.31s
Test loss: 0.5045 score: 0.7674 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0865;  Loss pred: 0.0865; Loss self: 0.0000; time: 0.33s
Val loss: 0.5077 score: 0.7597 time: 0.21s
Test loss: 0.5378 score: 0.7597 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 0.33s
Val loss: 0.5086 score: 0.7752 time: 0.22s
Test loss: 0.5398 score: 0.7907 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.33s
Val loss: 0.4326 score: 0.7984 time: 0.22s
Test loss: 0.4693 score: 0.7907 time: 0.23s
Epoch 34/1000, LR 0.000285
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.35s
Val loss: 0.4149 score: 0.8140 time: 0.22s
Test loss: 0.4510 score: 0.7907 time: 0.22s
Epoch 35/1000, LR 0.000285
Train loss: 0.0675;  Loss pred: 0.0675; Loss self: 0.0000; time: 0.35s
Val loss: 0.3999 score: 0.8372 time: 0.23s
Test loss: 0.4379 score: 0.8062 time: 0.23s
Epoch 36/1000, LR 0.000285
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.33s
Val loss: 0.3565 score: 0.8992 time: 0.22s
Test loss: 0.3940 score: 0.8217 time: 0.21s
Epoch 37/1000, LR 0.000285
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.32s
Val loss: 0.2962 score: 0.9302 time: 0.22s
Test loss: 0.3479 score: 0.8682 time: 0.21s
Epoch 38/1000, LR 0.000284
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.32s
Val loss: 0.2826 score: 0.9225 time: 0.23s
Test loss: 0.3762 score: 0.8217 time: 0.21s
Epoch 39/1000, LR 0.000284
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.32s
Val loss: 0.2737 score: 0.9302 time: 0.22s
Test loss: 0.3354 score: 0.8605 time: 0.21s
Epoch 40/1000, LR 0.000284
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.32s
Val loss: 0.2988 score: 0.9225 time: 0.22s
Test loss: 0.3349 score: 0.8605 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.32s
Val loss: 0.3120 score: 0.9147 time: 0.22s
Test loss: 0.3451 score: 0.8450 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.31s
Val loss: 0.3136 score: 0.9225 time: 0.22s
Test loss: 0.3427 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.32s
Val loss: 0.3088 score: 0.9147 time: 0.22s
Test loss: 0.3288 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.32s
Val loss: 0.3079 score: 0.9225 time: 0.21s
Test loss: 0.3239 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.32s
Val loss: 0.2996 score: 0.9225 time: 0.22s
Test loss: 0.3306 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.33s
Val loss: 0.3419 score: 0.9225 time: 0.22s
Test loss: 0.3731 score: 0.8527 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.35s
Val loss: 0.3381 score: 0.9147 time: 0.22s
Test loss: 0.3892 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.35s
Val loss: 0.3210 score: 0.9302 time: 0.22s
Test loss: 0.4046 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.36s
Val loss: 0.3257 score: 0.9225 time: 0.24s
Test loss: 0.4379 score: 0.8295 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.46s
Val loss: 0.3468 score: 0.9225 time: 0.21s
Test loss: 0.4554 score: 0.8527 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.32s
Val loss: 0.4778 score: 0.9302 time: 0.21s
Test loss: 0.5282 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.34s
Val loss: 0.3972 score: 0.9225 time: 0.22s
Test loss: 0.4813 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.43s
Val loss: 0.3548 score: 0.9380 time: 0.22s
Test loss: 0.4732 score: 0.8295 time: 0.23s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.32s
Val loss: 0.3704 score: 0.9225 time: 0.23s
Test loss: 0.5525 score: 0.8217 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.33s
Val loss: 0.3855 score: 0.9225 time: 0.21s
Test loss: 0.5714 score: 0.8295 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.34s
Val loss: 0.3912 score: 0.9380 time: 0.27s
Test loss: 0.5453 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.33s
Val loss: 0.4295 score: 0.9302 time: 0.24s
Test loss: 0.5984 score: 0.8450 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.32s
Val loss: 0.4930 score: 0.9225 time: 0.22s
Test loss: 0.6325 score: 0.8527 time: 0.22s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.33s
Val loss: 0.4614 score: 0.9225 time: 0.22s
Test loss: 0.6306 score: 0.8527 time: 0.35s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.0493,   Val_Loss: 0.2737,   Val_Precision: 0.9508,   Val_Recall: 0.9062,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2737,   Test_Precision: 0.9434,   Test_Recall: 0.7692,   Test_accuracy: 0.8475,   Test_Score: 0.8605,   Test_loss: 0.3354


[0.2476020900066942, 0.3409118049312383, 0.23448326298967004, 0.22234646207652986, 0.35054287197999656, 0.20937599008902907, 0.2196406559087336, 0.34501197119243443, 0.21942159300670028, 0.21773542487062514, 0.36032102489843965, 0.2169563800562173, 0.2291356499772519, 0.3676870730705559, 0.21877310192212462, 0.21947248512879014, 0.3715999948326498, 0.2171907548326999, 0.21393226413056254, 0.21230623591691256, 0.20541736599989235, 0.20213541202247143, 0.21693538618274033, 0.21264714398421347, 0.2107442980632186, 0.2212368450127542, 0.21837228490039706, 0.2229541928973049, 0.22731508710421622, 0.20734008098952472, 0.2135990709066391, 0.22257497697137296, 0.23991382704116404, 0.22709121205843985, 0.23305282485671341, 0.21931374911218882, 0.21632506512105465, 0.21640024380758405, 0.2201271839439869, 0.2452706629410386, 0.21792660793289542, 0.24547227704897523, 0.2230546090286225, 0.21012304397299886, 0.2242493089288473, 0.22275575203821063, 0.22337441588751972, 0.2235856200568378, 0.2191774300299585, 0.20706631219945848, 0.2059244450647384, 0.22271724720485508, 0.2360682941507548, 0.21061486494727433, 0.21140997111797333, 0.2111084358766675, 0.217817610129714, 0.2219495470635593, 0.3536405360791832]
[0.001919396046563521, 0.0026427271700095994, 0.0018176997130982173, 0.0017236159850893787, 0.0027173866044960973, 0.0016230696906126285, 0.001702640743478555, 0.002674511404592515, 0.001700942581447289, 0.0016878715106250011, 0.002793186239522788, 0.0016818324035365684, 0.0017762453486608674, 0.0028502873881438443, 0.00169591551877616, 0.001701337094021629, 0.0028806201149817814, 0.0016836492622689914, 0.0016583896444229654, 0.0016457847745497097, 0.0015923826821697081, 0.0015669411784687707, 0.0016816696603313204, 0.0016484274727458408, 0.001633676729172237, 0.001715014302424451, 0.0016928084100805974, 0.0017283270767232937, 0.0017621324581722187, 0.0016072874495311994, 0.0016558067512142565, 0.0017253874183827361, 0.0018597971088462329, 0.001760396992701084, 0.001806611045400879, 0.0017001065822650297, 0.0016769384893105012, 0.001677521269826233, 0.0017064122786355574, 0.001901322968535183, 0.0016893535498674064, 0.0019028858685967071, 0.0017291054963459109, 0.0016288608059922393, 0.0017383667358825373, 0.0017267887754900048, 0.0017315846192830985, 0.0017332218609057194, 0.0016990498451934766, 0.0016051652108485153, 0.001596313527633631, 0.0017264902884097293, 0.0018299867763624403, 0.0016326733716842971, 0.001638836985410646, 0.001636499502919903, 0.0016885086056566978, 0.0017205391245237154, 0.0027413995044897922]
[520.9972177396094, 378.396987531773, 550.1458754678068, 580.175635785917, 368.0006364738213, 616.1164895036328, 587.3229592503259, 373.90006947917976, 587.9093221060557, 592.4621594150318, 358.0140793514895, 594.5895666519407, 562.985288464745, 350.84181481475696, 589.6520132804962, 587.7729954363101, 347.14747522559895, 593.9479334623039, 602.9945998293716, 607.6128637619717, 627.9897484425323, 638.1860491899314, 594.6471079242646, 606.6387612032858, 612.1162052095135, 583.0855163052213, 590.734305220275, 578.594187100212, 567.4942285764688, 622.1662467977784, 603.9352111994154, 579.579976847944, 537.6930608416595, 568.0536857005415, 553.5225761769349, 588.1984167532092, 596.324794483765, 596.1176278281024, 586.0248502194307, 525.9495711927468, 591.942403103535, 525.5175922544714, 578.3337119182623, 613.9260005036701, 575.2526088761806, 579.1096248678322, 577.505707121616, 576.9601818185215, 588.5642512660519, 622.9888320787768, 626.4433538205969, 579.2097451767888, 546.4520361112947, 612.4923804988507, 610.1888161557621, 611.0603750357168, 592.2386161668854, 581.2131707710064, 364.7771871127233]
Elapsed: 0.23629237855074264~0.04494722482626469
Time per graph: 0.001831723864734439~0.0003484280994284084
Speed: 560.3765204221002~76.44893038144282
Total Time: 0.3541
best val loss: 0.2737314658340558 test_score: 0.8605

Testing...
Test loss: 0.4732 score: 0.8295 time: 0.21s
test Score 0.8295
Epoch Time List: [0.8022106168791652, 0.891143934102729, 0.8067530700936913, 0.7503123451024294, 0.8931535731535405, 0.7718180490192026, 0.7452568998560309, 0.8918661980424076, 0.7565448058303446, 0.7839048330206424, 1.055396526819095, 0.7573611331172287, 0.9043581888545305, 0.9907316479366273, 0.7808056941721588, 0.7670201398432255, 6.909710790961981, 0.7433194338809699, 0.7411446901969612, 0.7506542962510139, 0.7945610580500215, 0.7065277670044452, 0.7512301800306886, 0.8944106369744986, 0.7254131860099733, 0.7665402796119452, 0.8038276529405266, 0.7628954499959946, 0.7738526430912316, 0.8708459238987416, 0.7464304461609572, 0.7712018939200789, 0.7889771505724639, 0.7906160203274339, 0.8054529200308025, 0.7594503429718316, 0.7473466149531305, 0.7643948260229081, 0.7547983308322728, 0.7799405292607844, 0.7468008301220834, 0.7764104497618973, 0.7548283857759088, 0.7297644186764956, 0.7559147910214961, 0.7690943689085543, 0.7849885451141745, 0.7933449021074921, 0.8147073599975556, 0.8713222581427544, 0.7277458540629596, 0.7774857508484274, 0.8835665581282228, 0.7557803560048342, 0.7445399449206889, 0.818081091856584, 0.7849581630434841, 0.7545527312904596, 0.895152160897851]
Total Epoch List: [59]
Total Time List: [0.3541039580013603]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c7570aec0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1286;  Loss pred: 1.1286; Loss self: 0.0000; time: 0.32s
Val loss: 1.9856 score: 0.3178 time: 0.27s
Test loss: 1.6066 score: 0.4031 time: 0.33s
Epoch 2/1000, LR 0.000015
Train loss: 1.0734;  Loss pred: 1.0734; Loss self: 0.0000; time: 0.32s
Val loss: 1.1820 score: 0.3101 time: 0.23s
Test loss: 1.0021 score: 0.3101 time: 0.22s
Epoch 3/1000, LR 0.000045
Train loss: 1.0041;  Loss pred: 1.0041; Loss self: 0.0000; time: 0.33s
Val loss: 0.7898 score: 0.3798 time: 0.23s
Test loss: 0.7411 score: 0.4341 time: 0.22s
Epoch 4/1000, LR 0.000075
Train loss: 0.8515;  Loss pred: 0.8515; Loss self: 0.0000; time: 0.33s
Val loss: 0.6622 score: 0.4884 time: 0.25s
Test loss: 0.6611 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000105
Train loss: 0.7393;  Loss pred: 0.7393; Loss self: 0.0000; time: 0.62s
Val loss: 0.6571 score: 0.4884 time: 0.22s
Test loss: 0.6561 score: 0.5194 time: 0.22s
Epoch 6/1000, LR 0.000135
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.33s
Val loss: 0.6615 score: 0.4961 time: 0.21s
Test loss: 0.6537 score: 0.5426 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.32s
Val loss: 0.6617 score: 0.5271 time: 0.23s
Test loss: 0.6491 score: 0.5659 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.33s
Val loss: 0.6552 score: 0.5891 time: 0.28s
Test loss: 0.6441 score: 0.5969 time: 0.39s
Epoch 9/1000, LR 0.000225
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.32s
Val loss: 0.6447 score: 0.6124 time: 0.24s
Test loss: 0.6387 score: 0.5969 time: 0.26s
Epoch 10/1000, LR 0.000255
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 0.36s
Val loss: 0.6334 score: 0.6357 time: 0.23s
Test loss: 0.6317 score: 0.6202 time: 0.22s
Epoch 11/1000, LR 0.000285
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.34s
Val loss: 0.6233 score: 0.6744 time: 0.23s
Test loss: 0.6248 score: 0.6589 time: 0.23s
Epoch 12/1000, LR 0.000285
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.44s
Val loss: 0.6147 score: 0.6667 time: 0.22s
Test loss: 0.6196 score: 0.6822 time: 0.22s
Epoch 13/1000, LR 0.000285
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.33s
Val loss: 0.6105 score: 0.6667 time: 0.22s
Test loss: 0.6146 score: 0.6822 time: 0.22s
Epoch 14/1000, LR 0.000285
Train loss: 0.4888;  Loss pred: 0.4888; Loss self: 0.0000; time: 0.32s
Val loss: 0.6083 score: 0.6667 time: 0.22s
Test loss: 0.6084 score: 0.6899 time: 0.23s
Epoch 15/1000, LR 0.000285
Train loss: 0.4625;  Loss pred: 0.4625; Loss self: 0.0000; time: 0.34s
Val loss: 0.6088 score: 0.6667 time: 0.24s
Test loss: 0.6035 score: 0.6899 time: 0.35s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.4619;  Loss pred: 0.4619; Loss self: 0.0000; time: 0.33s
Val loss: 0.6123 score: 0.6667 time: 0.21s
Test loss: 0.6039 score: 0.7209 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.4422;  Loss pred: 0.4422; Loss self: 0.0000; time: 0.34s
Val loss: 0.6258 score: 0.6667 time: 0.21s
Test loss: 0.6170 score: 0.7132 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.4325;  Loss pred: 0.4325; Loss self: 0.0000; time: 0.35s
Val loss: 0.6670 score: 0.6279 time: 0.22s
Test loss: 0.6522 score: 0.6822 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.3791;  Loss pred: 0.3791; Loss self: 0.0000; time: 0.34s
Val loss: 0.7324 score: 0.5659 time: 0.31s
Test loss: 0.6913 score: 0.6512 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.3738;  Loss pred: 0.3738; Loss self: 0.0000; time: 0.33s
Val loss: 0.7904 score: 0.5116 time: 0.23s
Test loss: 0.7270 score: 0.5581 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.3459;  Loss pred: 0.3459; Loss self: 0.0000; time: 0.36s
Val loss: 0.7575 score: 0.5271 time: 0.23s
Test loss: 0.7036 score: 0.5814 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.3321;  Loss pred: 0.3321; Loss self: 0.0000; time: 0.32s
Val loss: 0.6736 score: 0.5581 time: 0.21s
Test loss: 0.6458 score: 0.6202 time: 0.40s
     INFO: Early stopping counter 8 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.3189;  Loss pred: 0.3189; Loss self: 0.0000; time: 0.34s
Val loss: 0.6233 score: 0.6047 time: 0.23s
Test loss: 0.6099 score: 0.6667 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.3128;  Loss pred: 0.3128; Loss self: 0.0000; time: 0.34s
Val loss: 0.5948 score: 0.6667 time: 0.22s
Test loss: 0.5881 score: 0.6744 time: 0.22s
Epoch 25/1000, LR 0.000285
Train loss: 0.2890;  Loss pred: 0.2890; Loss self: 0.0000; time: 0.33s
Val loss: 0.5658 score: 0.7287 time: 0.22s
Test loss: 0.5603 score: 0.7674 time: 0.36s
Epoch 26/1000, LR 0.000285
Train loss: 0.2624;  Loss pred: 0.2624; Loss self: 0.0000; time: 0.34s
Val loss: 0.5535 score: 0.7752 time: 0.23s
Test loss: 0.5451 score: 0.8217 time: 0.23s
Epoch 27/1000, LR 0.000285
Train loss: 0.2410;  Loss pred: 0.2410; Loss self: 0.0000; time: 0.35s
Val loss: 0.5476 score: 0.7674 time: 0.23s
Test loss: 0.5354 score: 0.8372 time: 0.23s
Epoch 28/1000, LR 0.000285
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.34s
Val loss: 0.5312 score: 0.8062 time: 0.24s
Test loss: 0.5205 score: 0.8527 time: 0.37s
Epoch 29/1000, LR 0.000285
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.34s
Val loss: 0.4903 score: 0.8992 time: 0.24s
Test loss: 0.4762 score: 0.8992 time: 0.22s
Epoch 30/1000, LR 0.000285
Train loss: 0.1772;  Loss pred: 0.1772; Loss self: 0.0000; time: 0.34s
Val loss: 0.4737 score: 0.9070 time: 0.23s
Test loss: 0.4620 score: 0.8992 time: 0.22s
Epoch 31/1000, LR 0.000285
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.32s
Val loss: 0.4783 score: 0.8682 time: 0.23s
Test loss: 0.4806 score: 0.8760 time: 0.34s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.1554;  Loss pred: 0.1554; Loss self: 0.0000; time: 0.34s
Val loss: 0.4700 score: 0.8605 time: 0.24s
Test loss: 0.4773 score: 0.8760 time: 0.23s
Epoch 33/1000, LR 0.000285
Train loss: 0.1372;  Loss pred: 0.1372; Loss self: 0.0000; time: 0.33s
Val loss: 0.4536 score: 0.8605 time: 0.24s
Test loss: 0.4609 score: 0.8760 time: 0.23s
Epoch 34/1000, LR 0.000285
Train loss: 0.1211;  Loss pred: 0.1211; Loss self: 0.0000; time: 0.34s
Val loss: 0.4588 score: 0.8217 time: 0.22s
Test loss: 0.4704 score: 0.8372 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.39s
Val loss: 0.4673 score: 0.8062 time: 0.23s
Test loss: 0.4930 score: 0.7907 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.36s
Val loss: 0.4517 score: 0.7907 time: 0.22s
Test loss: 0.4861 score: 0.7984 time: 0.22s
Epoch 37/1000, LR 0.000285
Train loss: 0.0996;  Loss pred: 0.0996; Loss self: 0.0000; time: 0.33s
Val loss: 0.4132 score: 0.8295 time: 0.23s
Test loss: 0.4422 score: 0.8295 time: 0.38s
Epoch 38/1000, LR 0.000284
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.36s
Val loss: 0.3886 score: 0.8295 time: 0.25s
Test loss: 0.4136 score: 0.8295 time: 0.23s
Epoch 39/1000, LR 0.000284
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.36s
Val loss: 0.3922 score: 0.8140 time: 0.24s
Test loss: 0.4229 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.34s
Val loss: 0.3395 score: 0.8837 time: 0.21s
Test loss: 0.3546 score: 0.8682 time: 0.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.36s
Val loss: 0.3202 score: 0.9070 time: 0.22s
Test loss: 0.3242 score: 0.8915 time: 0.22s
Epoch 42/1000, LR 0.000284
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.33s
Val loss: 0.3160 score: 0.8915 time: 0.23s
Test loss: 0.3270 score: 0.8837 time: 0.22s
Epoch 43/1000, LR 0.000284
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.33s
Val loss: 0.3126 score: 0.8992 time: 0.25s
Test loss: 0.3224 score: 0.8760 time: 0.26s
Epoch 44/1000, LR 0.000284
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.33s
Val loss: 0.3091 score: 0.8915 time: 0.24s
Test loss: 0.3253 score: 0.8682 time: 0.22s
Epoch 45/1000, LR 0.000284
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.33s
Val loss: 0.3103 score: 0.8992 time: 0.22s
Test loss: 0.3357 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.33s
Val loss: 0.3103 score: 0.8837 time: 0.21s
Test loss: 0.3422 score: 0.8682 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.31s
Val loss: 0.3170 score: 0.8837 time: 0.21s
Test loss: 0.3554 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.31s
Val loss: 0.3322 score: 0.8837 time: 0.21s
Test loss: 0.3653 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.32s
Val loss: 0.3402 score: 0.8837 time: 0.28s
Test loss: 0.3873 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.34s
Val loss: 0.3519 score: 0.8915 time: 0.23s
Test loss: 0.4013 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.36s
Val loss: 0.3633 score: 0.8915 time: 0.23s
Test loss: 0.4020 score: 0.8605 time: 0.24s
     INFO: Early stopping counter 7 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.37s
Val loss: 0.3663 score: 0.8992 time: 0.23s
Test loss: 0.4291 score: 0.8450 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.34s
Val loss: 0.3652 score: 0.8915 time: 0.22s
Test loss: 0.4044 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.35s
Val loss: 0.3880 score: 0.8837 time: 0.25s
Test loss: 0.4090 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.32s
Val loss: 0.4051 score: 0.8760 time: 0.22s
Test loss: 0.4266 score: 0.8760 time: 0.38s
     INFO: Early stopping counter 11 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.34s
Val loss: 0.4218 score: 0.8760 time: 0.24s
Test loss: 0.4473 score: 0.8760 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.33s
Val loss: 0.4269 score: 0.8760 time: 0.21s
Test loss: 0.4610 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.32s
Val loss: 0.4470 score: 0.8760 time: 0.28s
Test loss: 0.4709 score: 0.8760 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.38s
Val loss: 0.4536 score: 0.8760 time: 0.27s
Test loss: 0.4731 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.32s
Val loss: 0.4470 score: 0.8837 time: 0.21s
Test loss: 0.4985 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 16 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.33s
Val loss: 0.4609 score: 0.8837 time: 0.22s
Test loss: 0.5289 score: 0.8682 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.32s
Val loss: 0.4612 score: 0.8992 time: 0.21s
Test loss: 0.6161 score: 0.8605 time: 0.33s
     INFO: Early stopping counter 18 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.36s
Val loss: 0.4697 score: 0.8992 time: 0.21s
Test loss: 0.6495 score: 0.8605 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.33s
Val loss: 0.4772 score: 0.8915 time: 0.22s
Test loss: 0.6615 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 043,   Train_Loss: 0.0349,   Val_Loss: 0.3091,   Val_Precision: 0.9322,   Val_Recall: 0.8462,   Val_accuracy: 0.8871,   Val_Score: 0.8915,   Val_Loss: 0.3091,   Test_Precision: 0.8983,   Test_Recall: 0.8281,   Test_accuracy: 0.8618,   Test_Score: 0.8682,   Test_loss: 0.3253


[0.2476020900066942, 0.3409118049312383, 0.23448326298967004, 0.22234646207652986, 0.35054287197999656, 0.20937599008902907, 0.2196406559087336, 0.34501197119243443, 0.21942159300670028, 0.21773542487062514, 0.36032102489843965, 0.2169563800562173, 0.2291356499772519, 0.3676870730705559, 0.21877310192212462, 0.21947248512879014, 0.3715999948326498, 0.2171907548326999, 0.21393226413056254, 0.21230623591691256, 0.20541736599989235, 0.20213541202247143, 0.21693538618274033, 0.21264714398421347, 0.2107442980632186, 0.2212368450127542, 0.21837228490039706, 0.2229541928973049, 0.22731508710421622, 0.20734008098952472, 0.2135990709066391, 0.22257497697137296, 0.23991382704116404, 0.22709121205843985, 0.23305282485671341, 0.21931374911218882, 0.21632506512105465, 0.21640024380758405, 0.2201271839439869, 0.2452706629410386, 0.21792660793289542, 0.24547227704897523, 0.2230546090286225, 0.21012304397299886, 0.2242493089288473, 0.22275575203821063, 0.22337441588751972, 0.2235856200568378, 0.2191774300299585, 0.20706631219945848, 0.2059244450647384, 0.22271724720485508, 0.2360682941507548, 0.21061486494727433, 0.21140997111797333, 0.2111084358766675, 0.217817610129714, 0.2219495470635593, 0.3536405360791832, 0.33650506916455925, 0.22986176004633307, 0.22991908411495388, 0.2585217650048435, 0.22225869400426745, 0.2094130900222808, 0.2178493980318308, 0.3951819599606097, 0.26507348101586103, 0.22714649513363838, 0.23102209507487714, 0.2263029059395194, 0.22673094598576427, 0.23086428409442306, 0.3578802850097418, 0.22006481792777777, 0.22000459604896605, 0.22902295202948153, 0.23863152787089348, 0.22409533406607807, 0.22061545494943857, 0.4087203328963369, 0.23557652393355966, 0.22222468117251992, 0.3614933039061725, 0.2394609870389104, 0.23762482614256442, 0.3731064940802753, 0.22920797602273524, 0.22646140703000128, 0.3478798430878669, 0.2358754628803581, 0.23397926799952984, 0.23631089320406318, 0.23239894793368876, 0.22642729501239955, 0.3833242559339851, 0.23472641501575708, 0.2365520151797682, 0.219966754084453, 0.22568573988974094, 0.22887847106903791, 0.2627645079046488, 0.22247422207146883, 0.21852315496653318, 0.21485588909126818, 0.2093091697897762, 0.22364392993040383, 0.23313475400209427, 0.23871031193993986, 0.24233181402087212, 0.23414424108341336, 0.23303148802369833, 0.22115539107471704, 0.38723878003656864, 0.22671226388774812, 0.21668170019984245, 0.23030411708168685, 0.21639515412971377, 0.222170326160267, 0.21581994812004268, 0.33682037284597754, 0.2193187850061804, 0.22611939697526395]
[0.001919396046563521, 0.0026427271700095994, 0.0018176997130982173, 0.0017236159850893787, 0.0027173866044960973, 0.0016230696906126285, 0.001702640743478555, 0.002674511404592515, 0.001700942581447289, 0.0016878715106250011, 0.002793186239522788, 0.0016818324035365684, 0.0017762453486608674, 0.0028502873881438443, 0.00169591551877616, 0.001701337094021629, 0.0028806201149817814, 0.0016836492622689914, 0.0016583896444229654, 0.0016457847745497097, 0.0015923826821697081, 0.0015669411784687707, 0.0016816696603313204, 0.0016484274727458408, 0.001633676729172237, 0.001715014302424451, 0.0016928084100805974, 0.0017283270767232937, 0.0017621324581722187, 0.0016072874495311994, 0.0016558067512142565, 0.0017253874183827361, 0.0018597971088462329, 0.001760396992701084, 0.001806611045400879, 0.0017001065822650297, 0.0016769384893105012, 0.001677521269826233, 0.0017064122786355574, 0.001901322968535183, 0.0016893535498674064, 0.0019028858685967071, 0.0017291054963459109, 0.0016288608059922393, 0.0017383667358825373, 0.0017267887754900048, 0.0017315846192830985, 0.0017332218609057194, 0.0016990498451934766, 0.0016051652108485153, 0.001596313527633631, 0.0017264902884097293, 0.0018299867763624403, 0.0016326733716842971, 0.001638836985410646, 0.001636499502919903, 0.0016885086056566978, 0.0017205391245237154, 0.0027413995044897922, 0.002608566427632242, 0.0017818741088863029, 0.0017823184815112703, 0.002004044689960027, 0.0017229356124361817, 0.001623357286994425, 0.0016887550235025642, 0.0030634260462062764, 0.0020548331861694655, 0.0017608255436716153, 0.001790868954068815, 0.001754286092554414, 0.001757604232447785, 0.0017896456131350626, 0.0027742657752693163, 0.001705928821145564, 0.0017054619848757059, 0.0017753717211587716, 0.0018498568052007247, 0.0017371731322951788, 0.0017101973251894462, 0.0031683746736150147, 0.0018261746041361214, 0.001722671947073798, 0.00280227367369126, 0.0018562867212318635, 0.0018420529158338327, 0.0028922984037230643, 0.0017768060156801181, 0.0017555147831783044, 0.002696742969673387, 0.0018284919603128536, 0.0018137927751901539, 0.0018318673891787843, 0.001801542232044099, 0.0017552503489333298, 0.002971505859953373, 0.0018195846125252488, 0.0018337365517811489, 0.001705168636313589, 0.001749501859610395, 0.0017742517137134721, 0.002036934169803479, 0.0017246063726470452, 0.0016939779454770015, 0.0016655495278392882, 0.0016225517037967148, 0.001733673875429487, 0.0018072461550549944, 0.0018504675344181384, 0.001878541193960249, 0.0018150716363055299, 0.0018064456435945607, 0.0017143828765481942, 0.003001851008035416, 0.0017574594099825436, 0.0016797031023243602, 0.001785303233191371, 0.0016774818149590215, 0.0017222505903896667, 0.0016730228536437418, 0.0026110106422168804, 0.0017001456202029488, 0.0017528635424439065]
[520.9972177396094, 378.396987531773, 550.1458754678068, 580.175635785917, 368.0006364738213, 616.1164895036328, 587.3229592503259, 373.90006947917976, 587.9093221060557, 592.4621594150318, 358.0140793514895, 594.5895666519407, 562.985288464745, 350.84181481475696, 589.6520132804962, 587.7729954363101, 347.14747522559895, 593.9479334623039, 602.9945998293716, 607.6128637619717, 627.9897484425323, 638.1860491899314, 594.6471079242646, 606.6387612032858, 612.1162052095135, 583.0855163052213, 590.734305220275, 578.594187100212, 567.4942285764688, 622.1662467977784, 603.9352111994154, 579.579976847944, 537.6930608416595, 568.0536857005415, 553.5225761769349, 588.1984167532092, 596.324794483765, 596.1176278281024, 586.0248502194307, 525.9495711927468, 591.942403103535, 525.5175922544714, 578.3337119182623, 613.9260005036701, 575.2526088761806, 579.1096248678322, 577.505707121616, 576.9601818185215, 588.5642512660519, 622.9888320787768, 626.4433538205969, 579.2097451767888, 546.4520361112947, 612.4923804988507, 610.1888161557621, 611.0603750357168, 592.2386161668854, 581.2131707710064, 364.7771871127233, 383.35232310249637, 561.2068748364128, 561.0669531699386, 498.99086832237566, 580.4047422213466, 616.0073373936408, 592.1521985621981, 326.43190497070833, 486.657509101339, 567.9154323913504, 558.3881487967178, 570.0324503763813, 568.9562994550356, 558.7698439626947, 360.45573171623164, 586.1909287214461, 586.351386819613, 563.2623230853917, 540.5823830193666, 575.6478622708062, 584.7278470566112, 315.6192379416516, 547.5927645336267, 580.4935766781607, 356.85308304765346, 538.709881702102, 542.8725697314375, 345.7457912063175, 562.8076397620841, 569.6334827722335, 370.81769054212606, 546.8987677850663, 551.3308982582987, 545.8910431547637, 555.0799654945428, 569.7192999321735, 336.52970821187995, 549.5759818567514, 545.3346060145215, 586.4522597377254, 571.5912758290493, 563.6178859351477, 490.9338823141638, 579.8424590448027, 590.3264577145445, 600.4024397264827, 616.3131798265871, 576.8097530755418, 553.3280550648453, 540.4039689431462, 532.3279591712593, 550.9424421591648, 553.573257820339, 583.3002730483632, 333.12779259303, 569.0031839824584, 595.3433071691109, 560.1289357508308, 596.1316486905872, 580.635597154155, 597.7204661741835, 382.99346001552465, 588.18491081995, 570.4950646675915]
Elapsed: 0.24378664993370572~0.049641285931573215
Time per graph: 0.001889818991734153~0.00038481617001219547
Speed: 545.4081460106503~80.53592985074752
Total Time: 0.2268
best val loss: 0.3090544098093571 test_score: 0.8682

Testing...
Test loss: 0.4620 score: 0.8992 time: 0.22s
test Score 0.8992
Epoch Time List: [0.8022106168791652, 0.891143934102729, 0.8067530700936913, 0.7503123451024294, 0.8931535731535405, 0.7718180490192026, 0.7452568998560309, 0.8918661980424076, 0.7565448058303446, 0.7839048330206424, 1.055396526819095, 0.7573611331172287, 0.9043581888545305, 0.9907316479366273, 0.7808056941721588, 0.7670201398432255, 6.909710790961981, 0.7433194338809699, 0.7411446901969612, 0.7506542962510139, 0.7945610580500215, 0.7065277670044452, 0.7512301800306886, 0.8944106369744986, 0.7254131860099733, 0.7665402796119452, 0.8038276529405266, 0.7628954499959946, 0.7738526430912316, 0.8708459238987416, 0.7464304461609572, 0.7712018939200789, 0.7889771505724639, 0.7906160203274339, 0.8054529200308025, 0.7594503429718316, 0.7473466149531305, 0.7643948260229081, 0.7547983308322728, 0.7799405292607844, 0.7468008301220834, 0.7764104497618973, 0.7548283857759088, 0.7297644186764956, 0.7559147910214961, 0.7690943689085543, 0.7849885451141745, 0.7933449021074921, 0.8147073599975556, 0.8713222581427544, 0.7277458540629596, 0.7774857508484274, 0.8835665581282228, 0.7557803560048342, 0.7445399449206889, 0.818081091856584, 0.7849581630434841, 0.7545527312904596, 0.895152160897851, 0.92356200912036, 0.7795584937557578, 0.7770601599477232, 0.8352229320444167, 1.0570922987535596, 0.7428112381603569, 0.7596123127732426, 1.0006078609731048, 0.8183176158927381, 0.8089765990152955, 0.8006641501560807, 0.8868347122333944, 0.7666173779871315, 0.7751864432357252, 0.9335080550517887, 0.7571620249655098, 0.7615282151382416, 0.7971873870119452, 0.8818655461072922, 0.7710321058984846, 0.8003320379648358, 0.937616921029985, 0.7964722299948335, 0.7691197663079947, 0.9022725701797754, 0.8044518760871142, 0.8103556260466576, 0.9561962347943336, 0.7991995201446116, 0.7946692400146276, 0.8934919617604464, 0.803195137064904, 0.7903444939292967, 0.7964047100394964, 0.8429125309921801, 0.8055884051136672, 0.9331485270522535, 0.8430745501536876, 0.8270600656978786, 0.7631161969620734, 0.8016498731449246, 0.7846597179304808, 0.8347181738354266, 0.7870472113136202, 0.7620013121049851, 0.7533563908655196, 0.7222650600597262, 0.7434631809592247, 0.8311362899839878, 0.8005983650218695, 0.8255246579647064, 0.8246130184270442, 0.7935111790429801, 0.8106368698645383, 0.9243357111699879, 0.7961968861054629, 0.7552294707857072, 0.8233809270896018, 0.8555646669119596, 0.7491722379345447, 0.7599524701945484, 0.8580600190907717, 0.7871071158442646, 0.7636302157770842]
Total Epoch List: [59, 64]
Total Time List: [0.3541039580013603, 0.22682759095914662]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x717c757097e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.39s
Val loss: 0.7003 score: 0.4961 time: 0.27s
Test loss: 0.6449 score: 0.5234 time: 0.21s
Epoch 2/1000, LR 0.000020
Train loss: 0.7155;  Loss pred: 0.7155; Loss self: 0.0000; time: 0.38s
Val loss: 0.6207 score: 0.6202 time: 0.22s
Test loss: 0.5978 score: 0.7031 time: 0.20s
Epoch 3/1000, LR 0.000050
Train loss: 0.7057;  Loss pred: 0.7057; Loss self: 0.0000; time: 0.39s
Val loss: 0.6003 score: 0.6279 time: 0.22s
Test loss: 0.5916 score: 0.6953 time: 0.83s
Epoch 4/1000, LR 0.000080
Train loss: 0.7032;  Loss pred: 0.7032; Loss self: 0.0000; time: 4.42s
Val loss: 0.5945 score: 0.6202 time: 8.78s
Test loss: 0.5849 score: 0.6797 time: 0.22s
Epoch 5/1000, LR 0.000110
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.42s
Val loss: 0.5822 score: 0.6744 time: 0.23s
Test loss: 0.5805 score: 0.7031 time: 0.21s
Epoch 6/1000, LR 0.000140
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.40s
Val loss: 0.5806 score: 0.6589 time: 0.22s
Test loss: 0.5745 score: 0.7109 time: 0.20s
Epoch 7/1000, LR 0.000170
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.43s
Val loss: 0.5762 score: 0.6357 time: 0.24s
Test loss: 0.5699 score: 0.7266 time: 0.30s
Epoch 8/1000, LR 0.000200
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.41s
Val loss: 0.5718 score: 0.6434 time: 0.23s
Test loss: 0.5667 score: 0.7344 time: 0.22s
Epoch 9/1000, LR 0.000230
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.43s
Val loss: 0.5670 score: 0.6357 time: 0.24s
Test loss: 0.5620 score: 0.7266 time: 0.22s
Epoch 10/1000, LR 0.000260
Train loss: 0.4621;  Loss pred: 0.4621; Loss self: 0.0000; time: 0.44s
Val loss: 0.5589 score: 0.6822 time: 0.31s
Test loss: 0.5581 score: 0.7578 time: 0.20s
Epoch 11/1000, LR 0.000290
Train loss: 0.3951;  Loss pred: 0.3951; Loss self: 0.0000; time: 0.41s
Val loss: 0.5431 score: 0.7829 time: 0.24s
Test loss: 0.5494 score: 0.7656 time: 0.20s
Epoch 12/1000, LR 0.000290
Train loss: 0.3824;  Loss pred: 0.3824; Loss self: 0.0000; time: 0.41s
Val loss: 0.5298 score: 0.8062 time: 0.23s
Test loss: 0.5432 score: 0.8125 time: 0.21s
Epoch 13/1000, LR 0.000290
Train loss: 0.3485;  Loss pred: 0.3485; Loss self: 0.0000; time: 0.56s
Val loss: 0.5117 score: 0.8062 time: 0.24s
Test loss: 0.5284 score: 0.7656 time: 0.21s
Epoch 14/1000, LR 0.000290
Train loss: 0.3194;  Loss pred: 0.3194; Loss self: 0.0000; time: 0.40s
Val loss: 0.4918 score: 0.8295 time: 0.24s
Test loss: 0.5179 score: 0.8359 time: 0.25s
Epoch 15/1000, LR 0.000290
Train loss: 0.3306;  Loss pred: 0.3306; Loss self: 0.0000; time: 0.43s
Val loss: 0.4699 score: 0.8527 time: 0.24s
Test loss: 0.5028 score: 0.8438 time: 0.21s
Epoch 16/1000, LR 0.000290
Train loss: 0.2540;  Loss pred: 0.2540; Loss self: 0.0000; time: 0.53s
Val loss: 0.4544 score: 0.8372 time: 0.23s
Test loss: 0.4993 score: 0.8047 time: 0.20s
Epoch 17/1000, LR 0.000290
Train loss: 0.2534;  Loss pred: 0.2534; Loss self: 0.0000; time: 0.39s
Val loss: 0.4318 score: 0.8605 time: 0.23s
Test loss: 0.4857 score: 0.8359 time: 0.21s
Epoch 18/1000, LR 0.000290
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.39s
Val loss: 0.4071 score: 0.8915 time: 0.22s
Test loss: 0.4663 score: 0.8594 time: 0.31s
Epoch 19/1000, LR 0.000290
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.38s
Val loss: 0.3936 score: 0.8760 time: 0.22s
Test loss: 0.4548 score: 0.8438 time: 0.20s
Epoch 20/1000, LR 0.000290
Train loss: 0.2107;  Loss pred: 0.2107; Loss self: 0.0000; time: 0.38s
Val loss: 0.3725 score: 0.8605 time: 2.49s
Test loss: 0.4480 score: 0.8203 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.37s
Val loss: 0.3684 score: 0.8682 time: 3.08s
Test loss: 0.4590 score: 0.8359 time: 0.21s
Epoch 22/1000, LR 0.000290
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.39s
Val loss: 0.3133 score: 0.8992 time: 0.23s
Test loss: 0.4070 score: 0.8750 time: 0.20s
Epoch 23/1000, LR 0.000290
Train loss: 0.1322;  Loss pred: 0.1322; Loss self: 0.0000; time: 0.38s
Val loss: 0.2922 score: 0.8992 time: 0.22s
Test loss: 0.3777 score: 0.8984 time: 0.20s
Epoch 24/1000, LR 0.000290
Train loss: 0.1185;  Loss pred: 0.1185; Loss self: 0.0000; time: 0.36s
Val loss: 0.2722 score: 0.9070 time: 0.25s
Test loss: 0.3866 score: 0.8984 time: 0.20s
Epoch 25/1000, LR 0.000290
Train loss: 0.1097;  Loss pred: 0.1097; Loss self: 0.0000; time: 0.38s
Val loss: 0.3009 score: 0.8682 time: 0.23s
Test loss: 0.4793 score: 0.8750 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000290
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.37s
Val loss: 0.2508 score: 0.9302 time: 0.21s
Test loss: 0.3697 score: 0.9062 time: 0.18s
Epoch 27/1000, LR 0.000290
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.38s
Val loss: 0.3538 score: 0.8140 time: 0.23s
Test loss: 0.4424 score: 0.8047 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000290
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 0.39s
Val loss: 0.3535 score: 0.8372 time: 0.23s
Test loss: 0.4369 score: 0.8203 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.39s
Val loss: 0.2578 score: 0.8992 time: 0.23s
Test loss: 0.3942 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.37s
Val loss: 0.1872 score: 0.9225 time: 0.23s
Test loss: 0.3984 score: 0.9062 time: 0.20s
Epoch 31/1000, LR 0.000290
Train loss: 0.0496;  Loss pred: 0.0496; Loss self: 0.0000; time: 0.38s
Val loss: 0.1913 score: 0.9225 time: 0.22s
Test loss: 0.4204 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000290
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.43s
Val loss: 0.2096 score: 0.9070 time: 0.22s
Test loss: 0.4661 score: 0.9297 time: 0.43s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.39s
Val loss: 0.2418 score: 0.8915 time: 0.22s
Test loss: 0.5537 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.38s
Val loss: 0.3593 score: 0.8682 time: 0.24s
Test loss: 0.6993 score: 0.8750 time: 0.33s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.37s
Val loss: 0.3148 score: 0.8605 time: 0.22s
Test loss: 0.6594 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.37s
Val loss: 0.3948 score: 0.8450 time: 0.22s
Test loss: 0.7424 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.37s
Val loss: 0.2169 score: 0.9225 time: 0.35s
Test loss: 0.5662 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.36s
Val loss: 0.2377 score: 0.9225 time: 0.23s
Test loss: 0.6219 score: 0.9062 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.37s
Val loss: 0.1826 score: 0.9302 time: 0.22s
Test loss: 0.6354 score: 0.9062 time: 0.21s
Epoch 40/1000, LR 0.000289
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.38s
Val loss: 0.2194 score: 0.9302 time: 0.22s
Test loss: 0.6463 score: 0.8984 time: 0.31s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.37s
Val loss: 0.2033 score: 0.9302 time: 0.21s
Test loss: 0.6832 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.37s
Val loss: 0.2111 score: 0.9225 time: 0.21s
Test loss: 0.6543 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.38s
Val loss: 0.2468 score: 0.9225 time: 0.23s
Test loss: 0.6923 score: 0.9062 time: 0.32s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.36s
Val loss: 0.5581 score: 0.8605 time: 0.21s
Test loss: 0.9746 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.39s
Val loss: 0.4065 score: 0.8837 time: 0.24s
Test loss: 0.8682 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.39s
Val loss: 0.2359 score: 0.9225 time: 0.23s
Test loss: 0.6895 score: 0.8984 time: 0.32s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.37s
Val loss: 0.2595 score: 0.9147 time: 0.26s
Test loss: 0.6937 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.38s
Val loss: 0.2641 score: 0.9147 time: 0.21s
Test loss: 0.6697 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.38s
Val loss: 0.2344 score: 0.9302 time: 0.23s
Test loss: 0.6662 score: 0.8984 time: 0.35s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0417;  Loss pred: 0.0417; Loss self: 0.0000; time: 0.38s
Val loss: 0.3657 score: 0.8837 time: 0.23s
Test loss: 0.7474 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.37s
Val loss: 0.4967 score: 0.8605 time: 0.23s
Test loss: 0.8840 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.37s
Val loss: 0.2249 score: 0.9302 time: 0.22s
Test loss: 0.5772 score: 0.9062 time: 0.34s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.37s
Val loss: 0.2533 score: 0.9225 time: 0.22s
Test loss: 0.6795 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.38s
Val loss: 0.3349 score: 0.9147 time: 0.22s
Test loss: 0.8396 score: 0.8906 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.38s
Val loss: 0.5279 score: 0.8605 time: 0.33s
Test loss: 0.9751 score: 0.8750 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.37s
Val loss: 0.6815 score: 0.8527 time: 0.21s
Test loss: 1.0861 score: 0.8359 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.36s
Val loss: 0.2978 score: 0.9302 time: 0.21s
Test loss: 0.7236 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.36s
Val loss: 0.3290 score: 0.9147 time: 0.22s
Test loss: 0.7192 score: 0.8984 time: 0.30s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.37s
Val loss: 0.3205 score: 0.8682 time: 0.21s
Test loss: 0.4537 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.0252,   Val_Loss: 0.1826,   Val_Precision: 1.0000,   Val_Recall: 0.8615,   Val_accuracy: 0.9256,   Val_Score: 0.9302,   Val_Loss: 0.1826,   Test_Precision: 0.9333,   Test_Recall: 0.8750,   Test_accuracy: 0.9032,   Test_Score: 0.9062,   Test_loss: 0.6354


[0.2476020900066942, 0.3409118049312383, 0.23448326298967004, 0.22234646207652986, 0.35054287197999656, 0.20937599008902907, 0.2196406559087336, 0.34501197119243443, 0.21942159300670028, 0.21773542487062514, 0.36032102489843965, 0.2169563800562173, 0.2291356499772519, 0.3676870730705559, 0.21877310192212462, 0.21947248512879014, 0.3715999948326498, 0.2171907548326999, 0.21393226413056254, 0.21230623591691256, 0.20541736599989235, 0.20213541202247143, 0.21693538618274033, 0.21264714398421347, 0.2107442980632186, 0.2212368450127542, 0.21837228490039706, 0.2229541928973049, 0.22731508710421622, 0.20734008098952472, 0.2135990709066391, 0.22257497697137296, 0.23991382704116404, 0.22709121205843985, 0.23305282485671341, 0.21931374911218882, 0.21632506512105465, 0.21640024380758405, 0.2201271839439869, 0.2452706629410386, 0.21792660793289542, 0.24547227704897523, 0.2230546090286225, 0.21012304397299886, 0.2242493089288473, 0.22275575203821063, 0.22337441588751972, 0.2235856200568378, 0.2191774300299585, 0.20706631219945848, 0.2059244450647384, 0.22271724720485508, 0.2360682941507548, 0.21061486494727433, 0.21140997111797333, 0.2111084358766675, 0.217817610129714, 0.2219495470635593, 0.3536405360791832, 0.33650506916455925, 0.22986176004633307, 0.22991908411495388, 0.2585217650048435, 0.22225869400426745, 0.2094130900222808, 0.2178493980318308, 0.3951819599606097, 0.26507348101586103, 0.22714649513363838, 0.23102209507487714, 0.2263029059395194, 0.22673094598576427, 0.23086428409442306, 0.3578802850097418, 0.22006481792777777, 0.22000459604896605, 0.22902295202948153, 0.23863152787089348, 0.22409533406607807, 0.22061545494943857, 0.4087203328963369, 0.23557652393355966, 0.22222468117251992, 0.3614933039061725, 0.2394609870389104, 0.23762482614256442, 0.3731064940802753, 0.22920797602273524, 0.22646140703000128, 0.3478798430878669, 0.2358754628803581, 0.23397926799952984, 0.23631089320406318, 0.23239894793368876, 0.22642729501239955, 0.3833242559339851, 0.23472641501575708, 0.2365520151797682, 0.219966754084453, 0.22568573988974094, 0.22887847106903791, 0.2627645079046488, 0.22247422207146883, 0.21852315496653318, 0.21485588909126818, 0.2093091697897762, 0.22364392993040383, 0.23313475400209427, 0.23871031193993986, 0.24233181402087212, 0.23414424108341336, 0.23303148802369833, 0.22115539107471704, 0.38723878003656864, 0.22671226388774812, 0.21668170019984245, 0.23030411708168685, 0.21639515412971377, 0.222170326160267, 0.21581994812004268, 0.33682037284597754, 0.2193187850061804, 0.22611939697526395, 0.21048783883452415, 0.2056866278871894, 0.8384723549243063, 0.22127461014315486, 0.21144027612172067, 0.21041695191524923, 0.30187167087569833, 0.2277880529873073, 0.22486357088200748, 0.20799436094239354, 0.21018423605710268, 0.21922444412484765, 0.21495725982822478, 0.25706933508627117, 0.21474417112767696, 0.21005452401004732, 0.21557969599962234, 0.31236693798564374, 0.21049476508051157, 0.21645132289268076, 0.21265265299007297, 0.20110491919331253, 0.20049318997189403, 0.20888529112562537, 0.2269341628998518, 0.18948336807079613, 0.2114884031470865, 0.21167415101081133, 0.2029814829584211, 0.2105323071591556, 0.20457832515239716, 0.43831783486530185, 0.20209300494752824, 0.33071113494224846, 0.2059530559927225, 0.20234998082742095, 0.20053954212926328, 0.22152190189808607, 0.22047406085766852, 0.3167391668539494, 0.19160346989519894, 0.20199794112704694, 0.3292627530172467, 0.19624837511219084, 0.21737782005220652, 0.3226996688172221, 0.2043333558831364, 0.21350164897739887, 0.3528215338010341, 0.2115313799586147, 0.21708548301830888, 0.34465781692415476, 0.20410293200984597, 0.2215405721217394, 0.21227559191174805, 0.19273418211378157, 0.19601427903398871, 0.3071003088261932, 0.2116462541744113]
[0.001919396046563521, 0.0026427271700095994, 0.0018176997130982173, 0.0017236159850893787, 0.0027173866044960973, 0.0016230696906126285, 0.001702640743478555, 0.002674511404592515, 0.001700942581447289, 0.0016878715106250011, 0.002793186239522788, 0.0016818324035365684, 0.0017762453486608674, 0.0028502873881438443, 0.00169591551877616, 0.001701337094021629, 0.0028806201149817814, 0.0016836492622689914, 0.0016583896444229654, 0.0016457847745497097, 0.0015923826821697081, 0.0015669411784687707, 0.0016816696603313204, 0.0016484274727458408, 0.001633676729172237, 0.001715014302424451, 0.0016928084100805974, 0.0017283270767232937, 0.0017621324581722187, 0.0016072874495311994, 0.0016558067512142565, 0.0017253874183827361, 0.0018597971088462329, 0.001760396992701084, 0.001806611045400879, 0.0017001065822650297, 0.0016769384893105012, 0.001677521269826233, 0.0017064122786355574, 0.001901322968535183, 0.0016893535498674064, 0.0019028858685967071, 0.0017291054963459109, 0.0016288608059922393, 0.0017383667358825373, 0.0017267887754900048, 0.0017315846192830985, 0.0017332218609057194, 0.0016990498451934766, 0.0016051652108485153, 0.001596313527633631, 0.0017264902884097293, 0.0018299867763624403, 0.0016326733716842971, 0.001638836985410646, 0.001636499502919903, 0.0016885086056566978, 0.0017205391245237154, 0.0027413995044897922, 0.002608566427632242, 0.0017818741088863029, 0.0017823184815112703, 0.002004044689960027, 0.0017229356124361817, 0.001623357286994425, 0.0016887550235025642, 0.0030634260462062764, 0.0020548331861694655, 0.0017608255436716153, 0.001790868954068815, 0.001754286092554414, 0.001757604232447785, 0.0017896456131350626, 0.0027742657752693163, 0.001705928821145564, 0.0017054619848757059, 0.0017753717211587716, 0.0018498568052007247, 0.0017371731322951788, 0.0017101973251894462, 0.0031683746736150147, 0.0018261746041361214, 0.001722671947073798, 0.00280227367369126, 0.0018562867212318635, 0.0018420529158338327, 0.0028922984037230643, 0.0017768060156801181, 0.0017555147831783044, 0.002696742969673387, 0.0018284919603128536, 0.0018137927751901539, 0.0018318673891787843, 0.001801542232044099, 0.0017552503489333298, 0.002971505859953373, 0.0018195846125252488, 0.0018337365517811489, 0.001705168636313589, 0.001749501859610395, 0.0017742517137134721, 0.002036934169803479, 0.0017246063726470452, 0.0016939779454770015, 0.0016655495278392882, 0.0016225517037967148, 0.001733673875429487, 0.0018072461550549944, 0.0018504675344181384, 0.001878541193960249, 0.0018150716363055299, 0.0018064456435945607, 0.0017143828765481942, 0.003001851008035416, 0.0017574594099825436, 0.0016797031023243602, 0.001785303233191371, 0.0016774818149590215, 0.0017222505903896667, 0.0016730228536437418, 0.0026110106422168804, 0.0017001456202029488, 0.0017528635424439065, 0.00164443624089472, 0.001606926780368667, 0.006550565272846143, 0.0017287078917433973, 0.0016518771572009427, 0.0016438824368378846, 0.002358372428716393, 0.0017795941639633384, 0.0017567466475156834, 0.0016249559448624495, 0.0016420643441961147, 0.0017126909697253723, 0.001679353592408006, 0.0020083541803614935, 0.0016776888369349763, 0.0016410509688284947, 0.0016842163749970496, 0.0024403667030128418, 0.0016444903521914966, 0.0016910259600990685, 0.001661348851484945, 0.001571132181197754, 0.001566353046655422, 0.0016319163369189482, 0.0017729231476550922, 0.0014803388130530948, 0.0016522531495866133, 0.0016537043047719635, 0.001585792835612665, 0.0016447836496809032, 0.0015982681652531028, 0.0034243580848851707, 0.0015788516011525644, 0.002583680741736316, 0.0016090082499431446, 0.0015808592252142262, 0.0015667151728848694, 0.0017306398585787974, 0.0017224536004505353, 0.00247452474104648, 0.0014969021085562417, 0.0015781089150550542, 0.00257236525794724, 0.001533190430563991, 0.0016982642191578634, 0.002521091162634548, 0.001596354342837003, 0.0016679816326359287, 0.002756418232820579, 0.0016525889059266774, 0.0016959803360805381, 0.002692639194719959, 0.0015945541563269217, 0.001730785719701089, 0.0016584030618105317, 0.0015057357977639185, 0.0015313615549530368, 0.0023992211627046345, 0.0016534863607375883]
[520.9972177396094, 378.396987531773, 550.1458754678068, 580.175635785917, 368.0006364738213, 616.1164895036328, 587.3229592503259, 373.90006947917976, 587.9093221060557, 592.4621594150318, 358.0140793514895, 594.5895666519407, 562.985288464745, 350.84181481475696, 589.6520132804962, 587.7729954363101, 347.14747522559895, 593.9479334623039, 602.9945998293716, 607.6128637619717, 627.9897484425323, 638.1860491899314, 594.6471079242646, 606.6387612032858, 612.1162052095135, 583.0855163052213, 590.734305220275, 578.594187100212, 567.4942285764688, 622.1662467977784, 603.9352111994154, 579.579976847944, 537.6930608416595, 568.0536857005415, 553.5225761769349, 588.1984167532092, 596.324794483765, 596.1176278281024, 586.0248502194307, 525.9495711927468, 591.942403103535, 525.5175922544714, 578.3337119182623, 613.9260005036701, 575.2526088761806, 579.1096248678322, 577.505707121616, 576.9601818185215, 588.5642512660519, 622.9888320787768, 626.4433538205969, 579.2097451767888, 546.4520361112947, 612.4923804988507, 610.1888161557621, 611.0603750357168, 592.2386161668854, 581.2131707710064, 364.7771871127233, 383.35232310249637, 561.2068748364128, 561.0669531699386, 498.99086832237566, 580.4047422213466, 616.0073373936408, 592.1521985621981, 326.43190497070833, 486.657509101339, 567.9154323913504, 558.3881487967178, 570.0324503763813, 568.9562994550356, 558.7698439626947, 360.45573171623164, 586.1909287214461, 586.351386819613, 563.2623230853917, 540.5823830193666, 575.6478622708062, 584.7278470566112, 315.6192379416516, 547.5927645336267, 580.4935766781607, 356.85308304765346, 538.709881702102, 542.8725697314375, 345.7457912063175, 562.8076397620841, 569.6334827722335, 370.81769054212606, 546.8987677850663, 551.3308982582987, 545.8910431547637, 555.0799654945428, 569.7192999321735, 336.52970821187995, 549.5759818567514, 545.3346060145215, 586.4522597377254, 571.5912758290493, 563.6178859351477, 490.9338823141638, 579.8424590448027, 590.3264577145445, 600.4024397264827, 616.3131798265871, 576.8097530755418, 553.3280550648453, 540.4039689431462, 532.3279591712593, 550.9424421591648, 553.573257820339, 583.3002730483632, 333.12779259303, 569.0031839824584, 595.3433071691109, 560.1289357508308, 596.1316486905872, 580.635597154155, 597.7204661741835, 382.99346001552465, 588.18491081995, 570.4950646675915, 608.1111417587773, 622.3058898617498, 152.658581106774, 578.4667292699767, 605.3718919961764, 608.3160070275861, 424.0212393189639, 561.9258706563173, 569.2340448829987, 615.4012994393204, 608.989534140063, 583.8764947539536, 595.4672110273759, 497.9201426612935, 596.0580877601428, 609.3655949722726, 593.7479381185519, 409.7744813373393, 608.0911321050746, 591.3569771225838, 601.9205413156792, 636.483684802159, 638.4256742981822, 612.7765115018066, 564.0402412945098, 675.5210301738764, 605.2341314950411, 604.7030276902462, 630.5993932767731, 607.9826974167729, 625.6772309805966, 292.02553448306594, 633.3717489788137, 387.04472415890206, 621.5008531095696, 632.5673937630263, 638.2781103464077, 577.8209689572276, 580.5671628765117, 404.11800432316494, 668.0463567283617, 633.6698249785337, 388.7472810910239, 652.2346996596799, 588.8365242105147, 396.6536453822626, 626.4273370677989, 599.5269854498874, 362.78964784553875, 605.1111661307306, 589.6294778458519, 371.38284325687476, 627.1345479438054, 577.772273376916, 602.9897212733483, 664.1271340463861, 653.0136509993994, 416.80192536844044, 604.7827328638618]
Elapsed: 0.24338035304022565~0.06685870030008141
Time per graph: 0.0018914309974818752~0.0005208137524950315
Speed: 551.2417510405979~89.40257039912431
Total Time: 0.2124
best val loss: 0.1826383369115665 test_score: 0.9062

Testing...
Test loss: 0.3697 score: 0.9062 time: 0.21s
test Score 0.9062
Epoch Time List: [0.8022106168791652, 0.891143934102729, 0.8067530700936913, 0.7503123451024294, 0.8931535731535405, 0.7718180490192026, 0.7452568998560309, 0.8918661980424076, 0.7565448058303446, 0.7839048330206424, 1.055396526819095, 0.7573611331172287, 0.9043581888545305, 0.9907316479366273, 0.7808056941721588, 0.7670201398432255, 6.909710790961981, 0.7433194338809699, 0.7411446901969612, 0.7506542962510139, 0.7945610580500215, 0.7065277670044452, 0.7512301800306886, 0.8944106369744986, 0.7254131860099733, 0.7665402796119452, 0.8038276529405266, 0.7628954499959946, 0.7738526430912316, 0.8708459238987416, 0.7464304461609572, 0.7712018939200789, 0.7889771505724639, 0.7906160203274339, 0.8054529200308025, 0.7594503429718316, 0.7473466149531305, 0.7643948260229081, 0.7547983308322728, 0.7799405292607844, 0.7468008301220834, 0.7764104497618973, 0.7548283857759088, 0.7297644186764956, 0.7559147910214961, 0.7690943689085543, 0.7849885451141745, 0.7933449021074921, 0.8147073599975556, 0.8713222581427544, 0.7277458540629596, 0.7774857508484274, 0.8835665581282228, 0.7557803560048342, 0.7445399449206889, 0.818081091856584, 0.7849581630434841, 0.7545527312904596, 0.895152160897851, 0.92356200912036, 0.7795584937557578, 0.7770601599477232, 0.8352229320444167, 1.0570922987535596, 0.7428112381603569, 0.7596123127732426, 1.0006078609731048, 0.8183176158927381, 0.8089765990152955, 0.8006641501560807, 0.8868347122333944, 0.7666173779871315, 0.7751864432357252, 0.9335080550517887, 0.7571620249655098, 0.7615282151382416, 0.7971873870119452, 0.8818655461072922, 0.7710321058984846, 0.8003320379648358, 0.937616921029985, 0.7964722299948335, 0.7691197663079947, 0.9022725701797754, 0.8044518760871142, 0.8103556260466576, 0.9561962347943336, 0.7991995201446116, 0.7946692400146276, 0.8934919617604464, 0.803195137064904, 0.7903444939292967, 0.7964047100394964, 0.8429125309921801, 0.8055884051136672, 0.9331485270522535, 0.8430745501536876, 0.8270600656978786, 0.7631161969620734, 0.8016498731449246, 0.7846597179304808, 0.8347181738354266, 0.7870472113136202, 0.7620013121049851, 0.7533563908655196, 0.7222650600597262, 0.7434631809592247, 0.8311362899839878, 0.8005983650218695, 0.8255246579647064, 0.8246130184270442, 0.7935111790429801, 0.8106368698645383, 0.9243357111699879, 0.7961968861054629, 0.7552294707857072, 0.8233809270896018, 0.8555646669119596, 0.7491722379345447, 0.7599524701945484, 0.8580600190907717, 0.7871071158442646, 0.7636302157770842, 0.8648471317719668, 0.8070155179593712, 1.4404157707467675, 13.416808526031673, 0.859806285938248, 0.8265062859281898, 0.9672322487458587, 0.8691922530997545, 0.8852924569509923, 0.9465556361246854, 0.8517151991836727, 0.8498560739681125, 1.0034959851764143, 0.891040104907006, 0.8718861120287329, 0.9591641840524971, 0.8350586742162704, 0.9156358486507088, 0.80761459027417, 3.083564310101792, 3.662767448928207, 0.8150747241452336, 0.7888405970297754, 0.8166457838378847, 0.8187711539212614, 0.7657078420743346, 0.814669675892219, 0.8241451629437506, 0.8144555068574846, 0.8006511160638183, 0.8022628638427705, 1.0746051587630063, 0.7990862741135061, 0.9412512411363423, 0.7876121660228819, 0.7876736118923873, 0.912318890914321, 0.8092447246890515, 0.8116990481503308, 0.9122643701266497, 0.7618593939114362, 0.7784262571949512, 0.9420851618051529, 0.7553160225506872, 0.84294369420968, 0.9349750250112265, 0.8247693390585482, 0.7950996032450348, 0.9608463696204126, 0.8144523170776665, 0.803512993035838, 0.9326236131601036, 0.7928595980629325, 0.8185701051261276, 0.9132089510094374, 0.7659516043495387, 0.7609499329701066, 0.8780425612349063, 0.7919961861334741]
Total Epoch List: [59, 64, 59]
Total Time List: [0.3541039580013603, 0.22682759095914662, 0.21239888621494174]
T-times Epoch Time: 0.9672275399991034 ~ 0.010929423396651147
T-times Total Epoch: 60.666666666666664 ~ 0.27216552697590773
T-times Total Time: 0.22771085979830885 ~ 0.026506241096233712
T-times Inference Elapsed: 0.2618052101798287 ~ 0.014189088235821223
T-times Time Per Graph: 0.002033920287645189 ~ 0.00010975585378745356
T-times Speed: 563.4598032443795 ~ 9.198631280207906
T-times cross validation test micro f1 score:0.8794067623182774 ~ 0.007404843690140547
T-times cross validation test precision:0.9415110264217361 ~ 0.013644712514219224
T-times cross validation test recall:0.8256677350427352 ~ 0.0023628387344654667
T-times cross validation test f1_score:0.8794067623182774 ~ 0.007421492097595014
