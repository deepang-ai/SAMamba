Namespace(seed=15, model='I2BGNNT', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e933561d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6738;  Loss pred: 0.6738; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4961 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4961 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4961 time: 0.16s
Epoch 19/1000, LR 0.000285
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.26s
Val loss: 0.6842 score: 0.5271 time: 0.16s
Test loss: 0.6836 score: 0.5116 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.4844;  Loss pred: 0.4844; Loss self: 0.0000; time: 0.27s
Val loss: 0.6808 score: 0.5504 time: 0.17s
Test loss: 0.6798 score: 0.5659 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4532;  Loss pred: 0.4532; Loss self: 0.0000; time: 0.26s
Val loss: 0.6767 score: 0.6589 time: 0.17s
Test loss: 0.6752 score: 0.6357 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4209;  Loss pred: 0.4209; Loss self: 0.0000; time: 0.26s
Val loss: 0.6716 score: 0.7442 time: 0.17s
Test loss: 0.6695 score: 0.7132 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.26s
Val loss: 0.6656 score: 0.7674 time: 0.17s
Test loss: 0.6629 score: 0.7829 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3568;  Loss pred: 0.3568; Loss self: 0.0000; time: 0.27s
Val loss: 0.6582 score: 0.7752 time: 0.17s
Test loss: 0.6547 score: 0.7984 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 0.3253;  Loss pred: 0.3253; Loss self: 0.0000; time: 0.26s
Val loss: 0.6491 score: 0.7984 time: 0.17s
Test loss: 0.6446 score: 0.8217 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2908;  Loss pred: 0.2908; Loss self: 0.0000; time: 0.26s
Val loss: 0.6383 score: 0.7907 time: 0.17s
Test loss: 0.6327 score: 0.8295 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2566;  Loss pred: 0.2566; Loss self: 0.0000; time: 0.26s
Val loss: 0.6254 score: 0.7907 time: 0.17s
Test loss: 0.6186 score: 0.8295 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 0.27s
Val loss: 0.6104 score: 0.7984 time: 0.17s
Test loss: 0.6021 score: 0.8295 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.27s
Val loss: 0.5929 score: 0.7984 time: 0.17s
Test loss: 0.5830 score: 0.8295 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1728;  Loss pred: 0.1728; Loss self: 0.0000; time: 0.27s
Val loss: 0.5726 score: 0.7984 time: 0.17s
Test loss: 0.5609 score: 0.8450 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1476;  Loss pred: 0.1476; Loss self: 0.0000; time: 0.27s
Val loss: 0.5501 score: 0.8062 time: 0.18s
Test loss: 0.5369 score: 0.8527 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 0.27s
Val loss: 0.5252 score: 0.8217 time: 0.17s
Test loss: 0.5106 score: 0.8605 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1115;  Loss pred: 0.1115; Loss self: 0.0000; time: 0.27s
Val loss: 0.4989 score: 0.8217 time: 0.17s
Test loss: 0.4831 score: 0.8605 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.27s
Val loss: 0.4719 score: 0.8217 time: 0.17s
Test loss: 0.4546 score: 0.8605 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.27s
Val loss: 0.4440 score: 0.8295 time: 0.17s
Test loss: 0.4248 score: 0.8682 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.27s
Val loss: 0.4184 score: 0.8295 time: 0.17s
Test loss: 0.3971 score: 0.8682 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.27s
Val loss: 0.3948 score: 0.8295 time: 0.17s
Test loss: 0.3712 score: 0.8682 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0579;  Loss pred: 0.0579; Loss self: 0.0000; time: 0.27s
Val loss: 0.3734 score: 0.8450 time: 0.17s
Test loss: 0.3475 score: 0.8682 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.28s
Val loss: 0.3539 score: 0.8527 time: 0.17s
Test loss: 0.3243 score: 0.8837 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.27s
Val loss: 0.3357 score: 0.8527 time: 0.17s
Test loss: 0.3010 score: 0.8837 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.27s
Val loss: 0.3213 score: 0.8605 time: 0.17s
Test loss: 0.2823 score: 0.8837 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.27s
Val loss: 0.3093 score: 0.8605 time: 0.17s
Test loss: 0.2656 score: 0.8837 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.27s
Val loss: 0.2979 score: 0.8682 time: 0.18s
Test loss: 0.2495 score: 0.8915 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.27s
Val loss: 0.2900 score: 0.8682 time: 0.18s
Test loss: 0.2367 score: 0.8992 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.27s
Val loss: 0.2835 score: 0.8682 time: 0.17s
Test loss: 0.2260 score: 0.8992 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.27s
Val loss: 0.2782 score: 0.8682 time: 0.17s
Test loss: 0.2171 score: 0.9070 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.26s
Val loss: 0.2751 score: 0.8682 time: 0.18s
Test loss: 0.2105 score: 0.9070 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.27s
Val loss: 0.2747 score: 0.8682 time: 0.18s
Test loss: 0.2068 score: 0.9147 time: 0.19s
Epoch 49/1000, LR 0.000284
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.27s
Val loss: 0.2712 score: 0.8682 time: 0.18s
Test loss: 0.2014 score: 0.9147 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.26s
Val loss: 0.2731 score: 0.8682 time: 0.17s
Test loss: 0.1996 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.27s
Val loss: 0.2752 score: 0.8682 time: 0.18s
Test loss: 0.1972 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.27s
Val loss: 0.2786 score: 0.8605 time: 0.18s
Test loss: 0.1958 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.28s
Val loss: 0.2834 score: 0.8605 time: 0.19s
Test loss: 0.1940 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.27s
Val loss: 0.2869 score: 0.8682 time: 0.18s
Test loss: 0.1920 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.27s
Val loss: 0.2929 score: 0.8682 time: 0.17s
Test loss: 0.1920 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.27s
Val loss: 0.2998 score: 0.8682 time: 0.17s
Test loss: 0.1925 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.27s
Val loss: 0.3093 score: 0.8682 time: 0.18s
Test loss: 0.1955 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.27s
Val loss: 0.3191 score: 0.8605 time: 0.18s
Test loss: 0.1998 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.27s
Val loss: 0.3291 score: 0.8605 time: 0.17s
Test loss: 0.2044 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.26s
Val loss: 0.3397 score: 0.8605 time: 0.17s
Test loss: 0.2104 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.26s
Val loss: 0.3509 score: 0.8605 time: 0.17s
Test loss: 0.2209 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.27s
Val loss: 0.3617 score: 0.8605 time: 0.17s
Test loss: 0.2300 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.26s
Val loss: 0.3690 score: 0.8605 time: 0.17s
Test loss: 0.2349 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.26s
Val loss: 0.3759 score: 0.8605 time: 0.17s
Test loss: 0.2404 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.27s
Val loss: 0.3809 score: 0.8605 time: 0.17s
Test loss: 0.2439 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.28s
Val loss: 0.3834 score: 0.8605 time: 0.17s
Test loss: 0.2451 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.27s
Val loss: 0.3855 score: 0.8605 time: 0.17s
Test loss: 0.2463 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.27s
Val loss: 0.3868 score: 0.8605 time: 0.17s
Test loss: 0.2459 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.27s
Val loss: 0.3897 score: 0.8605 time: 0.17s
Test loss: 0.2461 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0326,   Val_Loss: 0.2712,   Val_Precision: 0.9796,   Val_Recall: 0.7500,   Val_accuracy: 0.8496,   Val_Score: 0.8682,   Val_Loss: 0.2712,   Test_Precision: 0.9821,   Test_Recall: 0.8462,   Test_accuracy: 0.9091,   Test_Score: 0.9147,   Test_loss: 0.2014


[0.18543413700535893, 0.1723783470224589, 0.16878295713104308, 0.16897127288393676, 0.18368708691559732, 0.17179834889248013, 0.1730889449827373, 0.16861047502607107, 0.1698588989675045, 0.1772177719976753, 0.17707654694095254, 0.18939505983144045, 0.19938081200234592, 0.1711140510160476, 0.1682812599465251, 0.17777504189871252, 0.17384412209503353, 0.16730278194881976, 0.16525158402509987, 0.17743603908456862, 0.17443113680928946, 0.17839923198334873, 0.17310673603788018, 0.17516428511589766, 0.17783124302513897, 0.1769693971145898, 0.17086323513649404, 0.17400231305509806, 0.17800032789818943, 0.1789642870426178, 0.17700109095312655, 0.17051518592052162, 0.17707603005692363, 0.1741100049111992, 0.1755172300618142, 0.17553013609722257, 0.17163445497862995, 0.1681620718445629, 0.17324698506854475, 0.18717705295421183, 0.17774125095456839, 0.17982185701839626, 0.1775973029434681, 0.17434748797677457, 0.17738485895097256, 0.18106475821696222, 0.1807046791072935, 0.19030387187376618, 0.18639707285910845, 0.18688628100790083, 0.1746854919474572, 0.1857338948175311, 0.18735633604228497, 0.17620361293666065, 0.18294509290717542, 0.18142586410976946, 0.17568628210574389, 0.18147971504367888, 0.1778096118941903, 0.18065574578940868, 0.16672177286818624, 0.17820612201467156, 0.17606546892784536, 0.17349208798259497, 0.16720386408269405, 0.17832439695484936, 0.17484108684584498, 0.17346730479039252, 0.17938135098665953]
[0.0014374739302741002, 0.0013362662559880535, 0.0013083950165197137, 0.0013098548285576494, 0.00142393090632246, 0.0013317701464533344, 0.0013417747673080411, 0.0013070579459385355, 0.0013167356509108876, 0.0013737811782765527, 0.001372686410394981, 0.0014681787583832593, 0.001545587689940666, 0.001326465511752307, 0.0013045058910583342, 0.0013781010999900196, 0.0013476288534498722, 0.0012969207903009284, 0.0012810200312023247, 0.0013754731711982063, 0.0013521793551107709, 0.001382939782816657, 0.0013419126824641874, 0.001357862675317036, 0.0013785367676367362, 0.0013718557915859675, 0.001324521202608481, 0.0013488551399620005, 0.0013798475030867398, 0.0013873200545939364, 0.001372101480256795, 0.0013218231466707103, 0.0013726824035420436, 0.0013496899605519319, 0.0013605986826497223, 0.0013606987294358338, 0.0013304996509971314, 0.0013035819522834332, 0.0013429998842522848, 0.0014509849066217972, 0.0013778391546865765, 0.001393967883863537, 0.0013767232786315356, 0.00135153091454864, 0.0013750764259765314, 0.001403602776875676, 0.0014008114659480116, 0.0014752238129749316, 0.0014449385492954143, 0.0014487308605263631, 0.0013541511003678853, 0.0014397976342444272, 0.001452374698002209, 0.0013659194801291523, 0.0014181790147843056, 0.001406402047362554, 0.0013619091636104177, 0.001406819496462627, 0.0013783690844510878, 0.001400432137902393, 0.0012924168439394281, 0.0013814428063152835, 0.0013648485963398864, 0.0013448999068418213, 0.0012961539851371632, 0.0013823596663166617, 0.0013553572623708912, 0.0013447077890728102, 0.0013905531084237172]
[695.6647901150584, 748.353852025236, 764.2951764368271, 763.4433818144207, 702.2812662888732, 750.88032470402, 745.281566150083, 765.0770213419636, 759.4538807453287, 727.9179652574132, 728.4985065979178, 681.1159705792147, 647.0030827163157, 753.8831512317009, 766.5737708464535, 725.6361670469911, 742.0440705466069, 771.057112723104, 780.6279180985421, 727.0225409986565, 739.5468627888345, 723.097283356245, 745.2049697925764, 736.4514970311842, 725.4068396843181, 728.9395912699581, 754.98980162086, 741.36945500921, 724.7177661031272, 720.814203390649, 728.8090672512396, 756.5308585484454, 728.5006330813442, 740.9108974857215, 734.970577843374, 734.9165383689416, 751.5973410820203, 767.1170947467778, 744.6017022978002, 689.1870449074578, 725.7741200042138, 717.3766422999566, 726.3623819842728, 739.9016842570427, 727.2323058624431, 712.4522809978558, 713.8719408776692, 677.8632443462278, 692.0709538046606, 690.2593347370766, 738.4700272579092, 694.5420496712909, 688.5275551657118, 732.1075762865953, 705.1295989964233, 711.0342322633236, 734.2633611106651, 710.8232452808957, 725.495087840158, 714.065303798175, 773.7441713866019, 723.8808551671392, 732.6820005396197, 743.5497578018745, 771.5132703882994, 723.4007359781623, 737.8128466665136, 743.6559884058604, 719.1383011135514]
Elapsed: 0.1767872826034284~0.006286501525486196
Time per graph: 0.0013704440511893676~4.873256996500926e-05
Speed: 730.5911941480435~25.367260276649212
Total Time: 0.1798
best val loss: 0.27119351562487987 test_score: 0.9147

Testing...
Test loss: 0.2495 score: 0.8915 time: 0.17s
test Score 0.8915
Epoch Time List: [0.8130465019494295, 0.5956747210584581, 0.5902281829621643, 0.5930412178859115, 0.6080039029475302, 0.6181288450025022, 0.5987870851531625, 0.5988249361980706, 0.5953997226897627, 0.6130038269329816, 0.610873123863712, 0.6415075960103422, 0.6662550021428615, 0.5965893431566656, 0.5963127082213759, 0.6048164791427553, 0.6242989187594503, 0.5919713373295963, 0.5872596111148596, 0.612943772925064, 0.6034148188773543, 0.6015886499080807, 0.6020882916636765, 0.6039122280199081, 0.6108939859550446, 0.6098721909802407, 0.5990523570217192, 0.6095221650321037, 0.6150794129353017, 0.6112177609466016, 0.6175237877760082, 0.6056104449089617, 0.6157239647582173, 0.6073933059815317, 0.6091247161384672, 0.6057290358003229, 0.6080771058332175, 0.5947395160328597, 0.6165289077907801, 0.6225725207477808, 0.6183976850006729, 0.6203250058460981, 0.6150457351468503, 0.6168661019764841, 0.6192654229234904, 0.6225569960661232, 0.6196731009986252, 0.6341119180433452, 0.6323173649143428, 0.6165207042358816, 0.6148477420210838, 0.624348817858845, 0.6467210091650486, 0.616730572655797, 0.6147041830699891, 0.6223325270693749, 0.6129485249985009, 0.6234947531484067, 0.6131765188183635, 0.6090205619111657, 0.5954159768298268, 0.6178178680129349, 0.6048888443037868, 0.6054496350698173, 0.5981034410651773, 0.6191427879966795, 0.6113271997310221, 0.6076659199316055, 0.6170872389338911]
Total Epoch List: [69]
Total Time List: [0.17984144110232592]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e93355f30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 0.27s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6065;  Loss pred: 0.6065; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5039 time: 0.18s
Test loss: 0.6886 score: 0.5116 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.24s
Val loss: 0.6850 score: 0.5891 time: 0.18s
Test loss: 0.6869 score: 0.5271 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5547;  Loss pred: 0.5547; Loss self: 0.0000; time: 0.25s
Val loss: 0.6824 score: 0.6357 time: 0.18s
Test loss: 0.6848 score: 0.5814 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.25s
Val loss: 0.6791 score: 0.7054 time: 0.18s
Test loss: 0.6821 score: 0.6899 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.25s
Val loss: 0.6751 score: 0.7442 time: 0.18s
Test loss: 0.6787 score: 0.7287 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4899;  Loss pred: 0.4899; Loss self: 0.0000; time: 0.25s
Val loss: 0.6701 score: 0.7752 time: 0.18s
Test loss: 0.6746 score: 0.7674 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4663;  Loss pred: 0.4663; Loss self: 0.0000; time: 0.25s
Val loss: 0.6640 score: 0.8527 time: 0.18s
Test loss: 0.6694 score: 0.8062 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.4449;  Loss pred: 0.4449; Loss self: 0.0000; time: 0.25s
Val loss: 0.6567 score: 0.8760 time: 0.18s
Test loss: 0.6632 score: 0.8217 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.4189;  Loss pred: 0.4189; Loss self: 0.0000; time: 0.24s
Val loss: 0.6479 score: 0.8760 time: 0.18s
Test loss: 0.6557 score: 0.8295 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3951;  Loss pred: 0.3951; Loss self: 0.0000; time: 0.24s
Val loss: 0.6378 score: 0.8760 time: 0.18s
Test loss: 0.6469 score: 0.8295 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.24s
Val loss: 0.6260 score: 0.8837 time: 0.18s
Test loss: 0.6366 score: 0.8605 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 0.25s
Val loss: 0.6128 score: 0.8837 time: 0.18s
Test loss: 0.6247 score: 0.8682 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.3183;  Loss pred: 0.3183; Loss self: 0.0000; time: 0.25s
Val loss: 0.5979 score: 0.8837 time: 0.18s
Test loss: 0.6110 score: 0.8837 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.3228;  Loss pred: 0.3228; Loss self: 0.0000; time: 0.25s
Val loss: 0.5809 score: 0.8992 time: 0.18s
Test loss: 0.5953 score: 0.8915 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.2724;  Loss pred: 0.2724; Loss self: 0.0000; time: 0.25s
Val loss: 0.5639 score: 0.8915 time: 0.18s
Test loss: 0.5793 score: 0.8915 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.2443;  Loss pred: 0.2443; Loss self: 0.0000; time: 0.25s
Val loss: 0.5441 score: 0.8992 time: 0.18s
Test loss: 0.5604 score: 0.9070 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.25s
Val loss: 0.5232 score: 0.8992 time: 0.18s
Test loss: 0.5401 score: 0.9147 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.25s
Val loss: 0.4996 score: 0.8992 time: 0.19s
Test loss: 0.5170 score: 0.9147 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1868;  Loss pred: 0.1868; Loss self: 0.0000; time: 0.25s
Val loss: 0.4748 score: 0.9147 time: 0.18s
Test loss: 0.4925 score: 0.9225 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.25s
Val loss: 0.4491 score: 0.9147 time: 0.18s
Test loss: 0.4670 score: 0.9225 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.25s
Val loss: 0.4215 score: 0.9225 time: 0.18s
Test loss: 0.4397 score: 0.9070 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.25s
Val loss: 0.3924 score: 0.9302 time: 0.19s
Test loss: 0.4112 score: 0.9147 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.25s
Val loss: 0.3649 score: 0.9225 time: 0.18s
Test loss: 0.3841 score: 0.9147 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.1033;  Loss pred: 0.1033; Loss self: 0.0000; time: 0.25s
Val loss: 0.3386 score: 0.9302 time: 0.18s
Test loss: 0.3580 score: 0.9225 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.25s
Val loss: 0.3130 score: 0.9302 time: 0.18s
Test loss: 0.3324 score: 0.9225 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.25s
Val loss: 0.2904 score: 0.9302 time: 0.18s
Test loss: 0.3094 score: 0.9147 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0800;  Loss pred: 0.0800; Loss self: 0.0000; time: 0.25s
Val loss: 0.2728 score: 0.9302 time: 0.20s
Test loss: 0.2908 score: 0.9147 time: 0.19s
Epoch 43/1000, LR 0.000284
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.26s
Val loss: 0.2563 score: 0.9380 time: 0.20s
Test loss: 0.2732 score: 0.9147 time: 0.19s
Epoch 44/1000, LR 0.000284
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.25s
Val loss: 0.2427 score: 0.9380 time: 0.19s
Test loss: 0.2591 score: 0.9147 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.26s
Val loss: 0.2317 score: 0.9380 time: 0.19s
Test loss: 0.2483 score: 0.9147 time: 0.19s
Epoch 46/1000, LR 0.000284
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.27s
Val loss: 0.2239 score: 0.9302 time: 0.19s
Test loss: 0.2411 score: 0.9147 time: 0.19s
Epoch 47/1000, LR 0.000284
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.26s
Val loss: 0.2167 score: 0.9302 time: 0.19s
Test loss: 0.2348 score: 0.9147 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.26s
Val loss: 0.2088 score: 0.9302 time: 0.19s
Test loss: 0.2279 score: 0.9147 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.26s
Val loss: 0.2017 score: 0.9380 time: 0.19s
Test loss: 0.2235 score: 0.9225 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.26s
Val loss: 0.1981 score: 0.9380 time: 0.18s
Test loss: 0.2234 score: 0.9225 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.26s
Val loss: 0.1981 score: 0.9380 time: 0.19s
Test loss: 0.2268 score: 0.9225 time: 0.37s
Epoch 52/1000, LR 0.000284
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.27s
Val loss: 0.2012 score: 0.9380 time: 0.20s
Test loss: 0.2327 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.25s
Val loss: 0.2054 score: 0.9380 time: 0.21s
Test loss: 0.2394 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.26s
Val loss: 0.2099 score: 0.9380 time: 0.18s
Test loss: 0.2475 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.28s
Val loss: 0.2148 score: 0.9380 time: 0.18s
Test loss: 0.2552 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.26s
Val loss: 0.2191 score: 0.9380 time: 0.19s
Test loss: 0.2628 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.25s
Val loss: 0.2241 score: 0.9380 time: 0.18s
Test loss: 0.2708 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.25s
Val loss: 0.2270 score: 0.9380 time: 0.18s
Test loss: 0.2770 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.25s
Val loss: 0.2278 score: 0.9380 time: 0.18s
Test loss: 0.2816 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.25s
Val loss: 0.2297 score: 0.9380 time: 0.18s
Test loss: 0.2867 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.25s
Val loss: 0.2311 score: 0.9380 time: 0.18s
Test loss: 0.2907 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.26s
Val loss: 0.2329 score: 0.9302 time: 0.18s
Test loss: 0.2949 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.25s
Val loss: 0.2339 score: 0.9380 time: 0.18s
Test loss: 0.2983 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.25s
Val loss: 0.2359 score: 0.9380 time: 0.19s
Test loss: 0.3031 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.25s
Val loss: 0.2380 score: 0.9380 time: 0.18s
Test loss: 0.3074 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.25s
Val loss: 0.2401 score: 0.9380 time: 0.18s
Test loss: 0.3108 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.25s
Val loss: 0.2423 score: 0.9380 time: 0.19s
Test loss: 0.3144 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.25s
Val loss: 0.2438 score: 0.9380 time: 0.19s
Test loss: 0.3168 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.26s
Val loss: 0.2449 score: 0.9380 time: 0.18s
Test loss: 0.3176 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.27s
Val loss: 0.2463 score: 0.9380 time: 0.18s
Test loss: 0.3178 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.25s
Val loss: 0.2481 score: 0.9380 time: 0.20s
Test loss: 0.3173 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0378,   Val_Loss: 0.1981,   Val_Precision: 0.9524,   Val_Recall: 0.9231,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1981,   Test_Precision: 0.9500,   Test_Recall: 0.8906,   Test_accuracy: 0.9194,   Test_Score: 0.9225,   Test_loss: 0.2268


[0.18543413700535893, 0.1723783470224589, 0.16878295713104308, 0.16897127288393676, 0.18368708691559732, 0.17179834889248013, 0.1730889449827373, 0.16861047502607107, 0.1698588989675045, 0.1772177719976753, 0.17707654694095254, 0.18939505983144045, 0.19938081200234592, 0.1711140510160476, 0.1682812599465251, 0.17777504189871252, 0.17384412209503353, 0.16730278194881976, 0.16525158402509987, 0.17743603908456862, 0.17443113680928946, 0.17839923198334873, 0.17310673603788018, 0.17516428511589766, 0.17783124302513897, 0.1769693971145898, 0.17086323513649404, 0.17400231305509806, 0.17800032789818943, 0.1789642870426178, 0.17700109095312655, 0.17051518592052162, 0.17707603005692363, 0.1741100049111992, 0.1755172300618142, 0.17553013609722257, 0.17163445497862995, 0.1681620718445629, 0.17324698506854475, 0.18717705295421183, 0.17774125095456839, 0.17982185701839626, 0.1775973029434681, 0.17434748797677457, 0.17738485895097256, 0.18106475821696222, 0.1807046791072935, 0.19030387187376618, 0.18639707285910845, 0.18688628100790083, 0.1746854919474572, 0.1857338948175311, 0.18735633604228497, 0.17620361293666065, 0.18294509290717542, 0.18142586410976946, 0.17568628210574389, 0.18147971504367888, 0.1778096118941903, 0.18065574578940868, 0.16672177286818624, 0.17820612201467156, 0.17606546892784536, 0.17349208798259497, 0.16720386408269405, 0.17832439695484936, 0.17484108684584498, 0.17346730479039252, 0.17938135098665953, 0.18738674093037844, 0.18549872399307787, 0.2784011960029602, 0.17504309001378715, 0.17724763602018356, 0.17223338689655066, 0.17561984085477889, 0.1799760297872126, 0.1776832058094442, 0.17389791808091104, 0.17517169378697872, 0.1793372540269047, 0.1923224290367216, 0.1730256958398968, 0.1774326330050826, 0.18895116308704019, 0.17460802802816033, 0.17429601587355137, 0.1780424399767071, 0.17823977605439723, 0.1735318349674344, 0.17354286811314523, 0.175344925140962, 0.17623163806274533, 0.17612915090285242, 0.17454964178614318, 0.17127331998199224, 0.17558388807810843, 0.1787347448989749, 0.17577360686846077, 0.17811024910770357, 0.18336522905156016, 0.17698403098620474, 0.17137151304632425, 0.17390190507285297, 0.17979356599971652, 0.17810400598682463, 0.17452807305380702, 0.18136681918986142, 0.1769318210426718, 0.17345611285418272, 0.1930540429893881, 0.1922977960202843, 0.1766298650763929, 0.19024704000912607, 0.19399501592852175, 0.17888672300614417, 0.1780951819382608, 0.18431729497388005, 0.18266991921700537, 0.370107684051618, 0.19173855986446142, 0.175435284152627, 0.17644824017770588, 0.1931169310119003, 0.17941200081259012, 0.176149089820683, 0.17643259302712977, 0.18055598088540137, 0.17604292486794293, 0.17409514007158577, 0.18074373411946, 0.17743097804486752, 0.1762908729724586, 0.17985345306806266, 0.1797805039677769, 0.17697352496907115, 0.17510111699812114, 0.1805147030390799, 0.17716123699210584, 0.17134463414549828]
[0.0014374739302741002, 0.0013362662559880535, 0.0013083950165197137, 0.0013098548285576494, 0.00142393090632246, 0.0013317701464533344, 0.0013417747673080411, 0.0013070579459385355, 0.0013167356509108876, 0.0013737811782765527, 0.001372686410394981, 0.0014681787583832593, 0.001545587689940666, 0.001326465511752307, 0.0013045058910583342, 0.0013781010999900196, 0.0013476288534498722, 0.0012969207903009284, 0.0012810200312023247, 0.0013754731711982063, 0.0013521793551107709, 0.001382939782816657, 0.0013419126824641874, 0.001357862675317036, 0.0013785367676367362, 0.0013718557915859675, 0.001324521202608481, 0.0013488551399620005, 0.0013798475030867398, 0.0013873200545939364, 0.001372101480256795, 0.0013218231466707103, 0.0013726824035420436, 0.0013496899605519319, 0.0013605986826497223, 0.0013606987294358338, 0.0013304996509971314, 0.0013035819522834332, 0.0013429998842522848, 0.0014509849066217972, 0.0013778391546865765, 0.001393967883863537, 0.0013767232786315356, 0.00135153091454864, 0.0013750764259765314, 0.001403602776875676, 0.0014008114659480116, 0.0014752238129749316, 0.0014449385492954143, 0.0014487308605263631, 0.0013541511003678853, 0.0014397976342444272, 0.001452374698002209, 0.0013659194801291523, 0.0014181790147843056, 0.001406402047362554, 0.0013619091636104177, 0.001406819496462627, 0.0013783690844510878, 0.001400432137902393, 0.0012924168439394281, 0.0013814428063152835, 0.0013648485963398864, 0.0013448999068418213, 0.0012961539851371632, 0.0013823596663166617, 0.0013553572623708912, 0.0013447077890728102, 0.0013905531084237172, 0.0014526103948091353, 0.001437974604597503, 0.0021581488062244977, 0.0013569231784014508, 0.0013740126823270043, 0.001335142534081788, 0.0013613941151533247, 0.0013951630216062993, 0.0013773891923212728, 0.0013480458765962097, 0.001357920106875804, 0.001390211271526393, 0.0014908715429203224, 0.001341284463875169, 0.0013754467674812606, 0.0014647376983491487, 0.0013535506048694599, 0.0013511319059965222, 0.001380173953307807, 0.0013817036903441646, 0.0013452080230033675, 0.0013452935512646917, 0.0013592629855888527, 0.0013661367291685684, 0.0013653422550608714, 0.0013530979983421953, 0.0013277001548991647, 0.001361115411458205, 0.0013855406581315883, 0.0013625860997555099, 0.0013806996054860742, 0.0014214358841206215, 0.0013719692324511996, 0.001328461341444374, 0.0013480767835104882, 0.0013937485736412133, 0.001380651209200191, 0.001352930798866721, 0.0014059443348051273, 0.0013715645042067582, 0.0013446210298773853, 0.0014965429689099852, 0.0014906805893045295, 0.0013692237602821154, 0.001474783255884698, 0.0015038373327792385, 0.0013867187829933656, 0.0013805828057229519, 0.0014288162401075973, 0.0014160458854031424, 0.0028690518143536276, 0.0014863454253059025, 0.00135996344304362, 0.0013678158153310534, 0.0014970304729604676, 0.0013907907039735668, 0.0013654968203153722, 0.0013676945195901533, 0.001399658766553499, 0.001364673836185604, 0.001349574729237099, 0.00140111421798031, 0.0013754339383323063, 0.001366595914515183, 0.001394212814481106, 0.001393647317579666, 0.001371887790457916, 0.0013573729999854353, 0.0013993387832486814, 0.0013733429224194252, 0.0013282529778720796]
[695.6647901150584, 748.353852025236, 764.2951764368271, 763.4433818144207, 702.2812662888732, 750.88032470402, 745.281566150083, 765.0770213419636, 759.4538807453287, 727.9179652574132, 728.4985065979178, 681.1159705792147, 647.0030827163157, 753.8831512317009, 766.5737708464535, 725.6361670469911, 742.0440705466069, 771.057112723104, 780.6279180985421, 727.0225409986565, 739.5468627888345, 723.097283356245, 745.2049697925764, 736.4514970311842, 725.4068396843181, 728.9395912699581, 754.98980162086, 741.36945500921, 724.7177661031272, 720.814203390649, 728.8090672512396, 756.5308585484454, 728.5006330813442, 740.9108974857215, 734.970577843374, 734.9165383689416, 751.5973410820203, 767.1170947467778, 744.6017022978002, 689.1870449074578, 725.7741200042138, 717.3766422999566, 726.3623819842728, 739.9016842570427, 727.2323058624431, 712.4522809978558, 713.8719408776692, 677.8632443462278, 692.0709538046606, 690.2593347370766, 738.4700272579092, 694.5420496712909, 688.5275551657118, 732.1075762865953, 705.1295989964233, 711.0342322633236, 734.2633611106651, 710.8232452808957, 725.495087840158, 714.065303798175, 773.7441713866019, 723.8808551671392, 732.6820005396197, 743.5497578018745, 771.5132703882994, 723.4007359781623, 737.8128466665136, 743.6559884058604, 719.1383011135514, 688.415836464804, 695.4225733909296, 463.3600783763456, 736.9613961330288, 727.7953201322837, 748.9837035921605, 734.5411507727699, 716.762116335814, 726.0112142412922, 741.8145163760903, 736.4203497219888, 719.3151289170908, 670.7485998701121, 745.5540021024715, 727.036497262061, 682.7160938965815, 738.7976455423642, 740.1201877935476, 724.5463498302809, 723.7441768364337, 743.3794497949532, 743.3321887701862, 735.6928060295744, 731.9911533368995, 732.4170890436675, 739.0447707595399, 753.1821069011982, 734.6915563380986, 721.739917288683, 733.8985772564617, 724.2705046243211, 703.5139686364786, 728.8793191180908, 752.7505459155828, 741.7975090379708, 717.4895235138914, 724.2958926456874, 739.1361042542954, 711.2657131895669, 729.0944005425018, 743.703971438844, 668.2066741647619, 670.8345216103912, 730.3408171897044, 678.0657401755734, 664.9655372977755, 721.1267434060452, 724.331779198382, 699.8800629006672, 706.1918051584212, 348.54720817417206, 672.7911177135635, 735.3138829687832, 731.0925848287325, 667.9890744123866, 719.0154472149865, 732.3341842488086, 731.1574227113687, 714.459855427753, 732.775827808861, 740.974158996953, 713.7176877995632, 727.0432785834015, 731.7451994247784, 717.2506159844594, 717.5416530321966, 728.9225889722474, 736.7171735482658, 714.6232291785816, 728.150255610081, 752.8686301927547]
Elapsed: 0.17990191719394977~0.01919941168610907
Time per graph: 0.0013945885053794555~0.00014883264872952764
Speed: 721.745722572674~45.49633801690894
Total Time: 0.1719
best val loss: 0.1980680817839249 test_score: 0.9225

Testing...
Test loss: 0.2732 score: 0.9147 time: 0.17s
test Score 0.9147
Epoch Time List: [0.8130465019494295, 0.5956747210584581, 0.5902281829621643, 0.5930412178859115, 0.6080039029475302, 0.6181288450025022, 0.5987870851531625, 0.5988249361980706, 0.5953997226897627, 0.6130038269329816, 0.610873123863712, 0.6415075960103422, 0.6662550021428615, 0.5965893431566656, 0.5963127082213759, 0.6048164791427553, 0.6242989187594503, 0.5919713373295963, 0.5872596111148596, 0.612943772925064, 0.6034148188773543, 0.6015886499080807, 0.6020882916636765, 0.6039122280199081, 0.6108939859550446, 0.6098721909802407, 0.5990523570217192, 0.6095221650321037, 0.6150794129353017, 0.6112177609466016, 0.6175237877760082, 0.6056104449089617, 0.6157239647582173, 0.6073933059815317, 0.6091247161384672, 0.6057290358003229, 0.6080771058332175, 0.5947395160328597, 0.6165289077907801, 0.6225725207477808, 0.6183976850006729, 0.6203250058460981, 0.6150457351468503, 0.6168661019764841, 0.6192654229234904, 0.6225569960661232, 0.6196731009986252, 0.6341119180433452, 0.6323173649143428, 0.6165207042358816, 0.6148477420210838, 0.624348817858845, 0.6467210091650486, 0.616730572655797, 0.6147041830699891, 0.6223325270693749, 0.6129485249985009, 0.6234947531484067, 0.6131765188183635, 0.6090205619111657, 0.5954159768298268, 0.6178178680129349, 0.6048888443037868, 0.6054496350698173, 0.5981034410651773, 0.6191427879966795, 0.6113271997310221, 0.6076659199316055, 0.6170872389338911, 0.7133586381096393, 0.6323182708583772, 0.7190272200386971, 0.5869296682067215, 0.5990317550022155, 0.6447275099344552, 0.5941620669327676, 0.6086161669809371, 0.5976376759354025, 0.5968286441639066, 0.6093069759663194, 0.6259013200178742, 0.6246942791622132, 0.5975857831072062, 0.6263941971119493, 0.6137983880471438, 0.5949949608184397, 0.5951021236833185, 0.6018469650298357, 0.5968773439526558, 0.5999125372618437, 0.5960889039561152, 0.5970656219869852, 0.5916739101521671, 0.5897946399636567, 0.5932882698252797, 0.595967625034973, 0.5999870740342885, 0.5993420530576259, 0.6053681762423366, 0.6075410828925669, 0.6137979719787836, 0.6105861200485379, 0.5982315989676863, 0.5951246190816164, 0.6093093580566347, 0.6074358220212162, 0.6015504298266023, 0.611983927898109, 0.6076911471318454, 0.6038318758364767, 0.6382508738897741, 0.6477029959205538, 0.6176764890551567, 0.636108084814623, 0.6516661401838064, 0.6190283941105008, 0.622091932920739, 0.6280129116494209, 0.6187148140743375, 0.8079453143291175, 0.6600253311917186, 0.6350337958429009, 0.6149420160800219, 0.6477315730880946, 0.6165868169628084, 0.609299129107967, 0.6038167162332684, 0.6107747261412442, 0.6084672489669174, 0.6057863950263709, 0.6127993662375957, 0.6053333731833845, 0.6104603549465537, 0.6065276470035315, 0.6053598511498421, 0.6068562639411539, 0.6164901978336275, 0.6078854480292648, 0.6236260738223791, 0.6181745200883597]
Total Epoch List: [69, 71]
Total Time List: [0.17984144110232592, 0.17191882920451462]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e93308c40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.16s
Epoch 14/1000, LR 0.000290
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.17s
Epoch 15/1000, LR 0.000290
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.18s
Epoch 16/1000, LR 0.000290
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.18s
Epoch 18/1000, LR 0.000290
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.23s
Epoch 19/1000, LR 0.000290
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5000 time: 0.16s
Epoch 20/1000, LR 0.000290
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5000 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.5000 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6680 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.5000 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6595 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6642 score: 0.5000 time: 0.23s
Epoch 24/1000, LR 0.000290
Train loss: 0.5518;  Loss pred: 0.5518; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6478 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6535 score: 0.5000 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.27s
Val loss: 0.6327 score: 0.5349 time: 0.19s
Test loss: 0.6396 score: 0.5312 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.5201;  Loss pred: 0.5201; Loss self: 0.0000; time: 0.27s
Val loss: 0.6154 score: 0.5814 time: 0.17s
Test loss: 0.6240 score: 0.5703 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.29s
Val loss: 0.5936 score: 0.6434 time: 0.17s
Test loss: 0.6039 score: 0.5859 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.4875;  Loss pred: 0.4875; Loss self: 0.0000; time: 0.27s
Val loss: 0.5703 score: 0.8682 time: 0.18s
Test loss: 0.5823 score: 0.8438 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.4712;  Loss pred: 0.4712; Loss self: 0.0000; time: 0.27s
Val loss: 0.5411 score: 0.9225 time: 0.17s
Test loss: 0.5553 score: 0.8984 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 0.27s
Val loss: 0.5109 score: 0.9225 time: 0.17s
Test loss: 0.5272 score: 0.8984 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.4289;  Loss pred: 0.4289; Loss self: 0.0000; time: 0.27s
Val loss: 0.4785 score: 0.9225 time: 0.17s
Test loss: 0.4964 score: 0.9141 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 0.4121;  Loss pred: 0.4121; Loss self: 0.0000; time: 0.28s
Val loss: 0.4507 score: 0.9147 time: 0.17s
Test loss: 0.4699 score: 0.9141 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3988;  Loss pred: 0.3988; Loss self: 0.0000; time: 0.28s
Val loss: 0.4242 score: 0.9225 time: 0.17s
Test loss: 0.4455 score: 0.9219 time: 0.17s
Epoch 34/1000, LR 0.000290
Train loss: 0.3767;  Loss pred: 0.3767; Loss self: 0.0000; time: 0.28s
Val loss: 0.4059 score: 0.9380 time: 0.17s
Test loss: 0.4305 score: 0.9062 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 0.3587;  Loss pred: 0.3587; Loss self: 0.0000; time: 0.28s
Val loss: 0.3718 score: 0.9380 time: 0.17s
Test loss: 0.3998 score: 0.9062 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.3375;  Loss pred: 0.3375; Loss self: 0.0000; time: 0.27s
Val loss: 0.3512 score: 0.9380 time: 0.17s
Test loss: 0.3822 score: 0.9062 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.3110;  Loss pred: 0.3110; Loss self: 0.0000; time: 0.29s
Val loss: 0.3325 score: 0.9380 time: 0.18s
Test loss: 0.3667 score: 0.9062 time: 0.18s
Epoch 38/1000, LR 0.000289
Train loss: 0.2921;  Loss pred: 0.2921; Loss self: 0.0000; time: 0.28s
Val loss: 0.3092 score: 0.9380 time: 0.18s
Test loss: 0.3472 score: 0.8984 time: 0.17s
Epoch 39/1000, LR 0.000289
Train loss: 0.2638;  Loss pred: 0.2638; Loss self: 0.0000; time: 0.29s
Val loss: 0.2869 score: 0.9225 time: 0.18s
Test loss: 0.3270 score: 0.8984 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.2499;  Loss pred: 0.2499; Loss self: 0.0000; time: 0.28s
Val loss: 0.2651 score: 0.9225 time: 0.18s
Test loss: 0.3059 score: 0.8984 time: 0.18s
Epoch 41/1000, LR 0.000289
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.28s
Val loss: 0.2624 score: 0.9225 time: 0.17s
Test loss: 0.3026 score: 0.9219 time: 0.16s
Epoch 42/1000, LR 0.000289
Train loss: 0.2159;  Loss pred: 0.2159; Loss self: 0.0000; time: 0.27s
Val loss: 0.2451 score: 0.9225 time: 0.18s
Test loss: 0.2854 score: 0.9219 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 0.27s
Val loss: 0.2334 score: 0.9225 time: 0.17s
Test loss: 0.2729 score: 0.9219 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 0.1816;  Loss pred: 0.1816; Loss self: 0.0000; time: 0.28s
Val loss: 0.2273 score: 0.9225 time: 0.17s
Test loss: 0.2653 score: 0.9297 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 0.1743;  Loss pred: 0.1743; Loss self: 0.0000; time: 0.27s
Val loss: 0.2168 score: 0.9302 time: 0.17s
Test loss: 0.2564 score: 0.9062 time: 0.18s
Epoch 46/1000, LR 0.000289
Train loss: 0.1782;  Loss pred: 0.1782; Loss self: 0.0000; time: 0.27s
Val loss: 0.2102 score: 0.9302 time: 0.17s
Test loss: 0.2500 score: 0.9062 time: 0.16s
Epoch 47/1000, LR 0.000289
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.29s
Val loss: 0.2079 score: 0.9225 time: 0.17s
Test loss: 0.2546 score: 0.8906 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.27s
Val loss: 0.2016 score: 0.9302 time: 0.17s
Test loss: 0.2469 score: 0.8906 time: 0.16s
Epoch 49/1000, LR 0.000289
Train loss: 0.1490;  Loss pred: 0.1490; Loss self: 0.0000; time: 0.27s
Val loss: 0.2057 score: 0.9302 time: 0.17s
Test loss: 0.2566 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 0.28s
Val loss: 0.2005 score: 0.9147 time: 0.17s
Test loss: 0.2498 score: 0.8984 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 0.1394;  Loss pred: 0.1394; Loss self: 0.0000; time: 0.28s
Val loss: 0.1993 score: 0.9225 time: 0.17s
Test loss: 0.2474 score: 0.8984 time: 0.17s
Epoch 52/1000, LR 0.000289
Train loss: 0.1294;  Loss pred: 0.1294; Loss self: 0.0000; time: 0.28s
Val loss: 0.1980 score: 0.9225 time: 0.19s
Test loss: 0.2423 score: 0.8984 time: 0.18s
Epoch 53/1000, LR 0.000289
Train loss: 0.1210;  Loss pred: 0.1210; Loss self: 0.0000; time: 0.33s
Val loss: 0.2020 score: 0.9147 time: 0.17s
Test loss: 0.2549 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.29s
Val loss: 0.1965 score: 0.9147 time: 0.17s
Test loss: 0.2499 score: 0.8984 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.29s
Val loss: 0.2021 score: 0.9225 time: 0.18s
Test loss: 0.2614 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.29s
Val loss: 0.1927 score: 0.9225 time: 0.17s
Test loss: 0.2487 score: 0.8984 time: 0.17s
Epoch 57/1000, LR 0.000288
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.29s
Val loss: 0.1988 score: 0.9147 time: 0.17s
Test loss: 0.2620 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0967;  Loss pred: 0.0967; Loss self: 0.0000; time: 0.29s
Val loss: 0.1880 score: 0.9147 time: 0.18s
Test loss: 0.2491 score: 0.8984 time: 0.17s
Epoch 59/1000, LR 0.000288
Train loss: 0.0942;  Loss pred: 0.0942; Loss self: 0.0000; time: 0.29s
Val loss: 0.1823 score: 0.9302 time: 0.17s
Test loss: 0.2455 score: 0.8984 time: 0.17s
Epoch 60/1000, LR 0.000288
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.29s
Val loss: 0.1954 score: 0.9147 time: 0.17s
Test loss: 0.2664 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.31s
Val loss: 0.1989 score: 0.9147 time: 0.18s
Test loss: 0.2738 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.29s
Val loss: 0.2109 score: 0.9225 time: 0.17s
Test loss: 0.2899 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0668;  Loss pred: 0.0668; Loss self: 0.0000; time: 0.29s
Val loss: 0.2094 score: 0.9147 time: 0.17s
Test loss: 0.2911 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.29s
Val loss: 0.1844 score: 0.9302 time: 0.17s
Test loss: 0.2570 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0622;  Loss pred: 0.0622; Loss self: 0.0000; time: 0.29s
Val loss: 0.2023 score: 0.9147 time: 0.18s
Test loss: 0.2685 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.29s
Val loss: 0.1993 score: 0.9147 time: 0.17s
Test loss: 0.2677 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.29s
Val loss: 0.1964 score: 0.9147 time: 0.17s
Test loss: 0.2636 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0550;  Loss pred: 0.0550; Loss self: 0.0000; time: 0.29s
Val loss: 0.1810 score: 0.9147 time: 0.18s
Test loss: 0.2584 score: 0.9062 time: 0.17s
Epoch 69/1000, LR 0.000288
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.29s
Val loss: 0.1803 score: 0.9147 time: 0.17s
Test loss: 0.2584 score: 0.9141 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.0627;  Loss pred: 0.0627; Loss self: 0.0000; time: 0.29s
Val loss: 0.1777 score: 0.9225 time: 0.17s
Test loss: 0.2590 score: 0.9062 time: 0.17s
Epoch 71/1000, LR 0.000287
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.29s
Val loss: 0.1750 score: 0.9302 time: 0.17s
Test loss: 0.2661 score: 0.9062 time: 0.17s
Epoch 72/1000, LR 0.000287
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.28s
Val loss: 0.1790 score: 0.9457 time: 0.18s
Test loss: 0.2845 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.29s
Val loss: 0.1867 score: 0.9225 time: 0.17s
Test loss: 0.2989 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.29s
Val loss: 0.1762 score: 0.9380 time: 0.17s
Test loss: 0.2790 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.28s
Val loss: 0.1838 score: 0.9302 time: 0.18s
Test loss: 0.2905 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.29s
Val loss: 0.1879 score: 0.9225 time: 0.18s
Test loss: 0.2946 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.29s
Val loss: 0.1798 score: 0.9380 time: 0.17s
Test loss: 0.2811 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.28s
Val loss: 0.1791 score: 0.9225 time: 0.18s
Test loss: 0.2691 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.28s
Val loss: 0.1788 score: 0.9302 time: 0.17s
Test loss: 0.2752 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.28s
Val loss: 0.1812 score: 0.9225 time: 0.17s
Test loss: 0.2665 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.29s
Val loss: 0.1833 score: 0.9225 time: 0.17s
Test loss: 0.2551 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.29s
Val loss: 0.2090 score: 0.9147 time: 0.17s
Test loss: 0.2527 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.28s
Val loss: 0.1980 score: 0.9147 time: 0.18s
Test loss: 0.2391 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.29s
Val loss: 0.1841 score: 0.9225 time: 0.18s
Test loss: 0.2469 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.29s
Val loss: 0.1844 score: 0.9302 time: 0.17s
Test loss: 0.2495 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.28s
Val loss: 0.1871 score: 0.9302 time: 0.17s
Test loss: 0.2613 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.28s
Val loss: 0.1923 score: 0.9457 time: 0.17s
Test loss: 0.2791 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.29s
Val loss: 0.1954 score: 0.9380 time: 0.17s
Test loss: 0.2896 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.28s
Val loss: 0.2156 score: 0.9147 time: 0.18s
Test loss: 0.3312 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.28s
Val loss: 0.2053 score: 0.9147 time: 0.17s
Test loss: 0.3137 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.29s
Val loss: 0.2074 score: 0.9302 time: 0.17s
Test loss: 0.3210 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 070,   Train_Loss: 0.0519,   Val_Loss: 0.1750,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1750,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9062,   Test_loss: 0.2661


[0.18543413700535893, 0.1723783470224589, 0.16878295713104308, 0.16897127288393676, 0.18368708691559732, 0.17179834889248013, 0.1730889449827373, 0.16861047502607107, 0.1698588989675045, 0.1772177719976753, 0.17707654694095254, 0.18939505983144045, 0.19938081200234592, 0.1711140510160476, 0.1682812599465251, 0.17777504189871252, 0.17384412209503353, 0.16730278194881976, 0.16525158402509987, 0.17743603908456862, 0.17443113680928946, 0.17839923198334873, 0.17310673603788018, 0.17516428511589766, 0.17783124302513897, 0.1769693971145898, 0.17086323513649404, 0.17400231305509806, 0.17800032789818943, 0.1789642870426178, 0.17700109095312655, 0.17051518592052162, 0.17707603005692363, 0.1741100049111992, 0.1755172300618142, 0.17553013609722257, 0.17163445497862995, 0.1681620718445629, 0.17324698506854475, 0.18717705295421183, 0.17774125095456839, 0.17982185701839626, 0.1775973029434681, 0.17434748797677457, 0.17738485895097256, 0.18106475821696222, 0.1807046791072935, 0.19030387187376618, 0.18639707285910845, 0.18688628100790083, 0.1746854919474572, 0.1857338948175311, 0.18735633604228497, 0.17620361293666065, 0.18294509290717542, 0.18142586410976946, 0.17568628210574389, 0.18147971504367888, 0.1778096118941903, 0.18065574578940868, 0.16672177286818624, 0.17820612201467156, 0.17606546892784536, 0.17349208798259497, 0.16720386408269405, 0.17832439695484936, 0.17484108684584498, 0.17346730479039252, 0.17938135098665953, 0.18738674093037844, 0.18549872399307787, 0.2784011960029602, 0.17504309001378715, 0.17724763602018356, 0.17223338689655066, 0.17561984085477889, 0.1799760297872126, 0.1776832058094442, 0.17389791808091104, 0.17517169378697872, 0.1793372540269047, 0.1923224290367216, 0.1730256958398968, 0.1774326330050826, 0.18895116308704019, 0.17460802802816033, 0.17429601587355137, 0.1780424399767071, 0.17823977605439723, 0.1735318349674344, 0.17354286811314523, 0.175344925140962, 0.17623163806274533, 0.17612915090285242, 0.17454964178614318, 0.17127331998199224, 0.17558388807810843, 0.1787347448989749, 0.17577360686846077, 0.17811024910770357, 0.18336522905156016, 0.17698403098620474, 0.17137151304632425, 0.17390190507285297, 0.17979356599971652, 0.17810400598682463, 0.17452807305380702, 0.18136681918986142, 0.1769318210426718, 0.17345611285418272, 0.1930540429893881, 0.1922977960202843, 0.1766298650763929, 0.19024704000912607, 0.19399501592852175, 0.17888672300614417, 0.1780951819382608, 0.18431729497388005, 0.18266991921700537, 0.370107684051618, 0.19173855986446142, 0.175435284152627, 0.17644824017770588, 0.1931169310119003, 0.17941200081259012, 0.176149089820683, 0.17643259302712977, 0.18055598088540137, 0.17604292486794293, 0.17409514007158577, 0.18074373411946, 0.17743097804486752, 0.1762908729724586, 0.17985345306806266, 0.1797805039677769, 0.17697352496907115, 0.17510111699812114, 0.1805147030390799, 0.17716123699210584, 0.17134463414549828, 0.17104043695144355, 0.17085812496952713, 0.1660997560247779, 0.17076085996814072, 0.16767889820039272, 0.16700198804028332, 0.1710642350371927, 0.1707907908130437, 0.16651802393607795, 0.16715585510246456, 0.17199854901991785, 0.16816882882267237, 0.16693200380541384, 0.1790171298198402, 0.18068204610608518, 0.17743208119645715, 0.1834781039506197, 0.23494806187227368, 0.16380576300434768, 0.18229443882592022, 0.16618205909617245, 0.16314972913824022, 0.23780927690677345, 0.16946561611257493, 0.16661416087299585, 0.16704736999236047, 0.17206847900524735, 0.16493775905109942, 0.16912631411105394, 0.17240080400370061, 0.1675642989575863, 0.16361357201822102, 0.1696169578935951, 0.17184126283973455, 0.16259902785532176, 0.16294918698258698, 0.1812348000239581, 0.1791645959019661, 0.17674923595041037, 0.1818245209287852, 0.16816407185979187, 0.16254073311574757, 0.1675067190080881, 0.1722145259846002, 0.1818448919802904, 0.16337683401070535, 0.1684557090047747, 0.16904042707756162, 0.16629519406706095, 0.1686583620030433, 0.17109062406234443, 0.18511513294652104, 0.17779665114358068, 0.1792930611409247, 0.17162976786494255, 0.17339798202738166, 0.17742406320758164, 0.17502930713817477, 0.1715654751751572, 0.17977680708281696, 0.1731731379404664, 0.17290787398815155, 0.1789600548800081, 0.17302745510824025, 0.17043523001484573, 0.17462991597130895, 0.1763118440285325, 0.17254852992482483, 0.16935128509067, 0.17484325403347611, 0.17673684586770833, 0.17370336409658194, 0.17114997608587146, 0.17745049390941858, 0.17295428412035108, 0.16932719596661627, 0.17345413682051003, 0.17219435283914208, 0.16817389195784926, 0.1735171191394329, 0.17079882510006428, 0.17156049399636686, 0.17406435101293027, 0.1685356239322573, 0.17594503914006054, 0.17180382879450917, 0.16923805605620146, 0.17656158306635916, 0.17280224710702896, 0.17050906992517412, 0.17622657609172165]
[0.0014374739302741002, 0.0013362662559880535, 0.0013083950165197137, 0.0013098548285576494, 0.00142393090632246, 0.0013317701464533344, 0.0013417747673080411, 0.0013070579459385355, 0.0013167356509108876, 0.0013737811782765527, 0.001372686410394981, 0.0014681787583832593, 0.001545587689940666, 0.001326465511752307, 0.0013045058910583342, 0.0013781010999900196, 0.0013476288534498722, 0.0012969207903009284, 0.0012810200312023247, 0.0013754731711982063, 0.0013521793551107709, 0.001382939782816657, 0.0013419126824641874, 0.001357862675317036, 0.0013785367676367362, 0.0013718557915859675, 0.001324521202608481, 0.0013488551399620005, 0.0013798475030867398, 0.0013873200545939364, 0.001372101480256795, 0.0013218231466707103, 0.0013726824035420436, 0.0013496899605519319, 0.0013605986826497223, 0.0013606987294358338, 0.0013304996509971314, 0.0013035819522834332, 0.0013429998842522848, 0.0014509849066217972, 0.0013778391546865765, 0.001393967883863537, 0.0013767232786315356, 0.00135153091454864, 0.0013750764259765314, 0.001403602776875676, 0.0014008114659480116, 0.0014752238129749316, 0.0014449385492954143, 0.0014487308605263631, 0.0013541511003678853, 0.0014397976342444272, 0.001452374698002209, 0.0013659194801291523, 0.0014181790147843056, 0.001406402047362554, 0.0013619091636104177, 0.001406819496462627, 0.0013783690844510878, 0.001400432137902393, 0.0012924168439394281, 0.0013814428063152835, 0.0013648485963398864, 0.0013448999068418213, 0.0012961539851371632, 0.0013823596663166617, 0.0013553572623708912, 0.0013447077890728102, 0.0013905531084237172, 0.0014526103948091353, 0.001437974604597503, 0.0021581488062244977, 0.0013569231784014508, 0.0013740126823270043, 0.001335142534081788, 0.0013613941151533247, 0.0013951630216062993, 0.0013773891923212728, 0.0013480458765962097, 0.001357920106875804, 0.001390211271526393, 0.0014908715429203224, 0.001341284463875169, 0.0013754467674812606, 0.0014647376983491487, 0.0013535506048694599, 0.0013511319059965222, 0.001380173953307807, 0.0013817036903441646, 0.0013452080230033675, 0.0013452935512646917, 0.0013592629855888527, 0.0013661367291685684, 0.0013653422550608714, 0.0013530979983421953, 0.0013277001548991647, 0.001361115411458205, 0.0013855406581315883, 0.0013625860997555099, 0.0013806996054860742, 0.0014214358841206215, 0.0013719692324511996, 0.001328461341444374, 0.0013480767835104882, 0.0013937485736412133, 0.001380651209200191, 0.001352930798866721, 0.0014059443348051273, 0.0013715645042067582, 0.0013446210298773853, 0.0014965429689099852, 0.0014906805893045295, 0.0013692237602821154, 0.001474783255884698, 0.0015038373327792385, 0.0013867187829933656, 0.0013805828057229519, 0.0014288162401075973, 0.0014160458854031424, 0.0028690518143536276, 0.0014863454253059025, 0.00135996344304362, 0.0013678158153310534, 0.0014970304729604676, 0.0013907907039735668, 0.0013654968203153722, 0.0013676945195901533, 0.001399658766553499, 0.001364673836185604, 0.001349574729237099, 0.00140111421798031, 0.0013754339383323063, 0.001366595914515183, 0.001394212814481106, 0.001393647317579666, 0.001371887790457916, 0.0013573729999854353, 0.0013993387832486814, 0.0013733429224194252, 0.0013282529778720796, 0.0013362534136831528, 0.0013348291013244307, 0.0012976543439435773, 0.0013340692185010994, 0.0013099913921905681, 0.0013047030315647135, 0.001336439336228068, 0.001334303053226904, 0.001300922062000609, 0.0013059051179880043, 0.0013437386642181082, 0.0013138189751771279, 0.0013041562797297956, 0.0013985713267175015, 0.0014115784852037905, 0.0013861881343473215, 0.0014334226871142164, 0.0018355317333771382, 0.0012797325234714663, 0.0014241753033275018, 0.0012982973366888473, 0.0012746072588925017, 0.0018578849758341676, 0.0013239501258794917, 0.00130167313182028, 0.0013050575780653162, 0.001344284992228495, 0.0012885762425867142, 0.001321299328992609, 0.001346881281278911, 0.0013090960856061429, 0.0012782310313923517, 0.0013251324835437117, 0.0013425098659354262, 0.0012703049051197013, 0.0012730405233014608, 0.0014158968751871726, 0.0013997234054841101, 0.001380853405862581, 0.0014205040697561344, 0.001313781811404624, 0.001269849477466778, 0.0013086462422506884, 0.001345425984254689, 0.0014206632185960189, 0.0012763815157086356, 0.0013160602265998023, 0.0013206283365434501, 0.0012991812036489137, 0.0013176434531487757, 0.0013366455004870659, 0.0014462119761446957, 0.001389036337059224, 0.0014007270401634742, 0.0013408575614448637, 0.0013546717345889192, 0.0013861254938092316, 0.0013674164620169904, 0.0013403552748059155, 0.0014045063053345075, 0.0013529151401598938, 0.001350842765532434, 0.0013981254287500633, 0.001351776993033127, 0.0013315252344909823, 0.0013642962185258511, 0.0013774362814729102, 0.001348035390037694, 0.0013230569147708593, 0.0013659629221365321, 0.0013807566083414713, 0.0013570575320045464, 0.0013371091881708708, 0.0013863319836673327, 0.0013512053446902428, 0.0013228687184891896, 0.0013551104439102346, 0.0013452683815557975, 0.0013138585309206974, 0.0013556024932768196, 0.0013343658210942522, 0.001340316359346616, 0.0013598777422885178, 0.0013166845619707601, 0.001374570618281723, 0.001342217412457103, 0.0013221723129390739, 0.001379387367705931, 0.0013500175555236638, 0.0013321021087904228, 0.0013767701257165754]
[695.6647901150584, 748.353852025236, 764.2951764368271, 763.4433818144207, 702.2812662888732, 750.88032470402, 745.281566150083, 765.0770213419636, 759.4538807453287, 727.9179652574132, 728.4985065979178, 681.1159705792147, 647.0030827163157, 753.8831512317009, 766.5737708464535, 725.6361670469911, 742.0440705466069, 771.057112723104, 780.6279180985421, 727.0225409986565, 739.5468627888345, 723.097283356245, 745.2049697925764, 736.4514970311842, 725.4068396843181, 728.9395912699581, 754.98980162086, 741.36945500921, 724.7177661031272, 720.814203390649, 728.8090672512396, 756.5308585484454, 728.5006330813442, 740.9108974857215, 734.970577843374, 734.9165383689416, 751.5973410820203, 767.1170947467778, 744.6017022978002, 689.1870449074578, 725.7741200042138, 717.3766422999566, 726.3623819842728, 739.9016842570427, 727.2323058624431, 712.4522809978558, 713.8719408776692, 677.8632443462278, 692.0709538046606, 690.2593347370766, 738.4700272579092, 694.5420496712909, 688.5275551657118, 732.1075762865953, 705.1295989964233, 711.0342322633236, 734.2633611106651, 710.8232452808957, 725.495087840158, 714.065303798175, 773.7441713866019, 723.8808551671392, 732.6820005396197, 743.5497578018745, 771.5132703882994, 723.4007359781623, 737.8128466665136, 743.6559884058604, 719.1383011135514, 688.415836464804, 695.4225733909296, 463.3600783763456, 736.9613961330288, 727.7953201322837, 748.9837035921605, 734.5411507727699, 716.762116335814, 726.0112142412922, 741.8145163760903, 736.4203497219888, 719.3151289170908, 670.7485998701121, 745.5540021024715, 727.036497262061, 682.7160938965815, 738.7976455423642, 740.1201877935476, 724.5463498302809, 723.7441768364337, 743.3794497949532, 743.3321887701862, 735.6928060295744, 731.9911533368995, 732.4170890436675, 739.0447707595399, 753.1821069011982, 734.6915563380986, 721.739917288683, 733.8985772564617, 724.2705046243211, 703.5139686364786, 728.8793191180908, 752.7505459155828, 741.7975090379708, 717.4895235138914, 724.2958926456874, 739.1361042542954, 711.2657131895669, 729.0944005425018, 743.703971438844, 668.2066741647619, 670.8345216103912, 730.3408171897044, 678.0657401755734, 664.9655372977755, 721.1267434060452, 724.331779198382, 699.8800629006672, 706.1918051584212, 348.54720817417206, 672.7911177135635, 735.3138829687832, 731.0925848287325, 667.9890744123866, 719.0154472149865, 732.3341842488086, 731.1574227113687, 714.459855427753, 732.775827808861, 740.974158996953, 713.7176877995632, 727.0432785834015, 731.7451994247784, 717.2506159844594, 717.5416530321966, 728.9225889722474, 736.7171735482658, 714.6232291785816, 728.150255610081, 752.8686301927547, 748.3610442151627, 749.1595733175056, 770.6212402919222, 749.5862929238075, 763.3637945725732, 766.4579416211772, 748.2569338480969, 749.4549289845219, 768.6855571210475, 765.7524166385767, 744.1923244665122, 761.1398669783871, 766.7792698948527, 715.0153738294041, 708.4267792985165, 721.4027989576207, 697.6309283992232, 544.8012593931737, 781.4132888389444, 702.160750620769, 770.2395835998408, 784.5553938464886, 538.2464539017074, 755.3154612494986, 768.2420229428753, 766.2497171063142, 743.8898788435071, 776.0503158063659, 756.8307786566611, 742.4559342382908, 763.8858682683907, 782.3311869613428, 754.6415263519674, 744.8734831480928, 787.2125786255777, 785.520949016323, 706.2661253968841, 714.4268618228461, 724.1898348907845, 703.9754558194792, 761.1613978205821, 787.4949100226426, 764.1484518231146, 743.2590210853994, 703.8965934433479, 783.4648086742379, 759.8436452894094, 757.2153135963692, 769.7155694612686, 758.9306482040324, 748.1415226667099, 691.4615675260793, 719.9235709824078, 713.9149679607051, 745.7913716968066, 738.1862147610648, 721.4353999448387, 731.3061000632994, 746.0708506144393, 711.9939555998168, 739.1446590521674, 740.278606448953, 715.2434105243398, 739.7669920067161, 751.0184366744516, 732.9786496663603, 725.9863947613513, 741.8202870564384, 755.8253835007471, 732.084292914685, 724.2406039983932, 736.8884342897924, 747.8820793745147, 721.3279443749473, 740.0799618870994, 755.9329100638734, 737.9472311603275, 743.3460963703792, 761.1169516852333, 737.6793750081986, 749.4196750183139, 746.0925124330236, 735.3602231308787, 759.4833484667281, 727.4999092080452, 745.0357823695421, 756.3310698717379, 724.9595171101988, 740.7311082055566, 750.6932039226493, 726.3376661950188]
Elapsed: 0.17732928868471867~0.016703762453771084
Time per graph: 0.0013787819025776816~0.000128771476286319
Speed: 729.1616949474744~43.35176299287874
Total Time: 0.1770
best val loss: 0.17504319464860035 test_score: 0.9062

Testing...
Test loss: 0.2845 score: 0.9062 time: 0.17s
test Score 0.9062
Epoch Time List: [0.8130465019494295, 0.5956747210584581, 0.5902281829621643, 0.5930412178859115, 0.6080039029475302, 0.6181288450025022, 0.5987870851531625, 0.5988249361980706, 0.5953997226897627, 0.6130038269329816, 0.610873123863712, 0.6415075960103422, 0.6662550021428615, 0.5965893431566656, 0.5963127082213759, 0.6048164791427553, 0.6242989187594503, 0.5919713373295963, 0.5872596111148596, 0.612943772925064, 0.6034148188773543, 0.6015886499080807, 0.6020882916636765, 0.6039122280199081, 0.6108939859550446, 0.6098721909802407, 0.5990523570217192, 0.6095221650321037, 0.6150794129353017, 0.6112177609466016, 0.6175237877760082, 0.6056104449089617, 0.6157239647582173, 0.6073933059815317, 0.6091247161384672, 0.6057290358003229, 0.6080771058332175, 0.5947395160328597, 0.6165289077907801, 0.6225725207477808, 0.6183976850006729, 0.6203250058460981, 0.6150457351468503, 0.6168661019764841, 0.6192654229234904, 0.6225569960661232, 0.6196731009986252, 0.6341119180433452, 0.6323173649143428, 0.6165207042358816, 0.6148477420210838, 0.624348817858845, 0.6467210091650486, 0.616730572655797, 0.6147041830699891, 0.6223325270693749, 0.6129485249985009, 0.6234947531484067, 0.6131765188183635, 0.6090205619111657, 0.5954159768298268, 0.6178178680129349, 0.6048888443037868, 0.6054496350698173, 0.5981034410651773, 0.6191427879966795, 0.6113271997310221, 0.6076659199316055, 0.6170872389338911, 0.7133586381096393, 0.6323182708583772, 0.7190272200386971, 0.5869296682067215, 0.5990317550022155, 0.6447275099344552, 0.5941620669327676, 0.6086161669809371, 0.5976376759354025, 0.5968286441639066, 0.6093069759663194, 0.6259013200178742, 0.6246942791622132, 0.5975857831072062, 0.6263941971119493, 0.6137983880471438, 0.5949949608184397, 0.5951021236833185, 0.6018469650298357, 0.5968773439526558, 0.5999125372618437, 0.5960889039561152, 0.5970656219869852, 0.5916739101521671, 0.5897946399636567, 0.5932882698252797, 0.595967625034973, 0.5999870740342885, 0.5993420530576259, 0.6053681762423366, 0.6075410828925669, 0.6137979719787836, 0.6105861200485379, 0.5982315989676863, 0.5951246190816164, 0.6093093580566347, 0.6074358220212162, 0.6015504298266023, 0.611983927898109, 0.6076911471318454, 0.6038318758364767, 0.6382508738897741, 0.6477029959205538, 0.6176764890551567, 0.636108084814623, 0.6516661401838064, 0.6190283941105008, 0.622091932920739, 0.6280129116494209, 0.6187148140743375, 0.8079453143291175, 0.6600253311917186, 0.6350337958429009, 0.6149420160800219, 0.6477315730880946, 0.6165868169628084, 0.609299129107967, 0.6038167162332684, 0.6107747261412442, 0.6084672489669174, 0.6057863950263709, 0.6127993662375957, 0.6053333731833845, 0.6104603549465537, 0.6065276470035315, 0.6053598511498421, 0.6068562639411539, 0.6164901978336275, 0.6078854480292648, 0.6236260738223791, 0.6181745200883597, 0.6146709069143981, 0.6139385169371963, 0.607290742918849, 0.6085652960464358, 0.6084889410994947, 0.6129070441238582, 0.617905659833923, 0.6165489719714969, 0.6123190030921251, 0.612516607856378, 0.6171519269701093, 0.6122924727387726, 0.608094149036333, 0.6354156890884042, 0.6449201630894095, 0.6751644350588322, 0.6465565639082342, 0.7005072447936982, 0.6126263081096113, 0.6193881630897522, 0.6558055221103132, 0.6152749380562454, 0.6685875630937517, 0.6108937442768365, 0.6165621040854603, 0.6028678971342742, 0.6232780353166163, 0.6123761960770935, 0.6080407500267029, 0.6065984859596938, 0.6088857769500464, 0.6012114929035306, 0.6078306559938937, 0.6132524930872023, 0.6052015458699316, 0.5990686197765172, 0.6400954241398722, 0.6369759531226009, 0.6379637732170522, 0.6364178203511983, 0.6089113601483405, 0.6166768688708544, 0.6033651002217084, 0.6137725582811981, 0.620636987965554, 0.6007891308981925, 0.6254706108011305, 0.6040162390563637, 0.6043195342645049, 0.6085016450379044, 0.615536866011098, 0.6514128251001239, 0.6785815022885799, 0.6407331638038158, 0.6325612559448928, 0.6357308968435973, 0.639385252725333, 0.6349178990349174, 0.6305192958097905, 0.6411813129670918, 0.6530777127481997, 0.632301646983251, 0.6391906151548028, 0.6329727359116077, 0.63489621412009, 0.6312731199432164, 0.6296000736765563, 0.6316083630081266, 0.6280612589325756, 0.6292802100069821, 0.6319491791073233, 0.6302834679372609, 0.6260321037843823, 0.6328393360599875, 0.6297914830502123, 0.6294296700507402, 0.6241669331211597, 0.6258592626545578, 0.6193774999119341, 0.6247383237350732, 0.6269401051104069, 0.6239230171777308, 0.6294043858069927, 0.6244554070290178, 0.6312893347349018, 0.6239054291509092, 0.6218090031761676, 0.6313806280959398, 0.6284322699066252, 0.623004584107548, 0.6345123671926558]
Total Epoch List: [69, 71, 91]
Total Time List: [0.17984144110232592, 0.17191882920451462, 0.17704724799841642]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e95d4e1a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6712;  Loss pred: 0.6712; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6414;  Loss pred: 0.6414; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.27s
Val loss: 0.6919 score: 0.5194 time: 0.24s
Test loss: 0.6917 score: 0.5659 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.27s
Val loss: 0.6914 score: 0.7132 time: 0.17s
Test loss: 0.6912 score: 0.7209 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 0.5796;  Loss pred: 0.5796; Loss self: 0.0000; time: 0.27s
Val loss: 0.6907 score: 0.8682 time: 0.18s
Test loss: 0.6904 score: 0.9070 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.32s
Val loss: 0.6897 score: 0.8915 time: 0.18s
Test loss: 0.6894 score: 0.8837 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.27s
Val loss: 0.6885 score: 0.8760 time: 0.17s
Test loss: 0.6881 score: 0.8837 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.27s
Val loss: 0.6870 score: 0.8760 time: 0.18s
Test loss: 0.6864 score: 0.8760 time: 0.28s
Epoch 20/1000, LR 0.000285
Train loss: 0.4963;  Loss pred: 0.4963; Loss self: 0.0000; time: 0.26s
Val loss: 0.6850 score: 0.8760 time: 0.17s
Test loss: 0.6842 score: 0.8760 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4594;  Loss pred: 0.4594; Loss self: 0.0000; time: 0.27s
Val loss: 0.6823 score: 0.8682 time: 0.18s
Test loss: 0.6812 score: 0.8682 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4388;  Loss pred: 0.4388; Loss self: 0.0000; time: 0.27s
Val loss: 0.6788 score: 0.8760 time: 0.18s
Test loss: 0.6773 score: 0.8760 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.4117;  Loss pred: 0.4117; Loss self: 0.0000; time: 0.27s
Val loss: 0.6743 score: 0.8450 time: 0.18s
Test loss: 0.6725 score: 0.8527 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3789;  Loss pred: 0.3789; Loss self: 0.0000; time: 0.27s
Val loss: 0.6689 score: 0.8450 time: 0.17s
Test loss: 0.6667 score: 0.8605 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3478;  Loss pred: 0.3478; Loss self: 0.0000; time: 0.27s
Val loss: 0.6624 score: 0.8450 time: 0.18s
Test loss: 0.6600 score: 0.8450 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3218;  Loss pred: 0.3218; Loss self: 0.0000; time: 0.27s
Val loss: 0.6547 score: 0.8605 time: 0.18s
Test loss: 0.6519 score: 0.8915 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2936;  Loss pred: 0.2936; Loss self: 0.0000; time: 0.27s
Val loss: 0.6454 score: 0.8682 time: 0.17s
Test loss: 0.6423 score: 0.8915 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2630;  Loss pred: 0.2630; Loss self: 0.0000; time: 0.27s
Val loss: 0.6345 score: 0.8682 time: 0.17s
Test loss: 0.6312 score: 0.8915 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2390;  Loss pred: 0.2390; Loss self: 0.0000; time: 0.27s
Val loss: 0.6220 score: 0.8682 time: 0.18s
Test loss: 0.6183 score: 0.8915 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2105;  Loss pred: 0.2105; Loss self: 0.0000; time: 0.27s
Val loss: 0.6076 score: 0.8915 time: 0.18s
Test loss: 0.6034 score: 0.8992 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.1944;  Loss pred: 0.1944; Loss self: 0.0000; time: 0.27s
Val loss: 0.5905 score: 0.8915 time: 0.18s
Test loss: 0.5858 score: 0.9147 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 0.27s
Val loss: 0.5717 score: 0.8992 time: 0.18s
Test loss: 0.5661 score: 0.9225 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1553;  Loss pred: 0.1553; Loss self: 0.0000; time: 0.27s
Val loss: 0.5491 score: 0.9070 time: 0.18s
Test loss: 0.5420 score: 0.9302 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.27s
Val loss: 0.5243 score: 0.9070 time: 0.18s
Test loss: 0.5150 score: 0.9302 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1172;  Loss pred: 0.1172; Loss self: 0.0000; time: 0.27s
Val loss: 0.4978 score: 0.9380 time: 0.18s
Test loss: 0.4853 score: 0.9302 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.0894;  Loss pred: 0.0894; Loss self: 0.0000; time: 0.27s
Val loss: 0.4692 score: 0.9302 time: 0.18s
Test loss: 0.4535 score: 0.9302 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.27s
Val loss: 0.4410 score: 0.9302 time: 0.18s
Test loss: 0.4231 score: 0.9225 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.27s
Val loss: 0.4117 score: 0.9302 time: 0.18s
Test loss: 0.3916 score: 0.9225 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 0.0603;  Loss pred: 0.0603; Loss self: 0.0000; time: 0.27s
Val loss: 0.3830 score: 0.9225 time: 0.18s
Test loss: 0.3605 score: 0.9225 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.27s
Val loss: 0.3548 score: 0.9302 time: 0.18s
Test loss: 0.3300 score: 0.9225 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.27s
Val loss: 0.3277 score: 0.9380 time: 0.18s
Test loss: 0.3005 score: 0.9225 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.26s
Val loss: 0.3035 score: 0.9380 time: 0.18s
Test loss: 0.2746 score: 0.9225 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.27s
Val loss: 0.2824 score: 0.9380 time: 0.18s
Test loss: 0.2508 score: 0.9225 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.28s
Val loss: 0.2632 score: 0.9457 time: 0.17s
Test loss: 0.2294 score: 0.9225 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.27s
Val loss: 0.2498 score: 0.9457 time: 0.18s
Test loss: 0.2126 score: 0.9225 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.27s
Val loss: 0.2384 score: 0.9457 time: 0.18s
Test loss: 0.1981 score: 0.9225 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.28s
Val loss: 0.2298 score: 0.9457 time: 0.18s
Test loss: 0.1906 score: 0.9225 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.27s
Val loss: 0.2243 score: 0.9457 time: 0.18s
Test loss: 0.1867 score: 0.9225 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.27s
Val loss: 0.2212 score: 0.9457 time: 0.18s
Test loss: 0.1861 score: 0.9225 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.27s
Val loss: 0.2204 score: 0.9535 time: 0.18s
Test loss: 0.1857 score: 0.9225 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.27s
Val loss: 0.2198 score: 0.9535 time: 0.18s
Test loss: 0.1837 score: 0.9225 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.27s
Val loss: 0.2195 score: 0.9535 time: 0.18s
Test loss: 0.1808 score: 0.9225 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.27s
Val loss: 0.2213 score: 0.9457 time: 0.18s
Test loss: 0.1797 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.27s
Val loss: 0.2244 score: 0.9457 time: 0.17s
Test loss: 0.1771 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.27s
Val loss: 0.2294 score: 0.9457 time: 0.17s
Test loss: 0.1767 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.27s
Val loss: 0.2349 score: 0.9457 time: 0.18s
Test loss: 0.1785 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.26s
Val loss: 0.2405 score: 0.9457 time: 0.18s
Test loss: 0.1824 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.27s
Val loss: 0.2462 score: 0.9457 time: 0.17s
Test loss: 0.1859 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.27s
Val loss: 0.2519 score: 0.9457 time: 0.17s
Test loss: 0.1900 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.27s
Val loss: 0.2572 score: 0.9457 time: 0.18s
Test loss: 0.1981 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.27s
Val loss: 0.2625 score: 0.9457 time: 0.18s
Test loss: 0.2060 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.27s
Val loss: 0.2676 score: 0.9380 time: 0.17s
Test loss: 0.2133 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.27s
Val loss: 0.2719 score: 0.9380 time: 0.17s
Test loss: 0.2184 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.27s
Val loss: 0.2757 score: 0.9457 time: 0.18s
Test loss: 0.2239 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.27s
Val loss: 0.2783 score: 0.9380 time: 0.17s
Test loss: 0.2270 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.27s
Val loss: 0.2799 score: 0.9457 time: 0.18s
Test loss: 0.2263 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.27s
Val loss: 0.2810 score: 0.9457 time: 0.18s
Test loss: 0.2259 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.27s
Val loss: 0.2832 score: 0.9535 time: 0.18s
Test loss: 0.2256 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.27s
Val loss: 0.2856 score: 0.9457 time: 0.17s
Test loss: 0.2305 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.26s
Val loss: 0.2876 score: 0.9457 time: 0.17s
Test loss: 0.2335 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.26s
Val loss: 0.2886 score: 0.9457 time: 0.18s
Test loss: 0.2345 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.27s
Val loss: 0.2900 score: 0.9535 time: 0.20s
Test loss: 0.2352 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.0137,   Val_Loss: 0.2195,   Val_Precision: 0.9531,   Val_Recall: 0.9531,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.2195,   Test_Precision: 0.9508,   Test_Recall: 0.8923,   Test_accuracy: 0.9206,   Test_Score: 0.9225,   Test_loss: 0.1808


[0.1858497450593859, 0.1823831119108945, 0.18040198809467256, 0.18730334704741836, 0.1842186488211155, 0.1796942981891334, 0.1859238320030272, 0.18378118402324617, 0.18101651803590357, 0.18378874310292304, 0.18392640491947532, 0.18285516509786248, 0.1869229159783572, 0.17932669608853757, 0.18623917712830007, 0.18296569399535656, 0.17936349497176707, 0.18456271500326693, 0.28064281214028597, 0.17885788902640343, 0.18761395406909287, 0.18383610900491476, 0.17949093179777265, 0.18322760402224958, 0.18284789402969182, 0.1805664850398898, 0.1826075369026512, 0.1866957221645862, 0.18392970808781683, 0.18202147795818746, 0.18620812706649303, 0.1832968818489462, 0.18079400411807, 0.18552278890274465, 0.1862617889419198, 0.18404808710329235, 0.18180730985477567, 0.1850523988250643, 0.18294882006011903, 0.18482218706049025, 0.1850754979532212, 0.1835078219883144, 0.18103371909819543, 0.1859353028703481, 0.18602465093135834, 0.18149034585803747, 0.1885022590868175, 0.18396054208278656, 0.1817872819956392, 0.1829149080440402, 0.18638891307637095, 0.1829501308966428, 0.18161159311421216, 0.18046820303425193, 0.18468995206058025, 0.18280812492594123, 0.1804063490126282, 0.1816598828881979, 0.18542106496170163, 0.18318424094468355, 0.18291742890141904, 0.18005969002842903, 0.18553335312753916, 0.1866461280733347, 0.17647811281494796, 0.19057129905559123, 0.1862899970728904, 0.18090413394384086, 0.18154355115257204, 0.18359986087307334, 0.19162110495381057, 0.19501268002204597]
[0.0014406956981347745, 0.0014138225729526707, 0.0013984650239897097, 0.0014519639306001422, 0.0014280515412489574, 0.0013929790557297163, 0.0014412700155273426, 0.0014246603412654741, 0.0014032288219837487, 0.0014247189387823491, 0.0014257860846470954, 0.0014174818999834302, 0.0014490148525454047, 0.001390129427042927, 0.001443714551382171, 0.001418338713142299, 0.0013904146897036206, 0.0014307187209555575, 0.0021755256755060927, 0.0013864952637705691, 0.0014543717369697121, 0.0014250861163171687, 0.001391402572075757, 0.0014203690234282913, 0.00141742553511389, 0.0013997401941076729, 0.0014155623015709394, 0.0014472536601905907, 0.0014258116906032312, 0.0014110192089781973, 0.0014434738532286282, 0.0014209060608445442, 0.0014015039078920156, 0.0014381611542848421, 0.0014438898367590684, 0.0014267293573898632, 0.001409358991122292, 0.0014345147195741418, 0.0014182079074427831, 0.0014327301322518624, 0.0014346937825831102, 0.0014225412557233674, 0.0014033621635519025, 0.0014413589369794426, 0.001442051557607429, 0.0014069019058762595, 0.0014612578223784302, 0.001426050713820051, 0.001409203736400304, 0.0014179450235972109, 0.0014448752951656664, 0.0014182180689662233, 0.001407841807086916, 0.001398978318094976, 0.0014317050547331804, 0.0014171172474879165, 0.0013984988295552574, 0.0014082161464201387, 0.0014373725966023382, 0.0014200328755401826, 0.0014179645651272795, 0.001395811550607977, 0.0014382430475003035, 0.0014468692098708116, 0.0013680473861623873, 0.0014772968919038079, 0.0014441085044410108, 0.0014023576274716347, 0.0014073143500199382, 0.0014232547354501809, 0.0014854349221225627, 0.0015117262017212865]
[694.1091038827076, 707.3023299603777, 715.0697249095865, 688.7223428385502, 700.2548375287713, 717.885883414196, 693.8325152307513, 701.9216939188019, 712.6421466930084, 701.8928244574754, 701.3674847636879, 705.4763803415689, 690.1240510015167, 719.3574789127316, 692.657699572695, 705.050204675385, 719.209892850858, 698.9494058846966, 459.65902000553035, 721.2429974556879, 687.582118505391, 701.7119797533968, 718.699260781267, 704.0423886366781, 705.5044340792479, 714.4182929157755, 706.4330541228998, 690.9638769670196, 701.3548890014507, 708.7075736723381, 692.7731997107485, 703.7762928575515, 713.5192377052216, 695.3323673224037, 692.5736122948159, 700.9037802582648, 709.5424276561972, 697.0998529013809, 705.1152336353367, 697.9681500997482, 697.0128484139236, 702.9673100703827, 712.5744344346616, 693.7897107681114, 693.4564819992594, 710.7816087413506, 684.3419310990174, 701.2373335035454, 709.620599328255, 705.2459604273525, 692.1012514684475, 705.1101814891742, 710.3070778024303, 714.807361247546, 698.4678839360283, 705.6579134667027, 715.0524397063774, 710.1182602842076, 695.713833952171, 704.2090484134732, 705.2362411540503, 716.4290907067129, 695.2927752635557, 691.1474742691416, 730.9688320118613, 676.9120042696966, 692.4687424281062, 713.0848653798383, 710.5732986989242, 702.614911506826, 673.203507677794, 661.4954472981792]
Elapsed: 0.18506419892138284~0.011738697290875146
Time per graph: 0.001434606193189014~9.09976534176368e-05
Speed: 698.9826486443447~30.612098095479645
Total Time: 0.1955
best val loss: 0.21949162790479587 test_score: 0.9225

Testing...
Test loss: 0.1857 score: 0.9225 time: 0.18s
test Score 0.9225
Epoch Time List: [0.6240425310097635, 0.6221138262189925, 0.6226655405480415, 0.6310527329333127, 0.6271062032319605, 0.6214880901388824, 0.6262937111314386, 0.6225781959947199, 0.6190061608795077, 0.62494887993671, 0.6204372176434845, 0.7201197370886803, 0.6286700069904327, 0.6855709988158196, 0.6228766723070294, 0.6252848880831152, 0.6668796569574624, 0.6228304281830788, 0.7205240929033607, 0.6137998520862311, 0.631049043033272, 0.6235324337612838, 0.6214899872429669, 0.626433664932847, 0.627000903012231, 0.6226259970571846, 0.6225820230320096, 0.6296154938172549, 0.6319578329566866, 0.6275291461497545, 0.6310963432770222, 0.628041811985895, 0.6242153709754348, 0.6259767939336598, 0.6250264700502157, 0.627432530047372, 0.6251342878676951, 0.6305987709201872, 0.6243568507488817, 0.6262121440377086, 0.6258929730392992, 0.6225024389568716, 0.6246295729652047, 0.6323174161370844, 0.630838364129886, 0.6284477671142668, 0.6343340540770441, 0.6296607288531959, 0.6246544998139143, 0.6236778993625194, 0.6288703149184585, 0.625350131187588, 0.6247397100087255, 0.6200772761367261, 0.6263347996864468, 0.623692360939458, 0.616864250972867, 0.6208627880550921, 0.6272215787321329, 0.623274183832109, 0.6221337311435491, 0.6201037017162889, 0.6251560270320624, 0.6263634259812534, 0.6108649701345712, 0.6327784787863493, 0.626334928907454, 0.6195952058769763, 0.6162825550418347, 0.6133393850177526, 0.633644554996863, 0.6636572419665754]
Total Epoch List: [72]
Total Time List: [0.1955154580064118]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e93330370>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5039 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5039 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.19s
Epoch 12/1000, LR 0.000285
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.30s
Val loss: 0.6918 score: 0.9070 time: 0.17s
Test loss: 0.6915 score: 0.8837 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4961 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4961 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4961 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4961 time: 0.19s
Epoch 20/1000, LR 0.000285
Train loss: 0.4807;  Loss pred: 0.4807; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4961 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4543;  Loss pred: 0.4543; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6794 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4313;  Loss pred: 0.4313; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6759 score: 0.4961 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.4020;  Loss pred: 0.4020; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6735 score: 0.5039 time: 0.17s
Test loss: 0.6710 score: 0.5039 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3775;  Loss pred: 0.3775; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6685 score: 0.5039 time: 0.17s
Test loss: 0.6652 score: 0.5039 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3495;  Loss pred: 0.3495; Loss self: 0.0000; time: 0.28s
Val loss: 0.6618 score: 0.5271 time: 0.17s
Test loss: 0.6575 score: 0.5194 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 0.3147;  Loss pred: 0.3147; Loss self: 0.0000; time: 0.30s
Val loss: 0.6534 score: 0.5271 time: 0.20s
Test loss: 0.6479 score: 0.5271 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2841;  Loss pred: 0.2841; Loss self: 0.0000; time: 0.28s
Val loss: 0.6421 score: 0.5349 time: 0.17s
Test loss: 0.6351 score: 0.5426 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.29s
Val loss: 0.6292 score: 0.5581 time: 0.17s
Test loss: 0.6206 score: 0.5504 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2279;  Loss pred: 0.2279; Loss self: 0.0000; time: 0.33s
Val loss: 0.6136 score: 0.5659 time: 0.18s
Test loss: 0.6034 score: 0.5814 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.28s
Val loss: 0.5957 score: 0.5736 time: 0.17s
Test loss: 0.5839 score: 0.5969 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1795;  Loss pred: 0.1795; Loss self: 0.0000; time: 0.30s
Val loss: 0.5759 score: 0.5736 time: 0.17s
Test loss: 0.5624 score: 0.6124 time: 0.27s
Epoch 32/1000, LR 0.000285
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 0.28s
Val loss: 0.5517 score: 0.5969 time: 0.17s
Test loss: 0.5365 score: 0.6357 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1333;  Loss pred: 0.1333; Loss self: 0.0000; time: 0.29s
Val loss: 0.5256 score: 0.6744 time: 0.17s
Test loss: 0.5088 score: 0.6744 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.29s
Val loss: 0.4974 score: 0.7287 time: 0.21s
Test loss: 0.4791 score: 0.7287 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.29s
Val loss: 0.4661 score: 0.7984 time: 0.18s
Test loss: 0.4465 score: 0.8527 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.28s
Val loss: 0.4364 score: 0.8605 time: 0.17s
Test loss: 0.4155 score: 0.9070 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.28s
Val loss: 0.4033 score: 0.8992 time: 0.17s
Test loss: 0.3816 score: 0.9147 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.28s
Val loss: 0.3729 score: 0.9070 time: 0.17s
Test loss: 0.3507 score: 0.9070 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.28s
Val loss: 0.3463 score: 0.9070 time: 0.17s
Test loss: 0.3234 score: 0.9302 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.28s
Val loss: 0.3209 score: 0.8837 time: 0.17s
Test loss: 0.2977 score: 0.9302 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.28s
Val loss: 0.2982 score: 0.8915 time: 0.17s
Test loss: 0.2749 score: 0.9302 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.28s
Val loss: 0.2767 score: 0.8992 time: 0.17s
Test loss: 0.2540 score: 0.9302 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0316;  Loss pred: 0.0316; Loss self: 0.0000; time: 0.29s
Val loss: 0.2593 score: 0.9070 time: 0.17s
Test loss: 0.2378 score: 0.9147 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.28s
Val loss: 0.2466 score: 0.9070 time: 0.17s
Test loss: 0.2261 score: 0.9070 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.28s
Val loss: 0.2384 score: 0.9070 time: 0.18s
Test loss: 0.2194 score: 0.9147 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.29s
Val loss: 0.2359 score: 0.8992 time: 0.17s
Test loss: 0.2179 score: 0.9147 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.29s
Val loss: 0.2375 score: 0.9070 time: 0.18s
Test loss: 0.2199 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.31s
Val loss: 0.2424 score: 0.8992 time: 0.18s
Test loss: 0.2243 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.30s
Val loss: 0.2498 score: 0.8915 time: 0.18s
Test loss: 0.2303 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.29s
Val loss: 0.2597 score: 0.8915 time: 0.17s
Test loss: 0.2380 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.28s
Val loss: 0.2706 score: 0.8760 time: 0.17s
Test loss: 0.2464 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.29s
Val loss: 0.2826 score: 0.8837 time: 0.19s
Test loss: 0.2561 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.30s
Val loss: 0.2948 score: 0.8760 time: 0.18s
Test loss: 0.2667 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.29s
Val loss: 0.3030 score: 0.8760 time: 0.17s
Test loss: 0.2739 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.29s
Val loss: 0.3132 score: 0.8760 time: 0.17s
Test loss: 0.2843 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.29s
Val loss: 0.3249 score: 0.8837 time: 0.18s
Test loss: 0.2965 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.28s
Val loss: 0.3366 score: 0.8837 time: 0.18s
Test loss: 0.3084 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.28s
Val loss: 0.3477 score: 0.8837 time: 0.17s
Test loss: 0.3199 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.29s
Val loss: 0.3596 score: 0.8760 time: 0.17s
Test loss: 0.3321 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.28s
Val loss: 0.3712 score: 0.8760 time: 0.17s
Test loss: 0.3440 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.28s
Val loss: 0.3797 score: 0.8682 time: 0.17s
Test loss: 0.3526 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.29s
Val loss: 0.3877 score: 0.8682 time: 0.17s
Test loss: 0.3602 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.28s
Val loss: 0.3932 score: 0.8682 time: 0.17s
Test loss: 0.3652 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.29s
Val loss: 0.4005 score: 0.8682 time: 0.17s
Test loss: 0.3709 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.29s
Val loss: 0.4071 score: 0.8682 time: 0.17s
Test loss: 0.3743 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.28s
Val loss: 0.4144 score: 0.8682 time: 0.17s
Test loss: 0.3762 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.0238,   Val_Loss: 0.2359,   Val_Precision: 0.9483,   Val_Recall: 0.8462,   Val_accuracy: 0.8943,   Val_Score: 0.8992,   Val_Loss: 0.2359,   Test_Precision: 0.9649,   Test_Recall: 0.8594,   Test_accuracy: 0.9091,   Test_Score: 0.9147,   Test_loss: 0.2179


[0.1858497450593859, 0.1823831119108945, 0.18040198809467256, 0.18730334704741836, 0.1842186488211155, 0.1796942981891334, 0.1859238320030272, 0.18378118402324617, 0.18101651803590357, 0.18378874310292304, 0.18392640491947532, 0.18285516509786248, 0.1869229159783572, 0.17932669608853757, 0.18623917712830007, 0.18296569399535656, 0.17936349497176707, 0.18456271500326693, 0.28064281214028597, 0.17885788902640343, 0.18761395406909287, 0.18383610900491476, 0.17949093179777265, 0.18322760402224958, 0.18284789402969182, 0.1805664850398898, 0.1826075369026512, 0.1866957221645862, 0.18392970808781683, 0.18202147795818746, 0.18620812706649303, 0.1832968818489462, 0.18079400411807, 0.18552278890274465, 0.1862617889419198, 0.18404808710329235, 0.18180730985477567, 0.1850523988250643, 0.18294882006011903, 0.18482218706049025, 0.1850754979532212, 0.1835078219883144, 0.18103371909819543, 0.1859353028703481, 0.18602465093135834, 0.18149034585803747, 0.1885022590868175, 0.18396054208278656, 0.1817872819956392, 0.1829149080440402, 0.18638891307637095, 0.1829501308966428, 0.18161159311421216, 0.18046820303425193, 0.18468995206058025, 0.18280812492594123, 0.1804063490126282, 0.1816598828881979, 0.18542106496170163, 0.18318424094468355, 0.18291742890141904, 0.18005969002842903, 0.18553335312753916, 0.1866461280733347, 0.17647811281494796, 0.19057129905559123, 0.1862899970728904, 0.18090413394384086, 0.18154355115257204, 0.18359986087307334, 0.19162110495381057, 0.19501268002204597, 0.17320452095009387, 0.17325257696211338, 0.17920694290660322, 0.1741414030548185, 0.17082839692011476, 0.1755992379039526, 0.18612807081080973, 0.1730376509949565, 0.17568615498021245, 0.17895989678800106, 0.19288643496111035, 0.186289899982512, 0.17332805809564888, 0.17019663518294692, 0.17211401392705739, 0.1790465679951012, 0.17960361507721245, 0.1817018729634583, 0.19001079304143786, 0.1758200698532164, 0.1713195729535073, 0.17778940917924047, 0.17623615288175642, 0.17176993703469634, 0.1692962411325425, 0.1721703209914267, 0.1705483489204198, 0.17471687006764114, 0.18114617210812867, 0.17358006979338825, 0.2723633898422122, 0.17404037108644843, 0.17771459184587002, 0.18267009919509292, 0.17094867210835218, 0.1690224870108068, 0.17390044615603983, 0.17207219498232007, 0.17282027495093644, 0.17543746903538704, 0.17226128792390227, 0.17678371793590486, 0.17161538102664053, 0.1757310840766877, 0.17648517899215221, 0.17525455285795033, 0.1862394018098712, 0.18054466391913593, 0.1738675490487367, 0.1780996338929981, 0.17327199294231832, 0.17898157006129622, 0.1821873621083796, 0.1714694129768759, 0.17549788183532655, 0.1814820149447769, 0.17494153999723494, 0.17310402705334127, 0.17633396899327636, 0.1738412780687213, 0.17066727206110954, 0.17710148892365396, 0.17465642094612122, 0.171490082051605, 0.17617515102028847, 0.17323633399792016]
[0.0014406956981347745, 0.0014138225729526707, 0.0013984650239897097, 0.0014519639306001422, 0.0014280515412489574, 0.0013929790557297163, 0.0014412700155273426, 0.0014246603412654741, 0.0014032288219837487, 0.0014247189387823491, 0.0014257860846470954, 0.0014174818999834302, 0.0014490148525454047, 0.001390129427042927, 0.001443714551382171, 0.001418338713142299, 0.0013904146897036206, 0.0014307187209555575, 0.0021755256755060927, 0.0013864952637705691, 0.0014543717369697121, 0.0014250861163171687, 0.001391402572075757, 0.0014203690234282913, 0.00141742553511389, 0.0013997401941076729, 0.0014155623015709394, 0.0014472536601905907, 0.0014258116906032312, 0.0014110192089781973, 0.0014434738532286282, 0.0014209060608445442, 0.0014015039078920156, 0.0014381611542848421, 0.0014438898367590684, 0.0014267293573898632, 0.001409358991122292, 0.0014345147195741418, 0.0014182079074427831, 0.0014327301322518624, 0.0014346937825831102, 0.0014225412557233674, 0.0014033621635519025, 0.0014413589369794426, 0.001442051557607429, 0.0014069019058762595, 0.0014612578223784302, 0.001426050713820051, 0.001409203736400304, 0.0014179450235972109, 0.0014448752951656664, 0.0014182180689662233, 0.001407841807086916, 0.001398978318094976, 0.0014317050547331804, 0.0014171172474879165, 0.0013984988295552574, 0.0014082161464201387, 0.0014373725966023382, 0.0014200328755401826, 0.0014179645651272795, 0.001395811550607977, 0.0014382430475003035, 0.0014468692098708116, 0.0013680473861623873, 0.0014772968919038079, 0.0014441085044410108, 0.0014023576274716347, 0.0014073143500199382, 0.0014232547354501809, 0.0014854349221225627, 0.0015117262017212865, 0.0013426707050394873, 0.0013430432322644448, 0.0013892011078031256, 0.0013499333570140969, 0.0013242511389156182, 0.0013612344023562217, 0.0014428532620993002, 0.0013413771394957868, 0.0013619081781411817, 0.0013872860216124112, 0.0014952436818690725, 0.0014441077518024185, 0.0013436283573306114, 0.0013193537611081157, 0.001334217162225251, 0.0013879578914348931, 0.001392276085869864, 0.0014085416508795216, 0.001472951884042154, 0.0013629462779319102, 0.0013280587050659481, 0.001378212474257678, 0.0013661717277655538, 0.00133154989949377, 0.0013123739622677712, 0.001334653651096331, 0.0013220802241893008, 0.0013543943416096213, 0.001404233892311075, 0.0013455819363828547, 0.0021113441073039706, 0.0013491501634608405, 0.001377632494929225, 0.0014160472805821156, 0.0013251835047159083, 0.001310251837293076, 0.001348065474077828, 0.0013338929843590703, 0.0013396920538832281, 0.0013599803801192794, 0.0013353588211155215, 0.0013704164181077897, 0.0013303517909041902, 0.0013622564657107573, 0.001368102162729862, 0.001358562425255429, 0.0014437162930997767, 0.0013995710381328366, 0.0013478104577421448, 0.0013806173169999853, 0.0013431937437389017, 0.0013874540314829165, 0.0014123051326230976, 0.0013292202556346969, 0.0013604486963978803, 0.0014068373251533092, 0.0013561359689708136, 0.001341891682584041, 0.0013669299921959407, 0.0013476068067342737, 0.0013230021090008491, 0.0013728797590980928, 0.0013539257437683815, 0.0013293804810201937, 0.0013656988451185154, 0.0013429173178133346]
[694.1091038827076, 707.3023299603777, 715.0697249095865, 688.7223428385502, 700.2548375287713, 717.885883414196, 693.8325152307513, 701.9216939188019, 712.6421466930084, 701.8928244574754, 701.3674847636879, 705.4763803415689, 690.1240510015167, 719.3574789127316, 692.657699572695, 705.050204675385, 719.209892850858, 698.9494058846966, 459.65902000553035, 721.2429974556879, 687.582118505391, 701.7119797533968, 718.699260781267, 704.0423886366781, 705.5044340792479, 714.4182929157755, 706.4330541228998, 690.9638769670196, 701.3548890014507, 708.7075736723381, 692.7731997107485, 703.7762928575515, 713.5192377052216, 695.3323673224037, 692.5736122948159, 700.9037802582648, 709.5424276561972, 697.0998529013809, 705.1152336353367, 697.9681500997482, 697.0128484139236, 702.9673100703827, 712.5744344346616, 693.7897107681114, 693.4564819992594, 710.7816087413506, 684.3419310990174, 701.2373335035454, 709.620599328255, 705.2459604273525, 692.1012514684475, 705.1101814891742, 710.3070778024303, 714.807361247546, 698.4678839360283, 705.6579134667027, 715.0524397063774, 710.1182602842076, 695.713833952171, 704.2090484134732, 705.2362411540503, 716.4290907067129, 695.2927752635557, 691.1474742691416, 730.9688320118613, 676.9120042696966, 692.4687424281062, 713.0848653798383, 710.5732986989242, 702.614911506826, 673.203507677794, 661.4954472981792, 744.784254431611, 744.5776695616454, 719.8381820911401, 740.777309342062, 755.1437719123761, 734.6273340352368, 693.071171038582, 745.5024918464708, 734.2638924195779, 720.8318864466914, 668.7873101392998, 692.4691033282529, 744.2534198866579, 757.9468293326479, 749.5031755791293, 720.4829528121952, 718.2483489797368, 709.954156751829, 678.9088026797903, 733.7046339914198, 752.9787622982694, 725.5775279052035, 731.9724011823557, 751.0045251628804, 761.9779336920159, 749.2580559597354, 756.3837516843577, 738.3374023931291, 712.1320781926217, 743.1728778168384, 473.6319373713675, 741.2073370949306, 725.8829939630413, 706.191109373774, 754.6124717379266, 763.2120570545866, 741.8037322586802, 749.6853283777451, 746.4401965372582, 735.3047254345635, 748.8623912819387, 729.7052098812095, 751.6808763194414, 734.0761634618119, 730.9395652183136, 736.0721755660112, 692.656863941681, 714.5046394601712, 741.9440873572106, 724.3136730842633, 744.494235966592, 720.7445993228301, 708.0622854798265, 752.3207653215488, 735.0516066116597, 710.8142370981134, 737.3891872795845, 745.2166318479073, 731.5663609030362, 742.0562103150492, 755.8566938001443, 728.3959089446738, 738.5929432264875, 752.2300908409454, 732.2258516761212, 744.6474825630329]
Elapsed: 0.18142426432198103~0.012799224064870338
Time per graph: 0.0014063896459068294~9.921879120054525e-05
Speed: 713.6204010236237~37.2385160332521
Total Time: 0.1739
best val loss: 0.2359337802369927 test_score: 0.9147

Testing...
Test loss: 0.6915 score: 0.8837 time: 0.17s
test Score 0.8837
Epoch Time List: [0.6240425310097635, 0.6221138262189925, 0.6226655405480415, 0.6310527329333127, 0.6271062032319605, 0.6214880901388824, 0.6262937111314386, 0.6225781959947199, 0.6190061608795077, 0.62494887993671, 0.6204372176434845, 0.7201197370886803, 0.6286700069904327, 0.6855709988158196, 0.6228766723070294, 0.6252848880831152, 0.6668796569574624, 0.6228304281830788, 0.7205240929033607, 0.6137998520862311, 0.631049043033272, 0.6235324337612838, 0.6214899872429669, 0.626433664932847, 0.627000903012231, 0.6226259970571846, 0.6225820230320096, 0.6296154938172549, 0.6319578329566866, 0.6275291461497545, 0.6310963432770222, 0.628041811985895, 0.6242153709754348, 0.6259767939336598, 0.6250264700502157, 0.627432530047372, 0.6251342878676951, 0.6305987709201872, 0.6243568507488817, 0.6262121440377086, 0.6258929730392992, 0.6225024389568716, 0.6246295729652047, 0.6323174161370844, 0.630838364129886, 0.6284477671142668, 0.6343340540770441, 0.6296607288531959, 0.6246544998139143, 0.6236778993625194, 0.6288703149184585, 0.625350131187588, 0.6247397100087255, 0.6200772761367261, 0.6263347996864468, 0.623692360939458, 0.616864250972867, 0.6208627880550921, 0.6272215787321329, 0.623274183832109, 0.6221337311435491, 0.6201037017162889, 0.6251560270320624, 0.6263634259812534, 0.6108649701345712, 0.6327784787863493, 0.626334928907454, 0.6195952058769763, 0.6162825550418347, 0.6133393850177526, 0.633644554996863, 0.6636572419665754, 0.6232919353060424, 0.6171943119261414, 0.6226451001130044, 0.6274183252826333, 0.6218560680281371, 0.6208632760681212, 0.6240079537965357, 0.6500159201677889, 0.6408078649546951, 0.6262266980484128, 0.6712542050518095, 0.692071029683575, 0.6405447779688984, 0.6141836568713188, 0.6186262730043381, 0.6195051879622042, 0.6283271911088377, 0.6378062050789595, 0.6624798958655447, 0.6282363880891353, 0.6212422109674662, 0.6259307032451034, 0.6242025478277355, 0.6180183799006045, 0.6203272852580994, 0.6676125170197338, 0.617757469881326, 0.627587212016806, 0.6871054449584335, 0.6214957349002361, 0.7401526430621743, 0.623747066128999, 0.6386269151698798, 0.6773707850370556, 0.6334853917360306, 0.6124067700002342, 0.6159610298927873, 0.6163073177449405, 0.6234865912701935, 0.6206158772110939, 0.6194411478936672, 0.6207806773018092, 0.6191794141195714, 0.6177555231843144, 0.6370587865822017, 0.6340028862468898, 0.6482064900919795, 0.6643105207476765, 0.6442299261689186, 0.62940460187383, 0.6245557989459485, 0.6577501541469246, 0.654516956070438, 0.6273260607849807, 0.6281142900697887, 0.6402629390358925, 0.6322966597508639, 0.6257552709430456, 0.6293020688463002, 0.6253847400657833, 0.6214850009419024, 0.6322019889485091, 0.628025118028745, 0.6275990677531809, 0.630090267630294, 0.6226993231102824]
Total Epoch List: [72, 66]
Total Time List: [0.1955154580064118, 0.17390540800988674]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e93308d30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
Epoch 5/1000, LR 0.000110
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.16s
Epoch 12/1000, LR 0.000290
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.18s
Epoch 13/1000, LR 0.000290
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.18s
Epoch 14/1000, LR 0.000290
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.21s
Epoch 19/1000, LR 0.000290
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.20s
Epoch 20/1000, LR 0.000290
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6828 score: 0.5000 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6769 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6776 score: 0.5000 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6698 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6707 score: 0.5000 time: 0.19s
Epoch 24/1000, LR 0.000290
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6614 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6622 score: 0.5000 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6485 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6493 score: 0.5000 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.4879;  Loss pred: 0.4879; Loss self: 0.0000; time: 0.28s
Val loss: 0.6354 score: 0.5039 time: 0.17s
Test loss: 0.6360 score: 0.5312 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 0.4706;  Loss pred: 0.4706; Loss self: 0.0000; time: 0.28s
Val loss: 0.6170 score: 0.5969 time: 0.17s
Test loss: 0.6173 score: 0.6250 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.4507;  Loss pred: 0.4507; Loss self: 0.0000; time: 0.28s
Val loss: 0.5936 score: 0.7442 time: 0.17s
Test loss: 0.5938 score: 0.7891 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.4248;  Loss pred: 0.4248; Loss self: 0.0000; time: 0.28s
Val loss: 0.5721 score: 0.8140 time: 0.17s
Test loss: 0.5717 score: 0.8203 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.4038;  Loss pred: 0.4038; Loss self: 0.0000; time: 0.28s
Val loss: 0.5451 score: 0.8605 time: 0.17s
Test loss: 0.5445 score: 0.8750 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.3784;  Loss pred: 0.3784; Loss self: 0.0000; time: 0.27s
Val loss: 0.5182 score: 0.8760 time: 0.17s
Test loss: 0.5166 score: 0.8828 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 0.3592;  Loss pred: 0.3592; Loss self: 0.0000; time: 0.27s
Val loss: 0.4941 score: 0.8837 time: 0.17s
Test loss: 0.4902 score: 0.8906 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3390;  Loss pred: 0.3390; Loss self: 0.0000; time: 0.27s
Val loss: 0.4848 score: 0.8760 time: 0.17s
Test loss: 0.4781 score: 0.8828 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.3162;  Loss pred: 0.3162; Loss self: 0.0000; time: 0.28s
Val loss: 0.4814 score: 0.8682 time: 0.17s
Test loss: 0.4727 score: 0.8750 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.2977;  Loss pred: 0.2977; Loss self: 0.0000; time: 0.27s
Val loss: 0.4664 score: 0.8760 time: 0.17s
Test loss: 0.4572 score: 0.8750 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 0.27s
Val loss: 0.4293 score: 0.8837 time: 0.20s
Test loss: 0.4210 score: 0.8906 time: 0.19s
Epoch 37/1000, LR 0.000290
Train loss: 0.2608;  Loss pred: 0.2608; Loss self: 0.0000; time: 0.28s
Val loss: 0.4047 score: 0.9070 time: 0.18s
Test loss: 0.3966 score: 0.8984 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 0.2225;  Loss pred: 0.2225; Loss self: 0.0000; time: 0.27s
Val loss: 0.4075 score: 0.8915 time: 0.18s
Test loss: 0.3984 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.2132;  Loss pred: 0.2132; Loss self: 0.0000; time: 0.27s
Val loss: 0.4015 score: 0.8837 time: 0.19s
Test loss: 0.3922 score: 0.8984 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.1995;  Loss pred: 0.1995; Loss self: 0.0000; time: 0.31s
Val loss: 0.3931 score: 0.8915 time: 0.19s
Test loss: 0.3842 score: 0.9062 time: 0.19s
Epoch 41/1000, LR 0.000289
Train loss: 0.1712;  Loss pred: 0.1712; Loss self: 0.0000; time: 0.31s
Val loss: 0.3887 score: 0.8915 time: 0.18s
Test loss: 0.3805 score: 0.8984 time: 0.18s
Epoch 42/1000, LR 0.000289
Train loss: 0.1555;  Loss pred: 0.1555; Loss self: 0.0000; time: 0.29s
Val loss: 0.3565 score: 0.9070 time: 0.19s
Test loss: 0.3501 score: 0.8984 time: 0.18s
Epoch 43/1000, LR 0.000289
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 0.29s
Val loss: 0.3186 score: 0.9147 time: 0.20s
Test loss: 0.3132 score: 0.8906 time: 0.20s
Epoch 44/1000, LR 0.000289
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 0.27s
Val loss: 0.3034 score: 0.9147 time: 0.19s
Test loss: 0.2985 score: 0.9062 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 0.1149;  Loss pred: 0.1149; Loss self: 0.0000; time: 0.27s
Val loss: 0.3253 score: 0.9147 time: 0.18s
Test loss: 0.3208 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 0.31s
Val loss: 0.3421 score: 0.9070 time: 0.20s
Test loss: 0.3392 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.29s
Val loss: 0.3548 score: 0.8992 time: 0.19s
Test loss: 0.3524 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 0.29s
Val loss: 0.3384 score: 0.9070 time: 0.18s
Test loss: 0.3376 score: 0.8984 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.30s
Val loss: 0.2994 score: 0.9147 time: 0.18s
Test loss: 0.2986 score: 0.9062 time: 0.17s
Epoch 50/1000, LR 0.000289
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.28s
Val loss: 0.2836 score: 0.9070 time: 0.21s
Test loss: 0.2799 score: 0.9141 time: 0.17s
Epoch 51/1000, LR 0.000289
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.33s
Val loss: 0.2812 score: 0.9070 time: 0.19s
Test loss: 0.2751 score: 0.9141 time: 0.18s
Epoch 52/1000, LR 0.000289
Train loss: 0.0536;  Loss pred: 0.0536; Loss self: 0.0000; time: 0.29s
Val loss: 0.2813 score: 0.8992 time: 0.19s
Test loss: 0.2741 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.29s
Val loss: 0.2870 score: 0.8992 time: 0.19s
Test loss: 0.2804 score: 0.9141 time: 0.28s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.28s
Val loss: 0.3165 score: 0.9147 time: 0.19s
Test loss: 0.3205 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.29s
Val loss: 0.3677 score: 0.9225 time: 0.19s
Test loss: 0.3851 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.29s
Val loss: 0.3931 score: 0.9070 time: 0.19s
Test loss: 0.4144 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.28s
Val loss: 0.3897 score: 0.9070 time: 0.19s
Test loss: 0.4143 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.29s
Val loss: 0.3339 score: 0.9225 time: 0.19s
Test loss: 0.3568 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.29s
Val loss: 0.3293 score: 0.9302 time: 0.19s
Test loss: 0.3586 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.29s
Val loss: 0.3608 score: 0.9147 time: 0.19s
Test loss: 0.4005 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.30s
Val loss: 0.3039 score: 0.9225 time: 0.19s
Test loss: 0.3312 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.29s
Val loss: 0.2966 score: 0.8992 time: 0.19s
Test loss: 0.3213 score: 0.9141 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.30s
Val loss: 0.3020 score: 0.9302 time: 0.19s
Test loss: 0.3354 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.29s
Val loss: 0.2858 score: 0.8992 time: 0.18s
Test loss: 0.3065 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.29s
Val loss: 0.2855 score: 0.8992 time: 0.18s
Test loss: 0.3009 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.28s
Val loss: 0.3158 score: 0.9302 time: 0.18s
Test loss: 0.3558 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.29s
Val loss: 0.3059 score: 0.9147 time: 0.17s
Test loss: 0.3371 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.28s
Val loss: 0.3104 score: 0.9147 time: 0.18s
Test loss: 0.3371 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.29s
Val loss: 0.3318 score: 0.9147 time: 0.18s
Test loss: 0.3727 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.29s
Val loss: 0.3312 score: 0.9147 time: 0.19s
Test loss: 0.3705 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.28s
Val loss: 0.3877 score: 0.9225 time: 0.17s
Test loss: 0.4471 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0653,   Val_Loss: 0.2812,   Val_Precision: 0.9492,   Val_Recall: 0.8615,   Val_accuracy: 0.9032,   Val_Score: 0.9070,   Val_Loss: 0.2812,   Test_Precision: 0.9649,   Test_Recall: 0.8594,   Test_accuracy: 0.9091,   Test_Score: 0.9141,   Test_loss: 0.2751


[0.1858497450593859, 0.1823831119108945, 0.18040198809467256, 0.18730334704741836, 0.1842186488211155, 0.1796942981891334, 0.1859238320030272, 0.18378118402324617, 0.18101651803590357, 0.18378874310292304, 0.18392640491947532, 0.18285516509786248, 0.1869229159783572, 0.17932669608853757, 0.18623917712830007, 0.18296569399535656, 0.17936349497176707, 0.18456271500326693, 0.28064281214028597, 0.17885788902640343, 0.18761395406909287, 0.18383610900491476, 0.17949093179777265, 0.18322760402224958, 0.18284789402969182, 0.1805664850398898, 0.1826075369026512, 0.1866957221645862, 0.18392970808781683, 0.18202147795818746, 0.18620812706649303, 0.1832968818489462, 0.18079400411807, 0.18552278890274465, 0.1862617889419198, 0.18404808710329235, 0.18180730985477567, 0.1850523988250643, 0.18294882006011903, 0.18482218706049025, 0.1850754979532212, 0.1835078219883144, 0.18103371909819543, 0.1859353028703481, 0.18602465093135834, 0.18149034585803747, 0.1885022590868175, 0.18396054208278656, 0.1817872819956392, 0.1829149080440402, 0.18638891307637095, 0.1829501308966428, 0.18161159311421216, 0.18046820303425193, 0.18468995206058025, 0.18280812492594123, 0.1804063490126282, 0.1816598828881979, 0.18542106496170163, 0.18318424094468355, 0.18291742890141904, 0.18005969002842903, 0.18553335312753916, 0.1866461280733347, 0.17647811281494796, 0.19057129905559123, 0.1862899970728904, 0.18090413394384086, 0.18154355115257204, 0.18359986087307334, 0.19162110495381057, 0.19501268002204597, 0.17320452095009387, 0.17325257696211338, 0.17920694290660322, 0.1741414030548185, 0.17082839692011476, 0.1755992379039526, 0.18612807081080973, 0.1730376509949565, 0.17568615498021245, 0.17895989678800106, 0.19288643496111035, 0.186289899982512, 0.17332805809564888, 0.17019663518294692, 0.17211401392705739, 0.1790465679951012, 0.17960361507721245, 0.1817018729634583, 0.19001079304143786, 0.1758200698532164, 0.1713195729535073, 0.17778940917924047, 0.17623615288175642, 0.17176993703469634, 0.1692962411325425, 0.1721703209914267, 0.1705483489204198, 0.17471687006764114, 0.18114617210812867, 0.17358006979338825, 0.2723633898422122, 0.17404037108644843, 0.17771459184587002, 0.18267009919509292, 0.17094867210835218, 0.1690224870108068, 0.17390044615603983, 0.17207219498232007, 0.17282027495093644, 0.17543746903538704, 0.17226128792390227, 0.17678371793590486, 0.17161538102664053, 0.1757310840766877, 0.17648517899215221, 0.17525455285795033, 0.1862394018098712, 0.18054466391913593, 0.1738675490487367, 0.1780996338929981, 0.17327199294231832, 0.17898157006129622, 0.1821873621083796, 0.1714694129768759, 0.17549788183532655, 0.1814820149447769, 0.17494153999723494, 0.17310402705334127, 0.17633396899327636, 0.1738412780687213, 0.17066727206110954, 0.17710148892365396, 0.17465642094612122, 0.171490082051605, 0.17617515102028847, 0.17323633399792016, 0.17588306707330048, 0.1754434851463884, 0.17380134807899594, 0.17503959778696299, 0.1680815159343183, 0.1718170209787786, 0.17679582885466516, 0.17365523101761937, 0.17498730518855155, 0.17731954506598413, 0.16951910010538995, 0.18432165309786797, 0.18955363193526864, 0.169521015137434, 0.17192706419155002, 0.17478732811287045, 0.1718218750320375, 0.21099402895197272, 0.20663036382757127, 0.18324374896474183, 0.17364795692265034, 0.17205591988749802, 0.1941906928550452, 0.1880794819444418, 0.16728554596193135, 0.16832943493500352, 0.17105837888084352, 0.16817700304090977, 0.16854111407883465, 0.1692834140267223, 0.1692120679654181, 0.16799293202348053, 0.16635373095050454, 0.16847660508938134, 0.1684409889858216, 0.19282162887975574, 0.17614470911212265, 0.1853949259966612, 0.17144223186187446, 0.19610733911395073, 0.18442375189624727, 0.18524232716299593, 0.207657549995929, 0.1724666329100728, 0.17109287111088634, 0.18660916900262237, 0.18267254694364965, 0.2661215590778738, 0.17658597696572542, 0.17441184096969664, 0.18650210183113813, 0.1835233559831977, 0.28294451301917434, 0.1858494549524039, 0.1868965830653906, 0.18373296596109867, 0.1849030361045152, 0.18948841094970703, 0.18767078616656363, 0.18613693211227655, 0.18859398807398975, 0.19177946890704334, 0.17849577008746564, 0.1803547430317849, 0.1747800090815872, 0.17081794887781143, 0.1728600061032921, 0.1746472311206162, 0.1743986550718546, 0.18033596989698708, 0.18049618299119174]
[0.0014406956981347745, 0.0014138225729526707, 0.0013984650239897097, 0.0014519639306001422, 0.0014280515412489574, 0.0013929790557297163, 0.0014412700155273426, 0.0014246603412654741, 0.0014032288219837487, 0.0014247189387823491, 0.0014257860846470954, 0.0014174818999834302, 0.0014490148525454047, 0.001390129427042927, 0.001443714551382171, 0.001418338713142299, 0.0013904146897036206, 0.0014307187209555575, 0.0021755256755060927, 0.0013864952637705691, 0.0014543717369697121, 0.0014250861163171687, 0.001391402572075757, 0.0014203690234282913, 0.00141742553511389, 0.0013997401941076729, 0.0014155623015709394, 0.0014472536601905907, 0.0014258116906032312, 0.0014110192089781973, 0.0014434738532286282, 0.0014209060608445442, 0.0014015039078920156, 0.0014381611542848421, 0.0014438898367590684, 0.0014267293573898632, 0.001409358991122292, 0.0014345147195741418, 0.0014182079074427831, 0.0014327301322518624, 0.0014346937825831102, 0.0014225412557233674, 0.0014033621635519025, 0.0014413589369794426, 0.001442051557607429, 0.0014069019058762595, 0.0014612578223784302, 0.001426050713820051, 0.001409203736400304, 0.0014179450235972109, 0.0014448752951656664, 0.0014182180689662233, 0.001407841807086916, 0.001398978318094976, 0.0014317050547331804, 0.0014171172474879165, 0.0013984988295552574, 0.0014082161464201387, 0.0014373725966023382, 0.0014200328755401826, 0.0014179645651272795, 0.001395811550607977, 0.0014382430475003035, 0.0014468692098708116, 0.0013680473861623873, 0.0014772968919038079, 0.0014441085044410108, 0.0014023576274716347, 0.0014073143500199382, 0.0014232547354501809, 0.0014854349221225627, 0.0015117262017212865, 0.0013426707050394873, 0.0013430432322644448, 0.0013892011078031256, 0.0013499333570140969, 0.0013242511389156182, 0.0013612344023562217, 0.0014428532620993002, 0.0013413771394957868, 0.0013619081781411817, 0.0013872860216124112, 0.0014952436818690725, 0.0014441077518024185, 0.0013436283573306114, 0.0013193537611081157, 0.001334217162225251, 0.0013879578914348931, 0.001392276085869864, 0.0014085416508795216, 0.001472951884042154, 0.0013629462779319102, 0.0013280587050659481, 0.001378212474257678, 0.0013661717277655538, 0.00133154989949377, 0.0013123739622677712, 0.001334653651096331, 0.0013220802241893008, 0.0013543943416096213, 0.001404233892311075, 0.0013455819363828547, 0.0021113441073039706, 0.0013491501634608405, 0.001377632494929225, 0.0014160472805821156, 0.0013251835047159083, 0.001310251837293076, 0.001348065474077828, 0.0013338929843590703, 0.0013396920538832281, 0.0013599803801192794, 0.0013353588211155215, 0.0013704164181077897, 0.0013303517909041902, 0.0013622564657107573, 0.001368102162729862, 0.001358562425255429, 0.0014437162930997767, 0.0013995710381328366, 0.0013478104577421448, 0.0013806173169999853, 0.0013431937437389017, 0.0013874540314829165, 0.0014123051326230976, 0.0013292202556346969, 0.0013604486963978803, 0.0014068373251533092, 0.0013561359689708136, 0.001341891682584041, 0.0013669299921959407, 0.0013476068067342737, 0.0013230021090008491, 0.0013728797590980928, 0.0013539257437683815, 0.0013293804810201937, 0.0013656988451185154, 0.0013429173178133346, 0.00137408646151016, 0.0013706522277061595, 0.0013578230318671558, 0.0013674968577106483, 0.0013131368432368618, 0.0013423204763967078, 0.0013812174129270716, 0.0013566814923251513, 0.001367088321785559, 0.001385308945828001, 0.001324367969573359, 0.0014400129148270935, 0.0014808877494942863, 0.0013243829307612032, 0.0013431801889964845, 0.0013655260008818004, 0.001342358398687793, 0.001648390851187287, 0.0016142997174029006, 0.0014315917887870455, 0.0013566246634582058, 0.0013441868741210783, 0.0015171147879300406, 0.0014693709526909515, 0.0013069183278275887, 0.001315073710429715, 0.00133639358500659, 0.0013138828362571076, 0.0013167274537408957, 0.001322526672083768, 0.001321969280979829, 0.0013124447814334417, 0.0012996385230508167, 0.0013162234772607917, 0.0013159452264517313, 0.0015064189756230917, 0.0013761305399384582, 0.0014483978593489155, 0.0013393924364208942, 0.00153208858682774, 0.0014408105616894318, 0.0014472056809609057, 0.0016223246093431953, 0.0013473955696099438, 0.0013366630555537995, 0.0014578841328329872, 0.0014271292729972629, 0.0020790746802958893, 0.0013795779450447299, 0.001362592507575755, 0.0014570476705557667, 0.001433776218618732, 0.0022105040079622995, 0.0014519488668156555, 0.001460129555198364, 0.0014354137965710834, 0.001444554969566525, 0.0014803782105445862, 0.0014661780169262784, 0.0014541947821271606, 0.001473390531828045, 0.001498277100836276, 0.0013944982038083253, 0.0014090214299358195, 0.0013654688209499, 0.0013345152256079018, 0.0013504687976819696, 0.001364431493129814, 0.001362489492748864, 0.0014088747648202116, 0.0014101264296186855]
[694.1091038827076, 707.3023299603777, 715.0697249095865, 688.7223428385502, 700.2548375287713, 717.885883414196, 693.8325152307513, 701.9216939188019, 712.6421466930084, 701.8928244574754, 701.3674847636879, 705.4763803415689, 690.1240510015167, 719.3574789127316, 692.657699572695, 705.050204675385, 719.209892850858, 698.9494058846966, 459.65902000553035, 721.2429974556879, 687.582118505391, 701.7119797533968, 718.699260781267, 704.0423886366781, 705.5044340792479, 714.4182929157755, 706.4330541228998, 690.9638769670196, 701.3548890014507, 708.7075736723381, 692.7731997107485, 703.7762928575515, 713.5192377052216, 695.3323673224037, 692.5736122948159, 700.9037802582648, 709.5424276561972, 697.0998529013809, 705.1152336353367, 697.9681500997482, 697.0128484139236, 702.9673100703827, 712.5744344346616, 693.7897107681114, 693.4564819992594, 710.7816087413506, 684.3419310990174, 701.2373335035454, 709.620599328255, 705.2459604273525, 692.1012514684475, 705.1101814891742, 710.3070778024303, 714.807361247546, 698.4678839360283, 705.6579134667027, 715.0524397063774, 710.1182602842076, 695.713833952171, 704.2090484134732, 705.2362411540503, 716.4290907067129, 695.2927752635557, 691.1474742691416, 730.9688320118613, 676.9120042696966, 692.4687424281062, 713.0848653798383, 710.5732986989242, 702.614911506826, 673.203507677794, 661.4954472981792, 744.784254431611, 744.5776695616454, 719.8381820911401, 740.777309342062, 755.1437719123761, 734.6273340352368, 693.071171038582, 745.5024918464708, 734.2638924195779, 720.8318864466914, 668.7873101392998, 692.4691033282529, 744.2534198866579, 757.9468293326479, 749.5031755791293, 720.4829528121952, 718.2483489797368, 709.954156751829, 678.9088026797903, 733.7046339914198, 752.9787622982694, 725.5775279052035, 731.9724011823557, 751.0045251628804, 761.9779336920159, 749.2580559597354, 756.3837516843577, 738.3374023931291, 712.1320781926217, 743.1728778168384, 473.6319373713675, 741.2073370949306, 725.8829939630413, 706.191109373774, 754.6124717379266, 763.2120570545866, 741.8037322586802, 749.6853283777451, 746.4401965372582, 735.3047254345635, 748.8623912819387, 729.7052098812095, 751.6808763194414, 734.0761634618119, 730.9395652183136, 736.0721755660112, 692.656863941681, 714.5046394601712, 741.9440873572106, 724.3136730842633, 744.494235966592, 720.7445993228301, 708.0622854798265, 752.3207653215488, 735.0516066116597, 710.8142370981134, 737.3891872795845, 745.2166318479073, 731.5663609030362, 742.0562103150492, 755.8566938001443, 728.3959089446738, 738.5929432264875, 752.2300908409454, 732.2258516761212, 744.6474825630329, 727.7562424281305, 729.5796700185133, 736.4729987124243, 731.2631062817346, 761.5352544179749, 744.9785782038991, 723.9989813629725, 737.0926821491078, 731.4816344081532, 721.8606383879942, 755.0771560279783, 694.4382162850761, 675.2706275958415, 755.0686261300872, 744.5017490520903, 732.3185346556867, 744.9575321892712, 606.6522386239463, 619.4636530128424, 698.5231459362286, 737.1235588853848, 743.9441786350344, 659.1459050797371, 680.5633377797737, 765.1587545353653, 760.413649873085, 748.2825503050202, 761.1028718882775, 759.4586086581126, 756.128417753877, 756.4472294385018, 761.936817568666, 769.4447204077678, 759.7494021920289, 759.9100478493053, 663.8259449608802, 726.6752469898101, 690.4180322729284, 746.6071726313357, 652.7037722215176, 694.0537684756027, 690.9867845018593, 616.399451898134, 742.1725457279699, 748.1316969486263, 685.925566702466, 700.7073703279842, 480.98320347861784, 724.8593699195279, 733.8951259750735, 686.3193430168051, 697.458910961278, 452.385517690975, 688.7294882451005, 684.8707338604254, 696.6632217056853, 692.2547227815603, 675.503052447746, 682.0454190797498, 687.665787479463, 678.7066825787822, 667.433280160153, 717.1038279354074, 709.7124137037076, 732.3492009904272, 749.3357743779037, 740.4836022249929, 732.9059795491385, 733.950614167651, 709.7862953969591, 709.1562706688729]
Elapsed: 0.18156564915240808~0.01502466976942976
Time per graph: 0.0014112267831084832~0.00011712926298552752
Speed: 712.2195303829907~44.345597593013714
Total Time: 0.1813
best val loss: 0.28117289729127587 test_score: 0.9141

Testing...
Test loss: 0.3586 score: 0.8906 time: 0.16s
test Score 0.8906
Epoch Time List: [0.6240425310097635, 0.6221138262189925, 0.6226655405480415, 0.6310527329333127, 0.6271062032319605, 0.6214880901388824, 0.6262937111314386, 0.6225781959947199, 0.6190061608795077, 0.62494887993671, 0.6204372176434845, 0.7201197370886803, 0.6286700069904327, 0.6855709988158196, 0.6228766723070294, 0.6252848880831152, 0.6668796569574624, 0.6228304281830788, 0.7205240929033607, 0.6137998520862311, 0.631049043033272, 0.6235324337612838, 0.6214899872429669, 0.626433664932847, 0.627000903012231, 0.6226259970571846, 0.6225820230320096, 0.6296154938172549, 0.6319578329566866, 0.6275291461497545, 0.6310963432770222, 0.628041811985895, 0.6242153709754348, 0.6259767939336598, 0.6250264700502157, 0.627432530047372, 0.6251342878676951, 0.6305987709201872, 0.6243568507488817, 0.6262121440377086, 0.6258929730392992, 0.6225024389568716, 0.6246295729652047, 0.6323174161370844, 0.630838364129886, 0.6284477671142668, 0.6343340540770441, 0.6296607288531959, 0.6246544998139143, 0.6236778993625194, 0.6288703149184585, 0.625350131187588, 0.6247397100087255, 0.6200772761367261, 0.6263347996864468, 0.623692360939458, 0.616864250972867, 0.6208627880550921, 0.6272215787321329, 0.623274183832109, 0.6221337311435491, 0.6201037017162889, 0.6251560270320624, 0.6263634259812534, 0.6108649701345712, 0.6327784787863493, 0.626334928907454, 0.6195952058769763, 0.6162825550418347, 0.6133393850177526, 0.633644554996863, 0.6636572419665754, 0.6232919353060424, 0.6171943119261414, 0.6226451001130044, 0.6274183252826333, 0.6218560680281371, 0.6208632760681212, 0.6240079537965357, 0.6500159201677889, 0.6408078649546951, 0.6262266980484128, 0.6712542050518095, 0.692071029683575, 0.6405447779688984, 0.6141836568713188, 0.6186262730043381, 0.6195051879622042, 0.6283271911088377, 0.6378062050789595, 0.6624798958655447, 0.6282363880891353, 0.6212422109674662, 0.6259307032451034, 0.6242025478277355, 0.6180183799006045, 0.6203272852580994, 0.6676125170197338, 0.617757469881326, 0.627587212016806, 0.6871054449584335, 0.6214957349002361, 0.7401526430621743, 0.623747066128999, 0.6386269151698798, 0.6773707850370556, 0.6334853917360306, 0.6124067700002342, 0.6159610298927873, 0.6163073177449405, 0.6234865912701935, 0.6206158772110939, 0.6194411478936672, 0.6207806773018092, 0.6191794141195714, 0.6177555231843144, 0.6370587865822017, 0.6340028862468898, 0.6482064900919795, 0.6643105207476765, 0.6442299261689186, 0.62940460187383, 0.6245557989459485, 0.6577501541469246, 0.654516956070438, 0.6273260607849807, 0.6281142900697887, 0.6402629390358925, 0.6322966597508639, 0.6257552709430456, 0.6293020688463002, 0.6253847400657833, 0.6214850009419024, 0.6322019889485091, 0.628025118028745, 0.6275990677531809, 0.630090267630294, 0.6226993231102824, 0.6197831849567592, 0.6195917101576924, 0.6176571692340076, 0.6284404559992254, 0.6077808609697968, 0.6144745778292418, 0.6213685409165919, 0.6166004887782037, 0.6241116130258888, 0.625167298130691, 0.6096683440264314, 0.6479833361227065, 0.6542063341476023, 0.6212488310411572, 0.6399776481557637, 0.613354176050052, 0.6154867550358176, 0.6924769717734307, 0.6942872470244765, 0.6955650700256228, 0.6493713618256152, 0.6314692841842771, 0.6625790179241449, 0.6618667761795223, 0.6117283327039331, 0.6112528571393341, 0.6175431800074875, 0.6155307348817587, 0.6174662560224533, 0.613971863174811, 0.6090351461898535, 0.6095337050501257, 0.6053256781306118, 0.6099810348823667, 0.6052496677730232, 0.6571266760583967, 0.6337181949056685, 0.632629418047145, 0.6237758609931916, 0.6870052472222596, 0.6758156490977854, 0.6544869013596326, 0.6869588596746325, 0.6269211219623685, 0.6182497893460095, 0.7000789560843259, 0.657191946869716, 0.7219757700804621, 0.6498963348567486, 0.6591265022289008, 0.701832186896354, 0.655578383943066, 0.7569118412211537, 0.6523341690190136, 0.6662748998496681, 0.6550985549110919, 0.6549635550472885, 0.6659686130005866, 0.6655868990346789, 0.6638571466319263, 0.6691763610579073, 0.6684395282063633, 0.6596033682581037, 0.6453553880564868, 0.6378762628883123, 0.6263580301310867, 0.6292885241564363, 0.6292390401940793, 0.6413584570400417, 0.6615802550222725, 0.6330088842660189]
Total Epoch List: [72, 66, 71]
Total Time List: [0.1955154580064118, 0.17390540800988674, 0.1813117559067905]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e933090c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.20s
Epoch 2/1000, LR 0.000015
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6673;  Loss pred: 0.6673; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6295;  Loss pred: 0.6295; Loss self: 0.0000; time: 0.26s
Val loss: 0.6909 score: 0.5271 time: 0.18s
Test loss: 0.6912 score: 0.5349 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.26s
Val loss: 0.6900 score: 0.7984 time: 0.18s
Test loss: 0.6905 score: 0.7442 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.25s
Val loss: 0.6888 score: 0.9302 time: 0.18s
Test loss: 0.6895 score: 0.8682 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 0.26s
Val loss: 0.6871 score: 0.8915 time: 0.19s
Test loss: 0.6881 score: 0.8140 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.26s
Val loss: 0.6850 score: 0.8682 time: 0.18s
Test loss: 0.6864 score: 0.7984 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.25s
Val loss: 0.6824 score: 0.8682 time: 0.18s
Test loss: 0.6842 score: 0.7984 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.25s
Val loss: 0.6788 score: 0.8682 time: 0.18s
Test loss: 0.6813 score: 0.7984 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.25s
Val loss: 0.6744 score: 0.8605 time: 0.18s
Test loss: 0.6776 score: 0.7984 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4842;  Loss pred: 0.4842; Loss self: 0.0000; time: 0.25s
Val loss: 0.6690 score: 0.8837 time: 0.18s
Test loss: 0.6731 score: 0.8062 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4559;  Loss pred: 0.4559; Loss self: 0.0000; time: 0.25s
Val loss: 0.6625 score: 0.8992 time: 0.18s
Test loss: 0.6677 score: 0.8217 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.4271;  Loss pred: 0.4271; Loss self: 0.0000; time: 0.25s
Val loss: 0.6546 score: 0.9147 time: 0.18s
Test loss: 0.6609 score: 0.8605 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3986;  Loss pred: 0.3986; Loss self: 0.0000; time: 0.25s
Val loss: 0.6451 score: 0.9380 time: 0.18s
Test loss: 0.6528 score: 0.8760 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3693;  Loss pred: 0.3693; Loss self: 0.0000; time: 0.25s
Val loss: 0.6340 score: 0.9457 time: 0.18s
Test loss: 0.6431 score: 0.8682 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.25s
Val loss: 0.6201 score: 0.9380 time: 0.18s
Test loss: 0.6312 score: 0.8760 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.3175;  Loss pred: 0.3175; Loss self: 0.0000; time: 0.25s
Val loss: 0.6044 score: 0.9457 time: 0.18s
Test loss: 0.6174 score: 0.8682 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2896;  Loss pred: 0.2896; Loss self: 0.0000; time: 0.25s
Val loss: 0.5860 score: 0.9380 time: 0.18s
Test loss: 0.6013 score: 0.8682 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2584;  Loss pred: 0.2584; Loss self: 0.0000; time: 0.25s
Val loss: 0.5643 score: 0.9457 time: 0.18s
Test loss: 0.5820 score: 0.8682 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.2440;  Loss pred: 0.2440; Loss self: 0.0000; time: 0.25s
Val loss: 0.5396 score: 0.9457 time: 0.18s
Test loss: 0.5601 score: 0.8760 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.2080;  Loss pred: 0.2080; Loss self: 0.0000; time: 0.25s
Val loss: 0.5125 score: 0.9457 time: 0.17s
Test loss: 0.5356 score: 0.8915 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1933;  Loss pred: 0.1933; Loss self: 0.0000; time: 0.25s
Val loss: 0.4829 score: 0.9457 time: 0.18s
Test loss: 0.5089 score: 0.8837 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1703;  Loss pred: 0.1703; Loss self: 0.0000; time: 0.25s
Val loss: 0.4544 score: 0.9457 time: 0.18s
Test loss: 0.4828 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.25s
Val loss: 0.4256 score: 0.9457 time: 0.18s
Test loss: 0.4563 score: 0.8915 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1262;  Loss pred: 0.1262; Loss self: 0.0000; time: 0.25s
Val loss: 0.3970 score: 0.9457 time: 0.18s
Test loss: 0.4300 score: 0.8915 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.25s
Val loss: 0.3677 score: 0.9457 time: 0.18s
Test loss: 0.4032 score: 0.8915 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0932;  Loss pred: 0.0932; Loss self: 0.0000; time: 0.25s
Val loss: 0.3403 score: 0.9457 time: 0.18s
Test loss: 0.3781 score: 0.8915 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0941;  Loss pred: 0.0941; Loss self: 0.0000; time: 0.25s
Val loss: 0.3140 score: 0.9457 time: 0.18s
Test loss: 0.3540 score: 0.8837 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.25s
Val loss: 0.2913 score: 0.9457 time: 0.17s
Test loss: 0.3326 score: 0.8915 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0737;  Loss pred: 0.0737; Loss self: 0.0000; time: 0.25s
Val loss: 0.2711 score: 0.9457 time: 0.18s
Test loss: 0.3135 score: 0.8915 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.25s
Val loss: 0.2528 score: 0.9457 time: 0.18s
Test loss: 0.2961 score: 0.8915 time: 0.32s
Epoch 42/1000, LR 0.000284
Train loss: 0.0622;  Loss pred: 0.0622; Loss self: 0.0000; time: 0.25s
Val loss: 0.2363 score: 0.9457 time: 0.18s
Test loss: 0.2802 score: 0.8992 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.25s
Val loss: 0.2212 score: 0.9457 time: 0.18s
Test loss: 0.2652 score: 0.8992 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0686;  Loss pred: 0.0686; Loss self: 0.0000; time: 0.25s
Val loss: 0.2092 score: 0.9457 time: 0.18s
Test loss: 0.2532 score: 0.8992 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.25s
Val loss: 0.2011 score: 0.9457 time: 0.18s
Test loss: 0.2451 score: 0.8992 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.26s
Val loss: 0.1969 score: 0.9457 time: 0.17s
Test loss: 0.2406 score: 0.8992 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.25s
Val loss: 0.1936 score: 0.9457 time: 0.18s
Test loss: 0.2368 score: 0.8992 time: 0.19s
Epoch 48/1000, LR 0.000284
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.26s
Val loss: 0.1927 score: 0.9457 time: 0.18s
Test loss: 0.2356 score: 0.8915 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.28s
Val loss: 0.1920 score: 0.9457 time: 0.18s
Test loss: 0.2346 score: 0.8915 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.26s
Val loss: 0.1931 score: 0.9457 time: 0.18s
Test loss: 0.2347 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.25s
Val loss: 0.1954 score: 0.9457 time: 0.18s
Test loss: 0.2368 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.26s
Val loss: 0.1983 score: 0.9457 time: 0.19s
Test loss: 0.2398 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.26s
Val loss: 0.2021 score: 0.9457 time: 0.19s
Test loss: 0.2443 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.27s
Val loss: 0.2047 score: 0.9457 time: 0.19s
Test loss: 0.2482 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.26s
Val loss: 0.2078 score: 0.9380 time: 0.19s
Test loss: 0.2545 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.26s
Val loss: 0.2088 score: 0.9380 time: 0.19s
Test loss: 0.2592 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.26s
Val loss: 0.2109 score: 0.9380 time: 0.19s
Test loss: 0.2672 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.26s
Val loss: 0.2145 score: 0.9535 time: 0.19s
Test loss: 0.2786 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.26s
Val loss: 0.2200 score: 0.9535 time: 0.18s
Test loss: 0.2901 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.26s
Val loss: 0.2255 score: 0.9535 time: 0.20s
Test loss: 0.2981 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.26s
Val loss: 0.2303 score: 0.9535 time: 0.18s
Test loss: 0.3062 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.35s
Val loss: 0.2340 score: 0.9535 time: 0.19s
Test loss: 0.3088 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.26s
Val loss: 0.2375 score: 0.9535 time: 0.19s
Test loss: 0.3109 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.26s
Val loss: 0.2406 score: 0.9535 time: 0.18s
Test loss: 0.3133 score: 0.8992 time: 0.27s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.25s
Val loss: 0.2438 score: 0.9535 time: 0.19s
Test loss: 0.3136 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.26s
Val loss: 0.2479 score: 0.9535 time: 0.19s
Test loss: 0.3144 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.35s
Val loss: 0.2528 score: 0.9457 time: 0.18s
Test loss: 0.3144 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.25s
Val loss: 0.2563 score: 0.9457 time: 0.19s
Test loss: 0.3176 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.26s
Val loss: 0.2584 score: 0.9457 time: 0.26s
Test loss: 0.3217 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0386,   Val_Loss: 0.1920,   Val_Precision: 0.9385,   Val_Recall: 0.9531,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1920,   Test_Precision: 0.9048,   Test_Recall: 0.8769,   Test_accuracy: 0.8906,   Test_Score: 0.8915,   Test_loss: 0.2346


[0.20203094091266394, 0.1853464161977172, 0.17602922022342682, 0.1783220369834453, 0.1776237089652568, 0.1756696121301502, 0.17862878693267703, 0.17769696610048413, 0.1769575399812311, 0.1781231260392815, 0.18059307499788702, 0.17705765296705067, 0.17381702503189445, 0.17979861213825643, 0.1781919680070132, 0.18841162300668657, 0.17806491418741643, 0.1773999440483749, 0.17496636603027582, 0.1766059179790318, 0.17815742478705943, 0.17528726207092404, 0.17512690112926066, 0.17670662002637982, 0.17973256018012762, 0.17605704790912569, 0.17893565515987575, 0.17716158693656325, 0.17450813297182322, 0.17254091217182577, 0.17640171688981354, 0.17838772712275386, 0.17508103186264634, 0.17380196694284678, 0.17627852596342564, 0.1780499799642712, 0.17578871711157262, 0.17452096613124013, 0.18506414792500436, 0.17762348288670182, 0.32079543685540557, 0.17634347104467452, 0.1767204429488629, 0.1749326919671148, 0.18402411695569754, 0.1772565939463675, 0.19778340193443, 0.17278868006542325, 0.17942229215987027, 0.18031846010126173, 0.18444913905113935, 0.17944411607459188, 0.19054289697669446, 0.18833354581147432, 0.18499739398248494, 0.19513083179481328, 0.189334083115682, 0.18352914601564407, 0.19229462509974837, 0.17693545599468052, 0.17335508298128843, 0.18784612300805748, 0.18016563379205763, 0.2779016599524766, 0.18423546804115176, 0.1854449629317969, 0.17896088608540595, 0.17952875909395516, 0.17862862395122647]
[0.0015661313249043715, 0.0014367939240133116, 0.0013645675986312157, 0.0013823413719646922, 0.001376927976474859, 0.0013617799389934123, 0.001384719278547884, 0.0013774958612440631, 0.0013717638758234969, 0.001380799426661097, 0.0013999463178130777, 0.001372539945481013, 0.0013474187986968562, 0.0013937876909942359, 0.0013813330853256838, 0.0014605552171060975, 0.0013803481719954762, 0.0013751933647160844, 0.0013563284188393475, 0.0013690381238684636, 0.0013810653084268172, 0.0013588159850459228, 0.0013575728769710128, 0.0013698187598944172, 0.001393275660311067, 0.0013647833171250053, 0.0013870981020145407, 0.001373345635167157, 0.001352776224587777, 0.0013375264509443857, 0.001367455169688477, 0.001382850597850805, 0.0013572173012608244, 0.0013473020693243935, 0.0013665002012668655, 0.0013802324028238076, 0.0013627032334230435, 0.001352875706443722, 0.0014346057978682509, 0.001376926223927921, 0.0024867863322124463, 0.0013670036515091048, 0.0013699259143322706, 0.0013560673795900372, 0.0014265435422922289, 0.001374082123615252, 0.0015332046661583722, 0.0013394471322901028, 0.0013908704818594594, 0.0013978175201648196, 0.0014298382872181346, 0.0013910396594929602, 0.0014770767207495695, 0.0014599499675308086, 0.0014340883254456197, 0.0015126421069365371, 0.0014677060706642015, 0.0014227065582608069, 0.0014906560085251811, 0.0013715926821293064, 0.0013438378525681275, 0.001456171496186492, 0.00139663282009347, 0.002154276433740129, 0.0014281819227996261, 0.0014375578521844722, 0.0013872936905845422, 0.001391695806929885, 0.0013847180151257865]
[638.51605807135, 695.9940345562982, 732.8328776112595, 723.4103096970334, 726.2543989847234, 734.3330382287542, 722.1680346999082, 725.9549942290688, 728.9884342519811, 724.2181454391927, 714.3131042068436, 728.5762452979432, 742.1597509008639, 717.4693868093111, 723.9383539157212, 684.6711362144675, 724.4549022398946, 727.1704661012926, 737.2845588944683, 730.4398486539733, 724.0787194481832, 735.9348219370586, 736.6087058480266, 730.0235836141402, 717.7330577760449, 732.717045594137, 720.9295424365862, 728.1488173064931, 739.2205612607686, 747.6487655955747, 731.2853994532136, 723.1439184783789, 736.801689067051, 742.2240511375829, 731.7964527725004, 724.5156670384691, 733.835493651871, 739.1662036926367, 697.0555963777281, 726.2553233588118, 402.1254206871561, 731.526941347998, 729.966481791404, 737.4264841488315, 700.9950768086327, 727.7585399109695, 652.2286437502305, 746.5766851807452, 718.9742057528582, 715.4009629826988, 699.3797892666453, 718.8867644251812, 677.0129039015196, 684.9549794444565, 697.3071199706379, 661.094911621388, 681.3353300006833, 702.8856331571662, 670.8455836094444, 729.0794220683407, 744.1373958092938, 686.7322994708103, 716.0078050672436, 464.1929811504545, 700.1909098805335, 695.6241785194442, 720.8279016814714, 718.5478284985456, 722.168693608829]
Elapsed: 0.18344915711212958~0.021193351937469372
Time per graph: 0.0014220864892413145~0.00016428954990286334
Speed: 709.2241067878726~52.96358930694538
Total Time: 0.1790
best val loss: 0.19203837038299373 test_score: 0.8915

Testing...
Test loss: 0.2786 score: 0.8915 time: 0.17s
test Score 0.8915
Epoch Time List: [0.6661168579012156, 0.6219913868699223, 0.6049694691319019, 0.6071528389584273, 0.6053894399665296, 0.6042453250847757, 0.6051759708207101, 0.6052343600895256, 0.608471259009093, 0.6108704849611968, 0.6113304847385734, 0.6081901208963245, 0.6086786701343954, 0.6123820031061769, 0.6009960069786757, 0.6331635930109769, 0.607346594100818, 0.6040266079362482, 0.6020079629961401, 0.6002151456195861, 0.6003056759946048, 0.6001440298277885, 0.597418359015137, 0.6008394709788263, 0.6019242061302066, 0.6017876591067761, 0.6023785211145878, 0.6037684171460569, 0.60045470087789, 0.6002323541324586, 0.5993462211918086, 0.6030933456495404, 0.6019829660654068, 0.6030284066218883, 0.6009932179003954, 0.6014040268491954, 0.6023323314730078, 0.5969630007166415, 0.6044365197885782, 0.6063149508554488, 0.7433271049521863, 0.6008604371454567, 0.5999474707059562, 0.6009368076920509, 0.6099493319634348, 0.602409953949973, 0.6259372790809721, 0.6099378049839288, 0.631322214147076, 0.6111119522247463, 0.6144098320510238, 0.6214022669009864, 0.635857624001801, 0.6398152629844844, 0.6297871142160147, 0.6466092069167644, 0.636951986933127, 0.6272488296963274, 0.6276343159843236, 0.6314875998068601, 0.6078621109481901, 0.7199800498783588, 0.6300883330404758, 0.7103174061048776, 0.615858888020739, 0.6320563650224358, 0.6962913481984288, 0.6092617339454591, 0.698530909139663]
Total Epoch List: [69]
Total Time List: [0.17903915490023792]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e909703a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4961 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.19s
Epoch 12/1000, LR 0.000285
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.26s
Val loss: 0.6897 score: 0.7829 time: 0.17s
Test loss: 0.6903 score: 0.7752 time: 0.19s
Epoch 16/1000, LR 0.000285
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.27s
Val loss: 0.6882 score: 0.9225 time: 0.19s
Test loss: 0.6891 score: 0.8915 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.26s
Val loss: 0.6861 score: 0.8527 time: 0.18s
Test loss: 0.6873 score: 0.8062 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 0.25s
Val loss: 0.6835 score: 0.7984 time: 0.18s
Test loss: 0.6850 score: 0.7287 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.25s
Val loss: 0.6801 score: 0.7907 time: 0.18s
Test loss: 0.6820 score: 0.7287 time: 0.19s
Epoch 20/1000, LR 0.000285
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.26s
Val loss: 0.6759 score: 0.7984 time: 0.19s
Test loss: 0.6785 score: 0.7287 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4892;  Loss pred: 0.4892; Loss self: 0.0000; time: 0.25s
Val loss: 0.6712 score: 0.7984 time: 0.19s
Test loss: 0.6743 score: 0.7287 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4584;  Loss pred: 0.4584; Loss self: 0.0000; time: 0.27s
Val loss: 0.6655 score: 0.8062 time: 0.19s
Test loss: 0.6694 score: 0.7364 time: 0.20s
Epoch 23/1000, LR 0.000285
Train loss: 0.4330;  Loss pred: 0.4330; Loss self: 0.0000; time: 0.26s
Val loss: 0.6589 score: 0.8062 time: 0.19s
Test loss: 0.6637 score: 0.7442 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.4026;  Loss pred: 0.4026; Loss self: 0.0000; time: 0.26s
Val loss: 0.6508 score: 0.8062 time: 0.18s
Test loss: 0.6566 score: 0.7519 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3758;  Loss pred: 0.3758; Loss self: 0.0000; time: 0.26s
Val loss: 0.6410 score: 0.8372 time: 0.18s
Test loss: 0.6482 score: 0.7984 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 0.26s
Val loss: 0.6290 score: 0.8605 time: 0.18s
Test loss: 0.6378 score: 0.8217 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.3078;  Loss pred: 0.3078; Loss self: 0.0000; time: 0.26s
Val loss: 0.6150 score: 0.8760 time: 0.18s
Test loss: 0.6256 score: 0.8295 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2794;  Loss pred: 0.2794; Loss self: 0.0000; time: 0.26s
Val loss: 0.5983 score: 0.8760 time: 0.18s
Test loss: 0.6109 score: 0.8450 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2590;  Loss pred: 0.2590; Loss self: 0.0000; time: 0.26s
Val loss: 0.5792 score: 0.8837 time: 0.18s
Test loss: 0.5940 score: 0.8450 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2295;  Loss pred: 0.2295; Loss self: 0.0000; time: 0.25s
Val loss: 0.5575 score: 0.8837 time: 0.18s
Test loss: 0.5746 score: 0.8605 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.2148;  Loss pred: 0.2148; Loss self: 0.0000; time: 0.26s
Val loss: 0.5343 score: 0.8837 time: 0.18s
Test loss: 0.5540 score: 0.8605 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1774;  Loss pred: 0.1774; Loss self: 0.0000; time: 0.26s
Val loss: 0.5089 score: 0.9070 time: 0.18s
Test loss: 0.5312 score: 0.8682 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.25s
Val loss: 0.4817 score: 0.9070 time: 0.18s
Test loss: 0.5070 score: 0.8760 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.26s
Val loss: 0.4525 score: 0.9147 time: 0.18s
Test loss: 0.4811 score: 0.8837 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1225;  Loss pred: 0.1225; Loss self: 0.0000; time: 0.26s
Val loss: 0.4236 score: 0.9225 time: 0.18s
Test loss: 0.4555 score: 0.8837 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1088;  Loss pred: 0.1088; Loss self: 0.0000; time: 0.25s
Val loss: 0.3950 score: 0.9225 time: 0.18s
Test loss: 0.4303 score: 0.8837 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.1151;  Loss pred: 0.1151; Loss self: 0.0000; time: 0.26s
Val loss: 0.3690 score: 0.9225 time: 0.17s
Test loss: 0.4072 score: 0.8837 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0864;  Loss pred: 0.0864; Loss self: 0.0000; time: 0.25s
Val loss: 0.3443 score: 0.9225 time: 0.18s
Test loss: 0.3852 score: 0.8837 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.25s
Val loss: 0.3218 score: 0.9225 time: 0.18s
Test loss: 0.3649 score: 0.8760 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0724;  Loss pred: 0.0724; Loss self: 0.0000; time: 0.25s
Val loss: 0.3010 score: 0.9225 time: 0.18s
Test loss: 0.3463 score: 0.8760 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.26s
Val loss: 0.2819 score: 0.9225 time: 0.19s
Test loss: 0.3292 score: 0.8760 time: 0.19s
Epoch 42/1000, LR 0.000284
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.26s
Val loss: 0.2635 score: 0.9302 time: 0.19s
Test loss: 0.3126 score: 0.8915 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.25s
Val loss: 0.2483 score: 0.9302 time: 0.18s
Test loss: 0.2981 score: 0.8915 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.25s
Val loss: 0.2350 score: 0.9302 time: 0.18s
Test loss: 0.2843 score: 0.8915 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.26s
Val loss: 0.2217 score: 0.9302 time: 0.18s
Test loss: 0.2707 score: 0.8992 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.26s
Val loss: 0.2084 score: 0.9302 time: 0.18s
Test loss: 0.2576 score: 0.8992 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 0.26s
Val loss: 0.1944 score: 0.9302 time: 0.18s
Test loss: 0.2464 score: 0.8992 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.26s
Val loss: 0.1829 score: 0.9380 time: 0.17s
Test loss: 0.2393 score: 0.9147 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.26s
Val loss: 0.1736 score: 0.9380 time: 0.18s
Test loss: 0.2354 score: 0.9147 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.26s
Val loss: 0.1673 score: 0.9380 time: 0.18s
Test loss: 0.2354 score: 0.9147 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.26s
Val loss: 0.1639 score: 0.9380 time: 0.18s
Test loss: 0.2381 score: 0.9147 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.25s
Val loss: 0.1628 score: 0.9380 time: 0.18s
Test loss: 0.2418 score: 0.9147 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.26s
Val loss: 0.1633 score: 0.9457 time: 0.18s
Test loss: 0.2470 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.26s
Val loss: 0.1659 score: 0.9457 time: 0.17s
Test loss: 0.2518 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.25s
Val loss: 0.1702 score: 0.9380 time: 0.18s
Test loss: 0.2580 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.25s
Val loss: 0.1722 score: 0.9380 time: 0.18s
Test loss: 0.2640 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.25s
Val loss: 0.1767 score: 0.9380 time: 0.18s
Test loss: 0.2696 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.26s
Val loss: 0.1805 score: 0.9380 time: 0.18s
Test loss: 0.2752 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.25s
Val loss: 0.1839 score: 0.9380 time: 0.18s
Test loss: 0.2811 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.26s
Val loss: 0.1908 score: 0.9380 time: 0.19s
Test loss: 0.2846 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.26s
Val loss: 0.1964 score: 0.9380 time: 0.19s
Test loss: 0.2883 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.26s
Val loss: 0.2033 score: 0.9302 time: 0.19s
Test loss: 0.2932 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.27s
Val loss: 0.2076 score: 0.9302 time: 0.19s
Test loss: 0.2973 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.27s
Val loss: 0.2085 score: 0.9302 time: 0.19s
Test loss: 0.3003 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.25s
Val loss: 0.2081 score: 0.9302 time: 0.18s
Test loss: 0.3035 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.25s
Val loss: 0.2088 score: 0.9302 time: 0.18s
Test loss: 0.3073 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.25s
Val loss: 0.2090 score: 0.9302 time: 0.17s
Test loss: 0.3108 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.25s
Val loss: 0.2144 score: 0.9302 time: 0.18s
Test loss: 0.3154 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.25s
Val loss: 0.2186 score: 0.9302 time: 0.18s
Test loss: 0.3186 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.25s
Val loss: 0.2248 score: 0.9302 time: 0.18s
Test loss: 0.3184 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.25s
Val loss: 0.2330 score: 0.9302 time: 0.18s
Test loss: 0.3167 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.25s
Val loss: 0.2386 score: 0.9302 time: 0.18s
Test loss: 0.3151 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.0246,   Val_Loss: 0.1628,   Val_Precision: 1.0000,   Val_Recall: 0.8769,   Val_accuracy: 0.9344,   Val_Score: 0.9380,   Val_Loss: 0.1628,   Test_Precision: 0.9344,   Test_Recall: 0.8906,   Test_accuracy: 0.9120,   Test_Score: 0.9147,   Test_loss: 0.2418


[0.20203094091266394, 0.1853464161977172, 0.17602922022342682, 0.1783220369834453, 0.1776237089652568, 0.1756696121301502, 0.17862878693267703, 0.17769696610048413, 0.1769575399812311, 0.1781231260392815, 0.18059307499788702, 0.17705765296705067, 0.17381702503189445, 0.17979861213825643, 0.1781919680070132, 0.18841162300668657, 0.17806491418741643, 0.1773999440483749, 0.17496636603027582, 0.1766059179790318, 0.17815742478705943, 0.17528726207092404, 0.17512690112926066, 0.17670662002637982, 0.17973256018012762, 0.17605704790912569, 0.17893565515987575, 0.17716158693656325, 0.17450813297182322, 0.17254091217182577, 0.17640171688981354, 0.17838772712275386, 0.17508103186264634, 0.17380196694284678, 0.17627852596342564, 0.1780499799642712, 0.17578871711157262, 0.17452096613124013, 0.18506414792500436, 0.17762348288670182, 0.32079543685540557, 0.17634347104467452, 0.1767204429488629, 0.1749326919671148, 0.18402411695569754, 0.1772565939463675, 0.19778340193443, 0.17278868006542325, 0.17942229215987027, 0.18031846010126173, 0.18444913905113935, 0.17944411607459188, 0.19054289697669446, 0.18833354581147432, 0.18499739398248494, 0.19513083179481328, 0.189334083115682, 0.18352914601564407, 0.19229462509974837, 0.17693545599468052, 0.17335508298128843, 0.18784612300805748, 0.18016563379205763, 0.2779016599524766, 0.18423546804115176, 0.1854449629317969, 0.17896088608540595, 0.17952875909395516, 0.17862862395122647, 0.18446812499314547, 0.1810460400301963, 0.17804349004290998, 0.18175965803675354, 0.1818988830782473, 0.18181254994124174, 0.18310497980564833, 0.18575795809738338, 0.1840633109677583, 0.18075177911669016, 0.19566972088068724, 0.19001130387187004, 0.17952689388766885, 0.17778945108875632, 0.18952951091341674, 0.18621797394007444, 0.17845414695329964, 0.18414502800442278, 0.19049029401503503, 0.1894830649252981, 0.181661956012249, 0.20185325597412884, 0.18658236297778785, 0.18092650594189763, 0.18536559608764946, 0.18703376594930887, 0.18274672399275005, 0.18463222589343786, 0.18545056111179292, 0.18311799108050764, 0.1846157459076494, 0.1808960479684174, 0.1771977050229907, 0.17958932206965983, 0.1843078602105379, 0.18128236406482756, 0.18031333084218204, 0.17969846283085644, 0.18298609601333737, 0.18143084715120494, 0.1909846030175686, 0.18868259387090802, 0.1834075169172138, 0.1829707392025739, 0.18415644415654242, 0.18262418103404343, 0.18006488401442766, 0.1836602760013193, 0.18243679800070822, 0.18048262502998114, 0.18312495085410774, 0.1835723367985338, 0.17895553610287607, 0.18307400704361498, 0.18400325789116323, 0.18293872894719243, 0.18048258707858622, 0.1848295540548861, 0.18254127097316086, 0.19172607897780836, 0.19718291889876127, 0.1958854820113629, 0.19514121720567346, 0.1963924141600728, 0.18248430802486837, 0.17965130391530693, 0.1798708001151681, 0.18558943318203092, 0.18152691703289747, 0.1802628489676863, 0.1841897030826658, 0.1846458399668336]
[0.0015661313249043715, 0.0014367939240133116, 0.0013645675986312157, 0.0013823413719646922, 0.001376927976474859, 0.0013617799389934123, 0.001384719278547884, 0.0013774958612440631, 0.0013717638758234969, 0.001380799426661097, 0.0013999463178130777, 0.001372539945481013, 0.0013474187986968562, 0.0013937876909942359, 0.0013813330853256838, 0.0014605552171060975, 0.0013803481719954762, 0.0013751933647160844, 0.0013563284188393475, 0.0013690381238684636, 0.0013810653084268172, 0.0013588159850459228, 0.0013575728769710128, 0.0013698187598944172, 0.001393275660311067, 0.0013647833171250053, 0.0013870981020145407, 0.001373345635167157, 0.001352776224587777, 0.0013375264509443857, 0.001367455169688477, 0.001382850597850805, 0.0013572173012608244, 0.0013473020693243935, 0.0013665002012668655, 0.0013802324028238076, 0.0013627032334230435, 0.001352875706443722, 0.0014346057978682509, 0.001376926223927921, 0.0024867863322124463, 0.0013670036515091048, 0.0013699259143322706, 0.0013560673795900372, 0.0014265435422922289, 0.001374082123615252, 0.0015332046661583722, 0.0013394471322901028, 0.0013908704818594594, 0.0013978175201648196, 0.0014298382872181346, 0.0013910396594929602, 0.0014770767207495695, 0.0014599499675308086, 0.0014340883254456197, 0.0015126421069365371, 0.0014677060706642015, 0.0014227065582608069, 0.0014906560085251811, 0.0013715926821293064, 0.0013438378525681275, 0.001456171496186492, 0.00139663282009347, 0.002154276433740129, 0.0014281819227996261, 0.0014375578521844722, 0.0013872936905845422, 0.001391695806929885, 0.0013847180151257865, 0.001429985465063143, 0.0014034576746526845, 0.0013801820933558914, 0.001408989597184136, 0.0014100688610716847, 0.0014093996119476104, 0.001419418448105801, 0.0014399841712975456, 0.0014268473718430875, 0.0014011765822999236, 0.0015168195417107537, 0.0014729558439679849, 0.0013916813479664253, 0.001378212799137646, 0.001469221014832688, 0.0014435501855819724, 0.0013833654802581368, 0.0014274808372435875, 0.0014766689458529847, 0.0014688609684131635, 0.001408232217149217, 0.0015647539222800686, 0.001446374906804557, 0.00140253105381316, 0.001436942605330616, 0.0014498741546458051, 0.0014166412712616283, 0.0014312575650654098, 0.0014376012489286272, 0.0014195193107016096, 0.0014311298132375922, 0.0014022949454916079, 0.0013736256203332612, 0.0013921652873617042, 0.0014287431024072707, 0.001405289643913392, 0.0013977777584665274, 0.001393011339774081, 0.0014184968683204448, 0.001406440675590736, 0.001480500798585803, 0.0014626557664411475, 0.001421763697032665, 0.0014183778232757665, 0.0014275693345468404, 0.001415691325845298, 0.0013958518140653308, 0.0014237230697776689, 0.0014142387441915367, 0.0013990901165114816, 0.0014195732624349437, 0.0014230413705312698, 0.0013872522178517525, 0.0014191783491753099, 0.0014263818441175443, 0.0014181296817611815, 0.0013990898223146218, 0.001432787240735551, 0.0014150486121950454, 0.0014862486742465764, 0.0015285497589051262, 0.0015184921086152164, 0.0015127226139974687, 0.001522421815194363, 0.001414607038952468, 0.001392645766785325, 0.0013943472877144814, 0.0014386777766048908, 0.0014071854033557943, 0.0013973864261060953, 0.00142782715567958, 0.00143136310051809]
[638.51605807135, 695.9940345562982, 732.8328776112595, 723.4103096970334, 726.2543989847234, 734.3330382287542, 722.1680346999082, 725.9549942290688, 728.9884342519811, 724.2181454391927, 714.3131042068436, 728.5762452979432, 742.1597509008639, 717.4693868093111, 723.9383539157212, 684.6711362144675, 724.4549022398946, 727.1704661012926, 737.2845588944683, 730.4398486539733, 724.0787194481832, 735.9348219370586, 736.6087058480266, 730.0235836141402, 717.7330577760449, 732.717045594137, 720.9295424365862, 728.1488173064931, 739.2205612607686, 747.6487655955747, 731.2853994532136, 723.1439184783789, 736.801689067051, 742.2240511375829, 731.7964527725004, 724.5156670384691, 733.835493651871, 739.1662036926367, 697.0555963777281, 726.2553233588118, 402.1254206871561, 731.526941347998, 729.966481791404, 737.4264841488315, 700.9950768086327, 727.7585399109695, 652.2286437502305, 746.5766851807452, 718.9742057528582, 715.4009629826988, 699.3797892666453, 718.8867644251812, 677.0129039015196, 684.9549794444565, 697.3071199706379, 661.094911621388, 681.3353300006833, 702.8856331571662, 670.8455836094444, 729.0794220683407, 744.1373958092938, 686.7322994708103, 716.0078050672436, 464.1929811504545, 700.1909098805335, 695.6241785194442, 720.8279016814714, 718.5478284985456, 722.168693608829, 699.3078072691064, 712.5259407965198, 724.5420765954988, 709.7284479590899, 709.185223223763, 709.521977672555, 704.5138812550235, 694.452077968966, 700.8458085522349, 713.6859212695211, 659.2742066548959, 678.90697748692, 718.5552938977273, 725.5773568680429, 680.6327910534809, 692.736567102339, 722.8747675656901, 700.5347980229022, 677.1998576988824, 680.7996274013038, 710.1101564231857, 639.0781232507525, 691.3836760410046, 712.9966907194173, 695.9220196341224, 689.7150327121277, 705.8950069338464, 698.6862633311559, 695.6031797727293, 704.4638226906131, 698.7486325490885, 713.1167399661605, 728.0003992335159, 718.3055123397758, 699.9158899280865, 711.5970749028234, 715.4213135406296, 717.8692458901163, 704.9715951675232, 711.0147035387597, 675.447119620074, 683.6878662387832, 703.3517609762299, 705.0307637287248, 700.4913707518342, 706.3686707290574, 716.4084252522213, 702.3837860238942, 707.0941905015194, 714.7502424600206, 704.4370491204768, 702.7202586715142, 720.8494512616913, 704.633072073784, 701.0745433447852, 705.154128611211, 714.7503927557868, 697.9403302660827, 706.6895026657667, 672.8349147271265, 654.2148818997445, 658.5480387592837, 661.0597281661801, 656.8481809834899, 706.9100976201214, 718.057688358413, 717.1814431102969, 695.0826767894348, 710.638411694183, 715.6216643570544, 700.3648838182, 698.6347486798035]
Elapsed: 0.18390955238979548~0.015250884559922837
Time per graph: 0.0014256554448821353~0.00011822391131723131
Speed: 704.6378733139~39.52182988833529
Total Time: 0.1853
best val loss: 0.1627978553721147 test_score: 0.9147

Testing...
Test loss: 0.2470 score: 0.9147 time: 0.17s
test Score 0.9147
Epoch Time List: [0.6661168579012156, 0.6219913868699223, 0.6049694691319019, 0.6071528389584273, 0.6053894399665296, 0.6042453250847757, 0.6051759708207101, 0.6052343600895256, 0.608471259009093, 0.6108704849611968, 0.6113304847385734, 0.6081901208963245, 0.6086786701343954, 0.6123820031061769, 0.6009960069786757, 0.6331635930109769, 0.607346594100818, 0.6040266079362482, 0.6020079629961401, 0.6002151456195861, 0.6003056759946048, 0.6001440298277885, 0.597418359015137, 0.6008394709788263, 0.6019242061302066, 0.6017876591067761, 0.6023785211145878, 0.6037684171460569, 0.60045470087789, 0.6002323541324586, 0.5993462211918086, 0.6030933456495404, 0.6019829660654068, 0.6030284066218883, 0.6009932179003954, 0.6014040268491954, 0.6023323314730078, 0.5969630007166415, 0.6044365197885782, 0.6063149508554488, 0.7433271049521863, 0.6008604371454567, 0.5999474707059562, 0.6009368076920509, 0.6099493319634348, 0.602409953949973, 0.6259372790809721, 0.6099378049839288, 0.631322214147076, 0.6111119522247463, 0.6144098320510238, 0.6214022669009864, 0.635857624001801, 0.6398152629844844, 0.6297871142160147, 0.6466092069167644, 0.636951986933127, 0.6272488296963274, 0.6276343159843236, 0.6314875998068601, 0.6078621109481901, 0.7199800498783588, 0.6300883330404758, 0.7103174061048776, 0.615858888020739, 0.6320563650224358, 0.6962913481984288, 0.6092617339454591, 0.698530909139663, 0.609235831303522, 0.6051841529551893, 0.6038987287320197, 0.6037865977268666, 0.6011772761121392, 0.6049569481983781, 0.6079992591403425, 0.609706066781655, 0.6105356891639531, 0.6053359899669886, 0.6326770901214331, 0.6281608866993338, 0.6076191009487957, 0.6120175351388752, 0.6220867238007486, 0.6382422060705721, 0.61558210500516, 0.6108099780976772, 0.6157621880993247, 0.6276538968086243, 0.6201300036627799, 0.6597985478583723, 0.6292589050717652, 0.6194383471738547, 0.6240476979874074, 0.6267745629884303, 0.6168920500203967, 0.6161984889768064, 0.6159804142080247, 0.6144813110586256, 0.6225729181896895, 0.6129548682365566, 0.6015492409933358, 0.6134035508148372, 0.6202768760267645, 0.6094679259695113, 0.6046571081969887, 0.5982663838658482, 0.6091763698495924, 0.604811254888773, 0.6356428058352321, 0.6331657830160111, 0.6107243211008608, 0.612901482032612, 0.6184739470481873, 0.6155875008553267, 0.6148801827803254, 0.6129633919335902, 0.6116173828486353, 0.6128221929538995, 0.615804785862565, 0.6118762900587171, 0.6079371841624379, 0.6097144191153347, 0.608685900690034, 0.610279395012185, 0.6080473870970309, 0.6133473198860884, 0.6085422360338271, 0.6428862800821662, 0.6462124681565911, 0.6461507177446038, 0.6498566020745784, 0.652776874601841, 0.608717282069847, 0.6062878498341888, 0.6030472512356937, 0.6082515686284751, 0.6064124514814466, 0.6074223050381988, 0.6105213647242635, 0.608780886977911]
Total Epoch List: [69, 72]
Total Time List: [0.17903915490023792, 0.18526657298207283]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x705e95dd2fb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7019;  Loss pred: 0.7019; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.7031;  Loss pred: 0.7031; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5000 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6992 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.18s
Epoch 12/1000, LR 0.000290
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.18s
Epoch 13/1000, LR 0.000290
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.18s
Epoch 14/1000, LR 0.000290
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.18s
Epoch 15/1000, LR 0.000290
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.18s
Epoch 18/1000, LR 0.000290
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.18s
Epoch 19/1000, LR 0.000290
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.17s
Epoch 21/1000, LR 0.000290
Train loss: 0.5641;  Loss pred: 0.5641; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.5000 time: 0.18s
Epoch 22/1000, LR 0.000290
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.29s
Val loss: 0.6758 score: 0.5116 time: 0.19s
Test loss: 0.6723 score: 0.5391 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.29s
Val loss: 0.6681 score: 0.5659 time: 0.19s
Test loss: 0.6628 score: 0.6328 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.29s
Val loss: 0.6573 score: 0.6744 time: 0.19s
Test loss: 0.6498 score: 0.6719 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 0.4803;  Loss pred: 0.4803; Loss self: 0.0000; time: 0.29s
Val loss: 0.6446 score: 0.8062 time: 0.20s
Test loss: 0.6345 score: 0.7891 time: 0.18s
Epoch 26/1000, LR 0.000290
Train loss: 0.4644;  Loss pred: 0.4644; Loss self: 0.0000; time: 0.29s
Val loss: 0.6258 score: 0.9225 time: 0.19s
Test loss: 0.6122 score: 0.9375 time: 0.18s
Epoch 27/1000, LR 0.000290
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.28s
Val loss: 0.6048 score: 0.9302 time: 0.18s
Test loss: 0.5874 score: 0.9375 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.4206;  Loss pred: 0.4206; Loss self: 0.0000; time: 0.28s
Val loss: 0.5836 score: 0.9535 time: 0.18s
Test loss: 0.5626 score: 0.9297 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.4029;  Loss pred: 0.4029; Loss self: 0.0000; time: 0.28s
Val loss: 0.5515 score: 0.9380 time: 0.18s
Test loss: 0.5272 score: 0.9297 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 0.28s
Val loss: 0.5215 score: 0.9302 time: 0.18s
Test loss: 0.4939 score: 0.9297 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.3564;  Loss pred: 0.3564; Loss self: 0.0000; time: 0.28s
Val loss: 0.4774 score: 0.9457 time: 0.17s
Test loss: 0.4495 score: 0.9219 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 0.3368;  Loss pred: 0.3368; Loss self: 0.0000; time: 0.27s
Val loss: 0.4391 score: 0.9457 time: 0.18s
Test loss: 0.4118 score: 0.9219 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.28s
Val loss: 0.4010 score: 0.9535 time: 0.18s
Test loss: 0.3751 score: 0.9219 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 0.28s
Val loss: 0.3616 score: 0.9457 time: 0.18s
Test loss: 0.3392 score: 0.9219 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.2783;  Loss pred: 0.2783; Loss self: 0.0000; time: 0.28s
Val loss: 0.3364 score: 0.9535 time: 0.18s
Test loss: 0.3170 score: 0.9297 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 0.2580;  Loss pred: 0.2580; Loss self: 0.0000; time: 0.28s
Val loss: 0.3079 score: 0.9535 time: 0.18s
Test loss: 0.2925 score: 0.9297 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.2449;  Loss pred: 0.2449; Loss self: 0.0000; time: 0.28s
Val loss: 0.2884 score: 0.9380 time: 0.18s
Test loss: 0.2753 score: 0.9219 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.28s
Val loss: 0.2717 score: 0.9457 time: 0.17s
Test loss: 0.2608 score: 0.9219 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.27s
Val loss: 0.2488 score: 0.9457 time: 0.18s
Test loss: 0.2430 score: 0.9219 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.28s
Val loss: 0.2329 score: 0.9457 time: 0.18s
Test loss: 0.2315 score: 0.9219 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.1879;  Loss pred: 0.1879; Loss self: 0.0000; time: 0.28s
Val loss: 0.2373 score: 0.9535 time: 0.17s
Test loss: 0.2362 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.27s
Val loss: 0.2359 score: 0.9302 time: 0.18s
Test loss: 0.2373 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.1618;  Loss pred: 0.1618; Loss self: 0.0000; time: 0.27s
Val loss: 0.2257 score: 0.9380 time: 0.18s
Test loss: 0.2309 score: 0.9297 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 0.1531;  Loss pred: 0.1531; Loss self: 0.0000; time: 0.28s
Val loss: 0.2176 score: 0.9457 time: 0.17s
Test loss: 0.2263 score: 0.9297 time: 0.16s
Epoch 45/1000, LR 0.000289
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.28s
Val loss: 0.2094 score: 0.9457 time: 0.18s
Test loss: 0.2218 score: 0.9297 time: 0.16s
Epoch 46/1000, LR 0.000289
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.27s
Val loss: 0.1855 score: 0.9457 time: 0.18s
Test loss: 0.2029 score: 0.9219 time: 0.16s
Epoch 47/1000, LR 0.000289
Train loss: 0.1294;  Loss pred: 0.1294; Loss self: 0.0000; time: 0.28s
Val loss: 0.1784 score: 0.9380 time: 0.18s
Test loss: 0.1983 score: 0.9219 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.28s
Val loss: 0.1725 score: 0.9457 time: 0.18s
Test loss: 0.1961 score: 0.9219 time: 0.17s
Epoch 49/1000, LR 0.000289
Train loss: 0.1110;  Loss pred: 0.1110; Loss self: 0.0000; time: 0.28s
Val loss: 0.1910 score: 0.9225 time: 0.18s
Test loss: 0.2087 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1082;  Loss pred: 0.1082; Loss self: 0.0000; time: 0.28s
Val loss: 0.1726 score: 0.9380 time: 0.18s
Test loss: 0.1990 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.29s
Val loss: 0.1626 score: 0.9457 time: 0.18s
Test loss: 0.1956 score: 0.9219 time: 0.17s
Epoch 52/1000, LR 0.000289
Train loss: 0.0946;  Loss pred: 0.0946; Loss self: 0.0000; time: 0.28s
Val loss: 0.1694 score: 0.9457 time: 0.18s
Test loss: 0.2093 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0994;  Loss pred: 0.0994; Loss self: 0.0000; time: 0.28s
Val loss: 0.1814 score: 0.9380 time: 0.18s
Test loss: 0.2243 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.29s
Val loss: 0.1709 score: 0.9457 time: 0.19s
Test loss: 0.2128 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.29s
Val loss: 0.1674 score: 0.9457 time: 0.19s
Test loss: 0.2082 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.29s
Val loss: 0.1823 score: 0.9380 time: 0.19s
Test loss: 0.2241 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0826;  Loss pred: 0.0826; Loss self: 0.0000; time: 0.28s
Val loss: 0.1743 score: 0.9457 time: 0.18s
Test loss: 0.2142 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.28s
Val loss: 0.1756 score: 0.9457 time: 0.18s
Test loss: 0.2157 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.28s
Val loss: 0.1795 score: 0.9457 time: 0.18s
Test loss: 0.2213 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.28s
Val loss: 0.1839 score: 0.9380 time: 0.18s
Test loss: 0.2274 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.28s
Val loss: 0.2079 score: 0.9457 time: 0.18s
Test loss: 0.2539 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.29s
Val loss: 0.1963 score: 0.9380 time: 0.18s
Test loss: 0.2424 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.28s
Val loss: 0.2151 score: 0.9457 time: 0.18s
Test loss: 0.2649 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.28s
Val loss: 0.1963 score: 0.9380 time: 0.18s
Test loss: 0.2458 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.28s
Val loss: 0.1958 score: 0.9380 time: 0.18s
Test loss: 0.2499 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.28s
Val loss: 0.2177 score: 0.9457 time: 0.18s
Test loss: 0.2768 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.28s
Val loss: 0.1845 score: 0.9535 time: 0.18s
Test loss: 0.2345 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.28s
Val loss: 0.1936 score: 0.9535 time: 0.18s
Test loss: 0.2460 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.28s
Val loss: 0.1953 score: 0.9457 time: 0.18s
Test loss: 0.2469 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.28s
Val loss: 0.1866 score: 0.9535 time: 0.18s
Test loss: 0.2378 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.28s
Val loss: 0.1808 score: 0.9457 time: 0.18s
Test loss: 0.2264 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0980,   Val_Loss: 0.1626,   Val_Precision: 0.9531,   Val_Recall: 0.9385,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1626,   Test_Precision: 0.9500,   Test_Recall: 0.8906,   Test_accuracy: 0.9194,   Test_Score: 0.9219,   Test_loss: 0.1956


[0.20203094091266394, 0.1853464161977172, 0.17602922022342682, 0.1783220369834453, 0.1776237089652568, 0.1756696121301502, 0.17862878693267703, 0.17769696610048413, 0.1769575399812311, 0.1781231260392815, 0.18059307499788702, 0.17705765296705067, 0.17381702503189445, 0.17979861213825643, 0.1781919680070132, 0.18841162300668657, 0.17806491418741643, 0.1773999440483749, 0.17496636603027582, 0.1766059179790318, 0.17815742478705943, 0.17528726207092404, 0.17512690112926066, 0.17670662002637982, 0.17973256018012762, 0.17605704790912569, 0.17893565515987575, 0.17716158693656325, 0.17450813297182322, 0.17254091217182577, 0.17640171688981354, 0.17838772712275386, 0.17508103186264634, 0.17380196694284678, 0.17627852596342564, 0.1780499799642712, 0.17578871711157262, 0.17452096613124013, 0.18506414792500436, 0.17762348288670182, 0.32079543685540557, 0.17634347104467452, 0.1767204429488629, 0.1749326919671148, 0.18402411695569754, 0.1772565939463675, 0.19778340193443, 0.17278868006542325, 0.17942229215987027, 0.18031846010126173, 0.18444913905113935, 0.17944411607459188, 0.19054289697669446, 0.18833354581147432, 0.18499739398248494, 0.19513083179481328, 0.189334083115682, 0.18352914601564407, 0.19229462509974837, 0.17693545599468052, 0.17335508298128843, 0.18784612300805748, 0.18016563379205763, 0.2779016599524766, 0.18423546804115176, 0.1854449629317969, 0.17896088608540595, 0.17952875909395516, 0.17862862395122647, 0.18446812499314547, 0.1810460400301963, 0.17804349004290998, 0.18175965803675354, 0.1818988830782473, 0.18181254994124174, 0.18310497980564833, 0.18575795809738338, 0.1840633109677583, 0.18075177911669016, 0.19566972088068724, 0.19001130387187004, 0.17952689388766885, 0.17778945108875632, 0.18952951091341674, 0.18621797394007444, 0.17845414695329964, 0.18414502800442278, 0.19049029401503503, 0.1894830649252981, 0.181661956012249, 0.20185325597412884, 0.18658236297778785, 0.18092650594189763, 0.18536559608764946, 0.18703376594930887, 0.18274672399275005, 0.18463222589343786, 0.18545056111179292, 0.18311799108050764, 0.1846157459076494, 0.1808960479684174, 0.1771977050229907, 0.17958932206965983, 0.1843078602105379, 0.18128236406482756, 0.18031333084218204, 0.17969846283085644, 0.18298609601333737, 0.18143084715120494, 0.1909846030175686, 0.18868259387090802, 0.1834075169172138, 0.1829707392025739, 0.18415644415654242, 0.18262418103404343, 0.18006488401442766, 0.1836602760013193, 0.18243679800070822, 0.18048262502998114, 0.18312495085410774, 0.1835723367985338, 0.17895553610287607, 0.18307400704361498, 0.18400325789116323, 0.18293872894719243, 0.18048258707858622, 0.1848295540548861, 0.18254127097316086, 0.19172607897780836, 0.19718291889876127, 0.1958854820113629, 0.19514121720567346, 0.1963924141600728, 0.18248430802486837, 0.17965130391530693, 0.1798708001151681, 0.18558943318203092, 0.18152691703289747, 0.1802628489676863, 0.1841897030826658, 0.1846458399668336, 0.1778900211211294, 0.17887406097725034, 0.2600172138772905, 0.17535934783518314, 0.16299515892751515, 0.1690242770127952, 0.16800784994848073, 0.1803615370299667, 0.1796007480006665, 0.17975635407492518, 0.18068832089193165, 0.1796750840730965, 0.1820862761233002, 0.1812251261435449, 0.1776706010568887, 0.1784538219217211, 0.18375674285925925, 0.17980439588427544, 0.17849785508587956, 0.17882850696332753, 0.1796751101501286, 0.17726718494668603, 0.17813567398115993, 0.1848339238204062, 0.1820911280810833, 0.18329638009890914, 0.16925582708790898, 0.16560708894394338, 0.17021927307359874, 0.165810703067109, 0.169344900874421, 0.16842502099461854, 0.16795909078791738, 0.16475713485851884, 0.1700857300311327, 0.16900889202952385, 0.1668152129277587, 0.16884083906188607, 0.16762671316973865, 0.16458884114399552, 0.16618971503339708, 0.1674977489747107, 0.1627415989059955, 0.16430812911130488, 0.16918654181063175, 0.1686132620088756, 0.16667932295240462, 0.17110119597055018, 0.17036526300944388, 0.16891827201470733, 0.17063953983597457, 0.16878004209138453, 0.1668990480247885, 0.17693880503065884, 0.17916732002049685, 0.17725624796003103, 0.16838562698103487, 0.1703447881154716, 0.1688519511371851, 0.17204166110605001, 0.16704904194921255, 0.1694787589367479, 0.1689946590922773, 0.1680106478743255, 0.16809273604303598, 0.17042396985925734, 0.1704954809974879, 0.16665616701357067, 0.1677053750026971, 0.17103450396098197, 0.16684900200925767]
[0.0015661313249043715, 0.0014367939240133116, 0.0013645675986312157, 0.0013823413719646922, 0.001376927976474859, 0.0013617799389934123, 0.001384719278547884, 0.0013774958612440631, 0.0013717638758234969, 0.001380799426661097, 0.0013999463178130777, 0.001372539945481013, 0.0013474187986968562, 0.0013937876909942359, 0.0013813330853256838, 0.0014605552171060975, 0.0013803481719954762, 0.0013751933647160844, 0.0013563284188393475, 0.0013690381238684636, 0.0013810653084268172, 0.0013588159850459228, 0.0013575728769710128, 0.0013698187598944172, 0.001393275660311067, 0.0013647833171250053, 0.0013870981020145407, 0.001373345635167157, 0.001352776224587777, 0.0013375264509443857, 0.001367455169688477, 0.001382850597850805, 0.0013572173012608244, 0.0013473020693243935, 0.0013665002012668655, 0.0013802324028238076, 0.0013627032334230435, 0.001352875706443722, 0.0014346057978682509, 0.001376926223927921, 0.0024867863322124463, 0.0013670036515091048, 0.0013699259143322706, 0.0013560673795900372, 0.0014265435422922289, 0.001374082123615252, 0.0015332046661583722, 0.0013394471322901028, 0.0013908704818594594, 0.0013978175201648196, 0.0014298382872181346, 0.0013910396594929602, 0.0014770767207495695, 0.0014599499675308086, 0.0014340883254456197, 0.0015126421069365371, 0.0014677060706642015, 0.0014227065582608069, 0.0014906560085251811, 0.0013715926821293064, 0.0013438378525681275, 0.001456171496186492, 0.00139663282009347, 0.002154276433740129, 0.0014281819227996261, 0.0014375578521844722, 0.0013872936905845422, 0.001391695806929885, 0.0013847180151257865, 0.001429985465063143, 0.0014034576746526845, 0.0013801820933558914, 0.001408989597184136, 0.0014100688610716847, 0.0014093996119476104, 0.001419418448105801, 0.0014399841712975456, 0.0014268473718430875, 0.0014011765822999236, 0.0015168195417107537, 0.0014729558439679849, 0.0013916813479664253, 0.001378212799137646, 0.001469221014832688, 0.0014435501855819724, 0.0013833654802581368, 0.0014274808372435875, 0.0014766689458529847, 0.0014688609684131635, 0.001408232217149217, 0.0015647539222800686, 0.001446374906804557, 0.00140253105381316, 0.001436942605330616, 0.0014498741546458051, 0.0014166412712616283, 0.0014312575650654098, 0.0014376012489286272, 0.0014195193107016096, 0.0014311298132375922, 0.0014022949454916079, 0.0013736256203332612, 0.0013921652873617042, 0.0014287431024072707, 0.001405289643913392, 0.0013977777584665274, 0.001393011339774081, 0.0014184968683204448, 0.001406440675590736, 0.001480500798585803, 0.0014626557664411475, 0.001421763697032665, 0.0014183778232757665, 0.0014275693345468404, 0.001415691325845298, 0.0013958518140653308, 0.0014237230697776689, 0.0014142387441915367, 0.0013990901165114816, 0.0014195732624349437, 0.0014230413705312698, 0.0013872522178517525, 0.0014191783491753099, 0.0014263818441175443, 0.0014181296817611815, 0.0013990898223146218, 0.001432787240735551, 0.0014150486121950454, 0.0014862486742465764, 0.0015285497589051262, 0.0015184921086152164, 0.0015127226139974687, 0.001522421815194363, 0.001414607038952468, 0.001392645766785325, 0.0013943472877144814, 0.0014386777766048908, 0.0014071854033557943, 0.0013973864261060953, 0.00142782715567958, 0.00143136310051809, 0.0013897657900088234, 0.0013974536013847683, 0.002031384483416332, 0.0013699949049623683, 0.001273399679121212, 0.0013205021641624626, 0.0013125613277225057, 0.001409074508046615, 0.001403130843755207, 0.001404346516210353, 0.001411627506968216, 0.0014037115943210665, 0.0014225490322132828, 0.0014158212979964446, 0.001388051570756943, 0.001394170483763446, 0.0014355995535879629, 0.0014047218428459018, 0.001394514492858434, 0.0013970977106509963, 0.0014037117980478797, 0.0013848998823959846, 0.001391684952977812, 0.0014440150298469234, 0.0014225869381334633, 0.0014320029695227277, 0.001322311149124289, 0.0012938053823745577, 0.0013298380708874902, 0.001295396117711789, 0.001323007038081414, 0.0013158204765204573, 0.0013121803967806045, 0.0012871651160821784, 0.0013287947658682242, 0.001320381968980655, 0.0013032438509981148, 0.001319069055170985, 0.0013095836966385832, 0.001285850321437465, 0.0012983571486984147, 0.0013085761638649274, 0.0012714187414530898, 0.0012836572586820694, 0.0013217698578955606, 0.0013172911094443407, 0.001302182210565661, 0.0013367280935199233, 0.0013309786172612803, 0.001319674000114901, 0.0013331214049685514, 0.0013185940788389416, 0.0013038988126936601, 0.0013823344143020222, 0.0013997446876601316, 0.0013848144371877424, 0.001315512710789335, 0.0013308186571521219, 0.0013191558682592586, 0.0013440754773910157, 0.001305070640228223, 0.001324052804193343, 0.0013202707741584163, 0.001312583186518168, 0.0013132245003362186, 0.001331437264525448, 0.0013319959452928742, 0.0013020013047935208, 0.0013101982422085712, 0.0013362070621951716, 0.0013035078281973256]
[638.51605807135, 695.9940345562982, 732.8328776112595, 723.4103096970334, 726.2543989847234, 734.3330382287542, 722.1680346999082, 725.9549942290688, 728.9884342519811, 724.2181454391927, 714.3131042068436, 728.5762452979432, 742.1597509008639, 717.4693868093111, 723.9383539157212, 684.6711362144675, 724.4549022398946, 727.1704661012926, 737.2845588944683, 730.4398486539733, 724.0787194481832, 735.9348219370586, 736.6087058480266, 730.0235836141402, 717.7330577760449, 732.717045594137, 720.9295424365862, 728.1488173064931, 739.2205612607686, 747.6487655955747, 731.2853994532136, 723.1439184783789, 736.801689067051, 742.2240511375829, 731.7964527725004, 724.5156670384691, 733.835493651871, 739.1662036926367, 697.0555963777281, 726.2553233588118, 402.1254206871561, 731.526941347998, 729.966481791404, 737.4264841488315, 700.9950768086327, 727.7585399109695, 652.2286437502305, 746.5766851807452, 718.9742057528582, 715.4009629826988, 699.3797892666453, 718.8867644251812, 677.0129039015196, 684.9549794444565, 697.3071199706379, 661.094911621388, 681.3353300006833, 702.8856331571662, 670.8455836094444, 729.0794220683407, 744.1373958092938, 686.7322994708103, 716.0078050672436, 464.1929811504545, 700.1909098805335, 695.6241785194442, 720.8279016814714, 718.5478284985456, 722.168693608829, 699.3078072691064, 712.5259407965198, 724.5420765954988, 709.7284479590899, 709.185223223763, 709.521977672555, 704.5138812550235, 694.452077968966, 700.8458085522349, 713.6859212695211, 659.2742066548959, 678.90697748692, 718.5552938977273, 725.5773568680429, 680.6327910534809, 692.736567102339, 722.8747675656901, 700.5347980229022, 677.1998576988824, 680.7996274013038, 710.1101564231857, 639.0781232507525, 691.3836760410046, 712.9966907194173, 695.9220196341224, 689.7150327121277, 705.8950069338464, 698.6862633311559, 695.6031797727293, 704.4638226906131, 698.7486325490885, 713.1167399661605, 728.0003992335159, 718.3055123397758, 699.9158899280865, 711.5970749028234, 715.4213135406296, 717.8692458901163, 704.9715951675232, 711.0147035387597, 675.447119620074, 683.6878662387832, 703.3517609762299, 705.0307637287248, 700.4913707518342, 706.3686707290574, 716.4084252522213, 702.3837860238942, 707.0941905015194, 714.7502424600206, 704.4370491204768, 702.7202586715142, 720.8494512616913, 704.633072073784, 701.0745433447852, 705.154128611211, 714.7503927557868, 697.9403302660827, 706.6895026657667, 672.8349147271265, 654.2148818997445, 658.5480387592837, 661.0597281661801, 656.8481809834899, 706.9100976201214, 718.057688358413, 717.1814431102969, 695.0826767894348, 710.638411694183, 715.6216643570544, 700.3648838182, 698.6347486798035, 719.5457012894606, 715.5872645854413, 492.2751001416654, 729.929721911972, 785.299396879157, 757.287664601638, 761.8691628947752, 709.6856797063836, 712.6919092760404, 712.0749675788799, 708.4021776734305, 712.3970508227299, 702.9634672375005, 706.3038262068235, 720.4343275622477, 717.2723936175897, 696.5730781266417, 711.8847087719844, 717.0954515863294, 715.7695502442965, 712.3969474294399, 722.0738572595748, 718.5534325569038, 692.5135676087856, 702.9447362366986, 698.3225742424892, 756.2516588189231, 772.9137732946139, 751.9712526598316, 771.9646418011642, 755.8538777315732, 759.9820931837071, 762.0903363999876, 776.90110422178, 752.5616639124912, 757.3566009629832, 767.3161083661591, 758.1104234685989, 763.6014426315651, 777.6954932686799, 770.2041006224568, 764.1893743857167, 786.5229348885573, 779.0241462325386, 756.5613590191393, 759.1336439079283, 767.9416842636832, 748.0952969027246, 751.3268710940479, 757.762901984075, 750.1192286561404, 758.3835056202645, 766.930677645261, 723.4139508166169, 714.4159994431838, 722.1184103415205, 760.1598918797062, 751.4171781600542, 758.0605325431235, 744.0058365926737, 766.242047882654, 755.2568876656194, 757.4203864638545, 761.8564752856969, 761.4844223085807, 751.0680575373717, 750.7530360988627, 768.048385449649, 763.2432770741036, 748.3870040000852, 767.1607169271404]
Elapsed: 0.18041113810723577~0.015048737698390505
Time per graph: 0.0014020542401608786~0.00011532208301849069
Speed: 716.657389366622~42.788028177276125
Total Time: 0.1676
best val loss: 0.16256537946850755 test_score: 0.9219

Testing...
Test loss: 0.5626 score: 0.9297 time: 0.18s
test Score 0.9297
Epoch Time List: [0.6661168579012156, 0.6219913868699223, 0.6049694691319019, 0.6071528389584273, 0.6053894399665296, 0.6042453250847757, 0.6051759708207101, 0.6052343600895256, 0.608471259009093, 0.6108704849611968, 0.6113304847385734, 0.6081901208963245, 0.6086786701343954, 0.6123820031061769, 0.6009960069786757, 0.6331635930109769, 0.607346594100818, 0.6040266079362482, 0.6020079629961401, 0.6002151456195861, 0.6003056759946048, 0.6001440298277885, 0.597418359015137, 0.6008394709788263, 0.6019242061302066, 0.6017876591067761, 0.6023785211145878, 0.6037684171460569, 0.60045470087789, 0.6002323541324586, 0.5993462211918086, 0.6030933456495404, 0.6019829660654068, 0.6030284066218883, 0.6009932179003954, 0.6014040268491954, 0.6023323314730078, 0.5969630007166415, 0.6044365197885782, 0.6063149508554488, 0.7433271049521863, 0.6008604371454567, 0.5999474707059562, 0.6009368076920509, 0.6099493319634348, 0.602409953949973, 0.6259372790809721, 0.6099378049839288, 0.631322214147076, 0.6111119522247463, 0.6144098320510238, 0.6214022669009864, 0.635857624001801, 0.6398152629844844, 0.6297871142160147, 0.6466092069167644, 0.636951986933127, 0.6272488296963274, 0.6276343159843236, 0.6314875998068601, 0.6078621109481901, 0.7199800498783588, 0.6300883330404758, 0.7103174061048776, 0.615858888020739, 0.6320563650224358, 0.6962913481984288, 0.6092617339454591, 0.698530909139663, 0.609235831303522, 0.6051841529551893, 0.6038987287320197, 0.6037865977268666, 0.6011772761121392, 0.6049569481983781, 0.6079992591403425, 0.609706066781655, 0.6105356891639531, 0.6053359899669886, 0.6326770901214331, 0.6281608866993338, 0.6076191009487957, 0.6120175351388752, 0.6220867238007486, 0.6382422060705721, 0.61558210500516, 0.6108099780976772, 0.6157621880993247, 0.6276538968086243, 0.6201300036627799, 0.6597985478583723, 0.6292589050717652, 0.6194383471738547, 0.6240476979874074, 0.6267745629884303, 0.6168920500203967, 0.6161984889768064, 0.6159804142080247, 0.6144813110586256, 0.6225729181896895, 0.6129548682365566, 0.6015492409933358, 0.6134035508148372, 0.6202768760267645, 0.6094679259695113, 0.6046571081969887, 0.5982663838658482, 0.6091763698495924, 0.604811254888773, 0.6356428058352321, 0.6331657830160111, 0.6107243211008608, 0.612901482032612, 0.6184739470481873, 0.6155875008553267, 0.6148801827803254, 0.6129633919335902, 0.6116173828486353, 0.6128221929538995, 0.615804785862565, 0.6118762900587171, 0.6079371841624379, 0.6097144191153347, 0.608685900690034, 0.610279395012185, 0.6080473870970309, 0.6133473198860884, 0.6085422360338271, 0.6428862800821662, 0.6462124681565911, 0.6461507177446038, 0.6498566020745784, 0.652776874601841, 0.608717282069847, 0.6062878498341888, 0.6030472512356937, 0.6082515686284751, 0.6064124514814466, 0.6074223050381988, 0.6105213647242635, 0.608780886977911, 0.7292424510233104, 0.6334763010963798, 0.7236094537656754, 0.6411348730325699, 0.615068546263501, 0.7148120631463826, 0.6578456431161612, 0.7354044010862708, 0.6538946686778218, 0.660911878105253, 0.6571158990263939, 0.6539291969966143, 0.657852518139407, 0.6530724589247257, 0.6522359370719641, 0.6550776967778802, 0.6585268552880734, 0.6557852793484926, 0.6480189110152423, 0.6510525271296501, 0.6456353452522308, 0.650617535924539, 0.6476209859829396, 0.6621213380713016, 0.6613662440795451, 0.6630619748029858, 0.6195945632643998, 0.6230330823455006, 0.6258170120418072, 0.6230423341039568, 0.6176454860251397, 0.6131014646962285, 0.61971170688048, 0.6159807147923857, 0.6216090291272849, 0.61984295793809, 0.6195957651361823, 0.6190477781929076, 0.6140417249407619, 0.6171801937744021, 0.6128015681169927, 0.6130807942245156, 0.6092366189695895, 0.6081090881489217, 0.6186631578020751, 0.6207797138486058, 0.6189105187077075, 0.6244937099982053, 0.6256669491995126, 0.6287285417784005, 0.6281224659178406, 0.6250948070082814, 0.6200097010005265, 0.6509563040453941, 0.6505152850877494, 0.6521270060911775, 0.6268528851214796, 0.6253461220767349, 0.6281369922216982, 0.6299387647304684, 0.6257159202359617, 0.6310408348217607, 0.6205256241373718, 0.6216582132037729, 0.6242228457704186, 0.6245343361515552, 0.6277593839913607, 0.62031847727485, 0.6236588980536908, 0.6289317379705608, 0.6210409488994628]
Total Epoch List: [69, 72, 71]
Total Time List: [0.17903915490023792, 0.18526657298207283, 0.16759291081689298]
T-times Epoch Time: 0.6267750629306613 ~ 0.006801350197778636
T-times Total Epoch: 72.44444444444446 ~ 3.247030932489433
T-times Total Time: 0.1790487532141722 ~ 0.0032298456142061463
T-times Inference Elapsed: 0.17976869198145418 ~ 0.001788153660706903
T-times Time Per Graph: 0.0013973543086156814 ~ 1.3656124036774488e-05
T-times Speed: 719.346204899029 ~ 7.173169069897298
T-times cross validation test micro f1 score:0.9100979632876998 ~ 0.0032120220274718106
T-times cross validation test precision:0.9500278985573978 ~ 0.014353254278139091
T-times cross validation test recall:0.8739316239316239 ~ 0.008811026399228617
T-times cross validation test f1_score:0.9100979632876998 ~ 0.0022917403817610507
