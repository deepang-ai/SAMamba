Namespace(seed=15, model='AEtransGAT', dataset='exchange/averVolume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/averVolume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
Data(edge_index=[2, 106], edge_attr=[106, 2], x=[19, 14887], y=[1, 1], num_nodes=19)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dcd3aff10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.3399;  Loss pred: 3.3399; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 3.3439;  Loss pred: 3.3439; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 3.3418;  Loss pred: 3.3418; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 3.2755;  Loss pred: 3.2755; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.25s
Epoch 5/1000, LR 0.000105
Train loss: 3.2419;  Loss pred: 3.2419; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.20s
Epoch 6/1000, LR 0.000135
Train loss: 3.1880;  Loss pred: 3.1880; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 3.1107;  Loss pred: 3.1107; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 3.0626;  Loss pred: 3.0626; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 2.9418;  Loss pred: 2.9418; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 2.8669;  Loss pred: 2.8669; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 2.7675;  Loss pred: 2.7675; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 2.6531;  Loss pred: 2.6531; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 2.5846;  Loss pred: 2.5846; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.24s
Epoch 14/1000, LR 0.000285
Train loss: 2.4941;  Loss pred: 2.4941; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.23s
Epoch 15/1000, LR 0.000285
Train loss: 2.4229;  Loss pred: 2.4229; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.15s
Epoch 16/1000, LR 0.000285
Train loss: 2.3337;  Loss pred: 2.3337; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 2.2676;  Loss pred: 2.2676; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 2.1877;  Loss pred: 2.1877; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 2.1232;  Loss pred: 2.1232; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 2.0592;  Loss pred: 2.0592; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.25s
Test loss: 0.6928 score: 0.5116 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 1.9822;  Loss pred: 1.9822; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.17s
Test loss: 0.6927 score: 0.5116 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 1.9205;  Loss pred: 1.9205; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
Test loss: 0.6926 score: 0.5116 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.8783;  Loss pred: 1.8783; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.18s
Test loss: 0.6925 score: 0.5194 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.8061;  Loss pred: 1.8061; Loss self: 0.0000; time: 0.28s
Val loss: 0.6927 score: 0.5194 time: 0.17s
Test loss: 0.6925 score: 0.5504 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 1.7729;  Loss pred: 1.7729; Loss self: 0.0000; time: 0.26s
Val loss: 0.6926 score: 0.8527 time: 0.18s
Test loss: 0.6924 score: 0.9147 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.7162;  Loss pred: 1.7162; Loss self: 0.0000; time: 0.35s
Val loss: 0.6925 score: 0.8682 time: 0.17s
Test loss: 0.6923 score: 0.8992 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 1.6747;  Loss pred: 1.6747; Loss self: 0.0000; time: 0.27s
Val loss: 0.6924 score: 0.8527 time: 0.16s
Test loss: 0.6922 score: 0.8837 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.6322;  Loss pred: 1.6322; Loss self: 0.0000; time: 0.26s
Val loss: 0.6923 score: 0.8605 time: 0.16s
Test loss: 0.6921 score: 0.8682 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.5901;  Loss pred: 1.5901; Loss self: 0.0000; time: 0.26s
Val loss: 0.6922 score: 0.8605 time: 0.16s
Test loss: 0.6919 score: 0.8837 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 1.5474;  Loss pred: 1.5474; Loss self: 0.0000; time: 0.28s
Val loss: 0.6921 score: 0.8605 time: 0.17s
Test loss: 0.6918 score: 0.8915 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.5106;  Loss pred: 1.5106; Loss self: 0.0000; time: 0.28s
Val loss: 0.6920 score: 0.8295 time: 0.17s
Test loss: 0.6917 score: 0.8450 time: 0.19s
Epoch 32/1000, LR 0.000285
Train loss: 1.4858;  Loss pred: 1.4858; Loss self: 0.0000; time: 0.28s
Val loss: 0.6919 score: 0.7907 time: 0.23s
Test loss: 0.6915 score: 0.8140 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 1.4404;  Loss pred: 1.4404; Loss self: 0.0000; time: 0.27s
Val loss: 0.6917 score: 0.7984 time: 0.17s
Test loss: 0.6914 score: 0.8217 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 1.4155;  Loss pred: 1.4155; Loss self: 0.0000; time: 0.26s
Val loss: 0.6916 score: 0.8062 time: 0.38s
Test loss: 0.6912 score: 0.8217 time: 0.25s
Epoch 35/1000, LR 0.000285
Train loss: 1.3907;  Loss pred: 1.3907; Loss self: 0.0000; time: 0.38s
Val loss: 0.6914 score: 0.8295 time: 0.24s
Test loss: 0.6910 score: 0.8450 time: 0.24s
Epoch 36/1000, LR 0.000285
Train loss: 1.3641;  Loss pred: 1.3641; Loss self: 0.0000; time: 0.38s
Val loss: 0.6913 score: 0.8450 time: 0.25s
Test loss: 0.6908 score: 0.8527 time: 0.24s
Epoch 37/1000, LR 0.000285
Train loss: 1.3468;  Loss pred: 1.3468; Loss self: 0.0000; time: 0.40s
Val loss: 0.6911 score: 0.8450 time: 0.18s
Test loss: 0.6906 score: 0.8527 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.3199;  Loss pred: 1.3199; Loss self: 0.0000; time: 0.27s
Val loss: 0.6909 score: 0.7984 time: 0.18s
Test loss: 0.6904 score: 0.8372 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 1.2947;  Loss pred: 1.2947; Loss self: 0.0000; time: 0.29s
Val loss: 0.6907 score: 0.7984 time: 0.23s
Test loss: 0.6902 score: 0.8217 time: 0.25s
Epoch 40/1000, LR 0.000284
Train loss: 1.2665;  Loss pred: 1.2665; Loss self: 0.0000; time: 0.39s
Val loss: 0.6904 score: 0.7752 time: 0.26s
Test loss: 0.6899 score: 0.7829 time: 0.25s
Epoch 41/1000, LR 0.000284
Train loss: 1.2534;  Loss pred: 1.2534; Loss self: 0.0000; time: 0.39s
Val loss: 0.6902 score: 0.7984 time: 0.27s
Test loss: 0.6896 score: 0.8295 time: 0.29s
Epoch 42/1000, LR 0.000284
Train loss: 1.2335;  Loss pred: 1.2335; Loss self: 0.0000; time: 0.29s
Val loss: 0.6900 score: 0.8062 time: 0.18s
Test loss: 0.6893 score: 0.8450 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 1.2168;  Loss pred: 1.2168; Loss self: 0.0000; time: 0.29s
Val loss: 0.6897 score: 0.7907 time: 0.18s
Test loss: 0.6890 score: 0.8217 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 1.2040;  Loss pred: 1.2040; Loss self: 0.0000; time: 0.28s
Val loss: 0.6894 score: 0.7984 time: 0.17s
Test loss: 0.6887 score: 0.8372 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.1910;  Loss pred: 1.1910; Loss self: 0.0000; time: 0.28s
Val loss: 0.6891 score: 0.8217 time: 0.17s
Test loss: 0.6883 score: 0.8682 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 1.1798;  Loss pred: 1.1798; Loss self: 0.0000; time: 0.33s
Val loss: 0.6888 score: 0.8062 time: 0.25s
Test loss: 0.6880 score: 0.8527 time: 0.25s
Epoch 47/1000, LR 0.000284
Train loss: 1.1632;  Loss pred: 1.1632; Loss self: 0.0000; time: 0.39s
Val loss: 0.6885 score: 0.8062 time: 0.25s
Test loss: 0.6875 score: 0.8605 time: 0.25s
Epoch 48/1000, LR 0.000284
Train loss: 1.1498;  Loss pred: 1.1498; Loss self: 0.0000; time: 0.39s
Val loss: 0.6881 score: 0.7907 time: 0.26s
Test loss: 0.6871 score: 0.8295 time: 0.21s
Epoch 49/1000, LR 0.000284
Train loss: 1.1451;  Loss pred: 1.1451; Loss self: 0.0000; time: 0.28s
Val loss: 0.6876 score: 0.7907 time: 0.19s
Test loss: 0.6865 score: 0.8372 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 1.1338;  Loss pred: 1.1338; Loss self: 0.0000; time: 0.29s
Val loss: 0.6871 score: 0.8062 time: 0.18s
Test loss: 0.6859 score: 0.8760 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 1.1244;  Loss pred: 1.1244; Loss self: 0.0000; time: 0.34s
Val loss: 0.6866 score: 0.8372 time: 0.26s
Test loss: 0.6854 score: 0.8915 time: 0.26s
Epoch 52/1000, LR 0.000284
Train loss: 1.1138;  Loss pred: 1.1138; Loss self: 0.0000; time: 0.39s
Val loss: 0.6861 score: 0.8372 time: 0.27s
Test loss: 0.6848 score: 0.8915 time: 0.26s
Epoch 53/1000, LR 0.000284
Train loss: 1.1085;  Loss pred: 1.1085; Loss self: 0.0000; time: 0.37s
Val loss: 0.6855 score: 0.8527 time: 0.18s
Test loss: 0.6841 score: 0.8915 time: 0.18s
Epoch 54/1000, LR 0.000284
Train loss: 1.0995;  Loss pred: 1.0995; Loss self: 0.0000; time: 0.28s
Val loss: 0.6849 score: 0.8605 time: 0.17s
Test loss: 0.6834 score: 0.8915 time: 0.16s
Epoch 55/1000, LR 0.000284
Train loss: 1.0881;  Loss pred: 1.0881; Loss self: 0.0000; time: 0.27s
Val loss: 0.6843 score: 0.8682 time: 0.17s
Test loss: 0.6826 score: 0.8915 time: 0.18s
Epoch 56/1000, LR 0.000284
Train loss: 1.0849;  Loss pred: 1.0849; Loss self: 0.0000; time: 0.29s
Val loss: 0.6836 score: 0.8527 time: 0.18s
Test loss: 0.6819 score: 0.8915 time: 0.18s
Epoch 57/1000, LR 0.000283
Train loss: 1.0765;  Loss pred: 1.0765; Loss self: 0.0000; time: 0.27s
Val loss: 0.6829 score: 0.8527 time: 0.16s
Test loss: 0.6810 score: 0.8915 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 1.0691;  Loss pred: 1.0691; Loss self: 0.0000; time: 0.27s
Val loss: 0.6821 score: 0.8527 time: 0.17s
Test loss: 0.6801 score: 0.8915 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 1.0655;  Loss pred: 1.0655; Loss self: 0.0000; time: 0.28s
Val loss: 0.6813 score: 0.8450 time: 0.21s
Test loss: 0.6791 score: 0.8915 time: 0.20s
Epoch 60/1000, LR 0.000283
Train loss: 1.0593;  Loss pred: 1.0593; Loss self: 0.0000; time: 0.28s
Val loss: 0.6804 score: 0.8605 time: 0.23s
Test loss: 0.6781 score: 0.8915 time: 0.19s
Epoch 61/1000, LR 0.000283
Train loss: 1.0530;  Loss pred: 1.0530; Loss self: 0.0000; time: 0.27s
Val loss: 0.6795 score: 0.8605 time: 0.17s
Test loss: 0.6769 score: 0.8915 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 1.0483;  Loss pred: 1.0483; Loss self: 0.0000; time: 0.29s
Val loss: 0.6784 score: 0.8682 time: 0.18s
Test loss: 0.6757 score: 0.8915 time: 0.17s
Epoch 63/1000, LR 0.000283
Train loss: 1.0433;  Loss pred: 1.0433; Loss self: 0.0000; time: 0.29s
Val loss: 0.6773 score: 0.8605 time: 0.17s
Test loss: 0.6743 score: 0.8915 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 1.0380;  Loss pred: 1.0380; Loss self: 0.0000; time: 0.29s
Val loss: 0.6761 score: 0.8450 time: 0.18s
Test loss: 0.6730 score: 0.8915 time: 0.17s
Epoch 65/1000, LR 0.000283
Train loss: 1.0362;  Loss pred: 1.0362; Loss self: 0.0000; time: 0.29s
Val loss: 0.6748 score: 0.8450 time: 0.20s
Test loss: 0.6715 score: 0.8915 time: 0.18s
Epoch 66/1000, LR 0.000283
Train loss: 1.0313;  Loss pred: 1.0313; Loss self: 0.0000; time: 0.37s
Val loss: 0.6735 score: 0.8450 time: 0.21s
Test loss: 0.6700 score: 0.8915 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 1.0269;  Loss pred: 1.0269; Loss self: 0.0000; time: 0.28s
Val loss: 0.6721 score: 0.8450 time: 0.18s
Test loss: 0.6684 score: 0.8915 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 1.0233;  Loss pred: 1.0233; Loss self: 0.0000; time: 0.28s
Val loss: 0.6707 score: 0.8450 time: 0.17s
Test loss: 0.6667 score: 0.8915 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 1.0199;  Loss pred: 1.0199; Loss self: 0.0000; time: 0.28s
Val loss: 0.6691 score: 0.8450 time: 0.19s
Test loss: 0.6649 score: 0.8915 time: 0.18s
Epoch 70/1000, LR 0.000283
Train loss: 1.0152;  Loss pred: 1.0152; Loss self: 0.0000; time: 0.29s
Val loss: 0.6675 score: 0.8527 time: 0.18s
Test loss: 0.6630 score: 0.8915 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 1.0107;  Loss pred: 1.0107; Loss self: 0.0000; time: 0.30s
Val loss: 0.6657 score: 0.8682 time: 0.24s
Test loss: 0.6609 score: 0.8992 time: 0.25s
Epoch 72/1000, LR 0.000282
Train loss: 1.0086;  Loss pred: 1.0086; Loss self: 0.0000; time: 0.40s
Val loss: 0.6639 score: 0.8682 time: 0.26s
Test loss: 0.6588 score: 0.8992 time: 0.21s
Epoch 73/1000, LR 0.000282
Train loss: 1.0037;  Loss pred: 1.0037; Loss self: 0.0000; time: 0.29s
Val loss: 0.6620 score: 0.8682 time: 0.18s
Test loss: 0.6565 score: 0.8992 time: 0.18s
Epoch 74/1000, LR 0.000282
Train loss: 0.9993;  Loss pred: 0.9993; Loss self: 0.0000; time: 0.29s
Val loss: 0.6599 score: 0.8682 time: 0.18s
Test loss: 0.6542 score: 0.8992 time: 0.18s
Epoch 75/1000, LR 0.000282
Train loss: 0.9974;  Loss pred: 0.9974; Loss self: 0.0000; time: 0.29s
Val loss: 0.6578 score: 0.8682 time: 0.19s
Test loss: 0.6517 score: 0.8992 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9917;  Loss pred: 0.9917; Loss self: 0.0000; time: 0.29s
Val loss: 0.6556 score: 0.8605 time: 0.18s
Test loss: 0.6491 score: 0.8992 time: 0.18s
Epoch 77/1000, LR 0.000282
Train loss: 0.9891;  Loss pred: 0.9891; Loss self: 0.0000; time: 0.29s
Val loss: 0.6532 score: 0.8682 time: 0.18s
Test loss: 0.6464 score: 0.8992 time: 0.19s
Epoch 78/1000, LR 0.000282
Train loss: 0.9865;  Loss pred: 0.9865; Loss self: 0.0000; time: 0.30s
Val loss: 0.6508 score: 0.8682 time: 0.23s
Test loss: 0.6435 score: 0.8992 time: 0.23s
Epoch 79/1000, LR 0.000282
Train loss: 0.9831;  Loss pred: 0.9831; Loss self: 0.0000; time: 0.29s
Val loss: 0.6482 score: 0.8682 time: 0.19s
Test loss: 0.6406 score: 0.8992 time: 0.18s
Epoch 80/1000, LR 0.000282
Train loss: 0.9789;  Loss pred: 0.9789; Loss self: 0.0000; time: 0.29s
Val loss: 0.6455 score: 0.8760 time: 0.19s
Test loss: 0.6374 score: 0.8992 time: 0.18s
Epoch 81/1000, LR 0.000281
Train loss: 0.9741;  Loss pred: 0.9741; Loss self: 0.0000; time: 0.29s
Val loss: 0.6428 score: 0.8760 time: 0.18s
Test loss: 0.6342 score: 0.8992 time: 0.17s
Epoch 82/1000, LR 0.000281
Train loss: 0.9718;  Loss pred: 0.9718; Loss self: 0.0000; time: 0.28s
Val loss: 0.6399 score: 0.8760 time: 0.17s
Test loss: 0.6309 score: 0.8992 time: 0.17s
Epoch 83/1000, LR 0.000281
Train loss: 0.9689;  Loss pred: 0.9689; Loss self: 0.0000; time: 0.27s
Val loss: 0.6368 score: 0.8760 time: 0.18s
Test loss: 0.6273 score: 0.8992 time: 0.18s
Epoch 84/1000, LR 0.000281
Train loss: 0.9643;  Loss pred: 0.9643; Loss self: 0.0000; time: 0.27s
Val loss: 0.6337 score: 0.8760 time: 0.17s
Test loss: 0.6237 score: 0.8992 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.9609;  Loss pred: 0.9609; Loss self: 0.0000; time: 0.30s
Val loss: 0.6304 score: 0.8760 time: 0.19s
Test loss: 0.6199 score: 0.8992 time: 0.27s
Epoch 86/1000, LR 0.000281
Train loss: 0.9572;  Loss pred: 0.9572; Loss self: 0.0000; time: 0.29s
Val loss: 0.6270 score: 0.8760 time: 0.18s
Test loss: 0.6159 score: 0.8992 time: 0.18s
Epoch 87/1000, LR 0.000281
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.28s
Val loss: 0.6234 score: 0.8760 time: 0.18s
Test loss: 0.6118 score: 0.8992 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.9490;  Loss pred: 0.9490; Loss self: 0.0000; time: 0.29s
Val loss: 0.6197 score: 0.8760 time: 0.25s
Test loss: 0.6076 score: 0.8992 time: 0.26s
Epoch 89/1000, LR 0.000281
Train loss: 0.9455;  Loss pred: 0.9455; Loss self: 0.0000; time: 0.39s
Val loss: 0.6159 score: 0.8837 time: 0.26s
Test loss: 0.6032 score: 0.8992 time: 0.26s
Epoch 90/1000, LR 0.000281
Train loss: 0.9414;  Loss pred: 0.9414; Loss self: 0.0000; time: 0.39s
Val loss: 0.6120 score: 0.8760 time: 0.26s
Test loss: 0.5987 score: 0.8992 time: 0.26s
Epoch 91/1000, LR 0.000280
Train loss: 0.9374;  Loss pred: 0.9374; Loss self: 0.0000; time: 0.39s
Val loss: 0.6080 score: 0.8760 time: 0.25s
Test loss: 0.5940 score: 0.8992 time: 0.18s
Epoch 92/1000, LR 0.000280
Train loss: 0.9339;  Loss pred: 0.9339; Loss self: 0.0000; time: 0.29s
Val loss: 0.6038 score: 0.8760 time: 0.18s
Test loss: 0.5892 score: 0.8992 time: 0.18s
Epoch 93/1000, LR 0.000280
Train loss: 0.9305;  Loss pred: 0.9305; Loss self: 0.0000; time: 0.29s
Val loss: 0.5995 score: 0.8760 time: 0.18s
Test loss: 0.5843 score: 0.8992 time: 0.18s
Epoch 94/1000, LR 0.000280
Train loss: 0.9258;  Loss pred: 0.9258; Loss self: 0.0000; time: 0.29s
Val loss: 0.5951 score: 0.8760 time: 0.18s
Test loss: 0.5791 score: 0.8992 time: 0.18s
Epoch 95/1000, LR 0.000280
Train loss: 0.9209;  Loss pred: 0.9209; Loss self: 0.0000; time: 0.29s
Val loss: 0.5906 score: 0.8760 time: 0.18s
Test loss: 0.5738 score: 0.8992 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.9173;  Loss pred: 0.9173; Loss self: 0.0000; time: 0.27s
Val loss: 0.5859 score: 0.8682 time: 0.21s
Test loss: 0.5683 score: 0.8992 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.9139;  Loss pred: 0.9139; Loss self: 0.0000; time: 0.34s
Val loss: 0.5811 score: 0.8682 time: 0.17s
Test loss: 0.5627 score: 0.8992 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.9083;  Loss pred: 0.9083; Loss self: 0.0000; time: 0.28s
Val loss: 0.5761 score: 0.8760 time: 0.17s
Test loss: 0.5569 score: 0.9070 time: 0.26s
Epoch 99/1000, LR 0.000279
Train loss: 0.9025;  Loss pred: 0.9025; Loss self: 0.0000; time: 0.28s
Val loss: 0.5710 score: 0.8682 time: 0.18s
Test loss: 0.5510 score: 0.8992 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8996;  Loss pred: 0.8996; Loss self: 0.0000; time: 0.27s
Val loss: 0.5657 score: 0.8760 time: 0.17s
Test loss: 0.5449 score: 0.9070 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.8945;  Loss pred: 0.8945; Loss self: 0.0000; time: 0.29s
Val loss: 0.5604 score: 0.8760 time: 0.19s
Test loss: 0.5388 score: 0.9070 time: 0.18s
Epoch 102/1000, LR 0.000279
Train loss: 0.8913;  Loss pred: 0.8913; Loss self: 0.0000; time: 0.27s
Val loss: 0.5549 score: 0.8760 time: 0.18s
Test loss: 0.5324 score: 0.9147 time: 0.18s
Epoch 103/1000, LR 0.000279
Train loss: 0.8864;  Loss pred: 0.8864; Loss self: 0.0000; time: 0.31s
Val loss: 0.5494 score: 0.8760 time: 0.22s
Test loss: 0.5262 score: 0.9070 time: 0.27s
Epoch 104/1000, LR 0.000279
Train loss: 0.8812;  Loss pred: 0.8812; Loss self: 0.0000; time: 0.28s
Val loss: 0.5438 score: 0.8760 time: 0.18s
Test loss: 0.5198 score: 0.9070 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.8754;  Loss pred: 0.8754; Loss self: 0.0000; time: 0.28s
Val loss: 0.5382 score: 0.8760 time: 0.17s
Test loss: 0.5133 score: 0.9070 time: 0.16s
Epoch 106/1000, LR 0.000279
Train loss: 0.8715;  Loss pred: 0.8715; Loss self: 0.0000; time: 0.29s
Val loss: 0.5325 score: 0.8760 time: 0.19s
Test loss: 0.5066 score: 0.9147 time: 0.18s
Epoch 107/1000, LR 0.000278
Train loss: 0.8663;  Loss pred: 0.8663; Loss self: 0.0000; time: 0.28s
Val loss: 0.5266 score: 0.8760 time: 0.19s
Test loss: 0.4999 score: 0.9147 time: 0.24s
Epoch 108/1000, LR 0.000278
Train loss: 0.8610;  Loss pred: 0.8610; Loss self: 0.0000; time: 0.30s
Val loss: 0.5207 score: 0.8760 time: 0.17s
Test loss: 0.4932 score: 0.9147 time: 0.16s
Epoch 109/1000, LR 0.000278
Train loss: 0.8559;  Loss pred: 0.8559; Loss self: 0.0000; time: 0.27s
Val loss: 0.5147 score: 0.8760 time: 0.17s
Test loss: 0.4861 score: 0.9147 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.8507;  Loss pred: 0.8507; Loss self: 0.0000; time: 0.28s
Val loss: 0.5086 score: 0.8760 time: 0.17s
Test loss: 0.4791 score: 0.9147 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.8454;  Loss pred: 0.8454; Loss self: 0.0000; time: 0.29s
Val loss: 0.5025 score: 0.8760 time: 0.17s
Test loss: 0.4722 score: 0.9147 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.8403;  Loss pred: 0.8403; Loss self: 0.0000; time: 0.28s
Val loss: 0.4965 score: 0.8837 time: 0.20s
Test loss: 0.4652 score: 0.9147 time: 0.21s
Epoch 113/1000, LR 0.000278
Train loss: 0.8360;  Loss pred: 0.8360; Loss self: 0.0000; time: 0.38s
Val loss: 0.4904 score: 0.8760 time: 0.26s
Test loss: 0.4583 score: 0.9147 time: 0.22s
Epoch 114/1000, LR 0.000277
Train loss: 0.8308;  Loss pred: 0.8308; Loss self: 0.0000; time: 0.35s
Val loss: 0.4844 score: 0.8760 time: 0.18s
Test loss: 0.4514 score: 0.9147 time: 0.18s
Epoch 115/1000, LR 0.000277
Train loss: 0.8262;  Loss pred: 0.8262; Loss self: 0.0000; time: 0.28s
Val loss: 0.4785 score: 0.8837 time: 0.18s
Test loss: 0.4443 score: 0.9147 time: 0.18s
Epoch 116/1000, LR 0.000277
Train loss: 0.8212;  Loss pred: 0.8212; Loss self: 0.0000; time: 0.28s
Val loss: 0.4726 score: 0.8837 time: 0.18s
Test loss: 0.4374 score: 0.9147 time: 0.18s
Epoch 117/1000, LR 0.000277
Train loss: 0.8159;  Loss pred: 0.8159; Loss self: 0.0000; time: 0.27s
Val loss: 0.4667 score: 0.8837 time: 0.17s
Test loss: 0.4306 score: 0.9147 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.8100;  Loss pred: 0.8100; Loss self: 0.0000; time: 0.28s
Val loss: 0.4609 score: 0.8837 time: 0.18s
Test loss: 0.4240 score: 0.9147 time: 0.18s
Epoch 119/1000, LR 0.000277
Train loss: 0.8055;  Loss pred: 0.8055; Loss self: 0.0000; time: 0.29s
Val loss: 0.4551 score: 0.8837 time: 0.18s
Test loss: 0.4173 score: 0.9147 time: 0.18s
Epoch 120/1000, LR 0.000277
Train loss: 0.8000;  Loss pred: 0.8000; Loss self: 0.0000; time: 0.34s
Val loss: 0.4494 score: 0.8837 time: 0.18s
Test loss: 0.4108 score: 0.9147 time: 0.17s
Epoch 121/1000, LR 0.000276
Train loss: 0.7983;  Loss pred: 0.7983; Loss self: 0.0000; time: 0.28s
Val loss: 0.4437 score: 0.8837 time: 0.18s
Test loss: 0.4040 score: 0.9147 time: 0.17s
Epoch 122/1000, LR 0.000276
Train loss: 0.7932;  Loss pred: 0.7932; Loss self: 0.0000; time: 0.28s
Val loss: 0.4383 score: 0.8837 time: 0.18s
Test loss: 0.3981 score: 0.9147 time: 0.18s
Epoch 123/1000, LR 0.000276
Train loss: 0.7866;  Loss pred: 0.7866; Loss self: 0.0000; time: 0.28s
Val loss: 0.4329 score: 0.8837 time: 0.17s
Test loss: 0.3917 score: 0.9147 time: 0.18s
Epoch 124/1000, LR 0.000276
Train loss: 0.7811;  Loss pred: 0.7811; Loss self: 0.0000; time: 0.29s
Val loss: 0.4275 score: 0.8837 time: 0.18s
Test loss: 0.3854 score: 0.9070 time: 0.18s
Epoch 125/1000, LR 0.000276
Train loss: 0.7760;  Loss pred: 0.7760; Loss self: 0.0000; time: 0.28s
Val loss: 0.4222 score: 0.8837 time: 0.18s
Test loss: 0.3792 score: 0.9147 time: 0.20s
Epoch 126/1000, LR 0.000276
Train loss: 0.7724;  Loss pred: 0.7724; Loss self: 0.0000; time: 0.29s
Val loss: 0.4170 score: 0.8837 time: 0.26s
Test loss: 0.3733 score: 0.9147 time: 0.18s
Epoch 127/1000, LR 0.000275
Train loss: 0.7686;  Loss pred: 0.7686; Loss self: 0.0000; time: 0.29s
Val loss: 0.4118 score: 0.8837 time: 0.18s
Test loss: 0.3672 score: 0.9147 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7637;  Loss pred: 0.7637; Loss self: 0.0000; time: 0.29s
Val loss: 0.4067 score: 0.8837 time: 0.18s
Test loss: 0.3613 score: 0.9147 time: 0.19s
Epoch 129/1000, LR 0.000275
Train loss: 0.7615;  Loss pred: 0.7615; Loss self: 0.0000; time: 0.38s
Val loss: 0.4017 score: 0.8915 time: 0.25s
Test loss: 0.3555 score: 0.9147 time: 0.24s
Epoch 130/1000, LR 0.000275
Train loss: 0.7553;  Loss pred: 0.7553; Loss self: 0.0000; time: 0.30s
Val loss: 0.3969 score: 0.8837 time: 0.18s
Test loss: 0.3501 score: 0.9147 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 0.27s
Val loss: 0.3921 score: 0.8915 time: 0.16s
Test loss: 0.3445 score: 0.9147 time: 0.17s
Epoch 132/1000, LR 0.000275
Train loss: 0.7452;  Loss pred: 0.7452; Loss self: 0.0000; time: 0.28s
Val loss: 0.3875 score: 0.8915 time: 0.17s
Test loss: 0.3393 score: 0.9147 time: 0.17s
Epoch 133/1000, LR 0.000274
Train loss: 0.7429;  Loss pred: 0.7429; Loss self: 0.0000; time: 0.35s
Val loss: 0.3830 score: 0.8915 time: 0.17s
Test loss: 0.3342 score: 0.9147 time: 0.17s
Epoch 134/1000, LR 0.000274
Train loss: 0.7371;  Loss pred: 0.7371; Loss self: 0.0000; time: 0.28s
Val loss: 0.3785 score: 0.8915 time: 0.17s
Test loss: 0.3290 score: 0.9147 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.7352;  Loss pred: 0.7352; Loss self: 0.0000; time: 0.28s
Val loss: 0.3742 score: 0.8915 time: 0.17s
Test loss: 0.3241 score: 0.9225 time: 0.17s
Epoch 136/1000, LR 0.000274
Train loss: 0.7288;  Loss pred: 0.7288; Loss self: 0.0000; time: 0.28s
Val loss: 0.3699 score: 0.8915 time: 0.17s
Test loss: 0.3192 score: 0.9225 time: 0.17s
Epoch 137/1000, LR 0.000274
Train loss: 0.7266;  Loss pred: 0.7266; Loss self: 0.0000; time: 0.30s
Val loss: 0.3658 score: 0.8915 time: 0.25s
Test loss: 0.3145 score: 0.9225 time: 0.25s
Epoch 138/1000, LR 0.000274
Train loss: 0.7198;  Loss pred: 0.7198; Loss self: 0.0000; time: 0.38s
Val loss: 0.3615 score: 0.8915 time: 0.26s
Test loss: 0.3097 score: 0.9225 time: 0.25s
Epoch 139/1000, LR 0.000273
Train loss: 0.7171;  Loss pred: 0.7171; Loss self: 0.0000; time: 0.38s
Val loss: 0.3577 score: 0.8915 time: 0.21s
Test loss: 0.3053 score: 0.9225 time: 0.18s
Epoch 140/1000, LR 0.000273
Train loss: 0.7139;  Loss pred: 0.7139; Loss self: 0.0000; time: 0.28s
Val loss: 0.3541 score: 0.8915 time: 0.17s
Test loss: 0.3012 score: 0.9225 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.7086;  Loss pred: 0.7086; Loss self: 0.0000; time: 0.28s
Val loss: 0.3501 score: 0.8915 time: 0.17s
Test loss: 0.2967 score: 0.9225 time: 0.17s
Epoch 142/1000, LR 0.000273
Train loss: 0.7067;  Loss pred: 0.7067; Loss self: 0.0000; time: 0.28s
Val loss: 0.3461 score: 0.8915 time: 0.25s
Test loss: 0.2921 score: 0.9225 time: 0.24s
Epoch 143/1000, LR 0.000273
Train loss: 0.7033;  Loss pred: 0.7033; Loss self: 0.0000; time: 0.39s
Val loss: 0.3423 score: 0.8992 time: 0.26s
Test loss: 0.2878 score: 0.9225 time: 0.25s
Epoch 144/1000, LR 0.000272
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.39s
Val loss: 0.3388 score: 0.8992 time: 0.25s
Test loss: 0.2839 score: 0.9225 time: 0.26s
Epoch 145/1000, LR 0.000272
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.39s
Val loss: 0.3356 score: 0.8992 time: 0.26s
Test loss: 0.2802 score: 0.9225 time: 0.26s
Epoch 146/1000, LR 0.000272
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.39s
Val loss: 0.3324 score: 0.8992 time: 0.24s
Test loss: 0.2766 score: 0.9225 time: 0.19s
Epoch 147/1000, LR 0.000272
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.28s
Val loss: 0.3295 score: 0.8992 time: 0.18s
Test loss: 0.2732 score: 0.9225 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.28s
Val loss: 0.3262 score: 0.8992 time: 0.18s
Test loss: 0.2696 score: 0.9225 time: 0.18s
Epoch 149/1000, LR 0.000272
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.29s
Val loss: 0.3230 score: 0.8992 time: 0.19s
Test loss: 0.2661 score: 0.9225 time: 0.18s
Epoch 150/1000, LR 0.000271
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.29s
Val loss: 0.3204 score: 0.8992 time: 0.18s
Test loss: 0.2631 score: 0.9225 time: 0.18s
Epoch 151/1000, LR 0.000271
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.29s
Val loss: 0.3174 score: 0.8992 time: 0.17s
Test loss: 0.2598 score: 0.9225 time: 0.18s
Epoch 152/1000, LR 0.000271
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.28s
Val loss: 0.3153 score: 0.8992 time: 0.18s
Test loss: 0.2573 score: 0.9225 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.29s
Val loss: 0.3127 score: 0.8992 time: 0.20s
Test loss: 0.2543 score: 0.9225 time: 0.18s
Epoch 154/1000, LR 0.000271
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.31s
Val loss: 0.3112 score: 0.8992 time: 0.24s
Test loss: 0.2523 score: 0.9225 time: 0.18s
Epoch 155/1000, LR 0.000270
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.28s
Val loss: 0.3082 score: 0.8992 time: 0.18s
Test loss: 0.2492 score: 0.9225 time: 0.18s
Epoch 156/1000, LR 0.000270
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.29s
Val loss: 0.3056 score: 0.8992 time: 0.18s
Test loss: 0.2463 score: 0.9225 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.28s
Val loss: 0.3027 score: 0.8992 time: 0.18s
Test loss: 0.2433 score: 0.9225 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.29s
Val loss: 0.3004 score: 0.8992 time: 0.19s
Test loss: 0.2408 score: 0.9225 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.27s
Val loss: 0.2982 score: 0.8992 time: 0.17s
Test loss: 0.2384 score: 0.9225 time: 0.17s
Epoch 160/1000, LR 0.000269
Train loss: 0.6495;  Loss pred: 0.6495; Loss self: 0.0000; time: 0.27s
Val loss: 0.2971 score: 0.8915 time: 0.20s
Test loss: 0.2368 score: 0.9225 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.34s
Val loss: 0.2950 score: 0.8915 time: 0.20s
Test loss: 0.2345 score: 0.9225 time: 0.17s
Epoch 162/1000, LR 0.000269
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.29s
Val loss: 0.2937 score: 0.8915 time: 0.18s
Test loss: 0.2329 score: 0.9225 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.28s
Val loss: 0.2920 score: 0.8915 time: 0.17s
Test loss: 0.2309 score: 0.9225 time: 0.17s
Epoch 164/1000, LR 0.000269
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.30s
Val loss: 0.2900 score: 0.8915 time: 0.18s
Test loss: 0.2287 score: 0.9225 time: 0.24s
Epoch 165/1000, LR 0.000268
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.39s
Val loss: 0.2884 score: 0.8915 time: 0.25s
Test loss: 0.2269 score: 0.9225 time: 0.25s
Epoch 166/1000, LR 0.000268
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.39s
Val loss: 0.2859 score: 0.8915 time: 0.26s
Test loss: 0.2244 score: 0.9225 time: 0.25s
Epoch 167/1000, LR 0.000268
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.40s
Val loss: 0.2840 score: 0.8992 time: 0.17s
Test loss: 0.2225 score: 0.9225 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.27s
Val loss: 0.2829 score: 0.8992 time: 0.16s
Test loss: 0.2210 score: 0.9225 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.28s
Val loss: 0.2818 score: 0.8992 time: 0.18s
Test loss: 0.2196 score: 0.9225 time: 0.17s
Epoch 170/1000, LR 0.000267
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.29s
Val loss: 0.2808 score: 0.8992 time: 0.18s
Test loss: 0.2183 score: 0.9225 time: 0.18s
Epoch 171/1000, LR 0.000267
Train loss: 0.6219;  Loss pred: 0.6219; Loss self: 0.0000; time: 0.29s
Val loss: 0.2787 score: 0.8992 time: 0.18s
Test loss: 0.2163 score: 0.9225 time: 0.20s
Epoch 172/1000, LR 0.000267
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.34s
Val loss: 0.2780 score: 0.8992 time: 0.26s
Test loss: 0.2152 score: 0.9225 time: 0.25s
Epoch 173/1000, LR 0.000267
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.36s
Val loss: 0.2769 score: 0.8992 time: 0.18s
Test loss: 0.2138 score: 0.9225 time: 0.17s
Epoch 174/1000, LR 0.000266
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.27s
Val loss: 0.2766 score: 0.8992 time: 0.17s
Test loss: 0.2131 score: 0.9225 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.28s
Val loss: 0.2754 score: 0.8992 time: 0.18s
Test loss: 0.2117 score: 0.9225 time: 0.17s
Epoch 176/1000, LR 0.000266
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.28s
Val loss: 0.2739 score: 0.9070 time: 0.17s
Test loss: 0.2102 score: 0.9225 time: 0.17s
Epoch 177/1000, LR 0.000266
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.28s
Val loss: 0.2723 score: 0.9070 time: 0.17s
Test loss: 0.2086 score: 0.9225 time: 0.17s
Epoch 178/1000, LR 0.000265
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.30s
Val loss: 0.2720 score: 0.9070 time: 0.23s
Test loss: 0.2079 score: 0.9225 time: 0.25s
Epoch 179/1000, LR 0.000265
Train loss: 0.6065;  Loss pred: 0.6065; Loss self: 0.0000; time: 0.40s
Val loss: 0.2723 score: 0.9070 time: 0.19s
Test loss: 0.2076 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.28s
Val loss: 0.2714 score: 0.9070 time: 0.18s
Test loss: 0.2065 score: 0.9225 time: 0.17s
Epoch 181/1000, LR 0.000265
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.27s
Val loss: 0.2700 score: 0.9070 time: 0.17s
Test loss: 0.2051 score: 0.9225 time: 0.17s
Epoch 182/1000, LR 0.000265
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.27s
Val loss: 0.2687 score: 0.8992 time: 0.17s
Test loss: 0.2038 score: 0.9225 time: 0.17s
Epoch 183/1000, LR 0.000264
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.27s
Val loss: 0.2681 score: 0.8992 time: 0.17s
Test loss: 0.2030 score: 0.9225 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.27s
Val loss: 0.2666 score: 0.8992 time: 0.20s
Test loss: 0.2015 score: 0.9225 time: 0.17s
Epoch 185/1000, LR 0.000264
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.38s
Val loss: 0.2658 score: 0.8992 time: 0.25s
Test loss: 0.2006 score: 0.9225 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.27s
Val loss: 0.2653 score: 0.8992 time: 0.17s
Test loss: 0.1999 score: 0.9225 time: 0.17s
Epoch 187/1000, LR 0.000263
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.27s
Val loss: 0.2656 score: 0.8992 time: 0.16s
Test loss: 0.1997 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.37s
Val loss: 0.2655 score: 0.8992 time: 0.18s
Test loss: 0.1992 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.26s
Val loss: 0.2650 score: 0.8992 time: 0.17s
Test loss: 0.1985 score: 0.9225 time: 0.18s
Epoch 190/1000, LR 0.000263
Train loss: 0.5885;  Loss pred: 0.5885; Loss self: 0.0000; time: 0.27s
Val loss: 0.2645 score: 0.8992 time: 0.26s
Test loss: 0.1979 score: 0.9225 time: 0.17s
Epoch 191/1000, LR 0.000262
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.27s
Val loss: 0.2633 score: 0.8992 time: 0.17s
Test loss: 0.1967 score: 0.9225 time: 0.22s
Epoch 192/1000, LR 0.000262
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.27s
Val loss: 0.2626 score: 0.8992 time: 0.17s
Test loss: 0.1960 score: 0.9225 time: 0.17s
Epoch 193/1000, LR 0.000262
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.31s
Val loss: 0.2633 score: 0.8992 time: 0.17s
Test loss: 0.1960 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 194/1000, LR 0.000262
Train loss: 0.5845;  Loss pred: 0.5845; Loss self: 0.0000; time: 0.26s
Val loss: 0.2636 score: 0.8992 time: 0.17s
Test loss: 0.1959 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.28s
Val loss: 0.2637 score: 0.8992 time: 0.17s
Test loss: 0.1956 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.29s
Val loss: 0.2627 score: 0.8992 time: 0.25s
Test loss: 0.1947 score: 0.9225 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 197/1000, LR 0.000261
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.38s
Val loss: 0.2622 score: 0.8992 time: 0.23s
Test loss: 0.1940 score: 0.9225 time: 0.23s
Epoch 198/1000, LR 0.000261
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.29s
Val loss: 0.2617 score: 0.8992 time: 0.17s
Test loss: 0.1934 score: 0.9225 time: 0.16s
Epoch 199/1000, LR 0.000260
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.28s
Val loss: 0.2612 score: 0.8992 time: 0.17s
Test loss: 0.1928 score: 0.9225 time: 0.16s
Epoch 200/1000, LR 0.000260
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.27s
Val loss: 0.2625 score: 0.8915 time: 0.17s
Test loss: 0.1933 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.28s
Val loss: 0.2626 score: 0.8915 time: 0.18s
Test loss: 0.1931 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 0.27s
Val loss: 0.2623 score: 0.8915 time: 0.17s
Test loss: 0.1927 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.27s
Val loss: 0.2618 score: 0.8915 time: 0.17s
Test loss: 0.1921 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.27s
Val loss: 0.2606 score: 0.8915 time: 0.21s
Test loss: 0.1911 score: 0.9225 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5722;  Loss pred: 0.5722; Loss self: 0.0000; time: 0.29s
Val loss: 0.2598 score: 0.8915 time: 0.23s
Test loss: 0.1904 score: 0.9225 time: 0.17s
Epoch 206/1000, LR 0.000259
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.28s
Val loss: 0.2616 score: 0.8915 time: 0.17s
Test loss: 0.1913 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.28s
Val loss: 0.2608 score: 0.8915 time: 0.18s
Test loss: 0.1906 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5693;  Loss pred: 0.5693; Loss self: 0.0000; time: 0.37s
Val loss: 0.2618 score: 0.8915 time: 0.26s
Test loss: 0.1910 score: 0.9225 time: 0.26s
     INFO: Early stopping counter 3 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.39s
Val loss: 0.2618 score: 0.8915 time: 0.27s
Test loss: 0.1908 score: 0.9225 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.38s
Val loss: 0.2618 score: 0.8915 time: 0.23s
Test loss: 0.1905 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.29s
Val loss: 0.2616 score: 0.8915 time: 0.17s
Test loss: 0.1902 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.29s
Val loss: 0.2622 score: 0.8915 time: 0.18s
Test loss: 0.1903 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.29s
Val loss: 0.2611 score: 0.8915 time: 0.18s
Test loss: 0.1894 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.28s
Val loss: 0.2610 score: 0.8915 time: 0.19s
Test loss: 0.1892 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.29s
Val loss: 0.2608 score: 0.8915 time: 0.18s
Test loss: 0.1889 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.27s
Val loss: 0.2615 score: 0.8915 time: 0.21s
Test loss: 0.1891 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.31s
Val loss: 0.2615 score: 0.8915 time: 0.22s
Test loss: 0.1889 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.29s
Val loss: 0.2632 score: 0.8915 time: 0.17s
Test loss: 0.1897 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5578;  Loss pred: 0.5578; Loss self: 0.0000; time: 0.28s
Val loss: 0.2664 score: 0.8915 time: 0.17s
Test loss: 0.1916 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 0.29s
Val loss: 0.2667 score: 0.8915 time: 0.17s
Test loss: 0.1916 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.27s
Val loss: 0.2666 score: 0.8915 time: 0.18s
Test loss: 0.1913 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.34s
Val loss: 0.2672 score: 0.8915 time: 0.17s
Test loss: 0.1914 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.27s
Val loss: 0.2654 score: 0.8915 time: 0.19s
Test loss: 0.1901 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.29s
Val loss: 0.2625 score: 0.8915 time: 0.20s
Test loss: 0.1881 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.27s
Val loss: 0.2610 score: 0.8915 time: 0.17s
Test loss: 0.1870 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 204,   Train_Loss: 0.5722,   Val_Loss: 0.2598,   Val_Precision: 0.9630,   Val_Recall: 0.8125,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.2598,   Test_Precision: 0.9825,   Test_Recall: 0.8615,   Test_accuracy: 0.9180,   Test_Score: 0.9225,   Test_loss: 0.1904


[0.18550081993453205, 0.17881224979646504, 0.17573861312121153, 0.2577355320099741, 0.2022567328531295, 0.17211303091607988, 0.166310572065413, 0.17223499296233058, 0.1801803819835186, 0.18138296413235366, 0.18385094893164933, 0.17019705404527485, 0.2469776349607855, 0.22974095190875232, 0.15848247497342527, 0.17440710798837245, 0.17386351386085153, 0.17969722906127572, 0.1796211798209697, 0.17906930297613144, 0.17334501794539392, 0.1796172980684787, 0.1747308778576553, 0.16008602804504335, 0.1703171960543841, 0.1642515070270747, 0.1655973067972809, 0.16576109500601888, 0.16391611401923, 0.17513206996954978, 0.198749439092353, 0.166643472854048, 0.16440593986772, 0.2519094788003713, 0.2454936089925468, 0.2414765718858689, 0.1716188699938357, 0.18237916403450072, 0.24969004304148257, 0.2515728371217847, 0.2983441250398755, 0.1735198860988021, 0.1800017210189253, 0.17322517721913755, 0.17736758897081017, 0.2539075929671526, 0.256005696952343, 0.21315478906035423, 0.18524248898029327, 0.18239466985687613, 0.262146086897701, 0.26021554390899837, 0.18385815899819136, 0.17073560901917517, 0.18151607899926603, 0.18426130688749254, 0.1716756538953632, 0.1702865760307759, 0.20834450190886855, 0.19112535985186696, 0.17619286989793181, 0.17856394685804844, 0.1769043488893658, 0.17274278309196234, 0.18005245691165328, 0.1758513138629496, 0.17481690202839673, 0.17525833495892584, 0.18266787007451057, 0.18278150400146842, 0.2582981828600168, 0.2172350890468806, 0.18220732104964554, 0.1808900791220367, 0.1837259188760072, 0.18281870894134045, 0.19448036095127463, 0.23501229286193848, 0.18756187101826072, 0.18166669085621834, 0.17910037911497056, 0.1715611049439758, 0.1838181670755148, 0.1751820920035243, 0.271491696825251, 0.18272928497754037, 0.18109903600998223, 0.26629261393100023, 0.26028010388836265, 0.2623029858805239, 0.1870569819584489, 0.18665865994989872, 0.1815332469996065, 0.18404843588359654, 0.18266013101674616, 0.17633383790962398, 0.17049042391590774, 0.2610649422276765, 0.17873506899923086, 0.1685474650003016, 0.18420575582422316, 0.18685633200220764, 0.27880440605804324, 0.18067441089078784, 0.16723427292890847, 0.1812123078852892, 0.24023600085638463, 0.16767417290247977, 0.1707209530286491, 0.17213853588327765, 0.17620230000466108, 0.2123954410199076, 0.22854979988187551, 0.18016470689326525, 0.18492684420198202, 0.18200580193661153, 0.1706594240386039, 0.18210552306845784, 0.18528857314959168, 0.1781629470642656, 0.17862609191797674, 0.18180767120793462, 0.18767557595856488, 0.1813076580874622, 0.2069702590815723, 0.18682906194590032, 0.17848302493803203, 0.1987478209193796, 0.24616809701547027, 0.1744152179453522, 0.1703364858403802, 0.176741466159001, 0.17538737412542105, 0.17286431905813515, 0.17562659294344485, 0.17685328307561576, 0.2559926831163466, 0.2536992128007114, 0.18110278784297407, 0.17706706700846553, 0.17453126003965735, 0.24760881578549743, 0.24962356011383235, 0.2624865039251745, 0.26454356289468706, 0.1914407960139215, 0.17118542804382741, 0.18134447210468352, 0.18281584000214934, 0.1835720429662615, 0.18879491998814046, 0.18703188421204686, 0.18715200200676918, 0.18853451195172966, 0.18700764421373606, 0.18035285198129714, 0.1802933090366423, 0.17964144004508853, 0.17333353590220213, 0.18540627998299897, 0.17697920300997794, 0.1782700449693948, 0.17348658293485641, 0.24418295198120177, 0.2518044509924948, 0.25598523393273354, 0.16908934502862394, 0.17368543916381896, 0.17799440305680037, 0.18218423798680305, 0.20566654298454523, 0.25751592009328306, 0.17747252387925982, 0.17573684989474714, 0.17484365799464285, 0.1744628290180117, 0.17125962884165347, 0.25380060193128884, 0.17809432791545987, 0.1718235390726477, 0.17202024697326124, 0.17807015497237444, 0.17170979408547282, 0.1765435580164194, 0.17278189095668495, 0.17237219284288585, 0.17188983992673457, 0.18410809198394418, 0.18117198208346963, 0.1722851269878447, 0.22295977198518813, 0.1733949889894575, 0.17004524497315288, 0.17876523197628558, 0.17158495704643428, 0.2559531629085541, 0.22993701579980552, 0.16811464610509574, 0.16746042692102492, 0.18059596722014248, 0.18346759700216353, 0.17166436091065407, 0.17095946590416133, 0.17611711099743843, 0.17317450512200594, 0.16605116985738277, 0.19472197513096035, 0.26222699298523366, 0.25810577697120607, 0.18595697521232069, 0.1793225659057498, 0.17529605003073812, 0.17913555004633963, 0.17679625586606562, 0.18948329705744982, 0.1835967020597309, 0.1707203600089997, 0.17111023585312068, 0.16761534404940903, 0.18101090611889958, 0.17756476090289652, 0.17675953591242433, 0.19757452001795173, 0.17322861216962337, 0.17119141900911927]
[0.0014379908522056748, 0.0013861414712904266, 0.0013623148303969887, 0.0019979498605424352, 0.0015678816500242598, 0.0013342095419851153, 0.001289229240817155, 0.001335154984204113, 0.0013967471471590589, 0.0014060694893980904, 0.0014252011545089094, 0.0013193570081029059, 0.0019145553097735311, 0.0017809376116957544, 0.0012285463176234517, 0.0013519930851811817, 0.0013477791772159033, 0.001393001775668804, 0.0013924122466741837, 0.0013881341315979182, 0.0013437598290340614, 0.0013923821555696023, 0.0013545029291291107, 0.0012409769615894834, 0.0013202883415068536, 0.0012732674963339124, 0.0012837000526921, 0.0012849697287288285, 0.0012706675505366667, 0.001357612945500386, 0.00154069332629731, 0.0012918098670856436, 0.0012744646501373644, 0.0019527866573672192, 0.0019030512325003627, 0.001871911409967976, 0.0013303788371615169, 0.0014137919692596955, 0.0019355817290037409, 0.0019501770319518192, 0.0023127451553478723, 0.001345115396114745, 0.0013953621784412815, 0.001342830831156105, 0.0013749425501613192, 0.001968275914474051, 0.001984540286452271, 0.0016523627058942188, 0.0014359882866689401, 0.0014139121694331483, 0.00203214020850931, 0.0020171747589844834, 0.0014252570464976075, 0.001323531852861823, 0.0014071013875912096, 0.0014283822239340507, 0.0013308190224446761, 0.001320050976982759, 0.0016150736582082833, 0.001481591936836178, 0.0013658362007591613, 0.001384216642310453, 0.0013713515417780294, 0.0013390913417981576, 0.0013957554799352968, 0.001363188479557749, 0.0013551697831658662, 0.001358591743867642, 0.0014160300005776013, 0.0014169108837323134, 0.00200231149503889, 0.0016839929383479116, 0.0014124598530980276, 0.0014022486753646256, 0.0014242319292713736, 0.001417199294118918, 0.0015075996972967027, 0.0018218007198599882, 0.001453967992389618, 0.0014082689213660337, 0.0013883750318989965, 0.0013299310460773317, 0.001424947031593138, 0.0013580007132056148, 0.002104586797094969, 0.0014165060850972121, 0.0014038684962014126, 0.0020642838289224824, 0.0020176752239407956, 0.002033356479693984, 0.001450054123708906, 0.0014469663562007653, 0.0014072344728651666, 0.0014267320611131514, 0.0014159700078817533, 0.0013669289760435967, 0.0013216311931465716, 0.002023759242075012, 0.0013855431705366733, 0.001306569496126369, 0.0014279515955366137, 0.001448498697691532, 0.0021612744655662266, 0.0014005768286107585, 0.0012963897126271975, 0.0014047465727541797, 0.0018622945802820514, 0.0012997997899417035, 0.0013234182405321634, 0.001334407254909129, 0.0013659093023617138, 0.0016464762869760278, 0.0017717038750532986, 0.0013966256348315136, 0.0014335414279223412, 0.0014108976894310972, 0.0013229412716170845, 0.0014116707214609134, 0.0014363455282914083, 0.0013811081167772528, 0.00138469838696106, 0.0014093617923095706, 0.0014548494260353867, 0.001405485721608234, 0.001604420613035444, 0.001448287301906204, 0.0013835893406048995, 0.001540680782320772, 0.001908279821825351, 0.0013520559530647456, 0.0013204378747316294, 0.001370088884953496, 0.0013595920474838842, 0.0013400334810708152, 0.001361446456925929, 0.0013709556827567113, 0.001984439404002687, 0.0019666605643466, 0.0014038975801780936, 0.0013726129225462443, 0.0013529555041833904, 0.001919448184383701, 0.0019350663574715687, 0.002034779100195151, 0.002050725293757264, 0.0014840371784024923, 0.0013270188220451738, 0.001405771101586694, 0.0014171770542802274, 0.001423039092761717, 0.0014635265115359726, 0.001449859567535247, 0.001450790713230769, 0.0014615078445870517, 0.0014496716605715972, 0.0013980841238860242, 0.001397622550671646, 0.0013925693026751048, 0.0013436708209473033, 0.001437257984364333, 0.0013719318062788987, 0.0013819383330960838, 0.0013448572320531504, 0.001892891100629471, 0.0019519724883139133, 0.0019843816583932834, 0.0013107701165009607, 0.0013463987532078988, 0.0013798015740837239, 0.0014122809146263802, 0.001594314286701901, 0.0019962474425835896, 0.0013757559990640296, 0.001362301161974784, 0.0013553771937569214, 0.0013524250311473774, 0.0013275940220283215, 0.0019674465265991383, 0.0013805761853911617, 0.0013319654191678116, 0.0013334902866144283, 0.0013803887982354608, 0.0013310836750811846, 0.0013685547133055768, 0.0013393945035401933, 0.0013362185491696578, 0.001332479379277012, 0.0014271945115034432, 0.0014044339696392995, 0.0013355436200608117, 0.0017283703254665746, 0.0013441472014686627, 0.0013181801935903323, 0.0013857769920642292, 0.0013301159460963898, 0.0019841330458027447, 0.0017824574868201977, 0.001303214310892215, 0.0012981428443490303, 0.0013999687381406395, 0.0014222294341252988, 0.001330731479927551, 0.0013252671775516383, 0.001365248922460763, 0.0013424380242015965, 0.0012872183709874633, 0.001509472675433801, 0.0020327673874824316, 0.0020008199765209774, 0.0014415269396303929, 0.0013900974101220915, 0.0013588841087654117, 0.0013886476747778265, 0.0013705136113648499, 0.0014688627678872078, 0.0014232302485250455, 0.0013234136434806178, 0.0013264359368458968, 0.0012993437523210003, 0.0014031853187511595, 0.0013764710147511358, 0.0013702289605614288, 0.0015315854264957498, 0.0013428574586792509, 0.0013270652636365836]
[695.4147159324007, 721.4270842564513, 734.0447139583679, 500.5130607874735, 637.8032423458283, 749.5074563116534, 775.6572441423743, 748.9767194301422, 715.9491981307923, 711.2024032525445, 701.6553395542093, 757.9449639926444, 522.3145003412244, 561.5019826819372, 813.9701252244517, 739.6487533558569, 741.961307093119, 717.8741746541442, 718.1781131188184, 720.3914789191685, 744.1806031058636, 718.193633838201, 738.2782115081571, 805.816732261627, 757.4103084623875, 785.3809218245775, 778.9981763285426, 778.2284497777716, 786.987910077384, 736.5869656107487, 649.0584355312697, 774.1077270574082, 784.6431832316557, 512.0887098584632, 525.4719278819044, 534.2133151574238, 751.6655948418347, 707.3176406028324, 516.640545328307, 512.7739603204935, 432.38659377910784, 743.4306401431562, 716.6598145272011, 744.6954424922266, 727.3031152339216, 508.0588512242264, 503.89503646090395, 605.1940027651641, 696.3845104333675, 707.2575097793451, 492.0920297785735, 495.74286786308744, 701.6278238773673, 755.5541620231788, 710.680842772731, 700.0927225527911, 751.4169719057885, 757.5465019432056, 619.1668069859869, 674.949677530927, 732.1522152101243, 722.4302680907367, 729.2076244020169, 746.7750472176011, 716.4578712930127, 733.5742745745797, 737.9149184273133, 736.0562910188146, 706.199727118845, 705.7606879028834, 499.4227933454367, 593.8267181696464, 707.9847245262539, 713.1402707440396, 702.1328334575342, 705.6170604584631, 663.3060498706079, 548.9074568358133, 687.7730494991745, 710.0916485680809, 720.2664820558005, 751.9186825132983, 701.7804717147745, 736.3766383004764, 475.15265294847103, 705.9623749737523, 712.3174305184567, 484.4295081854036, 495.61990360711434, 491.7976803312413, 689.6294308258158, 691.1010720564749, 710.613632114891, 700.9024520131618, 706.229647827053, 731.5669047373432, 756.640736981378, 494.1299237624109, 721.7386085578713, 765.3630388316382, 700.3038500224563, 690.3699682945501, 462.689961840646, 713.9915351819055, 771.3729831853192, 711.87217637369, 536.9719756412255, 769.3492549686058, 755.6190245631549, 749.3964052736646, 732.1130314223342, 607.3576691691277, 564.4284093299265, 716.0114887341576, 697.5731433512312, 708.7686141177398, 755.8914529725569, 708.3804918509023, 696.2113087019812, 724.0562761541438, 722.1789303840084, 709.5410173999857, 687.3563559942435, 711.4978008141876, 623.2779558398184, 690.4707364925605, 722.7578087316025, 649.0637200612518, 524.0321616163491, 739.6143611758597, 757.3245353956873, 729.8796530518108, 735.5147463907578, 746.2500110078624, 734.5129108183547, 729.4181807461526, 503.9206528468259, 508.4761540089346, 712.3026737271985, 728.5375094276145, 739.1226074382798, 520.9830659331299, 516.7781436222364, 491.45383884869483, 487.63235282861143, 673.8375659001082, 753.568814087219, 711.3533624864673, 705.6281337464159, 702.7213834718218, 683.2810968012454, 689.7219719700124, 689.2792949943124, 684.2248597595107, 689.8113739808541, 715.2645416074567, 715.5007620043315, 718.0971159417452, 744.2298994741797, 695.7693127321728, 728.8992028782449, 723.6212905098362, 743.5733520005926, 528.2924092503025, 512.3023024078511, 503.9353169640166, 762.9102825974192, 742.722018731392, 724.7418895460123, 708.0744274339717, 627.2289023192931, 500.9399028738526, 726.8730797324029, 734.0520788739589, 737.8019968213689, 739.4125197103272, 753.2423191181462, 508.2730262197094, 724.3352526153185, 750.7702419367481, 749.91172417077, 724.4335807986065, 751.2675714687948, 730.697859776919, 746.6060203747816, 748.380570395024, 750.4806570009274, 700.6753402846081, 712.0306270125535, 748.7587713192526, 578.5797090273749, 743.9661362292499, 758.6216246174176, 721.6168299276043, 751.8141579572739, 503.9984602420741, 561.0231982497057, 767.3335012070067, 770.3312500262344, 714.3016645701277, 703.1214345630677, 751.4664040670647, 754.5648280880597, 732.4671593203472, 744.9133456978332, 776.8689622048125, 662.4830089836599, 491.9402023851303, 499.7950898804991, 693.7088530973966, 719.3740472562787, 735.8979279760148, 720.125067116101, 729.6534610875791, 680.7987933674609, 702.6270001191605, 755.6216493053297, 753.8999602030373, 769.6192775881775, 712.6642408787486, 726.4955013824238, 729.8050389989322, 652.918200121549, 744.6806759248567, 753.5424424113702]
Elapsed: 0.19215771962785058~0.030284981991531998
Time per graph: 0.0014895947257972913~0.00023476730225993798
Speed: 685.1252635185853~88.12255875801394
Total Time: 0.1720
best val loss: 0.25977431148175123 test_score: 0.9225

Testing...
Test loss: 0.2102 score: 0.9225 time: 0.17s
test Score 0.9225
Epoch Time List: [0.8239756787661463, 0.6236044911202043, 0.6331821989733726, 0.7595500519964844, 0.8482453180477023, 0.6088090999983251, 0.6496180091053247, 0.6217460769694299, 0.6414959239773452, 0.6955828811042011, 0.6305475670378655, 0.6140511280391365, 0.7810982340015471, 0.9098405940458179, 0.6253328800667077, 0.7103549810126424, 0.6144525369163603, 0.6261607820633799, 0.7403728240169585, 0.8069042132701725, 0.7039818640332669, 0.6274044478777796, 0.6319562022108585, 0.6077718730084598, 0.6086278920993209, 0.6744760458823293, 0.5840120769571513, 0.5775033691897988, 0.5803266412112862, 0.6173808411695063, 0.6415360688697547, 0.6634452647995204, 0.5952710779383779, 0.8803626713342965, 0.8622529450803995, 0.8690097869839519, 0.7460525771602988, 0.6345903708133847, 0.7680564159527421, 0.8936272521968931, 0.9535178251098841, 0.6384823969565332, 0.6465099221095443, 0.6171660223044455, 0.6212789013516158, 0.8357093741651624, 0.8920101597905159, 0.8591716901864856, 0.648965593893081, 0.6517131519503891, 0.8611799110658467, 0.912932557053864, 0.7318892430048436, 0.6145636541768909, 0.6246371557936072, 0.6446521619800478, 0.6023623659275472, 0.6043679108843207, 0.6955953948199749, 0.6919526541605592, 0.6144061929080635, 0.6417774681467563, 0.6362417410127819, 0.6294366419315338, 0.6680072690360248, 0.7484603878110647, 0.6260786920320243, 0.6285013589076698, 0.6494267389643937, 0.6553798092063516, 0.7968141629826277, 0.8774844631552696, 0.646077929995954, 0.6490584048442543, 0.6537276052404195, 0.6500776971224695, 0.6638517118990421, 0.7627610019408166, 0.6585549777373672, 0.6570739890448749, 0.6481804309878498, 0.6201347578316927, 0.6345694190822542, 0.6158518770243973, 0.7596582188270986, 0.6522200659383088, 0.6398014891892672, 0.803307123016566, 0.9047670061700046, 0.9042480438947678, 0.8195602307096124, 0.6509504611603916, 0.6492633558809757, 0.6422780039720237, 0.6522263651713729, 0.6530720512382686, 0.6776470129843801, 0.7032519681379199, 0.6360432712826878, 0.601852540159598, 0.6623046428430825, 0.6343062610831112, 0.799636181909591, 0.6404277591500431, 0.6151040629483759, 0.652504411060363, 0.7059083867352456, 0.6308846545871347, 0.6026614818256348, 0.6165637541562319, 0.6278122852090746, 0.6860151079017669, 0.8645860392134637, 0.704924781806767, 0.6335008840542287, 0.6308942709583789, 0.6110741279553622, 0.6322687782812864, 0.651691421167925, 0.6932510212063789, 0.635333722922951, 0.6406801079865545, 0.6393647838849574, 0.6420045951381326, 0.6643908920232207, 0.732912341831252, 0.6456972660962492, 0.6640320960432291, 0.8765742559917271, 0.6433778791688383, 0.6036749398335814, 0.6230798892211169, 0.695938667980954, 0.6151980138383806, 0.6206345860846341, 0.6237334469333291, 0.8002478370908648, 0.8897736098151654, 0.7645444849040359, 0.6256802210118622, 0.6202262958977371, 0.7715411582030356, 0.8878010818734765, 0.9020431230310351, 0.9107847211416811, 0.8177629171404988, 0.6311936478596181, 0.6369975300040096, 0.6541366190649569, 0.6461082701571286, 0.6461186690721661, 0.6482937510591, 0.6731798411346972, 0.7248378631193191, 0.647709395037964, 0.6428006319329143, 0.6390416282229125, 0.646687283180654, 0.6046730659436435, 0.6568674987647682, 0.7146894389297813, 0.6473342534154654, 0.6135214022360742, 0.717304821126163, 0.888358797878027, 0.9052669282536954, 0.7358207150828093, 0.605680878739804, 0.6313637411221862, 0.6470294140744954, 0.6743892948143184, 0.8503534540068358, 0.7194087677635252, 0.6168382270261645, 0.6314596091397107, 0.6155268729198724, 0.6231117912102491, 0.7831663859542459, 0.7641757891979069, 0.6200609146617353, 0.6099010270554572, 0.6142331941518933, 0.6120095746591687, 0.6412484492175281, 0.7934351381845772, 0.603149127913639, 0.5974100418388844, 0.725382428150624, 0.6142530750948936, 0.6932932510972023, 0.6594680009875447, 0.6176397919189185, 0.6404625850263983, 0.6069277813658118, 0.6120993439108133, 0.7925973229575902, 0.8405770200770348, 0.6251732830423862, 0.6072609222028404, 0.6201574197039008, 0.6400704202242196, 0.6067351647652686, 0.6108383378013968, 0.6555246589705348, 0.681946184951812, 0.6144063179381192, 0.6519754009786993, 0.8943191240541637, 0.9133193730376661, 0.7977412638720125, 0.6352967619895935, 0.636064640013501, 0.6356404989492148, 0.6431184522807598, 0.6530271170195192, 0.6593148258980364, 0.6987321241758764, 0.626027753110975, 0.610611867858097, 0.6353965988382697, 0.6271204901859164, 0.6825660460162908, 0.6571799630764872, 0.6587495189160109, 0.6076941550709307]
Total Epoch List: [225]
Total Time List: [0.17203435278497636]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2b4bdc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2575;  Loss pred: 2.2575; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 2.2526;  Loss pred: 2.2526; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 2.2504;  Loss pred: 2.2504; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 2.2174;  Loss pred: 2.2174; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 2.1873;  Loss pred: 2.1873; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 2.1540;  Loss pred: 2.1540; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 2.1126;  Loss pred: 2.1126; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 2.0453;  Loss pred: 2.0453; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.22s
Epoch 9/1000, LR 0.000225
Train loss: 1.9877;  Loss pred: 1.9877; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.21s
Epoch 10/1000, LR 0.000255
Train loss: 1.9346;  Loss pred: 1.9346; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.8723;  Loss pred: 1.8723; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 1.8024;  Loss pred: 1.8024; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 1.7624;  Loss pred: 1.7624; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 1.6976;  Loss pred: 1.6976; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.25s
Epoch 15/1000, LR 0.000285
Train loss: 1.6446;  Loss pred: 1.6446; Loss self: 0.0000; time: 0.37s
Val loss: 0.6923 score: 0.5039 time: 0.26s
Test loss: 0.6924 score: 0.5349 time: 0.20s
Epoch 16/1000, LR 0.000285
Train loss: 1.5923;  Loss pred: 1.5923; Loss self: 0.0000; time: 0.26s
Val loss: 0.6922 score: 0.8992 time: 0.19s
Test loss: 0.6923 score: 0.8760 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 1.5703;  Loss pred: 1.5703; Loss self: 0.0000; time: 0.26s
Val loss: 0.6921 score: 0.6047 time: 0.20s
Test loss: 0.6923 score: 0.5271 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 1.5204;  Loss pred: 1.5204; Loss self: 0.0000; time: 0.26s
Val loss: 0.6921 score: 0.5659 time: 0.18s
Test loss: 0.6923 score: 0.5426 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 1.4767;  Loss pred: 1.4767; Loss self: 0.0000; time: 0.26s
Val loss: 0.6920 score: 0.5659 time: 0.23s
Test loss: 0.6922 score: 0.5426 time: 0.19s
Epoch 20/1000, LR 0.000285
Train loss: 1.4534;  Loss pred: 1.4534; Loss self: 0.0000; time: 0.37s
Val loss: 0.6919 score: 0.5581 time: 0.21s
Test loss: 0.6922 score: 0.5116 time: 0.22s
Epoch 21/1000, LR 0.000285
Train loss: 1.4095;  Loss pred: 1.4095; Loss self: 0.0000; time: 0.26s
Val loss: 0.6918 score: 0.5736 time: 0.18s
Test loss: 0.6921 score: 0.5194 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 1.3887;  Loss pred: 1.3887; Loss self: 0.0000; time: 0.26s
Val loss: 0.6916 score: 0.5736 time: 0.18s
Test loss: 0.6919 score: 0.5194 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.3489;  Loss pred: 1.3489; Loss self: 0.0000; time: 0.26s
Val loss: 0.6914 score: 0.5659 time: 0.19s
Test loss: 0.6918 score: 0.5194 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.3253;  Loss pred: 1.3253; Loss self: 0.0000; time: 0.26s
Val loss: 0.6913 score: 0.5736 time: 0.18s
Test loss: 0.6917 score: 0.5349 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 1.3072;  Loss pred: 1.3072; Loss self: 0.0000; time: 0.26s
Val loss: 0.6910 score: 0.5736 time: 0.18s
Test loss: 0.6915 score: 0.5426 time: 0.19s
Epoch 26/1000, LR 0.000285
Train loss: 1.2869;  Loss pred: 1.2869; Loss self: 0.0000; time: 0.27s
Val loss: 0.6907 score: 0.5736 time: 0.24s
Test loss: 0.6912 score: 0.5271 time: 0.21s
Epoch 27/1000, LR 0.000285
Train loss: 1.2604;  Loss pred: 1.2604; Loss self: 0.0000; time: 0.34s
Val loss: 0.6904 score: 0.5736 time: 0.18s
Test loss: 0.6910 score: 0.5426 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.2453;  Loss pred: 1.2453; Loss self: 0.0000; time: 0.26s
Val loss: 0.6901 score: 0.6047 time: 0.18s
Test loss: 0.6907 score: 0.5271 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.2295;  Loss pred: 1.2295; Loss self: 0.0000; time: 0.26s
Val loss: 0.6897 score: 0.6434 time: 0.19s
Test loss: 0.6905 score: 0.5349 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 1.2169;  Loss pred: 1.2169; Loss self: 0.0000; time: 0.26s
Val loss: 0.6894 score: 0.7054 time: 0.18s
Test loss: 0.6902 score: 0.5891 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.1933;  Loss pred: 1.1933; Loss self: 0.0000; time: 0.27s
Val loss: 0.6891 score: 0.7054 time: 0.26s
Test loss: 0.6899 score: 0.6047 time: 0.25s
Epoch 32/1000, LR 0.000285
Train loss: 1.1820;  Loss pred: 1.1820; Loss self: 0.0000; time: 0.37s
Val loss: 0.6887 score: 0.7519 time: 0.26s
Test loss: 0.6896 score: 0.6899 time: 0.26s
Epoch 33/1000, LR 0.000285
Train loss: 1.1663;  Loss pred: 1.1663; Loss self: 0.0000; time: 0.35s
Val loss: 0.6883 score: 0.7829 time: 0.18s
Test loss: 0.6893 score: 0.7519 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 1.1580;  Loss pred: 1.1580; Loss self: 0.0000; time: 0.26s
Val loss: 0.6879 score: 0.7829 time: 0.18s
Test loss: 0.6890 score: 0.7752 time: 0.19s
Epoch 35/1000, LR 0.000285
Train loss: 1.1426;  Loss pred: 1.1426; Loss self: 0.0000; time: 0.25s
Val loss: 0.6875 score: 0.7829 time: 0.18s
Test loss: 0.6887 score: 0.7597 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 1.1338;  Loss pred: 1.1338; Loss self: 0.0000; time: 0.26s
Val loss: 0.6870 score: 0.7907 time: 0.18s
Test loss: 0.6883 score: 0.7829 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.1278;  Loss pred: 1.1278; Loss self: 0.0000; time: 0.26s
Val loss: 0.6865 score: 0.7984 time: 0.19s
Test loss: 0.6879 score: 0.7829 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 1.1144;  Loss pred: 1.1144; Loss self: 0.0000; time: 0.26s
Val loss: 0.6859 score: 0.8295 time: 0.19s
Test loss: 0.6874 score: 0.7984 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 1.1069;  Loss pred: 1.1069; Loss self: 0.0000; time: 0.27s
Val loss: 0.6853 score: 0.9070 time: 0.20s
Test loss: 0.6869 score: 0.8372 time: 0.24s
Epoch 40/1000, LR 0.000284
Train loss: 1.1010;  Loss pred: 1.1010; Loss self: 0.0000; time: 0.30s
Val loss: 0.6847 score: 0.9070 time: 0.19s
Test loss: 0.6864 score: 0.8605 time: 0.19s
Epoch 41/1000, LR 0.000284
Train loss: 1.0939;  Loss pred: 1.0939; Loss self: 0.0000; time: 0.27s
Val loss: 0.6840 score: 0.9070 time: 0.20s
Test loss: 0.6859 score: 0.8527 time: 0.19s
Epoch 42/1000, LR 0.000284
Train loss: 1.0821;  Loss pred: 1.0821; Loss self: 0.0000; time: 0.26s
Val loss: 0.6833 score: 0.9070 time: 0.18s
Test loss: 0.6853 score: 0.8605 time: 0.27s
Epoch 43/1000, LR 0.000284
Train loss: 1.0812;  Loss pred: 1.0812; Loss self: 0.0000; time: 0.27s
Val loss: 0.6825 score: 0.9070 time: 0.25s
Test loss: 0.6847 score: 0.8682 time: 0.27s
Epoch 44/1000, LR 0.000284
Train loss: 1.0734;  Loss pred: 1.0734; Loss self: 0.0000; time: 0.39s
Val loss: 0.6817 score: 0.9147 time: 0.23s
Test loss: 0.6841 score: 0.8605 time: 0.28s
Epoch 45/1000, LR 0.000284
Train loss: 1.0696;  Loss pred: 1.0696; Loss self: 0.0000; time: 0.26s
Val loss: 0.6809 score: 0.9225 time: 0.20s
Test loss: 0.6834 score: 0.8682 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 1.0620;  Loss pred: 1.0620; Loss self: 0.0000; time: 0.26s
Val loss: 0.6800 score: 0.9302 time: 0.20s
Test loss: 0.6826 score: 0.8760 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 1.0571;  Loss pred: 1.0571; Loss self: 0.0000; time: 0.26s
Val loss: 0.6790 score: 0.9302 time: 0.28s
Test loss: 0.6818 score: 0.8760 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 1.0526;  Loss pred: 1.0526; Loss self: 0.0000; time: 0.26s
Val loss: 0.6780 score: 0.9380 time: 0.19s
Test loss: 0.6810 score: 0.8760 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 1.0466;  Loss pred: 1.0466; Loss self: 0.0000; time: 0.27s
Val loss: 0.6768 score: 0.9380 time: 0.19s
Test loss: 0.6800 score: 0.8837 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 1.0418;  Loss pred: 1.0418; Loss self: 0.0000; time: 0.32s
Val loss: 0.6754 score: 0.9380 time: 0.29s
Test loss: 0.6789 score: 0.8837 time: 0.25s
Epoch 51/1000, LR 0.000284
Train loss: 1.0386;  Loss pred: 1.0386; Loss self: 0.0000; time: 0.38s
Val loss: 0.6739 score: 0.9302 time: 0.22s
Test loss: 0.6777 score: 0.8837 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 1.0341;  Loss pred: 1.0341; Loss self: 0.0000; time: 0.26s
Val loss: 0.6725 score: 0.9225 time: 0.18s
Test loss: 0.6766 score: 0.8760 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 1.0301;  Loss pred: 1.0301; Loss self: 0.0000; time: 0.31s
Val loss: 0.6709 score: 0.9225 time: 0.26s
Test loss: 0.6754 score: 0.8682 time: 0.25s
Epoch 54/1000, LR 0.000284
Train loss: 1.0270;  Loss pred: 1.0270; Loss self: 0.0000; time: 0.37s
Val loss: 0.6693 score: 0.9225 time: 0.26s
Test loss: 0.6740 score: 0.8682 time: 0.25s
Epoch 55/1000, LR 0.000284
Train loss: 1.0214;  Loss pred: 1.0214; Loss self: 0.0000; time: 0.37s
Val loss: 0.6675 score: 0.9225 time: 0.26s
Test loss: 0.6726 score: 0.8760 time: 0.25s
Epoch 56/1000, LR 0.000284
Train loss: 1.0173;  Loss pred: 1.0173; Loss self: 0.0000; time: 0.38s
Val loss: 0.6657 score: 0.9380 time: 0.27s
Test loss: 0.6711 score: 0.8837 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 1.0134;  Loss pred: 1.0134; Loss self: 0.0000; time: 0.26s
Val loss: 0.6637 score: 0.9380 time: 0.18s
Test loss: 0.6694 score: 0.8837 time: 0.18s
Epoch 58/1000, LR 0.000283
Train loss: 1.0114;  Loss pred: 1.0114; Loss self: 0.0000; time: 0.26s
Val loss: 0.6616 score: 0.9380 time: 0.19s
Test loss: 0.6677 score: 0.8837 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 1.0057;  Loss pred: 1.0057; Loss self: 0.0000; time: 0.26s
Val loss: 0.6593 score: 0.9380 time: 0.18s
Test loss: 0.6658 score: 0.8837 time: 0.17s
Epoch 60/1000, LR 0.000283
Train loss: 1.0027;  Loss pred: 1.0027; Loss self: 0.0000; time: 0.26s
Val loss: 0.6569 score: 0.9380 time: 0.18s
Test loss: 0.6639 score: 0.8837 time: 0.17s
Epoch 61/1000, LR 0.000283
Train loss: 1.0005;  Loss pred: 1.0005; Loss self: 0.0000; time: 0.26s
Val loss: 0.6544 score: 0.9380 time: 0.21s
Test loss: 0.6619 score: 0.8837 time: 0.18s
Epoch 62/1000, LR 0.000283
Train loss: 0.9971;  Loss pred: 0.9971; Loss self: 0.0000; time: 0.33s
Val loss: 0.6517 score: 0.9380 time: 0.19s
Test loss: 0.6597 score: 0.8915 time: 0.17s
Epoch 63/1000, LR 0.000283
Train loss: 0.9917;  Loss pred: 0.9917; Loss self: 0.0000; time: 0.26s
Val loss: 0.6490 score: 0.9380 time: 0.18s
Test loss: 0.6574 score: 0.8837 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9884;  Loss pred: 0.9884; Loss self: 0.0000; time: 0.27s
Val loss: 0.6460 score: 0.9380 time: 0.19s
Test loss: 0.6550 score: 0.8837 time: 0.18s
Epoch 65/1000, LR 0.000283
Train loss: 0.9860;  Loss pred: 0.9860; Loss self: 0.0000; time: 0.26s
Val loss: 0.6429 score: 0.9457 time: 0.19s
Test loss: 0.6524 score: 0.8837 time: 0.18s
Epoch 66/1000, LR 0.000283
Train loss: 0.9804;  Loss pred: 0.9804; Loss self: 0.0000; time: 0.27s
Val loss: 0.6395 score: 0.9457 time: 0.19s
Test loss: 0.6497 score: 0.8837 time: 0.25s
Epoch 67/1000, LR 0.000283
Train loss: 0.9767;  Loss pred: 0.9767; Loss self: 0.0000; time: 0.34s
Val loss: 0.6360 score: 0.9380 time: 0.18s
Test loss: 0.6468 score: 0.8837 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 0.9715;  Loss pred: 0.9715; Loss self: 0.0000; time: 0.26s
Val loss: 0.6322 score: 0.9380 time: 0.17s
Test loss: 0.6437 score: 0.8837 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 0.9679;  Loss pred: 0.9679; Loss self: 0.0000; time: 0.25s
Val loss: 0.6282 score: 0.9380 time: 0.18s
Test loss: 0.6404 score: 0.8837 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9646;  Loss pred: 0.9646; Loss self: 0.0000; time: 0.26s
Val loss: 0.6240 score: 0.9380 time: 0.18s
Test loss: 0.6370 score: 0.8837 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9609;  Loss pred: 0.9609; Loss self: 0.0000; time: 0.25s
Val loss: 0.6196 score: 0.9380 time: 0.18s
Test loss: 0.6333 score: 0.8837 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9567;  Loss pred: 0.9567; Loss self: 0.0000; time: 0.26s
Val loss: 0.6150 score: 0.9380 time: 0.19s
Test loss: 0.6295 score: 0.8760 time: 0.20s
Epoch 73/1000, LR 0.000282
Train loss: 0.9532;  Loss pred: 0.9532; Loss self: 0.0000; time: 0.27s
Val loss: 0.6103 score: 0.9380 time: 0.26s
Test loss: 0.6256 score: 0.8760 time: 0.19s
Epoch 74/1000, LR 0.000282
Train loss: 0.9486;  Loss pred: 0.9486; Loss self: 0.0000; time: 0.26s
Val loss: 0.6053 score: 0.9380 time: 0.19s
Test loss: 0.6216 score: 0.8760 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9448;  Loss pred: 0.9448; Loss self: 0.0000; time: 0.27s
Val loss: 0.6002 score: 0.9380 time: 0.19s
Test loss: 0.6174 score: 0.8760 time: 0.17s
Epoch 76/1000, LR 0.000282
Train loss: 0.9407;  Loss pred: 0.9407; Loss self: 0.0000; time: 0.26s
Val loss: 0.5949 score: 0.9380 time: 0.18s
Test loss: 0.6130 score: 0.8837 time: 0.25s
Epoch 77/1000, LR 0.000282
Train loss: 0.9355;  Loss pred: 0.9355; Loss self: 0.0000; time: 0.37s
Val loss: 0.5895 score: 0.9380 time: 0.26s
Test loss: 0.6084 score: 0.8837 time: 0.23s
Epoch 78/1000, LR 0.000282
Train loss: 0.9314;  Loss pred: 0.9314; Loss self: 0.0000; time: 0.26s
Val loss: 0.5839 score: 0.9380 time: 0.18s
Test loss: 0.6038 score: 0.8760 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.9271;  Loss pred: 0.9271; Loss self: 0.0000; time: 0.26s
Val loss: 0.5781 score: 0.9380 time: 0.23s
Test loss: 0.5990 score: 0.8837 time: 0.22s
Epoch 80/1000, LR 0.000282
Train loss: 0.9231;  Loss pred: 0.9231; Loss self: 0.0000; time: 0.27s
Val loss: 0.5722 score: 0.9380 time: 0.19s
Test loss: 0.5940 score: 0.8837 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.9171;  Loss pred: 0.9171; Loss self: 0.0000; time: 0.30s
Val loss: 0.5661 score: 0.9380 time: 0.26s
Test loss: 0.5889 score: 0.8837 time: 0.25s
Epoch 82/1000, LR 0.000281
Train loss: 0.9130;  Loss pred: 0.9130; Loss self: 0.0000; time: 0.38s
Val loss: 0.5598 score: 0.9380 time: 0.19s
Test loss: 0.5837 score: 0.8837 time: 0.17s
Epoch 83/1000, LR 0.000281
Train loss: 0.9077;  Loss pred: 0.9077; Loss self: 0.0000; time: 0.27s
Val loss: 0.5535 score: 0.9380 time: 0.18s
Test loss: 0.5783 score: 0.8837 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9040;  Loss pred: 0.9040; Loss self: 0.0000; time: 0.27s
Val loss: 0.5469 score: 0.9380 time: 0.18s
Test loss: 0.5729 score: 0.8837 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.8990;  Loss pred: 0.8990; Loss self: 0.0000; time: 0.27s
Val loss: 0.5402 score: 0.9380 time: 0.18s
Test loss: 0.5673 score: 0.8837 time: 0.17s
Epoch 86/1000, LR 0.000281
Train loss: 0.8927;  Loss pred: 0.8927; Loss self: 0.0000; time: 0.27s
Val loss: 0.5334 score: 0.9380 time: 0.18s
Test loss: 0.5615 score: 0.8837 time: 0.19s
Epoch 87/1000, LR 0.000281
Train loss: 0.8890;  Loss pred: 0.8890; Loss self: 0.0000; time: 0.28s
Val loss: 0.5264 score: 0.9380 time: 0.26s
Test loss: 0.5557 score: 0.8837 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.8831;  Loss pred: 0.8831; Loss self: 0.0000; time: 0.26s
Val loss: 0.5194 score: 0.9380 time: 0.18s
Test loss: 0.5499 score: 0.8837 time: 0.18s
Epoch 89/1000, LR 0.000281
Train loss: 0.8782;  Loss pred: 0.8782; Loss self: 0.0000; time: 0.26s
Val loss: 0.5121 score: 0.9380 time: 0.18s
Test loss: 0.5438 score: 0.8837 time: 0.18s
Epoch 90/1000, LR 0.000281
Train loss: 0.8723;  Loss pred: 0.8723; Loss self: 0.0000; time: 0.26s
Val loss: 0.5047 score: 0.9380 time: 0.18s
Test loss: 0.5376 score: 0.8837 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8685;  Loss pred: 0.8685; Loss self: 0.0000; time: 0.28s
Val loss: 0.4973 score: 0.9380 time: 0.18s
Test loss: 0.5314 score: 0.8837 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.8621;  Loss pred: 0.8621; Loss self: 0.0000; time: 0.27s
Val loss: 0.4900 score: 0.9380 time: 0.19s
Test loss: 0.5251 score: 0.8837 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 0.30s
Val loss: 0.4824 score: 0.9380 time: 0.22s
Test loss: 0.5187 score: 0.8837 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8523;  Loss pred: 0.8523; Loss self: 0.0000; time: 0.26s
Val loss: 0.4749 score: 0.9380 time: 0.18s
Test loss: 0.5123 score: 0.8837 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.8466;  Loss pred: 0.8466; Loss self: 0.0000; time: 0.26s
Val loss: 0.4672 score: 0.9380 time: 0.18s
Test loss: 0.5059 score: 0.8837 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.8421;  Loss pred: 0.8421; Loss self: 0.0000; time: 0.26s
Val loss: 0.4596 score: 0.9380 time: 0.18s
Test loss: 0.4995 score: 0.8837 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8356;  Loss pred: 0.8356; Loss self: 0.0000; time: 0.27s
Val loss: 0.4522 score: 0.9380 time: 0.19s
Test loss: 0.4931 score: 0.8760 time: 0.24s
Epoch 98/1000, LR 0.000280
Train loss: 0.8300;  Loss pred: 0.8300; Loss self: 0.0000; time: 0.36s
Val loss: 0.4445 score: 0.9380 time: 0.26s
Test loss: 0.4866 score: 0.8760 time: 0.25s
Epoch 99/1000, LR 0.000279
Train loss: 0.8239;  Loss pred: 0.8239; Loss self: 0.0000; time: 0.37s
Val loss: 0.4369 score: 0.9380 time: 0.21s
Test loss: 0.4802 score: 0.8760 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8198;  Loss pred: 0.8198; Loss self: 0.0000; time: 0.27s
Val loss: 0.4293 score: 0.9380 time: 0.19s
Test loss: 0.4737 score: 0.8837 time: 0.18s
Epoch 101/1000, LR 0.000279
Train loss: 0.8143;  Loss pred: 0.8143; Loss self: 0.0000; time: 0.27s
Val loss: 0.4219 score: 0.9380 time: 0.18s
Test loss: 0.4674 score: 0.8837 time: 0.17s
Epoch 102/1000, LR 0.000279
Train loss: 0.8088;  Loss pred: 0.8088; Loss self: 0.0000; time: 0.26s
Val loss: 0.4146 score: 0.9380 time: 0.19s
Test loss: 0.4612 score: 0.8837 time: 0.18s
Epoch 103/1000, LR 0.000279
Train loss: 0.8041;  Loss pred: 0.8041; Loss self: 0.0000; time: 0.27s
Val loss: 0.4076 score: 0.9380 time: 0.19s
Test loss: 0.4551 score: 0.8837 time: 0.18s
Epoch 104/1000, LR 0.000279
Train loss: 0.7989;  Loss pred: 0.7989; Loss self: 0.0000; time: 0.25s
Val loss: 0.4006 score: 0.9380 time: 0.17s
Test loss: 0.4491 score: 0.8837 time: 0.17s
Epoch 105/1000, LR 0.000279
Train loss: 0.7933;  Loss pred: 0.7933; Loss self: 0.0000; time: 0.29s
Val loss: 0.3934 score: 0.9380 time: 0.19s
Test loss: 0.4429 score: 0.8837 time: 0.19s
Epoch 106/1000, LR 0.000279
Train loss: 0.7882;  Loss pred: 0.7882; Loss self: 0.0000; time: 0.29s
Val loss: 0.3864 score: 0.9380 time: 0.22s
Test loss: 0.4369 score: 0.8837 time: 0.17s
Epoch 107/1000, LR 0.000278
Train loss: 0.7834;  Loss pred: 0.7834; Loss self: 0.0000; time: 0.25s
Val loss: 0.3794 score: 0.9380 time: 0.18s
Test loss: 0.4309 score: 0.8837 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.7782;  Loss pred: 0.7782; Loss self: 0.0000; time: 0.26s
Val loss: 0.3729 score: 0.9457 time: 0.18s
Test loss: 0.4253 score: 0.8837 time: 0.18s
Epoch 109/1000, LR 0.000278
Train loss: 0.7737;  Loss pred: 0.7737; Loss self: 0.0000; time: 0.29s
Val loss: 0.3662 score: 0.9457 time: 0.25s
Test loss: 0.4196 score: 0.8837 time: 0.25s
Epoch 110/1000, LR 0.000278
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 0.36s
Val loss: 0.3602 score: 0.9457 time: 0.26s
Test loss: 0.4143 score: 0.8837 time: 0.25s
Epoch 111/1000, LR 0.000278
Train loss: 0.7637;  Loss pred: 0.7637; Loss self: 0.0000; time: 0.37s
Val loss: 0.3541 score: 0.9457 time: 0.19s
Test loss: 0.4089 score: 0.8837 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.7607;  Loss pred: 0.7607; Loss self: 0.0000; time: 0.27s
Val loss: 0.3481 score: 0.9457 time: 0.18s
Test loss: 0.4038 score: 0.8837 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7570;  Loss pred: 0.7570; Loss self: 0.0000; time: 0.27s
Val loss: 0.3421 score: 0.9457 time: 0.19s
Test loss: 0.3985 score: 0.8837 time: 0.18s
Epoch 114/1000, LR 0.000277
Train loss: 0.7522;  Loss pred: 0.7522; Loss self: 0.0000; time: 0.26s
Val loss: 0.3364 score: 0.9457 time: 0.19s
Test loss: 0.3936 score: 0.8915 time: 0.18s
Epoch 115/1000, LR 0.000277
Train loss: 0.7464;  Loss pred: 0.7464; Loss self: 0.0000; time: 0.29s
Val loss: 0.3312 score: 0.9457 time: 0.23s
Test loss: 0.3888 score: 0.8915 time: 0.23s
Epoch 116/1000, LR 0.000277
Train loss: 0.7430;  Loss pred: 0.7430; Loss self: 0.0000; time: 0.36s
Val loss: 0.3257 score: 0.9457 time: 0.26s
Test loss: 0.3840 score: 0.8915 time: 0.25s
Epoch 117/1000, LR 0.000277
Train loss: 0.7398;  Loss pred: 0.7398; Loss self: 0.0000; time: 0.37s
Val loss: 0.3202 score: 0.9457 time: 0.26s
Test loss: 0.3792 score: 0.8915 time: 0.24s
Epoch 118/1000, LR 0.000277
Train loss: 0.7355;  Loss pred: 0.7355; Loss self: 0.0000; time: 0.28s
Val loss: 0.3152 score: 0.9457 time: 0.20s
Test loss: 0.3748 score: 0.8915 time: 0.19s
Epoch 119/1000, LR 0.000277
Train loss: 0.7312;  Loss pred: 0.7312; Loss self: 0.0000; time: 0.26s
Val loss: 0.3101 score: 0.9457 time: 0.20s
Test loss: 0.3703 score: 0.8915 time: 0.19s
Epoch 120/1000, LR 0.000277
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 0.27s
Val loss: 0.3052 score: 0.9457 time: 0.19s
Test loss: 0.3659 score: 0.8915 time: 0.18s
Epoch 121/1000, LR 0.000276
Train loss: 0.7244;  Loss pred: 0.7244; Loss self: 0.0000; time: 0.27s
Val loss: 0.3004 score: 0.9457 time: 0.19s
Test loss: 0.3617 score: 0.8915 time: 0.18s
Epoch 122/1000, LR 0.000276
Train loss: 0.7206;  Loss pred: 0.7206; Loss self: 0.0000; time: 0.31s
Val loss: 0.2957 score: 0.9457 time: 0.20s
Test loss: 0.3576 score: 0.8915 time: 0.24s
Epoch 123/1000, LR 0.000276
Train loss: 0.7166;  Loss pred: 0.7166; Loss self: 0.0000; time: 0.28s
Val loss: 0.2917 score: 0.9457 time: 0.18s
Test loss: 0.3539 score: 0.8992 time: 0.18s
Epoch 124/1000, LR 0.000276
Train loss: 0.7129;  Loss pred: 0.7129; Loss self: 0.0000; time: 0.26s
Val loss: 0.2877 score: 0.9457 time: 0.18s
Test loss: 0.3501 score: 0.8992 time: 0.18s
Epoch 125/1000, LR 0.000276
Train loss: 0.7101;  Loss pred: 0.7101; Loss self: 0.0000; time: 0.28s
Val loss: 0.2839 score: 0.9457 time: 0.24s
Test loss: 0.3465 score: 0.9147 time: 0.25s
Epoch 126/1000, LR 0.000276
Train loss: 0.7075;  Loss pred: 0.7075; Loss self: 0.0000; time: 0.37s
Val loss: 0.2804 score: 0.9457 time: 0.26s
Test loss: 0.3432 score: 0.9225 time: 0.25s
Epoch 127/1000, LR 0.000275
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.37s
Val loss: 0.2768 score: 0.9457 time: 0.27s
Test loss: 0.3397 score: 0.9225 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.26s
Val loss: 0.2725 score: 0.9457 time: 0.18s
Test loss: 0.3360 score: 0.9147 time: 0.18s
Epoch 129/1000, LR 0.000275
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.36s
Val loss: 0.2686 score: 0.9457 time: 0.18s
Test loss: 0.3325 score: 0.9147 time: 0.17s
Epoch 130/1000, LR 0.000275
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.26s
Val loss: 0.2651 score: 0.9457 time: 0.19s
Test loss: 0.3292 score: 0.9147 time: 0.18s
Epoch 131/1000, LR 0.000275
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.26s
Val loss: 0.2618 score: 0.9457 time: 0.19s
Test loss: 0.3260 score: 0.9147 time: 0.28s
Epoch 132/1000, LR 0.000275
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.26s
Val loss: 0.2586 score: 0.9457 time: 0.19s
Test loss: 0.3229 score: 0.9147 time: 0.27s
Epoch 133/1000, LR 0.000274
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.27s
Val loss: 0.2557 score: 0.9457 time: 0.19s
Test loss: 0.3200 score: 0.9147 time: 0.18s
Epoch 134/1000, LR 0.000274
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.31s
Val loss: 0.2529 score: 0.9457 time: 0.19s
Test loss: 0.3171 score: 0.9147 time: 0.19s
Epoch 135/1000, LR 0.000274
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.26s
Val loss: 0.2501 score: 0.9457 time: 0.19s
Test loss: 0.3143 score: 0.9225 time: 0.18s
Epoch 136/1000, LR 0.000274
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.26s
Val loss: 0.2472 score: 0.9457 time: 0.19s
Test loss: 0.3114 score: 0.9225 time: 0.25s
Epoch 137/1000, LR 0.000274
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.26s
Val loss: 0.2442 score: 0.9457 time: 0.18s
Test loss: 0.3086 score: 0.9147 time: 0.17s
Epoch 138/1000, LR 0.000274
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.25s
Val loss: 0.2420 score: 0.9457 time: 0.18s
Test loss: 0.3061 score: 0.9225 time: 0.21s
Epoch 139/1000, LR 0.000273
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.26s
Val loss: 0.2392 score: 0.9457 time: 0.23s
Test loss: 0.3035 score: 0.9225 time: 0.18s
Epoch 140/1000, LR 0.000273
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.25s
Val loss: 0.2365 score: 0.9457 time: 0.17s
Test loss: 0.3009 score: 0.9147 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 0.26s
Val loss: 0.2344 score: 0.9457 time: 0.18s
Test loss: 0.2985 score: 0.9225 time: 0.17s
Epoch 142/1000, LR 0.000273
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 0.26s
Val loss: 0.2324 score: 0.9457 time: 0.19s
Test loss: 0.2961 score: 0.9225 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.26s
Val loss: 0.2302 score: 0.9457 time: 0.18s
Test loss: 0.2939 score: 0.9225 time: 0.17s
Epoch 144/1000, LR 0.000272
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.25s
Val loss: 0.2277 score: 0.9457 time: 0.21s
Test loss: 0.2916 score: 0.9225 time: 0.18s
Epoch 145/1000, LR 0.000272
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.26s
Val loss: 0.2255 score: 0.9457 time: 0.18s
Test loss: 0.2893 score: 0.9225 time: 0.25s
Epoch 146/1000, LR 0.000272
Train loss: 0.6507;  Loss pred: 0.6507; Loss self: 0.0000; time: 0.25s
Val loss: 0.2228 score: 0.9457 time: 0.18s
Test loss: 0.2872 score: 0.9147 time: 0.17s
Epoch 147/1000, LR 0.000272
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.25s
Val loss: 0.2211 score: 0.9457 time: 0.18s
Test loss: 0.2851 score: 0.9147 time: 0.24s
Epoch 148/1000, LR 0.000272
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.36s
Val loss: 0.2196 score: 0.9457 time: 0.26s
Test loss: 0.2832 score: 0.9147 time: 0.25s
Epoch 149/1000, LR 0.000272
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.37s
Val loss: 0.2181 score: 0.9457 time: 0.26s
Test loss: 0.2813 score: 0.9225 time: 0.25s
Epoch 150/1000, LR 0.000271
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.37s
Val loss: 0.2168 score: 0.9457 time: 0.26s
Test loss: 0.2795 score: 0.9225 time: 0.23s
Epoch 151/1000, LR 0.000271
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.27s
Val loss: 0.2151 score: 0.9457 time: 0.18s
Test loss: 0.2776 score: 0.9225 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6416;  Loss pred: 0.6416; Loss self: 0.0000; time: 0.25s
Val loss: 0.2141 score: 0.9457 time: 0.18s
Test loss: 0.2759 score: 0.9225 time: 0.17s
Epoch 153/1000, LR 0.000271
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.25s
Val loss: 0.2125 score: 0.9457 time: 0.18s
Test loss: 0.2741 score: 0.9225 time: 0.17s
Epoch 154/1000, LR 0.000271
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.26s
Val loss: 0.2104 score: 0.9457 time: 0.18s
Test loss: 0.2722 score: 0.9225 time: 0.17s
Epoch 155/1000, LR 0.000270
Train loss: 0.6356;  Loss pred: 0.6356; Loss self: 0.0000; time: 0.26s
Val loss: 0.2083 score: 0.9457 time: 0.18s
Test loss: 0.2704 score: 0.9225 time: 0.17s
Epoch 156/1000, LR 0.000270
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.25s
Val loss: 0.2065 score: 0.9457 time: 0.18s
Test loss: 0.2688 score: 0.9147 time: 0.17s
Epoch 157/1000, LR 0.000270
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.29s
Val loss: 0.2043 score: 0.9457 time: 0.18s
Test loss: 0.2672 score: 0.9147 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6289;  Loss pred: 0.6289; Loss self: 0.0000; time: 0.34s
Val loss: 0.2026 score: 0.9457 time: 0.18s
Test loss: 0.2658 score: 0.9147 time: 0.17s
Epoch 159/1000, LR 0.000270
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.25s
Val loss: 0.2009 score: 0.9457 time: 0.18s
Test loss: 0.2643 score: 0.9147 time: 0.17s
Epoch 160/1000, LR 0.000269
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.26s
Val loss: 0.2001 score: 0.9457 time: 0.19s
Test loss: 0.2626 score: 0.9147 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.26s
Val loss: 0.1998 score: 0.9457 time: 0.19s
Test loss: 0.2610 score: 0.9147 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.26s
Val loss: 0.1989 score: 0.9457 time: 0.19s
Test loss: 0.2595 score: 0.9147 time: 0.20s
Epoch 163/1000, LR 0.000269
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.28s
Val loss: 0.1975 score: 0.9457 time: 0.19s
Test loss: 0.2580 score: 0.9147 time: 0.24s
Epoch 164/1000, LR 0.000269
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 0.28s
Val loss: 0.1969 score: 0.9535 time: 0.18s
Test loss: 0.2566 score: 0.9225 time: 0.17s
Epoch 165/1000, LR 0.000268
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.26s
Val loss: 0.1955 score: 0.9535 time: 0.17s
Test loss: 0.2551 score: 0.9225 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.25s
Val loss: 0.1937 score: 0.9457 time: 0.18s
Test loss: 0.2538 score: 0.9147 time: 0.18s
Epoch 167/1000, LR 0.000268
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.26s
Val loss: 0.1921 score: 0.9457 time: 0.19s
Test loss: 0.2526 score: 0.9147 time: 0.18s
Epoch 168/1000, LR 0.000268
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.26s
Val loss: 0.1904 score: 0.9457 time: 0.19s
Test loss: 0.2514 score: 0.9147 time: 0.18s
Epoch 169/1000, LR 0.000267
Train loss: 0.6167;  Loss pred: 0.6167; Loss self: 0.0000; time: 0.28s
Val loss: 0.1897 score: 0.9457 time: 0.24s
Test loss: 0.2500 score: 0.9147 time: 0.21s
Epoch 170/1000, LR 0.000267
Train loss: 0.6103;  Loss pred: 0.6103; Loss self: 0.0000; time: 0.26s
Val loss: 0.1886 score: 0.9457 time: 0.19s
Test loss: 0.2489 score: 0.9147 time: 0.18s
Epoch 171/1000, LR 0.000267
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.26s
Val loss: 0.1877 score: 0.9457 time: 0.18s
Test loss: 0.2478 score: 0.9147 time: 0.17s
Epoch 172/1000, LR 0.000267
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.27s
Val loss: 0.1869 score: 0.9535 time: 0.18s
Test loss: 0.2465 score: 0.9147 time: 0.17s
Epoch 173/1000, LR 0.000267
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.26s
Val loss: 0.1857 score: 0.9457 time: 0.20s
Test loss: 0.2456 score: 0.9147 time: 0.18s
Epoch 174/1000, LR 0.000266
Train loss: 0.6040;  Loss pred: 0.6040; Loss self: 0.0000; time: 0.36s
Val loss: 0.1848 score: 0.9535 time: 0.26s
Test loss: 0.2444 score: 0.9147 time: 0.25s
Epoch 175/1000, LR 0.000266
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.34s
Val loss: 0.1837 score: 0.9457 time: 0.20s
Test loss: 0.2435 score: 0.9147 time: 0.18s
Epoch 176/1000, LR 0.000266
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.25s
Val loss: 0.1830 score: 0.9535 time: 0.18s
Test loss: 0.2423 score: 0.9147 time: 0.18s
Epoch 177/1000, LR 0.000266
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.25s
Val loss: 0.1823 score: 0.9535 time: 0.19s
Test loss: 0.2412 score: 0.9147 time: 0.23s
Epoch 178/1000, LR 0.000265
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.36s
Val loss: 0.1820 score: 0.9535 time: 0.26s
Test loss: 0.2400 score: 0.9147 time: 0.24s
Epoch 179/1000, LR 0.000265
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.36s
Val loss: 0.1811 score: 0.9535 time: 0.26s
Test loss: 0.2390 score: 0.9147 time: 0.25s
Epoch 180/1000, LR 0.000265
Train loss: 0.5982;  Loss pred: 0.5982; Loss self: 0.0000; time: 0.39s
Val loss: 0.1800 score: 0.9535 time: 0.18s
Test loss: 0.2381 score: 0.9147 time: 0.16s
Epoch 181/1000, LR 0.000265
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 0.26s
Val loss: 0.1787 score: 0.9535 time: 0.18s
Test loss: 0.2375 score: 0.9147 time: 0.17s
Epoch 182/1000, LR 0.000265
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.26s
Val loss: 0.1775 score: 0.9535 time: 0.23s
Test loss: 0.2370 score: 0.9225 time: 0.17s
Epoch 183/1000, LR 0.000264
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.26s
Val loss: 0.1764 score: 0.9380 time: 0.18s
Test loss: 0.2366 score: 0.9225 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.29s
Val loss: 0.1756 score: 0.9380 time: 0.19s
Test loss: 0.2358 score: 0.9225 time: 0.26s
Epoch 185/1000, LR 0.000264
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.33s
Val loss: 0.1751 score: 0.9457 time: 0.18s
Test loss: 0.2348 score: 0.9225 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 0.26s
Val loss: 0.1745 score: 0.9457 time: 0.17s
Test loss: 0.2340 score: 0.9225 time: 0.17s
Epoch 187/1000, LR 0.000263
Train loss: 0.5943;  Loss pred: 0.5943; Loss self: 0.0000; time: 0.26s
Val loss: 0.1745 score: 0.9535 time: 0.18s
Test loss: 0.2324 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.25s
Val loss: 0.1740 score: 0.9535 time: 0.19s
Test loss: 0.2316 score: 0.9147 time: 0.17s
Epoch 189/1000, LR 0.000263
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.25s
Val loss: 0.1734 score: 0.9535 time: 0.18s
Test loss: 0.2308 score: 0.9147 time: 0.17s
Epoch 190/1000, LR 0.000263
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 0.26s
Val loss: 0.1727 score: 0.9535 time: 0.18s
Test loss: 0.2301 score: 0.9225 time: 0.17s
Epoch 191/1000, LR 0.000262
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.25s
Val loss: 0.1721 score: 0.9535 time: 0.18s
Test loss: 0.2293 score: 0.9225 time: 0.21s
Epoch 192/1000, LR 0.000262
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.28s
Val loss: 0.1709 score: 0.9457 time: 0.19s
Test loss: 0.2290 score: 0.9225 time: 0.27s
Epoch 193/1000, LR 0.000262
Train loss: 0.5867;  Loss pred: 0.5867; Loss self: 0.0000; time: 0.27s
Val loss: 0.1698 score: 0.9457 time: 0.19s
Test loss: 0.2291 score: 0.9225 time: 0.18s
Epoch 194/1000, LR 0.000262
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.25s
Val loss: 0.1690 score: 0.9457 time: 0.18s
Test loss: 0.2288 score: 0.9225 time: 0.24s
Epoch 195/1000, LR 0.000261
Train loss: 0.5825;  Loss pred: 0.5825; Loss self: 0.0000; time: 0.36s
Val loss: 0.1685 score: 0.9457 time: 0.26s
Test loss: 0.2281 score: 0.9225 time: 0.25s
Epoch 196/1000, LR 0.000261
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.36s
Val loss: 0.1680 score: 0.9457 time: 0.26s
Test loss: 0.2273 score: 0.9225 time: 0.25s
Epoch 197/1000, LR 0.000261
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.38s
Val loss: 0.1676 score: 0.9457 time: 0.18s
Test loss: 0.2266 score: 0.9225 time: 0.17s
Epoch 198/1000, LR 0.000261
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.26s
Val loss: 0.1676 score: 0.9457 time: 0.18s
Test loss: 0.2252 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.26s
Val loss: 0.1674 score: 0.9457 time: 0.18s
Test loss: 0.2242 score: 0.9225 time: 0.17s
Epoch 200/1000, LR 0.000260
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.25s
Val loss: 0.1672 score: 0.9457 time: 0.18s
Test loss: 0.2234 score: 0.9225 time: 0.17s
Epoch 201/1000, LR 0.000260
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.25s
Val loss: 0.1667 score: 0.9457 time: 0.17s
Test loss: 0.2229 score: 0.9225 time: 0.17s
Epoch 202/1000, LR 0.000260
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.26s
Val loss: 0.1658 score: 0.9457 time: 0.18s
Test loss: 0.2226 score: 0.9225 time: 0.17s
Epoch 203/1000, LR 0.000259
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 0.32s
Val loss: 0.1650 score: 0.9457 time: 0.18s
Test loss: 0.2224 score: 0.9225 time: 0.17s
Epoch 204/1000, LR 0.000259
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.25s
Val loss: 0.1643 score: 0.9457 time: 0.18s
Test loss: 0.2224 score: 0.9225 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.26s
Val loss: 0.1636 score: 0.9457 time: 0.18s
Test loss: 0.2223 score: 0.9225 time: 0.18s
Epoch 206/1000, LR 0.000259
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.25s
Val loss: 0.1633 score: 0.9457 time: 0.19s
Test loss: 0.2216 score: 0.9225 time: 0.17s
Epoch 207/1000, LR 0.000258
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.25s
Val loss: 0.1630 score: 0.9457 time: 0.23s
Test loss: 0.2208 score: 0.9225 time: 0.17s
Epoch 208/1000, LR 0.000258
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.26s
Val loss: 0.1629 score: 0.9457 time: 0.23s
Test loss: 0.2200 score: 0.9225 time: 0.25s
Epoch 209/1000, LR 0.000258
Train loss: 0.5722;  Loss pred: 0.5722; Loss self: 0.0000; time: 0.35s
Val loss: 0.1628 score: 0.9457 time: 0.18s
Test loss: 0.2193 score: 0.9225 time: 0.17s
Epoch 210/1000, LR 0.000258
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.25s
Val loss: 0.1623 score: 0.9457 time: 0.18s
Test loss: 0.2190 score: 0.9225 time: 0.17s
Epoch 211/1000, LR 0.000257
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.26s
Val loss: 0.1617 score: 0.9457 time: 0.19s
Test loss: 0.2189 score: 0.9225 time: 0.17s
Epoch 212/1000, LR 0.000257
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.26s
Val loss: 0.1610 score: 0.9457 time: 0.18s
Test loss: 0.2190 score: 0.9225 time: 0.17s
Epoch 213/1000, LR 0.000257
Train loss: 0.5697;  Loss pred: 0.5697; Loss self: 0.0000; time: 0.26s
Val loss: 0.1605 score: 0.9457 time: 0.18s
Test loss: 0.2190 score: 0.9225 time: 0.17s
Epoch 214/1000, LR 0.000256
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.29s
Val loss: 0.1601 score: 0.9457 time: 0.20s
Test loss: 0.2187 score: 0.9225 time: 0.24s
Epoch 215/1000, LR 0.000256
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.30s
Val loss: 0.1600 score: 0.9457 time: 0.18s
Test loss: 0.2178 score: 0.9225 time: 0.17s
Epoch 216/1000, LR 0.000256
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.25s
Val loss: 0.1598 score: 0.9457 time: 0.18s
Test loss: 0.2173 score: 0.9225 time: 0.17s
Epoch 217/1000, LR 0.000256
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.26s
Val loss: 0.1596 score: 0.9457 time: 0.18s
Test loss: 0.2167 score: 0.9225 time: 0.17s
Epoch 218/1000, LR 0.000255
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.34s
Val loss: 0.1595 score: 0.9457 time: 0.18s
Test loss: 0.2162 score: 0.9225 time: 0.18s
Epoch 219/1000, LR 0.000255
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.26s
Val loss: 0.1591 score: 0.9457 time: 0.19s
Test loss: 0.2160 score: 0.9225 time: 0.18s
Epoch 220/1000, LR 0.000255
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.27s
Val loss: 0.1587 score: 0.9457 time: 0.19s
Test loss: 0.2159 score: 0.9225 time: 0.26s
Epoch 221/1000, LR 0.000255
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.25s
Val loss: 0.1582 score: 0.9457 time: 0.19s
Test loss: 0.2162 score: 0.9225 time: 0.18s
Epoch 222/1000, LR 0.000254
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.26s
Val loss: 0.1578 score: 0.9457 time: 0.19s
Test loss: 0.2164 score: 0.9225 time: 0.24s
Epoch 223/1000, LR 0.000254
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.39s
Val loss: 0.1575 score: 0.9457 time: 0.18s
Test loss: 0.2164 score: 0.9225 time: 0.17s
Epoch 224/1000, LR 0.000254
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.26s
Val loss: 0.1572 score: 0.9457 time: 0.20s
Test loss: 0.2161 score: 0.9225 time: 0.25s
Epoch 225/1000, LR 0.000253
Train loss: 0.5588;  Loss pred: 0.5588; Loss self: 0.0000; time: 0.38s
Val loss: 0.1572 score: 0.9457 time: 0.34s
Test loss: 0.2153 score: 0.9225 time: 0.25s
Epoch 226/1000, LR 0.000253
Train loss: 0.5584;  Loss pred: 0.5584; Loss self: 0.0000; time: 0.37s
Val loss: 0.1570 score: 0.9457 time: 0.26s
Test loss: 0.2152 score: 0.9225 time: 0.25s
Epoch 227/1000, LR 0.000253
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.29s
Val loss: 0.1569 score: 0.9457 time: 0.18s
Test loss: 0.2147 score: 0.9225 time: 0.17s
Epoch 228/1000, LR 0.000253
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.26s
Val loss: 0.1569 score: 0.9457 time: 0.18s
Test loss: 0.2141 score: 0.9225 time: 0.17s
Epoch 229/1000, LR 0.000252
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.27s
Val loss: 0.1569 score: 0.9380 time: 0.18s
Test loss: 0.2135 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5591;  Loss pred: 0.5591; Loss self: 0.0000; time: 0.26s
Val loss: 0.1568 score: 0.9380 time: 0.18s
Test loss: 0.2133 score: 0.9302 time: 0.17s
Epoch 231/1000, LR 0.000252
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.28s
Val loss: 0.1564 score: 0.9380 time: 0.18s
Test loss: 0.2134 score: 0.9302 time: 0.24s
Epoch 232/1000, LR 0.000251
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.37s
Val loss: 0.1559 score: 0.9457 time: 0.26s
Test loss: 0.2138 score: 0.9225 time: 0.25s
Epoch 233/1000, LR 0.000251
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.30s
Val loss: 0.1557 score: 0.9457 time: 0.17s
Test loss: 0.2139 score: 0.9225 time: 0.17s
Epoch 234/1000, LR 0.000251
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.25s
Val loss: 0.1555 score: 0.9457 time: 0.18s
Test loss: 0.2140 score: 0.9225 time: 0.16s
Epoch 235/1000, LR 0.000250
Train loss: 0.5518;  Loss pred: 0.5518; Loss self: 0.0000; time: 0.25s
Val loss: 0.1552 score: 0.9457 time: 0.19s
Test loss: 0.2147 score: 0.9225 time: 0.26s
Epoch 236/1000, LR 0.000250
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 0.37s
Val loss: 0.1550 score: 0.9457 time: 0.27s
Test loss: 0.2148 score: 0.9225 time: 0.26s
Epoch 237/1000, LR 0.000250
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.37s
Val loss: 0.1549 score: 0.9457 time: 0.27s
Test loss: 0.2142 score: 0.9225 time: 0.26s
Epoch 238/1000, LR 0.000250
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.37s
Val loss: 0.1548 score: 0.9457 time: 0.22s
Test loss: 0.2141 score: 0.9225 time: 0.19s
Epoch 239/1000, LR 0.000249
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.27s
Val loss: 0.1549 score: 0.9457 time: 0.19s
Test loss: 0.2135 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.25s
Val loss: 0.1548 score: 0.9457 time: 0.17s
Test loss: 0.2134 score: 0.9302 time: 0.17s
Epoch 241/1000, LR 0.000249
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.25s
Val loss: 0.1547 score: 0.9457 time: 0.18s
Test loss: 0.2134 score: 0.9302 time: 0.17s
Epoch 242/1000, LR 0.000248
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.30s
Val loss: 0.1547 score: 0.9457 time: 0.26s
Test loss: 0.2129 score: 0.9380 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.37s
Val loss: 0.1546 score: 0.9457 time: 0.19s
Test loss: 0.2130 score: 0.9380 time: 0.18s
Epoch 244/1000, LR 0.000248
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.26s
Val loss: 0.1547 score: 0.9380 time: 0.19s
Test loss: 0.2125 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.25s
Val loss: 0.1547 score: 0.9380 time: 0.18s
Test loss: 0.2122 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.25s
Val loss: 0.1546 score: 0.9380 time: 0.18s
Test loss: 0.2122 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.25s
Val loss: 0.1544 score: 0.9380 time: 0.18s
Test loss: 0.2125 score: 0.9380 time: 0.17s
Epoch 248/1000, LR 0.000247
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.25s
Val loss: 0.1542 score: 0.9457 time: 0.17s
Test loss: 0.2129 score: 0.9380 time: 0.16s
Epoch 249/1000, LR 0.000246
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.24s
Val loss: 0.1541 score: 0.9457 time: 0.17s
Test loss: 0.2128 score: 0.9380 time: 0.19s
Epoch 250/1000, LR 0.000246
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.25s
Val loss: 0.1541 score: 0.9380 time: 0.23s
Test loss: 0.2125 score: 0.9380 time: 0.20s
Epoch 251/1000, LR 0.000246
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.26s
Val loss: 0.1540 score: 0.9457 time: 0.19s
Test loss: 0.2132 score: 0.9380 time: 0.17s
Epoch 252/1000, LR 0.000245
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.26s
Val loss: 0.1539 score: 0.9457 time: 0.18s
Test loss: 0.2139 score: 0.9302 time: 0.18s
Epoch 253/1000, LR 0.000245
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 0.27s
Val loss: 0.1539 score: 0.9457 time: 0.20s
Test loss: 0.2141 score: 0.9302 time: 0.18s
Epoch 254/1000, LR 0.000245
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.27s
Val loss: 0.1539 score: 0.9457 time: 0.20s
Test loss: 0.2142 score: 0.9302 time: 0.18s
Epoch 255/1000, LR 0.000244
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.26s
Val loss: 0.1539 score: 0.9457 time: 0.19s
Test loss: 0.2138 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.29s
Val loss: 0.1540 score: 0.9380 time: 0.19s
Test loss: 0.2131 score: 0.9380 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.39s
Val loss: 0.1542 score: 0.9380 time: 0.18s
Test loss: 0.2124 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.27s
Val loss: 0.1544 score: 0.9380 time: 0.18s
Test loss: 0.2119 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.26s
Val loss: 0.1547 score: 0.9380 time: 0.31s
Test loss: 0.2115 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.26s
Val loss: 0.1547 score: 0.9380 time: 0.19s
Test loss: 0.2114 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.26s
Val loss: 0.1544 score: 0.9380 time: 0.20s
Test loss: 0.2121 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.34s
Val loss: 0.1543 score: 0.9380 time: 0.20s
Test loss: 0.2123 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.26s
Val loss: 0.1542 score: 0.9380 time: 0.19s
Test loss: 0.2127 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.25s
Val loss: 0.1540 score: 0.9380 time: 0.17s
Test loss: 0.2132 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.25s
Val loss: 0.1540 score: 0.9380 time: 0.17s
Test loss: 0.2136 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.25s
Val loss: 0.1539 score: 0.9457 time: 0.17s
Test loss: 0.2144 score: 0.9380 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.25s
Val loss: 0.1540 score: 0.9457 time: 0.21s
Test loss: 0.2151 score: 0.9380 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 268/1000, LR 0.000240
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.25s
Val loss: 0.1540 score: 0.9457 time: 0.18s
Test loss: 0.2152 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 269/1000, LR 0.000240
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.26s
Val loss: 0.1540 score: 0.9457 time: 0.25s
Test loss: 0.2150 score: 0.9380 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 270/1000, LR 0.000240
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.28s
Val loss: 0.1541 score: 0.9380 time: 0.21s
Test loss: 0.2138 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 271/1000, LR 0.000239
Train loss: 0.5353;  Loss pred: 0.5353; Loss self: 0.0000; time: 0.28s
Val loss: 0.1542 score: 0.9380 time: 0.20s
Test loss: 0.2133 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 272/1000, LR 0.000239
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.28s
Val loss: 0.1544 score: 0.9380 time: 0.19s
Test loss: 0.2123 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 273/1000, LR 0.000239
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 0.26s
Val loss: 0.1545 score: 0.9380 time: 0.18s
Test loss: 0.2122 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 274/1000, LR 0.000238
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.27s
Val loss: 0.1545 score: 0.9380 time: 0.28s
Test loss: 0.2124 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 253,   Train_Loss: 0.5440,   Val_Loss: 0.1539,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.1539,   Test_Precision: 0.9825,   Test_Recall: 0.8750,   Test_accuracy: 0.9256,   Test_Score: 0.9302,   Test_loss: 0.2142


[0.18550081993453205, 0.17881224979646504, 0.17573861312121153, 0.2577355320099741, 0.2022567328531295, 0.17211303091607988, 0.166310572065413, 0.17223499296233058, 0.1801803819835186, 0.18138296413235366, 0.18385094893164933, 0.17019705404527485, 0.2469776349607855, 0.22974095190875232, 0.15848247497342527, 0.17440710798837245, 0.17386351386085153, 0.17969722906127572, 0.1796211798209697, 0.17906930297613144, 0.17334501794539392, 0.1796172980684787, 0.1747308778576553, 0.16008602804504335, 0.1703171960543841, 0.1642515070270747, 0.1655973067972809, 0.16576109500601888, 0.16391611401923, 0.17513206996954978, 0.198749439092353, 0.166643472854048, 0.16440593986772, 0.2519094788003713, 0.2454936089925468, 0.2414765718858689, 0.1716188699938357, 0.18237916403450072, 0.24969004304148257, 0.2515728371217847, 0.2983441250398755, 0.1735198860988021, 0.1800017210189253, 0.17322517721913755, 0.17736758897081017, 0.2539075929671526, 0.256005696952343, 0.21315478906035423, 0.18524248898029327, 0.18239466985687613, 0.262146086897701, 0.26021554390899837, 0.18385815899819136, 0.17073560901917517, 0.18151607899926603, 0.18426130688749254, 0.1716756538953632, 0.1702865760307759, 0.20834450190886855, 0.19112535985186696, 0.17619286989793181, 0.17856394685804844, 0.1769043488893658, 0.17274278309196234, 0.18005245691165328, 0.1758513138629496, 0.17481690202839673, 0.17525833495892584, 0.18266787007451057, 0.18278150400146842, 0.2582981828600168, 0.2172350890468806, 0.18220732104964554, 0.1808900791220367, 0.1837259188760072, 0.18281870894134045, 0.19448036095127463, 0.23501229286193848, 0.18756187101826072, 0.18166669085621834, 0.17910037911497056, 0.1715611049439758, 0.1838181670755148, 0.1751820920035243, 0.271491696825251, 0.18272928497754037, 0.18109903600998223, 0.26629261393100023, 0.26028010388836265, 0.2623029858805239, 0.1870569819584489, 0.18665865994989872, 0.1815332469996065, 0.18404843588359654, 0.18266013101674616, 0.17633383790962398, 0.17049042391590774, 0.2610649422276765, 0.17873506899923086, 0.1685474650003016, 0.18420575582422316, 0.18685633200220764, 0.27880440605804324, 0.18067441089078784, 0.16723427292890847, 0.1812123078852892, 0.24023600085638463, 0.16767417290247977, 0.1707209530286491, 0.17213853588327765, 0.17620230000466108, 0.2123954410199076, 0.22854979988187551, 0.18016470689326525, 0.18492684420198202, 0.18200580193661153, 0.1706594240386039, 0.18210552306845784, 0.18528857314959168, 0.1781629470642656, 0.17862609191797674, 0.18180767120793462, 0.18767557595856488, 0.1813076580874622, 0.2069702590815723, 0.18682906194590032, 0.17848302493803203, 0.1987478209193796, 0.24616809701547027, 0.1744152179453522, 0.1703364858403802, 0.176741466159001, 0.17538737412542105, 0.17286431905813515, 0.17562659294344485, 0.17685328307561576, 0.2559926831163466, 0.2536992128007114, 0.18110278784297407, 0.17706706700846553, 0.17453126003965735, 0.24760881578549743, 0.24962356011383235, 0.2624865039251745, 0.26454356289468706, 0.1914407960139215, 0.17118542804382741, 0.18134447210468352, 0.18281584000214934, 0.1835720429662615, 0.18879491998814046, 0.18703188421204686, 0.18715200200676918, 0.18853451195172966, 0.18700764421373606, 0.18035285198129714, 0.1802933090366423, 0.17964144004508853, 0.17333353590220213, 0.18540627998299897, 0.17697920300997794, 0.1782700449693948, 0.17348658293485641, 0.24418295198120177, 0.2518044509924948, 0.25598523393273354, 0.16908934502862394, 0.17368543916381896, 0.17799440305680037, 0.18218423798680305, 0.20566654298454523, 0.25751592009328306, 0.17747252387925982, 0.17573684989474714, 0.17484365799464285, 0.1744628290180117, 0.17125962884165347, 0.25380060193128884, 0.17809432791545987, 0.1718235390726477, 0.17202024697326124, 0.17807015497237444, 0.17170979408547282, 0.1765435580164194, 0.17278189095668495, 0.17237219284288585, 0.17188983992673457, 0.18410809198394418, 0.18117198208346963, 0.1722851269878447, 0.22295977198518813, 0.1733949889894575, 0.17004524497315288, 0.17876523197628558, 0.17158495704643428, 0.2559531629085541, 0.22993701579980552, 0.16811464610509574, 0.16746042692102492, 0.18059596722014248, 0.18346759700216353, 0.17166436091065407, 0.17095946590416133, 0.17611711099743843, 0.17317450512200594, 0.16605116985738277, 0.19472197513096035, 0.26222699298523366, 0.25810577697120607, 0.18595697521232069, 0.1793225659057498, 0.17529605003073812, 0.17913555004633963, 0.17679625586606562, 0.18948329705744982, 0.1835967020597309, 0.1707203600089997, 0.17111023585312068, 0.16761534404940903, 0.18101090611889958, 0.17756476090289652, 0.17675953591242433, 0.19757452001795173, 0.17322861216962337, 0.17119141900911927, 0.17452029697597027, 0.17029769299551845, 0.17299293586984277, 0.17455594101920724, 0.17398693319410086, 0.17272941279225051, 0.1763999629765749, 0.22262349678203464, 0.2139774188399315, 0.18146938900463283, 0.17692648898810148, 0.1807891079224646, 0.19723347993567586, 0.2580507849343121, 0.20674447296187282, 0.18569155992008746, 0.19239081302657723, 0.1749525482300669, 0.18949348898604512, 0.22923872503452003, 0.1755779990926385, 0.1865103088784963, 0.17524691694416106, 0.18264123587869108, 0.19052681792527437, 0.2119445251300931, 0.17786865797825158, 0.17893137596547604, 0.17846690490841866, 0.17551181698217988, 0.2580480619799346, 0.26010519498959184, 0.1866161951329559, 0.19031167193315923, 0.17996056308038533, 0.1774817209225148, 0.1862564340699464, 0.1772159629035741, 0.24158332613296807, 0.19553559413179755, 0.19151569297537208, 0.2729948149062693, 0.2728574341163039, 0.2816914359573275, 0.18495361902751029, 0.18312097596935928, 0.18133807787671685, 0.18411702709272504, 0.1820234141778201, 0.25595991499722004, 0.18304675980471075, 0.17512629297561944, 0.25368777313269675, 0.25392396794632077, 0.2580220850650221, 0.17654875293374062, 0.18604496796615422, 0.17178136110305786, 0.17708529205992818, 0.1751048678997904, 0.1834209298249334, 0.17630506493151188, 0.17604802502319217, 0.18446787516586483, 0.18562750495038927, 0.25264889816753566, 0.17205475619994104, 0.1711400339845568, 0.1727178420405835, 0.17171857506036758, 0.17442130902782083, 0.20533895585685968, 0.19190697302110493, 0.17884392710402608, 0.1738188900053501, 0.2518704270478338, 0.23945941892452538, 0.1741945561952889, 0.22102413792163134, 0.1781378409359604, 0.2562004930805415, 0.17736345413140953, 0.17874477989971638, 0.1766625780146569, 0.1788401089143008, 0.1995390320662409, 0.18516373983584344, 0.1823311229236424, 0.18149121780879796, 0.1769669100176543, 0.17595831002108753, 0.17889089905656874, 0.17528471793048084, 0.1741878530010581, 0.17528234492056072, 0.17819887399673462, 0.2491455008275807, 0.2520424018148333, 0.17948159901425242, 0.1839366969652474, 0.17395076411776245, 0.18474864400923252, 0.1836959079373628, 0.16979615413583815, 0.19004696211777627, 0.1743371351622045, 0.17799349292181432, 0.18063295306637883, 0.25775938108563423, 0.25170430005528033, 0.17178659397177398, 0.17136393091641366, 0.1836027349345386, 0.1827275101095438, 0.23441973398439586, 0.25793682411313057, 0.2428867369890213, 0.19541060994379222, 0.19513134798035026, 0.18671209388412535, 0.18771112989634275, 0.24483182816766202, 0.18081361101940274, 0.17977983807213604, 0.25385904987342656, 0.2578676789999008, 0.17892734403721988, 0.18731484096497297, 0.17540455609560013, 0.1870598099194467, 0.2826372650451958, 0.273664278909564, 0.18143965094350278, 0.18970200698822737, 0.18698874511756003, 0.2532944530248642, 0.17386857490055263, 0.2124169380404055, 0.18575779302045703, 0.1734295340720564, 0.17131974501535296, 0.17338656797073781, 0.17154585500247777, 0.18662565294653177, 0.2559188478626311, 0.17275197082199156, 0.24819549615494907, 0.2516547569539398, 0.25789276603609324, 0.23396851192228496, 0.17394864209927619, 0.1747530100401491, 0.17457638285122812, 0.17835925403051078, 0.17381290486082435, 0.17381499311886728, 0.17778858984820545, 0.17591822193935513, 0.17575586307793856, 0.18484813789837062, 0.1835363400168717, 0.20741556282155216, 0.24064873601309955, 0.17527725780382752, 0.17036231979727745, 0.18218352808617055, 0.18191710812970996, 0.1806134250946343, 0.2171987690962851, 0.18087451090104878, 0.17922521894797683, 0.1757134550716728, 0.18301929696463048, 0.25799153093248606, 0.18556735781021416, 0.18065059185028076, 0.2371318368241191, 0.2452766161877662, 0.2515466879121959, 0.1692174430936575, 0.17246132204309106, 0.17548922798596323, 0.17128448095172644, 0.2626408620271832, 0.17279089614748955, 0.17293653590604663, 0.1742003569379449, 0.1725699428934604, 0.17794292187318206, 0.1758512209635228, 0.21863044914789498, 0.270936856046319, 0.18388434499502182, 0.24743138789199293, 0.25151279708370566, 0.2496237640734762, 0.1712364531122148, 0.17392198694869876, 0.17388543207198381, 0.17877988307736814, 0.17019603098742664, 0.1776791859883815, 0.17421392095275223, 0.17846848210319877, 0.18023592210374773, 0.17611114913597703, 0.17901504784822464, 0.2537353939842433, 0.17392689594998956, 0.17309205001220107, 0.17414473090320826, 0.17518595699220896, 0.1732407060917467, 0.24212972121313214, 0.17506200820207596, 0.17119618598371744, 0.17159436200745404, 0.1854628291912377, 0.18660645792260766, 0.2673787400126457, 0.18534455494955182, 0.2421274660155177, 0.17499829805456102, 0.25071236910298467, 0.2580100509803742, 0.25318739213980734, 0.17544849682599306, 0.17864723200909793, 0.17602591612376273, 0.17477965308353305, 0.23975294292904437, 0.2549684988334775, 0.1714331889525056, 0.16904307692311704, 0.25949396495707333, 0.2643295209854841, 0.2665666099637747, 0.19757532887160778, 0.1848187129944563, 0.17126381606794894, 0.17829404002986848, 0.2621964728459716, 0.18008065992034972, 0.17856100806966424, 0.1802681158296764, 0.17251577321439981, 0.17282319604419172, 0.16651584999635816, 0.19459276902489364, 0.20056068012490869, 0.17295738798566163, 0.1801064710598439, 0.18168204301036894, 0.18079151189886034, 0.1816691409330815, 0.24750289507210255, 0.17800687602721155, 0.17522952798753977, 0.17230025096796453, 0.19023019610904157, 0.1931332158856094, 0.18348581693135202, 0.1826104549691081, 0.17597109614871442, 0.17038143100216985, 0.16882831207476556, 0.2247430521529168, 0.196894426131621, 0.2282135549467057, 0.19376973202452064, 0.193491404177621, 0.1876798931043595, 0.18744920985773206, 0.20055786496959627]
[0.0014379908522056748, 0.0013861414712904266, 0.0013623148303969887, 0.0019979498605424352, 0.0015678816500242598, 0.0013342095419851153, 0.001289229240817155, 0.001335154984204113, 0.0013967471471590589, 0.0014060694893980904, 0.0014252011545089094, 0.0013193570081029059, 0.0019145553097735311, 0.0017809376116957544, 0.0012285463176234517, 0.0013519930851811817, 0.0013477791772159033, 0.001393001775668804, 0.0013924122466741837, 0.0013881341315979182, 0.0013437598290340614, 0.0013923821555696023, 0.0013545029291291107, 0.0012409769615894834, 0.0013202883415068536, 0.0012732674963339124, 0.0012837000526921, 0.0012849697287288285, 0.0012706675505366667, 0.001357612945500386, 0.00154069332629731, 0.0012918098670856436, 0.0012744646501373644, 0.0019527866573672192, 0.0019030512325003627, 0.001871911409967976, 0.0013303788371615169, 0.0014137919692596955, 0.0019355817290037409, 0.0019501770319518192, 0.0023127451553478723, 0.001345115396114745, 0.0013953621784412815, 0.001342830831156105, 0.0013749425501613192, 0.001968275914474051, 0.001984540286452271, 0.0016523627058942188, 0.0014359882866689401, 0.0014139121694331483, 0.00203214020850931, 0.0020171747589844834, 0.0014252570464976075, 0.001323531852861823, 0.0014071013875912096, 0.0014283822239340507, 0.0013308190224446761, 0.001320050976982759, 0.0016150736582082833, 0.001481591936836178, 0.0013658362007591613, 0.001384216642310453, 0.0013713515417780294, 0.0013390913417981576, 0.0013957554799352968, 0.001363188479557749, 0.0013551697831658662, 0.001358591743867642, 0.0014160300005776013, 0.0014169108837323134, 0.00200231149503889, 0.0016839929383479116, 0.0014124598530980276, 0.0014022486753646256, 0.0014242319292713736, 0.001417199294118918, 0.0015075996972967027, 0.0018218007198599882, 0.001453967992389618, 0.0014082689213660337, 0.0013883750318989965, 0.0013299310460773317, 0.001424947031593138, 0.0013580007132056148, 0.002104586797094969, 0.0014165060850972121, 0.0014038684962014126, 0.0020642838289224824, 0.0020176752239407956, 0.002033356479693984, 0.001450054123708906, 0.0014469663562007653, 0.0014072344728651666, 0.0014267320611131514, 0.0014159700078817533, 0.0013669289760435967, 0.0013216311931465716, 0.002023759242075012, 0.0013855431705366733, 0.001306569496126369, 0.0014279515955366137, 0.001448498697691532, 0.0021612744655662266, 0.0014005768286107585, 0.0012963897126271975, 0.0014047465727541797, 0.0018622945802820514, 0.0012997997899417035, 0.0013234182405321634, 0.001334407254909129, 0.0013659093023617138, 0.0016464762869760278, 0.0017717038750532986, 0.0013966256348315136, 0.0014335414279223412, 0.0014108976894310972, 0.0013229412716170845, 0.0014116707214609134, 0.0014363455282914083, 0.0013811081167772528, 0.00138469838696106, 0.0014093617923095706, 0.0014548494260353867, 0.001405485721608234, 0.001604420613035444, 0.001448287301906204, 0.0013835893406048995, 0.001540680782320772, 0.001908279821825351, 0.0013520559530647456, 0.0013204378747316294, 0.001370088884953496, 0.0013595920474838842, 0.0013400334810708152, 0.001361446456925929, 0.0013709556827567113, 0.001984439404002687, 0.0019666605643466, 0.0014038975801780936, 0.0013726129225462443, 0.0013529555041833904, 0.001919448184383701, 0.0019350663574715687, 0.002034779100195151, 0.002050725293757264, 0.0014840371784024923, 0.0013270188220451738, 0.001405771101586694, 0.0014171770542802274, 0.001423039092761717, 0.0014635265115359726, 0.001449859567535247, 0.001450790713230769, 0.0014615078445870517, 0.0014496716605715972, 0.0013980841238860242, 0.001397622550671646, 0.0013925693026751048, 0.0013436708209473033, 0.001437257984364333, 0.0013719318062788987, 0.0013819383330960838, 0.0013448572320531504, 0.001892891100629471, 0.0019519724883139133, 0.0019843816583932834, 0.0013107701165009607, 0.0013463987532078988, 0.0013798015740837239, 0.0014122809146263802, 0.001594314286701901, 0.0019962474425835896, 0.0013757559990640296, 0.001362301161974784, 0.0013553771937569214, 0.0013524250311473774, 0.0013275940220283215, 0.0019674465265991383, 0.0013805761853911617, 0.0013319654191678116, 0.0013334902866144283, 0.0013803887982354608, 0.0013310836750811846, 0.0013685547133055768, 0.0013393945035401933, 0.0013362185491696578, 0.001332479379277012, 0.0014271945115034432, 0.0014044339696392995, 0.0013355436200608117, 0.0017283703254665746, 0.0013441472014686627, 0.0013181801935903323, 0.0013857769920642292, 0.0013301159460963898, 0.0019841330458027447, 0.0017824574868201977, 0.001303214310892215, 0.0012981428443490303, 0.0013999687381406395, 0.0014222294341252988, 0.001330731479927551, 0.0013252671775516383, 0.001365248922460763, 0.0013424380242015965, 0.0012872183709874633, 0.001509472675433801, 0.0020327673874824316, 0.0020008199765209774, 0.0014415269396303929, 0.0013900974101220915, 0.0013588841087654117, 0.0013886476747778265, 0.0013705136113648499, 0.0014688627678872078, 0.0014232302485250455, 0.0013234136434806178, 0.0013264359368458968, 0.0012993437523210003, 0.0014031853187511595, 0.0013764710147511358, 0.0013702289605614288, 0.0015315854264957498, 0.0013428574586792509, 0.0013270652636365836, 0.001352870519193568, 0.001320137155004019, 0.0013410305106189362, 0.0013531468296062578, 0.00134873591623334, 0.0013389876960639574, 0.0013674415734618208, 0.001725763540946005, 0.0016587396809297015, 0.0014067394496483166, 0.0013715231704503992, 0.0014014659528873226, 0.0015289417049277197, 0.002000393681661334, 0.0016026703330377738, 0.001439469456744864, 0.0014914016513688157, 0.0013562213041090458, 0.0014689417750856212, 0.0017770443801125584, 0.0013610697604080502, 0.0014458163478953201, 0.0013585032321252796, 0.0014158235339433417, 0.0014769520769401113, 0.001642980814961962, 0.001378826806032958, 0.0013870649299649305, 0.0013834643791350283, 0.001360556720792092, 0.002000372573487865, 0.0020163193410045878, 0.0014466371715733015, 0.001475284278551622, 0.0013950431246541497, 0.001375827293972983, 0.0014438483261236155, 0.0013737671542912722, 0.0018727389622710702, 0.001515779799471299, 0.0014846177750028843, 0.002116238875242398, 0.002115173907878325, 0.0021836545423048644, 0.001433748984709382, 0.0014195424493748782, 0.0014057215339280376, 0.0014272637759125972, 0.0014110342184327138, 0.0019841853875753493, 0.0014189671302690756, 0.001357568162601701, 0.0019665718847495873, 0.0019684028522970604, 0.002000171202054435, 0.0013685949839824854, 0.0014422090540011955, 0.0013316384581632393, 0.0013727542020149472, 0.0013574020767425613, 0.0014218676730614994, 0.0013667059297016425, 0.0013647133722728075, 0.0014299835284175568, 0.0014389729065921648, 0.001958518590446013, 0.001333757799999543, 0.0013266669301128433, 0.0013388980003146006, 0.0013311517446540123, 0.001352103170758301, 0.0015917748516035635, 0.0014876509536519763, 0.0013863870318141557, 0.0013474332558554272, 0.001952483930603363, 0.0018562745653063983, 0.0013503453968627046, 0.0017133654102452042, 0.0013809134956276, 0.0019860503339576858, 0.0013749104971427094, 0.0013856184488350107, 0.0013694773489508288, 0.0013863574334441922, 0.001546814202063883, 0.0014353778281848328, 0.0014134195575476155, 0.0014069086651844803, 0.001371836511764762, 0.0013640179071402133, 0.001386751155477277, 0.0013587962630269832, 0.0013502934341167296, 0.0013587778676012459, 0.0013813866201297256, 0.0019313604715316332, 0.001953817068332041, 0.0013913302249166854, 0.0014258658679476542, 0.0013484555357966082, 0.0014321600310793219, 0.0014239992863361456, 0.0013162492568669624, 0.001473232264478886, 0.001351450660172128, 0.0013797945187737544, 0.0014002554501269676, 0.00199813473709794, 0.0019511961244595374, 0.0013316790230370075, 0.0013284025652435167, 0.0014232770149964233, 0.0014164923264305721, 0.0018172072401891153, 0.0019995102644428725, 0.001882842922395514, 0.001514810929796839, 0.0015126461083748081, 0.0014473805727451578, 0.0014551250379561454, 0.0018979211485865273, 0.001401655899375215, 0.0013936421555979539, 0.0019678996114219113, 0.0019989742558131846, 0.0013870336747071308, 0.0014520530307362246, 0.0013597252410511638, 0.0014500760458871838, 0.002190986550737952, 0.0021214285186787907, 0.0014065089220426572, 0.0014705581937071888, 0.0014495251559500778, 0.001963522891665614, 0.0013478184100818033, 0.0016466429305457791, 0.00143998289163145, 0.001344414992806639, 0.0013280600388787052, 0.0013440819222537816, 0.0013298128294765717, 0.0014467104879576107, 0.0019838670376948146, 0.0013391625645115626, 0.0019239960942244113, 0.001950812069410386, 0.0019991687289619632, 0.001813709394746395, 0.0013484390860409006, 0.0013546744964352644, 0.001353305293420373, 0.0013826298762055099, 0.0013473868593862352, 0.0013474030474330797, 0.0013782061228543058, 0.0013637071468167064, 0.0013624485509917718, 0.0014329313015377567, 0.0014227623257121837, 0.0016078725800120323, 0.0018654940776209267, 0.0013587384325878104, 0.001320638137963391, 0.001412275411520702, 0.0014102101405403873, 0.001400104070501041, 0.0016837113883432953, 0.0014021279914809982, 0.001389342782542456, 0.0013621198067571534, 0.0014187542400358952, 0.0019999343483138453, 0.0014385066496915826, 0.0014003921848858975, 0.0018382312932102255, 0.0019013691177346217, 0.0019499743249007436, 0.0013117631247570349, 0.0013369094732022563, 0.0013603816122942885, 0.001327786674044391, 0.002035975674629327, 0.0013394643112208493, 0.0013405933015972608, 0.001350390363860038, 0.0013377514952981426, 0.001379402495140946, 0.0013631877594071533, 0.0016948096833170154, 0.0021002857057854187, 0.0014254600387210994, 0.0019180727743565342, 0.0019497116053000438, 0.001935067938554079, 0.0013274143652109674, 0.001348232456966657, 0.001347949085829332, 0.001385890566491226, 0.001319349077421912, 0.0013773580309176861, 0.0013504955112616452, 0.0013834766054511532, 0.0013971776907267266, 0.0013652027064804421, 0.0013877135492110437, 0.001966941038637545, 0.0013482705112402291, 0.0013417988373038842, 0.0013499591542884362, 0.001358030674358209, 0.0013429512100135402, 0.001876974583047536, 0.0013570698310238446, 0.001327102216928042, 0.0013301888527709614, 0.001437696350319672, 0.00144656168932254, 0.002072703410950742, 0.00143677949573296, 0.001876957100895486, 0.0013565759539113256, 0.0019435067372324394, 0.002000077914576544, 0.001962692962324088, 0.0013600658668681633, 0.0013848622636364182, 0.001364541985455525, 0.001354881031655295, 0.0018585499451863904, 0.00197649999095719, 0.0013289394492442294, 0.0013104114490164111, 0.0020115811236982428, 0.0020490660541510396, 0.002066407829176548, 0.00153159169667913, 0.0014327032015074132, 0.001327626481146891, 0.0013821243413168099, 0.002032530797255594, 0.0013959741079096878, 0.001384193861005149, 0.0013974272544936153, 0.001337331575305425, 0.00133971469801699, 0.0012908205426074276, 0.0015084710777123539, 0.0015547339544566565, 0.001340754945625284, 0.0013961741942623558, 0.0014083879303129375, 0.0014014845883632584, 0.0014082879142099341, 0.0019186270935821902, 0.0013798982637768338, 0.0013583684340119362, 0.0013356608602167793, 0.001474652683015826, 0.0014971567122915455, 0.0014223706738864497, 0.0014155849222411481, 0.001364117024408639, 0.001320786286838526, 0.0013087466052307407, 0.001742194202735789, 0.001526313380865279, 0.0017690973251682613, 0.0015020909459265166, 0.0014999333657179923, 0.0014548828922818567, 0.0014530946500599384, 0.0015547121315472579]
[695.4147159324007, 721.4270842564513, 734.0447139583679, 500.5130607874735, 637.8032423458283, 749.5074563116534, 775.6572441423743, 748.9767194301422, 715.9491981307923, 711.2024032525445, 701.6553395542093, 757.9449639926444, 522.3145003412244, 561.5019826819372, 813.9701252244517, 739.6487533558569, 741.961307093119, 717.8741746541442, 718.1781131188184, 720.3914789191685, 744.1806031058636, 718.193633838201, 738.2782115081571, 805.816732261627, 757.4103084623875, 785.3809218245775, 778.9981763285426, 778.2284497777716, 786.987910077384, 736.5869656107487, 649.0584355312697, 774.1077270574082, 784.6431832316557, 512.0887098584632, 525.4719278819044, 534.2133151574238, 751.6655948418347, 707.3176406028324, 516.640545328307, 512.7739603204935, 432.38659377910784, 743.4306401431562, 716.6598145272011, 744.6954424922266, 727.3031152339216, 508.0588512242264, 503.89503646090395, 605.1940027651641, 696.3845104333675, 707.2575097793451, 492.0920297785735, 495.74286786308744, 701.6278238773673, 755.5541620231788, 710.680842772731, 700.0927225527911, 751.4169719057885, 757.5465019432056, 619.1668069859869, 674.949677530927, 732.1522152101243, 722.4302680907367, 729.2076244020169, 746.7750472176011, 716.4578712930127, 733.5742745745797, 737.9149184273133, 736.0562910188146, 706.199727118845, 705.7606879028834, 499.4227933454367, 593.8267181696464, 707.9847245262539, 713.1402707440396, 702.1328334575342, 705.6170604584631, 663.3060498706079, 548.9074568358133, 687.7730494991745, 710.0916485680809, 720.2664820558005, 751.9186825132983, 701.7804717147745, 736.3766383004764, 475.15265294847103, 705.9623749737523, 712.3174305184567, 484.4295081854036, 495.61990360711434, 491.7976803312413, 689.6294308258158, 691.1010720564749, 710.613632114891, 700.9024520131618, 706.229647827053, 731.5669047373432, 756.640736981378, 494.1299237624109, 721.7386085578713, 765.3630388316382, 700.3038500224563, 690.3699682945501, 462.689961840646, 713.9915351819055, 771.3729831853192, 711.87217637369, 536.9719756412255, 769.3492549686058, 755.6190245631549, 749.3964052736646, 732.1130314223342, 607.3576691691277, 564.4284093299265, 716.0114887341576, 697.5731433512312, 708.7686141177398, 755.8914529725569, 708.3804918509023, 696.2113087019812, 724.0562761541438, 722.1789303840084, 709.5410173999857, 687.3563559942435, 711.4978008141876, 623.2779558398184, 690.4707364925605, 722.7578087316025, 649.0637200612518, 524.0321616163491, 739.6143611758597, 757.3245353956873, 729.8796530518108, 735.5147463907578, 746.2500110078624, 734.5129108183547, 729.4181807461526, 503.9206528468259, 508.4761540089346, 712.3026737271985, 728.5375094276145, 739.1226074382798, 520.9830659331299, 516.7781436222364, 491.45383884869483, 487.63235282861143, 673.8375659001082, 753.568814087219, 711.3533624864673, 705.6281337464159, 702.7213834718218, 683.2810968012454, 689.7219719700124, 689.2792949943124, 684.2248597595107, 689.8113739808541, 715.2645416074567, 715.5007620043315, 718.0971159417452, 744.2298994741797, 695.7693127321728, 728.8992028782449, 723.6212905098362, 743.5733520005926, 528.2924092503025, 512.3023024078511, 503.9353169640166, 762.9102825974192, 742.722018731392, 724.7418895460123, 708.0744274339717, 627.2289023192931, 500.9399028738526, 726.8730797324029, 734.0520788739589, 737.8019968213689, 739.4125197103272, 753.2423191181462, 508.2730262197094, 724.3352526153185, 750.7702419367481, 749.91172417077, 724.4335807986065, 751.2675714687948, 730.697859776919, 746.6060203747816, 748.380570395024, 750.4806570009274, 700.6753402846081, 712.0306270125535, 748.7587713192526, 578.5797090273749, 743.9661362292499, 758.6216246174176, 721.6168299276043, 751.8141579572739, 503.9984602420741, 561.0231982497057, 767.3335012070067, 770.3312500262344, 714.3016645701277, 703.1214345630677, 751.4664040670647, 754.5648280880597, 732.4671593203472, 744.9133456978332, 776.8689622048125, 662.4830089836599, 491.9402023851303, 499.7950898804991, 693.7088530973966, 719.3740472562787, 735.8979279760148, 720.125067116101, 729.6534610875791, 680.7987933674609, 702.6270001191605, 755.6216493053297, 753.8999602030373, 769.6192775881775, 712.6642408787486, 726.4955013824238, 729.8050389989322, 652.918200121549, 744.6806759248567, 753.5424424113702, 739.1690378441313, 757.4970496129666, 745.695188947239, 739.0181007119403, 741.4349895810097, 746.8328521162412, 731.2926704929672, 579.4536599445333, 602.8673525429339, 710.8636928110597, 729.1163733468729, 713.5385614896916, 654.0471731374969, 499.90159895401007, 623.9586391448046, 694.700394867248, 670.5101868984757, 737.3427898309994, 680.7621765278698, 562.7321473742034, 734.716198308747, 691.6507767087455, 736.1042479343776, 706.302710772724, 677.0700387732003, 608.6498338224064, 725.25424920996, 720.9467836702376, 722.8230918567065, 734.9932455721638, 499.9068739761775, 495.9531854223902, 691.2583332228648, 677.8354616384593, 716.8237184409003, 726.8354134131874, 692.5935237842875, 727.9253961461184, 533.9772494439375, 659.7264327897747, 673.5740450083573, 472.5364474204067, 472.7743644507574, 457.94789451654395, 697.4721591190511, 704.4523398651224, 711.3784457762973, 700.6413368549179, 708.7000350074683, 503.9851650263326, 704.7379595116994, 736.6112638378007, 508.49908297521233, 508.02608766443996, 499.9572031498456, 730.6763591154573, 693.3807531062491, 750.9545807045284, 728.4625306789711, 736.7013924126002, 703.300327411514, 731.6862964209895, 732.7545991101339, 699.3087543509095, 694.9401169534466, 510.58999637694035, 749.7613134861087, 753.7686945395874, 746.8828841069522, 751.2291547647079, 739.5885326111389, 628.2295508014805, 672.2006916643579, 721.299303190575, 742.1517879675184, 512.1681076734786, 538.7134094761133, 740.5512710476357, 583.6466605549645, 724.1583221297423, 503.5119115069244, 727.3200707087223, 721.6993977243678, 730.2055786217353, 721.3147027427505, 646.4900559263809, 696.6806790269234, 707.5040066199961, 710.7781938843027, 728.9498358033765, 733.1282051102909, 721.1099093375774, 735.9455035387758, 740.5797693551938, 735.9554669265965, 723.9102981221073, 517.769735241069, 511.81864270112686, 718.7366321031955, 701.3282402498126, 741.5891540014659, 698.2459908802006, 702.2475429555394, 759.7345220010052, 678.7795951195257, 739.9456224858661, 724.7455953722124, 714.1554063648354, 500.46675103220747, 512.506143008556, 750.9317055392332, 752.7838519467813, 702.6039129863367, 705.9692321241911, 550.2949679508931, 500.1224638767393, 531.1117502716129, 660.1483923370664, 661.093162811494, 690.9032902820862, 687.2261653916631, 526.8922793471941, 713.4418657573144, 717.5443107710398, 508.15600257039904, 500.2565676330829, 720.9630294024028, 688.6800818100816, 735.4426981343152, 689.6190050420295, 456.4153986537194, 471.3804831014491, 710.9802037712716, 680.0138915135756, 689.881093746565, 509.2886893474013, 741.939709770923, 607.2962033539053, 694.4526951060059, 743.8179471000778, 752.9780060578514, 744.0022690902512, 751.9855259582737, 691.2233016377362, 504.06603920490846, 746.7353303478384, 519.7515748612335, 512.6070397453714, 500.2079041718676, 551.3562442233623, 741.5982007285631, 738.1847097818948, 738.9315661897534, 723.2593604474977, 742.1773435251716, 742.1684268156341, 725.5808716978926, 733.2952696876995, 733.972669479568, 697.8701623217006, 702.8580824273906, 621.9398305757019, 536.051018331457, 735.9768267505553, 757.2096937486145, 708.0771865334862, 709.114174726331, 714.232621038049, 593.926017798074, 713.2016521143335, 719.7647784012162, 734.1498119616476, 704.8437085021149, 500.0164134603243, 695.1653648694645, 714.085675993314, 544.0011840151157, 525.9368055748406, 512.8272650722729, 762.332757436843, 747.9938021567999, 735.0878539981853, 753.1330292343089, 491.1650038166893, 746.5671101670146, 745.9383832580261, 740.5266112396857, 747.522991762481, 724.9515667273175, 733.5746621103003, 590.0367515264835, 476.1256991110369, 701.52790877055, 521.3566520360392, 512.8963674841073, 516.7777213792399, 753.3442655195831, 741.7118575010918, 741.8677830733832, 721.5576930664756, 757.9495200421565, 726.0276395482549, 740.4689550325057, 722.8167039903785, 715.7285767137186, 732.491955409353, 720.6098121392052, 508.40364828255207, 741.6909230478781, 745.2681968403917, 740.7631533319245, 736.3603922073332, 744.6286898166, 532.7722650225542, 736.8817559266995, 753.5214599481162, 751.7729515751587, 695.5571666977174, 691.2944033989482, 482.461694575638, 696.0010237965284, 532.7772273127102, 737.1500262235713, 514.5338479371591, 499.9805221146695, 509.5040432691356, 735.2585079594046, 722.0934718620798, 732.8466332724603, 738.0721824544793, 538.0538750599527, 505.9448543259111, 752.4797315398396, 763.1190957241677, 497.12138785709243, 488.02721511792146, 483.9315772426658, 652.9155271396728, 697.9812699153977, 753.2239031087527, 723.5239045477316, 491.9974651061824, 716.3456645319777, 722.4421579747781, 715.6007561641334, 747.7577127958084, 746.4275800513149, 774.7010269762388, 662.9228858113296, 643.1968615167197, 745.8484514734592, 716.2430047121251, 710.0316457396823, 713.5290736003474, 710.082071932721, 521.2060245292068, 724.6911067653366, 736.177295467993, 748.6930476031908, 678.1257793902294, 667.932749985405, 703.0516154186624, 706.4217655107574, 733.0749357325204, 757.1247596714759, 764.0898520792673, 573.9888230770644, 655.1734476920409, 565.2600259880494, 665.7386509863969, 666.6962832187663, 687.3405449366358, 688.1864164586602, 643.2058898291317]
Elapsed: 0.19473675566861678~0.031080923780210178
Time per graph: 0.0015095872532450913~0.00024093739364504016
Speed: 676.7076364465523~89.91391327371642
Total Time: 0.2020
best val loss: 0.15388567145018614 test_score: 0.9302

Testing...
Test loss: 0.2566 score: 0.9225 time: 0.21s
test Score 0.9225
Epoch Time List: [0.8239756787661463, 0.6236044911202043, 0.6331821989733726, 0.7595500519964844, 0.8482453180477023, 0.6088090999983251, 0.6496180091053247, 0.6217460769694299, 0.6414959239773452, 0.6955828811042011, 0.6305475670378655, 0.6140511280391365, 0.7810982340015471, 0.9098405940458179, 0.6253328800667077, 0.7103549810126424, 0.6144525369163603, 0.6261607820633799, 0.7403728240169585, 0.8069042132701725, 0.7039818640332669, 0.6274044478777796, 0.6319562022108585, 0.6077718730084598, 0.6086278920993209, 0.6744760458823293, 0.5840120769571513, 0.5775033691897988, 0.5803266412112862, 0.6173808411695063, 0.6415360688697547, 0.6634452647995204, 0.5952710779383779, 0.8803626713342965, 0.8622529450803995, 0.8690097869839519, 0.7460525771602988, 0.6345903708133847, 0.7680564159527421, 0.8936272521968931, 0.9535178251098841, 0.6384823969565332, 0.6465099221095443, 0.6171660223044455, 0.6212789013516158, 0.8357093741651624, 0.8920101597905159, 0.8591716901864856, 0.648965593893081, 0.6517131519503891, 0.8611799110658467, 0.912932557053864, 0.7318892430048436, 0.6145636541768909, 0.6246371557936072, 0.6446521619800478, 0.6023623659275472, 0.6043679108843207, 0.6955953948199749, 0.6919526541605592, 0.6144061929080635, 0.6417774681467563, 0.6362417410127819, 0.6294366419315338, 0.6680072690360248, 0.7484603878110647, 0.6260786920320243, 0.6285013589076698, 0.6494267389643937, 0.6553798092063516, 0.7968141629826277, 0.8774844631552696, 0.646077929995954, 0.6490584048442543, 0.6537276052404195, 0.6500776971224695, 0.6638517118990421, 0.7627610019408166, 0.6585549777373672, 0.6570739890448749, 0.6481804309878498, 0.6201347578316927, 0.6345694190822542, 0.6158518770243973, 0.7596582188270986, 0.6522200659383088, 0.6398014891892672, 0.803307123016566, 0.9047670061700046, 0.9042480438947678, 0.8195602307096124, 0.6509504611603916, 0.6492633558809757, 0.6422780039720237, 0.6522263651713729, 0.6530720512382686, 0.6776470129843801, 0.7032519681379199, 0.6360432712826878, 0.601852540159598, 0.6623046428430825, 0.6343062610831112, 0.799636181909591, 0.6404277591500431, 0.6151040629483759, 0.652504411060363, 0.7059083867352456, 0.6308846545871347, 0.6026614818256348, 0.6165637541562319, 0.6278122852090746, 0.6860151079017669, 0.8645860392134637, 0.704924781806767, 0.6335008840542287, 0.6308942709583789, 0.6110741279553622, 0.6322687782812864, 0.651691421167925, 0.6932510212063789, 0.635333722922951, 0.6406801079865545, 0.6393647838849574, 0.6420045951381326, 0.6643908920232207, 0.732912341831252, 0.6456972660962492, 0.6640320960432291, 0.8765742559917271, 0.6433778791688383, 0.6036749398335814, 0.6230798892211169, 0.695938667980954, 0.6151980138383806, 0.6206345860846341, 0.6237334469333291, 0.8002478370908648, 0.8897736098151654, 0.7645444849040359, 0.6256802210118622, 0.6202262958977371, 0.7715411582030356, 0.8878010818734765, 0.9020431230310351, 0.9107847211416811, 0.8177629171404988, 0.6311936478596181, 0.6369975300040096, 0.6541366190649569, 0.6461082701571286, 0.6461186690721661, 0.6482937510591, 0.6731798411346972, 0.7248378631193191, 0.647709395037964, 0.6428006319329143, 0.6390416282229125, 0.646687283180654, 0.6046730659436435, 0.6568674987647682, 0.7146894389297813, 0.6473342534154654, 0.6135214022360742, 0.717304821126163, 0.888358797878027, 0.9052669282536954, 0.7358207150828093, 0.605680878739804, 0.6313637411221862, 0.6470294140744954, 0.6743892948143184, 0.8503534540068358, 0.7194087677635252, 0.6168382270261645, 0.6314596091397107, 0.6155268729198724, 0.6231117912102491, 0.7831663859542459, 0.7641757891979069, 0.6200609146617353, 0.6099010270554572, 0.6142331941518933, 0.6120095746591687, 0.6412484492175281, 0.7934351381845772, 0.603149127913639, 0.5974100418388844, 0.725382428150624, 0.6142530750948936, 0.6932932510972023, 0.6594680009875447, 0.6176397919189185, 0.6404625850263983, 0.6069277813658118, 0.6120993439108133, 0.7925973229575902, 0.8405770200770348, 0.6251732830423862, 0.6072609222028404, 0.6201574197039008, 0.6400704202242196, 0.6067351647652686, 0.6108383378013968, 0.6555246589705348, 0.681946184951812, 0.6144063179381192, 0.6519754009786993, 0.8943191240541637, 0.9133193730376661, 0.7977412638720125, 0.6352967619895935, 0.636064640013501, 0.6356404989492148, 0.6431184522807598, 0.6530271170195192, 0.6593148258980364, 0.6987321241758764, 0.626027753110975, 0.610611867858097, 0.6353965988382697, 0.6271204901859164, 0.6825660460162908, 0.6571799630764872, 0.6587495189160109, 0.6076941550709307, 0.6156731639057398, 0.6724251343403012, 0.5952292159199715, 0.6017583741340786, 0.6193952669855207, 0.6315567381680012, 0.6156106491107494, 0.6814493809361011, 0.7199494889937341, 0.6152782540302724, 0.6094155588652939, 0.6189218659419566, 0.6390399720985442, 0.8013825949747115, 0.8310768560040742, 0.6346857619937509, 0.6464912141673267, 0.6062832460738719, 0.6676808786578476, 0.8079148412216455, 0.6051028280053288, 0.6240410751197487, 0.614067945163697, 0.6189588399138302, 0.6188382091931999, 0.711321973009035, 0.697069653077051, 0.616029336117208, 0.6216701250523329, 0.6131518222391605, 0.7885286097880453, 0.8812710151541978, 0.713173417840153, 0.6269093537703156, 0.6112594909500331, 0.6183263801503927, 0.6254691160283983, 0.6211975610349327, 0.714544738875702, 0.6851020918693393, 0.6616590300109237, 0.712365924147889, 0.786045276094228, 0.8927098710555583, 0.640504885930568, 0.6412885978352278, 0.7147476421669126, 0.6335964999161661, 0.6411878520157188, 0.8657821379601955, 0.7753057780209929, 0.6102908849716187, 0.8123872990254313, 0.8791983600240201, 0.8833416518755257, 0.8205396239645779, 0.6235360712744296, 0.6109609568957239, 0.609842962352559, 0.6186385049950331, 0.6462256808299571, 0.6901703539770097, 0.6074202409945428, 0.6354922978207469, 0.6310316666495055, 0.7035565003752708, 0.6823145411908627, 0.597781425807625, 0.597758874297142, 0.6057640670333058, 0.5970489969477057, 0.6471218802034855, 0.7142437170259655, 0.6286489411722869, 0.6256425194442272, 0.6879950165748596, 0.8563078239094466, 0.6177655472420156, 0.7049224870279431, 0.6299086881335825, 0.8155472571961582, 0.738897348055616, 0.6226484919898212, 0.6232100939378142, 0.6245280993171036, 0.6489461550954729, 0.7154522640630603, 0.6241815427783877, 0.6201015843544155, 0.6160422649700195, 0.6298476650845259, 0.627048980910331, 0.6896379210520536, 0.6085178840439767, 0.612151691922918, 0.6141019291244447, 0.7061797780916095, 0.8671191390603781, 0.7491929698735476, 0.6415973198600113, 0.6184769370593131, 0.6319432030431926, 0.6416898861061782, 0.5891728582791984, 0.6648313959594816, 0.6815066360868514, 0.601274810032919, 0.6093038029503077, 0.7958739502355456, 0.8713451700750738, 0.7270444571040571, 0.6102439239621162, 0.6339271292090416, 0.6331090531311929, 0.7506456898991019, 0.8792399947997183, 0.8709449348971248, 0.6619887868873775, 0.6483040368184447, 0.6447684129234403, 0.6495258840732276, 0.7439531460404396, 0.6326329847797751, 0.6210048149805516, 0.7642069749999791, 0.8901292991358787, 0.8068793998099864, 0.6250999430194497, 0.7070559007115662, 0.6386692749802023, 0.7301406271290034, 0.7188807618804276, 0.6376237298827618, 0.6923763549420983, 0.6367924029473215, 0.6970563549548388, 0.6060872920788825, 0.6445523304864764, 0.6671042467933148, 0.5948961379472166, 0.6051361998543143, 0.6155504009220749, 0.6032647960819304, 0.6438573014456779, 0.6881738079246134, 0.5969999399967492, 0.6752008642069995, 0.8665209200698882, 0.874881638912484, 0.8661948288790882, 0.621627502143383, 0.5999377898406237, 0.6024609538726509, 0.6134882748592645, 0.6089413689915091, 0.5989137659780681, 0.6512886362615973, 0.693124873097986, 0.6108656437136233, 0.634737053886056, 0.6353650079108775, 0.6609001737087965, 0.70361099508591, 0.6258009327575564, 0.5980432338546962, 0.6146341860294342, 0.6229544538073242, 0.6279873279854655, 0.732424238929525, 0.6260704970918596, 0.6212669510859996, 0.6178928010631353, 0.6310255548451096, 0.8751377069856972, 0.7204944400582463, 0.608454855857417, 0.673918766900897, 0.8582129268907011, 0.8687733886763453, 0.7409706409089267, 0.6030031158588827, 0.662564214784652, 0.5987753001973033, 0.7386029539629817, 0.6718219420872629, 0.6050235431175679, 0.60654824716039, 0.6095457072369754, 0.6067442079074681, 0.6094715869985521, 0.6416015068534762, 0.7347282289993018, 0.6357091399841011, 0.6748069219756871, 0.8645675200968981, 0.8688317618798465, 0.7306257400196046, 0.6102859911043197, 0.6035954568069428, 0.6036460129544139, 0.5908117818180472, 0.6179231978021562, 0.6724123181775212, 0.6076554309111089, 0.6158420578576624, 0.6148065030574799, 0.654575782129541, 0.7435534331016243, 0.7030974589288235, 0.6028881198726594, 0.6140766448806971, 0.6037519171368331, 0.6092787052039057, 0.7352862739935517, 0.6512830839492381, 0.5975960660725832, 0.6002903587650508, 0.7050651160534471, 0.6319006588310003, 0.7182214139029384, 0.6239992049522698, 0.6876171531621367, 0.7350355710368603, 0.7009442588314414, 0.9758739820681512, 0.8766256410162896, 0.6430177758447826, 0.6133581469766796, 0.6151921020355076, 0.6118284049443901, 0.6976040420122445, 0.8782316038850695, 0.6437109098769724, 0.594265443040058, 0.7008984489366412, 0.8961281687952578, 0.9044947263319045, 0.7847866301890463, 0.6424846388399601, 0.5861296840012074, 0.6042355101089925, 0.816940694116056, 0.73330098669976, 0.6211436809971929, 0.6131949780974537, 0.5958885110449046, 0.5992711188737303, 0.5863240249454975, 0.602168181212619, 0.6758261281065643, 0.6106846688780934, 0.6153189858887345, 0.6385804410092533, 0.6372093537356704, 0.6289109552744776, 0.7256259662099183, 0.7414413739461452, 0.619613341987133, 0.7300378112122416, 0.6431440967135131, 0.6512566928286105, 0.71576386410743, 0.629283539019525, 0.5939375872258097, 0.5882061112206429, 0.5924044167622924, 0.6828851597383618, 0.62166987080127, 0.7273851491045207, 0.6861924780532718, 0.6679112778510898, 0.6530661219730973, 0.6283599250018597, 0.7428010401781648]
Total Epoch List: [225, 274]
Total Time List: [0.17203435278497636, 0.20201673894189298]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2b4be20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1610;  Loss pred: 2.1610; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 2.1541;  Loss pred: 2.1541; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 2.1355;  Loss pred: 2.1355; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.22s
Epoch 4/1000, LR 0.000080
Train loss: 2.1036;  Loss pred: 2.1036; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.22s
Epoch 5/1000, LR 0.000110
Train loss: 2.0866;  Loss pred: 2.0866; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.18s
Epoch 6/1000, LR 0.000140
Train loss: 2.0300;  Loss pred: 2.0300; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 1.9796;  Loss pred: 1.9796; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
Test loss: 0.6929 score: 0.5234 time: 0.17s
Epoch 8/1000, LR 0.000200
Train loss: 1.8969;  Loss pred: 1.8969; Loss self: 0.0000; time: 0.29s
Val loss: 0.6928 score: 0.7752 time: 0.16s
Test loss: 0.6928 score: 0.7266 time: 0.16s
Epoch 9/1000, LR 0.000230
Train loss: 1.8406;  Loss pred: 1.8406; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.20s
Epoch 10/1000, LR 0.000260
Train loss: 1.7593;  Loss pred: 1.7593; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.22s
Epoch 11/1000, LR 0.000290
Train loss: 1.6894;  Loss pred: 1.6894; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000290
Train loss: 1.6322;  Loss pred: 1.6322; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000290
Train loss: 1.5607;  Loss pred: 1.5607; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000290
Train loss: 1.5055;  Loss pred: 1.5055; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000290
Train loss: 1.4673;  Loss pred: 1.4673; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000290
Train loss: 1.4134;  Loss pred: 1.4134; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000290
Train loss: 1.3640;  Loss pred: 1.3640; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.16s
Epoch 18/1000, LR 0.000290
Train loss: 1.3365;  Loss pred: 1.3365; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000290
Train loss: 1.3033;  Loss pred: 1.3033; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000290
Train loss: 1.2696;  Loss pred: 1.2696; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000290
Train loss: 1.2448;  Loss pred: 1.2448; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.21s
Epoch 22/1000, LR 0.000290
Train loss: 1.2170;  Loss pred: 1.2170; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 1.1963;  Loss pred: 1.1963; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 1.1779;  Loss pred: 1.1779; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 1.1596;  Loss pred: 1.1596; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.17s
Epoch 26/1000, LR 0.000290
Train loss: 1.1450;  Loss pred: 1.1450; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.19s
Epoch 27/1000, LR 0.000290
Train loss: 1.1292;  Loss pred: 1.1292; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 1.1178;  Loss pred: 1.1178; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 1.1050;  Loss pred: 1.1050; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 1.0922;  Loss pred: 1.0922; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 1.0851;  Loss pred: 1.0851; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.19s
Epoch 32/1000, LR 0.000290
Train loss: 1.0778;  Loss pred: 1.0778; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.20s
Epoch 33/1000, LR 0.000290
Train loss: 1.0706;  Loss pred: 1.0706; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 1.0555;  Loss pred: 1.0555; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 1.0479;  Loss pred: 1.0479; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 1.0444;  Loss pred: 1.0444; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 0.19s
Epoch 38/1000, LR 0.000289
Train loss: 1.0376;  Loss pred: 1.0376; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5000 time: 0.18s
Epoch 39/1000, LR 0.000289
Train loss: 1.0352;  Loss pred: 1.0352; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.5000 time: 0.18s
Epoch 40/1000, LR 0.000289
Train loss: 1.0298;  Loss pred: 1.0298; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 1.0239;  Loss pred: 1.0239; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5000 time: 0.18s
Epoch 42/1000, LR 0.000289
Train loss: 1.0208;  Loss pred: 1.0208; Loss self: 0.0000; time: 0.43s
Val loss: 0.6855 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.5000 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 1.0190;  Loss pred: 1.0190; Loss self: 0.0000; time: 0.32s
Val loss: 0.6848 score: 0.5349 time: 0.17s
Test loss: 0.6860 score: 0.5156 time: 0.18s
Epoch 44/1000, LR 0.000289
Train loss: 1.0167;  Loss pred: 1.0167; Loss self: 0.0000; time: 0.31s
Val loss: 0.6842 score: 0.5349 time: 0.18s
Test loss: 0.6854 score: 0.5156 time: 0.18s
Epoch 45/1000, LR 0.000289
Train loss: 1.0116;  Loss pred: 1.0116; Loss self: 0.0000; time: 0.30s
Val loss: 0.6834 score: 0.5581 time: 0.18s
Test loss: 0.6847 score: 0.5391 time: 0.18s
Epoch 46/1000, LR 0.000289
Train loss: 1.0102;  Loss pred: 1.0102; Loss self: 0.0000; time: 0.31s
Val loss: 0.6827 score: 0.6202 time: 0.20s
Test loss: 0.6839 score: 0.5547 time: 0.18s
Epoch 47/1000, LR 0.000289
Train loss: 1.0061;  Loss pred: 1.0061; Loss self: 0.0000; time: 0.36s
Val loss: 0.6820 score: 0.6202 time: 0.16s
Test loss: 0.6834 score: 0.5625 time: 0.17s
Epoch 48/1000, LR 0.000289
Train loss: 1.0038;  Loss pred: 1.0038; Loss self: 0.0000; time: 0.29s
Val loss: 0.6815 score: 0.6202 time: 0.16s
Test loss: 0.6829 score: 0.5547 time: 0.16s
Epoch 49/1000, LR 0.000289
Train loss: 1.0009;  Loss pred: 1.0009; Loss self: 0.0000; time: 0.28s
Val loss: 0.6808 score: 0.5736 time: 0.15s
Test loss: 0.6824 score: 0.5469 time: 0.16s
Epoch 50/1000, LR 0.000289
Train loss: 0.9979;  Loss pred: 0.9979; Loss self: 0.0000; time: 0.29s
Val loss: 0.6801 score: 0.5736 time: 0.16s
Test loss: 0.6817 score: 0.5469 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 0.9959;  Loss pred: 0.9959; Loss self: 0.0000; time: 0.30s
Val loss: 0.6795 score: 0.5581 time: 0.16s
Test loss: 0.6813 score: 0.5391 time: 0.16s
Epoch 52/1000, LR 0.000289
Train loss: 0.9933;  Loss pred: 0.9933; Loss self: 0.0000; time: 0.30s
Val loss: 0.6789 score: 0.5581 time: 0.16s
Test loss: 0.6807 score: 0.5391 time: 0.19s
Epoch 53/1000, LR 0.000289
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.6778 score: 0.5736 time: 0.21s
Test loss: 0.6797 score: 0.5469 time: 0.18s
Epoch 54/1000, LR 0.000289
Train loss: 0.9900;  Loss pred: 0.9900; Loss self: 0.0000; time: 0.29s
Val loss: 0.6764 score: 0.6202 time: 0.16s
Test loss: 0.6785 score: 0.5625 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 0.9873;  Loss pred: 0.9873; Loss self: 0.0000; time: 0.29s
Val loss: 0.6750 score: 0.6434 time: 0.16s
Test loss: 0.6773 score: 0.5859 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 0.9848;  Loss pred: 0.9848; Loss self: 0.0000; time: 0.29s
Val loss: 0.6736 score: 0.7364 time: 0.16s
Test loss: 0.6760 score: 0.6406 time: 0.17s
Epoch 57/1000, LR 0.000288
Train loss: 0.9833;  Loss pred: 0.9833; Loss self: 0.0000; time: 0.28s
Val loss: 0.6722 score: 0.7752 time: 0.16s
Test loss: 0.6747 score: 0.6953 time: 0.16s
Epoch 58/1000, LR 0.000288
Train loss: 0.9824;  Loss pred: 0.9824; Loss self: 0.0000; time: 0.28s
Val loss: 0.6706 score: 0.8527 time: 0.16s
Test loss: 0.6732 score: 0.7891 time: 0.16s
Epoch 59/1000, LR 0.000288
Train loss: 0.9793;  Loss pred: 0.9793; Loss self: 0.0000; time: 0.37s
Val loss: 0.6690 score: 0.8527 time: 0.15s
Test loss: 0.6718 score: 0.7969 time: 0.16s
Epoch 60/1000, LR 0.000288
Train loss: 0.9774;  Loss pred: 0.9774; Loss self: 0.0000; time: 0.29s
Val loss: 0.6675 score: 0.7752 time: 0.16s
Test loss: 0.6706 score: 0.7344 time: 0.15s
Epoch 61/1000, LR 0.000288
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 0.28s
Val loss: 0.6660 score: 0.7674 time: 0.15s
Test loss: 0.6693 score: 0.6953 time: 0.15s
Epoch 62/1000, LR 0.000288
Train loss: 0.9734;  Loss pred: 0.9734; Loss self: 0.0000; time: 0.28s
Val loss: 0.6643 score: 0.7752 time: 0.16s
Test loss: 0.6678 score: 0.7109 time: 0.16s
Epoch 63/1000, LR 0.000288
Train loss: 0.9717;  Loss pred: 0.9717; Loss self: 0.0000; time: 0.28s
Val loss: 0.6627 score: 0.7519 time: 0.15s
Test loss: 0.6665 score: 0.6875 time: 0.15s
Epoch 64/1000, LR 0.000288
Train loss: 0.9689;  Loss pred: 0.9689; Loss self: 0.0000; time: 0.29s
Val loss: 0.6615 score: 0.6822 time: 0.16s
Test loss: 0.6656 score: 0.6016 time: 0.16s
Epoch 65/1000, LR 0.000288
Train loss: 0.9678;  Loss pred: 0.9678; Loss self: 0.0000; time: 0.31s
Val loss: 0.6606 score: 0.6279 time: 0.16s
Test loss: 0.6650 score: 0.5703 time: 0.16s
Epoch 66/1000, LR 0.000288
Train loss: 0.9663;  Loss pred: 0.9663; Loss self: 0.0000; time: 0.35s
Val loss: 0.6593 score: 0.6279 time: 0.16s
Test loss: 0.6638 score: 0.5625 time: 0.16s
Epoch 67/1000, LR 0.000288
Train loss: 0.9649;  Loss pred: 0.9649; Loss self: 0.0000; time: 0.29s
Val loss: 0.6571 score: 0.6279 time: 0.15s
Test loss: 0.6619 score: 0.5703 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.9629;  Loss pred: 0.9629; Loss self: 0.0000; time: 0.28s
Val loss: 0.6552 score: 0.6279 time: 0.15s
Test loss: 0.6602 score: 0.5703 time: 0.16s
Epoch 69/1000, LR 0.000288
Train loss: 0.9610;  Loss pred: 0.9610; Loss self: 0.0000; time: 0.27s
Val loss: 0.6532 score: 0.6357 time: 0.15s
Test loss: 0.6584 score: 0.5625 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.9582;  Loss pred: 0.9582; Loss self: 0.0000; time: 0.29s
Val loss: 0.6504 score: 0.7442 time: 0.16s
Test loss: 0.6558 score: 0.6641 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 0.9562;  Loss pred: 0.9562; Loss self: 0.0000; time: 0.28s
Val loss: 0.6480 score: 0.7597 time: 0.19s
Test loss: 0.6536 score: 0.6875 time: 0.16s
Epoch 72/1000, LR 0.000287
Train loss: 0.9544;  Loss pred: 0.9544; Loss self: 0.0000; time: 0.38s
Val loss: 0.6463 score: 0.7597 time: 0.16s
Test loss: 0.6522 score: 0.6719 time: 0.16s
Epoch 73/1000, LR 0.000287
Train loss: 0.9525;  Loss pred: 0.9525; Loss self: 0.0000; time: 0.28s
Val loss: 0.6450 score: 0.6977 time: 0.15s
Test loss: 0.6512 score: 0.6250 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9512;  Loss pred: 0.9512; Loss self: 0.0000; time: 0.29s
Val loss: 0.6436 score: 0.6512 time: 0.16s
Test loss: 0.6501 score: 0.5859 time: 0.17s
Epoch 75/1000, LR 0.000287
Train loss: 0.9497;  Loss pred: 0.9497; Loss self: 0.0000; time: 0.29s
Val loss: 0.6423 score: 0.6279 time: 0.16s
Test loss: 0.6491 score: 0.5625 time: 0.17s
Epoch 76/1000, LR 0.000287
Train loss: 0.9492;  Loss pred: 0.9492; Loss self: 0.0000; time: 0.30s
Val loss: 0.6400 score: 0.6279 time: 0.16s
Test loss: 0.6471 score: 0.5625 time: 0.17s
Epoch 77/1000, LR 0.000287
Train loss: 0.9471;  Loss pred: 0.9471; Loss self: 0.0000; time: 0.32s
Val loss: 0.6360 score: 0.7674 time: 0.23s
Test loss: 0.6432 score: 0.6641 time: 0.21s
Epoch 78/1000, LR 0.000287
Train loss: 0.9432;  Loss pred: 0.9432; Loss self: 0.0000; time: 0.36s
Val loss: 0.6314 score: 0.8605 time: 0.16s
Test loss: 0.6387 score: 0.7969 time: 0.16s
Epoch 79/1000, LR 0.000287
Train loss: 0.9386;  Loss pred: 0.9386; Loss self: 0.0000; time: 0.29s
Val loss: 0.6273 score: 0.9147 time: 0.15s
Test loss: 0.6345 score: 0.8984 time: 0.16s
Epoch 80/1000, LR 0.000287
Train loss: 0.9362;  Loss pred: 0.9362; Loss self: 0.0000; time: 0.29s
Val loss: 0.6244 score: 0.9225 time: 0.16s
Test loss: 0.6312 score: 0.9141 time: 0.15s
Epoch 81/1000, LR 0.000286
Train loss: 0.9348;  Loss pred: 0.9348; Loss self: 0.0000; time: 0.28s
Val loss: 0.6230 score: 0.8915 time: 0.15s
Test loss: 0.6292 score: 0.8672 time: 0.17s
Epoch 82/1000, LR 0.000286
Train loss: 0.9333;  Loss pred: 0.9333; Loss self: 0.0000; time: 0.28s
Val loss: 0.6212 score: 0.8837 time: 0.15s
Test loss: 0.6273 score: 0.8359 time: 0.16s
Epoch 83/1000, LR 0.000286
Train loss: 0.9318;  Loss pred: 0.9318; Loss self: 0.0000; time: 0.28s
Val loss: 0.6174 score: 0.8915 time: 0.15s
Test loss: 0.6239 score: 0.8359 time: 0.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.9288;  Loss pred: 0.9288; Loss self: 0.0000; time: 0.28s
Val loss: 0.6123 score: 0.8992 time: 0.18s
Test loss: 0.6194 score: 0.8594 time: 0.17s
Epoch 85/1000, LR 0.000286
Train loss: 0.9254;  Loss pred: 0.9254; Loss self: 0.0000; time: 0.43s
Val loss: 0.6067 score: 0.8992 time: 0.17s
Test loss: 0.6147 score: 0.8750 time: 0.16s
Epoch 86/1000, LR 0.000286
Train loss: 0.9214;  Loss pred: 0.9214; Loss self: 0.0000; time: 0.30s
Val loss: 0.6007 score: 0.9147 time: 0.17s
Test loss: 0.6097 score: 0.8828 time: 0.16s
Epoch 87/1000, LR 0.000286
Train loss: 0.9171;  Loss pred: 0.9171; Loss self: 0.0000; time: 0.27s
Val loss: 0.5950 score: 0.9225 time: 0.15s
Test loss: 0.6049 score: 0.8906 time: 0.16s
Epoch 88/1000, LR 0.000286
Train loss: 0.9135;  Loss pred: 0.9135; Loss self: 0.0000; time: 0.28s
Val loss: 0.5895 score: 0.9225 time: 0.16s
Test loss: 0.6002 score: 0.8984 time: 0.16s
Epoch 89/1000, LR 0.000286
Train loss: 0.9089;  Loss pred: 0.9089; Loss self: 0.0000; time: 0.34s
Val loss: 0.5841 score: 0.9225 time: 0.16s
Test loss: 0.5953 score: 0.8984 time: 0.21s
Epoch 90/1000, LR 0.000285
Train loss: 0.9056;  Loss pred: 0.9056; Loss self: 0.0000; time: 0.48s
Val loss: 0.5784 score: 0.9225 time: 0.24s
Test loss: 0.5904 score: 0.9062 time: 0.22s
Epoch 91/1000, LR 0.000285
Train loss: 0.9022;  Loss pred: 0.9022; Loss self: 0.0000; time: 0.49s
Val loss: 0.5728 score: 0.9147 time: 0.16s
Test loss: 0.5857 score: 0.9219 time: 0.17s
Epoch 92/1000, LR 0.000285
Train loss: 0.8984;  Loss pred: 0.8984; Loss self: 0.0000; time: 0.29s
Val loss: 0.5672 score: 0.9225 time: 0.16s
Test loss: 0.5808 score: 0.9062 time: 0.16s
Epoch 93/1000, LR 0.000285
Train loss: 0.8947;  Loss pred: 0.8947; Loss self: 0.0000; time: 0.28s
Val loss: 0.5606 score: 0.9147 time: 0.16s
Test loss: 0.5748 score: 0.9297 time: 0.17s
Epoch 94/1000, LR 0.000285
Train loss: 0.8900;  Loss pred: 0.8900; Loss self: 0.0000; time: 0.42s
Val loss: 0.5539 score: 0.9070 time: 0.24s
Test loss: 0.5685 score: 0.9219 time: 0.22s
Epoch 95/1000, LR 0.000285
Train loss: 0.8857;  Loss pred: 0.8857; Loss self: 0.0000; time: 0.48s
Val loss: 0.5472 score: 0.9225 time: 0.24s
Test loss: 0.5622 score: 0.9062 time: 0.22s
Epoch 96/1000, LR 0.000285
Train loss: 0.8812;  Loss pred: 0.8812; Loss self: 0.0000; time: 0.29s
Val loss: 0.5409 score: 0.9225 time: 0.17s
Test loss: 0.5566 score: 0.9062 time: 0.17s
Epoch 97/1000, LR 0.000285
Train loss: 0.8772;  Loss pred: 0.8772; Loss self: 0.0000; time: 0.30s
Val loss: 0.5347 score: 0.9147 time: 0.16s
Test loss: 0.5511 score: 0.9219 time: 0.16s
Epoch 98/1000, LR 0.000285
Train loss: 0.8718;  Loss pred: 0.8718; Loss self: 0.0000; time: 0.30s
Val loss: 0.5279 score: 0.9225 time: 0.21s
Test loss: 0.5449 score: 0.9141 time: 0.22s
Epoch 99/1000, LR 0.000284
Train loss: 0.8688;  Loss pred: 0.8688; Loss self: 0.0000; time: 0.48s
Val loss: 0.5212 score: 0.9070 time: 0.25s
Test loss: 0.5390 score: 0.9219 time: 0.22s
Epoch 100/1000, LR 0.000284
Train loss: 0.8643;  Loss pred: 0.8643; Loss self: 0.0000; time: 0.45s
Val loss: 0.5147 score: 0.9147 time: 0.22s
Test loss: 0.5333 score: 0.9297 time: 0.17s
Epoch 101/1000, LR 0.000284
Train loss: 0.8610;  Loss pred: 0.8610; Loss self: 0.0000; time: 0.29s
Val loss: 0.5076 score: 0.9147 time: 0.16s
Test loss: 0.5269 score: 0.9219 time: 0.17s
Epoch 102/1000, LR 0.000284
Train loss: 0.8541;  Loss pred: 0.8541; Loss self: 0.0000; time: 0.30s
Val loss: 0.4993 score: 0.9225 time: 0.21s
Test loss: 0.5190 score: 0.9141 time: 0.21s
Epoch 103/1000, LR 0.000284
Train loss: 0.8495;  Loss pred: 0.8495; Loss self: 0.0000; time: 0.46s
Val loss: 0.4918 score: 0.9225 time: 0.16s
Test loss: 0.5119 score: 0.8906 time: 0.16s
Epoch 104/1000, LR 0.000284
Train loss: 0.8456;  Loss pred: 0.8456; Loss self: 0.0000; time: 0.28s
Val loss: 0.4860 score: 0.9147 time: 0.15s
Test loss: 0.5063 score: 0.8828 time: 0.16s
Epoch 105/1000, LR 0.000284
Train loss: 0.8412;  Loss pred: 0.8412; Loss self: 0.0000; time: 0.29s
Val loss: 0.4790 score: 0.9302 time: 0.18s
Test loss: 0.5001 score: 0.8984 time: 0.17s
Epoch 106/1000, LR 0.000283
Train loss: 0.8365;  Loss pred: 0.8365; Loss self: 0.0000; time: 0.28s
Val loss: 0.4727 score: 0.9302 time: 0.15s
Test loss: 0.4950 score: 0.9141 time: 0.16s
Epoch 107/1000, LR 0.000283
Train loss: 0.8322;  Loss pred: 0.8322; Loss self: 0.0000; time: 0.29s
Val loss: 0.4701 score: 0.9302 time: 0.17s
Test loss: 0.4936 score: 0.9062 time: 0.19s
Epoch 108/1000, LR 0.000283
Train loss: 0.8319;  Loss pred: 0.8319; Loss self: 0.0000; time: 0.43s
Val loss: 0.4697 score: 0.9147 time: 0.25s
Test loss: 0.4942 score: 0.9062 time: 0.23s
Epoch 109/1000, LR 0.000283
Train loss: 0.8296;  Loss pred: 0.8296; Loss self: 0.0000; time: 0.39s
Val loss: 0.4624 score: 0.9147 time: 0.17s
Test loss: 0.4874 score: 0.9062 time: 0.18s
Epoch 110/1000, LR 0.000283
Train loss: 0.8246;  Loss pred: 0.8246; Loss self: 0.0000; time: 0.32s
Val loss: 0.4515 score: 0.9302 time: 0.18s
Test loss: 0.4765 score: 0.9062 time: 0.18s
Epoch 111/1000, LR 0.000283
Train loss: 0.8165;  Loss pred: 0.8165; Loss self: 0.0000; time: 0.30s
Val loss: 0.4429 score: 0.9302 time: 0.16s
Test loss: 0.4676 score: 0.9141 time: 0.17s
Epoch 112/1000, LR 0.000283
Train loss: 0.8115;  Loss pred: 0.8115; Loss self: 0.0000; time: 0.30s
Val loss: 0.4367 score: 0.9302 time: 0.17s
Test loss: 0.4616 score: 0.8906 time: 0.20s
Epoch 113/1000, LR 0.000282
Train loss: 0.8080;  Loss pred: 0.8080; Loss self: 0.0000; time: 0.40s
Val loss: 0.4326 score: 0.9147 time: 0.16s
Test loss: 0.4579 score: 0.8828 time: 0.17s
Epoch 114/1000, LR 0.000282
Train loss: 0.8059;  Loss pred: 0.8059; Loss self: 0.0000; time: 0.30s
Val loss: 0.4309 score: 0.9070 time: 0.16s
Test loss: 0.4566 score: 0.8750 time: 0.18s
Epoch 115/1000, LR 0.000282
Train loss: 0.8066;  Loss pred: 0.8066; Loss self: 0.0000; time: 0.31s
Val loss: 0.4284 score: 0.8915 time: 0.27s
Test loss: 0.4550 score: 0.8672 time: 0.18s
Epoch 116/1000, LR 0.000282
Train loss: 0.8051;  Loss pred: 0.8051; Loss self: 0.0000; time: 0.30s
Val loss: 0.4238 score: 0.8915 time: 0.18s
Test loss: 0.4514 score: 0.8516 time: 0.18s
Epoch 117/1000, LR 0.000282
Train loss: 0.8014;  Loss pred: 0.8014; Loss self: 0.0000; time: 0.30s
Val loss: 0.4170 score: 0.8915 time: 0.17s
Test loss: 0.4453 score: 0.8516 time: 0.28s
Epoch 118/1000, LR 0.000282
Train loss: 0.7964;  Loss pred: 0.7964; Loss self: 0.0000; time: 0.33s
Val loss: 0.4069 score: 0.8915 time: 0.17s
Test loss: 0.4359 score: 0.8672 time: 0.22s
Epoch 119/1000, LR 0.000282
Train loss: 0.7888;  Loss pred: 0.7888; Loss self: 0.0000; time: 0.31s
Val loss: 0.3925 score: 0.9147 time: 0.17s
Test loss: 0.4224 score: 0.8750 time: 0.17s
Epoch 120/1000, LR 0.000281
Train loss: 0.7776;  Loss pred: 0.7776; Loss self: 0.0000; time: 0.33s
Val loss: 0.3831 score: 0.9225 time: 0.16s
Test loss: 0.4138 score: 0.9141 time: 0.18s
Epoch 121/1000, LR 0.000281
Train loss: 0.7738;  Loss pred: 0.7738; Loss self: 0.0000; time: 0.31s
Val loss: 0.3778 score: 0.9225 time: 0.17s
Test loss: 0.4095 score: 0.9141 time: 0.19s
Epoch 122/1000, LR 0.000281
Train loss: 0.7701;  Loss pred: 0.7701; Loss self: 0.0000; time: 0.32s
Val loss: 0.3729 score: 0.9147 time: 0.17s
Test loss: 0.4054 score: 0.9141 time: 0.17s
Epoch 123/1000, LR 0.000281
Train loss: 0.7647;  Loss pred: 0.7647; Loss self: 0.0000; time: 0.29s
Val loss: 0.3666 score: 0.9147 time: 0.16s
Test loss: 0.3996 score: 0.9141 time: 0.17s
Epoch 124/1000, LR 0.000281
Train loss: 0.7616;  Loss pred: 0.7616; Loss self: 0.0000; time: 0.33s
Val loss: 0.3607 score: 0.9147 time: 0.22s
Test loss: 0.3943 score: 0.9141 time: 0.20s
Epoch 125/1000, LR 0.000281
Train loss: 0.7579;  Loss pred: 0.7579; Loss self: 0.0000; time: 0.37s
Val loss: 0.3545 score: 0.9147 time: 0.18s
Test loss: 0.3885 score: 0.9141 time: 0.17s
Epoch 126/1000, LR 0.000280
Train loss: 0.7533;  Loss pred: 0.7533; Loss self: 0.0000; time: 0.31s
Val loss: 0.3478 score: 0.9147 time: 0.16s
Test loss: 0.3822 score: 0.9141 time: 0.17s
Epoch 127/1000, LR 0.000280
Train loss: 0.7497;  Loss pred: 0.7497; Loss self: 0.0000; time: 0.29s
Val loss: 0.3414 score: 0.9302 time: 0.17s
Test loss: 0.3762 score: 0.9141 time: 0.18s
Epoch 128/1000, LR 0.000280
Train loss: 0.7439;  Loss pred: 0.7439; Loss self: 0.0000; time: 0.29s
Val loss: 0.3354 score: 0.9302 time: 0.16s
Test loss: 0.3707 score: 0.9141 time: 0.17s
Epoch 129/1000, LR 0.000280
Train loss: 0.7428;  Loss pred: 0.7428; Loss self: 0.0000; time: 0.37s
Val loss: 0.3299 score: 0.9225 time: 0.16s
Test loss: 0.3656 score: 0.9141 time: 0.16s
Epoch 130/1000, LR 0.000280
Train loss: 0.7390;  Loss pred: 0.7390; Loss self: 0.0000; time: 0.31s
Val loss: 0.3244 score: 0.9302 time: 0.16s
Test loss: 0.3607 score: 0.9141 time: 0.17s
Epoch 131/1000, LR 0.000280
Train loss: 0.7353;  Loss pred: 0.7353; Loss self: 0.0000; time: 0.30s
Val loss: 0.3191 score: 0.9225 time: 0.16s
Test loss: 0.3559 score: 0.9141 time: 0.17s
Epoch 132/1000, LR 0.000279
Train loss: 0.7303;  Loss pred: 0.7303; Loss self: 0.0000; time: 0.31s
Val loss: 0.3146 score: 0.9302 time: 0.16s
Test loss: 0.3517 score: 0.9141 time: 0.17s
Epoch 133/1000, LR 0.000279
Train loss: 0.7313;  Loss pred: 0.7313; Loss self: 0.0000; time: 0.30s
Val loss: 0.3118 score: 0.9225 time: 0.16s
Test loss: 0.3493 score: 0.9062 time: 0.16s
Epoch 134/1000, LR 0.000279
Train loss: 0.7255;  Loss pred: 0.7255; Loss self: 0.0000; time: 0.30s
Val loss: 0.3109 score: 0.9147 time: 0.21s
Test loss: 0.3491 score: 0.8750 time: 0.16s
Epoch 135/1000, LR 0.000279
Train loss: 0.7278;  Loss pred: 0.7278; Loss self: 0.0000; time: 0.42s
Val loss: 0.3089 score: 0.9147 time: 0.17s
Test loss: 0.3478 score: 0.8750 time: 0.16s
Epoch 136/1000, LR 0.000279
Train loss: 0.7250;  Loss pred: 0.7250; Loss self: 0.0000; time: 0.29s
Val loss: 0.3037 score: 0.9147 time: 0.15s
Test loss: 0.3427 score: 0.8750 time: 0.16s
Epoch 137/1000, LR 0.000279
Train loss: 0.7215;  Loss pred: 0.7215; Loss self: 0.0000; time: 0.29s
Val loss: 0.2977 score: 0.9147 time: 0.15s
Test loss: 0.3366 score: 0.8906 time: 0.16s
Epoch 138/1000, LR 0.000278
Train loss: 0.7162;  Loss pred: 0.7162; Loss self: 0.0000; time: 0.31s
Val loss: 0.2926 score: 0.9147 time: 0.18s
Test loss: 0.3315 score: 0.8984 time: 0.16s
Epoch 139/1000, LR 0.000278
Train loss: 0.7119;  Loss pred: 0.7119; Loss self: 0.0000; time: 0.49s
Val loss: 0.2887 score: 0.9147 time: 0.22s
Test loss: 0.3279 score: 0.8984 time: 0.16s
Epoch 140/1000, LR 0.000278
Train loss: 0.7080;  Loss pred: 0.7080; Loss self: 0.0000; time: 0.29s
Val loss: 0.2856 score: 0.9147 time: 0.15s
Test loss: 0.3253 score: 0.9062 time: 0.16s
Epoch 141/1000, LR 0.000278
Train loss: 0.7044;  Loss pred: 0.7044; Loss self: 0.0000; time: 0.29s
Val loss: 0.2822 score: 0.9147 time: 0.15s
Test loss: 0.3224 score: 0.9062 time: 0.16s
Epoch 142/1000, LR 0.000278
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.29s
Val loss: 0.2755 score: 0.9380 time: 0.16s
Test loss: 0.3153 score: 0.9141 time: 0.16s
Epoch 143/1000, LR 0.000277
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.36s
Val loss: 0.2710 score: 0.9225 time: 0.17s
Test loss: 0.3109 score: 0.9141 time: 0.22s
Epoch 144/1000, LR 0.000277
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.48s
Val loss: 0.2678 score: 0.9225 time: 0.25s
Test loss: 0.3081 score: 0.9219 time: 0.21s
Epoch 145/1000, LR 0.000277
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.49s
Val loss: 0.2660 score: 0.9302 time: 0.25s
Test loss: 0.3070 score: 0.9219 time: 0.16s
Epoch 146/1000, LR 0.000277
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.30s
Val loss: 0.2632 score: 0.9302 time: 0.15s
Test loss: 0.3044 score: 0.9219 time: 0.16s
Epoch 147/1000, LR 0.000277
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.29s
Val loss: 0.2580 score: 0.9225 time: 0.15s
Test loss: 0.2990 score: 0.9219 time: 0.17s
Epoch 148/1000, LR 0.000277
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.28s
Val loss: 0.2558 score: 0.9380 time: 0.16s
Test loss: 0.2971 score: 0.9141 time: 0.17s
Epoch 149/1000, LR 0.000276
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.29s
Val loss: 0.2579 score: 0.9147 time: 0.16s
Test loss: 0.3009 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 150/1000, LR 0.000276
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.30s
Val loss: 0.2623 score: 0.9147 time: 0.17s
Test loss: 0.3074 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 151/1000, LR 0.000276
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.29s
Val loss: 0.2646 score: 0.9147 time: 0.17s
Test loss: 0.3113 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 152/1000, LR 0.000276
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.31s
Val loss: 0.2619 score: 0.9147 time: 0.16s
Test loss: 0.3089 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 153/1000, LR 0.000276
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.36s
Val loss: 0.2545 score: 0.9147 time: 0.17s
Test loss: 0.3004 score: 0.8828 time: 0.17s
Epoch 154/1000, LR 0.000275
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.32s
Val loss: 0.2469 score: 0.9147 time: 0.15s
Test loss: 0.2916 score: 0.8906 time: 0.17s
Epoch 155/1000, LR 0.000275
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.30s
Val loss: 0.2396 score: 0.9302 time: 0.17s
Test loss: 0.2830 score: 0.9141 time: 0.19s
Epoch 156/1000, LR 0.000275
Train loss: 0.6673;  Loss pred: 0.6673; Loss self: 0.0000; time: 0.30s
Val loss: 0.2345 score: 0.9380 time: 0.17s
Test loss: 0.2771 score: 0.9219 time: 0.18s
Epoch 157/1000, LR 0.000275
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.30s
Val loss: 0.2312 score: 0.9225 time: 0.17s
Test loss: 0.2734 score: 0.9219 time: 0.18s
Epoch 158/1000, LR 0.000275
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.28s
Val loss: 0.2290 score: 0.9302 time: 0.18s
Test loss: 0.2711 score: 0.9219 time: 0.17s
Epoch 159/1000, LR 0.000274
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.42s
Val loss: 0.2277 score: 0.9302 time: 0.18s
Test loss: 0.2701 score: 0.9219 time: 0.17s
Epoch 160/1000, LR 0.000274
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.29s
Val loss: 0.2272 score: 0.9380 time: 0.17s
Test loss: 0.2701 score: 0.9219 time: 0.17s
Epoch 161/1000, LR 0.000274
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.29s
Val loss: 0.2283 score: 0.9225 time: 0.17s
Test loss: 0.2722 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000274
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.30s
Val loss: 0.2324 score: 0.9225 time: 0.17s
Test loss: 0.2785 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 163/1000, LR 0.000273
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.29s
Val loss: 0.2370 score: 0.9147 time: 0.16s
Test loss: 0.2854 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 0.29s
Val loss: 0.2380 score: 0.9147 time: 0.16s
Test loss: 0.2873 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 165/1000, LR 0.000273
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.32s
Val loss: 0.2354 score: 0.9147 time: 0.16s
Test loss: 0.2844 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 166/1000, LR 0.000273
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.42s
Val loss: 0.2301 score: 0.9147 time: 0.16s
Test loss: 0.2779 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 167/1000, LR 0.000273
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.28s
Val loss: 0.2232 score: 0.9225 time: 0.15s
Test loss: 0.2690 score: 0.8984 time: 0.16s
Epoch 168/1000, LR 0.000272
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.29s
Val loss: 0.2166 score: 0.9225 time: 0.16s
Test loss: 0.2605 score: 0.9141 time: 0.17s
Epoch 169/1000, LR 0.000272
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.29s
Val loss: 0.2116 score: 0.9457 time: 0.16s
Test loss: 0.2536 score: 0.9219 time: 0.17s
Epoch 170/1000, LR 0.000272
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.29s
Val loss: 0.2089 score: 0.9380 time: 0.15s
Test loss: 0.2499 score: 0.9219 time: 0.17s
Epoch 171/1000, LR 0.000272
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.30s
Val loss: 0.2068 score: 0.9380 time: 0.21s
Test loss: 0.2471 score: 0.9219 time: 0.17s
Epoch 172/1000, LR 0.000271
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.44s
Val loss: 0.2049 score: 0.9147 time: 0.24s
Test loss: 0.2446 score: 0.9219 time: 0.17s
Epoch 173/1000, LR 0.000271
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 0.30s
Val loss: 0.2036 score: 0.9225 time: 0.16s
Test loss: 0.2431 score: 0.9219 time: 0.17s
Epoch 174/1000, LR 0.000271
Train loss: 0.6304;  Loss pred: 0.6304; Loss self: 0.0000; time: 0.30s
Val loss: 0.2028 score: 0.9380 time: 0.16s
Test loss: 0.2428 score: 0.9297 time: 0.16s
Epoch 175/1000, LR 0.000271
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.29s
Val loss: 0.2017 score: 0.9380 time: 0.16s
Test loss: 0.2415 score: 0.9297 time: 0.17s
Epoch 176/1000, LR 0.000271
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.29s
Val loss: 0.2004 score: 0.9380 time: 0.19s
Test loss: 0.2398 score: 0.9219 time: 0.17s
Epoch 177/1000, LR 0.000270
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.48s
Val loss: 0.1990 score: 0.9380 time: 0.24s
Test loss: 0.2381 score: 0.9219 time: 0.17s
Epoch 178/1000, LR 0.000270
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
Val loss: 0.1982 score: 0.9380 time: 0.17s
Test loss: 0.2377 score: 0.9297 time: 0.16s
Epoch 179/1000, LR 0.000270
Train loss: 0.6217;  Loss pred: 0.6217; Loss self: 0.0000; time: 0.30s
Val loss: 0.1974 score: 0.9380 time: 0.17s
Test loss: 0.2372 score: 0.9297 time: 0.17s
Epoch 180/1000, LR 0.000270
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.35s
Val loss: 0.1966 score: 0.9380 time: 0.17s
Test loss: 0.2368 score: 0.9219 time: 0.22s
Epoch 181/1000, LR 0.000269
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.50s
Val loss: 0.1947 score: 0.9380 time: 0.25s
Test loss: 0.2341 score: 0.9219 time: 0.22s
Epoch 182/1000, LR 0.000269
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.50s
Val loss: 0.1923 score: 0.9225 time: 0.25s
Test loss: 0.2295 score: 0.9219 time: 0.23s
Epoch 183/1000, LR 0.000269
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.38s
Val loss: 0.1917 score: 0.9302 time: 0.18s
Test loss: 0.2276 score: 0.9297 time: 0.17s
Epoch 184/1000, LR 0.000269
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.30s
Val loss: 0.1915 score: 0.9302 time: 0.17s
Test loss: 0.2266 score: 0.9297 time: 0.17s
Epoch 185/1000, LR 0.000268
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.30s
Val loss: 0.1908 score: 0.9302 time: 0.16s
Test loss: 0.2256 score: 0.9297 time: 0.21s
Epoch 186/1000, LR 0.000268
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.49s
Val loss: 0.1889 score: 0.9302 time: 0.23s
Test loss: 0.2246 score: 0.9297 time: 0.21s
Epoch 187/1000, LR 0.000268
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.46s
Val loss: 0.1892 score: 0.9380 time: 0.17s
Test loss: 0.2274 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.6054;  Loss pred: 0.6054; Loss self: 0.0000; time: 0.29s
Val loss: 0.1911 score: 0.9225 time: 0.15s
Test loss: 0.2317 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.30s
Val loss: 0.1918 score: 0.9302 time: 0.19s
Test loss: 0.2333 score: 0.9062 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.29s
Val loss: 0.1907 score: 0.9302 time: 0.17s
Test loss: 0.2319 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.6082;  Loss pred: 0.6082; Loss self: 0.0000; time: 0.29s
Val loss: 0.1884 score: 0.9302 time: 0.16s
Test loss: 0.2285 score: 0.9141 time: 0.16s
Epoch 192/1000, LR 0.000267
Train loss: 0.6048;  Loss pred: 0.6048; Loss self: 0.0000; time: 0.29s
Val loss: 0.1862 score: 0.9380 time: 0.16s
Test loss: 0.2249 score: 0.9141 time: 0.17s
Epoch 193/1000, LR 0.000266
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.33s
Val loss: 0.1835 score: 0.9147 time: 0.17s
Test loss: 0.2201 score: 0.9219 time: 0.21s
Epoch 194/1000, LR 0.000266
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 0.33s
Val loss: 0.1828 score: 0.9302 time: 0.17s
Test loss: 0.2169 score: 0.9297 time: 0.17s
Epoch 195/1000, LR 0.000266
Train loss: 0.6002;  Loss pred: 0.6002; Loss self: 0.0000; time: 0.29s
Val loss: 0.1839 score: 0.9302 time: 0.17s
Test loss: 0.2169 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 196/1000, LR 0.000266
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.30s
Val loss: 0.1841 score: 0.9302 time: 0.17s
Test loss: 0.2168 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 197/1000, LR 0.000265
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.29s
Val loss: 0.1828 score: 0.9302 time: 0.17s
Test loss: 0.2155 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 198/1000, LR 0.000265
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.30s
Val loss: 0.1813 score: 0.9302 time: 0.17s
Test loss: 0.2142 score: 0.9297 time: 0.17s
Epoch 199/1000, LR 0.000265
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.30s
Val loss: 0.1800 score: 0.9302 time: 0.24s
Test loss: 0.2132 score: 0.9297 time: 0.21s
Epoch 200/1000, LR 0.000265
Train loss: 0.5934;  Loss pred: 0.5934; Loss self: 0.0000; time: 0.38s
Val loss: 0.1790 score: 0.9302 time: 0.16s
Test loss: 0.2127 score: 0.9297 time: 0.17s
Epoch 201/1000, LR 0.000264
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.30s
Val loss: 0.1782 score: 0.9302 time: 0.16s
Test loss: 0.2120 score: 0.9297 time: 0.17s
Epoch 202/1000, LR 0.000264
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.30s
Val loss: 0.1779 score: 0.9302 time: 0.16s
Test loss: 0.2109 score: 0.9297 time: 0.26s
Epoch 203/1000, LR 0.000264
Train loss: 0.5928;  Loss pred: 0.5928; Loss self: 0.0000; time: 0.29s
Val loss: 0.1772 score: 0.9302 time: 0.16s
Test loss: 0.2101 score: 0.9297 time: 0.16s
Epoch 204/1000, LR 0.000264
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.30s
Val loss: 0.1763 score: 0.9302 time: 0.19s
Test loss: 0.2096 score: 0.9297 time: 0.16s
Epoch 205/1000, LR 0.000263
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.36s
Val loss: 0.1771 score: 0.9302 time: 0.24s
Test loss: 0.2097 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.5890;  Loss pred: 0.5890; Loss self: 0.0000; time: 0.30s
Val loss: 0.1839 score: 0.9302 time: 0.16s
Test loss: 0.2155 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 207/1000, LR 0.000263
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.31s
Val loss: 0.1891 score: 0.9302 time: 0.17s
Test loss: 0.2212 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 208/1000, LR 0.000263
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.36s
Val loss: 0.1898 score: 0.9302 time: 0.15s
Test loss: 0.2221 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 209/1000, LR 0.000262
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.34s
Val loss: 0.1858 score: 0.9302 time: 0.17s
Test loss: 0.2181 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.44s
Val loss: 0.1812 score: 0.9302 time: 0.16s
Test loss: 0.2141 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 211/1000, LR 0.000262
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.29s
Val loss: 0.1784 score: 0.9302 time: 0.15s
Test loss: 0.2125 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 212/1000, LR 0.000261
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.29s
Val loss: 0.1773 score: 0.9302 time: 0.16s
Test loss: 0.2130 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 213/1000, LR 0.000261
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.32s
Val loss: 0.1772 score: 0.9302 time: 0.22s
Test loss: 0.2138 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 214/1000, LR 0.000261
Train loss: 0.5869;  Loss pred: 0.5869; Loss self: 0.0000; time: 0.50s
Val loss: 0.1766 score: 0.9302 time: 0.23s
Test loss: 0.2123 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 215/1000, LR 0.000261
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.49s
Val loss: 0.1814 score: 0.9302 time: 0.23s
Test loss: 0.2135 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 216/1000, LR 0.000260
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.49s
Val loss: 0.2005 score: 0.9380 time: 0.23s
Test loss: 0.2339 score: 0.9219 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 217/1000, LR 0.000260
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.30s
Val loss: 0.2162 score: 0.9380 time: 0.15s
Test loss: 0.2533 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 218/1000, LR 0.000260
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.29s
Val loss: 0.2260 score: 0.9380 time: 0.16s
Test loss: 0.2657 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 219/1000, LR 0.000260
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 0.30s
Val loss: 0.2227 score: 0.9457 time: 0.16s
Test loss: 0.2616 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 220/1000, LR 0.000259
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.29s
Val loss: 0.2084 score: 0.9380 time: 0.15s
Test loss: 0.2435 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 221/1000, LR 0.000259
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.29s
Val loss: 0.1920 score: 0.9457 time: 0.15s
Test loss: 0.2239 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 222/1000, LR 0.000259
Train loss: 0.5901;  Loss pred: 0.5901; Loss self: 0.0000; time: 0.30s
Val loss: 0.1802 score: 0.9302 time: 0.23s
Test loss: 0.2117 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 223/1000, LR 0.000258
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.32s
Val loss: 0.1747 score: 0.9302 time: 0.15s
Test loss: 0.2078 score: 0.9297 time: 0.17s
Epoch 224/1000, LR 0.000258
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.29s
Val loss: 0.1733 score: 0.9302 time: 0.15s
Test loss: 0.2084 score: 0.9141 time: 0.16s
Epoch 225/1000, LR 0.000258
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.29s
Val loss: 0.1738 score: 0.9147 time: 0.15s
Test loss: 0.2105 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 226/1000, LR 0.000258
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.29s
Val loss: 0.1756 score: 0.9302 time: 0.15s
Test loss: 0.2143 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 227/1000, LR 0.000257
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.43s
Val loss: 0.1791 score: 0.9225 time: 0.23s
Test loss: 0.2201 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 228/1000, LR 0.000257
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 0.36s
Val loss: 0.1808 score: 0.9302 time: 0.15s
Test loss: 0.2228 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 229/1000, LR 0.000257
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.29s
Val loss: 0.1803 score: 0.9302 time: 0.15s
Test loss: 0.2225 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 230/1000, LR 0.000256
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 0.28s
Val loss: 0.1783 score: 0.9225 time: 0.16s
Test loss: 0.2199 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 231/1000, LR 0.000256
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.29s
Val loss: 0.1756 score: 0.9225 time: 0.17s
Test loss: 0.2163 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 232/1000, LR 0.000256
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.29s
Val loss: 0.1721 score: 0.9302 time: 0.16s
Test loss: 0.2113 score: 0.9062 time: 0.19s
Epoch 233/1000, LR 0.000255
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.42s
Val loss: 0.1693 score: 0.9225 time: 0.17s
Test loss: 0.2068 score: 0.9062 time: 0.17s
Epoch 234/1000, LR 0.000255
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.29s
Val loss: 0.1675 score: 0.9380 time: 0.16s
Test loss: 0.2035 score: 0.9062 time: 0.16s
Epoch 235/1000, LR 0.000255
Train loss: 0.5720;  Loss pred: 0.5720; Loss self: 0.0000; time: 0.28s
Val loss: 0.1665 score: 0.9302 time: 0.15s
Test loss: 0.2011 score: 0.9141 time: 0.16s
Epoch 236/1000, LR 0.000255
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.29s
Val loss: 0.1659 score: 0.9302 time: 0.16s
Test loss: 0.1997 score: 0.9219 time: 0.16s
Epoch 237/1000, LR 0.000254
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.34s
Val loss: 0.1658 score: 0.9302 time: 0.23s
Test loss: 0.1984 score: 0.9219 time: 0.20s
Epoch 238/1000, LR 0.000254
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.48s
Val loss: 0.1663 score: 0.9302 time: 0.21s
Test loss: 0.1977 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 239/1000, LR 0.000254
Train loss: 0.5697;  Loss pred: 0.5697; Loss self: 0.0000; time: 0.29s
Val loss: 0.1661 score: 0.9302 time: 0.15s
Test loss: 0.1972 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 240/1000, LR 0.000253
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.28s
Val loss: 0.1653 score: 0.9302 time: 0.17s
Test loss: 0.1966 score: 0.9297 time: 0.19s
Epoch 241/1000, LR 0.000253
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.43s
Val loss: 0.1642 score: 0.9302 time: 0.23s
Test loss: 0.1963 score: 0.9297 time: 0.20s
Epoch 242/1000, LR 0.000253
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.48s
Val loss: 0.1633 score: 0.9302 time: 0.23s
Test loss: 0.1963 score: 0.9219 time: 0.21s
Epoch 243/1000, LR 0.000252
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.43s
Val loss: 0.1629 score: 0.9302 time: 0.16s
Test loss: 0.1972 score: 0.9141 time: 0.16s
Epoch 244/1000, LR 0.000252
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.28s
Val loss: 0.1633 score: 0.9302 time: 0.15s
Test loss: 0.1996 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 245/1000, LR 0.000252
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.29s
Val loss: 0.1646 score: 0.9302 time: 0.15s
Test loss: 0.2029 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 246/1000, LR 0.000252
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.28s
Val loss: 0.1637 score: 0.9147 time: 0.16s
Test loss: 0.2012 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 247/1000, LR 0.000251
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.44s
Val loss: 0.1624 score: 0.9302 time: 0.24s
Test loss: 0.1959 score: 0.9141 time: 0.22s
Epoch 248/1000, LR 0.000251
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.39s
Val loss: 0.1640 score: 0.9302 time: 0.16s
Test loss: 0.1958 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000251
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 0.30s
Val loss: 0.1693 score: 0.9302 time: 0.16s
Test loss: 0.2000 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 250/1000, LR 0.000250
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.29s
Val loss: 0.1810 score: 0.9457 time: 0.16s
Test loss: 0.2126 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 251/1000, LR 0.000250
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.30s
Val loss: 0.1823 score: 0.9457 time: 0.16s
Test loss: 0.2148 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 252/1000, LR 0.000250
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.29s
Val loss: 0.1731 score: 0.9380 time: 0.17s
Test loss: 0.2060 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 253/1000, LR 0.000249
Train loss: 0.5716;  Loss pred: 0.5716; Loss self: 0.0000; time: 0.29s
Val loss: 0.1654 score: 0.9302 time: 0.17s
Test loss: 0.2013 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 254/1000, LR 0.000249
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.40s
Val loss: 0.1667 score: 0.9225 time: 0.19s
Test loss: 0.2054 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 255/1000, LR 0.000249
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.28s
Val loss: 0.1707 score: 0.9225 time: 0.15s
Test loss: 0.2117 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 256/1000, LR 0.000248
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.28s
Val loss: 0.1748 score: 0.9225 time: 0.15s
Test loss: 0.2181 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 257/1000, LR 0.000248
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.28s
Val loss: 0.1785 score: 0.9302 time: 0.15s
Test loss: 0.2238 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 258/1000, LR 0.000248
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.29s
Val loss: 0.1793 score: 0.9225 time: 0.16s
Test loss: 0.2254 score: 0.9062 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 259/1000, LR 0.000247
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.41s
Val loss: 0.1776 score: 0.9302 time: 0.24s
Test loss: 0.2230 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 260/1000, LR 0.000247
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.29s
Val loss: 0.1735 score: 0.9225 time: 0.16s
Test loss: 0.2169 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 261/1000, LR 0.000247
Train loss: 0.5644;  Loss pred: 0.5644; Loss self: 0.0000; time: 0.29s
Val loss: 0.1675 score: 0.9302 time: 0.16s
Test loss: 0.2078 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 262/1000, LR 0.000246
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.29s
Val loss: 0.1636 score: 0.9225 time: 0.16s
Test loss: 0.2019 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 263/1000, LR 0.000246
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.29s
Val loss: 0.1616 score: 0.9380 time: 0.16s
Test loss: 0.1982 score: 0.9062 time: 0.16s
Epoch 264/1000, LR 0.000246
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.33s
Val loss: 0.1610 score: 0.9302 time: 0.16s
Test loss: 0.1964 score: 0.9219 time: 0.20s
Epoch 265/1000, LR 0.000245
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.48s
Val loss: 0.1607 score: 0.9302 time: 0.23s
Test loss: 0.1955 score: 0.9219 time: 0.21s
Epoch 266/1000, LR 0.000245
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.28s
Val loss: 0.1603 score: 0.9302 time: 0.15s
Test loss: 0.1950 score: 0.9219 time: 0.16s
Epoch 267/1000, LR 0.000245
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.29s
Val loss: 0.1596 score: 0.9380 time: 0.15s
Test loss: 0.1950 score: 0.9219 time: 0.15s
Epoch 268/1000, LR 0.000244
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.29s
Val loss: 0.1594 score: 0.9380 time: 0.15s
Test loss: 0.1963 score: 0.9062 time: 0.16s
Epoch 269/1000, LR 0.000244
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.28s
Val loss: 0.1602 score: 0.9302 time: 0.15s
Test loss: 0.1984 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 270/1000, LR 0.000244
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.28s
Val loss: 0.1612 score: 0.9225 time: 0.17s
Test loss: 0.2005 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 271/1000, LR 0.000243
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.36s
Val loss: 0.1618 score: 0.9302 time: 0.15s
Test loss: 0.2018 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 272/1000, LR 0.000243
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.28s
Val loss: 0.1618 score: 0.9302 time: 0.15s
Test loss: 0.2021 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 273/1000, LR 0.000243
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.29s
Val loss: 0.1584 score: 0.9302 time: 0.19s
Test loss: 0.1965 score: 0.9062 time: 0.16s
Epoch 274/1000, LR 0.000242
Train loss: 0.5554;  Loss pred: 0.5554; Loss self: 0.0000; time: 0.28s
Val loss: 0.1589 score: 0.9302 time: 0.15s
Test loss: 0.1914 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 275/1000, LR 0.000242
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.28s
Val loss: 0.1662 score: 0.9457 time: 0.15s
Test loss: 0.1967 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 276/1000, LR 0.000242
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.41s
Val loss: 0.1713 score: 0.9457 time: 0.23s
Test loss: 0.2017 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 277/1000, LR 0.000241
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.30s
Val loss: 0.1704 score: 0.9457 time: 0.16s
Test loss: 0.2008 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 278/1000, LR 0.000241
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.29s
Val loss: 0.1646 score: 0.9457 time: 0.15s
Test loss: 0.1952 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 279/1000, LR 0.000241
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.29s
Val loss: 0.1582 score: 0.9302 time: 0.15s
Test loss: 0.1908 score: 0.9297 time: 0.16s
Epoch 280/1000, LR 0.000240
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.29s
Val loss: 0.1570 score: 0.9302 time: 0.15s
Test loss: 0.1946 score: 0.9141 time: 0.16s
Epoch 281/1000, LR 0.000240
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 0.29s
Val loss: 0.1621 score: 0.9302 time: 0.15s
Test loss: 0.2030 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 282/1000, LR 0.000240
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.28s
Val loss: 0.1683 score: 0.9225 time: 0.15s
Test loss: 0.2118 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 283/1000, LR 0.000239
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.34s
Val loss: 0.1735 score: 0.9225 time: 0.16s
Test loss: 0.2190 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 284/1000, LR 0.000239
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.29s
Val loss: 0.1759 score: 0.9302 time: 0.16s
Test loss: 0.2223 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 285/1000, LR 0.000239
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.29s
Val loss: 0.1755 score: 0.9302 time: 0.16s
Test loss: 0.2217 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 286/1000, LR 0.000238
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.27s
Val loss: 0.1733 score: 0.9225 time: 0.15s
Test loss: 0.2186 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 287/1000, LR 0.000238
Train loss: 0.5582;  Loss pred: 0.5582; Loss self: 0.0000; time: 0.27s
Val loss: 0.1696 score: 0.9225 time: 0.16s
Test loss: 0.2136 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 288/1000, LR 0.000237
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
Val loss: 0.1663 score: 0.9225 time: 0.15s
Test loss: 0.2091 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 289/1000, LR 0.000237
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.35s
Val loss: 0.1634 score: 0.9302 time: 0.22s
Test loss: 0.2054 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 290/1000, LR 0.000237
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.35s
Val loss: 0.1610 score: 0.9302 time: 0.15s
Test loss: 0.2021 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 291/1000, LR 0.000236
Train loss: 0.5518;  Loss pred: 0.5518; Loss self: 0.0000; time: 0.30s
Val loss: 0.1594 score: 0.9225 time: 0.25s
Test loss: 0.1999 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 292/1000, LR 0.000236
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.29s
Val loss: 0.1585 score: 0.9302 time: 0.15s
Test loss: 0.1986 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 293/1000, LR 0.000236
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.30s
Val loss: 0.1555 score: 0.9380 time: 0.16s
Test loss: 0.1937 score: 0.9141 time: 0.16s
Epoch 294/1000, LR 0.000235
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.34s
Val loss: 0.1588 score: 0.9302 time: 0.17s
Test loss: 0.1924 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 295/1000, LR 0.000235
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.31s
Val loss: 0.1674 score: 0.9457 time: 0.16s
Test loss: 0.1999 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 296/1000, LR 0.000235
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 0.38s
Val loss: 0.1756 score: 0.9457 time: 0.16s
Test loss: 0.2088 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 297/1000, LR 0.000234
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.43s
Val loss: 0.1822 score: 0.9457 time: 0.16s
Test loss: 0.2166 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 298/1000, LR 0.000234
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.28s
Val loss: 0.1788 score: 0.9457 time: 0.15s
Test loss: 0.2131 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 299/1000, LR 0.000234
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.29s
Val loss: 0.1711 score: 0.9457 time: 0.16s
Test loss: 0.2052 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 300/1000, LR 0.000233
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.32s
Val loss: 0.1640 score: 0.9380 time: 0.16s
Test loss: 0.1988 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 301/1000, LR 0.000233
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.48s
Val loss: 0.1592 score: 0.9302 time: 0.22s
Test loss: 0.1955 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 302/1000, LR 0.000232
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.29s
Val loss: 0.1566 score: 0.9302 time: 0.16s
Test loss: 0.1949 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 303/1000, LR 0.000232
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.29s
Val loss: 0.1558 score: 0.9302 time: 0.16s
Test loss: 0.1956 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 304/1000, LR 0.000232
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.30s
Val loss: 0.1559 score: 0.9380 time: 0.17s
Test loss: 0.1968 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 305/1000, LR 0.000231
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.30s
Val loss: 0.1565 score: 0.9302 time: 0.17s
Test loss: 0.1983 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 306/1000, LR 0.000231
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.42s
Val loss: 0.1572 score: 0.9302 time: 0.17s
Test loss: 0.1998 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 307/1000, LR 0.000231
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.30s
Val loss: 0.1580 score: 0.9302 time: 0.17s
Test loss: 0.2014 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 308/1000, LR 0.000230
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.1586 score: 0.9302 time: 0.16s
Test loss: 0.2027 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 309/1000, LR 0.000230
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.29s
Val loss: 0.1590 score: 0.9302 time: 0.16s
Test loss: 0.2037 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 310/1000, LR 0.000229
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.29s
Val loss: 0.1590 score: 0.9302 time: 0.18s
Test loss: 0.2040 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 311/1000, LR 0.000229
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.42s
Val loss: 0.1590 score: 0.9302 time: 0.16s
Test loss: 0.2043 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 312/1000, LR 0.000229
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 0.29s
Val loss: 0.1562 score: 0.9302 time: 0.15s
Test loss: 0.2004 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 313/1000, LR 0.000228
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.29s
Val loss: 0.1550 score: 0.9302 time: 0.15s
Test loss: 0.1956 score: 0.9141 time: 0.16s
Epoch 314/1000, LR 0.000228
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.43s
Val loss: 0.1566 score: 0.9302 time: 0.23s
Test loss: 0.1960 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 315/1000, LR 0.000228
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.49s
Val loss: 0.1546 score: 0.9302 time: 0.24s
Test loss: 0.1951 score: 0.9219 time: 0.21s
Epoch 316/1000, LR 0.000227
Train loss: 0.5435;  Loss pred: 0.5435; Loss self: 0.0000; time: 0.48s
Val loss: 0.1533 score: 0.9302 time: 0.23s
Test loss: 0.1949 score: 0.9141 time: 0.21s
Epoch 317/1000, LR 0.000227
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.39s
Val loss: 0.1526 score: 0.9302 time: 0.15s
Test loss: 0.1957 score: 0.9141 time: 0.17s
Epoch 318/1000, LR 0.000226
Train loss: 0.5412;  Loss pred: 0.5412; Loss self: 0.0000; time: 0.29s
Val loss: 0.1528 score: 0.9302 time: 0.15s
Test loss: 0.1969 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 319/1000, LR 0.000226
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.29s
Val loss: 0.1534 score: 0.9302 time: 0.15s
Test loss: 0.1985 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 320/1000, LR 0.000226
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.29s
Val loss: 0.1548 score: 0.9302 time: 0.16s
Test loss: 0.2010 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 321/1000, LR 0.000225
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.37s
Val loss: 0.1588 score: 0.9302 time: 0.17s
Test loss: 0.2070 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 322/1000, LR 0.000225
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.31s
Val loss: 0.1645 score: 0.9225 time: 0.17s
Test loss: 0.2146 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 323/1000, LR 0.000225
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.32s
Val loss: 0.1691 score: 0.9225 time: 0.25s
Test loss: 0.2205 score: 0.9062 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 324/1000, LR 0.000224
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.41s
Val loss: 0.1718 score: 0.9302 time: 0.17s
Test loss: 0.2240 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 325/1000, LR 0.000224
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.30s
Val loss: 0.1727 score: 0.9302 time: 0.16s
Test loss: 0.2253 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 326/1000, LR 0.000223
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.31s
Val loss: 0.1666 score: 0.9225 time: 0.17s
Test loss: 0.2172 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 327/1000, LR 0.000223
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.45s
Val loss: 0.1561 score: 0.9302 time: 0.21s
Test loss: 0.2028 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 328/1000, LR 0.000223
Train loss: 0.5402;  Loss pred: 0.5402; Loss self: 0.0000; time: 0.32s
Val loss: 0.1531 score: 0.9302 time: 0.17s
Test loss: 0.1976 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 329/1000, LR 0.000222
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.30s
Val loss: 0.1533 score: 0.9302 time: 0.17s
Test loss: 0.1965 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 330/1000, LR 0.000222
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.31s
Val loss: 0.1540 score: 0.9302 time: 0.17s
Test loss: 0.1967 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 331/1000, LR 0.000221
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.33s
Val loss: 0.1543 score: 0.9302 time: 0.24s
Test loss: 0.1970 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 332/1000, LR 0.000221
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.42s
Val loss: 0.1534 score: 0.9380 time: 0.21s
Test loss: 0.1974 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 333/1000, LR 0.000221
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.30s
Val loss: 0.1565 score: 0.9302 time: 0.17s
Test loss: 0.2033 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 334/1000, LR 0.000220
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.31s
Val loss: 0.1623 score: 0.9302 time: 0.16s
Test loss: 0.2112 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 335/1000, LR 0.000220
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.44s
Val loss: 0.1678 score: 0.9225 time: 0.25s
Test loss: 0.2183 score: 0.9062 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 336/1000, LR 0.000219
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.50s
Val loss: 0.1729 score: 0.9302 time: 0.24s
Test loss: 0.2248 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 337/1000, LR 0.000219
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.30s
Val loss: 0.1766 score: 0.9302 time: 0.17s
Test loss: 0.2295 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 316,   Train_Loss: 0.5419,   Val_Loss: 0.1526,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1526,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9141,   Test_loss: 0.1957


[0.18550081993453205, 0.17881224979646504, 0.17573861312121153, 0.2577355320099741, 0.2022567328531295, 0.17211303091607988, 0.166310572065413, 0.17223499296233058, 0.1801803819835186, 0.18138296413235366, 0.18385094893164933, 0.17019705404527485, 0.2469776349607855, 0.22974095190875232, 0.15848247497342527, 0.17440710798837245, 0.17386351386085153, 0.17969722906127572, 0.1796211798209697, 0.17906930297613144, 0.17334501794539392, 0.1796172980684787, 0.1747308778576553, 0.16008602804504335, 0.1703171960543841, 0.1642515070270747, 0.1655973067972809, 0.16576109500601888, 0.16391611401923, 0.17513206996954978, 0.198749439092353, 0.166643472854048, 0.16440593986772, 0.2519094788003713, 0.2454936089925468, 0.2414765718858689, 0.1716188699938357, 0.18237916403450072, 0.24969004304148257, 0.2515728371217847, 0.2983441250398755, 0.1735198860988021, 0.1800017210189253, 0.17322517721913755, 0.17736758897081017, 0.2539075929671526, 0.256005696952343, 0.21315478906035423, 0.18524248898029327, 0.18239466985687613, 0.262146086897701, 0.26021554390899837, 0.18385815899819136, 0.17073560901917517, 0.18151607899926603, 0.18426130688749254, 0.1716756538953632, 0.1702865760307759, 0.20834450190886855, 0.19112535985186696, 0.17619286989793181, 0.17856394685804844, 0.1769043488893658, 0.17274278309196234, 0.18005245691165328, 0.1758513138629496, 0.17481690202839673, 0.17525833495892584, 0.18266787007451057, 0.18278150400146842, 0.2582981828600168, 0.2172350890468806, 0.18220732104964554, 0.1808900791220367, 0.1837259188760072, 0.18281870894134045, 0.19448036095127463, 0.23501229286193848, 0.18756187101826072, 0.18166669085621834, 0.17910037911497056, 0.1715611049439758, 0.1838181670755148, 0.1751820920035243, 0.271491696825251, 0.18272928497754037, 0.18109903600998223, 0.26629261393100023, 0.26028010388836265, 0.2623029858805239, 0.1870569819584489, 0.18665865994989872, 0.1815332469996065, 0.18404843588359654, 0.18266013101674616, 0.17633383790962398, 0.17049042391590774, 0.2610649422276765, 0.17873506899923086, 0.1685474650003016, 0.18420575582422316, 0.18685633200220764, 0.27880440605804324, 0.18067441089078784, 0.16723427292890847, 0.1812123078852892, 0.24023600085638463, 0.16767417290247977, 0.1707209530286491, 0.17213853588327765, 0.17620230000466108, 0.2123954410199076, 0.22854979988187551, 0.18016470689326525, 0.18492684420198202, 0.18200580193661153, 0.1706594240386039, 0.18210552306845784, 0.18528857314959168, 0.1781629470642656, 0.17862609191797674, 0.18180767120793462, 0.18767557595856488, 0.1813076580874622, 0.2069702590815723, 0.18682906194590032, 0.17848302493803203, 0.1987478209193796, 0.24616809701547027, 0.1744152179453522, 0.1703364858403802, 0.176741466159001, 0.17538737412542105, 0.17286431905813515, 0.17562659294344485, 0.17685328307561576, 0.2559926831163466, 0.2536992128007114, 0.18110278784297407, 0.17706706700846553, 0.17453126003965735, 0.24760881578549743, 0.24962356011383235, 0.2624865039251745, 0.26454356289468706, 0.1914407960139215, 0.17118542804382741, 0.18134447210468352, 0.18281584000214934, 0.1835720429662615, 0.18879491998814046, 0.18703188421204686, 0.18715200200676918, 0.18853451195172966, 0.18700764421373606, 0.18035285198129714, 0.1802933090366423, 0.17964144004508853, 0.17333353590220213, 0.18540627998299897, 0.17697920300997794, 0.1782700449693948, 0.17348658293485641, 0.24418295198120177, 0.2518044509924948, 0.25598523393273354, 0.16908934502862394, 0.17368543916381896, 0.17799440305680037, 0.18218423798680305, 0.20566654298454523, 0.25751592009328306, 0.17747252387925982, 0.17573684989474714, 0.17484365799464285, 0.1744628290180117, 0.17125962884165347, 0.25380060193128884, 0.17809432791545987, 0.1718235390726477, 0.17202024697326124, 0.17807015497237444, 0.17170979408547282, 0.1765435580164194, 0.17278189095668495, 0.17237219284288585, 0.17188983992673457, 0.18410809198394418, 0.18117198208346963, 0.1722851269878447, 0.22295977198518813, 0.1733949889894575, 0.17004524497315288, 0.17876523197628558, 0.17158495704643428, 0.2559531629085541, 0.22993701579980552, 0.16811464610509574, 0.16746042692102492, 0.18059596722014248, 0.18346759700216353, 0.17166436091065407, 0.17095946590416133, 0.17611711099743843, 0.17317450512200594, 0.16605116985738277, 0.19472197513096035, 0.26222699298523366, 0.25810577697120607, 0.18595697521232069, 0.1793225659057498, 0.17529605003073812, 0.17913555004633963, 0.17679625586606562, 0.18948329705744982, 0.1835967020597309, 0.1707203600089997, 0.17111023585312068, 0.16761534404940903, 0.18101090611889958, 0.17756476090289652, 0.17675953591242433, 0.19757452001795173, 0.17322861216962337, 0.17119141900911927, 0.17452029697597027, 0.17029769299551845, 0.17299293586984277, 0.17455594101920724, 0.17398693319410086, 0.17272941279225051, 0.1763999629765749, 0.22262349678203464, 0.2139774188399315, 0.18146938900463283, 0.17692648898810148, 0.1807891079224646, 0.19723347993567586, 0.2580507849343121, 0.20674447296187282, 0.18569155992008746, 0.19239081302657723, 0.1749525482300669, 0.18949348898604512, 0.22923872503452003, 0.1755779990926385, 0.1865103088784963, 0.17524691694416106, 0.18264123587869108, 0.19052681792527437, 0.2119445251300931, 0.17786865797825158, 0.17893137596547604, 0.17846690490841866, 0.17551181698217988, 0.2580480619799346, 0.26010519498959184, 0.1866161951329559, 0.19031167193315923, 0.17996056308038533, 0.1774817209225148, 0.1862564340699464, 0.1772159629035741, 0.24158332613296807, 0.19553559413179755, 0.19151569297537208, 0.2729948149062693, 0.2728574341163039, 0.2816914359573275, 0.18495361902751029, 0.18312097596935928, 0.18133807787671685, 0.18411702709272504, 0.1820234141778201, 0.25595991499722004, 0.18304675980471075, 0.17512629297561944, 0.25368777313269675, 0.25392396794632077, 0.2580220850650221, 0.17654875293374062, 0.18604496796615422, 0.17178136110305786, 0.17708529205992818, 0.1751048678997904, 0.1834209298249334, 0.17630506493151188, 0.17604802502319217, 0.18446787516586483, 0.18562750495038927, 0.25264889816753566, 0.17205475619994104, 0.1711400339845568, 0.1727178420405835, 0.17171857506036758, 0.17442130902782083, 0.20533895585685968, 0.19190697302110493, 0.17884392710402608, 0.1738188900053501, 0.2518704270478338, 0.23945941892452538, 0.1741945561952889, 0.22102413792163134, 0.1781378409359604, 0.2562004930805415, 0.17736345413140953, 0.17874477989971638, 0.1766625780146569, 0.1788401089143008, 0.1995390320662409, 0.18516373983584344, 0.1823311229236424, 0.18149121780879796, 0.1769669100176543, 0.17595831002108753, 0.17889089905656874, 0.17528471793048084, 0.1741878530010581, 0.17528234492056072, 0.17819887399673462, 0.2491455008275807, 0.2520424018148333, 0.17948159901425242, 0.1839366969652474, 0.17395076411776245, 0.18474864400923252, 0.1836959079373628, 0.16979615413583815, 0.19004696211777627, 0.1743371351622045, 0.17799349292181432, 0.18063295306637883, 0.25775938108563423, 0.25170430005528033, 0.17178659397177398, 0.17136393091641366, 0.1836027349345386, 0.1827275101095438, 0.23441973398439586, 0.25793682411313057, 0.2428867369890213, 0.19541060994379222, 0.19513134798035026, 0.18671209388412535, 0.18771112989634275, 0.24483182816766202, 0.18081361101940274, 0.17977983807213604, 0.25385904987342656, 0.2578676789999008, 0.17892734403721988, 0.18731484096497297, 0.17540455609560013, 0.1870598099194467, 0.2826372650451958, 0.273664278909564, 0.18143965094350278, 0.18970200698822737, 0.18698874511756003, 0.2532944530248642, 0.17386857490055263, 0.2124169380404055, 0.18575779302045703, 0.1734295340720564, 0.17131974501535296, 0.17338656797073781, 0.17154585500247777, 0.18662565294653177, 0.2559188478626311, 0.17275197082199156, 0.24819549615494907, 0.2516547569539398, 0.25789276603609324, 0.23396851192228496, 0.17394864209927619, 0.1747530100401491, 0.17457638285122812, 0.17835925403051078, 0.17381290486082435, 0.17381499311886728, 0.17778858984820545, 0.17591822193935513, 0.17575586307793856, 0.18484813789837062, 0.1835363400168717, 0.20741556282155216, 0.24064873601309955, 0.17527725780382752, 0.17036231979727745, 0.18218352808617055, 0.18191710812970996, 0.1806134250946343, 0.2171987690962851, 0.18087451090104878, 0.17922521894797683, 0.1757134550716728, 0.18301929696463048, 0.25799153093248606, 0.18556735781021416, 0.18065059185028076, 0.2371318368241191, 0.2452766161877662, 0.2515466879121959, 0.1692174430936575, 0.17246132204309106, 0.17548922798596323, 0.17128448095172644, 0.2626408620271832, 0.17279089614748955, 0.17293653590604663, 0.1742003569379449, 0.1725699428934604, 0.17794292187318206, 0.1758512209635228, 0.21863044914789498, 0.270936856046319, 0.18388434499502182, 0.24743138789199293, 0.25151279708370566, 0.2496237640734762, 0.1712364531122148, 0.17392198694869876, 0.17388543207198381, 0.17877988307736814, 0.17019603098742664, 0.1776791859883815, 0.17421392095275223, 0.17846848210319877, 0.18023592210374773, 0.17611114913597703, 0.17901504784822464, 0.2537353939842433, 0.17392689594998956, 0.17309205001220107, 0.17414473090320826, 0.17518595699220896, 0.1732407060917467, 0.24212972121313214, 0.17506200820207596, 0.17119618598371744, 0.17159436200745404, 0.1854628291912377, 0.18660645792260766, 0.2673787400126457, 0.18534455494955182, 0.2421274660155177, 0.17499829805456102, 0.25071236910298467, 0.2580100509803742, 0.25318739213980734, 0.17544849682599306, 0.17864723200909793, 0.17602591612376273, 0.17477965308353305, 0.23975294292904437, 0.2549684988334775, 0.1714331889525056, 0.16904307692311704, 0.25949396495707333, 0.2643295209854841, 0.2665666099637747, 0.19757532887160778, 0.1848187129944563, 0.17126381606794894, 0.17829404002986848, 0.2621964728459716, 0.18008065992034972, 0.17856100806966424, 0.1802681158296764, 0.17251577321439981, 0.17282319604419172, 0.16651584999635816, 0.19459276902489364, 0.20056068012490869, 0.17295738798566163, 0.1801064710598439, 0.18168204301036894, 0.18079151189886034, 0.1816691409330815, 0.24750289507210255, 0.17800687602721155, 0.17522952798753977, 0.17230025096796453, 0.19023019610904157, 0.1931332158856094, 0.18348581693135202, 0.1826104549691081, 0.17597109614871442, 0.17038143100216985, 0.16882831207476556, 0.2247430521529168, 0.196894426131621, 0.2282135549467057, 0.19376973202452064, 0.193491404177621, 0.1876798931043595, 0.18744920985773206, 0.20055786496959627, 0.175065488088876, 0.17976589291356504, 0.22534258500672877, 0.22723098215647042, 0.18117403192445636, 0.17790270992554724, 0.17472069198265672, 0.1622117292135954, 0.20917682512663305, 0.22501043998636305, 0.16271342290565372, 0.16832473198883235, 0.17548996419645846, 0.16697654500603676, 0.1718598478473723, 0.17119381902739406, 0.16553500993177295, 0.17004690109752119, 0.17114146403037012, 0.17962164292111993, 0.2126688959542662, 0.16629514400847256, 0.1779258360620588, 0.16858728509396315, 0.1724277581088245, 0.19894921989180148, 0.17373279388993979, 0.15965189598500729, 0.17137591098435223, 0.1724698799662292, 0.19313151109963655, 0.2014699128922075, 0.1651811699848622, 0.16665575606748462, 0.16901137004606426, 0.16712553799152374, 0.1946448190137744, 0.18007399886846542, 0.18577736802399158, 0.17885270505212247, 0.18052550102584064, 0.17563309194520116, 0.18468626611866057, 0.18055069516412914, 0.1806547991000116, 0.1848658740054816, 0.17515908810310066, 0.16362340305931866, 0.16290351818315685, 0.16621990804560483, 0.16774574504233897, 0.19069868209771812, 0.17992774792946875, 0.17704916116781533, 0.16691446700133383, 0.17025015596300364, 0.16033868095837533, 0.16046234592795372, 0.16577698197215796, 0.1579161451663822, 0.1588152281474322, 0.15997144719585776, 0.1587108199018985, 0.16203047311864793, 0.1610960850957781, 0.16406345483846962, 0.16846944694407284, 0.1658655011560768, 0.16795043903402984, 0.16310676303692162, 0.1633470670785755, 0.16292893304489553, 0.16502681700512767, 0.1698398389853537, 0.17104030097834766, 0.17413800116628408, 0.21580709796398878, 0.16756550408899784, 0.1629116570111364, 0.15932470001280308, 0.17327481997199357, 0.16671910602599382, 0.16749451402574778, 0.17556091607548296, 0.169311068020761, 0.16936009819619358, 0.16085693682543933, 0.1671372780110687, 0.21719976514577866, 0.22168284002691507, 0.16955437208525836, 0.1687934820074588, 0.17186668096110225, 0.22378090117126703, 0.22184555395506322, 0.1723607659805566, 0.1689465360250324, 0.21970888692885637, 0.22381329000927508, 0.17590432381257415, 0.17215043399482965, 0.21335349697619677, 0.16281040315516293, 0.15951230400241911, 0.1734630549326539, 0.16270107589662075, 0.18994586309418082, 0.23018663609400392, 0.18052364303730428, 0.18035739404149354, 0.17405814211815596, 0.20722755999304354, 0.1755996011197567, 0.18178128194995224, 0.1814324629958719, 0.18825670005753636, 0.28123287903144956, 0.22007524501532316, 0.17757667298428714, 0.1799576231278479, 0.1909770960919559, 0.17816191702149808, 0.1696896101348102, 0.203480483032763, 0.17168086301535368, 0.17379282205365598, 0.17953020613640547, 0.17510191118344665, 0.16206706408411264, 0.1729994281195104, 0.17121905297972262, 0.17249592207372189, 0.16249209688976407, 0.1673749207984656, 0.16215806105174124, 0.1615515409503132, 0.1616979141253978, 0.1656985878944397, 0.1653595280367881, 0.16339910286478698, 0.16529526002705097, 0.16390427784062922, 0.2254083917941898, 0.211305360076949, 0.1685708409640938, 0.1630837400443852, 0.17100252211093903, 0.17395075899548829, 0.16494075115770102, 0.17293692007660866, 0.19643389387056231, 0.17668908485211432, 0.17631819401867688, 0.17300537694245577, 0.18994259997271, 0.18895114003680646, 0.18420634302310646, 0.17117783008143306, 0.1747947910334915, 0.1737947000656277, 0.17634148802608252, 0.17975779413245618, 0.17296864511445165, 0.17102812812663615, 0.16627254988998175, 0.172027942026034, 0.16041352413594723, 0.17170787788927555, 0.17871811101213098, 0.1721370059531182, 0.17055571102537215, 0.17400261503644288, 0.17427617288194597, 0.1690486459992826, 0.17550906096585095, 0.17630783305503428, 0.1708446650300175, 0.1677685349714011, 0.17720049107447267, 0.2239040059503168, 0.22603826690465212, 0.23533653910271823, 0.1774923950433731, 0.17947506997734308, 0.21587945614010096, 0.21343482797965407, 0.16669466393068433, 0.16868172492831945, 0.1927080019377172, 0.17086361208930612, 0.1690295720472932, 0.1694393358193338, 0.21429661684669554, 0.17451243894174695, 0.17417883896268904, 0.17256637592799962, 0.17129354900680482, 0.17838375712744892, 0.2115322989411652, 0.17188040306791663, 0.1726808389648795, 0.2673204520251602, 0.1650698739103973, 0.16435409407131374, 0.16925655282102525, 0.17891919310204685, 0.17530062422156334, 0.16205245000310242, 0.22197715798392892, 0.1720176269300282, 0.15549430600367486, 0.1707879789173603, 0.21778722200542688, 0.21128833503462374, 0.21588607295416296, 0.22458503604866564, 0.15945854783058167, 0.1649374880362302, 0.15972631284967065, 0.162288699997589, 0.17576210899278522, 0.21990880300290883, 0.17463967204093933, 0.16579113202169538, 0.16279543796554208, 0.16466137999668717, 0.21131724095903337, 0.1687450751196593, 0.16101689916104078, 0.16887930710799992, 0.16886411793529987, 0.19401957490481436, 0.17015409609302878, 0.16424693609587848, 0.1606183152180165, 0.16088722203858197, 0.20704972697421908, 0.21042486978694797, 0.16165932803414762, 0.19072245387360454, 0.20924456394277513, 0.2113736318424344, 0.16292546200565994, 0.16066016792319715, 0.15603687311522663, 0.17437456105835736, 0.22388331196270883, 0.17902747308835387, 0.1743712869938463, 0.1748793099541217, 0.17435788689181209, 0.17483803583309054, 0.18711674492806196, 0.16785186785273254, 0.15843295399099588, 0.15981076122261584, 0.15983384498395026, 0.19599513919092715, 0.1763769609387964, 0.17060361499898136, 0.16837294306606054, 0.16972210793755949, 0.16948823910206556, 0.20281223789788783, 0.21900763805024326, 0.1614062360022217, 0.15932618896476924, 0.1635982149746269, 0.1678636569995433, 0.1681686551310122, 0.16212251991964877, 0.16224072291515768, 0.16293327696621418, 0.16285159508697689, 0.1678236669395119, 0.21736822393722832, 0.16248768800869584, 0.16309824003838003, 0.16572673991322517, 0.16016498603858054, 0.16246983711607754, 0.16263687401078641, 0.22102406108751893, 0.16364687588065863, 0.1669483131263405, 0.1550221999641508, 0.1711618711706251, 0.16917410399764776, 0.21337098418734968, 0.16456486308015883, 0.16958125308156013, 0.17161678592674434, 0.16713958396576345, 0.1644738190807402, 0.1683349257800728, 0.17366059101186693, 0.16776191489771008, 0.1642566581722349, 0.17283225804567337, 0.20372362504713237, 0.17985083092935383, 0.1662731149699539, 0.17042930400930345, 0.17334502609446645, 0.18747103796340525, 0.16700755897909403, 0.1801787461154163, 0.1636218570638448, 0.16451483103446662, 0.18374159699305892, 0.16576619516126812, 0.16314402292482555, 0.16313459514640272, 0.2135592121630907, 0.21114243706688285, 0.21349413110874593, 0.17172108800150454, 0.1662461890373379, 0.16389115108177066, 0.16603085305541754, 0.1820646880660206, 0.1817328231409192, 0.23211034992709756, 0.18022186099551618, 0.17072966205887496, 0.18835920211859047, 0.17901198007166386, 0.18009500298649073, 0.1764424790162593, 0.18501270515844226, 0.22200705390423536, 0.179495750926435, 0.18148206104524434, 0.18716480815783143, 0.22604064480401576, 0.2195584310684353, 0.18568302714265883]
[0.0014379908522056748, 0.0013861414712904266, 0.0013623148303969887, 0.0019979498605424352, 0.0015678816500242598, 0.0013342095419851153, 0.001289229240817155, 0.001335154984204113, 0.0013967471471590589, 0.0014060694893980904, 0.0014252011545089094, 0.0013193570081029059, 0.0019145553097735311, 0.0017809376116957544, 0.0012285463176234517, 0.0013519930851811817, 0.0013477791772159033, 0.001393001775668804, 0.0013924122466741837, 0.0013881341315979182, 0.0013437598290340614, 0.0013923821555696023, 0.0013545029291291107, 0.0012409769615894834, 0.0013202883415068536, 0.0012732674963339124, 0.0012837000526921, 0.0012849697287288285, 0.0012706675505366667, 0.001357612945500386, 0.00154069332629731, 0.0012918098670856436, 0.0012744646501373644, 0.0019527866573672192, 0.0019030512325003627, 0.001871911409967976, 0.0013303788371615169, 0.0014137919692596955, 0.0019355817290037409, 0.0019501770319518192, 0.0023127451553478723, 0.001345115396114745, 0.0013953621784412815, 0.001342830831156105, 0.0013749425501613192, 0.001968275914474051, 0.001984540286452271, 0.0016523627058942188, 0.0014359882866689401, 0.0014139121694331483, 0.00203214020850931, 0.0020171747589844834, 0.0014252570464976075, 0.001323531852861823, 0.0014071013875912096, 0.0014283822239340507, 0.0013308190224446761, 0.001320050976982759, 0.0016150736582082833, 0.001481591936836178, 0.0013658362007591613, 0.001384216642310453, 0.0013713515417780294, 0.0013390913417981576, 0.0013957554799352968, 0.001363188479557749, 0.0013551697831658662, 0.001358591743867642, 0.0014160300005776013, 0.0014169108837323134, 0.00200231149503889, 0.0016839929383479116, 0.0014124598530980276, 0.0014022486753646256, 0.0014242319292713736, 0.001417199294118918, 0.0015075996972967027, 0.0018218007198599882, 0.001453967992389618, 0.0014082689213660337, 0.0013883750318989965, 0.0013299310460773317, 0.001424947031593138, 0.0013580007132056148, 0.002104586797094969, 0.0014165060850972121, 0.0014038684962014126, 0.0020642838289224824, 0.0020176752239407956, 0.002033356479693984, 0.001450054123708906, 0.0014469663562007653, 0.0014072344728651666, 0.0014267320611131514, 0.0014159700078817533, 0.0013669289760435967, 0.0013216311931465716, 0.002023759242075012, 0.0013855431705366733, 0.001306569496126369, 0.0014279515955366137, 0.001448498697691532, 0.0021612744655662266, 0.0014005768286107585, 0.0012963897126271975, 0.0014047465727541797, 0.0018622945802820514, 0.0012997997899417035, 0.0013234182405321634, 0.001334407254909129, 0.0013659093023617138, 0.0016464762869760278, 0.0017717038750532986, 0.0013966256348315136, 0.0014335414279223412, 0.0014108976894310972, 0.0013229412716170845, 0.0014116707214609134, 0.0014363455282914083, 0.0013811081167772528, 0.00138469838696106, 0.0014093617923095706, 0.0014548494260353867, 0.001405485721608234, 0.001604420613035444, 0.001448287301906204, 0.0013835893406048995, 0.001540680782320772, 0.001908279821825351, 0.0013520559530647456, 0.0013204378747316294, 0.001370088884953496, 0.0013595920474838842, 0.0013400334810708152, 0.001361446456925929, 0.0013709556827567113, 0.001984439404002687, 0.0019666605643466, 0.0014038975801780936, 0.0013726129225462443, 0.0013529555041833904, 0.001919448184383701, 0.0019350663574715687, 0.002034779100195151, 0.002050725293757264, 0.0014840371784024923, 0.0013270188220451738, 0.001405771101586694, 0.0014171770542802274, 0.001423039092761717, 0.0014635265115359726, 0.001449859567535247, 0.001450790713230769, 0.0014615078445870517, 0.0014496716605715972, 0.0013980841238860242, 0.001397622550671646, 0.0013925693026751048, 0.0013436708209473033, 0.001437257984364333, 0.0013719318062788987, 0.0013819383330960838, 0.0013448572320531504, 0.001892891100629471, 0.0019519724883139133, 0.0019843816583932834, 0.0013107701165009607, 0.0013463987532078988, 0.0013798015740837239, 0.0014122809146263802, 0.001594314286701901, 0.0019962474425835896, 0.0013757559990640296, 0.001362301161974784, 0.0013553771937569214, 0.0013524250311473774, 0.0013275940220283215, 0.0019674465265991383, 0.0013805761853911617, 0.0013319654191678116, 0.0013334902866144283, 0.0013803887982354608, 0.0013310836750811846, 0.0013685547133055768, 0.0013393945035401933, 0.0013362185491696578, 0.001332479379277012, 0.0014271945115034432, 0.0014044339696392995, 0.0013355436200608117, 0.0017283703254665746, 0.0013441472014686627, 0.0013181801935903323, 0.0013857769920642292, 0.0013301159460963898, 0.0019841330458027447, 0.0017824574868201977, 0.001303214310892215, 0.0012981428443490303, 0.0013999687381406395, 0.0014222294341252988, 0.001330731479927551, 0.0013252671775516383, 0.001365248922460763, 0.0013424380242015965, 0.0012872183709874633, 0.001509472675433801, 0.0020327673874824316, 0.0020008199765209774, 0.0014415269396303929, 0.0013900974101220915, 0.0013588841087654117, 0.0013886476747778265, 0.0013705136113648499, 0.0014688627678872078, 0.0014232302485250455, 0.0013234136434806178, 0.0013264359368458968, 0.0012993437523210003, 0.0014031853187511595, 0.0013764710147511358, 0.0013702289605614288, 0.0015315854264957498, 0.0013428574586792509, 0.0013270652636365836, 0.001352870519193568, 0.001320137155004019, 0.0013410305106189362, 0.0013531468296062578, 0.00134873591623334, 0.0013389876960639574, 0.0013674415734618208, 0.001725763540946005, 0.0016587396809297015, 0.0014067394496483166, 0.0013715231704503992, 0.0014014659528873226, 0.0015289417049277197, 0.002000393681661334, 0.0016026703330377738, 0.001439469456744864, 0.0014914016513688157, 0.0013562213041090458, 0.0014689417750856212, 0.0017770443801125584, 0.0013610697604080502, 0.0014458163478953201, 0.0013585032321252796, 0.0014158235339433417, 0.0014769520769401113, 0.001642980814961962, 0.001378826806032958, 0.0013870649299649305, 0.0013834643791350283, 0.001360556720792092, 0.002000372573487865, 0.0020163193410045878, 0.0014466371715733015, 0.001475284278551622, 0.0013950431246541497, 0.001375827293972983, 0.0014438483261236155, 0.0013737671542912722, 0.0018727389622710702, 0.001515779799471299, 0.0014846177750028843, 0.002116238875242398, 0.002115173907878325, 0.0021836545423048644, 0.001433748984709382, 0.0014195424493748782, 0.0014057215339280376, 0.0014272637759125972, 0.0014110342184327138, 0.0019841853875753493, 0.0014189671302690756, 0.001357568162601701, 0.0019665718847495873, 0.0019684028522970604, 0.002000171202054435, 0.0013685949839824854, 0.0014422090540011955, 0.0013316384581632393, 0.0013727542020149472, 0.0013574020767425613, 0.0014218676730614994, 0.0013667059297016425, 0.0013647133722728075, 0.0014299835284175568, 0.0014389729065921648, 0.001958518590446013, 0.001333757799999543, 0.0013266669301128433, 0.0013388980003146006, 0.0013311517446540123, 0.001352103170758301, 0.0015917748516035635, 0.0014876509536519763, 0.0013863870318141557, 0.0013474332558554272, 0.001952483930603363, 0.0018562745653063983, 0.0013503453968627046, 0.0017133654102452042, 0.0013809134956276, 0.0019860503339576858, 0.0013749104971427094, 0.0013856184488350107, 0.0013694773489508288, 0.0013863574334441922, 0.001546814202063883, 0.0014353778281848328, 0.0014134195575476155, 0.0014069086651844803, 0.001371836511764762, 0.0013640179071402133, 0.001386751155477277, 0.0013587962630269832, 0.0013502934341167296, 0.0013587778676012459, 0.0013813866201297256, 0.0019313604715316332, 0.001953817068332041, 0.0013913302249166854, 0.0014258658679476542, 0.0013484555357966082, 0.0014321600310793219, 0.0014239992863361456, 0.0013162492568669624, 0.001473232264478886, 0.001351450660172128, 0.0013797945187737544, 0.0014002554501269676, 0.00199813473709794, 0.0019511961244595374, 0.0013316790230370075, 0.0013284025652435167, 0.0014232770149964233, 0.0014164923264305721, 0.0018172072401891153, 0.0019995102644428725, 0.001882842922395514, 0.001514810929796839, 0.0015126461083748081, 0.0014473805727451578, 0.0014551250379561454, 0.0018979211485865273, 0.001401655899375215, 0.0013936421555979539, 0.0019678996114219113, 0.0019989742558131846, 0.0013870336747071308, 0.0014520530307362246, 0.0013597252410511638, 0.0014500760458871838, 0.002190986550737952, 0.0021214285186787907, 0.0014065089220426572, 0.0014705581937071888, 0.0014495251559500778, 0.001963522891665614, 0.0013478184100818033, 0.0016466429305457791, 0.00143998289163145, 0.001344414992806639, 0.0013280600388787052, 0.0013440819222537816, 0.0013298128294765717, 0.0014467104879576107, 0.0019838670376948146, 0.0013391625645115626, 0.0019239960942244113, 0.001950812069410386, 0.0019991687289619632, 0.001813709394746395, 0.0013484390860409006, 0.0013546744964352644, 0.001353305293420373, 0.0013826298762055099, 0.0013473868593862352, 0.0013474030474330797, 0.0013782061228543058, 0.0013637071468167064, 0.0013624485509917718, 0.0014329313015377567, 0.0014227623257121837, 0.0016078725800120323, 0.0018654940776209267, 0.0013587384325878104, 0.001320638137963391, 0.001412275411520702, 0.0014102101405403873, 0.001400104070501041, 0.0016837113883432953, 0.0014021279914809982, 0.001389342782542456, 0.0013621198067571534, 0.0014187542400358952, 0.0019999343483138453, 0.0014385066496915826, 0.0014003921848858975, 0.0018382312932102255, 0.0019013691177346217, 0.0019499743249007436, 0.0013117631247570349, 0.0013369094732022563, 0.0013603816122942885, 0.001327786674044391, 0.002035975674629327, 0.0013394643112208493, 0.0013405933015972608, 0.001350390363860038, 0.0013377514952981426, 0.001379402495140946, 0.0013631877594071533, 0.0016948096833170154, 0.0021002857057854187, 0.0014254600387210994, 0.0019180727743565342, 0.0019497116053000438, 0.001935067938554079, 0.0013274143652109674, 0.001348232456966657, 0.001347949085829332, 0.001385890566491226, 0.001319349077421912, 0.0013773580309176861, 0.0013504955112616452, 0.0013834766054511532, 0.0013971776907267266, 0.0013652027064804421, 0.0013877135492110437, 0.001966941038637545, 0.0013482705112402291, 0.0013417988373038842, 0.0013499591542884362, 0.001358030674358209, 0.0013429512100135402, 0.001876974583047536, 0.0013570698310238446, 0.001327102216928042, 0.0013301888527709614, 0.001437696350319672, 0.00144656168932254, 0.002072703410950742, 0.00143677949573296, 0.001876957100895486, 0.0013565759539113256, 0.0019435067372324394, 0.002000077914576544, 0.001962692962324088, 0.0013600658668681633, 0.0013848622636364182, 0.001364541985455525, 0.001354881031655295, 0.0018585499451863904, 0.00197649999095719, 0.0013289394492442294, 0.0013104114490164111, 0.0020115811236982428, 0.0020490660541510396, 0.002066407829176548, 0.00153159169667913, 0.0014327032015074132, 0.001327626481146891, 0.0013821243413168099, 0.002032530797255594, 0.0013959741079096878, 0.001384193861005149, 0.0013974272544936153, 0.001337331575305425, 0.00133971469801699, 0.0012908205426074276, 0.0015084710777123539, 0.0015547339544566565, 0.001340754945625284, 0.0013961741942623558, 0.0014083879303129375, 0.0014014845883632584, 0.0014082879142099341, 0.0019186270935821902, 0.0013798982637768338, 0.0013583684340119362, 0.0013356608602167793, 0.001474652683015826, 0.0014971567122915455, 0.0014223706738864497, 0.0014155849222411481, 0.001364117024408639, 0.001320786286838526, 0.0013087466052307407, 0.001742194202735789, 0.001526313380865279, 0.0017690973251682613, 0.0015020909459265166, 0.0014999333657179923, 0.0014548828922818567, 0.0014530946500599384, 0.0015547121315472579, 0.0013676991256943438, 0.0014044210383872269, 0.0017604889453650685, 0.0017752420480974251, 0.0014154221244098153, 0.0013898649212933378, 0.0013650054061145056, 0.001267279134481214, 0.0016341939463018207, 0.0017578940623934614, 0.0012711986164504196, 0.0013150369686627528, 0.0013710153452848317, 0.0013045042578596622, 0.001342655061307596, 0.001337451711151516, 0.0012932422650919762, 0.0013284914148243843, 0.0013370426877372665, 0.0014032940853212494, 0.0016614757496427046, 0.0012991808125661919, 0.0013900455942348344, 0.001317088164796587, 0.0013470918602251913, 0.001554290780404699, 0.0013572874522651546, 0.0012472804373828694, 0.0013388743045652518, 0.0013474209372361656, 0.0015088399304659106, 0.0015739836944703711, 0.001290477890506736, 0.0013019980942772236, 0.001320401328484877, 0.0013056682655587792, 0.0015206626485451125, 0.0014068281161598861, 0.0014513856876874343, 0.0013972867582197068, 0.00141035547676438, 0.001372133530821884, 0.0014428614540520357, 0.0014105523059697589, 0.0014113656179688405, 0.001444264640667825, 0.0013684303758054739, 0.001278307836400927, 0.0012726837358059129, 0.0012985930316062877, 0.0013105136331432732, 0.0014898334538884228, 0.0014056855306989746, 0.0013831965716235572, 0.0013040192734479206, 0.001330079343460966, 0.0012526459449873073, 0.0012536120775621384, 0.001295132671657484, 0.0012337198841123609, 0.0012407439699018141, 0.0012497769312176388, 0.001239928280483582, 0.001265863071239437, 0.0012585631648107665, 0.001281745740925544, 0.001316167554250569, 0.00129582422778185, 0.0013121128049533581, 0.0012742715862259502, 0.001276148961551371, 0.0012728822894132463, 0.00128927200785256, 0.0013268737420730758, 0.001336252351393341, 0.0013604531341115944, 0.0016859929528436624, 0.0013091055006952956, 0.0012727473203995032, 0.001244724218850024, 0.0013537095310311997, 0.0013024930158280768, 0.0013085508908261545, 0.0013715696568397107, 0.0013227427189121954, 0.0013231257671577623, 0.0012566948189487448, 0.0013057599844614742, 0.0016968731652013958, 0.001731897187710274, 0.001324643531916081, 0.001318699078183272, 0.0013427084450086113, 0.0017482882904005237, 0.0017331683902739314, 0.0013465684842230985, 0.0013198948126955656, 0.0017164756791316904, 0.0017485413281974616, 0.0013742525297857355, 0.0013449252655846067, 0.0016668241951265372, 0.0012719562746497104, 0.0012461898750188993, 0.0013551801166613586, 0.0012711021554423496, 0.0014839520554232877, 0.0017983330944844056, 0.0014103409612289397, 0.0014090421409491682, 0.0013598292352980934, 0.0016189653124456527, 0.0013718718837480992, 0.001420166265234002, 0.0014174411171552492, 0.0014707554691995028, 0.0021971318674331997, 0.0017193378516822122, 0.0013873177576897433, 0.0014059189306863118, 0.0014920085632184055, 0.0013918899767304538, 0.0013257000791782048, 0.001589691273693461, 0.0013412567423074506, 0.0013577564222941874, 0.0014025797354406677, 0.001367983681120677, 0.00126614893815713, 0.001351558032183675, 0.001337648851404083, 0.0013476243912009522, 0.0012694695069512818, 0.0013076165687380126, 0.0012668598519667285, 0.001262121413674322, 0.0012632649541046703, 0.0012945202179253101, 0.001291871312787407, 0.0012765554911311483, 0.0012913692189613357, 0.0012805021706299158, 0.001761003060892108, 0.001650823125601164, 0.0013169596950319828, 0.0012740917190967593, 0.0013359572039917111, 0.0013589903046522522, 0.0012885996184195392, 0.0013510696880985051, 0.001534639795863768, 0.0013803834754071431, 0.001377485890770913, 0.0013516045073629357, 0.001483926562286797, 0.0014761807815375505, 0.0014391120548680192, 0.0013373267975111958, 0.0013655843049491523, 0.0013577710942627164, 0.0013776678752037697, 0.001404357766659814, 0.0013513175399566535, 0.001336157250989345, 0.0012990042960154824, 0.0013439682970783906, 0.0012532306573120877, 0.0013414677960099652, 0.0013962352422822732, 0.001344820359008736, 0.00133246649238572, 0.00135939542997221, 0.0013615326006402029, 0.0013206925468693953, 0.0013711645387957105, 0.0013774049457424553, 0.0013347239455470117, 0.001310691679464071, 0.0013843788365193177, 0.00174925004648685, 0.0017659239601925947, 0.0018385667117399862, 0.0013866593362763524, 0.0014021489841979928, 0.0016865582510945387, 0.0016674595935910475, 0.0013023020619584713, 0.0013178259760024957, 0.0015055312651384156, 0.001334871969447704, 0.001320543531619478, 0.0013237448110885452, 0.001674192319114809, 0.001363378429232398, 0.0013607721793960081, 0.001348174811937497, 0.0013382308516156627, 0.0013936231025581947, 0.0016525960854778532, 0.0013428156489680987, 0.0013490690544131212, 0.002088441031446564, 0.0012896083899249788, 0.0012840163599321386, 0.0013223168189142598, 0.001397806196109741, 0.0013695361267309636, 0.0012660347656492377, 0.0017341965467494447, 0.0013438877103908453, 0.0012147992656537099, 0.0013342810852918774, 0.0017014626719173975, 0.001650690117457998, 0.001686609944954398, 0.0017545705941302003, 0.0012457699049264193, 0.0012885741252830485, 0.001247861819138052, 0.001267880468731164, 0.0013731414765061345, 0.0017180375234602252, 0.0013643724378198385, 0.0012952432189194951, 0.0012718393591057975, 0.0012864170312241185, 0.0016509159449924482, 0.0013183208993723383, 0.0012579445246956311, 0.0013193695867812494, 0.0013192509213695303, 0.0015157779289438622, 0.0013293288757267874, 0.0012831791882490506, 0.001254830587640754, 0.0012569314221764216, 0.0016175759919860866, 0.001643944295210531, 0.0012629635002667783, 0.0014900191708875354, 0.0016347231558029307, 0.0016513564987690188, 0.0012728551719192183, 0.0012551575618999777, 0.001219038071212708, 0.0013623012582684169, 0.0017490883747086627, 0.0013986521335027646, 0.0013622756796394242, 0.0013662446090165759, 0.001362170991342282, 0.0013659221549460199, 0.001461849569750484, 0.001311342717599473, 0.0012377574530546553, 0.0012485215720516862, 0.0012487019139371114, 0.0015312120249291183, 0.0013779450073343469, 0.0013328407421795418, 0.001315413617703598, 0.0013259539682621835, 0.0013241268679848872, 0.0015844706085772486, 0.0017109971722675255, 0.001260986218767357, 0.0012447358512872597, 0.0012781110544892726, 0.001311434820308932, 0.0013138176182110328, 0.001266582186872256, 0.0012675056477746693, 0.0012729162262985483, 0.001272278086617007, 0.0013111223979649367, 0.0016981892495095963, 0.0012694350625679363, 0.001274205000299844, 0.0012947401555720717, 0.0012512889534264104, 0.0012692956024693558, 0.0012706005782092689, 0.0017267504772462416, 0.0012784912178176455, 0.0013042836962995352, 0.001211110937219928, 0.0013372021185205085, 0.0013216726874816231, 0.0016669608139636694, 0.0012856629928137409, 0.0013248535396996886, 0.0013407561400526902, 0.001305777999732527, 0.001284951711568283, 0.0013151166076568188, 0.0013567233672802104, 0.00131063996013836, 0.001283255141970585, 0.0013502520159818232, 0.0015915908206807217, 0.0014050846166355768, 0.0012990087107027648, 0.0013314789375726832, 0.0013542580163630191, 0.0014646174840891035, 0.001304746554524172, 0.0014076464540266898, 0.0012782957583112875, 0.0012852721174567705, 0.0014354812265082728, 0.0012950483996974071, 0.0012745626791001996, 0.0012744890245812712, 0.0016684313450241461, 0.0016495502895850223, 0.0016679228992870776, 0.0013415710000117542, 0.0012987983518542023, 0.0012803996178263333, 0.0012971160394954495, 0.001422380375515786, 0.0014197876807884313, 0.0018133621088054497, 0.0014079832890274702, 0.0013338254848349607, 0.001471556266551488, 0.0013985310943098739, 0.0014069922108319588, 0.0013784568673145259, 0.0014454117590503301, 0.0017344301086268388, 0.0014023105541127734, 0.0014178286019159714, 0.001462225063733058, 0.001765942537531373, 0.0017153002427221509, 0.001450648649552022]
[695.4147159324007, 721.4270842564513, 734.0447139583679, 500.5130607874735, 637.8032423458283, 749.5074563116534, 775.6572441423743, 748.9767194301422, 715.9491981307923, 711.2024032525445, 701.6553395542093, 757.9449639926444, 522.3145003412244, 561.5019826819372, 813.9701252244517, 739.6487533558569, 741.961307093119, 717.8741746541442, 718.1781131188184, 720.3914789191685, 744.1806031058636, 718.193633838201, 738.2782115081571, 805.816732261627, 757.4103084623875, 785.3809218245775, 778.9981763285426, 778.2284497777716, 786.987910077384, 736.5869656107487, 649.0584355312697, 774.1077270574082, 784.6431832316557, 512.0887098584632, 525.4719278819044, 534.2133151574238, 751.6655948418347, 707.3176406028324, 516.640545328307, 512.7739603204935, 432.38659377910784, 743.4306401431562, 716.6598145272011, 744.6954424922266, 727.3031152339216, 508.0588512242264, 503.89503646090395, 605.1940027651641, 696.3845104333675, 707.2575097793451, 492.0920297785735, 495.74286786308744, 701.6278238773673, 755.5541620231788, 710.680842772731, 700.0927225527911, 751.4169719057885, 757.5465019432056, 619.1668069859869, 674.949677530927, 732.1522152101243, 722.4302680907367, 729.2076244020169, 746.7750472176011, 716.4578712930127, 733.5742745745797, 737.9149184273133, 736.0562910188146, 706.199727118845, 705.7606879028834, 499.4227933454367, 593.8267181696464, 707.9847245262539, 713.1402707440396, 702.1328334575342, 705.6170604584631, 663.3060498706079, 548.9074568358133, 687.7730494991745, 710.0916485680809, 720.2664820558005, 751.9186825132983, 701.7804717147745, 736.3766383004764, 475.15265294847103, 705.9623749737523, 712.3174305184567, 484.4295081854036, 495.61990360711434, 491.7976803312413, 689.6294308258158, 691.1010720564749, 710.613632114891, 700.9024520131618, 706.229647827053, 731.5669047373432, 756.640736981378, 494.1299237624109, 721.7386085578713, 765.3630388316382, 700.3038500224563, 690.3699682945501, 462.689961840646, 713.9915351819055, 771.3729831853192, 711.87217637369, 536.9719756412255, 769.3492549686058, 755.6190245631549, 749.3964052736646, 732.1130314223342, 607.3576691691277, 564.4284093299265, 716.0114887341576, 697.5731433512312, 708.7686141177398, 755.8914529725569, 708.3804918509023, 696.2113087019812, 724.0562761541438, 722.1789303840084, 709.5410173999857, 687.3563559942435, 711.4978008141876, 623.2779558398184, 690.4707364925605, 722.7578087316025, 649.0637200612518, 524.0321616163491, 739.6143611758597, 757.3245353956873, 729.8796530518108, 735.5147463907578, 746.2500110078624, 734.5129108183547, 729.4181807461526, 503.9206528468259, 508.4761540089346, 712.3026737271985, 728.5375094276145, 739.1226074382798, 520.9830659331299, 516.7781436222364, 491.45383884869483, 487.63235282861143, 673.8375659001082, 753.568814087219, 711.3533624864673, 705.6281337464159, 702.7213834718218, 683.2810968012454, 689.7219719700124, 689.2792949943124, 684.2248597595107, 689.8113739808541, 715.2645416074567, 715.5007620043315, 718.0971159417452, 744.2298994741797, 695.7693127321728, 728.8992028782449, 723.6212905098362, 743.5733520005926, 528.2924092503025, 512.3023024078511, 503.9353169640166, 762.9102825974192, 742.722018731392, 724.7418895460123, 708.0744274339717, 627.2289023192931, 500.9399028738526, 726.8730797324029, 734.0520788739589, 737.8019968213689, 739.4125197103272, 753.2423191181462, 508.2730262197094, 724.3352526153185, 750.7702419367481, 749.91172417077, 724.4335807986065, 751.2675714687948, 730.697859776919, 746.6060203747816, 748.380570395024, 750.4806570009274, 700.6753402846081, 712.0306270125535, 748.7587713192526, 578.5797090273749, 743.9661362292499, 758.6216246174176, 721.6168299276043, 751.8141579572739, 503.9984602420741, 561.0231982497057, 767.3335012070067, 770.3312500262344, 714.3016645701277, 703.1214345630677, 751.4664040670647, 754.5648280880597, 732.4671593203472, 744.9133456978332, 776.8689622048125, 662.4830089836599, 491.9402023851303, 499.7950898804991, 693.7088530973966, 719.3740472562787, 735.8979279760148, 720.125067116101, 729.6534610875791, 680.7987933674609, 702.6270001191605, 755.6216493053297, 753.8999602030373, 769.6192775881775, 712.6642408787486, 726.4955013824238, 729.8050389989322, 652.918200121549, 744.6806759248567, 753.5424424113702, 739.1690378441313, 757.4970496129666, 745.695188947239, 739.0181007119403, 741.4349895810097, 746.8328521162412, 731.2926704929672, 579.4536599445333, 602.8673525429339, 710.8636928110597, 729.1163733468729, 713.5385614896916, 654.0471731374969, 499.90159895401007, 623.9586391448046, 694.700394867248, 670.5101868984757, 737.3427898309994, 680.7621765278698, 562.7321473742034, 734.716198308747, 691.6507767087455, 736.1042479343776, 706.302710772724, 677.0700387732003, 608.6498338224064, 725.25424920996, 720.9467836702376, 722.8230918567065, 734.9932455721638, 499.9068739761775, 495.9531854223902, 691.2583332228648, 677.8354616384593, 716.8237184409003, 726.8354134131874, 692.5935237842875, 727.9253961461184, 533.9772494439375, 659.7264327897747, 673.5740450083573, 472.5364474204067, 472.7743644507574, 457.94789451654395, 697.4721591190511, 704.4523398651224, 711.3784457762973, 700.6413368549179, 708.7000350074683, 503.9851650263326, 704.7379595116994, 736.6112638378007, 508.49908297521233, 508.02608766443996, 499.9572031498456, 730.6763591154573, 693.3807531062491, 750.9545807045284, 728.4625306789711, 736.7013924126002, 703.300327411514, 731.6862964209895, 732.7545991101339, 699.3087543509095, 694.9401169534466, 510.58999637694035, 749.7613134861087, 753.7686945395874, 746.8828841069522, 751.2291547647079, 739.5885326111389, 628.2295508014805, 672.2006916643579, 721.299303190575, 742.1517879675184, 512.1681076734786, 538.7134094761133, 740.5512710476357, 583.6466605549645, 724.1583221297423, 503.5119115069244, 727.3200707087223, 721.6993977243678, 730.2055786217353, 721.3147027427505, 646.4900559263809, 696.6806790269234, 707.5040066199961, 710.7781938843027, 728.9498358033765, 733.1282051102909, 721.1099093375774, 735.9455035387758, 740.5797693551938, 735.9554669265965, 723.9102981221073, 517.769735241069, 511.81864270112686, 718.7366321031955, 701.3282402498126, 741.5891540014659, 698.2459908802006, 702.2475429555394, 759.7345220010052, 678.7795951195257, 739.9456224858661, 724.7455953722124, 714.1554063648354, 500.46675103220747, 512.506143008556, 750.9317055392332, 752.7838519467813, 702.6039129863367, 705.9692321241911, 550.2949679508931, 500.1224638767393, 531.1117502716129, 660.1483923370664, 661.093162811494, 690.9032902820862, 687.2261653916631, 526.8922793471941, 713.4418657573144, 717.5443107710398, 508.15600257039904, 500.2565676330829, 720.9630294024028, 688.6800818100816, 735.4426981343152, 689.6190050420295, 456.4153986537194, 471.3804831014491, 710.9802037712716, 680.0138915135756, 689.881093746565, 509.2886893474013, 741.939709770923, 607.2962033539053, 694.4526951060059, 743.8179471000778, 752.9780060578514, 744.0022690902512, 751.9855259582737, 691.2233016377362, 504.06603920490846, 746.7353303478384, 519.7515748612335, 512.6070397453714, 500.2079041718676, 551.3562442233623, 741.5982007285631, 738.1847097818948, 738.9315661897534, 723.2593604474977, 742.1773435251716, 742.1684268156341, 725.5808716978926, 733.2952696876995, 733.972669479568, 697.8701623217006, 702.8580824273906, 621.9398305757019, 536.051018331457, 735.9768267505553, 757.2096937486145, 708.0771865334862, 709.114174726331, 714.232621038049, 593.926017798074, 713.2016521143335, 719.7647784012162, 734.1498119616476, 704.8437085021149, 500.0164134603243, 695.1653648694645, 714.085675993314, 544.0011840151157, 525.9368055748406, 512.8272650722729, 762.332757436843, 747.9938021567999, 735.0878539981853, 753.1330292343089, 491.1650038166893, 746.5671101670146, 745.9383832580261, 740.5266112396857, 747.522991762481, 724.9515667273175, 733.5746621103003, 590.0367515264835, 476.1256991110369, 701.52790877055, 521.3566520360392, 512.8963674841073, 516.7777213792399, 753.3442655195831, 741.7118575010918, 741.8677830733832, 721.5576930664756, 757.9495200421565, 726.0276395482549, 740.4689550325057, 722.8167039903785, 715.7285767137186, 732.491955409353, 720.6098121392052, 508.40364828255207, 741.6909230478781, 745.2681968403917, 740.7631533319245, 736.3603922073332, 744.6286898166, 532.7722650225542, 736.8817559266995, 753.5214599481162, 751.7729515751587, 695.5571666977174, 691.2944033989482, 482.461694575638, 696.0010237965284, 532.7772273127102, 737.1500262235713, 514.5338479371591, 499.9805221146695, 509.5040432691356, 735.2585079594046, 722.0934718620798, 732.8466332724603, 738.0721824544793, 538.0538750599527, 505.9448543259111, 752.4797315398396, 763.1190957241677, 497.12138785709243, 488.02721511792146, 483.9315772426658, 652.9155271396728, 697.9812699153977, 753.2239031087527, 723.5239045477316, 491.9974651061824, 716.3456645319777, 722.4421579747781, 715.6007561641334, 747.7577127958084, 746.4275800513149, 774.7010269762388, 662.9228858113296, 643.1968615167197, 745.8484514734592, 716.2430047121251, 710.0316457396823, 713.5290736003474, 710.082071932721, 521.2060245292068, 724.6911067653366, 736.177295467993, 748.6930476031908, 678.1257793902294, 667.932749985405, 703.0516154186624, 706.4217655107574, 733.0749357325204, 757.1247596714759, 764.0898520792673, 573.9888230770644, 655.1734476920409, 565.2600259880494, 665.7386509863969, 666.6962832187663, 687.3405449366358, 688.1864164586602, 643.2058898291317, 731.1549603370019, 712.0371830575498, 568.0240155058925, 563.303466742311, 706.503016135181, 719.4943801225306, 732.597831129845, 789.0921366817659, 611.922472398701, 568.8624937036591, 786.6591318296977, 760.4348956188581, 729.3864386268029, 766.5747305729219, 744.7929321668901, 747.6905458807349, 773.2503236188924, 752.7335057202397, 747.919276752743, 712.6090036722949, 601.8745685665572, 769.7158011630125, 719.4008629267018, 759.2506156597659, 742.3398726741854, 643.3802558744025, 736.76360768761, 801.7443150942619, 746.8961026365441, 742.1585729929382, 662.7608269163534, 635.3305968245684, 774.9067282410603, 768.0502793324968, 757.3454967267198, 765.8913265936167, 657.6080506460429, 710.8188900358527, 688.9967349708058, 715.6727093543112, 709.0411009671034, 728.7920435855958, 693.0672360756931, 708.9421610016065, 708.5336267714562, 692.3938811778996, 730.7642520076248, 782.2841818880637, 785.7411640188527, 770.0641969124503, 763.0595933607308, 671.2159653752093, 711.3966660116021, 722.9630412011708, 766.8598312630213, 751.8348472339442, 798.3101721613226, 797.6949312299781, 772.1216689871785, 810.556766473357, 805.96805163529, 800.1427895022155, 806.4982594073844, 789.9748580396424, 794.5568629051342, 780.1859355334418, 759.7816833962329, 771.7096027072805, 762.1295945172543, 784.7620639189885, 783.6075804068624, 785.6185982923563, 775.6315144587853, 753.6512090725557, 748.3616391449392, 735.0492089189253, 593.1222893389681, 763.8803743998305, 785.701909540151, 803.3908112785659, 738.710910337051, 767.7584354371659, 764.2041337564252, 729.091661523149, 756.0049174357849, 755.7860521060849, 795.738141768201, 765.8375290252314, 589.3192375879865, 577.4014803512044, 754.9200791804811, 758.3231205239538, 744.7633205238287, 571.9880442434956, 576.9779818347296, 742.6284007953366, 757.6361315927456, 582.5890877206415, 571.905269766132, 727.6682984574265, 743.5357380733896, 599.9432951140266, 786.190547529155, 802.4459354436933, 737.9092916915092, 786.7188295751062, 673.876218807323, 556.0705094440287, 709.0483985720888, 709.7019818913105, 735.3864544475588, 617.6784593916797, 728.9310407527955, 704.1429052923104, 705.496678413677, 679.922679834926, 455.1388174840186, 581.6192547739195, 720.8153968022932, 711.2785653379325, 670.2374400874108, 718.447590483407, 754.3184282073025, 629.052959243223, 745.5694114757144, 736.5091290161678, 712.9719435778218, 731.0028721839591, 789.7965001301444, 739.8868388835125, 747.5803526092332, 742.0465276002, 787.7306185964, 764.7501751718435, 789.3532962210116, 792.3168002425163, 791.599574381245, 772.4869694215134, 774.0709079160132, 783.3580341375571, 774.3718723637477, 780.9436195708058, 567.858183899697, 605.758414994241, 759.3246807570027, 784.8728509976731, 748.526971531795, 735.8404225377361, 776.036237870763, 740.1542709520777, 651.618707331353, 724.4363742510398, 725.9602488126741, 739.8613977331743, 673.887795672958, 677.4238037148991, 694.8729229369911, 747.7603842688483, 732.2872680769681, 736.5011703559725, 725.8643523585762, 712.0692630756362, 740.0185155830044, 748.4149034550833, 769.8203947957393, 744.0651704164956, 797.9377093637229, 745.4521107210921, 716.2116882004849, 743.5937397148699, 750.4879152417154, 735.6211282985136, 734.4664384310684, 757.1784987887049, 729.3070756324378, 726.0029108295203, 749.21859560268, 762.955938202713, 722.3456279599417, 571.6735591966254, 566.2757981328587, 543.9019392740001, 721.157658437823, 713.1909741902243, 592.9234874342599, 599.7146820489942, 767.8710102755629, 758.8255340309867, 664.2173584539023, 749.1355147817993, 757.2639417449781, 755.4326118020266, 597.3029433850988, 733.4720709663968, 734.8768700164488, 741.7435714904612, 747.255227894864, 717.5541207406485, 605.1085372811149, 744.7038621931915, 741.2519001371838, 478.8260644866508, 775.429198361662, 778.8062763101016, 756.2484161859859, 715.4067586644827, 730.1742396434377, 789.8677249097407, 576.6359077778046, 744.109788539675, 823.1812681100678, 749.4672681965266, 587.7296143518028, 605.8072253682376, 592.905314587741, 569.940020279283, 802.7164535324558, 776.0515909632593, 801.3707805330081, 788.7178836351613, 728.2570784653831, 582.0594639783787, 732.9377025513081, 772.0557694439891, 786.2628191527884, 777.3528923574868, 605.7243574593825, 758.5406561301629, 794.9476152312499, 757.937737855253, 758.0059136603733, 659.727246917207, 752.2592928354674, 779.3143850505712, 796.9203252210573, 795.588352997385, 618.208977478816, 608.2931172992911, 791.7885194534664, 671.1323045624616, 611.7243745218913, 605.5627605216902, 785.6353354735514, 796.7127238482025, 820.3189249087131, 734.0520269878281, 571.7264001406248, 714.9740639908897, 734.0658098401099, 731.9333546866113, 734.1222257380485, 732.1061426370371, 684.0649138547717, 762.5771559021482, 807.912727596271, 800.9473143156889, 800.8316387111449, 653.0774208400637, 725.7183666092113, 750.2771849281404, 760.2171564452588, 754.173993921234, 755.2146430816276, 631.1256230230329, 584.4545018591319, 793.0300784551974, 803.3833033457155, 782.4046247683817, 762.5235997351602, 761.1406531156551, 789.5263413339455, 788.9511196700994, 785.5976531211734, 785.9916872882756, 762.7052985687328, 588.8625194681808, 787.7519925888159, 784.8030731041564, 772.3557469785566, 799.1759195681343, 787.8385445081085, 787.0293915727301, 579.1224691564952, 782.1719743268761, 766.7043625839705, 825.6881919467036, 747.8301044769569, 756.6169820044074, 599.8941256586699, 777.8088080543157, 754.8004138077596, 745.8477870261337, 765.8269630862505, 778.2393618352401, 760.388846264917, 737.0699319528012, 762.9860453014368, 779.2682587379978, 740.602486175782, 628.3021911198891, 711.7009097960683, 769.8177785574657, 751.0445503727029, 738.4117265080619, 682.7721305142924, 766.4323745730755, 710.4056541608277, 782.2915733687997, 778.0453543011171, 696.6304968212254, 772.1719128286277, 784.5828348794646, 784.6281770284734, 599.3653877232375, 606.2258339826489, 599.548096873921, 745.3947647878781, 769.94246148555, 781.0061687597557, 770.9410488740689, 703.0468201147519, 704.3306640361069, 551.4618371830593, 710.235702222521, 749.7232669262829, 679.5526768021216, 715.0359431182079, 710.7359886581725, 725.4488868760719, 691.8443784192151, 576.5582568165329, 713.1088025167785, 705.3038700507649, 683.8892485175994, 566.269841032262, 582.9883160355752, 689.3467969027594]
Elapsed: 0.18834892221036897~0.0282529840326829
Time per graph: 0.0014644364433360044~0.00021796213845393862
Speed: 695.4683116598488~85.46232178818936
Total Time: 0.1881
best val loss: 0.1526230379881323 test_score: 0.9141

Testing...
Test loss: 0.2536 score: 0.9219 time: 0.18s
test Score 0.9219
Epoch Time List: [0.8239756787661463, 0.6236044911202043, 0.6331821989733726, 0.7595500519964844, 0.8482453180477023, 0.6088090999983251, 0.6496180091053247, 0.6217460769694299, 0.6414959239773452, 0.6955828811042011, 0.6305475670378655, 0.6140511280391365, 0.7810982340015471, 0.9098405940458179, 0.6253328800667077, 0.7103549810126424, 0.6144525369163603, 0.6261607820633799, 0.7403728240169585, 0.8069042132701725, 0.7039818640332669, 0.6274044478777796, 0.6319562022108585, 0.6077718730084598, 0.6086278920993209, 0.6744760458823293, 0.5840120769571513, 0.5775033691897988, 0.5803266412112862, 0.6173808411695063, 0.6415360688697547, 0.6634452647995204, 0.5952710779383779, 0.8803626713342965, 0.8622529450803995, 0.8690097869839519, 0.7460525771602988, 0.6345903708133847, 0.7680564159527421, 0.8936272521968931, 0.9535178251098841, 0.6384823969565332, 0.6465099221095443, 0.6171660223044455, 0.6212789013516158, 0.8357093741651624, 0.8920101597905159, 0.8591716901864856, 0.648965593893081, 0.6517131519503891, 0.8611799110658467, 0.912932557053864, 0.7318892430048436, 0.6145636541768909, 0.6246371557936072, 0.6446521619800478, 0.6023623659275472, 0.6043679108843207, 0.6955953948199749, 0.6919526541605592, 0.6144061929080635, 0.6417774681467563, 0.6362417410127819, 0.6294366419315338, 0.6680072690360248, 0.7484603878110647, 0.6260786920320243, 0.6285013589076698, 0.6494267389643937, 0.6553798092063516, 0.7968141629826277, 0.8774844631552696, 0.646077929995954, 0.6490584048442543, 0.6537276052404195, 0.6500776971224695, 0.6638517118990421, 0.7627610019408166, 0.6585549777373672, 0.6570739890448749, 0.6481804309878498, 0.6201347578316927, 0.6345694190822542, 0.6158518770243973, 0.7596582188270986, 0.6522200659383088, 0.6398014891892672, 0.803307123016566, 0.9047670061700046, 0.9042480438947678, 0.8195602307096124, 0.6509504611603916, 0.6492633558809757, 0.6422780039720237, 0.6522263651713729, 0.6530720512382686, 0.6776470129843801, 0.7032519681379199, 0.6360432712826878, 0.601852540159598, 0.6623046428430825, 0.6343062610831112, 0.799636181909591, 0.6404277591500431, 0.6151040629483759, 0.652504411060363, 0.7059083867352456, 0.6308846545871347, 0.6026614818256348, 0.6165637541562319, 0.6278122852090746, 0.6860151079017669, 0.8645860392134637, 0.704924781806767, 0.6335008840542287, 0.6308942709583789, 0.6110741279553622, 0.6322687782812864, 0.651691421167925, 0.6932510212063789, 0.635333722922951, 0.6406801079865545, 0.6393647838849574, 0.6420045951381326, 0.6643908920232207, 0.732912341831252, 0.6456972660962492, 0.6640320960432291, 0.8765742559917271, 0.6433778791688383, 0.6036749398335814, 0.6230798892211169, 0.695938667980954, 0.6151980138383806, 0.6206345860846341, 0.6237334469333291, 0.8002478370908648, 0.8897736098151654, 0.7645444849040359, 0.6256802210118622, 0.6202262958977371, 0.7715411582030356, 0.8878010818734765, 0.9020431230310351, 0.9107847211416811, 0.8177629171404988, 0.6311936478596181, 0.6369975300040096, 0.6541366190649569, 0.6461082701571286, 0.6461186690721661, 0.6482937510591, 0.6731798411346972, 0.7248378631193191, 0.647709395037964, 0.6428006319329143, 0.6390416282229125, 0.646687283180654, 0.6046730659436435, 0.6568674987647682, 0.7146894389297813, 0.6473342534154654, 0.6135214022360742, 0.717304821126163, 0.888358797878027, 0.9052669282536954, 0.7358207150828093, 0.605680878739804, 0.6313637411221862, 0.6470294140744954, 0.6743892948143184, 0.8503534540068358, 0.7194087677635252, 0.6168382270261645, 0.6314596091397107, 0.6155268729198724, 0.6231117912102491, 0.7831663859542459, 0.7641757891979069, 0.6200609146617353, 0.6099010270554572, 0.6142331941518933, 0.6120095746591687, 0.6412484492175281, 0.7934351381845772, 0.603149127913639, 0.5974100418388844, 0.725382428150624, 0.6142530750948936, 0.6932932510972023, 0.6594680009875447, 0.6176397919189185, 0.6404625850263983, 0.6069277813658118, 0.6120993439108133, 0.7925973229575902, 0.8405770200770348, 0.6251732830423862, 0.6072609222028404, 0.6201574197039008, 0.6400704202242196, 0.6067351647652686, 0.6108383378013968, 0.6555246589705348, 0.681946184951812, 0.6144063179381192, 0.6519754009786993, 0.8943191240541637, 0.9133193730376661, 0.7977412638720125, 0.6352967619895935, 0.636064640013501, 0.6356404989492148, 0.6431184522807598, 0.6530271170195192, 0.6593148258980364, 0.6987321241758764, 0.626027753110975, 0.610611867858097, 0.6353965988382697, 0.6271204901859164, 0.6825660460162908, 0.6571799630764872, 0.6587495189160109, 0.6076941550709307, 0.6156731639057398, 0.6724251343403012, 0.5952292159199715, 0.6017583741340786, 0.6193952669855207, 0.6315567381680012, 0.6156106491107494, 0.6814493809361011, 0.7199494889937341, 0.6152782540302724, 0.6094155588652939, 0.6189218659419566, 0.6390399720985442, 0.8013825949747115, 0.8310768560040742, 0.6346857619937509, 0.6464912141673267, 0.6062832460738719, 0.6676808786578476, 0.8079148412216455, 0.6051028280053288, 0.6240410751197487, 0.614067945163697, 0.6189588399138302, 0.6188382091931999, 0.711321973009035, 0.697069653077051, 0.616029336117208, 0.6216701250523329, 0.6131518222391605, 0.7885286097880453, 0.8812710151541978, 0.713173417840153, 0.6269093537703156, 0.6112594909500331, 0.6183263801503927, 0.6254691160283983, 0.6211975610349327, 0.714544738875702, 0.6851020918693393, 0.6616590300109237, 0.712365924147889, 0.786045276094228, 0.8927098710555583, 0.640504885930568, 0.6412885978352278, 0.7147476421669126, 0.6335964999161661, 0.6411878520157188, 0.8657821379601955, 0.7753057780209929, 0.6102908849716187, 0.8123872990254313, 0.8791983600240201, 0.8833416518755257, 0.8205396239645779, 0.6235360712744296, 0.6109609568957239, 0.609842962352559, 0.6186385049950331, 0.6462256808299571, 0.6901703539770097, 0.6074202409945428, 0.6354922978207469, 0.6310316666495055, 0.7035565003752708, 0.6823145411908627, 0.597781425807625, 0.597758874297142, 0.6057640670333058, 0.5970489969477057, 0.6471218802034855, 0.7142437170259655, 0.6286489411722869, 0.6256425194442272, 0.6879950165748596, 0.8563078239094466, 0.6177655472420156, 0.7049224870279431, 0.6299086881335825, 0.8155472571961582, 0.738897348055616, 0.6226484919898212, 0.6232100939378142, 0.6245280993171036, 0.6489461550954729, 0.7154522640630603, 0.6241815427783877, 0.6201015843544155, 0.6160422649700195, 0.6298476650845259, 0.627048980910331, 0.6896379210520536, 0.6085178840439767, 0.612151691922918, 0.6141019291244447, 0.7061797780916095, 0.8671191390603781, 0.7491929698735476, 0.6415973198600113, 0.6184769370593131, 0.6319432030431926, 0.6416898861061782, 0.5891728582791984, 0.6648313959594816, 0.6815066360868514, 0.601274810032919, 0.6093038029503077, 0.7958739502355456, 0.8713451700750738, 0.7270444571040571, 0.6102439239621162, 0.6339271292090416, 0.6331090531311929, 0.7506456898991019, 0.8792399947997183, 0.8709449348971248, 0.6619887868873775, 0.6483040368184447, 0.6447684129234403, 0.6495258840732276, 0.7439531460404396, 0.6326329847797751, 0.6210048149805516, 0.7642069749999791, 0.8901292991358787, 0.8068793998099864, 0.6250999430194497, 0.7070559007115662, 0.6386692749802023, 0.7301406271290034, 0.7188807618804276, 0.6376237298827618, 0.6923763549420983, 0.6367924029473215, 0.6970563549548388, 0.6060872920788825, 0.6445523304864764, 0.6671042467933148, 0.5948961379472166, 0.6051361998543143, 0.6155504009220749, 0.6032647960819304, 0.6438573014456779, 0.6881738079246134, 0.5969999399967492, 0.6752008642069995, 0.8665209200698882, 0.874881638912484, 0.8661948288790882, 0.621627502143383, 0.5999377898406237, 0.6024609538726509, 0.6134882748592645, 0.6089413689915091, 0.5989137659780681, 0.6512886362615973, 0.693124873097986, 0.6108656437136233, 0.634737053886056, 0.6353650079108775, 0.6609001737087965, 0.70361099508591, 0.6258009327575564, 0.5980432338546962, 0.6146341860294342, 0.6229544538073242, 0.6279873279854655, 0.732424238929525, 0.6260704970918596, 0.6212669510859996, 0.6178928010631353, 0.6310255548451096, 0.8751377069856972, 0.7204944400582463, 0.608454855857417, 0.673918766900897, 0.8582129268907011, 0.8687733886763453, 0.7409706409089267, 0.6030031158588827, 0.662564214784652, 0.5987753001973033, 0.7386029539629817, 0.6718219420872629, 0.6050235431175679, 0.60654824716039, 0.6095457072369754, 0.6067442079074681, 0.6094715869985521, 0.6416015068534762, 0.7347282289993018, 0.6357091399841011, 0.6748069219756871, 0.8645675200968981, 0.8688317618798465, 0.7306257400196046, 0.6102859911043197, 0.6035954568069428, 0.6036460129544139, 0.5908117818180472, 0.6179231978021562, 0.6724123181775212, 0.6076554309111089, 0.6158420578576624, 0.6148065030574799, 0.654575782129541, 0.7435534331016243, 0.7030974589288235, 0.6028881198726594, 0.6140766448806971, 0.6037519171368331, 0.6092787052039057, 0.7352862739935517, 0.6512830839492381, 0.5975960660725832, 0.6002903587650508, 0.7050651160534471, 0.6319006588310003, 0.7182214139029384, 0.6239992049522698, 0.6876171531621367, 0.7350355710368603, 0.7009442588314414, 0.9758739820681512, 0.8766256410162896, 0.6430177758447826, 0.6133581469766796, 0.6151921020355076, 0.6118284049443901, 0.6976040420122445, 0.8782316038850695, 0.6437109098769724, 0.594265443040058, 0.7008984489366412, 0.8961281687952578, 0.9044947263319045, 0.7847866301890463, 0.6424846388399601, 0.5861296840012074, 0.6042355101089925, 0.816940694116056, 0.73330098669976, 0.6211436809971929, 0.6131949780974537, 0.5958885110449046, 0.5992711188737303, 0.5863240249454975, 0.602168181212619, 0.6758261281065643, 0.6106846688780934, 0.6153189858887345, 0.6385804410092533, 0.6372093537356704, 0.6289109552744776, 0.7256259662099183, 0.7414413739461452, 0.619613341987133, 0.7300378112122416, 0.6431440967135131, 0.6512566928286105, 0.71576386410743, 0.629283539019525, 0.5939375872258097, 0.5882061112206429, 0.5924044167622924, 0.6828851597383618, 0.62166987080127, 0.7273851491045207, 0.6861924780532718, 0.6679112778510898, 0.6530661219730973, 0.6283599250018597, 0.7428010401781648, 0.6783521759789437, 0.6330018660519272, 0.7046921169385314, 0.8907065207604319, 0.6542398575693369, 0.6534726649988443, 0.6421177389565855, 0.6029917348641902, 0.7285297398921102, 0.9359881300479174, 0.6079178729560226, 0.6235068431124091, 0.6220906090456992, 0.6135642291046679, 0.7188157299533486, 0.612794587155804, 0.605497342068702, 0.6155622103251517, 0.6230763921048492, 0.6800774610601366, 0.8329520868137479, 0.6206448408775032, 0.6429237208794802, 0.653781226137653, 0.6206026240251958, 0.6475279941223562, 0.8385061821900308, 0.6043198679108173, 0.6632323618978262, 0.6335771372541785, 0.6478779490571469, 0.938539544120431, 0.6433877360541373, 0.5969598924275488, 0.6221351427957416, 0.6254337860736996, 0.6740003689192235, 0.6608678682241589, 0.6445578671991825, 0.6293234662152827, 0.639492629095912, 0.7670825978275388, 0.6720331918913871, 0.6677847637329251, 0.6496185031719506, 0.6837010872550309, 0.6940553619060665, 0.6015513550955802, 0.5839336109347641, 0.6066337923984975, 0.6197240520268679, 0.638352194102481, 0.6881648029666394, 0.6200323652010411, 0.6123955650255084, 0.6179084510076791, 0.5958498772233725, 0.5942724947817624, 0.6775329990778118, 0.5962363190483302, 0.5870312328916043, 0.5935498480685055, 0.5903438930399716, 0.6068799982313067, 0.6188179550226778, 0.6733142118901014, 0.6061184839345515, 0.5925432001240551, 0.591337715042755, 0.6092265760526061, 0.6208667659666389, 0.6993371588177979, 0.5939544278662652, 0.6129748309031129, 0.6161849950440228, 0.6294468378182501, 0.7653221511282027, 0.6850449899211526, 0.600660297088325, 0.5960968411527574, 0.60174071486108, 0.5889881029725075, 0.5891500909347087, 0.6326416318770498, 0.7573651077691466, 0.6338383420370519, 0.5829231420066208, 0.6010836400091648, 0.7154348930343986, 0.936658653896302, 0.8217878742143512, 0.6198547938838601, 0.6095131631009281, 0.884388173231855, 0.9399803581181914, 0.6272317538969219, 0.6254242588765919, 0.7320124991238117, 0.9474973038304597, 0.8406035148072988, 0.6244918338488787, 0.7272069819737226, 0.7807191000320017, 0.587189873913303, 0.635024243965745, 0.5946856839582324, 0.6387356100603938, 0.9011281032580882, 0.7377160028554499, 0.6737120992038399, 0.6353940228000283, 0.6730633920524269, 0.7317429939284921, 0.6402780138887465, 0.7565516869071871, 0.6643470900598913, 0.7453443049453199, 0.7217255630530417, 0.6565129889640957, 0.6716713670175523, 0.6725824496243149, 0.665760399075225, 0.6152853411622345, 0.7540585547685623, 0.7159969101194292, 0.6374808540567756, 0.6337386148516089, 0.6263781439047307, 0.6880007251165807, 0.6363666621036828, 0.6337256683036685, 0.6339226076379418, 0.624730427749455, 0.669228533282876, 0.7461008811369538, 0.6008299230597913, 0.5988845867104828, 0.6458880151621997, 0.8690965741407126, 0.5951409798581153, 0.6014103626366705, 0.616571296704933, 0.7471350559499115, 0.932778991991654, 0.8964547240175307, 0.6162507147528231, 0.6072071678936481, 0.6118362238630652, 0.6138884481042624, 0.6366758330259472, 0.6509818909689784, 0.6442737120669335, 0.707143876934424, 0.6366646841634065, 0.6615591377485543, 0.6555059289094061, 0.6488988599739969, 0.6248574622441083, 0.7696925140917301, 0.6327305240556598, 0.6351916249841452, 0.6370839290320873, 0.6232336608227342, 0.6200941209681332, 0.6339579438790679, 0.7444985141046345, 0.5907714602071792, 0.6167592690326273, 0.6201484426856041, 0.6094751842319965, 0.6732481680810452, 0.8526882263831794, 0.6268333699554205, 0.6182875037193298, 0.6278069668915123, 0.6505893841385841, 0.880066704005003, 0.6252722369972616, 0.6448406151030213, 0.7394226975739002, 0.9625697243027389, 0.9755679629743099, 0.7283275842200965, 0.6447950240690261, 0.6740999452304095, 0.9348175507038832, 0.7915327781811357, 0.6123648658394814, 0.6762415871489793, 0.6252825341653079, 0.6117296321317554, 0.6171923179645091, 0.7089021902065724, 0.6628034990280867, 0.6314087470527738, 0.6368808089755476, 0.6298961937427521, 0.6387999679427594, 0.746101011056453, 0.7078327848576009, 0.6281642569229007, 0.726072974735871, 0.6086840759962797, 0.6459745729807764, 0.7668918608687818, 0.6295017437078059, 0.6495584920048714, 0.6671862609218806, 0.7241223289165646, 0.7665198100730777, 0.5840601990930736, 0.6176968570798635, 0.7503251840826124, 0.9335379640106112, 0.9314602839294821, 0.9445174629800022, 0.6005662311799824, 0.6150564609561116, 0.6071735990699381, 0.5942688970826566, 0.6117265580687672, 0.750377259682864, 0.6390065629966557, 0.6039007022045553, 0.6010572239756584, 0.6053520806599408, 0.8651714990846813, 0.682079449063167, 0.5951521119568497, 0.600883862003684, 0.6204468801151961, 0.633786530001089, 0.7486993910279125, 0.613060433184728, 0.5871860331390053, 0.6021455528680235, 0.7735466121230274, 0.8918699019122869, 0.6003990252502263, 0.6368420980870724, 0.857851147884503, 0.9209503589663655, 0.7448863699100912, 0.5854107898194343, 0.5831261910498142, 0.6112064279150218, 0.9018360069021583, 0.7296041077934206, 0.6336176351178437, 0.6262753119226545, 0.638036193093285, 0.6286119672004133, 0.645695521030575, 0.7500236951746047, 0.5859044038224965, 0.5855058042798191, 0.583674028981477, 0.6391212141606957, 0.8227831579279155, 0.6146934288553894, 0.6125879830215126, 0.6132168550975621, 0.6160125499591231, 0.6895047700963914, 0.9157922510057688, 0.5905007687397301, 0.5941154977772385, 0.6002687502186745, 0.6004301109351218, 0.6196521418169141, 0.6703148779924959, 0.5922064322512597, 0.6368963201530278, 0.5937157650478184, 0.5905470678117126, 0.8529709980357438, 0.615580677986145, 0.6014821899589151, 0.6001413899939507, 0.5980981031898409, 0.5978812787216157, 0.5923903540242463, 0.7215294549241662, 0.6071907430887222, 0.6055444430094212, 0.5689365109428763, 0.6000354061834514, 0.6119327638298273, 0.7737957490608096, 0.6645022921729833, 0.719326829072088, 0.6107287660706788, 0.6170830512419343, 0.6753781822044402, 0.6342310630716383, 0.7045133369974792, 0.7479706360027194, 0.5955923441797495, 0.626780326012522, 0.6788121131248772, 0.8752194228582084, 0.612266535172239, 0.6184706999920309, 0.6396454169880599, 0.6560491737909615, 0.75083729904145, 0.6411968350876123, 0.6173058289568871, 0.6083578581456095, 0.6500431739259511, 0.7427159699145705, 0.6038824098650366, 0.6002903638873249, 0.8719093250110745, 0.9308555000461638, 0.9263591761700809, 0.7100507020950317, 0.6041438379324973, 0.6002409688662738, 0.6151189019437879, 0.7162545358296484, 0.6586565771140158, 0.8007212560623884, 0.7605669018812478, 0.6258325160015374, 0.6617079412098974, 0.8357450568582863, 0.6637381443288177, 0.6420111970510334, 0.6568141628522426, 0.7834732790943235, 0.8071348229423165, 0.6442251100670546, 0.6543672399129719, 0.9081602157093585, 0.9560130683239549, 0.6549623771570623]
Total Epoch List: [225, 274, 337]
Total Time List: [0.17203435278497636, 0.20201673894189298, 0.18811199814081192]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2b73f10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.8958;  Loss pred: 1.8958; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.19s
Epoch 2/1000, LR 0.000015
Train loss: 1.8881;  Loss pred: 1.8881; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.19s
Epoch 3/1000, LR 0.000045
Train loss: 1.8912;  Loss pred: 1.8912; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.24s
Epoch 4/1000, LR 0.000075
Train loss: 1.8661;  Loss pred: 1.8661; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.19s
Epoch 5/1000, LR 0.000105
Train loss: 1.8428;  Loss pred: 1.8428; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 1.8111;  Loss pred: 1.8111; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 1.7813;  Loss pred: 1.7813; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.20s
Epoch 8/1000, LR 0.000195
Train loss: 1.7521;  Loss pred: 1.7521; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.21s
Epoch 9/1000, LR 0.000225
Train loss: 1.7114;  Loss pred: 1.7114; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.19s
Epoch 10/1000, LR 0.000255
Train loss: 1.6588;  Loss pred: 1.6588; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.6271;  Loss pred: 1.6271; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 1.5715;  Loss pred: 1.5715; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 1.5205;  Loss pred: 1.5205; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.18s
Test loss: 0.6924 score: 0.5116 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 1.4860;  Loss pred: 1.4860; Loss self: 0.0000; time: 0.29s
Val loss: 0.6926 score: 0.5039 time: 0.18s
Test loss: 0.6923 score: 0.5426 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 1.4541;  Loss pred: 1.4541; Loss self: 0.0000; time: 0.30s
Val loss: 0.6925 score: 0.5194 time: 0.18s
Test loss: 0.6923 score: 0.5581 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 1.4195;  Loss pred: 1.4195; Loss self: 0.0000; time: 0.29s
Val loss: 0.6924 score: 0.5504 time: 0.17s
Test loss: 0.6921 score: 0.6202 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 1.3811;  Loss pred: 1.3811; Loss self: 0.0000; time: 0.30s
Val loss: 0.6923 score: 0.7907 time: 0.17s
Test loss: 0.6920 score: 0.8062 time: 0.26s
Epoch 18/1000, LR 0.000285
Train loss: 1.3473;  Loss pred: 1.3473; Loss self: 0.0000; time: 0.28s
Val loss: 0.6922 score: 0.5736 time: 0.19s
Test loss: 0.6919 score: 0.5039 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 1.3248;  Loss pred: 1.3248; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 1.3050;  Loss pred: 1.3050; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 1.2789;  Loss pred: 1.2789; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4961 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 1.2570;  Loss pred: 1.2570; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.23s
Epoch 23/1000, LR 0.000285
Train loss: 1.2340;  Loss pred: 1.2340; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4961 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.2138;  Loss pred: 1.2138; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4961 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.2048;  Loss pred: 1.2048; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4961 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 1.1851;  Loss pred: 1.1851; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4961 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.1711;  Loss pred: 1.1711; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4961 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.1579;  Loss pred: 1.1579; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4961 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.1453;  Loss pred: 1.1453; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4961 time: 0.25s
Epoch 30/1000, LR 0.000285
Train loss: 1.1359;  Loss pred: 1.1359; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4961 time: 0.27s
Epoch 31/1000, LR 0.000285
Train loss: 1.1235;  Loss pred: 1.1235; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4961 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.1147;  Loss pred: 1.1147; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4961 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.1034;  Loss pred: 1.1034; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4961 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 1.0954;  Loss pred: 1.0954; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4961 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 1.0893;  Loss pred: 1.0893; Loss self: 0.0000; time: 0.29s
Val loss: 0.6882 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4961 time: 0.34s
Epoch 36/1000, LR 0.000285
Train loss: 1.0799;  Loss pred: 1.0799; Loss self: 0.0000; time: 0.26s
Val loss: 0.6878 score: 0.5426 time: 0.17s
Test loss: 0.6870 score: 0.5039 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.0731;  Loss pred: 1.0731; Loss self: 0.0000; time: 0.28s
Val loss: 0.6873 score: 0.6047 time: 0.18s
Test loss: 0.6864 score: 0.5504 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.0657;  Loss pred: 1.0657; Loss self: 0.0000; time: 0.27s
Val loss: 0.6868 score: 0.6434 time: 0.25s
Test loss: 0.6859 score: 0.5814 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 1.0614;  Loss pred: 1.0614; Loss self: 0.0000; time: 0.26s
Val loss: 0.6863 score: 0.6822 time: 0.17s
Test loss: 0.6853 score: 0.6357 time: 0.22s
Epoch 40/1000, LR 0.000284
Train loss: 1.0563;  Loss pred: 1.0563; Loss self: 0.0000; time: 0.36s
Val loss: 0.6857 score: 0.7209 time: 0.24s
Test loss: 0.6847 score: 0.6899 time: 0.25s
Epoch 41/1000, LR 0.000284
Train loss: 1.0490;  Loss pred: 1.0490; Loss self: 0.0000; time: 0.37s
Val loss: 0.6851 score: 0.7519 time: 0.25s
Test loss: 0.6840 score: 0.7442 time: 0.25s
Epoch 42/1000, LR 0.000284
Train loss: 1.0416;  Loss pred: 1.0416; Loss self: 0.0000; time: 0.33s
Val loss: 0.6844 score: 0.7519 time: 0.18s
Test loss: 0.6832 score: 0.7752 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.0421;  Loss pred: 1.0421; Loss self: 0.0000; time: 0.29s
Val loss: 0.6837 score: 0.7984 time: 0.18s
Test loss: 0.6824 score: 0.7984 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 1.0337;  Loss pred: 1.0337; Loss self: 0.0000; time: 0.26s
Val loss: 0.6830 score: 0.8062 time: 0.17s
Test loss: 0.6816 score: 0.8062 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.0301;  Loss pred: 1.0301; Loss self: 0.0000; time: 0.27s
Val loss: 0.6822 score: 0.8372 time: 0.18s
Test loss: 0.6807 score: 0.8295 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 1.0269;  Loss pred: 1.0269; Loss self: 0.0000; time: 0.27s
Val loss: 0.6813 score: 0.8682 time: 0.16s
Test loss: 0.6797 score: 0.8450 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0218;  Loss pred: 1.0218; Loss self: 0.0000; time: 0.27s
Val loss: 0.6804 score: 0.8682 time: 0.17s
Test loss: 0.6787 score: 0.8450 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 1.0191;  Loss pred: 1.0191; Loss self: 0.0000; time: 0.30s
Val loss: 0.6794 score: 0.8682 time: 0.17s
Test loss: 0.6776 score: 0.8450 time: 0.26s
Epoch 49/1000, LR 0.000284
Train loss: 1.0157;  Loss pred: 1.0157; Loss self: 0.0000; time: 0.28s
Val loss: 0.6783 score: 0.8682 time: 0.18s
Test loss: 0.6765 score: 0.8760 time: 0.19s
Epoch 50/1000, LR 0.000284
Train loss: 1.0117;  Loss pred: 1.0117; Loss self: 0.0000; time: 0.27s
Val loss: 0.6772 score: 0.8682 time: 0.18s
Test loss: 0.6752 score: 0.8760 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 1.0084;  Loss pred: 1.0084; Loss self: 0.0000; time: 0.27s
Val loss: 0.6761 score: 0.8682 time: 0.18s
Test loss: 0.6739 score: 0.8760 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 1.0045;  Loss pred: 1.0045; Loss self: 0.0000; time: 0.28s
Val loss: 0.6748 score: 0.8682 time: 0.18s
Test loss: 0.6725 score: 0.8760 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 1.0021;  Loss pred: 1.0021; Loss self: 0.0000; time: 0.27s
Val loss: 0.6735 score: 0.8837 time: 0.19s
Test loss: 0.6710 score: 0.8837 time: 0.18s
Epoch 54/1000, LR 0.000284
Train loss: 0.9991;  Loss pred: 0.9991; Loss self: 0.0000; time: 0.27s
Val loss: 0.6721 score: 0.8837 time: 0.18s
Test loss: 0.6695 score: 0.8837 time: 0.23s
Epoch 55/1000, LR 0.000284
Train loss: 0.9958;  Loss pred: 0.9958; Loss self: 0.0000; time: 0.28s
Val loss: 0.6706 score: 0.8760 time: 0.17s
Test loss: 0.6678 score: 0.8992 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 0.9920;  Loss pred: 0.9920; Loss self: 0.0000; time: 0.27s
Val loss: 0.6690 score: 0.8760 time: 0.17s
Test loss: 0.6660 score: 0.9070 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.27s
Val loss: 0.6673 score: 0.8760 time: 0.16s
Test loss: 0.6642 score: 0.9070 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 0.9844;  Loss pred: 0.9844; Loss self: 0.0000; time: 0.29s
Val loss: 0.6655 score: 0.8760 time: 0.17s
Test loss: 0.6622 score: 0.9070 time: 0.23s
Epoch 59/1000, LR 0.000283
Train loss: 0.9845;  Loss pred: 0.9845; Loss self: 0.0000; time: 0.38s
Val loss: 0.6635 score: 0.8760 time: 0.25s
Test loss: 0.6600 score: 0.9070 time: 0.25s
Epoch 60/1000, LR 0.000283
Train loss: 0.9812;  Loss pred: 0.9812; Loss self: 0.0000; time: 0.38s
Val loss: 0.6615 score: 0.8760 time: 0.17s
Test loss: 0.6578 score: 0.8992 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 0.9772;  Loss pred: 0.9772; Loss self: 0.0000; time: 0.27s
Val loss: 0.6593 score: 0.8760 time: 0.16s
Test loss: 0.6555 score: 0.8992 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 0.9736;  Loss pred: 0.9736; Loss self: 0.0000; time: 0.27s
Val loss: 0.6571 score: 0.8837 time: 0.18s
Test loss: 0.6530 score: 0.8915 time: 0.17s
Epoch 63/1000, LR 0.000283
Train loss: 0.9711;  Loss pred: 0.9711; Loss self: 0.0000; time: 0.34s
Val loss: 0.6546 score: 0.8760 time: 0.17s
Test loss: 0.6504 score: 0.8992 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 0.26s
Val loss: 0.6521 score: 0.8760 time: 0.18s
Test loss: 0.6475 score: 0.9070 time: 0.18s
Epoch 65/1000, LR 0.000283
Train loss: 0.9644;  Loss pred: 0.9644; Loss self: 0.0000; time: 0.27s
Val loss: 0.6494 score: 0.8760 time: 0.17s
Test loss: 0.6446 score: 0.9070 time: 0.17s
Epoch 66/1000, LR 0.000283
Train loss: 0.9610;  Loss pred: 0.9610; Loss self: 0.0000; time: 0.28s
Val loss: 0.6465 score: 0.8837 time: 0.17s
Test loss: 0.6414 score: 0.9070 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 0.9573;  Loss pred: 0.9573; Loss self: 0.0000; time: 0.28s
Val loss: 0.6436 score: 0.8915 time: 0.17s
Test loss: 0.6382 score: 0.9070 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 0.9537;  Loss pred: 0.9537; Loss self: 0.0000; time: 0.29s
Val loss: 0.6406 score: 0.8837 time: 0.17s
Test loss: 0.6348 score: 0.9070 time: 0.23s
Epoch 69/1000, LR 0.000283
Train loss: 0.9505;  Loss pred: 0.9505; Loss self: 0.0000; time: 0.29s
Val loss: 0.6375 score: 0.8837 time: 0.17s
Test loss: 0.6314 score: 0.9070 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9471;  Loss pred: 0.9471; Loss self: 0.0000; time: 0.27s
Val loss: 0.6342 score: 0.8915 time: 0.17s
Test loss: 0.6278 score: 0.9070 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9436;  Loss pred: 0.9436; Loss self: 0.0000; time: 0.27s
Val loss: 0.6308 score: 0.8915 time: 0.17s
Test loss: 0.6241 score: 0.9070 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9407;  Loss pred: 0.9407; Loss self: 0.0000; time: 0.27s
Val loss: 0.6272 score: 0.8915 time: 0.17s
Test loss: 0.6202 score: 0.9070 time: 0.17s
Epoch 73/1000, LR 0.000282
Train loss: 0.9374;  Loss pred: 0.9374; Loss self: 0.0000; time: 0.28s
Val loss: 0.6236 score: 0.8915 time: 0.17s
Test loss: 0.6162 score: 0.9070 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 0.9332;  Loss pred: 0.9332; Loss self: 0.0000; time: 0.34s
Val loss: 0.6197 score: 0.8915 time: 0.17s
Test loss: 0.6120 score: 0.9070 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9288;  Loss pred: 0.9288; Loss self: 0.0000; time: 0.27s
Val loss: 0.6158 score: 0.8915 time: 0.17s
Test loss: 0.6077 score: 0.9070 time: 0.17s
Epoch 76/1000, LR 0.000282
Train loss: 0.9249;  Loss pred: 0.9249; Loss self: 0.0000; time: 0.27s
Val loss: 0.6117 score: 0.8915 time: 0.17s
Test loss: 0.6032 score: 0.9147 time: 0.17s
Epoch 77/1000, LR 0.000282
Train loss: 0.9211;  Loss pred: 0.9211; Loss self: 0.0000; time: 0.27s
Val loss: 0.6076 score: 0.9070 time: 0.17s
Test loss: 0.5985 score: 0.9147 time: 0.17s
Epoch 78/1000, LR 0.000282
Train loss: 0.9173;  Loss pred: 0.9173; Loss self: 0.0000; time: 0.27s
Val loss: 0.6032 score: 0.9070 time: 0.17s
Test loss: 0.5936 score: 0.9147 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.9123;  Loss pred: 0.9123; Loss self: 0.0000; time: 0.32s
Val loss: 0.5986 score: 0.9070 time: 0.17s
Test loss: 0.5886 score: 0.9147 time: 0.22s
Epoch 80/1000, LR 0.000282
Train loss: 0.9088;  Loss pred: 0.9088; Loss self: 0.0000; time: 0.27s
Val loss: 0.5939 score: 0.9070 time: 0.17s
Test loss: 0.5833 score: 0.9147 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.9044;  Loss pred: 0.9044; Loss self: 0.0000; time: 0.27s
Val loss: 0.5891 score: 0.8992 time: 0.17s
Test loss: 0.5779 score: 0.9147 time: 0.17s
Epoch 82/1000, LR 0.000281
Train loss: 0.8988;  Loss pred: 0.8988; Loss self: 0.0000; time: 0.27s
Val loss: 0.5841 score: 0.8992 time: 0.17s
Test loss: 0.5723 score: 0.9147 time: 0.20s
Epoch 83/1000, LR 0.000281
Train loss: 0.8941;  Loss pred: 0.8941; Loss self: 0.0000; time: 0.32s
Val loss: 0.5789 score: 0.8992 time: 0.25s
Test loss: 0.5665 score: 0.9147 time: 0.25s
Epoch 84/1000, LR 0.000281
Train loss: 0.8907;  Loss pred: 0.8907; Loss self: 0.0000; time: 0.38s
Val loss: 0.5735 score: 0.8992 time: 0.25s
Test loss: 0.5605 score: 0.9147 time: 0.21s
Epoch 85/1000, LR 0.000281
Train loss: 0.8848;  Loss pred: 0.8848; Loss self: 0.0000; time: 0.34s
Val loss: 0.5679 score: 0.9070 time: 0.17s
Test loss: 0.5543 score: 0.9147 time: 0.18s
Epoch 86/1000, LR 0.000281
Train loss: 0.8797;  Loss pred: 0.8797; Loss self: 0.0000; time: 0.27s
Val loss: 0.5623 score: 0.9070 time: 0.16s
Test loss: 0.5480 score: 0.9147 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.8767;  Loss pred: 0.8767; Loss self: 0.0000; time: 0.28s
Val loss: 0.5566 score: 0.9070 time: 0.20s
Test loss: 0.5416 score: 0.9147 time: 0.23s
Epoch 88/1000, LR 0.000281
Train loss: 0.8694;  Loss pred: 0.8694; Loss self: 0.0000; time: 0.38s
Val loss: 0.5508 score: 0.9070 time: 0.26s
Test loss: 0.5351 score: 0.9147 time: 0.25s
Epoch 89/1000, LR 0.000281
Train loss: 0.8651;  Loss pred: 0.8651; Loss self: 0.0000; time: 0.38s
Val loss: 0.5449 score: 0.9070 time: 0.25s
Test loss: 0.5285 score: 0.9147 time: 0.25s
Epoch 90/1000, LR 0.000281
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 0.38s
Val loss: 0.5389 score: 0.9070 time: 0.25s
Test loss: 0.5216 score: 0.9147 time: 0.25s
Epoch 91/1000, LR 0.000280
Train loss: 0.8555;  Loss pred: 0.8555; Loss self: 0.0000; time: 0.37s
Val loss: 0.5329 score: 0.8992 time: 0.17s
Test loss: 0.5147 score: 0.9147 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.8505;  Loss pred: 0.8505; Loss self: 0.0000; time: 0.27s
Val loss: 0.5268 score: 0.8992 time: 0.17s
Test loss: 0.5076 score: 0.9225 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.8441;  Loss pred: 0.8441; Loss self: 0.0000; time: 0.28s
Val loss: 0.5206 score: 0.8992 time: 0.17s
Test loss: 0.5004 score: 0.9225 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8396;  Loss pred: 0.8396; Loss self: 0.0000; time: 0.28s
Val loss: 0.5142 score: 0.8992 time: 0.17s
Test loss: 0.4931 score: 0.9225 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.8341;  Loss pred: 0.8341; Loss self: 0.0000; time: 0.28s
Val loss: 0.5079 score: 0.8992 time: 0.17s
Test loss: 0.4859 score: 0.9225 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.8291;  Loss pred: 0.8291; Loss self: 0.0000; time: 0.27s
Val loss: 0.5018 score: 0.8915 time: 0.20s
Test loss: 0.4786 score: 0.9225 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8242;  Loss pred: 0.8242; Loss self: 0.0000; time: 0.30s
Val loss: 0.4957 score: 0.8915 time: 0.22s
Test loss: 0.4715 score: 0.9225 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.8179;  Loss pred: 0.8179; Loss self: 0.0000; time: 0.27s
Val loss: 0.4897 score: 0.8915 time: 0.17s
Test loss: 0.4644 score: 0.9225 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.8137;  Loss pred: 0.8137; Loss self: 0.0000; time: 0.27s
Val loss: 0.4837 score: 0.8915 time: 0.18s
Test loss: 0.4573 score: 0.9302 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8078;  Loss pred: 0.8078; Loss self: 0.0000; time: 0.29s
Val loss: 0.4777 score: 0.8915 time: 0.17s
Test loss: 0.4503 score: 0.9302 time: 0.17s
Epoch 101/1000, LR 0.000279
Train loss: 0.8034;  Loss pred: 0.8034; Loss self: 0.0000; time: 0.29s
Val loss: 0.4718 score: 0.8915 time: 0.18s
Test loss: 0.4433 score: 0.9225 time: 0.25s
Epoch 102/1000, LR 0.000279
Train loss: 0.7990;  Loss pred: 0.7990; Loss self: 0.0000; time: 0.27s
Val loss: 0.4659 score: 0.8992 time: 0.17s
Test loss: 0.4364 score: 0.9225 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.7922;  Loss pred: 0.7922; Loss self: 0.0000; time: 0.27s
Val loss: 0.4603 score: 0.8992 time: 0.17s
Test loss: 0.4296 score: 0.9302 time: 0.20s
Epoch 104/1000, LR 0.000279
Train loss: 0.7889;  Loss pred: 0.7889; Loss self: 0.0000; time: 0.27s
Val loss: 0.4547 score: 0.8915 time: 0.17s
Test loss: 0.4228 score: 0.9302 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.7844;  Loss pred: 0.7844; Loss self: 0.0000; time: 0.27s
Val loss: 0.4495 score: 0.8915 time: 0.18s
Test loss: 0.4162 score: 0.9302 time: 0.18s
Epoch 106/1000, LR 0.000279
Train loss: 0.7786;  Loss pred: 0.7786; Loss self: 0.0000; time: 0.28s
Val loss: 0.4442 score: 0.8915 time: 0.18s
Test loss: 0.4096 score: 0.9302 time: 0.19s
Epoch 107/1000, LR 0.000278
Train loss: 0.7724;  Loss pred: 0.7724; Loss self: 0.0000; time: 0.27s
Val loss: 0.4391 score: 0.8837 time: 0.19s
Test loss: 0.4032 score: 0.9302 time: 0.18s
Epoch 108/1000, LR 0.000278
Train loss: 0.7683;  Loss pred: 0.7683; Loss self: 0.0000; time: 0.36s
Val loss: 0.4338 score: 0.8837 time: 0.17s
Test loss: 0.3967 score: 0.9302 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.7662;  Loss pred: 0.7662; Loss self: 0.0000; time: 0.27s
Val loss: 0.4287 score: 0.8837 time: 0.17s
Test loss: 0.3903 score: 0.9380 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.7599;  Loss pred: 0.7599; Loss self: 0.0000; time: 0.27s
Val loss: 0.4238 score: 0.8837 time: 0.18s
Test loss: 0.3842 score: 0.9380 time: 0.18s
Epoch 111/1000, LR 0.000278
Train loss: 0.7542;  Loss pred: 0.7542; Loss self: 0.0000; time: 0.27s
Val loss: 0.4187 score: 0.8837 time: 0.17s
Test loss: 0.3779 score: 0.9380 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.7499;  Loss pred: 0.7499; Loss self: 0.0000; time: 0.32s
Val loss: 0.4139 score: 0.8837 time: 0.25s
Test loss: 0.3718 score: 0.9380 time: 0.25s
Epoch 113/1000, LR 0.000278
Train loss: 0.7467;  Loss pred: 0.7467; Loss self: 0.0000; time: 0.38s
Val loss: 0.4092 score: 0.8837 time: 0.22s
Test loss: 0.3659 score: 0.9380 time: 0.22s
Epoch 114/1000, LR 0.000277
Train loss: 0.7413;  Loss pred: 0.7413; Loss self: 0.0000; time: 0.27s
Val loss: 0.4046 score: 0.8837 time: 0.17s
Test loss: 0.3600 score: 0.9380 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.7376;  Loss pred: 0.7376; Loss self: 0.0000; time: 0.27s
Val loss: 0.4002 score: 0.8837 time: 0.18s
Test loss: 0.3543 score: 0.9380 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7336;  Loss pred: 0.7336; Loss self: 0.0000; time: 0.26s
Val loss: 0.3958 score: 0.8837 time: 0.18s
Test loss: 0.3486 score: 0.9380 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7302;  Loss pred: 0.7302; Loss self: 0.0000; time: 0.27s
Val loss: 0.3917 score: 0.8837 time: 0.18s
Test loss: 0.3432 score: 0.9380 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 0.29s
Val loss: 0.3876 score: 0.8837 time: 0.17s
Test loss: 0.3378 score: 0.9380 time: 0.25s
Epoch 119/1000, LR 0.000277
Train loss: 0.7214;  Loss pred: 0.7214; Loss self: 0.0000; time: 0.28s
Val loss: 0.3836 score: 0.8837 time: 0.16s
Test loss: 0.3325 score: 0.9380 time: 0.27s
Epoch 120/1000, LR 0.000277
Train loss: 0.7163;  Loss pred: 0.7163; Loss self: 0.0000; time: 0.27s
Val loss: 0.3795 score: 0.8837 time: 0.18s
Test loss: 0.3271 score: 0.9380 time: 0.19s
Epoch 121/1000, LR 0.000276
Train loss: 0.7159;  Loss pred: 0.7159; Loss self: 0.0000; time: 0.28s
Val loss: 0.3756 score: 0.8837 time: 0.18s
Test loss: 0.3219 score: 0.9380 time: 0.19s
Epoch 122/1000, LR 0.000276
Train loss: 0.7112;  Loss pred: 0.7112; Loss self: 0.0000; time: 0.37s
Val loss: 0.3717 score: 0.8837 time: 0.18s
Test loss: 0.3168 score: 0.9380 time: 0.20s
Epoch 123/1000, LR 0.000276
Train loss: 0.7070;  Loss pred: 0.7070; Loss self: 0.0000; time: 0.28s
Val loss: 0.3681 score: 0.8837 time: 0.25s
Test loss: 0.3119 score: 0.9380 time: 0.24s
Epoch 124/1000, LR 0.000276
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.35s
Val loss: 0.3646 score: 0.8915 time: 0.27s
Test loss: 0.3073 score: 0.9380 time: 0.17s
Epoch 125/1000, LR 0.000276
Train loss: 0.7006;  Loss pred: 0.7006; Loss self: 0.0000; time: 0.27s
Val loss: 0.3611 score: 0.8915 time: 0.16s
Test loss: 0.3025 score: 0.9380 time: 0.17s
Epoch 126/1000, LR 0.000276
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.27s
Val loss: 0.3579 score: 0.8915 time: 0.17s
Test loss: 0.2981 score: 0.9380 time: 0.17s
Epoch 127/1000, LR 0.000275
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.35s
Val loss: 0.3547 score: 0.8992 time: 0.18s
Test loss: 0.2938 score: 0.9380 time: 0.18s
Epoch 128/1000, LR 0.000275
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.35s
Val loss: 0.3514 score: 0.8992 time: 0.18s
Test loss: 0.2893 score: 0.9380 time: 0.18s
Epoch 129/1000, LR 0.000275
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.28s
Val loss: 0.3482 score: 0.8992 time: 0.18s
Test loss: 0.2851 score: 0.9380 time: 0.18s
Epoch 130/1000, LR 0.000275
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.27s
Val loss: 0.3451 score: 0.8992 time: 0.18s
Test loss: 0.2809 score: 0.9380 time: 0.18s
Epoch 131/1000, LR 0.000275
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.28s
Val loss: 0.3421 score: 0.8992 time: 0.18s
Test loss: 0.2768 score: 0.9380 time: 0.26s
Epoch 132/1000, LR 0.000275
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.38s
Val loss: 0.3390 score: 0.8992 time: 0.26s
Test loss: 0.2725 score: 0.9380 time: 0.20s
Epoch 133/1000, LR 0.000274
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.27s
Val loss: 0.3361 score: 0.8915 time: 0.17s
Test loss: 0.2683 score: 0.9380 time: 0.17s
Epoch 134/1000, LR 0.000274
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.27s
Val loss: 0.3332 score: 0.8837 time: 0.17s
Test loss: 0.2642 score: 0.9457 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.27s
Val loss: 0.3305 score: 0.8837 time: 0.17s
Test loss: 0.2604 score: 0.9457 time: 0.17s
Epoch 136/1000, LR 0.000274
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.27s
Val loss: 0.3278 score: 0.8837 time: 0.17s
Test loss: 0.2567 score: 0.9380 time: 0.19s
Epoch 137/1000, LR 0.000274
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.28s
Val loss: 0.3253 score: 0.8915 time: 0.23s
Test loss: 0.2532 score: 0.9380 time: 0.18s
Epoch 138/1000, LR 0.000274
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.28s
Val loss: 0.3228 score: 0.8992 time: 0.17s
Test loss: 0.2498 score: 0.9380 time: 0.18s
Epoch 139/1000, LR 0.000273
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.28s
Val loss: 0.3204 score: 0.8915 time: 0.18s
Test loss: 0.2463 score: 0.9380 time: 0.18s
Epoch 140/1000, LR 0.000273
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.28s
Val loss: 0.3181 score: 0.8992 time: 0.20s
Test loss: 0.2431 score: 0.9380 time: 0.26s
Epoch 141/1000, LR 0.000273
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.40s
Val loss: 0.3158 score: 0.8992 time: 0.19s
Test loss: 0.2401 score: 0.9380 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6515;  Loss pred: 0.6515; Loss self: 0.0000; time: 0.28s
Val loss: 0.3135 score: 0.8992 time: 0.18s
Test loss: 0.2371 score: 0.9380 time: 0.18s
Epoch 143/1000, LR 0.000273
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.29s
Val loss: 0.3113 score: 0.8992 time: 0.18s
Test loss: 0.2342 score: 0.9380 time: 0.19s
Epoch 144/1000, LR 0.000272
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.27s
Val loss: 0.3092 score: 0.8992 time: 0.16s
Test loss: 0.2314 score: 0.9380 time: 0.17s
Epoch 145/1000, LR 0.000272
Train loss: 0.6438;  Loss pred: 0.6438; Loss self: 0.0000; time: 0.27s
Val loss: 0.3071 score: 0.8992 time: 0.19s
Test loss: 0.2286 score: 0.9380 time: 0.18s
Epoch 146/1000, LR 0.000272
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.34s
Val loss: 0.3051 score: 0.8992 time: 0.18s
Test loss: 0.2255 score: 0.9380 time: 0.18s
Epoch 147/1000, LR 0.000272
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 0.28s
Val loss: 0.3031 score: 0.8992 time: 0.18s
Test loss: 0.2229 score: 0.9380 time: 0.18s
Epoch 148/1000, LR 0.000272
Train loss: 0.6376;  Loss pred: 0.6376; Loss self: 0.0000; time: 0.28s
Val loss: 0.3012 score: 0.8992 time: 0.18s
Test loss: 0.2205 score: 0.9380 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.28s
Val loss: 0.2994 score: 0.8992 time: 0.17s
Test loss: 0.2182 score: 0.9380 time: 0.18s
Epoch 150/1000, LR 0.000271
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.27s
Val loss: 0.2976 score: 0.8992 time: 0.17s
Test loss: 0.2160 score: 0.9380 time: 0.18s
Epoch 151/1000, LR 0.000271
Train loss: 0.6322;  Loss pred: 0.6322; Loss self: 0.0000; time: 0.31s
Val loss: 0.2957 score: 0.8992 time: 0.18s
Test loss: 0.2135 score: 0.9380 time: 0.25s
Epoch 152/1000, LR 0.000271
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.37s
Val loss: 0.2938 score: 0.8915 time: 0.20s
Test loss: 0.2111 score: 0.9380 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 0.28s
Val loss: 0.2921 score: 0.8992 time: 0.17s
Test loss: 0.2091 score: 0.9380 time: 0.17s
Epoch 154/1000, LR 0.000271
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 0.28s
Val loss: 0.2903 score: 0.8915 time: 0.17s
Test loss: 0.2069 score: 0.9380 time: 0.17s
Epoch 155/1000, LR 0.000270
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.28s
Val loss: 0.2886 score: 0.8837 time: 0.18s
Test loss: 0.2048 score: 0.9380 time: 0.18s
Epoch 156/1000, LR 0.000270
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.30s
Val loss: 0.2869 score: 0.8915 time: 0.25s
Test loss: 0.2029 score: 0.9380 time: 0.25s
Epoch 157/1000, LR 0.000270
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.41s
Val loss: 0.2853 score: 0.8992 time: 0.19s
Test loss: 0.2013 score: 0.9380 time: 0.19s
Epoch 158/1000, LR 0.000270
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.30s
Val loss: 0.2837 score: 0.8915 time: 0.18s
Test loss: 0.1995 score: 0.9380 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.29s
Val loss: 0.2821 score: 0.8915 time: 0.18s
Test loss: 0.1977 score: 0.9380 time: 0.19s
Epoch 160/1000, LR 0.000269
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 0.34s
Val loss: 0.2806 score: 0.8915 time: 0.26s
Test loss: 0.1960 score: 0.9380 time: 0.27s
Epoch 161/1000, LR 0.000269
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.38s
Val loss: 0.2792 score: 0.8992 time: 0.26s
Test loss: 0.1945 score: 0.9380 time: 0.25s
Epoch 162/1000, LR 0.000269
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.35s
Val loss: 0.2776 score: 0.8915 time: 0.17s
Test loss: 0.1928 score: 0.9380 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.28s
Val loss: 0.2760 score: 0.8915 time: 0.17s
Test loss: 0.1910 score: 0.9380 time: 0.18s
Epoch 164/1000, LR 0.000269
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 0.27s
Val loss: 0.2745 score: 0.8915 time: 0.17s
Test loss: 0.1895 score: 0.9380 time: 0.18s
Epoch 165/1000, LR 0.000268
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 0.28s
Val loss: 0.2730 score: 0.8915 time: 0.18s
Test loss: 0.1882 score: 0.9380 time: 0.18s
Epoch 166/1000, LR 0.000268
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.28s
Val loss: 0.2715 score: 0.8837 time: 0.19s
Test loss: 0.1864 score: 0.9380 time: 0.19s
Epoch 167/1000, LR 0.000268
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.35s
Val loss: 0.2701 score: 0.8837 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.30s
Val loss: 0.2688 score: 0.8837 time: 0.18s
Test loss: 0.1839 score: 0.9380 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.27s
Val loss: 0.2673 score: 0.8837 time: 0.18s
Test loss: 0.1820 score: 0.9457 time: 0.19s
Epoch 170/1000, LR 0.000267
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.27s
Val loss: 0.2660 score: 0.8837 time: 0.17s
Test loss: 0.1808 score: 0.9380 time: 0.17s
Epoch 171/1000, LR 0.000267
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.28s
Val loss: 0.2648 score: 0.8837 time: 0.19s
Test loss: 0.1794 score: 0.9457 time: 0.19s
Epoch 172/1000, LR 0.000267
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 0.29s
Val loss: 0.2635 score: 0.8837 time: 0.19s
Test loss: 0.1783 score: 0.9457 time: 0.20s
Epoch 173/1000, LR 0.000267
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.34s
Val loss: 0.2623 score: 0.8837 time: 0.17s
Test loss: 0.1771 score: 0.9457 time: 0.18s
Epoch 174/1000, LR 0.000266
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.29s
Val loss: 0.2612 score: 0.8915 time: 0.18s
Test loss: 0.1761 score: 0.9457 time: 0.18s
Epoch 175/1000, LR 0.000266
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.29s
Val loss: 0.2601 score: 0.8915 time: 0.18s
Test loss: 0.1752 score: 0.9457 time: 0.19s
Epoch 176/1000, LR 0.000266
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.29s
Val loss: 0.2589 score: 0.8915 time: 0.18s
Test loss: 0.1742 score: 0.9457 time: 0.19s
Epoch 177/1000, LR 0.000266
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.31s
Val loss: 0.2577 score: 0.8915 time: 0.25s
Test loss: 0.1730 score: 0.9535 time: 0.25s
Epoch 178/1000, LR 0.000265
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.28s
Val loss: 0.2566 score: 0.8915 time: 0.17s
Test loss: 0.1720 score: 0.9535 time: 0.17s
Epoch 179/1000, LR 0.000265
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.27s
Val loss: 0.2553 score: 0.8915 time: 0.17s
Test loss: 0.1706 score: 0.9535 time: 0.18s
Epoch 180/1000, LR 0.000265
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.28s
Val loss: 0.2542 score: 0.8915 time: 0.18s
Test loss: 0.1694 score: 0.9535 time: 0.18s
Epoch 181/1000, LR 0.000265
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.28s
Val loss: 0.2531 score: 0.8915 time: 0.17s
Test loss: 0.1684 score: 0.9535 time: 0.19s
Epoch 182/1000, LR 0.000265
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.28s
Val loss: 0.2522 score: 0.8915 time: 0.17s
Test loss: 0.1674 score: 0.9535 time: 0.19s
Epoch 183/1000, LR 0.000264
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 0.28s
Val loss: 0.2512 score: 0.8915 time: 0.23s
Test loss: 0.1662 score: 0.9535 time: 0.21s
Epoch 184/1000, LR 0.000264
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 0.27s
Val loss: 0.2504 score: 0.8915 time: 0.17s
Test loss: 0.1656 score: 0.9535 time: 0.18s
Epoch 185/1000, LR 0.000264
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.27s
Val loss: 0.2498 score: 0.8915 time: 0.18s
Test loss: 0.1656 score: 0.9535 time: 0.19s
Epoch 186/1000, LR 0.000264
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.27s
Val loss: 0.2490 score: 0.8915 time: 0.16s
Test loss: 0.1649 score: 0.9535 time: 0.17s
Epoch 187/1000, LR 0.000263
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.27s
Val loss: 0.2481 score: 0.8915 time: 0.18s
Test loss: 0.1640 score: 0.9535 time: 0.19s
Epoch 188/1000, LR 0.000263
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.30s
Val loss: 0.2474 score: 0.8915 time: 0.18s
Test loss: 0.1633 score: 0.9535 time: 0.24s
Epoch 189/1000, LR 0.000263
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.29s
Val loss: 0.2465 score: 0.8992 time: 0.18s
Test loss: 0.1625 score: 0.9535 time: 0.19s
Epoch 190/1000, LR 0.000263
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.27s
Val loss: 0.2457 score: 0.8992 time: 0.18s
Test loss: 0.1618 score: 0.9535 time: 0.19s
Epoch 191/1000, LR 0.000262
Train loss: 0.5743;  Loss pred: 0.5743; Loss self: 0.0000; time: 0.26s
Val loss: 0.2449 score: 0.8992 time: 0.16s
Test loss: 0.1610 score: 0.9535 time: 0.17s
Epoch 192/1000, LR 0.000262
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.26s
Val loss: 0.2441 score: 0.8992 time: 0.18s
Test loss: 0.1601 score: 0.9535 time: 0.18s
Epoch 193/1000, LR 0.000262
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.26s
Val loss: 0.2434 score: 0.8992 time: 0.16s
Test loss: 0.1595 score: 0.9535 time: 0.18s
Epoch 194/1000, LR 0.000262
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.30s
Val loss: 0.2425 score: 0.8992 time: 0.22s
Test loss: 0.1584 score: 0.9535 time: 0.16s
Epoch 195/1000, LR 0.000261
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 0.26s
Val loss: 0.2418 score: 0.8992 time: 0.16s
Test loss: 0.1575 score: 0.9535 time: 0.17s
Epoch 196/1000, LR 0.000261
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.27s
Val loss: 0.2412 score: 0.8992 time: 0.16s
Test loss: 0.1568 score: 0.9535 time: 0.17s
Epoch 197/1000, LR 0.000261
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.27s
Val loss: 0.2405 score: 0.8992 time: 0.16s
Test loss: 0.1561 score: 0.9535 time: 0.17s
Epoch 198/1000, LR 0.000261
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 0.27s
Val loss: 0.2399 score: 0.8992 time: 0.16s
Test loss: 0.1555 score: 0.9535 time: 0.20s
Epoch 199/1000, LR 0.000260
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.32s
Val loss: 0.2394 score: 0.9070 time: 0.26s
Test loss: 0.1551 score: 0.9535 time: 0.26s
Epoch 200/1000, LR 0.000260
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.36s
Val loss: 0.2391 score: 0.9070 time: 0.18s
Test loss: 0.1548 score: 0.9535 time: 0.18s
Epoch 201/1000, LR 0.000260
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.27s
Val loss: 0.2385 score: 0.9070 time: 0.18s
Test loss: 0.1541 score: 0.9535 time: 0.18s
Epoch 202/1000, LR 0.000260
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.28s
Val loss: 0.2380 score: 0.9070 time: 0.18s
Test loss: 0.1534 score: 0.9535 time: 0.18s
Epoch 203/1000, LR 0.000259
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.28s
Val loss: 0.2376 score: 0.9070 time: 0.18s
Test loss: 0.1527 score: 0.9535 time: 0.18s
Epoch 204/1000, LR 0.000259
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.28s
Val loss: 0.2372 score: 0.9070 time: 0.18s
Test loss: 0.1521 score: 0.9535 time: 0.21s
Epoch 205/1000, LR 0.000259
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.28s
Val loss: 0.2369 score: 0.9070 time: 0.26s
Test loss: 0.1516 score: 0.9535 time: 0.18s
Epoch 206/1000, LR 0.000259
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.27s
Val loss: 0.2365 score: 0.9070 time: 0.17s
Test loss: 0.1511 score: 0.9535 time: 0.17s
Epoch 207/1000, LR 0.000258
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.27s
Val loss: 0.2362 score: 0.9070 time: 0.16s
Test loss: 0.1506 score: 0.9535 time: 0.16s
Epoch 208/1000, LR 0.000258
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.37s
Val loss: 0.2360 score: 0.9070 time: 0.18s
Test loss: 0.1504 score: 0.9535 time: 0.18s
Epoch 209/1000, LR 0.000258
Train loss: 0.5552;  Loss pred: 0.5552; Loss self: 0.0000; time: 0.28s
Val loss: 0.2358 score: 0.9070 time: 0.18s
Test loss: 0.1501 score: 0.9612 time: 0.18s
Epoch 210/1000, LR 0.000258
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.28s
Val loss: 0.2356 score: 0.8992 time: 0.19s
Test loss: 0.1499 score: 0.9612 time: 0.18s
Epoch 211/1000, LR 0.000257
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.27s
Val loss: 0.2355 score: 0.8992 time: 0.17s
Test loss: 0.1497 score: 0.9612 time: 0.21s
Epoch 212/1000, LR 0.000257
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.28s
Val loss: 0.2351 score: 0.8992 time: 0.23s
Test loss: 0.1492 score: 0.9612 time: 0.27s
Epoch 213/1000, LR 0.000257
Train loss: 0.5567;  Loss pred: 0.5567; Loss self: 0.0000; time: 0.27s
Val loss: 0.2347 score: 0.8992 time: 0.17s
Test loss: 0.1486 score: 0.9612 time: 0.17s
Epoch 214/1000, LR 0.000256
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.28s
Val loss: 0.2344 score: 0.8992 time: 0.17s
Test loss: 0.1481 score: 0.9612 time: 0.18s
Epoch 215/1000, LR 0.000256
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.28s
Val loss: 0.2339 score: 0.9070 time: 0.18s
Test loss: 0.1475 score: 0.9612 time: 0.26s
Epoch 216/1000, LR 0.000256
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.28s
Val loss: 0.2335 score: 0.9070 time: 0.17s
Test loss: 0.1471 score: 0.9612 time: 0.18s
Epoch 217/1000, LR 0.000256
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.29s
Val loss: 0.2332 score: 0.9070 time: 0.19s
Test loss: 0.1467 score: 0.9612 time: 0.18s
Epoch 218/1000, LR 0.000255
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.34s
Val loss: 0.2328 score: 0.9070 time: 0.18s
Test loss: 0.1461 score: 0.9535 time: 0.18s
Epoch 219/1000, LR 0.000255
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.28s
Val loss: 0.2326 score: 0.9070 time: 0.18s
Test loss: 0.1459 score: 0.9535 time: 0.18s
Epoch 220/1000, LR 0.000255
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.29s
Val loss: 0.2324 score: 0.9070 time: 0.18s
Test loss: 0.1457 score: 0.9612 time: 0.19s
Epoch 221/1000, LR 0.000255
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.28s
Val loss: 0.2322 score: 0.9070 time: 0.18s
Test loss: 0.1453 score: 0.9535 time: 0.19s
Epoch 222/1000, LR 0.000254
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 0.28s
Val loss: 0.2323 score: 0.8992 time: 0.24s
Test loss: 0.1452 score: 0.9612 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5460;  Loss pred: 0.5460; Loss self: 0.0000; time: 0.38s
Val loss: 0.2321 score: 0.8992 time: 0.25s
Test loss: 0.1449 score: 0.9612 time: 0.25s
Epoch 224/1000, LR 0.000254
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.34s
Val loss: 0.2321 score: 0.9070 time: 0.17s
Test loss: 0.1446 score: 0.9612 time: 0.17s
Epoch 225/1000, LR 0.000253
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.30s
Val loss: 0.2318 score: 0.9070 time: 0.19s
Test loss: 0.1441 score: 0.9612 time: 0.19s
Epoch 226/1000, LR 0.000253
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.37s
Val loss: 0.2314 score: 0.9070 time: 0.26s
Test loss: 0.1436 score: 0.9612 time: 0.27s
Epoch 227/1000, LR 0.000253
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.40s
Val loss: 0.2312 score: 0.9070 time: 0.19s
Test loss: 0.1432 score: 0.9535 time: 0.19s
Epoch 228/1000, LR 0.000253
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.29s
Val loss: 0.2311 score: 0.9070 time: 0.17s
Test loss: 0.1428 score: 0.9535 time: 0.18s
Epoch 229/1000, LR 0.000252
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.29s
Val loss: 0.2309 score: 0.9070 time: 0.18s
Test loss: 0.1425 score: 0.9535 time: 0.18s
Epoch 230/1000, LR 0.000252
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.29s
Val loss: 0.2310 score: 0.9147 time: 0.18s
Test loss: 0.1424 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.29s
Val loss: 0.2310 score: 0.9147 time: 0.18s
Test loss: 0.1422 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.30s
Val loss: 0.2311 score: 0.9070 time: 0.19s
Test loss: 0.1420 score: 0.9612 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.32s
Val loss: 0.2311 score: 0.9147 time: 0.18s
Test loss: 0.1419 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.29s
Val loss: 0.2313 score: 0.9147 time: 0.18s
Test loss: 0.1419 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.38s
Val loss: 0.2310 score: 0.9147 time: 0.25s
Test loss: 0.1416 score: 0.9612 time: 0.26s
     INFO: Early stopping counter 6 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.39s
Val loss: 0.2309 score: 0.9147 time: 0.26s
Test loss: 0.1414 score: 0.9612 time: 0.26s
     INFO: Early stopping counter 7 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.40s
Val loss: 0.2307 score: 0.9147 time: 0.18s
Test loss: 0.1411 score: 0.9612 time: 0.18s
Epoch 238/1000, LR 0.000250
Train loss: 0.5331;  Loss pred: 0.5331; Loss self: 0.0000; time: 0.29s
Val loss: 0.2304 score: 0.9225 time: 0.18s
Test loss: 0.1408 score: 0.9535 time: 0.18s
Epoch 239/1000, LR 0.000249
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.29s
Val loss: 0.2305 score: 0.9147 time: 0.17s
Test loss: 0.1407 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.28s
Val loss: 0.2304 score: 0.9147 time: 0.18s
Test loss: 0.1406 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.29s
Val loss: 0.2305 score: 0.9147 time: 0.18s
Test loss: 0.1405 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.31s
Val loss: 0.2303 score: 0.9225 time: 0.25s
Test loss: 0.1403 score: 0.9535 time: 0.26s
Epoch 243/1000, LR 0.000248
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.41s
Val loss: 0.2302 score: 0.9302 time: 0.24s
Test loss: 0.1400 score: 0.9535 time: 0.20s
Epoch 244/1000, LR 0.000248
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.31s
Val loss: 0.2303 score: 0.9302 time: 0.20s
Test loss: 0.1400 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.34s
Val loss: 0.2303 score: 0.9302 time: 0.19s
Test loss: 0.1398 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.31s
Val loss: 0.2302 score: 0.9302 time: 0.19s
Test loss: 0.1398 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.31s
Val loss: 0.2304 score: 0.9302 time: 0.18s
Test loss: 0.1397 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.35s
Val loss: 0.2305 score: 0.9302 time: 0.21s
Test loss: 0.1396 score: 0.9535 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.38s
Val loss: 0.2307 score: 0.9302 time: 0.20s
Test loss: 0.1395 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.32s
Val loss: 0.2309 score: 0.9225 time: 0.18s
Test loss: 0.1395 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.32s
Val loss: 0.2310 score: 0.9225 time: 0.18s
Test loss: 0.1394 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.32s
Val loss: 0.2311 score: 0.9225 time: 0.18s
Test loss: 0.1393 score: 0.9535 time: 0.26s
     INFO: Early stopping counter 9 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.41s
Val loss: 0.2311 score: 0.9225 time: 0.26s
Test loss: 0.1393 score: 0.9535 time: 0.26s
     INFO: Early stopping counter 10 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.41s
Val loss: 0.2312 score: 0.9225 time: 0.23s
Test loss: 0.1392 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.30s
Val loss: 0.2312 score: 0.9302 time: 0.18s
Test loss: 0.1392 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.30s
Val loss: 0.2313 score: 0.9302 time: 0.17s
Test loss: 0.1391 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.29s
Val loss: 0.2313 score: 0.9302 time: 0.18s
Test loss: 0.1391 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.31s
Val loss: 0.2313 score: 0.9302 time: 0.18s
Test loss: 0.1392 score: 0.9535 time: 0.26s
     INFO: Early stopping counter 15 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.38s
Val loss: 0.2314 score: 0.9302 time: 0.18s
Test loss: 0.1392 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.30s
Val loss: 0.2314 score: 0.9302 time: 0.17s
Test loss: 0.1392 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.29s
Val loss: 0.2316 score: 0.9302 time: 0.18s
Test loss: 0.1392 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.30s
Val loss: 0.2316 score: 0.9302 time: 0.18s
Test loss: 0.1393 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.29s
Val loss: 0.2318 score: 0.9302 time: 0.17s
Test loss: 0.1393 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 242,   Train_Loss: 0.5306,   Val_Loss: 0.2302,   Val_Precision: 0.9508,   Val_Recall: 0.9062,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2302,   Test_Precision: 0.9836,   Test_Recall: 0.9231,   Test_accuracy: 0.9524,   Test_Score: 0.9535,   Test_loss: 0.1400


[0.19555586413480341, 0.1909084590151906, 0.24847080814652145, 0.19288980099372566, 0.19383490993641317, 0.1862070350907743, 0.2076902671251446, 0.21620360994711518, 0.19409007905051112, 0.18385196197777987, 0.18302034912630916, 0.19677526992745697, 0.18619965598918498, 0.18455329700373113, 0.18718935106880963, 0.18790399306453764, 0.2625683229416609, 0.18833425594493747, 0.17697427910752594, 0.17435620818287134, 0.1696395999751985, 0.23384293587878346, 0.17312781908549368, 0.17624261300079525, 0.18764558108523488, 0.17418877803720534, 0.17387836519628763, 0.17295143590308726, 0.25845810817554593, 0.2712989260908216, 0.17145546386018395, 0.17271277401596308, 0.1809214809909463, 0.1887400469277054, 0.34398250887170434, 0.17259777197614312, 0.17296866397373378, 0.17910086107440293, 0.22473544999957085, 0.24988236906938255, 0.25183648709207773, 0.18846895406022668, 0.18484524707309902, 0.17317458800971508, 0.18546287692151964, 0.17097281105816364, 0.17147964495234191, 0.2653619688935578, 0.19469138490967453, 0.1879120608791709, 0.18262865906581283, 0.1834622509777546, 0.1796323829330504, 0.2358199800364673, 0.17932576406747103, 0.1771650610025972, 0.1704649010207504, 0.2297176499851048, 0.25605336483567953, 0.18071211082860827, 0.17375806183554232, 0.17315553897060454, 0.17537340498529375, 0.1853686268441379, 0.1726419001352042, 0.1711529151070863, 0.17228090204298496, 0.23269312200136483, 0.17818828020244837, 0.17891321913339198, 0.1764405129943043, 0.1735034789890051, 0.17418907186947763, 0.17243342706933618, 0.17726940009742975, 0.1764804709237069, 0.17298475396819413, 0.17232893803156912, 0.22620482300408185, 0.17139372415840626, 0.17405310715548694, 0.20454979198984802, 0.25184304895810783, 0.21801338414661586, 0.18168765702284873, 0.1784256030805409, 0.23621190804988146, 0.24977327208034694, 0.25211025099270046, 0.24976587295532227, 0.17187732993625104, 0.17342588189058006, 0.1755876347888261, 0.17322244797833264, 0.17410647892393172, 0.17895297589711845, 0.17665552720427513, 0.17939129704609513, 0.17906447779387236, 0.17742380104027689, 0.2564876559190452, 0.17449749214574695, 0.20589374983683228, 0.17968621896579862, 0.18401836999692023, 0.18974814098328352, 0.1817194470204413, 0.17464973195455968, 0.17724909400567412, 0.18932599807158113, 0.1707296841777861, 0.253844051156193, 0.22669502603821456, 0.17422130005434155, 0.17190482490696013, 0.17820006608963013, 0.17567916098050773, 0.25804635998792946, 0.278405831893906, 0.19036092795431614, 0.19149508699774742, 0.20319574885070324, 0.23995877616107464, 0.17401337204501033, 0.17154936911538243, 0.1756247030571103, 0.18875375692732632, 0.186109553091228, 0.18294598790816963, 0.18837940110825002, 0.26424073288217187, 0.20493202400393784, 0.1738430808763951, 0.17454204219393432, 0.1743202048819512, 0.1907002991065383, 0.1804999599698931, 0.18132023210637271, 0.1809795149601996, 0.2621866229455918, 0.18945994600653648, 0.1813092811498791, 0.19337132899090648, 0.17371272598393261, 0.18385328794829547, 0.18678574287332594, 0.1886805489193648, 0.1770104030147195, 0.18075660895556211, 0.18257841607555747, 0.2508895299397409, 0.1836648068856448, 0.17678741086274385, 0.17725213314406574, 0.18034103303216398, 0.2560262840706855, 0.1899039449635893, 0.1839322498999536, 0.19472340098582208, 0.273139791097492, 0.2559899219777435, 0.1781013309955597, 0.18627611687406898, 0.1799804049078375, 0.1862126470077783, 0.19052876601926982, 0.17749475594609976, 0.17712987796403468, 0.19497802387923002, 0.17622835002839565, 0.19319331203587353, 0.20098470500670373, 0.18520730920135975, 0.18840247299522161, 0.19258209085091949, 0.19648555689491332, 0.2565794079564512, 0.17665872490033507, 0.18164591095410287, 0.18217366212047637, 0.19844938395544887, 0.19764221902005374, 0.21437810990028083, 0.18468522490002215, 0.1958201618399471, 0.17259489907883108, 0.19099656702019274, 0.239906009985134, 0.1978597640991211, 0.19524818495847285, 0.17264269897714257, 0.18827829393558204, 0.1805127840489149, 0.16895666299387813, 0.1722031719982624, 0.17321598599664867, 0.17023229389451444, 0.20042550703510642, 0.26235083397477865, 0.18480457714758813, 0.18427201686426997, 0.18673045886680484, 0.18457944807596505, 0.2157289618626237, 0.18597575393505394, 0.17128843907266855, 0.16931545012630522, 0.1850365619175136, 0.1876047309488058, 0.18159466097131371, 0.21657354710623622, 0.2710528769530356, 0.17199363792315125, 0.17998598120175302, 0.2659593350253999, 0.1818254510872066, 0.18663285998627543, 0.18303545005619526, 0.18370212893933058, 0.18958029709756374, 0.1972081868443638, 0.2602203330025077, 0.24967554188333452, 0.17483145697042346, 0.19133749487809837, 0.2750267190858722, 0.19501711311750114, 0.18155672191642225, 0.1835993528366089, 0.18357619806192815, 0.18515150994062424, 0.24038277007639408, 0.18165262788534164, 0.18674142402596772, 0.2604908391367644, 0.26028697611764073, 0.18762688501738012, 0.18348342808894813, 0.18016302306205034, 0.17901434982195497, 0.17949669994413853, 0.2621333079878241, 0.20083365100435913, 0.19529818813316524, 0.18586750701069832, 0.1855183730367571, 0.18650009599514306, 0.20328830601647496, 0.18371170992031693, 0.19206703500822186, 0.19181288802064955, 0.26692610303871334, 0.2602372639812529, 0.19038053113035858, 0.1949137169867754, 0.1826921950560063, 0.19367050589062274, 0.267529658973217, 0.1825879260431975, 0.18187707592733204, 0.18225802411325276, 0.18431074009276927, 0.18882760987617075]
[0.0015159369312775458, 0.0014799105350014774, 0.0019261302957094687, 0.00149526977514516, 0.0015025962010574664, 0.001443465388300576, 0.001610002070737555, 0.0016759969763342263, 0.0015045742562055126, 0.0014252090075796888, 0.0014187623963279779, 0.0015253896893601315, 0.0014434081859626743, 0.001430645713207218, 0.0014510802408434856, 0.0014566201012754857, 0.0020354133561369062, 0.0014599554724413757, 0.0013718936364924491, 0.0013515985130455144, 0.0013150356587224691, 0.001812735937044833, 0.0013420761169418115, 0.001366221806207715, 0.0014546169076374797, 0.0013503006049395763, 0.001347894303847191, 0.0013407088054502888, 0.0020035512261670227, 0.002103092450316447, 0.0013291121229471623, 0.0013388587133020395, 0.00140249210070501, 0.0014631011389744605, 0.00266653107652484, 0.0013379672246212645, 0.0013408423563855331, 0.0013883787680186274, 0.0017421352713145027, 0.0019370726284448259, 0.001952220830171145, 0.0014609996438777262, 0.0014329088920395272, 0.0013424386667419774, 0.0014376967203218577, 0.0013253706283578577, 0.0013292995732739684, 0.0020570695263066496, 0.0015092355419354614, 0.001456682642474193, 0.0014157260392698668, 0.001422187992075617, 0.0013924990925042666, 0.0018280618607478087, 0.001390122202073419, 0.0013733725659116063, 0.0013214333412461272, 0.0017807569766287194, 0.001984909804927748, 0.0014008690761907617, 0.0013469617196553668, 0.0013422909997721283, 0.0013594837595759205, 0.0014369660995669606, 0.001338309303373676, 0.0013267667837758628, 0.001335510868550271, 0.0018038226511733707, 0.0013813044976933983, 0.00138692417932862, 0.0013677559146845295, 0.0013449882092170936, 0.001350302882709129, 0.0013366932330956293, 0.0013741813961041067, 0.0013680656660752472, 0.0013409670850247607, 0.0013358832405547993, 0.0017535257597215647, 0.0013286335206078004, 0.0013492488926781932, 0.0015856573022468839, 0.001952271697349673, 0.0016900262336946967, 0.001408431449789525, 0.0013831442099266737, 0.001831100062402182, 0.0019362269153515267, 0.0019543430309511664, 0.001936169557793196, 0.0013323824026065972, 0.001344386681322326, 0.0013611444557273342, 0.0013428096742506407, 0.0013496626273173, 0.001387232371295492, 0.0013694226915060088, 0.0013906302096596522, 0.0013880967270842818, 0.0013753783026378052, 0.0019882764024732186, 0.0013526937375639297, 0.001596075580130483, 0.0013929164260914621, 0.0014264989922241878, 0.0014709158215758412, 0.00140867788387939, 0.0013538738911206176, 0.0013740239845401094, 0.0014676433959037298, 0.001323485923858807, 0.00196778334229607, 0.0017573257832419734, 0.001350552713599547, 0.0013325955419144196, 0.0013813958611599234, 0.0013618539610892072, 0.002000359379751391, 0.0021581847433636125, 0.0014756661081729934, 0.001484458038742228, 0.001575160843803901, 0.0018601455516362375, 0.0013489408685659716, 0.0013298400706618792, 0.001361431806644266, 0.0014632074180412893, 0.0014427097138854884, 0.0014181859527765087, 0.0014603054349476745, 0.002048377774280402, 0.0015886203411157973, 0.001347620781987559, 0.0013530390867746848, 0.0013513194176895443, 0.0014782968922987464, 0.0013992244958906442, 0.0014055831946230442, 0.0014029419764356559, 0.0020324544414386963, 0.0014686817519886548, 0.0014054983034874349, 0.0014990025503171045, 0.001346610278945214, 0.0014252192864208952, 0.0014479514951420615, 0.001462639914103603, 0.0013721736667807712, 0.0014012140229113343, 0.0014153365587252517, 0.001944880077052255, 0.0014237581929119753, 0.0013704450454476269, 0.0013740475437524476, 0.001397992504125302, 0.001984699876516942, 0.0014721236043689094, 0.0014258313945732838, 0.0015094837285722642, 0.002117362721685984, 0.001984417999827469, 0.001380630472833796, 0.0014440009060005348, 0.0013951969372700582, 0.001443508891533165, 0.0014769671784439521, 0.00137592834066744, 0.0013730998291785633, 0.0015114575494513955, 0.0013661112405301988, 0.0014976225739215003, 0.001558020969044215, 0.0014357155752043392, 0.0014604842867846637, 0.0014928844252009262, 0.0015231438518985528, 0.001988987658577141, 0.0013694474798475588, 0.0014081078368535106, 0.0014121989311664835, 0.0015383673174840997, 0.0015321102249616569, 0.0016618458131804716, 0.001431668410077691, 0.0015179857506972645, 0.0013379449540994656, 0.0014805935427921917, 0.0018597365115126667, 0.0015337966209234193, 0.0015135518213835104, 0.0013383154959468416, 0.0014595216584153646, 0.0013993239073559296, 0.0013097415735959545, 0.0013349083100640496, 0.0013427595813693695, 0.0013196301852287941, 0.0015536861010473366, 0.0020337273951533228, 0.0014325936212991328, 0.0014284652470098447, 0.0014475229369519756, 0.0014308484346974035, 0.0016723175338187882, 0.0014416725111244492, 0.0013278173571524693, 0.0013125228691961645, 0.0014343919528489427, 0.0014543002399132234, 0.0014077105501652226, 0.0016788647062498932, 0.002101185092659191, 0.0013332840149081493, 0.0013952401643546747, 0.002061700271514728, 0.001409499620831059, 0.0014467663564827554, 0.0014188794577999631, 0.0014240475111576014, 0.0014696147061826646, 0.0015287456344524326, 0.0020172118837403696, 0.0019354693169250738, 0.0013552826121738253, 0.0014832363944038633, 0.0021319900704331175, 0.0015117605668023345, 0.0014074164489645134, 0.0014232507971830147, 0.0014230713028056446, 0.0014352830227955368, 0.0018634323261735976, 0.0014081599060879197, 0.0014476079381857963, 0.0020193088305175536, 0.0020177284970359748, 0.0014544719768789156, 0.00142235215572828, 0.0013966125818763591, 0.0013877081381546896, 0.0013914472863886707, 0.002032041147192435, 0.0015568500077857296, 0.0015139394428927539, 0.001440833387679832, 0.001438126922765559, 0.001445737178256923, 0.0015758783412129843, 0.0014241217823280383, 0.0014888917442497818, 0.0014869216125631749, 0.002069194597199328, 0.0020173431316376196, 0.0014758180707779734, 0.0015109590464091117, 0.0014162185663256304, 0.0015013217510900987, 0.002073873325373775, 0.0014154102794046318, 0.0014098998133901708, 0.0014128529001027346, 0.0014287654270757307, 0.0014637799215207036]
[659.6580499937136, 675.7165222822078, 519.1756768623284, 668.7756394346435, 665.5147931934345, 692.777262347331, 621.1172135585465, 596.6597876490325, 664.6398447106011, 701.651473350014, 704.8396564415484, 655.5701844421661, 692.804717144551, 698.9850741999586, 689.1417661498298, 686.5208019059689, 491.3006967282266, 684.9523967520556, 728.9194828228244, 739.8646790064392, 760.435653107293, 551.6523281544386, 745.1142207035915, 731.9455709580176, 687.4662289084437, 740.5758364780918, 741.8979345381733, 745.8741196707075, 499.1137670650387, 475.490271409387, 752.3819719457589, 746.9048003830748, 713.0164936382289, 683.4797495277295, 375.01906833324864, 747.4024636762442, 745.7998289192391, 720.2645438226576, 574.0082394666545, 516.2429045331396, 512.2371324725252, 684.4628636224992, 697.8810764281409, 744.9129891549857, 695.5569876907904, 754.5059310986891, 752.2758752845099, 486.12844009966096, 662.5870993719034, 686.4913268283941, 706.3513506580204, 703.1419232703172, 718.1333225873797, 547.0274400839631, 719.3612176745778, 728.1345388869247, 756.754025183736, 561.5589398914908, 503.80122941475446, 713.8425831478817, 742.4115959701213, 744.9949378858707, 735.5733328597772, 695.9106413862908, 747.2114237561906, 753.7119652288001, 748.7771335665159, 554.3782252371145, 723.9533366248152, 721.0199482455331, 731.1246029088811, 743.5009416046046, 740.574587231635, 748.1148069285231, 727.7059657735615, 730.9590649028081, 745.7304591346739, 748.5684149946328, 570.2796177677972, 752.6529960967243, 741.153100385392, 630.653293484661, 512.2237859400208, 591.7067913282168, 710.0097062937917, 722.990410416434, 546.1198000769663, 516.4683912156275, 511.6808994955744, 516.4836912009805, 750.5352802946487, 743.8336111872288, 734.6758793985941, 744.7071756896995, 740.9259023402625, 720.85976415482, 730.2347231447291, 719.0984296571147, 720.41089103388, 727.0726883520874, 502.94818102558537, 739.2656387992917, 626.5367457838355, 717.9181616846965, 701.0169691328043, 679.8485578383858, 709.8854972054203, 738.6212309421877, 727.7893335571601, 681.364425984577, 755.5803820597963, 508.1860276513823, 569.0464508835502, 740.4375926466137, 750.4152374421052, 723.905455428486, 734.2931243524839, 499.9101712034775, 463.35236270897843, 677.6600712461232, 673.6465254668253, 634.8558015098137, 537.5923400834796, 741.3223391052547, 751.9701218675765, 734.5208148653853, 683.4301054450925, 693.1401309462401, 705.126149389797, 684.7882477653258, 488.19119820380854, 629.4770211097959, 742.0485149576981, 739.0769489030478, 740.017487286446, 676.4541041853937, 714.6815989406139, 711.4484605574589, 712.7878535223682, 492.01594860455447, 680.8827022232415, 711.4914315575621, 667.1102726199207, 742.6053518493039, 701.6464129609599, 690.6308694421341, 683.6952761629387, 728.7707264825157, 713.6668514937335, 706.5457285302289, 514.1705197143278, 702.3664586995116, 729.6899670087619, 727.7768549908074, 715.3114176571936, 503.85451817277004, 679.290785795595, 701.3451967785261, 662.4781579764651, 472.28563616333713, 503.9260881966112, 724.3067712010313, 692.5203411192525, 716.7446926572756, 692.7563840205308, 677.0631159546434, 726.7820354037599, 728.2791671441946, 661.6130240395861, 732.0048106857622, 667.7249778504048, 641.8398852574243, 696.5167873571855, 684.7043881598719, 669.8442177567837, 656.53680625998, 502.76832824360935, 730.2215051805534, 710.1728815277041, 708.1155338178843, 650.039810801126, 652.6945540259849, 601.7405417932132, 698.4857617594104, 658.7677121084073, 747.4149044293625, 675.4048096914837, 537.7105809395671, 651.9769220758564, 660.6975630909802, 747.2079663043223, 685.1559853422953, 714.6308261748592, 763.5093977008419, 749.1151208370404, 744.7349576758802, 757.7880615292402, 643.6306531453826, 491.7079852408685, 698.03465904948, 700.0520328326253, 690.8353397879022, 698.8860425398449, 597.9725618952696, 693.6388065137196, 753.1156258903859, 761.8914865936282, 697.1595162771462, 687.6159217711943, 710.373307838483, 595.6406113472455, 475.92189926230293, 750.0277426403335, 716.7224865996595, 485.0365563881415, 709.4716346290234, 691.196609265305, 704.7815052242319, 702.2237616124934, 680.4504580642824, 654.1310584727726, 495.7337442142035, 516.6705518167158, 737.8534860681459, 674.2013638371624, 469.04533649954953, 661.4804102975071, 710.5217512099817, 702.6168557075545, 702.7054779535348, 696.7266971863675, 536.6441195390317, 710.146621613557, 690.7947750364249, 495.218950606826, 495.60681799805633, 687.5347314328145, 703.060768722198, 716.0181806872258, 720.6126219953959, 718.67616530079, 492.1160190981604, 642.322635449176, 660.5284013799481, 694.0427731274995, 695.3489182143753, 691.6886520174204, 634.5667516632537, 702.1871390558197, 671.6405029862509, 672.5304088331778, 483.2798236345234, 495.701491886623, 677.5902936822374, 661.8313066635144, 706.1056984971559, 666.0797389193272, 482.18952805122245, 706.5089285776792, 709.2702548810562, 707.7877675215061, 699.9049536400881, 683.1628069888484]
Elapsed: 0.19643898506935673~0.02997609313330036
Time per graph: 0.0015227828299950131~0.0002323728149868245
Speed: 669.3053018643408~83.45851280503669
Total Time: 0.1898
best val loss: 0.23019906765846318 test_score: 0.9535

Testing...
Test loss: 0.1400 score: 0.9535 time: 0.19s
test Score 0.9535
Epoch Time List: [0.6992513318546116, 0.6556087271310389, 0.7580386449117213, 0.8286614753305912, 0.6462946541141719, 0.631576957879588, 0.6655413750559092, 0.6668955388013273, 0.7012166550848633, 0.6358412296976894, 0.6371782629285008, 0.6523519458714873, 0.7072837031446397, 0.6485546519979835, 0.6553817891981453, 0.6392604268621653, 0.7291037449613214, 0.6511636071372777, 0.6186616832856089, 0.6091485810466111, 0.6076428131200373, 0.7305049560964108, 0.6901833289302886, 0.6226829171646386, 0.6240186039358377, 0.6910081212408841, 0.6027533719316125, 0.6140857820864767, 0.7150368499569595, 0.7123725388664752, 0.5922684296965599, 0.6011161538772285, 0.7183375260792673, 0.6561041662935168, 0.8599477210082114, 0.5998430298641324, 0.6230881698429585, 0.6945768152363598, 0.6515220266301185, 0.8498574697878212, 0.8606860809959471, 0.6955731539055705, 0.6498421777505428, 0.6036149237770587, 0.6309135060291737, 0.5975567661225796, 0.6062078650575131, 0.7314100377261639, 0.6496395720168948, 0.6342612239532173, 0.6323260359931737, 0.6302479768637568, 0.6323235458694398, 0.684377009049058, 0.6267482549883425, 0.6059373538009822, 0.6019792389124632, 0.6870478929486126, 0.8753250890877098, 0.7310672528110445, 0.6059742111247033, 0.6221048012375832, 0.6785717131569982, 0.6252567849587649, 0.6107012848369777, 0.6162685810122639, 0.6177883967757225, 0.6922158882953227, 0.6286076207179576, 0.6183335559908301, 0.6048268128652126, 0.6039398580323905, 0.6200827478896827, 0.6734565428923815, 0.6078863823786378, 0.6148446060251445, 0.6019329952541739, 0.6051112967543304, 0.7077911507803947, 0.6095163659192622, 0.602489828132093, 0.6390453178901225, 0.8187367760110646, 0.843420404009521, 0.6832954417914152, 0.6085957821924239, 0.7099477211013436, 0.8880798008758575, 0.8801922623533756, 0.8818422961048782, 0.7081265621818602, 0.6084602910559624, 0.6203891031909734, 0.612826009048149, 0.6139529731590301, 0.6452527537476271, 0.6991021109279245, 0.6158617560286075, 0.6263722409494221, 0.6294698230922222, 0.729784871218726, 0.6127379250247031, 0.6475764743518084, 0.614023903850466, 0.6330125860404223, 0.6428066971711814, 0.638398117152974, 0.7044880860485137, 0.6110677500255406, 0.6332698110491037, 0.600988607853651, 0.8146764000412077, 0.8243281757459044, 0.6120568686164916, 0.6195135631132871, 0.6148133240640163, 0.6173145161010325, 0.7130357751157135, 0.7158320709131658, 0.637345609953627, 0.644713374087587, 0.7517394777387381, 0.7645990499295294, 0.7905039971228689, 0.604502891888842, 0.6155776497907937, 0.7070700731128454, 0.7172898689750582, 0.6357637322507799, 0.6400095496792346, 0.7258474959526211, 0.8379479080904275, 0.6103005507029593, 0.6073234782088548, 0.6085759059060365, 0.6204776139929891, 0.6834475921932608, 0.6281572347506881, 0.6330926001537591, 0.7382162590511143, 0.7722826190292835, 0.635851738974452, 0.6658529967535287, 0.6090274120215327, 0.6412365310825408, 0.7034799149259925, 0.6516829421743751, 0.627736551919952, 0.6239908100105822, 0.6197608020156622, 0.7310183169320226, 0.7513484128285199, 0.6284190369769931, 0.6238662041723728, 0.6368040291126817, 0.809371598996222, 0.7772148721851408, 0.6581145708914846, 0.6636810589116067, 0.8750810762867332, 0.8832525359466672, 0.7002648350317031, 0.6325781277846545, 0.6197130139917135, 0.6437282310798764, 0.6514649011660367, 0.6988972479011863, 0.6546521270647645, 0.6438497412018478, 0.6143954440485686, 0.6497618479188532, 0.6761770960874856, 0.6932455240748823, 0.6630704021081328, 0.6604978409595788, 0.669176768977195, 0.81479362398386, 0.625688573345542, 0.6229714618530124, 0.6393743169028312, 0.6523202441167086, 0.6479931240901351, 0.7164495489560068, 0.6230184282176197, 0.6468159209471196, 0.6007866368163377, 0.635342005873099, 0.7222232830245048, 0.6667053063865751, 0.6413330261129886, 0.5881421100348234, 0.6220906453672796, 0.6003622731659561, 0.683411325328052, 0.584728044224903, 0.5977546337526292, 0.5964157569687814, 0.6286438191309571, 0.8397109629586339, 0.7202279260382056, 0.6316011268645525, 0.6448572061490268, 0.6375760398805141, 0.6673816787078977, 0.719821535050869, 0.6105381930246949, 0.5950351380743086, 0.7367052384652197, 0.6383207109756768, 0.6535808849148452, 0.6548748470377177, 0.7795964051038027, 0.6018632999621332, 0.6205232390202582, 0.7173348891083151, 0.6280108781065792, 0.6539987057913095, 0.7032698369584978, 0.6388475941494107, 0.6548783387988806, 0.6499590361490846, 0.7780415117740631, 0.8797439138870686, 0.6835707121063024, 0.6738753831014037, 0.8954118289984763, 0.7747814301401377, 0.6404906890820712, 0.6505805980414152, 0.6539959162473679, 0.6558579187840223, 0.7196212569251657, 0.6779294719453901, 0.6531295729801059, 0.8847974510863423, 0.9085708491038531, 0.7650458451826125, 0.6492475320119411, 0.6363466752227396, 0.6381139049772173, 0.6446226120460778, 0.81516751809977, 0.8452537900302559, 0.6926459060050547, 0.7113753180019557, 0.6731200190261006, 0.6740522542968392, 0.7585477558895946, 0.7506352926138788, 0.6943104940000921, 0.6936483783647418, 0.7608585979323834, 0.9283731249161065, 0.8300203320104629, 0.6749288041610271, 0.6477659028023481, 0.6659326471854001, 0.7469927712809294, 0.7297601448372006, 0.648411616217345, 0.6467287389095873, 0.65651715407148, 0.6528507336042821]
Total Epoch List: [263]
Total Time List: [0.1897998140193522]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2b77a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2743;  Loss pred: 2.2743; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.25s
Epoch 2/1000, LR 0.000015
Train loss: 2.2807;  Loss pred: 2.2807; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.26s
Epoch 3/1000, LR 0.000045
Train loss: 2.2672;  Loss pred: 2.2672; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 2.2394;  Loss pred: 2.2394; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 2.2200;  Loss pred: 2.2200; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 2.1607;  Loss pred: 2.1607; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 0.24s
Epoch 7/1000, LR 0.000165
Train loss: 2.1357;  Loss pred: 2.1357; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.25s
Epoch 8/1000, LR 0.000195
Train loss: 2.0717;  Loss pred: 2.0717; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.26s
Epoch 9/1000, LR 0.000225
Train loss: 2.0269;  Loss pred: 2.0269; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 1.9585;  Loss pred: 1.9585; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.8883;  Loss pred: 1.8883; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 1.8420;  Loss pred: 1.8420; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 1.7840;  Loss pred: 1.7840; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 1.7384;  Loss pred: 1.7384; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.23s
Epoch 15/1000, LR 0.000285
Train loss: 1.6806;  Loss pred: 1.6806; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 1.6244;  Loss pred: 1.6244; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.17s
Test loss: 0.6922 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 1.5943;  Loss pred: 1.5943; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.17s
Test loss: 0.6921 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 1.5433;  Loss pred: 1.5433; Loss self: 0.0000; time: 0.29s
Val loss: 0.6921 score: 0.5736 time: 0.19s
Test loss: 0.6920 score: 0.5736 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 1.4924;  Loss pred: 1.4924; Loss self: 0.0000; time: 0.40s
Val loss: 0.6920 score: 0.8140 time: 0.25s
Test loss: 0.6918 score: 0.8062 time: 0.24s
Epoch 20/1000, LR 0.000285
Train loss: 1.4626;  Loss pred: 1.4626; Loss self: 0.0000; time: 0.40s
Val loss: 0.6920 score: 0.8605 time: 0.19s
Test loss: 0.6917 score: 0.8837 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 1.4255;  Loss pred: 1.4255; Loss self: 0.0000; time: 0.40s
Val loss: 0.6919 score: 0.8062 time: 0.18s
Test loss: 0.6916 score: 0.8450 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 1.4033;  Loss pred: 1.4033; Loss self: 0.0000; time: 0.30s
Val loss: 0.6918 score: 0.8295 time: 0.18s
Test loss: 0.6914 score: 0.8682 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.3731;  Loss pred: 1.3731; Loss self: 0.0000; time: 0.38s
Val loss: 0.6917 score: 0.8527 time: 0.18s
Test loss: 0.6913 score: 0.9070 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.3415;  Loss pred: 1.3415; Loss self: 0.0000; time: 0.32s
Val loss: 0.6915 score: 0.8295 time: 0.26s
Test loss: 0.6912 score: 0.8837 time: 0.24s
Epoch 25/1000, LR 0.000285
Train loss: 1.3217;  Loss pred: 1.3217; Loss self: 0.0000; time: 0.38s
Val loss: 0.6914 score: 0.8372 time: 0.18s
Test loss: 0.6910 score: 0.8915 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.2998;  Loss pred: 1.2998; Loss self: 0.0000; time: 0.30s
Val loss: 0.6913 score: 0.8295 time: 0.18s
Test loss: 0.6908 score: 0.8915 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.2754;  Loss pred: 1.2754; Loss self: 0.0000; time: 0.30s
Val loss: 0.6911 score: 0.8217 time: 0.24s
Test loss: 0.6906 score: 0.8915 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.2555;  Loss pred: 1.2555; Loss self: 0.0000; time: 0.29s
Val loss: 0.6909 score: 0.8372 time: 0.17s
Test loss: 0.6904 score: 0.8915 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.2408;  Loss pred: 1.2408; Loss self: 0.0000; time: 0.31s
Val loss: 0.6907 score: 0.8140 time: 0.23s
Test loss: 0.6902 score: 0.8605 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 1.2218;  Loss pred: 1.2218; Loss self: 0.0000; time: 0.29s
Val loss: 0.6905 score: 0.7674 time: 0.20s
Test loss: 0.6900 score: 0.8062 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.2054;  Loss pred: 1.2054; Loss self: 0.0000; time: 0.38s
Val loss: 0.6903 score: 0.8062 time: 0.17s
Test loss: 0.6897 score: 0.8372 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.1897;  Loss pred: 1.1897; Loss self: 0.0000; time: 0.30s
Val loss: 0.6901 score: 0.8217 time: 0.17s
Test loss: 0.6895 score: 0.8527 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 1.1751;  Loss pred: 1.1751; Loss self: 0.0000; time: 0.30s
Val loss: 0.6899 score: 0.8217 time: 0.17s
Test loss: 0.6892 score: 0.8837 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 1.1619;  Loss pred: 1.1619; Loss self: 0.0000; time: 0.35s
Val loss: 0.6897 score: 0.8217 time: 0.25s
Test loss: 0.6889 score: 0.8605 time: 0.27s
Epoch 35/1000, LR 0.000285
Train loss: 1.1488;  Loss pred: 1.1488; Loss self: 0.0000; time: 0.31s
Val loss: 0.6894 score: 0.8372 time: 0.17s
Test loss: 0.6886 score: 0.8837 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 1.1398;  Loss pred: 1.1398; Loss self: 0.0000; time: 0.29s
Val loss: 0.6891 score: 0.8372 time: 0.17s
Test loss: 0.6883 score: 0.8837 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.1318;  Loss pred: 1.1318; Loss self: 0.0000; time: 0.29s
Val loss: 0.6888 score: 0.8450 time: 0.17s
Test loss: 0.6879 score: 0.8992 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.1230;  Loss pred: 1.1230; Loss self: 0.0000; time: 0.29s
Val loss: 0.6885 score: 0.8605 time: 0.17s
Test loss: 0.6875 score: 0.8915 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 1.1123;  Loss pred: 1.1123; Loss self: 0.0000; time: 0.36s
Val loss: 0.6881 score: 0.8837 time: 0.26s
Test loss: 0.6871 score: 0.8992 time: 0.25s
Epoch 40/1000, LR 0.000284
Train loss: 1.1031;  Loss pred: 1.1031; Loss self: 0.0000; time: 0.40s
Val loss: 0.6877 score: 0.8837 time: 0.22s
Test loss: 0.6866 score: 0.9147 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 1.0967;  Loss pred: 1.0967; Loss self: 0.0000; time: 0.31s
Val loss: 0.6874 score: 0.8837 time: 0.17s
Test loss: 0.6862 score: 0.9070 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 1.0879;  Loss pred: 1.0879; Loss self: 0.0000; time: 0.30s
Val loss: 0.6869 score: 0.8760 time: 0.17s
Test loss: 0.6857 score: 0.8915 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 1.0805;  Loss pred: 1.0805; Loss self: 0.0000; time: 0.31s
Val loss: 0.6865 score: 0.8760 time: 0.17s
Test loss: 0.6851 score: 0.9070 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 1.0756;  Loss pred: 1.0756; Loss self: 0.0000; time: 0.32s
Val loss: 0.6860 score: 0.8760 time: 0.23s
Test loss: 0.6846 score: 0.8992 time: 0.25s
Epoch 45/1000, LR 0.000284
Train loss: 1.0672;  Loss pred: 1.0672; Loss self: 0.0000; time: 0.45s
Val loss: 0.6855 score: 0.8760 time: 0.26s
Test loss: 0.6840 score: 0.8915 time: 0.21s
Epoch 46/1000, LR 0.000284
Train loss: 1.0623;  Loss pred: 1.0623; Loss self: 0.0000; time: 0.38s
Val loss: 0.6850 score: 0.8682 time: 0.18s
Test loss: 0.6833 score: 0.8837 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0596;  Loss pred: 1.0596; Loss self: 0.0000; time: 0.32s
Val loss: 0.6844 score: 0.8450 time: 0.18s
Test loss: 0.6826 score: 0.8760 time: 0.25s
Epoch 48/1000, LR 0.000284
Train loss: 1.0512;  Loss pred: 1.0512; Loss self: 0.0000; time: 0.43s
Val loss: 0.6837 score: 0.8605 time: 0.25s
Test loss: 0.6819 score: 0.8760 time: 0.23s
Epoch 49/1000, LR 0.000284
Train loss: 1.0473;  Loss pred: 1.0473; Loss self: 0.0000; time: 0.31s
Val loss: 0.6831 score: 0.8527 time: 0.19s
Test loss: 0.6810 score: 0.8760 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 1.0423;  Loss pred: 1.0423; Loss self: 0.0000; time: 0.31s
Val loss: 0.6823 score: 0.8527 time: 0.17s
Test loss: 0.6802 score: 0.8760 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 1.0387;  Loss pred: 1.0387; Loss self: 0.0000; time: 0.31s
Val loss: 0.6815 score: 0.8527 time: 0.18s
Test loss: 0.6793 score: 0.8760 time: 0.25s
Epoch 52/1000, LR 0.000284
Train loss: 1.0349;  Loss pred: 1.0349; Loss self: 0.0000; time: 0.41s
Val loss: 0.6807 score: 0.8527 time: 0.25s
Test loss: 0.6783 score: 0.8682 time: 0.25s
Epoch 53/1000, LR 0.000284
Train loss: 1.0306;  Loss pred: 1.0306; Loss self: 0.0000; time: 0.39s
Val loss: 0.6798 score: 0.8450 time: 0.17s
Test loss: 0.6773 score: 0.8760 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 1.0278;  Loss pred: 1.0278; Loss self: 0.0000; time: 0.31s
Val loss: 0.6789 score: 0.8372 time: 0.18s
Test loss: 0.6762 score: 0.8760 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 1.0227;  Loss pred: 1.0227; Loss self: 0.0000; time: 0.30s
Val loss: 0.6779 score: 0.8372 time: 0.17s
Test loss: 0.6750 score: 0.8837 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 1.0204;  Loss pred: 1.0204; Loss self: 0.0000; time: 0.31s
Val loss: 0.6768 score: 0.8372 time: 0.17s
Test loss: 0.6738 score: 0.8837 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 1.0158;  Loss pred: 1.0158; Loss self: 0.0000; time: 0.31s
Val loss: 0.6757 score: 0.8295 time: 0.17s
Test loss: 0.6725 score: 0.8837 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 1.0140;  Loss pred: 1.0140; Loss self: 0.0000; time: 0.32s
Val loss: 0.6745 score: 0.8295 time: 0.17s
Test loss: 0.6711 score: 0.8837 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 1.0106;  Loss pred: 1.0106; Loss self: 0.0000; time: 0.35s
Val loss: 0.6733 score: 0.8295 time: 0.17s
Test loss: 0.6696 score: 0.8837 time: 0.18s
Epoch 60/1000, LR 0.000283
Train loss: 1.0067;  Loss pred: 1.0067; Loss self: 0.0000; time: 0.39s
Val loss: 0.6719 score: 0.8295 time: 0.17s
Test loss: 0.6681 score: 0.8837 time: 0.17s
Epoch 61/1000, LR 0.000283
Train loss: 1.0024;  Loss pred: 1.0024; Loss self: 0.0000; time: 0.31s
Val loss: 0.6705 score: 0.8295 time: 0.17s
Test loss: 0.6664 score: 0.8837 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 0.9996;  Loss pred: 0.9996; Loss self: 0.0000; time: 0.31s
Val loss: 0.6690 score: 0.8295 time: 0.17s
Test loss: 0.6647 score: 0.8837 time: 0.17s
Epoch 63/1000, LR 0.000283
Train loss: 0.9965;  Loss pred: 0.9965; Loss self: 0.0000; time: 0.31s
Val loss: 0.6674 score: 0.8295 time: 0.17s
Test loss: 0.6628 score: 0.8837 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9931;  Loss pred: 0.9931; Loss self: 0.0000; time: 0.30s
Val loss: 0.6657 score: 0.8295 time: 0.17s
Test loss: 0.6609 score: 0.8837 time: 0.18s
Epoch 65/1000, LR 0.000283
Train loss: 0.9898;  Loss pred: 0.9898; Loss self: 0.0000; time: 0.36s
Val loss: 0.6639 score: 0.8295 time: 0.19s
Test loss: 0.6588 score: 0.8837 time: 0.17s
Epoch 66/1000, LR 0.000283
Train loss: 0.9852;  Loss pred: 0.9852; Loss self: 0.0000; time: 0.31s
Val loss: 0.6620 score: 0.8295 time: 0.18s
Test loss: 0.6566 score: 0.8837 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 0.9839;  Loss pred: 0.9839; Loss self: 0.0000; time: 0.30s
Val loss: 0.6600 score: 0.8372 time: 0.17s
Test loss: 0.6543 score: 0.8837 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 0.9799;  Loss pred: 0.9799; Loss self: 0.0000; time: 0.31s
Val loss: 0.6578 score: 0.8372 time: 0.17s
Test loss: 0.6518 score: 0.8837 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 0.9767;  Loss pred: 0.9767; Loss self: 0.0000; time: 0.30s
Val loss: 0.6555 score: 0.8372 time: 0.17s
Test loss: 0.6492 score: 0.8837 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.31s
Val loss: 0.6530 score: 0.8372 time: 0.19s
Test loss: 0.6463 score: 0.8837 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9684;  Loss pred: 0.9684; Loss self: 0.0000; time: 0.39s
Val loss: 0.6504 score: 0.8372 time: 0.25s
Test loss: 0.6434 score: 0.8837 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 0.31s
Val loss: 0.6477 score: 0.8372 time: 0.17s
Test loss: 0.6403 score: 0.8837 time: 0.17s
Epoch 73/1000, LR 0.000282
Train loss: 0.9618;  Loss pred: 0.9618; Loss self: 0.0000; time: 0.31s
Val loss: 0.6448 score: 0.8450 time: 0.17s
Test loss: 0.6371 score: 0.8837 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 0.9577;  Loss pred: 0.9577; Loss self: 0.0000; time: 0.31s
Val loss: 0.6419 score: 0.8450 time: 0.17s
Test loss: 0.6338 score: 0.8837 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9563;  Loss pred: 0.9563; Loss self: 0.0000; time: 0.31s
Val loss: 0.6388 score: 0.8450 time: 0.17s
Test loss: 0.6302 score: 0.8837 time: 0.17s
Epoch 76/1000, LR 0.000282
Train loss: 0.9518;  Loss pred: 0.9518; Loss self: 0.0000; time: 0.32s
Val loss: 0.6356 score: 0.8450 time: 0.23s
Test loss: 0.6266 score: 0.8837 time: 0.21s
Epoch 77/1000, LR 0.000282
Train loss: 0.9470;  Loss pred: 0.9470; Loss self: 0.0000; time: 0.30s
Val loss: 0.6322 score: 0.8450 time: 0.17s
Test loss: 0.6228 score: 0.8915 time: 0.18s
Epoch 78/1000, LR 0.000282
Train loss: 0.9431;  Loss pred: 0.9431; Loss self: 0.0000; time: 0.30s
Val loss: 0.6287 score: 0.8527 time: 0.17s
Test loss: 0.6189 score: 0.8915 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.9398;  Loss pred: 0.9398; Loss self: 0.0000; time: 0.30s
Val loss: 0.6250 score: 0.8527 time: 0.17s
Test loss: 0.6147 score: 0.8915 time: 0.17s
Epoch 80/1000, LR 0.000282
Train loss: 0.9357;  Loss pred: 0.9357; Loss self: 0.0000; time: 0.30s
Val loss: 0.6212 score: 0.8527 time: 0.16s
Test loss: 0.6105 score: 0.8915 time: 0.16s
Epoch 81/1000, LR 0.000281
Train loss: 0.9321;  Loss pred: 0.9321; Loss self: 0.0000; time: 0.30s
Val loss: 0.6172 score: 0.8527 time: 0.17s
Test loss: 0.6060 score: 0.8915 time: 0.17s
Epoch 82/1000, LR 0.000281
Train loss: 0.9278;  Loss pred: 0.9278; Loss self: 0.0000; time: 0.33s
Val loss: 0.6131 score: 0.8527 time: 0.24s
Test loss: 0.6014 score: 0.8915 time: 0.21s
Epoch 83/1000, LR 0.000281
Train loss: 0.9226;  Loss pred: 0.9226; Loss self: 0.0000; time: 0.31s
Val loss: 0.6088 score: 0.8527 time: 0.17s
Test loss: 0.5967 score: 0.8915 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9193;  Loss pred: 0.9193; Loss self: 0.0000; time: 0.31s
Val loss: 0.6044 score: 0.8527 time: 0.18s
Test loss: 0.5918 score: 0.8915 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.9153;  Loss pred: 0.9153; Loss self: 0.0000; time: 0.32s
Val loss: 0.5999 score: 0.8527 time: 0.18s
Test loss: 0.5867 score: 0.8915 time: 0.18s
Epoch 86/1000, LR 0.000281
Train loss: 0.9099;  Loss pred: 0.9099; Loss self: 0.0000; time: 0.30s
Val loss: 0.5953 score: 0.8450 time: 0.17s
Test loss: 0.5815 score: 0.8915 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.9047;  Loss pred: 0.9047; Loss self: 0.0000; time: 0.30s
Val loss: 0.5904 score: 0.8527 time: 0.17s
Test loss: 0.5761 score: 0.8915 time: 0.16s
Epoch 88/1000, LR 0.000281
Train loss: 0.9000;  Loss pred: 0.9000; Loss self: 0.0000; time: 0.30s
Val loss: 0.5854 score: 0.8527 time: 0.17s
Test loss: 0.5705 score: 0.8915 time: 0.19s
Epoch 89/1000, LR 0.000281
Train loss: 0.8959;  Loss pred: 0.8959; Loss self: 0.0000; time: 0.31s
Val loss: 0.5803 score: 0.8527 time: 0.23s
Test loss: 0.5649 score: 0.8915 time: 0.26s
Epoch 90/1000, LR 0.000281
Train loss: 0.8909;  Loss pred: 0.8909; Loss self: 0.0000; time: 0.32s
Val loss: 0.5749 score: 0.8527 time: 0.17s
Test loss: 0.5591 score: 0.8915 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8856;  Loss pred: 0.8856; Loss self: 0.0000; time: 0.30s
Val loss: 0.5697 score: 0.8527 time: 0.18s
Test loss: 0.5533 score: 0.8915 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.8797;  Loss pred: 0.8797; Loss self: 0.0000; time: 0.30s
Val loss: 0.5642 score: 0.8527 time: 0.18s
Test loss: 0.5473 score: 0.8915 time: 0.16s
Epoch 93/1000, LR 0.000280
Train loss: 0.8752;  Loss pred: 0.8752; Loss self: 0.0000; time: 0.29s
Val loss: 0.5585 score: 0.8527 time: 0.17s
Test loss: 0.5412 score: 0.8915 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8691;  Loss pred: 0.8691; Loss self: 0.0000; time: 0.31s
Val loss: 0.5528 score: 0.8527 time: 0.25s
Test loss: 0.5351 score: 0.8915 time: 0.25s
Epoch 95/1000, LR 0.000280
Train loss: 0.8651;  Loss pred: 0.8651; Loss self: 0.0000; time: 0.39s
Val loss: 0.5470 score: 0.8527 time: 0.18s
Test loss: 0.5288 score: 0.8837 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.8600;  Loss pred: 0.8600; Loss self: 0.0000; time: 0.30s
Val loss: 0.5412 score: 0.8527 time: 0.18s
Test loss: 0.5225 score: 0.8837 time: 0.18s
Epoch 97/1000, LR 0.000280
Train loss: 0.8533;  Loss pred: 0.8533; Loss self: 0.0000; time: 0.30s
Val loss: 0.5354 score: 0.8527 time: 0.17s
Test loss: 0.5162 score: 0.8837 time: 0.16s
Epoch 98/1000, LR 0.000280
Train loss: 0.8489;  Loss pred: 0.8489; Loss self: 0.0000; time: 0.30s
Val loss: 0.5296 score: 0.8527 time: 0.19s
Test loss: 0.5100 score: 0.8837 time: 0.22s
Epoch 99/1000, LR 0.000279
Train loss: 0.8425;  Loss pred: 0.8425; Loss self: 0.0000; time: 0.38s
Val loss: 0.5237 score: 0.8527 time: 0.17s
Test loss: 0.5036 score: 0.8837 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8374;  Loss pred: 0.8374; Loss self: 0.0000; time: 0.31s
Val loss: 0.5177 score: 0.8527 time: 0.18s
Test loss: 0.4972 score: 0.8837 time: 0.18s
Epoch 101/1000, LR 0.000279
Train loss: 0.8317;  Loss pred: 0.8317; Loss self: 0.0000; time: 0.31s
Val loss: 0.5117 score: 0.8527 time: 0.18s
Test loss: 0.4907 score: 0.8837 time: 0.18s
Epoch 102/1000, LR 0.000279
Train loss: 0.8265;  Loss pred: 0.8265; Loss self: 0.0000; time: 0.31s
Val loss: 0.5056 score: 0.8527 time: 0.33s
Test loss: 0.4843 score: 0.8837 time: 0.18s
Epoch 103/1000, LR 0.000279
Train loss: 0.8209;  Loss pred: 0.8209; Loss self: 0.0000; time: 0.31s
Val loss: 0.4996 score: 0.8527 time: 0.18s
Test loss: 0.4779 score: 0.8837 time: 0.18s
Epoch 104/1000, LR 0.000279
Train loss: 0.8149;  Loss pred: 0.8149; Loss self: 0.0000; time: 0.36s
Val loss: 0.4936 score: 0.8527 time: 0.17s
Test loss: 0.4716 score: 0.8837 time: 0.16s
Epoch 105/1000, LR 0.000279
Train loss: 0.8099;  Loss pred: 0.8099; Loss self: 0.0000; time: 0.30s
Val loss: 0.4874 score: 0.8527 time: 0.18s
Test loss: 0.4651 score: 0.8837 time: 0.18s
Epoch 106/1000, LR 0.000279
Train loss: 0.8032;  Loss pred: 0.8032; Loss self: 0.0000; time: 0.38s
Val loss: 0.4814 score: 0.8527 time: 0.18s
Test loss: 0.4588 score: 0.8837 time: 0.17s
Epoch 107/1000, LR 0.000278
Train loss: 0.7983;  Loss pred: 0.7983; Loss self: 0.0000; time: 0.30s
Val loss: 0.4753 score: 0.8527 time: 0.17s
Test loss: 0.4526 score: 0.8837 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.7929;  Loss pred: 0.7929; Loss self: 0.0000; time: 0.30s
Val loss: 0.4695 score: 0.8450 time: 0.22s
Test loss: 0.4465 score: 0.8837 time: 0.22s
Epoch 109/1000, LR 0.000278
Train loss: 0.7861;  Loss pred: 0.7861; Loss self: 0.0000; time: 0.34s
Val loss: 0.4635 score: 0.8682 time: 0.20s
Test loss: 0.4404 score: 0.8760 time: 0.18s
Epoch 110/1000, LR 0.000278
Train loss: 0.7825;  Loss pred: 0.7825; Loss self: 0.0000; time: 0.31s
Val loss: 0.4579 score: 0.8682 time: 0.17s
Test loss: 0.4347 score: 0.8760 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.7768;  Loss pred: 0.7768; Loss self: 0.0000; time: 0.32s
Val loss: 0.4527 score: 0.8605 time: 0.17s
Test loss: 0.4292 score: 0.8837 time: 0.16s
Epoch 112/1000, LR 0.000278
Train loss: 0.7704;  Loss pred: 0.7704; Loss self: 0.0000; time: 0.30s
Val loss: 0.4472 score: 0.8605 time: 0.17s
Test loss: 0.4236 score: 0.8837 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7663;  Loss pred: 0.7663; Loss self: 0.0000; time: 0.31s
Val loss: 0.4418 score: 0.8682 time: 0.17s
Test loss: 0.4181 score: 0.8760 time: 0.29s
Epoch 114/1000, LR 0.000277
Train loss: 0.7618;  Loss pred: 0.7618; Loss self: 0.0000; time: 0.31s
Val loss: 0.4364 score: 0.8682 time: 0.26s
Test loss: 0.4127 score: 0.8760 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.7555;  Loss pred: 0.7555; Loss self: 0.0000; time: 0.31s
Val loss: 0.4310 score: 0.8682 time: 0.17s
Test loss: 0.4074 score: 0.8760 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7523;  Loss pred: 0.7523; Loss self: 0.0000; time: 0.30s
Val loss: 0.4262 score: 0.8682 time: 0.17s
Test loss: 0.4025 score: 0.8760 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7469;  Loss pred: 0.7469; Loss self: 0.0000; time: 0.30s
Val loss: 0.4213 score: 0.8682 time: 0.17s
Test loss: 0.3976 score: 0.8760 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.7424;  Loss pred: 0.7424; Loss self: 0.0000; time: 0.30s
Val loss: 0.4166 score: 0.8682 time: 0.17s
Test loss: 0.3930 score: 0.8760 time: 0.17s
Epoch 119/1000, LR 0.000277
Train loss: 0.7379;  Loss pred: 0.7379; Loss self: 0.0000; time: 0.31s
Val loss: 0.4121 score: 0.8682 time: 0.19s
Test loss: 0.3885 score: 0.8760 time: 0.17s
Epoch 120/1000, LR 0.000277
Train loss: 0.7334;  Loss pred: 0.7334; Loss self: 0.0000; time: 0.37s
Val loss: 0.4074 score: 0.8682 time: 0.18s
Test loss: 0.3839 score: 0.8760 time: 0.17s
Epoch 121/1000, LR 0.000276
Train loss: 0.7290;  Loss pred: 0.7290; Loss self: 0.0000; time: 0.30s
Val loss: 0.4028 score: 0.8682 time: 0.17s
Test loss: 0.3796 score: 0.8760 time: 0.17s
Epoch 122/1000, LR 0.000276
Train loss: 0.7246;  Loss pred: 0.7246; Loss self: 0.0000; time: 0.30s
Val loss: 0.3986 score: 0.8682 time: 0.17s
Test loss: 0.3755 score: 0.8760 time: 0.16s
Epoch 123/1000, LR 0.000276
Train loss: 0.7208;  Loss pred: 0.7208; Loss self: 0.0000; time: 0.30s
Val loss: 0.3945 score: 0.8682 time: 0.17s
Test loss: 0.3716 score: 0.8760 time: 0.17s
Epoch 124/1000, LR 0.000276
Train loss: 0.7162;  Loss pred: 0.7162; Loss self: 0.0000; time: 0.40s
Val loss: 0.3904 score: 0.8682 time: 0.25s
Test loss: 0.3676 score: 0.8760 time: 0.26s
Epoch 125/1000, LR 0.000276
Train loss: 0.7139;  Loss pred: 0.7139; Loss self: 0.0000; time: 0.41s
Val loss: 0.3864 score: 0.8682 time: 0.25s
Test loss: 0.3639 score: 0.8760 time: 0.26s
Epoch 126/1000, LR 0.000276
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.38s
Val loss: 0.3825 score: 0.8682 time: 0.18s
Test loss: 0.3603 score: 0.8760 time: 0.17s
Epoch 127/1000, LR 0.000275
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.30s
Val loss: 0.3786 score: 0.8682 time: 0.17s
Test loss: 0.3567 score: 0.8760 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.31s
Val loss: 0.3746 score: 0.8682 time: 0.17s
Test loss: 0.3531 score: 0.8760 time: 0.17s
Epoch 129/1000, LR 0.000275
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.30s
Val loss: 0.3709 score: 0.8682 time: 0.17s
Test loss: 0.3498 score: 0.8760 time: 0.17s
Epoch 130/1000, LR 0.000275
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.30s
Val loss: 0.3676 score: 0.8682 time: 0.17s
Test loss: 0.3468 score: 0.8760 time: 0.18s
Epoch 131/1000, LR 0.000275
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.36s
Val loss: 0.3644 score: 0.8760 time: 0.17s
Test loss: 0.3439 score: 0.8760 time: 0.17s
Epoch 132/1000, LR 0.000275
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.30s
Val loss: 0.3614 score: 0.8760 time: 0.17s
Test loss: 0.3412 score: 0.8760 time: 0.16s
Epoch 133/1000, LR 0.000274
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.30s
Val loss: 0.3584 score: 0.8760 time: 0.16s
Test loss: 0.3387 score: 0.8760 time: 0.17s
Epoch 134/1000, LR 0.000274
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.30s
Val loss: 0.3551 score: 0.8760 time: 0.16s
Test loss: 0.3358 score: 0.8760 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.30s
Val loss: 0.3523 score: 0.8760 time: 0.17s
Test loss: 0.3334 score: 0.8760 time: 0.17s
Epoch 136/1000, LR 0.000274
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.30s
Val loss: 0.3495 score: 0.8682 time: 0.22s
Test loss: 0.3310 score: 0.8760 time: 0.17s
Epoch 137/1000, LR 0.000274
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.36s
Val loss: 0.3457 score: 0.8682 time: 0.17s
Test loss: 0.3279 score: 0.8760 time: 0.17s
Epoch 138/1000, LR 0.000274
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.30s
Val loss: 0.3429 score: 0.8682 time: 0.17s
Test loss: 0.3256 score: 0.8760 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.31s
Val loss: 0.3400 score: 0.8682 time: 0.17s
Test loss: 0.3233 score: 0.8760 time: 0.17s
Epoch 140/1000, LR 0.000273
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.30s
Val loss: 0.3373 score: 0.8682 time: 0.17s
Test loss: 0.3210 score: 0.8760 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.35s
Val loss: 0.3350 score: 0.8682 time: 0.25s
Test loss: 0.3192 score: 0.8760 time: 0.17s
Epoch 142/1000, LR 0.000273
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 0.30s
Val loss: 0.3327 score: 0.8760 time: 0.17s
Test loss: 0.3173 score: 0.8760 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.31s
Val loss: 0.3302 score: 0.8760 time: 0.18s
Test loss: 0.3154 score: 0.8760 time: 0.18s
Epoch 144/1000, LR 0.000272
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.30s
Val loss: 0.3274 score: 0.8760 time: 0.17s
Test loss: 0.3132 score: 0.8760 time: 0.20s
Epoch 145/1000, LR 0.000272
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.31s
Val loss: 0.3252 score: 0.8760 time: 0.17s
Test loss: 0.3115 score: 0.8760 time: 0.19s
Epoch 146/1000, LR 0.000272
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.35s
Val loss: 0.3234 score: 0.8760 time: 0.24s
Test loss: 0.3101 score: 0.8760 time: 0.22s
Epoch 147/1000, LR 0.000272
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.38s
Val loss: 0.3226 score: 0.8760 time: 0.18s
Test loss: 0.3093 score: 0.8760 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.29s
Val loss: 0.3205 score: 0.8760 time: 0.18s
Test loss: 0.3077 score: 0.8760 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 0.30s
Val loss: 0.3185 score: 0.8760 time: 0.17s
Test loss: 0.3062 score: 0.8760 time: 0.17s
Epoch 150/1000, LR 0.000271
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 0.30s
Val loss: 0.3166 score: 0.8760 time: 0.17s
Test loss: 0.3048 score: 0.8760 time: 0.17s
Epoch 151/1000, LR 0.000271
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.30s
Val loss: 0.3148 score: 0.8760 time: 0.17s
Test loss: 0.3034 score: 0.8760 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.30s
Val loss: 0.3125 score: 0.8760 time: 0.19s
Test loss: 0.3018 score: 0.8760 time: 0.17s
Epoch 153/1000, LR 0.000271
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.39s
Val loss: 0.3103 score: 0.8760 time: 0.25s
Test loss: 0.3002 score: 0.8760 time: 0.25s
Epoch 154/1000, LR 0.000271
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.33s
Val loss: 0.3093 score: 0.8760 time: 0.17s
Test loss: 0.2994 score: 0.8760 time: 0.17s
Epoch 155/1000, LR 0.000270
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.30s
Val loss: 0.3078 score: 0.8760 time: 0.18s
Test loss: 0.2983 score: 0.8760 time: 0.18s
Epoch 156/1000, LR 0.000270
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.30s
Val loss: 0.3061 score: 0.8760 time: 0.17s
Test loss: 0.2971 score: 0.8760 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.30s
Val loss: 0.3042 score: 0.8760 time: 0.18s
Test loss: 0.2957 score: 0.8760 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.31s
Val loss: 0.3017 score: 0.8760 time: 0.25s
Test loss: 0.2939 score: 0.8760 time: 0.24s
Epoch 159/1000, LR 0.000270
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.38s
Val loss: 0.3001 score: 0.8760 time: 0.18s
Test loss: 0.2928 score: 0.8760 time: 0.18s
Epoch 160/1000, LR 0.000269
Train loss: 0.6223;  Loss pred: 0.6223; Loss self: 0.0000; time: 0.29s
Val loss: 0.2987 score: 0.8760 time: 0.17s
Test loss: 0.2918 score: 0.8760 time: 0.17s
Epoch 161/1000, LR 0.000269
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 0.29s
Val loss: 0.2968 score: 0.8760 time: 0.18s
Test loss: 0.2904 score: 0.8760 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.31s
Val loss: 0.2956 score: 0.8837 time: 0.18s
Test loss: 0.2896 score: 0.8760 time: 0.18s
Epoch 163/1000, LR 0.000269
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.30s
Val loss: 0.2943 score: 0.8837 time: 0.19s
Test loss: 0.2886 score: 0.8760 time: 0.17s
Epoch 164/1000, LR 0.000269
Train loss: 0.6145;  Loss pred: 0.6145; Loss self: 0.0000; time: 0.37s
Val loss: 0.2931 score: 0.8837 time: 0.18s
Test loss: 0.2878 score: 0.8760 time: 0.17s
Epoch 165/1000, LR 0.000268
Train loss: 0.6132;  Loss pred: 0.6132; Loss self: 0.0000; time: 0.30s
Val loss: 0.2920 score: 0.8837 time: 0.17s
Test loss: 0.2870 score: 0.8837 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.30s
Val loss: 0.2910 score: 0.8837 time: 0.18s
Test loss: 0.2864 score: 0.8837 time: 0.17s
Epoch 167/1000, LR 0.000268
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 0.29s
Val loss: 0.2903 score: 0.8837 time: 0.17s
Test loss: 0.2859 score: 0.8837 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.31s
Val loss: 0.2879 score: 0.8837 time: 0.17s
Test loss: 0.2842 score: 0.8837 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.37s
Val loss: 0.2867 score: 0.8915 time: 0.17s
Test loss: 0.2834 score: 0.8837 time: 0.16s
Epoch 170/1000, LR 0.000267
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.29s
Val loss: 0.2849 score: 0.8915 time: 0.16s
Test loss: 0.2821 score: 0.8837 time: 0.16s
Epoch 171/1000, LR 0.000267
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 0.29s
Val loss: 0.2826 score: 0.8915 time: 0.17s
Test loss: 0.2806 score: 0.8837 time: 0.17s
Epoch 172/1000, LR 0.000267
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.30s
Val loss: 0.2817 score: 0.8915 time: 0.17s
Test loss: 0.2799 score: 0.8837 time: 0.17s
Epoch 173/1000, LR 0.000267
Train loss: 0.6035;  Loss pred: 0.6035; Loss self: 0.0000; time: 0.32s
Val loss: 0.2809 score: 0.8915 time: 0.25s
Test loss: 0.2794 score: 0.8837 time: 0.18s
Epoch 174/1000, LR 0.000266
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.30s
Val loss: 0.2787 score: 0.8915 time: 0.18s
Test loss: 0.2779 score: 0.8915 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.30s
Val loss: 0.2782 score: 0.8915 time: 0.17s
Test loss: 0.2775 score: 0.8837 time: 0.17s
Epoch 176/1000, LR 0.000266
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 0.29s
Val loss: 0.2768 score: 0.8915 time: 0.17s
Test loss: 0.2765 score: 0.8915 time: 0.17s
Epoch 177/1000, LR 0.000266
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 0.29s
Val loss: 0.2754 score: 0.8915 time: 0.18s
Test loss: 0.2755 score: 0.8915 time: 0.17s
Epoch 178/1000, LR 0.000265
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.39s
Val loss: 0.2754 score: 0.8915 time: 0.26s
Test loss: 0.2755 score: 0.8915 time: 0.24s
Epoch 179/1000, LR 0.000265
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.40s
Val loss: 0.2754 score: 0.8915 time: 0.24s
Test loss: 0.2756 score: 0.8837 time: 0.25s
     INFO: Early stopping counter 1 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.30s
Val loss: 0.2747 score: 0.8915 time: 0.17s
Test loss: 0.2750 score: 0.8837 time: 0.17s
Epoch 181/1000, LR 0.000265
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.29s
Val loss: 0.2729 score: 0.8915 time: 0.17s
Test loss: 0.2737 score: 0.8915 time: 0.17s
Epoch 182/1000, LR 0.000265
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.30s
Val loss: 0.2718 score: 0.8915 time: 0.24s
Test loss: 0.2730 score: 0.8915 time: 0.17s
Epoch 183/1000, LR 0.000264
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 0.29s
Val loss: 0.2708 score: 0.8915 time: 0.17s
Test loss: 0.2723 score: 0.8915 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.30s
Val loss: 0.2685 score: 0.8915 time: 0.17s
Test loss: 0.2706 score: 0.8915 time: 0.16s
Epoch 185/1000, LR 0.000264
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.30s
Val loss: 0.2671 score: 0.8915 time: 0.17s
Test loss: 0.2696 score: 0.8915 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.30s
Val loss: 0.2659 score: 0.8915 time: 0.19s
Test loss: 0.2687 score: 0.8915 time: 0.17s
Epoch 187/1000, LR 0.000263
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.36s
Val loss: 0.2652 score: 0.8915 time: 0.17s
Test loss: 0.2682 score: 0.8915 time: 0.17s
Epoch 188/1000, LR 0.000263
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.30s
Val loss: 0.2641 score: 0.8915 time: 0.17s
Test loss: 0.2674 score: 0.8915 time: 0.17s
Epoch 189/1000, LR 0.000263
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.30s
Val loss: 0.2638 score: 0.8915 time: 0.17s
Test loss: 0.2672 score: 0.8915 time: 0.17s
Epoch 190/1000, LR 0.000263
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.30s
Val loss: 0.2635 score: 0.8915 time: 0.18s
Test loss: 0.2670 score: 0.8915 time: 0.16s
Epoch 191/1000, LR 0.000262
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 0.30s
Val loss: 0.2624 score: 0.8915 time: 0.17s
Test loss: 0.2662 score: 0.8915 time: 0.16s
Epoch 192/1000, LR 0.000262
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.32s
Val loss: 0.2618 score: 0.8915 time: 0.24s
Test loss: 0.2658 score: 0.8915 time: 0.24s
Epoch 193/1000, LR 0.000262
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.40s
Val loss: 0.2597 score: 0.8915 time: 0.19s
Test loss: 0.2643 score: 0.8915 time: 0.17s
Epoch 194/1000, LR 0.000262
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.30s
Val loss: 0.2594 score: 0.8915 time: 0.17s
Test loss: 0.2641 score: 0.8915 time: 0.16s
Epoch 195/1000, LR 0.000261
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.31s
Val loss: 0.2593 score: 0.8915 time: 0.21s
Test loss: 0.2641 score: 0.8915 time: 0.19s
Epoch 196/1000, LR 0.000261
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.35s
Val loss: 0.2582 score: 0.8915 time: 0.26s
Test loss: 0.2633 score: 0.8915 time: 0.26s
Epoch 197/1000, LR 0.000261
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.42s
Val loss: 0.2561 score: 0.8915 time: 0.26s
Test loss: 0.2618 score: 0.8915 time: 0.26s
Epoch 198/1000, LR 0.000261
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 0.37s
Val loss: 0.2544 score: 0.8915 time: 0.18s
Test loss: 0.2606 score: 0.8915 time: 0.19s
Epoch 199/1000, LR 0.000260
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.31s
Val loss: 0.2534 score: 0.8915 time: 0.26s
Test loss: 0.2598 score: 0.8915 time: 0.18s
Epoch 200/1000, LR 0.000260
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.30s
Val loss: 0.2528 score: 0.8915 time: 0.18s
Test loss: 0.2594 score: 0.8915 time: 0.18s
Epoch 201/1000, LR 0.000260
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.31s
Val loss: 0.2517 score: 0.8915 time: 0.18s
Test loss: 0.2586 score: 0.8915 time: 0.18s
Epoch 202/1000, LR 0.000260
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.39s
Val loss: 0.2522 score: 0.8915 time: 0.19s
Test loss: 0.2590 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.31s
Val loss: 0.2524 score: 0.8915 time: 0.18s
Test loss: 0.2592 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.31s
Val loss: 0.2513 score: 0.8915 time: 0.18s
Test loss: 0.2585 score: 0.8915 time: 0.18s
Epoch 205/1000, LR 0.000259
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 0.31s
Val loss: 0.2498 score: 0.8992 time: 0.19s
Test loss: 0.2574 score: 0.8915 time: 0.19s
Epoch 206/1000, LR 0.000259
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.31s
Val loss: 0.2483 score: 0.8992 time: 0.19s
Test loss: 0.2562 score: 0.8915 time: 0.18s
Epoch 207/1000, LR 0.000258
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 0.30s
Val loss: 0.2469 score: 0.8992 time: 0.17s
Test loss: 0.2552 score: 0.8915 time: 0.17s
Epoch 208/1000, LR 0.000258
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.31s
Val loss: 0.2468 score: 0.8992 time: 0.17s
Test loss: 0.2551 score: 0.8915 time: 0.25s
Epoch 209/1000, LR 0.000258
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.32s
Val loss: 0.2460 score: 0.8992 time: 0.18s
Test loss: 0.2545 score: 0.8915 time: 0.16s
Epoch 210/1000, LR 0.000258
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.30s
Val loss: 0.2448 score: 0.8992 time: 0.17s
Test loss: 0.2536 score: 0.8915 time: 0.16s
Epoch 211/1000, LR 0.000257
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.30s
Val loss: 0.2443 score: 0.8992 time: 0.17s
Test loss: 0.2533 score: 0.8915 time: 0.16s
Epoch 212/1000, LR 0.000257
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.30s
Val loss: 0.2440 score: 0.8992 time: 0.18s
Test loss: 0.2530 score: 0.8915 time: 0.17s
Epoch 213/1000, LR 0.000257
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.33s
Val loss: 0.2439 score: 0.8992 time: 0.23s
Test loss: 0.2530 score: 0.8915 time: 0.25s
Epoch 214/1000, LR 0.000256
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.43s
Val loss: 0.2433 score: 0.8992 time: 0.17s
Test loss: 0.2525 score: 0.8915 time: 0.17s
Epoch 215/1000, LR 0.000256
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.30s
Val loss: 0.2415 score: 0.8992 time: 0.18s
Test loss: 0.2511 score: 0.8915 time: 0.19s
Epoch 216/1000, LR 0.000256
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.33s
Val loss: 0.2410 score: 0.8992 time: 0.18s
Test loss: 0.2507 score: 0.8915 time: 0.17s
Epoch 217/1000, LR 0.000256
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.29s
Val loss: 0.2394 score: 0.8992 time: 0.17s
Test loss: 0.2494 score: 0.8915 time: 0.17s
Epoch 218/1000, LR 0.000255
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.30s
Val loss: 0.2388 score: 0.8992 time: 0.17s
Test loss: 0.2489 score: 0.8915 time: 0.16s
Epoch 219/1000, LR 0.000255
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.34s
Val loss: 0.2379 score: 0.8992 time: 0.17s
Test loss: 0.2482 score: 0.8915 time: 0.24s
Epoch 220/1000, LR 0.000255
Train loss: 0.5569;  Loss pred: 0.5569; Loss self: 0.0000; time: 0.30s
Val loss: 0.2377 score: 0.8992 time: 0.17s
Test loss: 0.2480 score: 0.8915 time: 0.18s
Epoch 221/1000, LR 0.000255
Train loss: 0.5581;  Loss pred: 0.5581; Loss self: 0.0000; time: 0.30s
Val loss: 0.2369 score: 0.8992 time: 0.17s
Test loss: 0.2474 score: 0.8915 time: 0.17s
Epoch 222/1000, LR 0.000254
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.35s
Val loss: 0.2370 score: 0.8992 time: 0.25s
Test loss: 0.2475 score: 0.8915 time: 0.25s
     INFO: Early stopping counter 1 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.40s
Val loss: 0.2369 score: 0.8992 time: 0.24s
Test loss: 0.2475 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.30s
Val loss: 0.2362 score: 0.8992 time: 0.17s
Test loss: 0.2469 score: 0.8915 time: 0.17s
Epoch 225/1000, LR 0.000253
Train loss: 0.5548;  Loss pred: 0.5548; Loss self: 0.0000; time: 0.30s
Val loss: 0.2338 score: 0.8992 time: 0.17s
Test loss: 0.2449 score: 0.8992 time: 0.17s
Epoch 226/1000, LR 0.000253
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.30s
Val loss: 0.2334 score: 0.8992 time: 0.17s
Test loss: 0.2445 score: 0.8992 time: 0.17s
Epoch 227/1000, LR 0.000253
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.30s
Val loss: 0.2330 score: 0.8992 time: 0.17s
Test loss: 0.2442 score: 0.8992 time: 0.16s
Epoch 228/1000, LR 0.000253
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.30s
Val loss: 0.2330 score: 0.8992 time: 0.17s
Test loss: 0.2442 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.30s
Val loss: 0.2323 score: 0.8992 time: 0.23s
Test loss: 0.2436 score: 0.8992 time: 0.18s
Epoch 230/1000, LR 0.000252
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.30s
Val loss: 0.2321 score: 0.8992 time: 0.16s
Test loss: 0.2434 score: 0.8992 time: 0.17s
Epoch 231/1000, LR 0.000252
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.31s
Val loss: 0.2317 score: 0.8992 time: 0.17s
Test loss: 0.2431 score: 0.8992 time: 0.16s
Epoch 232/1000, LR 0.000251
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.30s
Val loss: 0.2320 score: 0.8915 time: 0.17s
Test loss: 0.2433 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.30s
Val loss: 0.2314 score: 0.8915 time: 0.18s
Test loss: 0.2428 score: 0.8992 time: 0.17s
Epoch 234/1000, LR 0.000251
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.30s
Val loss: 0.2311 score: 0.8915 time: 0.22s
Test loss: 0.2425 score: 0.8992 time: 0.18s
Epoch 235/1000, LR 0.000250
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.38s
Val loss: 0.2298 score: 0.8915 time: 0.18s
Test loss: 0.2413 score: 0.8992 time: 0.18s
Epoch 236/1000, LR 0.000250
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.31s
Val loss: 0.2298 score: 0.8915 time: 0.18s
Test loss: 0.2413 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.31s
Val loss: 0.2307 score: 0.8915 time: 0.19s
Test loss: 0.2422 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5441;  Loss pred: 0.5441; Loss self: 0.0000; time: 0.31s
Val loss: 0.2306 score: 0.8915 time: 0.18s
Test loss: 0.2420 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.30s
Val loss: 0.2296 score: 0.8915 time: 0.17s
Test loss: 0.2412 score: 0.8992 time: 0.17s
Epoch 240/1000, LR 0.000249
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.30s
Val loss: 0.2290 score: 0.8915 time: 0.21s
Test loss: 0.2407 score: 0.9070 time: 0.17s
Epoch 241/1000, LR 0.000249
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.38s
Val loss: 0.2283 score: 0.8915 time: 0.17s
Test loss: 0.2401 score: 0.9070 time: 0.16s
Epoch 242/1000, LR 0.000248
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.30s
Val loss: 0.2286 score: 0.8915 time: 0.17s
Test loss: 0.2404 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.30s
Val loss: 0.2286 score: 0.8915 time: 0.17s
Test loss: 0.2404 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5387;  Loss pred: 0.5387; Loss self: 0.0000; time: 0.31s
Val loss: 0.2293 score: 0.8915 time: 0.18s
Test loss: 0.2411 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.31s
Val loss: 0.2297 score: 0.8915 time: 0.18s
Test loss: 0.2414 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.34s
Val loss: 0.2275 score: 0.8915 time: 0.21s
Test loss: 0.2396 score: 0.9070 time: 0.18s
Epoch 247/1000, LR 0.000247
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.31s
Val loss: 0.2262 score: 0.8915 time: 0.18s
Test loss: 0.2385 score: 0.9070 time: 0.18s
Epoch 248/1000, LR 0.000247
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.29s
Val loss: 0.2264 score: 0.8915 time: 0.17s
Test loss: 0.2388 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.29s
Val loss: 0.2270 score: 0.8915 time: 0.17s
Test loss: 0.2393 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.30s
Val loss: 0.2263 score: 0.8915 time: 0.19s
Test loss: 0.2387 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.38s
Val loss: 0.2275 score: 0.8915 time: 0.18s
Test loss: 0.2398 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.30s
Val loss: 0.2280 score: 0.8915 time: 0.18s
Test loss: 0.2403 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.30s
Val loss: 0.2268 score: 0.8915 time: 0.18s
Test loss: 0.2392 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.30s
Val loss: 0.2277 score: 0.8915 time: 0.17s
Test loss: 0.2400 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.31s
Val loss: 0.2272 score: 0.8915 time: 0.18s
Test loss: 0.2396 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.32s
Val loss: 0.2274 score: 0.8915 time: 0.18s
Test loss: 0.2398 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.33s
Val loss: 0.2270 score: 0.8915 time: 0.22s
Test loss: 0.2395 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.30s
Val loss: 0.2280 score: 0.8915 time: 0.19s
Test loss: 0.2404 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.31s
Val loss: 0.2270 score: 0.8915 time: 0.19s
Test loss: 0.2396 score: 0.9070 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5296;  Loss pred: 0.5296; Loss self: 0.0000; time: 0.31s
Val loss: 0.2281 score: 0.8915 time: 0.18s
Test loss: 0.2405 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.30s
Val loss: 0.2281 score: 0.8915 time: 0.18s
Test loss: 0.2406 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.30s
Val loss: 0.2297 score: 0.8915 time: 0.19s
Test loss: 0.2419 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.41s
Val loss: 0.2290 score: 0.8915 time: 0.26s
Test loss: 0.2414 score: 0.9070 time: 0.26s
     INFO: Early stopping counter 16 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.43s
Val loss: 0.2287 score: 0.8992 time: 0.19s
Test loss: 0.2412 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.29s
Val loss: 0.2287 score: 0.8992 time: 0.17s
Test loss: 0.2412 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.30s
Val loss: 0.2282 score: 0.8992 time: 0.18s
Test loss: 0.2408 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.31s
Val loss: 0.2290 score: 0.8992 time: 0.19s
Test loss: 0.2415 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 246,   Train_Loss: 0.5357,   Val_Loss: 0.2262,   Val_Precision: 0.9474,   Val_Recall: 0.8308,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.2262,   Test_Precision: 0.9643,   Test_Recall: 0.8438,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.2385


[0.19555586413480341, 0.1909084590151906, 0.24847080814652145, 0.19288980099372566, 0.19383490993641317, 0.1862070350907743, 0.2076902671251446, 0.21620360994711518, 0.19409007905051112, 0.18385196197777987, 0.18302034912630916, 0.19677526992745697, 0.18619965598918498, 0.18455329700373113, 0.18718935106880963, 0.18790399306453764, 0.2625683229416609, 0.18833425594493747, 0.17697427910752594, 0.17435620818287134, 0.1696395999751985, 0.23384293587878346, 0.17312781908549368, 0.17624261300079525, 0.18764558108523488, 0.17418877803720534, 0.17387836519628763, 0.17295143590308726, 0.25845810817554593, 0.2712989260908216, 0.17145546386018395, 0.17271277401596308, 0.1809214809909463, 0.1887400469277054, 0.34398250887170434, 0.17259777197614312, 0.17296866397373378, 0.17910086107440293, 0.22473544999957085, 0.24988236906938255, 0.25183648709207773, 0.18846895406022668, 0.18484524707309902, 0.17317458800971508, 0.18546287692151964, 0.17097281105816364, 0.17147964495234191, 0.2653619688935578, 0.19469138490967453, 0.1879120608791709, 0.18262865906581283, 0.1834622509777546, 0.1796323829330504, 0.2358199800364673, 0.17932576406747103, 0.1771650610025972, 0.1704649010207504, 0.2297176499851048, 0.25605336483567953, 0.18071211082860827, 0.17375806183554232, 0.17315553897060454, 0.17537340498529375, 0.1853686268441379, 0.1726419001352042, 0.1711529151070863, 0.17228090204298496, 0.23269312200136483, 0.17818828020244837, 0.17891321913339198, 0.1764405129943043, 0.1735034789890051, 0.17418907186947763, 0.17243342706933618, 0.17726940009742975, 0.1764804709237069, 0.17298475396819413, 0.17232893803156912, 0.22620482300408185, 0.17139372415840626, 0.17405310715548694, 0.20454979198984802, 0.25184304895810783, 0.21801338414661586, 0.18168765702284873, 0.1784256030805409, 0.23621190804988146, 0.24977327208034694, 0.25211025099270046, 0.24976587295532227, 0.17187732993625104, 0.17342588189058006, 0.1755876347888261, 0.17322244797833264, 0.17410647892393172, 0.17895297589711845, 0.17665552720427513, 0.17939129704609513, 0.17906447779387236, 0.17742380104027689, 0.2564876559190452, 0.17449749214574695, 0.20589374983683228, 0.17968621896579862, 0.18401836999692023, 0.18974814098328352, 0.1817194470204413, 0.17464973195455968, 0.17724909400567412, 0.18932599807158113, 0.1707296841777861, 0.253844051156193, 0.22669502603821456, 0.17422130005434155, 0.17190482490696013, 0.17820006608963013, 0.17567916098050773, 0.25804635998792946, 0.278405831893906, 0.19036092795431614, 0.19149508699774742, 0.20319574885070324, 0.23995877616107464, 0.17401337204501033, 0.17154936911538243, 0.1756247030571103, 0.18875375692732632, 0.186109553091228, 0.18294598790816963, 0.18837940110825002, 0.26424073288217187, 0.20493202400393784, 0.1738430808763951, 0.17454204219393432, 0.1743202048819512, 0.1907002991065383, 0.1804999599698931, 0.18132023210637271, 0.1809795149601996, 0.2621866229455918, 0.18945994600653648, 0.1813092811498791, 0.19337132899090648, 0.17371272598393261, 0.18385328794829547, 0.18678574287332594, 0.1886805489193648, 0.1770104030147195, 0.18075660895556211, 0.18257841607555747, 0.2508895299397409, 0.1836648068856448, 0.17678741086274385, 0.17725213314406574, 0.18034103303216398, 0.2560262840706855, 0.1899039449635893, 0.1839322498999536, 0.19472340098582208, 0.273139791097492, 0.2559899219777435, 0.1781013309955597, 0.18627611687406898, 0.1799804049078375, 0.1862126470077783, 0.19052876601926982, 0.17749475594609976, 0.17712987796403468, 0.19497802387923002, 0.17622835002839565, 0.19319331203587353, 0.20098470500670373, 0.18520730920135975, 0.18840247299522161, 0.19258209085091949, 0.19648555689491332, 0.2565794079564512, 0.17665872490033507, 0.18164591095410287, 0.18217366212047637, 0.19844938395544887, 0.19764221902005374, 0.21437810990028083, 0.18468522490002215, 0.1958201618399471, 0.17259489907883108, 0.19099656702019274, 0.239906009985134, 0.1978597640991211, 0.19524818495847285, 0.17264269897714257, 0.18827829393558204, 0.1805127840489149, 0.16895666299387813, 0.1722031719982624, 0.17321598599664867, 0.17023229389451444, 0.20042550703510642, 0.26235083397477865, 0.18480457714758813, 0.18427201686426997, 0.18673045886680484, 0.18457944807596505, 0.2157289618626237, 0.18597575393505394, 0.17128843907266855, 0.16931545012630522, 0.1850365619175136, 0.1876047309488058, 0.18159466097131371, 0.21657354710623622, 0.2710528769530356, 0.17199363792315125, 0.17998598120175302, 0.2659593350253999, 0.1818254510872066, 0.18663285998627543, 0.18303545005619526, 0.18370212893933058, 0.18958029709756374, 0.1972081868443638, 0.2602203330025077, 0.24967554188333452, 0.17483145697042346, 0.19133749487809837, 0.2750267190858722, 0.19501711311750114, 0.18155672191642225, 0.1835993528366089, 0.18357619806192815, 0.18515150994062424, 0.24038277007639408, 0.18165262788534164, 0.18674142402596772, 0.2604908391367644, 0.26028697611764073, 0.18762688501738012, 0.18348342808894813, 0.18016302306205034, 0.17901434982195497, 0.17949669994413853, 0.2621333079878241, 0.20083365100435913, 0.19529818813316524, 0.18586750701069832, 0.1855183730367571, 0.18650009599514306, 0.20328830601647496, 0.18371170992031693, 0.19206703500822186, 0.19181288802064955, 0.26692610303871334, 0.2602372639812529, 0.19038053113035858, 0.1949137169867754, 0.1826921950560063, 0.19367050589062274, 0.267529658973217, 0.1825879260431975, 0.18187707592733204, 0.18225802411325276, 0.18431074009276927, 0.18882760987617075, 0.255717916181311, 0.26169913494959474, 0.18319300306029618, 0.1839629178866744, 0.1862410621251911, 0.2475755859632045, 0.24969659303314984, 0.26085420907475054, 0.18384515005163848, 0.187513334909454, 0.18795637786388397, 0.17387826996855438, 0.18367151892744005, 0.23427330888807774, 0.177532423986122, 0.1737637750338763, 0.17468983214348555, 0.176743492949754, 0.24750934494659305, 0.18413024302572012, 0.1810240182094276, 0.18184911413118243, 0.179198777070269, 0.24086894583888352, 0.1706286990083754, 0.1759111639112234, 0.17355251708067954, 0.17075701104477048, 0.18457963201217353, 0.17336174100637436, 0.1763790170662105, 0.18198595009744167, 0.17965830210596323, 0.2770573340822011, 0.18070919415913522, 0.17193050403147936, 0.1717212088406086, 0.19188775401562452, 0.2537652561441064, 0.18396667600609362, 0.17261794116348028, 0.1778134210035205, 0.17602461390197277, 0.2582412438932806, 0.2151003701146692, 0.17681256704963744, 0.25903963716700673, 0.233127657789737, 0.17945770593360066, 0.17025071009993553, 0.25049082189798355, 0.2517603449523449, 0.17283408297225833, 0.1731192150618881, 0.17293639411218464, 0.17674023192375898, 0.170663442928344, 0.17019757395610213, 0.18144707195460796, 0.17233932297676802, 0.17119883396662772, 0.17349056899547577, 0.17230190406553447, 0.1811121569480747, 0.1794330149423331, 0.17293489002622664, 0.1763404170051217, 0.17091099289245903, 0.1738349311053753, 0.17782090906985104, 0.17803430114872754, 0.17494606599211693, 0.17082742182537913, 0.17183471098542213, 0.16980582592077553, 0.21162347705103457, 0.18009215500205755, 0.17245603702031076, 0.1716078429017216, 0.16669521294534206, 0.17254407401196659, 0.21785010281018913, 0.17379093682393432, 0.17083906894549727, 0.18758026789873838, 0.17773015494458377, 0.16847046883776784, 0.1950112811755389, 0.2624988742172718, 0.1731771870981902, 0.17807900113984942, 0.16865618899464607, 0.1742205920163542, 0.2579721938818693, 0.1696036101784557, 0.18139837286435068, 0.16921271616593003, 0.2277770999353379, 0.17747960495762527, 0.1814232049509883, 0.18174015101976693, 0.18583002290688455, 0.18348900508135557, 0.1683588761370629, 0.1823289778549224, 0.178529858822003, 0.17496909410692751, 0.22392134997062385, 0.18231622502207756, 0.16933309589512646, 0.1688178579788655, 0.1754051682073623, 0.29056417802348733, 0.17902651499025524, 0.17589312908239663, 0.17652066983282566, 0.1749450119677931, 0.1723243691958487, 0.17681220499798656, 0.17048252979293466, 0.17372915102168918, 0.16892183991149068, 0.17593690613284707, 0.25983148301020265, 0.26652122591622174, 0.17564081703312695, 0.17611563089303672, 0.1760838849004358, 0.179349445970729, 0.18120863707736135, 0.1744786777999252, 0.16843671398237348, 0.17422605492174625, 0.17369201011024415, 0.17858880502171814, 0.17226895107887685, 0.16992806200869381, 0.17040388192981482, 0.1702922279946506, 0.1783898959401995, 0.17083196504972875, 0.1739247520454228, 0.18832896812818944, 0.21232229005545378, 0.19415981485508382, 0.22203593491576612, 0.17682105908170342, 0.17103215702809393, 0.17031127982772887, 0.17255298793315887, 0.17424389882944524, 0.17519814800471067, 0.2511157940607518, 0.17651173588819802, 0.18854690995067358, 0.18329713796265423, 0.1759414398111403, 0.24270398193039, 0.1864347408991307, 0.1752130889799446, 0.18422953807748854, 0.1808561149518937, 0.17863368499092758, 0.17094187601469457, 0.17235037498176098, 0.17753868107683957, 0.16857546288520098, 0.17283138702623546, 0.16928646294400096, 0.1673447450157255, 0.17721124598756433, 0.17386369011364877, 0.18724343902431428, 0.17772852908819914, 0.1752837668173015, 0.1749375849030912, 0.17599552194587886, 0.24752387683838606, 0.2574840681627393, 0.17657584697008133, 0.17100043804384768, 0.1712948251515627, 0.1709085840266198, 0.16737215500324965, 0.17100739805027843, 0.17683722195215523, 0.17199050402268767, 0.17192210699431598, 0.16986796213313937, 0.16938819200731814, 0.16801352612674236, 0.24261692678555846, 0.17756873299367726, 0.16815252299420536, 0.1976566449739039, 0.26431840495206416, 0.2625128929503262, 0.19222759711556137, 0.18548188684508204, 0.18197120097465813, 0.18920605606399477, 0.1847350860480219, 0.18384804809466004, 0.1845924430526793, 0.19338235701434314, 0.18691285396926105, 0.17094813496805727, 0.25390254403464496, 0.1672968950588256, 0.16847549891099334, 0.1694149449467659, 0.16977505898103118, 0.2598656159825623, 0.17160307709127665, 0.19352124189026654, 0.17041722894646227, 0.17337822308763862, 0.16922708111815155, 0.24868702609091997, 0.18132934998720884, 0.17315312009304762, 0.2537576830945909, 0.17342131212353706, 0.17203642497770488, 0.17138129705563188, 0.1707505809608847, 0.1682572381105274, 0.17553282785229385, 0.1800102370325476, 0.17287137196399271, 0.16849935404025018, 0.17157818004488945, 0.17781222495250404, 0.18380661914125085, 0.18159721698611975, 0.17829674994572997, 0.18202725402079523, 0.1788796738255769, 0.17569194897077978, 0.17648683907464147, 0.1673181070946157, 0.17171638016588986, 0.18421436799690127, 0.17957426700741053, 0.1830901107750833, 0.18613654794171453, 0.18657702091149986, 0.16856374498456717, 0.17092510894872248, 0.17732354393228889, 0.16954101994633675, 0.17260724399238825, 0.17040471197105944, 0.17631987389177084, 0.18799089291132987, 0.19812754588201642, 0.190679294988513, 0.19247345393523574, 0.2409065079409629, 0.19053888600319624, 0.18502894812263548, 0.1882072240114212, 0.26240520807914436, 0.1850721319206059, 0.16999676986597478, 0.18315886985510588, 0.18275967799127102]
[0.0015159369312775458, 0.0014799105350014774, 0.0019261302957094687, 0.00149526977514516, 0.0015025962010574664, 0.001443465388300576, 0.001610002070737555, 0.0016759969763342263, 0.0015045742562055126, 0.0014252090075796888, 0.0014187623963279779, 0.0015253896893601315, 0.0014434081859626743, 0.001430645713207218, 0.0014510802408434856, 0.0014566201012754857, 0.0020354133561369062, 0.0014599554724413757, 0.0013718936364924491, 0.0013515985130455144, 0.0013150356587224691, 0.001812735937044833, 0.0013420761169418115, 0.001366221806207715, 0.0014546169076374797, 0.0013503006049395763, 0.001347894303847191, 0.0013407088054502888, 0.0020035512261670227, 0.002103092450316447, 0.0013291121229471623, 0.0013388587133020395, 0.00140249210070501, 0.0014631011389744605, 0.00266653107652484, 0.0013379672246212645, 0.0013408423563855331, 0.0013883787680186274, 0.0017421352713145027, 0.0019370726284448259, 0.001952220830171145, 0.0014609996438777262, 0.0014329088920395272, 0.0013424386667419774, 0.0014376967203218577, 0.0013253706283578577, 0.0013292995732739684, 0.0020570695263066496, 0.0015092355419354614, 0.001456682642474193, 0.0014157260392698668, 0.001422187992075617, 0.0013924990925042666, 0.0018280618607478087, 0.001390122202073419, 0.0013733725659116063, 0.0013214333412461272, 0.0017807569766287194, 0.001984909804927748, 0.0014008690761907617, 0.0013469617196553668, 0.0013422909997721283, 0.0013594837595759205, 0.0014369660995669606, 0.001338309303373676, 0.0013267667837758628, 0.001335510868550271, 0.0018038226511733707, 0.0013813044976933983, 0.00138692417932862, 0.0013677559146845295, 0.0013449882092170936, 0.001350302882709129, 0.0013366932330956293, 0.0013741813961041067, 0.0013680656660752472, 0.0013409670850247607, 0.0013358832405547993, 0.0017535257597215647, 0.0013286335206078004, 0.0013492488926781932, 0.0015856573022468839, 0.001952271697349673, 0.0016900262336946967, 0.001408431449789525, 0.0013831442099266737, 0.001831100062402182, 0.0019362269153515267, 0.0019543430309511664, 0.001936169557793196, 0.0013323824026065972, 0.001344386681322326, 0.0013611444557273342, 0.0013428096742506407, 0.0013496626273173, 0.001387232371295492, 0.0013694226915060088, 0.0013906302096596522, 0.0013880967270842818, 0.0013753783026378052, 0.0019882764024732186, 0.0013526937375639297, 0.001596075580130483, 0.0013929164260914621, 0.0014264989922241878, 0.0014709158215758412, 0.00140867788387939, 0.0013538738911206176, 0.0013740239845401094, 0.0014676433959037298, 0.001323485923858807, 0.00196778334229607, 0.0017573257832419734, 0.001350552713599547, 0.0013325955419144196, 0.0013813958611599234, 0.0013618539610892072, 0.002000359379751391, 0.0021581847433636125, 0.0014756661081729934, 0.001484458038742228, 0.001575160843803901, 0.0018601455516362375, 0.0013489408685659716, 0.0013298400706618792, 0.001361431806644266, 0.0014632074180412893, 0.0014427097138854884, 0.0014181859527765087, 0.0014603054349476745, 0.002048377774280402, 0.0015886203411157973, 0.001347620781987559, 0.0013530390867746848, 0.0013513194176895443, 0.0014782968922987464, 0.0013992244958906442, 0.0014055831946230442, 0.0014029419764356559, 0.0020324544414386963, 0.0014686817519886548, 0.0014054983034874349, 0.0014990025503171045, 0.001346610278945214, 0.0014252192864208952, 0.0014479514951420615, 0.001462639914103603, 0.0013721736667807712, 0.0014012140229113343, 0.0014153365587252517, 0.001944880077052255, 0.0014237581929119753, 0.0013704450454476269, 0.0013740475437524476, 0.001397992504125302, 0.001984699876516942, 0.0014721236043689094, 0.0014258313945732838, 0.0015094837285722642, 0.002117362721685984, 0.001984417999827469, 0.001380630472833796, 0.0014440009060005348, 0.0013951969372700582, 0.001443508891533165, 0.0014769671784439521, 0.00137592834066744, 0.0013730998291785633, 0.0015114575494513955, 0.0013661112405301988, 0.0014976225739215003, 0.001558020969044215, 0.0014357155752043392, 0.0014604842867846637, 0.0014928844252009262, 0.0015231438518985528, 0.001988987658577141, 0.0013694474798475588, 0.0014081078368535106, 0.0014121989311664835, 0.0015383673174840997, 0.0015321102249616569, 0.0016618458131804716, 0.001431668410077691, 0.0015179857506972645, 0.0013379449540994656, 0.0014805935427921917, 0.0018597365115126667, 0.0015337966209234193, 0.0015135518213835104, 0.0013383154959468416, 0.0014595216584153646, 0.0013993239073559296, 0.0013097415735959545, 0.0013349083100640496, 0.0013427595813693695, 0.0013196301852287941, 0.0015536861010473366, 0.0020337273951533228, 0.0014325936212991328, 0.0014284652470098447, 0.0014475229369519756, 0.0014308484346974035, 0.0016723175338187882, 0.0014416725111244492, 0.0013278173571524693, 0.0013125228691961645, 0.0014343919528489427, 0.0014543002399132234, 0.0014077105501652226, 0.0016788647062498932, 0.002101185092659191, 0.0013332840149081493, 0.0013952401643546747, 0.002061700271514728, 0.001409499620831059, 0.0014467663564827554, 0.0014188794577999631, 0.0014240475111576014, 0.0014696147061826646, 0.0015287456344524326, 0.0020172118837403696, 0.0019354693169250738, 0.0013552826121738253, 0.0014832363944038633, 0.0021319900704331175, 0.0015117605668023345, 0.0014074164489645134, 0.0014232507971830147, 0.0014230713028056446, 0.0014352830227955368, 0.0018634323261735976, 0.0014081599060879197, 0.0014476079381857963, 0.0020193088305175536, 0.0020177284970359748, 0.0014544719768789156, 0.00142235215572828, 0.0013966125818763591, 0.0013877081381546896, 0.0013914472863886707, 0.002032041147192435, 0.0015568500077857296, 0.0015139394428927539, 0.001440833387679832, 0.001438126922765559, 0.001445737178256923, 0.0015758783412129843, 0.0014241217823280383, 0.0014888917442497818, 0.0014869216125631749, 0.002069194597199328, 0.0020173431316376196, 0.0014758180707779734, 0.0015109590464091117, 0.0014162185663256304, 0.0015013217510900987, 0.002073873325373775, 0.0014154102794046318, 0.0014098998133901708, 0.0014128529001027346, 0.0014287654270757307, 0.0014637799215207036, 0.0019823094277621007, 0.002028675464725541, 0.0014201007989170246, 0.0014260691309044527, 0.0014437291637611712, 0.0019191905888620504, 0.0019356325041329444, 0.0020221256517422522, 0.001425156201950686, 0.0014535917434841394, 0.0014570261849913486, 0.0013478935656477083, 0.0014238102242437212, 0.0018160721619230832, 0.00137622034097769, 0.001347006008014545, 0.0013541847452983376, 0.001370104596509721, 0.0019186770926092483, 0.0014273662250055823, 0.0014032869628637797, 0.0014096830552804839, 0.0013891378067462713, 0.0018672011305339807, 0.0013227030930881814, 0.0013636524334203364, 0.001345368349462632, 0.0013236977600369804, 0.0014308498605594847, 0.0013438894651656927, 0.0013672792020636474, 0.0014107437992049742, 0.0013927000163252963, 0.0021477312719550474, 0.0014008464663498854, 0.0013327946048951888, 0.0013311721615551055, 0.0014875019691133683, 0.0019671725282488866, 0.001426098263613129, 0.0013381235749106999, 0.0013783986124303914, 0.0013645318907129671, 0.0020018701076998497, 0.001667444729571079, 0.0013706400546483523, 0.0020080592028450133, 0.0018071911456568758, 0.0013911450072372144, 0.0013197729465111282, 0.001941789317038632, 0.0019516305810259294, 0.001339799092808204, 0.0013420094190844038, 0.001340592202420036, 0.0013700793172384416, 0.0013229724258011163, 0.0013193610384193964, 0.001406566449260527, 0.0013359637440059535, 0.0013271227439273466, 0.0013448881317478741, 0.0013356736749266238, 0.001403970208899804, 0.0013909536042041326, 0.0013405805428389661, 0.0013669799767838892, 0.0013248914177710002, 0.0013475576054680254, 0.0013784566594562096, 0.001380110861618043, 0.0013561710542024567, 0.0013242435800416986, 0.0013320520231428074, 0.0013163242319439964, 0.0016404920701630586, 0.0013960632170702135, 0.0013368685040334167, 0.0013302933558272992, 0.0012922109530646671, 0.0013375509613330744, 0.0016887604869006909, 0.0013472165645266225, 0.0013243338677945526, 0.001454110603866189, 0.0013777531391053006, 0.0013059726266493631, 0.001511715357949914, 0.0020348749939323395, 0.0013424588147146527, 0.0013804573731771273, 0.0013074123177879541, 0.0013505472249329784, 0.0019997844486966615, 0.0013147566680500444, 0.001406188936932951, 0.0013117264819064343, 0.0017657139529871156, 0.001375810891144382, 0.0014063814337285914, 0.0014088383799981932, 0.0014405428132316632, 0.0014223953882275626, 0.001305107566953976, 0.0014134029291079256, 0.0013839523939690155, 0.0013563495667203683, 0.001735824418376929, 0.0014133040699385857, 0.0013126596581017555, 0.0013086655657276395, 0.0013597299861035836, 0.0022524354885541656, 0.0013878024417849242, 0.0013635126285457104, 0.0013683772855257804, 0.0013561628834712644, 0.0013358478232236334, 0.0013706372480464075, 0.0013215699983948424, 0.001346737604819296, 0.001309471627220858, 0.0013638519855259464, 0.002014197542714749, 0.002066056014854432, 0.0013615567211870305, 0.0013652374487832305, 0.0013649913558173318, 0.0013903057827188293, 0.00140471811687877, 0.0013525478899219009, 0.0013057109611036705, 0.0013505895730367926, 0.0013464496907770866, 0.001384409341253629, 0.00133541822541765, 0.0013172717985170063, 0.0013209603250373242, 0.0013200947906562063, 0.0013828674103891433, 0.0013242787988351067, 0.0013482538918249828, 0.0014599144816138717, 0.0016459092252360758, 0.001505114843837859, 0.0017212087977966366, 0.0013707058843542902, 0.0013258306746363872, 0.0013202424792847199, 0.0013376200614973557, 0.0013507278979026764, 0.0013581251783310905, 0.0019466340624864481, 0.00136830803014107, 0.0014616039531059968, 0.0014209080462221258, 0.001363887130318917, 0.001881426216514651, 0.0014452305496056644, 0.0013582409998445317, 0.0014281359540890585, 0.0014019853872239822, 0.0013847572479916866, 0.0013251308218193377, 0.0013360494184632634, 0.001376268845556896, 0.001306786533993806, 0.0013397781940018253, 0.0013122981623565966, 0.0012972460853932209, 0.0013737305890508863, 0.001347780543516657, 0.0014514995273202658, 0.0013777405355674351, 0.0013587888900566008, 0.0013561053093262884, 0.0013643063716734796, 0.0019187897429332253, 0.0019960005283933277, 0.0013688050152719483, 0.0013255847910375788, 0.0013278668616400209, 0.0013248727443924014, 0.0012974585659166638, 0.0013256387445758017, 0.001370831177923684, 0.001333259721106106, 0.001332729511583845, 0.0013168059080088323, 0.0013130867597466523, 0.001302430435091026, 0.0018807513704306859, 0.001376501806152537, 0.0013035079301876385, 0.0015322220540612704, 0.0020489798833493345, 0.0020349836662815987, 0.0014901364117485378, 0.0014378440840704034, 0.0014106294649198305, 0.0014667136128991842, 0.001432054930604821, 0.0014251786674004654, 0.0014309491709510024, 0.001499088038870877, 0.001448936852474892, 0.0013251793408376534, 0.001968236775462364, 0.0012968751554947723, 0.0013060116194650647, 0.0013132941468741543, 0.0013160857285351255, 0.0020144621393997078, 0.001330256411560284, 0.0015001646658160197, 0.0013210637902826533, 0.0013440172332375086, 0.0013118378381252057, 0.0019278064038055812, 0.001405653875869836, 0.001342272248783315, 0.001967113822438689, 0.0013443512567716052, 0.0013336156975015882, 0.0013285371864777665, 0.0013236479144254627, 0.001304319675275406, 0.001360719595754216, 0.0013954281940507566, 0.0013400881547596334, 0.0013061965429476759, 0.0013300634112006933, 0.0013783893407170857, 0.0014248575127228749, 0.0014077303642334865, 0.0013821453484165113, 0.0014110639846573274, 0.0013866641381827666, 0.0013619530927967425, 0.001368115031586368, 0.0012970395898807417, 0.0013311347299681385, 0.0014280183565651262, 0.0013920485814527949, 0.0014193031843029713, 0.0014429189762923606, 0.0014463334954379833, 0.0013066956975547842, 0.0013250008445637401, 0.0013746011157541773, 0.0013142714724522228, 0.0013380406511037848, 0.001320966759465577, 0.0013668207278431847, 0.0014572937434986812, 0.0015358724486978018, 0.0014781340696783953, 0.0014920422785677189, 0.0018674923096198676, 0.0014770456279317539, 0.0014343329311832208, 0.001458970728770707, 0.0020341488998383284, 0.0014346676893070224, 0.001317804417565696, 0.0014198362004271773, 0.0014167416898548141]
[659.6580499937136, 675.7165222822078, 519.1756768623284, 668.7756394346435, 665.5147931934345, 692.777262347331, 621.1172135585465, 596.6597876490325, 664.6398447106011, 701.651473350014, 704.8396564415484, 655.5701844421661, 692.804717144551, 698.9850741999586, 689.1417661498298, 686.5208019059689, 491.3006967282266, 684.9523967520556, 728.9194828228244, 739.8646790064392, 760.435653107293, 551.6523281544386, 745.1142207035915, 731.9455709580176, 687.4662289084437, 740.5758364780918, 741.8979345381733, 745.8741196707075, 499.1137670650387, 475.490271409387, 752.3819719457589, 746.9048003830748, 713.0164936382289, 683.4797495277295, 375.01906833324864, 747.4024636762442, 745.7998289192391, 720.2645438226576, 574.0082394666545, 516.2429045331396, 512.2371324725252, 684.4628636224992, 697.8810764281409, 744.9129891549857, 695.5569876907904, 754.5059310986891, 752.2758752845099, 486.12844009966096, 662.5870993719034, 686.4913268283941, 706.3513506580204, 703.1419232703172, 718.1333225873797, 547.0274400839631, 719.3612176745778, 728.1345388869247, 756.754025183736, 561.5589398914908, 503.80122941475446, 713.8425831478817, 742.4115959701213, 744.9949378858707, 735.5733328597772, 695.9106413862908, 747.2114237561906, 753.7119652288001, 748.7771335665159, 554.3782252371145, 723.9533366248152, 721.0199482455331, 731.1246029088811, 743.5009416046046, 740.574587231635, 748.1148069285231, 727.7059657735615, 730.9590649028081, 745.7304591346739, 748.5684149946328, 570.2796177677972, 752.6529960967243, 741.153100385392, 630.653293484661, 512.2237859400208, 591.7067913282168, 710.0097062937917, 722.990410416434, 546.1198000769663, 516.4683912156275, 511.6808994955744, 516.4836912009805, 750.5352802946487, 743.8336111872288, 734.6758793985941, 744.7071756896995, 740.9259023402625, 720.85976415482, 730.2347231447291, 719.0984296571147, 720.41089103388, 727.0726883520874, 502.94818102558537, 739.2656387992917, 626.5367457838355, 717.9181616846965, 701.0169691328043, 679.8485578383858, 709.8854972054203, 738.6212309421877, 727.7893335571601, 681.364425984577, 755.5803820597963, 508.1860276513823, 569.0464508835502, 740.4375926466137, 750.4152374421052, 723.905455428486, 734.2931243524839, 499.9101712034775, 463.35236270897843, 677.6600712461232, 673.6465254668253, 634.8558015098137, 537.5923400834796, 741.3223391052547, 751.9701218675765, 734.5208148653853, 683.4301054450925, 693.1401309462401, 705.126149389797, 684.7882477653258, 488.19119820380854, 629.4770211097959, 742.0485149576981, 739.0769489030478, 740.017487286446, 676.4541041853937, 714.6815989406139, 711.4484605574589, 712.7878535223682, 492.01594860455447, 680.8827022232415, 711.4914315575621, 667.1102726199207, 742.6053518493039, 701.6464129609599, 690.6308694421341, 683.6952761629387, 728.7707264825157, 713.6668514937335, 706.5457285302289, 514.1705197143278, 702.3664586995116, 729.6899670087619, 727.7768549908074, 715.3114176571936, 503.85451817277004, 679.290785795595, 701.3451967785261, 662.4781579764651, 472.28563616333713, 503.9260881966112, 724.3067712010313, 692.5203411192525, 716.7446926572756, 692.7563840205308, 677.0631159546434, 726.7820354037599, 728.2791671441946, 661.6130240395861, 732.0048106857622, 667.7249778504048, 641.8398852574243, 696.5167873571855, 684.7043881598719, 669.8442177567837, 656.53680625998, 502.76832824360935, 730.2215051805534, 710.1728815277041, 708.1155338178843, 650.039810801126, 652.6945540259849, 601.7405417932132, 698.4857617594104, 658.7677121084073, 747.4149044293625, 675.4048096914837, 537.7105809395671, 651.9769220758564, 660.6975630909802, 747.2079663043223, 685.1559853422953, 714.6308261748592, 763.5093977008419, 749.1151208370404, 744.7349576758802, 757.7880615292402, 643.6306531453826, 491.7079852408685, 698.03465904948, 700.0520328326253, 690.8353397879022, 698.8860425398449, 597.9725618952696, 693.6388065137196, 753.1156258903859, 761.8914865936282, 697.1595162771462, 687.6159217711943, 710.373307838483, 595.6406113472455, 475.92189926230293, 750.0277426403335, 716.7224865996595, 485.0365563881415, 709.4716346290234, 691.196609265305, 704.7815052242319, 702.2237616124934, 680.4504580642824, 654.1310584727726, 495.7337442142035, 516.6705518167158, 737.8534860681459, 674.2013638371624, 469.04533649954953, 661.4804102975071, 710.5217512099817, 702.6168557075545, 702.7054779535348, 696.7266971863675, 536.6441195390317, 710.146621613557, 690.7947750364249, 495.218950606826, 495.60681799805633, 687.5347314328145, 703.060768722198, 716.0181806872258, 720.6126219953959, 718.67616530079, 492.1160190981604, 642.322635449176, 660.5284013799481, 694.0427731274995, 695.3489182143753, 691.6886520174204, 634.5667516632537, 702.1871390558197, 671.6405029862509, 672.5304088331778, 483.2798236345234, 495.701491886623, 677.5902936822374, 661.8313066635144, 706.1056984971559, 666.0797389193272, 482.18952805122245, 706.5089285776792, 709.2702548810562, 707.7877675215061, 699.9049536400881, 683.1628069888484, 504.46211171428234, 492.9324662263266, 704.1753661166902, 701.2282773176446, 692.6506889940646, 521.0529927582294, 516.6269929156539, 494.52911056165357, 701.6774713054243, 687.9510732518902, 686.3294636025623, 741.898340852652, 702.3407916115825, 550.6389123552643, 726.6278300243573, 742.3871861373331, 738.4516798553155, 729.8712832198756, 521.1924423614603, 700.591048380796, 712.61262055712, 709.3793149134722, 719.8709841050722, 535.5609439428846, 756.0275659938541, 733.3246914624592, 743.2908618665072, 755.4594637767331, 698.8853460900393, 744.1088169232032, 731.3795152377734, 708.8459297595714, 718.0297180138954, 465.6075986125187, 713.8541046583422, 750.3031572360245, 751.2176327604218, 672.2680176323088, 508.3438212154008, 701.2139524428167, 747.3151349767792, 725.4795463242674, 732.8520548372824, 499.5329098294997, 599.7200280558819, 729.5861496303327, 497.99328554815634, 553.3448979114609, 718.8323250255411, 757.7060907662484, 514.9889286264442, 512.3920529439142, 746.3805621065253, 745.151252874408, 745.9389948671943, 729.8847500417853, 755.8736527667667, 757.9426486612086, 710.9511253632768, 748.5233072279718, 753.509805009223, 743.5562679108166, 748.6858644982545, 712.2658256286165, 718.931240393294, 745.9454826058313, 731.5396106625589, 754.7788343911245, 742.0833038545213, 725.4489962669498, 724.5794724255697, 737.3701104305641, 755.1480823252406, 750.7214302641328, 759.6912491105348, 609.5731995221432, 716.2999409859145, 748.0167248932388, 751.7138949988273, 773.8674537840387, 747.6350650620047, 592.1502828593868, 742.2711584246103, 755.0965993683495, 687.7055963564259, 725.819431374615, 765.7128331744791, 661.5001923087772, 491.4306790254116, 744.901809306199, 724.3975941817722, 764.869648537446, 740.4406018083711, 500.0538936342562, 760.5970171523302, 711.1419907634229, 762.3540530695257, 566.3431487916079, 726.8440789621986, 711.0446540443903, 709.8046264194495, 694.1827697273607, 703.0393997874904, 766.220367822957, 707.5123302816088, 722.5682070841436, 737.2730633283454, 576.0951334784444, 707.561820043053, 761.8120918304944, 764.1371685698657, 735.440131658478, 443.96388046695995, 720.5636550933348, 733.3998813539233, 730.7926041872024, 737.3745530038235, 748.5882617878029, 729.5876435762394, 756.6757729175026, 742.5351430163555, 763.6667944629992, 733.217395005197, 496.47563299684754, 484.0139825882005, 734.453427050899, 732.4733150934669, 732.6053719960873, 719.2662308031531, 711.8865970220145, 739.3453551265695, 765.8662826531961, 740.4173850916869, 742.6939207976368, 722.329711452587, 748.8290791353035, 759.1447726473814, 757.0250075237836, 757.521359131271, 723.1351266847751, 755.1279993907957, 741.7000655910662, 684.9716285398742, 607.566920865012, 664.4011279897567, 580.9870372961872, 729.5511104273681, 754.2441271953924, 757.4366191745166, 747.596442954498, 740.3415606894149, 736.3091532025298, 513.7072340770065, 730.8295924397243, 684.1798682022853, 703.7753094992843, 733.1985013790478, 531.5116751442444, 691.9311249494782, 736.2463657881502, 700.2134475620379, 713.2741960884928, 722.1482331652713, 754.6424726783205, 748.475308009347, 726.6022210910095, 765.2359233790053, 746.3922046776032, 762.0219464486803, 770.8637638300371, 727.9447716825629, 741.9605549363245, 688.9426976570797, 725.826071153623, 735.9494968775795, 737.4058586178671, 732.9731948502039, 521.1618436480252, 501.0018713797365, 730.564243148484, 754.3840324369345, 753.0875488261833, 754.7894725984487, 770.7375220059477, 754.353328983301, 729.4844296688964, 750.0414091639809, 750.3398036196994, 759.4133607071367, 761.5643007419712, 767.7953256137712, 531.7023907157932, 726.4792501762872, 767.1606569022186, 652.6469171680595, 488.0477393293705, 491.4044356077015, 671.0795012562589, 695.4857004864482, 708.903383112611, 681.7963583383853, 698.2972361106673, 701.666410586966, 698.8368422166977, 667.0722292956233, 690.1612021889882, 754.6148428241376, 508.06895413540235, 771.0842448967179, 765.6899717397573, 761.4440393114952, 759.8289217170177, 496.41042164137735, 751.7347718152173, 666.5934898926889, 756.96571759494, 744.0380787314537, 762.2893401436992, 518.7242858131152, 711.4126864134229, 745.0053451573904, 508.3589920385341, 743.8532116981479, 749.8412037841276, 752.7075720411048, 755.4879126856449, 766.6832134452397, 734.9052685948296, 716.6259104290583, 746.2195650698563, 765.581569939937, 751.8438531417581, 725.4844262505437, 701.8245621550043, 710.363309201264, 723.5129077746233, 708.6850850656832, 721.1551611268372, 734.2396777751874, 730.9326898049441, 770.9864893884592, 751.2387570444771, 700.2711102435286, 718.3657332967231, 704.5710959150044, 693.0396068180771, 691.4034717125712, 765.289119625401, 754.7165000708, 727.4837685922786, 760.8778102245178, 747.3614491271799, 757.021320055449, 731.6248426946082, 686.2034538069126, 651.0957344458232, 676.5286184206382, 670.2222948802407, 535.4774393708493, 677.0271554848707, 697.1882038398661, 685.4147107136107, 491.60609632828687, 697.0255254602012, 758.8379479310308, 704.3065951545229, 705.8449731245495]
Elapsed: 0.19197372661877143~0.028915985958187864
Time per graph: 0.0014881684234013288~0.00022415492990843306
Speed: 684.3652861273412~83.00464779388152
Total Time: 0.1844
best val loss: 0.2262137985766627 test_score: 0.9070

Testing...
Test loss: 0.2574 score: 0.8915 time: 0.18s
test Score 0.8915
Epoch Time List: [0.6992513318546116, 0.6556087271310389, 0.7580386449117213, 0.8286614753305912, 0.6462946541141719, 0.631576957879588, 0.6655413750559092, 0.6668955388013273, 0.7012166550848633, 0.6358412296976894, 0.6371782629285008, 0.6523519458714873, 0.7072837031446397, 0.6485546519979835, 0.6553817891981453, 0.6392604268621653, 0.7291037449613214, 0.6511636071372777, 0.6186616832856089, 0.6091485810466111, 0.6076428131200373, 0.7305049560964108, 0.6901833289302886, 0.6226829171646386, 0.6240186039358377, 0.6910081212408841, 0.6027533719316125, 0.6140857820864767, 0.7150368499569595, 0.7123725388664752, 0.5922684296965599, 0.6011161538772285, 0.7183375260792673, 0.6561041662935168, 0.8599477210082114, 0.5998430298641324, 0.6230881698429585, 0.6945768152363598, 0.6515220266301185, 0.8498574697878212, 0.8606860809959471, 0.6955731539055705, 0.6498421777505428, 0.6036149237770587, 0.6309135060291737, 0.5975567661225796, 0.6062078650575131, 0.7314100377261639, 0.6496395720168948, 0.6342612239532173, 0.6323260359931737, 0.6302479768637568, 0.6323235458694398, 0.684377009049058, 0.6267482549883425, 0.6059373538009822, 0.6019792389124632, 0.6870478929486126, 0.8753250890877098, 0.7310672528110445, 0.6059742111247033, 0.6221048012375832, 0.6785717131569982, 0.6252567849587649, 0.6107012848369777, 0.6162685810122639, 0.6177883967757225, 0.6922158882953227, 0.6286076207179576, 0.6183335559908301, 0.6048268128652126, 0.6039398580323905, 0.6200827478896827, 0.6734565428923815, 0.6078863823786378, 0.6148446060251445, 0.6019329952541739, 0.6051112967543304, 0.7077911507803947, 0.6095163659192622, 0.602489828132093, 0.6390453178901225, 0.8187367760110646, 0.843420404009521, 0.6832954417914152, 0.6085957821924239, 0.7099477211013436, 0.8880798008758575, 0.8801922623533756, 0.8818422961048782, 0.7081265621818602, 0.6084602910559624, 0.6203891031909734, 0.612826009048149, 0.6139529731590301, 0.6452527537476271, 0.6991021109279245, 0.6158617560286075, 0.6263722409494221, 0.6294698230922222, 0.729784871218726, 0.6127379250247031, 0.6475764743518084, 0.614023903850466, 0.6330125860404223, 0.6428066971711814, 0.638398117152974, 0.7044880860485137, 0.6110677500255406, 0.6332698110491037, 0.600988607853651, 0.8146764000412077, 0.8243281757459044, 0.6120568686164916, 0.6195135631132871, 0.6148133240640163, 0.6173145161010325, 0.7130357751157135, 0.7158320709131658, 0.637345609953627, 0.644713374087587, 0.7517394777387381, 0.7645990499295294, 0.7905039971228689, 0.604502891888842, 0.6155776497907937, 0.7070700731128454, 0.7172898689750582, 0.6357637322507799, 0.6400095496792346, 0.7258474959526211, 0.8379479080904275, 0.6103005507029593, 0.6073234782088548, 0.6085759059060365, 0.6204776139929891, 0.6834475921932608, 0.6281572347506881, 0.6330926001537591, 0.7382162590511143, 0.7722826190292835, 0.635851738974452, 0.6658529967535287, 0.6090274120215327, 0.6412365310825408, 0.7034799149259925, 0.6516829421743751, 0.627736551919952, 0.6239908100105822, 0.6197608020156622, 0.7310183169320226, 0.7513484128285199, 0.6284190369769931, 0.6238662041723728, 0.6368040291126817, 0.809371598996222, 0.7772148721851408, 0.6581145708914846, 0.6636810589116067, 0.8750810762867332, 0.8832525359466672, 0.7002648350317031, 0.6325781277846545, 0.6197130139917135, 0.6437282310798764, 0.6514649011660367, 0.6988972479011863, 0.6546521270647645, 0.6438497412018478, 0.6143954440485686, 0.6497618479188532, 0.6761770960874856, 0.6932455240748823, 0.6630704021081328, 0.6604978409595788, 0.669176768977195, 0.81479362398386, 0.625688573345542, 0.6229714618530124, 0.6393743169028312, 0.6523202441167086, 0.6479931240901351, 0.7164495489560068, 0.6230184282176197, 0.6468159209471196, 0.6007866368163377, 0.635342005873099, 0.7222232830245048, 0.6667053063865751, 0.6413330261129886, 0.5881421100348234, 0.6220906453672796, 0.6003622731659561, 0.683411325328052, 0.584728044224903, 0.5977546337526292, 0.5964157569687814, 0.6286438191309571, 0.8397109629586339, 0.7202279260382056, 0.6316011268645525, 0.6448572061490268, 0.6375760398805141, 0.6673816787078977, 0.719821535050869, 0.6105381930246949, 0.5950351380743086, 0.7367052384652197, 0.6383207109756768, 0.6535808849148452, 0.6548748470377177, 0.7795964051038027, 0.6018632999621332, 0.6205232390202582, 0.7173348891083151, 0.6280108781065792, 0.6539987057913095, 0.7032698369584978, 0.6388475941494107, 0.6548783387988806, 0.6499590361490846, 0.7780415117740631, 0.8797439138870686, 0.6835707121063024, 0.6738753831014037, 0.8954118289984763, 0.7747814301401377, 0.6404906890820712, 0.6505805980414152, 0.6539959162473679, 0.6558579187840223, 0.7196212569251657, 0.6779294719453901, 0.6531295729801059, 0.8847974510863423, 0.9085708491038531, 0.7650458451826125, 0.6492475320119411, 0.6363466752227396, 0.6381139049772173, 0.6446226120460778, 0.81516751809977, 0.8452537900302559, 0.6926459060050547, 0.7113753180019557, 0.6731200190261006, 0.6740522542968392, 0.7585477558895946, 0.7506352926138788, 0.6943104940000921, 0.6936483783647418, 0.7608585979323834, 0.9283731249161065, 0.8300203320104629, 0.6749288041610271, 0.6477659028023481, 0.6659326471854001, 0.7469927712809294, 0.7297601448372006, 0.648411616217345, 0.6467287389095873, 0.65651715407148, 0.6528507336042821, 0.7752271792851388, 0.928728812141344, 0.7570554150734097, 0.6637789560481906, 0.6796596441417933, 0.8238104998599738, 0.900602500885725, 0.9075243182014674, 0.6783042431343347, 0.6752287258859724, 0.6628715042024851, 0.6402469971217215, 0.6528697467874736, 0.8251898370217532, 0.6485760398209095, 0.6407876431476325, 0.629431425826624, 0.6541107010561973, 0.8907031221315265, 0.7655843729153275, 0.7613559612073004, 0.6511314050294459, 0.7334678492043167, 0.819333653897047, 0.7234963790979236, 0.6477558792103082, 0.7122914688661695, 0.6255609602667391, 0.7128589188214391, 0.6570408109109849, 0.7187627621460706, 0.6489658730570227, 0.6429147829767317, 0.8666112541686743, 0.6565437309909612, 0.6273810530547053, 0.633779562311247, 0.652578730834648, 0.8692904380150139, 0.8018657648935914, 0.6499106611590832, 0.6423972533084452, 0.6585654958616942, 0.8043994239997119, 0.924622425576672, 0.7378780518192798, 0.756924951216206, 0.9011404148768634, 0.6774429481010884, 0.6432299658190459, 0.7323750539217144, 0.9070879130158573, 0.7316030801739544, 0.6535730597097427, 0.6392454889137298, 0.6486110230907798, 0.6458242149092257, 0.6505235501099378, 0.7009654738940299, 0.7306172910612077, 0.6417175356764346, 0.6440761019475758, 0.646937791723758, 0.6495898091234267, 0.7214496564120054, 0.6494862879626453, 0.6393965408205986, 0.6458092948887497, 0.6375821481924504, 0.6752560089807957, 0.8202634369954467, 0.647152537945658, 0.6474309500772506, 0.6460275719873607, 0.6416279962286353, 0.7513882622588426, 0.6461248081177473, 0.6391964619979262, 0.6352238282561302, 0.6207901469897479, 0.6368072957266122, 0.7756155338138342, 0.6497164848260581, 0.6567544757854193, 0.6853652549907565, 0.6400641032960266, 0.6332969702780247, 0.6578618821222335, 0.800869305850938, 0.6517800430301577, 0.6580826828721911, 0.648259487003088, 0.6335473279468715, 0.8168578250333667, 0.7274117120541632, 0.6504434361122549, 0.6354017599951476, 0.7072915516328067, 0.726625886047259, 0.6656821619253606, 0.6715004758443683, 0.8161129388026893, 0.6657418722752482, 0.69714400684461, 0.6628461210057139, 0.7347706318832934, 0.632998115150258, 0.7419676259160042, 0.7223288821987808, 0.6405423739925027, 0.6466040029190481, 0.6392961740493774, 0.7674479898996651, 0.7433126219548285, 0.6512618060223758, 0.6402723442297429, 0.6342141050845385, 0.6429292580578476, 0.6679579489864409, 0.7137613778468221, 0.645693679805845, 0.6341268932446837, 0.6422227588482201, 0.9073021169751883, 0.9236162330489606, 0.7222300069406629, 0.645324908895418, 0.6493126940913498, 0.6505928980186582, 0.6474701901897788, 0.7080665959510952, 0.6420167230535299, 0.6309657238889486, 0.6380694899708033, 0.6462005558423698, 0.6844972369726747, 0.6955576858017594, 0.6423341990448534, 0.6452375708613545, 0.638932716101408, 0.7622208138927817, 0.6384434399660677, 0.6732571676839143, 0.674054364906624, 0.6706578051671386, 0.8095523419324309, 0.7266110689379275, 0.6306087323464453, 0.6300327607896179, 0.6362709780223668, 0.6398331550881267, 0.6650007772259414, 0.8878744738176465, 0.6749356251675636, 0.6601201719604433, 0.6500324769876897, 0.6529211248271167, 0.8046543879900128, 0.7373364882078022, 0.6296390960924327, 0.64795400085859, 0.6660693499725312, 0.6680893779266626, 0.7106413238216192, 0.6354931241367012, 0.6489453818649054, 0.620910185854882, 0.6441617968957871, 0.6993983280844986, 0.6189482619520277, 0.6346988717559725, 0.6389899051282555, 0.7492122519761324, 0.6517568461131305, 0.6480030720122159, 0.6312830580864102, 0.6480089162942022, 0.8845131469424814, 0.8958606158848852, 0.6381098839920014, 0.630570919951424, 0.7007766517344862, 0.6291786688379943, 0.6290653189644217, 0.6348731720354408, 0.6658204048871994, 0.7012529871426523, 0.6385965859517455, 0.6327164780814201, 0.6381075729150325, 0.6336469079833478, 0.7972431257367134, 0.7613470838405192, 0.6285540498793125, 0.7102303521241993, 0.8701261039823294, 0.9363337799441069, 0.7435663321521133, 0.7521433930378407, 0.6611869782209396, 0.6819204003550112, 0.7623585748951882, 0.6722173120360821, 0.6711150130722672, 0.6813904920127243, 0.6820796350948513, 0.6321363958995789, 0.7317758090794086, 0.6547327481675893, 0.6278898559976369, 0.6296600920613855, 0.6411182091105729, 0.8133306268136948, 0.765783904120326, 0.6730363231617957, 0.6766599561087787, 0.630267548840493, 0.6343631709460169, 0.7539905321318656, 0.6495472600217909, 0.631879563909024, 0.8492024692241102, 0.8074877760373056, 0.6320586930960417, 0.6347312179859728, 0.6331402838695794, 0.6302169018890709, 0.6403719307854772, 0.7060466927941889, 0.6340941339731216, 0.6400276718195528, 0.6359993400983512, 0.6518976520746946, 0.7012435228098184, 0.7330840569920838, 0.6587330410256982, 0.6725292548071593, 0.6662149750627577, 0.6362803019583225, 0.6857316151726991, 0.7090473191346973, 0.6335667569655925, 0.6573207711335272, 0.6627178827766329, 0.6675752599257976, 0.7318057280499488, 0.6687121649738401, 0.6259959638118744, 0.6277771999593824, 0.664135173894465, 0.7218919389415532, 0.6500199357979, 0.6433444339782, 0.6459128679707646, 0.6792848368640989, 0.6915094831492752, 0.7326171481981874, 0.6775028121192008, 0.7282716729678214, 0.6799117350019515, 0.665798946050927, 0.6796007468365133, 0.92921481304802, 0.802509075962007, 0.6265783470589668, 0.665549793979153, 0.6781852501444519]
Total Epoch List: [263, 267]
Total Time List: [0.1897998140193522, 0.18440872197970748]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2b77df0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.7387;  Loss pred: 2.7387; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7000 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 2.7582;  Loss pred: 2.7582; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.16s
Epoch 3/1000, LR 0.000050
Train loss: 2.7137;  Loss pred: 2.7137; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.5000 time: 0.16s
Epoch 4/1000, LR 0.000080
Train loss: 2.6558;  Loss pred: 2.6558; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 2.6155;  Loss pred: 2.6155; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 2.5402;  Loss pred: 2.5402; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 0.22s
Epoch 7/1000, LR 0.000170
Train loss: 2.4528;  Loss pred: 2.4528; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.18s
Epoch 8/1000, LR 0.000200
Train loss: 2.3422;  Loss pred: 2.3422; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 2.2276;  Loss pred: 2.2276; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 2.1186;  Loss pred: 2.1186; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 1.9970;  Loss pred: 1.9970; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.19s
Epoch 12/1000, LR 0.000290
Train loss: 1.8839;  Loss pred: 1.8839; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.19s
Epoch 13/1000, LR 0.000290
Train loss: 1.7854;  Loss pred: 1.7854; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 1.6845;  Loss pred: 1.6845; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.18s
Epoch 15/1000, LR 0.000290
Train loss: 1.6140;  Loss pred: 1.6140; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.18s
Epoch 16/1000, LR 0.000290
Train loss: 1.5445;  Loss pred: 1.5445; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.18s
Epoch 17/1000, LR 0.000290
Train loss: 1.4845;  Loss pred: 1.4845; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000290
Train loss: 1.4268;  Loss pred: 1.4268; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.18s
Epoch 19/1000, LR 0.000290
Train loss: 1.3866;  Loss pred: 1.3866; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.20s
Epoch 20/1000, LR 0.000290
Train loss: 1.3361;  Loss pred: 1.3361; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.17s
Epoch 21/1000, LR 0.000290
Train loss: 1.3028;  Loss pred: 1.3028; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 1.2721;  Loss pred: 1.2721; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 23/1000, LR 0.000290
Train loss: 1.2364;  Loss pred: 1.2364; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 24/1000, LR 0.000290
Train loss: 1.2083;  Loss pred: 1.2083; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 1.1815;  Loss pred: 1.1815; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.18s
Epoch 26/1000, LR 0.000290
Train loss: 1.1694;  Loss pred: 1.1694; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.19s
Epoch 27/1000, LR 0.000290
Train loss: 1.1482;  Loss pred: 1.1482; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 1.1307;  Loss pred: 1.1307; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.18s
Epoch 29/1000, LR 0.000290
Train loss: 1.1158;  Loss pred: 1.1158; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000290
Train loss: 1.1031;  Loss pred: 1.1031; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.22s
Epoch 31/1000, LR 0.000290
Train loss: 1.0905;  Loss pred: 1.0905; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.18s
Epoch 32/1000, LR 0.000290
Train loss: 1.0827;  Loss pred: 1.0827; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.18s
Epoch 33/1000, LR 0.000290
Train loss: 1.0742;  Loss pred: 1.0742; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.18s
Epoch 34/1000, LR 0.000290
Train loss: 1.0626;  Loss pred: 1.0626; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.18s
Epoch 35/1000, LR 0.000290
Train loss: 1.0525;  Loss pred: 1.0525; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.18s
Epoch 36/1000, LR 0.000290
Train loss: 1.0483;  Loss pred: 1.0483; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.18s
Epoch 37/1000, LR 0.000290
Train loss: 1.0409;  Loss pred: 1.0409; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.18s
Epoch 38/1000, LR 0.000289
Train loss: 1.0358;  Loss pred: 1.0358; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 0.18s
Epoch 39/1000, LR 0.000289
Train loss: 1.0287;  Loss pred: 1.0287; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 1.0268;  Loss pred: 1.0268; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.22s
Epoch 41/1000, LR 0.000289
Train loss: 1.0205;  Loss pred: 1.0205; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 1.0179;  Loss pred: 1.0179; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5000 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 1.0144;  Loss pred: 1.0144; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 1.0099;  Loss pred: 1.0099; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5000 time: 0.18s
Epoch 45/1000, LR 0.000289
Train loss: 1.0064;  Loss pred: 1.0064; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5000 time: 0.18s
Epoch 46/1000, LR 0.000289
Train loss: 1.0035;  Loss pred: 1.0035; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5000 time: 0.17s
Epoch 47/1000, LR 0.000289
Train loss: 1.0011;  Loss pred: 1.0011; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5000 time: 0.18s
Epoch 48/1000, LR 0.000289
Train loss: 0.9969;  Loss pred: 0.9969; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 0.18s
Epoch 49/1000, LR 0.000289
Train loss: 0.9972;  Loss pred: 0.9972; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.18s
Epoch 50/1000, LR 0.000289
Train loss: 0.9933;  Loss pred: 0.9933; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.5000 time: 0.18s
Epoch 51/1000, LR 0.000289
Train loss: 0.9908;  Loss pred: 0.9908; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.5000 time: 0.19s
Epoch 52/1000, LR 0.000289
Train loss: 0.9884;  Loss pred: 0.9884; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.5000 time: 0.23s
Epoch 53/1000, LR 0.000289
Train loss: 0.9864;  Loss pred: 0.9864; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.5000 time: 0.23s
Epoch 54/1000, LR 0.000289
Train loss: 0.9849;  Loss pred: 0.9849; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.5000 time: 0.23s
Epoch 55/1000, LR 0.000289
Train loss: 0.9822;  Loss pred: 0.9822; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.5000 time: 0.18s
Epoch 56/1000, LR 0.000289
Train loss: 0.9794;  Loss pred: 0.9794; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6780 score: 0.5000 time: 0.18s
Epoch 57/1000, LR 0.000288
Train loss: 0.9781;  Loss pred: 0.9781; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6771 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.5000 time: 0.18s
Epoch 58/1000, LR 0.000288
Train loss: 0.9763;  Loss pred: 0.9763; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6763 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6759 score: 0.5000 time: 0.18s
Epoch 59/1000, LR 0.000288
Train loss: 0.9743;  Loss pred: 0.9743; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6752 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6747 score: 0.5000 time: 0.19s
Epoch 60/1000, LR 0.000288
Train loss: 0.9724;  Loss pred: 0.9724; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6733 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6729 score: 0.5000 time: 0.18s
Epoch 61/1000, LR 0.000288
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6712 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6708 score: 0.5000 time: 0.22s
Epoch 62/1000, LR 0.000288
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 0.31s
Val loss: 0.6692 score: 0.5039 time: 0.19s
Test loss: 0.6689 score: 0.5078 time: 0.18s
Epoch 63/1000, LR 0.000288
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 0.31s
Val loss: 0.6674 score: 0.5271 time: 0.19s
Test loss: 0.6671 score: 0.5234 time: 0.18s
Epoch 64/1000, LR 0.000288
Train loss: 0.9632;  Loss pred: 0.9632; Loss self: 0.0000; time: 0.29s
Val loss: 0.6657 score: 0.6047 time: 0.17s
Test loss: 0.6653 score: 0.6016 time: 0.18s
Epoch 65/1000, LR 0.000288
Train loss: 0.9616;  Loss pred: 0.9616; Loss self: 0.0000; time: 0.30s
Val loss: 0.6639 score: 0.6279 time: 0.22s
Test loss: 0.6634 score: 0.6172 time: 0.18s
Epoch 66/1000, LR 0.000288
Train loss: 0.9587;  Loss pred: 0.9587; Loss self: 0.0000; time: 0.36s
Val loss: 0.6620 score: 0.6357 time: 0.24s
Test loss: 0.6614 score: 0.6250 time: 0.18s
Epoch 67/1000, LR 0.000288
Train loss: 0.9565;  Loss pred: 0.9565; Loss self: 0.0000; time: 0.31s
Val loss: 0.6599 score: 0.6434 time: 0.19s
Test loss: 0.6592 score: 0.6406 time: 0.18s
Epoch 68/1000, LR 0.000288
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.30s
Val loss: 0.6580 score: 0.6434 time: 0.18s
Test loss: 0.6572 score: 0.6250 time: 0.18s
Epoch 69/1000, LR 0.000288
Train loss: 0.9518;  Loss pred: 0.9518; Loss self: 0.0000; time: 0.30s
Val loss: 0.6563 score: 0.6357 time: 0.19s
Test loss: 0.6553 score: 0.6250 time: 0.19s
Epoch 70/1000, LR 0.000287
Train loss: 0.9495;  Loss pred: 0.9495; Loss self: 0.0000; time: 0.30s
Val loss: 0.6538 score: 0.6744 time: 0.20s
Test loss: 0.6527 score: 0.6484 time: 0.19s
Epoch 71/1000, LR 0.000287
Train loss: 0.9473;  Loss pred: 0.9473; Loss self: 0.0000; time: 0.38s
Val loss: 0.6511 score: 0.7364 time: 0.18s
Test loss: 0.6499 score: 0.7344 time: 0.17s
Epoch 72/1000, LR 0.000287
Train loss: 0.9452;  Loss pred: 0.9452; Loss self: 0.0000; time: 0.30s
Val loss: 0.6486 score: 0.8140 time: 0.19s
Test loss: 0.6475 score: 0.8516 time: 0.17s
Epoch 73/1000, LR 0.000287
Train loss: 0.9432;  Loss pred: 0.9432; Loss self: 0.0000; time: 0.29s
Val loss: 0.6468 score: 0.8682 time: 0.17s
Test loss: 0.6457 score: 0.8672 time: 0.18s
Epoch 74/1000, LR 0.000287
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 0.29s
Val loss: 0.6447 score: 0.8760 time: 0.17s
Test loss: 0.6437 score: 0.8828 time: 0.17s
Epoch 75/1000, LR 0.000287
Train loss: 0.9396;  Loss pred: 0.9396; Loss self: 0.0000; time: 0.30s
Val loss: 0.6420 score: 0.8760 time: 0.18s
Test loss: 0.6408 score: 0.8750 time: 0.17s
Epoch 76/1000, LR 0.000287
Train loss: 0.9364;  Loss pred: 0.9364; Loss self: 0.0000; time: 0.30s
Val loss: 0.6389 score: 0.8682 time: 0.18s
Test loss: 0.6375 score: 0.8750 time: 0.21s
Epoch 77/1000, LR 0.000287
Train loss: 0.9333;  Loss pred: 0.9333; Loss self: 0.0000; time: 0.48s
Val loss: 0.6358 score: 0.8682 time: 0.25s
Test loss: 0.6343 score: 0.8750 time: 0.20s
Epoch 78/1000, LR 0.000287
Train loss: 0.9308;  Loss pred: 0.9308; Loss self: 0.0000; time: 0.30s
Val loss: 0.6329 score: 0.8682 time: 0.18s
Test loss: 0.6313 score: 0.8750 time: 0.17s
Epoch 79/1000, LR 0.000287
Train loss: 0.9281;  Loss pred: 0.9281; Loss self: 0.0000; time: 0.29s
Val loss: 0.6296 score: 0.8682 time: 0.17s
Test loss: 0.6280 score: 0.8750 time: 0.17s
Epoch 80/1000, LR 0.000287
Train loss: 0.9250;  Loss pred: 0.9250; Loss self: 0.0000; time: 0.29s
Val loss: 0.6261 score: 0.8760 time: 0.17s
Test loss: 0.6244 score: 0.8750 time: 0.17s
Epoch 81/1000, LR 0.000286
Train loss: 0.9221;  Loss pred: 0.9221; Loss self: 0.0000; time: 0.35s
Val loss: 0.6224 score: 0.8837 time: 0.19s
Test loss: 0.6206 score: 0.8828 time: 0.18s
Epoch 82/1000, LR 0.000286
Train loss: 0.9189;  Loss pred: 0.9189; Loss self: 0.0000; time: 0.49s
Val loss: 0.6185 score: 0.8915 time: 0.26s
Test loss: 0.6168 score: 0.8984 time: 0.22s
Epoch 83/1000, LR 0.000286
Train loss: 0.9160;  Loss pred: 0.9160; Loss self: 0.0000; time: 0.42s
Val loss: 0.6145 score: 0.8837 time: 0.19s
Test loss: 0.6128 score: 0.9062 time: 0.19s
Epoch 84/1000, LR 0.000286
Train loss: 0.9117;  Loss pred: 0.9117; Loss self: 0.0000; time: 0.33s
Val loss: 0.6102 score: 0.8760 time: 0.19s
Test loss: 0.6085 score: 0.9062 time: 0.18s
Epoch 85/1000, LR 0.000286
Train loss: 0.9091;  Loss pred: 0.9091; Loss self: 0.0000; time: 0.32s
Val loss: 0.6058 score: 0.8760 time: 0.18s
Test loss: 0.6041 score: 0.8984 time: 0.17s
Epoch 86/1000, LR 0.000286
Train loss: 0.9046;  Loss pred: 0.9046; Loss self: 0.0000; time: 0.29s
Val loss: 0.6009 score: 0.8760 time: 0.17s
Test loss: 0.5991 score: 0.8984 time: 0.16s
Epoch 87/1000, LR 0.000286
Train loss: 0.9001;  Loss pred: 0.9001; Loss self: 0.0000; time: 0.30s
Val loss: 0.5965 score: 0.8837 time: 0.18s
Test loss: 0.5946 score: 0.8906 time: 0.20s
Epoch 88/1000, LR 0.000286
Train loss: 0.8963;  Loss pred: 0.8963; Loss self: 0.0000; time: 0.43s
Val loss: 0.5923 score: 0.8682 time: 0.27s
Test loss: 0.5905 score: 0.8828 time: 0.17s
Epoch 89/1000, LR 0.000286
Train loss: 0.8930;  Loss pred: 0.8930; Loss self: 0.0000; time: 0.29s
Val loss: 0.5877 score: 0.8605 time: 0.18s
Test loss: 0.5856 score: 0.8906 time: 0.18s
Epoch 90/1000, LR 0.000285
Train loss: 0.8897;  Loss pred: 0.8897; Loss self: 0.0000; time: 0.31s
Val loss: 0.5813 score: 0.8760 time: 0.18s
Test loss: 0.5788 score: 0.8984 time: 0.18s
Epoch 91/1000, LR 0.000285
Train loss: 0.8828;  Loss pred: 0.8828; Loss self: 0.0000; time: 0.40s
Val loss: 0.5758 score: 0.8915 time: 0.19s
Test loss: 0.5728 score: 0.8984 time: 0.18s
Epoch 92/1000, LR 0.000285
Train loss: 0.8786;  Loss pred: 0.8786; Loss self: 0.0000; time: 0.29s
Val loss: 0.5711 score: 0.8915 time: 0.17s
Test loss: 0.5678 score: 0.8984 time: 0.18s
Epoch 93/1000, LR 0.000285
Train loss: 0.8756;  Loss pred: 0.8756; Loss self: 0.0000; time: 0.30s
Val loss: 0.5667 score: 0.8915 time: 0.36s
Test loss: 0.5632 score: 0.9062 time: 0.22s
Epoch 94/1000, LR 0.000285
Train loss: 0.8714;  Loss pred: 0.8714; Loss self: 0.0000; time: 0.38s
Val loss: 0.5621 score: 0.8915 time: 0.18s
Test loss: 0.5581 score: 0.9062 time: 0.17s
Epoch 95/1000, LR 0.000285
Train loss: 0.8673;  Loss pred: 0.8673; Loss self: 0.0000; time: 0.31s
Val loss: 0.5572 score: 0.8915 time: 0.25s
Test loss: 0.5528 score: 0.9062 time: 0.17s
Epoch 96/1000, LR 0.000285
Train loss: 0.8632;  Loss pred: 0.8632; Loss self: 0.0000; time: 0.30s
Val loss: 0.5525 score: 0.8837 time: 0.18s
Test loss: 0.5476 score: 0.8828 time: 0.18s
Epoch 97/1000, LR 0.000285
Train loss: 0.8592;  Loss pred: 0.8592; Loss self: 0.0000; time: 0.30s
Val loss: 0.5479 score: 0.8837 time: 0.18s
Test loss: 0.5425 score: 0.8828 time: 0.18s
Epoch 98/1000, LR 0.000285
Train loss: 0.8538;  Loss pred: 0.8538; Loss self: 0.0000; time: 0.30s
Val loss: 0.5428 score: 0.8837 time: 0.18s
Test loss: 0.5370 score: 0.8828 time: 0.17s
Epoch 99/1000, LR 0.000284
Train loss: 0.8518;  Loss pred: 0.8518; Loss self: 0.0000; time: 0.32s
Val loss: 0.5378 score: 0.8837 time: 0.19s
Test loss: 0.5316 score: 0.8828 time: 0.20s
Epoch 100/1000, LR 0.000284
Train loss: 0.8459;  Loss pred: 0.8459; Loss self: 0.0000; time: 0.31s
Val loss: 0.5317 score: 0.8837 time: 0.18s
Test loss: 0.5252 score: 0.8984 time: 0.16s
Epoch 101/1000, LR 0.000284
Train loss: 0.8408;  Loss pred: 0.8408; Loss self: 0.0000; time: 0.29s
Val loss: 0.5258 score: 0.8915 time: 0.17s
Test loss: 0.5190 score: 0.9062 time: 0.17s
Epoch 102/1000, LR 0.000284
Train loss: 0.8354;  Loss pred: 0.8354; Loss self: 0.0000; time: 0.30s
Val loss: 0.5203 score: 0.8837 time: 0.18s
Test loss: 0.5135 score: 0.9062 time: 0.18s
Epoch 103/1000, LR 0.000284
Train loss: 0.8315;  Loss pred: 0.8315; Loss self: 0.0000; time: 0.31s
Val loss: 0.5148 score: 0.8837 time: 0.18s
Test loss: 0.5076 score: 0.9062 time: 0.18s
Epoch 104/1000, LR 0.000284
Train loss: 0.8266;  Loss pred: 0.8266; Loss self: 0.0000; time: 0.30s
Val loss: 0.5088 score: 0.8915 time: 0.22s
Test loss: 0.5009 score: 0.9062 time: 0.18s
Epoch 105/1000, LR 0.000284
Train loss: 0.8209;  Loss pred: 0.8209; Loss self: 0.0000; time: 0.40s
Val loss: 0.5037 score: 0.8837 time: 0.18s
Test loss: 0.4953 score: 0.9062 time: 0.17s
Epoch 106/1000, LR 0.000283
Train loss: 0.8167;  Loss pred: 0.8167; Loss self: 0.0000; time: 0.30s
Val loss: 0.4977 score: 0.8837 time: 0.17s
Test loss: 0.4890 score: 0.9062 time: 0.17s
Epoch 107/1000, LR 0.000283
Train loss: 0.8112;  Loss pred: 0.8112; Loss self: 0.0000; time: 0.29s
Val loss: 0.4917 score: 0.8837 time: 0.17s
Test loss: 0.4833 score: 0.9062 time: 0.16s
Epoch 108/1000, LR 0.000283
Train loss: 0.8075;  Loss pred: 0.8075; Loss self: 0.0000; time: 0.30s
Val loss: 0.4891 score: 0.8837 time: 0.17s
Test loss: 0.4810 score: 0.8984 time: 0.17s
Epoch 109/1000, LR 0.000283
Train loss: 0.8046;  Loss pred: 0.8046; Loss self: 0.0000; time: 0.30s
Val loss: 0.4857 score: 0.8760 time: 0.17s
Test loss: 0.4776 score: 0.8984 time: 0.16s
Epoch 110/1000, LR 0.000283
Train loss: 0.8010;  Loss pred: 0.8010; Loss self: 0.0000; time: 0.31s
Val loss: 0.4839 score: 0.8605 time: 0.18s
Test loss: 0.4759 score: 0.8984 time: 0.17s
Epoch 111/1000, LR 0.000283
Train loss: 0.7994;  Loss pred: 0.7994; Loss self: 0.0000; time: 0.48s
Val loss: 0.4853 score: 0.8295 time: 0.26s
Test loss: 0.4774 score: 0.8594 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000283
Train loss: 0.8006;  Loss pred: 0.8006; Loss self: 0.0000; time: 0.32s
Val loss: 0.4785 score: 0.8295 time: 0.25s
Test loss: 0.4702 score: 0.8828 time: 0.22s
Epoch 113/1000, LR 0.000282
Train loss: 0.7924;  Loss pred: 0.7924; Loss self: 0.0000; time: 0.49s
Val loss: 0.4643 score: 0.8837 time: 0.26s
Test loss: 0.4551 score: 0.8984 time: 0.21s
Epoch 114/1000, LR 0.000282
Train loss: 0.7813;  Loss pred: 0.7813; Loss self: 0.0000; time: 0.49s
Val loss: 0.4556 score: 0.8837 time: 0.22s
Test loss: 0.4449 score: 0.9062 time: 0.17s
Epoch 115/1000, LR 0.000282
Train loss: 0.7756;  Loss pred: 0.7756; Loss self: 0.0000; time: 0.30s
Val loss: 0.4502 score: 0.8915 time: 0.17s
Test loss: 0.4386 score: 0.9062 time: 0.18s
Epoch 116/1000, LR 0.000282
Train loss: 0.7699;  Loss pred: 0.7699; Loss self: 0.0000; time: 0.30s
Val loss: 0.4450 score: 0.8915 time: 0.17s
Test loss: 0.4327 score: 0.9062 time: 0.17s
Epoch 117/1000, LR 0.000282
Train loss: 0.7646;  Loss pred: 0.7646; Loss self: 0.0000; time: 0.29s
Val loss: 0.4399 score: 0.8915 time: 0.17s
Test loss: 0.4270 score: 0.9062 time: 0.16s
Epoch 118/1000, LR 0.000282
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 0.31s
Val loss: 0.4355 score: 0.8915 time: 0.17s
Test loss: 0.4219 score: 0.9062 time: 0.20s
Epoch 119/1000, LR 0.000282
Train loss: 0.7576;  Loss pred: 0.7576; Loss self: 0.0000; time: 0.49s
Val loss: 0.4350 score: 0.8837 time: 0.26s
Test loss: 0.4207 score: 0.8828 time: 0.22s
Epoch 120/1000, LR 0.000281
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.49s
Val loss: 0.4329 score: 0.8837 time: 0.18s
Test loss: 0.4182 score: 0.8828 time: 0.16s
Epoch 121/1000, LR 0.000281
Train loss: 0.7529;  Loss pred: 0.7529; Loss self: 0.0000; time: 0.30s
Val loss: 0.4272 score: 0.8837 time: 0.17s
Test loss: 0.4119 score: 0.8828 time: 0.16s
Epoch 122/1000, LR 0.000281
Train loss: 0.7478;  Loss pred: 0.7478; Loss self: 0.0000; time: 0.29s
Val loss: 0.4212 score: 0.8837 time: 0.17s
Test loss: 0.4054 score: 0.8828 time: 0.17s
Epoch 123/1000, LR 0.000281
Train loss: 0.7421;  Loss pred: 0.7421; Loss self: 0.0000; time: 0.29s
Val loss: 0.4166 score: 0.8837 time: 0.20s
Test loss: 0.4002 score: 0.8828 time: 0.17s
Epoch 124/1000, LR 0.000281
Train loss: 0.7383;  Loss pred: 0.7383; Loss self: 0.0000; time: 0.47s
Val loss: 0.4149 score: 0.8837 time: 0.25s
Test loss: 0.3979 score: 0.8828 time: 0.22s
Epoch 125/1000, LR 0.000281
Train loss: 0.7343;  Loss pred: 0.7343; Loss self: 0.0000; time: 0.43s
Val loss: 0.4152 score: 0.8760 time: 0.17s
Test loss: 0.3978 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 126/1000, LR 0.000280
Train loss: 0.7356;  Loss pred: 0.7356; Loss self: 0.0000; time: 0.29s
Val loss: 0.4128 score: 0.8760 time: 0.18s
Test loss: 0.3947 score: 0.8828 time: 0.18s
Epoch 127/1000, LR 0.000280
Train loss: 0.7305;  Loss pred: 0.7305; Loss self: 0.0000; time: 0.30s
Val loss: 0.4070 score: 0.8760 time: 0.19s
Test loss: 0.3883 score: 0.8828 time: 0.18s
Epoch 128/1000, LR 0.000280
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 0.30s
Val loss: 0.3992 score: 0.8837 time: 0.18s
Test loss: 0.3800 score: 0.8828 time: 0.18s
Epoch 129/1000, LR 0.000280
Train loss: 0.7179;  Loss pred: 0.7179; Loss self: 0.0000; time: 0.30s
Val loss: 0.3904 score: 0.8837 time: 0.19s
Test loss: 0.3712 score: 0.9062 time: 0.18s
Epoch 130/1000, LR 0.000280
Train loss: 0.7121;  Loss pred: 0.7121; Loss self: 0.0000; time: 0.30s
Val loss: 0.3842 score: 0.8915 time: 0.18s
Test loss: 0.3655 score: 0.9062 time: 0.17s
Epoch 131/1000, LR 0.000280
Train loss: 0.7073;  Loss pred: 0.7073; Loss self: 0.0000; time: 0.32s
Val loss: 0.3813 score: 0.8992 time: 0.18s
Test loss: 0.3634 score: 0.8984 time: 0.20s
Epoch 132/1000, LR 0.000279
Train loss: 0.7055;  Loss pred: 0.7055; Loss self: 0.0000; time: 0.36s
Val loss: 0.3833 score: 0.8837 time: 0.18s
Test loss: 0.3664 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 133/1000, LR 0.000279
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.28s
Val loss: 0.3780 score: 0.8915 time: 0.17s
Test loss: 0.3596 score: 0.9062 time: 0.16s
Epoch 134/1000, LR 0.000279
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.28s
Val loss: 0.3739 score: 0.8915 time: 0.17s
Test loss: 0.3527 score: 0.9062 time: 0.16s
Epoch 135/1000, LR 0.000279
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.28s
Val loss: 0.3764 score: 0.8837 time: 0.18s
Test loss: 0.3533 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 136/1000, LR 0.000279
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.30s
Val loss: 0.3780 score: 0.8837 time: 0.17s
Test loss: 0.3541 score: 0.8906 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 137/1000, LR 0.000279
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.48s
Val loss: 0.3755 score: 0.8837 time: 0.26s
Test loss: 0.3510 score: 0.8906 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 138/1000, LR 0.000278
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.29s
Val loss: 0.3709 score: 0.8837 time: 0.17s
Test loss: 0.3460 score: 0.8906 time: 0.16s
Epoch 139/1000, LR 0.000278
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.28s
Val loss: 0.3663 score: 0.8837 time: 0.17s
Test loss: 0.3410 score: 0.8984 time: 0.16s
Epoch 140/1000, LR 0.000278
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.28s
Val loss: 0.3613 score: 0.8915 time: 0.17s
Test loss: 0.3359 score: 0.9062 time: 0.17s
Epoch 141/1000, LR 0.000278
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.28s
Val loss: 0.3578 score: 0.8915 time: 0.17s
Test loss: 0.3322 score: 0.9062 time: 0.19s
Epoch 142/1000, LR 0.000278
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.38s
Val loss: 0.3543 score: 0.8837 time: 0.19s
Test loss: 0.3289 score: 0.9062 time: 0.16s
Epoch 143/1000, LR 0.000277
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.29s
Val loss: 0.3522 score: 0.8915 time: 0.16s
Test loss: 0.3269 score: 0.9062 time: 0.16s
Epoch 144/1000, LR 0.000277
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.29s
Val loss: 0.3505 score: 0.8992 time: 0.17s
Test loss: 0.3250 score: 0.9062 time: 0.16s
Epoch 145/1000, LR 0.000277
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.28s
Val loss: 0.3489 score: 0.8992 time: 0.17s
Test loss: 0.3228 score: 0.9062 time: 0.16s
Epoch 146/1000, LR 0.000277
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.29s
Val loss: 0.3483 score: 0.8915 time: 0.26s
Test loss: 0.3212 score: 0.9062 time: 0.21s
Epoch 147/1000, LR 0.000277
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.47s
Val loss: 0.3524 score: 0.8837 time: 0.23s
Test loss: 0.3236 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 148/1000, LR 0.000277
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.29s
Val loss: 0.3640 score: 0.8837 time: 0.17s
Test loss: 0.3344 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 149/1000, LR 0.000276
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.28s
Val loss: 0.3738 score: 0.8760 time: 0.17s
Test loss: 0.3447 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 150/1000, LR 0.000276
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.29s
Val loss: 0.3771 score: 0.8760 time: 0.18s
Test loss: 0.3480 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 151/1000, LR 0.000276
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.29s
Val loss: 0.3746 score: 0.8760 time: 0.25s
Test loss: 0.3449 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 152/1000, LR 0.000276
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.47s
Val loss: 0.3645 score: 0.8837 time: 0.17s
Test loss: 0.3336 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 153/1000, LR 0.000276
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.29s
Val loss: 0.3488 score: 0.8837 time: 0.17s
Test loss: 0.3176 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 154/1000, LR 0.000275
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.29s
Val loss: 0.3406 score: 0.8837 time: 0.17s
Test loss: 0.3104 score: 0.9141 time: 0.16s
Epoch 155/1000, LR 0.000275
Train loss: 0.6412;  Loss pred: 0.6412; Loss self: 0.0000; time: 0.28s
Val loss: 0.3385 score: 0.8915 time: 0.17s
Test loss: 0.3088 score: 0.9062 time: 0.18s
Epoch 156/1000, LR 0.000275
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
Val loss: 0.3377 score: 0.8915 time: 0.25s
Test loss: 0.3082 score: 0.9141 time: 0.21s
Epoch 157/1000, LR 0.000275
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.38s
Val loss: 0.3368 score: 0.8915 time: 0.17s
Test loss: 0.3069 score: 0.9141 time: 0.16s
Epoch 158/1000, LR 0.000275
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.28s
Val loss: 0.3366 score: 0.8992 time: 0.17s
Test loss: 0.3054 score: 0.9141 time: 0.16s
Epoch 159/1000, LR 0.000274
Train loss: 0.6356;  Loss pred: 0.6356; Loss self: 0.0000; time: 0.28s
Val loss: 0.3426 score: 0.8915 time: 0.17s
Test loss: 0.3095 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 160/1000, LR 0.000274
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.28s
Val loss: 0.3513 score: 0.8837 time: 0.17s
Test loss: 0.3169 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 161/1000, LR 0.000274
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.29s
Val loss: 0.3577 score: 0.8837 time: 0.17s
Test loss: 0.3227 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 162/1000, LR 0.000274
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.29s
Val loss: 0.3596 score: 0.8837 time: 0.17s
Test loss: 0.3242 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 163/1000, LR 0.000273
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 0.35s
Val loss: 0.3553 score: 0.8837 time: 0.19s
Test loss: 0.3196 score: 0.8828 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.36s
Val loss: 0.3516 score: 0.8837 time: 0.17s
Test loss: 0.3157 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 165/1000, LR 0.000273
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.3532 score: 0.8837 time: 0.17s
Test loss: 0.3168 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 166/1000, LR 0.000273
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.29s
Val loss: 0.3516 score: 0.8837 time: 0.17s
Test loss: 0.3150 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 167/1000, LR 0.000273
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.28s
Val loss: 0.3425 score: 0.8837 time: 0.18s
Test loss: 0.3063 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 168/1000, LR 0.000272
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.32s
Val loss: 0.3320 score: 0.8992 time: 0.18s
Test loss: 0.2976 score: 0.9141 time: 0.17s
Epoch 169/1000, LR 0.000272
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.37s
Val loss: 0.3301 score: 0.8915 time: 0.17s
Test loss: 0.2967 score: 0.9141 time: 0.17s
Epoch 170/1000, LR 0.000272
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.29s
Val loss: 0.3296 score: 0.8915 time: 0.18s
Test loss: 0.2959 score: 0.9141 time: 0.16s
Epoch 171/1000, LR 0.000272
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.28s
Val loss: 0.3293 score: 0.8915 time: 0.18s
Test loss: 0.2952 score: 0.9219 time: 0.17s
Epoch 172/1000, LR 0.000271
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.29s
Val loss: 0.3295 score: 0.8992 time: 0.20s
Test loss: 0.2949 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000271
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.47s
Val loss: 0.3300 score: 0.8992 time: 0.17s
Test loss: 0.2950 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000271
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.40s
Val loss: 0.3308 score: 0.8992 time: 0.17s
Test loss: 0.2954 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 175/1000, LR 0.000271
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.28s
Val loss: 0.3285 score: 0.8992 time: 0.16s
Test loss: 0.2934 score: 0.9219 time: 0.17s
Epoch 176/1000, LR 0.000271
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.30s
Val loss: 0.3284 score: 0.8837 time: 0.17s
Test loss: 0.2955 score: 0.9141 time: 0.16s
Epoch 177/1000, LR 0.000270
Train loss: 0.6141;  Loss pred: 0.6141; Loss self: 0.0000; time: 0.42s
Val loss: 0.3323 score: 0.8760 time: 0.18s
Test loss: 0.3009 score: 0.9219 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 178/1000, LR 0.000270
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.30s
Val loss: 0.3328 score: 0.8760 time: 0.18s
Test loss: 0.3014 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 179/1000, LR 0.000270
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.31s
Val loss: 0.3288 score: 0.8760 time: 0.19s
Test loss: 0.2962 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 180/1000, LR 0.000270
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.30s
Val loss: 0.3259 score: 0.8992 time: 0.18s
Test loss: 0.2907 score: 0.9219 time: 0.17s
Epoch 181/1000, LR 0.000269
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 0.30s
Val loss: 0.3356 score: 0.8992 time: 0.19s
Test loss: 0.2980 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 182/1000, LR 0.000269
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.58s
Val loss: 0.3487 score: 0.8837 time: 0.26s
Test loss: 0.3104 score: 0.8906 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 183/1000, LR 0.000269
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.40s
Val loss: 0.3561 score: 0.8837 time: 0.18s
Test loss: 0.3178 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 184/1000, LR 0.000269
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.28s
Val loss: 0.3450 score: 0.8837 time: 0.17s
Test loss: 0.3062 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 185/1000, LR 0.000268
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.29s
Val loss: 0.3257 score: 0.8992 time: 0.18s
Test loss: 0.2884 score: 0.9219 time: 0.16s
Epoch 186/1000, LR 0.000268
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.30s
Val loss: 0.3253 score: 0.8837 time: 0.17s
Test loss: 0.2899 score: 0.9141 time: 0.17s
Epoch 187/1000, LR 0.000268
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.33s
Val loss: 0.3302 score: 0.8760 time: 0.22s
Test loss: 0.2962 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.48s
Val loss: 0.3297 score: 0.8760 time: 0.25s
Test loss: 0.2955 score: 0.9141 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.28s
Val loss: 0.3234 score: 0.8837 time: 0.17s
Test loss: 0.2876 score: 0.9219 time: 0.17s
Epoch 190/1000, LR 0.000267
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.30s
Val loss: 0.3207 score: 0.8915 time: 0.18s
Test loss: 0.2837 score: 0.9219 time: 0.16s
Epoch 191/1000, LR 0.000267
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.28s
Val loss: 0.3216 score: 0.8992 time: 0.17s
Test loss: 0.2838 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 192/1000, LR 0.000267
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 0.29s
Val loss: 0.3243 score: 0.8992 time: 0.18s
Test loss: 0.2860 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 193/1000, LR 0.000266
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.29s
Val loss: 0.3266 score: 0.8992 time: 0.17s
Test loss: 0.2880 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 194/1000, LR 0.000266
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.33s
Val loss: 0.3284 score: 0.9070 time: 0.17s
Test loss: 0.2895 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 195/1000, LR 0.000266
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.35s
Val loss: 0.3292 score: 0.9070 time: 0.18s
Test loss: 0.2901 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 196/1000, LR 0.000266
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.31s
Val loss: 0.3292 score: 0.9070 time: 0.19s
Test loss: 0.2897 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 197/1000, LR 0.000265
Train loss: 0.5905;  Loss pred: 0.5905; Loss self: 0.0000; time: 0.30s
Val loss: 0.3285 score: 0.8992 time: 0.18s
Test loss: 0.2887 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 198/1000, LR 0.000265
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.31s
Val loss: 0.3279 score: 0.8992 time: 0.19s
Test loss: 0.2877 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 199/1000, LR 0.000265
Train loss: 0.5890;  Loss pred: 0.5890; Loss self: 0.0000; time: 0.33s
Val loss: 0.3269 score: 0.8992 time: 0.24s
Test loss: 0.2864 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 200/1000, LR 0.000265
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.38s
Val loss: 0.3238 score: 0.9070 time: 0.18s
Test loss: 0.2834 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 201/1000, LR 0.000264
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.31s
Val loss: 0.3207 score: 0.8992 time: 0.18s
Test loss: 0.2809 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 202/1000, LR 0.000264
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 0.30s
Val loss: 0.3208 score: 0.8837 time: 0.19s
Test loss: 0.2812 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 203/1000, LR 0.000264
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 0.31s
Val loss: 0.3207 score: 0.8837 time: 0.19s
Test loss: 0.2808 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 204/1000, LR 0.000264
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.33s
Val loss: 0.3215 score: 0.8992 time: 0.26s
Test loss: 0.2806 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 205/1000, LR 0.000263
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.38s
Val loss: 0.3229 score: 0.8992 time: 0.18s
Test loss: 0.2816 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.30s
Val loss: 0.3222 score: 0.8992 time: 0.18s
Test loss: 0.2809 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 207/1000, LR 0.000263
Train loss: 0.5796;  Loss pred: 0.5796; Loss self: 0.0000; time: 0.32s
Val loss: 0.3220 score: 0.8992 time: 0.19s
Test loss: 0.2806 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 208/1000, LR 0.000263
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.29s
Val loss: 0.3246 score: 0.9070 time: 0.17s
Test loss: 0.2829 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 209/1000, LR 0.000262
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.30s
Val loss: 0.3364 score: 0.9070 time: 0.19s
Test loss: 0.2948 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.29s
Val loss: 0.3494 score: 0.8915 time: 0.20s
Test loss: 0.3079 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 189,   Train_Loss: 0.5992,   Val_Loss: 0.3207,   Val_Precision: 0.9474,   Val_Recall: 0.8308,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.3207,   Test_Precision: 0.9655,   Test_Recall: 0.8750,   Test_accuracy: 0.9180,   Test_Score: 0.9219,   Test_loss: 0.2837


[0.19555586413480341, 0.1909084590151906, 0.24847080814652145, 0.19288980099372566, 0.19383490993641317, 0.1862070350907743, 0.2076902671251446, 0.21620360994711518, 0.19409007905051112, 0.18385196197777987, 0.18302034912630916, 0.19677526992745697, 0.18619965598918498, 0.18455329700373113, 0.18718935106880963, 0.18790399306453764, 0.2625683229416609, 0.18833425594493747, 0.17697427910752594, 0.17435620818287134, 0.1696395999751985, 0.23384293587878346, 0.17312781908549368, 0.17624261300079525, 0.18764558108523488, 0.17418877803720534, 0.17387836519628763, 0.17295143590308726, 0.25845810817554593, 0.2712989260908216, 0.17145546386018395, 0.17271277401596308, 0.1809214809909463, 0.1887400469277054, 0.34398250887170434, 0.17259777197614312, 0.17296866397373378, 0.17910086107440293, 0.22473544999957085, 0.24988236906938255, 0.25183648709207773, 0.18846895406022668, 0.18484524707309902, 0.17317458800971508, 0.18546287692151964, 0.17097281105816364, 0.17147964495234191, 0.2653619688935578, 0.19469138490967453, 0.1879120608791709, 0.18262865906581283, 0.1834622509777546, 0.1796323829330504, 0.2358199800364673, 0.17932576406747103, 0.1771650610025972, 0.1704649010207504, 0.2297176499851048, 0.25605336483567953, 0.18071211082860827, 0.17375806183554232, 0.17315553897060454, 0.17537340498529375, 0.1853686268441379, 0.1726419001352042, 0.1711529151070863, 0.17228090204298496, 0.23269312200136483, 0.17818828020244837, 0.17891321913339198, 0.1764405129943043, 0.1735034789890051, 0.17418907186947763, 0.17243342706933618, 0.17726940009742975, 0.1764804709237069, 0.17298475396819413, 0.17232893803156912, 0.22620482300408185, 0.17139372415840626, 0.17405310715548694, 0.20454979198984802, 0.25184304895810783, 0.21801338414661586, 0.18168765702284873, 0.1784256030805409, 0.23621190804988146, 0.24977327208034694, 0.25211025099270046, 0.24976587295532227, 0.17187732993625104, 0.17342588189058006, 0.1755876347888261, 0.17322244797833264, 0.17410647892393172, 0.17895297589711845, 0.17665552720427513, 0.17939129704609513, 0.17906447779387236, 0.17742380104027689, 0.2564876559190452, 0.17449749214574695, 0.20589374983683228, 0.17968621896579862, 0.18401836999692023, 0.18974814098328352, 0.1817194470204413, 0.17464973195455968, 0.17724909400567412, 0.18932599807158113, 0.1707296841777861, 0.253844051156193, 0.22669502603821456, 0.17422130005434155, 0.17190482490696013, 0.17820006608963013, 0.17567916098050773, 0.25804635998792946, 0.278405831893906, 0.19036092795431614, 0.19149508699774742, 0.20319574885070324, 0.23995877616107464, 0.17401337204501033, 0.17154936911538243, 0.1756247030571103, 0.18875375692732632, 0.186109553091228, 0.18294598790816963, 0.18837940110825002, 0.26424073288217187, 0.20493202400393784, 0.1738430808763951, 0.17454204219393432, 0.1743202048819512, 0.1907002991065383, 0.1804999599698931, 0.18132023210637271, 0.1809795149601996, 0.2621866229455918, 0.18945994600653648, 0.1813092811498791, 0.19337132899090648, 0.17371272598393261, 0.18385328794829547, 0.18678574287332594, 0.1886805489193648, 0.1770104030147195, 0.18075660895556211, 0.18257841607555747, 0.2508895299397409, 0.1836648068856448, 0.17678741086274385, 0.17725213314406574, 0.18034103303216398, 0.2560262840706855, 0.1899039449635893, 0.1839322498999536, 0.19472340098582208, 0.273139791097492, 0.2559899219777435, 0.1781013309955597, 0.18627611687406898, 0.1799804049078375, 0.1862126470077783, 0.19052876601926982, 0.17749475594609976, 0.17712987796403468, 0.19497802387923002, 0.17622835002839565, 0.19319331203587353, 0.20098470500670373, 0.18520730920135975, 0.18840247299522161, 0.19258209085091949, 0.19648555689491332, 0.2565794079564512, 0.17665872490033507, 0.18164591095410287, 0.18217366212047637, 0.19844938395544887, 0.19764221902005374, 0.21437810990028083, 0.18468522490002215, 0.1958201618399471, 0.17259489907883108, 0.19099656702019274, 0.239906009985134, 0.1978597640991211, 0.19524818495847285, 0.17264269897714257, 0.18827829393558204, 0.1805127840489149, 0.16895666299387813, 0.1722031719982624, 0.17321598599664867, 0.17023229389451444, 0.20042550703510642, 0.26235083397477865, 0.18480457714758813, 0.18427201686426997, 0.18673045886680484, 0.18457944807596505, 0.2157289618626237, 0.18597575393505394, 0.17128843907266855, 0.16931545012630522, 0.1850365619175136, 0.1876047309488058, 0.18159466097131371, 0.21657354710623622, 0.2710528769530356, 0.17199363792315125, 0.17998598120175302, 0.2659593350253999, 0.1818254510872066, 0.18663285998627543, 0.18303545005619526, 0.18370212893933058, 0.18958029709756374, 0.1972081868443638, 0.2602203330025077, 0.24967554188333452, 0.17483145697042346, 0.19133749487809837, 0.2750267190858722, 0.19501711311750114, 0.18155672191642225, 0.1835993528366089, 0.18357619806192815, 0.18515150994062424, 0.24038277007639408, 0.18165262788534164, 0.18674142402596772, 0.2604908391367644, 0.26028697611764073, 0.18762688501738012, 0.18348342808894813, 0.18016302306205034, 0.17901434982195497, 0.17949669994413853, 0.2621333079878241, 0.20083365100435913, 0.19529818813316524, 0.18586750701069832, 0.1855183730367571, 0.18650009599514306, 0.20328830601647496, 0.18371170992031693, 0.19206703500822186, 0.19181288802064955, 0.26692610303871334, 0.2602372639812529, 0.19038053113035858, 0.1949137169867754, 0.1826921950560063, 0.19367050589062274, 0.267529658973217, 0.1825879260431975, 0.18187707592733204, 0.18225802411325276, 0.18431074009276927, 0.18882760987617075, 0.255717916181311, 0.26169913494959474, 0.18319300306029618, 0.1839629178866744, 0.1862410621251911, 0.2475755859632045, 0.24969659303314984, 0.26085420907475054, 0.18384515005163848, 0.187513334909454, 0.18795637786388397, 0.17387826996855438, 0.18367151892744005, 0.23427330888807774, 0.177532423986122, 0.1737637750338763, 0.17468983214348555, 0.176743492949754, 0.24750934494659305, 0.18413024302572012, 0.1810240182094276, 0.18184911413118243, 0.179198777070269, 0.24086894583888352, 0.1706286990083754, 0.1759111639112234, 0.17355251708067954, 0.17075701104477048, 0.18457963201217353, 0.17336174100637436, 0.1763790170662105, 0.18198595009744167, 0.17965830210596323, 0.2770573340822011, 0.18070919415913522, 0.17193050403147936, 0.1717212088406086, 0.19188775401562452, 0.2537652561441064, 0.18396667600609362, 0.17261794116348028, 0.1778134210035205, 0.17602461390197277, 0.2582412438932806, 0.2151003701146692, 0.17681256704963744, 0.25903963716700673, 0.233127657789737, 0.17945770593360066, 0.17025071009993553, 0.25049082189798355, 0.2517603449523449, 0.17283408297225833, 0.1731192150618881, 0.17293639411218464, 0.17674023192375898, 0.170663442928344, 0.17019757395610213, 0.18144707195460796, 0.17233932297676802, 0.17119883396662772, 0.17349056899547577, 0.17230190406553447, 0.1811121569480747, 0.1794330149423331, 0.17293489002622664, 0.1763404170051217, 0.17091099289245903, 0.1738349311053753, 0.17782090906985104, 0.17803430114872754, 0.17494606599211693, 0.17082742182537913, 0.17183471098542213, 0.16980582592077553, 0.21162347705103457, 0.18009215500205755, 0.17245603702031076, 0.1716078429017216, 0.16669521294534206, 0.17254407401196659, 0.21785010281018913, 0.17379093682393432, 0.17083906894549727, 0.18758026789873838, 0.17773015494458377, 0.16847046883776784, 0.1950112811755389, 0.2624988742172718, 0.1731771870981902, 0.17807900113984942, 0.16865618899464607, 0.1742205920163542, 0.2579721938818693, 0.1696036101784557, 0.18139837286435068, 0.16921271616593003, 0.2277770999353379, 0.17747960495762527, 0.1814232049509883, 0.18174015101976693, 0.18583002290688455, 0.18348900508135557, 0.1683588761370629, 0.1823289778549224, 0.178529858822003, 0.17496909410692751, 0.22392134997062385, 0.18231622502207756, 0.16933309589512646, 0.1688178579788655, 0.1754051682073623, 0.29056417802348733, 0.17902651499025524, 0.17589312908239663, 0.17652066983282566, 0.1749450119677931, 0.1723243691958487, 0.17681220499798656, 0.17048252979293466, 0.17372915102168918, 0.16892183991149068, 0.17593690613284707, 0.25983148301020265, 0.26652122591622174, 0.17564081703312695, 0.17611563089303672, 0.1760838849004358, 0.179349445970729, 0.18120863707736135, 0.1744786777999252, 0.16843671398237348, 0.17422605492174625, 0.17369201011024415, 0.17858880502171814, 0.17226895107887685, 0.16992806200869381, 0.17040388192981482, 0.1702922279946506, 0.1783898959401995, 0.17083196504972875, 0.1739247520454228, 0.18832896812818944, 0.21232229005545378, 0.19415981485508382, 0.22203593491576612, 0.17682105908170342, 0.17103215702809393, 0.17031127982772887, 0.17255298793315887, 0.17424389882944524, 0.17519814800471067, 0.2511157940607518, 0.17651173588819802, 0.18854690995067358, 0.18329713796265423, 0.1759414398111403, 0.24270398193039, 0.1864347408991307, 0.1752130889799446, 0.18422953807748854, 0.1808561149518937, 0.17863368499092758, 0.17094187601469457, 0.17235037498176098, 0.17753868107683957, 0.16857546288520098, 0.17283138702623546, 0.16928646294400096, 0.1673447450157255, 0.17721124598756433, 0.17386369011364877, 0.18724343902431428, 0.17772852908819914, 0.1752837668173015, 0.1749375849030912, 0.17599552194587886, 0.24752387683838606, 0.2574840681627393, 0.17657584697008133, 0.17100043804384768, 0.1712948251515627, 0.1709085840266198, 0.16737215500324965, 0.17100739805027843, 0.17683722195215523, 0.17199050402268767, 0.17192210699431598, 0.16986796213313937, 0.16938819200731814, 0.16801352612674236, 0.24261692678555846, 0.17756873299367726, 0.16815252299420536, 0.1976566449739039, 0.26431840495206416, 0.2625128929503262, 0.19222759711556137, 0.18548188684508204, 0.18197120097465813, 0.18920605606399477, 0.1847350860480219, 0.18384804809466004, 0.1845924430526793, 0.19338235701434314, 0.18691285396926105, 0.17094813496805727, 0.25390254403464496, 0.1672968950588256, 0.16847549891099334, 0.1694149449467659, 0.16977505898103118, 0.2598656159825623, 0.17160307709127665, 0.19352124189026654, 0.17041722894646227, 0.17337822308763862, 0.16922708111815155, 0.24868702609091997, 0.18132934998720884, 0.17315312009304762, 0.2537576830945909, 0.17342131212353706, 0.17203642497770488, 0.17138129705563188, 0.1707505809608847, 0.1682572381105274, 0.17553282785229385, 0.1800102370325476, 0.17287137196399271, 0.16849935404025018, 0.17157818004488945, 0.17781222495250404, 0.18380661914125085, 0.18159721698611975, 0.17829674994572997, 0.18202725402079523, 0.1788796738255769, 0.17569194897077978, 0.17648683907464147, 0.1673181070946157, 0.17171638016588986, 0.18421436799690127, 0.17957426700741053, 0.1830901107750833, 0.18613654794171453, 0.18657702091149986, 0.16856374498456717, 0.17092510894872248, 0.17732354393228889, 0.16954101994633675, 0.17260724399238825, 0.17040471197105944, 0.17631987389177084, 0.18799089291132987, 0.19812754588201642, 0.190679294988513, 0.19247345393523574, 0.2409065079409629, 0.19053888600319624, 0.18502894812263548, 0.1882072240114212, 0.26240520807914436, 0.1850721319206059, 0.16999676986597478, 0.18315886985510588, 0.18275967799127102, 0.1749974440317601, 0.1667003978509456, 0.166149492142722, 0.16612145397812128, 0.16582569386810064, 0.2220472409389913, 0.1814057701267302, 0.17197162890806794, 0.17091248789802194, 0.17887057992629707, 0.19169186078943312, 0.19431672897189856, 0.17315971781499684, 0.1845893410500139, 0.1839692909270525, 0.17986791813746095, 0.1838355700019747, 0.18448369204998016, 0.20177395292557776, 0.17738769901916385, 0.17923819506540895, 0.1788706339430064, 0.1807686190586537, 0.1873614110518247, 0.18427940807305276, 0.19619011203758419, 0.17816470214165747, 0.18851320096291602, 0.17999364016577601, 0.21973813814111054, 0.18185846600681543, 0.18134925118647516, 0.18282375996932387, 0.1804453399963677, 0.18510437197983265, 0.18472824222408235, 0.18066494492813945, 0.18349466612562537, 0.16973366914317012, 0.22287699999287724, 0.1717456819023937, 0.17174579692073166, 0.1721538540441543, 0.18096020608209074, 0.18096794001758099, 0.17761753592640162, 0.1861488271970302, 0.18325159302912652, 0.18398239999078214, 0.18562970194034278, 0.19322896795347333, 0.2346468640025705, 0.23453737399540842, 0.23249738407321274, 0.18403235799632967, 0.18356838612817228, 0.18654282204806805, 0.18635739106684923, 0.189770698081702, 0.18861317378468812, 0.22409958904609084, 0.1889005450066179, 0.1849889699369669, 0.1816963499877602, 0.1838385360315442, 0.18449698598124087, 0.18166032899171114, 0.18842428596690297, 0.1941243289038539, 0.19624929316341877, 0.17532986705191433, 0.17150486400350928, 0.1798176469746977, 0.1718176258727908, 0.17537506786175072, 0.21352389501407743, 0.19957086304202676, 0.16997027886100113, 0.17105223191902041, 0.1698907008394599, 0.18327428004704416, 0.22839198517613113, 0.19471944915130734, 0.18506264500319958, 0.17671879893168807, 0.16788267181254923, 0.2037138279993087, 0.17620553192682564, 0.181044738041237, 0.18360555008985102, 0.18871729006059468, 0.1829112689010799, 0.2220134800300002, 0.17874244693666697, 0.17783914785832167, 0.18791380198672414, 0.1863215819466859, 0.17814287706278265, 0.20092672598548234, 0.1691566628869623, 0.17025947105139494, 0.18085044296458364, 0.18002746999263763, 0.18189892312511802, 0.17670338903553784, 0.1710983959492296, 0.16687872493639588, 0.17394258198328316, 0.16765146306715906, 0.17120651388540864, 0.18129280302673578, 0.22819048608653247, 0.21778789488598704, 0.17517623701132834, 0.17989630298689008, 0.17468497902154922, 0.1683345320634544, 0.20212923013605177, 0.21988601610064507, 0.1689626609440893, 0.16884712921455503, 0.17149344296194613, 0.1786858458071947, 0.22000592318363488, 0.17229658598080277, 0.17990892892703414, 0.18510417314246297, 0.18225307995453477, 0.1801110878586769, 0.1785019151866436, 0.2042526458390057, 0.16519753402099013, 0.16511571709997952, 0.1686116571072489, 0.16373702115379274, 0.19213099381886423, 0.22522998601198196, 0.1688637959305197, 0.16903204610571265, 0.16959236306138337, 0.19501113891601562, 0.16774281300604343, 0.16328306985087693, 0.1676060790196061, 0.1665363679639995, 0.2135991700924933, 0.17130151204764843, 0.1759681508410722, 0.1670765441376716, 0.17182830697856843, 0.21770573616959155, 0.17026171600446105, 0.1682556620799005, 0.16555226687341928, 0.1795877800323069, 0.2156533000525087, 0.16851080092601478, 0.16918560513295233, 0.16535176802426577, 0.17255017114803195, 0.16997619182802737, 0.1673321989364922, 0.18056924897246063, 0.1767573170363903, 0.178675846895203, 0.17044449388049543, 0.17776351911015809, 0.1781016760505736, 0.1723497051279992, 0.16869953298009932, 0.17466354882344604, 0.18063764600083232, 0.16966456011869013, 0.1767952071968466, 0.17241203505545855, 0.16257464489899576, 0.23322977498173714, 0.17992225708439946, 0.17749709589406848, 0.17716116807423532, 0.18079731613397598, 0.22818358801305294, 0.16485005198046565, 0.16778529691509902, 0.1677681610453874, 0.17300594202242792, 0.22184830205515027, 0.2301273460034281, 0.17069204105064273, 0.16747161699458957, 0.16710947290994227, 0.16881925286725163, 0.16729191900230944, 0.17567442101426423, 0.18586639710702002, 0.17790850903838873, 0.17912278603762388, 0.18081780709326267, 0.20481054205447435, 0.1814613218884915, 0.17892857710830867, 0.18326981808058918, 0.18180107488296926, 0.22826656489633024, 0.18663049302995205, 0.18940957891754806, 0.1884046031627804, 0.17064065020531416, 0.18552513583563268, 0.1790951290167868]
[0.0015159369312775458, 0.0014799105350014774, 0.0019261302957094687, 0.00149526977514516, 0.0015025962010574664, 0.001443465388300576, 0.001610002070737555, 0.0016759969763342263, 0.0015045742562055126, 0.0014252090075796888, 0.0014187623963279779, 0.0015253896893601315, 0.0014434081859626743, 0.001430645713207218, 0.0014510802408434856, 0.0014566201012754857, 0.0020354133561369062, 0.0014599554724413757, 0.0013718936364924491, 0.0013515985130455144, 0.0013150356587224691, 0.001812735937044833, 0.0013420761169418115, 0.001366221806207715, 0.0014546169076374797, 0.0013503006049395763, 0.001347894303847191, 0.0013407088054502888, 0.0020035512261670227, 0.002103092450316447, 0.0013291121229471623, 0.0013388587133020395, 0.00140249210070501, 0.0014631011389744605, 0.00266653107652484, 0.0013379672246212645, 0.0013408423563855331, 0.0013883787680186274, 0.0017421352713145027, 0.0019370726284448259, 0.001952220830171145, 0.0014609996438777262, 0.0014329088920395272, 0.0013424386667419774, 0.0014376967203218577, 0.0013253706283578577, 0.0013292995732739684, 0.0020570695263066496, 0.0015092355419354614, 0.001456682642474193, 0.0014157260392698668, 0.001422187992075617, 0.0013924990925042666, 0.0018280618607478087, 0.001390122202073419, 0.0013733725659116063, 0.0013214333412461272, 0.0017807569766287194, 0.001984909804927748, 0.0014008690761907617, 0.0013469617196553668, 0.0013422909997721283, 0.0013594837595759205, 0.0014369660995669606, 0.001338309303373676, 0.0013267667837758628, 0.001335510868550271, 0.0018038226511733707, 0.0013813044976933983, 0.00138692417932862, 0.0013677559146845295, 0.0013449882092170936, 0.001350302882709129, 0.0013366932330956293, 0.0013741813961041067, 0.0013680656660752472, 0.0013409670850247607, 0.0013358832405547993, 0.0017535257597215647, 0.0013286335206078004, 0.0013492488926781932, 0.0015856573022468839, 0.001952271697349673, 0.0016900262336946967, 0.001408431449789525, 0.0013831442099266737, 0.001831100062402182, 0.0019362269153515267, 0.0019543430309511664, 0.001936169557793196, 0.0013323824026065972, 0.001344386681322326, 0.0013611444557273342, 0.0013428096742506407, 0.0013496626273173, 0.001387232371295492, 0.0013694226915060088, 0.0013906302096596522, 0.0013880967270842818, 0.0013753783026378052, 0.0019882764024732186, 0.0013526937375639297, 0.001596075580130483, 0.0013929164260914621, 0.0014264989922241878, 0.0014709158215758412, 0.00140867788387939, 0.0013538738911206176, 0.0013740239845401094, 0.0014676433959037298, 0.001323485923858807, 0.00196778334229607, 0.0017573257832419734, 0.001350552713599547, 0.0013325955419144196, 0.0013813958611599234, 0.0013618539610892072, 0.002000359379751391, 0.0021581847433636125, 0.0014756661081729934, 0.001484458038742228, 0.001575160843803901, 0.0018601455516362375, 0.0013489408685659716, 0.0013298400706618792, 0.001361431806644266, 0.0014632074180412893, 0.0014427097138854884, 0.0014181859527765087, 0.0014603054349476745, 0.002048377774280402, 0.0015886203411157973, 0.001347620781987559, 0.0013530390867746848, 0.0013513194176895443, 0.0014782968922987464, 0.0013992244958906442, 0.0014055831946230442, 0.0014029419764356559, 0.0020324544414386963, 0.0014686817519886548, 0.0014054983034874349, 0.0014990025503171045, 0.001346610278945214, 0.0014252192864208952, 0.0014479514951420615, 0.001462639914103603, 0.0013721736667807712, 0.0014012140229113343, 0.0014153365587252517, 0.001944880077052255, 0.0014237581929119753, 0.0013704450454476269, 0.0013740475437524476, 0.001397992504125302, 0.001984699876516942, 0.0014721236043689094, 0.0014258313945732838, 0.0015094837285722642, 0.002117362721685984, 0.001984417999827469, 0.001380630472833796, 0.0014440009060005348, 0.0013951969372700582, 0.001443508891533165, 0.0014769671784439521, 0.00137592834066744, 0.0013730998291785633, 0.0015114575494513955, 0.0013661112405301988, 0.0014976225739215003, 0.001558020969044215, 0.0014357155752043392, 0.0014604842867846637, 0.0014928844252009262, 0.0015231438518985528, 0.001988987658577141, 0.0013694474798475588, 0.0014081078368535106, 0.0014121989311664835, 0.0015383673174840997, 0.0015321102249616569, 0.0016618458131804716, 0.001431668410077691, 0.0015179857506972645, 0.0013379449540994656, 0.0014805935427921917, 0.0018597365115126667, 0.0015337966209234193, 0.0015135518213835104, 0.0013383154959468416, 0.0014595216584153646, 0.0013993239073559296, 0.0013097415735959545, 0.0013349083100640496, 0.0013427595813693695, 0.0013196301852287941, 0.0015536861010473366, 0.0020337273951533228, 0.0014325936212991328, 0.0014284652470098447, 0.0014475229369519756, 0.0014308484346974035, 0.0016723175338187882, 0.0014416725111244492, 0.0013278173571524693, 0.0013125228691961645, 0.0014343919528489427, 0.0014543002399132234, 0.0014077105501652226, 0.0016788647062498932, 0.002101185092659191, 0.0013332840149081493, 0.0013952401643546747, 0.002061700271514728, 0.001409499620831059, 0.0014467663564827554, 0.0014188794577999631, 0.0014240475111576014, 0.0014696147061826646, 0.0015287456344524326, 0.0020172118837403696, 0.0019354693169250738, 0.0013552826121738253, 0.0014832363944038633, 0.0021319900704331175, 0.0015117605668023345, 0.0014074164489645134, 0.0014232507971830147, 0.0014230713028056446, 0.0014352830227955368, 0.0018634323261735976, 0.0014081599060879197, 0.0014476079381857963, 0.0020193088305175536, 0.0020177284970359748, 0.0014544719768789156, 0.00142235215572828, 0.0013966125818763591, 0.0013877081381546896, 0.0013914472863886707, 0.002032041147192435, 0.0015568500077857296, 0.0015139394428927539, 0.001440833387679832, 0.001438126922765559, 0.001445737178256923, 0.0015758783412129843, 0.0014241217823280383, 0.0014888917442497818, 0.0014869216125631749, 0.002069194597199328, 0.0020173431316376196, 0.0014758180707779734, 0.0015109590464091117, 0.0014162185663256304, 0.0015013217510900987, 0.002073873325373775, 0.0014154102794046318, 0.0014098998133901708, 0.0014128529001027346, 0.0014287654270757307, 0.0014637799215207036, 0.0019823094277621007, 0.002028675464725541, 0.0014201007989170246, 0.0014260691309044527, 0.0014437291637611712, 0.0019191905888620504, 0.0019356325041329444, 0.0020221256517422522, 0.001425156201950686, 0.0014535917434841394, 0.0014570261849913486, 0.0013478935656477083, 0.0014238102242437212, 0.0018160721619230832, 0.00137622034097769, 0.001347006008014545, 0.0013541847452983376, 0.001370104596509721, 0.0019186770926092483, 0.0014273662250055823, 0.0014032869628637797, 0.0014096830552804839, 0.0013891378067462713, 0.0018672011305339807, 0.0013227030930881814, 0.0013636524334203364, 0.001345368349462632, 0.0013236977600369804, 0.0014308498605594847, 0.0013438894651656927, 0.0013672792020636474, 0.0014107437992049742, 0.0013927000163252963, 0.0021477312719550474, 0.0014008464663498854, 0.0013327946048951888, 0.0013311721615551055, 0.0014875019691133683, 0.0019671725282488866, 0.001426098263613129, 0.0013381235749106999, 0.0013783986124303914, 0.0013645318907129671, 0.0020018701076998497, 0.001667444729571079, 0.0013706400546483523, 0.0020080592028450133, 0.0018071911456568758, 0.0013911450072372144, 0.0013197729465111282, 0.001941789317038632, 0.0019516305810259294, 0.001339799092808204, 0.0013420094190844038, 0.001340592202420036, 0.0013700793172384416, 0.0013229724258011163, 0.0013193610384193964, 0.001406566449260527, 0.0013359637440059535, 0.0013271227439273466, 0.0013448881317478741, 0.0013356736749266238, 0.001403970208899804, 0.0013909536042041326, 0.0013405805428389661, 0.0013669799767838892, 0.0013248914177710002, 0.0013475576054680254, 0.0013784566594562096, 0.001380110861618043, 0.0013561710542024567, 0.0013242435800416986, 0.0013320520231428074, 0.0013163242319439964, 0.0016404920701630586, 0.0013960632170702135, 0.0013368685040334167, 0.0013302933558272992, 0.0012922109530646671, 0.0013375509613330744, 0.0016887604869006909, 0.0013472165645266225, 0.0013243338677945526, 0.001454110603866189, 0.0013777531391053006, 0.0013059726266493631, 0.001511715357949914, 0.0020348749939323395, 0.0013424588147146527, 0.0013804573731771273, 0.0013074123177879541, 0.0013505472249329784, 0.0019997844486966615, 0.0013147566680500444, 0.001406188936932951, 0.0013117264819064343, 0.0017657139529871156, 0.001375810891144382, 0.0014063814337285914, 0.0014088383799981932, 0.0014405428132316632, 0.0014223953882275626, 0.001305107566953976, 0.0014134029291079256, 0.0013839523939690155, 0.0013563495667203683, 0.001735824418376929, 0.0014133040699385857, 0.0013126596581017555, 0.0013086655657276395, 0.0013597299861035836, 0.0022524354885541656, 0.0013878024417849242, 0.0013635126285457104, 0.0013683772855257804, 0.0013561628834712644, 0.0013358478232236334, 0.0013706372480464075, 0.0013215699983948424, 0.001346737604819296, 0.001309471627220858, 0.0013638519855259464, 0.002014197542714749, 0.002066056014854432, 0.0013615567211870305, 0.0013652374487832305, 0.0013649913558173318, 0.0013903057827188293, 0.00140471811687877, 0.0013525478899219009, 0.0013057109611036705, 0.0013505895730367926, 0.0013464496907770866, 0.001384409341253629, 0.00133541822541765, 0.0013172717985170063, 0.0013209603250373242, 0.0013200947906562063, 0.0013828674103891433, 0.0013242787988351067, 0.0013482538918249828, 0.0014599144816138717, 0.0016459092252360758, 0.001505114843837859, 0.0017212087977966366, 0.0013707058843542902, 0.0013258306746363872, 0.0013202424792847199, 0.0013376200614973557, 0.0013507278979026764, 0.0013581251783310905, 0.0019466340624864481, 0.00136830803014107, 0.0014616039531059968, 0.0014209080462221258, 0.001363887130318917, 0.001881426216514651, 0.0014452305496056644, 0.0013582409998445317, 0.0014281359540890585, 0.0014019853872239822, 0.0013847572479916866, 0.0013251308218193377, 0.0013360494184632634, 0.001376268845556896, 0.001306786533993806, 0.0013397781940018253, 0.0013122981623565966, 0.0012972460853932209, 0.0013737305890508863, 0.001347780543516657, 0.0014514995273202658, 0.0013777405355674351, 0.0013587888900566008, 0.0013561053093262884, 0.0013643063716734796, 0.0019187897429332253, 0.0019960005283933277, 0.0013688050152719483, 0.0013255847910375788, 0.0013278668616400209, 0.0013248727443924014, 0.0012974585659166638, 0.0013256387445758017, 0.001370831177923684, 0.001333259721106106, 0.001332729511583845, 0.0013168059080088323, 0.0013130867597466523, 0.001302430435091026, 0.0018807513704306859, 0.001376501806152537, 0.0013035079301876385, 0.0015322220540612704, 0.0020489798833493345, 0.0020349836662815987, 0.0014901364117485378, 0.0014378440840704034, 0.0014106294649198305, 0.0014667136128991842, 0.001432054930604821, 0.0014251786674004654, 0.0014309491709510024, 0.001499088038870877, 0.001448936852474892, 0.0013251793408376534, 0.001968236775462364, 0.0012968751554947723, 0.0013060116194650647, 0.0013132941468741543, 0.0013160857285351255, 0.0020144621393997078, 0.001330256411560284, 0.0015001646658160197, 0.0013210637902826533, 0.0013440172332375086, 0.0013118378381252057, 0.0019278064038055812, 0.001405653875869836, 0.001342272248783315, 0.001967113822438689, 0.0013443512567716052, 0.0013336156975015882, 0.0013285371864777665, 0.0013236479144254627, 0.001304319675275406, 0.001360719595754216, 0.0013954281940507566, 0.0013400881547596334, 0.0013061965429476759, 0.0013300634112006933, 0.0013783893407170857, 0.0014248575127228749, 0.0014077303642334865, 0.0013821453484165113, 0.0014110639846573274, 0.0013866641381827666, 0.0013619530927967425, 0.001368115031586368, 0.0012970395898807417, 0.0013311347299681385, 0.0014280183565651262, 0.0013920485814527949, 0.0014193031843029713, 0.0014429189762923606, 0.0014463334954379833, 0.0013066956975547842, 0.0013250008445637401, 0.0013746011157541773, 0.0013142714724522228, 0.0013380406511037848, 0.001320966759465577, 0.0013668207278431847, 0.0014572937434986812, 0.0015358724486978018, 0.0014781340696783953, 0.0014920422785677189, 0.0018674923096198676, 0.0014770456279317539, 0.0014343329311832208, 0.001458970728770707, 0.0020341488998383284, 0.0014346676893070224, 0.001317804417565696, 0.0014198362004271773, 0.0014167416898548141, 0.0013671675314981258, 0.0013023468582105124, 0.0012980429073650157, 0.0012978238592040725, 0.0012955132333445363, 0.0017347440698358696, 0.0014172325791150797, 0.0013435283508442808, 0.0013352538117032964, 0.0013974264056741958, 0.0014975926624174463, 0.0015180994450929575, 0.0013528102954296628, 0.0014421042269532336, 0.0014372600853675976, 0.0014052181104489136, 0.0014362153906404274, 0.00144127884414047, 0.0015763590072310762, 0.0013858413985872176, 0.0014002983989485074, 0.0013974268276797375, 0.0014122548363957321, 0.0014637610238423804, 0.0014396828755707247, 0.0015327352502936265, 0.001391911735481699, 0.0014727593825227814, 0.0014062003137951251, 0.001716704204227426, 0.0014207692656782456, 0.0014167910248943372, 0.0014283106247603428, 0.0014097292187216226, 0.0014461279060924426, 0.0014431893923756434, 0.0014114448822510894, 0.0014335520791064482, 0.0013260442901810165, 0.0017412265624443535, 0.0013417631398624508, 0.0013417640384432161, 0.0013449519847199554, 0.0014137516100163339, 0.0014138120313873515, 0.0013876369994250126, 0.0014542877124767983, 0.001431653070540051, 0.0014373624999279855, 0.001450232046408928, 0.0015096013121365104, 0.0018331786250200821, 0.0018323232343391282, 0.0018163858130719746, 0.0014377527968463255, 0.001434128016626346, 0.0014573657972505316, 0.0014559171177097596, 0.0014825835787632968, 0.001473540420192876, 0.0017507780394225847, 0.0014757855078642024, 0.0014452263276325539, 0.0014195027342793765, 0.0014362385627464391, 0.0014413827029784443, 0.0014192213202477433, 0.0014720647341164295, 0.0015165963195613585, 0.0015331976028392091, 0.0013697645863430807, 0.0013398817500274163, 0.0014048253669898259, 0.0013423252021311782, 0.0013701177176699275, 0.0016681554297974799, 0.001559147367515834, 0.0013278928036015714, 0.001336345561867347, 0.0013272711003082804, 0.0014318303128675325, 0.0017843123841885244, 0.0015212456964945886, 0.0014458019140874967, 0.001380615616653813, 0.0013115833735355409, 0.0015915142812445993, 0.0013766057181783253, 0.001414412015947164, 0.001434418360076961, 0.001474353828598396, 0.0014289942882896867, 0.0017344803127343766, 0.0013964253666927107, 0.001389368342643138, 0.0014680765780212823, 0.0014556373589584837, 0.0013917412270529894, 0.0015697400467615807, 0.001321536428804393, 0.001330152117589023, 0.0014128940856608097, 0.0014064646093174815, 0.0014210853369149845, 0.0013804952268401394, 0.0013367062183533562, 0.0013037400385655928, 0.0013589264217443997, 0.0013097770552121801, 0.001337550889729755, 0.0014163500236463733, 0.001782738172551035, 0.0017014679287967738, 0.0013685643516510027, 0.0014054398670850787, 0.0013647263986058533, 0.0013151135317457374, 0.0015791346104379045, 0.0017178595007862896, 0.0013200207886256976, 0.0013191181969887111, 0.0013397925231402041, 0.0013959831703687087, 0.0017187962748721475, 0.0013460670779750217, 0.0014055385072424542, 0.001446126352675492, 0.0014238521871448029, 0.0014071178738959134, 0.0013945462123956531, 0.001595723795617232, 0.0012906057345389854, 0.00128996653984359, 0.001317278571150382, 0.0012791954777640058, 0.0015010233892098768, 0.001759609265718609, 0.0013192484057071852, 0.00132056286020088, 0.0013249403364170576, 0.001523524522781372, 0.0013104907266097143, 0.001275648983209976, 0.0013094224923406728, 0.0013010653747187462, 0.0016687435163476039, 0.0013382930628722534, 0.0013747511784458766, 0.0013052855010755593, 0.001342408648270066, 0.001700826063824934, 0.001330169656284852, 0.0013144973599992227, 0.0012933770849485882, 0.0014030295315023977, 0.0016847914066602243, 0.0013164906322344905, 0.00132176254010119, 0.0012918106876895763, 0.0013480482120939996, 0.0013279389986564638, 0.0013072828041913453, 0.0014106972575973487, 0.0013809165393467993, 0.0013959050538687734, 0.0013315976084413705, 0.00138877749304811, 0.0013914193441451062, 0.0013464820713124936, 0.001317965101407026, 0.0013645589751831722, 0.0014112316093815025, 0.0013255043759272667, 0.001381212556225364, 0.0013469690238707699, 0.0012701144132734044, 0.0018221076170448214, 0.0014056426334718708, 0.00138669606167241, 0.0013840716255799634, 0.0014124790322966874, 0.001782684281351976, 0.001287891031097388, 0.0013108226321492111, 0.001310688758167089, 0.001351608922050218, 0.0017331898598058615, 0.001797869890651782, 0.0013335315707081463, 0.001308372007770231, 0.001305542757108924, 0.0013189004130254034, 0.0013069681172055425, 0.0013724564141739393, 0.001452081227398594, 0.001389910226862412, 0.0013993967659189366, 0.0014126391179161146, 0.0016000823598005809, 0.00141766657725384, 0.0013978795086586615, 0.001431795453754603, 0.0014203208975231973, 0.00178333253825258, 0.0014580507267965004, 0.0014797623352933442, 0.001471910962209222, 0.0013331300797290169, 0.0014494151237158803, 0.001399180695443647]
[659.6580499937136, 675.7165222822078, 519.1756768623284, 668.7756394346435, 665.5147931934345, 692.777262347331, 621.1172135585465, 596.6597876490325, 664.6398447106011, 701.651473350014, 704.8396564415484, 655.5701844421661, 692.804717144551, 698.9850741999586, 689.1417661498298, 686.5208019059689, 491.3006967282266, 684.9523967520556, 728.9194828228244, 739.8646790064392, 760.435653107293, 551.6523281544386, 745.1142207035915, 731.9455709580176, 687.4662289084437, 740.5758364780918, 741.8979345381733, 745.8741196707075, 499.1137670650387, 475.490271409387, 752.3819719457589, 746.9048003830748, 713.0164936382289, 683.4797495277295, 375.01906833324864, 747.4024636762442, 745.7998289192391, 720.2645438226576, 574.0082394666545, 516.2429045331396, 512.2371324725252, 684.4628636224992, 697.8810764281409, 744.9129891549857, 695.5569876907904, 754.5059310986891, 752.2758752845099, 486.12844009966096, 662.5870993719034, 686.4913268283941, 706.3513506580204, 703.1419232703172, 718.1333225873797, 547.0274400839631, 719.3612176745778, 728.1345388869247, 756.754025183736, 561.5589398914908, 503.80122941475446, 713.8425831478817, 742.4115959701213, 744.9949378858707, 735.5733328597772, 695.9106413862908, 747.2114237561906, 753.7119652288001, 748.7771335665159, 554.3782252371145, 723.9533366248152, 721.0199482455331, 731.1246029088811, 743.5009416046046, 740.574587231635, 748.1148069285231, 727.7059657735615, 730.9590649028081, 745.7304591346739, 748.5684149946328, 570.2796177677972, 752.6529960967243, 741.153100385392, 630.653293484661, 512.2237859400208, 591.7067913282168, 710.0097062937917, 722.990410416434, 546.1198000769663, 516.4683912156275, 511.6808994955744, 516.4836912009805, 750.5352802946487, 743.8336111872288, 734.6758793985941, 744.7071756896995, 740.9259023402625, 720.85976415482, 730.2347231447291, 719.0984296571147, 720.41089103388, 727.0726883520874, 502.94818102558537, 739.2656387992917, 626.5367457838355, 717.9181616846965, 701.0169691328043, 679.8485578383858, 709.8854972054203, 738.6212309421877, 727.7893335571601, 681.364425984577, 755.5803820597963, 508.1860276513823, 569.0464508835502, 740.4375926466137, 750.4152374421052, 723.905455428486, 734.2931243524839, 499.9101712034775, 463.35236270897843, 677.6600712461232, 673.6465254668253, 634.8558015098137, 537.5923400834796, 741.3223391052547, 751.9701218675765, 734.5208148653853, 683.4301054450925, 693.1401309462401, 705.126149389797, 684.7882477653258, 488.19119820380854, 629.4770211097959, 742.0485149576981, 739.0769489030478, 740.017487286446, 676.4541041853937, 714.6815989406139, 711.4484605574589, 712.7878535223682, 492.01594860455447, 680.8827022232415, 711.4914315575621, 667.1102726199207, 742.6053518493039, 701.6464129609599, 690.6308694421341, 683.6952761629387, 728.7707264825157, 713.6668514937335, 706.5457285302289, 514.1705197143278, 702.3664586995116, 729.6899670087619, 727.7768549908074, 715.3114176571936, 503.85451817277004, 679.290785795595, 701.3451967785261, 662.4781579764651, 472.28563616333713, 503.9260881966112, 724.3067712010313, 692.5203411192525, 716.7446926572756, 692.7563840205308, 677.0631159546434, 726.7820354037599, 728.2791671441946, 661.6130240395861, 732.0048106857622, 667.7249778504048, 641.8398852574243, 696.5167873571855, 684.7043881598719, 669.8442177567837, 656.53680625998, 502.76832824360935, 730.2215051805534, 710.1728815277041, 708.1155338178843, 650.039810801126, 652.6945540259849, 601.7405417932132, 698.4857617594104, 658.7677121084073, 747.4149044293625, 675.4048096914837, 537.7105809395671, 651.9769220758564, 660.6975630909802, 747.2079663043223, 685.1559853422953, 714.6308261748592, 763.5093977008419, 749.1151208370404, 744.7349576758802, 757.7880615292402, 643.6306531453826, 491.7079852408685, 698.03465904948, 700.0520328326253, 690.8353397879022, 698.8860425398449, 597.9725618952696, 693.6388065137196, 753.1156258903859, 761.8914865936282, 697.1595162771462, 687.6159217711943, 710.373307838483, 595.6406113472455, 475.92189926230293, 750.0277426403335, 716.7224865996595, 485.0365563881415, 709.4716346290234, 691.196609265305, 704.7815052242319, 702.2237616124934, 680.4504580642824, 654.1310584727726, 495.7337442142035, 516.6705518167158, 737.8534860681459, 674.2013638371624, 469.04533649954953, 661.4804102975071, 710.5217512099817, 702.6168557075545, 702.7054779535348, 696.7266971863675, 536.6441195390317, 710.146621613557, 690.7947750364249, 495.218950606826, 495.60681799805633, 687.5347314328145, 703.060768722198, 716.0181806872258, 720.6126219953959, 718.67616530079, 492.1160190981604, 642.322635449176, 660.5284013799481, 694.0427731274995, 695.3489182143753, 691.6886520174204, 634.5667516632537, 702.1871390558197, 671.6405029862509, 672.5304088331778, 483.2798236345234, 495.701491886623, 677.5902936822374, 661.8313066635144, 706.1056984971559, 666.0797389193272, 482.18952805122245, 706.5089285776792, 709.2702548810562, 707.7877675215061, 699.9049536400881, 683.1628069888484, 504.46211171428234, 492.9324662263266, 704.1753661166902, 701.2282773176446, 692.6506889940646, 521.0529927582294, 516.6269929156539, 494.52911056165357, 701.6774713054243, 687.9510732518902, 686.3294636025623, 741.898340852652, 702.3407916115825, 550.6389123552643, 726.6278300243573, 742.3871861373331, 738.4516798553155, 729.8712832198756, 521.1924423614603, 700.591048380796, 712.61262055712, 709.3793149134722, 719.8709841050722, 535.5609439428846, 756.0275659938541, 733.3246914624592, 743.2908618665072, 755.4594637767331, 698.8853460900393, 744.1088169232032, 731.3795152377734, 708.8459297595714, 718.0297180138954, 465.6075986125187, 713.8541046583422, 750.3031572360245, 751.2176327604218, 672.2680176323088, 508.3438212154008, 701.2139524428167, 747.3151349767792, 725.4795463242674, 732.8520548372824, 499.5329098294997, 599.7200280558819, 729.5861496303327, 497.99328554815634, 553.3448979114609, 718.8323250255411, 757.7060907662484, 514.9889286264442, 512.3920529439142, 746.3805621065253, 745.151252874408, 745.9389948671943, 729.8847500417853, 755.8736527667667, 757.9426486612086, 710.9511253632768, 748.5233072279718, 753.509805009223, 743.5562679108166, 748.6858644982545, 712.2658256286165, 718.931240393294, 745.9454826058313, 731.5396106625589, 754.7788343911245, 742.0833038545213, 725.4489962669498, 724.5794724255697, 737.3701104305641, 755.1480823252406, 750.7214302641328, 759.6912491105348, 609.5731995221432, 716.2999409859145, 748.0167248932388, 751.7138949988273, 773.8674537840387, 747.6350650620047, 592.1502828593868, 742.2711584246103, 755.0965993683495, 687.7055963564259, 725.819431374615, 765.7128331744791, 661.5001923087772, 491.4306790254116, 744.901809306199, 724.3975941817722, 764.869648537446, 740.4406018083711, 500.0538936342562, 760.5970171523302, 711.1419907634229, 762.3540530695257, 566.3431487916079, 726.8440789621986, 711.0446540443903, 709.8046264194495, 694.1827697273607, 703.0393997874904, 766.220367822957, 707.5123302816088, 722.5682070841436, 737.2730633283454, 576.0951334784444, 707.561820043053, 761.8120918304944, 764.1371685698657, 735.440131658478, 443.96388046695995, 720.5636550933348, 733.3998813539233, 730.7926041872024, 737.3745530038235, 748.5882617878029, 729.5876435762394, 756.6757729175026, 742.5351430163555, 763.6667944629992, 733.217395005197, 496.47563299684754, 484.0139825882005, 734.453427050899, 732.4733150934669, 732.6053719960873, 719.2662308031531, 711.8865970220145, 739.3453551265695, 765.8662826531961, 740.4173850916869, 742.6939207976368, 722.329711452587, 748.8290791353035, 759.1447726473814, 757.0250075237836, 757.521359131271, 723.1351266847751, 755.1279993907957, 741.7000655910662, 684.9716285398742, 607.566920865012, 664.4011279897567, 580.9870372961872, 729.5511104273681, 754.2441271953924, 757.4366191745166, 747.596442954498, 740.3415606894149, 736.3091532025298, 513.7072340770065, 730.8295924397243, 684.1798682022853, 703.7753094992843, 733.1985013790478, 531.5116751442444, 691.9311249494782, 736.2463657881502, 700.2134475620379, 713.2741960884928, 722.1482331652713, 754.6424726783205, 748.475308009347, 726.6022210910095, 765.2359233790053, 746.3922046776032, 762.0219464486803, 770.8637638300371, 727.9447716825629, 741.9605549363245, 688.9426976570797, 725.826071153623, 735.9494968775795, 737.4058586178671, 732.9731948502039, 521.1618436480252, 501.0018713797365, 730.564243148484, 754.3840324369345, 753.0875488261833, 754.7894725984487, 770.7375220059477, 754.353328983301, 729.4844296688964, 750.0414091639809, 750.3398036196994, 759.4133607071367, 761.5643007419712, 767.7953256137712, 531.7023907157932, 726.4792501762872, 767.1606569022186, 652.6469171680595, 488.0477393293705, 491.4044356077015, 671.0795012562589, 695.4857004864482, 708.903383112611, 681.7963583383853, 698.2972361106673, 701.666410586966, 698.8368422166977, 667.0722292956233, 690.1612021889882, 754.6148428241376, 508.06895413540235, 771.0842448967179, 765.6899717397573, 761.4440393114952, 759.8289217170177, 496.41042164137735, 751.7347718152173, 666.5934898926889, 756.96571759494, 744.0380787314537, 762.2893401436992, 518.7242858131152, 711.4126864134229, 745.0053451573904, 508.3589920385341, 743.8532116981479, 749.8412037841276, 752.7075720411048, 755.4879126856449, 766.6832134452397, 734.9052685948296, 716.6259104290583, 746.2195650698563, 765.581569939937, 751.8438531417581, 725.4844262505437, 701.8245621550043, 710.363309201264, 723.5129077746233, 708.6850850656832, 721.1551611268372, 734.2396777751874, 730.9326898049441, 770.9864893884592, 751.2387570444771, 700.2711102435286, 718.3657332967231, 704.5710959150044, 693.0396068180771, 691.4034717125712, 765.289119625401, 754.7165000708, 727.4837685922786, 760.8778102245178, 747.3614491271799, 757.021320055449, 731.6248426946082, 686.2034538069126, 651.0957344458232, 676.5286184206382, 670.2222948802407, 535.4774393708493, 677.0271554848707, 697.1882038398661, 685.4147107136107, 491.60609632828687, 697.0255254602012, 758.8379479310308, 704.3065951545229, 705.8449731245495, 731.4392544885936, 767.8445981541726, 770.390558221197, 770.5205856003283, 771.8948554607733, 576.4539089011635, 705.6004884000057, 744.3088189181824, 748.9212846540128, 715.6011908316163, 667.738314359646, 658.7183752898133, 739.2019438190277, 693.4311551896099, 695.7682956486176, 711.6332991755551, 696.274393462729, 693.8282651310033, 634.3732585107825, 721.5832930228814, 714.1335023669999, 715.6009747289468, 708.0875025021243, 683.1716268650157, 694.5974123666479, 652.428395450832, 718.4363595109211, 678.9975415312158, 711.1362372698873, 582.5115343327496, 703.8440541734417, 705.8203944188446, 700.1278172020815, 709.3560853529196, 691.5017653604949, 692.9097492560512, 708.49383675905, 697.567960435252, 754.1226242627924, 574.3078020795805, 745.2880246080645, 745.2875254879029, 743.520966815941, 707.3378328378677, 707.3076036980077, 720.649564990242, 687.621844990287, 698.4932457294127, 695.7187209559884, 689.5448231724055, 662.4265572376316, 545.5005782587309, 545.7552364447719, 550.5438287412867, 695.5298589531349, 697.2878211754122, 686.1695271609924, 686.8522856390732, 674.4982302003872, 678.6376446118168, 571.1746306401038, 677.6052445773287, 691.9331463038834, 704.4720491557624, 696.2631598526053, 693.77827133184, 704.6117372485901, 679.3179517341167, 659.3712427636825, 652.2316485156107, 730.0524557068181, 746.3345179375257, 711.8322486892012, 744.9759554631944, 729.8642934861368, 599.4645235914279, 641.376191137907, 753.072836367329, 748.3094407128098, 753.4255810796556, 698.4067811759732, 560.4399817326737, 657.3560091603239, 691.6576816341676, 724.3145651384792, 762.437234397362, 628.3324075596595, 726.4244124478205, 707.0075683218428, 697.1466817716603, 678.2632368178923, 699.7928600518516, 576.5415684791016, 716.1141754166188, 719.7515369449093, 681.1633772863743, 686.9842916889035, 718.5243783555215, 637.048154605617, 756.6949939508742, 751.793713498391, 707.7671356606328, 711.0026042427562, 703.6875084299209, 724.3777309458235, 748.1075394650791, 767.0240772080797, 735.8750142751215, 763.4887143736098, 747.6351050852687, 706.0401618983378, 560.9348671594526, 587.7277984940741, 730.6927137139181, 711.5210144664728, 732.7476049569772, 760.3906247337883, 633.2582373852812, 582.119783103499, 757.5638267342151, 758.0821811743666, 746.3842219810282, 716.3410141512514, 581.8025176220405, 742.9050278121107, 711.4710801925406, 691.502508165964, 702.3200926532004, 710.6725161775406, 717.0791409501784, 626.6748686373986, 774.8299680050685, 775.2139060298814, 759.140869593512, 781.7413502336399, 666.2121371249183, 568.3079871664626, 758.0073590947025, 757.2528579577673, 754.7509669033319, 656.3727626611374, 763.0729311507874, 783.9147078561161, 763.6954503602874, 768.6008861900364, 599.2532646291327, 747.2204913427507, 727.4043591877304, 766.115918070029, 744.9296466382865, 587.949597709675, 751.7838008671677, 760.7470584806586, 773.1697210637917, 712.7433724999188, 593.5452875927876, 759.5952265172534, 756.5655476387182, 774.1072353167442, 741.813231180095, 753.0466391993498, 764.945424810798, 708.8693159460491, 724.1567259908551, 716.3811014427406, 750.977617908533, 720.0577522358782, 718.6905976317328, 742.6760603096945, 758.7454318270078, 732.8375088117863, 708.6009081374445, 754.4297990721022, 724.0015271312348, 742.4075700911898, 787.3306448217909, 548.8150044736909, 711.4183763265968, 721.138559226858, 722.5059610487809, 707.9751112297947, 560.9518244260316, 776.4632067884802, 762.8797180289833, 762.9576386986286, 739.8589811638175, 576.9708346389773, 556.2137756461731, 749.8885080530708, 764.3086171678586, 765.9649556131451, 758.2073598006667, 765.1296055623163, 728.6205883644653, 688.6667089495412, 719.4709274550798, 714.5936194466863, 707.8948808066223, 624.967829858852, 705.3844790057044, 715.3692387690497, 698.42378489029, 704.0662442859449, 560.7479135550693, 685.8471942173859, 675.7841959815545, 679.3889207123501, 750.1143475835969, 689.9334660150985, 714.7039715859743]
Elapsed: 0.1895364818232436~0.026336517549110185
Time per graph: 0.001472426808327677~0.0002036678823213775
Speed: 689.709393952777~77.06486076576913
Total Time: 0.1811
best val loss: 0.3207113846789959 test_score: 0.9219

Testing...
Test loss: 0.2895 score: 0.9062 time: 0.20s
test Score 0.9062
Epoch Time List: [0.6992513318546116, 0.6556087271310389, 0.7580386449117213, 0.8286614753305912, 0.6462946541141719, 0.631576957879588, 0.6655413750559092, 0.6668955388013273, 0.7012166550848633, 0.6358412296976894, 0.6371782629285008, 0.6523519458714873, 0.7072837031446397, 0.6485546519979835, 0.6553817891981453, 0.6392604268621653, 0.7291037449613214, 0.6511636071372777, 0.6186616832856089, 0.6091485810466111, 0.6076428131200373, 0.7305049560964108, 0.6901833289302886, 0.6226829171646386, 0.6240186039358377, 0.6910081212408841, 0.6027533719316125, 0.6140857820864767, 0.7150368499569595, 0.7123725388664752, 0.5922684296965599, 0.6011161538772285, 0.7183375260792673, 0.6561041662935168, 0.8599477210082114, 0.5998430298641324, 0.6230881698429585, 0.6945768152363598, 0.6515220266301185, 0.8498574697878212, 0.8606860809959471, 0.6955731539055705, 0.6498421777505428, 0.6036149237770587, 0.6309135060291737, 0.5975567661225796, 0.6062078650575131, 0.7314100377261639, 0.6496395720168948, 0.6342612239532173, 0.6323260359931737, 0.6302479768637568, 0.6323235458694398, 0.684377009049058, 0.6267482549883425, 0.6059373538009822, 0.6019792389124632, 0.6870478929486126, 0.8753250890877098, 0.7310672528110445, 0.6059742111247033, 0.6221048012375832, 0.6785717131569982, 0.6252567849587649, 0.6107012848369777, 0.6162685810122639, 0.6177883967757225, 0.6922158882953227, 0.6286076207179576, 0.6183335559908301, 0.6048268128652126, 0.6039398580323905, 0.6200827478896827, 0.6734565428923815, 0.6078863823786378, 0.6148446060251445, 0.6019329952541739, 0.6051112967543304, 0.7077911507803947, 0.6095163659192622, 0.602489828132093, 0.6390453178901225, 0.8187367760110646, 0.843420404009521, 0.6832954417914152, 0.6085957821924239, 0.7099477211013436, 0.8880798008758575, 0.8801922623533756, 0.8818422961048782, 0.7081265621818602, 0.6084602910559624, 0.6203891031909734, 0.612826009048149, 0.6139529731590301, 0.6452527537476271, 0.6991021109279245, 0.6158617560286075, 0.6263722409494221, 0.6294698230922222, 0.729784871218726, 0.6127379250247031, 0.6475764743518084, 0.614023903850466, 0.6330125860404223, 0.6428066971711814, 0.638398117152974, 0.7044880860485137, 0.6110677500255406, 0.6332698110491037, 0.600988607853651, 0.8146764000412077, 0.8243281757459044, 0.6120568686164916, 0.6195135631132871, 0.6148133240640163, 0.6173145161010325, 0.7130357751157135, 0.7158320709131658, 0.637345609953627, 0.644713374087587, 0.7517394777387381, 0.7645990499295294, 0.7905039971228689, 0.604502891888842, 0.6155776497907937, 0.7070700731128454, 0.7172898689750582, 0.6357637322507799, 0.6400095496792346, 0.7258474959526211, 0.8379479080904275, 0.6103005507029593, 0.6073234782088548, 0.6085759059060365, 0.6204776139929891, 0.6834475921932608, 0.6281572347506881, 0.6330926001537591, 0.7382162590511143, 0.7722826190292835, 0.635851738974452, 0.6658529967535287, 0.6090274120215327, 0.6412365310825408, 0.7034799149259925, 0.6516829421743751, 0.627736551919952, 0.6239908100105822, 0.6197608020156622, 0.7310183169320226, 0.7513484128285199, 0.6284190369769931, 0.6238662041723728, 0.6368040291126817, 0.809371598996222, 0.7772148721851408, 0.6581145708914846, 0.6636810589116067, 0.8750810762867332, 0.8832525359466672, 0.7002648350317031, 0.6325781277846545, 0.6197130139917135, 0.6437282310798764, 0.6514649011660367, 0.6988972479011863, 0.6546521270647645, 0.6438497412018478, 0.6143954440485686, 0.6497618479188532, 0.6761770960874856, 0.6932455240748823, 0.6630704021081328, 0.6604978409595788, 0.669176768977195, 0.81479362398386, 0.625688573345542, 0.6229714618530124, 0.6393743169028312, 0.6523202441167086, 0.6479931240901351, 0.7164495489560068, 0.6230184282176197, 0.6468159209471196, 0.6007866368163377, 0.635342005873099, 0.7222232830245048, 0.6667053063865751, 0.6413330261129886, 0.5881421100348234, 0.6220906453672796, 0.6003622731659561, 0.683411325328052, 0.584728044224903, 0.5977546337526292, 0.5964157569687814, 0.6286438191309571, 0.8397109629586339, 0.7202279260382056, 0.6316011268645525, 0.6448572061490268, 0.6375760398805141, 0.6673816787078977, 0.719821535050869, 0.6105381930246949, 0.5950351380743086, 0.7367052384652197, 0.6383207109756768, 0.6535808849148452, 0.6548748470377177, 0.7795964051038027, 0.6018632999621332, 0.6205232390202582, 0.7173348891083151, 0.6280108781065792, 0.6539987057913095, 0.7032698369584978, 0.6388475941494107, 0.6548783387988806, 0.6499590361490846, 0.7780415117740631, 0.8797439138870686, 0.6835707121063024, 0.6738753831014037, 0.8954118289984763, 0.7747814301401377, 0.6404906890820712, 0.6505805980414152, 0.6539959162473679, 0.6558579187840223, 0.7196212569251657, 0.6779294719453901, 0.6531295729801059, 0.8847974510863423, 0.9085708491038531, 0.7650458451826125, 0.6492475320119411, 0.6363466752227396, 0.6381139049772173, 0.6446226120460778, 0.81516751809977, 0.8452537900302559, 0.6926459060050547, 0.7113753180019557, 0.6731200190261006, 0.6740522542968392, 0.7585477558895946, 0.7506352926138788, 0.6943104940000921, 0.6936483783647418, 0.7608585979323834, 0.9283731249161065, 0.8300203320104629, 0.6749288041610271, 0.6477659028023481, 0.6659326471854001, 0.7469927712809294, 0.7297601448372006, 0.648411616217345, 0.6467287389095873, 0.65651715407148, 0.6528507336042821, 0.7752271792851388, 0.928728812141344, 0.7570554150734097, 0.6637789560481906, 0.6796596441417933, 0.8238104998599738, 0.900602500885725, 0.9075243182014674, 0.6783042431343347, 0.6752287258859724, 0.6628715042024851, 0.6402469971217215, 0.6528697467874736, 0.8251898370217532, 0.6485760398209095, 0.6407876431476325, 0.629431425826624, 0.6541107010561973, 0.8907031221315265, 0.7655843729153275, 0.7613559612073004, 0.6511314050294459, 0.7334678492043167, 0.819333653897047, 0.7234963790979236, 0.6477558792103082, 0.7122914688661695, 0.6255609602667391, 0.7128589188214391, 0.6570408109109849, 0.7187627621460706, 0.6489658730570227, 0.6429147829767317, 0.8666112541686743, 0.6565437309909612, 0.6273810530547053, 0.633779562311247, 0.652578730834648, 0.8692904380150139, 0.8018657648935914, 0.6499106611590832, 0.6423972533084452, 0.6585654958616942, 0.8043994239997119, 0.924622425576672, 0.7378780518192798, 0.756924951216206, 0.9011404148768634, 0.6774429481010884, 0.6432299658190459, 0.7323750539217144, 0.9070879130158573, 0.7316030801739544, 0.6535730597097427, 0.6392454889137298, 0.6486110230907798, 0.6458242149092257, 0.6505235501099378, 0.7009654738940299, 0.7306172910612077, 0.6417175356764346, 0.6440761019475758, 0.646937791723758, 0.6495898091234267, 0.7214496564120054, 0.6494862879626453, 0.6393965408205986, 0.6458092948887497, 0.6375821481924504, 0.6752560089807957, 0.8202634369954467, 0.647152537945658, 0.6474309500772506, 0.6460275719873607, 0.6416279962286353, 0.7513882622588426, 0.6461248081177473, 0.6391964619979262, 0.6352238282561302, 0.6207901469897479, 0.6368072957266122, 0.7756155338138342, 0.6497164848260581, 0.6567544757854193, 0.6853652549907565, 0.6400641032960266, 0.6332969702780247, 0.6578618821222335, 0.800869305850938, 0.6517800430301577, 0.6580826828721911, 0.648259487003088, 0.6335473279468715, 0.8168578250333667, 0.7274117120541632, 0.6504434361122549, 0.6354017599951476, 0.7072915516328067, 0.726625886047259, 0.6656821619253606, 0.6715004758443683, 0.8161129388026893, 0.6657418722752482, 0.69714400684461, 0.6628461210057139, 0.7347706318832934, 0.632998115150258, 0.7419676259160042, 0.7223288821987808, 0.6405423739925027, 0.6466040029190481, 0.6392961740493774, 0.7674479898996651, 0.7433126219548285, 0.6512618060223758, 0.6402723442297429, 0.6342141050845385, 0.6429292580578476, 0.6679579489864409, 0.7137613778468221, 0.645693679805845, 0.6341268932446837, 0.6422227588482201, 0.9073021169751883, 0.9236162330489606, 0.7222300069406629, 0.645324908895418, 0.6493126940913498, 0.6505928980186582, 0.6474701901897788, 0.7080665959510952, 0.6420167230535299, 0.6309657238889486, 0.6380694899708033, 0.6462005558423698, 0.6844972369726747, 0.6955576858017594, 0.6423341990448534, 0.6452375708613545, 0.638932716101408, 0.7622208138927817, 0.6384434399660677, 0.6732571676839143, 0.674054364906624, 0.6706578051671386, 0.8095523419324309, 0.7266110689379275, 0.6306087323464453, 0.6300327607896179, 0.6362709780223668, 0.6398331550881267, 0.6650007772259414, 0.8878744738176465, 0.6749356251675636, 0.6601201719604433, 0.6500324769876897, 0.6529211248271167, 0.8046543879900128, 0.7373364882078022, 0.6296390960924327, 0.64795400085859, 0.6660693499725312, 0.6680893779266626, 0.7106413238216192, 0.6354931241367012, 0.6489453818649054, 0.620910185854882, 0.6441617968957871, 0.6993983280844986, 0.6189482619520277, 0.6346988717559725, 0.6389899051282555, 0.7492122519761324, 0.6517568461131305, 0.6480030720122159, 0.6312830580864102, 0.6480089162942022, 0.8845131469424814, 0.8958606158848852, 0.6381098839920014, 0.630570919951424, 0.7007766517344862, 0.6291786688379943, 0.6290653189644217, 0.6348731720354408, 0.6658204048871994, 0.7012529871426523, 0.6385965859517455, 0.6327164780814201, 0.6381075729150325, 0.6336469079833478, 0.7972431257367134, 0.7613470838405192, 0.6285540498793125, 0.7102303521241993, 0.8701261039823294, 0.9363337799441069, 0.7435663321521133, 0.7521433930378407, 0.6611869782209396, 0.6819204003550112, 0.7623585748951882, 0.6722173120360821, 0.6711150130722672, 0.6813904920127243, 0.6820796350948513, 0.6321363958995789, 0.7317758090794086, 0.6547327481675893, 0.6278898559976369, 0.6296600920613855, 0.6411182091105729, 0.8133306268136948, 0.765783904120326, 0.6730363231617957, 0.6766599561087787, 0.630267548840493, 0.6343631709460169, 0.7539905321318656, 0.6495472600217909, 0.631879563909024, 0.8492024692241102, 0.8074877760373056, 0.6320586930960417, 0.6347312179859728, 0.6331402838695794, 0.6302169018890709, 0.6403719307854772, 0.7060466927941889, 0.6340941339731216, 0.6400276718195528, 0.6359993400983512, 0.6518976520746946, 0.7012435228098184, 0.7330840569920838, 0.6587330410256982, 0.6725292548071593, 0.6662149750627577, 0.6362803019583225, 0.6857316151726991, 0.7090473191346973, 0.6335667569655925, 0.6573207711335272, 0.6627178827766329, 0.6675752599257976, 0.7318057280499488, 0.6687121649738401, 0.6259959638118744, 0.6277771999593824, 0.664135173894465, 0.7218919389415532, 0.6500199357979, 0.6433444339782, 0.6459128679707646, 0.6792848368640989, 0.6915094831492752, 0.7326171481981874, 0.6775028121192008, 0.7282716729678214, 0.6799117350019515, 0.665798946050927, 0.6796007468365133, 0.92921481304802, 0.802509075962007, 0.6265783470589668, 0.665549793979153, 0.6781852501444519, 0.7745836640242487, 0.6493313882965595, 0.6673851970117539, 0.6267217679414898, 0.6630194608587772, 0.8915445238817483, 0.8194126279558986, 0.6264349522534758, 0.632719358894974, 0.7367723060306162, 0.6635164599865675, 0.7983923780266196, 0.7033412419259548, 0.6577054031658918, 0.6669302200898528, 0.6678870578762144, 0.6756767139304429, 0.661741497926414, 0.7550976059865206, 0.6525264210067689, 0.6618225071579218, 0.6493681750725955, 0.6634811877738684, 0.7062452600803226, 0.7645982999820262, 0.6873227441683412, 0.6447961428202689, 0.663193472661078, 0.6629011216573417, 0.7603911401238292, 0.6568444324657321, 0.6651922918390483, 0.6696039037778974, 0.6715445830486715, 0.6790599068626761, 0.9072982482612133, 0.6668664531316608, 0.6702976580709219, 0.6279153153300285, 0.7946437578648329, 0.6428607280831784, 0.6351025747135282, 0.6380876321345568, 0.6944537600502372, 0.711732802214101, 0.6545540560036898, 0.6663177909795195, 0.6803176940884441, 0.6689856369048357, 0.6948156701400876, 0.745479048229754, 0.9931873290333897, 1.003749545197934, 1.0025889298412949, 0.845997859025374, 0.6892293998971581, 0.6855967009905726, 0.6803717769216746, 0.6855408761184663, 0.685179753927514, 0.7703076871111989, 0.6888961757067591, 0.6804534860420972, 0.6380201720166951, 0.6919130149763077, 0.7802447876892984, 0.6737551831174642, 0.6618346751201898, 0.6791746369563043, 0.69218984618783, 0.7295983841177076, 0.6558691607788205, 0.639870879938826, 0.6241628380957991, 0.6497594655957073, 0.6845763400197029, 0.9227710859850049, 0.6431650491431355, 0.629550137091428, 0.6248695030808449, 0.7223653492983431, 0.9819018917623907, 0.7977961450815201, 0.703797492897138, 0.6700200580526143, 0.6227574001532048, 0.6778153809718788, 0.8732365800533444, 0.6503931442275643, 0.6679681949317455, 0.7680575386621058, 0.642025870969519, 0.8771241079084575, 0.7318786911200732, 0.7381942209322006, 0.6627237191423774, 0.6580925062298775, 0.6500034981872886, 0.7070795870386064, 0.654120254330337, 0.6260948036797345, 0.6554954540915787, 0.663992925779894, 0.6993222278542817, 0.7522403891198337, 0.6356463059782982, 0.6245199721306562, 0.6333232859615237, 0.6317224418744445, 0.6562373631168157, 0.9158860470633954, 0.791849649976939, 0.9582942589186132, 0.8806315800175071, 0.6494201312307268, 0.6473419719841331, 0.6259200007189065, 0.6774836061522365, 0.9622735006269068, 0.8274729012046009, 0.6340978448279202, 0.6254093591123819, 0.6615915752481669, 0.9428172518964857, 0.7696464201435447, 0.6489958518650383, 0.6707352979574353, 0.6625496638007462, 0.6669485829770565, 0.647794911172241, 0.7029222098644823, 0.6991819466929883, 0.609526279848069, 0.6175418279599398, 0.6158592319115996, 0.6606015218421817, 0.956072713015601, 0.6254907811526209, 0.6187565601430833, 0.6194826147984713, 0.6428938570898026, 0.7362047231290489, 0.6150660899002105, 0.6231725537218153, 0.6141172770876437, 0.7534005551133305, 0.8645247409585863, 0.6300483553204685, 0.6101592699997127, 0.638274289900437, 0.757677182322368, 0.8126075731124729, 0.6214488840196282, 0.619476392865181, 0.6225803277920932, 0.754210505168885, 0.7183589108753949, 0.613906032871455, 0.6036124078091234, 0.6180216008797288, 0.6206644952762872, 0.6240933910012245, 0.7075589536689222, 0.7082498769741505, 0.6401262851431966, 0.6227243740577251, 0.6366887462791055, 0.6723358568269759, 0.7063515931367874, 0.6318076681345701, 0.6290453122928739, 0.6591417288873345, 0.8084893119521439, 0.7408036270644516, 0.6157045401632786, 0.6221154381055385, 0.8273418068420142, 0.6609862791374326, 0.6721774141769856, 0.6600540317595005, 0.6664790690410882, 1.0653353747911751, 0.7426938840653747, 0.6177005937788635, 0.6246068249456584, 0.6363241660874337, 0.7759905026759952, 0.9520138364750892, 0.6167964695487171, 0.6365461440291256, 0.6202320228330791, 0.6315204899292439, 0.6180982729420066, 0.6725223062094301, 0.719052704051137, 0.6682660300284624, 0.6596966530196369, 0.6685274648480117, 0.7691098870709538, 0.7347978199832141, 0.6676144839730114, 0.6684712746646255, 0.6725342581048608, 0.8166883296798915, 0.7438560700975358, 0.670346459839493, 0.6904776529408991, 0.6281749019399285, 0.6696242219768465, 0.670759913045913]
Total Epoch List: [263, 267, 210]
Total Time List: [0.1897998140193522, 0.18440872197970748, 0.1810537299606949]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2ba1330>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5936;  Loss pred: 2.5936; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 2.6010;  Loss pred: 2.6010; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 2.5582;  Loss pred: 2.5582; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 2.5346;  Loss pred: 2.5346; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 2.5028;  Loss pred: 2.5028; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.22s
Epoch 6/1000, LR 0.000135
Train loss: 2.4817;  Loss pred: 2.4817; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 2.4455;  Loss pred: 2.4455; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 2.3719;  Loss pred: 2.3719; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 2.3081;  Loss pred: 2.3081; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 2.2615;  Loss pred: 2.2615; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 2.1742;  Loss pred: 2.1742; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 2.1008;  Loss pred: 2.1008; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 2.0530;  Loss pred: 2.0530; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.21s
Epoch 14/1000, LR 0.000285
Train loss: 1.9668;  Loss pred: 1.9668; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 1.9104;  Loss pred: 1.9104; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4961 time: 0.24s
Epoch 16/1000, LR 0.000285
Train loss: 1.8722;  Loss pred: 1.8722; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 1.8112;  Loss pred: 1.8112; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 1.7657;  Loss pred: 1.7657; Loss self: 0.0000; time: 0.27s
Val loss: 0.6913 score: 0.5116 time: 0.19s
Test loss: 0.6917 score: 0.5039 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 1.7043;  Loss pred: 1.7043; Loss self: 0.0000; time: 0.26s
Val loss: 0.6911 score: 0.5194 time: 0.18s
Test loss: 0.6916 score: 0.5194 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 1.6671;  Loss pred: 1.6671; Loss self: 0.0000; time: 0.26s
Val loss: 0.6908 score: 0.5194 time: 0.17s
Test loss: 0.6914 score: 0.5194 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 1.6161;  Loss pred: 1.6161; Loss self: 0.0000; time: 0.29s
Val loss: 0.6905 score: 0.5349 time: 0.18s
Test loss: 0.6911 score: 0.5194 time: 0.23s
Epoch 22/1000, LR 0.000285
Train loss: 1.5882;  Loss pred: 1.5882; Loss self: 0.0000; time: 0.26s
Val loss: 0.6903 score: 0.6047 time: 0.18s
Test loss: 0.6909 score: 0.5659 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 1.5430;  Loss pred: 1.5430; Loss self: 0.0000; time: 0.26s
Val loss: 0.6899 score: 0.6899 time: 0.17s
Test loss: 0.6906 score: 0.6124 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.5010;  Loss pred: 1.5010; Loss self: 0.0000; time: 0.26s
Val loss: 0.6896 score: 0.7287 time: 0.18s
Test loss: 0.6903 score: 0.6744 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.4709;  Loss pred: 1.4709; Loss self: 0.0000; time: 0.27s
Val loss: 0.6892 score: 0.6899 time: 0.17s
Test loss: 0.6901 score: 0.6279 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.4373;  Loss pred: 1.4373; Loss self: 0.0000; time: 0.26s
Val loss: 0.6889 score: 0.8217 time: 0.17s
Test loss: 0.6898 score: 0.7287 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.4054;  Loss pred: 1.4054; Loss self: 0.0000; time: 0.34s
Val loss: 0.6885 score: 0.8372 time: 0.25s
Test loss: 0.6894 score: 0.7287 time: 0.25s
Epoch 28/1000, LR 0.000285
Train loss: 1.3738;  Loss pred: 1.3738; Loss self: 0.0000; time: 0.35s
Val loss: 0.6881 score: 0.8760 time: 0.18s
Test loss: 0.6891 score: 0.7984 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 1.3537;  Loss pred: 1.3537; Loss self: 0.0000; time: 0.27s
Val loss: 0.6876 score: 0.8760 time: 0.18s
Test loss: 0.6887 score: 0.8062 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 1.3281;  Loss pred: 1.3281; Loss self: 0.0000; time: 0.37s
Val loss: 0.6871 score: 0.8992 time: 0.26s
Test loss: 0.6883 score: 0.8217 time: 0.25s
Epoch 31/1000, LR 0.000285
Train loss: 1.3047;  Loss pred: 1.3047; Loss self: 0.0000; time: 0.37s
Val loss: 0.6866 score: 0.8837 time: 0.26s
Test loss: 0.6879 score: 0.8140 time: 0.26s
Epoch 32/1000, LR 0.000285
Train loss: 1.2837;  Loss pred: 1.2837; Loss self: 0.0000; time: 0.37s
Val loss: 0.6861 score: 0.8992 time: 0.22s
Test loss: 0.6875 score: 0.8295 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 1.2681;  Loss pred: 1.2681; Loss self: 0.0000; time: 0.27s
Val loss: 0.6855 score: 0.9147 time: 0.18s
Test loss: 0.6870 score: 0.8372 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 1.2456;  Loss pred: 1.2456; Loss self: 0.0000; time: 0.26s
Val loss: 0.6849 score: 0.9147 time: 0.18s
Test loss: 0.6865 score: 0.8372 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 1.2352;  Loss pred: 1.2352; Loss self: 0.0000; time: 0.27s
Val loss: 0.6842 score: 0.9147 time: 0.19s
Test loss: 0.6860 score: 0.8372 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 1.2100;  Loss pred: 1.2100; Loss self: 0.0000; time: 0.27s
Val loss: 0.6836 score: 0.9147 time: 0.23s
Test loss: 0.6854 score: 0.8372 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 1.1968;  Loss pred: 1.1968; Loss self: 0.0000; time: 0.37s
Val loss: 0.6828 score: 0.9147 time: 0.26s
Test loss: 0.6849 score: 0.8372 time: 0.26s
Epoch 38/1000, LR 0.000284
Train loss: 1.1843;  Loss pred: 1.1843; Loss self: 0.0000; time: 0.37s
Val loss: 0.6821 score: 0.9147 time: 0.27s
Test loss: 0.6842 score: 0.8372 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 1.1680;  Loss pred: 1.1680; Loss self: 0.0000; time: 0.27s
Val loss: 0.6812 score: 0.9225 time: 0.19s
Test loss: 0.6836 score: 0.8372 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 1.1603;  Loss pred: 1.1603; Loss self: 0.0000; time: 0.27s
Val loss: 0.6804 score: 0.9225 time: 0.19s
Test loss: 0.6829 score: 0.8372 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 1.1473;  Loss pred: 1.1473; Loss self: 0.0000; time: 0.27s
Val loss: 0.6794 score: 0.9225 time: 0.19s
Test loss: 0.6821 score: 0.8450 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 1.1340;  Loss pred: 1.1340; Loss self: 0.0000; time: 0.27s
Val loss: 0.6785 score: 0.9225 time: 0.19s
Test loss: 0.6813 score: 0.8527 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.1282;  Loss pred: 1.1282; Loss self: 0.0000; time: 0.27s
Val loss: 0.6774 score: 0.9225 time: 0.19s
Test loss: 0.6805 score: 0.8527 time: 0.28s
Epoch 44/1000, LR 0.000284
Train loss: 1.1161;  Loss pred: 1.1161; Loss self: 0.0000; time: 0.27s
Val loss: 0.6763 score: 0.9225 time: 0.26s
Test loss: 0.6796 score: 0.8527 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.1080;  Loss pred: 1.1080; Loss self: 0.0000; time: 0.27s
Val loss: 0.6752 score: 0.9225 time: 0.17s
Test loss: 0.6786 score: 0.8527 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 1.0981;  Loss pred: 1.0981; Loss self: 0.0000; time: 0.26s
Val loss: 0.6740 score: 0.9225 time: 0.29s
Test loss: 0.6776 score: 0.8527 time: 0.20s
Epoch 47/1000, LR 0.000284
Train loss: 1.0926;  Loss pred: 1.0926; Loss self: 0.0000; time: 0.31s
Val loss: 0.6726 score: 0.9225 time: 0.26s
Test loss: 0.6766 score: 0.8527 time: 0.26s
Epoch 48/1000, LR 0.000284
Train loss: 1.0826;  Loss pred: 1.0826; Loss self: 0.0000; time: 0.37s
Val loss: 0.6712 score: 0.9225 time: 0.36s
Test loss: 0.6754 score: 0.8527 time: 0.25s
Epoch 49/1000, LR 0.000284
Train loss: 1.0739;  Loss pred: 1.0739; Loss self: 0.0000; time: 0.37s
Val loss: 0.6697 score: 0.9225 time: 0.27s
Test loss: 0.6742 score: 0.8682 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 1.0727;  Loss pred: 1.0727; Loss self: 0.0000; time: 0.27s
Val loss: 0.6682 score: 0.9302 time: 0.26s
Test loss: 0.6729 score: 0.8682 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 1.0584;  Loss pred: 1.0584; Loss self: 0.0000; time: 0.26s
Val loss: 0.6665 score: 0.9302 time: 0.17s
Test loss: 0.6716 score: 0.8682 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 1.0549;  Loss pred: 1.0549; Loss self: 0.0000; time: 0.26s
Val loss: 0.6648 score: 0.9302 time: 0.18s
Test loss: 0.6702 score: 0.8682 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 1.0495;  Loss pred: 1.0495; Loss self: 0.0000; time: 0.26s
Val loss: 0.6630 score: 0.9380 time: 0.18s
Test loss: 0.6687 score: 0.8682 time: 0.18s
Epoch 54/1000, LR 0.000284
Train loss: 1.0452;  Loss pred: 1.0452; Loss self: 0.0000; time: 0.27s
Val loss: 0.6610 score: 0.9380 time: 0.19s
Test loss: 0.6671 score: 0.8605 time: 0.18s
Epoch 55/1000, LR 0.000284
Train loss: 1.0389;  Loss pred: 1.0389; Loss self: 0.0000; time: 0.27s
Val loss: 0.6589 score: 0.9380 time: 0.19s
Test loss: 0.6654 score: 0.8605 time: 0.18s
Epoch 56/1000, LR 0.000284
Train loss: 1.0339;  Loss pred: 1.0339; Loss self: 0.0000; time: 0.35s
Val loss: 0.6566 score: 0.9457 time: 0.19s
Test loss: 0.6635 score: 0.8682 time: 0.18s
Epoch 57/1000, LR 0.000283
Train loss: 1.0261;  Loss pred: 1.0261; Loss self: 0.0000; time: 0.28s
Val loss: 0.6542 score: 0.9457 time: 0.19s
Test loss: 0.6616 score: 0.8605 time: 0.18s
Epoch 58/1000, LR 0.000283
Train loss: 1.0237;  Loss pred: 1.0237; Loss self: 0.0000; time: 0.27s
Val loss: 0.6518 score: 0.9457 time: 0.25s
Test loss: 0.6596 score: 0.8605 time: 0.25s
Epoch 59/1000, LR 0.000283
Train loss: 1.0172;  Loss pred: 1.0172; Loss self: 0.0000; time: 0.37s
Val loss: 0.6492 score: 0.9457 time: 0.26s
Test loss: 0.6575 score: 0.8605 time: 0.25s
Epoch 60/1000, LR 0.000283
Train loss: 1.0141;  Loss pred: 1.0141; Loss self: 0.0000; time: 0.38s
Val loss: 0.6465 score: 0.9457 time: 0.26s
Test loss: 0.6553 score: 0.8605 time: 0.25s
Epoch 61/1000, LR 0.000283
Train loss: 1.0079;  Loss pred: 1.0079; Loss self: 0.0000; time: 0.37s
Val loss: 0.6436 score: 0.9457 time: 0.26s
Test loss: 0.6530 score: 0.8605 time: 0.18s
Epoch 62/1000, LR 0.000283
Train loss: 1.0045;  Loss pred: 1.0045; Loss self: 0.0000; time: 0.26s
Val loss: 0.6407 score: 0.9457 time: 0.17s
Test loss: 0.6506 score: 0.8605 time: 0.18s
Epoch 63/1000, LR 0.000283
Train loss: 0.9989;  Loss pred: 0.9989; Loss self: 0.0000; time: 0.26s
Val loss: 0.6375 score: 0.9457 time: 0.17s
Test loss: 0.6480 score: 0.8605 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9949;  Loss pred: 0.9949; Loss self: 0.0000; time: 0.28s
Val loss: 0.6343 score: 0.9380 time: 0.23s
Test loss: 0.6454 score: 0.8605 time: 0.25s
Epoch 65/1000, LR 0.000283
Train loss: 0.9917;  Loss pred: 0.9917; Loss self: 0.0000; time: 0.37s
Val loss: 0.6308 score: 0.9457 time: 0.26s
Test loss: 0.6426 score: 0.8605 time: 0.25s
Epoch 66/1000, LR 0.000283
Train loss: 0.9866;  Loss pred: 0.9866; Loss self: 0.0000; time: 0.36s
Val loss: 0.6272 score: 0.9380 time: 0.25s
Test loss: 0.6396 score: 0.8605 time: 0.19s
Epoch 67/1000, LR 0.000283
Train loss: 0.9823;  Loss pred: 0.9823; Loss self: 0.0000; time: 0.27s
Val loss: 0.6234 score: 0.9380 time: 0.17s
Test loss: 0.6366 score: 0.8605 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 0.9760;  Loss pred: 0.9760; Loss self: 0.0000; time: 0.26s
Val loss: 0.6195 score: 0.9380 time: 0.20s
Test loss: 0.6334 score: 0.8605 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 0.9715;  Loss pred: 0.9715; Loss self: 0.0000; time: 0.26s
Val loss: 0.6154 score: 0.9380 time: 0.18s
Test loss: 0.6301 score: 0.8605 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.27s
Val loss: 0.6113 score: 0.9380 time: 0.17s
Test loss: 0.6267 score: 0.8605 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9638;  Loss pred: 0.9638; Loss self: 0.0000; time: 0.26s
Val loss: 0.6069 score: 0.9380 time: 0.19s
Test loss: 0.6232 score: 0.8605 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9586;  Loss pred: 0.9586; Loss self: 0.0000; time: 0.31s
Val loss: 0.6024 score: 0.9380 time: 0.25s
Test loss: 0.6196 score: 0.8605 time: 0.25s
Epoch 73/1000, LR 0.000282
Train loss: 0.9549;  Loss pred: 0.9549; Loss self: 0.0000; time: 0.36s
Val loss: 0.5977 score: 0.9380 time: 0.18s
Test loss: 0.6158 score: 0.8605 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 0.9500;  Loss pred: 0.9500; Loss self: 0.0000; time: 0.27s
Val loss: 0.5930 score: 0.9225 time: 0.17s
Test loss: 0.6119 score: 0.8682 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9464;  Loss pred: 0.9464; Loss self: 0.0000; time: 0.29s
Val loss: 0.5881 score: 0.9225 time: 0.25s
Test loss: 0.6079 score: 0.8682 time: 0.25s
Epoch 76/1000, LR 0.000282
Train loss: 0.9417;  Loss pred: 0.9417; Loss self: 0.0000; time: 0.37s
Val loss: 0.5831 score: 0.9225 time: 0.26s
Test loss: 0.6037 score: 0.8682 time: 0.25s
Epoch 77/1000, LR 0.000282
Train loss: 0.9371;  Loss pred: 0.9371; Loss self: 0.0000; time: 0.36s
Val loss: 0.5777 score: 0.9380 time: 0.18s
Test loss: 0.5995 score: 0.8605 time: 0.17s
Epoch 78/1000, LR 0.000282
Train loss: 0.9321;  Loss pred: 0.9321; Loss self: 0.0000; time: 0.26s
Val loss: 0.5723 score: 0.9380 time: 0.17s
Test loss: 0.5952 score: 0.8605 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.9287;  Loss pred: 0.9287; Loss self: 0.0000; time: 0.31s
Val loss: 0.5668 score: 0.9380 time: 0.25s
Test loss: 0.5907 score: 0.8605 time: 0.25s
Epoch 80/1000, LR 0.000282
Train loss: 0.9219;  Loss pred: 0.9219; Loss self: 0.0000; time: 0.38s
Val loss: 0.5613 score: 0.9380 time: 0.26s
Test loss: 0.5862 score: 0.8682 time: 0.25s
Epoch 81/1000, LR 0.000281
Train loss: 0.9196;  Loss pred: 0.9196; Loss self: 0.0000; time: 0.37s
Val loss: 0.5555 score: 0.9380 time: 0.25s
Test loss: 0.5815 score: 0.8682 time: 0.26s
Epoch 82/1000, LR 0.000281
Train loss: 0.9149;  Loss pred: 0.9149; Loss self: 0.0000; time: 0.26s
Val loss: 0.5496 score: 0.9380 time: 0.17s
Test loss: 0.5768 score: 0.8682 time: 0.17s
Epoch 83/1000, LR 0.000281
Train loss: 0.9108;  Loss pred: 0.9108; Loss self: 0.0000; time: 0.26s
Val loss: 0.5437 score: 0.9225 time: 0.19s
Test loss: 0.5719 score: 0.8682 time: 0.18s
Epoch 84/1000, LR 0.000281
Train loss: 0.9061;  Loss pred: 0.9061; Loss self: 0.0000; time: 0.28s
Val loss: 0.5377 score: 0.9225 time: 0.18s
Test loss: 0.5670 score: 0.8682 time: 0.18s
Epoch 85/1000, LR 0.000281
Train loss: 0.9007;  Loss pred: 0.9007; Loss self: 0.0000; time: 0.28s
Val loss: 0.5314 score: 0.9302 time: 0.19s
Test loss: 0.5620 score: 0.8682 time: 0.19s
Epoch 86/1000, LR 0.000281
Train loss: 0.8953;  Loss pred: 0.8953; Loss self: 0.0000; time: 0.26s
Val loss: 0.5251 score: 0.9302 time: 0.28s
Test loss: 0.5568 score: 0.8682 time: 0.26s
Epoch 87/1000, LR 0.000281
Train loss: 0.8904;  Loss pred: 0.8904; Loss self: 0.0000; time: 0.30s
Val loss: 0.5188 score: 0.9302 time: 0.18s
Test loss: 0.5517 score: 0.8682 time: 0.17s
Epoch 88/1000, LR 0.000281
Train loss: 0.8870;  Loss pred: 0.8870; Loss self: 0.0000; time: 0.26s
Val loss: 0.5123 score: 0.9225 time: 0.18s
Test loss: 0.5463 score: 0.8682 time: 0.17s
Epoch 89/1000, LR 0.000281
Train loss: 0.8815;  Loss pred: 0.8815; Loss self: 0.0000; time: 0.26s
Val loss: 0.5055 score: 0.9302 time: 0.19s
Test loss: 0.5409 score: 0.8682 time: 0.18s
Epoch 90/1000, LR 0.000281
Train loss: 0.8758;  Loss pred: 0.8758; Loss self: 0.0000; time: 0.27s
Val loss: 0.4989 score: 0.9302 time: 0.20s
Test loss: 0.5355 score: 0.8682 time: 0.18s
Epoch 91/1000, LR 0.000280
Train loss: 0.8723;  Loss pred: 0.8723; Loss self: 0.0000; time: 0.27s
Val loss: 0.4922 score: 0.9225 time: 0.17s
Test loss: 0.5300 score: 0.8682 time: 0.24s
Epoch 92/1000, LR 0.000280
Train loss: 0.8662;  Loss pred: 0.8662; Loss self: 0.0000; time: 0.35s
Val loss: 0.4853 score: 0.9225 time: 0.22s
Test loss: 0.5244 score: 0.8682 time: 0.22s
Epoch 93/1000, LR 0.000280
Train loss: 0.8629;  Loss pred: 0.8629; Loss self: 0.0000; time: 0.26s
Val loss: 0.4785 score: 0.9225 time: 0.18s
Test loss: 0.5187 score: 0.8682 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8575;  Loss pred: 0.8575; Loss self: 0.0000; time: 0.25s
Val loss: 0.4717 score: 0.9225 time: 0.17s
Test loss: 0.5131 score: 0.8682 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.8510;  Loss pred: 0.8510; Loss self: 0.0000; time: 0.27s
Val loss: 0.4650 score: 0.9225 time: 0.22s
Test loss: 0.5076 score: 0.8682 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.8479;  Loss pred: 0.8479; Loss self: 0.0000; time: 0.26s
Val loss: 0.4585 score: 0.9225 time: 0.22s
Test loss: 0.5021 score: 0.8682 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8427;  Loss pred: 0.8427; Loss self: 0.0000; time: 0.32s
Val loss: 0.4518 score: 0.9225 time: 0.25s
Test loss: 0.4966 score: 0.8682 time: 0.20s
Epoch 98/1000, LR 0.000280
Train loss: 0.8380;  Loss pred: 0.8380; Loss self: 0.0000; time: 0.27s
Val loss: 0.4449 score: 0.9225 time: 0.20s
Test loss: 0.4910 score: 0.8682 time: 0.18s
Epoch 99/1000, LR 0.000279
Train loss: 0.8324;  Loss pred: 0.8324; Loss self: 0.0000; time: 0.26s
Val loss: 0.4381 score: 0.9225 time: 0.20s
Test loss: 0.4855 score: 0.8682 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8278;  Loss pred: 0.8278; Loss self: 0.0000; time: 0.29s
Val loss: 0.4311 score: 0.9225 time: 0.21s
Test loss: 0.4800 score: 0.8682 time: 0.25s
Epoch 101/1000, LR 0.000279
Train loss: 0.8240;  Loss pred: 0.8240; Loss self: 0.0000; time: 0.38s
Val loss: 0.4243 score: 0.9225 time: 0.25s
Test loss: 0.4745 score: 0.8682 time: 0.25s
Epoch 102/1000, LR 0.000279
Train loss: 0.8187;  Loss pred: 0.8187; Loss self: 0.0000; time: 0.35s
Val loss: 0.4177 score: 0.9225 time: 0.18s
Test loss: 0.4691 score: 0.8682 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.8144;  Loss pred: 0.8144; Loss self: 0.0000; time: 0.25s
Val loss: 0.4110 score: 0.9225 time: 0.17s
Test loss: 0.4637 score: 0.8682 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.8117;  Loss pred: 0.8117; Loss self: 0.0000; time: 0.26s
Val loss: 0.4049 score: 0.9302 time: 0.18s
Test loss: 0.4584 score: 0.8682 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.8046;  Loss pred: 0.8046; Loss self: 0.0000; time: 0.26s
Val loss: 0.3984 score: 0.9302 time: 0.18s
Test loss: 0.4531 score: 0.8682 time: 0.17s
Epoch 106/1000, LR 0.000279
Train loss: 0.8007;  Loss pred: 0.8007; Loss self: 0.0000; time: 0.31s
Val loss: 0.3922 score: 0.9302 time: 0.25s
Test loss: 0.4479 score: 0.8682 time: 0.25s
Epoch 107/1000, LR 0.000278
Train loss: 0.7956;  Loss pred: 0.7956; Loss self: 0.0000; time: 0.38s
Val loss: 0.3866 score: 0.9302 time: 0.27s
Test loss: 0.4428 score: 0.8682 time: 0.18s
Epoch 108/1000, LR 0.000278
Train loss: 0.7913;  Loss pred: 0.7913; Loss self: 0.0000; time: 0.27s
Val loss: 0.3806 score: 0.9302 time: 0.18s
Test loss: 0.4378 score: 0.8682 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.7867;  Loss pred: 0.7867; Loss self: 0.0000; time: 0.27s
Val loss: 0.3747 score: 0.9302 time: 0.17s
Test loss: 0.4328 score: 0.8682 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.7826;  Loss pred: 0.7826; Loss self: 0.0000; time: 0.27s
Val loss: 0.3683 score: 0.9380 time: 0.18s
Test loss: 0.4278 score: 0.8682 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.7789;  Loss pred: 0.7789; Loss self: 0.0000; time: 0.29s
Val loss: 0.3620 score: 0.9302 time: 0.18s
Test loss: 0.4229 score: 0.8682 time: 0.24s
Epoch 112/1000, LR 0.000278
Train loss: 0.7743;  Loss pred: 0.7743; Loss self: 0.0000; time: 0.37s
Val loss: 0.3561 score: 0.9302 time: 0.26s
Test loss: 0.4182 score: 0.8682 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7707;  Loss pred: 0.7707; Loss self: 0.0000; time: 0.26s
Val loss: 0.3506 score: 0.9380 time: 0.18s
Test loss: 0.4135 score: 0.8682 time: 0.17s
Epoch 114/1000, LR 0.000277
Train loss: 0.7669;  Loss pred: 0.7669; Loss self: 0.0000; time: 0.26s
Val loss: 0.3449 score: 0.9302 time: 0.17s
Test loss: 0.4088 score: 0.8760 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.7619;  Loss pred: 0.7619; Loss self: 0.0000; time: 0.25s
Val loss: 0.3398 score: 0.9302 time: 0.17s
Test loss: 0.4043 score: 0.8760 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7578;  Loss pred: 0.7578; Loss self: 0.0000; time: 0.26s
Val loss: 0.3348 score: 0.9302 time: 0.19s
Test loss: 0.3999 score: 0.8760 time: 0.18s
Epoch 117/1000, LR 0.000277
Train loss: 0.7542;  Loss pred: 0.7542; Loss self: 0.0000; time: 0.27s
Val loss: 0.3297 score: 0.9302 time: 0.19s
Test loss: 0.3956 score: 0.8837 time: 0.21s
Epoch 118/1000, LR 0.000277
Train loss: 0.7502;  Loss pred: 0.7502; Loss self: 0.0000; time: 0.27s
Val loss: 0.3242 score: 0.9302 time: 0.25s
Test loss: 0.3913 score: 0.8760 time: 0.21s
Epoch 119/1000, LR 0.000277
Train loss: 0.7462;  Loss pred: 0.7462; Loss self: 0.0000; time: 0.27s
Val loss: 0.3191 score: 0.9225 time: 0.18s
Test loss: 0.3872 score: 0.8760 time: 0.18s
Epoch 120/1000, LR 0.000277
Train loss: 0.7423;  Loss pred: 0.7423; Loss self: 0.0000; time: 0.27s
Val loss: 0.3144 score: 0.9225 time: 0.18s
Test loss: 0.3831 score: 0.8837 time: 0.19s
Epoch 121/1000, LR 0.000276
Train loss: 0.7398;  Loss pred: 0.7398; Loss self: 0.0000; time: 0.28s
Val loss: 0.3094 score: 0.9225 time: 0.19s
Test loss: 0.3792 score: 0.8837 time: 0.19s
Epoch 122/1000, LR 0.000276
Train loss: 0.7331;  Loss pred: 0.7331; Loss self: 0.0000; time: 0.26s
Val loss: 0.3051 score: 0.9225 time: 0.17s
Test loss: 0.3754 score: 0.8837 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.7327;  Loss pred: 0.7327; Loss self: 0.0000; time: 0.27s
Val loss: 0.3005 score: 0.9302 time: 0.19s
Test loss: 0.3717 score: 0.8837 time: 0.19s
Epoch 124/1000, LR 0.000276
Train loss: 0.7291;  Loss pred: 0.7291; Loss self: 0.0000; time: 0.31s
Val loss: 0.2966 score: 0.9302 time: 0.19s
Test loss: 0.3681 score: 0.8837 time: 0.26s
Epoch 125/1000, LR 0.000276
Train loss: 0.7262;  Loss pred: 0.7262; Loss self: 0.0000; time: 0.27s
Val loss: 0.2926 score: 0.9302 time: 0.18s
Test loss: 0.3646 score: 0.8915 time: 0.18s
Epoch 126/1000, LR 0.000276
Train loss: 0.7210;  Loss pred: 0.7210; Loss self: 0.0000; time: 0.27s
Val loss: 0.2885 score: 0.9302 time: 0.18s
Test loss: 0.3612 score: 0.8837 time: 0.19s
Epoch 127/1000, LR 0.000275
Train loss: 0.7191;  Loss pred: 0.7191; Loss self: 0.0000; time: 0.26s
Val loss: 0.2850 score: 0.9380 time: 0.17s
Test loss: 0.3579 score: 0.8915 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7162;  Loss pred: 0.7162; Loss self: 0.0000; time: 0.26s
Val loss: 0.2810 score: 0.9302 time: 0.17s
Test loss: 0.3546 score: 0.8837 time: 0.16s
Epoch 129/1000, LR 0.000275
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.33s
Val loss: 0.2767 score: 0.9302 time: 0.17s
Test loss: 0.3514 score: 0.8837 time: 0.22s
Epoch 130/1000, LR 0.000275
Train loss: 0.7091;  Loss pred: 0.7091; Loss self: 0.0000; time: 0.27s
Val loss: 0.2729 score: 0.9302 time: 0.17s
Test loss: 0.3483 score: 0.8837 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.7075;  Loss pred: 0.7075; Loss self: 0.0000; time: 0.26s
Val loss: 0.2693 score: 0.9302 time: 0.28s
Test loss: 0.3453 score: 0.8837 time: 0.18s
Epoch 132/1000, LR 0.000275
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.27s
Val loss: 0.2662 score: 0.9302 time: 0.18s
Test loss: 0.3424 score: 0.8837 time: 0.18s
Epoch 133/1000, LR 0.000274
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.27s
Val loss: 0.2633 score: 0.9302 time: 0.18s
Test loss: 0.3396 score: 0.8837 time: 0.19s
Epoch 134/1000, LR 0.000274
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.30s
Val loss: 0.2604 score: 0.9302 time: 0.24s
Test loss: 0.3369 score: 0.8837 time: 0.18s
Epoch 135/1000, LR 0.000274
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.27s
Val loss: 0.2575 score: 0.9302 time: 0.19s
Test loss: 0.3341 score: 0.8837 time: 0.18s
Epoch 136/1000, LR 0.000274
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.28s
Val loss: 0.2555 score: 0.9302 time: 0.19s
Test loss: 0.3316 score: 0.8837 time: 0.20s
Epoch 137/1000, LR 0.000274
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.41s
Val loss: 0.2533 score: 0.9302 time: 0.26s
Test loss: 0.3291 score: 0.8915 time: 0.25s
Epoch 138/1000, LR 0.000274
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.28s
Val loss: 0.2501 score: 0.9302 time: 0.19s
Test loss: 0.3266 score: 0.8837 time: 0.18s
Epoch 139/1000, LR 0.000273
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.27s
Val loss: 0.2475 score: 0.9302 time: 0.19s
Test loss: 0.3241 score: 0.8837 time: 0.18s
Epoch 140/1000, LR 0.000273
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.26s
Val loss: 0.2437 score: 0.9302 time: 0.19s
Test loss: 0.3217 score: 0.8837 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.26s
Val loss: 0.2409 score: 0.9302 time: 0.19s
Test loss: 0.3194 score: 0.8837 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.28s
Val loss: 0.2383 score: 0.9302 time: 0.19s
Test loss: 0.3171 score: 0.8837 time: 0.20s
Epoch 143/1000, LR 0.000273
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.27s
Val loss: 0.2366 score: 0.9302 time: 0.26s
Test loss: 0.3148 score: 0.8837 time: 0.25s
Epoch 144/1000, LR 0.000272
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.37s
Val loss: 0.2344 score: 0.9302 time: 0.22s
Test loss: 0.3126 score: 0.8837 time: 0.18s
Epoch 145/1000, LR 0.000272
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.28s
Val loss: 0.2329 score: 0.9302 time: 0.18s
Test loss: 0.3105 score: 0.8837 time: 0.18s
Epoch 146/1000, LR 0.000272
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.26s
Val loss: 0.2317 score: 0.9302 time: 0.17s
Test loss: 0.3085 score: 0.8915 time: 0.17s
Epoch 147/1000, LR 0.000272
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.26s
Val loss: 0.2305 score: 0.9302 time: 0.17s
Test loss: 0.3065 score: 0.8915 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.27s
Val loss: 0.2284 score: 0.9302 time: 0.18s
Test loss: 0.3044 score: 0.8915 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6600;  Loss pred: 0.6600; Loss self: 0.0000; time: 0.27s
Val loss: 0.2260 score: 0.9302 time: 0.19s
Test loss: 0.3024 score: 0.8915 time: 0.18s
Epoch 150/1000, LR 0.000271
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.32s
Val loss: 0.2241 score: 0.9302 time: 0.21s
Test loss: 0.3005 score: 0.8915 time: 0.18s
Epoch 151/1000, LR 0.000271
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.28s
Val loss: 0.2220 score: 0.9302 time: 0.18s
Test loss: 0.2986 score: 0.8915 time: 0.18s
Epoch 152/1000, LR 0.000271
Train loss: 0.6566;  Loss pred: 0.6566; Loss self: 0.0000; time: 0.27s
Val loss: 0.2201 score: 0.9302 time: 0.19s
Test loss: 0.2967 score: 0.8915 time: 0.19s
Epoch 153/1000, LR 0.000271
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.36s
Val loss: 0.2186 score: 0.9302 time: 0.26s
Test loss: 0.2948 score: 0.8915 time: 0.25s
Epoch 154/1000, LR 0.000271
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.37s
Val loss: 0.2168 score: 0.9302 time: 0.26s
Test loss: 0.2930 score: 0.8915 time: 0.24s
Epoch 155/1000, LR 0.000270
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.37s
Val loss: 0.2153 score: 0.9302 time: 0.19s
Test loss: 0.2912 score: 0.8915 time: 0.21s
Epoch 156/1000, LR 0.000270
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.27s
Val loss: 0.2130 score: 0.9302 time: 0.17s
Test loss: 0.2896 score: 0.8915 time: 0.17s
Epoch 157/1000, LR 0.000270
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.26s
Val loss: 0.2126 score: 0.9302 time: 0.18s
Test loss: 0.2877 score: 0.8915 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6430;  Loss pred: 0.6430; Loss self: 0.0000; time: 0.27s
Val loss: 0.2124 score: 0.9302 time: 0.18s
Test loss: 0.2859 score: 0.8915 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.36s
Val loss: 0.2112 score: 0.9302 time: 0.26s
Test loss: 0.2842 score: 0.8915 time: 0.26s
Epoch 160/1000, LR 0.000269
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.30s
Val loss: 0.2111 score: 0.9302 time: 0.18s
Test loss: 0.2824 score: 0.8915 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.6391;  Loss pred: 0.6391; Loss self: 0.0000; time: 0.26s
Val loss: 0.2099 score: 0.9302 time: 0.19s
Test loss: 0.2808 score: 0.8915 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.25s
Val loss: 0.2073 score: 0.9302 time: 0.17s
Test loss: 0.2792 score: 0.8915 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.26s
Val loss: 0.2059 score: 0.9302 time: 0.18s
Test loss: 0.2777 score: 0.8915 time: 0.18s
Epoch 164/1000, LR 0.000269
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.26s
Val loss: 0.2042 score: 0.9302 time: 0.18s
Test loss: 0.2762 score: 0.8915 time: 0.17s
Epoch 165/1000, LR 0.000268
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.32s
Val loss: 0.2038 score: 0.9302 time: 0.25s
Test loss: 0.2746 score: 0.8915 time: 0.20s
Epoch 166/1000, LR 0.000268
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.27s
Val loss: 0.2025 score: 0.9302 time: 0.17s
Test loss: 0.2732 score: 0.8915 time: 0.17s
Epoch 167/1000, LR 0.000268
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.27s
Val loss: 0.2022 score: 0.9302 time: 0.17s
Test loss: 0.2715 score: 0.8915 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.27s
Val loss: 0.2016 score: 0.9302 time: 0.23s
Test loss: 0.2700 score: 0.8915 time: 0.24s
Epoch 169/1000, LR 0.000267
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.37s
Val loss: 0.2012 score: 0.9302 time: 0.25s
Test loss: 0.2686 score: 0.8915 time: 0.25s
Epoch 170/1000, LR 0.000267
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.38s
Val loss: 0.1989 score: 0.9302 time: 0.18s
Test loss: 0.2673 score: 0.8915 time: 0.17s
Epoch 171/1000, LR 0.000267
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.26s
Val loss: 0.1977 score: 0.9302 time: 0.19s
Test loss: 0.2661 score: 0.8837 time: 0.19s
Epoch 172/1000, LR 0.000267
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 0.35s
Val loss: 0.1962 score: 0.9302 time: 0.26s
Test loss: 0.2650 score: 0.8837 time: 0.25s
Epoch 173/1000, LR 0.000267
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.38s
Val loss: 0.1952 score: 0.9302 time: 0.26s
Test loss: 0.2638 score: 0.8837 time: 0.25s
Epoch 174/1000, LR 0.000266
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.37s
Val loss: 0.1944 score: 0.9302 time: 0.26s
Test loss: 0.2625 score: 0.8837 time: 0.26s
Epoch 175/1000, LR 0.000266
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.28s
Val loss: 0.1937 score: 0.9302 time: 0.18s
Test loss: 0.2612 score: 0.8837 time: 0.17s
Epoch 176/1000, LR 0.000266
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.27s
Val loss: 0.1935 score: 0.9302 time: 0.18s
Test loss: 0.2598 score: 0.8837 time: 0.17s
Epoch 177/1000, LR 0.000266
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.27s
Val loss: 0.1927 score: 0.9302 time: 0.18s
Test loss: 0.2587 score: 0.8837 time: 0.17s
Epoch 178/1000, LR 0.000265
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.28s
Val loss: 0.1926 score: 0.9302 time: 0.19s
Test loss: 0.2573 score: 0.8837 time: 0.19s
Epoch 179/1000, LR 0.000265
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.27s
Val loss: 0.1930 score: 0.9302 time: 0.18s
Test loss: 0.2559 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.27s
Val loss: 0.1920 score: 0.9302 time: 0.19s
Test loss: 0.2549 score: 0.8837 time: 0.19s
Epoch 181/1000, LR 0.000265
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.28s
Val loss: 0.1908 score: 0.9302 time: 0.19s
Test loss: 0.2540 score: 0.8837 time: 0.19s
Epoch 182/1000, LR 0.000265
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.34s
Val loss: 0.1898 score: 0.9302 time: 0.19s
Test loss: 0.2531 score: 0.8837 time: 0.18s
Epoch 183/1000, LR 0.000264
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.26s
Val loss: 0.1883 score: 0.9302 time: 0.18s
Test loss: 0.2524 score: 0.8837 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.27s
Val loss: 0.1870 score: 0.9302 time: 0.18s
Test loss: 0.2518 score: 0.8837 time: 0.17s
Epoch 185/1000, LR 0.000264
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.26s
Val loss: 0.1865 score: 0.9302 time: 0.18s
Test loss: 0.2509 score: 0.8837 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.28s
Val loss: 0.1865 score: 0.9302 time: 0.18s
Test loss: 0.2498 score: 0.8837 time: 0.25s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.37s
Val loss: 0.1863 score: 0.9380 time: 0.26s
Test loss: 0.2488 score: 0.8837 time: 0.18s
Epoch 188/1000, LR 0.000263
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.27s
Val loss: 0.1858 score: 0.9380 time: 0.17s
Test loss: 0.2479 score: 0.8837 time: 0.18s
Epoch 189/1000, LR 0.000263
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.27s
Val loss: 0.1862 score: 0.9380 time: 0.18s
Test loss: 0.2467 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.26s
Val loss: 0.1857 score: 0.9380 time: 0.18s
Test loss: 0.2458 score: 0.8915 time: 0.18s
Epoch 191/1000, LR 0.000262
Train loss: 0.5993;  Loss pred: 0.5993; Loss self: 0.0000; time: 0.28s
Val loss: 0.1856 score: 0.9380 time: 0.19s
Test loss: 0.2449 score: 0.8915 time: 0.18s
Epoch 192/1000, LR 0.000262
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.29s
Val loss: 0.1851 score: 0.9380 time: 0.20s
Test loss: 0.2442 score: 0.8915 time: 0.19s
Epoch 193/1000, LR 0.000262
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 0.38s
Val loss: 0.1839 score: 0.9380 time: 0.27s
Test loss: 0.2438 score: 0.8915 time: 0.26s
Epoch 194/1000, LR 0.000262
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 0.31s
Val loss: 0.1830 score: 0.9380 time: 0.18s
Test loss: 0.2434 score: 0.8915 time: 0.18s
Epoch 195/1000, LR 0.000261
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.27s
Val loss: 0.1831 score: 0.9380 time: 0.19s
Test loss: 0.2424 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 0.26s
Val loss: 0.1818 score: 0.9457 time: 0.17s
Test loss: 0.2423 score: 0.8915 time: 0.17s
Epoch 197/1000, LR 0.000261
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.27s
Val loss: 0.1812 score: 0.9457 time: 0.17s
Test loss: 0.2418 score: 0.8915 time: 0.17s
Epoch 198/1000, LR 0.000261
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.27s
Val loss: 0.1818 score: 0.9380 time: 0.17s
Test loss: 0.2407 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.26s
Val loss: 0.1823 score: 0.9380 time: 0.17s
Test loss: 0.2397 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.29s
Val loss: 0.1820 score: 0.9380 time: 0.19s
Test loss: 0.2391 score: 0.8915 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.27s
Val loss: 0.1824 score: 0.9380 time: 0.18s
Test loss: 0.2382 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 0.27s
Val loss: 0.1813 score: 0.9380 time: 0.18s
Test loss: 0.2380 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.27s
Val loss: 0.1805 score: 0.9457 time: 0.18s
Test loss: 0.2377 score: 0.8915 time: 0.17s
Epoch 204/1000, LR 0.000259
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.27s
Val loss: 0.1803 score: 0.9457 time: 0.18s
Test loss: 0.2372 score: 0.8915 time: 0.18s
Epoch 205/1000, LR 0.000259
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.28s
Val loss: 0.1798 score: 0.9457 time: 0.19s
Test loss: 0.2369 score: 0.8915 time: 0.24s
Epoch 206/1000, LR 0.000259
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 0.37s
Val loss: 0.1794 score: 0.9457 time: 0.22s
Test loss: 0.2366 score: 0.8915 time: 0.17s
Epoch 207/1000, LR 0.000258
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.27s
Val loss: 0.1796 score: 0.9457 time: 0.18s
Test loss: 0.2359 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 0.27s
Val loss: 0.1797 score: 0.9457 time: 0.17s
Test loss: 0.2353 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.28s
Val loss: 0.1794 score: 0.9457 time: 0.18s
Test loss: 0.2349 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.30s
Val loss: 0.1795 score: 0.9457 time: 0.19s
Test loss: 0.2344 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.29s
Val loss: 0.1798 score: 0.9457 time: 0.19s
Test loss: 0.2337 score: 0.8915 time: 0.26s
     INFO: Early stopping counter 5 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.27s
Val loss: 0.1789 score: 0.9457 time: 0.18s
Test loss: 0.2338 score: 0.8915 time: 0.17s
Epoch 213/1000, LR 0.000257
Train loss: 0.5743;  Loss pred: 0.5743; Loss self: 0.0000; time: 0.27s
Val loss: 0.1789 score: 0.9457 time: 0.18s
Test loss: 0.2334 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.26s
Val loss: 0.1785 score: 0.9457 time: 0.18s
Test loss: 0.2332 score: 0.8915 time: 0.17s
Epoch 215/1000, LR 0.000256
Train loss: 0.5739;  Loss pred: 0.5739; Loss self: 0.0000; time: 0.28s
Val loss: 0.1777 score: 0.9457 time: 0.19s
Test loss: 0.2334 score: 0.8915 time: 0.25s
Epoch 216/1000, LR 0.000256
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.38s
Val loss: 0.1786 score: 0.9457 time: 0.27s
Test loss: 0.2324 score: 0.8915 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.38s
Val loss: 0.1779 score: 0.9457 time: 0.24s
Test loss: 0.2324 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.26s
Val loss: 0.1778 score: 0.9457 time: 0.27s
Test loss: 0.2321 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.27s
Val loss: 0.1777 score: 0.9457 time: 0.19s
Test loss: 0.2317 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.27s
Val loss: 0.1770 score: 0.9457 time: 0.19s
Test loss: 0.2319 score: 0.8837 time: 0.18s
Epoch 221/1000, LR 0.000255
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.30s
Val loss: 0.1773 score: 0.9457 time: 0.26s
Test loss: 0.2314 score: 0.8837 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.39s
Val loss: 0.1777 score: 0.9457 time: 0.26s
Test loss: 0.2308 score: 0.8915 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.36s
Val loss: 0.1778 score: 0.9457 time: 0.19s
Test loss: 0.2305 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.26s
Val loss: 0.1777 score: 0.9457 time: 0.17s
Test loss: 0.2303 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.26s
Val loss: 0.1774 score: 0.9457 time: 0.18s
Test loss: 0.2303 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.27s
Val loss: 0.1777 score: 0.9457 time: 0.18s
Test loss: 0.2298 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.28s
Val loss: 0.1775 score: 0.9457 time: 0.19s
Test loss: 0.2298 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.28s
Val loss: 0.1781 score: 0.9457 time: 0.20s
Test loss: 0.2291 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.34s
Val loss: 0.1784 score: 0.9457 time: 0.18s
Test loss: 0.2287 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.26s
Val loss: 0.1782 score: 0.9457 time: 0.19s
Test loss: 0.2287 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5597;  Loss pred: 0.5597; Loss self: 0.0000; time: 0.27s
Val loss: 0.1773 score: 0.9457 time: 0.19s
Test loss: 0.2291 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.27s
Val loss: 0.1765 score: 0.9457 time: 0.20s
Test loss: 0.2297 score: 0.8837 time: 0.24s
Epoch 233/1000, LR 0.000251
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.29s
Val loss: 0.1765 score: 0.9457 time: 0.20s
Test loss: 0.2296 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.26s
Val loss: 0.1763 score: 0.9457 time: 0.20s
Test loss: 0.2296 score: 0.8837 time: 0.18s
Epoch 235/1000, LR 0.000250
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.27s
Val loss: 0.1761 score: 0.9457 time: 0.19s
Test loss: 0.2298 score: 0.8837 time: 0.18s
Epoch 236/1000, LR 0.000250
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.26s
Val loss: 0.1762 score: 0.9457 time: 0.18s
Test loss: 0.2296 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.27s
Val loss: 0.1767 score: 0.9457 time: 0.18s
Test loss: 0.2290 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.28s
Val loss: 0.1771 score: 0.9457 time: 0.27s
Test loss: 0.2285 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.27s
Val loss: 0.1780 score: 0.9457 time: 0.19s
Test loss: 0.2277 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5548;  Loss pred: 0.5548; Loss self: 0.0000; time: 0.27s
Val loss: 0.1778 score: 0.9457 time: 0.19s
Test loss: 0.2279 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.27s
Val loss: 0.1787 score: 0.9457 time: 0.19s
Test loss: 0.2271 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.28s
Val loss: 0.1787 score: 0.9457 time: 0.19s
Test loss: 0.2270 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.29s
Val loss: 0.1787 score: 0.9457 time: 0.24s
Test loss: 0.2270 score: 0.8915 time: 0.26s
     INFO: Early stopping counter 8 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.28s
Val loss: 0.1785 score: 0.9457 time: 0.18s
Test loss: 0.2270 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.28s
Val loss: 0.1775 score: 0.9457 time: 0.18s
Test loss: 0.2279 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.27s
Val loss: 0.1773 score: 0.9457 time: 0.20s
Test loss: 0.2281 score: 0.8915 time: 0.24s
     INFO: Early stopping counter 11 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.38s
Val loss: 0.1770 score: 0.9457 time: 0.27s
Test loss: 0.2284 score: 0.8915 time: 0.26s
     INFO: Early stopping counter 12 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.39s
Val loss: 0.1779 score: 0.9457 time: 0.26s
Test loss: 0.2275 score: 0.8915 time: 0.26s
     INFO: Early stopping counter 13 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.39s
Val loss: 0.1783 score: 0.9457 time: 0.22s
Test loss: 0.2272 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.27s
Val loss: 0.1786 score: 0.9457 time: 0.19s
Test loss: 0.2270 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5490;  Loss pred: 0.5490; Loss self: 0.0000; time: 0.28s
Val loss: 0.1781 score: 0.9457 time: 0.18s
Test loss: 0.2275 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.27s
Val loss: 0.1779 score: 0.9457 time: 0.18s
Test loss: 0.2278 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.28s
Val loss: 0.1777 score: 0.9457 time: 0.19s
Test loss: 0.2281 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.36s
Val loss: 0.1772 score: 0.9457 time: 0.25s
Test loss: 0.2288 score: 0.8992 time: 0.25s
     INFO: Early stopping counter 19 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.39s
Val loss: 0.1780 score: 0.9457 time: 0.25s
Test loss: 0.2280 score: 0.8915 time: 0.24s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 234,   Train_Loss: 0.5604,   Val_Loss: 0.1761,   Val_Precision: 0.9385,   Val_Recall: 0.9531,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1761,   Test_Precision: 0.9167,   Test_Recall: 0.8462,   Test_accuracy: 0.8800,   Test_Score: 0.8837,   Test_loss: 0.2298


[0.18194310809485614, 0.1810774840414524, 0.17691346304491162, 0.18729655584320426, 0.21967729600146413, 0.1842338410206139, 0.19775674818083644, 0.17130612907931209, 0.17523492616601288, 0.18254406796768308, 0.1720045858528465, 0.17406337801367044, 0.22099581314250827, 0.17332209693267941, 0.24705747794359922, 0.1816440438851714, 0.19131378619931638, 0.1946081870701164, 0.1734392608050257, 0.17236929200589657, 0.23319614306092262, 0.17839026707224548, 0.17149156494997442, 0.1774467770010233, 0.17231285106390715, 0.17357777897268534, 0.25593708897940814, 0.1849010311998427, 0.1884904899634421, 0.2578901220113039, 0.26025395817123353, 0.1885067990515381, 0.17830685409717262, 0.1800132051575929, 0.17874571913853288, 0.18423096416518092, 0.2644944149069488, 0.18424635007977486, 0.18014067295007408, 0.18017646810039878, 0.18079442321322858, 0.18134824419394135, 0.2855063711758703, 0.17315450101159513, 0.1681035771034658, 0.20053225103765726, 0.2600499589461833, 0.25797628005966544, 0.18810250284150243, 0.18069669185206294, 0.17859323509037495, 0.18145917891524732, 0.18137782905250788, 0.18655900610610843, 0.1859878678806126, 0.18516107695177197, 0.1858946280553937, 0.25787682086229324, 0.25175379286520183, 0.2517485909629613, 0.1796361031010747, 0.18246362986974418, 0.1710216428618878, 0.2518417008686811, 0.24956991500221193, 0.19545258884318173, 0.17422436899505556, 0.17271967907436192, 0.17633475898765028, 0.1720998368691653, 0.1758254780434072, 0.2517764091026038, 0.17556831683032215, 0.17309878603555262, 0.2517390118446201, 0.2559383499901742, 0.17312748590484262, 0.17444416298530996, 0.24968568701297045, 0.2517590359784663, 0.2615265778731555, 0.17709587118588388, 0.18203573185019195, 0.18339021294377744, 0.19229137105867267, 0.26225168304517865, 0.17514603002928197, 0.1761805519927293, 0.18875654810108244, 0.18336466094478965, 0.24432305479422212, 0.22729339404031634, 0.17452223505824804, 0.17223694315180182, 0.17059823009185493, 0.1778185770381242, 0.20545099396258593, 0.18572978186421096, 0.17029164801351726, 0.25595966703258455, 0.253158297855407, 0.1710975910536945, 0.1740097040310502, 0.18111294717527926, 0.176220121094957, 0.2496106110047549, 0.1802713149227202, 0.1736392981838435, 0.1728567269165069, 0.17516811820678413, 0.24846512009389699, 0.17208930710330606, 0.17906918213702738, 0.1699134160298854, 0.1700999829918146, 0.18262516101822257, 0.20953723485581577, 0.21919363108463585, 0.18200379004701972, 0.19160219095647335, 0.1921800789423287, 0.17261441494338214, 0.19008711609058082, 0.2658604809548706, 0.1844589530956, 0.19039395288564265, 0.17890097782947123, 0.16870063310489058, 0.22866753116250038, 0.17490224610082805, 0.18366583390161395, 0.18337107310071588, 0.1961535830050707, 0.18272562697529793, 0.1851648238953203, 0.20201167417690158, 0.2528105832170695, 0.18329941295087337, 0.1814502498600632, 0.17065685079433024, 0.18320432608015835, 0.19955118210054934, 0.25595016218721867, 0.18608309607952833, 0.18295096699148417, 0.17021998902782798, 0.16992938006296754, 0.17783734016120434, 0.18483066093176603, 0.18115724716335535, 0.18467249488458037, 0.19079823000356555, 0.25166575890034437, 0.2475501918233931, 0.21393612190149724, 0.16957823582924902, 0.17746347794309258, 0.187363610137254, 0.2648962759412825, 0.1810049309860915, 0.1818247709888965, 0.170418132096529, 0.184733172878623, 0.17296810494735837, 0.20656663295812905, 0.17813694896176457, 0.16875282395631075, 0.2475821920670569, 0.25594360986724496, 0.1758754758629948, 0.196519257966429, 0.2518946649506688, 0.24958509486168623, 0.2627222998999059, 0.17695791414007545, 0.17490509594790637, 0.17617671191692352, 0.19144063792191446, 0.1934461600612849, 0.19326914916746318, 0.19408264989033341, 0.18182275514118373, 0.17733828397467732, 0.17302818107418716, 0.1749809740576893, 0.2539609130471945, 0.18777691898867488, 0.1841343909036368, 0.18069227505475283, 0.18459049495868385, 0.18371691601350904, 0.1906355288811028, 0.26370978192426264, 0.18525653891265392, 0.1825488128233701, 0.17453856184147298, 0.1729956460185349, 0.17193303513340652, 0.17579146497882903, 0.256762936944142, 0.1737316099461168, 0.17355965008027852, 0.17359850509092212, 0.1823210760485381, 0.2480911989696324, 0.17861528601497412, 0.17339223297312856, 0.17337769200094044, 0.18363975593820214, 0.179949234938249, 0.2616397449746728, 0.17663365183398128, 0.17618722189217806, 0.1780414180830121, 0.258358056191355, 0.2434195070527494, 0.18314689910039306, 0.1812009729910642, 0.1789727909490466, 0.1823384310118854, 0.2603700279723853, 0.2684615380130708, 0.18201788002625108, 0.17522011906839907, 0.18180147395469248, 0.1828178740106523, 0.1864203200675547, 0.17747685895301402, 0.18347616191022098, 0.18440603790804744, 0.18550198199227452, 0.24564243713393807, 0.1854056480806321, 0.18509095907211304, 0.18427512887865305, 0.1761972070671618, 0.1950718560256064, 0.19367907708510756, 0.1900793721433729, 0.19145685178227723, 0.18849589093588293, 0.187677931971848, 0.26900579989887774, 0.1907864399254322, 0.17997291102074087, 0.2404005927965045, 0.2622568979859352, 0.26427928218618035, 0.18160887993872166, 0.18139180401340127, 0.18633005302399397, 0.1848584038671106, 0.1844587530940771, 0.2497118308674544, 0.24928120616823435]
[0.0014104116906577995, 0.0014037014266779257, 0.0013714221941466018, 0.0014519112856062346, 0.0017029247752051483, 0.0014281693102373171, 0.0015329980479134608, 0.0013279544889869153, 0.001358410280356689, 0.0014150702943231247, 0.0013333688825802055, 0.0013493285117338793, 0.0017131458383140175, 0.0013435821467649568, 0.0019151742476248002, 0.0014080933634509412, 0.001483052606196251, 0.0015085905974427628, 0.0013444903938374085, 0.0013361960620612138, 0.0018077220392319583, 0.001382870287381748, 0.0013293919763563908, 0.0013755564108606456, 0.0013357585353791252, 0.0013455641780828322, 0.0019840084417008384, 0.0014333413271305634, 0.0014611665888638922, 0.001999148232645767, 0.0020174725439630506, 0.0014612930159033962, 0.0013822236751718808, 0.001395451202772038, 0.0013856257297560689, 0.0014281470090324103, 0.002050344301604254, 0.0014282662796881772, 0.0013964393251943727, 0.0013967168069798355, 0.0014015071566916945, 0.0014058003425886927, 0.0022132276835338783, 0.001342282953578257, 0.0013031285046780294, 0.0015545135739353276, 0.002015891154621576, 0.0019998161244935307, 0.0014581589367558328, 0.001400749549240798, 0.0013844436828711235, 0.0014066603016685839, 0.0014060296825775804, 0.0014461938457837862, 0.0014417664176791674, 0.001435357185672651, 0.0014410436283363852, 0.0019990451229635136, 0.0019515797896527273, 0.0019515394648291576, 0.001392527931016083, 0.0014144467431763115, 0.0013257491694719985, 0.0019522612470440394, 0.0019346505038931158, 0.0015151363476215637, 0.00135057650383764, 0.0013389122408865265, 0.0013669361161833355, 0.001334107262551669, 0.0013629882018868775, 0.00195175510932251, 0.0013609947041110244, 0.0013418510545391677, 0.0019514652080978303, 0.0019840182169780945, 0.0013420735341460668, 0.001352280333219457, 0.0019355479613408562, 0.001951620433941599, 0.0020273378129701974, 0.001372836210743286, 0.001411129704265054, 0.0014216295577037011, 0.0014906307834005633, 0.0020329587832959586, 0.0013577211630176897, 0.0013657407131219327, 0.0014632290550471508, 0.0014214314801921678, 0.0018939771689474583, 0.0017619642948861732, 0.0013528855430871942, 0.0013351701019519522, 0.0013224668999368598, 0.0013784385816908854, 0.0015926433640510538, 0.0014397657508853562, 0.0013200902946784285, 0.0019841834653688724, 0.0019624674252357133, 0.0013263379151449185, 0.0013489124343492264, 0.0014039763346920873, 0.0013660474503485037, 0.0019349659767810457, 0.0013974520536644975, 0.0013460410711925853, 0.0013399746272597433, 0.001357892389199877, 0.0019260862022782712, 0.0013340256364597369, 0.0013881331948606775, 0.0013171582637975614, 0.0013186045193163923, 0.0014156989226218803, 0.0016243196500450835, 0.001699175434764619, 0.0014108820933877498, 0.0014852883019881656, 0.0014897680538165014, 0.0013380962398711793, 0.0014735435355858978, 0.0020609339608904697, 0.0014299143650821707, 0.0014759221153925787, 0.001386829285499777, 0.0013077568457743457, 0.0017726165206395379, 0.001355831365122698, 0.001423766154276077, 0.0014214811868272549, 0.0015205704108920209, 0.0014164777284906816, 0.001435386231746669, 0.001565981970363578, 0.0019597719629230194, 0.001420925681789716, 0.0014065910841865364, 0.001322921323987056, 0.0014201885742647934, 0.0015469083883763515, 0.001984109784397044, 0.0014425046207715375, 0.0014182245503215826, 0.0013195347986653331, 0.0013172820159919965, 0.001378584032257398, 0.001432795821176481, 0.0014043197454523672, 0.0014315697277874448, 0.0014790560465392677, 0.001950897355816623, 0.0019189937350650628, 0.0016584195496240097, 0.001314559967668597, 0.0013756858755278494, 0.0014524310863353023, 0.0020534595034207947, 0.001403138999892182, 0.0014094943487511357, 0.0013210707914459612, 0.001432040099834287, 0.0013408380228477393, 0.0016012917283575894, 0.00138090658109895, 0.0013081614260179127, 0.0019192417989694333, 0.001984058991218953, 0.0013633757818836806, 0.0015234051005149534, 0.001952671821323014, 0.0019347681772223738, 0.002036606975968263, 0.0013717667762796547, 0.0013558534569605145, 0.0013657109450924303, 0.001484035952883058, 0.0014995826361339916, 0.0014982104586625054, 0.0015045166658165382, 0.00140947872202468, 0.0013747153796486613, 0.0013413037292572649, 0.0013564416593619326, 0.0019686892484278642, 0.0014556350309199602, 0.0014273983790979597, 0.00140071531050196, 0.0014309340694471616, 0.0014241621396396049, 0.0014777947975279286, 0.002044261875381881, 0.0014360972008732862, 0.0014151070761501557, 0.0013530121072982402, 0.0013410515195235264, 0.0013328142258403606, 0.0013627245347196048, 0.0019904103639080775, 0.0013467566662489674, 0.0013454236440331667, 0.001345724845666063, 0.0014133416747948689, 0.001923187588911879, 0.0013846146202711172, 0.0013441258370009966, 0.0013440131162863601, 0.0014235639995209468, 0.0013949553095988295, 0.002028215077323045, 0.0013692531149921028, 0.0013657924177688221, 0.0013801660316512566, 0.002002775629390349, 0.0018869729228895302, 0.001419743403879016, 0.0014046587053570868, 0.0013873859763491986, 0.0014134762093944604, 0.002018372309863452, 0.002081097193899774, 0.001410991318032954, 0.0013582954966542564, 0.0014093137515867635, 0.0014171928217880023, 0.0014451187602136024, 0.0013757896042869303, 0.0014222958287614029, 0.0014295041698298252, 0.0014379998604052288, 0.0019042049390227758, 0.0014372530858963728, 0.0014348136362179306, 0.0014284893711523493, 0.0013658698222260605, 0.0015121849304310573, 0.0015013881944581982, 0.0014734835049873868, 0.0014841616417230793, 0.0014612084568673095, 0.0014548676897042482, 0.0020853162782858738, 0.0014789646505847457, 0.0013951388451220223, 0.0018635704867946085, 0.002032999209193296, 0.0020486766060944213, 0.0014078207747187726, 0.0014061380156077617, 0.0014444190156898758, 0.00143301088269078, 0.0014299128146827683, 0.0019357506268794916, 0.0019324124509165453]
[709.012841161017, 712.4022110361839, 729.1700573813978, 688.7473152896237, 587.2249993424001, 700.1970934621409, 652.3165514535939, 753.037855056984, 736.1546172467288, 706.6786745589436, 749.9800040817643, 741.1093675883316, 583.7214658759841, 744.2790174071417, 522.145700967001, 710.1801811985038, 674.2849146564063, 662.8703650248893, 743.7762326778898, 748.393165040018, 553.1823910410845, 723.1336222382405, 752.2235862599448, 726.9785463573457, 748.6383006462859, 743.1826859606251, 504.03011347206075, 697.670527648792, 684.3846606002227, 500.21303256565074, 495.66969473380584, 684.3254495278505, 723.4719083187814, 716.614094433054, 721.6956054764111, 700.2080273777376, 487.7229640005185, 700.1495548983503, 716.1070172961566, 715.9647503364203, 713.5175837136175, 711.3385661569573, 451.8287962146272, 744.9994036907053, 767.3840272928993, 643.2880463490909, 496.05852861025147, 500.0459731032812, 685.7962975043296, 713.9034958404926, 722.3117938074256, 710.9036906876504, 711.222538465025, 691.4702361065816, 693.593627745689, 696.6906983026461, 693.9415159514992, 500.23883328733245, 512.40538834333, 512.415976218827, 718.1184504287337, 706.99021000563, 754.2904970464862, 512.2265278348999, 516.8892252051161, 660.006607042187, 740.424549929987, 746.8749403156368, 731.5630834249451, 749.5649173570635, 733.682066077778, 512.3593606715949, 734.7567165246104, 745.2391952275436, 512.4354745605427, 504.02763011073756, 745.1156546621561, 739.4916390000574, 516.6495586641249, 512.3947170302708, 493.25770653630104, 728.4190147188618, 708.6520799452812, 703.4181264599325, 670.8569359601635, 491.8938879708806, 736.5282557556857, 732.2034046375541, 683.4199994530427, 703.5161482879265, 527.9894691421919, 567.5483906809827, 739.1608293175098, 748.9682389817222, 756.1625928389922, 725.4585102901958, 627.8869598630017, 694.5574301827011, 757.5239391056945, 503.98565326926234, 509.56259815619075, 753.9556764391658, 741.3379657089774, 712.262717889269, 732.0389930414801, 516.8049526449925, 715.5880571199057, 742.9193814376, 746.2827874920337, 736.4353817383416, 519.1875622270436, 749.6107815842425, 720.3919650522923, 759.2102084352815, 758.3775008737515, 706.3648802868308, 615.6423706209825, 588.5207492647907, 708.7764489227039, 673.2699629165785, 671.2454314201401, 747.3304013590767, 678.6362098235452, 485.21690601281034, 699.3425791218865, 677.5425271908817, 721.0692840536794, 764.6681439529247, 564.1378089149324, 737.5548506428773, 702.3625312321434, 703.491547596208, 657.6479410863745, 705.9765077037556, 696.6766002646804, 638.5769561368752, 510.2634484618762, 703.7665747166014, 710.9386738209867, 755.902850659458, 704.1318442641904, 646.4506932111273, 504.0043690444743, 693.2386805562816, 705.1069591012579, 757.8428405309718, 759.1388843541881, 725.3819691807428, 697.9361505806818, 712.0885419708135, 698.5339104268046, 676.1069009790568, 512.5846303591977, 521.1064433027424, 602.9837264199617, 760.7108268887295, 726.9101310037809, 688.5008241755189, 486.9830636222097, 712.6877665554449, 709.4742883403806, 756.9617059699448, 698.3044679515037, 745.802239316088, 624.4958256455109, 724.1619481632002, 764.4316520202201, 521.0390897785602, 504.01727187840646, 733.473495193211, 656.424216816638, 512.1188256419143, 516.8577878077557, 491.0127539578767, 728.9868928828288, 737.5428331626272, 732.2193642757404, 673.8381223563255, 666.852213345212, 667.4629683821112, 664.6652860154762, 709.4821541991961, 727.4233014368195, 745.5432935788087, 737.2230077852356, 507.9521822951844, 686.9853904024285, 700.5752666133383, 713.920946321091, 698.8442174602411, 702.1672407701118, 676.6839358704002, 489.17411807290773, 696.3316963447204, 706.6603063851055, 739.0916863241144, 745.6835068911436, 750.2921116928236, 733.8240227734365, 502.4089595456823, 742.5246334850037, 743.2603139055199, 743.0939565547314, 707.5429939084893, 519.9700776801448, 722.2226209081867, 743.9779613426615, 744.0403578523832, 702.4622709878282, 716.8688438395827, 493.04435766243176, 730.3251597903193, 732.1756856972556, 724.5505084656961, 499.307054332593, 529.9493108087081, 704.3526296849169, 711.9167070165873, 720.779953846315, 707.475649999376, 495.4487311945201, 480.5157601150272, 708.7215826346032, 736.2168265029169, 709.5652042521318, 705.6202830172039, 691.9846503495605, 726.8553250322738, 703.0886119316285, 699.5432550008194, 695.4103595797288, 525.1535585834541, 695.7716840637912, 696.9546251567074, 700.0402104450446, 732.1341929717908, 661.2947794122932, 666.050261811781, 678.6638578682699, 673.7810571893111, 684.3650509277122, 687.3477272722198, 479.54356392498795, 676.1486825291091, 716.7745371698381, 536.6043340383798, 491.8841067315539, 488.11998781320153, 710.3176895509024, 711.167743777825, 692.3198802685281, 697.8314066410223, 699.343337392115, 516.5954674709524, 517.4878683511374]
Elapsed: 0.19754862141813717~0.03114311001616167
Time per graph: 0.001531384662156102~0.00024141945748962535
Speed: 666.9778372178992~89.01477257079523
Total Time: 0.2502
best val loss: 0.17608059503773385 test_score: 0.8837

Testing...
Test loss: 0.6635 score: 0.8682 time: 0.19s
test Score 0.8682
Epoch Time List: [0.6421636352315545, 0.6376376689877361, 0.6347821648232639, 0.6749577689915895, 0.7327770818956196, 0.6372906931210309, 0.6769602941349149, 0.60661904909648, 0.5986411243211478, 0.6284742660354823, 0.6696629300713539, 0.6127120780292898, 0.6684479489922523, 0.6243134569376707, 0.693208635551855, 0.7037727469578385, 0.6353238711599261, 0.6488673519343138, 0.6107555909547955, 0.6025967199821025, 0.6983441519550979, 0.6146188830025494, 0.6011120402254164, 0.6071335619781166, 0.6059105119202286, 0.5989531793165952, 0.8409325410611928, 0.713565019890666, 0.6341685769148171, 0.8835169360972941, 0.8895649330224842, 0.7792185200378299, 0.6271086784545332, 0.6218221788294613, 0.6353310409467667, 0.6802317290566862, 0.892087183194235, 0.8284281897358596, 0.6305841880384833, 0.635555423097685, 0.6342155693564564, 0.6360159728210419, 0.744021327001974, 0.6934842101763934, 0.6072305766865611, 0.7457802731078118, 0.824458017013967, 0.9846647218801081, 0.8253581968601793, 0.7113312359433621, 0.5997320858296007, 0.616832478903234, 0.6251789128873497, 0.6426537572406232, 0.6505569519940764, 0.7131162472069263, 0.6487868749536574, 0.7719614051748067, 0.8732066822703928, 0.8886971327010542, 0.8143330339808017, 0.6069000933784992, 0.5981512099970132, 0.7563648710492998, 0.8734207330271602, 0.8065842560026795, 0.6107647179160267, 0.6332573569379747, 0.6069363038986921, 0.6065964249428362, 0.6244919809978455, 0.8072861761320382, 0.712404313031584, 0.6117468120064586, 0.7904154108837247, 0.8816457388456911, 0.7004174720495939, 0.6056740330532193, 0.8107360580470413, 0.8839550609700382, 0.8767378102056682, 0.6105905210133642, 0.6260883372742683, 0.6401293405797333, 0.6511246038135141, 0.7983039440587163, 0.6529384281020612, 0.6109164271038026, 0.63776197982952, 0.6462766109034419, 0.6864231640938669, 0.791377189103514, 0.6068599859718233, 0.5876305608544499, 0.6532423959579319, 0.6535533717833459, 0.7644841279834509, 0.6533192088827491, 0.6216716549824923, 0.7609188209753484, 0.881825947901234, 0.6959277910646051, 0.5972939559724182, 0.610103587852791, 0.6089651598595083, 0.8113707010634243, 0.8289844247046858, 0.6205518732313067, 0.6100021318998188, 0.616467890329659, 0.7096995208412409, 0.7957345577888191, 0.6140806479379535, 0.5975700449198484, 0.5940972499083728, 0.619359944248572, 0.6591601499821991, 0.7354638478718698, 0.6317201310303062, 0.6426003510132432, 0.6536789073143154, 0.6007164991460741, 0.6411610869690776, 0.7649151298683137, 0.6361806911882013, 0.6434625496622175, 0.6031008092686534, 0.5937262333463877, 0.7226679380983114, 0.6102280712220818, 0.7195046218112111, 0.6330400458537042, 0.643298780079931, 0.7153286130633205, 0.6434841719456017, 0.6660190410912037, 0.918470935896039, 0.6436123729217798, 0.6381792107131332, 0.6172451789025217, 0.6298211188986897, 0.6583035299554467, 0.7815484099555761, 0.7784271908458322, 0.6397619438357651, 0.5986714821774513, 0.599218871910125, 0.6278898869641125, 0.6406251038424671, 0.7070430000312626, 0.6383038468193263, 0.6488507450558245, 0.867982380092144, 0.8712407480925322, 0.7726480490528047, 0.6024569345172495, 0.620835242094472, 0.6314942869357765, 0.8854031530208886, 0.6626819947268814, 0.6295180700253695, 0.5837912496645004, 0.6159224978182465, 0.6128300139680505, 0.7662526529747993, 0.6149343647994101, 0.6046041289810091, 0.7453113808296621, 0.8716567200608552, 0.7320824698545039, 0.6339042212348431, 0.8556400150991976, 0.8799091407563537, 0.8887908812612295, 0.6264352810103446, 0.6184740939643234, 0.6174445399083197, 0.6515414749737829, 0.6477790533099324, 0.6517657225485891, 0.6570326858200133, 0.7022780252154917, 0.6153155830688775, 0.6147483119275421, 0.6179750340525061, 0.7073602639138699, 0.8170477240346372, 0.6220514629967511, 0.6221255669370294, 0.625462899915874, 0.6479472320061177, 0.6768860009033233, 0.9082401869818568, 0.6743556940928102, 0.6336579909548163, 0.6081463401205838, 0.6095313041005284, 0.6108137618284672, 0.6065173901151866, 0.7303514040540904, 0.6265119460877031, 0.6173051937948912, 0.6153114777989686, 0.6228014396037906, 0.7070346218533814, 0.7600782469380647, 0.6176635320298374, 0.6171265051234514, 0.6388158521149307, 0.6612847382202744, 0.735343508888036, 0.6225111149251461, 0.6200548477936536, 0.6113127637654543, 0.7243289288599044, 0.8965882840566337, 0.7933253520168364, 0.7062912208493799, 0.627127654151991, 0.6389275647234172, 0.817690584808588, 0.9133732228074223, 0.7218783048447222, 0.6026122088078409, 0.6236832740250975, 0.632180365268141, 0.6475933478213847, 0.6551336266566068, 0.699165927246213, 0.6296772758942097, 0.6369424718432128, 0.7095465529710054, 0.6710772276856005, 0.6399033258203417, 0.633300710003823, 0.6053304660599679, 0.6477532910648733, 0.7370213600806892, 0.6456991520244628, 0.6478800640907139, 0.6421952550299466, 0.643898748094216, 0.7979205001611263, 0.6515111788176, 0.638760935747996, 0.7075647909659892, 0.9052087711170316, 0.9071920001879334, 0.7823877278715372, 0.6382705541327596, 0.6403593318536878, 0.6379118978511542, 0.6504314837511629, 0.8623707499355078, 0.8838891820050776]
Total Epoch List: [255]
Total Time List: [0.25019272416830063]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dc2bc7730>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0290;  Loss pred: 2.0290; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 2.0278;  Loss pred: 2.0278; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 2.0004;  Loss pred: 2.0004; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 1.9843;  Loss pred: 1.9843; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 1.9632;  Loss pred: 1.9632; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4961 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 1.9199;  Loss pred: 1.9199; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4961 time: 0.24s
Epoch 7/1000, LR 0.000165
Train loss: 1.8865;  Loss pred: 1.8865; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 1.8544;  Loss pred: 1.8544; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.19s
Epoch 9/1000, LR 0.000225
Train loss: 1.7937;  Loss pred: 1.7937; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 1.7520;  Loss pred: 1.7520; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.6942;  Loss pred: 1.6942; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 1.6315;  Loss pred: 1.6315; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 1.5766;  Loss pred: 1.5766; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 1.5382;  Loss pred: 1.5382; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 1.4962;  Loss pred: 1.4962; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 1.4640;  Loss pred: 1.4640; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 1.4132;  Loss pred: 1.4132; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4961 time: 0.23s
Epoch 18/1000, LR 0.000285
Train loss: 1.3946;  Loss pred: 1.3946; Loss self: 0.0000; time: 0.26s
Val loss: 0.6901 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4961 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 1.3597;  Loss pred: 1.3597; Loss self: 0.0000; time: 0.27s
Val loss: 0.6897 score: 0.5116 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4961 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 1.3276;  Loss pred: 1.3276; Loss self: 0.0000; time: 0.26s
Val loss: 0.6894 score: 0.5271 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4961 time: 0.21s
Epoch 21/1000, LR 0.000285
Train loss: 1.2974;  Loss pred: 1.2974; Loss self: 0.0000; time: 0.28s
Val loss: 0.6890 score: 0.5426 time: 0.18s
Test loss: 0.6899 score: 0.5116 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 1.2679;  Loss pred: 1.2679; Loss self: 0.0000; time: 0.26s
Val loss: 0.6885 score: 0.5426 time: 0.25s
Test loss: 0.6896 score: 0.5194 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.2539;  Loss pred: 1.2539; Loss self: 0.0000; time: 0.26s
Val loss: 0.6881 score: 0.5426 time: 0.17s
Test loss: 0.6892 score: 0.5194 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.2274;  Loss pred: 1.2274; Loss self: 0.0000; time: 0.25s
Val loss: 0.6876 score: 0.5426 time: 0.17s
Test loss: 0.6887 score: 0.5349 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.2111;  Loss pred: 1.2111; Loss self: 0.0000; time: 0.26s
Val loss: 0.6871 score: 0.5659 time: 0.17s
Test loss: 0.6883 score: 0.5581 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 1.1886;  Loss pred: 1.1886; Loss self: 0.0000; time: 0.26s
Val loss: 0.6865 score: 0.5814 time: 0.19s
Test loss: 0.6878 score: 0.5581 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.1715;  Loss pred: 1.1715; Loss self: 0.0000; time: 0.34s
Val loss: 0.6859 score: 0.6202 time: 0.25s
Test loss: 0.6873 score: 0.5581 time: 0.24s
Epoch 28/1000, LR 0.000285
Train loss: 1.1615;  Loss pred: 1.1615; Loss self: 0.0000; time: 0.28s
Val loss: 0.6853 score: 0.6357 time: 0.17s
Test loss: 0.6867 score: 0.5581 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.1467;  Loss pred: 1.1467; Loss self: 0.0000; time: 0.26s
Val loss: 0.6846 score: 0.6589 time: 0.18s
Test loss: 0.6861 score: 0.5814 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 1.1352;  Loss pred: 1.1352; Loss self: 0.0000; time: 0.26s
Val loss: 0.6839 score: 0.6822 time: 0.18s
Test loss: 0.6855 score: 0.6279 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 1.1226;  Loss pred: 1.1226; Loss self: 0.0000; time: 0.26s
Val loss: 0.6831 score: 0.7209 time: 0.17s
Test loss: 0.6849 score: 0.6822 time: 0.19s
Epoch 32/1000, LR 0.000285
Train loss: 1.1105;  Loss pred: 1.1105; Loss self: 0.0000; time: 0.27s
Val loss: 0.6823 score: 0.7674 time: 0.25s
Test loss: 0.6842 score: 0.7287 time: 0.25s
Epoch 33/1000, LR 0.000285
Train loss: 1.0999;  Loss pred: 1.0999; Loss self: 0.0000; time: 0.37s
Val loss: 0.6814 score: 0.8140 time: 0.25s
Test loss: 0.6834 score: 0.7519 time: 0.24s
Epoch 34/1000, LR 0.000285
Train loss: 1.0905;  Loss pred: 1.0905; Loss self: 0.0000; time: 0.28s
Val loss: 0.6805 score: 0.8372 time: 0.17s
Test loss: 0.6826 score: 0.7674 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 1.0843;  Loss pred: 1.0843; Loss self: 0.0000; time: 0.27s
Val loss: 0.6796 score: 0.8372 time: 0.18s
Test loss: 0.6818 score: 0.7752 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 1.0751;  Loss pred: 1.0751; Loss self: 0.0000; time: 0.27s
Val loss: 0.6786 score: 0.8372 time: 0.17s
Test loss: 0.6810 score: 0.8140 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.0693;  Loss pred: 1.0693; Loss self: 0.0000; time: 0.29s
Val loss: 0.6775 score: 0.8527 time: 0.25s
Test loss: 0.6801 score: 0.8217 time: 0.25s
Epoch 38/1000, LR 0.000284
Train loss: 1.0607;  Loss pred: 1.0607; Loss self: 0.0000; time: 0.37s
Val loss: 0.6764 score: 0.8837 time: 0.23s
Test loss: 0.6791 score: 0.8450 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 1.0548;  Loss pred: 1.0548; Loss self: 0.0000; time: 0.27s
Val loss: 0.6753 score: 0.8760 time: 0.18s
Test loss: 0.6781 score: 0.8682 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 1.0483;  Loss pred: 1.0483; Loss self: 0.0000; time: 0.27s
Val loss: 0.6741 score: 0.8915 time: 0.17s
Test loss: 0.6770 score: 0.8837 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 1.0444;  Loss pred: 1.0444; Loss self: 0.0000; time: 0.26s
Val loss: 0.6728 score: 0.8915 time: 0.17s
Test loss: 0.6759 score: 0.8837 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 1.0386;  Loss pred: 1.0386; Loss self: 0.0000; time: 0.31s
Val loss: 0.6714 score: 0.8915 time: 0.18s
Test loss: 0.6748 score: 0.8915 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 1.0327;  Loss pred: 1.0327; Loss self: 0.0000; time: 0.31s
Val loss: 0.6700 score: 0.8992 time: 0.23s
Test loss: 0.6735 score: 0.8992 time: 0.25s
Epoch 44/1000, LR 0.000284
Train loss: 1.0276;  Loss pred: 1.0276; Loss self: 0.0000; time: 0.30s
Val loss: 0.6686 score: 0.8992 time: 0.22s
Test loss: 0.6722 score: 0.8992 time: 0.22s
Epoch 45/1000, LR 0.000284
Train loss: 1.0228;  Loss pred: 1.0228; Loss self: 0.0000; time: 0.25s
Val loss: 0.6670 score: 0.8992 time: 0.17s
Test loss: 0.6709 score: 0.9070 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 1.0183;  Loss pred: 1.0183; Loss self: 0.0000; time: 0.26s
Val loss: 0.6653 score: 0.9070 time: 0.20s
Test loss: 0.6694 score: 0.9070 time: 0.21s
Epoch 47/1000, LR 0.000284
Train loss: 1.0150;  Loss pred: 1.0150; Loss self: 0.0000; time: 0.32s
Val loss: 0.6635 score: 0.8992 time: 0.23s
Test loss: 0.6679 score: 0.9070 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 1.0106;  Loss pred: 1.0106; Loss self: 0.0000; time: 0.37s
Val loss: 0.6617 score: 0.9070 time: 0.19s
Test loss: 0.6664 score: 0.9070 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 1.0075;  Loss pred: 1.0075; Loss self: 0.0000; time: 0.26s
Val loss: 0.6598 score: 0.9070 time: 0.18s
Test loss: 0.6647 score: 0.9070 time: 0.27s
Epoch 50/1000, LR 0.000284
Train loss: 1.0021;  Loss pred: 1.0021; Loss self: 0.0000; time: 0.26s
Val loss: 0.6578 score: 0.8992 time: 0.18s
Test loss: 0.6630 score: 0.9070 time: 0.18s
Epoch 51/1000, LR 0.000284
Train loss: 0.9985;  Loss pred: 0.9985; Loss self: 0.0000; time: 0.29s
Val loss: 0.6557 score: 0.9225 time: 0.19s
Test loss: 0.6612 score: 0.9147 time: 0.25s
Epoch 52/1000, LR 0.000284
Train loss: 0.9955;  Loss pred: 0.9955; Loss self: 0.0000; time: 0.37s
Val loss: 0.6536 score: 0.9225 time: 0.25s
Test loss: 0.6593 score: 0.9147 time: 0.25s
Epoch 53/1000, LR 0.000284
Train loss: 0.9911;  Loss pred: 0.9911; Loss self: 0.0000; time: 0.37s
Val loss: 0.6513 score: 0.9302 time: 0.26s
Test loss: 0.6573 score: 0.9147 time: 0.23s
Epoch 54/1000, LR 0.000284
Train loss: 0.9886;  Loss pred: 0.9886; Loss self: 0.0000; time: 0.34s
Val loss: 0.6489 score: 0.9302 time: 0.19s
Test loss: 0.6552 score: 0.9070 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.9846;  Loss pred: 0.9846; Loss self: 0.0000; time: 0.26s
Val loss: 0.6465 score: 0.9225 time: 0.18s
Test loss: 0.6530 score: 0.9070 time: 0.18s
Epoch 56/1000, LR 0.000284
Train loss: 0.9793;  Loss pred: 0.9793; Loss self: 0.0000; time: 0.26s
Val loss: 0.6439 score: 0.9302 time: 0.17s
Test loss: 0.6507 score: 0.8992 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 0.9761;  Loss pred: 0.9761; Loss self: 0.0000; time: 0.26s
Val loss: 0.6411 score: 0.9302 time: 0.18s
Test loss: 0.6483 score: 0.8992 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 0.9743;  Loss pred: 0.9743; Loss self: 0.0000; time: 0.27s
Val loss: 0.6383 score: 0.9302 time: 0.20s
Test loss: 0.6458 score: 0.8992 time: 0.18s
Epoch 59/1000, LR 0.000283
Train loss: 0.9709;  Loss pred: 0.9709; Loss self: 0.0000; time: 0.38s
Val loss: 0.6352 score: 0.9302 time: 0.25s
Test loss: 0.6432 score: 0.8992 time: 0.25s
Epoch 60/1000, LR 0.000283
Train loss: 0.9676;  Loss pred: 0.9676; Loss self: 0.0000; time: 0.29s
Val loss: 0.6321 score: 0.9302 time: 0.18s
Test loss: 0.6404 score: 0.8992 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 0.9633;  Loss pred: 0.9633; Loss self: 0.0000; time: 0.26s
Val loss: 0.6288 score: 0.9302 time: 0.18s
Test loss: 0.6376 score: 0.8992 time: 0.18s
Epoch 62/1000, LR 0.000283
Train loss: 0.9589;  Loss pred: 0.9589; Loss self: 0.0000; time: 0.26s
Val loss: 0.6255 score: 0.9302 time: 0.18s
Test loss: 0.6347 score: 0.8992 time: 0.18s
Epoch 63/1000, LR 0.000283
Train loss: 0.9570;  Loss pred: 0.9570; Loss self: 0.0000; time: 0.27s
Val loss: 0.6220 score: 0.9302 time: 0.19s
Test loss: 0.6316 score: 0.8992 time: 0.20s
Epoch 64/1000, LR 0.000283
Train loss: 0.9530;  Loss pred: 0.9530; Loss self: 0.0000; time: 0.28s
Val loss: 0.6184 score: 0.9302 time: 0.20s
Test loss: 0.6284 score: 0.9070 time: 0.19s
Epoch 65/1000, LR 0.000283
Train loss: 0.9492;  Loss pred: 0.9492; Loss self: 0.0000; time: 0.27s
Val loss: 0.6148 score: 0.9225 time: 0.19s
Test loss: 0.6252 score: 0.9070 time: 0.19s
Epoch 66/1000, LR 0.000283
Train loss: 0.9457;  Loss pred: 0.9457; Loss self: 0.0000; time: 0.31s
Val loss: 0.6109 score: 0.9225 time: 0.20s
Test loss: 0.6218 score: 0.9070 time: 0.27s
Epoch 67/1000, LR 0.000283
Train loss: 0.9420;  Loss pred: 0.9420; Loss self: 0.0000; time: 0.28s
Val loss: 0.6069 score: 0.9302 time: 0.19s
Test loss: 0.6183 score: 0.9070 time: 0.19s
Epoch 68/1000, LR 0.000283
Train loss: 0.9378;  Loss pred: 0.9378; Loss self: 0.0000; time: 0.27s
Val loss: 0.6027 score: 0.9302 time: 0.20s
Test loss: 0.6146 score: 0.9070 time: 0.19s
Epoch 69/1000, LR 0.000283
Train loss: 0.9340;  Loss pred: 0.9340; Loss self: 0.0000; time: 0.27s
Val loss: 0.5985 score: 0.9302 time: 0.19s
Test loss: 0.6109 score: 0.9070 time: 0.29s
Epoch 70/1000, LR 0.000283
Train loss: 0.9317;  Loss pred: 0.9317; Loss self: 0.0000; time: 0.27s
Val loss: 0.5941 score: 0.9302 time: 0.19s
Test loss: 0.6070 score: 0.8992 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 0.9273;  Loss pred: 0.9273; Loss self: 0.0000; time: 0.27s
Val loss: 0.5895 score: 0.9302 time: 0.18s
Test loss: 0.6030 score: 0.9070 time: 0.26s
Epoch 72/1000, LR 0.000282
Train loss: 0.9235;  Loss pred: 0.9235; Loss self: 0.0000; time: 0.37s
Val loss: 0.5849 score: 0.9225 time: 0.21s
Test loss: 0.5989 score: 0.9070 time: 0.23s
Epoch 73/1000, LR 0.000282
Train loss: 0.9189;  Loss pred: 0.9189; Loss self: 0.0000; time: 0.26s
Val loss: 0.5801 score: 0.9225 time: 0.18s
Test loss: 0.5946 score: 0.9070 time: 0.18s
Epoch 74/1000, LR 0.000282
Train loss: 0.9148;  Loss pred: 0.9148; Loss self: 0.0000; time: 0.26s
Val loss: 0.5751 score: 0.9225 time: 0.18s
Test loss: 0.5903 score: 0.9070 time: 0.19s
Epoch 75/1000, LR 0.000282
Train loss: 0.9119;  Loss pred: 0.9119; Loss self: 0.0000; time: 0.26s
Val loss: 0.5701 score: 0.9302 time: 0.18s
Test loss: 0.5858 score: 0.9070 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9079;  Loss pred: 0.9079; Loss self: 0.0000; time: 0.27s
Val loss: 0.5650 score: 0.9147 time: 0.19s
Test loss: 0.5812 score: 0.9070 time: 0.18s
Epoch 77/1000, LR 0.000282
Train loss: 0.9038;  Loss pred: 0.9038; Loss self: 0.0000; time: 0.31s
Val loss: 0.5598 score: 0.9147 time: 0.24s
Test loss: 0.5766 score: 0.9070 time: 0.21s
Epoch 78/1000, LR 0.000282
Train loss: 0.8999;  Loss pred: 0.8999; Loss self: 0.0000; time: 0.27s
Val loss: 0.5544 score: 0.9225 time: 0.19s
Test loss: 0.5718 score: 0.8915 time: 0.18s
Epoch 79/1000, LR 0.000282
Train loss: 0.8953;  Loss pred: 0.8953; Loss self: 0.0000; time: 0.27s
Val loss: 0.5489 score: 0.9225 time: 0.18s
Test loss: 0.5668 score: 0.8915 time: 0.17s
Epoch 80/1000, LR 0.000282
Train loss: 0.8907;  Loss pred: 0.8907; Loss self: 0.0000; time: 0.26s
Val loss: 0.5431 score: 0.9147 time: 0.18s
Test loss: 0.5617 score: 0.8992 time: 0.18s
Epoch 81/1000, LR 0.000281
Train loss: 0.8869;  Loss pred: 0.8869; Loss self: 0.0000; time: 0.26s
Val loss: 0.5372 score: 0.9147 time: 0.18s
Test loss: 0.5564 score: 0.8992 time: 0.18s
Epoch 82/1000, LR 0.000281
Train loss: 0.8818;  Loss pred: 0.8818; Loss self: 0.0000; time: 0.26s
Val loss: 0.5312 score: 0.9147 time: 0.21s
Test loss: 0.5510 score: 0.9070 time: 0.18s
Epoch 83/1000, LR 0.000281
Train loss: 0.8774;  Loss pred: 0.8774; Loss self: 0.0000; time: 0.27s
Val loss: 0.5251 score: 0.9147 time: 0.24s
Test loss: 0.5455 score: 0.8992 time: 0.19s
Epoch 84/1000, LR 0.000281
Train loss: 0.8726;  Loss pred: 0.8726; Loss self: 0.0000; time: 0.26s
Val loss: 0.5189 score: 0.9225 time: 0.19s
Test loss: 0.5399 score: 0.8915 time: 0.18s
Epoch 85/1000, LR 0.000281
Train loss: 0.8683;  Loss pred: 0.8683; Loss self: 0.0000; time: 0.26s
Val loss: 0.5127 score: 0.9225 time: 0.18s
Test loss: 0.5344 score: 0.8915 time: 0.18s
Epoch 86/1000, LR 0.000281
Train loss: 0.8652;  Loss pred: 0.8652; Loss self: 0.0000; time: 0.27s
Val loss: 0.5064 score: 0.9225 time: 0.18s
Test loss: 0.5287 score: 0.8915 time: 0.18s
Epoch 87/1000, LR 0.000281
Train loss: 0.8597;  Loss pred: 0.8597; Loss self: 0.0000; time: 0.27s
Val loss: 0.5000 score: 0.9225 time: 0.18s
Test loss: 0.5229 score: 0.8915 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.8541;  Loss pred: 0.8541; Loss self: 0.0000; time: 0.26s
Val loss: 0.4934 score: 0.9225 time: 0.18s
Test loss: 0.5170 score: 0.8915 time: 0.19s
Epoch 89/1000, LR 0.000281
Train loss: 0.8501;  Loss pred: 0.8501; Loss self: 0.0000; time: 0.27s
Val loss: 0.4868 score: 0.9225 time: 0.26s
Test loss: 0.5111 score: 0.8915 time: 0.26s
Epoch 90/1000, LR 0.000281
Train loss: 0.8456;  Loss pred: 0.8456; Loss self: 0.0000; time: 0.28s
Val loss: 0.4802 score: 0.9225 time: 0.18s
Test loss: 0.5052 score: 0.8915 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8407;  Loss pred: 0.8407; Loss self: 0.0000; time: 0.26s
Val loss: 0.4736 score: 0.9147 time: 0.18s
Test loss: 0.4992 score: 0.8992 time: 0.18s
Epoch 92/1000, LR 0.000280
Train loss: 0.8362;  Loss pred: 0.8362; Loss self: 0.0000; time: 0.27s
Val loss: 0.4671 score: 0.9147 time: 0.18s
Test loss: 0.4934 score: 0.9070 time: 0.18s
Epoch 93/1000, LR 0.000280
Train loss: 0.8309;  Loss pred: 0.8309; Loss self: 0.0000; time: 0.26s
Val loss: 0.4606 score: 0.9147 time: 0.17s
Test loss: 0.4875 score: 0.9070 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8275;  Loss pred: 0.8275; Loss self: 0.0000; time: 0.27s
Val loss: 0.4541 score: 0.9147 time: 0.20s
Test loss: 0.4816 score: 0.8992 time: 0.18s
Epoch 95/1000, LR 0.000280
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 0.32s
Val loss: 0.4477 score: 0.9225 time: 0.19s
Test loss: 0.4758 score: 0.8915 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.8180;  Loss pred: 0.8180; Loss self: 0.0000; time: 0.27s
Val loss: 0.4413 score: 0.9225 time: 0.18s
Test loss: 0.4699 score: 0.8915 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8127;  Loss pred: 0.8127; Loss self: 0.0000; time: 0.27s
Val loss: 0.4349 score: 0.9225 time: 0.18s
Test loss: 0.4641 score: 0.8992 time: 0.19s
Epoch 98/1000, LR 0.000280
Train loss: 0.8070;  Loss pred: 0.8070; Loss self: 0.0000; time: 0.34s
Val loss: 0.4285 score: 0.9225 time: 0.18s
Test loss: 0.4583 score: 0.8915 time: 0.18s
Epoch 99/1000, LR 0.000279
Train loss: 0.8026;  Loss pred: 0.8026; Loss self: 0.0000; time: 0.27s
Val loss: 0.4221 score: 0.9225 time: 0.18s
Test loss: 0.4526 score: 0.8992 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 0.31s
Val loss: 0.4158 score: 0.9147 time: 0.19s
Test loss: 0.4468 score: 0.8992 time: 0.18s
Epoch 101/1000, LR 0.000279
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.26s
Val loss: 0.4096 score: 0.9225 time: 0.18s
Test loss: 0.4411 score: 0.8992 time: 0.18s
Epoch 102/1000, LR 0.000279
Train loss: 0.7899;  Loss pred: 0.7899; Loss self: 0.0000; time: 0.26s
Val loss: 0.4034 score: 0.9147 time: 0.18s
Test loss: 0.4354 score: 0.9070 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.7852;  Loss pred: 0.7852; Loss self: 0.0000; time: 0.26s
Val loss: 0.3972 score: 0.9147 time: 0.18s
Test loss: 0.4298 score: 0.9070 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.27s
Val loss: 0.3911 score: 0.9147 time: 0.18s
Test loss: 0.4243 score: 0.9070 time: 0.19s
Epoch 105/1000, LR 0.000279
Train loss: 0.7776;  Loss pred: 0.7776; Loss self: 0.0000; time: 0.35s
Val loss: 0.3852 score: 0.9147 time: 0.19s
Test loss: 0.4188 score: 0.9070 time: 0.19s
Epoch 106/1000, LR 0.000279
Train loss: 0.7720;  Loss pred: 0.7720; Loss self: 0.0000; time: 0.27s
Val loss: 0.3793 score: 0.9147 time: 0.19s
Test loss: 0.4134 score: 0.9070 time: 0.19s
Epoch 107/1000, LR 0.000278
Train loss: 0.7692;  Loss pred: 0.7692; Loss self: 0.0000; time: 0.27s
Val loss: 0.3735 score: 0.9147 time: 0.19s
Test loss: 0.4081 score: 0.9070 time: 0.19s
Epoch 108/1000, LR 0.000278
Train loss: 0.7652;  Loss pred: 0.7652; Loss self: 0.0000; time: 0.26s
Val loss: 0.3678 score: 0.9225 time: 0.17s
Test loss: 0.4028 score: 0.9070 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.7594;  Loss pred: 0.7594; Loss self: 0.0000; time: 0.25s
Val loss: 0.3623 score: 0.9225 time: 0.17s
Test loss: 0.3977 score: 0.9070 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.7565;  Loss pred: 0.7565; Loss self: 0.0000; time: 0.26s
Val loss: 0.3568 score: 0.9225 time: 0.18s
Test loss: 0.3926 score: 0.9070 time: 0.18s
Epoch 111/1000, LR 0.000278
Train loss: 0.7518;  Loss pred: 0.7518; Loss self: 0.0000; time: 0.33s
Val loss: 0.3515 score: 0.9225 time: 0.17s
Test loss: 0.3876 score: 0.8992 time: 0.18s
Epoch 112/1000, LR 0.000278
Train loss: 0.7494;  Loss pred: 0.7494; Loss self: 0.0000; time: 0.26s
Val loss: 0.3460 score: 0.9225 time: 0.18s
Test loss: 0.3827 score: 0.9070 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7454;  Loss pred: 0.7454; Loss self: 0.0000; time: 0.26s
Val loss: 0.3408 score: 0.9225 time: 0.18s
Test loss: 0.3779 score: 0.9070 time: 0.17s
Epoch 114/1000, LR 0.000277
Train loss: 0.7405;  Loss pred: 0.7405; Loss self: 0.0000; time: 0.26s
Val loss: 0.3358 score: 0.9225 time: 0.17s
Test loss: 0.3732 score: 0.9070 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.7374;  Loss pred: 0.7374; Loss self: 0.0000; time: 0.26s
Val loss: 0.3309 score: 0.9225 time: 0.17s
Test loss: 0.3686 score: 0.9070 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7311;  Loss pred: 0.7311; Loss self: 0.0000; time: 0.27s
Val loss: 0.3261 score: 0.9225 time: 0.18s
Test loss: 0.3641 score: 0.8992 time: 0.26s
Epoch 117/1000, LR 0.000277
Train loss: 0.7316;  Loss pred: 0.7316; Loss self: 0.0000; time: 0.27s
Val loss: 0.3215 score: 0.9225 time: 0.19s
Test loss: 0.3597 score: 0.8992 time: 0.19s
Epoch 118/1000, LR 0.000277
Train loss: 0.7272;  Loss pred: 0.7272; Loss self: 0.0000; time: 0.28s
Val loss: 0.3169 score: 0.9225 time: 0.18s
Test loss: 0.3554 score: 0.8992 time: 0.18s
Epoch 119/1000, LR 0.000277
Train loss: 0.7215;  Loss pred: 0.7215; Loss self: 0.0000; time: 0.28s
Val loss: 0.3124 score: 0.9225 time: 0.26s
Test loss: 0.3511 score: 0.8992 time: 0.26s
Epoch 120/1000, LR 0.000277
Train loss: 0.7189;  Loss pred: 0.7189; Loss self: 0.0000; time: 0.37s
Val loss: 0.3078 score: 0.9225 time: 0.27s
Test loss: 0.3469 score: 0.8992 time: 0.26s
Epoch 121/1000, LR 0.000276
Train loss: 0.7147;  Loss pred: 0.7147; Loss self: 0.0000; time: 0.40s
Val loss: 0.3034 score: 0.9225 time: 0.19s
Test loss: 0.3427 score: 0.9070 time: 0.18s
Epoch 122/1000, LR 0.000276
Train loss: 0.7130;  Loss pred: 0.7130; Loss self: 0.0000; time: 0.25s
Val loss: 0.2991 score: 0.9225 time: 0.18s
Test loss: 0.3387 score: 0.9147 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.26s
Val loss: 0.2949 score: 0.9225 time: 0.17s
Test loss: 0.3348 score: 0.9147 time: 0.17s
Epoch 124/1000, LR 0.000276
Train loss: 0.7069;  Loss pred: 0.7069; Loss self: 0.0000; time: 0.26s
Val loss: 0.2909 score: 0.9225 time: 0.18s
Test loss: 0.3309 score: 0.9147 time: 0.17s
Epoch 125/1000, LR 0.000276
Train loss: 0.7040;  Loss pred: 0.7040; Loss self: 0.0000; time: 0.25s
Val loss: 0.2872 score: 0.9225 time: 0.18s
Test loss: 0.3271 score: 0.9070 time: 0.18s
Epoch 126/1000, LR 0.000276
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.35s
Val loss: 0.2834 score: 0.9225 time: 0.21s
Test loss: 0.3234 score: 0.9070 time: 0.18s
Epoch 127/1000, LR 0.000275
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.26s
Val loss: 0.2796 score: 0.9225 time: 0.19s
Test loss: 0.3198 score: 0.9070 time: 0.18s
Epoch 128/1000, LR 0.000275
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.26s
Val loss: 0.2759 score: 0.9225 time: 0.18s
Test loss: 0.3163 score: 0.9147 time: 0.17s
Epoch 129/1000, LR 0.000275
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.26s
Val loss: 0.2724 score: 0.9225 time: 0.19s
Test loss: 0.3129 score: 0.9147 time: 0.19s
Epoch 130/1000, LR 0.000275
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.28s
Val loss: 0.2689 score: 0.9225 time: 0.19s
Test loss: 0.3096 score: 0.9147 time: 0.29s
Epoch 131/1000, LR 0.000275
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.36s
Val loss: 0.2656 score: 0.9225 time: 0.25s
Test loss: 0.3064 score: 0.9147 time: 0.25s
Epoch 132/1000, LR 0.000275
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.37s
Val loss: 0.2624 score: 0.9225 time: 0.33s
Test loss: 0.3031 score: 0.9147 time: 0.22s
Epoch 133/1000, LR 0.000274
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.26s
Val loss: 0.2593 score: 0.9225 time: 0.18s
Test loss: 0.3000 score: 0.9147 time: 0.18s
Epoch 134/1000, LR 0.000274
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.27s
Val loss: 0.2565 score: 0.9225 time: 0.18s
Test loss: 0.2969 score: 0.9147 time: 0.18s
Epoch 135/1000, LR 0.000274
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.27s
Val loss: 0.2537 score: 0.9225 time: 0.17s
Test loss: 0.2940 score: 0.9225 time: 0.20s
Epoch 136/1000, LR 0.000274
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.26s
Val loss: 0.2509 score: 0.9225 time: 0.19s
Test loss: 0.2911 score: 0.9225 time: 0.19s
Epoch 137/1000, LR 0.000274
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.26s
Val loss: 0.2481 score: 0.9225 time: 0.18s
Test loss: 0.2882 score: 0.9147 time: 0.18s
Epoch 138/1000, LR 0.000274
Train loss: 0.6641;  Loss pred: 0.6641; Loss self: 0.0000; time: 0.35s
Val loss: 0.2453 score: 0.9225 time: 0.19s
Test loss: 0.2855 score: 0.9147 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.32s
Val loss: 0.2425 score: 0.9147 time: 0.20s
Test loss: 0.2830 score: 0.9225 time: 0.17s
Epoch 140/1000, LR 0.000273
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.27s
Val loss: 0.2399 score: 0.9147 time: 0.17s
Test loss: 0.2805 score: 0.9225 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.25s
Val loss: 0.2375 score: 0.9147 time: 0.17s
Test loss: 0.2777 score: 0.9225 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.26s
Val loss: 0.2352 score: 0.9147 time: 0.17s
Test loss: 0.2752 score: 0.9225 time: 0.18s
Epoch 143/1000, LR 0.000273
Train loss: 0.6510;  Loss pred: 0.6510; Loss self: 0.0000; time: 0.27s
Val loss: 0.2331 score: 0.9225 time: 0.17s
Test loss: 0.2727 score: 0.9225 time: 0.19s
Epoch 144/1000, LR 0.000272
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.26s
Val loss: 0.2308 score: 0.9225 time: 0.20s
Test loss: 0.2703 score: 0.9225 time: 0.18s
Epoch 145/1000, LR 0.000272
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.27s
Val loss: 0.2288 score: 0.9225 time: 0.20s
Test loss: 0.2679 score: 0.9225 time: 0.23s
Epoch 146/1000, LR 0.000272
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.27s
Val loss: 0.2267 score: 0.9225 time: 0.18s
Test loss: 0.2656 score: 0.9225 time: 0.18s
Epoch 147/1000, LR 0.000272
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.26s
Val loss: 0.2248 score: 0.9225 time: 0.18s
Test loss: 0.2634 score: 0.9225 time: 0.18s
Epoch 148/1000, LR 0.000272
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.27s
Val loss: 0.2228 score: 0.9225 time: 0.19s
Test loss: 0.2612 score: 0.9225 time: 0.19s
Epoch 149/1000, LR 0.000272
Train loss: 0.6377;  Loss pred: 0.6377; Loss self: 0.0000; time: 0.27s
Val loss: 0.2206 score: 0.9147 time: 0.20s
Test loss: 0.2589 score: 0.9225 time: 0.19s
Epoch 150/1000, LR 0.000271
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 0.27s
Val loss: 0.2185 score: 0.9147 time: 0.25s
Test loss: 0.2568 score: 0.9225 time: 0.19s
Epoch 151/1000, LR 0.000271
Train loss: 0.6339;  Loss pred: 0.6339; Loss self: 0.0000; time: 0.27s
Val loss: 0.2166 score: 0.9147 time: 0.20s
Test loss: 0.2547 score: 0.9225 time: 0.18s
Epoch 152/1000, LR 0.000271
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.27s
Val loss: 0.2147 score: 0.9147 time: 0.19s
Test loss: 0.2527 score: 0.9225 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 0.28s
Val loss: 0.2130 score: 0.9147 time: 0.19s
Test loss: 0.2507 score: 0.9225 time: 0.18s
Epoch 154/1000, LR 0.000271
Train loss: 0.6296;  Loss pred: 0.6296; Loss self: 0.0000; time: 0.27s
Val loss: 0.2114 score: 0.9147 time: 0.19s
Test loss: 0.2488 score: 0.9225 time: 0.18s
Epoch 155/1000, LR 0.000270
Train loss: 0.6257;  Loss pred: 0.6257; Loss self: 0.0000; time: 0.30s
Val loss: 0.2100 score: 0.9147 time: 0.24s
Test loss: 0.2469 score: 0.9225 time: 0.21s
Epoch 156/1000, LR 0.000270
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.28s
Val loss: 0.2089 score: 0.9225 time: 0.18s
Test loss: 0.2452 score: 0.9302 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.28s
Val loss: 0.2074 score: 0.9225 time: 0.18s
Test loss: 0.2434 score: 0.9302 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.28s
Val loss: 0.2062 score: 0.9225 time: 0.18s
Test loss: 0.2418 score: 0.9302 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.27s
Val loss: 0.2046 score: 0.9225 time: 0.18s
Test loss: 0.2400 score: 0.9302 time: 0.18s
Epoch 160/1000, LR 0.000269
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.26s
Val loss: 0.2030 score: 0.9147 time: 0.18s
Test loss: 0.2383 score: 0.9225 time: 0.20s
Epoch 161/1000, LR 0.000269
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.28s
Val loss: 0.2016 score: 0.9147 time: 0.25s
Test loss: 0.2367 score: 0.9225 time: 0.23s
Epoch 162/1000, LR 0.000269
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.28s
Val loss: 0.2001 score: 0.9147 time: 0.18s
Test loss: 0.2351 score: 0.9302 time: 0.18s
Epoch 163/1000, LR 0.000269
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.27s
Val loss: 0.1990 score: 0.9147 time: 0.18s
Test loss: 0.2337 score: 0.9302 time: 0.17s
Epoch 164/1000, LR 0.000269
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.27s
Val loss: 0.1979 score: 0.9147 time: 0.18s
Test loss: 0.2322 score: 0.9302 time: 0.18s
Epoch 165/1000, LR 0.000268
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.27s
Val loss: 0.1971 score: 0.9147 time: 0.17s
Test loss: 0.2310 score: 0.9380 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.29s
Val loss: 0.1959 score: 0.9147 time: 0.24s
Test loss: 0.2296 score: 0.9380 time: 0.18s
Epoch 167/1000, LR 0.000268
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.27s
Val loss: 0.1950 score: 0.9147 time: 0.18s
Test loss: 0.2284 score: 0.9380 time: 0.18s
Epoch 168/1000, LR 0.000268
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.26s
Val loss: 0.1938 score: 0.9147 time: 0.18s
Test loss: 0.2271 score: 0.9380 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.26s
Val loss: 0.1931 score: 0.9147 time: 0.17s
Test loss: 0.2261 score: 0.9380 time: 0.17s
Epoch 170/1000, LR 0.000267
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.26s
Val loss: 0.1923 score: 0.9225 time: 0.18s
Test loss: 0.2250 score: 0.9380 time: 0.17s
Epoch 171/1000, LR 0.000267
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.26s
Val loss: 0.1920 score: 0.9225 time: 0.18s
Test loss: 0.2241 score: 0.9380 time: 0.18s
Epoch 172/1000, LR 0.000267
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.26s
Val loss: 0.1909 score: 0.9225 time: 0.18s
Test loss: 0.2230 score: 0.9380 time: 0.18s
Epoch 173/1000, LR 0.000267
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.29s
Val loss: 0.1895 score: 0.9147 time: 0.18s
Test loss: 0.2217 score: 0.9380 time: 0.24s
Epoch 174/1000, LR 0.000266
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 0.33s
Val loss: 0.1887 score: 0.9147 time: 0.18s
Test loss: 0.2207 score: 0.9380 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.26s
Val loss: 0.1878 score: 0.9147 time: 0.20s
Test loss: 0.2197 score: 0.9380 time: 0.18s
Epoch 176/1000, LR 0.000266
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.25s
Val loss: 0.1871 score: 0.9147 time: 0.18s
Test loss: 0.2188 score: 0.9302 time: 0.18s
Epoch 177/1000, LR 0.000266
Train loss: 0.5927;  Loss pred: 0.5927; Loss self: 0.0000; time: 0.26s
Val loss: 0.1864 score: 0.9147 time: 0.18s
Test loss: 0.2179 score: 0.9302 time: 0.18s
Epoch 178/1000, LR 0.000265
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.30s
Val loss: 0.1856 score: 0.9147 time: 0.18s
Test loss: 0.2170 score: 0.9302 time: 0.23s
Epoch 179/1000, LR 0.000265
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 0.36s
Val loss: 0.1848 score: 0.9147 time: 0.25s
Test loss: 0.2161 score: 0.9302 time: 0.20s
Epoch 180/1000, LR 0.000265
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.27s
Val loss: 0.1841 score: 0.9225 time: 0.17s
Test loss: 0.2153 score: 0.9302 time: 0.18s
Epoch 181/1000, LR 0.000265
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.26s
Val loss: 0.1839 score: 0.9225 time: 0.18s
Test loss: 0.2147 score: 0.9302 time: 0.18s
Epoch 182/1000, LR 0.000265
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.26s
Val loss: 0.1835 score: 0.9302 time: 0.22s
Test loss: 0.2140 score: 0.9302 time: 0.18s
Epoch 183/1000, LR 0.000264
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.27s
Val loss: 0.1826 score: 0.9225 time: 0.19s
Test loss: 0.2132 score: 0.9302 time: 0.18s
Epoch 184/1000, LR 0.000264
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 0.27s
Val loss: 0.1824 score: 0.9302 time: 0.18s
Test loss: 0.2127 score: 0.9302 time: 0.19s
Epoch 185/1000, LR 0.000264
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.27s
Val loss: 0.1820 score: 0.9302 time: 0.21s
Test loss: 0.2120 score: 0.9302 time: 0.19s
Epoch 186/1000, LR 0.000264
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.34s
Val loss: 0.1819 score: 0.9302 time: 0.19s
Test loss: 0.2116 score: 0.9302 time: 0.19s
Epoch 187/1000, LR 0.000263
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.27s
Val loss: 0.1816 score: 0.9302 time: 0.19s
Test loss: 0.2111 score: 0.9302 time: 0.19s
Epoch 188/1000, LR 0.000263
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.27s
Val loss: 0.1810 score: 0.9302 time: 0.18s
Test loss: 0.2105 score: 0.9302 time: 0.19s
Epoch 189/1000, LR 0.000263
Train loss: 0.5771;  Loss pred: 0.5771; Loss self: 0.0000; time: 0.27s
Val loss: 0.1807 score: 0.9302 time: 0.18s
Test loss: 0.2100 score: 0.9302 time: 0.18s
Epoch 190/1000, LR 0.000263
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.27s
Val loss: 0.1797 score: 0.9225 time: 0.19s
Test loss: 0.2092 score: 0.9302 time: 0.18s
Epoch 191/1000, LR 0.000262
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.27s
Val loss: 0.1791 score: 0.9225 time: 0.17s
Test loss: 0.2086 score: 0.9302 time: 0.22s
Epoch 192/1000, LR 0.000262
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.27s
Val loss: 0.1781 score: 0.9225 time: 0.23s
Test loss: 0.2078 score: 0.9302 time: 0.19s
Epoch 193/1000, LR 0.000262
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.26s
Val loss: 0.1777 score: 0.9225 time: 0.17s
Test loss: 0.2073 score: 0.9302 time: 0.17s
Epoch 194/1000, LR 0.000262
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.25s
Val loss: 0.1777 score: 0.9225 time: 0.17s
Test loss: 0.2070 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.26s
Val loss: 0.1777 score: 0.9225 time: 0.17s
Test loss: 0.2067 score: 0.9302 time: 0.17s
Epoch 196/1000, LR 0.000261
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.26s
Val loss: 0.1781 score: 0.9302 time: 0.17s
Test loss: 0.2066 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 197/1000, LR 0.000261
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.26s
Val loss: 0.1785 score: 0.9302 time: 0.18s
Test loss: 0.2065 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 198/1000, LR 0.000261
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.36s
Val loss: 0.1780 score: 0.9302 time: 0.18s
Test loss: 0.2060 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.27s
Val loss: 0.1775 score: 0.9302 time: 0.18s
Test loss: 0.2056 score: 0.9302 time: 0.18s
Epoch 200/1000, LR 0.000260
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.26s
Val loss: 0.1769 score: 0.9302 time: 0.18s
Test loss: 0.2051 score: 0.9302 time: 0.17s
Epoch 201/1000, LR 0.000260
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.27s
Val loss: 0.1768 score: 0.9302 time: 0.18s
Test loss: 0.2049 score: 0.9302 time: 0.17s
Epoch 202/1000, LR 0.000260
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 0.26s
Val loss: 0.1769 score: 0.9302 time: 0.21s
Test loss: 0.2047 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.34s
Val loss: 0.1762 score: 0.9302 time: 0.17s
Test loss: 0.2042 score: 0.9302 time: 0.18s
Epoch 204/1000, LR 0.000259
Train loss: 0.5647;  Loss pred: 0.5647; Loss self: 0.0000; time: 0.28s
Val loss: 0.1751 score: 0.9225 time: 0.18s
Test loss: 0.2036 score: 0.9302 time: 0.18s
Epoch 205/1000, LR 0.000259
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.25s
Val loss: 0.1749 score: 0.9225 time: 0.17s
Test loss: 0.2033 score: 0.9302 time: 0.17s
Epoch 206/1000, LR 0.000259
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.28s
Val loss: 0.1751 score: 0.9302 time: 0.17s
Test loss: 0.2032 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.27s
Val loss: 0.1755 score: 0.9302 time: 0.18s
Test loss: 0.2033 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.33s
Val loss: 0.1757 score: 0.9302 time: 0.18s
Test loss: 0.2033 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.26s
Val loss: 0.1757 score: 0.9302 time: 0.17s
Test loss: 0.2032 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.26s
Val loss: 0.1756 score: 0.9302 time: 0.17s
Test loss: 0.2031 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.26s
Val loss: 0.1759 score: 0.9302 time: 0.17s
Test loss: 0.2033 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.27s
Val loss: 0.1746 score: 0.9302 time: 0.18s
Test loss: 0.2026 score: 0.9302 time: 0.19s
Epoch 213/1000, LR 0.000257
Train loss: 0.5578;  Loss pred: 0.5578; Loss self: 0.0000; time: 0.26s
Val loss: 0.1743 score: 0.9302 time: 0.22s
Test loss: 0.2024 score: 0.9302 time: 0.19s
Epoch 214/1000, LR 0.000256
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.33s
Val loss: 0.1739 score: 0.9302 time: 0.19s
Test loss: 0.2021 score: 0.9302 time: 0.19s
Epoch 215/1000, LR 0.000256
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.27s
Val loss: 0.1744 score: 0.9302 time: 0.19s
Test loss: 0.2024 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5542;  Loss pred: 0.5542; Loss self: 0.0000; time: 0.27s
Val loss: 0.1745 score: 0.9302 time: 0.19s
Test loss: 0.2024 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5552;  Loss pred: 0.5552; Loss self: 0.0000; time: 0.28s
Val loss: 0.1743 score: 0.9302 time: 0.19s
Test loss: 0.2024 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.32s
Val loss: 0.1744 score: 0.9302 time: 0.21s
Test loss: 0.2024 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.28s
Val loss: 0.1749 score: 0.9302 time: 0.19s
Test loss: 0.2027 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.28s
Val loss: 0.1746 score: 0.9302 time: 0.19s
Test loss: 0.2025 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.27s
Val loss: 0.1739 score: 0.9302 time: 0.20s
Test loss: 0.2022 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.27s
Val loss: 0.1731 score: 0.9302 time: 0.17s
Test loss: 0.2019 score: 0.9302 time: 0.25s
Epoch 223/1000, LR 0.000254
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.27s
Val loss: 0.1729 score: 0.9302 time: 0.17s
Test loss: 0.2018 score: 0.9302 time: 0.17s
Epoch 224/1000, LR 0.000254
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.30s
Val loss: 0.1725 score: 0.9302 time: 0.18s
Test loss: 0.2016 score: 0.9302 time: 0.19s
Epoch 225/1000, LR 0.000253
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.35s
Val loss: 0.1722 score: 0.9302 time: 0.25s
Test loss: 0.2015 score: 0.9302 time: 0.25s
Epoch 226/1000, LR 0.000253
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.38s
Val loss: 0.1727 score: 0.9302 time: 0.18s
Test loss: 0.2018 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.26s
Val loss: 0.1719 score: 0.9225 time: 0.17s
Test loss: 0.2013 score: 0.9302 time: 0.17s
Epoch 228/1000, LR 0.000253
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.27s
Val loss: 0.1725 score: 0.9302 time: 0.17s
Test loss: 0.2016 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.36s
Val loss: 0.1728 score: 0.9302 time: 0.18s
Test loss: 0.2018 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5474;  Loss pred: 0.5474; Loss self: 0.0000; time: 0.32s
Val loss: 0.1740 score: 0.9302 time: 0.27s
Test loss: 0.2025 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 3 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.28s
Val loss: 0.1749 score: 0.9302 time: 0.19s
Test loss: 0.2031 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.28s
Val loss: 0.1753 score: 0.9302 time: 0.19s
Test loss: 0.2035 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5490;  Loss pred: 0.5490; Loss self: 0.0000; time: 0.27s
Val loss: 0.1750 score: 0.9302 time: 0.19s
Test loss: 0.2033 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.28s
Val loss: 0.1741 score: 0.9302 time: 0.18s
Test loss: 0.2029 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.28s
Val loss: 0.1723 score: 0.9302 time: 0.19s
Test loss: 0.2020 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.27s
Val loss: 0.1712 score: 0.9225 time: 0.17s
Test loss: 0.2014 score: 0.9302 time: 0.18s
Epoch 237/1000, LR 0.000250
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 0.27s
Val loss: 0.1709 score: 0.9225 time: 0.18s
Test loss: 0.2013 score: 0.9302 time: 0.25s
Epoch 238/1000, LR 0.000250
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.26s
Val loss: 0.1718 score: 0.9302 time: 0.17s
Test loss: 0.2018 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.27s
Val loss: 0.1729 score: 0.9302 time: 0.18s
Test loss: 0.2025 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.25s
Val loss: 0.1737 score: 0.9302 time: 0.17s
Test loss: 0.2030 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.25s
Val loss: 0.1730 score: 0.9302 time: 0.18s
Test loss: 0.2026 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.26s
Val loss: 0.1733 score: 0.9302 time: 0.18s
Test loss: 0.2029 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.28s
Val loss: 0.1728 score: 0.9302 time: 0.27s
Test loss: 0.2026 score: 0.9302 time: 0.25s
     INFO: Early stopping counter 6 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.34s
Val loss: 0.1723 score: 0.9302 time: 0.18s
Test loss: 0.2025 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5435;  Loss pred: 0.5435; Loss self: 0.0000; time: 0.26s
Val loss: 0.1716 score: 0.9302 time: 0.19s
Test loss: 0.2021 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.27s
Val loss: 0.1716 score: 0.9302 time: 0.19s
Test loss: 0.2022 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.33s
Val loss: 0.1717 score: 0.9302 time: 0.26s
Test loss: 0.2024 score: 0.9302 time: 0.27s
     INFO: Early stopping counter 10 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.38s
Val loss: 0.1721 score: 0.9302 time: 0.27s
Test loss: 0.2027 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 11 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.35s
Val loss: 0.1728 score: 0.9302 time: 0.19s
Test loss: 0.2032 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.27s
Val loss: 0.1726 score: 0.9302 time: 0.18s
Test loss: 0.2032 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 0.27s
Val loss: 0.1722 score: 0.9302 time: 0.19s
Test loss: 0.2031 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.26s
Val loss: 0.1726 score: 0.9302 time: 0.21s
Test loss: 0.2035 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 15 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.38s
Val loss: 0.1730 score: 0.9302 time: 0.26s
Test loss: 0.2039 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 16 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.31s
Val loss: 0.1736 score: 0.9302 time: 0.19s
Test loss: 0.2044 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.27s
Val loss: 0.1746 score: 0.9302 time: 0.19s
Test loss: 0.2052 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.26s
Val loss: 0.1738 score: 0.9302 time: 0.17s
Test loss: 0.2048 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.26s
Val loss: 0.1734 score: 0.9302 time: 0.17s
Test loss: 0.2047 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 236,   Train_Loss: 0.5443,   Val_Loss: 0.1709,   Val_Precision: 0.9661,   Val_Recall: 0.8769,   Val_accuracy: 0.9194,   Val_Score: 0.9225,   Val_Loss: 0.1709,   Test_Precision: 0.9508,   Test_Recall: 0.9062,   Test_accuracy: 0.9280,   Test_Score: 0.9302,   Test_loss: 0.2013


[0.18194310809485614, 0.1810774840414524, 0.17691346304491162, 0.18729655584320426, 0.21967729600146413, 0.1842338410206139, 0.19775674818083644, 0.17130612907931209, 0.17523492616601288, 0.18254406796768308, 0.1720045858528465, 0.17406337801367044, 0.22099581314250827, 0.17332209693267941, 0.24705747794359922, 0.1816440438851714, 0.19131378619931638, 0.1946081870701164, 0.1734392608050257, 0.17236929200589657, 0.23319614306092262, 0.17839026707224548, 0.17149156494997442, 0.1774467770010233, 0.17231285106390715, 0.17357777897268534, 0.25593708897940814, 0.1849010311998427, 0.1884904899634421, 0.2578901220113039, 0.26025395817123353, 0.1885067990515381, 0.17830685409717262, 0.1800132051575929, 0.17874571913853288, 0.18423096416518092, 0.2644944149069488, 0.18424635007977486, 0.18014067295007408, 0.18017646810039878, 0.18079442321322858, 0.18134824419394135, 0.2855063711758703, 0.17315450101159513, 0.1681035771034658, 0.20053225103765726, 0.2600499589461833, 0.25797628005966544, 0.18810250284150243, 0.18069669185206294, 0.17859323509037495, 0.18145917891524732, 0.18137782905250788, 0.18655900610610843, 0.1859878678806126, 0.18516107695177197, 0.1858946280553937, 0.25787682086229324, 0.25175379286520183, 0.2517485909629613, 0.1796361031010747, 0.18246362986974418, 0.1710216428618878, 0.2518417008686811, 0.24956991500221193, 0.19545258884318173, 0.17422436899505556, 0.17271967907436192, 0.17633475898765028, 0.1720998368691653, 0.1758254780434072, 0.2517764091026038, 0.17556831683032215, 0.17309878603555262, 0.2517390118446201, 0.2559383499901742, 0.17312748590484262, 0.17444416298530996, 0.24968568701297045, 0.2517590359784663, 0.2615265778731555, 0.17709587118588388, 0.18203573185019195, 0.18339021294377744, 0.19229137105867267, 0.26225168304517865, 0.17514603002928197, 0.1761805519927293, 0.18875654810108244, 0.18336466094478965, 0.24432305479422212, 0.22729339404031634, 0.17452223505824804, 0.17223694315180182, 0.17059823009185493, 0.1778185770381242, 0.20545099396258593, 0.18572978186421096, 0.17029164801351726, 0.25595966703258455, 0.253158297855407, 0.1710975910536945, 0.1740097040310502, 0.18111294717527926, 0.176220121094957, 0.2496106110047549, 0.1802713149227202, 0.1736392981838435, 0.1728567269165069, 0.17516811820678413, 0.24846512009389699, 0.17208930710330606, 0.17906918213702738, 0.1699134160298854, 0.1700999829918146, 0.18262516101822257, 0.20953723485581577, 0.21919363108463585, 0.18200379004701972, 0.19160219095647335, 0.1921800789423287, 0.17261441494338214, 0.19008711609058082, 0.2658604809548706, 0.1844589530956, 0.19039395288564265, 0.17890097782947123, 0.16870063310489058, 0.22866753116250038, 0.17490224610082805, 0.18366583390161395, 0.18337107310071588, 0.1961535830050707, 0.18272562697529793, 0.1851648238953203, 0.20201167417690158, 0.2528105832170695, 0.18329941295087337, 0.1814502498600632, 0.17065685079433024, 0.18320432608015835, 0.19955118210054934, 0.25595016218721867, 0.18608309607952833, 0.18295096699148417, 0.17021998902782798, 0.16992938006296754, 0.17783734016120434, 0.18483066093176603, 0.18115724716335535, 0.18467249488458037, 0.19079823000356555, 0.25166575890034437, 0.2475501918233931, 0.21393612190149724, 0.16957823582924902, 0.17746347794309258, 0.187363610137254, 0.2648962759412825, 0.1810049309860915, 0.1818247709888965, 0.170418132096529, 0.184733172878623, 0.17296810494735837, 0.20656663295812905, 0.17813694896176457, 0.16875282395631075, 0.2475821920670569, 0.25594360986724496, 0.1758754758629948, 0.196519257966429, 0.2518946649506688, 0.24958509486168623, 0.2627222998999059, 0.17695791414007545, 0.17490509594790637, 0.17617671191692352, 0.19144063792191446, 0.1934461600612849, 0.19326914916746318, 0.19408264989033341, 0.18182275514118373, 0.17733828397467732, 0.17302818107418716, 0.1749809740576893, 0.2539609130471945, 0.18777691898867488, 0.1841343909036368, 0.18069227505475283, 0.18459049495868385, 0.18371691601350904, 0.1906355288811028, 0.26370978192426264, 0.18525653891265392, 0.1825488128233701, 0.17453856184147298, 0.1729956460185349, 0.17193303513340652, 0.17579146497882903, 0.256762936944142, 0.1737316099461168, 0.17355965008027852, 0.17359850509092212, 0.1823210760485381, 0.2480911989696324, 0.17861528601497412, 0.17339223297312856, 0.17337769200094044, 0.18363975593820214, 0.179949234938249, 0.2616397449746728, 0.17663365183398128, 0.17618722189217806, 0.1780414180830121, 0.258358056191355, 0.2434195070527494, 0.18314689910039306, 0.1812009729910642, 0.1789727909490466, 0.1823384310118854, 0.2603700279723853, 0.2684615380130708, 0.18201788002625108, 0.17522011906839907, 0.18180147395469248, 0.1828178740106523, 0.1864203200675547, 0.17747685895301402, 0.18347616191022098, 0.18440603790804744, 0.18550198199227452, 0.24564243713393807, 0.1854056480806321, 0.18509095907211304, 0.18427512887865305, 0.1761972070671618, 0.1950718560256064, 0.19367907708510756, 0.1900793721433729, 0.19145685178227723, 0.18849589093588293, 0.187677931971848, 0.26900579989887774, 0.1907864399254322, 0.17997291102074087, 0.2404005927965045, 0.2622568979859352, 0.26427928218618035, 0.18160887993872166, 0.18139180401340127, 0.18633005302399397, 0.1848584038671106, 0.1844587530940771, 0.2497118308674544, 0.24928120616823435, 0.1828203839249909, 0.18846390000544488, 0.18418922484852374, 0.18215775093995035, 0.1969443189445883, 0.24548207502812147, 0.1772198211401701, 0.19065732206217945, 0.1845097930636257, 0.1818082898389548, 0.18786520301364362, 0.18468002206645906, 0.1769981449469924, 0.17430006293579936, 0.1745982519350946, 0.18816523998975754, 0.2373682230245322, 0.18043289706110954, 0.17804995807819068, 0.22075137100182474, 0.18032849393785, 0.18522052094340324, 0.17713477299548686, 0.1782013380434364, 0.18257504096254706, 0.17921589501202106, 0.24556887103244662, 0.1764913978986442, 0.18099956214427948, 0.18312228401191533, 0.1912062680348754, 0.2517709010280669, 0.24655942805111408, 0.17647729511372745, 0.17361364397220314, 0.17288523213937879, 0.25389853306114674, 0.1762142670340836, 0.17598926578648388, 0.17972279386594892, 0.17255719588138163, 0.17231586598791182, 0.25707174511626363, 0.22716698795557022, 0.18313147500157356, 0.21000455389730632, 0.1811813679523766, 0.18035486107692122, 0.27866839384660125, 0.18586038798093796, 0.2565322360023856, 0.2560587900225073, 0.23841863917186856, 0.1777489040978253, 0.18221259489655495, 0.17682257085107267, 0.17809650884009898, 0.1854242228437215, 0.2550868410617113, 0.18101903214119375, 0.18502388312481344, 0.18346702586859465, 0.2001961509231478, 0.19274651515297592, 0.1951567609794438, 0.27766083902679384, 0.18988049984909594, 0.19043012196198106, 0.29892264399677515, 0.18934996891766787, 0.2615138820838183, 0.2330024188850075, 0.1821164251305163, 0.1898289320524782, 0.1875723721459508, 0.18286613700911403, 0.20958646293729544, 0.1828267420642078, 0.17815189389511943, 0.18261727108620107, 0.18445237795822322, 0.18867202615365386, 0.18992137000896037, 0.18066136399284005, 0.18228729511611164, 0.18281646282412112, 0.18098044791258872, 0.19126119883731008, 0.2683097799308598, 0.17830231110565364, 0.17962953192181885, 0.18001126893796027, 0.17820453294552863, 0.18292957707308233, 0.1798315888736397, 0.17787883803248405, 0.19010688294656575, 0.18478836305439472, 0.17857452691532671, 0.1809070350136608, 0.18095966801047325, 0.17887460510246456, 0.17714431905187666, 0.19524625805206597, 0.1949843850452453, 0.19412237592041492, 0.19035442196764052, 0.1756480378098786, 0.1779089921619743, 0.18532796995714307, 0.1808192259632051, 0.17669163784012198, 0.1769482281524688, 0.17746085301041603, 0.1778341990429908, 0.2624270371161401, 0.19030858296900988, 0.18917977204546332, 0.26651475112885237, 0.2663838570006192, 0.18761433800682425, 0.17218495113775134, 0.17164404084905982, 0.17705318611115217, 0.18369746091775596, 0.18808535486459732, 0.1860991409048438, 0.17434588493779302, 0.19224829296581447, 0.2954132598824799, 0.25378149119205773, 0.22513856599107385, 0.18854105682112277, 0.18716653413139284, 0.2034830329939723, 0.18998328200541437, 0.18854643497616053, 0.17805938608944416, 0.17602359293960035, 0.17628279910422862, 0.1810183150228113, 0.18351093493402004, 0.18980058492161334, 0.18885866506025195, 0.23808425688184798, 0.18548454903066158, 0.18692944711074233, 0.19351318292319775, 0.19458089000545442, 0.1929779089987278, 0.18904771492816508, 0.18653275887481868, 0.18542655184864998, 0.18583872797898948, 0.21696176589466631, 0.18533158604986966, 0.18565997411496937, 0.18492789193987846, 0.18029221799224615, 0.20795786986127496, 0.23823034297674894, 0.1795910589862615, 0.17847021808847785, 0.18484873697161674, 0.17541624908335507, 0.18328081397339702, 0.18872282817028463, 0.1749839698895812, 0.1755301859229803, 0.17625133809633553, 0.1849250509403646, 0.18636670894920826, 0.24787111696787179, 0.1780181140638888, 0.18493730504997075, 0.18525625113397837, 0.1814237260259688, 0.23807618394494057, 0.20117195299826562, 0.18139604991301894, 0.1815563919954002, 0.1877986949402839, 0.18681888794526458, 0.1954327509738505, 0.1989432831760496, 0.19252366991713643, 0.19507409119978547, 0.19217727216891944, 0.1876100399531424, 0.18763565085828304, 0.22718774899840355, 0.19161074911244214, 0.17641262710094452, 0.18145847809500992, 0.1776104138698429, 0.18301342497579753, 0.1973751860205084, 0.18290682206861675, 0.1832054378464818, 0.17527244007214904, 0.17423165892250836, 0.17997434991411865, 0.1826927789952606, 0.17950378311797976, 0.17653193301521242, 0.17452921392396092, 0.17599434405565262, 0.17513539618812501, 0.17306757206097245, 0.17601104103960097, 0.17395266005769372, 0.19736740388907492, 0.19349885499104857, 0.19423192599788308, 0.18832763796672225, 0.19512028503231704, 0.22587548196315765, 0.1867214220110327, 0.19050655304454267, 0.18700717203319073, 0.17424272396601737, 0.2508977029938251, 0.1742381788790226, 0.193232131190598, 0.2536582669708878, 0.1759396439883858, 0.1720433251466602, 0.17989904596470296, 0.19620425696484745, 0.25995666603557765, 0.1874880890827626, 0.1891601209063083, 0.18760377494618297, 0.18606566893868148, 0.18765862309373915, 0.1815169530455023, 0.2588996170088649, 0.18069302197545767, 0.1791647640056908, 0.17778965504840016, 0.17781350389122963, 0.17459592199884355, 0.2500207500997931, 0.17657265882007778, 0.1887004568707198, 0.19495862000621855, 0.2706981480587274, 0.26432584901340306, 0.19644498801790178, 0.19482051790691912, 0.1898872631136328, 0.24875263404101133, 0.2680976218543947, 0.18953233095817268, 0.19012766983360052, 0.17419727193191648, 0.18699585110880435]
[0.0014104116906577995, 0.0014037014266779257, 0.0013714221941466018, 0.0014519112856062346, 0.0017029247752051483, 0.0014281693102373171, 0.0015329980479134608, 0.0013279544889869153, 0.001358410280356689, 0.0014150702943231247, 0.0013333688825802055, 0.0013493285117338793, 0.0017131458383140175, 0.0013435821467649568, 0.0019151742476248002, 0.0014080933634509412, 0.001483052606196251, 0.0015085905974427628, 0.0013444903938374085, 0.0013361960620612138, 0.0018077220392319583, 0.001382870287381748, 0.0013293919763563908, 0.0013755564108606456, 0.0013357585353791252, 0.0013455641780828322, 0.0019840084417008384, 0.0014333413271305634, 0.0014611665888638922, 0.001999148232645767, 0.0020174725439630506, 0.0014612930159033962, 0.0013822236751718808, 0.001395451202772038, 0.0013856257297560689, 0.0014281470090324103, 0.002050344301604254, 0.0014282662796881772, 0.0013964393251943727, 0.0013967168069798355, 0.0014015071566916945, 0.0014058003425886927, 0.0022132276835338783, 0.001342282953578257, 0.0013031285046780294, 0.0015545135739353276, 0.002015891154621576, 0.0019998161244935307, 0.0014581589367558328, 0.001400749549240798, 0.0013844436828711235, 0.0014066603016685839, 0.0014060296825775804, 0.0014461938457837862, 0.0014417664176791674, 0.001435357185672651, 0.0014410436283363852, 0.0019990451229635136, 0.0019515797896527273, 0.0019515394648291576, 0.001392527931016083, 0.0014144467431763115, 0.0013257491694719985, 0.0019522612470440394, 0.0019346505038931158, 0.0015151363476215637, 0.00135057650383764, 0.0013389122408865265, 0.0013669361161833355, 0.001334107262551669, 0.0013629882018868775, 0.00195175510932251, 0.0013609947041110244, 0.0013418510545391677, 0.0019514652080978303, 0.0019840182169780945, 0.0013420735341460668, 0.001352280333219457, 0.0019355479613408562, 0.001951620433941599, 0.0020273378129701974, 0.001372836210743286, 0.001411129704265054, 0.0014216295577037011, 0.0014906307834005633, 0.0020329587832959586, 0.0013577211630176897, 0.0013657407131219327, 0.0014632290550471508, 0.0014214314801921678, 0.0018939771689474583, 0.0017619642948861732, 0.0013528855430871942, 0.0013351701019519522, 0.0013224668999368598, 0.0013784385816908854, 0.0015926433640510538, 0.0014397657508853562, 0.0013200902946784285, 0.0019841834653688724, 0.0019624674252357133, 0.0013263379151449185, 0.0013489124343492264, 0.0014039763346920873, 0.0013660474503485037, 0.0019349659767810457, 0.0013974520536644975, 0.0013460410711925853, 0.0013399746272597433, 0.001357892389199877, 0.0019260862022782712, 0.0013340256364597369, 0.0013881331948606775, 0.0013171582637975614, 0.0013186045193163923, 0.0014156989226218803, 0.0016243196500450835, 0.001699175434764619, 0.0014108820933877498, 0.0014852883019881656, 0.0014897680538165014, 0.0013380962398711793, 0.0014735435355858978, 0.0020609339608904697, 0.0014299143650821707, 0.0014759221153925787, 0.001386829285499777, 0.0013077568457743457, 0.0017726165206395379, 0.001355831365122698, 0.001423766154276077, 0.0014214811868272549, 0.0015205704108920209, 0.0014164777284906816, 0.001435386231746669, 0.001565981970363578, 0.0019597719629230194, 0.001420925681789716, 0.0014065910841865364, 0.001322921323987056, 0.0014201885742647934, 0.0015469083883763515, 0.001984109784397044, 0.0014425046207715375, 0.0014182245503215826, 0.0013195347986653331, 0.0013172820159919965, 0.001378584032257398, 0.001432795821176481, 0.0014043197454523672, 0.0014315697277874448, 0.0014790560465392677, 0.001950897355816623, 0.0019189937350650628, 0.0016584195496240097, 0.001314559967668597, 0.0013756858755278494, 0.0014524310863353023, 0.0020534595034207947, 0.001403138999892182, 0.0014094943487511357, 0.0013210707914459612, 0.001432040099834287, 0.0013408380228477393, 0.0016012917283575894, 0.00138090658109895, 0.0013081614260179127, 0.0019192417989694333, 0.001984058991218953, 0.0013633757818836806, 0.0015234051005149534, 0.001952671821323014, 0.0019347681772223738, 0.002036606975968263, 0.0013717667762796547, 0.0013558534569605145, 0.0013657109450924303, 0.001484035952883058, 0.0014995826361339916, 0.0014982104586625054, 0.0015045166658165382, 0.00140947872202468, 0.0013747153796486613, 0.0013413037292572649, 0.0013564416593619326, 0.0019686892484278642, 0.0014556350309199602, 0.0014273983790979597, 0.00140071531050196, 0.0014309340694471616, 0.0014241621396396049, 0.0014777947975279286, 0.002044261875381881, 0.0014360972008732862, 0.0014151070761501557, 0.0013530121072982402, 0.0013410515195235264, 0.0013328142258403606, 0.0013627245347196048, 0.0019904103639080775, 0.0013467566662489674, 0.0013454236440331667, 0.001345724845666063, 0.0014133416747948689, 0.001923187588911879, 0.0013846146202711172, 0.0013441258370009966, 0.0013440131162863601, 0.0014235639995209468, 0.0013949553095988295, 0.002028215077323045, 0.0013692531149921028, 0.0013657924177688221, 0.0013801660316512566, 0.002002775629390349, 0.0018869729228895302, 0.001419743403879016, 0.0014046587053570868, 0.0013873859763491986, 0.0014134762093944604, 0.002018372309863452, 0.002081097193899774, 0.001410991318032954, 0.0013582954966542564, 0.0014093137515867635, 0.0014171928217880023, 0.0014451187602136024, 0.0013757896042869303, 0.0014222958287614029, 0.0014295041698298252, 0.0014379998604052288, 0.0019042049390227758, 0.0014372530858963728, 0.0014348136362179306, 0.0014284893711523493, 0.0013658698222260605, 0.0015121849304310573, 0.0015013881944581982, 0.0014734835049873868, 0.0014841616417230793, 0.0014612084568673095, 0.0014548676897042482, 0.0020853162782858738, 0.0014789646505847457, 0.0013951388451220223, 0.0018635704867946085, 0.002032999209193296, 0.0020486766060944213, 0.0014078207747187726, 0.0014061380156077617, 0.0014444190156898758, 0.00143301088269078, 0.0014299128146827683, 0.0019357506268794916, 0.0019324124509165453, 0.0014172122784883016, 0.0014609604651584875, 0.0014278234484381685, 0.0014120755886817857, 0.0015267001468572737, 0.0019029618219234222, 0.0013737970631020938, 0.0014779637369161198, 0.0014303084733614394, 0.0014093665878988746, 0.0014563194032065397, 0.0014316280780345663, 0.0013720786429999412, 0.001351163278572088, 0.0013534748212022836, 0.0014586452712384305, 0.0018400637443762185, 0.0013987046283806942, 0.0013802322331642688, 0.0017112509379986413, 0.0013978953018437983, 0.0014358179918093274, 0.0013731377751588128, 0.001381405721266949, 0.0014153103950585043, 0.0013892705039691556, 0.0019036346591662528, 0.0013681503713073194, 0.0014030973809634067, 0.0014195525892396537, 0.0014822191320532977, 0.0019517124110702859, 0.0019113133957450704, 0.001368041047393236, 0.001345842201334908, 0.0013401955979796805, 0.0019682056826445483, 0.0013660020700316559, 0.0013642578743138286, 0.0013931999524492164, 0.0013376526812510204, 0.0013357819068830373, 0.00199280422570747, 0.0017609844027563584, 0.0014196238372215005, 0.0016279422782736923, 0.001404506728312997, 0.0013980996982707072, 0.002160220107337994, 0.0014407782014026198, 0.0019886219845146173, 0.0019849518606395917, 0.0018482065052082834, 0.0013778984813784907, 0.001412500735632209, 0.0013707176034966873, 0.001380593091783713, 0.0014373970763079187, 0.0019774173725714056, 0.0014032483111720445, 0.0014342936676342128, 0.001422225006733292, 0.0015519081466910682, 0.0014941590321936118, 0.0015128431083677814, 0.002152409604858867, 0.0014719418592953175, 0.0014762024958293106, 0.0023172297984246134, 0.00146782921641603, 0.0020272393959985917, 0.0018062203014341666, 0.0014117552335698937, 0.0014715421089339395, 0.0014540493964802386, 0.0014175669535590235, 0.0016247012630798096, 0.0014172615663892076, 0.0013810224332954995, 0.001415637760358148, 0.0014298633950249862, 0.0014625738461523554, 0.001472258682240003, 0.0014004756898669771, 0.0014130798071016405, 0.001417181882357528, 0.0014029492086247188, 0.0014826449522272098, 0.0020799207746578278, 0.0013821884581833616, 0.0013924769916420067, 0.0013954361933175214, 0.0013814304879498343, 0.0014180587370006383, 0.0013940433246018582, 0.001378905721182047, 0.0014736967670276415, 0.0014324679306542227, 0.0013842986582583467, 0.0014023801163849674, 0.0014027881241121956, 0.001386624845755539, 0.0013732117755959432, 0.0015135368841245425, 0.001511506860815855, 0.0015048246195381002, 0.001475615674167756, 0.0013616126962006092, 0.001379139474123832, 0.0014366509299003339, 0.0014016994260713574, 0.0013697026189156743, 0.0013716916911044093, 0.0013756655272125274, 0.0013785596825038047, 0.002034318117179381, 0.0014752603330931, 0.0014665098608175452, 0.002066005822704282, 0.0020649911395396836, 0.0014543747132311957, 0.0013347670630833438, 0.0013305739600702313, 0.0013725053186911021, 0.0014240113249438446, 0.0014580260067023048, 0.001442628999262355, 0.0013515184878898684, 0.0014902968446962362, 0.002290025270406821, 0.0019672983813337807, 0.0017452602014811926, 0.0014615585800087036, 0.0014509033653596344, 0.001577387852666452, 0.0014727386201970106, 0.0014616002711330272, 0.001380305318522823, 0.0013645239762759716, 0.0013665333263893692, 0.0014032427521148162, 0.0014225653870854266, 0.0014713223637334368, 0.001464020659381798, 0.00184561439443293, 0.0014378647211679193, 0.0014490654814786226, 0.0015001021932030832, 0.0015083789922903444, 0.0014959527829358744, 0.001465486162233838, 0.0014459903788745635, 0.0014374151306096898, 0.001440610294410771, 0.0016818741542222195, 0.0014366789616268966, 0.0014392246055423981, 0.0014335495499215385, 0.0013976140929631484, 0.0016120765105525191, 0.0018467468447809995, 0.0013921787518314844, 0.0013834900627013786, 0.0014329359455164089, 0.0013598158843670936, 0.0014207815036697442, 0.0014629676602347646, 0.0013564648828649705, 0.001360699115682018, 0.0013662894426072522, 0.001433527526669493, 0.0014447031701489011, 0.001921481526882727, 0.001379985380340223, 0.001433622519767215, 0.0014360949700308401, 0.0014063854730695254, 0.0018455518135266711, 0.0015594725038625243, 0.0014061709295582865, 0.0014074138914372109, 0.0014558038367463867, 0.0014482084336842214, 0.0015149825656887634, 0.0015421959936127874, 0.0014924315497452437, 0.001512202257362678, 0.0014897462958830964, 0.0014543413949856, 0.0014545399291339771, 0.0017611453410728958, 0.001485354644282497, 0.0013675397449685622, 0.0014066548689535654, 0.0013768249137197123, 0.0014187087207426165, 0.0015300402017093676, 0.001417882341617184, 0.001420197192608386, 0.0013587010858306127, 0.0013506330149031657, 0.001395149999334253, 0.0014162230929865161, 0.0013915021947130215, 0.0013684645970171506, 0.0013529396428214025, 0.0013642972407414931, 0.001357638730140504, 0.0013416090857439726, 0.001364426674725589, 0.0013484702330053776, 0.001529979875109108, 0.0014999911239616168, 0.0015056738449448302, 0.0014599041702846687, 0.001512560349087729, 0.0017509727283965709, 0.001447452883806455, 0.0014767949848414161, 0.0014496680002572925, 0.0013507187904342431, 0.0019449434340606596, 0.0013506835572017256, 0.001497923497601535, 0.0019663431548130835, 0.001363873209212293, 0.0013336691871834124, 0.0013945662477883951, 0.001520963232285639, 0.002015167953764168, 0.0014533960394012605, 0.0014663575264054905, 0.0014542928290401781, 0.0014423695266564457, 0.001454718008478598, 0.0014071081631434287, 0.0020069737752625182, 0.001400721100584943, 0.0013888741395789985, 0.0013782143802201564, 0.0013783992549707723, 0.0013534567596809577, 0.0019381453496107991, 0.0013687803009308356, 0.0014627942393079054, 0.0015113071318311516, 0.0020984352562692047, 0.0020490375892511866, 0.0015228293644798587, 0.0015102365729218537, 0.0014719942877025798, 0.0019283149925659793, 0.0020782761384061602, 0.0014692428756447494, 0.0014738579056868257, 0.0013503664490846239, 0.001449580241153522]
[709.012841161017, 712.4022110361839, 729.1700573813978, 688.7473152896237, 587.2249993424001, 700.1970934621409, 652.3165514535939, 753.037855056984, 736.1546172467288, 706.6786745589436, 749.9800040817643, 741.1093675883316, 583.7214658759841, 744.2790174071417, 522.145700967001, 710.1801811985038, 674.2849146564063, 662.8703650248893, 743.7762326778898, 748.393165040018, 553.1823910410845, 723.1336222382405, 752.2235862599448, 726.9785463573457, 748.6383006462859, 743.1826859606251, 504.03011347206075, 697.670527648792, 684.3846606002227, 500.21303256565074, 495.66969473380584, 684.3254495278505, 723.4719083187814, 716.614094433054, 721.6956054764111, 700.2080273777376, 487.7229640005185, 700.1495548983503, 716.1070172961566, 715.9647503364203, 713.5175837136175, 711.3385661569573, 451.8287962146272, 744.9994036907053, 767.3840272928993, 643.2880463490909, 496.05852861025147, 500.0459731032812, 685.7962975043296, 713.9034958404926, 722.3117938074256, 710.9036906876504, 711.222538465025, 691.4702361065816, 693.593627745689, 696.6906983026461, 693.9415159514992, 500.23883328733245, 512.40538834333, 512.415976218827, 718.1184504287337, 706.99021000563, 754.2904970464862, 512.2265278348999, 516.8892252051161, 660.006607042187, 740.424549929987, 746.8749403156368, 731.5630834249451, 749.5649173570635, 733.682066077778, 512.3593606715949, 734.7567165246104, 745.2391952275436, 512.4354745605427, 504.02763011073756, 745.1156546621561, 739.4916390000574, 516.6495586641249, 512.3947170302708, 493.25770653630104, 728.4190147188618, 708.6520799452812, 703.4181264599325, 670.8569359601635, 491.8938879708806, 736.5282557556857, 732.2034046375541, 683.4199994530427, 703.5161482879265, 527.9894691421919, 567.5483906809827, 739.1608293175098, 748.9682389817222, 756.1625928389922, 725.4585102901958, 627.8869598630017, 694.5574301827011, 757.5239391056945, 503.98565326926234, 509.56259815619075, 753.9556764391658, 741.3379657089774, 712.262717889269, 732.0389930414801, 516.8049526449925, 715.5880571199057, 742.9193814376, 746.2827874920337, 736.4353817383416, 519.1875622270436, 749.6107815842425, 720.3919650522923, 759.2102084352815, 758.3775008737515, 706.3648802868308, 615.6423706209825, 588.5207492647907, 708.7764489227039, 673.2699629165785, 671.2454314201401, 747.3304013590767, 678.6362098235452, 485.21690601281034, 699.3425791218865, 677.5425271908817, 721.0692840536794, 764.6681439529247, 564.1378089149324, 737.5548506428773, 702.3625312321434, 703.491547596208, 657.6479410863745, 705.9765077037556, 696.6766002646804, 638.5769561368752, 510.2634484618762, 703.7665747166014, 710.9386738209867, 755.902850659458, 704.1318442641904, 646.4506932111273, 504.0043690444743, 693.2386805562816, 705.1069591012579, 757.8428405309718, 759.1388843541881, 725.3819691807428, 697.9361505806818, 712.0885419708135, 698.5339104268046, 676.1069009790568, 512.5846303591977, 521.1064433027424, 602.9837264199617, 760.7108268887295, 726.9101310037809, 688.5008241755189, 486.9830636222097, 712.6877665554449, 709.4742883403806, 756.9617059699448, 698.3044679515037, 745.802239316088, 624.4958256455109, 724.1619481632002, 764.4316520202201, 521.0390897785602, 504.01727187840646, 733.473495193211, 656.424216816638, 512.1188256419143, 516.8577878077557, 491.0127539578767, 728.9868928828288, 737.5428331626272, 732.2193642757404, 673.8381223563255, 666.852213345212, 667.4629683821112, 664.6652860154762, 709.4821541991961, 727.4233014368195, 745.5432935788087, 737.2230077852356, 507.9521822951844, 686.9853904024285, 700.5752666133383, 713.920946321091, 698.8442174602411, 702.1672407701118, 676.6839358704002, 489.17411807290773, 696.3316963447204, 706.6603063851055, 739.0916863241144, 745.6835068911436, 750.2921116928236, 733.8240227734365, 502.4089595456823, 742.5246334850037, 743.2603139055199, 743.0939565547314, 707.5429939084893, 519.9700776801448, 722.2226209081867, 743.9779613426615, 744.0403578523832, 702.4622709878282, 716.8688438395827, 493.04435766243176, 730.3251597903193, 732.1756856972556, 724.5505084656961, 499.307054332593, 529.9493108087081, 704.3526296849169, 711.9167070165873, 720.779953846315, 707.475649999376, 495.4487311945201, 480.5157601150272, 708.7215826346032, 736.2168265029169, 709.5652042521318, 705.6202830172039, 691.9846503495605, 726.8553250322738, 703.0886119316285, 699.5432550008194, 695.4103595797288, 525.1535585834541, 695.7716840637912, 696.9546251567074, 700.0402104450446, 732.1341929717908, 661.2947794122932, 666.050261811781, 678.6638578682699, 673.7810571893111, 684.3650509277122, 687.3477272722198, 479.54356392498795, 676.1486825291091, 716.7745371698381, 536.6043340383798, 491.8841067315539, 488.11998781320153, 710.3176895509024, 711.167743777825, 692.3198802685281, 697.8314066410223, 699.343337392115, 516.5954674709524, 517.4878683511374, 705.6105956594382, 684.4812189298485, 700.366702265504, 708.1773865473658, 655.0074695797398, 525.4966171571682, 727.9095485485726, 676.6065871728177, 699.1498817383427, 709.5386030761731, 686.6625534193868, 698.5054396061204, 728.8211977511576, 740.1030029892478, 738.8390122482707, 685.5676426050949, 543.4594334333781, 714.9472302509774, 724.515756096666, 584.3678316223625, 715.361156648154, 696.4671049565711, 728.2590415112155, 723.9002883836729, 706.5587898537714, 719.802225083591, 525.3108810479299, 730.9138096015441, 712.7089064291257, 704.4473079617459, 674.6640752199129, 512.3705697252891, 523.2004349606825, 730.9722189297405, 743.0291597396221, 746.1597407926731, 508.0769803775619, 732.0633123029058, 732.9992509685628, 717.7720600995003, 747.5782122043552, 748.6252020986246, 501.8054393401277, 567.8642005203242, 704.4119532095265, 614.272393650482, 711.9937411771147, 715.2565737886133, 462.9157911284719, 694.0693571199818, 502.8607788644557, 503.7905552418686, 541.0650796769623, 725.7428711290619, 707.9642330610318, 729.5448730278284, 724.3263825896807, 695.7019855422194, 505.71013174604315, 712.6322490741238, 697.2072892502163, 703.1236233828427, 644.3680330773248, 669.2728005879503, 661.0070763245953, 464.5955852188133, 679.3746598651278, 677.4138391076311, 431.5497758055147, 681.278168342827, 493.2816528594607, 553.6423210424468, 708.3380859664379, 679.5592147372876, 687.7345449340727, 705.4340519785281, 615.497767327628, 705.5860567416111, 724.1011991483186, 706.3953985990074, 699.3675084482636, 683.726160310288, 679.2284617255755, 714.0430978098477, 707.674113644787, 705.625729801504, 712.7841791081509, 674.4703096299711, 480.78754353733115, 723.4903417688209, 718.1447205248265, 716.6218024075984, 723.8873100912149, 705.1894071151934, 717.3378203906268, 725.2127427122172, 678.5656468643413, 698.0959074897333, 722.3874660531337, 713.073430175111, 712.8660293106527, 721.1755963128755, 728.2197966632076, 660.7040835865843, 661.591439591771, 664.529266079489, 677.6832325015779, 734.4232341475376, 725.0898250413002, 696.0633088995208, 713.4197113876055, 730.0854843890497, 729.0267969727629, 726.9208831788292, 725.3947817360748, 491.5652038662067, 677.8464638192727, 681.891084893574, 484.0257413655582, 484.26357907904367, 687.5807114236001, 749.1943932823576, 751.5553663377099, 728.5946264701225, 702.2416061469418, 685.8588224100017, 693.1789119110457, 739.9084873498877, 671.0072584256378, 436.6764039343334, 508.3113011672506, 572.980463973971, 684.2011081034091, 689.2257774535733, 633.9594908820792, 679.0071138802814, 684.181591745877, 724.4773939364237, 732.8563054855053, 731.7787138365537, 712.6350722231829, 702.9553854454557, 679.6607083865223, 683.0504703548809, 541.824989562488, 695.4757184582292, 690.0999387409342, 666.6212505594413, 662.9633567632666, 668.4702962599229, 682.3674121055512, 691.5675336500616, 695.693247347301, 694.150252764238, 594.5748066165204, 696.0497276772252, 694.8185822762053, 697.5691911414798, 715.5050918811588, 620.3179523143492, 541.4927350903838, 718.298565241315, 722.8096731301541, 697.867900605714, 735.3936746116445, 703.8379915680873, 683.5421090849873, 737.2103860793757, 734.9163297565412, 731.9093369350259, 697.5799078817097, 692.1837098875702, 520.4317533160612, 724.6453580207196, 697.5336856192628, 696.332778032448, 711.0426118220893, 541.8433623324271, 641.2424698243703, 711.151097622339, 710.5230423573755, 686.9057319116054, 690.5083389522973, 660.0736025931529, 648.4260133871666, 670.0474806839208, 661.2872022450405, 671.255235044714, 687.5964635593016, 687.5026116301894, 567.8123075241459, 673.2398917990737, 731.2401732228913, 710.9064363058134, 726.3087630353378, 704.8663234244127, 653.5775980806228, 705.2771380588865, 704.1275713011117, 735.9970566216715, 740.3935702487589, 716.7688065635857, 706.1034415779866, 718.6478065212369, 730.7459777766303, 739.1312726372724, 732.9781004735462, 736.5729761528743, 745.3736044471274, 732.9085677697691, 741.5810712938534, 653.6033684290695, 666.6706115959583, 664.1544603815851, 684.9764665067082, 661.1306455330065, 571.1111222821478, 690.8687745125347, 677.1420611963846, 689.8131157082286, 740.3465525777646, 514.1537704838009, 740.3658648749281, 667.5908359814058, 508.5582328558811, 733.2059851645238, 749.8111297839224, 717.0688388492644, 657.4780893928923, 496.23655345058575, 688.0437079021896, 681.9619240140694, 687.6194257658495, 693.3036101491261, 687.4184509792656, 710.677420679613, 498.2626142532415, 713.9179952257438, 720.0076461234452, 725.5765244883454, 725.4792081422041, 738.8488718588424, 515.9571753484903, 730.5774340264487, 683.6231461187132, 661.6788731674717, 476.54555794010724, 488.03399471331636, 656.6723910932479, 662.1479163792867, 679.3504623993844, 518.5874734445306, 481.1680130085623, 680.6226639425894, 678.4914584652546, 740.5397258483965, 689.8548777156601]
Elapsed: 0.19623353222596052~0.029085529989392993
Time per graph: 0.0015211901722942676~0.00022546922472397668
Speed: 669.4887956950738~82.14497446276533
Total Time: 0.1883
best val loss: 0.17085778039641852 test_score: 0.9302

Testing...
Test loss: 0.6573 score: 0.9147 time: 0.17s
test Score 0.9147
Epoch Time List: [0.6421636352315545, 0.6376376689877361, 0.6347821648232639, 0.6749577689915895, 0.7327770818956196, 0.6372906931210309, 0.6769602941349149, 0.60661904909648, 0.5986411243211478, 0.6284742660354823, 0.6696629300713539, 0.6127120780292898, 0.6684479489922523, 0.6243134569376707, 0.693208635551855, 0.7037727469578385, 0.6353238711599261, 0.6488673519343138, 0.6107555909547955, 0.6025967199821025, 0.6983441519550979, 0.6146188830025494, 0.6011120402254164, 0.6071335619781166, 0.6059105119202286, 0.5989531793165952, 0.8409325410611928, 0.713565019890666, 0.6341685769148171, 0.8835169360972941, 0.8895649330224842, 0.7792185200378299, 0.6271086784545332, 0.6218221788294613, 0.6353310409467667, 0.6802317290566862, 0.892087183194235, 0.8284281897358596, 0.6305841880384833, 0.635555423097685, 0.6342155693564564, 0.6360159728210419, 0.744021327001974, 0.6934842101763934, 0.6072305766865611, 0.7457802731078118, 0.824458017013967, 0.9846647218801081, 0.8253581968601793, 0.7113312359433621, 0.5997320858296007, 0.616832478903234, 0.6251789128873497, 0.6426537572406232, 0.6505569519940764, 0.7131162472069263, 0.6487868749536574, 0.7719614051748067, 0.8732066822703928, 0.8886971327010542, 0.8143330339808017, 0.6069000933784992, 0.5981512099970132, 0.7563648710492998, 0.8734207330271602, 0.8065842560026795, 0.6107647179160267, 0.6332573569379747, 0.6069363038986921, 0.6065964249428362, 0.6244919809978455, 0.8072861761320382, 0.712404313031584, 0.6117468120064586, 0.7904154108837247, 0.8816457388456911, 0.7004174720495939, 0.6056740330532193, 0.8107360580470413, 0.8839550609700382, 0.8767378102056682, 0.6105905210133642, 0.6260883372742683, 0.6401293405797333, 0.6511246038135141, 0.7983039440587163, 0.6529384281020612, 0.6109164271038026, 0.63776197982952, 0.6462766109034419, 0.6864231640938669, 0.791377189103514, 0.6068599859718233, 0.5876305608544499, 0.6532423959579319, 0.6535533717833459, 0.7644841279834509, 0.6533192088827491, 0.6216716549824923, 0.7609188209753484, 0.881825947901234, 0.6959277910646051, 0.5972939559724182, 0.610103587852791, 0.6089651598595083, 0.8113707010634243, 0.8289844247046858, 0.6205518732313067, 0.6100021318998188, 0.616467890329659, 0.7096995208412409, 0.7957345577888191, 0.6140806479379535, 0.5975700449198484, 0.5940972499083728, 0.619359944248572, 0.6591601499821991, 0.7354638478718698, 0.6317201310303062, 0.6426003510132432, 0.6536789073143154, 0.6007164991460741, 0.6411610869690776, 0.7649151298683137, 0.6361806911882013, 0.6434625496622175, 0.6031008092686534, 0.5937262333463877, 0.7226679380983114, 0.6102280712220818, 0.7195046218112111, 0.6330400458537042, 0.643298780079931, 0.7153286130633205, 0.6434841719456017, 0.6660190410912037, 0.918470935896039, 0.6436123729217798, 0.6381792107131332, 0.6172451789025217, 0.6298211188986897, 0.6583035299554467, 0.7815484099555761, 0.7784271908458322, 0.6397619438357651, 0.5986714821774513, 0.599218871910125, 0.6278898869641125, 0.6406251038424671, 0.7070430000312626, 0.6383038468193263, 0.6488507450558245, 0.867982380092144, 0.8712407480925322, 0.7726480490528047, 0.6024569345172495, 0.620835242094472, 0.6314942869357765, 0.8854031530208886, 0.6626819947268814, 0.6295180700253695, 0.5837912496645004, 0.6159224978182465, 0.6128300139680505, 0.7662526529747993, 0.6149343647994101, 0.6046041289810091, 0.7453113808296621, 0.8716567200608552, 0.7320824698545039, 0.6339042212348431, 0.8556400150991976, 0.8799091407563537, 0.8887908812612295, 0.6264352810103446, 0.6184740939643234, 0.6174445399083197, 0.6515414749737829, 0.6477790533099324, 0.6517657225485891, 0.6570326858200133, 0.7022780252154917, 0.6153155830688775, 0.6147483119275421, 0.6179750340525061, 0.7073602639138699, 0.8170477240346372, 0.6220514629967511, 0.6221255669370294, 0.625462899915874, 0.6479472320061177, 0.6768860009033233, 0.9082401869818568, 0.6743556940928102, 0.6336579909548163, 0.6081463401205838, 0.6095313041005284, 0.6108137618284672, 0.6065173901151866, 0.7303514040540904, 0.6265119460877031, 0.6173051937948912, 0.6153114777989686, 0.6228014396037906, 0.7070346218533814, 0.7600782469380647, 0.6176635320298374, 0.6171265051234514, 0.6388158521149307, 0.6612847382202744, 0.735343508888036, 0.6225111149251461, 0.6200548477936536, 0.6113127637654543, 0.7243289288599044, 0.8965882840566337, 0.7933253520168364, 0.7062912208493799, 0.627127654151991, 0.6389275647234172, 0.817690584808588, 0.9133732228074223, 0.7218783048447222, 0.6026122088078409, 0.6236832740250975, 0.632180365268141, 0.6475933478213847, 0.6551336266566068, 0.699165927246213, 0.6296772758942097, 0.6369424718432128, 0.7095465529710054, 0.6710772276856005, 0.6399033258203417, 0.633300710003823, 0.6053304660599679, 0.6477532910648733, 0.7370213600806892, 0.6456991520244628, 0.6478800640907139, 0.6421952550299466, 0.643898748094216, 0.7979205001611263, 0.6515111788176, 0.638760935747996, 0.7075647909659892, 0.9052087711170316, 0.9071920001879334, 0.7823877278715372, 0.6382705541327596, 0.6403593318536878, 0.6379118978511542, 0.6504314837511629, 0.8623707499355078, 0.8838891820050776, 0.6837741611525416, 0.610546737909317, 0.6106360929552466, 0.6122466975357383, 0.6467016497626901, 0.7335337570402771, 0.6567441779188812, 0.6344589707441628, 0.6342179132625461, 0.6195497100707144, 0.6321413870900869, 0.7209030594676733, 0.6105253060813993, 0.6007360338699073, 0.5989757759962231, 0.6202635189983994, 0.7677717788610607, 0.614027373958379, 0.6641626989003271, 0.652903103036806, 0.6334722677711397, 0.6923630479723215, 0.6017354163341224, 0.5996845038607717, 0.609424389898777, 0.6235906279180199, 0.8281758110970259, 0.6254277799744159, 0.6114401840604842, 0.6148912240751088, 0.6179018770344555, 0.7597660680767149, 0.861511467024684, 0.6271877037361264, 0.6106450462248176, 0.6031675203703344, 0.790647875983268, 0.772336991969496, 0.6140181317459792, 0.6092214912641793, 0.5954598218668252, 0.6557676987722516, 0.7910961098968983, 0.7418949559796602, 0.6033803378231823, 0.6577897630631924, 0.731669356347993, 0.7326224690768868, 0.7168897201772779, 0.6202310738153756, 0.7249983809888363, 0.8758149247150868, 0.8623441378585994, 0.7023510911967605, 0.6211418870370835, 0.607696128776297, 0.613464840920642, 0.653326909057796, 0.881981483893469, 0.6474748449400067, 0.6174328657798469, 0.6240841618273407, 0.6578048940282315, 0.664819624973461, 0.6559207960963249, 0.7854113108478487, 0.6560958288609982, 0.6531581918243319, 0.7624774931464344, 0.6445355881005526, 0.7116441926918924, 0.8106810201425105, 0.6213830208871514, 0.6264679150190204, 0.6160363319795579, 0.6329141960013658, 0.762531611835584, 0.6367355030961335, 0.6157180438749492, 0.6181100157555193, 0.6211737198755145, 0.657455347944051, 0.6949706629384309, 0.6273466511629522, 0.621135972905904, 0.6218936368823051, 0.6244249290321022, 0.6230098227970302, 0.793878176016733, 0.6300864333752543, 0.6212294430006295, 0.6213507303036749, 0.6084160758182406, 0.6494168420322239, 0.6913713470567018, 0.6153029780834913, 0.631358589977026, 0.7003013798967004, 0.6176022179424763, 0.6718774633482099, 0.6230390288401395, 0.609550196910277, 0.6080111127812415, 0.6497728822287172, 0.7331118730362505, 0.649261939805001, 0.6455104053020477, 0.6016080228146166, 0.5904790747445077, 0.614355708938092, 0.6840785648673773, 0.6043071299791336, 0.6085275469813496, 0.6062554472591728, 0.6070867110975087, 0.7051566138397902, 0.6484022592194378, 0.644408794818446, 0.8012531637214124, 0.902599633904174, 0.7746701380237937, 0.5936298479791731, 0.6012563921976835, 0.6110786660574377, 0.611807671142742, 0.7422640591394156, 0.6334115588106215, 0.6021480788476765, 0.633173723006621, 0.7656571571715176, 0.8585104888770729, 0.9223442571237683, 0.6205378449521959, 0.6288092930335552, 0.6461293592583388, 0.6317391486372799, 0.6271535980049521, 0.7047562387306243, 0.6864501661621034, 0.6123544361907989, 0.6014785028528422, 0.6121442893054336, 0.6265811019111425, 0.6475774112623185, 0.7009611318353564, 0.6299060757737607, 0.6281356308609247, 0.6462750760838389, 0.6548851591069251, 0.704154907958582, 0.6534252448473126, 0.6439691081177443, 0.649118437198922, 0.6432396860327572, 0.7548822939861566, 0.6440899940207601, 0.6422222349792719, 0.6433351589366794, 0.6262167587410659, 0.645479897968471, 0.7587968641892076, 0.6309614046476781, 0.619128031656146, 0.6258940978441387, 0.6100664916448295, 0.7040984090417624, 0.636136335786432, 0.6108980858698487, 0.6042089003603905, 0.6118947709910572, 0.6207397419493645, 0.6232438979204744, 0.7124826097860932, 0.6790870719123632, 0.6324978319462389, 0.6148596310522407, 0.6181039540097117, 0.7125785169191658, 0.8110160927753896, 0.6185789287555963, 0.6176790660247207, 0.6614403710700572, 0.6358418648596853, 0.6488327572587878, 0.6665504991542548, 0.7127905550878495, 0.6436773319728673, 0.637819031951949, 0.6358656480442733, 0.6346185761503875, 0.6691246849950403, 0.6869759082328528, 0.5952736649196595, 0.5936888579744846, 0.5989572440739721, 0.6051724350545555, 0.6350987567566335, 0.7223053041379899, 0.6289139459840953, 0.6138065848499537, 0.6149815092794597, 0.6387717181351036, 0.6876337539870292, 0.6300305081531405, 0.5972206501755863, 0.6194997399579734, 0.6194886022713035, 0.6780743482522666, 0.6007922110147774, 0.607094454113394, 0.6030991331208497, 0.6484303688630462, 0.669587975833565, 0.7120329581666738, 0.6427611319813877, 0.6542494290042669, 0.6861883110832423, 0.70998086896725, 0.6509015080519021, 0.6459413538686931, 0.6415619209874421, 0.687870895024389, 0.615359028801322, 0.6691649237181991, 0.8567845290526748, 0.7315538208931684, 0.600164548959583, 0.6145262520294636, 0.7352747691329569, 0.8466746367048472, 0.6535925508942455, 0.6483875911217183, 0.6367520499043167, 0.646316773025319, 0.6493179539684206, 0.6178182500880212, 0.7021832410246134, 0.6063761338591576, 0.6222410111222416, 0.6000199681147933, 0.6044941910076886, 0.6068474899511784, 0.7977545778267086, 0.6850984757766128, 0.6330660067033023, 0.6558946487493813, 0.8588135219179094, 0.902905142866075, 0.7266943610738963, 0.6453648721799254, 0.6460720137692988, 0.715919905109331, 0.9086252423003316, 0.685586160980165, 0.642233096063137, 0.598607970867306, 0.6206793352030218]
Total Epoch List: [255, 257]
Total Time List: [0.25019272416830063, 0.18831530096940696]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d8dcd383910>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5306;  Loss pred: 2.5306; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.22s
Epoch 2/1000, LR 0.000020
Train loss: 2.5323;  Loss pred: 2.5323; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 2.4912;  Loss pred: 2.4912; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.16s
Epoch 4/1000, LR 0.000080
Train loss: 2.4701;  Loss pred: 2.4701; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 2.4338;  Loss pred: 2.4338; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.18s
Epoch 6/1000, LR 0.000140
Train loss: 2.3703;  Loss pred: 2.3703; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.16s
Epoch 7/1000, LR 0.000170
Train loss: 2.3089;  Loss pred: 2.3089; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.16s
Epoch 8/1000, LR 0.000200
Train loss: 2.2261;  Loss pred: 2.2261; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.21s
Epoch 9/1000, LR 0.000230
Train loss: 2.1523;  Loss pred: 2.1523; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.21s
Epoch 10/1000, LR 0.000260
Train loss: 2.0968;  Loss pred: 2.0968; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 1.9947;  Loss pred: 1.9947; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 1.9187;  Loss pred: 1.9187; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 1.8272;  Loss pred: 1.8272; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.16s
Epoch 14/1000, LR 0.000290
Train loss: 1.7653;  Loss pred: 1.7653; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
Epoch 15/1000, LR 0.000290
Train loss: 1.7084;  Loss pred: 1.7084; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.16s
Epoch 16/1000, LR 0.000290
Train loss: 1.6603;  Loss pred: 1.6603; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.16s
Epoch 17/1000, LR 0.000290
Train loss: 1.5844;  Loss pred: 1.5844; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.16s
Epoch 18/1000, LR 0.000290
Train loss: 1.5481;  Loss pred: 1.5481; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.18s
Epoch 19/1000, LR 0.000290
Train loss: 1.4994;  Loss pred: 1.4994; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.21s
Epoch 20/1000, LR 0.000290
Train loss: 1.4572;  Loss pred: 1.4572; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 1.4115;  Loss pred: 1.4115; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
Test loss: 0.6927 score: 0.5078 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 1.3744;  Loss pred: 1.3744; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
Test loss: 0.6925 score: 0.5078 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 1.3462;  Loss pred: 1.3462; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.17s
Test loss: 0.6924 score: 0.5078 time: 0.16s
Epoch 24/1000, LR 0.000290
Train loss: 1.3127;  Loss pred: 1.3127; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
Test loss: 0.6923 score: 0.5078 time: 0.17s
Epoch 25/1000, LR 0.000290
Train loss: 1.2852;  Loss pred: 1.2852; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.25s
Test loss: 0.6924 score: 0.5078 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000290
Train loss: 1.2713;  Loss pred: 1.2713; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.17s
Test loss: 0.6924 score: 0.5078 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 1.2375;  Loss pred: 1.2375; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
Test loss: 0.6923 score: 0.5078 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 1.2223;  Loss pred: 1.2223; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.19s
Test loss: 0.6922 score: 0.5078 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 1.2034;  Loss pred: 1.2034; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.18s
Test loss: 0.6921 score: 0.5078 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 1.1877;  Loss pred: 1.1877; Loss self: 0.0000; time: 0.30s
Val loss: 0.6921 score: 0.5116 time: 0.20s
Test loss: 0.6919 score: 0.5156 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 1.1716;  Loss pred: 1.1716; Loss self: 0.0000; time: 0.37s
Val loss: 0.6920 score: 0.5039 time: 0.19s
Test loss: 0.6918 score: 0.5234 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 1.1554;  Loss pred: 1.1554; Loss self: 0.0000; time: 0.33s
Val loss: 0.6919 score: 0.5194 time: 0.17s
Test loss: 0.6917 score: 0.5391 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 1.1417;  Loss pred: 1.1417; Loss self: 0.0000; time: 0.28s
Val loss: 0.6918 score: 0.5271 time: 0.19s
Test loss: 0.6915 score: 0.5781 time: 0.17s
Epoch 34/1000, LR 0.000290
Train loss: 1.1346;  Loss pred: 1.1346; Loss self: 0.0000; time: 0.30s
Val loss: 0.6917 score: 0.5271 time: 0.19s
Test loss: 0.6914 score: 0.5781 time: 0.15s
Epoch 35/1000, LR 0.000290
Train loss: 1.1257;  Loss pred: 1.1257; Loss self: 0.0000; time: 0.31s
Val loss: 0.6915 score: 0.5039 time: 0.18s
Test loss: 0.6912 score: 0.5234 time: 0.21s
Epoch 36/1000, LR 0.000290
Train loss: 1.1163;  Loss pred: 1.1163; Loss self: 0.0000; time: 0.30s
Val loss: 0.6914 score: 0.5116 time: 0.17s
Test loss: 0.6910 score: 0.5156 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 1.1067;  Loss pred: 1.1067; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.18s
Test loss: 0.6909 score: 0.5078 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 1.0944;  Loss pred: 1.0944; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.18s
Test loss: 0.6906 score: 0.5078 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 1.0901;  Loss pred: 1.0901; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.19s
Test loss: 0.6903 score: 0.5078 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 1.0799;  Loss pred: 1.0799; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.26s
Test loss: 0.6900 score: 0.5078 time: 0.21s
Epoch 41/1000, LR 0.000289
Train loss: 1.0738;  Loss pred: 1.0738; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5039 time: 0.18s
Test loss: 0.6897 score: 0.5078 time: 0.16s
Epoch 42/1000, LR 0.000289
Train loss: 1.0681;  Loss pred: 1.0681; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5039 time: 0.18s
Test loss: 0.6893 score: 0.5078 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 1.0623;  Loss pred: 1.0623; Loss self: 0.0000; time: 0.29s
Val loss: 0.6894 score: 0.5116 time: 0.18s
Test loss: 0.6888 score: 0.5234 time: 0.26s
Epoch 44/1000, LR 0.000289
Train loss: 1.0558;  Loss pred: 1.0558; Loss self: 0.0000; time: 0.28s
Val loss: 0.6890 score: 0.5271 time: 0.18s
Test loss: 0.6883 score: 0.5781 time: 0.16s
Epoch 45/1000, LR 0.000289
Train loss: 1.0499;  Loss pred: 1.0499; Loss self: 0.0000; time: 0.29s
Val loss: 0.6887 score: 0.5504 time: 0.20s
Test loss: 0.6879 score: 0.5859 time: 0.16s
Epoch 46/1000, LR 0.000289
Train loss: 1.0472;  Loss pred: 1.0472; Loss self: 0.0000; time: 0.37s
Val loss: 0.6883 score: 0.5814 time: 0.17s
Test loss: 0.6875 score: 0.6016 time: 0.16s
Epoch 47/1000, LR 0.000289
Train loss: 1.0409;  Loss pred: 1.0409; Loss self: 0.0000; time: 0.30s
Val loss: 0.6876 score: 0.6899 time: 0.17s
Test loss: 0.6867 score: 0.7344 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 1.0374;  Loss pred: 1.0374; Loss self: 0.0000; time: 0.29s
Val loss: 0.6871 score: 0.8682 time: 0.18s
Test loss: 0.6861 score: 0.9141 time: 0.16s
Epoch 49/1000, LR 0.000289
Train loss: 1.0330;  Loss pred: 1.0330; Loss self: 0.0000; time: 0.33s
Val loss: 0.6866 score: 0.8450 time: 0.17s
Test loss: 0.6856 score: 0.8906 time: 0.16s
Epoch 50/1000, LR 0.000289
Train loss: 1.0295;  Loss pred: 1.0295; Loss self: 0.0000; time: 0.47s
Val loss: 0.6861 score: 0.6512 time: 0.25s
Test loss: 0.6850 score: 0.6719 time: 0.21s
Epoch 51/1000, LR 0.000289
Train loss: 1.0255;  Loss pred: 1.0255; Loss self: 0.0000; time: 0.51s
Val loss: 0.6856 score: 0.5116 time: 0.23s
Test loss: 0.6843 score: 0.5234 time: 0.17s
Epoch 52/1000, LR 0.000289
Train loss: 1.0246;  Loss pred: 1.0246; Loss self: 0.0000; time: 0.30s
Val loss: 0.6849 score: 0.5116 time: 0.19s
Test loss: 0.6835 score: 0.5156 time: 0.17s
Epoch 53/1000, LR 0.000289
Train loss: 1.0201;  Loss pred: 1.0201; Loss self: 0.0000; time: 0.30s
Val loss: 0.6841 score: 0.5039 time: 0.18s
Test loss: 0.6826 score: 0.5156 time: 0.17s
Epoch 54/1000, LR 0.000289
Train loss: 1.0166;  Loss pred: 1.0166; Loss self: 0.0000; time: 0.30s
Val loss: 0.6832 score: 0.5659 time: 0.18s
Test loss: 0.6816 score: 0.5703 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 1.0122;  Loss pred: 1.0122; Loss self: 0.0000; time: 0.35s
Val loss: 0.6822 score: 0.6589 time: 0.26s
Test loss: 0.6805 score: 0.6797 time: 0.22s
Epoch 56/1000, LR 0.000289
Train loss: 1.0102;  Loss pred: 1.0102; Loss self: 0.0000; time: 0.49s
Val loss: 0.6812 score: 0.6667 time: 0.26s
Test loss: 0.6794 score: 0.6875 time: 0.22s
Epoch 57/1000, LR 0.000288
Train loss: 1.0078;  Loss pred: 1.0078; Loss self: 0.0000; time: 0.30s
Val loss: 0.6802 score: 0.6202 time: 0.19s
Test loss: 0.6783 score: 0.6406 time: 0.16s
Epoch 58/1000, LR 0.000288
Train loss: 1.0044;  Loss pred: 1.0044; Loss self: 0.0000; time: 0.29s
Val loss: 0.6792 score: 0.5426 time: 0.19s
Test loss: 0.6771 score: 0.5547 time: 0.17s
Epoch 59/1000, LR 0.000288
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6783 score: 0.4961 time: 0.18s
Test loss: 0.6760 score: 0.5156 time: 0.16s
Epoch 60/1000, LR 0.000288
Train loss: 0.9993;  Loss pred: 0.9993; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.16s
Epoch 61/1000, LR 0.000288
Train loss: 0.9960;  Loss pred: 0.9960; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6760 score: 0.4961 time: 0.18s
Test loss: 0.6734 score: 0.5156 time: 0.18s
Epoch 62/1000, LR 0.000288
Train loss: 0.9950;  Loss pred: 0.9950; Loss self: 0.0000; time: 0.41s
Val loss: 0.6746 score: 0.5426 time: 0.24s
Test loss: 0.6718 score: 0.5547 time: 0.17s
Epoch 63/1000, LR 0.000288
Train loss: 0.9926;  Loss pred: 0.9926; Loss self: 0.0000; time: 0.29s
Val loss: 0.6730 score: 0.5736 time: 0.18s
Test loss: 0.6701 score: 0.6016 time: 0.17s
Epoch 64/1000, LR 0.000288
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.29s
Val loss: 0.6714 score: 0.5736 time: 0.18s
Test loss: 0.6683 score: 0.5781 time: 0.16s
Epoch 65/1000, LR 0.000288
Train loss: 0.9876;  Loss pred: 0.9876; Loss self: 0.0000; time: 0.29s
Val loss: 0.6697 score: 0.5736 time: 0.17s
Test loss: 0.6664 score: 0.5781 time: 0.16s
Epoch 66/1000, LR 0.000288
Train loss: 0.9841;  Loss pred: 0.9841; Loss self: 0.0000; time: 0.29s
Val loss: 0.6676 score: 0.6202 time: 0.19s
Test loss: 0.6641 score: 0.6406 time: 0.16s
Epoch 67/1000, LR 0.000288
Train loss: 0.9822;  Loss pred: 0.9822; Loss self: 0.0000; time: 0.36s
Val loss: 0.6654 score: 0.7597 time: 0.18s
Test loss: 0.6618 score: 0.8047 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.9790;  Loss pred: 0.9790; Loss self: 0.0000; time: 0.29s
Val loss: 0.6633 score: 0.8140 time: 0.17s
Test loss: 0.6596 score: 0.8438 time: 0.16s
Epoch 69/1000, LR 0.000288
Train loss: 0.9768;  Loss pred: 0.9768; Loss self: 0.0000; time: 0.29s
Val loss: 0.6612 score: 0.8295 time: 0.17s
Test loss: 0.6573 score: 0.8594 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.9738;  Loss pred: 0.9738; Loss self: 0.0000; time: 0.29s
Val loss: 0.6594 score: 0.8605 time: 0.17s
Test loss: 0.6553 score: 0.9062 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 0.9718;  Loss pred: 0.9718; Loss self: 0.0000; time: 0.30s
Val loss: 0.6579 score: 0.8992 time: 0.17s
Test loss: 0.6538 score: 0.9062 time: 0.18s
Epoch 72/1000, LR 0.000287
Train loss: 0.9714;  Loss pred: 0.9714; Loss self: 0.0000; time: 0.39s
Val loss: 0.6561 score: 0.8992 time: 0.20s
Test loss: 0.6519 score: 0.9141 time: 0.16s
Epoch 73/1000, LR 0.000287
Train loss: 0.9688;  Loss pred: 0.9688; Loss self: 0.0000; time: 0.29s
Val loss: 0.6538 score: 0.9070 time: 0.17s
Test loss: 0.6493 score: 0.9141 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9660;  Loss pred: 0.9660; Loss self: 0.0000; time: 0.29s
Val loss: 0.6519 score: 0.8992 time: 0.17s
Test loss: 0.6474 score: 0.9141 time: 0.16s
Epoch 75/1000, LR 0.000287
Train loss: 0.9641;  Loss pred: 0.9641; Loss self: 0.0000; time: 0.28s
Val loss: 0.6495 score: 0.8992 time: 0.17s
Test loss: 0.6447 score: 0.9141 time: 0.18s
Epoch 76/1000, LR 0.000287
Train loss: 0.9618;  Loss pred: 0.9618; Loss self: 0.0000; time: 0.43s
Val loss: 0.6459 score: 0.8992 time: 0.26s
Test loss: 0.6406 score: 0.9062 time: 0.21s
Epoch 77/1000, LR 0.000287
Train loss: 0.9587;  Loss pred: 0.9587; Loss self: 0.0000; time: 0.49s
Val loss: 0.6422 score: 0.8605 time: 0.20s
Test loss: 0.6363 score: 0.8984 time: 0.17s
Epoch 78/1000, LR 0.000287
Train loss: 0.9535;  Loss pred: 0.9535; Loss self: 0.0000; time: 0.30s
Val loss: 0.6392 score: 0.8450 time: 0.19s
Test loss: 0.6329 score: 0.8828 time: 0.17s
Epoch 79/1000, LR 0.000287
Train loss: 0.9516;  Loss pred: 0.9516; Loss self: 0.0000; time: 0.29s
Val loss: 0.6365 score: 0.8450 time: 0.17s
Test loss: 0.6299 score: 0.8828 time: 0.16s
Epoch 80/1000, LR 0.000287
Train loss: 0.9491;  Loss pred: 0.9491; Loss self: 0.0000; time: 0.28s
Val loss: 0.6340 score: 0.8605 time: 0.18s
Test loss: 0.6273 score: 0.8984 time: 0.17s
Epoch 81/1000, LR 0.000286
Train loss: 0.9469;  Loss pred: 0.9469; Loss self: 0.0000; time: 0.30s
Val loss: 0.6315 score: 0.8760 time: 0.18s
Test loss: 0.6248 score: 0.9062 time: 0.17s
Epoch 82/1000, LR 0.000286
Train loss: 0.9448;  Loss pred: 0.9448; Loss self: 0.0000; time: 0.33s
Val loss: 0.6283 score: 0.8605 time: 0.19s
Test loss: 0.6213 score: 0.9062 time: 0.20s
Epoch 83/1000, LR 0.000286
Train loss: 0.9420;  Loss pred: 0.9420; Loss self: 0.0000; time: 0.33s
Val loss: 0.6248 score: 0.8605 time: 0.19s
Test loss: 0.6174 score: 0.8984 time: 0.17s
Epoch 84/1000, LR 0.000286
Train loss: 0.9396;  Loss pred: 0.9396; Loss self: 0.0000; time: 0.29s
Val loss: 0.6212 score: 0.8605 time: 0.17s
Test loss: 0.6136 score: 0.9062 time: 0.16s
Epoch 85/1000, LR 0.000286
Train loss: 0.9369;  Loss pred: 0.9369; Loss self: 0.0000; time: 0.29s
Val loss: 0.6175 score: 0.8760 time: 0.19s
Test loss: 0.6096 score: 0.9062 time: 0.22s
Epoch 86/1000, LR 0.000286
Train loss: 0.9338;  Loss pred: 0.9338; Loss self: 0.0000; time: 0.48s
Val loss: 0.6137 score: 0.8837 time: 0.26s
Test loss: 0.6055 score: 0.9062 time: 0.22s
Epoch 87/1000, LR 0.000286
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.48s
Val loss: 0.6094 score: 0.8915 time: 0.27s
Test loss: 0.6009 score: 0.9062 time: 0.22s
Epoch 88/1000, LR 0.000286
Train loss: 0.9272;  Loss pred: 0.9272; Loss self: 0.0000; time: 0.39s
Val loss: 0.6043 score: 0.8760 time: 0.19s
Test loss: 0.5951 score: 0.9062 time: 0.17s
Epoch 89/1000, LR 0.000286
Train loss: 0.9239;  Loss pred: 0.9239; Loss self: 0.0000; time: 0.29s
Val loss: 0.5994 score: 0.8605 time: 0.17s
Test loss: 0.5892 score: 0.8984 time: 0.16s
Epoch 90/1000, LR 0.000285
Train loss: 0.9187;  Loss pred: 0.9187; Loss self: 0.0000; time: 0.29s
Val loss: 0.5956 score: 0.8527 time: 0.17s
Test loss: 0.5845 score: 0.8828 time: 0.16s
Epoch 91/1000, LR 0.000285
Train loss: 0.9179;  Loss pred: 0.9179; Loss self: 0.0000; time: 0.29s
Val loss: 0.5924 score: 0.8295 time: 0.17s
Test loss: 0.5807 score: 0.8516 time: 0.17s
Epoch 92/1000, LR 0.000285
Train loss: 0.9148;  Loss pred: 0.9148; Loss self: 0.0000; time: 0.29s
Val loss: 0.5870 score: 0.8450 time: 0.17s
Test loss: 0.5750 score: 0.8828 time: 0.16s
Epoch 93/1000, LR 0.000285
Train loss: 0.9103;  Loss pred: 0.9103; Loss self: 0.0000; time: 0.30s
Val loss: 0.5813 score: 0.8605 time: 0.17s
Test loss: 0.5692 score: 0.8984 time: 0.17s
Epoch 94/1000, LR 0.000285
Train loss: 0.9067;  Loss pred: 0.9067; Loss self: 0.0000; time: 0.32s
Val loss: 0.5761 score: 0.8605 time: 0.22s
Test loss: 0.5640 score: 0.9062 time: 0.16s
Epoch 95/1000, LR 0.000285
Train loss: 0.9019;  Loss pred: 0.9019; Loss self: 0.0000; time: 0.29s
Val loss: 0.5724 score: 0.8915 time: 0.18s
Test loss: 0.5605 score: 0.9062 time: 0.17s
Epoch 96/1000, LR 0.000285
Train loss: 0.9000;  Loss pred: 0.9000; Loss self: 0.0000; time: 0.30s
Val loss: 0.5701 score: 0.9070 time: 0.18s
Test loss: 0.5582 score: 0.9219 time: 0.17s
Epoch 97/1000, LR 0.000285
Train loss: 0.8986;  Loss pred: 0.8986; Loss self: 0.0000; time: 0.31s
Val loss: 0.5655 score: 0.9070 time: 0.19s
Test loss: 0.5533 score: 0.9219 time: 0.17s
Epoch 98/1000, LR 0.000285
Train loss: 0.8942;  Loss pred: 0.8942; Loss self: 0.0000; time: 0.28s
Val loss: 0.5611 score: 0.9070 time: 0.18s
Test loss: 0.5486 score: 0.9219 time: 0.16s
Epoch 99/1000, LR 0.000284
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.32s
Val loss: 0.5584 score: 0.9070 time: 0.20s
Test loss: 0.5459 score: 0.9219 time: 0.16s
Epoch 100/1000, LR 0.000284
Train loss: 0.8888;  Loss pred: 0.8888; Loss self: 0.0000; time: 0.29s
Val loss: 0.5533 score: 0.9070 time: 0.26s
Test loss: 0.5403 score: 0.9219 time: 0.16s
Epoch 101/1000, LR 0.000284
Train loss: 0.8849;  Loss pred: 0.8849; Loss self: 0.0000; time: 0.29s
Val loss: 0.5468 score: 0.8992 time: 0.18s
Test loss: 0.5332 score: 0.9219 time: 0.18s
Epoch 102/1000, LR 0.000284
Train loss: 0.8798;  Loss pred: 0.8798; Loss self: 0.0000; time: 0.30s
Val loss: 0.5411 score: 0.8760 time: 0.19s
Test loss: 0.5269 score: 0.9062 time: 0.17s
Epoch 103/1000, LR 0.000284
Train loss: 0.8776;  Loss pred: 0.8776; Loss self: 0.0000; time: 0.30s
Val loss: 0.5377 score: 0.8605 time: 0.18s
Test loss: 0.5226 score: 0.8984 time: 0.18s
Epoch 104/1000, LR 0.000284
Train loss: 0.8751;  Loss pred: 0.8751; Loss self: 0.0000; time: 0.40s
Val loss: 0.5337 score: 0.8605 time: 0.26s
Test loss: 0.5182 score: 0.8906 time: 0.23s
Epoch 105/1000, LR 0.000284
Train loss: 0.8719;  Loss pred: 0.8719; Loss self: 0.0000; time: 0.45s
Val loss: 0.5307 score: 0.8605 time: 0.24s
Test loss: 0.5146 score: 0.8906 time: 0.18s
Epoch 106/1000, LR 0.000283
Train loss: 0.8697;  Loss pred: 0.8697; Loss self: 0.0000; time: 0.31s
Val loss: 0.5285 score: 0.8450 time: 0.19s
Test loss: 0.5117 score: 0.8828 time: 0.17s
Epoch 107/1000, LR 0.000283
Train loss: 0.8686;  Loss pred: 0.8686; Loss self: 0.0000; time: 0.30s
Val loss: 0.5251 score: 0.8372 time: 0.18s
Test loss: 0.5079 score: 0.8828 time: 0.17s
Epoch 108/1000, LR 0.000283
Train loss: 0.8645;  Loss pred: 0.8645; Loss self: 0.0000; time: 0.30s
Val loss: 0.5177 score: 0.8605 time: 0.19s
Test loss: 0.5007 score: 0.8828 time: 0.17s
Epoch 109/1000, LR 0.000283
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 0.30s
Val loss: 0.5081 score: 0.8605 time: 0.19s
Test loss: 0.4921 score: 0.8984 time: 0.19s
Epoch 110/1000, LR 0.000283
Train loss: 0.8536;  Loss pred: 0.8536; Loss self: 0.0000; time: 0.30s
Val loss: 0.5008 score: 0.8837 time: 0.19s
Test loss: 0.4855 score: 0.9141 time: 0.23s
Epoch 111/1000, LR 0.000283
Train loss: 0.8500;  Loss pred: 0.8500; Loss self: 0.0000; time: 0.30s
Val loss: 0.4955 score: 0.9070 time: 0.18s
Test loss: 0.4806 score: 0.9219 time: 0.17s
Epoch 112/1000, LR 0.000283
Train loss: 0.8451;  Loss pred: 0.8451; Loss self: 0.0000; time: 0.28s
Val loss: 0.4910 score: 0.9070 time: 0.17s
Test loss: 0.4764 score: 0.9219 time: 0.16s
Epoch 113/1000, LR 0.000282
Train loss: 0.8433;  Loss pred: 0.8433; Loss self: 0.0000; time: 0.29s
Val loss: 0.4873 score: 0.9070 time: 0.18s
Test loss: 0.4731 score: 0.9219 time: 0.16s
Epoch 114/1000, LR 0.000282
Train loss: 0.8391;  Loss pred: 0.8391; Loss self: 0.0000; time: 0.29s
Val loss: 0.4827 score: 0.9070 time: 0.19s
Test loss: 0.4688 score: 0.9219 time: 0.17s
Epoch 115/1000, LR 0.000282
Train loss: 0.8384;  Loss pred: 0.8384; Loss self: 0.0000; time: 0.30s
Val loss: 0.4783 score: 0.8992 time: 0.24s
Test loss: 0.4646 score: 0.9297 time: 0.21s
Epoch 116/1000, LR 0.000282
Train loss: 0.8331;  Loss pred: 0.8331; Loss self: 0.0000; time: 0.50s
Val loss: 0.4726 score: 0.9070 time: 0.26s
Test loss: 0.4590 score: 0.9297 time: 0.19s
Epoch 117/1000, LR 0.000282
Train loss: 0.8320;  Loss pred: 0.8320; Loss self: 0.0000; time: 0.37s
Val loss: 0.4695 score: 0.9070 time: 0.18s
Test loss: 0.4563 score: 0.9297 time: 0.16s
Epoch 118/1000, LR 0.000282
Train loss: 0.8264;  Loss pred: 0.8264; Loss self: 0.0000; time: 0.30s
Val loss: 0.4644 score: 0.9070 time: 0.17s
Test loss: 0.4513 score: 0.9297 time: 0.16s
Epoch 119/1000, LR 0.000282
Train loss: 0.8229;  Loss pred: 0.8229; Loss self: 0.0000; time: 0.30s
Val loss: 0.4561 score: 0.9070 time: 0.18s
Test loss: 0.4428 score: 0.9219 time: 0.16s
Epoch 120/1000, LR 0.000281
Train loss: 0.8178;  Loss pred: 0.8178; Loss self: 0.0000; time: 0.30s
Val loss: 0.4490 score: 0.9070 time: 0.18s
Test loss: 0.4356 score: 0.9219 time: 0.16s
Epoch 121/1000, LR 0.000281
Train loss: 0.8134;  Loss pred: 0.8134; Loss self: 0.0000; time: 0.30s
Val loss: 0.4438 score: 0.9070 time: 0.17s
Test loss: 0.4304 score: 0.9219 time: 0.17s
Epoch 122/1000, LR 0.000281
Train loss: 0.8086;  Loss pred: 0.8086; Loss self: 0.0000; time: 0.31s
Val loss: 0.4382 score: 0.9147 time: 0.26s
Test loss: 0.4244 score: 0.9219 time: 0.20s
Epoch 123/1000, LR 0.000281
Train loss: 0.8058;  Loss pred: 0.8058; Loss self: 0.0000; time: 0.34s
Val loss: 0.4339 score: 0.8992 time: 0.19s
Test loss: 0.4196 score: 0.9219 time: 0.17s
Epoch 124/1000, LR 0.000281
Train loss: 0.8044;  Loss pred: 0.8044; Loss self: 0.0000; time: 0.30s
Val loss: 0.4304 score: 0.8837 time: 0.19s
Test loss: 0.4154 score: 0.9141 time: 0.16s
Epoch 125/1000, LR 0.000281
Train loss: 0.8017;  Loss pred: 0.8017; Loss self: 0.0000; time: 0.28s
Val loss: 0.4278 score: 0.8837 time: 0.17s
Test loss: 0.4122 score: 0.9062 time: 0.16s
Epoch 126/1000, LR 0.000280
Train loss: 0.8012;  Loss pred: 0.8012; Loss self: 0.0000; time: 0.29s
Val loss: 0.4243 score: 0.8837 time: 0.18s
Test loss: 0.4083 score: 0.9062 time: 0.16s
Epoch 127/1000, LR 0.000280
Train loss: 0.7975;  Loss pred: 0.7975; Loss self: 0.0000; time: 0.33s
Val loss: 0.4214 score: 0.8605 time: 0.18s
Test loss: 0.4049 score: 0.9062 time: 0.19s
Epoch 128/1000, LR 0.000280
Train loss: 0.7968;  Loss pred: 0.7968; Loss self: 0.0000; time: 0.48s
Val loss: 0.4155 score: 0.8760 time: 0.27s
Test loss: 0.3993 score: 0.9062 time: 0.16s
Epoch 129/1000, LR 0.000280
Train loss: 0.7936;  Loss pred: 0.7936; Loss self: 0.0000; time: 0.35s
Val loss: 0.4050 score: 0.8915 time: 0.17s
Test loss: 0.3903 score: 0.9141 time: 0.16s
Epoch 130/1000, LR 0.000280
Train loss: 0.7860;  Loss pred: 0.7860; Loss self: 0.0000; time: 0.29s
Val loss: 0.3969 score: 0.9147 time: 0.17s
Test loss: 0.3837 score: 0.9219 time: 0.16s
Epoch 131/1000, LR 0.000280
Train loss: 0.7798;  Loss pred: 0.7798; Loss self: 0.0000; time: 0.30s
Val loss: 0.3919 score: 0.9225 time: 0.17s
Test loss: 0.3790 score: 0.9219 time: 0.25s
Epoch 132/1000, LR 0.000279
Train loss: 0.7764;  Loss pred: 0.7764; Loss self: 0.0000; time: 0.44s
Val loss: 0.3879 score: 0.9070 time: 0.27s
Test loss: 0.3748 score: 0.9219 time: 0.23s
Epoch 133/1000, LR 0.000279
Train loss: 0.7742;  Loss pred: 0.7742; Loss self: 0.0000; time: 0.30s
Val loss: 0.3846 score: 0.9070 time: 0.18s
Test loss: 0.3712 score: 0.9219 time: 0.26s
Epoch 134/1000, LR 0.000279
Train loss: 0.7736;  Loss pred: 0.7736; Loss self: 0.0000; time: 0.29s
Val loss: 0.3809 score: 0.8992 time: 0.17s
Test loss: 0.3673 score: 0.9141 time: 0.16s
Epoch 135/1000, LR 0.000279
Train loss: 0.7706;  Loss pred: 0.7706; Loss self: 0.0000; time: 0.29s
Val loss: 0.3761 score: 0.8992 time: 0.18s
Test loss: 0.3628 score: 0.9219 time: 0.17s
Epoch 136/1000, LR 0.000279
Train loss: 0.7660;  Loss pred: 0.7660; Loss self: 0.0000; time: 0.36s
Val loss: 0.3705 score: 0.9070 time: 0.24s
Test loss: 0.3578 score: 0.9219 time: 0.20s
Epoch 137/1000, LR 0.000279
Train loss: 0.7618;  Loss pred: 0.7618; Loss self: 0.0000; time: 0.29s
Val loss: 0.3630 score: 0.9225 time: 0.18s
Test loss: 0.3518 score: 0.9219 time: 0.17s
Epoch 138/1000, LR 0.000278
Train loss: 0.7574;  Loss pred: 0.7574; Loss self: 0.0000; time: 0.30s
Val loss: 0.3582 score: 0.9070 time: 0.17s
Test loss: 0.3488 score: 0.9219 time: 0.16s
Epoch 139/1000, LR 0.000278
Train loss: 0.7536;  Loss pred: 0.7536; Loss self: 0.0000; time: 0.29s
Val loss: 0.3556 score: 0.9070 time: 0.18s
Test loss: 0.3472 score: 0.9297 time: 0.17s
Epoch 140/1000, LR 0.000278
Train loss: 0.7515;  Loss pred: 0.7515; Loss self: 0.0000; time: 0.29s
Val loss: 0.3526 score: 0.9070 time: 0.19s
Test loss: 0.3449 score: 0.9297 time: 0.17s
Epoch 141/1000, LR 0.000278
Train loss: 0.7513;  Loss pred: 0.7513; Loss self: 0.0000; time: 0.35s
Val loss: 0.3473 score: 0.9070 time: 0.17s
Test loss: 0.3400 score: 0.9297 time: 0.16s
Epoch 142/1000, LR 0.000278
Train loss: 0.7471;  Loss pred: 0.7471; Loss self: 0.0000; time: 0.29s
Val loss: 0.3431 score: 0.9070 time: 0.18s
Test loss: 0.3363 score: 0.9297 time: 0.16s
Epoch 143/1000, LR 0.000277
Train loss: 0.7433;  Loss pred: 0.7433; Loss self: 0.0000; time: 0.29s
Val loss: 0.3393 score: 0.9070 time: 0.18s
Test loss: 0.3331 score: 0.9297 time: 0.17s
Epoch 144/1000, LR 0.000277
Train loss: 0.7387;  Loss pred: 0.7387; Loss self: 0.0000; time: 0.29s
Val loss: 0.3330 score: 0.9147 time: 0.18s
Test loss: 0.3268 score: 0.9297 time: 0.18s
Epoch 145/1000, LR 0.000277
Train loss: 0.7369;  Loss pred: 0.7369; Loss self: 0.0000; time: 0.30s
Val loss: 0.3294 score: 0.9070 time: 0.21s
Test loss: 0.3225 score: 0.9219 time: 0.17s
Epoch 146/1000, LR 0.000277
Train loss: 0.7340;  Loss pred: 0.7340; Loss self: 0.0000; time: 0.49s
Val loss: 0.3269 score: 0.9070 time: 0.26s
Test loss: 0.3199 score: 0.9219 time: 0.23s
Epoch 147/1000, LR 0.000277
Train loss: 0.7314;  Loss pred: 0.7314; Loss self: 0.0000; time: 0.49s
Val loss: 0.3229 score: 0.9070 time: 0.20s
Test loss: 0.3167 score: 0.9219 time: 0.17s
Epoch 148/1000, LR 0.000277
Train loss: 0.7280;  Loss pred: 0.7280; Loss self: 0.0000; time: 0.30s
Val loss: 0.3196 score: 0.9070 time: 0.19s
Test loss: 0.3138 score: 0.9219 time: 0.17s
Epoch 149/1000, LR 0.000276
Train loss: 0.7263;  Loss pred: 0.7263; Loss self: 0.0000; time: 0.31s
Val loss: 0.3153 score: 0.9070 time: 0.18s
Test loss: 0.3107 score: 0.9219 time: 0.17s
Epoch 150/1000, LR 0.000276
Train loss: 0.7221;  Loss pred: 0.7221; Loss self: 0.0000; time: 0.31s
Val loss: 0.3122 score: 0.9070 time: 0.19s
Test loss: 0.3101 score: 0.9297 time: 0.17s
Epoch 151/1000, LR 0.000276
Train loss: 0.7229;  Loss pred: 0.7229; Loss self: 0.0000; time: 0.33s
Val loss: 0.3200 score: 0.9302 time: 0.17s
Test loss: 0.3195 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 152/1000, LR 0.000276
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 0.28s
Val loss: 0.3397 score: 0.9302 time: 0.17s
Test loss: 0.3388 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 153/1000, LR 0.000276
Train loss: 0.7377;  Loss pred: 0.7377; Loss self: 0.0000; time: 0.28s
Val loss: 0.3513 score: 0.8837 time: 0.17s
Test loss: 0.3499 score: 0.8516 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 154/1000, LR 0.000275
Train loss: 0.7408;  Loss pred: 0.7408; Loss self: 0.0000; time: 0.29s
Val loss: 0.3445 score: 0.9070 time: 0.17s
Test loss: 0.3439 score: 0.8672 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 155/1000, LR 0.000275
Train loss: 0.7350;  Loss pred: 0.7350; Loss self: 0.0000; time: 0.29s
Val loss: 0.3390 score: 0.9147 time: 0.18s
Test loss: 0.3393 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 156/1000, LR 0.000275
Train loss: 0.7318;  Loss pred: 0.7318; Loss self: 0.0000; time: 0.28s
Val loss: 0.3323 score: 0.9225 time: 0.17s
Test loss: 0.3334 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 157/1000, LR 0.000275
Train loss: 0.7266;  Loss pred: 0.7266; Loss self: 0.0000; time: 0.31s
Val loss: 0.3280 score: 0.9225 time: 0.18s
Test loss: 0.3299 score: 0.8906 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 158/1000, LR 0.000275
Train loss: 0.7235;  Loss pred: 0.7235; Loss self: 0.0000; time: 0.31s
Val loss: 0.3188 score: 0.9535 time: 0.19s
Test loss: 0.3217 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 159/1000, LR 0.000274
Train loss: 0.7147;  Loss pred: 0.7147; Loss self: 0.0000; time: 0.31s
Val loss: 0.3044 score: 0.9457 time: 0.19s
Test loss: 0.3085 score: 0.9219 time: 0.17s
Epoch 160/1000, LR 0.000274
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.31s
Val loss: 0.2904 score: 0.9147 time: 0.18s
Test loss: 0.2952 score: 0.9297 time: 0.17s
Epoch 161/1000, LR 0.000274
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.31s
Val loss: 0.2834 score: 0.9147 time: 0.18s
Test loss: 0.2881 score: 0.9297 time: 0.17s
Epoch 162/1000, LR 0.000274
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.33s
Val loss: 0.2794 score: 0.9147 time: 0.18s
Test loss: 0.2834 score: 0.9219 time: 0.22s
Epoch 163/1000, LR 0.000273
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.2794 score: 0.9070 time: 0.18s
Test loss: 0.2819 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.30s
Val loss: 0.2791 score: 0.9225 time: 0.18s
Test loss: 0.2809 score: 0.9219 time: 0.16s
Epoch 165/1000, LR 0.000273
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.30s
Val loss: 0.2758 score: 0.9147 time: 0.18s
Test loss: 0.2783 score: 0.9219 time: 0.16s
Epoch 166/1000, LR 0.000273
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.30s
Val loss: 0.2745 score: 0.9225 time: 0.17s
Test loss: 0.2769 score: 0.9219 time: 0.16s
Epoch 167/1000, LR 0.000273
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.30s
Val loss: 0.2743 score: 0.9147 time: 0.25s
Test loss: 0.2763 score: 0.9219 time: 0.16s
Epoch 168/1000, LR 0.000272
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.29s
Val loss: 0.2724 score: 0.9147 time: 0.17s
Test loss: 0.2747 score: 0.9219 time: 0.16s
Epoch 169/1000, LR 0.000272
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.28s
Val loss: 0.2699 score: 0.9147 time: 0.17s
Test loss: 0.2728 score: 0.9219 time: 0.15s
Epoch 170/1000, LR 0.000272
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.29s
Val loss: 0.2672 score: 0.9147 time: 0.17s
Test loss: 0.2706 score: 0.9219 time: 0.15s
Epoch 171/1000, LR 0.000272
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.29s
Val loss: 0.2599 score: 0.9147 time: 0.17s
Test loss: 0.2656 score: 0.9219 time: 0.15s
Epoch 172/1000, LR 0.000271
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.32s
Val loss: 0.2525 score: 0.9147 time: 0.19s
Test loss: 0.2615 score: 0.9219 time: 0.16s
Epoch 173/1000, LR 0.000271
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.29s
Val loss: 0.2512 score: 0.9147 time: 0.17s
Test loss: 0.2623 score: 0.9297 time: 0.21s
Epoch 174/1000, LR 0.000271
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 0.30s
Val loss: 0.2562 score: 0.9457 time: 0.17s
Test loss: 0.2684 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 175/1000, LR 0.000271
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.28s
Val loss: 0.2607 score: 0.9535 time: 0.17s
Test loss: 0.2732 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 176/1000, LR 0.000271
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.29s
Val loss: 0.2657 score: 0.9535 time: 0.24s
Test loss: 0.2785 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 177/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.47s
Val loss: 0.2686 score: 0.9457 time: 0.25s
Test loss: 0.2817 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 178/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.47s
Val loss: 0.2611 score: 0.9535 time: 0.18s
Test loss: 0.2753 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 179/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.28s
Val loss: 0.2518 score: 0.9535 time: 0.17s
Test loss: 0.2670 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 180/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.29s
Val loss: 0.2431 score: 0.9535 time: 0.18s
Test loss: 0.2592 score: 0.9297 time: 0.16s
Epoch 181/1000, LR 0.000269
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.48s
Val loss: 0.2370 score: 0.9225 time: 0.25s
Test loss: 0.2537 score: 0.9297 time: 0.21s
Epoch 182/1000, LR 0.000269
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 0.49s
Val loss: 0.2324 score: 0.9147 time: 0.25s
Test loss: 0.2494 score: 0.9375 time: 0.21s
Epoch 183/1000, LR 0.000269
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.48s
Val loss: 0.2290 score: 0.9147 time: 0.18s
Test loss: 0.2463 score: 0.9297 time: 0.16s
Epoch 184/1000, LR 0.000269
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 0.28s
Val loss: 0.2270 score: 0.9147 time: 0.18s
Test loss: 0.2441 score: 0.9375 time: 0.17s
Epoch 185/1000, LR 0.000268
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.30s
Val loss: 0.2262 score: 0.9147 time: 0.19s
Test loss: 0.2427 score: 0.9297 time: 0.18s
Epoch 186/1000, LR 0.000268
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.32s
Val loss: 0.2279 score: 0.9147 time: 0.17s
Test loss: 0.2430 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000268
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.28s
Val loss: 0.2297 score: 0.9147 time: 0.18s
Test loss: 0.2441 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.45s
Val loss: 0.2319 score: 0.9225 time: 0.25s
Test loss: 0.2457 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.42s
Val loss: 0.2340 score: 0.9225 time: 0.22s
Test loss: 0.2472 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.28s
Val loss: 0.2278 score: 0.9225 time: 0.17s
Test loss: 0.2433 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.28s
Val loss: 0.2191 score: 0.9147 time: 0.17s
Test loss: 0.2382 score: 0.9297 time: 0.16s
Epoch 192/1000, LR 0.000267
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.28s
Val loss: 0.2160 score: 0.9147 time: 0.18s
Test loss: 0.2367 score: 0.9297 time: 0.16s
Epoch 193/1000, LR 0.000266
Train loss: 0.6421;  Loss pred: 0.6421; Loss self: 0.0000; time: 0.28s
Val loss: 0.2138 score: 0.9147 time: 0.18s
Test loss: 0.2361 score: 0.9375 time: 0.16s
Epoch 194/1000, LR 0.000266
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.29s
Val loss: 0.2123 score: 0.9147 time: 0.20s
Test loss: 0.2358 score: 0.9375 time: 0.16s
Epoch 195/1000, LR 0.000266
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.39s
Val loss: 0.2108 score: 0.9147 time: 0.18s
Test loss: 0.2348 score: 0.9375 time: 0.16s
Epoch 196/1000, LR 0.000266
Train loss: 0.6397;  Loss pred: 0.6397; Loss self: 0.0000; time: 0.29s
Val loss: 0.2092 score: 0.9147 time: 0.18s
Test loss: 0.2340 score: 0.9375 time: 0.16s
Epoch 197/1000, LR 0.000265
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.28s
Val loss: 0.2078 score: 0.9147 time: 0.18s
Test loss: 0.2332 score: 0.9375 time: 0.16s
Epoch 198/1000, LR 0.000265
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.29s
Val loss: 0.2072 score: 0.9225 time: 0.19s
Test loss: 0.2337 score: 0.9375 time: 0.19s
Epoch 199/1000, LR 0.000265
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.32s
Val loss: 0.2129 score: 0.9457 time: 0.17s
Test loss: 0.2402 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 200/1000, LR 0.000265
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.28s
Val loss: 0.2224 score: 0.9457 time: 0.17s
Test loss: 0.2495 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 201/1000, LR 0.000264
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.43s
Val loss: 0.2366 score: 0.9225 time: 0.25s
Test loss: 0.2627 score: 0.8672 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 202/1000, LR 0.000264
Train loss: 0.6523;  Loss pred: 0.6523; Loss self: 0.0000; time: 0.48s
Val loss: 0.2486 score: 0.9147 time: 0.26s
Test loss: 0.2738 score: 0.8594 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 203/1000, LR 0.000264
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.49s
Val loss: 0.2625 score: 0.8992 time: 0.25s
Test loss: 0.2868 score: 0.8672 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 204/1000, LR 0.000264
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.42s
Val loss: 0.2644 score: 0.8837 time: 0.21s
Test loss: 0.2892 score: 0.8672 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 205/1000, LR 0.000263
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 0.29s
Val loss: 0.2459 score: 0.9147 time: 0.17s
Test loss: 0.2734 score: 0.8594 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.29s
Val loss: 0.2224 score: 0.9302 time: 0.18s
Test loss: 0.2530 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 207/1000, LR 0.000263
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.30s
Val loss: 0.2051 score: 0.9302 time: 0.19s
Test loss: 0.2375 score: 0.9297 time: 0.17s
Epoch 208/1000, LR 0.000263
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.30s
Val loss: 0.1985 score: 0.9302 time: 0.21s
Test loss: 0.2310 score: 0.9453 time: 0.18s
Epoch 209/1000, LR 0.000262
Train loss: 0.6241;  Loss pred: 0.6241; Loss self: 0.0000; time: 0.38s
Val loss: 0.1991 score: 0.9070 time: 0.19s
Test loss: 0.2303 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.6247;  Loss pred: 0.6247; Loss self: 0.0000; time: 0.30s
Val loss: 0.2047 score: 0.9225 time: 0.19s
Test loss: 0.2335 score: 0.9297 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 211/1000, LR 0.000262
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.30s
Val loss: 0.2105 score: 0.9302 time: 0.18s
Test loss: 0.2371 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 212/1000, LR 0.000261
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.29s
Val loss: 0.2132 score: 0.9302 time: 0.18s
Test loss: 0.2390 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 213/1000, LR 0.000261
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.28s
Val loss: 0.2113 score: 0.9302 time: 0.17s
Test loss: 0.2379 score: 0.9297 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 214/1000, LR 0.000261
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.30s
Val loss: 0.2084 score: 0.9302 time: 0.26s
Test loss: 0.2363 score: 0.9297 time: 0.27s
     INFO: Early stopping counter 6 of 20
Epoch 215/1000, LR 0.000261
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.30s
Val loss: 0.2073 score: 0.9302 time: 0.19s
Test loss: 0.2358 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 216/1000, LR 0.000260
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.31s
Val loss: 0.2075 score: 0.9302 time: 0.19s
Test loss: 0.2363 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 217/1000, LR 0.000260
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.35s
Val loss: 0.2061 score: 0.9302 time: 0.17s
Test loss: 0.2357 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 218/1000, LR 0.000260
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 0.28s
Val loss: 0.2038 score: 0.9302 time: 0.18s
Test loss: 0.2345 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 219/1000, LR 0.000260
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.30s
Val loss: 0.2038 score: 0.9302 time: 0.31s
Test loss: 0.2347 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 220/1000, LR 0.000259
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.50s
Val loss: 0.2038 score: 0.9302 time: 0.26s
Test loss: 0.2349 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 221/1000, LR 0.000259
Train loss: 0.6223;  Loss pred: 0.6223; Loss self: 0.0000; time: 0.29s
Val loss: 0.2019 score: 0.9302 time: 0.22s
Test loss: 0.2339 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 222/1000, LR 0.000259
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.28s
Val loss: 0.1978 score: 0.9225 time: 0.18s
Test loss: 0.2315 score: 0.9297 time: 0.16s
Epoch 223/1000, LR 0.000258
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 0.30s
Val loss: 0.1894 score: 0.9070 time: 0.19s
Test loss: 0.2267 score: 0.9297 time: 0.16s
Epoch 224/1000, LR 0.000258
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 0.30s
Val loss: 0.1849 score: 0.9302 time: 0.18s
Test loss: 0.2249 score: 0.9375 time: 0.16s
Epoch 225/1000, LR 0.000258
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.35s
Val loss: 0.1844 score: 0.9302 time: 0.18s
Test loss: 0.2254 score: 0.9375 time: 0.19s
Epoch 226/1000, LR 0.000258
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 0.35s
Val loss: 0.1843 score: 0.9380 time: 0.18s
Test loss: 0.2261 score: 0.9375 time: 0.16s
Epoch 227/1000, LR 0.000257
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.30s
Val loss: 0.1830 score: 0.9380 time: 0.18s
Test loss: 0.2253 score: 0.9375 time: 0.16s
Epoch 228/1000, LR 0.000257
Train loss: 0.6086;  Loss pred: 0.6086; Loss self: 0.0000; time: 0.30s
Val loss: 0.1811 score: 0.9302 time: 0.18s
Test loss: 0.2237 score: 0.9375 time: 0.16s
Epoch 229/1000, LR 0.000257
Train loss: 0.6064;  Loss pred: 0.6064; Loss self: 0.0000; time: 0.30s
Val loss: 0.1822 score: 0.9302 time: 0.18s
Test loss: 0.2238 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 230/1000, LR 0.000256
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.31s
Val loss: 0.1899 score: 0.9225 time: 0.19s
Test loss: 0.2285 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 231/1000, LR 0.000256
Train loss: 0.6134;  Loss pred: 0.6134; Loss self: 0.0000; time: 0.32s
Val loss: 0.1968 score: 0.9302 time: 0.19s
Test loss: 0.2332 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 232/1000, LR 0.000256
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.32s
Val loss: 0.1996 score: 0.9302 time: 0.18s
Test loss: 0.2354 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 233/1000, LR 0.000255
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.30s
Val loss: 0.1976 score: 0.9302 time: 0.18s
Test loss: 0.2344 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 234/1000, LR 0.000255
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.30s
Val loss: 0.1927 score: 0.9302 time: 0.18s
Test loss: 0.2314 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 235/1000, LR 0.000255
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.31s
Val loss: 0.1840 score: 0.9225 time: 0.19s
Test loss: 0.2257 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 236/1000, LR 0.000255
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.50s
Val loss: 0.1776 score: 0.9302 time: 0.26s
Test loss: 0.2218 score: 0.9375 time: 0.22s
Epoch 237/1000, LR 0.000254
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.40s
Val loss: 0.1789 score: 0.9457 time: 0.19s
Test loss: 0.2236 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 238/1000, LR 0.000254
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.29s
Val loss: 0.1809 score: 0.9457 time: 0.18s
Test loss: 0.2255 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 239/1000, LR 0.000254
Train loss: 0.6023;  Loss pred: 0.6023; Loss self: 0.0000; time: 0.30s
Val loss: 0.1817 score: 0.9457 time: 0.17s
Test loss: 0.2263 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 240/1000, LR 0.000253
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.30s
Val loss: 0.1816 score: 0.9457 time: 0.19s
Test loss: 0.2263 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 241/1000, LR 0.000253
Train loss: 0.6013;  Loss pred: 0.6013; Loss self: 0.0000; time: 0.29s
Val loss: 0.1809 score: 0.9457 time: 0.21s
Test loss: 0.2258 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 242/1000, LR 0.000253
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.37s
Val loss: 0.1795 score: 0.9457 time: 0.18s
Test loss: 0.2247 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 243/1000, LR 0.000252
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.30s
Val loss: 0.1781 score: 0.9457 time: 0.18s
Test loss: 0.2236 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 244/1000, LR 0.000252
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.30s
Val loss: 0.1772 score: 0.9380 time: 0.18s
Test loss: 0.2227 score: 0.9375 time: 0.16s
Epoch 245/1000, LR 0.000252
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.29s
Val loss: 0.1769 score: 0.9302 time: 0.18s
Test loss: 0.2224 score: 0.9375 time: 0.26s
Epoch 246/1000, LR 0.000252
Train loss: 0.5969;  Loss pred: 0.5969; Loss self: 0.0000; time: 0.29s
Val loss: 0.1769 score: 0.9302 time: 0.18s
Test loss: 0.2224 score: 0.9375 time: 0.18s
Epoch 247/1000, LR 0.000251
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.32s
Val loss: 0.1770 score: 0.9302 time: 0.21s
Test loss: 0.2226 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 248/1000, LR 0.000251
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.30s
Val loss: 0.1775 score: 0.9302 time: 0.18s
Test loss: 0.2229 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 249/1000, LR 0.000251
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.30s
Val loss: 0.1777 score: 0.9225 time: 0.17s
Test loss: 0.2232 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 250/1000, LR 0.000250
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.29s
Val loss: 0.1774 score: 0.9225 time: 0.18s
Test loss: 0.2233 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 251/1000, LR 0.000250
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.29s
Val loss: 0.1767 score: 0.9225 time: 0.18s
Test loss: 0.2233 score: 0.9375 time: 0.16s
Epoch 252/1000, LR 0.000250
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.37s
Val loss: 0.1758 score: 0.9225 time: 0.18s
Test loss: 0.2232 score: 0.9375 time: 0.16s
Epoch 253/1000, LR 0.000249
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.29s
Val loss: 0.1744 score: 0.9302 time: 0.17s
Test loss: 0.2228 score: 0.9375 time: 0.16s
Epoch 254/1000, LR 0.000249
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.30s
Val loss: 0.1729 score: 0.9302 time: 0.18s
Test loss: 0.2225 score: 0.9375 time: 0.17s
Epoch 255/1000, LR 0.000249
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.29s
Val loss: 0.1721 score: 0.9302 time: 0.18s
Test loss: 0.2223 score: 0.9375 time: 0.17s
Epoch 256/1000, LR 0.000248
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.29s
Val loss: 0.1729 score: 0.9225 time: 0.17s
Test loss: 0.2229 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 257/1000, LR 0.000248
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.30s
Val loss: 0.1736 score: 0.9225 time: 0.18s
Test loss: 0.2236 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 258/1000, LR 0.000248
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.33s
Val loss: 0.1737 score: 0.9225 time: 0.19s
Test loss: 0.2239 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 259/1000, LR 0.000247
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.30s
Val loss: 0.1733 score: 0.9225 time: 0.18s
Test loss: 0.2238 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 260/1000, LR 0.000247
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.30s
Val loss: 0.1713 score: 0.9225 time: 0.18s
Test loss: 0.2228 score: 0.9375 time: 0.17s
Epoch 261/1000, LR 0.000247
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.30s
Val loss: 0.1693 score: 0.9302 time: 0.19s
Test loss: 0.2221 score: 0.9375 time: 0.17s
Epoch 262/1000, LR 0.000246
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.30s
Val loss: 0.1693 score: 0.9225 time: 0.18s
Test loss: 0.2224 score: 0.9375 time: 0.17s
Epoch 263/1000, LR 0.000246
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.36s
Val loss: 0.1691 score: 0.9225 time: 0.19s
Test loss: 0.2228 score: 0.9375 time: 0.20s
Epoch 264/1000, LR 0.000246
Train loss: 0.5885;  Loss pred: 0.5885; Loss self: 0.0000; time: 0.33s
Val loss: 0.1665 score: 0.9302 time: 0.18s
Test loss: 0.2222 score: 0.9375 time: 0.17s
Epoch 265/1000, LR 0.000245
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 0.30s
Val loss: 0.1647 score: 0.9380 time: 0.19s
Test loss: 0.2223 score: 0.9375 time: 0.17s
Epoch 266/1000, LR 0.000245
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.29s
Val loss: 0.1644 score: 0.9457 time: 0.17s
Test loss: 0.2238 score: 0.9375 time: 0.16s
Epoch 267/1000, LR 0.000245
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.29s
Val loss: 0.1666 score: 0.9302 time: 0.18s
Test loss: 0.2273 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 268/1000, LR 0.000244
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.29s
Val loss: 0.1738 score: 0.9147 time: 0.20s
Test loss: 0.2350 score: 0.8828 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 269/1000, LR 0.000244
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.47s
Val loss: 0.1830 score: 0.8992 time: 0.26s
Test loss: 0.2443 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 270/1000, LR 0.000244
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.48s
Val loss: 0.2010 score: 0.9147 time: 0.26s
Test loss: 0.2615 score: 0.8672 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 271/1000, LR 0.000243
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 0.29s
Val loss: 0.2160 score: 0.9070 time: 0.17s
Test loss: 0.2757 score: 0.8750 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 272/1000, LR 0.000243
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.29s
Val loss: 0.2163 score: 0.9070 time: 0.19s
Test loss: 0.2768 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 273/1000, LR 0.000243
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.29s
Val loss: 0.2048 score: 0.9147 time: 0.18s
Test loss: 0.2672 score: 0.8672 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 274/1000, LR 0.000242
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.30s
Val loss: 0.1890 score: 0.9070 time: 0.19s
Test loss: 0.2537 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 275/1000, LR 0.000242
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.32s
Val loss: 0.1745 score: 0.9147 time: 0.19s
Test loss: 0.2411 score: 0.8828 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 276/1000, LR 0.000242
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.31s
Val loss: 0.1652 score: 0.9225 time: 0.18s
Test loss: 0.2328 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 277/1000, LR 0.000241
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 0.30s
Val loss: 0.1605 score: 0.9380 time: 0.17s
Test loss: 0.2280 score: 0.9219 time: 0.16s
Epoch 278/1000, LR 0.000241
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.30s
Val loss: 0.1596 score: 0.9380 time: 0.18s
Test loss: 0.2266 score: 0.9453 time: 0.17s
Epoch 279/1000, LR 0.000241
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.30s
Val loss: 0.1608 score: 0.9225 time: 0.19s
Test loss: 0.2271 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 280/1000, LR 0.000240
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.48s
Val loss: 0.1625 score: 0.9225 time: 0.27s
Test loss: 0.2284 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 281/1000, LR 0.000240
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.46s
Val loss: 0.1645 score: 0.9302 time: 0.24s
Test loss: 0.2300 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 282/1000, LR 0.000240
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 0.32s
Val loss: 0.1663 score: 0.9380 time: 0.18s
Test loss: 0.2315 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 283/1000, LR 0.000239
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.30s
Val loss: 0.1663 score: 0.9380 time: 0.17s
Test loss: 0.2319 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 284/1000, LR 0.000239
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.30s
Val loss: 0.1646 score: 0.9302 time: 0.18s
Test loss: 0.2312 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 285/1000, LR 0.000239
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.30s
Val loss: 0.1627 score: 0.9302 time: 0.18s
Test loss: 0.2301 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 286/1000, LR 0.000238
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.30s
Val loss: 0.1614 score: 0.9225 time: 0.18s
Test loss: 0.2293 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 287/1000, LR 0.000238
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.32s
Val loss: 0.1599 score: 0.9225 time: 0.18s
Test loss: 0.2284 score: 0.9375 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 288/1000, LR 0.000237
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.33s
Val loss: 0.1579 score: 0.9380 time: 0.19s
Test loss: 0.2275 score: 0.9453 time: 0.18s
Epoch 289/1000, LR 0.000237
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 0.31s
Val loss: 0.1576 score: 0.9380 time: 0.19s
Test loss: 0.2289 score: 0.9219 time: 0.17s
Epoch 290/1000, LR 0.000237
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.30s
Val loss: 0.1606 score: 0.9302 time: 0.18s
Test loss: 0.2329 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 291/1000, LR 0.000236
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.30s
Val loss: 0.1614 score: 0.9225 time: 0.19s
Test loss: 0.2343 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 292/1000, LR 0.000236
Train loss: 0.5762;  Loss pred: 0.5762; Loss self: 0.0000; time: 0.30s
Val loss: 0.1578 score: 0.9302 time: 0.19s
Test loss: 0.2312 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 293/1000, LR 0.000236
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.36s
Val loss: 0.1559 score: 0.9457 time: 0.18s
Test loss: 0.2294 score: 0.9453 time: 0.16s
Epoch 294/1000, LR 0.000235
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 0.48s
Val loss: 0.1554 score: 0.9457 time: 0.20s
Test loss: 0.2291 score: 0.9453 time: 0.17s
Epoch 295/1000, LR 0.000235
Train loss: 0.5743;  Loss pred: 0.5743; Loss self: 0.0000; time: 0.30s
Val loss: 0.1553 score: 0.9380 time: 0.18s
Test loss: 0.2300 score: 0.9297 time: 0.17s
Epoch 296/1000, LR 0.000235
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.30s
Val loss: 0.1553 score: 0.9380 time: 0.18s
Test loss: 0.2305 score: 0.9219 time: 0.17s
Epoch 297/1000, LR 0.000234
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.30s
Val loss: 0.1550 score: 0.9380 time: 0.19s
Test loss: 0.2306 score: 0.9219 time: 0.17s
Epoch 298/1000, LR 0.000234
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.30s
Val loss: 0.1544 score: 0.9380 time: 0.19s
Test loss: 0.2303 score: 0.9297 time: 0.17s
Epoch 299/1000, LR 0.000234
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.32s
Val loss: 0.1538 score: 0.9457 time: 0.19s
Test loss: 0.2297 score: 0.9453 time: 0.22s
Epoch 300/1000, LR 0.000233
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.31s
Val loss: 0.1550 score: 0.9302 time: 0.18s
Test loss: 0.2298 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 301/1000, LR 0.000233
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.30s
Val loss: 0.1573 score: 0.9302 time: 0.19s
Test loss: 0.2314 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 302/1000, LR 0.000232
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.30s
Val loss: 0.1551 score: 0.9302 time: 0.26s
Test loss: 0.2303 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 303/1000, LR 0.000232
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.50s
Val loss: 0.1537 score: 0.9380 time: 0.26s
Test loss: 0.2298 score: 0.9453 time: 0.21s
Epoch 304/1000, LR 0.000232
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.46s
Val loss: 0.1528 score: 0.9380 time: 0.17s
Test loss: 0.2298 score: 0.9453 time: 0.16s
Epoch 305/1000, LR 0.000231
Train loss: 0.5684;  Loss pred: 0.5684; Loss self: 0.0000; time: 0.31s
Val loss: 0.1538 score: 0.9302 time: 0.17s
Test loss: 0.2324 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 306/1000, LR 0.000231
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.28s
Val loss: 0.1606 score: 0.9147 time: 0.17s
Test loss: 0.2399 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 307/1000, LR 0.000231
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.28s
Val loss: 0.1655 score: 0.9147 time: 0.17s
Test loss: 0.2450 score: 0.8828 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 308/1000, LR 0.000230
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 0.27s
Val loss: 0.1661 score: 0.9147 time: 0.17s
Test loss: 0.2458 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 309/1000, LR 0.000230
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.28s
Val loss: 0.1635 score: 0.9147 time: 0.19s
Test loss: 0.2436 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 310/1000, LR 0.000229
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.37s
Val loss: 0.1599 score: 0.9147 time: 0.17s
Test loss: 0.2405 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 311/1000, LR 0.000229
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.28s
Val loss: 0.1564 score: 0.9225 time: 0.17s
Test loss: 0.2376 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 312/1000, LR 0.000229
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.29s
Val loss: 0.1533 score: 0.9302 time: 0.17s
Test loss: 0.2347 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 313/1000, LR 0.000228
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 0.29s
Val loss: 0.1502 score: 0.9457 time: 0.18s
Test loss: 0.2313 score: 0.9453 time: 0.16s
Epoch 314/1000, LR 0.000228
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.29s
Val loss: 0.1526 score: 0.9302 time: 0.17s
Test loss: 0.2318 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 315/1000, LR 0.000228
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.31s
Val loss: 0.1576 score: 0.9380 time: 0.20s
Test loss: 0.2353 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 316/1000, LR 0.000227
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.42s
Val loss: 0.1581 score: 0.9380 time: 0.18s
Test loss: 0.2359 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 317/1000, LR 0.000227
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.30s
Val loss: 0.1573 score: 0.9380 time: 0.18s
Test loss: 0.2356 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 318/1000, LR 0.000226
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.29s
Val loss: 0.1554 score: 0.9380 time: 0.17s
Test loss: 0.2345 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 319/1000, LR 0.000226
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 0.28s
Val loss: 0.1533 score: 0.9302 time: 0.18s
Test loss: 0.2334 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 320/1000, LR 0.000226
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.29s
Val loss: 0.1512 score: 0.9302 time: 0.18s
Test loss: 0.2323 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 321/1000, LR 0.000225
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.31s
Val loss: 0.1497 score: 0.9302 time: 0.26s
Test loss: 0.2316 score: 0.9453 time: 0.16s
Epoch 322/1000, LR 0.000225
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.29s
Val loss: 0.1488 score: 0.9380 time: 0.20s
Test loss: 0.2313 score: 0.9453 time: 0.18s
Epoch 323/1000, LR 0.000225
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.28s
Val loss: 0.1483 score: 0.9380 time: 0.17s
Test loss: 0.2313 score: 0.9453 time: 0.15s
Epoch 324/1000, LR 0.000224
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.28s
Val loss: 0.1483 score: 0.9380 time: 0.17s
Test loss: 0.2316 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 325/1000, LR 0.000224
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 0.29s
Val loss: 0.1491 score: 0.9302 time: 0.17s
Test loss: 0.2325 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 326/1000, LR 0.000223
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.30s
Val loss: 0.1494 score: 0.9302 time: 0.27s
Test loss: 0.2330 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 327/1000, LR 0.000223
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.28s
Val loss: 0.1492 score: 0.9302 time: 0.18s
Test loss: 0.2332 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 328/1000, LR 0.000223
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.28s
Val loss: 0.1491 score: 0.9302 time: 0.17s
Test loss: 0.2335 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 329/1000, LR 0.000222
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 0.30s
Val loss: 0.1496 score: 0.9302 time: 0.18s
Test loss: 0.2340 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 330/1000, LR 0.000222
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.29s
Val loss: 0.1507 score: 0.9302 time: 0.19s
Test loss: 0.2349 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 331/1000, LR 0.000221
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.32s
Val loss: 0.1512 score: 0.9302 time: 0.18s
Test loss: 0.2355 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 332/1000, LR 0.000221
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.31s
Val loss: 0.1514 score: 0.9302 time: 0.19s
Test loss: 0.2359 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 333/1000, LR 0.000221
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.37s
Val loss: 0.1509 score: 0.9302 time: 0.18s
Test loss: 0.2359 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 334/1000, LR 0.000220
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.30s
Val loss: 0.1490 score: 0.9302 time: 0.18s
Test loss: 0.2353 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 335/1000, LR 0.000220
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.29s
Val loss: 0.1477 score: 0.9380 time: 0.18s
Test loss: 0.2350 score: 0.9453 time: 0.17s
Epoch 336/1000, LR 0.000219
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.30s
Val loss: 0.1470 score: 0.9380 time: 0.19s
Test loss: 0.2350 score: 0.9453 time: 0.17s
Epoch 337/1000, LR 0.000219
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.32s
Val loss: 0.1467 score: 0.9380 time: 0.19s
Test loss: 0.2353 score: 0.9453 time: 0.22s
Epoch 338/1000, LR 0.000219
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.49s
Val loss: 0.1464 score: 0.9380 time: 0.26s
Test loss: 0.2355 score: 0.9453 time: 0.19s
Epoch 339/1000, LR 0.000218
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.30s
Val loss: 0.1460 score: 0.9380 time: 0.19s
Test loss: 0.2357 score: 0.9453 time: 0.17s
Epoch 340/1000, LR 0.000218
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.30s
Val loss: 0.1458 score: 0.9457 time: 0.18s
Test loss: 0.2363 score: 0.9453 time: 0.17s
Epoch 341/1000, LR 0.000218
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.29s
Val loss: 0.1459 score: 0.9457 time: 0.18s
Test loss: 0.2367 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 342/1000, LR 0.000217
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.1460 score: 0.9457 time: 0.19s
Test loss: 0.2369 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 343/1000, LR 0.000217
Train loss: 0.5603;  Loss pred: 0.5603; Loss self: 0.0000; time: 0.30s
Val loss: 0.1463 score: 0.9457 time: 0.20s
Test loss: 0.2370 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 344/1000, LR 0.000216
Train loss: 0.5569;  Loss pred: 0.5569; Loss self: 0.0000; time: 0.37s
Val loss: 0.1468 score: 0.9380 time: 0.19s
Test loss: 0.2381 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 345/1000, LR 0.000216
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.31s
Val loss: 0.1506 score: 0.9302 time: 0.19s
Test loss: 0.2434 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 346/1000, LR 0.000215
Train loss: 0.5601;  Loss pred: 0.5601; Loss self: 0.0000; time: 0.30s
Val loss: 0.1555 score: 0.9147 time: 0.19s
Test loss: 0.2490 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 347/1000, LR 0.000215
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.31s
Val loss: 0.1558 score: 0.9147 time: 0.19s
Test loss: 0.2500 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 348/1000, LR 0.000215
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.30s
Val loss: 0.1485 score: 0.9302 time: 0.19s
Test loss: 0.2426 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 349/1000, LR 0.000214
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.28s
Val loss: 0.1512 score: 0.9302 time: 0.20s
Test loss: 0.2425 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 350/1000, LR 0.000214
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.42s
Val loss: 0.1653 score: 0.9457 time: 0.18s
Test loss: 0.2519 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 351/1000, LR 0.000213
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.29s
Val loss: 0.1789 score: 0.9380 time: 0.17s
Test loss: 0.2613 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 352/1000, LR 0.000213
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.29s
Val loss: 0.1875 score: 0.9302 time: 0.17s
Test loss: 0.2675 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 353/1000, LR 0.000213
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 0.28s
Val loss: 0.1885 score: 0.9302 time: 0.18s
Test loss: 0.2683 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 354/1000, LR 0.000212
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.31s
Val loss: 0.1845 score: 0.9302 time: 0.18s
Test loss: 0.2656 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 355/1000, LR 0.000212
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.43s
Val loss: 0.1783 score: 0.9380 time: 0.18s
Test loss: 0.2614 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 356/1000, LR 0.000211
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 0.30s
Val loss: 0.1709 score: 0.9457 time: 0.17s
Test loss: 0.2563 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 357/1000, LR 0.000211
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.29s
Val loss: 0.1643 score: 0.9457 time: 0.17s
Test loss: 0.2518 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 358/1000, LR 0.000211
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.29s
Val loss: 0.1585 score: 0.9457 time: 0.17s
Test loss: 0.2483 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 359/1000, LR 0.000210
Train loss: 0.5591;  Loss pred: 0.5591; Loss self: 0.0000; time: 0.29s
Val loss: 0.1513 score: 0.9380 time: 0.17s
Test loss: 0.2439 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 360/1000, LR 0.000210
Train loss: 0.5558;  Loss pred: 0.5558; Loss self: 0.0000; time: 0.30s
Val loss: 0.1450 score: 0.9380 time: 0.18s
Test loss: 0.2405 score: 0.9453 time: 0.16s
Epoch 361/1000, LR 0.000209
Train loss: 0.5538;  Loss pred: 0.5538; Loss self: 0.0000; time: 0.35s
Val loss: 0.1454 score: 0.9302 time: 0.18s
Test loss: 0.2421 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 362/1000, LR 0.000209
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.30s
Val loss: 0.1480 score: 0.9302 time: 0.17s
Test loss: 0.2453 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 363/1000, LR 0.000209
Train loss: 0.5588;  Loss pred: 0.5588; Loss self: 0.0000; time: 0.29s
Val loss: 0.1500 score: 0.9147 time: 0.17s
Test loss: 0.2476 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 364/1000, LR 0.000208
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.30s
Val loss: 0.1504 score: 0.9147 time: 0.18s
Test loss: 0.2482 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 365/1000, LR 0.000208
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.31s
Val loss: 0.1491 score: 0.9147 time: 0.22s
Test loss: 0.2471 score: 0.8828 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 366/1000, LR 0.000207
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.51s
Val loss: 0.1473 score: 0.9302 time: 0.26s
Test loss: 0.2457 score: 0.8906 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 367/1000, LR 0.000207
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.31s
Val loss: 0.1456 score: 0.9302 time: 0.17s
Test loss: 0.2445 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 368/1000, LR 0.000206
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.29s
Val loss: 0.1430 score: 0.9380 time: 0.17s
Test loss: 0.2419 score: 0.9297 time: 0.16s
Epoch 369/1000, LR 0.000206
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.28s
Val loss: 0.1470 score: 0.9302 time: 0.17s
Test loss: 0.2439 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 370/1000, LR 0.000206
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.29s
Val loss: 0.1579 score: 0.9535 time: 0.18s
Test loss: 0.2515 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 371/1000, LR 0.000205
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.41s
Val loss: 0.1671 score: 0.9457 time: 0.18s
Test loss: 0.2581 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 372/1000, LR 0.000205
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.29s
Val loss: 0.1716 score: 0.9457 time: 0.17s
Test loss: 0.2615 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 373/1000, LR 0.000204
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 0.29s
Val loss: 0.1739 score: 0.9457 time: 0.17s
Test loss: 0.2633 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 374/1000, LR 0.000204
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.28s
Val loss: 0.1767 score: 0.9380 time: 0.17s
Test loss: 0.2655 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 375/1000, LR 0.000204
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.29s
Val loss: 0.1755 score: 0.9457 time: 0.17s
Test loss: 0.2649 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 376/1000, LR 0.000203
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.30s
Val loss: 0.1720 score: 0.9457 time: 0.18s
Test loss: 0.2627 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 377/1000, LR 0.000203
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.32s
Val loss: 0.1658 score: 0.9457 time: 0.18s
Test loss: 0.2584 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 378/1000, LR 0.000202
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.39s
Val loss: 0.1585 score: 0.9535 time: 0.18s
Test loss: 0.2533 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 379/1000, LR 0.000202
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.29s
Val loss: 0.1520 score: 0.9380 time: 0.17s
Test loss: 0.2487 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 380/1000, LR 0.000201
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.29s
Val loss: 0.1473 score: 0.9380 time: 0.21s
Test loss: 0.2456 score: 0.9453 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 381/1000, LR 0.000201
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.28s
Val loss: 0.1444 score: 0.9302 time: 0.17s
Test loss: 0.2437 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 382/1000, LR 0.000201
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.28s
Val loss: 0.1424 score: 0.9380 time: 0.17s
Test loss: 0.2423 score: 0.9453 time: 0.16s
Epoch 383/1000, LR 0.000200
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.29s
Val loss: 0.1412 score: 0.9380 time: 0.17s
Test loss: 0.2416 score: 0.9453 time: 0.16s
Epoch 384/1000, LR 0.000200
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.30s
Val loss: 0.1408 score: 0.9380 time: 0.23s
Test loss: 0.2415 score: 0.9453 time: 0.16s
Epoch 385/1000, LR 0.000199
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.29s
Val loss: 0.1411 score: 0.9380 time: 0.18s
Test loss: 0.2419 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 386/1000, LR 0.000199
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.29s
Val loss: 0.1408 score: 0.9380 time: 0.18s
Test loss: 0.2417 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 387/1000, LR 0.000198
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.29s
Val loss: 0.1397 score: 0.9380 time: 0.18s
Test loss: 0.2413 score: 0.9375 time: 0.16s
Epoch 388/1000, LR 0.000198
Train loss: 0.5486;  Loss pred: 0.5486; Loss self: 0.0000; time: 0.29s
Val loss: 0.1398 score: 0.9380 time: 0.17s
Test loss: 0.2419 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 389/1000, LR 0.000198
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 0.29s
Val loss: 0.1396 score: 0.9380 time: 0.18s
Test loss: 0.2418 score: 0.9297 time: 0.24s
Epoch 390/1000, LR 0.000197
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.30s
Val loss: 0.1392 score: 0.9380 time: 0.23s
Test loss: 0.2416 score: 0.9297 time: 0.16s
Epoch 391/1000, LR 0.000197
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.30s
Val loss: 0.1392 score: 0.9457 time: 0.17s
Test loss: 0.2416 score: 0.9453 time: 0.16s
Epoch 392/1000, LR 0.000196
Train loss: 0.5476;  Loss pred: 0.5476; Loss self: 0.0000; time: 0.34s
Val loss: 0.1393 score: 0.9457 time: 0.26s
Test loss: 0.2417 score: 0.9453 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 393/1000, LR 0.000196
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.39s
Val loss: 0.1398 score: 0.9380 time: 0.19s
Test loss: 0.2421 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 394/1000, LR 0.000195
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.32s
Val loss: 0.1402 score: 0.9380 time: 0.23s
Test loss: 0.2425 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 395/1000, LR 0.000195
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.28s
Val loss: 0.1401 score: 0.9380 time: 0.18s
Test loss: 0.2426 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 396/1000, LR 0.000195
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.29s
Val loss: 0.1402 score: 0.9380 time: 0.18s
Test loss: 0.2427 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 397/1000, LR 0.000194
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.38s
Val loss: 0.1401 score: 0.9380 time: 0.19s
Test loss: 0.2427 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 398/1000, LR 0.000194
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.34s
Val loss: 0.1402 score: 0.9380 time: 0.17s
Test loss: 0.2427 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 399/1000, LR 0.000193
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.38s
Val loss: 0.1404 score: 0.9380 time: 0.17s
Test loss: 0.2428 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 400/1000, LR 0.000193
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.29s
Val loss: 0.1406 score: 0.9380 time: 0.17s
Test loss: 0.2430 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 401/1000, LR 0.000192
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.29s
Val loss: 0.1408 score: 0.9380 time: 0.20s
Test loss: 0.2432 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 402/1000, LR 0.000192
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.28s
Val loss: 0.1429 score: 0.9302 time: 0.17s
Test loss: 0.2443 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 403/1000, LR 0.000192
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.29s
Val loss: 0.1507 score: 0.9380 time: 0.18s
Test loss: 0.2491 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 404/1000, LR 0.000191
Train loss: 0.5486;  Loss pred: 0.5486; Loss self: 0.0000; time: 0.29s
Val loss: 0.1572 score: 0.9535 time: 0.24s
Test loss: 0.2535 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 405/1000, LR 0.000191
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.29s
Val loss: 0.1607 score: 0.9535 time: 0.19s
Test loss: 0.2561 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 406/1000, LR 0.000190
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 0.31s
Val loss: 0.1610 score: 0.9535 time: 0.20s
Test loss: 0.2563 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 407/1000, LR 0.000190
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.29s
Val loss: 0.1547 score: 0.9535 time: 0.17s
Test loss: 0.2517 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 408/1000, LR 0.000189
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.28s
Val loss: 0.1442 score: 0.9380 time: 0.17s
Test loss: 0.2443 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 409/1000, LR 0.000189
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.29s
Val loss: 0.1393 score: 0.9380 time: 0.21s
Test loss: 0.2413 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 410/1000, LR 0.000188
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.38s
Val loss: 0.1374 score: 0.9457 time: 0.19s
Test loss: 0.2410 score: 0.9453 time: 0.17s
Epoch 411/1000, LR 0.000188
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.29s
Val loss: 0.1378 score: 0.9380 time: 0.18s
Test loss: 0.2421 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 412/1000, LR 0.000188
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.29s
Val loss: 0.1387 score: 0.9302 time: 0.18s
Test loss: 0.2438 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 413/1000, LR 0.000187
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.28s
Val loss: 0.1391 score: 0.9302 time: 0.18s
Test loss: 0.2448 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 414/1000, LR 0.000187
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.28s
Val loss: 0.1389 score: 0.9302 time: 0.17s
Test loss: 0.2452 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 415/1000, LR 0.000186
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.28s
Val loss: 0.1381 score: 0.9302 time: 0.18s
Test loss: 0.2449 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 416/1000, LR 0.000186
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.28s
Val loss: 0.1371 score: 0.9302 time: 0.18s
Test loss: 0.2443 score: 0.9141 time: 0.16s
Epoch 417/1000, LR 0.000185
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.28s
Val loss: 0.1363 score: 0.9380 time: 0.18s
Test loss: 0.2438 score: 0.9297 time: 0.21s
Epoch 418/1000, LR 0.000185
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 0.48s
Val loss: 0.1359 score: 0.9457 time: 0.25s
Test loss: 0.2438 score: 0.9375 time: 0.21s
Epoch 419/1000, LR 0.000185
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.49s
Val loss: 0.1358 score: 0.9457 time: 0.25s
Test loss: 0.2440 score: 0.9453 time: 0.21s
Epoch 420/1000, LR 0.000184
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.48s
Val loss: 0.1359 score: 0.9457 time: 0.25s
Test loss: 0.2443 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 421/1000, LR 0.000184
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.29s
Val loss: 0.1363 score: 0.9380 time: 0.17s
Test loss: 0.2448 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 422/1000, LR 0.000183
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.29s
Val loss: 0.1365 score: 0.9380 time: 0.18s
Test loss: 0.2453 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 423/1000, LR 0.000183
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.29s
Val loss: 0.1365 score: 0.9380 time: 0.18s
Test loss: 0.2457 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 424/1000, LR 0.000182
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.29s
Val loss: 0.1365 score: 0.9380 time: 0.17s
Test loss: 0.2460 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 425/1000, LR 0.000182
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.28s
Val loss: 0.1365 score: 0.9380 time: 0.17s
Test loss: 0.2462 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 426/1000, LR 0.000181
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.42s
Val loss: 0.1365 score: 0.9380 time: 0.19s
Test loss: 0.2464 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 427/1000, LR 0.000181
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.29s
Val loss: 0.1365 score: 0.9457 time: 0.18s
Test loss: 0.2465 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 428/1000, LR 0.000181
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.30s
Val loss: 0.1366 score: 0.9457 time: 0.21s
Test loss: 0.2467 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 429/1000, LR 0.000180
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.28s
Val loss: 0.1368 score: 0.9380 time: 0.17s
Test loss: 0.2471 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 430/1000, LR 0.000180
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.29s
Val loss: 0.1388 score: 0.9380 time: 0.17s
Test loss: 0.2485 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 431/1000, LR 0.000179
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.31s
Val loss: 0.1452 score: 0.9380 time: 0.23s
Test loss: 0.2529 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 432/1000, LR 0.000179
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.36s
Val loss: 0.1457 score: 0.9380 time: 0.17s
Test loss: 0.2536 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 433/1000, LR 0.000178
Train loss: 0.5435;  Loss pred: 0.5435; Loss self: 0.0000; time: 0.28s
Val loss: 0.1387 score: 0.9380 time: 0.17s
Test loss: 0.2496 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 434/1000, LR 0.000178
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.28s
Val loss: 0.1369 score: 0.9457 time: 0.17s
Test loss: 0.2491 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 435/1000, LR 0.000177
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.29s
Val loss: 0.1372 score: 0.9380 time: 0.17s
Test loss: 0.2501 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 436/1000, LR 0.000177
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.29s
Val loss: 0.1379 score: 0.9302 time: 0.20s
Test loss: 0.2514 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 437/1000, LR 0.000176
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.43s
Val loss: 0.1393 score: 0.9302 time: 0.25s
Test loss: 0.2534 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 438/1000, LR 0.000176
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.29s
Val loss: 0.1431 score: 0.9225 time: 0.18s
Test loss: 0.2581 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 439/1000, LR 0.000176
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.28s
Val loss: 0.1461 score: 0.9147 time: 0.19s
Test loss: 0.2617 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 418,   Train_Loss: 0.5411,   Val_Loss: 0.1358,   Val_Precision: 0.9531,   Val_Recall: 0.9385,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1358,   Test_Precision: 1.0000,   Test_Recall: 0.8906,   Test_accuracy: 0.9421,   Test_Score: 0.9453,   Test_loss: 0.2440


[0.18194310809485614, 0.1810774840414524, 0.17691346304491162, 0.18729655584320426, 0.21967729600146413, 0.1842338410206139, 0.19775674818083644, 0.17130612907931209, 0.17523492616601288, 0.18254406796768308, 0.1720045858528465, 0.17406337801367044, 0.22099581314250827, 0.17332209693267941, 0.24705747794359922, 0.1816440438851714, 0.19131378619931638, 0.1946081870701164, 0.1734392608050257, 0.17236929200589657, 0.23319614306092262, 0.17839026707224548, 0.17149156494997442, 0.1774467770010233, 0.17231285106390715, 0.17357777897268534, 0.25593708897940814, 0.1849010311998427, 0.1884904899634421, 0.2578901220113039, 0.26025395817123353, 0.1885067990515381, 0.17830685409717262, 0.1800132051575929, 0.17874571913853288, 0.18423096416518092, 0.2644944149069488, 0.18424635007977486, 0.18014067295007408, 0.18017646810039878, 0.18079442321322858, 0.18134824419394135, 0.2855063711758703, 0.17315450101159513, 0.1681035771034658, 0.20053225103765726, 0.2600499589461833, 0.25797628005966544, 0.18810250284150243, 0.18069669185206294, 0.17859323509037495, 0.18145917891524732, 0.18137782905250788, 0.18655900610610843, 0.1859878678806126, 0.18516107695177197, 0.1858946280553937, 0.25787682086229324, 0.25175379286520183, 0.2517485909629613, 0.1796361031010747, 0.18246362986974418, 0.1710216428618878, 0.2518417008686811, 0.24956991500221193, 0.19545258884318173, 0.17422436899505556, 0.17271967907436192, 0.17633475898765028, 0.1720998368691653, 0.1758254780434072, 0.2517764091026038, 0.17556831683032215, 0.17309878603555262, 0.2517390118446201, 0.2559383499901742, 0.17312748590484262, 0.17444416298530996, 0.24968568701297045, 0.2517590359784663, 0.2615265778731555, 0.17709587118588388, 0.18203573185019195, 0.18339021294377744, 0.19229137105867267, 0.26225168304517865, 0.17514603002928197, 0.1761805519927293, 0.18875654810108244, 0.18336466094478965, 0.24432305479422212, 0.22729339404031634, 0.17452223505824804, 0.17223694315180182, 0.17059823009185493, 0.1778185770381242, 0.20545099396258593, 0.18572978186421096, 0.17029164801351726, 0.25595966703258455, 0.253158297855407, 0.1710975910536945, 0.1740097040310502, 0.18111294717527926, 0.176220121094957, 0.2496106110047549, 0.1802713149227202, 0.1736392981838435, 0.1728567269165069, 0.17516811820678413, 0.24846512009389699, 0.17208930710330606, 0.17906918213702738, 0.1699134160298854, 0.1700999829918146, 0.18262516101822257, 0.20953723485581577, 0.21919363108463585, 0.18200379004701972, 0.19160219095647335, 0.1921800789423287, 0.17261441494338214, 0.19008711609058082, 0.2658604809548706, 0.1844589530956, 0.19039395288564265, 0.17890097782947123, 0.16870063310489058, 0.22866753116250038, 0.17490224610082805, 0.18366583390161395, 0.18337107310071588, 0.1961535830050707, 0.18272562697529793, 0.1851648238953203, 0.20201167417690158, 0.2528105832170695, 0.18329941295087337, 0.1814502498600632, 0.17065685079433024, 0.18320432608015835, 0.19955118210054934, 0.25595016218721867, 0.18608309607952833, 0.18295096699148417, 0.17021998902782798, 0.16992938006296754, 0.17783734016120434, 0.18483066093176603, 0.18115724716335535, 0.18467249488458037, 0.19079823000356555, 0.25166575890034437, 0.2475501918233931, 0.21393612190149724, 0.16957823582924902, 0.17746347794309258, 0.187363610137254, 0.2648962759412825, 0.1810049309860915, 0.1818247709888965, 0.170418132096529, 0.184733172878623, 0.17296810494735837, 0.20656663295812905, 0.17813694896176457, 0.16875282395631075, 0.2475821920670569, 0.25594360986724496, 0.1758754758629948, 0.196519257966429, 0.2518946649506688, 0.24958509486168623, 0.2627222998999059, 0.17695791414007545, 0.17490509594790637, 0.17617671191692352, 0.19144063792191446, 0.1934461600612849, 0.19326914916746318, 0.19408264989033341, 0.18182275514118373, 0.17733828397467732, 0.17302818107418716, 0.1749809740576893, 0.2539609130471945, 0.18777691898867488, 0.1841343909036368, 0.18069227505475283, 0.18459049495868385, 0.18371691601350904, 0.1906355288811028, 0.26370978192426264, 0.18525653891265392, 0.1825488128233701, 0.17453856184147298, 0.1729956460185349, 0.17193303513340652, 0.17579146497882903, 0.256762936944142, 0.1737316099461168, 0.17355965008027852, 0.17359850509092212, 0.1823210760485381, 0.2480911989696324, 0.17861528601497412, 0.17339223297312856, 0.17337769200094044, 0.18363975593820214, 0.179949234938249, 0.2616397449746728, 0.17663365183398128, 0.17618722189217806, 0.1780414180830121, 0.258358056191355, 0.2434195070527494, 0.18314689910039306, 0.1812009729910642, 0.1789727909490466, 0.1823384310118854, 0.2603700279723853, 0.2684615380130708, 0.18201788002625108, 0.17522011906839907, 0.18180147395469248, 0.1828178740106523, 0.1864203200675547, 0.17747685895301402, 0.18347616191022098, 0.18440603790804744, 0.18550198199227452, 0.24564243713393807, 0.1854056480806321, 0.18509095907211304, 0.18427512887865305, 0.1761972070671618, 0.1950718560256064, 0.19367907708510756, 0.1900793721433729, 0.19145685178227723, 0.18849589093588293, 0.187677931971848, 0.26900579989887774, 0.1907864399254322, 0.17997291102074087, 0.2404005927965045, 0.2622568979859352, 0.26427928218618035, 0.18160887993872166, 0.18139180401340127, 0.18633005302399397, 0.1848584038671106, 0.1844587530940771, 0.2497118308674544, 0.24928120616823435, 0.1828203839249909, 0.18846390000544488, 0.18418922484852374, 0.18215775093995035, 0.1969443189445883, 0.24548207502812147, 0.1772198211401701, 0.19065732206217945, 0.1845097930636257, 0.1818082898389548, 0.18786520301364362, 0.18468002206645906, 0.1769981449469924, 0.17430006293579936, 0.1745982519350946, 0.18816523998975754, 0.2373682230245322, 0.18043289706110954, 0.17804995807819068, 0.22075137100182474, 0.18032849393785, 0.18522052094340324, 0.17713477299548686, 0.1782013380434364, 0.18257504096254706, 0.17921589501202106, 0.24556887103244662, 0.1764913978986442, 0.18099956214427948, 0.18312228401191533, 0.1912062680348754, 0.2517709010280669, 0.24655942805111408, 0.17647729511372745, 0.17361364397220314, 0.17288523213937879, 0.25389853306114674, 0.1762142670340836, 0.17598926578648388, 0.17972279386594892, 0.17255719588138163, 0.17231586598791182, 0.25707174511626363, 0.22716698795557022, 0.18313147500157356, 0.21000455389730632, 0.1811813679523766, 0.18035486107692122, 0.27866839384660125, 0.18586038798093796, 0.2565322360023856, 0.2560587900225073, 0.23841863917186856, 0.1777489040978253, 0.18221259489655495, 0.17682257085107267, 0.17809650884009898, 0.1854242228437215, 0.2550868410617113, 0.18101903214119375, 0.18502388312481344, 0.18346702586859465, 0.2001961509231478, 0.19274651515297592, 0.1951567609794438, 0.27766083902679384, 0.18988049984909594, 0.19043012196198106, 0.29892264399677515, 0.18934996891766787, 0.2615138820838183, 0.2330024188850075, 0.1821164251305163, 0.1898289320524782, 0.1875723721459508, 0.18286613700911403, 0.20958646293729544, 0.1828267420642078, 0.17815189389511943, 0.18261727108620107, 0.18445237795822322, 0.18867202615365386, 0.18992137000896037, 0.18066136399284005, 0.18228729511611164, 0.18281646282412112, 0.18098044791258872, 0.19126119883731008, 0.2683097799308598, 0.17830231110565364, 0.17962953192181885, 0.18001126893796027, 0.17820453294552863, 0.18292957707308233, 0.1798315888736397, 0.17787883803248405, 0.19010688294656575, 0.18478836305439472, 0.17857452691532671, 0.1809070350136608, 0.18095966801047325, 0.17887460510246456, 0.17714431905187666, 0.19524625805206597, 0.1949843850452453, 0.19412237592041492, 0.19035442196764052, 0.1756480378098786, 0.1779089921619743, 0.18532796995714307, 0.1808192259632051, 0.17669163784012198, 0.1769482281524688, 0.17746085301041603, 0.1778341990429908, 0.2624270371161401, 0.19030858296900988, 0.18917977204546332, 0.26651475112885237, 0.2663838570006192, 0.18761433800682425, 0.17218495113775134, 0.17164404084905982, 0.17705318611115217, 0.18369746091775596, 0.18808535486459732, 0.1860991409048438, 0.17434588493779302, 0.19224829296581447, 0.2954132598824799, 0.25378149119205773, 0.22513856599107385, 0.18854105682112277, 0.18716653413139284, 0.2034830329939723, 0.18998328200541437, 0.18854643497616053, 0.17805938608944416, 0.17602359293960035, 0.17628279910422862, 0.1810183150228113, 0.18351093493402004, 0.18980058492161334, 0.18885866506025195, 0.23808425688184798, 0.18548454903066158, 0.18692944711074233, 0.19351318292319775, 0.19458089000545442, 0.1929779089987278, 0.18904771492816508, 0.18653275887481868, 0.18542655184864998, 0.18583872797898948, 0.21696176589466631, 0.18533158604986966, 0.18565997411496937, 0.18492789193987846, 0.18029221799224615, 0.20795786986127496, 0.23823034297674894, 0.1795910589862615, 0.17847021808847785, 0.18484873697161674, 0.17541624908335507, 0.18328081397339702, 0.18872282817028463, 0.1749839698895812, 0.1755301859229803, 0.17625133809633553, 0.1849250509403646, 0.18636670894920826, 0.24787111696787179, 0.1780181140638888, 0.18493730504997075, 0.18525625113397837, 0.1814237260259688, 0.23807618394494057, 0.20117195299826562, 0.18139604991301894, 0.1815563919954002, 0.1877986949402839, 0.18681888794526458, 0.1954327509738505, 0.1989432831760496, 0.19252366991713643, 0.19507409119978547, 0.19217727216891944, 0.1876100399531424, 0.18763565085828304, 0.22718774899840355, 0.19161074911244214, 0.17641262710094452, 0.18145847809500992, 0.1776104138698429, 0.18301342497579753, 0.1973751860205084, 0.18290682206861675, 0.1832054378464818, 0.17527244007214904, 0.17423165892250836, 0.17997434991411865, 0.1826927789952606, 0.17950378311797976, 0.17653193301521242, 0.17452921392396092, 0.17599434405565262, 0.17513539618812501, 0.17306757206097245, 0.17601104103960097, 0.17395266005769372, 0.19736740388907492, 0.19349885499104857, 0.19423192599788308, 0.18832763796672225, 0.19512028503231704, 0.22587548196315765, 0.1867214220110327, 0.19050655304454267, 0.18700717203319073, 0.17424272396601737, 0.2508977029938251, 0.1742381788790226, 0.193232131190598, 0.2536582669708878, 0.1759396439883858, 0.1720433251466602, 0.17989904596470296, 0.19620425696484745, 0.25995666603557765, 0.1874880890827626, 0.1891601209063083, 0.18760377494618297, 0.18606566893868148, 0.18765862309373915, 0.1815169530455023, 0.2588996170088649, 0.18069302197545767, 0.1791647640056908, 0.17778965504840016, 0.17781350389122963, 0.17459592199884355, 0.2500207500997931, 0.17657265882007778, 0.1887004568707198, 0.19495862000621855, 0.2706981480587274, 0.26432584901340306, 0.19644498801790178, 0.19482051790691912, 0.1898872631136328, 0.24875263404101133, 0.2680976218543947, 0.18953233095817268, 0.19012766983360052, 0.17419727193191648, 0.18699585110880435, 0.22577393311075866, 0.17623036494478583, 0.1647976408712566, 0.16514491010457277, 0.18056812812574208, 0.1640947659034282, 0.16250847885385156, 0.21023767115548253, 0.21794700901955366, 0.17330139386467636, 0.17185818497091532, 0.17072942899540067, 0.16941588278859854, 0.21556176990270615, 0.16604522406123579, 0.16452762996777892, 0.16563216410577297, 0.18704536580480635, 0.2157288701273501, 0.1624748830217868, 0.16213042684830725, 0.16769705689512193, 0.1626168848015368, 0.17645448306575418, 0.20724883489310741, 0.16042549209669232, 0.17314649792388082, 0.17721441015601158, 0.17532575014047325, 0.1747484840452671, 0.17587114498019218, 0.15880209486931562, 0.17340418021194637, 0.158901731017977, 0.21474002697505057, 0.16060496494174004, 0.15970952901989222, 0.16607230016961694, 0.16582269803620875, 0.21551648015156388, 0.16368983802385628, 0.16419966495595872, 0.26204252103343606, 0.16545859584584832, 0.16941196494735777, 0.16090952092781663, 0.1604796841274947, 0.16458602319471538, 0.16758631588891149, 0.21116952900774777, 0.1769147519953549, 0.17323430301621556, 0.17223151098005474, 0.17500467807985842, 0.22399448603391647, 0.22187784407287836, 0.16857373202219605, 0.171415152028203, 0.16742289694957435, 0.16907427995465696, 0.18887602095492184, 0.1697176960296929, 0.17194964597001672, 0.16714583500288427, 0.16336239082738757, 0.16505186003632843, 0.16347465105354786, 0.1624466150533408, 0.1684177010320127, 0.16562998481094837, 0.1869254729244858, 0.16904310882091522, 0.1643490989226848, 0.16161779197864234, 0.18625684408470988, 0.2137932451441884, 0.173070196993649, 0.17321579391136765, 0.16040540812537074, 0.170095395995304, 0.1696894420310855, 0.20492040202952921, 0.17296820599585772, 0.16230866103433073, 0.22764927195385098, 0.22426156187430024, 0.22397272801026702, 0.17095335805788636, 0.1680271669756621, 0.1673610000871122, 0.16998405987396836, 0.16394431912340224, 0.1778700570575893, 0.16364286304451525, 0.17259694216772914, 0.16997966798953712, 0.17059752088971436, 0.16707306308671832, 0.16643850598484278, 0.16711999499239028, 0.1814079200848937, 0.17260657413862646, 0.1851093538571149, 0.23024866497144103, 0.17953891889192164, 0.17284952383488417, 0.17303291894495487, 0.17442444385960698, 0.1976531280670315, 0.23341029812581837, 0.177532376954332, 0.16159276803955436, 0.1608569100499153, 0.1738582430407405, 0.2177080838009715, 0.1917835830245167, 0.1660068950150162, 0.16445455490611494, 0.16553482902236283, 0.1659012499731034, 0.17597877397201955, 0.20093695004470646, 0.1715414598584175, 0.16818514792248607, 0.16028373502194881, 0.16804208396933973, 0.19503955193795264, 0.16477191098965704, 0.16005275398492813, 0.16471662488766015, 0.2532942178659141, 0.23058832786045969, 0.26556767895817757, 0.1616221119184047, 0.17084560403600335, 0.2009064150042832, 0.1722550659906119, 0.16551804379560053, 0.17008689395152032, 0.17326378705911338, 0.16343021485954523, 0.16482550697401166, 0.16998509294353426, 0.17999625205993652, 0.17440076684579253, 0.23461109888739884, 0.1727039678953588, 0.17344804503954947, 0.1750439831521362, 0.17223813384771347, 0.21650354517623782, 0.16061355895362794, 0.16050389711745083, 0.16462453198619187, 0.15975967003032565, 0.1630959310568869, 0.2190919101703912, 0.17442697612568736, 0.1722562371287495, 0.16944396402686834, 0.17379642999731004, 0.2235837250482291, 0.17066095303744078, 0.16934282309375703, 0.16933402698487043, 0.16909453202970326, 0.16050071292556822, 0.16406166111119092, 0.15671962988562882, 0.15939153102226555, 0.15712256892584264, 0.16067167604342103, 0.21225584601052105, 0.1595113780349493, 0.16785718919709325, 0.2114962381310761, 0.20939230709336698, 0.16642081108875573, 0.16091074305586517, 0.1626637759618461, 0.21358954394236207, 0.21554554207250476, 0.16248701605945826, 0.1728595329914242, 0.18525331001728773, 0.16444004396907985, 0.15955665009096265, 0.21346917399205267, 0.15841986401937902, 0.1580536919645965, 0.1607372029684484, 0.16183460201136768, 0.16116536385379732, 0.1649049089755863, 0.1616667581256479, 0.1598902470432222, 0.1598874528426677, 0.19071855396032333, 0.16138218785636127, 0.16042346297763288, 0.21359337400645018, 0.21366530493833125, 0.20936871389858425, 0.17074405401945114, 0.1598163959570229, 0.1701500229537487, 0.173159932019189, 0.17990965396165848, 0.17836462799459696, 0.2005334331188351, 0.16981459083035588, 0.17567799799144268, 0.19683183101005852, 0.27712853788398206, 0.1709831648040563, 0.17333291890099645, 0.16380047285929322, 0.17062167916446924, 0.21198001690208912, 0.21982227405533195, 0.20164031814783812, 0.16932063386775553, 0.16954036382958293, 0.1631240400020033, 0.19319236790761352, 0.16506412508897483, 0.1655097419861704, 0.16394094098359346, 0.16404129192233086, 0.1742478869855404, 0.22328108898364007, 0.17253439780324697, 0.17379973107017577, 0.16649032407440245, 0.16574338893406093, 0.22200660197995603, 0.16254237410612404, 0.16974553209729493, 0.16951719694770873, 0.1717707698699087, 0.1790359599981457, 0.1654216239694506, 0.16358332196250558, 0.1651852319482714, 0.2679743480402976, 0.18525746511295438, 0.1652490240521729, 0.16891955398023129, 0.16385789890773594, 0.16727870097383857, 0.16467628395184875, 0.16546210600063205, 0.16587469191290438, 0.17818142310716212, 0.1704944409430027, 0.1660828220192343, 0.1659924630075693, 0.2198369570542127, 0.17542751203291118, 0.17191107897087932, 0.1728973900899291, 0.17433293908834457, 0.20144529710523784, 0.17215728596784174, 0.17397734406404197, 0.16399783687666059, 0.16345347091555595, 0.18006675294600427, 0.21360461390577257, 0.17040258296765387, 0.1682844979222864, 0.16403632308356464, 0.16418516682460904, 0.17222495400346816, 0.22021231008693576, 0.1692581179086119, 0.1688013318926096, 0.17135395295917988, 0.17378104920499027, 0.217753920936957, 0.18586224387399852, 0.16806289204396307, 0.16515952092595398, 0.16569895297288895, 0.16904028202407062, 0.18588741100393236, 0.23030655295588076, 0.17954697599634528, 0.1773915800731629, 0.16829795902594924, 0.16865962697193027, 0.16705746809020638, 0.16830435278825462, 0.17339831800200045, 0.1762608529534191, 0.17138134711422026, 0.1739352480508387, 0.1729510761797428, 0.22814350691623986, 0.2286844700574875, 0.17729254998266697, 0.2199251470156014, 0.21769322594627738, 0.16674815700389445, 0.15563288587145507, 0.15816858992911875, 0.2498693810775876, 0.15727407205849886, 0.1635822888929397, 0.15795442601665854, 0.15785361593589187, 0.16038049710914493, 0.15978776407428086, 0.17337457300163805, 0.17414785595610738, 0.1697615480516106, 0.16329550300724804, 0.1692114758770913, 0.16299709002487361, 0.18743720301426947, 0.16612307517789304, 0.1857705779839307, 0.15667457901872694, 0.1719403371680528, 0.1602623830549419, 0.1695667440071702, 0.16543518798425794, 0.16473000007681549, 0.17685860604979098, 0.18654592311941087, 0.1760433588642627, 0.16544939810410142, 0.16951479180715978, 0.17188670695759356, 0.1776814260520041, 0.17709128092974424, 0.2270240201614797, 0.19560832902789116, 0.17292465385980904, 0.1735252388752997, 0.17696520197205245, 0.17260202788747847, 0.1767948039341718, 0.17644340777769685, 0.17895447299815714, 0.1765446171630174, 0.17708176583983004, 0.17692771297879517, 0.1671056840568781, 0.16207427997142076, 0.17151804105378687, 0.15928734512999654, 0.15999779687263072, 0.2094095810316503, 0.16031679115258157, 0.16268643992953002, 0.16643562400713563, 0.16745859500952065, 0.16200415208004415, 0.1638524669688195, 0.2184338690713048, 0.1682057569269091, 0.16653811186552048, 0.16955403704196215, 0.22602119809016585, 0.2314816650468856, 0.16147538111545146, 0.16063226480036974, 0.16793780401349068, 0.18144665891304612, 0.16299152304418385, 0.1618462831247598, 0.1630877370480448, 0.16231827903538942, 0.1632467322051525, 0.1635250428225845, 0.16940203495323658, 0.16425060899928212, 0.16285450593568385, 0.20307235792279243, 0.16028217296116054, 0.1656530611217022, 0.1663477618712932, 0.16398412082344294, 0.1664982489310205, 0.15991925611160696, 0.16048927302472293, 0.16509794699959457, 0.24864991707727313, 0.1622221521101892, 0.16254566004499793, 0.23047519498504698, 0.18806530395522714, 0.16553129511885345, 0.16449345787987113, 0.1596890229266137, 0.156828748062253, 0.16145758796483278, 0.16279624006710947, 0.15979277389124036, 0.17636276595294476, 0.16142995096743107, 0.1814476849976927, 0.16189292003400624, 0.17107803095132113, 0.15888861077837646, 0.15899793594144285, 0.16086505283601582, 0.17223319387994707, 0.17000518296845257, 0.17125764791853726, 0.16108612809330225, 0.15994410309940577, 0.17527880193665624, 0.2198148900642991, 0.16193470102734864, 0.20979869412258267, 0.2136036199517548, 0.2136823390610516, 0.21984219807200134, 0.16046411404386163, 0.16073382389731705, 0.17166815791279078, 0.1645055590197444, 0.1866522589698434, 0.1702259921003133, 0.1734896011184901, 0.164517740951851, 0.16006664582528174, 0.15859132003970444, 0.18829475296661258, 0.16781558888033032, 0.17016073688864708, 0.1616600858978927, 0.1713853960391134, 0.17411252995952964, 0.2219298528507352, 0.1590596039313823, 0.1721929640043527]
[0.0014104116906577995, 0.0014037014266779257, 0.0013714221941466018, 0.0014519112856062346, 0.0017029247752051483, 0.0014281693102373171, 0.0015329980479134608, 0.0013279544889869153, 0.001358410280356689, 0.0014150702943231247, 0.0013333688825802055, 0.0013493285117338793, 0.0017131458383140175, 0.0013435821467649568, 0.0019151742476248002, 0.0014080933634509412, 0.001483052606196251, 0.0015085905974427628, 0.0013444903938374085, 0.0013361960620612138, 0.0018077220392319583, 0.001382870287381748, 0.0013293919763563908, 0.0013755564108606456, 0.0013357585353791252, 0.0013455641780828322, 0.0019840084417008384, 0.0014333413271305634, 0.0014611665888638922, 0.001999148232645767, 0.0020174725439630506, 0.0014612930159033962, 0.0013822236751718808, 0.001395451202772038, 0.0013856257297560689, 0.0014281470090324103, 0.002050344301604254, 0.0014282662796881772, 0.0013964393251943727, 0.0013967168069798355, 0.0014015071566916945, 0.0014058003425886927, 0.0022132276835338783, 0.001342282953578257, 0.0013031285046780294, 0.0015545135739353276, 0.002015891154621576, 0.0019998161244935307, 0.0014581589367558328, 0.001400749549240798, 0.0013844436828711235, 0.0014066603016685839, 0.0014060296825775804, 0.0014461938457837862, 0.0014417664176791674, 0.001435357185672651, 0.0014410436283363852, 0.0019990451229635136, 0.0019515797896527273, 0.0019515394648291576, 0.001392527931016083, 0.0014144467431763115, 0.0013257491694719985, 0.0019522612470440394, 0.0019346505038931158, 0.0015151363476215637, 0.00135057650383764, 0.0013389122408865265, 0.0013669361161833355, 0.001334107262551669, 0.0013629882018868775, 0.00195175510932251, 0.0013609947041110244, 0.0013418510545391677, 0.0019514652080978303, 0.0019840182169780945, 0.0013420735341460668, 0.001352280333219457, 0.0019355479613408562, 0.001951620433941599, 0.0020273378129701974, 0.001372836210743286, 0.001411129704265054, 0.0014216295577037011, 0.0014906307834005633, 0.0020329587832959586, 0.0013577211630176897, 0.0013657407131219327, 0.0014632290550471508, 0.0014214314801921678, 0.0018939771689474583, 0.0017619642948861732, 0.0013528855430871942, 0.0013351701019519522, 0.0013224668999368598, 0.0013784385816908854, 0.0015926433640510538, 0.0014397657508853562, 0.0013200902946784285, 0.0019841834653688724, 0.0019624674252357133, 0.0013263379151449185, 0.0013489124343492264, 0.0014039763346920873, 0.0013660474503485037, 0.0019349659767810457, 0.0013974520536644975, 0.0013460410711925853, 0.0013399746272597433, 0.001357892389199877, 0.0019260862022782712, 0.0013340256364597369, 0.0013881331948606775, 0.0013171582637975614, 0.0013186045193163923, 0.0014156989226218803, 0.0016243196500450835, 0.001699175434764619, 0.0014108820933877498, 0.0014852883019881656, 0.0014897680538165014, 0.0013380962398711793, 0.0014735435355858978, 0.0020609339608904697, 0.0014299143650821707, 0.0014759221153925787, 0.001386829285499777, 0.0013077568457743457, 0.0017726165206395379, 0.001355831365122698, 0.001423766154276077, 0.0014214811868272549, 0.0015205704108920209, 0.0014164777284906816, 0.001435386231746669, 0.001565981970363578, 0.0019597719629230194, 0.001420925681789716, 0.0014065910841865364, 0.001322921323987056, 0.0014201885742647934, 0.0015469083883763515, 0.001984109784397044, 0.0014425046207715375, 0.0014182245503215826, 0.0013195347986653331, 0.0013172820159919965, 0.001378584032257398, 0.001432795821176481, 0.0014043197454523672, 0.0014315697277874448, 0.0014790560465392677, 0.001950897355816623, 0.0019189937350650628, 0.0016584195496240097, 0.001314559967668597, 0.0013756858755278494, 0.0014524310863353023, 0.0020534595034207947, 0.001403138999892182, 0.0014094943487511357, 0.0013210707914459612, 0.001432040099834287, 0.0013408380228477393, 0.0016012917283575894, 0.00138090658109895, 0.0013081614260179127, 0.0019192417989694333, 0.001984058991218953, 0.0013633757818836806, 0.0015234051005149534, 0.001952671821323014, 0.0019347681772223738, 0.002036606975968263, 0.0013717667762796547, 0.0013558534569605145, 0.0013657109450924303, 0.001484035952883058, 0.0014995826361339916, 0.0014982104586625054, 0.0015045166658165382, 0.00140947872202468, 0.0013747153796486613, 0.0013413037292572649, 0.0013564416593619326, 0.0019686892484278642, 0.0014556350309199602, 0.0014273983790979597, 0.00140071531050196, 0.0014309340694471616, 0.0014241621396396049, 0.0014777947975279286, 0.002044261875381881, 0.0014360972008732862, 0.0014151070761501557, 0.0013530121072982402, 0.0013410515195235264, 0.0013328142258403606, 0.0013627245347196048, 0.0019904103639080775, 0.0013467566662489674, 0.0013454236440331667, 0.001345724845666063, 0.0014133416747948689, 0.001923187588911879, 0.0013846146202711172, 0.0013441258370009966, 0.0013440131162863601, 0.0014235639995209468, 0.0013949553095988295, 0.002028215077323045, 0.0013692531149921028, 0.0013657924177688221, 0.0013801660316512566, 0.002002775629390349, 0.0018869729228895302, 0.001419743403879016, 0.0014046587053570868, 0.0013873859763491986, 0.0014134762093944604, 0.002018372309863452, 0.002081097193899774, 0.001410991318032954, 0.0013582954966542564, 0.0014093137515867635, 0.0014171928217880023, 0.0014451187602136024, 0.0013757896042869303, 0.0014222958287614029, 0.0014295041698298252, 0.0014379998604052288, 0.0019042049390227758, 0.0014372530858963728, 0.0014348136362179306, 0.0014284893711523493, 0.0013658698222260605, 0.0015121849304310573, 0.0015013881944581982, 0.0014734835049873868, 0.0014841616417230793, 0.0014612084568673095, 0.0014548676897042482, 0.0020853162782858738, 0.0014789646505847457, 0.0013951388451220223, 0.0018635704867946085, 0.002032999209193296, 0.0020486766060944213, 0.0014078207747187726, 0.0014061380156077617, 0.0014444190156898758, 0.00143301088269078, 0.0014299128146827683, 0.0019357506268794916, 0.0019324124509165453, 0.0014172122784883016, 0.0014609604651584875, 0.0014278234484381685, 0.0014120755886817857, 0.0015267001468572737, 0.0019029618219234222, 0.0013737970631020938, 0.0014779637369161198, 0.0014303084733614394, 0.0014093665878988746, 0.0014563194032065397, 0.0014316280780345663, 0.0013720786429999412, 0.001351163278572088, 0.0013534748212022836, 0.0014586452712384305, 0.0018400637443762185, 0.0013987046283806942, 0.0013802322331642688, 0.0017112509379986413, 0.0013978953018437983, 0.0014358179918093274, 0.0013731377751588128, 0.001381405721266949, 0.0014153103950585043, 0.0013892705039691556, 0.0019036346591662528, 0.0013681503713073194, 0.0014030973809634067, 0.0014195525892396537, 0.0014822191320532977, 0.0019517124110702859, 0.0019113133957450704, 0.001368041047393236, 0.001345842201334908, 0.0013401955979796805, 0.0019682056826445483, 0.0013660020700316559, 0.0013642578743138286, 0.0013931999524492164, 0.0013376526812510204, 0.0013357819068830373, 0.00199280422570747, 0.0017609844027563584, 0.0014196238372215005, 0.0016279422782736923, 0.001404506728312997, 0.0013980996982707072, 0.002160220107337994, 0.0014407782014026198, 0.0019886219845146173, 0.0019849518606395917, 0.0018482065052082834, 0.0013778984813784907, 0.001412500735632209, 0.0013707176034966873, 0.001380593091783713, 0.0014373970763079187, 0.0019774173725714056, 0.0014032483111720445, 0.0014342936676342128, 0.001422225006733292, 0.0015519081466910682, 0.0014941590321936118, 0.0015128431083677814, 0.002152409604858867, 0.0014719418592953175, 0.0014762024958293106, 0.0023172297984246134, 0.00146782921641603, 0.0020272393959985917, 0.0018062203014341666, 0.0014117552335698937, 0.0014715421089339395, 0.0014540493964802386, 0.0014175669535590235, 0.0016247012630798096, 0.0014172615663892076, 0.0013810224332954995, 0.001415637760358148, 0.0014298633950249862, 0.0014625738461523554, 0.001472258682240003, 0.0014004756898669771, 0.0014130798071016405, 0.001417181882357528, 0.0014029492086247188, 0.0014826449522272098, 0.0020799207746578278, 0.0013821884581833616, 0.0013924769916420067, 0.0013954361933175214, 0.0013814304879498343, 0.0014180587370006383, 0.0013940433246018582, 0.001378905721182047, 0.0014736967670276415, 0.0014324679306542227, 0.0013842986582583467, 0.0014023801163849674, 0.0014027881241121956, 0.001386624845755539, 0.0013732117755959432, 0.0015135368841245425, 0.001511506860815855, 0.0015048246195381002, 0.001475615674167756, 0.0013616126962006092, 0.001379139474123832, 0.0014366509299003339, 0.0014016994260713574, 0.0013697026189156743, 0.0013716916911044093, 0.0013756655272125274, 0.0013785596825038047, 0.002034318117179381, 0.0014752603330931, 0.0014665098608175452, 0.002066005822704282, 0.0020649911395396836, 0.0014543747132311957, 0.0013347670630833438, 0.0013305739600702313, 0.0013725053186911021, 0.0014240113249438446, 0.0014580260067023048, 0.001442628999262355, 0.0013515184878898684, 0.0014902968446962362, 0.002290025270406821, 0.0019672983813337807, 0.0017452602014811926, 0.0014615585800087036, 0.0014509033653596344, 0.001577387852666452, 0.0014727386201970106, 0.0014616002711330272, 0.001380305318522823, 0.0013645239762759716, 0.0013665333263893692, 0.0014032427521148162, 0.0014225653870854266, 0.0014713223637334368, 0.001464020659381798, 0.00184561439443293, 0.0014378647211679193, 0.0014490654814786226, 0.0015001021932030832, 0.0015083789922903444, 0.0014959527829358744, 0.001465486162233838, 0.0014459903788745635, 0.0014374151306096898, 0.001440610294410771, 0.0016818741542222195, 0.0014366789616268966, 0.0014392246055423981, 0.0014335495499215385, 0.0013976140929631484, 0.0016120765105525191, 0.0018467468447809995, 0.0013921787518314844, 0.0013834900627013786, 0.0014329359455164089, 0.0013598158843670936, 0.0014207815036697442, 0.0014629676602347646, 0.0013564648828649705, 0.001360699115682018, 0.0013662894426072522, 0.001433527526669493, 0.0014447031701489011, 0.001921481526882727, 0.001379985380340223, 0.001433622519767215, 0.0014360949700308401, 0.0014063854730695254, 0.0018455518135266711, 0.0015594725038625243, 0.0014061709295582865, 0.0014074138914372109, 0.0014558038367463867, 0.0014482084336842214, 0.0015149825656887634, 0.0015421959936127874, 0.0014924315497452437, 0.001512202257362678, 0.0014897462958830964, 0.0014543413949856, 0.0014545399291339771, 0.0017611453410728958, 0.001485354644282497, 0.0013675397449685622, 0.0014066548689535654, 0.0013768249137197123, 0.0014187087207426165, 0.0015300402017093676, 0.001417882341617184, 0.001420197192608386, 0.0013587010858306127, 0.0013506330149031657, 0.001395149999334253, 0.0014162230929865161, 0.0013915021947130215, 0.0013684645970171506, 0.0013529396428214025, 0.0013642972407414931, 0.001357638730140504, 0.0013416090857439726, 0.001364426674725589, 0.0013484702330053776, 0.001529979875109108, 0.0014999911239616168, 0.0015056738449448302, 0.0014599041702846687, 0.001512560349087729, 0.0017509727283965709, 0.001447452883806455, 0.0014767949848414161, 0.0014496680002572925, 0.0013507187904342431, 0.0019449434340606596, 0.0013506835572017256, 0.001497923497601535, 0.0019663431548130835, 0.001363873209212293, 0.0013336691871834124, 0.0013945662477883951, 0.001520963232285639, 0.002015167953764168, 0.0014533960394012605, 0.0014663575264054905, 0.0014542928290401781, 0.0014423695266564457, 0.001454718008478598, 0.0014071081631434287, 0.0020069737752625182, 0.001400721100584943, 0.0013888741395789985, 0.0013782143802201564, 0.0013783992549707723, 0.0013534567596809577, 0.0019381453496107991, 0.0013687803009308356, 0.0014627942393079054, 0.0015113071318311516, 0.0020984352562692047, 0.0020490375892511866, 0.0015228293644798587, 0.0015102365729218537, 0.0014719942877025798, 0.0019283149925659793, 0.0020782761384061602, 0.0014692428756447494, 0.0014738579056868257, 0.0013503664490846239, 0.001449580241153522, 0.001763858852427802, 0.0013767997261311393, 0.0012874815693066921, 0.0012901946101919748, 0.00141068850098236, 0.0012819903586205328, 0.0012695974910457153, 0.0016424818059022073, 0.001702711007965263, 0.001353917139567784, 0.001342642070085276, 0.0013338236640265677, 0.001323561584285926, 0.0016840763273648918, 0.0012972283129784046, 0.0012853721091232728, 0.0012940012820763513, 0.0014612919203500496, 0.0016853817978699226, 0.0012693350236077094, 0.0012666439597524004, 0.00131013325699314, 0.0012704444125120062, 0.0013785506489512045, 0.0016191315226024017, 0.0012533241570054088, 0.0013527070150303189, 0.0013844875793438405, 0.0013697324229724472, 0.0013652225316036493, 0.0013739933201577514, 0.0012406413661665283, 0.001354720157905831, 0.0012414197735779453, 0.0016776564607425826, 0.001254726288607344, 0.001247730695467908, 0.0012974398450751323, 0.0012954898284078809, 0.0016837225011840928, 0.0012788268595613772, 0.0012828098824684275, 0.0020472071955737192, 0.00129264528004569, 0.0013235309761512326, 0.0012571056322485674, 0.0012537475322460523, 0.001285828306208714, 0.001309268092882121, 0.0016497619453730294, 0.00138214649996371, 0.001353392992314184, 0.0013455586795316776, 0.001367224047498894, 0.0017499569221399724, 0.0017334206568193622, 0.0013169822814234067, 0.001339180875220336, 0.0013079913824185496, 0.0013208928121457575, 0.0014755939137103269, 0.0013259195002319757, 0.0013433566091407556, 0.0013058268359600333, 0.0012762686783389654, 0.001289467656533816, 0.0012771457113558426, 0.001269114180104225, 0.0013157632893125992, 0.0012939842563355342, 0.0014603552572225453, 0.0013206492876634002, 0.001283977335333475, 0.0012626389998331433, 0.001455131594411796, 0.001670259727688972, 0.0013521109140128829, 0.0013532483899325598, 0.001253167250979459, 0.0013288702812133124, 0.0013256987658678554, 0.001600940640855697, 0.0013513141093426384, 0.0012680364143307088, 0.0017785099371394608, 0.0017520434521429706, 0.001749786937580211, 0.0013355731098272372, 0.0013127122419973603, 0.001307507813180564, 0.0013280004677653778, 0.00128081499315158, 0.0013896098207624163, 0.0012784598675352754, 0.001348413610685384, 0.0013279661561682587, 0.0013327931319508934, 0.0013052583053649869, 0.0013003008280065842, 0.001305624960878049, 0.001417249375663232, 0.0013484888604580192, 0.0014461668270087102, 0.001798817695089383, 0.0014026478038431378, 0.0013503869049600326, 0.00135181967925746, 0.0013626909676531795, 0.0015441650630236836, 0.001823517954107956, 0.0013869716949557187, 0.0012624435003090184, 0.0012566946097649634, 0.001358267523755785, 0.00170084440469509, 0.0014983092423790367, 0.001296928867304814, 0.001284801210204023, 0.0012932408517372096, 0.0012961035154148703, 0.0013748341716564028, 0.0015698199222242692, 0.0013401676551438868, 0.0013139464681444224, 0.0012522166798589751, 0.0013128287810104666, 0.001523746499515255, 0.0012872805546066957, 0.001250412140507251, 0.0012868486319348449, 0.001978861077077454, 0.0018014713114098413, 0.0020747474918607622, 0.0012626727493625367, 0.0013347312815312762, 0.0015695813672209624, 0.0013457427030516556, 0.0012931097171531292, 0.0013288038589962525, 0.0013536233363993233, 0.0012767985535901971, 0.001287699273234466, 0.0013280085386213614, 0.001406220719218254, 0.0013625059909827542, 0.0018328992100578034, 0.0013492497491824906, 0.0013550628518714802, 0.0013675311183760641, 0.0013456104206852615, 0.001691433946689358, 0.0012547934293252183, 0.0012539366962300846, 0.001286129156142124, 0.0012481224221119191, 0.001274186961381929, 0.0017116555482061813, 0.0013627107509819325, 0.0013457518525683554, 0.001323780968959909, 0.0013577846093539847, 0.0017467478519392898, 0.001333288695605006, 0.0013229908054199768, 0.0013229220858193003, 0.0013210510314820567, 0.0012539118197310017, 0.001281731727431179, 0.0012243721084814752, 0.0012452463361114496, 0.0012275200697331456, 0.0012552474690892268, 0.0016582487969571957, 0.0012461826408980414, 0.001311384290602291, 0.001652314360399032, 0.0016358773991669295, 0.0013001625866309041, 0.0012571151801239466, 0.0012708107497019228, 0.0016686683120497037, 0.0016839495474414434, 0.0012694298129645176, 0.0013504651014955016, 0.0014472914845100604, 0.0012846878435084363, 0.0012465363288356457, 0.0016677279218129115, 0.0012376551876513986, 0.0012347944684734102, 0.0012557593981910031, 0.00126433282821381, 0.0012591044051077915, 0.001288319601371768, 0.0012630215478566242, 0.0012491425550251734, 0.0012491207253333414, 0.001489988702815026, 0.0012607983426278224, 0.0012533083045127569, 0.001668698234425392, 0.0016692601948307129, 0.0016356930773326894, 0.001333937922026962, 0.0012485655934142414, 0.0013292970543261617, 0.001352811968899914, 0.0014055441715754569, 0.0013934736562077887, 0.0015666674462408992, 0.0013266764908621553, 0.001372484359308146, 0.0015377486797660822, 0.00216506670221861, 0.0013358059750316897, 0.0013541634289140347, 0.0012796911942132283, 0.001332981868472416, 0.0016560938820475712, 0.0017173615160572808, 0.0015753149855299853, 0.00132281745209184, 0.0013245340924186166, 0.0012744065625156509, 0.0015093153742782306, 0.0012895634772576159, 0.0012930448592669563, 0.001280788601434324, 0.0012815725931432098, 0.0013613116170745343, 0.001744383507684688, 0.001347924982837867, 0.0013578103989857482, 0.0013007056568312692, 0.001294870226047351, 0.0017344265779684065, 0.001269862297704094, 0.0013261369695101166, 0.0013243531011539744, 0.0013419591396086616, 0.0013987184374855133, 0.0012923564372613328, 0.0012779947028320748, 0.0012905096245958703, 0.002093549594064825, 0.001447323946194956, 0.0012910080004076008, 0.001319684015470557, 0.001280139835216687, 0.0013068648513581138, 0.0012865334683738183, 0.0012926727031299379, 0.0012958960305695655, 0.001392042368024704, 0.0013319878198672086, 0.001297522047025268, 0.0012968161172466353, 0.0017174762269860366, 0.0013705274377571186, 0.0013430553044599947, 0.0013507608600775711, 0.001361976086627692, 0.0015737913836346706, 0.0013449787966237636, 0.001359198000500328, 0.0012812331005989108, 0.0012769802415277809, 0.0014067715073906584, 0.0016687860461388482, 0.0013312701794347959, 0.0013147226400178624, 0.0012815337740903487, 0.0012826966158172581, 0.001345507453152095, 0.0017204086725541856, 0.0013223290461610304, 0.0013187604054110125, 0.0013387027574935928, 0.0013576644469139865, 0.0017012025073199766, 0.0014520487802656135, 0.0013129913440934615, 0.0012903087572340155, 0.001294523070100695, 0.0013206272033130517, 0.0014522453984682215, 0.0017992699449678184, 0.0014027107499714475, 0.0013858717193215853, 0.0013148278048902284, 0.0013176533357182052, 0.0013051364694547374, 0.0013148777561582392, 0.0013546743593906285, 0.0013770379136985866, 0.0013389167743298458, 0.0013588691253971774, 0.0013511802826542407, 0.0017823711477831239, 0.001786597422324121, 0.0013850980467395857, 0.001718165211059386, 0.001700728327705292, 0.0013027199765929254, 0.0012158819208707428, 0.0012356921088212403, 0.0019521045396686532, 0.0012287036879570223, 0.0012779866319760913, 0.0012340189532551449, 0.0012332313744991552, 0.0012529726336651947, 0.0012483419068303192, 0.0013544888515752973, 0.0013605301246570889, 0.0013262620941532077, 0.0012757461172441253, 0.0013219646552897757, 0.0012734147658193251, 0.0014643531485489802, 0.0012978365248272894, 0.0014513326404994586, 0.0012240201485838043, 0.0013432838841254124, 0.0012520498676167335, 0.0013247401875560172, 0.0012924624061270151, 0.001286953125600121, 0.001381707859763992, 0.0014573900243703974, 0.0013753387411270523, 0.0012925734226882923, 0.0013243343109934358, 0.0013428648981061997, 0.001388136141031282, 0.001383525632263627, 0.0017736251575115602, 0.0015281900705303997, 0.0013509738582797581, 0.0013556659287132788, 0.0013825406404066598, 0.0013484533428709256, 0.0013812094057357172, 0.0013784641232632566, 0.0013980818202981027, 0.0013792548215860734, 0.0013834512956236722, 0.0013822477576468373, 0.00130551315669436, 0.0012662053122767247, 0.00133998469573271, 0.001244432383828098, 0.0012499827880674275, 0.001636012351809768, 0.0012524749308795435, 0.0012709878119494533, 0.001300278312555747, 0.00130827027351188, 0.001265657438125345, 0.0012800973981939023, 0.0017065146021195687, 0.0013141074759914773, 0.0013010789989493787, 0.0013246409143903293, 0.0017657906100794207, 0.0018084505081787938, 0.0012615264149644645, 0.0012549395687528886, 0.001312014093855396, 0.0014175520227581728, 0.0012733712737826863, 0.0012644240869121859, 0.00127412294568785, 0.0012681115549639799, 0.001275365095352754, 0.0012775393970514415, 0.0013234533980721608, 0.0012832078828068916, 0.00127230082762253, 0.001586502796271816, 0.0012522044762590667, 0.0012941645400132984, 0.001299591889619478, 0.001281125943933148, 0.0013007675697735976, 0.0012493691883719293, 0.001253822445505648, 0.0012898277109343326, 0.0019425774771661963, 0.0012673605633608531, 0.0012698879691015463, 0.0018005874608206796, 0.001469260187150212, 0.0012932132431160426, 0.0012851051396864932, 0.0012475704916141694, 0.0012252245942363515, 0.0012613874059752561, 0.0012718456255242927, 0.0012483810460253153, 0.001377834109007381, 0.0012611714919330552, 0.0014175600390444743, 0.0012647884377656737, 0.0013365471168071963, 0.001241317271706066, 0.0012421713745425222, 0.0012567582252813736, 0.0013455718271870865, 0.0013281654919410357, 0.0013379503743635723, 0.0012584853757289238, 0.0012495633054641075, 0.0013693656401301268, 0.0017173038286273368, 0.0012651148517761612, 0.0016390522978326771, 0.0016687782808730844, 0.0016693932739144657, 0.0017175171724375105, 0.001253625890967669, 0.0012557329991977895, 0.001341157483693678, 0.001285199679841753, 0.0014582207732019015, 0.0013298905632836977, 0.001355387508738204, 0.001285294851186336, 0.0012505206705100136, 0.001238994687810191, 0.0014710527575516608, 0.0013110592881275807, 0.0013293807569425553, 0.0012629694210772868, 0.0013389484065555735, 0.0013602541403088253, 0.0017338269753963687, 0.0012426531557139242, 0.0013452575312840054]
[709.012841161017, 712.4022110361839, 729.1700573813978, 688.7473152896237, 587.2249993424001, 700.1970934621409, 652.3165514535939, 753.037855056984, 736.1546172467288, 706.6786745589436, 749.9800040817643, 741.1093675883316, 583.7214658759841, 744.2790174071417, 522.145700967001, 710.1801811985038, 674.2849146564063, 662.8703650248893, 743.7762326778898, 748.393165040018, 553.1823910410845, 723.1336222382405, 752.2235862599448, 726.9785463573457, 748.6383006462859, 743.1826859606251, 504.03011347206075, 697.670527648792, 684.3846606002227, 500.21303256565074, 495.66969473380584, 684.3254495278505, 723.4719083187814, 716.614094433054, 721.6956054764111, 700.2080273777376, 487.7229640005185, 700.1495548983503, 716.1070172961566, 715.9647503364203, 713.5175837136175, 711.3385661569573, 451.8287962146272, 744.9994036907053, 767.3840272928993, 643.2880463490909, 496.05852861025147, 500.0459731032812, 685.7962975043296, 713.9034958404926, 722.3117938074256, 710.9036906876504, 711.222538465025, 691.4702361065816, 693.593627745689, 696.6906983026461, 693.9415159514992, 500.23883328733245, 512.40538834333, 512.415976218827, 718.1184504287337, 706.99021000563, 754.2904970464862, 512.2265278348999, 516.8892252051161, 660.006607042187, 740.424549929987, 746.8749403156368, 731.5630834249451, 749.5649173570635, 733.682066077778, 512.3593606715949, 734.7567165246104, 745.2391952275436, 512.4354745605427, 504.02763011073756, 745.1156546621561, 739.4916390000574, 516.6495586641249, 512.3947170302708, 493.25770653630104, 728.4190147188618, 708.6520799452812, 703.4181264599325, 670.8569359601635, 491.8938879708806, 736.5282557556857, 732.2034046375541, 683.4199994530427, 703.5161482879265, 527.9894691421919, 567.5483906809827, 739.1608293175098, 748.9682389817222, 756.1625928389922, 725.4585102901958, 627.8869598630017, 694.5574301827011, 757.5239391056945, 503.98565326926234, 509.56259815619075, 753.9556764391658, 741.3379657089774, 712.262717889269, 732.0389930414801, 516.8049526449925, 715.5880571199057, 742.9193814376, 746.2827874920337, 736.4353817383416, 519.1875622270436, 749.6107815842425, 720.3919650522923, 759.2102084352815, 758.3775008737515, 706.3648802868308, 615.6423706209825, 588.5207492647907, 708.7764489227039, 673.2699629165785, 671.2454314201401, 747.3304013590767, 678.6362098235452, 485.21690601281034, 699.3425791218865, 677.5425271908817, 721.0692840536794, 764.6681439529247, 564.1378089149324, 737.5548506428773, 702.3625312321434, 703.491547596208, 657.6479410863745, 705.9765077037556, 696.6766002646804, 638.5769561368752, 510.2634484618762, 703.7665747166014, 710.9386738209867, 755.902850659458, 704.1318442641904, 646.4506932111273, 504.0043690444743, 693.2386805562816, 705.1069591012579, 757.8428405309718, 759.1388843541881, 725.3819691807428, 697.9361505806818, 712.0885419708135, 698.5339104268046, 676.1069009790568, 512.5846303591977, 521.1064433027424, 602.9837264199617, 760.7108268887295, 726.9101310037809, 688.5008241755189, 486.9830636222097, 712.6877665554449, 709.4742883403806, 756.9617059699448, 698.3044679515037, 745.802239316088, 624.4958256455109, 724.1619481632002, 764.4316520202201, 521.0390897785602, 504.01727187840646, 733.473495193211, 656.424216816638, 512.1188256419143, 516.8577878077557, 491.0127539578767, 728.9868928828288, 737.5428331626272, 732.2193642757404, 673.8381223563255, 666.852213345212, 667.4629683821112, 664.6652860154762, 709.4821541991961, 727.4233014368195, 745.5432935788087, 737.2230077852356, 507.9521822951844, 686.9853904024285, 700.5752666133383, 713.920946321091, 698.8442174602411, 702.1672407701118, 676.6839358704002, 489.17411807290773, 696.3316963447204, 706.6603063851055, 739.0916863241144, 745.6835068911436, 750.2921116928236, 733.8240227734365, 502.4089595456823, 742.5246334850037, 743.2603139055199, 743.0939565547314, 707.5429939084893, 519.9700776801448, 722.2226209081867, 743.9779613426615, 744.0403578523832, 702.4622709878282, 716.8688438395827, 493.04435766243176, 730.3251597903193, 732.1756856972556, 724.5505084656961, 499.307054332593, 529.9493108087081, 704.3526296849169, 711.9167070165873, 720.779953846315, 707.475649999376, 495.4487311945201, 480.5157601150272, 708.7215826346032, 736.2168265029169, 709.5652042521318, 705.6202830172039, 691.9846503495605, 726.8553250322738, 703.0886119316285, 699.5432550008194, 695.4103595797288, 525.1535585834541, 695.7716840637912, 696.9546251567074, 700.0402104450446, 732.1341929717908, 661.2947794122932, 666.050261811781, 678.6638578682699, 673.7810571893111, 684.3650509277122, 687.3477272722198, 479.54356392498795, 676.1486825291091, 716.7745371698381, 536.6043340383798, 491.8841067315539, 488.11998781320153, 710.3176895509024, 711.167743777825, 692.3198802685281, 697.8314066410223, 699.343337392115, 516.5954674709524, 517.4878683511374, 705.6105956594382, 684.4812189298485, 700.366702265504, 708.1773865473658, 655.0074695797398, 525.4966171571682, 727.9095485485726, 676.6065871728177, 699.1498817383427, 709.5386030761731, 686.6625534193868, 698.5054396061204, 728.8211977511576, 740.1030029892478, 738.8390122482707, 685.5676426050949, 543.4594334333781, 714.9472302509774, 724.515756096666, 584.3678316223625, 715.361156648154, 696.4671049565711, 728.2590415112155, 723.9002883836729, 706.5587898537714, 719.802225083591, 525.3108810479299, 730.9138096015441, 712.7089064291257, 704.4473079617459, 674.6640752199129, 512.3705697252891, 523.2004349606825, 730.9722189297405, 743.0291597396221, 746.1597407926731, 508.0769803775619, 732.0633123029058, 732.9992509685628, 717.7720600995003, 747.5782122043552, 748.6252020986246, 501.8054393401277, 567.8642005203242, 704.4119532095265, 614.272393650482, 711.9937411771147, 715.2565737886133, 462.9157911284719, 694.0693571199818, 502.8607788644557, 503.7905552418686, 541.0650796769623, 725.7428711290619, 707.9642330610318, 729.5448730278284, 724.3263825896807, 695.7019855422194, 505.71013174604315, 712.6322490741238, 697.2072892502163, 703.1236233828427, 644.3680330773248, 669.2728005879503, 661.0070763245953, 464.5955852188133, 679.3746598651278, 677.4138391076311, 431.5497758055147, 681.278168342827, 493.2816528594607, 553.6423210424468, 708.3380859664379, 679.5592147372876, 687.7345449340727, 705.4340519785281, 615.497767327628, 705.5860567416111, 724.1011991483186, 706.3953985990074, 699.3675084482636, 683.726160310288, 679.2284617255755, 714.0430978098477, 707.674113644787, 705.625729801504, 712.7841791081509, 674.4703096299711, 480.78754353733115, 723.4903417688209, 718.1447205248265, 716.6218024075984, 723.8873100912149, 705.1894071151934, 717.3378203906268, 725.2127427122172, 678.5656468643413, 698.0959074897333, 722.3874660531337, 713.073430175111, 712.8660293106527, 721.1755963128755, 728.2197966632076, 660.7040835865843, 661.591439591771, 664.529266079489, 677.6832325015779, 734.4232341475376, 725.0898250413002, 696.0633088995208, 713.4197113876055, 730.0854843890497, 729.0267969727629, 726.9208831788292, 725.3947817360748, 491.5652038662067, 677.8464638192727, 681.891084893574, 484.0257413655582, 484.26357907904367, 687.5807114236001, 749.1943932823576, 751.5553663377099, 728.5946264701225, 702.2416061469418, 685.8588224100017, 693.1789119110457, 739.9084873498877, 671.0072584256378, 436.6764039343334, 508.3113011672506, 572.980463973971, 684.2011081034091, 689.2257774535733, 633.9594908820792, 679.0071138802814, 684.181591745877, 724.4773939364237, 732.8563054855053, 731.7787138365537, 712.6350722231829, 702.9553854454557, 679.6607083865223, 683.0504703548809, 541.824989562488, 695.4757184582292, 690.0999387409342, 666.6212505594413, 662.9633567632666, 668.4702962599229, 682.3674121055512, 691.5675336500616, 695.693247347301, 694.150252764238, 594.5748066165204, 696.0497276772252, 694.8185822762053, 697.5691911414798, 715.5050918811588, 620.3179523143492, 541.4927350903838, 718.298565241315, 722.8096731301541, 697.867900605714, 735.3936746116445, 703.8379915680873, 683.5421090849873, 737.2103860793757, 734.9163297565412, 731.9093369350259, 697.5799078817097, 692.1837098875702, 520.4317533160612, 724.6453580207196, 697.5336856192628, 696.332778032448, 711.0426118220893, 541.8433623324271, 641.2424698243703, 711.151097622339, 710.5230423573755, 686.9057319116054, 690.5083389522973, 660.0736025931529, 648.4260133871666, 670.0474806839208, 661.2872022450405, 671.255235044714, 687.5964635593016, 687.5026116301894, 567.8123075241459, 673.2398917990737, 731.2401732228913, 710.9064363058134, 726.3087630353378, 704.8663234244127, 653.5775980806228, 705.2771380588865, 704.1275713011117, 735.9970566216715, 740.3935702487589, 716.7688065635857, 706.1034415779866, 718.6478065212369, 730.7459777766303, 739.1312726372724, 732.9781004735462, 736.5729761528743, 745.3736044471274, 732.9085677697691, 741.5810712938534, 653.6033684290695, 666.6706115959583, 664.1544603815851, 684.9764665067082, 661.1306455330065, 571.1111222821478, 690.8687745125347, 677.1420611963846, 689.8131157082286, 740.3465525777646, 514.1537704838009, 740.3658648749281, 667.5908359814058, 508.5582328558811, 733.2059851645238, 749.8111297839224, 717.0688388492644, 657.4780893928923, 496.23655345058575, 688.0437079021896, 681.9619240140694, 687.6194257658495, 693.3036101491261, 687.4184509792656, 710.677420679613, 498.2626142532415, 713.9179952257438, 720.0076461234452, 725.5765244883454, 725.4792081422041, 738.8488718588424, 515.9571753484903, 730.5774340264487, 683.6231461187132, 661.6788731674717, 476.54555794010724, 488.03399471331636, 656.6723910932479, 662.1479163792867, 679.3504623993844, 518.5874734445306, 481.1680130085623, 680.6226639425894, 678.4914584652546, 740.5397258483965, 689.8548777156601, 566.9387880008567, 726.32205034645, 776.710147810892, 775.0768698771767, 708.8737161348029, 780.0370675767293, 787.6512099723362, 608.834750197251, 587.2987226381995, 738.5976370158322, 744.8001386821481, 749.7242903767237, 755.5371898614823, 593.7973141423585, 770.8743248935297, 777.984828597285, 772.796761372139, 684.3259625773144, 593.3373679862061, 787.8140769785077, 789.4878369731277, 763.2811354587543, 787.1261348796318, 725.3995352008254, 617.6150522921804, 797.8781821211513, 739.258382553436, 722.2888922368929, 730.0695984328871, 732.4813185036999, 727.8055761473336, 806.034706943483, 738.1598289242492, 805.5292990201535, 596.0695907655428, 796.9865691663542, 801.4549963644141, 770.7486430263685, 771.9088008811083, 593.9220977903076, 781.9666849529485, 779.5387404373321, 488.47034250470927, 773.6074354169722, 755.5546625043519, 795.478100126967, 797.6087483965205, 777.7088085333232, 763.785511490376, 606.1480583938969, 723.5123049736451, 738.8836839550108, 743.1857229355843, 731.4090194868438, 571.4426380148424, 576.8940136174972, 759.3116582549544, 746.7251201862255, 764.5310308932951, 757.0637002525019, 677.6932262383332, 754.193599102393, 744.403975233073, 765.7983221525756, 783.5340763055293, 775.513829240257, 782.9960129908596, 787.9511675756991, 760.01512439402, 772.8069295309084, 684.7648851567142, 757.2033009378903, 778.8299469789938, 791.9920104892603, 687.2230689240359, 598.7092806120842, 739.5842971432978, 738.9626379306727, 797.9780825092686, 752.5188982983045, 754.3191754767607, 624.6327780557208, 740.0203942860192, 788.6208855664582, 562.2684355693794, 570.7620999792406, 571.498151302298, 748.7422385505761, 761.7815755861678, 764.8137853703993, 753.011782957198, 780.7528841768121, 719.6264628090676, 782.1911546804248, 741.6122116208178, 753.0312390531253, 750.303986438043, 766.1318804789156, 769.0528056750084, 765.9167295082101, 705.5921259672658, 741.5708274077595, 691.4831548642466, 555.9207043214627, 712.9373441145265, 740.5285080349598, 739.7436324860201, 733.842099006641, 647.5991614794502, 548.3905424387162, 720.9952471538553, 792.114656818481, 795.738274223224, 736.2319885517626, 587.9432576193058, 667.4189624647752, 771.0523107393926, 778.3305246429545, 773.2511686873336, 771.5433127884925, 727.3604487115695, 637.0157403679178, 746.1752984126715, 761.0660131475653, 798.5838362356108, 761.7139526986249, 656.2771434212495, 776.8314346250116, 799.7363170149107, 777.0921732235494, 505.34118417088825, 555.101817978658, 481.98636408671496, 791.9708416173963, 749.2144777282404, 637.1125580896514, 743.0840960403228, 773.3295842842859, 752.5565140632393, 738.7579492091415, 783.2089073003142, 776.5788338826829, 753.0072065938107, 711.1259180962145, 733.941726948823, 545.5837366902806, 741.1526299010982, 737.9731490822716, 731.2447859961642, 743.1571461008328, 591.2143373717307, 796.9439244974077, 797.4884242613394, 777.5268877346676, 801.2034575165496, 784.8141837171545, 584.2296956581026, 733.8314453594991, 743.0790439496768, 755.4119778483424, 736.4938393842792, 572.4924744517482, 750.0251095628095, 755.8631518097021, 755.9024153570533, 756.9730284212586, 797.5042457248128, 780.1944655019033, 816.7451651934866, 803.0539588839231, 814.6506315105651, 796.6556592426931, 603.0458166681323, 802.4505936620703, 762.5529809730458, 605.2117102937368, 611.292753668001, 769.1345761542701, 795.4720584166392, 786.8992296725195, 599.2802720461882, 593.842019506688, 787.7552502604974, 740.4856289085905, 690.9458189333034, 778.3992080667907, 802.2229090860686, 599.6181912652427, 807.9794840900895, 809.8513765098987, 796.3308906471734, 790.9309777337294, 794.2153136334952, 776.2049098183611, 791.7521294050939, 800.5491414707648, 800.5631319047558, 671.1460282287419, 793.1482507470205, 797.8882740977015, 599.2695260113013, 599.067780503455, 611.3616385970719, 749.6600729968509, 800.9190748765301, 752.2773008076161, 739.201029403358, 711.4682129691538, 717.6310765152235, 638.297554723196, 753.7632624741386, 728.6057529311947, 650.3013224190295, 461.87953423110224, 748.6117135958137, 738.4633040946509, 781.438525577113, 750.1977511111911, 603.8305019058546, 582.2885808550085, 634.7936820162786, 755.9622065906735, 754.9824543768344, 784.6789473729809, 662.552053097723, 775.4562048598017, 773.3683737522554, 780.7689722411055, 780.29134311259, 734.5856653666154, 573.2684329991719, 741.8810488211593, 736.4798507560231, 768.8134473376266, 772.2781633898128, 576.5594304783627, 787.4869596553863, 754.0699211254222, 755.0856332262525, 745.1791716189043, 714.9401718030597, 773.780337349599, 782.4758567339676, 774.8876730099205, 477.65765990687856, 690.9303218737036, 774.5885383237572, 757.7571511642748, 781.1646606800043, 765.1900645738424, 777.2825383734498, 773.5910239140256, 771.6668439523542, 718.3689397464183, 750.757615861453, 770.6998137662673, 771.1193489198552, 582.2496895662301, 729.6461000711593, 744.5709768460148, 740.3234943767709, 734.2272818284501, 635.4082316110413, 743.5061448628428, 735.7279804943023, 780.4980994735082, 783.0974728345042, 710.8474935313724, 599.2379923799991, 751.1623226057388, 760.6167031446371, 780.3149789866557, 779.6075764672221, 743.214017623848, 581.2572419292453, 756.2414233455643, 758.2878556991058, 746.9918130834851, 736.5590240453237, 587.8194957373829, 688.6820977302682, 761.6196439516038, 775.0083027752682, 772.4852674291967, 757.2159633629418, 688.5888576784375, 555.7809726088021, 712.9053513137725, 721.567505894068, 760.5558661603504, 758.9249561265947, 766.2033997240013, 760.5269731855246, 738.1847844598083, 726.1964177254202, 746.87241146898, 735.9060422450284, 740.0936890787166, 561.050374521479, 559.7231852596852, 721.9705510046189, 582.0162075004534, 587.9833855353313, 767.6246760377119, 822.4482845208021, 809.2630784491508, 512.2676473923565, 813.8658732787803, 782.4807982957901, 810.360324987035, 810.8778455349661, 798.1020280345634, 801.0625891260133, 738.2858846250231, 735.0076134859875, 753.9987792823713, 783.85502137385, 756.4498763250211, 785.2900930959381, 682.8953801143492, 770.5130660682213, 689.0219182666918, 816.980015530793, 744.4442770569541, 798.6902326051052, 754.8649987322246, 773.7168951757706, 777.0290775226864, 723.7419928774299, 686.1581205292022, 727.0936025408021, 773.6504421700096, 755.0963466693393, 744.6765504186376, 720.3904360973372, 722.7910901541236, 563.8169912987842, 654.3688637192381, 740.2067729669726, 737.6448569074412, 723.3060430728897, 741.590360012716, 724.0031785530293, 725.4450682638635, 715.2657201327297, 725.0291855786667, 722.8299277056884, 723.459303491595, 765.9823226386027, 789.7613367313479, 746.2771800189817, 803.5792165130098, 800.0110157885289, 611.2423288820486, 798.4191741847924, 786.7896061616755, 769.0661224937771, 764.3680516531427, 790.1032063471858, 781.1905573832947, 585.9897118711756, 760.9727653710461, 768.592837796552, 754.921570922678, 566.3185625134922, 552.95956150166, 792.6904963208152, 796.8511192883672, 762.1869343350327, 705.4414821787425, 785.3169147042186, 790.8738929848067, 784.8536150960992, 788.5741566548572, 784.0892020989561, 782.7547254573896, 755.5989515434946, 779.2969583483215, 785.9776385343066, 630.3171997868133, 798.5916189882007, 772.6992736099262, 769.4723304966154, 780.5633823400132, 768.7768539417483, 800.4039232815675, 797.5610929478255, 775.297345159079, 514.7799826541722, 789.0414369121189, 787.4710402268842, 555.374299643415, 680.6146445304609, 773.2676767139076, 778.1464481917442, 801.5579133377463, 816.1768909179236, 792.777853388221, 786.2589452141805, 801.0374742422367, 725.7767778157415, 792.9135778888042, 705.4374929149835, 790.6460639113378, 748.1965936141817, 805.5958156657244, 805.0418971925583, 795.6979949553237, 743.1784612275189, 752.9182214624165, 747.4118765246982, 794.605975791171, 800.2795821765783, 730.2651466448165, 582.3081410115489, 790.4420682407193, 610.1086593285049, 599.2407807925282, 599.0200245956165, 582.2358087871658, 797.6861416192544, 796.347631732891, 745.6245908168091, 778.0892072141896, 685.7672160328925, 751.9415714409246, 737.7963818856119, 778.0315925773709, 799.6669096178626, 807.1059624697891, 679.7852727351159, 762.7420125509144, 752.2299347102793, 791.7848075427041, 746.8547668483257, 735.1567404698103, 576.7588197613495, 804.7297795059185, 743.3520918820145]
Elapsed: 0.18751855115073252~0.027568940956336052
Time per graph: 0.001458590404640937~0.00021242428963713015
Speed: 697.7539959622266~84.37414571665474
Total Time: 0.1745
best val loss: 0.13581565823839153 test_score: 0.9453

Testing...
Test loss: 0.3217 score: 0.9141 time: 0.17s
test Score 0.9141
Epoch Time List: [0.6421636352315545, 0.6376376689877361, 0.6347821648232639, 0.6749577689915895, 0.7327770818956196, 0.6372906931210309, 0.6769602941349149, 0.60661904909648, 0.5986411243211478, 0.6284742660354823, 0.6696629300713539, 0.6127120780292898, 0.6684479489922523, 0.6243134569376707, 0.693208635551855, 0.7037727469578385, 0.6353238711599261, 0.6488673519343138, 0.6107555909547955, 0.6025967199821025, 0.6983441519550979, 0.6146188830025494, 0.6011120402254164, 0.6071335619781166, 0.6059105119202286, 0.5989531793165952, 0.8409325410611928, 0.713565019890666, 0.6341685769148171, 0.8835169360972941, 0.8895649330224842, 0.7792185200378299, 0.6271086784545332, 0.6218221788294613, 0.6353310409467667, 0.6802317290566862, 0.892087183194235, 0.8284281897358596, 0.6305841880384833, 0.635555423097685, 0.6342155693564564, 0.6360159728210419, 0.744021327001974, 0.6934842101763934, 0.6072305766865611, 0.7457802731078118, 0.824458017013967, 0.9846647218801081, 0.8253581968601793, 0.7113312359433621, 0.5997320858296007, 0.616832478903234, 0.6251789128873497, 0.6426537572406232, 0.6505569519940764, 0.7131162472069263, 0.6487868749536574, 0.7719614051748067, 0.8732066822703928, 0.8886971327010542, 0.8143330339808017, 0.6069000933784992, 0.5981512099970132, 0.7563648710492998, 0.8734207330271602, 0.8065842560026795, 0.6107647179160267, 0.6332573569379747, 0.6069363038986921, 0.6065964249428362, 0.6244919809978455, 0.8072861761320382, 0.712404313031584, 0.6117468120064586, 0.7904154108837247, 0.8816457388456911, 0.7004174720495939, 0.6056740330532193, 0.8107360580470413, 0.8839550609700382, 0.8767378102056682, 0.6105905210133642, 0.6260883372742683, 0.6401293405797333, 0.6511246038135141, 0.7983039440587163, 0.6529384281020612, 0.6109164271038026, 0.63776197982952, 0.6462766109034419, 0.6864231640938669, 0.791377189103514, 0.6068599859718233, 0.5876305608544499, 0.6532423959579319, 0.6535533717833459, 0.7644841279834509, 0.6533192088827491, 0.6216716549824923, 0.7609188209753484, 0.881825947901234, 0.6959277910646051, 0.5972939559724182, 0.610103587852791, 0.6089651598595083, 0.8113707010634243, 0.8289844247046858, 0.6205518732313067, 0.6100021318998188, 0.616467890329659, 0.7096995208412409, 0.7957345577888191, 0.6140806479379535, 0.5975700449198484, 0.5940972499083728, 0.619359944248572, 0.6591601499821991, 0.7354638478718698, 0.6317201310303062, 0.6426003510132432, 0.6536789073143154, 0.6007164991460741, 0.6411610869690776, 0.7649151298683137, 0.6361806911882013, 0.6434625496622175, 0.6031008092686534, 0.5937262333463877, 0.7226679380983114, 0.6102280712220818, 0.7195046218112111, 0.6330400458537042, 0.643298780079931, 0.7153286130633205, 0.6434841719456017, 0.6660190410912037, 0.918470935896039, 0.6436123729217798, 0.6381792107131332, 0.6172451789025217, 0.6298211188986897, 0.6583035299554467, 0.7815484099555761, 0.7784271908458322, 0.6397619438357651, 0.5986714821774513, 0.599218871910125, 0.6278898869641125, 0.6406251038424671, 0.7070430000312626, 0.6383038468193263, 0.6488507450558245, 0.867982380092144, 0.8712407480925322, 0.7726480490528047, 0.6024569345172495, 0.620835242094472, 0.6314942869357765, 0.8854031530208886, 0.6626819947268814, 0.6295180700253695, 0.5837912496645004, 0.6159224978182465, 0.6128300139680505, 0.7662526529747993, 0.6149343647994101, 0.6046041289810091, 0.7453113808296621, 0.8716567200608552, 0.7320824698545039, 0.6339042212348431, 0.8556400150991976, 0.8799091407563537, 0.8887908812612295, 0.6264352810103446, 0.6184740939643234, 0.6174445399083197, 0.6515414749737829, 0.6477790533099324, 0.6517657225485891, 0.6570326858200133, 0.7022780252154917, 0.6153155830688775, 0.6147483119275421, 0.6179750340525061, 0.7073602639138699, 0.8170477240346372, 0.6220514629967511, 0.6221255669370294, 0.625462899915874, 0.6479472320061177, 0.6768860009033233, 0.9082401869818568, 0.6743556940928102, 0.6336579909548163, 0.6081463401205838, 0.6095313041005284, 0.6108137618284672, 0.6065173901151866, 0.7303514040540904, 0.6265119460877031, 0.6173051937948912, 0.6153114777989686, 0.6228014396037906, 0.7070346218533814, 0.7600782469380647, 0.6176635320298374, 0.6171265051234514, 0.6388158521149307, 0.6612847382202744, 0.735343508888036, 0.6225111149251461, 0.6200548477936536, 0.6113127637654543, 0.7243289288599044, 0.8965882840566337, 0.7933253520168364, 0.7062912208493799, 0.627127654151991, 0.6389275647234172, 0.817690584808588, 0.9133732228074223, 0.7218783048447222, 0.6026122088078409, 0.6236832740250975, 0.632180365268141, 0.6475933478213847, 0.6551336266566068, 0.699165927246213, 0.6296772758942097, 0.6369424718432128, 0.7095465529710054, 0.6710772276856005, 0.6399033258203417, 0.633300710003823, 0.6053304660599679, 0.6477532910648733, 0.7370213600806892, 0.6456991520244628, 0.6478800640907139, 0.6421952550299466, 0.643898748094216, 0.7979205001611263, 0.6515111788176, 0.638760935747996, 0.7075647909659892, 0.9052087711170316, 0.9071920001879334, 0.7823877278715372, 0.6382705541327596, 0.6403593318536878, 0.6379118978511542, 0.6504314837511629, 0.8623707499355078, 0.8838891820050776, 0.6837741611525416, 0.610546737909317, 0.6106360929552466, 0.6122466975357383, 0.6467016497626901, 0.7335337570402771, 0.6567441779188812, 0.6344589707441628, 0.6342179132625461, 0.6195497100707144, 0.6321413870900869, 0.7209030594676733, 0.6105253060813993, 0.6007360338699073, 0.5989757759962231, 0.6202635189983994, 0.7677717788610607, 0.614027373958379, 0.6641626989003271, 0.652903103036806, 0.6334722677711397, 0.6923630479723215, 0.6017354163341224, 0.5996845038607717, 0.609424389898777, 0.6235906279180199, 0.8281758110970259, 0.6254277799744159, 0.6114401840604842, 0.6148912240751088, 0.6179018770344555, 0.7597660680767149, 0.861511467024684, 0.6271877037361264, 0.6106450462248176, 0.6031675203703344, 0.790647875983268, 0.772336991969496, 0.6140181317459792, 0.6092214912641793, 0.5954598218668252, 0.6557676987722516, 0.7910961098968983, 0.7418949559796602, 0.6033803378231823, 0.6577897630631924, 0.731669356347993, 0.7326224690768868, 0.7168897201772779, 0.6202310738153756, 0.7249983809888363, 0.8758149247150868, 0.8623441378585994, 0.7023510911967605, 0.6211418870370835, 0.607696128776297, 0.613464840920642, 0.653326909057796, 0.881981483893469, 0.6474748449400067, 0.6174328657798469, 0.6240841618273407, 0.6578048940282315, 0.664819624973461, 0.6559207960963249, 0.7854113108478487, 0.6560958288609982, 0.6531581918243319, 0.7624774931464344, 0.6445355881005526, 0.7116441926918924, 0.8106810201425105, 0.6213830208871514, 0.6264679150190204, 0.6160363319795579, 0.6329141960013658, 0.762531611835584, 0.6367355030961335, 0.6157180438749492, 0.6181100157555193, 0.6211737198755145, 0.657455347944051, 0.6949706629384309, 0.6273466511629522, 0.621135972905904, 0.6218936368823051, 0.6244249290321022, 0.6230098227970302, 0.793878176016733, 0.6300864333752543, 0.6212294430006295, 0.6213507303036749, 0.6084160758182406, 0.6494168420322239, 0.6913713470567018, 0.6153029780834913, 0.631358589977026, 0.7003013798967004, 0.6176022179424763, 0.6718774633482099, 0.6230390288401395, 0.609550196910277, 0.6080111127812415, 0.6497728822287172, 0.7331118730362505, 0.649261939805001, 0.6455104053020477, 0.6016080228146166, 0.5904790747445077, 0.614355708938092, 0.6840785648673773, 0.6043071299791336, 0.6085275469813496, 0.6062554472591728, 0.6070867110975087, 0.7051566138397902, 0.6484022592194378, 0.644408794818446, 0.8012531637214124, 0.902599633904174, 0.7746701380237937, 0.5936298479791731, 0.6012563921976835, 0.6110786660574377, 0.611807671142742, 0.7422640591394156, 0.6334115588106215, 0.6021480788476765, 0.633173723006621, 0.7656571571715176, 0.8585104888770729, 0.9223442571237683, 0.6205378449521959, 0.6288092930335552, 0.6461293592583388, 0.6317391486372799, 0.6271535980049521, 0.7047562387306243, 0.6864501661621034, 0.6123544361907989, 0.6014785028528422, 0.6121442893054336, 0.6265811019111425, 0.6475774112623185, 0.7009611318353564, 0.6299060757737607, 0.6281356308609247, 0.6462750760838389, 0.6548851591069251, 0.704154907958582, 0.6534252448473126, 0.6439691081177443, 0.649118437198922, 0.6432396860327572, 0.7548822939861566, 0.6440899940207601, 0.6422222349792719, 0.6433351589366794, 0.6262167587410659, 0.645479897968471, 0.7587968641892076, 0.6309614046476781, 0.619128031656146, 0.6258940978441387, 0.6100664916448295, 0.7040984090417624, 0.636136335786432, 0.6108980858698487, 0.6042089003603905, 0.6118947709910572, 0.6207397419493645, 0.6232438979204744, 0.7124826097860932, 0.6790870719123632, 0.6324978319462389, 0.6148596310522407, 0.6181039540097117, 0.7125785169191658, 0.8110160927753896, 0.6185789287555963, 0.6176790660247207, 0.6614403710700572, 0.6358418648596853, 0.6488327572587878, 0.6665504991542548, 0.7127905550878495, 0.6436773319728673, 0.637819031951949, 0.6358656480442733, 0.6346185761503875, 0.6691246849950403, 0.6869759082328528, 0.5952736649196595, 0.5936888579744846, 0.5989572440739721, 0.6051724350545555, 0.6350987567566335, 0.7223053041379899, 0.6289139459840953, 0.6138065848499537, 0.6149815092794597, 0.6387717181351036, 0.6876337539870292, 0.6300305081531405, 0.5972206501755863, 0.6194997399579734, 0.6194886022713035, 0.6780743482522666, 0.6007922110147774, 0.607094454113394, 0.6030991331208497, 0.6484303688630462, 0.669587975833565, 0.7120329581666738, 0.6427611319813877, 0.6542494290042669, 0.6861883110832423, 0.70998086896725, 0.6509015080519021, 0.6459413538686931, 0.6415619209874421, 0.687870895024389, 0.615359028801322, 0.6691649237181991, 0.8567845290526748, 0.7315538208931684, 0.600164548959583, 0.6145262520294636, 0.7352747691329569, 0.8466746367048472, 0.6535925508942455, 0.6483875911217183, 0.6367520499043167, 0.646316773025319, 0.6493179539684206, 0.6178182500880212, 0.7021832410246134, 0.6063761338591576, 0.6222410111222416, 0.6000199681147933, 0.6044941910076886, 0.6068474899511784, 0.7977545778267086, 0.6850984757766128, 0.6330660067033023, 0.6558946487493813, 0.8588135219179094, 0.902905142866075, 0.7266943610738963, 0.6453648721799254, 0.6460720137692988, 0.715919905109331, 0.9086252423003316, 0.685586160980165, 0.642233096063137, 0.598607970867306, 0.6206793352030218, 0.7008287268690765, 0.664042733842507, 0.6244795080274343, 0.623355177231133, 0.7217722190544009, 0.6255104537121952, 0.6293828743509948, 0.6726594199426472, 0.9606431787833571, 0.7411989718675613, 0.6336256749927998, 0.6324791193474084, 0.6330515760928392, 0.8477036729454994, 0.7613902306184173, 0.6311679307837039, 0.6349467979744077, 0.6517317818943411, 0.9060183779802173, 0.8593846149742603, 0.6191147500649095, 0.6191403383854777, 0.6087591992691159, 0.6305544280912727, 0.7551561812870204, 0.7036447888240218, 0.6322199425194412, 0.660197711084038, 0.6516636067535728, 0.6698059500195086, 0.7327253839466721, 0.6568653390277177, 0.6419463520869613, 0.6421870538033545, 0.7021098250988871, 0.630774844205007, 0.6183384670875967, 0.6272931003477424, 0.6351111568510532, 0.9366177578922361, 0.822868671733886, 0.6300593549385667, 0.7199615007266402, 0.623714952962473, 0.6494133700616658, 0.7051474589388818, 0.6260885931551456, 0.6235646053683013, 0.6637742058373988, 0.9248062069527805, 0.9118070260155946, 0.6538921487517655, 0.6551701831631362, 0.6452271388843656, 0.8244635879527777, 0.9599852552637458, 0.6529281965922564, 0.6468716440722346, 0.6313831428997219, 0.6295948619954288, 0.6497673252597451, 0.8142724933568388, 0.6385012846440077, 0.6341381310485303, 0.6197229900863022, 0.646873323014006, 0.6921342560090125, 0.6245636912062764, 0.6267358660697937, 0.6258280579932034, 0.6515242252498865, 0.7534102669451386, 0.6229492081329226, 0.6190687930211425, 0.6365966442972422, 0.8955438311677426, 0.8580129221081734, 0.6571001610718668, 0.6223380868323147, 0.6270434451289475, 0.6410026650410146, 0.7216662669088691, 0.6918092689011246, 0.613945510936901, 0.6999618781264871, 0.9670576239004731, 0.9703757101669908, 0.7442471059039235, 0.6286068540066481, 0.6214435319416225, 0.6277233592700213, 0.6194018521346152, 0.6444012708961964, 0.7025322979316115, 0.6420934170018882, 0.651812834199518, 0.6586327268742025, 0.617346506100148, 0.6860833538230509, 0.7164858521427959, 0.6456241628620774, 0.6523214329499751, 0.666767009999603, 0.882738197920844, 0.8647894538007677, 0.6654647400137037, 0.6470044660381973, 0.6568453111685812, 0.6848585740663111, 0.7171362307853997, 0.6563325140159577, 0.6111448830924928, 0.62040173728019, 0.651017750846222, 0.7508644841145724, 0.9419003122020513, 0.7079342182260007, 0.6359059831593186, 0.6408873689360917, 0.6392792360857129, 0.6423504119738936, 0.7602632639463991, 0.6971194921061397, 0.6543769428972155, 0.6108843530528247, 0.6281499187462032, 0.6965683519374579, 0.9131752946414053, 0.6743307821452618, 0.6200979452114552, 0.7270696386694908, 0.9365598931908607, 0.7456370489671826, 0.6159007479436696, 0.6415577768348157, 0.7952102138660848, 0.6325216391123831, 0.631996275857091, 0.6334359648171812, 0.641665630042553, 0.6882841931656003, 0.637457488104701, 0.626494619762525, 0.6466793180443347, 0.6761620522011071, 0.9838705770671368, 0.8531537428498268, 0.6578396467957646, 0.6596772109623998, 0.659660177771002, 0.7121435231529176, 0.6067433152347803, 0.6082237688824534, 0.6240704793017358, 0.6245917461346835, 0.6114523378200829, 0.6954285418614745, 0.6693672540131956, 0.6632382278330624, 0.6611566261854023, 0.6628430809359998, 0.7363546211272478, 0.6448101201094687, 0.6474754698574543, 0.6462425759527832, 0.6349548592697829, 0.7049219638574868, 0.6181477007921785, 0.60357736190781, 0.613982770126313, 0.6165177971124649, 0.6677885658573359, 0.6715783211402595, 0.6241786202881485, 0.6122721598949283, 0.7384431918617338, 0.931164211826399, 0.8134109259117395, 0.6089842510409653, 0.6270187529735267, 0.9410143806599081, 0.9499796719755977, 0.812205953290686, 0.6361272148787975, 0.6596068118233234, 0.6519288211129606, 0.6108282529748976, 0.9022510151844472, 0.7856315451208502, 0.6057880129665136, 0.598160971654579, 0.6149830371141434, 0.6174890759866685, 0.6484557059593499, 0.7232292818371207, 0.6228403600398451, 0.6167925682384521, 0.6653604477178305, 0.652930773794651, 0.6102765230461955, 0.8900532650295645, 0.948269338812679, 0.9416759090963751, 0.7990639861673117, 0.6152085198555142, 0.6416411509271711, 0.6601601939182729, 0.6787915278691798, 0.743404685985297, 0.6755052858497947, 0.6386621799319983, 0.6411608210764825, 0.6465456967707723, 0.8274481208063662, 0.6615583910606802, 0.662290196865797, 0.6861348757520318, 0.6246653238777071, 0.8144479719921947, 0.9769311991985887, 0.7046447179745883, 0.6305029231589288, 0.6475506939459592, 0.6438182992860675, 0.7222794129047543, 0.686544548952952, 0.6368720671162009, 0.6419487176463008, 0.6395577099174261, 0.6683358747977763, 0.7238855923060328, 0.6658948478288949, 0.6541457569692284, 0.6439135270193219, 0.6592874492052943, 0.9750352308619767, 0.7422645972110331, 0.6365368580445647, 0.6345343559514731, 0.6590971928089857, 0.6814257020596415, 0.7180988788604736, 0.6410016610752791, 0.6345093627460301, 0.7322734582703561, 0.6479563917964697, 0.685029405169189, 0.6401873449794948, 0.6295762709341943, 0.6248094881884754, 0.6358604240231216, 0.7077852718066424, 0.6256071471143514, 0.6499350187368691, 0.6355980508960783, 0.6298783188685775, 0.649544222978875, 0.7334821738768369, 0.6481288750655949, 0.653400442795828, 0.6563496701419353, 0.6539159771054983, 0.7468570170458406, 0.6843711438123137, 0.6590399218257517, 0.6191673569846898, 0.6298113651573658, 0.6646026740781963, 0.93907400383614, 0.9113165382295847, 0.6281179313082248, 0.637474067742005, 0.6322271139360964, 0.6566701501142234, 0.7203469180967659, 0.6502365425694734, 0.6355070029385388, 0.6399888941086829, 0.6593615342862904, 0.9639342508744448, 0.8743638207670301, 0.6614739319775254, 0.6321664089336991, 0.6369004028383642, 0.6460095341317356, 0.6545315911062062, 0.7216029602568597, 0.6978158948477358, 0.6649680000264198, 0.6445156920235604, 0.6520332719665021, 0.6454942722339183, 0.7041996384505183, 0.8575837740208954, 0.6460278320591897, 0.6450897280592471, 0.6601703248452395, 0.6594590391032398, 0.7345221568830311, 0.7177228939253837, 0.6554075798485428, 0.7715314067900181, 0.9667448413092643, 0.793569462839514, 0.6272731889039278, 0.6006632628850639, 0.6989767469931394, 0.5940104580949992, 0.6333295628428459, 0.6915721658151597, 0.6049983769189566, 0.6138771339319646, 0.6227996698580682, 0.6263806188944727, 0.6829141641501337, 0.7675866917707026, 0.6297215849626809, 0.6312720249406993, 0.6184603637084365, 0.6564253401011229, 0.7315538499969989, 0.6669804949779063, 0.6039495377335697, 0.6183707299642265, 0.6123265749774873, 0.7328904357273132, 0.6271289261057973, 0.6147174788638949, 0.6530419841874391, 0.6562246079556644, 0.6712897717952728, 0.6584906191565096, 0.705963077954948, 0.6409005450550467, 0.6452458577696234, 0.6573794048745185, 0.7273136109579355, 0.9400416249409318, 0.6541412689257413, 0.6475835177116096, 0.6486559889744967, 0.6577832270413637, 0.6699911700561643, 0.7271786918863654, 0.66645557875745, 0.6618579272180796, 0.6636029412038624, 0.6597904129885137, 0.6436278803739697, 0.761700470931828, 0.6297441069036722, 0.6144100348465145, 0.6184136820957065, 0.6919560630340129, 0.7644760620314628, 0.6279556278605014, 0.6171622998081148, 0.6256102523766458, 0.6171838648151606, 0.6299662708770484, 0.7361535748932511, 0.6398982261307538, 0.6313327120151371, 0.645610534818843, 0.7490094709210098, 0.9962571479845792, 0.6436669661197811, 0.6210481440648437, 0.6168388172518462, 0.6432691582012922, 0.7432414977811277, 0.6232125868555158, 0.6175639918074012, 0.6151223788037896, 0.6200849390588701, 0.6303709528874606, 0.6663021261338145, 0.7232537392992526, 0.6237355452030897, 0.7035634657368064, 0.6141984798014164, 0.6138924530241638, 0.6187721488531679, 0.6880439012311399, 0.6303670869674534, 0.6236632929649204, 0.6223389271181077, 0.6224832532461733, 0.7060220220591873, 0.6884594922885299, 0.6254653225187212, 0.8245078842155635, 0.7643314620945603, 0.7146573108620942, 0.6207243183162063, 0.6251531657762825, 0.723774828016758, 0.6632238887250423, 0.7101498718839139, 0.6145482216961682, 0.6636962199117988, 0.6134856210555881, 0.650426106993109, 0.6839029390830547, 0.6441481721121818, 0.6679132680874318, 0.6180815799161792, 0.6096703549847007, 0.6688184670638293, 0.7314795528072864, 0.6349288220517337, 0.6219364739954472, 0.6124513440299779, 0.6183885450009257, 0.6782536031678319, 0.6195717367809266, 0.6652815067209303, 0.9395792069844902, 0.9506186691578478, 0.9456464529503137, 0.6159475198946893, 0.6235511000268161, 0.6358251643832773, 0.6153536490164697, 0.6390531475190073, 0.7715028689708561, 0.6374184100423008, 0.6691564521752298, 0.605585953919217, 0.6121013830415905, 0.7175623094663024, 0.7006712921429425, 0.617615298833698, 0.6088147179689258, 0.6304358961060643, 0.6580935348756611, 0.9040135107934475, 0.6234235719311982, 0.6391623769886792]
Total Epoch List: [255, 257, 439]
Total Time List: [0.25019272416830063, 0.18831530096940696, 0.17446499597281218]
T-times Epoch Time: 0.6818212191846765 ~ 0.004904265124994335
T-times Total Epoch: 280.77777777777777 ~ 28.752240921790627
T-times Total Time: 0.1922664863264395 ~ 0.008577750247208088
T-times Inference Elapsed: 0.18846798506144835 ~ 0.0008281074896596514
T-times Time Per Graph: 0.001465151218768206 ~ 5.671254692715266e-06
T-times Speed: 694.3105671916173 ~ 3.3846895045090304
T-times cross validation test micro f1 score:0.9194204696537581 ~ 0.0032031283920500773
T-times cross validation test precision:0.9661067416571694 ~ 0.007268163761373481
T-times cross validation test recall:0.8773771367521368 ~ 0.004856559576686803
T-times cross validation test f1_score:0.9194204696537581 ~ 0.002917464828403271
