Namespace(seed=15, model='AEtransGAT', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a331bfc40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.1044;  Loss pred: 3.1044; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.20s
Epoch 2/1000, LR 0.000015
Train loss: 3.1110;  Loss pred: 3.1110; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 3.0888;  Loss pred: 3.0888; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 3.0533;  Loss pred: 3.0533; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 2.9747;  Loss pred: 2.9747; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 2.9304;  Loss pred: 2.9304; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.19s
Epoch 7/1000, LR 0.000165
Train loss: 2.8790;  Loss pred: 2.8790; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 2.8053;  Loss pred: 2.8053; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 2.6875;  Loss pred: 2.6875; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 2.6158;  Loss pred: 2.6158; Loss self: 0.0000; time: 0.26s
Val loss: 0.6930 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 2.5248;  Loss pred: 2.5248; Loss self: 0.0000; time: 0.26s
Val loss: 0.6929 score: 0.5349 time: 0.18s
Test loss: 0.6927 score: 0.5271 time: 0.23s
Epoch 12/1000, LR 0.000285
Train loss: 2.4212;  Loss pred: 2.4212; Loss self: 0.0000; time: 0.36s
Val loss: 0.6929 score: 0.6124 time: 0.25s
Test loss: 0.6926 score: 0.6589 time: 0.24s
Epoch 13/1000, LR 0.000285
Train loss: 2.3353;  Loss pred: 2.3353; Loss self: 0.0000; time: 0.25s
Val loss: 0.6928 score: 0.4884 time: 0.16s
Test loss: 0.6925 score: 0.5736 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 2.2457;  Loss pred: 2.2457; Loss self: 0.0000; time: 0.25s
Val loss: 0.6928 score: 0.4961 time: 0.17s
Test loss: 0.6925 score: 0.5736 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 2.1475;  Loss pred: 2.1475; Loss self: 0.0000; time: 0.25s
Val loss: 0.6927 score: 0.5039 time: 0.17s
Test loss: 0.6924 score: 0.5504 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 2.0825;  Loss pred: 2.0825; Loss self: 0.0000; time: 0.25s
Val loss: 0.6926 score: 0.5116 time: 0.16s
Test loss: 0.6922 score: 0.5504 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 2.0033;  Loss pred: 2.0033; Loss self: 0.0000; time: 0.26s
Val loss: 0.6925 score: 0.5116 time: 0.17s
Test loss: 0.6921 score: 0.5504 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 1.9484;  Loss pred: 1.9484; Loss self: 0.0000; time: 0.28s
Val loss: 0.6924 score: 0.5116 time: 0.21s
Test loss: 0.6919 score: 0.5504 time: 0.26s
Epoch 19/1000, LR 0.000285
Train loss: 1.8820;  Loss pred: 1.8820; Loss self: 0.0000; time: 0.38s
Val loss: 0.6923 score: 0.5194 time: 0.17s
Test loss: 0.6918 score: 0.5504 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 1.8130;  Loss pred: 1.8130; Loss self: 0.0000; time: 0.26s
Val loss: 0.6922 score: 0.5039 time: 0.18s
Test loss: 0.6917 score: 0.5736 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 1.7688;  Loss pred: 1.7688; Loss self: 0.0000; time: 0.30s
Val loss: 0.6920 score: 0.5039 time: 0.18s
Test loss: 0.6915 score: 0.5814 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 1.6863;  Loss pred: 1.6863; Loss self: 0.0000; time: 0.26s
Val loss: 0.6918 score: 0.5271 time: 0.17s
Test loss: 0.6913 score: 0.6047 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 1.6547;  Loss pred: 1.6547; Loss self: 0.0000; time: 0.27s
Val loss: 0.6917 score: 0.6202 time: 0.17s
Test loss: 0.6912 score: 0.7054 time: 0.28s
Epoch 24/1000, LR 0.000285
Train loss: 1.6007;  Loss pred: 1.6007; Loss self: 0.0000; time: 0.29s
Val loss: 0.6915 score: 0.8605 time: 0.18s
Test loss: 0.6910 score: 0.9070 time: 0.22s
Epoch 25/1000, LR 0.000285
Train loss: 1.5548;  Loss pred: 1.5548; Loss self: 0.0000; time: 0.31s
Val loss: 0.6913 score: 0.9070 time: 0.18s
Test loss: 0.6908 score: 0.9302 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.5204;  Loss pred: 1.5204; Loss self: 0.0000; time: 0.31s
Val loss: 0.6910 score: 0.8915 time: 0.17s
Test loss: 0.6904 score: 0.9302 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.4823;  Loss pred: 1.4823; Loss self: 0.0000; time: 0.26s
Val loss: 0.6907 score: 0.8837 time: 0.19s
Test loss: 0.6901 score: 0.9225 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 1.4445;  Loss pred: 1.4445; Loss self: 0.0000; time: 0.29s
Val loss: 0.6905 score: 0.8915 time: 0.18s
Test loss: 0.6899 score: 0.9225 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 1.4166;  Loss pred: 1.4166; Loss self: 0.0000; time: 0.38s
Val loss: 0.6902 score: 0.8837 time: 0.41s
Test loss: 0.6896 score: 0.9147 time: 0.22s
Epoch 30/1000, LR 0.000285
Train loss: 1.3754;  Loss pred: 1.3754; Loss self: 0.0000; time: 0.37s
Val loss: 0.6900 score: 0.8605 time: 0.20s
Test loss: 0.6893 score: 0.9070 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 1.3587;  Loss pred: 1.3587; Loss self: 0.0000; time: 0.29s
Val loss: 0.6897 score: 0.7907 time: 0.19s
Test loss: 0.6889 score: 0.8915 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 1.3348;  Loss pred: 1.3348; Loss self: 0.0000; time: 0.28s
Val loss: 0.6894 score: 0.7907 time: 0.19s
Test loss: 0.6886 score: 0.8760 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 1.3036;  Loss pred: 1.3036; Loss self: 0.0000; time: 0.28s
Val loss: 0.6890 score: 0.7984 time: 0.19s
Test loss: 0.6882 score: 0.8915 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 1.2756;  Loss pred: 1.2756; Loss self: 0.0000; time: 0.28s
Val loss: 0.6887 score: 0.8605 time: 0.18s
Test loss: 0.6878 score: 0.9070 time: 0.20s
Epoch 35/1000, LR 0.000285
Train loss: 1.2527;  Loss pred: 1.2527; Loss self: 0.0000; time: 0.28s
Val loss: 0.6884 score: 0.8760 time: 0.19s
Test loss: 0.6873 score: 0.9147 time: 0.23s
Epoch 36/1000, LR 0.000285
Train loss: 1.2412;  Loss pred: 1.2412; Loss self: 0.0000; time: 0.29s
Val loss: 0.6880 score: 0.8682 time: 0.26s
Test loss: 0.6869 score: 0.9070 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 1.2239;  Loss pred: 1.2239; Loss self: 0.0000; time: 0.27s
Val loss: 0.6876 score: 0.8682 time: 0.18s
Test loss: 0.6864 score: 0.9070 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 1.2015;  Loss pred: 1.2015; Loss self: 0.0000; time: 0.27s
Val loss: 0.6871 score: 0.8682 time: 0.17s
Test loss: 0.6859 score: 0.9070 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 1.1857;  Loss pred: 1.1857; Loss self: 0.0000; time: 0.27s
Val loss: 0.6866 score: 0.8295 time: 0.18s
Test loss: 0.6853 score: 0.8992 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 1.1667;  Loss pred: 1.1667; Loss self: 0.0000; time: 0.39s
Val loss: 0.6861 score: 0.8140 time: 0.26s
Test loss: 0.6847 score: 0.8992 time: 0.25s
Epoch 41/1000, LR 0.000284
Train loss: 1.1590;  Loss pred: 1.1590; Loss self: 0.0000; time: 0.39s
Val loss: 0.6855 score: 0.8062 time: 0.26s
Test loss: 0.6840 score: 0.8605 time: 0.25s
Epoch 42/1000, LR 0.000284
Train loss: 1.1429;  Loss pred: 1.1429; Loss self: 0.0000; time: 0.41s
Val loss: 0.6850 score: 0.7829 time: 0.18s
Test loss: 0.6834 score: 0.8527 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.1369;  Loss pred: 1.1369; Loss self: 0.0000; time: 0.27s
Val loss: 0.6844 score: 0.7674 time: 0.18s
Test loss: 0.6827 score: 0.8527 time: 0.19s
Epoch 44/1000, LR 0.000284
Train loss: 1.1227;  Loss pred: 1.1227; Loss self: 0.0000; time: 0.27s
Val loss: 0.6837 score: 0.7984 time: 0.17s
Test loss: 0.6819 score: 0.8760 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.1138;  Loss pred: 1.1138; Loss self: 0.0000; time: 0.26s
Val loss: 0.6831 score: 0.8062 time: 0.17s
Test loss: 0.6811 score: 0.8837 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 1.1036;  Loss pred: 1.1036; Loss self: 0.0000; time: 0.27s
Val loss: 0.6824 score: 0.8062 time: 0.18s
Test loss: 0.6803 score: 0.8915 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0988;  Loss pred: 1.0988; Loss self: 0.0000; time: 0.27s
Val loss: 0.6816 score: 0.8062 time: 0.17s
Test loss: 0.6794 score: 0.8915 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 1.0864;  Loss pred: 1.0864; Loss self: 0.0000; time: 0.27s
Val loss: 0.6808 score: 0.7984 time: 0.18s
Test loss: 0.6785 score: 0.8915 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 1.0787;  Loss pred: 1.0787; Loss self: 0.0000; time: 0.34s
Val loss: 0.6800 score: 0.7984 time: 0.17s
Test loss: 0.6775 score: 0.8915 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.0708;  Loss pred: 1.0708; Loss self: 0.0000; time: 0.27s
Val loss: 0.6791 score: 0.8140 time: 0.20s
Test loss: 0.6765 score: 0.8915 time: 0.22s
Epoch 51/1000, LR 0.000284
Train loss: 1.0656;  Loss pred: 1.0656; Loss self: 0.0000; time: 0.27s
Val loss: 0.6782 score: 0.8217 time: 0.17s
Test loss: 0.6754 score: 0.8915 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 1.0563;  Loss pred: 1.0563; Loss self: 0.0000; time: 0.27s
Val loss: 0.6772 score: 0.8372 time: 0.17s
Test loss: 0.6743 score: 0.8992 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 1.0533;  Loss pred: 1.0533; Loss self: 0.0000; time: 0.27s
Val loss: 0.6762 score: 0.8605 time: 0.17s
Test loss: 0.6730 score: 0.8992 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 1.0473;  Loss pred: 1.0473; Loss self: 0.0000; time: 0.26s
Val loss: 0.6751 score: 0.8682 time: 0.22s
Test loss: 0.6717 score: 0.9070 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 1.0417;  Loss pred: 1.0417; Loss self: 0.0000; time: 0.32s
Val loss: 0.6740 score: 0.8682 time: 0.25s
Test loss: 0.6704 score: 0.8992 time: 0.24s
Epoch 56/1000, LR 0.000284
Train loss: 1.0366;  Loss pred: 1.0366; Loss self: 0.0000; time: 0.35s
Val loss: 0.6728 score: 0.8682 time: 0.17s
Test loss: 0.6690 score: 0.8992 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 1.0331;  Loss pred: 1.0331; Loss self: 0.0000; time: 0.26s
Val loss: 0.6716 score: 0.8682 time: 0.17s
Test loss: 0.6676 score: 0.8992 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 1.0271;  Loss pred: 1.0271; Loss self: 0.0000; time: 0.27s
Val loss: 0.6703 score: 0.8682 time: 0.16s
Test loss: 0.6660 score: 0.8992 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 1.0246;  Loss pred: 1.0246; Loss self: 0.0000; time: 0.27s
Val loss: 0.6689 score: 0.8682 time: 0.17s
Test loss: 0.6644 score: 0.8992 time: 0.17s
Epoch 60/1000, LR 0.000283
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.26s
Val loss: 0.6674 score: 0.8682 time: 0.17s
Test loss: 0.6627 score: 0.9070 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 1.0133;  Loss pred: 1.0133; Loss self: 0.0000; time: 0.27s
Val loss: 0.6659 score: 0.8605 time: 0.22s
Test loss: 0.6609 score: 0.9070 time: 0.25s
Epoch 62/1000, LR 0.000283
Train loss: 1.0103;  Loss pred: 1.0103; Loss self: 0.0000; time: 0.38s
Val loss: 0.6644 score: 0.8682 time: 0.25s
Test loss: 0.6591 score: 0.8992 time: 0.23s
Epoch 63/1000, LR 0.000283
Train loss: 1.0049;  Loss pred: 1.0049; Loss self: 0.0000; time: 0.28s
Val loss: 0.6627 score: 0.8605 time: 0.17s
Test loss: 0.6571 score: 0.8992 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 1.0024;  Loss pred: 1.0024; Loss self: 0.0000; time: 0.27s
Val loss: 0.6609 score: 0.8605 time: 0.18s
Test loss: 0.6550 score: 0.9147 time: 0.16s
Epoch 65/1000, LR 0.000283
Train loss: 0.9978;  Loss pred: 0.9978; Loss self: 0.0000; time: 0.27s
Val loss: 0.6591 score: 0.8605 time: 0.17s
Test loss: 0.6528 score: 0.9147 time: 0.17s
Epoch 66/1000, LR 0.000283
Train loss: 0.9957;  Loss pred: 0.9957; Loss self: 0.0000; time: 0.26s
Val loss: 0.6572 score: 0.8605 time: 0.17s
Test loss: 0.6506 score: 0.9147 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 0.9923;  Loss pred: 0.9923; Loss self: 0.0000; time: 0.27s
Val loss: 0.6552 score: 0.8605 time: 0.18s
Test loss: 0.6483 score: 0.9147 time: 0.18s
Epoch 68/1000, LR 0.000283
Train loss: 0.9866;  Loss pred: 0.9866; Loss self: 0.0000; time: 0.27s
Val loss: 0.6532 score: 0.8605 time: 0.17s
Test loss: 0.6459 score: 0.9070 time: 0.19s
Epoch 69/1000, LR 0.000283
Train loss: 0.9830;  Loss pred: 0.9830; Loss self: 0.0000; time: 0.27s
Val loss: 0.6510 score: 0.8605 time: 0.18s
Test loss: 0.6433 score: 0.9070 time: 0.22s
Epoch 70/1000, LR 0.000283
Train loss: 0.9775;  Loss pred: 0.9775; Loss self: 0.0000; time: 0.27s
Val loss: 0.6486 score: 0.8605 time: 0.18s
Test loss: 0.6405 score: 0.9147 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9755;  Loss pred: 0.9755; Loss self: 0.0000; time: 0.27s
Val loss: 0.6461 score: 0.8605 time: 0.18s
Test loss: 0.6375 score: 0.9147 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9720;  Loss pred: 0.9720; Loss self: 0.0000; time: 0.28s
Val loss: 0.6433 score: 0.8682 time: 0.18s
Test loss: 0.6343 score: 0.9147 time: 0.25s
Epoch 73/1000, LR 0.000282
Train loss: 0.9676;  Loss pred: 0.9676; Loss self: 0.0000; time: 0.39s
Val loss: 0.6405 score: 0.8682 time: 0.25s
Test loss: 0.6310 score: 0.9147 time: 0.26s
Epoch 74/1000, LR 0.000282
Train loss: 0.9634;  Loss pred: 0.9634; Loss self: 0.0000; time: 0.39s
Val loss: 0.6375 score: 0.8682 time: 0.24s
Test loss: 0.6277 score: 0.9147 time: 0.18s
Epoch 75/1000, LR 0.000282
Train loss: 0.9607;  Loss pred: 0.9607; Loss self: 0.0000; time: 0.27s
Val loss: 0.6345 score: 0.8605 time: 0.18s
Test loss: 0.6242 score: 0.9147 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9578;  Loss pred: 0.9578; Loss self: 0.0000; time: 0.27s
Val loss: 0.6314 score: 0.8682 time: 0.17s
Test loss: 0.6205 score: 0.9147 time: 0.18s
Epoch 77/1000, LR 0.000282
Train loss: 0.9549;  Loss pred: 0.9549; Loss self: 0.0000; time: 0.29s
Val loss: 0.6283 score: 0.8682 time: 0.26s
Test loss: 0.6169 score: 0.9147 time: 0.25s
Epoch 78/1000, LR 0.000282
Train loss: 0.9498;  Loss pred: 0.9498; Loss self: 0.0000; time: 0.38s
Val loss: 0.6251 score: 0.8682 time: 0.22s
Test loss: 0.6131 score: 0.9147 time: 0.18s
Epoch 79/1000, LR 0.000282
Train loss: 0.9459;  Loss pred: 0.9459; Loss self: 0.0000; time: 0.27s
Val loss: 0.6218 score: 0.8682 time: 0.18s
Test loss: 0.6092 score: 0.9147 time: 0.18s
Epoch 80/1000, LR 0.000282
Train loss: 0.9428;  Loss pred: 0.9428; Loss self: 0.0000; time: 0.27s
Val loss: 0.6184 score: 0.8682 time: 0.18s
Test loss: 0.6053 score: 0.9147 time: 0.24s
Epoch 81/1000, LR 0.000281
Train loss: 0.9399;  Loss pred: 0.9399; Loss self: 0.0000; time: 0.39s
Val loss: 0.6150 score: 0.8682 time: 0.26s
Test loss: 0.6014 score: 0.9147 time: 0.25s
Epoch 82/1000, LR 0.000281
Train loss: 0.9364;  Loss pred: 0.9364; Loss self: 0.0000; time: 0.41s
Val loss: 0.6114 score: 0.8682 time: 0.25s
Test loss: 0.5972 score: 0.9147 time: 0.27s
Epoch 83/1000, LR 0.000281
Train loss: 0.9312;  Loss pred: 0.9312; Loss self: 0.0000; time: 0.36s
Val loss: 0.6079 score: 0.8605 time: 0.17s
Test loss: 0.5931 score: 0.9070 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9286;  Loss pred: 0.9286; Loss self: 0.0000; time: 0.27s
Val loss: 0.6041 score: 0.8605 time: 0.17s
Test loss: 0.5888 score: 0.9070 time: 0.16s
Epoch 85/1000, LR 0.000281
Train loss: 0.9241;  Loss pred: 0.9241; Loss self: 0.0000; time: 0.27s
Val loss: 0.6003 score: 0.8605 time: 0.18s
Test loss: 0.5843 score: 0.9070 time: 0.18s
Epoch 86/1000, LR 0.000281
Train loss: 0.9203;  Loss pred: 0.9203; Loss self: 0.0000; time: 0.27s
Val loss: 0.5963 score: 0.8605 time: 0.18s
Test loss: 0.5796 score: 0.9070 time: 0.19s
Epoch 87/1000, LR 0.000281
Train loss: 0.9158;  Loss pred: 0.9158; Loss self: 0.0000; time: 0.28s
Val loss: 0.5922 score: 0.8682 time: 0.19s
Test loss: 0.5749 score: 0.9070 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.9127;  Loss pred: 0.9127; Loss self: 0.0000; time: 0.30s
Val loss: 0.5882 score: 0.8605 time: 0.19s
Test loss: 0.5703 score: 0.9070 time: 0.19s
Epoch 89/1000, LR 0.000281
Train loss: 0.9074;  Loss pred: 0.9074; Loss self: 0.0000; time: 0.36s
Val loss: 0.5838 score: 0.8605 time: 0.19s
Test loss: 0.5651 score: 0.9070 time: 0.18s
Epoch 90/1000, LR 0.000281
Train loss: 0.9037;  Loss pred: 0.9037; Loss self: 0.0000; time: 0.27s
Val loss: 0.5792 score: 0.8682 time: 0.17s
Test loss: 0.5598 score: 0.9070 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8995;  Loss pred: 0.8995; Loss self: 0.0000; time: 0.27s
Val loss: 0.5748 score: 0.8682 time: 0.17s
Test loss: 0.5547 score: 0.9070 time: 0.16s
Epoch 92/1000, LR 0.000280
Train loss: 0.8950;  Loss pred: 0.8950; Loss self: 0.0000; time: 0.27s
Val loss: 0.5703 score: 0.8682 time: 0.16s
Test loss: 0.5494 score: 0.9147 time: 0.18s
Epoch 93/1000, LR 0.000280
Train loss: 0.8910;  Loss pred: 0.8910; Loss self: 0.0000; time: 0.27s
Val loss: 0.5658 score: 0.8682 time: 0.23s
Test loss: 0.5443 score: 0.9070 time: 0.25s
Epoch 94/1000, LR 0.000280
Train loss: 0.8871;  Loss pred: 0.8871; Loss self: 0.0000; time: 0.40s
Val loss: 0.5613 score: 0.8682 time: 0.25s
Test loss: 0.5391 score: 0.9070 time: 0.25s
Epoch 95/1000, LR 0.000280
Train loss: 0.8828;  Loss pred: 0.8828; Loss self: 0.0000; time: 0.39s
Val loss: 0.5567 score: 0.8682 time: 0.19s
Test loss: 0.5337 score: 0.9070 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.8778;  Loss pred: 0.8778; Loss self: 0.0000; time: 0.27s
Val loss: 0.5520 score: 0.8682 time: 0.18s
Test loss: 0.5282 score: 0.9070 time: 0.18s
Epoch 97/1000, LR 0.000280
Train loss: 0.8732;  Loss pred: 0.8732; Loss self: 0.0000; time: 0.27s
Val loss: 0.5470 score: 0.8682 time: 0.18s
Test loss: 0.5225 score: 0.9070 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.8688;  Loss pred: 0.8688; Loss self: 0.0000; time: 0.27s
Val loss: 0.5422 score: 0.8682 time: 0.17s
Test loss: 0.5168 score: 0.9070 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.8651;  Loss pred: 0.8651; Loss self: 0.0000; time: 0.28s
Val loss: 0.5370 score: 0.8682 time: 0.17s
Test loss: 0.5107 score: 0.9070 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8605;  Loss pred: 0.8605; Loss self: 0.0000; time: 0.27s
Val loss: 0.5317 score: 0.8682 time: 0.18s
Test loss: 0.5045 score: 0.9070 time: 0.18s
Epoch 101/1000, LR 0.000279
Train loss: 0.8555;  Loss pred: 0.8555; Loss self: 0.0000; time: 0.32s
Val loss: 0.5261 score: 0.8682 time: 0.21s
Test loss: 0.4981 score: 0.9070 time: 0.17s
Epoch 102/1000, LR 0.000279
Train loss: 0.8503;  Loss pred: 0.8503; Loss self: 0.0000; time: 0.27s
Val loss: 0.5207 score: 0.8682 time: 0.18s
Test loss: 0.4918 score: 0.9070 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.8451;  Loss pred: 0.8451; Loss self: 0.0000; time: 0.27s
Val loss: 0.5154 score: 0.8682 time: 0.18s
Test loss: 0.4856 score: 0.9070 time: 0.16s
Epoch 104/1000, LR 0.000279
Train loss: 0.8407;  Loss pred: 0.8407; Loss self: 0.0000; time: 0.27s
Val loss: 0.5101 score: 0.8682 time: 0.18s
Test loss: 0.4794 score: 0.9070 time: 0.16s
Epoch 105/1000, LR 0.000279
Train loss: 0.8375;  Loss pred: 0.8375; Loss self: 0.0000; time: 0.27s
Val loss: 0.5048 score: 0.8682 time: 0.18s
Test loss: 0.4732 score: 0.9070 time: 0.18s
Epoch 106/1000, LR 0.000279
Train loss: 0.8306;  Loss pred: 0.8306; Loss self: 0.0000; time: 0.28s
Val loss: 0.4992 score: 0.8682 time: 0.19s
Test loss: 0.4666 score: 0.9070 time: 0.18s
Epoch 107/1000, LR 0.000278
Train loss: 0.8262;  Loss pred: 0.8262; Loss self: 0.0000; time: 0.27s
Val loss: 0.4934 score: 0.8682 time: 0.23s
Test loss: 0.4599 score: 0.9070 time: 0.18s
Epoch 108/1000, LR 0.000278
Train loss: 0.8207;  Loss pred: 0.8207; Loss self: 0.0000; time: 0.33s
Val loss: 0.4876 score: 0.8682 time: 0.16s
Test loss: 0.4531 score: 0.9070 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.8159;  Loss pred: 0.8159; Loss self: 0.0000; time: 0.31s
Val loss: 0.4818 score: 0.8682 time: 0.19s
Test loss: 0.4464 score: 0.9070 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.8110;  Loss pred: 0.8110; Loss self: 0.0000; time: 0.28s
Val loss: 0.4762 score: 0.8682 time: 0.17s
Test loss: 0.4399 score: 0.9070 time: 0.26s
Epoch 111/1000, LR 0.000278
Train loss: 0.8062;  Loss pred: 0.8062; Loss self: 0.0000; time: 0.26s
Val loss: 0.4707 score: 0.8682 time: 0.17s
Test loss: 0.4335 score: 0.9070 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.8001;  Loss pred: 0.8001; Loss self: 0.0000; time: 0.26s
Val loss: 0.4654 score: 0.8682 time: 0.17s
Test loss: 0.4272 score: 0.9070 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7967;  Loss pred: 0.7967; Loss self: 0.0000; time: 0.26s
Val loss: 0.4596 score: 0.8682 time: 0.17s
Test loss: 0.4205 score: 0.9070 time: 0.26s
Epoch 114/1000, LR 0.000277
Train loss: 0.7909;  Loss pred: 0.7909; Loss self: 0.0000; time: 0.38s
Val loss: 0.4543 score: 0.8682 time: 0.25s
Test loss: 0.4143 score: 0.9070 time: 0.25s
Epoch 115/1000, LR 0.000277
Train loss: 0.7853;  Loss pred: 0.7853; Loss self: 0.0000; time: 0.36s
Val loss: 0.4489 score: 0.8682 time: 0.17s
Test loss: 0.4079 score: 0.9070 time: 0.26s
Epoch 116/1000, LR 0.000277
Train loss: 0.7823;  Loss pred: 0.7823; Loss self: 0.0000; time: 0.27s
Val loss: 0.4431 score: 0.8682 time: 0.17s
Test loss: 0.4012 score: 0.9070 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7757;  Loss pred: 0.7757; Loss self: 0.0000; time: 0.27s
Val loss: 0.4376 score: 0.8682 time: 0.19s
Test loss: 0.3948 score: 0.9070 time: 0.19s
Epoch 118/1000, LR 0.000277
Train loss: 0.7706;  Loss pred: 0.7706; Loss self: 0.0000; time: 0.28s
Val loss: 0.4328 score: 0.8682 time: 0.19s
Test loss: 0.3890 score: 0.9070 time: 0.21s
Epoch 119/1000, LR 0.000277
Train loss: 0.7672;  Loss pred: 0.7672; Loss self: 0.0000; time: 0.35s
Val loss: 0.4274 score: 0.8682 time: 0.19s
Test loss: 0.3826 score: 0.9070 time: 0.18s
Epoch 120/1000, LR 0.000277
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 0.26s
Val loss: 0.4214 score: 0.8837 time: 0.17s
Test loss: 0.3759 score: 0.9070 time: 0.18s
Epoch 121/1000, LR 0.000276
Train loss: 0.7570;  Loss pred: 0.7570; Loss self: 0.0000; time: 0.26s
Val loss: 0.4159 score: 0.8837 time: 0.17s
Test loss: 0.3695 score: 0.9070 time: 0.17s
Epoch 122/1000, LR 0.000276
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 0.26s
Val loss: 0.4103 score: 0.8837 time: 0.17s
Test loss: 0.3631 score: 0.9070 time: 0.16s
Epoch 123/1000, LR 0.000276
Train loss: 0.7467;  Loss pred: 0.7467; Loss self: 0.0000; time: 0.26s
Val loss: 0.4051 score: 0.8837 time: 0.19s
Test loss: 0.3571 score: 0.9147 time: 0.19s
Epoch 124/1000, LR 0.000276
Train loss: 0.7408;  Loss pred: 0.7408; Loss self: 0.0000; time: 0.31s
Val loss: 0.4010 score: 0.8837 time: 0.21s
Test loss: 0.3519 score: 0.9070 time: 0.17s
Epoch 125/1000, LR 0.000276
Train loss: 0.7373;  Loss pred: 0.7373; Loss self: 0.0000; time: 0.28s
Val loss: 0.3962 score: 0.8837 time: 0.18s
Test loss: 0.3463 score: 0.9070 time: 0.18s
Epoch 126/1000, LR 0.000276
Train loss: 0.7327;  Loss pred: 0.7327; Loss self: 0.0000; time: 0.28s
Val loss: 0.3919 score: 0.8837 time: 0.18s
Test loss: 0.3411 score: 0.9070 time: 0.18s
Epoch 127/1000, LR 0.000275
Train loss: 0.7276;  Loss pred: 0.7276; Loss self: 0.0000; time: 0.27s
Val loss: 0.3874 score: 0.8837 time: 0.18s
Test loss: 0.3357 score: 0.9070 time: 0.19s
Epoch 128/1000, LR 0.000275
Train loss: 0.7251;  Loss pred: 0.7251; Loss self: 0.0000; time: 0.36s
Val loss: 0.3827 score: 0.8837 time: 0.21s
Test loss: 0.3302 score: 0.9070 time: 0.20s
Epoch 129/1000, LR 0.000275
Train loss: 0.7199;  Loss pred: 0.7199; Loss self: 0.0000; time: 0.28s
Val loss: 0.3783 score: 0.8837 time: 0.19s
Test loss: 0.3250 score: 0.9070 time: 0.18s
Epoch 130/1000, LR 0.000275
Train loss: 0.7149;  Loss pred: 0.7149; Loss self: 0.0000; time: 0.28s
Val loss: 0.3740 score: 0.8837 time: 0.17s
Test loss: 0.3198 score: 0.9070 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.7101;  Loss pred: 0.7101; Loss self: 0.0000; time: 0.27s
Val loss: 0.3696 score: 0.8837 time: 0.18s
Test loss: 0.3146 score: 0.9070 time: 0.18s
Epoch 132/1000, LR 0.000275
Train loss: 0.7069;  Loss pred: 0.7069; Loss self: 0.0000; time: 0.30s
Val loss: 0.3653 score: 0.8837 time: 0.19s
Test loss: 0.3095 score: 0.9070 time: 0.18s
Epoch 133/1000, LR 0.000274
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.35s
Val loss: 0.3605 score: 0.8837 time: 0.18s
Test loss: 0.3041 score: 0.9070 time: 0.18s
Epoch 134/1000, LR 0.000274
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.28s
Val loss: 0.3560 score: 0.8837 time: 0.18s
Test loss: 0.2990 score: 0.9070 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.27s
Val loss: 0.3527 score: 0.8837 time: 0.19s
Test loss: 0.2948 score: 0.9070 time: 0.19s
Epoch 136/1000, LR 0.000274
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.26s
Val loss: 0.3494 score: 0.8837 time: 0.19s
Test loss: 0.2906 score: 0.9070 time: 0.18s
Epoch 137/1000, LR 0.000274
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.27s
Val loss: 0.3457 score: 0.8837 time: 0.17s
Test loss: 0.2862 score: 0.9070 time: 0.17s
Epoch 138/1000, LR 0.000274
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.27s
Val loss: 0.3429 score: 0.8837 time: 0.17s
Test loss: 0.2823 score: 0.9070 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.29s
Val loss: 0.3396 score: 0.8837 time: 0.22s
Test loss: 0.2783 score: 0.9070 time: 0.17s
Epoch 140/1000, LR 0.000273
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.36s
Val loss: 0.3362 score: 0.8837 time: 0.17s
Test loss: 0.2742 score: 0.9070 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.27s
Val loss: 0.3328 score: 0.8837 time: 0.17s
Test loss: 0.2701 score: 0.9147 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.26s
Val loss: 0.3293 score: 0.8837 time: 0.17s
Test loss: 0.2660 score: 0.9147 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6651;  Loss pred: 0.6651; Loss self: 0.0000; time: 0.26s
Val loss: 0.3263 score: 0.8837 time: 0.17s
Test loss: 0.2622 score: 0.9147 time: 0.19s
Epoch 144/1000, LR 0.000272
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.31s
Val loss: 0.3230 score: 0.8837 time: 0.25s
Test loss: 0.2583 score: 0.9147 time: 0.25s
Epoch 145/1000, LR 0.000272
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.35s
Val loss: 0.3207 score: 0.8837 time: 0.18s
Test loss: 0.2551 score: 0.9147 time: 0.18s
Epoch 146/1000, LR 0.000272
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.28s
Val loss: 0.3182 score: 0.8837 time: 0.18s
Test loss: 0.2518 score: 0.9147 time: 0.17s
Epoch 147/1000, LR 0.000272
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.28s
Val loss: 0.3165 score: 0.8837 time: 0.17s
Test loss: 0.2491 score: 0.9147 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.28s
Val loss: 0.3152 score: 0.8837 time: 0.17s
Test loss: 0.2468 score: 0.9147 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 0.27s
Val loss: 0.3130 score: 0.8837 time: 0.17s
Test loss: 0.2438 score: 0.9147 time: 0.22s
Epoch 150/1000, LR 0.000271
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.28s
Val loss: 0.3102 score: 0.8915 time: 0.26s
Test loss: 0.2405 score: 0.9147 time: 0.20s
Epoch 151/1000, LR 0.000271
Train loss: 0.6414;  Loss pred: 0.6414; Loss self: 0.0000; time: 0.28s
Val loss: 0.3071 score: 0.8915 time: 0.20s
Test loss: 0.2369 score: 0.9147 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.29s
Val loss: 0.3047 score: 0.8915 time: 0.18s
Test loss: 0.2339 score: 0.9147 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.28s
Val loss: 0.3030 score: 0.8915 time: 0.18s
Test loss: 0.2314 score: 0.9147 time: 0.18s
Epoch 154/1000, LR 0.000271
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.30s
Val loss: 0.3019 score: 0.8915 time: 0.18s
Test loss: 0.2294 score: 0.9147 time: 0.25s
Epoch 155/1000, LR 0.000270
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.38s
Val loss: 0.3000 score: 0.8915 time: 0.25s
Test loss: 0.2268 score: 0.9147 time: 0.19s
Epoch 156/1000, LR 0.000270
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.27s
Val loss: 0.2988 score: 0.8915 time: 0.18s
Test loss: 0.2248 score: 0.9147 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.28s
Val loss: 0.2975 score: 0.8915 time: 0.19s
Test loss: 0.2228 score: 0.9147 time: 0.19s
Epoch 158/1000, LR 0.000270
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.28s
Val loss: 0.2954 score: 0.8915 time: 0.19s
Test loss: 0.2201 score: 0.9147 time: 0.19s
Epoch 159/1000, LR 0.000270
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.28s
Val loss: 0.2934 score: 0.8915 time: 0.19s
Test loss: 0.2176 score: 0.9147 time: 0.18s
Epoch 160/1000, LR 0.000269
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.29s
Val loss: 0.2920 score: 0.8915 time: 0.18s
Test loss: 0.2155 score: 0.9147 time: 0.22s
Epoch 161/1000, LR 0.000269
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 0.32s
Val loss: 0.2912 score: 0.8915 time: 0.22s
Test loss: 0.2139 score: 0.9147 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.29s
Val loss: 0.2903 score: 0.8915 time: 0.18s
Test loss: 0.2124 score: 0.9147 time: 0.18s
Epoch 163/1000, LR 0.000269
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.29s
Val loss: 0.2895 score: 0.8915 time: 0.20s
Test loss: 0.2108 score: 0.9147 time: 0.26s
Epoch 164/1000, LR 0.000269
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.39s
Val loss: 0.2895 score: 0.8915 time: 0.28s
Test loss: 0.2099 score: 0.9147 time: 0.27s
Epoch 165/1000, LR 0.000268
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.40s
Val loss: 0.2875 score: 0.8915 time: 0.27s
Test loss: 0.2075 score: 0.9147 time: 0.27s
Epoch 166/1000, LR 0.000268
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.40s
Val loss: 0.2866 score: 0.8915 time: 0.27s
Test loss: 0.2059 score: 0.9147 time: 0.26s
Epoch 167/1000, LR 0.000268
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.36s
Val loss: 0.2856 score: 0.8915 time: 0.19s
Test loss: 0.2044 score: 0.9147 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 0.28s
Val loss: 0.2858 score: 0.8915 time: 0.18s
Test loss: 0.2037 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 169/1000, LR 0.000267
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 0.29s
Val loss: 0.2853 score: 0.8915 time: 0.18s
Test loss: 0.2024 score: 0.9147 time: 0.18s
Epoch 170/1000, LR 0.000267
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.28s
Val loss: 0.2849 score: 0.8915 time: 0.18s
Test loss: 0.2014 score: 0.9147 time: 0.18s
Epoch 171/1000, LR 0.000267
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.29s
Val loss: 0.2837 score: 0.8915 time: 0.19s
Test loss: 0.1996 score: 0.9147 time: 0.24s
Epoch 172/1000, LR 0.000267
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.39s
Val loss: 0.2826 score: 0.8915 time: 0.26s
Test loss: 0.1980 score: 0.9147 time: 0.25s
Epoch 173/1000, LR 0.000267
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.39s
Val loss: 0.2810 score: 0.8915 time: 0.21s
Test loss: 0.1960 score: 0.9225 time: 0.17s
Epoch 174/1000, LR 0.000266
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.26s
Val loss: 0.2806 score: 0.8915 time: 0.17s
Test loss: 0.1950 score: 0.9225 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.27s
Val loss: 0.2815 score: 0.8915 time: 0.17s
Test loss: 0.1950 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.27s
Val loss: 0.2819 score: 0.8915 time: 0.17s
Test loss: 0.1945 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.27s
Val loss: 0.2839 score: 0.8915 time: 0.17s
Test loss: 0.1955 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 178/1000, LR 0.000265
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.28s
Val loss: 0.2832 score: 0.8915 time: 0.18s
Test loss: 0.1942 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 179/1000, LR 0.000265
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.28s
Val loss: 0.2832 score: 0.8915 time: 0.24s
Test loss: 0.1936 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.29s
Val loss: 0.2813 score: 0.8915 time: 0.19s
Test loss: 0.1914 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 181/1000, LR 0.000265
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.27s
Val loss: 0.2799 score: 0.8992 time: 0.18s
Test loss: 0.1898 score: 0.9147 time: 0.17s
Epoch 182/1000, LR 0.000265
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.28s
Val loss: 0.2784 score: 0.8992 time: 0.18s
Test loss: 0.1879 score: 0.9225 time: 0.17s
Epoch 183/1000, LR 0.000264
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.27s
Val loss: 0.2793 score: 0.8992 time: 0.17s
Test loss: 0.1880 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.28s
Val loss: 0.2797 score: 0.8992 time: 0.23s
Test loss: 0.1877 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.28s
Val loss: 0.2798 score: 0.8992 time: 0.17s
Test loss: 0.1873 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 186/1000, LR 0.000264
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.26s
Val loss: 0.2805 score: 0.8992 time: 0.18s
Test loss: 0.1872 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 0.27s
Val loss: 0.2822 score: 0.8992 time: 0.17s
Test loss: 0.1881 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.26s
Val loss: 0.2818 score: 0.8992 time: 0.17s
Test loss: 0.1872 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.27s
Val loss: 0.2828 score: 0.8992 time: 0.18s
Test loss: 0.1874 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.29s
Val loss: 0.2817 score: 0.8992 time: 0.26s
Test loss: 0.1860 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 191/1000, LR 0.000262
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.28s
Val loss: 0.2815 score: 0.8992 time: 0.19s
Test loss: 0.1853 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 192/1000, LR 0.000262
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 0.28s
Val loss: 0.2809 score: 0.8992 time: 0.19s
Test loss: 0.1843 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 193/1000, LR 0.000262
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.28s
Val loss: 0.2799 score: 0.8992 time: 0.19s
Test loss: 0.1830 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 194/1000, LR 0.000262
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.28s
Val loss: 0.2794 score: 0.8992 time: 0.18s
Test loss: 0.1821 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.28s
Val loss: 0.2793 score: 0.8992 time: 0.19s
Test loss: 0.1815 score: 0.9302 time: 0.23s
     INFO: Early stopping counter 13 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.30s
Val loss: 0.2802 score: 0.8992 time: 0.19s
Test loss: 0.1817 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 197/1000, LR 0.000261
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 0.35s
Val loss: 0.2809 score: 0.8992 time: 0.17s
Test loss: 0.1818 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 198/1000, LR 0.000261
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.32s
Val loss: 0.2822 score: 0.8992 time: 0.17s
Test loss: 0.1824 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.26s
Val loss: 0.2828 score: 0.8992 time: 0.17s
Test loss: 0.1825 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.26s
Val loss: 0.2811 score: 0.8992 time: 0.17s
Test loss: 0.1806 score: 0.9302 time: 0.27s
     INFO: Early stopping counter 18 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.27s
Val loss: 0.2819 score: 0.8992 time: 0.17s
Test loss: 0.1808 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.31s
Val loss: 0.2831 score: 0.8992 time: 0.19s
Test loss: 0.1813 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 181,   Train_Loss: 0.5866,   Val_Loss: 0.2784,   Val_Precision: 0.9474,   Val_Recall: 0.8438,   Val_accuracy: 0.8926,   Val_Score: 0.8992,   Val_Loss: 0.2784,   Test_Precision: 0.9825,   Test_Recall: 0.8615,   Test_accuracy: 0.9180,   Test_Score: 0.9225,   Test_loss: 0.1879


[0.20760776498354971, 0.16605692077428102, 0.17056229896843433, 0.17562605999410152, 0.183626146055758, 0.19163804687559605, 0.17879367503337562, 0.18349444796331227, 0.18030035379342735, 0.1825567870400846, 0.23094463488087058, 0.24746026215143502, 0.169254096923396, 0.17146165180020034, 0.16945469309575856, 0.16782164690084755, 0.16591966804116964, 0.2604686859995127, 0.1679470408707857, 0.18327770614996552, 0.17623785906471312, 0.16604165500029922, 0.2821390319149941, 0.22657390194945037, 0.1755640609189868, 0.1777474139817059, 0.18578408285975456, 0.18337985407561064, 0.2303225041832775, 0.1886288640089333, 0.1893351359758526, 0.18791426299139857, 0.18767704395577312, 0.20001850090920925, 0.235397047130391, 0.18361179088242352, 0.18147828592918813, 0.17513450188562274, 0.18334317998960614, 0.2581198059488088, 0.2517814820166677, 0.1826078798621893, 0.1942509119398892, 0.17257608915679157, 0.1775593189522624, 0.17831494892016053, 0.17423899401910603, 0.1748086200095713, 0.16898676799610257, 0.22426744806580245, 0.17320013791322708, 0.1734763530548662, 0.1705039970111102, 0.17772014415822923, 0.24498232803307474, 0.17179809999652207, 0.174724510172382, 0.1709221708588302, 0.1720204190351069, 0.18837493611499667, 0.2541725051123649, 0.2342766630463302, 0.16764483996666968, 0.16863659117370844, 0.17866831016726792, 0.17894160095602274, 0.18777737906202674, 0.19234586600214243, 0.227551860967651, 0.17522173700854182, 0.17584308120422065, 0.2531487508676946, 0.2624670311342925, 0.18280101101845503, 0.18204256193712354, 0.1826819470152259, 0.2561954839620739, 0.185029179090634, 0.18078141589649022, 0.25539965205825865, 0.2519312258809805, 0.27073586406186223, 0.17641372000798583, 0.1630205528344959, 0.18329317588359118, 0.19258446781896055, 0.1892592350486666, 0.1946041330229491, 0.1807088078930974, 0.1757434920873493, 0.16891504591330886, 0.18527165101841092, 0.25442420202307403, 0.2538715498521924, 0.18155026203021407, 0.1819934321101755, 0.1759739718399942, 0.17703308607451618, 0.172641467070207, 0.17946191085502505, 0.1737636341713369, 0.1749967080540955, 0.16470035980455577, 0.16791766905225813, 0.1856831240002066, 0.1856447970494628, 0.18546071089804173, 0.170321024954319, 0.1694547429215163, 0.263682221993804, 0.174704825039953, 0.1775144108105451, 0.2593510551378131, 0.2538635029923171, 0.26320866611786187, 0.17723405407741666, 0.1913991430774331, 0.2169508400838822, 0.18665050785057247, 0.1831160499714315, 0.1730460491962731, 0.17507549305446446, 0.1977320108562708, 0.17287560901604593, 0.1839523292146623, 0.18310444406233728, 0.19459805195219815, 0.20188505691476166, 0.18010040605440736, 0.17531064618378878, 0.18319220887497067, 0.1829089478123933, 0.1840755131561309, 0.1740313118789345, 0.1928228260949254, 0.18387638707645237, 0.1781983389519155, 0.17489577503874898, 0.1752992409747094, 0.17691366118378937, 0.18518893886357546, 0.17172211199067533, 0.19687853707000613, 0.25599591387435794, 0.18202265002764761, 0.1760478070937097, 0.17624175106175244, 0.17886801506392658, 0.22133147390559316, 0.20022541796788573, 0.17252260306850076, 0.1869047051295638, 0.18561848811805248, 0.25203357287682593, 0.1903204950504005, 0.18787872092798352, 0.19091328885406256, 0.19172335509210825, 0.18131729098968208, 0.22471544099971652, 0.18529939791187644, 0.18669098196551204, 0.26057369401678443, 0.27706012199632823, 0.27476208889856935, 0.26861713686957955, 0.1715617161244154, 0.1818888820707798, 0.1848716561216861, 0.18424214399419725, 0.24422782519832253, 0.25812930683605373, 0.17129328101873398, 0.17182862502522767, 0.17106351512484252, 0.16721125598996878, 0.17334144096821547, 0.21029490092769265, 0.1921279348898679, 0.18669158197008073, 0.1751058828085661, 0.17458516987971961, 0.17969605792313814, 0.19018812314607203, 0.18706377386115491, 0.19380989810451865, 0.19117926503531635, 0.1846109121106565, 0.21767481393180788, 0.19135821191594005, 0.18483771686442196, 0.18729169899597764, 0.190573571017012, 0.19275345280766487, 0.23504659603349864, 0.18853032100014389, 0.16704799211584032, 0.17499705497175455, 0.17892867815680802, 0.26982560590840876, 0.1906441890168935, 0.17672756290994585]
[0.0016093625192523235, 0.0012872629517386126, 0.0013221883640963902, 0.0013614423255356707, 0.0014234584965562636, 0.0014855662548495818, 0.0013859974808788807, 0.001422437581110948, 0.0013976771611893593, 0.0014151688917836015, 0.001790268487448609, 0.0019182966058250777, 0.0013120472629720619, 0.0013291600914744212, 0.0013136022720601438, 0.0013009429992313765, 0.0012861989770633305, 0.0020191371007714162, 0.0013019150455099667, 0.0014207574120152365, 0.0013661849539900242, 0.0012871446124054203, 0.002187124278410807, 0.0017563868368174447, 0.0013609617125502852, 0.0013778869300907434, 0.0014401866888353067, 0.0014215492564000824, 0.0017854457688626162, 0.001462239255883204, 0.0014677142323709506, 0.001456699713111617, 0.0014548608058587062, 0.0015505310148000717, 0.0018247833110883024, 0.001423347216142818, 0.001406808418055722, 0.001357631797562967, 0.0014212649611597375, 0.0020009287282853395, 0.0019517944342377343, 0.00141556496017201, 0.0015058210227898387, 0.0013377991407503222, 0.0013764288290873054, 0.0013822864257376785, 0.001350689876117101, 0.0013551055814695452, 0.0013099749457062214, 0.0017385073493473059, 0.0013426367280095122, 0.0013447779306578775, 0.0013217364109388387, 0.001377675536110304, 0.0018990878142098816, 0.001331768217027303, 0.0013544535672277674, 0.0013249780686731025, 0.0013334916204271853, 0.0014602708225968735, 0.001970329496995077, 0.0018160981631498466, 0.0012995724028424006, 0.0013072603966954143, 0.0013850256602113793, 0.0013871441934575407, 0.0014556385973800523, 0.0014910532248228095, 0.0017639679144779148, 0.0013583080388259055, 0.0013631246604978344, 0.001962393417579028, 0.002034628148327849, 0.0014170621009182561, 0.0014111826506753762, 0.001416139124149038, 0.0019860115035819684, 0.0014343347216328216, 0.0014014063247789939, 0.001979842264017509, 0.0019529552393874458, 0.002098727628386529, 0.0013675482171161691, 0.001263725215771286, 0.0014208773324309394, 0.0014929028513097718, 0.0014671258530904387, 0.0015085591707205356, 0.0014008434720395148, 0.001362352651839917, 0.0013094189605682856, 0.0014362143489799297, 0.0019722806358377833, 0.0019679965104821115, 0.0014073663723272408, 0.001410801799303686, 0.0013641393165891023, 0.0013723495044536138, 0.0013383059462806744, 0.001391177603527326, 0.001347004916056875, 0.0013565636283263218, 0.0012767469752291145, 0.0013016873569942492, 0.0014394040620171052, 0.0014391069538718046, 0.0014376799294421839, 0.0013203180229016977, 0.001313602658306328, 0.0020440482325101085, 0.001354300969301961, 0.0013760807039577137, 0.002010473295641962, 0.001967934131723388, 0.0020403772567276113, 0.0013739073959489663, 0.001483714287421962, 0.0016817894580145908, 0.0014469031616323447, 0.0014195042633444303, 0.0013414422418315743, 0.0013571743647632904, 0.0015328062857075254, 0.001340121000124387, 0.0014259870481756769, 0.0014194142950568782, 0.0015085120306371949, 0.0015650004411997028, 0.001396127178716336, 0.0013589972572386727, 0.001420094642441633, 0.0014178988202511109, 0.0014269419624506272, 0.001349079937046004, 0.0014947505898831428, 0.0014253983494298633, 0.001381382472495469, 0.0013557812018507674, 0.001358908844765189, 0.0013714237301068943, 0.001435573169485081, 0.0013311791627184134, 0.0015261902098450087, 0.0019844644486384337, 0.001411028294787966, 0.0013647116828969744, 0.0013662151245097088, 0.0013865737601854774, 0.0017157478597332803, 0.0015521350230068662, 0.0013373845199108586, 0.0014488736831749132, 0.0014389030086670734, 0.001953748626952139, 0.0014753526748093062, 0.0014564241932401823, 0.0014799479756128881, 0.0014862275588535523, 0.0014055603952688534, 0.0017419801627885, 0.0014364294411773368, 0.0014472169144613336, 0.0020199511164091816, 0.002147752883692467, 0.0021299386736323206, 0.002082303386585888, 0.001329935783910197, 0.001409991333882014, 0.0014331136133464038, 0.0014282336743736222, 0.0018932389550257562, 0.00200100237857406, 0.0013278548916180928, 0.0013320048451568036, 0.001326073760657694, 0.001296211286743944, 0.0013437321005288021, 0.0016301930304472299, 0.0014893638363555652, 0.0014472215656595405, 0.0013574099442524503, 0.001353373409920307, 0.0013929926970785902, 0.0014743265360160622, 0.00145010677411748, 0.0015024023108877415, 0.0014820098064753206, 0.0014310923419430736, 0.0016874016583861076, 0.0014833969915964344, 0.0014328505183288523, 0.0014518736356277336, 0.001477314504007845, 0.0014942128124625185, 0.001822066635918594, 0.0014614753565902626, 0.0012949456753165915, 0.0013565663176105004, 0.001387044016719442, 0.0020916713636310756, 0.0014778619303635156, 0.0013699811078290375]
[621.3640419963175, 776.8420575216373, 756.3218881323463, 734.5151397482385, 702.5143356264157, 673.1439925587521, 721.5020328650855, 703.018545965991, 715.4728057150521, 706.6294389354861, 558.5754354784764, 521.2958188860947, 762.1676659229413, 752.3548189674519, 761.265431150392, 768.6731859818765, 777.4846799234871, 495.26106950238676, 768.0992730276766, 703.84992648504, 731.9653148568506, 776.9134799322949, 457.2213887756817, 569.3506572914142, 734.7745280255647, 725.748955274685, 694.3544248480105, 703.4578615533803, 560.0842195487297, 683.8826108494757, 681.3315412119406, 686.4832820375361, 687.350979539082, 644.9403400866133, 548.0102727395064, 702.5692597410894, 710.8288429081545, 736.5767373709587, 703.5985740364769, 499.7679256956505, 512.3490376129421, 706.4317273567485, 664.089546410567, 747.4963688787667, 726.5177674773703, 723.439065435613, 740.3624012306603, 737.9498790902694, 763.3733784586921, 575.2060814557003, 744.8031020889183, 743.6172004330789, 756.5805040429301, 725.8603160098029, 526.5685938888779, 750.8814125570161, 738.3051174258808, 754.7294733726791, 749.910974078449, 684.804479090837, 507.529325183979, 550.6310288126699, 769.4838685500081, 764.9585365913869, 722.0082838374145, 720.9055876934032, 686.9837072195402, 670.6668704726055, 566.9037354888465, 736.210028517817, 733.6086192107949, 509.5818152680533, 491.4903004865268, 705.6853749401668, 708.625491903129, 706.1453094171824, 503.5217561410903, 697.1873335546235, 713.5689216742354, 505.090742921506, 512.0445055943295, 476.47917074822396, 731.2356430903474, 791.3112657087187, 703.790522359258, 669.835950224536, 681.6047838660481, 662.8841741238219, 713.855630525287, 734.0243355121422, 763.6975102040691, 696.2748984580536, 507.02723630160324, 508.1309822825977, 710.5470328571131, 708.8167880800543, 733.06295613589, 728.6773498695139, 747.2132981095463, 718.8154822680465, 742.3877879579888, 737.1567238860466, 783.2405475803444, 768.2336273966115, 694.7319563616157, 694.875385953475, 695.565111205244, 757.3932815082488, 761.2652073111161, 489.22524630056876, 738.3883070802376, 726.7015641771033, 497.3953159028114, 508.14708880742126, 490.105443345225, 727.8510931293836, 673.9842087370856, 594.6047498600293, 691.1312564082282, 704.4712903108476, 745.4663114191358, 736.8249990298141, 652.3981597181483, 746.2012757856808, 701.2686414503838, 704.5159425845634, 662.9048888510358, 638.9774556443044, 716.2671246894902, 735.8366580017137, 704.1784188979508, 705.2689414205869, 700.7993501589946, 741.2459206751214, 669.0079313353397, 701.5582699390553, 723.9124716802718, 737.582139828246, 735.8845325440454, 729.1692407291624, 696.5858802994244, 751.2136818291917, 655.2263233961869, 503.9142931918547, 708.7030102045326, 732.7555061866445, 731.9491506572716, 721.2021666025422, 582.8362217251746, 644.2738454949331, 747.728110436521, 690.1912924587757, 694.973875220644, 511.83657211829075, 677.8040376883127, 686.6131479011264, 675.6994276004006, 672.8444739454172, 711.4600008409611, 574.0593500210906, 696.1706376474523, 690.981420965639, 495.06148533815804, 465.60291344168826, 469.497085704649, 480.23741710355876, 751.9160038388172, 709.2242171778331, 697.7813836161542, 700.1655386948976, 528.1953434062927, 499.7495308889202, 753.0943375758652, 750.7480199009937, 754.1058647476963, 771.479164104472, 744.1959596012238, 613.4242886105692, 671.4276092851658, 690.9792002334287, 736.6971225120335, 738.8943750999841, 717.8788532755543, 678.2757927576944, 689.6043917928662, 665.6006801594432, 674.7593677388077, 698.7669283746179, 592.6271288345399, 674.1283726912499, 697.9095078014906, 688.7651758809154, 676.9039343261533, 669.2487118698726, 548.8273481808476, 684.2400697970569, 772.2331670442603, 737.1552625318253, 720.9576537917977, 478.08657582997716, 676.6531970642386, 729.9370730627563]
Elapsed: 0.19435214168405954~0.02953522544613578
Time per graph: 0.0015066057494888337~0.00022895523601655646
Speed: 676.6491965793944~85.50902708417127
Total Time: 0.1776
best val loss: 0.27837816348602606 test_score: 0.9225

Testing...
Test loss: 0.6908 score: 0.9302 time: 0.26s
test Score 0.9302
Epoch Time List: [1.0836947648786008, 0.5708195301704109, 0.589953602058813, 0.5854726540856063, 0.6093268170952797, 0.6215507951565087, 0.6941069930326194, 0.6255710572004318, 0.6100808072369546, 0.6161278311628848, 0.6689238180406392, 0.8476019520312548, 0.5766242421232164, 0.586501965066418, 0.5844179200939834, 0.5784046018961817, 0.5902710638474673, 0.7421582110691816, 0.7174279193859547, 0.6163106430321932, 0.6466278899461031, 0.5921160059515387, 0.7165219869930297, 0.6950624019373208, 0.6512760522309691, 0.6544777068775147, 0.6323277559131384, 0.6489459669683129, 1.0159847061149776, 0.753773788921535, 0.6668274598196149, 0.6565516230184585, 0.6575678871013224, 0.6556579628959298, 0.7039439573418349, 0.7332392169628292, 0.6241264860145748, 0.6121964640915394, 0.6307262841146439, 0.8964077108539641, 0.8994238651357591, 0.7698187651112676, 0.6455200638156384, 0.6094641529489309, 0.6036050640977919, 0.6161871505901217, 0.6098203489091247, 0.615702377166599, 0.6774818878620863, 0.6896966365166008, 0.603852248750627, 0.6072742631658912, 0.6058353190310299, 0.649913219967857, 0.8141458800528198, 0.6883733361028135, 0.6075125087518245, 0.6020360859110951, 0.6002620700746775, 0.6150864399969578, 0.7434393090661615, 0.8648080902639776, 0.620058850152418, 0.6100925570353866, 0.6069854509551078, 0.6089895707555115, 0.6363219949416816, 0.6232986380346119, 0.6704465737566352, 0.6136036787647754, 0.618670579046011, 0.7081901277415454, 0.8932457382325083, 0.8118357877247036, 0.6313791801221669, 0.6206254351418465, 0.7876678970642388, 0.7747827307321131, 0.6215538906399161, 0.6962253581732512, 0.8947601767722517, 0.9314866231288761, 0.701017543906346, 0.5935166077688336, 0.6309381008613855, 0.6407432141713798, 0.6513632847927511, 0.6830349741503596, 0.7243960769847035, 0.6154722208157182, 0.6060785339213908, 0.6115176861640066, 0.7549497007858008, 0.9040758379269391, 0.7627244300674647, 0.6264859489165246, 0.6201118370518088, 0.6138705709017813, 0.6202610936015844, 0.6306114050094038, 0.6984840319491923, 0.6219825057778507, 0.6094407942146063, 0.6150315413251519, 0.6382266511209309, 0.6498633152805269, 0.6824035979807377, 0.6593251440208405, 0.6638632609974593, 0.7035925416275859, 0.5943626621738076, 0.6015636257361621, 0.6890941220335662, 0.8806173091288656, 0.7947248867712915, 0.6144833690486848, 0.6449953860137612, 0.6764656975865364, 0.7164703099988401, 0.6101893286686391, 0.5969492038711905, 0.5941944501828402, 0.64245097595267, 0.6812034668400884, 0.6377661831211299, 0.6395013139117509, 0.640983346151188, 0.7597829701844603, 0.6368520788382739, 0.6210459012072533, 0.6249303610529751, 0.6647329272236675, 0.704937273170799, 0.6335280742496252, 0.648711069021374, 0.6300547479186207, 0.6154474157374352, 0.6098680929280818, 0.6802037602756172, 0.7083359891548753, 0.6217028400860727, 0.6023208361584693, 0.6252047962043434, 0.8119614911265671, 0.7071308696176857, 0.6252438726369292, 0.6202035881578922, 0.6239648910705, 0.6633785718586296, 0.7388974640052766, 0.6487703388556838, 0.6518367801327258, 0.6479652069974691, 0.7221035200636834, 0.8197508461307734, 0.6384308631531894, 0.651065044105053, 0.6603788938373327, 0.6467790068127215, 0.6931481338106096, 0.726722963154316, 0.653522894019261, 0.7409208337776363, 0.9489967718254775, 0.9411165530327708, 0.9322094789240509, 0.7123096061404794, 0.6359040278475732, 0.6487589322496206, 0.6393916851375252, 0.7109046161640435, 0.8947819168679416, 0.7660877150483429, 0.6062083761207759, 0.6066623837687075, 0.6005108479876071, 0.6142633031122386, 0.6662183662410825, 0.7127436762675643, 0.6534634241834283, 0.6197552247904241, 0.6247987577226013, 0.6125328431371599, 0.6997722347732633, 0.6334923699032515, 0.632179053267464, 0.6291042079683393, 0.61709213280119, 0.6675330428406596, 0.7309160926379263, 0.648935440229252, 0.6470874031074345, 0.6575603939127177, 0.6484825718216598, 0.6942376263905317, 0.6803573067300022, 0.6846679172012955, 0.6566675261128694, 0.6047672310378402, 0.6983086878899485, 0.6233348120003939, 0.6802933658473194]
Total Epoch List: [202]
Total Time List: [0.17763156397268176]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a30723ac0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3891;  Loss pred: 2.3891; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 2.3632;  Loss pred: 2.3632; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 2.3344;  Loss pred: 2.3344; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 2.3356;  Loss pred: 2.3356; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.23s
Epoch 5/1000, LR 0.000105
Train loss: 2.3055;  Loss pred: 2.3055; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 2.2687;  Loss pred: 2.2687; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 2.2042;  Loss pred: 2.2042; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.25s
Epoch 8/1000, LR 0.000195
Train loss: 2.1572;  Loss pred: 2.1572; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.27s
Epoch 9/1000, LR 0.000225
Train loss: 2.1059;  Loss pred: 2.1059; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 2.0349;  Loss pred: 2.0349; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.9643;  Loss pred: 1.9643; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.19s
Epoch 12/1000, LR 0.000285
Train loss: 1.9063;  Loss pred: 1.9063; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 1.8271;  Loss pred: 1.8271; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 1.7686;  Loss pred: 1.7686; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.25s
Epoch 15/1000, LR 0.000285
Train loss: 1.7180;  Loss pred: 1.7180; Loss self: 0.0000; time: 0.38s
Val loss: 0.6925 score: 0.5039 time: 0.21s
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 16/1000, LR 0.000285
Train loss: 1.6695;  Loss pred: 1.6695; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.5659 time: 0.18s
Test loss: 0.6926 score: 0.5426 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 1.6269;  Loss pred: 1.6269; Loss self: 0.0000; time: 0.26s
Val loss: 0.6923 score: 0.6202 time: 0.18s
Test loss: 0.6925 score: 0.5504 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 1.5668;  Loss pred: 1.5668; Loss self: 0.0000; time: 0.25s
Val loss: 0.6922 score: 0.7519 time: 0.19s
Test loss: 0.6924 score: 0.7364 time: 0.25s
Epoch 19/1000, LR 0.000285
Train loss: 1.5263;  Loss pred: 1.5263; Loss self: 0.0000; time: 0.37s
Val loss: 0.6921 score: 0.7597 time: 0.25s
Test loss: 0.6923 score: 0.7674 time: 0.25s
Epoch 20/1000, LR 0.000285
Train loss: 1.4846;  Loss pred: 1.4846; Loss self: 0.0000; time: 0.37s
Val loss: 0.6920 score: 0.8062 time: 0.26s
Test loss: 0.6922 score: 0.7752 time: 0.25s
Epoch 21/1000, LR 0.000285
Train loss: 1.4544;  Loss pred: 1.4544; Loss self: 0.0000; time: 0.37s
Val loss: 0.6918 score: 0.8837 time: 0.26s
Test loss: 0.6921 score: 0.8295 time: 0.25s
Epoch 22/1000, LR 0.000285
Train loss: 1.4167;  Loss pred: 1.4167; Loss self: 0.0000; time: 0.33s
Val loss: 0.6917 score: 0.7519 time: 0.18s
Test loss: 0.6920 score: 0.7442 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 1.3797;  Loss pred: 1.3797; Loss self: 0.0000; time: 0.26s
Val loss: 0.6915 score: 0.7674 time: 0.18s
Test loss: 0.6918 score: 0.7674 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.3518;  Loss pred: 1.3518; Loss self: 0.0000; time: 0.25s
Val loss: 0.6914 score: 0.7984 time: 0.19s
Test loss: 0.6917 score: 0.7752 time: 0.19s
Epoch 25/1000, LR 0.000285
Train loss: 1.3239;  Loss pred: 1.3239; Loss self: 0.0000; time: 0.25s
Val loss: 0.6912 score: 0.8915 time: 0.19s
Test loss: 0.6916 score: 0.8527 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 1.3021;  Loss pred: 1.3021; Loss self: 0.0000; time: 0.24s
Val loss: 0.6910 score: 0.8915 time: 0.18s
Test loss: 0.6914 score: 0.8527 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.2732;  Loss pred: 1.2732; Loss self: 0.0000; time: 0.32s
Val loss: 0.6908 score: 0.7829 time: 0.18s
Test loss: 0.6912 score: 0.7674 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.2600;  Loss pred: 1.2600; Loss self: 0.0000; time: 0.25s
Val loss: 0.6906 score: 0.7907 time: 0.18s
Test loss: 0.6910 score: 0.7674 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.2318;  Loss pred: 1.2318; Loss self: 0.0000; time: 0.24s
Val loss: 0.6904 score: 0.7287 time: 0.18s
Test loss: 0.6908 score: 0.7287 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 1.2152;  Loss pred: 1.2152; Loss self: 0.0000; time: 0.25s
Val loss: 0.6901 score: 0.7984 time: 0.18s
Test loss: 0.6906 score: 0.7752 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.2040;  Loss pred: 1.2040; Loss self: 0.0000; time: 0.25s
Val loss: 0.6899 score: 0.7829 time: 0.17s
Test loss: 0.6903 score: 0.7597 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.1805;  Loss pred: 1.1805; Loss self: 0.0000; time: 0.24s
Val loss: 0.6896 score: 0.7364 time: 0.18s
Test loss: 0.6901 score: 0.7209 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.1721;  Loss pred: 1.1721; Loss self: 0.0000; time: 0.26s
Val loss: 0.6893 score: 0.7984 time: 0.21s
Test loss: 0.6898 score: 0.7597 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 1.1545;  Loss pred: 1.1545; Loss self: 0.0000; time: 0.33s
Val loss: 0.6890 score: 0.7132 time: 0.19s
Test loss: 0.6895 score: 0.6899 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 1.1454;  Loss pred: 1.1454; Loss self: 0.0000; time: 0.24s
Val loss: 0.6886 score: 0.6899 time: 0.17s
Test loss: 0.6892 score: 0.6822 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 1.1273;  Loss pred: 1.1273; Loss self: 0.0000; time: 0.26s
Val loss: 0.6883 score: 0.7132 time: 0.18s
Test loss: 0.6889 score: 0.6822 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.1201;  Loss pred: 1.1201; Loss self: 0.0000; time: 0.28s
Val loss: 0.6879 score: 0.7287 time: 0.19s
Test loss: 0.6885 score: 0.6822 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 1.1130;  Loss pred: 1.1130; Loss self: 0.0000; time: 0.27s
Val loss: 0.6875 score: 0.7519 time: 0.20s
Test loss: 0.6882 score: 0.6977 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 1.1032;  Loss pred: 1.1032; Loss self: 0.0000; time: 0.31s
Val loss: 0.6870 score: 0.7752 time: 0.25s
Test loss: 0.6878 score: 0.7132 time: 0.22s
Epoch 40/1000, LR 0.000284
Train loss: 1.0952;  Loss pred: 1.0952; Loss self: 0.0000; time: 0.26s
Val loss: 0.6866 score: 0.7752 time: 0.20s
Test loss: 0.6873 score: 0.7287 time: 0.19s
Epoch 41/1000, LR 0.000284
Train loss: 1.0849;  Loss pred: 1.0849; Loss self: 0.0000; time: 0.25s
Val loss: 0.6861 score: 0.8140 time: 0.19s
Test loss: 0.6869 score: 0.7829 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 1.0770;  Loss pred: 1.0770; Loss self: 0.0000; time: 0.25s
Val loss: 0.6855 score: 0.8217 time: 0.18s
Test loss: 0.6864 score: 0.7829 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.0706;  Loss pred: 1.0706; Loss self: 0.0000; time: 0.24s
Val loss: 0.6850 score: 0.8140 time: 0.17s
Test loss: 0.6859 score: 0.7829 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 1.0656;  Loss pred: 1.0656; Loss self: 0.0000; time: 0.27s
Val loss: 0.6844 score: 0.8295 time: 0.18s
Test loss: 0.6854 score: 0.7829 time: 0.23s
Epoch 45/1000, LR 0.000284
Train loss: 1.0615;  Loss pred: 1.0615; Loss self: 0.0000; time: 0.27s
Val loss: 0.6838 score: 0.8450 time: 0.18s
Test loss: 0.6848 score: 0.7984 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 1.0547;  Loss pred: 1.0547; Loss self: 0.0000; time: 0.25s
Val loss: 0.6831 score: 0.8682 time: 0.18s
Test loss: 0.6842 score: 0.8140 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0508;  Loss pred: 1.0508; Loss self: 0.0000; time: 0.25s
Val loss: 0.6823 score: 0.8682 time: 0.17s
Test loss: 0.6836 score: 0.8450 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 1.0455;  Loss pred: 1.0455; Loss self: 0.0000; time: 0.25s
Val loss: 0.6816 score: 0.8682 time: 0.18s
Test loss: 0.6829 score: 0.8450 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 1.0440;  Loss pred: 1.0440; Loss self: 0.0000; time: 0.25s
Val loss: 0.6808 score: 0.9070 time: 0.17s
Test loss: 0.6822 score: 0.8605 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 1.0360;  Loss pred: 1.0360; Loss self: 0.0000; time: 0.25s
Val loss: 0.6799 score: 0.9070 time: 0.17s
Test loss: 0.6814 score: 0.8605 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 1.0319;  Loss pred: 1.0319; Loss self: 0.0000; time: 0.24s
Val loss: 0.6790 score: 0.9147 time: 0.18s
Test loss: 0.6806 score: 0.8682 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 1.0298;  Loss pred: 1.0298; Loss self: 0.0000; time: 0.25s
Val loss: 0.6780 score: 0.9147 time: 0.26s
Test loss: 0.6798 score: 0.8915 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 1.0255;  Loss pred: 1.0255; Loss self: 0.0000; time: 0.25s
Val loss: 0.6770 score: 0.9147 time: 0.18s
Test loss: 0.6788 score: 0.8992 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 1.0183;  Loss pred: 1.0183; Loss self: 0.0000; time: 0.25s
Val loss: 0.6759 score: 0.9147 time: 0.18s
Test loss: 0.6779 score: 0.8992 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 1.0170;  Loss pred: 1.0170; Loss self: 0.0000; time: 0.25s
Val loss: 0.6748 score: 0.9147 time: 0.18s
Test loss: 0.6769 score: 0.8992 time: 0.18s
Epoch 56/1000, LR 0.000284
Train loss: 1.0129;  Loss pred: 1.0129; Loss self: 0.0000; time: 0.26s
Val loss: 0.6736 score: 0.9147 time: 0.20s
Test loss: 0.6758 score: 0.8992 time: 0.19s
Epoch 57/1000, LR 0.000283
Train loss: 1.0104;  Loss pred: 1.0104; Loss self: 0.0000; time: 0.34s
Val loss: 0.6723 score: 0.9147 time: 0.18s
Test loss: 0.6746 score: 0.9070 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 1.0078;  Loss pred: 1.0078; Loss self: 0.0000; time: 0.25s
Val loss: 0.6709 score: 0.9147 time: 0.18s
Test loss: 0.6734 score: 0.9070 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 1.0045;  Loss pred: 1.0045; Loss self: 0.0000; time: 0.25s
Val loss: 0.6695 score: 0.9147 time: 0.18s
Test loss: 0.6721 score: 0.9070 time: 0.17s
Epoch 60/1000, LR 0.000283
Train loss: 1.0020;  Loss pred: 1.0020; Loss self: 0.0000; time: 0.25s
Val loss: 0.6679 score: 0.9225 time: 0.18s
Test loss: 0.6708 score: 0.9070 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 0.9994;  Loss pred: 0.9994; Loss self: 0.0000; time: 0.30s
Val loss: 0.6663 score: 0.9302 time: 0.20s
Test loss: 0.6694 score: 0.9070 time: 0.18s
Epoch 62/1000, LR 0.000283
Train loss: 0.9962;  Loss pred: 0.9962; Loss self: 0.0000; time: 0.25s
Val loss: 0.6646 score: 0.9302 time: 0.18s
Test loss: 0.6679 score: 0.9070 time: 0.18s
Epoch 63/1000, LR 0.000283
Train loss: 0.9927;  Loss pred: 0.9927; Loss self: 0.0000; time: 0.24s
Val loss: 0.6627 score: 0.9225 time: 0.18s
Test loss: 0.6662 score: 0.9070 time: 0.18s
Epoch 64/1000, LR 0.000283
Train loss: 0.9895;  Loss pred: 0.9895; Loss self: 0.0000; time: 0.24s
Val loss: 0.6608 score: 0.9225 time: 0.17s
Test loss: 0.6645 score: 0.9070 time: 0.17s
Epoch 65/1000, LR 0.000283
Train loss: 0.9858;  Loss pred: 0.9858; Loss self: 0.0000; time: 0.24s
Val loss: 0.6588 score: 0.9225 time: 0.18s
Test loss: 0.6627 score: 0.9070 time: 0.20s
Epoch 66/1000, LR 0.000283
Train loss: 0.9830;  Loss pred: 0.9830; Loss self: 0.0000; time: 0.24s
Val loss: 0.6566 score: 0.9225 time: 0.17s
Test loss: 0.6607 score: 0.9070 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 0.9818;  Loss pred: 0.9818; Loss self: 0.0000; time: 0.25s
Val loss: 0.6543 score: 0.9302 time: 0.20s
Test loss: 0.6587 score: 0.9070 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 0.9789;  Loss pred: 0.9789; Loss self: 0.0000; time: 0.28s
Val loss: 0.6520 score: 0.9302 time: 0.17s
Test loss: 0.6566 score: 0.9070 time: 0.16s
Epoch 69/1000, LR 0.000283
Train loss: 0.9766;  Loss pred: 0.9766; Loss self: 0.0000; time: 0.34s
Val loss: 0.6495 score: 0.9302 time: 0.17s
Test loss: 0.6544 score: 0.9070 time: 0.16s
Epoch 70/1000, LR 0.000283
Train loss: 0.9732;  Loss pred: 0.9732; Loss self: 0.0000; time: 0.24s
Val loss: 0.6469 score: 0.9302 time: 0.19s
Test loss: 0.6521 score: 0.9070 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 0.9693;  Loss pred: 0.9693; Loss self: 0.0000; time: 0.26s
Val loss: 0.6441 score: 0.9302 time: 0.17s
Test loss: 0.6496 score: 0.9070 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9679;  Loss pred: 0.9679; Loss self: 0.0000; time: 0.24s
Val loss: 0.6413 score: 0.9302 time: 0.18s
Test loss: 0.6471 score: 0.9070 time: 0.18s
Epoch 73/1000, LR 0.000282
Train loss: 0.9645;  Loss pred: 0.9645; Loss self: 0.0000; time: 0.35s
Val loss: 0.6382 score: 0.9302 time: 0.19s
Test loss: 0.6444 score: 0.9147 time: 0.19s
Epoch 74/1000, LR 0.000282
Train loss: 0.9628;  Loss pred: 0.9628; Loss self: 0.0000; time: 0.24s
Val loss: 0.6351 score: 0.9380 time: 0.17s
Test loss: 0.6417 score: 0.9147 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9580;  Loss pred: 0.9580; Loss self: 0.0000; time: 0.24s
Val loss: 0.6318 score: 0.9302 time: 0.18s
Test loss: 0.6387 score: 0.9147 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9541;  Loss pred: 0.9541; Loss self: 0.0000; time: 0.25s
Val loss: 0.6283 score: 0.9302 time: 0.19s
Test loss: 0.6356 score: 0.9147 time: 0.26s
Epoch 77/1000, LR 0.000282
Train loss: 0.9521;  Loss pred: 0.9521; Loss self: 0.0000; time: 0.37s
Val loss: 0.6247 score: 0.9302 time: 0.34s
Test loss: 0.6323 score: 0.9147 time: 0.27s
Epoch 78/1000, LR 0.000282
Train loss: 0.9498;  Loss pred: 0.9498; Loss self: 0.0000; time: 0.24s
Val loss: 0.6209 score: 0.9302 time: 0.17s
Test loss: 0.6290 score: 0.9147 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.9462;  Loss pred: 0.9462; Loss self: 0.0000; time: 0.24s
Val loss: 0.6171 score: 0.9225 time: 0.17s
Test loss: 0.6256 score: 0.9147 time: 0.24s
Epoch 80/1000, LR 0.000282
Train loss: 0.9429;  Loss pred: 0.9429; Loss self: 0.0000; time: 0.25s
Val loss: 0.6131 score: 0.9302 time: 0.18s
Test loss: 0.6220 score: 0.9147 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.9386;  Loss pred: 0.9386; Loss self: 0.0000; time: 0.25s
Val loss: 0.6089 score: 0.9302 time: 0.17s
Test loss: 0.6183 score: 0.9147 time: 0.16s
Epoch 82/1000, LR 0.000281
Train loss: 0.9369;  Loss pred: 0.9369; Loss self: 0.0000; time: 0.25s
Val loss: 0.6045 score: 0.9302 time: 0.26s
Test loss: 0.6144 score: 0.9147 time: 0.22s
Epoch 83/1000, LR 0.000281
Train loss: 0.9328;  Loss pred: 0.9328; Loss self: 0.0000; time: 0.25s
Val loss: 0.6001 score: 0.9302 time: 0.24s
Test loss: 0.6104 score: 0.9147 time: 0.24s
Epoch 84/1000, LR 0.000281
Train loss: 0.9286;  Loss pred: 0.9286; Loss self: 0.0000; time: 0.39s
Val loss: 0.5955 score: 0.9457 time: 0.18s
Test loss: 0.6063 score: 0.9225 time: 0.26s
Epoch 85/1000, LR 0.000281
Train loss: 0.9259;  Loss pred: 0.9259; Loss self: 0.0000; time: 0.25s
Val loss: 0.5908 score: 0.9457 time: 0.18s
Test loss: 0.6021 score: 0.9225 time: 0.17s
Epoch 86/1000, LR 0.000281
Train loss: 0.9213;  Loss pred: 0.9213; Loss self: 0.0000; time: 0.25s
Val loss: 0.5859 score: 0.9457 time: 0.17s
Test loss: 0.5977 score: 0.9225 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.9183;  Loss pred: 0.9183; Loss self: 0.0000; time: 0.24s
Val loss: 0.5809 score: 0.9457 time: 0.17s
Test loss: 0.5932 score: 0.9225 time: 0.16s
Epoch 88/1000, LR 0.000281
Train loss: 0.9127;  Loss pred: 0.9127; Loss self: 0.0000; time: 0.25s
Val loss: 0.5757 score: 0.9457 time: 0.17s
Test loss: 0.5885 score: 0.9225 time: 0.17s
Epoch 89/1000, LR 0.000281
Train loss: 0.9106;  Loss pred: 0.9106; Loss self: 0.0000; time: 0.26s
Val loss: 0.5703 score: 0.9380 time: 0.18s
Test loss: 0.5836 score: 0.9225 time: 0.24s
Epoch 90/1000, LR 0.000281
Train loss: 0.9064;  Loss pred: 0.9064; Loss self: 0.0000; time: 0.28s
Val loss: 0.5647 score: 0.9302 time: 0.18s
Test loss: 0.5786 score: 0.9147 time: 0.18s
Epoch 91/1000, LR 0.000280
Train loss: 0.9024;  Loss pred: 0.9024; Loss self: 0.0000; time: 0.26s
Val loss: 0.5590 score: 0.9302 time: 0.18s
Test loss: 0.5734 score: 0.9225 time: 0.18s
Epoch 92/1000, LR 0.000280
Train loss: 0.8978;  Loss pred: 0.8978; Loss self: 0.0000; time: 0.25s
Val loss: 0.5531 score: 0.9302 time: 0.18s
Test loss: 0.5680 score: 0.9225 time: 0.19s
Epoch 93/1000, LR 0.000280
Train loss: 0.8941;  Loss pred: 0.8941; Loss self: 0.0000; time: 0.32s
Val loss: 0.5472 score: 0.9302 time: 0.26s
Test loss: 0.5626 score: 0.9225 time: 0.25s
Epoch 94/1000, LR 0.000280
Train loss: 0.8915;  Loss pred: 0.8915; Loss self: 0.0000; time: 0.37s
Val loss: 0.5412 score: 0.9302 time: 0.25s
Test loss: 0.5572 score: 0.9147 time: 0.18s
Epoch 95/1000, LR 0.000280
Train loss: 0.8857;  Loss pred: 0.8857; Loss self: 0.0000; time: 0.26s
Val loss: 0.5351 score: 0.9302 time: 0.18s
Test loss: 0.5516 score: 0.9225 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.8821;  Loss pred: 0.8821; Loss self: 0.0000; time: 0.25s
Val loss: 0.5289 score: 0.9380 time: 0.18s
Test loss: 0.5459 score: 0.9225 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8769;  Loss pred: 0.8769; Loss self: 0.0000; time: 0.24s
Val loss: 0.5227 score: 0.9457 time: 0.17s
Test loss: 0.5402 score: 0.9225 time: 0.16s
Epoch 98/1000, LR 0.000280
Train loss: 0.8733;  Loss pred: 0.8733; Loss self: 0.0000; time: 0.25s
Val loss: 0.5165 score: 0.9457 time: 0.17s
Test loss: 0.5344 score: 0.9225 time: 0.16s
Epoch 99/1000, LR 0.000279
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.24s
Val loss: 0.5101 score: 0.9457 time: 0.18s
Test loss: 0.5285 score: 0.9225 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8636;  Loss pred: 0.8636; Loss self: 0.0000; time: 0.24s
Val loss: 0.5037 score: 0.9457 time: 0.19s
Test loss: 0.5226 score: 0.9225 time: 0.17s
Epoch 101/1000, LR 0.000279
Train loss: 0.8589;  Loss pred: 0.8589; Loss self: 0.0000; time: 0.32s
Val loss: 0.4973 score: 0.9457 time: 0.18s
Test loss: 0.5166 score: 0.9225 time: 0.16s
Epoch 102/1000, LR 0.000279
Train loss: 0.8564;  Loss pred: 0.8564; Loss self: 0.0000; time: 0.24s
Val loss: 0.4908 score: 0.9457 time: 0.17s
Test loss: 0.5106 score: 0.9225 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.8505;  Loss pred: 0.8505; Loss self: 0.0000; time: 0.24s
Val loss: 0.4843 score: 0.9457 time: 0.18s
Test loss: 0.5045 score: 0.9225 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.8460;  Loss pred: 0.8460; Loss self: 0.0000; time: 0.25s
Val loss: 0.4774 score: 0.9457 time: 0.18s
Test loss: 0.4982 score: 0.9225 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.8415;  Loss pred: 0.8415; Loss self: 0.0000; time: 0.25s
Val loss: 0.4707 score: 0.9457 time: 0.18s
Test loss: 0.4920 score: 0.9225 time: 0.19s
Epoch 106/1000, LR 0.000279
Train loss: 0.8380;  Loss pred: 0.8380; Loss self: 0.0000; time: 0.25s
Val loss: 0.4643 score: 0.9457 time: 0.26s
Test loss: 0.4858 score: 0.9225 time: 0.17s
Epoch 107/1000, LR 0.000278
Train loss: 0.8324;  Loss pred: 0.8324; Loss self: 0.0000; time: 0.24s
Val loss: 0.4575 score: 0.9457 time: 0.17s
Test loss: 0.4795 score: 0.9225 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.8286;  Loss pred: 0.8286; Loss self: 0.0000; time: 0.25s
Val loss: 0.4508 score: 0.9457 time: 0.19s
Test loss: 0.4731 score: 0.9225 time: 0.18s
Epoch 109/1000, LR 0.000278
Train loss: 0.8236;  Loss pred: 0.8236; Loss self: 0.0000; time: 0.24s
Val loss: 0.4438 score: 0.9457 time: 0.17s
Test loss: 0.4665 score: 0.9225 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.8171;  Loss pred: 0.8171; Loss self: 0.0000; time: 0.24s
Val loss: 0.4370 score: 0.9457 time: 0.17s
Test loss: 0.4601 score: 0.9225 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.8124;  Loss pred: 0.8124; Loss self: 0.0000; time: 0.28s
Val loss: 0.4303 score: 0.9457 time: 0.17s
Test loss: 0.4536 score: 0.9225 time: 0.25s
Epoch 112/1000, LR 0.000278
Train loss: 0.8086;  Loss pred: 0.8086; Loss self: 0.0000; time: 0.29s
Val loss: 0.4236 score: 0.9457 time: 0.17s
Test loss: 0.4471 score: 0.9302 time: 0.16s
Epoch 113/1000, LR 0.000278
Train loss: 0.8046;  Loss pred: 0.8046; Loss self: 0.0000; time: 0.25s
Val loss: 0.4169 score: 0.9457 time: 0.17s
Test loss: 0.4407 score: 0.9302 time: 0.17s
Epoch 114/1000, LR 0.000277
Train loss: 0.7976;  Loss pred: 0.7976; Loss self: 0.0000; time: 0.24s
Val loss: 0.4102 score: 0.9457 time: 0.17s
Test loss: 0.4343 score: 0.9302 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.7956;  Loss pred: 0.7956; Loss self: 0.0000; time: 0.25s
Val loss: 0.4038 score: 0.9457 time: 0.17s
Test loss: 0.4279 score: 0.9302 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7916;  Loss pred: 0.7916; Loss self: 0.0000; time: 0.24s
Val loss: 0.3971 score: 0.9457 time: 0.18s
Test loss: 0.4216 score: 0.9302 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7837;  Loss pred: 0.7837; Loss self: 0.0000; time: 0.37s
Val loss: 0.3907 score: 0.9457 time: 0.25s
Test loss: 0.4153 score: 0.9302 time: 0.19s
Epoch 118/1000, LR 0.000277
Train loss: 0.7810;  Loss pred: 0.7810; Loss self: 0.0000; time: 0.24s
Val loss: 0.3847 score: 0.9457 time: 0.17s
Test loss: 0.4092 score: 0.9302 time: 0.17s
Epoch 119/1000, LR 0.000277
Train loss: 0.7773;  Loss pred: 0.7773; Loss self: 0.0000; time: 0.24s
Val loss: 0.3784 score: 0.9457 time: 0.17s
Test loss: 0.4031 score: 0.9302 time: 0.17s
Epoch 120/1000, LR 0.000277
Train loss: 0.7711;  Loss pred: 0.7711; Loss self: 0.0000; time: 0.24s
Val loss: 0.3721 score: 0.9457 time: 0.18s
Test loss: 0.3969 score: 0.9302 time: 0.25s
Epoch 121/1000, LR 0.000276
Train loss: 0.7669;  Loss pred: 0.7669; Loss self: 0.0000; time: 0.37s
Val loss: 0.3661 score: 0.9457 time: 0.26s
Test loss: 0.3909 score: 0.9302 time: 0.25s
Epoch 122/1000, LR 0.000276
Train loss: 0.7629;  Loss pred: 0.7629; Loss self: 0.0000; time: 0.25s
Val loss: 0.3602 score: 0.9457 time: 0.17s
Test loss: 0.3850 score: 0.9302 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.7577;  Loss pred: 0.7577; Loss self: 0.0000; time: 0.24s
Val loss: 0.3543 score: 0.9457 time: 0.20s
Test loss: 0.3792 score: 0.9302 time: 0.17s
Epoch 124/1000, LR 0.000276
Train loss: 0.7547;  Loss pred: 0.7547; Loss self: 0.0000; time: 0.24s
Val loss: 0.3485 score: 0.9457 time: 0.18s
Test loss: 0.3734 score: 0.9302 time: 0.18s
Epoch 125/1000, LR 0.000276
Train loss: 0.7498;  Loss pred: 0.7498; Loss self: 0.0000; time: 0.25s
Val loss: 0.3428 score: 0.9457 time: 0.18s
Test loss: 0.3677 score: 0.9380 time: 0.17s
Epoch 126/1000, LR 0.000276
Train loss: 0.7464;  Loss pred: 0.7464; Loss self: 0.0000; time: 0.25s
Val loss: 0.3371 score: 0.9457 time: 0.17s
Test loss: 0.3621 score: 0.9302 time: 0.25s
Epoch 127/1000, LR 0.000275
Train loss: 0.7439;  Loss pred: 0.7439; Loss self: 0.0000; time: 0.24s
Val loss: 0.3316 score: 0.9457 time: 0.17s
Test loss: 0.3566 score: 0.9380 time: 0.18s
Epoch 128/1000, LR 0.000275
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.24s
Val loss: 0.3265 score: 0.9457 time: 0.18s
Test loss: 0.3512 score: 0.9380 time: 0.18s
Epoch 129/1000, LR 0.000275
Train loss: 0.7361;  Loss pred: 0.7361; Loss self: 0.0000; time: 0.25s
Val loss: 0.3215 score: 0.9457 time: 0.17s
Test loss: 0.3460 score: 0.9380 time: 0.16s
Epoch 130/1000, LR 0.000275
Train loss: 0.7337;  Loss pred: 0.7337; Loss self: 0.0000; time: 0.24s
Val loss: 0.3161 score: 0.9457 time: 0.18s
Test loss: 0.3407 score: 0.9380 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.7278;  Loss pred: 0.7278; Loss self: 0.0000; time: 0.24s
Val loss: 0.3111 score: 0.9457 time: 0.17s
Test loss: 0.3356 score: 0.9380 time: 0.16s
Epoch 132/1000, LR 0.000275
Train loss: 0.7240;  Loss pred: 0.7240; Loss self: 0.0000; time: 0.25s
Val loss: 0.3063 score: 0.9457 time: 0.17s
Test loss: 0.3307 score: 0.9380 time: 0.17s
Epoch 133/1000, LR 0.000274
Train loss: 0.7214;  Loss pred: 0.7214; Loss self: 0.0000; time: 0.24s
Val loss: 0.3018 score: 0.9457 time: 0.17s
Test loss: 0.3259 score: 0.9380 time: 0.19s
Epoch 134/1000, LR 0.000274
Train loss: 0.7150;  Loss pred: 0.7150; Loss self: 0.0000; time: 0.25s
Val loss: 0.2971 score: 0.9457 time: 0.24s
Test loss: 0.3211 score: 0.9457 time: 0.21s
Epoch 135/1000, LR 0.000274
Train loss: 0.7134;  Loss pred: 0.7134; Loss self: 0.0000; time: 0.23s
Val loss: 0.2924 score: 0.9457 time: 0.17s
Test loss: 0.3164 score: 0.9457 time: 0.19s
Epoch 136/1000, LR 0.000274
Train loss: 0.7096;  Loss pred: 0.7096; Loss self: 0.0000; time: 0.32s
Val loss: 0.2877 score: 0.9457 time: 0.26s
Test loss: 0.3118 score: 0.9457 time: 0.25s
Epoch 137/1000, LR 0.000274
Train loss: 0.7059;  Loss pred: 0.7059; Loss self: 0.0000; time: 0.39s
Val loss: 0.2834 score: 0.9457 time: 0.25s
Test loss: 0.3074 score: 0.9457 time: 0.24s
Epoch 138/1000, LR 0.000274
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.40s
Val loss: 0.2792 score: 0.9457 time: 0.25s
Test loss: 0.3031 score: 0.9457 time: 0.25s
Epoch 139/1000, LR 0.000273
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.38s
Val loss: 0.2748 score: 0.9457 time: 0.22s
Test loss: 0.2989 score: 0.9457 time: 0.16s
Epoch 140/1000, LR 0.000273
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.24s
Val loss: 0.2710 score: 0.9457 time: 0.17s
Test loss: 0.2947 score: 0.9457 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.27s
Val loss: 0.2672 score: 0.9457 time: 0.20s
Test loss: 0.2907 score: 0.9457 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.25s
Val loss: 0.2634 score: 0.9457 time: 0.18s
Test loss: 0.2868 score: 0.9457 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.24s
Val loss: 0.2598 score: 0.9457 time: 0.18s
Test loss: 0.2830 score: 0.9612 time: 0.17s
Epoch 144/1000, LR 0.000272
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.25s
Val loss: 0.2567 score: 0.9380 time: 0.18s
Test loss: 0.2794 score: 0.9535 time: 0.17s
Epoch 145/1000, LR 0.000272
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.24s
Val loss: 0.2535 score: 0.9380 time: 0.19s
Test loss: 0.2759 score: 0.9535 time: 0.17s
Epoch 146/1000, LR 0.000272
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.32s
Val loss: 0.2499 score: 0.9380 time: 0.18s
Test loss: 0.2724 score: 0.9535 time: 0.17s
Epoch 147/1000, LR 0.000272
Train loss: 0.6738;  Loss pred: 0.6738; Loss self: 0.0000; time: 0.24s
Val loss: 0.2462 score: 0.9457 time: 0.17s
Test loss: 0.2690 score: 0.9535 time: 0.24s
Epoch 148/1000, LR 0.000272
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.37s
Val loss: 0.2428 score: 0.9457 time: 0.25s
Test loss: 0.2657 score: 0.9535 time: 0.25s
Epoch 149/1000, LR 0.000272
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.38s
Val loss: 0.2395 score: 0.9457 time: 0.25s
Test loss: 0.2626 score: 0.9535 time: 0.24s
Epoch 150/1000, LR 0.000271
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.38s
Val loss: 0.2366 score: 0.9457 time: 0.21s
Test loss: 0.2595 score: 0.9535 time: 0.17s
Epoch 151/1000, LR 0.000271
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.24s
Val loss: 0.2338 score: 0.9457 time: 0.18s
Test loss: 0.2564 score: 0.9535 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.25s
Val loss: 0.2311 score: 0.9457 time: 0.19s
Test loss: 0.2536 score: 0.9535 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.25s
Val loss: 0.2285 score: 0.9457 time: 0.18s
Test loss: 0.2507 score: 0.9535 time: 0.17s
Epoch 154/1000, LR 0.000271
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.29s
Val loss: 0.2262 score: 0.9380 time: 0.20s
Test loss: 0.2480 score: 0.9535 time: 0.26s
Epoch 155/1000, LR 0.000270
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.39s
Val loss: 0.2239 score: 0.9380 time: 0.26s
Test loss: 0.2454 score: 0.9535 time: 0.25s
Epoch 156/1000, LR 0.000270
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.26s
Val loss: 0.2215 score: 0.9380 time: 0.18s
Test loss: 0.2429 score: 0.9535 time: 0.17s
Epoch 157/1000, LR 0.000270
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.25s
Val loss: 0.2193 score: 0.9380 time: 0.22s
Test loss: 0.2405 score: 0.9535 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 0.26s
Val loss: 0.2168 score: 0.9380 time: 0.19s
Test loss: 0.2380 score: 0.9535 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.26s
Val loss: 0.2144 score: 0.9380 time: 0.18s
Test loss: 0.2357 score: 0.9535 time: 0.18s
Epoch 160/1000, LR 0.000269
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.29s
Val loss: 0.2120 score: 0.9380 time: 0.19s
Test loss: 0.2335 score: 0.9535 time: 0.27s
Epoch 161/1000, LR 0.000269
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.27s
Val loss: 0.2097 score: 0.9380 time: 0.19s
Test loss: 0.2314 score: 0.9535 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.26s
Val loss: 0.2075 score: 0.9380 time: 0.18s
Test loss: 0.2294 score: 0.9535 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.26s
Val loss: 0.2055 score: 0.9380 time: 0.19s
Test loss: 0.2275 score: 0.9535 time: 0.18s
Epoch 164/1000, LR 0.000269
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.26s
Val loss: 0.2036 score: 0.9380 time: 0.20s
Test loss: 0.2256 score: 0.9535 time: 0.19s
Epoch 165/1000, LR 0.000268
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.27s
Val loss: 0.2019 score: 0.9380 time: 0.20s
Test loss: 0.2237 score: 0.9535 time: 0.19s
Epoch 166/1000, LR 0.000268
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.29s
Val loss: 0.2004 score: 0.9380 time: 0.19s
Test loss: 0.2219 score: 0.9535 time: 0.18s
Epoch 167/1000, LR 0.000268
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.33s
Val loss: 0.1988 score: 0.9380 time: 0.19s
Test loss: 0.2203 score: 0.9535 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.30s
Val loss: 0.1971 score: 0.9380 time: 0.20s
Test loss: 0.2186 score: 0.9535 time: 0.19s
Epoch 169/1000, LR 0.000267
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.25s
Val loss: 0.1951 score: 0.9380 time: 0.18s
Test loss: 0.2171 score: 0.9535 time: 0.18s
Epoch 170/1000, LR 0.000267
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.25s
Val loss: 0.1938 score: 0.9380 time: 0.19s
Test loss: 0.2156 score: 0.9535 time: 0.26s
Epoch 171/1000, LR 0.000267
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.26s
Val loss: 0.1924 score: 0.9380 time: 0.19s
Test loss: 0.2141 score: 0.9535 time: 0.19s
Epoch 172/1000, LR 0.000267
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.26s
Val loss: 0.1911 score: 0.9380 time: 0.19s
Test loss: 0.2127 score: 0.9535 time: 0.24s
Epoch 173/1000, LR 0.000267
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.28s
Val loss: 0.1898 score: 0.9380 time: 0.26s
Test loss: 0.2114 score: 0.9535 time: 0.20s
Epoch 174/1000, LR 0.000266
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.25s
Val loss: 0.1886 score: 0.9380 time: 0.18s
Test loss: 0.2101 score: 0.9535 time: 0.18s
Epoch 175/1000, LR 0.000266
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.26s
Val loss: 0.1877 score: 0.9380 time: 0.22s
Test loss: 0.2089 score: 0.9535 time: 0.22s
Epoch 176/1000, LR 0.000266
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.30s
Val loss: 0.1864 score: 0.9380 time: 0.18s
Test loss: 0.2077 score: 0.9535 time: 0.18s
Epoch 177/1000, LR 0.000266
Train loss: 0.6127;  Loss pred: 0.6127; Loss self: 0.0000; time: 0.26s
Val loss: 0.1850 score: 0.9380 time: 0.21s
Test loss: 0.2065 score: 0.9535 time: 0.19s
Epoch 178/1000, LR 0.000265
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.27s
Val loss: 0.1836 score: 0.9380 time: 0.20s
Test loss: 0.2054 score: 0.9535 time: 0.19s
Epoch 179/1000, LR 0.000265
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.40s
Val loss: 0.1823 score: 0.9380 time: 0.28s
Test loss: 0.2044 score: 0.9535 time: 0.27s
Epoch 180/1000, LR 0.000265
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.39s
Val loss: 0.1810 score: 0.9380 time: 0.20s
Test loss: 0.2034 score: 0.9535 time: 0.19s
Epoch 181/1000, LR 0.000265
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.28s
Val loss: 0.1801 score: 0.9380 time: 0.20s
Test loss: 0.2024 score: 0.9535 time: 0.20s
Epoch 182/1000, LR 0.000265
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.27s
Val loss: 0.1792 score: 0.9380 time: 0.20s
Test loss: 0.2015 score: 0.9535 time: 0.20s
Epoch 183/1000, LR 0.000264
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.28s
Val loss: 0.1782 score: 0.9380 time: 0.20s
Test loss: 0.2006 score: 0.9535 time: 0.20s
Epoch 184/1000, LR 0.000264
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.31s
Val loss: 0.1775 score: 0.9380 time: 0.21s
Test loss: 0.1997 score: 0.9535 time: 0.27s
Epoch 185/1000, LR 0.000264
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 0.39s
Val loss: 0.1768 score: 0.9380 time: 0.24s
Test loss: 0.1988 score: 0.9535 time: 0.19s
Epoch 186/1000, LR 0.000264
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 0.26s
Val loss: 0.1762 score: 0.9380 time: 0.18s
Test loss: 0.1980 score: 0.9535 time: 0.18s
Epoch 187/1000, LR 0.000263
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 0.26s
Val loss: 0.1753 score: 0.9380 time: 0.20s
Test loss: 0.1973 score: 0.9535 time: 0.27s
Epoch 188/1000, LR 0.000263
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.40s
Val loss: 0.1746 score: 0.9380 time: 0.27s
Test loss: 0.1966 score: 0.9535 time: 0.26s
Epoch 189/1000, LR 0.000263
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.40s
Val loss: 0.1738 score: 0.9380 time: 0.27s
Test loss: 0.1959 score: 0.9535 time: 0.27s
Epoch 190/1000, LR 0.000263
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.35s
Val loss: 0.1731 score: 0.9380 time: 0.19s
Test loss: 0.1952 score: 0.9535 time: 0.18s
Epoch 191/1000, LR 0.000262
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.26s
Val loss: 0.1725 score: 0.9380 time: 0.19s
Test loss: 0.1945 score: 0.9535 time: 0.19s
Epoch 192/1000, LR 0.000262
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.28s
Val loss: 0.1719 score: 0.9380 time: 0.21s
Test loss: 0.1939 score: 0.9535 time: 0.27s
Epoch 193/1000, LR 0.000262
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.40s
Val loss: 0.1713 score: 0.9380 time: 0.27s
Test loss: 0.1933 score: 0.9535 time: 0.27s
Epoch 194/1000, LR 0.000262
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 0.40s
Val loss: 0.1708 score: 0.9380 time: 0.21s
Test loss: 0.1927 score: 0.9535 time: 0.19s
Epoch 195/1000, LR 0.000261
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 0.28s
Val loss: 0.1701 score: 0.9380 time: 0.20s
Test loss: 0.1922 score: 0.9535 time: 0.19s
Epoch 196/1000, LR 0.000261
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.25s
Val loss: 0.1693 score: 0.9380 time: 0.18s
Test loss: 0.1917 score: 0.9535 time: 0.25s
Epoch 197/1000, LR 0.000261
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.37s
Val loss: 0.1684 score: 0.9380 time: 0.21s
Test loss: 0.1913 score: 0.9535 time: 0.17s
Epoch 198/1000, LR 0.000261
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 0.26s
Val loss: 0.1679 score: 0.9380 time: 0.18s
Test loss: 0.1908 score: 0.9535 time: 0.17s
Epoch 199/1000, LR 0.000260
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.27s
Val loss: 0.1678 score: 0.9380 time: 0.20s
Test loss: 0.1903 score: 0.9535 time: 0.19s
Epoch 200/1000, LR 0.000260
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.25s
Val loss: 0.1676 score: 0.9380 time: 0.18s
Test loss: 0.1899 score: 0.9457 time: 0.17s
Epoch 201/1000, LR 0.000260
Train loss: 0.5838;  Loss pred: 0.5838; Loss self: 0.0000; time: 0.25s
Val loss: 0.1675 score: 0.9380 time: 0.19s
Test loss: 0.1895 score: 0.9457 time: 0.24s
Epoch 202/1000, LR 0.000260
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.39s
Val loss: 0.1669 score: 0.9380 time: 0.26s
Test loss: 0.1891 score: 0.9457 time: 0.25s
Epoch 203/1000, LR 0.000259
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.39s
Val loss: 0.1665 score: 0.9380 time: 0.26s
Test loss: 0.1887 score: 0.9457 time: 0.25s
Epoch 204/1000, LR 0.000259
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.33s
Val loss: 0.1660 score: 0.9380 time: 0.18s
Test loss: 0.1884 score: 0.9457 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.26s
Val loss: 0.1655 score: 0.9380 time: 0.18s
Test loss: 0.1881 score: 0.9457 time: 0.17s
Epoch 206/1000, LR 0.000259
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.25s
Val loss: 0.1648 score: 0.9380 time: 0.20s
Test loss: 0.1879 score: 0.9535 time: 0.25s
Epoch 207/1000, LR 0.000258
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 0.39s
Val loss: 0.1645 score: 0.9380 time: 0.26s
Test loss: 0.1876 score: 0.9535 time: 0.26s
Epoch 208/1000, LR 0.000258
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.40s
Val loss: 0.1641 score: 0.9380 time: 0.26s
Test loss: 0.1874 score: 0.9535 time: 0.24s
Epoch 209/1000, LR 0.000258
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.26s
Val loss: 0.1640 score: 0.9380 time: 0.18s
Test loss: 0.1871 score: 0.9457 time: 0.17s
Epoch 210/1000, LR 0.000258
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.26s
Val loss: 0.1639 score: 0.9380 time: 0.18s
Test loss: 0.1868 score: 0.9457 time: 0.18s
Epoch 211/1000, LR 0.000257
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.26s
Val loss: 0.1639 score: 0.9380 time: 0.19s
Test loss: 0.1866 score: 0.9457 time: 0.18s
Epoch 212/1000, LR 0.000257
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.26s
Val loss: 0.1642 score: 0.9380 time: 0.18s
Test loss: 0.1864 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.26s
Val loss: 0.1647 score: 0.9380 time: 0.18s
Test loss: 0.1863 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.26s
Val loss: 0.1643 score: 0.9380 time: 0.18s
Test loss: 0.1861 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.34s
Val loss: 0.1635 score: 0.9380 time: 0.18s
Test loss: 0.1859 score: 0.9457 time: 0.18s
Epoch 216/1000, LR 0.000256
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.26s
Val loss: 0.1630 score: 0.9380 time: 0.18s
Test loss: 0.1857 score: 0.9457 time: 0.18s
Epoch 217/1000, LR 0.000256
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.26s
Val loss: 0.1625 score: 0.9380 time: 0.18s
Test loss: 0.1856 score: 0.9457 time: 0.17s
Epoch 218/1000, LR 0.000255
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.27s
Val loss: 0.1621 score: 0.9380 time: 0.18s
Test loss: 0.1855 score: 0.9457 time: 0.18s
Epoch 219/1000, LR 0.000255
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.27s
Val loss: 0.1620 score: 0.9380 time: 0.22s
Test loss: 0.1854 score: 0.9457 time: 0.19s
Epoch 220/1000, LR 0.000255
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.38s
Val loss: 0.1620 score: 0.9380 time: 0.22s
Test loss: 0.1852 score: 0.9457 time: 0.19s
Epoch 221/1000, LR 0.000255
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 0.27s
Val loss: 0.1621 score: 0.9380 time: 0.19s
Test loss: 0.1851 score: 0.9457 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 0.26s
Val loss: 0.1621 score: 0.9380 time: 0.18s
Test loss: 0.1851 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.26s
Val loss: 0.1620 score: 0.9380 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.18s
Epoch 224/1000, LR 0.000254
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.26s
Val loss: 0.1618 score: 0.9380 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.17s
Epoch 225/1000, LR 0.000253
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 0.27s
Val loss: 0.1617 score: 0.9380 time: 0.24s
Test loss: 0.1849 score: 0.9380 time: 0.24s
Epoch 226/1000, LR 0.000253
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.26s
Val loss: 0.1618 score: 0.9380 time: 0.18s
Test loss: 0.1849 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.26s
Val loss: 0.1618 score: 0.9302 time: 0.19s
Test loss: 0.1848 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.26s
Val loss: 0.1619 score: 0.9302 time: 0.18s
Test loss: 0.1848 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.27s
Val loss: 0.1616 score: 0.9302 time: 0.19s
Test loss: 0.1848 score: 0.9380 time: 0.23s
Epoch 230/1000, LR 0.000252
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.36s
Val loss: 0.1613 score: 0.9302 time: 0.18s
Test loss: 0.1849 score: 0.9380 time: 0.18s
Epoch 231/1000, LR 0.000252
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.26s
Val loss: 0.1610 score: 0.9380 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.18s
Epoch 232/1000, LR 0.000251
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.25s
Val loss: 0.1608 score: 0.9380 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.17s
Epoch 233/1000, LR 0.000251
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.26s
Val loss: 0.1608 score: 0.9380 time: 0.19s
Test loss: 0.1850 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.28s
Val loss: 0.1607 score: 0.9380 time: 0.19s
Test loss: 0.1851 score: 0.9380 time: 0.26s
Epoch 235/1000, LR 0.000250
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.34s
Val loss: 0.1609 score: 0.9302 time: 0.19s
Test loss: 0.1851 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.25s
Val loss: 0.1611 score: 0.9302 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.25s
Val loss: 0.1613 score: 0.9302 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 0.25s
Val loss: 0.1614 score: 0.9302 time: 0.18s
Test loss: 0.1850 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.26s
Val loss: 0.1616 score: 0.9302 time: 0.18s
Test loss: 0.1851 score: 0.9380 time: 0.27s
     INFO: Early stopping counter 5 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.26s
Val loss: 0.1615 score: 0.9302 time: 0.18s
Test loss: 0.1851 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.26s
Val loss: 0.1615 score: 0.9302 time: 0.19s
Test loss: 0.1852 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.25s
Val loss: 0.1612 score: 0.9302 time: 0.17s
Test loss: 0.1853 score: 0.9380 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.24s
Val loss: 0.1612 score: 0.9302 time: 0.17s
Test loss: 0.1854 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.25s
Val loss: 0.1610 score: 0.9302 time: 0.18s
Test loss: 0.1856 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.26s
Val loss: 0.1609 score: 0.9302 time: 0.21s
Test loss: 0.1857 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.27s
Val loss: 0.1607 score: 0.9302 time: 0.19s
Test loss: 0.1859 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.36s
Val loss: 0.1607 score: 0.9302 time: 0.20s
Test loss: 0.1860 score: 0.9380 time: 0.19s
Epoch 248/1000, LR 0.000247
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.24s
Val loss: 0.1609 score: 0.9302 time: 0.19s
Test loss: 0.1861 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.26s
Val loss: 0.1610 score: 0.9302 time: 0.18s
Test loss: 0.1862 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.25s
Val loss: 0.1614 score: 0.9302 time: 0.19s
Test loss: 0.1863 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5441;  Loss pred: 0.5441; Loss self: 0.0000; time: 0.37s
Val loss: 0.1616 score: 0.9302 time: 0.26s
Test loss: 0.1864 score: 0.9380 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.25s
Val loss: 0.1616 score: 0.9302 time: 0.20s
Test loss: 0.1866 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.25s
Val loss: 0.1617 score: 0.9302 time: 0.19s
Test loss: 0.1867 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.34s
Val loss: 0.1618 score: 0.9302 time: 0.20s
Test loss: 0.1869 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.26s
Val loss: 0.1620 score: 0.9302 time: 0.20s
Test loss: 0.1870 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.29s
Val loss: 0.1620 score: 0.9302 time: 0.32s
Test loss: 0.1872 score: 0.9380 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.33s
Val loss: 0.1620 score: 0.9302 time: 0.18s
Test loss: 0.1873 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.25s
Val loss: 0.1619 score: 0.9302 time: 0.18s
Test loss: 0.1875 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.34s
Val loss: 0.1619 score: 0.9302 time: 0.18s
Test loss: 0.1877 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.25s
Val loss: 0.1615 score: 0.9302 time: 0.18s
Test loss: 0.1880 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.25s
Val loss: 0.1614 score: 0.9302 time: 0.17s
Test loss: 0.1882 score: 0.9380 time: 0.25s
     INFO: Early stopping counter 14 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.25s
Val loss: 0.1616 score: 0.9302 time: 0.17s
Test loss: 0.1884 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.25s
Val loss: 0.1616 score: 0.9302 time: 0.26s
Test loss: 0.1887 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.24s
Val loss: 0.1619 score: 0.9302 time: 0.17s
Test loss: 0.1888 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.24s
Val loss: 0.1622 score: 0.9302 time: 0.18s
Test loss: 0.1890 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.24s
Val loss: 0.1624 score: 0.9302 time: 0.17s
Test loss: 0.1892 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.26s
Val loss: 0.1624 score: 0.9302 time: 0.25s
Test loss: 0.1895 score: 0.9380 time: 0.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 246,   Train_Loss: 0.5488,   Val_Loss: 0.1607,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1607,   Test_Precision: 0.9516,   Test_Recall: 0.9219,   Test_accuracy: 0.9365,   Test_Score: 0.9380,   Test_loss: 0.1860


[0.20760776498354971, 0.16605692077428102, 0.17056229896843433, 0.17562605999410152, 0.183626146055758, 0.19163804687559605, 0.17879367503337562, 0.18349444796331227, 0.18030035379342735, 0.1825567870400846, 0.23094463488087058, 0.24746026215143502, 0.169254096923396, 0.17146165180020034, 0.16945469309575856, 0.16782164690084755, 0.16591966804116964, 0.2604686859995127, 0.1679470408707857, 0.18327770614996552, 0.17623785906471312, 0.16604165500029922, 0.2821390319149941, 0.22657390194945037, 0.1755640609189868, 0.1777474139817059, 0.18578408285975456, 0.18337985407561064, 0.2303225041832775, 0.1886288640089333, 0.1893351359758526, 0.18791426299139857, 0.18767704395577312, 0.20001850090920925, 0.235397047130391, 0.18361179088242352, 0.18147828592918813, 0.17513450188562274, 0.18334317998960614, 0.2581198059488088, 0.2517814820166677, 0.1826078798621893, 0.1942509119398892, 0.17257608915679157, 0.1775593189522624, 0.17831494892016053, 0.17423899401910603, 0.1748086200095713, 0.16898676799610257, 0.22426744806580245, 0.17320013791322708, 0.1734763530548662, 0.1705039970111102, 0.17772014415822923, 0.24498232803307474, 0.17179809999652207, 0.174724510172382, 0.1709221708588302, 0.1720204190351069, 0.18837493611499667, 0.2541725051123649, 0.2342766630463302, 0.16764483996666968, 0.16863659117370844, 0.17866831016726792, 0.17894160095602274, 0.18777737906202674, 0.19234586600214243, 0.227551860967651, 0.17522173700854182, 0.17584308120422065, 0.2531487508676946, 0.2624670311342925, 0.18280101101845503, 0.18204256193712354, 0.1826819470152259, 0.2561954839620739, 0.185029179090634, 0.18078141589649022, 0.25539965205825865, 0.2519312258809805, 0.27073586406186223, 0.17641372000798583, 0.1630205528344959, 0.18329317588359118, 0.19258446781896055, 0.1892592350486666, 0.1946041330229491, 0.1807088078930974, 0.1757434920873493, 0.16891504591330886, 0.18527165101841092, 0.25442420202307403, 0.2538715498521924, 0.18155026203021407, 0.1819934321101755, 0.1759739718399942, 0.17703308607451618, 0.172641467070207, 0.17946191085502505, 0.1737636341713369, 0.1749967080540955, 0.16470035980455577, 0.16791766905225813, 0.1856831240002066, 0.1856447970494628, 0.18546071089804173, 0.170321024954319, 0.1694547429215163, 0.263682221993804, 0.174704825039953, 0.1775144108105451, 0.2593510551378131, 0.2538635029923171, 0.26320866611786187, 0.17723405407741666, 0.1913991430774331, 0.2169508400838822, 0.18665050785057247, 0.1831160499714315, 0.1730460491962731, 0.17507549305446446, 0.1977320108562708, 0.17287560901604593, 0.1839523292146623, 0.18310444406233728, 0.19459805195219815, 0.20188505691476166, 0.18010040605440736, 0.17531064618378878, 0.18319220887497067, 0.1829089478123933, 0.1840755131561309, 0.1740313118789345, 0.1928228260949254, 0.18387638707645237, 0.1781983389519155, 0.17489577503874898, 0.1752992409747094, 0.17691366118378937, 0.18518893886357546, 0.17172211199067533, 0.19687853707000613, 0.25599591387435794, 0.18202265002764761, 0.1760478070937097, 0.17624175106175244, 0.17886801506392658, 0.22133147390559316, 0.20022541796788573, 0.17252260306850076, 0.1869047051295638, 0.18561848811805248, 0.25203357287682593, 0.1903204950504005, 0.18787872092798352, 0.19091328885406256, 0.19172335509210825, 0.18131729098968208, 0.22471544099971652, 0.18529939791187644, 0.18669098196551204, 0.26057369401678443, 0.27706012199632823, 0.27476208889856935, 0.26861713686957955, 0.1715617161244154, 0.1818888820707798, 0.1848716561216861, 0.18424214399419725, 0.24422782519832253, 0.25812930683605373, 0.17129328101873398, 0.17182862502522767, 0.17106351512484252, 0.16721125598996878, 0.17334144096821547, 0.21029490092769265, 0.1921279348898679, 0.18669158197008073, 0.1751058828085661, 0.17458516987971961, 0.17969605792313814, 0.19018812314607203, 0.18706377386115491, 0.19380989810451865, 0.19117926503531635, 0.1846109121106565, 0.21767481393180788, 0.19135821191594005, 0.18483771686442196, 0.18729169899597764, 0.190573571017012, 0.19275345280766487, 0.23504659603349864, 0.18853032100014389, 0.16704799211584032, 0.17499705497175455, 0.17892867815680802, 0.26982560590840876, 0.1906441890168935, 0.17672756290994585, 0.17488995310850441, 0.18145604082383215, 0.17640410596504807, 0.2325800210237503, 0.17679447308182716, 0.1807174808345735, 0.2581492660101503, 0.27065136982128024, 0.18201127089560032, 0.18048020685091615, 0.19234415097162127, 0.17832101392559707, 0.19343808316625655, 0.25188571494072676, 0.233565263915807, 0.1765216630883515, 0.18203331297263503, 0.2514482119586319, 0.25591309601441026, 0.2535775820724666, 0.25424993596971035, 0.17430246993899345, 0.17443433916196227, 0.19006069097667933, 0.18727849097922444, 0.176635738927871, 0.1743895378895104, 0.17375703691504896, 0.18200277886353433, 0.17512308689765632, 0.17285614204593003, 0.17394361505284905, 0.18323494517244399, 0.1851992520969361, 0.18197311414405704, 0.17845230596140027, 0.1829737580846995, 0.1903204470872879, 0.22180069400928915, 0.19656386598944664, 0.18858051905408502, 0.18040824797935784, 0.1770752859301865, 0.23238638206385076, 0.17725733621045947, 0.17719315993599594, 0.17750552808865905, 0.1721761249937117, 0.17533176904544234, 0.1781147848814726, 0.18026747507974505, 0.17792277014814317, 0.17840653890743852, 0.17489333800040185, 0.18553963000886142, 0.1979256549384445, 0.17673884192481637, 0.17396782687865198, 0.17617574892938137, 0.18675140500999987, 0.182097113924101, 0.17962827812880278, 0.18008308298885822, 0.17333669099025428, 0.20666922186501324, 0.1728289918974042, 0.1768438220024109, 0.16937229200266302, 0.16902599507011473, 0.18150424701161683, 0.16999270487576723, 0.18201163318008184, 0.19065504800528288, 0.17562038311734796, 0.18519056285731494, 0.26217527384869754, 0.2749720369465649, 0.1723147730808705, 0.23983300803229213, 0.17482454795390368, 0.16836791788227856, 0.21998040191829205, 0.24796353606507182, 0.2622422210406512, 0.17585982894524932, 0.170207594987005, 0.16902252798900008, 0.16960572800599039, 0.2449571460019797, 0.18008339102379978, 0.18157325591892004, 0.1959526299033314, 0.2579661109484732, 0.18663215590640903, 0.18392242514528334, 0.1774893591646105, 0.16886833286844194, 0.16820145398378372, 0.17001055111177266, 0.1726017789915204, 0.16701702889986336, 0.17020741594024003, 0.17917330493219197, 0.18159364792518318, 0.19207064504735172, 0.1732495790347457, 0.17134076496586204, 0.18370019900612533, 0.17256067995913327, 0.17607944598421454, 0.25405735499225557, 0.16935505392029881, 0.1696208801586181, 0.17083327495492995, 0.17099002399481833, 0.17480947612784803, 0.19332399289123714, 0.17699351999908686, 0.17000415199436247, 0.25591849884949625, 0.2506128849927336, 0.17634902708232403, 0.17738017300143838, 0.18127540708519518, 0.17114142701029778, 0.25885300897061825, 0.18635550700128078, 0.1816740359645337, 0.16862304299138486, 0.17229706305079162, 0.16886398498900235, 0.17414654698222876, 0.1981652439571917, 0.21171443699859083, 0.1894701609853655, 0.25155637389980257, 0.24560700682923198, 0.25381532800383866, 0.16790473298169672, 0.17167749418877065, 0.18291741609573364, 0.17201759410090744, 0.17277813400141895, 0.17809282802045345, 0.1716849950607866, 0.16949958098120987, 0.2424674869980663, 0.24963090103119612, 0.24734934698790312, 0.1759705268777907, 0.17140902299433947, 0.18287807097658515, 0.17432033410295844, 0.26146151311695576, 0.2573076479602605, 0.175322940107435, 0.18466982897371054, 0.18426507990807295, 0.18436506809666753, 0.2735900869593024, 0.1833690379280597, 0.1793315268587321, 0.1834127618931234, 0.19779370189644396, 0.19816784583963454, 0.18166885594837368, 0.17929888586513698, 0.1948186159133911, 0.18695669109001756, 0.26214973302558064, 0.19676160207018256, 0.24533285293728113, 0.20691395411267877, 0.1799664821010083, 0.2222414780408144, 0.18053367198444903, 0.19861003779806197, 0.19866069802083075, 0.27492605708539486, 0.19803870003670454, 0.20590451592579484, 0.2039390979334712, 0.20430450094863772, 0.27628389396704733, 0.1975996200926602, 0.1800717180594802, 0.2710263270419091, 0.2684377860277891, 0.2704965409357101, 0.1848256061784923, 0.19544989708811045, 0.26960123097524047, 0.27289148489944637, 0.19525466090999544, 0.19217824307270348, 0.2556428511161357, 0.17868610308505595, 0.17655501002445817, 0.18999375705607235, 0.17690654704347253, 0.23976480006240308, 0.2538996930234134, 0.2580697510857135, 0.17947588488459587, 0.17676347913220525, 0.25461259204894304, 0.26009761495515704, 0.2441159849986434, 0.1790195421781391, 0.18140233401209116, 0.18160966783761978, 0.18507866701111197, 0.18639795598573983, 0.18185783503577113, 0.1804083869792521, 0.18854067684151232, 0.1778207328170538, 0.1837225779891014, 0.19516626512631774, 0.18977250810712576, 0.19297044002451003, 0.18083447683602571, 0.18197179702110589, 0.17911190120503306, 0.24398714303970337, 0.18637878890149295, 0.17905222601257265, 0.18395033408887684, 0.23923760605975986, 0.18375505786389112, 0.18074191198684275, 0.178380758035928, 0.1899072548840195, 0.26363762095570564, 0.18552728183567524, 0.1771739290561527, 0.17674743593670428, 0.18692478514276445, 0.27026362414471805, 0.1794868940487504, 0.19239011104218662, 0.16756133106537163, 0.169463629135862, 0.18631174601614475, 0.2086286989506334, 0.18740364396944642, 0.1958043109625578, 0.18795015383511782, 0.1743054601829499, 0.1843254161067307, 0.21176343387924135, 0.1900589179713279, 0.181353148072958, 0.19768848689273, 0.2045703891199082, 0.2231669679749757, 0.17577824601903558, 0.17418941692449152, 0.17416829406283796, 0.17365551693364978, 0.2580072549171746, 0.1759955349843949, 0.17314566485583782, 0.17057365411892533, 0.17762757185846567, 0.17965718894265592, 0.2560008999425918]
[0.0016093625192523235, 0.0012872629517386126, 0.0013221883640963902, 0.0013614423255356707, 0.0014234584965562636, 0.0014855662548495818, 0.0013859974808788807, 0.001422437581110948, 0.0013976771611893593, 0.0014151688917836015, 0.001790268487448609, 0.0019182966058250777, 0.0013120472629720619, 0.0013291600914744212, 0.0013136022720601438, 0.0013009429992313765, 0.0012861989770633305, 0.0020191371007714162, 0.0013019150455099667, 0.0014207574120152365, 0.0013661849539900242, 0.0012871446124054203, 0.002187124278410807, 0.0017563868368174447, 0.0013609617125502852, 0.0013778869300907434, 0.0014401866888353067, 0.0014215492564000824, 0.0017854457688626162, 0.001462239255883204, 0.0014677142323709506, 0.001456699713111617, 0.0014548608058587062, 0.0015505310148000717, 0.0018247833110883024, 0.001423347216142818, 0.001406808418055722, 0.001357631797562967, 0.0014212649611597375, 0.0020009287282853395, 0.0019517944342377343, 0.00141556496017201, 0.0015058210227898387, 0.0013377991407503222, 0.0013764288290873054, 0.0013822864257376785, 0.001350689876117101, 0.0013551055814695452, 0.0013099749457062214, 0.0017385073493473059, 0.0013426367280095122, 0.0013447779306578775, 0.0013217364109388387, 0.001377675536110304, 0.0018990878142098816, 0.001331768217027303, 0.0013544535672277674, 0.0013249780686731025, 0.0013334916204271853, 0.0014602708225968735, 0.001970329496995077, 0.0018160981631498466, 0.0012995724028424006, 0.0013072603966954143, 0.0013850256602113793, 0.0013871441934575407, 0.0014556385973800523, 0.0014910532248228095, 0.0017639679144779148, 0.0013583080388259055, 0.0013631246604978344, 0.001962393417579028, 0.002034628148327849, 0.0014170621009182561, 0.0014111826506753762, 0.001416139124149038, 0.0019860115035819684, 0.0014343347216328216, 0.0014014063247789939, 0.001979842264017509, 0.0019529552393874458, 0.002098727628386529, 0.0013675482171161691, 0.001263725215771286, 0.0014208773324309394, 0.0014929028513097718, 0.0014671258530904387, 0.0015085591707205356, 0.0014008434720395148, 0.001362352651839917, 0.0013094189605682856, 0.0014362143489799297, 0.0019722806358377833, 0.0019679965104821115, 0.0014073663723272408, 0.001410801799303686, 0.0013641393165891023, 0.0013723495044536138, 0.0013383059462806744, 0.001391177603527326, 0.001347004916056875, 0.0013565636283263218, 0.0012767469752291145, 0.0013016873569942492, 0.0014394040620171052, 0.0014391069538718046, 0.0014376799294421839, 0.0013203180229016977, 0.001313602658306328, 0.0020440482325101085, 0.001354300969301961, 0.0013760807039577137, 0.002010473295641962, 0.001967934131723388, 0.0020403772567276113, 0.0013739073959489663, 0.001483714287421962, 0.0016817894580145908, 0.0014469031616323447, 0.0014195042633444303, 0.0013414422418315743, 0.0013571743647632904, 0.0015328062857075254, 0.001340121000124387, 0.0014259870481756769, 0.0014194142950568782, 0.0015085120306371949, 0.0015650004411997028, 0.001396127178716336, 0.0013589972572386727, 0.001420094642441633, 0.0014178988202511109, 0.0014269419624506272, 0.001349079937046004, 0.0014947505898831428, 0.0014253983494298633, 0.001381382472495469, 0.0013557812018507674, 0.001358908844765189, 0.0013714237301068943, 0.001435573169485081, 0.0013311791627184134, 0.0015261902098450087, 0.0019844644486384337, 0.001411028294787966, 0.0013647116828969744, 0.0013662151245097088, 0.0013865737601854774, 0.0017157478597332803, 0.0015521350230068662, 0.0013373845199108586, 0.0014488736831749132, 0.0014389030086670734, 0.001953748626952139, 0.0014753526748093062, 0.0014564241932401823, 0.0014799479756128881, 0.0014862275588535523, 0.0014055603952688534, 0.0017419801627885, 0.0014364294411773368, 0.0014472169144613336, 0.0020199511164091816, 0.002147752883692467, 0.0021299386736323206, 0.002082303386585888, 0.001329935783910197, 0.001409991333882014, 0.0014331136133464038, 0.0014282336743736222, 0.0018932389550257562, 0.00200100237857406, 0.0013278548916180928, 0.0013320048451568036, 0.001326073760657694, 0.001296211286743944, 0.0013437321005288021, 0.0016301930304472299, 0.0014893638363555652, 0.0014472215656595405, 0.0013574099442524503, 0.001353373409920307, 0.0013929926970785902, 0.0014743265360160622, 0.00145010677411748, 0.0015024023108877415, 0.0014820098064753206, 0.0014310923419430736, 0.0016874016583861076, 0.0014833969915964344, 0.0014328505183288523, 0.0014518736356277336, 0.001477314504007845, 0.0014942128124625185, 0.001822066635918594, 0.0014614753565902626, 0.0012949456753165915, 0.0013565663176105004, 0.001387044016719442, 0.0020916713636310756, 0.0014778619303635156, 0.0013699811078290375, 0.0013557360706085613, 0.0014066359753785438, 0.0013674736896515354, 0.0018029458994089172, 0.0013704997913319935, 0.0014009107041439808, 0.0020011571008538786, 0.0020980726342734904, 0.001410940084462018, 0.0013990713709373344, 0.001491039930012568, 0.0013823334412836983, 0.0014995200245446244, 0.0019526024414009827, 0.0018105834412078063, 0.0013683849851810193, 0.0014111109532762406, 0.001949210945415751, 0.0019838224497241105, 0.001965717690484237, 0.0019709297361993052, 0.0013511819375115772, 0.0013522041795500951, 0.0014733386897417002, 0.0014517712479009647, 0.0013692692940145037, 0.0013518568828644216, 0.0013469537745352633, 0.00141087425475608, 0.0013575433092841574, 0.0013399700933793026, 0.0013484001166887524, 0.0014204259315693333, 0.0014356531170305125, 0.0014106442956903645, 0.001383351209003103, 0.001418401225462787, 0.001475352303002232, 0.0017193852248782104, 0.0015237508991429972, 0.0014618644887913568, 0.0013985135502275802, 0.0013726766351177249, 0.0018014448222003934, 0.001374087877600461, 0.0013735903871007437, 0.0013760118456485199, 0.0013346986433621063, 0.0013591610003522662, 0.0013807347665230434, 0.0013974222874398841, 0.0013792462802181642, 0.0013829964256390583, 0.0013557623100806345, 0.0014382917054950497, 0.0015343074026236006, 0.00137006854205284, 0.001348587805260868, 0.0013657034800727238, 0.0014476853101550377, 0.0014116055342953565, 0.0013924672723163007, 0.0013959928913864978, 0.0013436952789942192, 0.001602086991201653, 0.0013397596271116604, 0.0013708823411039603, 0.0013129635038966125, 0.001310279031551277, 0.0014070096667567196, 0.0013177729060136994, 0.0014109428928688514, 0.0014779461085680844, 0.0013613983187391314, 0.0014355857585838369, 0.002032366463943392, 0.002131566177880348, 0.0013357734347354303, 0.0018591706049014894, 0.00135522905390623, 0.0013051776580021593, 0.0017052744334751322, 0.0019221979539928047, 0.0020328854344236528, 0.0013632544879476692, 0.0013194387208294962, 0.001310252154953489, 0.001314773085317755, 0.0018988926046665095, 0.0013959952792542618, 0.0014075446195265118, 0.0015190126349095457, 0.001999737294174211, 0.0014467608984992948, 0.001425755233684367, 0.0013758865051520194, 0.0013090568439414103, 0.0013038872401843699, 0.0013179112489284702, 0.0013379982867559722, 0.0012947056503865378, 0.0013194373328700778, 0.0013889403483115655, 0.0014077026970944432, 0.001488919729049238, 0.0013430199925174085, 0.0013282229842314886, 0.00142403255043508, 0.0013376796896056842, 0.0013649569456140663, 0.001969436860405082, 0.0013128298753511537, 0.0013148905438652566, 0.0013242889531389918, 0.0013255040619753358, 0.0013551122180453336, 0.00149863560380804, 0.0013720427906905959, 0.001317861643367151, 0.0019838643321666376, 0.0019427355425793301, 0.0013670467215684034, 0.001375040100786344, 0.0014052357138387224, 0.0013266777287619983, 0.0020066124726404516, 0.001444616333343262, 0.0014083258601901836, 0.0013071553720262392, 0.0013356361476805551, 0.0013090231394496305, 0.001349973232420378, 0.0015361646818386953, 0.0016411971860355878, 0.0014687609378710505, 0.001950049410075989, 0.0019039302854979223, 0.0019675606822003, 0.001301587077377494, 0.0013308332882850439, 0.0014179644658584003, 0.0013334697217124607, 0.0013393653798559608, 0.0013805645582980889, 0.0013308914345797412, 0.0013139502401644176, 0.0018795929224656304, 0.001935123263807722, 0.001917436798355838, 0.0013641126114557417, 0.0013287521162351898, 0.0014176594649347686, 0.0013513204194027786, 0.002026833435015161, 0.0019946329299245, 0.0013590925589723642, 0.0014315490618117097, 0.0014284114721556042, 0.0014291865743927715, 0.002120853387281414, 0.0014214654102950365, 0.0013901668748738923, 0.0014218043557606465, 0.001533284510825147, 0.0015361848514700352, 0.0014082857050261525, 0.0013899138439157905, 0.0015102218287859775, 0.0014492766751164152, 0.0020321684730665165, 0.0015252837369781594, 0.001901805061529311, 0.0016039841404083626, 0.001395089008534948, 0.0017228021553551505, 0.001399485829336814, 0.0015396126961090076, 0.001540005411014192, 0.002131209744848022, 0.001535183721214764, 0.001596159038184456, 0.0015809232397943503, 0.0015837558213072692, 0.002141735612147654, 0.001531780000718296, 0.0013959047911587612, 0.0021009792793946444, 0.0020809130699828613, 0.0020968724103543417, 0.0014327566370425761, 0.0015151154813031818, 0.0020899320230638794, 0.0021154378674375688, 0.0015136020225581042, 0.0014897538222690193, 0.00198172752803206, 0.0013851635898066352, 0.0013686434885616912, 0.0014728198221400958, 0.0013713685817323452, 0.001858641860948861, 0.001968214674600104, 0.0020005407060908025, 0.0013912859293379524, 0.0013702595281566297, 0.0019737410236352173, 0.002016260581047729, 0.0018923719767336698, 0.0013877483889778225, 0.0014062196435045827, 0.001407826882462169, 0.0014347183489233487, 0.0014449453952382933, 0.0014097506591920243, 0.0013985146277461402, 0.0014615556344303282, 0.0013784552931554559, 0.0014242060309232667, 0.0015129167839249436, 0.001471104714008727, 0.0014958948839109303, 0.0014018176498916722, 0.0014106340854349293, 0.001388464350426613, 0.0018913732018581656, 0.0014447968131898678, 0.001388001752035447, 0.0014259715820843166, 0.0018545550857345726, 0.0014244578128983808, 0.0014011000929212616, 0.0013827965739219225, 0.0014721492626668178, 0.002043702488028726, 0.0014381959832222887, 0.0013734413105128116, 0.0013701351623000332, 0.0014490293421919726, 0.0020950668538350235, 0.0013913712716957394, 0.0014913962096293537, 0.0012989250470183847, 0.0013136715436888526, 0.001444277100900347, 0.0016172767360514218, 0.0014527414261197398, 0.0015178628756787427, 0.0014569779367063396, 0.001351205117697286, 0.0014288791946258195, 0.0016415770068158243, 0.0014733249455141698, 0.001405838357154713, 0.0015324688906413178, 0.001585816969921769, 0.0017299764959300442, 0.0013626220621630665, 0.001350305557554198, 0.0013501418144406044, 0.0013461667979352695, 0.00200005623966802, 0.0013643064727472474, 0.0013422144562468049, 0.001322276388518801, 0.0013769579213834548, 0.0013926913871523715, 0.001984503100330169]
[621.3640419963175, 776.8420575216373, 756.3218881323463, 734.5151397482385, 702.5143356264157, 673.1439925587521, 721.5020328650855, 703.018545965991, 715.4728057150521, 706.6294389354861, 558.5754354784764, 521.2958188860947, 762.1676659229413, 752.3548189674519, 761.265431150392, 768.6731859818765, 777.4846799234871, 495.26106950238676, 768.0992730276766, 703.84992648504, 731.9653148568506, 776.9134799322949, 457.2213887756817, 569.3506572914142, 734.7745280255647, 725.748955274685, 694.3544248480105, 703.4578615533803, 560.0842195487297, 683.8826108494757, 681.3315412119406, 686.4832820375361, 687.350979539082, 644.9403400866133, 548.0102727395064, 702.5692597410894, 710.8288429081545, 736.5767373709587, 703.5985740364769, 499.7679256956505, 512.3490376129421, 706.4317273567485, 664.089546410567, 747.4963688787667, 726.5177674773703, 723.439065435613, 740.3624012306603, 737.9498790902694, 763.3733784586921, 575.2060814557003, 744.8031020889183, 743.6172004330789, 756.5805040429301, 725.8603160098029, 526.5685938888779, 750.8814125570161, 738.3051174258808, 754.7294733726791, 749.910974078449, 684.804479090837, 507.529325183979, 550.6310288126699, 769.4838685500081, 764.9585365913869, 722.0082838374145, 720.9055876934032, 686.9837072195402, 670.6668704726055, 566.9037354888465, 736.210028517817, 733.6086192107949, 509.5818152680533, 491.4903004865268, 705.6853749401668, 708.625491903129, 706.1453094171824, 503.5217561410903, 697.1873335546235, 713.5689216742354, 505.090742921506, 512.0445055943295, 476.47917074822396, 731.2356430903474, 791.3112657087187, 703.790522359258, 669.835950224536, 681.6047838660481, 662.8841741238219, 713.855630525287, 734.0243355121422, 763.6975102040691, 696.2748984580536, 507.02723630160324, 508.1309822825977, 710.5470328571131, 708.8167880800543, 733.06295613589, 728.6773498695139, 747.2132981095463, 718.8154822680465, 742.3877879579888, 737.1567238860466, 783.2405475803444, 768.2336273966115, 694.7319563616157, 694.875385953475, 695.565111205244, 757.3932815082488, 761.2652073111161, 489.22524630056876, 738.3883070802376, 726.7015641771033, 497.3953159028114, 508.14708880742126, 490.105443345225, 727.8510931293836, 673.9842087370856, 594.6047498600293, 691.1312564082282, 704.4712903108476, 745.4663114191358, 736.8249990298141, 652.3981597181483, 746.2012757856808, 701.2686414503838, 704.5159425845634, 662.9048888510358, 638.9774556443044, 716.2671246894902, 735.8366580017137, 704.1784188979508, 705.2689414205869, 700.7993501589946, 741.2459206751214, 669.0079313353397, 701.5582699390553, 723.9124716802718, 737.582139828246, 735.8845325440454, 729.1692407291624, 696.5858802994244, 751.2136818291917, 655.2263233961869, 503.9142931918547, 708.7030102045326, 732.7555061866445, 731.9491506572716, 721.2021666025422, 582.8362217251746, 644.2738454949331, 747.728110436521, 690.1912924587757, 694.973875220644, 511.83657211829075, 677.8040376883127, 686.6131479011264, 675.6994276004006, 672.8444739454172, 711.4600008409611, 574.0593500210906, 696.1706376474523, 690.981420965639, 495.06148533815804, 465.60291344168826, 469.497085704649, 480.23741710355876, 751.9160038388172, 709.2242171778331, 697.7813836161542, 700.1655386948976, 528.1953434062927, 499.7495308889202, 753.0943375758652, 750.7480199009937, 754.1058647476963, 771.479164104472, 744.1959596012238, 613.4242886105692, 671.4276092851658, 690.9792002334287, 736.6971225120335, 738.8943750999841, 717.8788532755543, 678.2757927576944, 689.6043917928662, 665.6006801594432, 674.7593677388077, 698.7669283746179, 592.6271288345399, 674.1283726912499, 697.9095078014906, 688.7651758809154, 676.9039343261533, 669.2487118698726, 548.8273481808476, 684.2400697970569, 772.2331670442603, 737.1552625318253, 720.9576537917977, 478.08657582997716, 676.6531970642386, 729.9370730627563, 737.6066932785236, 710.915985019427, 731.2754955123295, 554.6478129642397, 729.660818866741, 713.8213713707362, 499.71089205005825, 476.62792205774844, 708.7473174888878, 714.7598191006017, 670.6728504524832, 723.4144600244599, 666.8800573728123, 512.1370222616873, 552.3081550623918, 730.788492149169, 708.661496587674, 513.0281062456829, 504.0773684858086, 508.72004908988674, 507.37475904563536, 740.0927826504725, 739.5332858183589, 678.7305641008559, 688.8137517848245, 730.316530408815, 739.7232744646169, 742.4159751473491, 708.7803867913698, 736.6247493991976, 746.2853125908774, 741.6196332403814, 704.0141817850138, 696.5470893612434, 708.895930076124, 722.8822250573946, 705.0191314335099, 677.8042085033348, 581.6032297653559, 656.2752485084208, 684.0579326383269, 715.0449131059688, 728.5036944729791, 555.1099804314515, 727.7554924262032, 728.0190727824719, 726.7379297368593, 749.2327987095272, 735.7480090591334, 724.2520607474765, 715.6032997241138, 725.0336755244492, 723.0676677547576, 737.5924176122912, 695.269253225518, 651.7598743837398, 729.8904903703954, 741.5164189524627, 732.2233666320817, 690.7578553055197, 708.4132044715868, 718.14973312554, 716.3360258996746, 744.2163529431454, 624.1858310390156, 746.4025484600279, 729.4572043248497, 761.6357934033965, 763.1962169279861, 710.727170983187, 758.856093820466, 708.7459067650238, 676.6146574646453, 734.5388827320993, 696.5797717208275, 492.03724709160196, 469.1386129021856, 748.6299502565454, 537.8742528327498, 737.882645828512, 766.1792200233522, 586.4158755738379, 520.2377819218838, 491.91163607481496, 733.5387551193498, 757.8980245261611, 763.2118720197775, 760.5875197531287, 526.6227260786155, 716.3348005977485, 710.4570513270072, 658.3223713998588, 500.0656850843744, 691.1992168417645, 701.382661185012, 726.8041341022613, 763.9087673145821, 766.9374844550207, 758.7764356765689, 747.3851124462484, 772.3763310227675, 757.898821783958, 719.9733244236354, 710.3772707575552, 671.6278792534761, 744.5905538052054, 752.8856313073073, 702.2311390947301, 747.5631182639664, 732.623840783579, 507.7593601017074, 761.7133177537734, 760.5195768313891, 755.1222092653401, 754.4299777623822, 737.9462650277324, 667.2736170547366, 728.8402422905965, 758.8049967407725, 504.06672663340345, 514.738098975798, 731.5038939215675, 727.2515175580189, 711.6243845441926, 753.7625591507889, 498.35232942817544, 692.2253174901529, 710.0629394570357, 765.019997928698, 748.7069002561695, 763.9284362997913, 740.7554283184503, 650.9718728873919, 609.3113054961794, 680.845993528046, 512.8075190469314, 525.2293151786682, 508.24353680503106, 768.2928152719928, 751.4089171068401, 705.2362905262403, 749.9232893835683, 746.622254867856, 724.3413529554639, 751.3760882500326, 761.063828318847, 532.0300944143855, 516.7629466829469, 521.5295757635814, 733.0773072560547, 752.5858192672858, 705.3880178805941, 740.0169387227597, 493.3804538272381, 501.3453778875753, 735.7850599639211, 698.5439945274675, 700.0783874207535, 699.6987082843799, 471.5083116998652, 703.4993554943009, 719.3381011115763, 703.3316475282658, 652.1946794217881, 650.9633258283084, 710.0831858414906, 719.4690551341728, 662.1543808593141, 689.9993749776409, 492.0851854821921, 655.6157230006046, 525.8162470110703, 623.44756086267, 716.8001424153929, 580.4497033461472, 714.5481426374129, 649.5139995449856, 649.3483677706276, 469.2170737382353, 651.3878346812573, 626.5039861801274, 632.5417799096191, 631.4104652664049, 466.9110390321412, 652.8352632434625, 716.3812362660387, 475.9685208738138, 480.55827724136316, 476.90073800485277, 697.955238276997, 660.0156967176387, 478.48446215680315, 472.71537273335315, 660.6756499373084, 671.2518437958539, 504.6102382162712, 721.9363888561329, 730.6504640232505, 678.9696777348772, 729.1985636252336, 538.0272665813558, 508.0746591848153, 499.8648600128065, 718.7595151457134, 729.7887585903313, 506.65208253016374, 495.9676390044586, 528.437332773259, 720.5917210515177, 711.126462085111, 710.3146078948894, 697.0009136290945, 692.0676748722985, 709.3452969712629, 715.0443621827608, 684.2024870231991, 725.4497153192945, 702.1456013296983, 660.9748868048848, 679.7612640877363, 668.4961695875033, 713.3595443581958, 708.9010611080464, 720.2201480309845, 528.7163839571997, 692.1388467020276, 720.4601856831532, 701.2762474118301, 539.2128859865649, 702.0214926304307, 713.7248830774273, 723.1721707002605, 679.2789463403234, 489.3080112480365, 695.3155283882053, 728.0980937049453, 729.8550008170795, 690.1171500690815, 477.31173741281725, 718.715428687302, 670.5126334259111, 769.8673624744155, 761.2252886227193, 692.38790767825, 618.3233689748721, 688.3537441834998, 658.821041098874, 686.3521916197379, 740.080086215328, 699.8492271152916, 609.1703257587077, 678.7368957843946, 711.3193312095336, 652.5417945557859, 630.5897962797886, 578.0425354636942, 733.8792081588423, 740.573120213839, 740.6629357778417, 742.8499956571392, 499.98594047834644, 732.973140548356, 745.0374233013932, 756.2715395078548, 726.2386050223537, 718.0341669554621, 503.9044785738185]
Elapsed: 0.19559390125806128~0.030441588980523345
Time per graph: 0.0015162317926981494~0.00023598130992653758
Speed: 673.077833806913~87.58409244224414
Total Time: 0.2573
best val loss: 0.1606837461969649 test_score: 0.9380

Testing...
Test loss: 0.6063 score: 0.9225 time: 0.25s
test Score 0.9225
Epoch Time List: [1.0836947648786008, 0.5708195301704109, 0.589953602058813, 0.5854726540856063, 0.6093268170952797, 0.6215507951565087, 0.6941069930326194, 0.6255710572004318, 0.6100808072369546, 0.6161278311628848, 0.6689238180406392, 0.8476019520312548, 0.5766242421232164, 0.586501965066418, 0.5844179200939834, 0.5784046018961817, 0.5902710638474673, 0.7421582110691816, 0.7174279193859547, 0.6163106430321932, 0.6466278899461031, 0.5921160059515387, 0.7165219869930297, 0.6950624019373208, 0.6512760522309691, 0.6544777068775147, 0.6323277559131384, 0.6489459669683129, 1.0159847061149776, 0.753773788921535, 0.6668274598196149, 0.6565516230184585, 0.6575678871013224, 0.6556579628959298, 0.7039439573418349, 0.7332392169628292, 0.6241264860145748, 0.6121964640915394, 0.6307262841146439, 0.8964077108539641, 0.8994238651357591, 0.7698187651112676, 0.6455200638156384, 0.6094641529489309, 0.6036050640977919, 0.6161871505901217, 0.6098203489091247, 0.615702377166599, 0.6774818878620863, 0.6896966365166008, 0.603852248750627, 0.6072742631658912, 0.6058353190310299, 0.649913219967857, 0.8141458800528198, 0.6883733361028135, 0.6075125087518245, 0.6020360859110951, 0.6002620700746775, 0.6150864399969578, 0.7434393090661615, 0.8648080902639776, 0.620058850152418, 0.6100925570353866, 0.6069854509551078, 0.6089895707555115, 0.6363219949416816, 0.6232986380346119, 0.6704465737566352, 0.6136036787647754, 0.618670579046011, 0.7081901277415454, 0.8932457382325083, 0.8118357877247036, 0.6313791801221669, 0.6206254351418465, 0.7876678970642388, 0.7747827307321131, 0.6215538906399161, 0.6962253581732512, 0.8947601767722517, 0.9314866231288761, 0.701017543906346, 0.5935166077688336, 0.6309381008613855, 0.6407432141713798, 0.6513632847927511, 0.6830349741503596, 0.7243960769847035, 0.6154722208157182, 0.6060785339213908, 0.6115176861640066, 0.7549497007858008, 0.9040758379269391, 0.7627244300674647, 0.6264859489165246, 0.6201118370518088, 0.6138705709017813, 0.6202610936015844, 0.6306114050094038, 0.6984840319491923, 0.6219825057778507, 0.6094407942146063, 0.6150315413251519, 0.6382266511209309, 0.6498633152805269, 0.6824035979807377, 0.6593251440208405, 0.6638632609974593, 0.7035925416275859, 0.5943626621738076, 0.6015636257361621, 0.6890941220335662, 0.8806173091288656, 0.7947248867712915, 0.6144833690486848, 0.6449953860137612, 0.6764656975865364, 0.7164703099988401, 0.6101893286686391, 0.5969492038711905, 0.5941944501828402, 0.64245097595267, 0.6812034668400884, 0.6377661831211299, 0.6395013139117509, 0.640983346151188, 0.7597829701844603, 0.6368520788382739, 0.6210459012072533, 0.6249303610529751, 0.6647329272236675, 0.704937273170799, 0.6335280742496252, 0.648711069021374, 0.6300547479186207, 0.6154474157374352, 0.6098680929280818, 0.6802037602756172, 0.7083359891548753, 0.6217028400860727, 0.6023208361584693, 0.6252047962043434, 0.8119614911265671, 0.7071308696176857, 0.6252438726369292, 0.6202035881578922, 0.6239648910705, 0.6633785718586296, 0.7388974640052766, 0.6487703388556838, 0.6518367801327258, 0.6479652069974691, 0.7221035200636834, 0.8197508461307734, 0.6384308631531894, 0.651065044105053, 0.6603788938373327, 0.6467790068127215, 0.6931481338106096, 0.726722963154316, 0.653522894019261, 0.7409208337776363, 0.9489967718254775, 0.9411165530327708, 0.9322094789240509, 0.7123096061404794, 0.6359040278475732, 0.6487589322496206, 0.6393916851375252, 0.7109046161640435, 0.8947819168679416, 0.7660877150483429, 0.6062083761207759, 0.6066623837687075, 0.6005108479876071, 0.6142633031122386, 0.6662183662410825, 0.7127436762675643, 0.6534634241834283, 0.6197552247904241, 0.6247987577226013, 0.6125328431371599, 0.6997722347732633, 0.6334923699032515, 0.632179053267464, 0.6291042079683393, 0.61709213280119, 0.6675330428406596, 0.7309160926379263, 0.648935440229252, 0.6470874031074345, 0.6575603939127177, 0.6484825718216598, 0.6942376263905317, 0.6803573067300022, 0.6846679172012955, 0.6566675261128694, 0.6047672310378402, 0.6983086878899485, 0.6233348120003939, 0.6802933658473194, 0.5872511498164386, 0.5915656390134245, 0.594648890895769, 0.6612352293450385, 0.6310501049738377, 0.6028778797481209, 0.863588151987642, 0.9238479672931135, 0.7922311220318079, 0.6111626101192087, 0.6354194260202348, 0.6099875490181148, 0.6137214349582791, 0.7636235100217164, 0.8218230397906154, 0.5983341792598367, 0.6148065449669957, 0.6794975029770285, 0.8775271722115576, 0.8817378350067884, 0.875477580819279, 0.6787507457192987, 0.6155202970840037, 0.6283550558146089, 0.6230632250662893, 0.5956224920228124, 0.6707753101363778, 0.598671963205561, 0.6053076770622283, 0.6012492759618908, 0.594231090741232, 0.5910418978892267, 0.6408593468368053, 0.6995939661283046, 0.5951496029738337, 0.613638998940587, 0.6463468901347369, 0.6537653508130461, 0.7811769121326506, 0.6573541220277548, 0.6293654218316078, 0.6112494759727269, 0.5896064059343189, 0.6710046350490302, 0.6156487644184381, 0.5936593981459737, 0.5956821858417243, 0.5887923401314765, 0.59258082206361, 0.5958939166739583, 0.597688095876947, 0.6851928732357919, 0.603892759885639, 0.6011480707675219, 0.601905383169651, 0.6529806789476424, 0.6935656077694148, 0.600198709173128, 0.6043534660711884, 0.6068133390508592, 0.6768191009759903, 0.600020547863096, 0.5954876700416207, 0.5839802040718496, 0.6239319259766489, 0.5837970380671322, 0.6258591148070991, 0.6142481509596109, 0.6758447310421616, 0.6003434900194407, 0.5955768590793014, 0.5961237610317767, 0.7280577688943595, 0.589003321249038, 0.6024844497442245, 0.7007818568963557, 0.9813217574264854, 0.5829691500402987, 0.6499205569270998, 0.5986722458619624, 0.5868608502205461, 0.7227052748203278, 0.7409216079395264, 0.8233081470243633, 0.5942134980577976, 0.5827470829244703, 0.573756986996159, 0.5829021069221199, 0.6759750819765031, 0.6369031690992415, 0.6175380360800773, 0.6225286100525409, 0.8315734460484236, 0.8016784519422799, 0.6196099151857197, 0.6043310479726642, 0.5797687559388578, 0.5782831415999681, 0.5891331559978426, 0.59620895027183, 0.6560178869403899, 0.5731202161405236, 0.6012967848218977, 0.6062209659721702, 0.6184693612158298, 0.6714234307873994, 0.5785830691456795, 0.622736866120249, 0.5831247952301055, 0.5887529947794974, 0.7063094358891249, 0.6229937756434083, 0.5837577690836042, 0.5806258788798004, 0.5844742232002318, 0.5927459474187344, 0.8115243359934539, 0.5845054523088038, 0.5775323531124741, 0.6708980200346559, 0.8687088578008115, 0.5972511500585824, 0.6188568465877324, 0.5990920211188495, 0.5964865107089281, 0.6829092719126493, 0.5954019140917808, 0.6017526730429381, 0.5848006368614733, 0.592802177881822, 0.5698603258933872, 0.5945176787208766, 0.6021748348139226, 0.6968223401345313, 0.5940822020638734, 0.8262709921691567, 0.8826532198581845, 0.8994047800078988, 0.7652884270064533, 0.5761468592099845, 0.6549335573799908, 0.5994360761251301, 0.5882691582664847, 0.5980078871361911, 0.5973883799742907, 0.6620294519234449, 0.6494378170464188, 0.8622203900013119, 0.8728752061724663, 0.7590983491390944, 0.5850772249978036, 0.6163665091153234, 0.5935067741665989, 0.7400479780044407, 0.9034185509663075, 0.6060359957627952, 0.6464530250523239, 0.6252386763226241, 0.6204369605984539, 0.7424287965986878, 0.6346537191420794, 0.6201111869886518, 0.6268028391059488, 0.656156204175204, 0.6644171779043972, 0.6591438336763531, 0.6927034170366824, 0.6869766837917268, 0.6076428540982306, 0.7045698440633714, 0.6393435739446431, 0.6947616839315742, 0.7391092958860099, 0.6117098298855126, 0.6931394340936095, 0.6631128159351647, 0.6580053528305143, 0.6686578320804983, 0.9487630070652813, 0.7845103670842946, 0.6766636909451336, 0.673095794627443, 0.6770206401124597, 0.7862482308410108, 0.825093308230862, 0.6160965643357486, 0.7186436566989869, 0.9349708030931652, 0.9328399051446468, 0.7164339500013739, 0.6432944678235799, 0.7501064508687705, 0.9396336181089282, 0.7951530646532774, 0.6664797868579626, 0.6847957281861454, 0.7467340142466128, 0.6072103059850633, 0.6476504609454423, 0.6063482060562819, 0.6789750999305397, 0.8916868309024721, 0.8968410920351744, 0.6910429487470537, 0.6078164200298488, 0.7036856519989669, 0.897607646882534, 0.8922558131162077, 0.6193339561577886, 0.6205139157827944, 0.6239268477074802, 0.6186383492313325, 0.6186270760372281, 0.6238115821033716, 0.6960387942381203, 0.6208159679081291, 0.6151325439568609, 0.629501367919147, 0.6758870258927345, 0.7802612478844821, 0.6488098960835487, 0.6178522030822933, 0.614881613990292, 0.6118694613687694, 0.7449868838302791, 0.6272542041260749, 0.628161568660289, 0.6252326830290258, 0.687682697083801, 0.7209784751757979, 0.6139421730767936, 0.6065604721661657, 0.6303212696220726, 0.7295896399300545, 0.7106932168826461, 0.5979523309506476, 0.5985118572134525, 0.6138559507671744, 0.7064715842716396, 0.6125282770954072, 0.6339753128122538, 0.5778564838692546, 0.5819487478584051, 0.6211138011422008, 0.6703301500529051, 0.6432869890704751, 0.7511339702177793, 0.6164917827118188, 0.6036817640997469, 0.6127982770558447, 0.8329731430858374, 0.6437051820103079, 0.6175000150687993, 0.7340018078684807, 0.6636861506849527, 0.8245460677426308, 0.6789909098297358, 0.5986729050055146, 0.6836168807931244, 0.5946051280479878, 0.6798210772685707, 0.5929212919436395, 0.6837522839196026, 0.5808408169541508, 0.5954931962769479, 0.5947088461834937, 0.7674977742135525]
Total Epoch List: [202, 267]
Total Time List: [0.17763156397268176, 0.25727027910761535]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a28953280>
Training...
Epoch 1/1000, LR 0.000300
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 0.21s
Epoch 2/1000, LR 0.000020
Train loss: 4.1959;  Loss pred: 4.1959; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.16s
Epoch 3/1000, LR 0.000050
Train loss: 4.1264;  Loss pred: 4.1264; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.16s
Epoch 4/1000, LR 0.000080
Train loss: 4.1145;  Loss pred: 4.1145; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.19s
Epoch 5/1000, LR 0.000110
Train loss: 3.9712;  Loss pred: 3.9712; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 0.22s
Epoch 6/1000, LR 0.000140
Train loss: 3.9010;  Loss pred: 3.9010; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.22s
Epoch 7/1000, LR 0.000170
Train loss: 3.8180;  Loss pred: 3.8180; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 0.18s
Epoch 8/1000, LR 0.000200
Train loss: 3.6023;  Loss pred: 3.6023; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.19s
Epoch 9/1000, LR 0.000230
Train loss: 3.4586;  Loss pred: 3.4586; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.16s
Epoch 10/1000, LR 0.000260
Train loss: 3.2938;  Loss pred: 3.2938; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.18s
Epoch 11/1000, LR 0.000290
Train loss: 3.1348;  Loss pred: 3.1348; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.16s
Epoch 12/1000, LR 0.000290
Train loss: 2.9730;  Loss pred: 2.9730; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 2.8324;  Loss pred: 2.8324; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 2.6542;  Loss pred: 2.6542; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000290
Train loss: 2.5370;  Loss pred: 2.5370; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000290
Train loss: 2.3920;  Loss pred: 2.3920; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.22s
Epoch 17/1000, LR 0.000290
Train loss: 2.2595;  Loss pred: 2.2595; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 2.1611;  Loss pred: 2.1611; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 2.0653;  Loss pred: 2.0653; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 1.9857;  Loss pred: 1.9857; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.22s
Epoch 21/1000, LR 0.000290
Train loss: 1.8906;  Loss pred: 1.8906; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.22s
Epoch 22/1000, LR 0.000290
Train loss: 1.8040;  Loss pred: 1.8040; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 1.7448;  Loss pred: 1.7448; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 1.6636;  Loss pred: 1.6636; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.17s
Epoch 25/1000, LR 0.000290
Train loss: 1.6190;  Loss pred: 1.6190; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.21s
Epoch 26/1000, LR 0.000290
Train loss: 1.5577;  Loss pred: 1.5577; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.21s
Epoch 27/1000, LR 0.000290
Train loss: 1.5038;  Loss pred: 1.5038; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.22s
Epoch 28/1000, LR 0.000290
Train loss: 1.4619;  Loss pred: 1.4619; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 1.4088;  Loss pred: 1.4088; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.18s
Epoch 30/1000, LR 0.000290
Train loss: 1.3697;  Loss pred: 1.3697; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.18s
Epoch 31/1000, LR 0.000290
Train loss: 1.3380;  Loss pred: 1.3380; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.15s
Epoch 32/1000, LR 0.000290
Train loss: 1.3033;  Loss pred: 1.3033; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 1.2752;  Loss pred: 1.2752; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.21s
Epoch 34/1000, LR 0.000290
Train loss: 1.2457;  Loss pred: 1.2457; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.15s
Epoch 35/1000, LR 0.000290
Train loss: 1.2273;  Loss pred: 1.2273; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 1.2006;  Loss pred: 1.2006; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.15s
Epoch 37/1000, LR 0.000290
Train loss: 1.1832;  Loss pred: 1.1832; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.15s
Epoch 38/1000, LR 0.000289
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.15s
Epoch 39/1000, LR 0.000289
Train loss: 1.1486;  Loss pred: 1.1486; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.21s
Epoch 40/1000, LR 0.000289
Train loss: 1.1332;  Loss pred: 1.1332; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.15s
Epoch 41/1000, LR 0.000289
Train loss: 1.1211;  Loss pred: 1.1211; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.15s
Epoch 42/1000, LR 0.000289
Train loss: 1.1115;  Loss pred: 1.1115; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 1.0979;  Loss pred: 1.0979; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 1.0905;  Loss pred: 1.0905; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 1.0774;  Loss pred: 1.0774; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.17s
Epoch 46/1000, LR 0.000289
Train loss: 1.0701;  Loss pred: 1.0701; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5000 time: 0.17s
Epoch 47/1000, LR 0.000289
Train loss: 1.0640;  Loss pred: 1.0640; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.5000 time: 0.17s
Epoch 48/1000, LR 0.000289
Train loss: 1.0588;  Loss pred: 1.0588; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.17s
Epoch 49/1000, LR 0.000289
Train loss: 1.0508;  Loss pred: 1.0508; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.5000 time: 0.22s
Epoch 50/1000, LR 0.000289
Train loss: 1.0411;  Loss pred: 1.0411; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5000 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 1.0375;  Loss pred: 1.0375; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.16s
Epoch 52/1000, LR 0.000289
Train loss: 1.0332;  Loss pred: 1.0332; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5000 time: 0.17s
Epoch 53/1000, LR 0.000289
Train loss: 1.0291;  Loss pred: 1.0291; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.17s
Epoch 54/1000, LR 0.000289
Train loss: 1.0243;  Loss pred: 1.0243; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.5000 time: 0.19s
Epoch 55/1000, LR 0.000289
Train loss: 1.0205;  Loss pred: 1.0205; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.5000 time: 0.17s
Epoch 56/1000, LR 0.000289
Train loss: 1.0163;  Loss pred: 1.0163; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5000 time: 0.16s
Epoch 57/1000, LR 0.000288
Train loss: 1.0136;  Loss pred: 1.0136; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6825 score: 0.5000 time: 0.15s
Epoch 58/1000, LR 0.000288
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.5000 time: 0.16s
Epoch 59/1000, LR 0.000288
Train loss: 1.0084;  Loss pred: 1.0084; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.5000 time: 0.17s
Epoch 60/1000, LR 0.000288
Train loss: 1.0039;  Loss pred: 1.0039; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.5000 time: 0.16s
Epoch 61/1000, LR 0.000288
Train loss: 1.0010;  Loss pred: 1.0010; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6768 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6789 score: 0.5000 time: 0.16s
Epoch 62/1000, LR 0.000288
Train loss: 0.9984;  Loss pred: 0.9984; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6759 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6780 score: 0.5000 time: 0.15s
Epoch 63/1000, LR 0.000288
Train loss: 0.9945;  Loss pred: 0.9945; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6750 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6771 score: 0.5000 time: 0.16s
Epoch 64/1000, LR 0.000288
Train loss: 0.9930;  Loss pred: 0.9930; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6738 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.5000 time: 0.15s
Epoch 65/1000, LR 0.000288
Train loss: 0.9900;  Loss pred: 0.9900; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6725 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.15s
Epoch 66/1000, LR 0.000288
Train loss: 0.9881;  Loss pred: 0.9881; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6734 score: 0.5000 time: 0.15s
Epoch 67/1000, LR 0.000288
Train loss: 0.9851;  Loss pred: 0.9851; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6694 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6721 score: 0.5000 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.9838;  Loss pred: 0.9838; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6679 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6708 score: 0.5000 time: 0.16s
Epoch 69/1000, LR 0.000288
Train loss: 0.9812;  Loss pred: 0.9812; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6664 score: 0.5039 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6695 score: 0.5000 time: 0.20s
Epoch 70/1000, LR 0.000287
Train loss: 0.9784;  Loss pred: 0.9784; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6650 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6683 score: 0.5000 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 0.9777;  Loss pred: 0.9777; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6635 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6670 score: 0.5000 time: 0.17s
Epoch 72/1000, LR 0.000287
Train loss: 0.9758;  Loss pred: 0.9758; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6620 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6654 score: 0.5000 time: 0.16s
Epoch 73/1000, LR 0.000287
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 0.28s
Val loss: 0.6607 score: 0.5349 time: 0.19s
Test loss: 0.6640 score: 0.5234 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9716;  Loss pred: 0.9716; Loss self: 0.0000; time: 0.50s
Val loss: 0.6589 score: 0.5349 time: 0.22s
Test loss: 0.6623 score: 0.5312 time: 0.21s
Epoch 75/1000, LR 0.000287
Train loss: 0.9689;  Loss pred: 0.9689; Loss self: 0.0000; time: 0.30s
Val loss: 0.6568 score: 0.5271 time: 0.18s
Test loss: 0.6605 score: 0.5312 time: 0.16s
Epoch 76/1000, LR 0.000287
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6549 score: 0.5039 time: 0.17s
Test loss: 0.6592 score: 0.5078 time: 0.16s
Epoch 77/1000, LR 0.000287
Train loss: 0.9649;  Loss pred: 0.9649; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6535 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6581 score: 0.5000 time: 0.16s
Epoch 78/1000, LR 0.000287
Train loss: 0.9634;  Loss pred: 0.9634; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6525 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6576 score: 0.5000 time: 0.20s
Epoch 79/1000, LR 0.000287
Train loss: 0.9636;  Loss pred: 0.9636; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6521 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6575 score: 0.5000 time: 0.17s
Epoch 80/1000, LR 0.000287
Train loss: 0.9628;  Loss pred: 0.9628; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6516 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6573 score: 0.5000 time: 0.23s
Epoch 81/1000, LR 0.000286
Train loss: 0.9628;  Loss pred: 0.9628; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6499 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6559 score: 0.5000 time: 0.17s
Epoch 82/1000, LR 0.000286
Train loss: 0.9600;  Loss pred: 0.9600; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6466 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6528 score: 0.5000 time: 0.16s
Epoch 83/1000, LR 0.000286
Train loss: 0.9562;  Loss pred: 0.9562; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6428 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6491 score: 0.5000 time: 0.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.9520;  Loss pred: 0.9520; Loss self: 0.0000; time: 0.30s
Val loss: 0.6382 score: 0.5426 time: 0.18s
Test loss: 0.6444 score: 0.5312 time: 0.17s
Epoch 85/1000, LR 0.000286
Train loss: 0.9483;  Loss pred: 0.9483; Loss self: 0.0000; time: 0.32s
Val loss: 0.6355 score: 0.6667 time: 0.18s
Test loss: 0.6410 score: 0.5703 time: 0.17s
Epoch 86/1000, LR 0.000286
Train loss: 0.9448;  Loss pred: 0.9448; Loss self: 0.0000; time: 0.30s
Val loss: 0.6360 score: 0.8760 time: 0.23s
Test loss: 0.6407 score: 0.8125 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.9445;  Loss pred: 0.9445; Loss self: 0.0000; time: 0.32s
Val loss: 0.6357 score: 0.9225 time: 0.24s
Test loss: 0.6401 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.9430;  Loss pred: 0.9430; Loss self: 0.0000; time: 0.30s
Val loss: 0.6327 score: 0.9070 time: 0.17s
Test loss: 0.6373 score: 0.8828 time: 0.16s
Epoch 89/1000, LR 0.000286
Train loss: 0.9402;  Loss pred: 0.9402; Loss self: 0.0000; time: 0.30s
Val loss: 0.6267 score: 0.8682 time: 0.17s
Test loss: 0.6322 score: 0.7969 time: 0.16s
Epoch 90/1000, LR 0.000285
Train loss: 0.9368;  Loss pred: 0.9368; Loss self: 0.0000; time: 0.30s
Val loss: 0.6215 score: 0.7132 time: 0.17s
Test loss: 0.6286 score: 0.6328 time: 0.16s
Epoch 91/1000, LR 0.000285
Train loss: 0.9335;  Loss pred: 0.9335; Loss self: 0.0000; time: 0.30s
Val loss: 0.6194 score: 0.5736 time: 0.17s
Test loss: 0.6274 score: 0.5547 time: 0.15s
Epoch 92/1000, LR 0.000285
Train loss: 0.9333;  Loss pred: 0.9333; Loss self: 0.0000; time: 0.30s
Val loss: 0.6169 score: 0.5736 time: 0.19s
Test loss: 0.6254 score: 0.5703 time: 0.16s
Epoch 93/1000, LR 0.000285
Train loss: 0.9317;  Loss pred: 0.9317; Loss self: 0.0000; time: 0.43s
Val loss: 0.6137 score: 0.5736 time: 0.17s
Test loss: 0.6225 score: 0.5703 time: 0.16s
Epoch 94/1000, LR 0.000285
Train loss: 0.9283;  Loss pred: 0.9283; Loss self: 0.0000; time: 0.31s
Val loss: 0.6090 score: 0.6667 time: 0.18s
Test loss: 0.6178 score: 0.6094 time: 0.16s
Epoch 95/1000, LR 0.000285
Train loss: 0.9246;  Loss pred: 0.9246; Loss self: 0.0000; time: 0.30s
Val loss: 0.6060 score: 0.8450 time: 0.17s
Test loss: 0.6138 score: 0.7969 time: 0.16s
Epoch 96/1000, LR 0.000285
Train loss: 0.9214;  Loss pred: 0.9214; Loss self: 0.0000; time: 0.31s
Val loss: 0.6080 score: 0.9225 time: 0.17s
Test loss: 0.6145 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 97/1000, LR 0.000285
Train loss: 0.9214;  Loss pred: 0.9214; Loss self: 0.0000; time: 0.30s
Val loss: 0.6105 score: 0.9302 time: 0.17s
Test loss: 0.6165 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 98/1000, LR 0.000285
Train loss: 0.9220;  Loss pred: 0.9220; Loss self: 0.0000; time: 0.31s
Val loss: 0.6080 score: 0.9380 time: 0.20s
Test loss: 0.6142 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 99/1000, LR 0.000284
Train loss: 0.9203;  Loss pred: 0.9203; Loss self: 0.0000; time: 0.39s
Val loss: 0.6037 score: 0.9380 time: 0.18s
Test loss: 0.6102 score: 0.9219 time: 0.16s
Epoch 100/1000, LR 0.000284
Train loss: 0.9161;  Loss pred: 0.9161; Loss self: 0.0000; time: 0.32s
Val loss: 0.5965 score: 0.9380 time: 0.18s
Test loss: 0.6036 score: 0.9297 time: 0.16s
Epoch 101/1000, LR 0.000284
Train loss: 0.9110;  Loss pred: 0.9110; Loss self: 0.0000; time: 0.31s
Val loss: 0.5872 score: 0.9302 time: 0.17s
Test loss: 0.5955 score: 0.9062 time: 0.16s
Epoch 102/1000, LR 0.000284
Train loss: 0.9055;  Loss pred: 0.9055; Loss self: 0.0000; time: 0.34s
Val loss: 0.5799 score: 0.9225 time: 0.25s
Test loss: 0.5892 score: 0.8984 time: 0.21s
Epoch 103/1000, LR 0.000284
Train loss: 0.9010;  Loss pred: 0.9010; Loss self: 0.0000; time: 0.52s
Val loss: 0.5727 score: 0.9147 time: 0.21s
Test loss: 0.5833 score: 0.8828 time: 0.21s
Epoch 104/1000, LR 0.000284
Train loss: 0.8972;  Loss pred: 0.8972; Loss self: 0.0000; time: 0.32s
Val loss: 0.5665 score: 0.8992 time: 0.18s
Test loss: 0.5787 score: 0.8359 time: 0.16s
Epoch 105/1000, LR 0.000284
Train loss: 0.8927;  Loss pred: 0.8927; Loss self: 0.0000; time: 0.31s
Val loss: 0.5612 score: 0.8992 time: 0.18s
Test loss: 0.5738 score: 0.8359 time: 0.17s
Epoch 106/1000, LR 0.000283
Train loss: 0.8907;  Loss pred: 0.8907; Loss self: 0.0000; time: 0.47s
Val loss: 0.5565 score: 0.9225 time: 0.26s
Test loss: 0.5687 score: 0.8906 time: 0.21s
Epoch 107/1000, LR 0.000283
Train loss: 0.8860;  Loss pred: 0.8860; Loss self: 0.0000; time: 0.52s
Val loss: 0.5529 score: 0.9302 time: 0.26s
Test loss: 0.5647 score: 0.8984 time: 0.22s
Epoch 108/1000, LR 0.000283
Train loss: 0.8820;  Loss pred: 0.8820; Loss self: 0.0000; time: 0.52s
Val loss: 0.5479 score: 0.9302 time: 0.25s
Test loss: 0.5603 score: 0.8984 time: 0.22s
Epoch 109/1000, LR 0.000283
Train loss: 0.8791;  Loss pred: 0.8791; Loss self: 0.0000; time: 0.34s
Val loss: 0.5440 score: 0.9302 time: 0.17s
Test loss: 0.5564 score: 0.9062 time: 0.17s
Epoch 110/1000, LR 0.000283
Train loss: 0.8767;  Loss pred: 0.8767; Loss self: 0.0000; time: 0.30s
Val loss: 0.5385 score: 0.9302 time: 0.17s
Test loss: 0.5514 score: 0.9141 time: 0.17s
Epoch 111/1000, LR 0.000283
Train loss: 0.8731;  Loss pred: 0.8731; Loss self: 0.0000; time: 0.31s
Val loss: 0.5308 score: 0.9302 time: 0.19s
Test loss: 0.5449 score: 0.8984 time: 0.19s
Epoch 112/1000, LR 0.000283
Train loss: 0.8675;  Loss pred: 0.8675; Loss self: 0.0000; time: 0.30s
Val loss: 0.5235 score: 0.9225 time: 0.20s
Test loss: 0.5390 score: 0.8984 time: 0.16s
Epoch 113/1000, LR 0.000282
Train loss: 0.8631;  Loss pred: 0.8631; Loss self: 0.0000; time: 0.34s
Val loss: 0.5179 score: 0.9302 time: 0.19s
Test loss: 0.5336 score: 0.8984 time: 0.19s
Epoch 114/1000, LR 0.000282
Train loss: 0.8590;  Loss pred: 0.8590; Loss self: 0.0000; time: 0.35s
Val loss: 0.5134 score: 0.9302 time: 0.17s
Test loss: 0.5289 score: 0.9141 time: 0.16s
Epoch 115/1000, LR 0.000282
Train loss: 0.8560;  Loss pred: 0.8560; Loss self: 0.0000; time: 0.31s
Val loss: 0.5076 score: 0.9380 time: 0.19s
Test loss: 0.5235 score: 0.9141 time: 0.16s
Epoch 116/1000, LR 0.000282
Train loss: 0.8521;  Loss pred: 0.8521; Loss self: 0.0000; time: 0.30s
Val loss: 0.5019 score: 0.9380 time: 0.17s
Test loss: 0.5182 score: 0.9141 time: 0.16s
Epoch 117/1000, LR 0.000282
Train loss: 0.8476;  Loss pred: 0.8476; Loss self: 0.0000; time: 0.31s
Val loss: 0.4946 score: 0.9380 time: 0.17s
Test loss: 0.5120 score: 0.9141 time: 0.17s
Epoch 118/1000, LR 0.000282
Train loss: 0.8437;  Loss pred: 0.8437; Loss self: 0.0000; time: 0.31s
Val loss: 0.4872 score: 0.9302 time: 0.18s
Test loss: 0.5064 score: 0.8984 time: 0.16s
Epoch 119/1000, LR 0.000282
Train loss: 0.8392;  Loss pred: 0.8392; Loss self: 0.0000; time: 0.34s
Val loss: 0.4822 score: 0.9302 time: 0.18s
Test loss: 0.5014 score: 0.9062 time: 0.16s
Epoch 120/1000, LR 0.000281
Train loss: 0.8346;  Loss pred: 0.8346; Loss self: 0.0000; time: 0.38s
Val loss: 0.4767 score: 0.9302 time: 0.17s
Test loss: 0.4965 score: 0.9141 time: 0.21s
Epoch 121/1000, LR 0.000281
Train loss: 0.8309;  Loss pred: 0.8309; Loss self: 0.0000; time: 0.32s
Val loss: 0.4707 score: 0.9302 time: 0.17s
Test loss: 0.4925 score: 0.8906 time: 0.15s
Epoch 122/1000, LR 0.000281
Train loss: 0.8293;  Loss pred: 0.8293; Loss self: 0.0000; time: 0.30s
Val loss: 0.4664 score: 0.9147 time: 0.17s
Test loss: 0.4905 score: 0.8906 time: 0.15s
Epoch 123/1000, LR 0.000281
Train loss: 0.8272;  Loss pred: 0.8272; Loss self: 0.0000; time: 0.31s
Val loss: 0.4633 score: 0.8992 time: 0.18s
Test loss: 0.4892 score: 0.8438 time: 0.16s
Epoch 124/1000, LR 0.000281
Train loss: 0.8253;  Loss pred: 0.8253; Loss self: 0.0000; time: 0.31s
Val loss: 0.4609 score: 0.8915 time: 0.17s
Test loss: 0.4885 score: 0.8281 time: 0.18s
Epoch 125/1000, LR 0.000281
Train loss: 0.8249;  Loss pred: 0.8249; Loss self: 0.0000; time: 0.31s
Val loss: 0.4615 score: 0.8450 time: 0.17s
Test loss: 0.4905 score: 0.7891 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 126/1000, LR 0.000280
Train loss: 0.8274;  Loss pred: 0.8274; Loss self: 0.0000; time: 0.30s
Val loss: 0.4604 score: 0.8605 time: 0.17s
Test loss: 0.4903 score: 0.7734 time: 0.16s
Epoch 127/1000, LR 0.000280
Train loss: 0.8257;  Loss pred: 0.8257; Loss self: 0.0000; time: 0.29s
Val loss: 0.4520 score: 0.8605 time: 0.17s
Test loss: 0.4823 score: 0.7891 time: 0.16s
Epoch 128/1000, LR 0.000280
Train loss: 0.8199;  Loss pred: 0.8199; Loss self: 0.0000; time: 0.30s
Val loss: 0.4423 score: 0.8915 time: 0.17s
Test loss: 0.4727 score: 0.8281 time: 0.16s
Epoch 129/1000, LR 0.000280
Train loss: 0.8108;  Loss pred: 0.8108; Loss self: 0.0000; time: 0.29s
Val loss: 0.4352 score: 0.8992 time: 0.17s
Test loss: 0.4659 score: 0.8359 time: 0.17s
Epoch 130/1000, LR 0.000280
Train loss: 0.8064;  Loss pred: 0.8064; Loss self: 0.0000; time: 0.35s
Val loss: 0.4289 score: 0.9070 time: 0.18s
Test loss: 0.4601 score: 0.8438 time: 0.19s
Epoch 131/1000, LR 0.000280
Train loss: 0.8031;  Loss pred: 0.8031; Loss self: 0.0000; time: 0.48s
Val loss: 0.4196 score: 0.9147 time: 0.21s
Test loss: 0.4502 score: 0.8828 time: 0.16s
Epoch 132/1000, LR 0.000279
Train loss: 0.7950;  Loss pred: 0.7950; Loss self: 0.0000; time: 0.30s
Val loss: 0.4111 score: 0.9302 time: 0.17s
Test loss: 0.4393 score: 0.9062 time: 0.16s
Epoch 133/1000, LR 0.000279
Train loss: 0.7875;  Loss pred: 0.7875; Loss self: 0.0000; time: 0.30s
Val loss: 0.4063 score: 0.9380 time: 0.16s
Test loss: 0.4328 score: 0.9141 time: 0.15s
Epoch 134/1000, LR 0.000279
Train loss: 0.7821;  Loss pred: 0.7821; Loss self: 0.0000; time: 0.29s
Val loss: 0.4029 score: 0.9380 time: 0.17s
Test loss: 0.4282 score: 0.9219 time: 0.15s
Epoch 135/1000, LR 0.000279
Train loss: 0.7811;  Loss pred: 0.7811; Loss self: 0.0000; time: 0.35s
Val loss: 0.3990 score: 0.9457 time: 0.19s
Test loss: 0.4244 score: 0.9141 time: 0.22s
Epoch 136/1000, LR 0.000279
Train loss: 0.7781;  Loss pred: 0.7781; Loss self: 0.0000; time: 0.53s
Val loss: 0.3933 score: 0.9535 time: 0.26s
Test loss: 0.4192 score: 0.9141 time: 0.23s
Epoch 137/1000, LR 0.000279
Train loss: 0.7741;  Loss pred: 0.7741; Loss self: 0.0000; time: 0.39s
Val loss: 0.3833 score: 0.9457 time: 0.18s
Test loss: 0.4106 score: 0.9219 time: 0.16s
Epoch 138/1000, LR 0.000278
Train loss: 0.7666;  Loss pred: 0.7666; Loss self: 0.0000; time: 0.31s
Val loss: 0.3721 score: 0.9380 time: 0.17s
Test loss: 0.4034 score: 0.9141 time: 0.16s
Epoch 139/1000, LR 0.000278
Train loss: 0.7616;  Loss pred: 0.7616; Loss self: 0.0000; time: 0.31s
Val loss: 0.3663 score: 0.9302 time: 0.18s
Test loss: 0.4013 score: 0.9062 time: 0.16s
Epoch 140/1000, LR 0.000278
Train loss: 0.7622;  Loss pred: 0.7622; Loss self: 0.0000; time: 0.30s
Val loss: 0.3611 score: 0.9302 time: 0.17s
Test loss: 0.3977 score: 0.8906 time: 0.17s
Epoch 141/1000, LR 0.000278
Train loss: 0.7567;  Loss pred: 0.7567; Loss self: 0.0000; time: 0.31s
Val loss: 0.3542 score: 0.9302 time: 0.19s
Test loss: 0.3908 score: 0.9062 time: 0.17s
Epoch 142/1000, LR 0.000278
Train loss: 0.7514;  Loss pred: 0.7514; Loss self: 0.0000; time: 0.52s
Val loss: 0.3475 score: 0.9302 time: 0.25s
Test loss: 0.3843 score: 0.9062 time: 0.21s
Epoch 143/1000, LR 0.000277
Train loss: 0.7475;  Loss pred: 0.7475; Loss self: 0.0000; time: 0.42s
Val loss: 0.3459 score: 0.9302 time: 0.17s
Test loss: 0.3852 score: 0.8906 time: 0.16s
Epoch 144/1000, LR 0.000277
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 0.31s
Val loss: 0.3501 score: 0.9147 time: 0.17s
Test loss: 0.3919 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 145/1000, LR 0.000277
Train loss: 0.7526;  Loss pred: 0.7526; Loss self: 0.0000; time: 0.30s
Val loss: 0.3444 score: 0.9147 time: 0.17s
Test loss: 0.3866 score: 0.8906 time: 0.21s
Epoch 146/1000, LR 0.000277
Train loss: 0.7475;  Loss pred: 0.7475; Loss self: 0.0000; time: 0.54s
Val loss: 0.3316 score: 0.9302 time: 0.26s
Test loss: 0.3716 score: 0.9062 time: 0.21s
Epoch 147/1000, LR 0.000277
Train loss: 0.7352;  Loss pred: 0.7352; Loss self: 0.0000; time: 0.50s
Val loss: 0.3266 score: 0.9457 time: 0.18s
Test loss: 0.3607 score: 0.9219 time: 0.17s
Epoch 148/1000, LR 0.000277
Train loss: 0.7332;  Loss pred: 0.7332; Loss self: 0.0000; time: 0.30s
Val loss: 0.3345 score: 0.9380 time: 0.26s
Test loss: 0.3665 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 149/1000, LR 0.000276
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.34s
Val loss: 0.3393 score: 0.9302 time: 0.23s
Test loss: 0.3718 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 150/1000, LR 0.000276
Train loss: 0.7402;  Loss pred: 0.7402; Loss self: 0.0000; time: 0.50s
Val loss: 0.3397 score: 0.9302 time: 0.31s
Test loss: 0.3728 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 151/1000, LR 0.000276
Train loss: 0.7384;  Loss pred: 0.7384; Loss self: 0.0000; time: 0.50s
Val loss: 0.3344 score: 0.9225 time: 0.25s
Test loss: 0.3682 score: 0.8750 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 152/1000, LR 0.000276
Train loss: 0.7352;  Loss pred: 0.7352; Loss self: 0.0000; time: 0.51s
Val loss: 0.3254 score: 0.9302 time: 0.22s
Test loss: 0.3600 score: 0.8828 time: 0.16s
Epoch 153/1000, LR 0.000276
Train loss: 0.7298;  Loss pred: 0.7298; Loss self: 0.0000; time: 0.31s
Val loss: 0.3151 score: 0.9302 time: 0.17s
Test loss: 0.3503 score: 0.8828 time: 0.16s
Epoch 154/1000, LR 0.000275
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 0.33s
Val loss: 0.3014 score: 0.9380 time: 0.16s
Test loss: 0.3374 score: 0.9062 time: 0.16s
Epoch 155/1000, LR 0.000275
Train loss: 0.7148;  Loss pred: 0.7148; Loss self: 0.0000; time: 0.32s
Val loss: 0.2865 score: 0.9457 time: 0.25s
Test loss: 0.3252 score: 0.9141 time: 0.22s
Epoch 156/1000, LR 0.000275
Train loss: 0.7050;  Loss pred: 0.7050; Loss self: 0.0000; time: 0.43s
Val loss: 0.2778 score: 0.9380 time: 0.22s
Test loss: 0.3198 score: 0.9219 time: 0.16s
Epoch 157/1000, LR 0.000275
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.29s
Val loss: 0.2727 score: 0.9380 time: 0.17s
Test loss: 0.3162 score: 0.9297 time: 0.16s
Epoch 158/1000, LR 0.000275
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.30s
Val loss: 0.2680 score: 0.9380 time: 0.17s
Test loss: 0.3121 score: 0.9297 time: 0.16s
Epoch 159/1000, LR 0.000274
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.32s
Val loss: 0.2636 score: 0.9380 time: 0.24s
Test loss: 0.3080 score: 0.9297 time: 0.21s
Epoch 160/1000, LR 0.000274
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.51s
Val loss: 0.2602 score: 0.9457 time: 0.25s
Test loss: 0.3045 score: 0.9141 time: 0.21s
Epoch 161/1000, LR 0.000274
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.51s
Val loss: 0.2568 score: 0.9380 time: 0.26s
Test loss: 0.3013 score: 0.9141 time: 0.21s
Epoch 162/1000, LR 0.000274
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.42s
Val loss: 0.2560 score: 0.9457 time: 0.23s
Test loss: 0.2998 score: 0.9141 time: 0.16s
Epoch 163/1000, LR 0.000273
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.30s
Val loss: 0.2609 score: 0.9380 time: 0.17s
Test loss: 0.3040 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.30s
Val loss: 0.2641 score: 0.9302 time: 0.17s
Test loss: 0.3079 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 165/1000, LR 0.000273
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.29s
Val loss: 0.2608 score: 0.9302 time: 0.16s
Test loss: 0.3054 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 166/1000, LR 0.000273
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.30s
Val loss: 0.2487 score: 0.9380 time: 0.17s
Test loss: 0.2945 score: 0.9062 time: 0.16s
Epoch 167/1000, LR 0.000273
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.30s
Val loss: 0.2354 score: 0.9457 time: 0.17s
Test loss: 0.2839 score: 0.9141 time: 0.17s
Epoch 168/1000, LR 0.000272
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.33s
Val loss: 0.2299 score: 0.9457 time: 0.25s
Test loss: 0.2799 score: 0.9141 time: 0.21s
Epoch 169/1000, LR 0.000272
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.50s
Val loss: 0.2266 score: 0.9380 time: 0.25s
Test loss: 0.2775 score: 0.9219 time: 0.16s
Epoch 170/1000, LR 0.000272
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.31s
Val loss: 0.2239 score: 0.9457 time: 0.17s
Test loss: 0.2756 score: 0.9141 time: 0.16s
Epoch 171/1000, LR 0.000272
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.31s
Val loss: 0.2213 score: 0.9457 time: 0.17s
Test loss: 0.2738 score: 0.9219 time: 0.16s
Epoch 172/1000, LR 0.000271
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.32s
Val loss: 0.2192 score: 0.9380 time: 0.25s
Test loss: 0.2721 score: 0.9141 time: 0.17s
Epoch 173/1000, LR 0.000271
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.31s
Val loss: 0.2173 score: 0.9380 time: 0.17s
Test loss: 0.2706 score: 0.9141 time: 0.17s
Epoch 174/1000, LR 0.000271
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.31s
Val loss: 0.2148 score: 0.9380 time: 0.17s
Test loss: 0.2686 score: 0.9141 time: 0.16s
Epoch 175/1000, LR 0.000271
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.30s
Val loss: 0.2113 score: 0.9457 time: 0.16s
Test loss: 0.2660 score: 0.9141 time: 0.16s
Epoch 176/1000, LR 0.000271
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.35s
Val loss: 0.2089 score: 0.9457 time: 0.17s
Test loss: 0.2640 score: 0.9141 time: 0.21s
Epoch 177/1000, LR 0.000270
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.49s
Val loss: 0.2086 score: 0.9457 time: 0.26s
Test loss: 0.2635 score: 0.9141 time: 0.19s
Epoch 178/1000, LR 0.000270
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.31s
Val loss: 0.2104 score: 0.9380 time: 0.17s
Test loss: 0.2647 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 179/1000, LR 0.000270
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.31s
Val loss: 0.2082 score: 0.9380 time: 0.17s
Test loss: 0.2628 score: 0.9141 time: 0.15s
Epoch 180/1000, LR 0.000270
Train loss: 0.6504;  Loss pred: 0.6504; Loss self: 0.0000; time: 0.30s
Val loss: 0.2033 score: 0.9457 time: 0.17s
Test loss: 0.2585 score: 0.9141 time: 0.16s
Epoch 181/1000, LR 0.000269
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.30s
Val loss: 0.2000 score: 0.9380 time: 0.17s
Test loss: 0.2556 score: 0.9219 time: 0.15s
Epoch 182/1000, LR 0.000269
Train loss: 0.6434;  Loss pred: 0.6434; Loss self: 0.0000; time: 0.30s
Val loss: 0.1968 score: 0.9380 time: 0.17s
Test loss: 0.2526 score: 0.9141 time: 0.16s
Epoch 183/1000, LR 0.000269
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 0.34s
Val loss: 0.2022 score: 0.9380 time: 0.22s
Test loss: 0.2590 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 184/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.50s
Val loss: 0.2039 score: 0.9380 time: 0.18s
Test loss: 0.2613 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 185/1000, LR 0.000268
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.31s
Val loss: 0.1969 score: 0.9380 time: 0.17s
Test loss: 0.2537 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 186/1000, LR 0.000268
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.33s
Val loss: 0.1929 score: 0.9380 time: 0.19s
Test loss: 0.2500 score: 0.9141 time: 0.21s
Epoch 187/1000, LR 0.000268
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.50s
Val loss: 0.1927 score: 0.9380 time: 0.25s
Test loss: 0.2501 score: 0.9219 time: 0.21s
Epoch 188/1000, LR 0.000268
Train loss: 0.6357;  Loss pred: 0.6357; Loss self: 0.0000; time: 0.51s
Val loss: 0.1904 score: 0.9380 time: 0.25s
Test loss: 0.2475 score: 0.9219 time: 0.22s
Epoch 189/1000, LR 0.000267
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.51s
Val loss: 0.1980 score: 0.9380 time: 0.26s
Test loss: 0.2561 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.32s
Val loss: 0.2034 score: 0.9302 time: 0.19s
Test loss: 0.2626 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.31s
Val loss: 0.1972 score: 0.9380 time: 0.19s
Test loss: 0.2560 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 192/1000, LR 0.000267
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.32s
Val loss: 0.1911 score: 0.9380 time: 0.18s
Test loss: 0.2495 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 193/1000, LR 0.000266
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.31s
Val loss: 0.1884 score: 0.9380 time: 0.18s
Test loss: 0.2469 score: 0.9141 time: 0.18s
Epoch 194/1000, LR 0.000266
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.32s
Val loss: 0.1898 score: 0.9380 time: 0.17s
Test loss: 0.2489 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 195/1000, LR 0.000266
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.36s
Val loss: 0.1944 score: 0.9302 time: 0.22s
Test loss: 0.2544 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 196/1000, LR 0.000266
Train loss: 0.6353;  Loss pred: 0.6353; Loss self: 0.0000; time: 0.31s
Val loss: 0.2103 score: 0.9302 time: 0.18s
Test loss: 0.2731 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 197/1000, LR 0.000265
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.31s
Val loss: 0.2086 score: 0.9302 time: 0.18s
Test loss: 0.2717 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 198/1000, LR 0.000265
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.32s
Val loss: 0.1989 score: 0.9302 time: 0.18s
Test loss: 0.2610 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 199/1000, LR 0.000265
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.31s
Val loss: 0.2004 score: 0.9302 time: 0.18s
Test loss: 0.2633 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 200/1000, LR 0.000265
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.37s
Val loss: 0.1965 score: 0.9302 time: 0.18s
Test loss: 0.2593 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 201/1000, LR 0.000264
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.44s
Val loss: 0.1914 score: 0.9380 time: 0.23s
Test loss: 0.2539 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 202/1000, LR 0.000264
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.32s
Val loss: 0.1866 score: 0.9380 time: 0.18s
Test loss: 0.2489 score: 0.9141 time: 0.17s
Epoch 203/1000, LR 0.000264
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.31s
Val loss: 0.1828 score: 0.9380 time: 0.18s
Test loss: 0.2449 score: 0.9219 time: 0.17s
Epoch 204/1000, LR 0.000264
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.31s
Val loss: 0.1820 score: 0.9380 time: 0.18s
Test loss: 0.2436 score: 0.9141 time: 0.17s
Epoch 205/1000, LR 0.000263
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.32s
Val loss: 0.1820 score: 0.9457 time: 0.18s
Test loss: 0.2441 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.6217;  Loss pred: 0.6217; Loss self: 0.0000; time: 0.31s
Val loss: 0.1819 score: 0.9457 time: 0.18s
Test loss: 0.2447 score: 0.9141 time: 0.16s
Epoch 207/1000, LR 0.000263
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.38s
Val loss: 0.1799 score: 0.9457 time: 0.17s
Test loss: 0.2429 score: 0.9141 time: 0.16s
Epoch 208/1000, LR 0.000263
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.31s
Val loss: 0.1762 score: 0.9302 time: 0.17s
Test loss: 0.2391 score: 0.9141 time: 0.16s
Epoch 209/1000, LR 0.000262
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.30s
Val loss: 0.1790 score: 0.9380 time: 0.17s
Test loss: 0.2408 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.31s
Val loss: 0.1809 score: 0.9380 time: 0.17s
Test loss: 0.2418 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 211/1000, LR 0.000262
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.32s
Val loss: 0.1801 score: 0.9380 time: 0.23s
Test loss: 0.2402 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 212/1000, LR 0.000261
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.30s
Val loss: 0.1775 score: 0.9380 time: 0.17s
Test loss: 0.2369 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 213/1000, LR 0.000261
Train loss: 0.6120;  Loss pred: 0.6120; Loss self: 0.0000; time: 0.30s
Val loss: 0.1764 score: 0.9380 time: 0.17s
Test loss: 0.2355 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 214/1000, LR 0.000261
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.30s
Val loss: 0.1760 score: 0.9302 time: 0.16s
Test loss: 0.2352 score: 0.9141 time: 0.16s
Epoch 215/1000, LR 0.000261
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.31s
Val loss: 0.1770 score: 0.9457 time: 0.17s
Test loss: 0.2366 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 216/1000, LR 0.000260
Train loss: 0.6155;  Loss pred: 0.6155; Loss self: 0.0000; time: 0.34s
Val loss: 0.1776 score: 0.9457 time: 0.18s
Test loss: 0.2374 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 217/1000, LR 0.000260
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.36s
Val loss: 0.1764 score: 0.9457 time: 0.17s
Test loss: 0.2362 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 218/1000, LR 0.000260
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.30s
Val loss: 0.1739 score: 0.9457 time: 0.17s
Test loss: 0.2337 score: 0.9219 time: 0.16s
Epoch 219/1000, LR 0.000260
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 0.30s
Val loss: 0.1717 score: 0.9380 time: 0.17s
Test loss: 0.2313 score: 0.9219 time: 0.17s
Epoch 220/1000, LR 0.000259
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.31s
Val loss: 0.1704 score: 0.9380 time: 0.18s
Test loss: 0.2301 score: 0.9219 time: 0.18s
Epoch 221/1000, LR 0.000259
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.44s
Val loss: 0.1690 score: 0.9380 time: 0.26s
Test loss: 0.2288 score: 0.9219 time: 0.21s
Epoch 222/1000, LR 0.000259
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.45s
Val loss: 0.1674 score: 0.9380 time: 0.23s
Test loss: 0.2273 score: 0.9219 time: 0.17s
Epoch 223/1000, LR 0.000258
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.29s
Val loss: 0.1658 score: 0.9380 time: 0.17s
Test loss: 0.2255 score: 0.9219 time: 0.16s
Epoch 224/1000, LR 0.000258
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.30s
Val loss: 0.1642 score: 0.9380 time: 0.17s
Test loss: 0.2238 score: 0.9141 time: 0.17s
Epoch 225/1000, LR 0.000258
Train loss: 0.6035;  Loss pred: 0.6035; Loss self: 0.0000; time: 0.29s
Val loss: 0.1629 score: 0.9380 time: 0.17s
Test loss: 0.2225 score: 0.9141 time: 0.16s
Epoch 226/1000, LR 0.000258
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.29s
Val loss: 0.1616 score: 0.9380 time: 0.16s
Test loss: 0.2213 score: 0.9141 time: 0.16s
Epoch 227/1000, LR 0.000257
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 0.33s
Val loss: 0.1608 score: 0.9380 time: 0.18s
Test loss: 0.2206 score: 0.9141 time: 0.15s
Epoch 228/1000, LR 0.000257
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.31s
Val loss: 0.1604 score: 0.9380 time: 0.21s
Test loss: 0.2206 score: 0.9219 time: 0.16s
Epoch 229/1000, LR 0.000257
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.30s
Val loss: 0.1598 score: 0.9380 time: 0.17s
Test loss: 0.2200 score: 0.9219 time: 0.16s
Epoch 230/1000, LR 0.000256
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.30s
Val loss: 0.1591 score: 0.9380 time: 0.17s
Test loss: 0.2193 score: 0.9219 time: 0.16s
Epoch 231/1000, LR 0.000256
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.29s
Val loss: 0.1585 score: 0.9380 time: 0.16s
Test loss: 0.2184 score: 0.9219 time: 0.15s
Epoch 232/1000, LR 0.000256
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.39s
Val loss: 0.1577 score: 0.9380 time: 0.18s
Test loss: 0.2171 score: 0.9141 time: 0.17s
Epoch 233/1000, LR 0.000255
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.33s
Val loss: 0.1567 score: 0.9302 time: 0.19s
Test loss: 0.2156 score: 0.9141 time: 0.21s
Epoch 234/1000, LR 0.000255
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.43s
Val loss: 0.1560 score: 0.9302 time: 0.18s
Test loss: 0.2146 score: 0.9141 time: 0.17s
Epoch 235/1000, LR 0.000255
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.1553 score: 0.9302 time: 0.18s
Test loss: 0.2139 score: 0.9141 time: 0.17s
Epoch 236/1000, LR 0.000255
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.31s
Val loss: 0.1547 score: 0.9302 time: 0.24s
Test loss: 0.2134 score: 0.9141 time: 0.17s
Epoch 237/1000, LR 0.000254
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 0.28s
Val loss: 0.1542 score: 0.9302 time: 0.17s
Test loss: 0.2129 score: 0.9141 time: 0.16s
Epoch 238/1000, LR 0.000254
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.28s
Val loss: 0.1541 score: 0.9302 time: 0.21s
Test loss: 0.2134 score: 0.9141 time: 0.16s
Epoch 239/1000, LR 0.000254
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.45s
Val loss: 0.1550 score: 0.9380 time: 0.24s
Test loss: 0.2157 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 240/1000, LR 0.000253
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.41s
Val loss: 0.1559 score: 0.9457 time: 0.17s
Test loss: 0.2177 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 241/1000, LR 0.000253
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.31s
Val loss: 0.1565 score: 0.9457 time: 0.18s
Test loss: 0.2189 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 242/1000, LR 0.000253
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.30s
Val loss: 0.1562 score: 0.9535 time: 0.18s
Test loss: 0.2186 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 243/1000, LR 0.000252
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.31s
Val loss: 0.1554 score: 0.9457 time: 0.18s
Test loss: 0.2176 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 244/1000, LR 0.000252
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.32s
Val loss: 0.1541 score: 0.9380 time: 0.19s
Test loss: 0.2158 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 245/1000, LR 0.000252
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.45s
Val loss: 0.1524 score: 0.9302 time: 0.27s
Test loss: 0.2130 score: 0.9141 time: 0.22s
Epoch 246/1000, LR 0.000252
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.49s
Val loss: 0.1508 score: 0.9302 time: 0.23s
Test loss: 0.2100 score: 0.9141 time: 0.17s
Epoch 247/1000, LR 0.000251
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.34s
Val loss: 0.1502 score: 0.9380 time: 0.18s
Test loss: 0.2084 score: 0.9062 time: 0.17s
Epoch 248/1000, LR 0.000251
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.32s
Val loss: 0.1510 score: 0.9380 time: 0.18s
Test loss: 0.2080 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000251
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.32s
Val loss: 0.1512 score: 0.9380 time: 0.18s
Test loss: 0.2077 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 250/1000, LR 0.000250
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.30s
Val loss: 0.1503 score: 0.9380 time: 0.17s
Test loss: 0.2067 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 251/1000, LR 0.000250
Train loss: 0.5885;  Loss pred: 0.5885; Loss self: 0.0000; time: 0.32s
Val loss: 0.1481 score: 0.9380 time: 0.18s
Test loss: 0.2048 score: 0.9141 time: 0.17s
Epoch 252/1000, LR 0.000250
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.41s
Val loss: 0.1461 score: 0.9302 time: 0.18s
Test loss: 0.2039 score: 0.9062 time: 0.17s
Epoch 253/1000, LR 0.000249
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.31s
Val loss: 0.1464 score: 0.9302 time: 0.18s
Test loss: 0.2058 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 254/1000, LR 0.000249
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.31s
Val loss: 0.1474 score: 0.9457 time: 0.17s
Test loss: 0.2081 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 255/1000, LR 0.000249
Train loss: 0.5869;  Loss pred: 0.5869; Loss self: 0.0000; time: 0.31s
Val loss: 0.1491 score: 0.9457 time: 0.17s
Test loss: 0.2112 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 256/1000, LR 0.000248
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.33s
Val loss: 0.1514 score: 0.9457 time: 0.19s
Test loss: 0.2155 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 257/1000, LR 0.000248
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.49s
Val loss: 0.1463 score: 0.9535 time: 0.19s
Test loss: 0.2078 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 258/1000, LR 0.000248
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.31s
Val loss: 0.1427 score: 0.9380 time: 0.19s
Test loss: 0.1982 score: 0.9141 time: 0.17s
Epoch 259/1000, LR 0.000247
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.32s
Val loss: 0.1492 score: 0.9457 time: 0.18s
Test loss: 0.2031 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 260/1000, LR 0.000247
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.35s
Val loss: 0.1560 score: 0.9535 time: 0.18s
Test loss: 0.2105 score: 0.9219 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 261/1000, LR 0.000247
Train loss: 0.5901;  Loss pred: 0.5901; Loss self: 0.0000; time: 0.51s
Val loss: 0.1581 score: 0.9612 time: 0.26s
Test loss: 0.2128 score: 0.9375 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 262/1000, LR 0.000246
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.51s
Val loss: 0.1548 score: 0.9535 time: 0.25s
Test loss: 0.2089 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 263/1000, LR 0.000246
Train loss: 0.5897;  Loss pred: 0.5897; Loss self: 0.0000; time: 0.32s
Val loss: 0.1482 score: 0.9535 time: 0.18s
Test loss: 0.2016 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 264/1000, LR 0.000246
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.30s
Val loss: 0.1412 score: 0.9457 time: 0.20s
Test loss: 0.1949 score: 0.9141 time: 0.18s
Epoch 265/1000, LR 0.000245
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.30s
Val loss: 0.1384 score: 0.9302 time: 0.17s
Test loss: 0.1939 score: 0.9062 time: 0.16s
Epoch 266/1000, LR 0.000245
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 0.32s
Val loss: 0.1394 score: 0.9457 time: 0.25s
Test loss: 0.1978 score: 0.9141 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 267/1000, LR 0.000245
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.31s
Val loss: 0.1418 score: 0.9457 time: 0.19s
Test loss: 0.2024 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 268/1000, LR 0.000244
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.31s
Val loss: 0.1438 score: 0.9457 time: 0.18s
Test loss: 0.2060 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 269/1000, LR 0.000244
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.51s
Val loss: 0.1444 score: 0.9457 time: 0.26s
Test loss: 0.2071 score: 0.9219 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 270/1000, LR 0.000244
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.52s
Val loss: 0.1435 score: 0.9457 time: 0.19s
Test loss: 0.2061 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 271/1000, LR 0.000243
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.30s
Val loss: 0.1425 score: 0.9457 time: 0.17s
Test loss: 0.2049 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 272/1000, LR 0.000243
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.30s
Val loss: 0.1409 score: 0.9457 time: 0.17s
Test loss: 0.2027 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 273/1000, LR 0.000243
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.30s
Val loss: 0.1390 score: 0.9535 time: 0.17s
Test loss: 0.1998 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 274/1000, LR 0.000242
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.30s
Val loss: 0.1378 score: 0.9535 time: 0.17s
Test loss: 0.1980 score: 0.9219 time: 0.15s
Epoch 275/1000, LR 0.000242
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.35s
Val loss: 0.1373 score: 0.9535 time: 0.18s
Test loss: 0.1974 score: 0.9219 time: 0.18s
Epoch 276/1000, LR 0.000242
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.33s
Val loss: 0.1366 score: 0.9535 time: 0.17s
Test loss: 0.1963 score: 0.9219 time: 0.16s
Epoch 277/1000, LR 0.000241
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.31s
Val loss: 0.1358 score: 0.9457 time: 0.18s
Test loss: 0.1950 score: 0.9141 time: 0.16s
Epoch 278/1000, LR 0.000241
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.30s
Val loss: 0.1352 score: 0.9302 time: 0.18s
Test loss: 0.1940 score: 0.9141 time: 0.16s
Epoch 279/1000, LR 0.000241
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.30s
Val loss: 0.1346 score: 0.9302 time: 0.17s
Test loss: 0.1929 score: 0.9141 time: 0.16s
Epoch 280/1000, LR 0.000240
Train loss: 0.5755;  Loss pred: 0.5755; Loss self: 0.0000; time: 0.30s
Val loss: 0.1342 score: 0.9302 time: 0.17s
Test loss: 0.1920 score: 0.9141 time: 0.17s
Epoch 281/1000, LR 0.000240
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.43s
Val loss: 0.1350 score: 0.9302 time: 0.25s
Test loss: 0.1899 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 282/1000, LR 0.000240
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 0.30s
Val loss: 0.1578 score: 0.9535 time: 0.17s
Test loss: 0.2138 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 283/1000, LR 0.000239
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.30s
Val loss: 0.1825 score: 0.9457 time: 0.17s
Test loss: 0.2435 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 284/1000, LR 0.000239
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.31s
Val loss: 0.1849 score: 0.9457 time: 0.17s
Test loss: 0.2464 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 285/1000, LR 0.000239
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 0.30s
Val loss: 0.1742 score: 0.9457 time: 0.18s
Test loss: 0.2334 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 286/1000, LR 0.000238
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.30s
Val loss: 0.1616 score: 0.9535 time: 0.17s
Test loss: 0.2185 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 287/1000, LR 0.000238
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.31s
Val loss: 0.1537 score: 0.9535 time: 0.25s
Test loss: 0.2096 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 288/1000, LR 0.000237
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.31s
Val loss: 0.1450 score: 0.9535 time: 0.16s
Test loss: 0.2004 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 289/1000, LR 0.000237
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.30s
Val loss: 0.1372 score: 0.9380 time: 0.17s
Test loss: 0.1936 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 290/1000, LR 0.000237
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.52s
Val loss: 0.1358 score: 0.9302 time: 0.25s
Test loss: 0.1971 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 291/1000, LR 0.000236
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.50s
Val loss: 0.1429 score: 0.9457 time: 0.22s
Test loss: 0.2099 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 292/1000, LR 0.000236
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.31s
Val loss: 0.1508 score: 0.9457 time: 0.17s
Test loss: 0.2218 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 293/1000, LR 0.000236
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.30s
Val loss: 0.1567 score: 0.9457 time: 0.17s
Test loss: 0.2302 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 294/1000, LR 0.000235
Train loss: 0.5797;  Loss pred: 0.5797; Loss self: 0.0000; time: 0.30s
Val loss: 0.1582 score: 0.9457 time: 0.17s
Test loss: 0.2325 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 295/1000, LR 0.000235
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.30s
Val loss: 0.1552 score: 0.9457 time: 0.17s
Test loss: 0.2286 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 296/1000, LR 0.000235
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.29s
Val loss: 0.1513 score: 0.9457 time: 0.16s
Test loss: 0.2234 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 297/1000, LR 0.000234
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.34s
Val loss: 0.1472 score: 0.9457 time: 0.17s
Test loss: 0.2178 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 298/1000, LR 0.000234
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.36s
Val loss: 0.1430 score: 0.9457 time: 0.16s
Test loss: 0.2117 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 299/1000, LR 0.000234
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.30s
Val loss: 0.1395 score: 0.9457 time: 0.17s
Test loss: 0.2062 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 300/1000, LR 0.000233
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.30s
Val loss: 0.1371 score: 0.9535 time: 0.17s
Test loss: 0.2023 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 279,   Train_Loss: 0.5755,   Val_Loss: 0.1342,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1342,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9141,   Test_loss: 0.1920


[0.20760776498354971, 0.16605692077428102, 0.17056229896843433, 0.17562605999410152, 0.183626146055758, 0.19163804687559605, 0.17879367503337562, 0.18349444796331227, 0.18030035379342735, 0.1825567870400846, 0.23094463488087058, 0.24746026215143502, 0.169254096923396, 0.17146165180020034, 0.16945469309575856, 0.16782164690084755, 0.16591966804116964, 0.2604686859995127, 0.1679470408707857, 0.18327770614996552, 0.17623785906471312, 0.16604165500029922, 0.2821390319149941, 0.22657390194945037, 0.1755640609189868, 0.1777474139817059, 0.18578408285975456, 0.18337985407561064, 0.2303225041832775, 0.1886288640089333, 0.1893351359758526, 0.18791426299139857, 0.18767704395577312, 0.20001850090920925, 0.235397047130391, 0.18361179088242352, 0.18147828592918813, 0.17513450188562274, 0.18334317998960614, 0.2581198059488088, 0.2517814820166677, 0.1826078798621893, 0.1942509119398892, 0.17257608915679157, 0.1775593189522624, 0.17831494892016053, 0.17423899401910603, 0.1748086200095713, 0.16898676799610257, 0.22426744806580245, 0.17320013791322708, 0.1734763530548662, 0.1705039970111102, 0.17772014415822923, 0.24498232803307474, 0.17179809999652207, 0.174724510172382, 0.1709221708588302, 0.1720204190351069, 0.18837493611499667, 0.2541725051123649, 0.2342766630463302, 0.16764483996666968, 0.16863659117370844, 0.17866831016726792, 0.17894160095602274, 0.18777737906202674, 0.19234586600214243, 0.227551860967651, 0.17522173700854182, 0.17584308120422065, 0.2531487508676946, 0.2624670311342925, 0.18280101101845503, 0.18204256193712354, 0.1826819470152259, 0.2561954839620739, 0.185029179090634, 0.18078141589649022, 0.25539965205825865, 0.2519312258809805, 0.27073586406186223, 0.17641372000798583, 0.1630205528344959, 0.18329317588359118, 0.19258446781896055, 0.1892592350486666, 0.1946041330229491, 0.1807088078930974, 0.1757434920873493, 0.16891504591330886, 0.18527165101841092, 0.25442420202307403, 0.2538715498521924, 0.18155026203021407, 0.1819934321101755, 0.1759739718399942, 0.17703308607451618, 0.172641467070207, 0.17946191085502505, 0.1737636341713369, 0.1749967080540955, 0.16470035980455577, 0.16791766905225813, 0.1856831240002066, 0.1856447970494628, 0.18546071089804173, 0.170321024954319, 0.1694547429215163, 0.263682221993804, 0.174704825039953, 0.1775144108105451, 0.2593510551378131, 0.2538635029923171, 0.26320866611786187, 0.17723405407741666, 0.1913991430774331, 0.2169508400838822, 0.18665050785057247, 0.1831160499714315, 0.1730460491962731, 0.17507549305446446, 0.1977320108562708, 0.17287560901604593, 0.1839523292146623, 0.18310444406233728, 0.19459805195219815, 0.20188505691476166, 0.18010040605440736, 0.17531064618378878, 0.18319220887497067, 0.1829089478123933, 0.1840755131561309, 0.1740313118789345, 0.1928228260949254, 0.18387638707645237, 0.1781983389519155, 0.17489577503874898, 0.1752992409747094, 0.17691366118378937, 0.18518893886357546, 0.17172211199067533, 0.19687853707000613, 0.25599591387435794, 0.18202265002764761, 0.1760478070937097, 0.17624175106175244, 0.17886801506392658, 0.22133147390559316, 0.20022541796788573, 0.17252260306850076, 0.1869047051295638, 0.18561848811805248, 0.25203357287682593, 0.1903204950504005, 0.18787872092798352, 0.19091328885406256, 0.19172335509210825, 0.18131729098968208, 0.22471544099971652, 0.18529939791187644, 0.18669098196551204, 0.26057369401678443, 0.27706012199632823, 0.27476208889856935, 0.26861713686957955, 0.1715617161244154, 0.1818888820707798, 0.1848716561216861, 0.18424214399419725, 0.24422782519832253, 0.25812930683605373, 0.17129328101873398, 0.17182862502522767, 0.17106351512484252, 0.16721125598996878, 0.17334144096821547, 0.21029490092769265, 0.1921279348898679, 0.18669158197008073, 0.1751058828085661, 0.17458516987971961, 0.17969605792313814, 0.19018812314607203, 0.18706377386115491, 0.19380989810451865, 0.19117926503531635, 0.1846109121106565, 0.21767481393180788, 0.19135821191594005, 0.18483771686442196, 0.18729169899597764, 0.190573571017012, 0.19275345280766487, 0.23504659603349864, 0.18853032100014389, 0.16704799211584032, 0.17499705497175455, 0.17892867815680802, 0.26982560590840876, 0.1906441890168935, 0.17672756290994585, 0.17488995310850441, 0.18145604082383215, 0.17640410596504807, 0.2325800210237503, 0.17679447308182716, 0.1807174808345735, 0.2581492660101503, 0.27065136982128024, 0.18201127089560032, 0.18048020685091615, 0.19234415097162127, 0.17832101392559707, 0.19343808316625655, 0.25188571494072676, 0.233565263915807, 0.1765216630883515, 0.18203331297263503, 0.2514482119586319, 0.25591309601441026, 0.2535775820724666, 0.25424993596971035, 0.17430246993899345, 0.17443433916196227, 0.19006069097667933, 0.18727849097922444, 0.176635738927871, 0.1743895378895104, 0.17375703691504896, 0.18200277886353433, 0.17512308689765632, 0.17285614204593003, 0.17394361505284905, 0.18323494517244399, 0.1851992520969361, 0.18197311414405704, 0.17845230596140027, 0.1829737580846995, 0.1903204470872879, 0.22180069400928915, 0.19656386598944664, 0.18858051905408502, 0.18040824797935784, 0.1770752859301865, 0.23238638206385076, 0.17725733621045947, 0.17719315993599594, 0.17750552808865905, 0.1721761249937117, 0.17533176904544234, 0.1781147848814726, 0.18026747507974505, 0.17792277014814317, 0.17840653890743852, 0.17489333800040185, 0.18553963000886142, 0.1979256549384445, 0.17673884192481637, 0.17396782687865198, 0.17617574892938137, 0.18675140500999987, 0.182097113924101, 0.17962827812880278, 0.18008308298885822, 0.17333669099025428, 0.20666922186501324, 0.1728289918974042, 0.1768438220024109, 0.16937229200266302, 0.16902599507011473, 0.18150424701161683, 0.16999270487576723, 0.18201163318008184, 0.19065504800528288, 0.17562038311734796, 0.18519056285731494, 0.26217527384869754, 0.2749720369465649, 0.1723147730808705, 0.23983300803229213, 0.17482454795390368, 0.16836791788227856, 0.21998040191829205, 0.24796353606507182, 0.2622422210406512, 0.17585982894524932, 0.170207594987005, 0.16902252798900008, 0.16960572800599039, 0.2449571460019797, 0.18008339102379978, 0.18157325591892004, 0.1959526299033314, 0.2579661109484732, 0.18663215590640903, 0.18392242514528334, 0.1774893591646105, 0.16886833286844194, 0.16820145398378372, 0.17001055111177266, 0.1726017789915204, 0.16701702889986336, 0.17020741594024003, 0.17917330493219197, 0.18159364792518318, 0.19207064504735172, 0.1732495790347457, 0.17134076496586204, 0.18370019900612533, 0.17256067995913327, 0.17607944598421454, 0.25405735499225557, 0.16935505392029881, 0.1696208801586181, 0.17083327495492995, 0.17099002399481833, 0.17480947612784803, 0.19332399289123714, 0.17699351999908686, 0.17000415199436247, 0.25591849884949625, 0.2506128849927336, 0.17634902708232403, 0.17738017300143838, 0.18127540708519518, 0.17114142701029778, 0.25885300897061825, 0.18635550700128078, 0.1816740359645337, 0.16862304299138486, 0.17229706305079162, 0.16886398498900235, 0.17414654698222876, 0.1981652439571917, 0.21171443699859083, 0.1894701609853655, 0.25155637389980257, 0.24560700682923198, 0.25381532800383866, 0.16790473298169672, 0.17167749418877065, 0.18291741609573364, 0.17201759410090744, 0.17277813400141895, 0.17809282802045345, 0.1716849950607866, 0.16949958098120987, 0.2424674869980663, 0.24963090103119612, 0.24734934698790312, 0.1759705268777907, 0.17140902299433947, 0.18287807097658515, 0.17432033410295844, 0.26146151311695576, 0.2573076479602605, 0.175322940107435, 0.18466982897371054, 0.18426507990807295, 0.18436506809666753, 0.2735900869593024, 0.1833690379280597, 0.1793315268587321, 0.1834127618931234, 0.19779370189644396, 0.19816784583963454, 0.18166885594837368, 0.17929888586513698, 0.1948186159133911, 0.18695669109001756, 0.26214973302558064, 0.19676160207018256, 0.24533285293728113, 0.20691395411267877, 0.1799664821010083, 0.2222414780408144, 0.18053367198444903, 0.19861003779806197, 0.19866069802083075, 0.27492605708539486, 0.19803870003670454, 0.20590451592579484, 0.2039390979334712, 0.20430450094863772, 0.27628389396704733, 0.1975996200926602, 0.1800717180594802, 0.2710263270419091, 0.2684377860277891, 0.2704965409357101, 0.1848256061784923, 0.19544989708811045, 0.26960123097524047, 0.27289148489944637, 0.19525466090999544, 0.19217824307270348, 0.2556428511161357, 0.17868610308505595, 0.17655501002445817, 0.18999375705607235, 0.17690654704347253, 0.23976480006240308, 0.2538996930234134, 0.2580697510857135, 0.17947588488459587, 0.17676347913220525, 0.25461259204894304, 0.26009761495515704, 0.2441159849986434, 0.1790195421781391, 0.18140233401209116, 0.18160966783761978, 0.18507866701111197, 0.18639795598573983, 0.18185783503577113, 0.1804083869792521, 0.18854067684151232, 0.1778207328170538, 0.1837225779891014, 0.19516626512631774, 0.18977250810712576, 0.19297044002451003, 0.18083447683602571, 0.18197179702110589, 0.17911190120503306, 0.24398714303970337, 0.18637878890149295, 0.17905222601257265, 0.18395033408887684, 0.23923760605975986, 0.18375505786389112, 0.18074191198684275, 0.178380758035928, 0.1899072548840195, 0.26363762095570564, 0.18552728183567524, 0.1771739290561527, 0.17674743593670428, 0.18692478514276445, 0.27026362414471805, 0.1794868940487504, 0.19239011104218662, 0.16756133106537163, 0.169463629135862, 0.18631174601614475, 0.2086286989506334, 0.18740364396944642, 0.1958043109625578, 0.18795015383511782, 0.1743054601829499, 0.1843254161067307, 0.21176343387924135, 0.1900589179713279, 0.181353148072958, 0.19768848689273, 0.2045703891199082, 0.2231669679749757, 0.17577824601903558, 0.17418941692449152, 0.17416829406283796, 0.17365551693364978, 0.2580072549171746, 0.1759955349843949, 0.17314566485583782, 0.17057365411892533, 0.17762757185846567, 0.17965718894265592, 0.2560008999425918, 0.21100113401189446, 0.164938576053828, 0.16857541492208838, 0.18972771894186735, 0.22092125704512, 0.22893994417972863, 0.18023107293993235, 0.19616690906696022, 0.16186167299747467, 0.17950190207920969, 0.1637683459557593, 0.17488967208191752, 0.17226162692531943, 0.1758949940558523, 0.20138650899752975, 0.22378034703433514, 0.17925537610426545, 0.1768131018616259, 0.1761487009935081, 0.21956717385910451, 0.22370822494849563, 0.171003035036847, 0.171049146912992, 0.1732418320607394, 0.2172932589892298, 0.21091777016408741, 0.2237785761244595, 0.17939010099507868, 0.1843594708479941, 0.18456132803112268, 0.15897795394994318, 0.1590717399958521, 0.21476645907387137, 0.15814144583418965, 0.15996963903307915, 0.15755490004085004, 0.15942927193827927, 0.15563799091614783, 0.21259614010341465, 0.1591314459219575, 0.15789569402113557, 0.16206849995069206, 0.16213698801584542, 0.17309296992607415, 0.17070303088985384, 0.1695958310738206, 0.17107285908423364, 0.17020790907554328, 0.2253897967748344, 0.16827471693977714, 0.1693569179624319, 0.1700023531448096, 0.16976194898597896, 0.19238493591547012, 0.17124307597987354, 0.16793241607956588, 0.1579308258369565, 0.16226200107485056, 0.17237664619460702, 0.16232578898780048, 0.1608270569704473, 0.1589201029855758, 0.16253073001280427, 0.153610227862373, 0.15260546701028943, 0.15546786691993475, 0.16336977784521878, 0.16326531302183867, 0.20197935379110277, 0.1641661908943206, 0.17061781114898622, 0.1603243718855083, 0.1667627259157598, 0.21837524184957147, 0.16855996777303517, 0.16479952004738152, 0.1647282380145043, 0.2004115351010114, 0.1757529708556831, 0.23275437019765377, 0.17925517191179097, 0.16737687098793685, 0.16051337611861527, 0.1730151418596506, 0.1741135842166841, 0.16091110394336283, 0.16746686585247517, 0.1648674109019339, 0.16028187097981572, 0.16041386011056602, 0.15956404991447926, 0.16842233389616013, 0.16320278495550156, 0.162811383837834, 0.16682179202325642, 0.16296356916427612, 0.16795968497171998, 0.1730338770430535, 0.16827017813920975, 0.168438641121611, 0.16655162908136845, 0.2111314618960023, 0.21380389295518398, 0.16842723498120904, 0.17146362294442952, 0.2156014391221106, 0.2231815829873085, 0.22295545600354671, 0.17176304012537003, 0.1709958810824901, 0.19769990001805127, 0.1656207488849759, 0.19697801279835403, 0.16938263410702348, 0.16812933003529906, 0.1667037399020046, 0.17270759004168212, 0.16815558588132262, 0.16787597304210067, 0.21707340003922582, 0.15686875395476818, 0.15939868101850152, 0.1597519489005208, 0.18077809503301978, 0.21111435210332274, 0.1605530900415033, 0.16245749313384295, 0.1596949161030352, 0.17410787311382592, 0.19715404300950468, 0.16488164896145463, 0.1625561269465834, 0.15932787884958088, 0.15668584289960563, 0.2207491269800812, 0.232179616112262, 0.16388188488781452, 0.1663646788801998, 0.16948968707583845, 0.17631041095592082, 0.17647208413109183, 0.21316322102211416, 0.16814658814109862, 0.16971703711897135, 0.21804053499363363, 0.21135538117960095, 0.1712513139937073, 0.16087470995262265, 0.21910531586036086, 0.20475508901290596, 0.21322147897444665, 0.16384090180508792, 0.164575019152835, 0.16028590593487024, 0.22153028892353177, 0.16214074403978884, 0.1666027109604329, 0.165466221049428, 0.21118851378560066, 0.21352710900828242, 0.21105025196447968, 0.16706584999337792, 0.16654836107045412, 0.16321445791982114, 0.16015936085022986, 0.16174943186342716, 0.17009893106296659, 0.21117240493185818, 0.16563088702969253, 0.16501735500060022, 0.16047663986682892, 0.17012052307836711, 0.1718135359697044, 0.16126263793557882, 0.16159411799162626, 0.20999366510659456, 0.19012841396033764, 0.16011744807474315, 0.15742460289038718, 0.16175810410641134, 0.15873170108534396, 0.16485673491843045, 0.21333184698596597, 0.16621825890615582, 0.16962978499941528, 0.21263027098029852, 0.21109333005733788, 0.21952272206544876, 0.17463316698558629, 0.17625791090540588, 0.17358171800151467, 0.1775259638670832, 0.1800673280376941, 0.15796529408544302, 0.2095820950344205, 0.16882191691547632, 0.1703468351624906, 0.17354538687504828, 0.1704018129967153, 0.17168978694826365, 0.17659354605711997, 0.1723913149908185, 0.1733581239823252, 0.17206341680139303, 0.1732272191438824, 0.16762169497087598, 0.16353210411034524, 0.1601741430349648, 0.16320846905000508, 0.1637477281037718, 0.18350897915661335, 0.16896027186885476, 0.16425130306743085, 0.16438771807588637, 0.16739102685824037, 0.1873992821201682, 0.16355511383153498, 0.16326053510420024, 0.177407321985811, 0.18593109003268182, 0.2152912369929254, 0.178038447862491, 0.16510534891858697, 0.1696096530649811, 0.16565810004249215, 0.1618417608551681, 0.1585249558556825, 0.16012232191860676, 0.15978939598426223, 0.15973014803603292, 0.15699266595765948, 0.17052764003165066, 0.21432920498773456, 0.17851881310343742, 0.17459638905711472, 0.17566157784312963, 0.15982119599357247, 0.16229328489862382, 0.20719488384202123, 0.16358727379702032, 0.1707466950174421, 0.17388128000311553, 0.17800294584594667, 0.17907141614705324, 0.22373970807529986, 0.1782726850360632, 0.17618377902545035, 0.17674058000557125, 0.1779751731082797, 0.1730477469973266, 0.17611091793514788, 0.1719937699381262, 0.17312194406986237, 0.16157087916508317, 0.16677416604943573, 0.21659703715704381, 0.1739705721847713, 0.1735336019191891, 0.16969356196932495, 0.22028598189353943, 0.21990375709719956, 0.18697840883396566, 0.16629786300472915, 0.18424765905365348, 0.1669994699768722, 0.23241687309928238, 0.1750372878741473, 0.17372106481343508, 0.2195935279596597, 0.173598927911371, 0.16064389096572995, 0.1590222690720111, 0.1638703888747841, 0.1591192849446088, 0.18779104808345437, 0.16118983086198568, 0.1622702379245311, 0.16610185895115137, 0.16395499696955085, 0.17365680914372206, 0.22057974198833108, 0.16603977582417428, 0.1636306750588119, 0.168163294903934, 0.1618857670109719, 0.18618393200449646, 0.17074863193556666, 0.15795743092894554, 0.16522689699195325, 0.20697393594309688, 0.16219526203349233, 0.15974005893804133, 0.16084119584411383, 0.16703948797658086, 0.16808531992137432, 0.16245691385120153, 0.16261806897819042, 0.16089175804518163, 0.1627139870543033, 0.1676722220145166]
[0.0016093625192523235, 0.0012872629517386126, 0.0013221883640963902, 0.0013614423255356707, 0.0014234584965562636, 0.0014855662548495818, 0.0013859974808788807, 0.001422437581110948, 0.0013976771611893593, 0.0014151688917836015, 0.001790268487448609, 0.0019182966058250777, 0.0013120472629720619, 0.0013291600914744212, 0.0013136022720601438, 0.0013009429992313765, 0.0012861989770633305, 0.0020191371007714162, 0.0013019150455099667, 0.0014207574120152365, 0.0013661849539900242, 0.0012871446124054203, 0.002187124278410807, 0.0017563868368174447, 0.0013609617125502852, 0.0013778869300907434, 0.0014401866888353067, 0.0014215492564000824, 0.0017854457688626162, 0.001462239255883204, 0.0014677142323709506, 0.001456699713111617, 0.0014548608058587062, 0.0015505310148000717, 0.0018247833110883024, 0.001423347216142818, 0.001406808418055722, 0.001357631797562967, 0.0014212649611597375, 0.0020009287282853395, 0.0019517944342377343, 0.00141556496017201, 0.0015058210227898387, 0.0013377991407503222, 0.0013764288290873054, 0.0013822864257376785, 0.001350689876117101, 0.0013551055814695452, 0.0013099749457062214, 0.0017385073493473059, 0.0013426367280095122, 0.0013447779306578775, 0.0013217364109388387, 0.001377675536110304, 0.0018990878142098816, 0.001331768217027303, 0.0013544535672277674, 0.0013249780686731025, 0.0013334916204271853, 0.0014602708225968735, 0.001970329496995077, 0.0018160981631498466, 0.0012995724028424006, 0.0013072603966954143, 0.0013850256602113793, 0.0013871441934575407, 0.0014556385973800523, 0.0014910532248228095, 0.0017639679144779148, 0.0013583080388259055, 0.0013631246604978344, 0.001962393417579028, 0.002034628148327849, 0.0014170621009182561, 0.0014111826506753762, 0.001416139124149038, 0.0019860115035819684, 0.0014343347216328216, 0.0014014063247789939, 0.001979842264017509, 0.0019529552393874458, 0.002098727628386529, 0.0013675482171161691, 0.001263725215771286, 0.0014208773324309394, 0.0014929028513097718, 0.0014671258530904387, 0.0015085591707205356, 0.0014008434720395148, 0.001362352651839917, 0.0013094189605682856, 0.0014362143489799297, 0.0019722806358377833, 0.0019679965104821115, 0.0014073663723272408, 0.001410801799303686, 0.0013641393165891023, 0.0013723495044536138, 0.0013383059462806744, 0.001391177603527326, 0.001347004916056875, 0.0013565636283263218, 0.0012767469752291145, 0.0013016873569942492, 0.0014394040620171052, 0.0014391069538718046, 0.0014376799294421839, 0.0013203180229016977, 0.001313602658306328, 0.0020440482325101085, 0.001354300969301961, 0.0013760807039577137, 0.002010473295641962, 0.001967934131723388, 0.0020403772567276113, 0.0013739073959489663, 0.001483714287421962, 0.0016817894580145908, 0.0014469031616323447, 0.0014195042633444303, 0.0013414422418315743, 0.0013571743647632904, 0.0015328062857075254, 0.001340121000124387, 0.0014259870481756769, 0.0014194142950568782, 0.0015085120306371949, 0.0015650004411997028, 0.001396127178716336, 0.0013589972572386727, 0.001420094642441633, 0.0014178988202511109, 0.0014269419624506272, 0.001349079937046004, 0.0014947505898831428, 0.0014253983494298633, 0.001381382472495469, 0.0013557812018507674, 0.001358908844765189, 0.0013714237301068943, 0.001435573169485081, 0.0013311791627184134, 0.0015261902098450087, 0.0019844644486384337, 0.001411028294787966, 0.0013647116828969744, 0.0013662151245097088, 0.0013865737601854774, 0.0017157478597332803, 0.0015521350230068662, 0.0013373845199108586, 0.0014488736831749132, 0.0014389030086670734, 0.001953748626952139, 0.0014753526748093062, 0.0014564241932401823, 0.0014799479756128881, 0.0014862275588535523, 0.0014055603952688534, 0.0017419801627885, 0.0014364294411773368, 0.0014472169144613336, 0.0020199511164091816, 0.002147752883692467, 0.0021299386736323206, 0.002082303386585888, 0.001329935783910197, 0.001409991333882014, 0.0014331136133464038, 0.0014282336743736222, 0.0018932389550257562, 0.00200100237857406, 0.0013278548916180928, 0.0013320048451568036, 0.001326073760657694, 0.001296211286743944, 0.0013437321005288021, 0.0016301930304472299, 0.0014893638363555652, 0.0014472215656595405, 0.0013574099442524503, 0.001353373409920307, 0.0013929926970785902, 0.0014743265360160622, 0.00145010677411748, 0.0015024023108877415, 0.0014820098064753206, 0.0014310923419430736, 0.0016874016583861076, 0.0014833969915964344, 0.0014328505183288523, 0.0014518736356277336, 0.001477314504007845, 0.0014942128124625185, 0.001822066635918594, 0.0014614753565902626, 0.0012949456753165915, 0.0013565663176105004, 0.001387044016719442, 0.0020916713636310756, 0.0014778619303635156, 0.0013699811078290375, 0.0013557360706085613, 0.0014066359753785438, 0.0013674736896515354, 0.0018029458994089172, 0.0013704997913319935, 0.0014009107041439808, 0.0020011571008538786, 0.0020980726342734904, 0.001410940084462018, 0.0013990713709373344, 0.001491039930012568, 0.0013823334412836983, 0.0014995200245446244, 0.0019526024414009827, 0.0018105834412078063, 0.0013683849851810193, 0.0014111109532762406, 0.001949210945415751, 0.0019838224497241105, 0.001965717690484237, 0.0019709297361993052, 0.0013511819375115772, 0.0013522041795500951, 0.0014733386897417002, 0.0014517712479009647, 0.0013692692940145037, 0.0013518568828644216, 0.0013469537745352633, 0.00141087425475608, 0.0013575433092841574, 0.0013399700933793026, 0.0013484001166887524, 0.0014204259315693333, 0.0014356531170305125, 0.0014106442956903645, 0.001383351209003103, 0.001418401225462787, 0.001475352303002232, 0.0017193852248782104, 0.0015237508991429972, 0.0014618644887913568, 0.0013985135502275802, 0.0013726766351177249, 0.0018014448222003934, 0.001374087877600461, 0.0013735903871007437, 0.0013760118456485199, 0.0013346986433621063, 0.0013591610003522662, 0.0013807347665230434, 0.0013974222874398841, 0.0013792462802181642, 0.0013829964256390583, 0.0013557623100806345, 0.0014382917054950497, 0.0015343074026236006, 0.00137006854205284, 0.001348587805260868, 0.0013657034800727238, 0.0014476853101550377, 0.0014116055342953565, 0.0013924672723163007, 0.0013959928913864978, 0.0013436952789942192, 0.001602086991201653, 0.0013397596271116604, 0.0013708823411039603, 0.0013129635038966125, 0.001310279031551277, 0.0014070096667567196, 0.0013177729060136994, 0.0014109428928688514, 0.0014779461085680844, 0.0013613983187391314, 0.0014355857585838369, 0.002032366463943392, 0.002131566177880348, 0.0013357734347354303, 0.0018591706049014894, 0.00135522905390623, 0.0013051776580021593, 0.0017052744334751322, 0.0019221979539928047, 0.0020328854344236528, 0.0013632544879476692, 0.0013194387208294962, 0.001310252154953489, 0.001314773085317755, 0.0018988926046665095, 0.0013959952792542618, 0.0014075446195265118, 0.0015190126349095457, 0.001999737294174211, 0.0014467608984992948, 0.001425755233684367, 0.0013758865051520194, 0.0013090568439414103, 0.0013038872401843699, 0.0013179112489284702, 0.0013379982867559722, 0.0012947056503865378, 0.0013194373328700778, 0.0013889403483115655, 0.0014077026970944432, 0.001488919729049238, 0.0013430199925174085, 0.0013282229842314886, 0.00142403255043508, 0.0013376796896056842, 0.0013649569456140663, 0.001969436860405082, 0.0013128298753511537, 0.0013148905438652566, 0.0013242889531389918, 0.0013255040619753358, 0.0013551122180453336, 0.00149863560380804, 0.0013720427906905959, 0.001317861643367151, 0.0019838643321666376, 0.0019427355425793301, 0.0013670467215684034, 0.001375040100786344, 0.0014052357138387224, 0.0013266777287619983, 0.0020066124726404516, 0.001444616333343262, 0.0014083258601901836, 0.0013071553720262392, 0.0013356361476805551, 0.0013090231394496305, 0.001349973232420378, 0.0015361646818386953, 0.0016411971860355878, 0.0014687609378710505, 0.001950049410075989, 0.0019039302854979223, 0.0019675606822003, 0.001301587077377494, 0.0013308332882850439, 0.0014179644658584003, 0.0013334697217124607, 0.0013393653798559608, 0.0013805645582980889, 0.0013308914345797412, 0.0013139502401644176, 0.0018795929224656304, 0.001935123263807722, 0.001917436798355838, 0.0013641126114557417, 0.0013287521162351898, 0.0014176594649347686, 0.0013513204194027786, 0.002026833435015161, 0.0019946329299245, 0.0013590925589723642, 0.0014315490618117097, 0.0014284114721556042, 0.0014291865743927715, 0.002120853387281414, 0.0014214654102950365, 0.0013901668748738923, 0.0014218043557606465, 0.001533284510825147, 0.0015361848514700352, 0.0014082857050261525, 0.0013899138439157905, 0.0015102218287859775, 0.0014492766751164152, 0.0020321684730665165, 0.0015252837369781594, 0.001901805061529311, 0.0016039841404083626, 0.001395089008534948, 0.0017228021553551505, 0.001399485829336814, 0.0015396126961090076, 0.001540005411014192, 0.002131209744848022, 0.001535183721214764, 0.001596159038184456, 0.0015809232397943503, 0.0015837558213072692, 0.002141735612147654, 0.001531780000718296, 0.0013959047911587612, 0.0021009792793946444, 0.0020809130699828613, 0.0020968724103543417, 0.0014327566370425761, 0.0015151154813031818, 0.0020899320230638794, 0.0021154378674375688, 0.0015136020225581042, 0.0014897538222690193, 0.00198172752803206, 0.0013851635898066352, 0.0013686434885616912, 0.0014728198221400958, 0.0013713685817323452, 0.001858641860948861, 0.001968214674600104, 0.0020005407060908025, 0.0013912859293379524, 0.0013702595281566297, 0.0019737410236352173, 0.002016260581047729, 0.0018923719767336698, 0.0013877483889778225, 0.0014062196435045827, 0.001407826882462169, 0.0014347183489233487, 0.0014449453952382933, 0.0014097506591920243, 0.0013985146277461402, 0.0014615556344303282, 0.0013784552931554559, 0.0014242060309232667, 0.0015129167839249436, 0.001471104714008727, 0.0014958948839109303, 0.0014018176498916722, 0.0014106340854349293, 0.001388464350426613, 0.0018913732018581656, 0.0014447968131898678, 0.001388001752035447, 0.0014259715820843166, 0.0018545550857345726, 0.0014244578128983808, 0.0014011000929212616, 0.0013827965739219225, 0.0014721492626668178, 0.002043702488028726, 0.0014381959832222887, 0.0013734413105128116, 0.0013701351623000332, 0.0014490293421919726, 0.0020950668538350235, 0.0013913712716957394, 0.0014913962096293537, 0.0012989250470183847, 0.0013136715436888526, 0.001444277100900347, 0.0016172767360514218, 0.0014527414261197398, 0.0015178628756787427, 0.0014569779367063396, 0.001351205117697286, 0.0014288791946258195, 0.0016415770068158243, 0.0014733249455141698, 0.001405838357154713, 0.0015324688906413178, 0.001585816969921769, 0.0017299764959300442, 0.0013626220621630665, 0.001350305557554198, 0.0013501418144406044, 0.0013461667979352695, 0.00200005623966802, 0.0013643064727472474, 0.0013422144562468049, 0.001322276388518801, 0.0013769579213834548, 0.0013926913871523715, 0.001984503100330169, 0.0016484463594679255, 0.0012885826254205313, 0.0013169954290788155, 0.0014822478042333387, 0.001725947320665, 0.00178859331390413, 0.0014080552573432215, 0.0015325539770856267, 0.0012645443202927709, 0.0014023586099938257, 0.0012794402027793694, 0.0013663255631399807, 0.001345793960354058, 0.001374179641061346, 0.0015733321015432011, 0.0017482839612057433, 0.0014004326258145738, 0.0013813523582939524, 0.001376161726511782, 0.001715368545774254, 0.001747720507410122, 0.0013359612112253672, 0.00133632146025775, 0.0013534518129745265, 0.0016976035858533578, 0.001647795079406933, 0.0017482701259723399, 0.0014014851640240522, 0.0014403083659999538, 0.001441885375243146, 0.0012420152652339311, 0.0012427479687175946, 0.0016778629615146201, 0.0012354800455796067, 0.0012497628049459308, 0.001230897656569141, 0.0012455411870178068, 0.001215921804032405, 0.001660907344557927, 0.001243214421265293, 0.0012335601095401216, 0.0012661601558647817, 0.0012666952188737923, 0.0013522888275474543, 0.001333617428826983, 0.0013249674302642234, 0.0013365067115955753, 0.001329749289652682, 0.0017608577873033937, 0.0013146462260920089, 0.0013231009215814993, 0.001328143383943825, 0.0013262652264529606, 0.0015030073118396103, 0.001337836531092762, 0.0013119720006216085, 0.0012338345768512227, 0.00126767188339727, 0.0013466925483953673, 0.0012681702264671912, 0.0012564613825816195, 0.001241563304574811, 0.0012697713282250334, 0.001200079905174789, 0.0011922302110178862, 0.0012145927103119902, 0.0012763263894157717, 0.0012755102579831146, 0.0015779637014929904, 0.0012825483663618797, 0.0013329516496014548, 0.0012525341553555336, 0.0013028337962168735, 0.001706056576949777, 0.0013168747482268373, 0.0012874962503701681, 0.001286939359488315, 0.0015657151179766515, 0.0013730700848100241, 0.00181839351716917, 0.001400431030560867, 0.0013076318045932567, 0.0012540107509266818, 0.0013516807957785204, 0.0013602623766928446, 0.0012571179995575221, 0.0013083348894724622, 0.0012880266476713587, 0.0012522021170298103, 0.001253233282113797, 0.0012465941399568692, 0.001315799483563751, 0.001275021757464856, 0.0012719639362330781, 0.0013032952501816908, 0.0012731528840959072, 0.0013121850388415623, 0.0013518271643988555, 0.0013146107667125762, 0.001315926883762586, 0.001301184602198191, 0.001649464546062518, 0.0016703429137123749, 0.0013158377732906956, 0.0013395595542533556, 0.001684386243141489, 0.0017436061170883477, 0.0017418395000277087, 0.0013418987509794533, 0.0013359053209569538, 0.0015445304688910255, 0.0012939121006638743, 0.0015388907249871409, 0.001323301828961121, 0.001313510390900774, 0.001302372967984411, 0.0013492780472006416, 0.001313715514697833, 0.0013115310393914115, 0.0016958859378064517, 0.0012255371402716264, 0.0012453021954570431, 0.0012480621007853188, 0.001412328867445467, 0.001649330875807209, 0.0012543210159492446, 0.001269199165108148, 0.0012476165320549626, 0.001360217758701765, 0.0015402659610117553, 0.0012881378825113643, 0.0012699697417701827, 0.0012447490535123507, 0.001224108147653169, 0.0017246025545318844, 0.001813903250877047, 0.001280327225686051, 0.0012997240537515609, 0.0013241381802799879, 0.0013774250855931314, 0.001378688157274155, 0.0016653376642352669, 0.001313645219852333, 0.0013259143524919637, 0.0017034416796377627, 0.0016512139154656325, 0.0013379008905758383, 0.0012568336715048645, 0.0017117602801590692, 0.0015996491329133278, 0.0016657928044878645, 0.0012800070453522494, 0.0012857423371315235, 0.0012522336401161738, 0.001730705382215092, 0.0012667245628108503, 0.001301583679378382, 0.0012927048519486561, 0.0016499102639500052, 0.0016681805391272064, 0.0016488300934724975, 0.001305201953073265, 0.0013011590708629228, 0.0012751129524986027, 0.0012512450066424208, 0.0012636674364330247, 0.0013288978989294264, 0.001649784413530142, 0.001293991304919473, 0.0012891980859421892, 0.001253723748959601, 0.001329066586549743, 0.0013422932497633155, 0.0012598643588717096, 0.0012624540468095802, 0.00164057550864527, 0.0014853782340651378, 0.0012509175630839309, 0.0012298797100811498, 0.0012637351883313386, 0.0012400914147292497, 0.0012879432415502379, 0.0016666550545778591, 0.0012985801477043424, 0.0013252326953079319, 0.0016611739920335822, 0.0016491666410729522, 0.0017150212661363184, 0.0013643216170748929, 0.0013770149289484834, 0.0013561071718868334, 0.0013869215927115874, 0.0014067760002944851, 0.0012341038600425236, 0.00163736011745641, 0.0013189212259021588, 0.0013308346497069579, 0.0013558233349613147, 0.0013312641640368383, 0.0013413264605333097, 0.0013796370785712497, 0.0013468071483657695, 0.0013543603436119156, 0.001344245443760883, 0.0013533376495615812, 0.0013095444919599686, 0.0012775945633620722, 0.0012513604924606625, 0.0012750661644531647, 0.0012792791258107172, 0.0014336638996610418, 0.0013200021239754278, 0.0012832133052143035, 0.0012842790474678623, 0.0013077423973300029, 0.0014640568915638141, 0.001277774326808867, 0.0012754729305015644, 0.0013859947030141484, 0.0014525866408803267, 0.0016819627890072297, 0.001390925373925711, 0.0012898855384264607, 0.0013250754145701649, 0.00129420390658197, 0.0012643887566810008, 0.0012384762176225195, 0.0012509556399891153, 0.0012483546561270487, 0.0012478917815315071, 0.0012265052027942147, 0.0013322471877472708, 0.0016744469139666762, 0.0013946782273706049, 0.0013640342895087088, 0.0013723560768994503, 0.001248603093699785, 0.0012679162882704986, 0.0016187100300157908, 0.0012780255765392212, 0.0013339585548237665, 0.0013584475000243401, 0.0013906480144214584, 0.0013989954386488535, 0.0017479664693382801, 0.0013927553518442437, 0.0013764357736363309, 0.0013807857812935254, 0.0013904310399084352, 0.0013519355234166142, 0.0013758665463683428, 0.001343701327641611, 0.0013525151880457997, 0.0012622724934772123, 0.0013029231722612167, 0.0016921643527894048, 0.0013591450951935258, 0.0013557312649936648, 0.0013257309528853511, 0.0017209842335432768, 0.0017179981023218716, 0.0014607688190153567, 0.0012992020547244465, 0.0014394348363566678, 0.001304683359194314, 0.0018157568210881436, 0.0013674788115167757, 0.0013571958188549615, 0.0017155744371848414, 0.0013562416243075859, 0.0012550303981697652, 0.0012423614771250868, 0.0012802374130842509, 0.0012431194136297563, 0.0014671175631519873, 0.0012592955536092632, 0.0012677362337853992, 0.00129767077305587, 0.001280898413824616, 0.0013566938214353286, 0.0017232792342838366, 0.0012971857486263616, 0.001278364648896968, 0.0013137757414369844, 0.001264732554773218, 0.0014545619687851286, 0.0013339736869966146, 0.001234042429132387, 0.0012908351327496348, 0.0016169838745554443, 0.0012671504846366588, 0.0012479692104534479, 0.0012565718425321393, 0.001304995999817038, 0.0013131665618857369, 0.001269194639462512, 0.0012704536638921127, 0.0012569668597279815, 0.0012712030238617444, 0.0013099392344884109]
[621.3640419963175, 776.8420575216373, 756.3218881323463, 734.5151397482385, 702.5143356264157, 673.1439925587521, 721.5020328650855, 703.018545965991, 715.4728057150521, 706.6294389354861, 558.5754354784764, 521.2958188860947, 762.1676659229413, 752.3548189674519, 761.265431150392, 768.6731859818765, 777.4846799234871, 495.26106950238676, 768.0992730276766, 703.84992648504, 731.9653148568506, 776.9134799322949, 457.2213887756817, 569.3506572914142, 734.7745280255647, 725.748955274685, 694.3544248480105, 703.4578615533803, 560.0842195487297, 683.8826108494757, 681.3315412119406, 686.4832820375361, 687.350979539082, 644.9403400866133, 548.0102727395064, 702.5692597410894, 710.8288429081545, 736.5767373709587, 703.5985740364769, 499.7679256956505, 512.3490376129421, 706.4317273567485, 664.089546410567, 747.4963688787667, 726.5177674773703, 723.439065435613, 740.3624012306603, 737.9498790902694, 763.3733784586921, 575.2060814557003, 744.8031020889183, 743.6172004330789, 756.5805040429301, 725.8603160098029, 526.5685938888779, 750.8814125570161, 738.3051174258808, 754.7294733726791, 749.910974078449, 684.804479090837, 507.529325183979, 550.6310288126699, 769.4838685500081, 764.9585365913869, 722.0082838374145, 720.9055876934032, 686.9837072195402, 670.6668704726055, 566.9037354888465, 736.210028517817, 733.6086192107949, 509.5818152680533, 491.4903004865268, 705.6853749401668, 708.625491903129, 706.1453094171824, 503.5217561410903, 697.1873335546235, 713.5689216742354, 505.090742921506, 512.0445055943295, 476.47917074822396, 731.2356430903474, 791.3112657087187, 703.790522359258, 669.835950224536, 681.6047838660481, 662.8841741238219, 713.855630525287, 734.0243355121422, 763.6975102040691, 696.2748984580536, 507.02723630160324, 508.1309822825977, 710.5470328571131, 708.8167880800543, 733.06295613589, 728.6773498695139, 747.2132981095463, 718.8154822680465, 742.3877879579888, 737.1567238860466, 783.2405475803444, 768.2336273966115, 694.7319563616157, 694.875385953475, 695.565111205244, 757.3932815082488, 761.2652073111161, 489.22524630056876, 738.3883070802376, 726.7015641771033, 497.3953159028114, 508.14708880742126, 490.105443345225, 727.8510931293836, 673.9842087370856, 594.6047498600293, 691.1312564082282, 704.4712903108476, 745.4663114191358, 736.8249990298141, 652.3981597181483, 746.2012757856808, 701.2686414503838, 704.5159425845634, 662.9048888510358, 638.9774556443044, 716.2671246894902, 735.8366580017137, 704.1784188979508, 705.2689414205869, 700.7993501589946, 741.2459206751214, 669.0079313353397, 701.5582699390553, 723.9124716802718, 737.582139828246, 735.8845325440454, 729.1692407291624, 696.5858802994244, 751.2136818291917, 655.2263233961869, 503.9142931918547, 708.7030102045326, 732.7555061866445, 731.9491506572716, 721.2021666025422, 582.8362217251746, 644.2738454949331, 747.728110436521, 690.1912924587757, 694.973875220644, 511.83657211829075, 677.8040376883127, 686.6131479011264, 675.6994276004006, 672.8444739454172, 711.4600008409611, 574.0593500210906, 696.1706376474523, 690.981420965639, 495.06148533815804, 465.60291344168826, 469.497085704649, 480.23741710355876, 751.9160038388172, 709.2242171778331, 697.7813836161542, 700.1655386948976, 528.1953434062927, 499.7495308889202, 753.0943375758652, 750.7480199009937, 754.1058647476963, 771.479164104472, 744.1959596012238, 613.4242886105692, 671.4276092851658, 690.9792002334287, 736.6971225120335, 738.8943750999841, 717.8788532755543, 678.2757927576944, 689.6043917928662, 665.6006801594432, 674.7593677388077, 698.7669283746179, 592.6271288345399, 674.1283726912499, 697.9095078014906, 688.7651758809154, 676.9039343261533, 669.2487118698726, 548.8273481808476, 684.2400697970569, 772.2331670442603, 737.1552625318253, 720.9576537917977, 478.08657582997716, 676.6531970642386, 729.9370730627563, 737.6066932785236, 710.915985019427, 731.2754955123295, 554.6478129642397, 729.660818866741, 713.8213713707362, 499.71089205005825, 476.62792205774844, 708.7473174888878, 714.7598191006017, 670.6728504524832, 723.4144600244599, 666.8800573728123, 512.1370222616873, 552.3081550623918, 730.788492149169, 708.661496587674, 513.0281062456829, 504.0773684858086, 508.72004908988674, 507.37475904563536, 740.0927826504725, 739.5332858183589, 678.7305641008559, 688.8137517848245, 730.316530408815, 739.7232744646169, 742.4159751473491, 708.7803867913698, 736.6247493991976, 746.2853125908774, 741.6196332403814, 704.0141817850138, 696.5470893612434, 708.895930076124, 722.8822250573946, 705.0191314335099, 677.8042085033348, 581.6032297653559, 656.2752485084208, 684.0579326383269, 715.0449131059688, 728.5036944729791, 555.1099804314515, 727.7554924262032, 728.0190727824719, 726.7379297368593, 749.2327987095272, 735.7480090591334, 724.2520607474765, 715.6032997241138, 725.0336755244492, 723.0676677547576, 737.5924176122912, 695.269253225518, 651.7598743837398, 729.8904903703954, 741.5164189524627, 732.2233666320817, 690.7578553055197, 708.4132044715868, 718.14973312554, 716.3360258996746, 744.2163529431454, 624.1858310390156, 746.4025484600279, 729.4572043248497, 761.6357934033965, 763.1962169279861, 710.727170983187, 758.856093820466, 708.7459067650238, 676.6146574646453, 734.5388827320993, 696.5797717208275, 492.03724709160196, 469.1386129021856, 748.6299502565454, 537.8742528327498, 737.882645828512, 766.1792200233522, 586.4158755738379, 520.2377819218838, 491.91163607481496, 733.5387551193498, 757.8980245261611, 763.2118720197775, 760.5875197531287, 526.6227260786155, 716.3348005977485, 710.4570513270072, 658.3223713998588, 500.0656850843744, 691.1992168417645, 701.382661185012, 726.8041341022613, 763.9087673145821, 766.9374844550207, 758.7764356765689, 747.3851124462484, 772.3763310227675, 757.898821783958, 719.9733244236354, 710.3772707575552, 671.6278792534761, 744.5905538052054, 752.8856313073073, 702.2311390947301, 747.5631182639664, 732.623840783579, 507.7593601017074, 761.7133177537734, 760.5195768313891, 755.1222092653401, 754.4299777623822, 737.9462650277324, 667.2736170547366, 728.8402422905965, 758.8049967407725, 504.06672663340345, 514.738098975798, 731.5038939215675, 727.2515175580189, 711.6243845441926, 753.7625591507889, 498.35232942817544, 692.2253174901529, 710.0629394570357, 765.019997928698, 748.7069002561695, 763.9284362997913, 740.7554283184503, 650.9718728873919, 609.3113054961794, 680.845993528046, 512.8075190469314, 525.2293151786682, 508.24353680503106, 768.2928152719928, 751.4089171068401, 705.2362905262403, 749.9232893835683, 746.622254867856, 724.3413529554639, 751.3760882500326, 761.063828318847, 532.0300944143855, 516.7629466829469, 521.5295757635814, 733.0773072560547, 752.5858192672858, 705.3880178805941, 740.0169387227597, 493.3804538272381, 501.3453778875753, 735.7850599639211, 698.5439945274675, 700.0783874207535, 699.6987082843799, 471.5083116998652, 703.4993554943009, 719.3381011115763, 703.3316475282658, 652.1946794217881, 650.9633258283084, 710.0831858414906, 719.4690551341728, 662.1543808593141, 689.9993749776409, 492.0851854821921, 655.6157230006046, 525.8162470110703, 623.44756086267, 716.8001424153929, 580.4497033461472, 714.5481426374129, 649.5139995449856, 649.3483677706276, 469.2170737382353, 651.3878346812573, 626.5039861801274, 632.5417799096191, 631.4104652664049, 466.9110390321412, 652.8352632434625, 716.3812362660387, 475.9685208738138, 480.55827724136316, 476.90073800485277, 697.955238276997, 660.0156967176387, 478.48446215680315, 472.71537273335315, 660.6756499373084, 671.2518437958539, 504.6102382162712, 721.9363888561329, 730.6504640232505, 678.9696777348772, 729.1985636252336, 538.0272665813558, 508.0746591848153, 499.8648600128065, 718.7595151457134, 729.7887585903313, 506.65208253016374, 495.9676390044586, 528.437332773259, 720.5917210515177, 711.126462085111, 710.3146078948894, 697.0009136290945, 692.0676748722985, 709.3452969712629, 715.0443621827608, 684.2024870231991, 725.4497153192945, 702.1456013296983, 660.9748868048848, 679.7612640877363, 668.4961695875033, 713.3595443581958, 708.9010611080464, 720.2201480309845, 528.7163839571997, 692.1388467020276, 720.4601856831532, 701.2762474118301, 539.2128859865649, 702.0214926304307, 713.7248830774273, 723.1721707002605, 679.2789463403234, 489.3080112480365, 695.3155283882053, 728.0980937049453, 729.8550008170795, 690.1171500690815, 477.31173741281725, 718.715428687302, 670.5126334259111, 769.8673624744155, 761.2252886227193, 692.38790767825, 618.3233689748721, 688.3537441834998, 658.821041098874, 686.3521916197379, 740.080086215328, 699.8492271152916, 609.1703257587077, 678.7368957843946, 711.3193312095336, 652.5417945557859, 630.5897962797886, 578.0425354636942, 733.8792081588423, 740.573120213839, 740.6629357778417, 742.8499956571392, 499.98594047834644, 732.973140548356, 745.0374233013932, 756.2715395078548, 726.2386050223537, 718.0341669554621, 503.9044785738185, 606.6318107692466, 776.046471737618, 759.3040779947574, 674.6510247098857, 579.3919594340251, 559.0985900630516, 710.199400758492, 652.505565840914, 790.7986963782157, 713.0843657774547, 781.5918226015311, 731.8899879922319, 743.0557941699449, 727.7068951681246, 635.5937179564004, 571.9894606310565, 714.0650550170818, 723.9282533495335, 726.6587790773285, 582.9651024343791, 572.1738663362487, 748.524726315057, 748.3229370627033, 738.851572264155, 589.0656737139939, 606.8715779633931, 571.9939871670732, 713.5287805178921, 694.2957658277131, 693.5364052994638, 805.1430831743054, 804.668384235551, 595.9962302864664, 809.4019839315698, 800.1518336459561, 812.4152277511699, 802.8638558266348, 822.421307590393, 602.0805454780884, 804.3664736307039, 810.66175232661, 789.7895028271557, 789.455888914692, 739.4869939239428, 749.8402303272051, 754.7355332353952, 748.2192130604117, 752.0214583165452, 567.9050331096961, 760.6609140564425, 755.8002444777247, 752.9307543817844, 753.9969985298168, 665.332757946498, 747.4754775781067, 762.2113882965513, 810.481420087955, 788.8476608947668, 742.5599860870516, 788.5376735154497, 795.8859809485948, 805.4361757594493, 787.5433771196135, 833.2778473233016, 838.7641839290699, 823.3212594723476, 783.4986475972985, 783.9999668691321, 633.7281390274377, 779.6976911183739, 750.2147585765729, 798.3814219550353, 767.5576139518084, 586.1470325842764, 759.3736620331532, 776.7012911396752, 777.0373892345623, 638.6857918905988, 728.2949436177968, 549.9359685117968, 714.0658684201707, 764.74126469496, 797.4413291600774, 739.8196402013948, 735.1522890982716, 795.4702743513162, 764.3303010922632, 776.381452827792, 798.5931235861292, 797.9360381439321, 802.1857057940276, 759.9942183375615, 784.3003416571607, 786.1858119668868, 767.2858470562148, 785.4516236752833, 762.0876403855596, 739.7395364848215, 760.6814315850175, 759.9206402264032, 768.5304593296165, 606.2573472022344, 598.6794638338528, 759.9721031713226, 746.5140290514224, 593.6880594173789, 573.5240259823719, 574.1057083526308, 745.2127064505418, 748.5560423426314, 647.4459521138492, 772.8500255055384, 649.8187192650447, 755.685496773677, 761.3186823091852, 767.8292045232082, 741.1370859213995, 761.1998098614291, 762.4676580007051, 589.6622984523668, 815.9687431246363, 803.0179370502002, 801.2421812750899, 708.0503861743887, 606.3064814151278, 797.2440765039884, 787.8984067207374, 801.5283336722777, 735.176403632924, 649.2385245877468, 776.3144097978019, 787.4203353901347, 803.3747823935001, 816.9212842159218, 579.8437427639285, 551.2973194774784, 781.0503283363046, 769.3940857011696, 755.2081911787717, 725.9922956676742, 725.327184921302, 600.4788226892148, 761.2405426424116, 754.1965271893841, 587.046807621057, 605.6150512261192, 747.4395204039334, 795.6502301554781, 584.193950280861, 625.1370875179176, 600.314755416081, 781.2456998819148, 777.7608087721437, 798.5730202130864, 577.79909294563, 789.4376010053907, 768.2948210272473, 773.5717851546505, 606.0935687532044, 599.4555004958881, 606.4906286941687, 766.1649583387245, 768.5455394295523, 784.2442491392509, 799.2039885804545, 791.3474472545695, 752.5032591334594, 606.1398033578463, 772.8027199241741, 775.675988743938, 797.6238791279555, 752.4077500104792, 744.9936891035759, 793.7362406978201, 792.1080395180777, 609.5421970706884, 673.2291998538517, 799.4131903741642, 813.0876473553809, 791.3050212049727, 806.3921644182425, 776.4317306377152, 600.0041803810965, 770.0718371275129, 754.5844616877939, 601.9839010216, 606.3668613557435, 583.0831487313552, 732.9650043543261, 726.2085391939943, 737.4048458195526, 721.0212929520318, 710.8452232556332, 810.3045719065678, 610.739194963091, 758.1953951161771, 751.4081484279014, 737.5592189734164, 751.165716778303, 745.5306589585963, 724.8283012483244, 742.4968015750517, 738.3559365989119, 743.9117645080015, 738.9138995164686, 763.6243030607691, 782.7209262447358, 799.1302314759915, 784.2730266698499, 781.6902346204314, 697.5135526788587, 757.574538583557, 779.2936653138853, 778.6469786076799, 764.6765923026464, 683.0335663608416, 782.6108092947956, 784.0229110991497, 721.5034789276477, 688.4270940244633, 594.543474169393, 718.9458318512275, 775.2625874229942, 754.6740276095044, 772.6757699573238, 790.8959920088053, 807.4438457281659, 799.3888576326344, 801.0544079696429, 801.3515392919123, 815.3247109933229, 750.6114549890132, 597.2121251852965, 717.0112649462565, 733.1194000703422, 728.6738601101906, 800.8950202396669, 788.6956017924891, 617.7758718096316, 782.4569541932881, 749.648477746082, 736.1344475823191, 719.089222887233, 714.7986136150648, 572.0933539294753, 718.0011899978203, 726.5141019679795, 724.2253023949858, 719.2014355964418, 739.6802456028362, 726.8146773679044, 744.2130028665997, 739.3632314361392, 792.2219688438873, 767.5049621417853, 590.9591455177364, 735.7566190220567, 737.6093078480955, 754.3008615915447, 581.0628479385506, 582.0728198992197, 684.5710197141655, 769.7032161884122, 694.717103367517, 766.4694984824008, 550.7345413141402, 731.2727565342115, 736.813351549874, 582.895138983851, 737.3317424249819, 796.7934493525568, 804.9187119952169, 781.1051214249987, 804.4279487841981, 681.6086352695405, 794.0947596725034, 788.8076189272024, 770.6114838705286, 780.7020363262958, 737.0859837351064, 580.2890095264113, 770.8996194715655, 782.2494159728573, 761.1649145737902, 790.6809990981154, 687.4921945300238, 749.6399739723936, 810.3449090507089, 774.6922706309358, 618.4353571707256, 789.1722507502641, 801.3018202882196, 795.8160179563494, 766.2858737806102, 761.5180198953417, 787.9012161787001, 787.1204030664437, 795.565923047811, 786.6564043893901, 763.3941893423355]
Elapsed: 0.1883361206047227~0.028376522601088604
Time per graph: 0.001464151536433846~0.00021869736047832735
Speed: 695.8673504472501~87.08863174541979
Total Time: 0.1696
best val loss: 0.13418721260372982 test_score: 0.9141

Testing...
Test loss: 0.2128 score: 0.9375 time: 0.15s
test Score 0.9375
Epoch Time List: [1.0836947648786008, 0.5708195301704109, 0.589953602058813, 0.5854726540856063, 0.6093268170952797, 0.6215507951565087, 0.6941069930326194, 0.6255710572004318, 0.6100808072369546, 0.6161278311628848, 0.6689238180406392, 0.8476019520312548, 0.5766242421232164, 0.586501965066418, 0.5844179200939834, 0.5784046018961817, 0.5902710638474673, 0.7421582110691816, 0.7174279193859547, 0.6163106430321932, 0.6466278899461031, 0.5921160059515387, 0.7165219869930297, 0.6950624019373208, 0.6512760522309691, 0.6544777068775147, 0.6323277559131384, 0.6489459669683129, 1.0159847061149776, 0.753773788921535, 0.6668274598196149, 0.6565516230184585, 0.6575678871013224, 0.6556579628959298, 0.7039439573418349, 0.7332392169628292, 0.6241264860145748, 0.6121964640915394, 0.6307262841146439, 0.8964077108539641, 0.8994238651357591, 0.7698187651112676, 0.6455200638156384, 0.6094641529489309, 0.6036050640977919, 0.6161871505901217, 0.6098203489091247, 0.615702377166599, 0.6774818878620863, 0.6896966365166008, 0.603852248750627, 0.6072742631658912, 0.6058353190310299, 0.649913219967857, 0.8141458800528198, 0.6883733361028135, 0.6075125087518245, 0.6020360859110951, 0.6002620700746775, 0.6150864399969578, 0.7434393090661615, 0.8648080902639776, 0.620058850152418, 0.6100925570353866, 0.6069854509551078, 0.6089895707555115, 0.6363219949416816, 0.6232986380346119, 0.6704465737566352, 0.6136036787647754, 0.618670579046011, 0.7081901277415454, 0.8932457382325083, 0.8118357877247036, 0.6313791801221669, 0.6206254351418465, 0.7876678970642388, 0.7747827307321131, 0.6215538906399161, 0.6962253581732512, 0.8947601767722517, 0.9314866231288761, 0.701017543906346, 0.5935166077688336, 0.6309381008613855, 0.6407432141713798, 0.6513632847927511, 0.6830349741503596, 0.7243960769847035, 0.6154722208157182, 0.6060785339213908, 0.6115176861640066, 0.7549497007858008, 0.9040758379269391, 0.7627244300674647, 0.6264859489165246, 0.6201118370518088, 0.6138705709017813, 0.6202610936015844, 0.6306114050094038, 0.6984840319491923, 0.6219825057778507, 0.6094407942146063, 0.6150315413251519, 0.6382266511209309, 0.6498633152805269, 0.6824035979807377, 0.6593251440208405, 0.6638632609974593, 0.7035925416275859, 0.5943626621738076, 0.6015636257361621, 0.6890941220335662, 0.8806173091288656, 0.7947248867712915, 0.6144833690486848, 0.6449953860137612, 0.6764656975865364, 0.7164703099988401, 0.6101893286686391, 0.5969492038711905, 0.5941944501828402, 0.64245097595267, 0.6812034668400884, 0.6377661831211299, 0.6395013139117509, 0.640983346151188, 0.7597829701844603, 0.6368520788382739, 0.6210459012072533, 0.6249303610529751, 0.6647329272236675, 0.704937273170799, 0.6335280742496252, 0.648711069021374, 0.6300547479186207, 0.6154474157374352, 0.6098680929280818, 0.6802037602756172, 0.7083359891548753, 0.6217028400860727, 0.6023208361584693, 0.6252047962043434, 0.8119614911265671, 0.7071308696176857, 0.6252438726369292, 0.6202035881578922, 0.6239648910705, 0.6633785718586296, 0.7388974640052766, 0.6487703388556838, 0.6518367801327258, 0.6479652069974691, 0.7221035200636834, 0.8197508461307734, 0.6384308631531894, 0.651065044105053, 0.6603788938373327, 0.6467790068127215, 0.6931481338106096, 0.726722963154316, 0.653522894019261, 0.7409208337776363, 0.9489967718254775, 0.9411165530327708, 0.9322094789240509, 0.7123096061404794, 0.6359040278475732, 0.6487589322496206, 0.6393916851375252, 0.7109046161640435, 0.8947819168679416, 0.7660877150483429, 0.6062083761207759, 0.6066623837687075, 0.6005108479876071, 0.6142633031122386, 0.6662183662410825, 0.7127436762675643, 0.6534634241834283, 0.6197552247904241, 0.6247987577226013, 0.6125328431371599, 0.6997722347732633, 0.6334923699032515, 0.632179053267464, 0.6291042079683393, 0.61709213280119, 0.6675330428406596, 0.7309160926379263, 0.648935440229252, 0.6470874031074345, 0.6575603939127177, 0.6484825718216598, 0.6942376263905317, 0.6803573067300022, 0.6846679172012955, 0.6566675261128694, 0.6047672310378402, 0.6983086878899485, 0.6233348120003939, 0.6802933658473194, 0.5872511498164386, 0.5915656390134245, 0.594648890895769, 0.6612352293450385, 0.6310501049738377, 0.6028778797481209, 0.863588151987642, 0.9238479672931135, 0.7922311220318079, 0.6111626101192087, 0.6354194260202348, 0.6099875490181148, 0.6137214349582791, 0.7636235100217164, 0.8218230397906154, 0.5983341792598367, 0.6148065449669957, 0.6794975029770285, 0.8775271722115576, 0.8817378350067884, 0.875477580819279, 0.6787507457192987, 0.6155202970840037, 0.6283550558146089, 0.6230632250662893, 0.5956224920228124, 0.6707753101363778, 0.598671963205561, 0.6053076770622283, 0.6012492759618908, 0.594231090741232, 0.5910418978892267, 0.6408593468368053, 0.6995939661283046, 0.5951496029738337, 0.613638998940587, 0.6463468901347369, 0.6537653508130461, 0.7811769121326506, 0.6573541220277548, 0.6293654218316078, 0.6112494759727269, 0.5896064059343189, 0.6710046350490302, 0.6156487644184381, 0.5936593981459737, 0.5956821858417243, 0.5887923401314765, 0.59258082206361, 0.5958939166739583, 0.597688095876947, 0.6851928732357919, 0.603892759885639, 0.6011480707675219, 0.601905383169651, 0.6529806789476424, 0.6935656077694148, 0.600198709173128, 0.6043534660711884, 0.6068133390508592, 0.6768191009759903, 0.600020547863096, 0.5954876700416207, 0.5839802040718496, 0.6239319259766489, 0.5837970380671322, 0.6258591148070991, 0.6142481509596109, 0.6758447310421616, 0.6003434900194407, 0.5955768590793014, 0.5961237610317767, 0.7280577688943595, 0.589003321249038, 0.6024844497442245, 0.7007818568963557, 0.9813217574264854, 0.5829691500402987, 0.6499205569270998, 0.5986722458619624, 0.5868608502205461, 0.7227052748203278, 0.7409216079395264, 0.8233081470243633, 0.5942134980577976, 0.5827470829244703, 0.573756986996159, 0.5829021069221199, 0.6759750819765031, 0.6369031690992415, 0.6175380360800773, 0.6225286100525409, 0.8315734460484236, 0.8016784519422799, 0.6196099151857197, 0.6043310479726642, 0.5797687559388578, 0.5782831415999681, 0.5891331559978426, 0.59620895027183, 0.6560178869403899, 0.5731202161405236, 0.6012967848218977, 0.6062209659721702, 0.6184693612158298, 0.6714234307873994, 0.5785830691456795, 0.622736866120249, 0.5831247952301055, 0.5887529947794974, 0.7063094358891249, 0.6229937756434083, 0.5837577690836042, 0.5806258788798004, 0.5844742232002318, 0.5927459474187344, 0.8115243359934539, 0.5845054523088038, 0.5775323531124741, 0.6708980200346559, 0.8687088578008115, 0.5972511500585824, 0.6188568465877324, 0.5990920211188495, 0.5964865107089281, 0.6829092719126493, 0.5954019140917808, 0.6017526730429381, 0.5848006368614733, 0.592802177881822, 0.5698603258933872, 0.5945176787208766, 0.6021748348139226, 0.6968223401345313, 0.5940822020638734, 0.8262709921691567, 0.8826532198581845, 0.8994047800078988, 0.7652884270064533, 0.5761468592099845, 0.6549335573799908, 0.5994360761251301, 0.5882691582664847, 0.5980078871361911, 0.5973883799742907, 0.6620294519234449, 0.6494378170464188, 0.8622203900013119, 0.8728752061724663, 0.7590983491390944, 0.5850772249978036, 0.6163665091153234, 0.5935067741665989, 0.7400479780044407, 0.9034185509663075, 0.6060359957627952, 0.6464530250523239, 0.6252386763226241, 0.6204369605984539, 0.7424287965986878, 0.6346537191420794, 0.6201111869886518, 0.6268028391059488, 0.656156204175204, 0.6644171779043972, 0.6591438336763531, 0.6927034170366824, 0.6869766837917268, 0.6076428540982306, 0.7045698440633714, 0.6393435739446431, 0.6947616839315742, 0.7391092958860099, 0.6117098298855126, 0.6931394340936095, 0.6631128159351647, 0.6580053528305143, 0.6686578320804983, 0.9487630070652813, 0.7845103670842946, 0.6766636909451336, 0.673095794627443, 0.6770206401124597, 0.7862482308410108, 0.825093308230862, 0.6160965643357486, 0.7186436566989869, 0.9349708030931652, 0.9328399051446468, 0.7164339500013739, 0.6432944678235799, 0.7501064508687705, 0.9396336181089282, 0.7951530646532774, 0.6664797868579626, 0.6847957281861454, 0.7467340142466128, 0.6072103059850633, 0.6476504609454423, 0.6063482060562819, 0.6789750999305397, 0.8916868309024721, 0.8968410920351744, 0.6910429487470537, 0.6078164200298488, 0.7036856519989669, 0.897607646882534, 0.8922558131162077, 0.6193339561577886, 0.6205139157827944, 0.6239268477074802, 0.6186383492313325, 0.6186270760372281, 0.6238115821033716, 0.6960387942381203, 0.6208159679081291, 0.6151325439568609, 0.629501367919147, 0.6758870258927345, 0.7802612478844821, 0.6488098960835487, 0.6178522030822933, 0.614881613990292, 0.6118694613687694, 0.7449868838302791, 0.6272542041260749, 0.628161568660289, 0.6252326830290258, 0.687682697083801, 0.7209784751757979, 0.6139421730767936, 0.6065604721661657, 0.6303212696220726, 0.7295896399300545, 0.7106932168826461, 0.5979523309506476, 0.5985118572134525, 0.6138559507671744, 0.7064715842716396, 0.6125282770954072, 0.6339753128122538, 0.5778564838692546, 0.5819487478584051, 0.6211138011422008, 0.6703301500529051, 0.6432869890704751, 0.7511339702177793, 0.6164917827118188, 0.6036817640997469, 0.6127982770558447, 0.8329731430858374, 0.6437051820103079, 0.6175000150687993, 0.7340018078684807, 0.6636861506849527, 0.8245460677426308, 0.6789909098297358, 0.5986729050055146, 0.6836168807931244, 0.5946051280479878, 0.6798210772685707, 0.5929212919436395, 0.6837522839196026, 0.5808408169541508, 0.5954931962769479, 0.5947088461834937, 0.7674977742135525, 0.9593390747904778, 0.7261932948604226, 0.6489888168871403, 0.7179556919727474, 0.7960337069816887, 0.9757141231093556, 0.6835303090047091, 0.7170171749312431, 0.6350770590361208, 0.642791214864701, 0.7264003590680659, 0.6676187519915402, 0.6576465172693133, 0.669837263179943, 0.7142363127786666, 1.0025137711782008, 0.7653988157398999, 0.6844780158717185, 0.6863465919159353, 1.000396914081648, 1.0048304370138794, 0.8541643132921308, 0.6587124720681459, 0.6481395920272917, 0.9241630360484123, 0.9709832859225571, 0.9984585680067539, 0.7634369342122227, 0.6964343800209463, 0.6884611069690436, 0.612500075250864, 0.6164593638386577, 0.7195106018334627, 0.6062569690402597, 0.614382638130337, 0.6069341418333352, 0.6031232518143952, 0.6091728028841317, 0.713529102737084, 0.6095581639092416, 0.6076225249562413, 0.6082050811965019, 0.6053863398265094, 0.6421594747807831, 0.6637129229493439, 0.7205370920710266, 0.6374515723437071, 0.6418640119954944, 0.7503847838379443, 0.6467603191267699, 0.6621481699403375, 0.666083704913035, 0.6562461566645652, 0.6838135339785367, 0.716578067978844, 0.6510485450271517, 0.6145552771631628, 0.6226569940336049, 0.6562225210946053, 0.6687589418143034, 0.746341560035944, 0.6221384890377522, 0.6168466648086905, 0.7069495161995292, 0.6678795139305294, 0.604001997737214, 0.6802738311234862, 0.6240454514045268, 0.8493662662804127, 0.6289693571161479, 0.640040977858007, 0.653488232055679, 0.6370774258393794, 0.9368724490050226, 0.648760112002492, 0.6284872367978096, 0.628180167870596, 0.6924817098770291, 0.6851741480641067, 0.9676291770301759, 0.7343093270901591, 0.6952536038588732, 0.6152496740687639, 0.6478971831966192, 0.6670610099099576, 0.6861768839880824, 0.7196433998178691, 0.6280634969007224, 0.6226591381710023, 0.6200428288429976, 0.6269538111519068, 0.6520718270912766, 0.7575930398888886, 0.6419542247895151, 0.6341678891330957, 0.6454678841400892, 0.6370985428802669, 0.6753622109536082, 0.7348253128584474, 0.6653418713249266, 0.6418045030441135, 0.795438454952091, 0.9377303009387106, 0.6560004211496562, 0.6515915677882731, 0.9354550510179251, 0.998246937058866, 0.990495225880295, 0.6781757902354002, 0.6325598361436278, 0.6925659901462495, 0.6668726508505642, 0.7260473181959242, 0.6897968170233071, 0.6601792951114476, 0.634342267876491, 0.6489695811178535, 0.6556690169963986, 0.685286738909781, 0.7553801699541509, 0.6389289917424321, 0.6274156298022717, 0.6366974308621138, 0.6542260360438377, 0.6839011448901147, 0.6269563271198422, 0.6127837093081325, 0.6209340658970177, 0.6344783059321344, 0.7187532603275031, 0.844768475741148, 0.6249564392492175, 0.6216285291593522, 0.6080585438758135, 0.7568438837770373, 1.0149544079322368, 0.7267422717995942, 0.6437192049343139, 0.6502546151168644, 0.6439153619576246, 0.6767759358044714, 0.9861783962696791, 0.7544639848638326, 0.6452359771355987, 0.681345894234255, 1.0009470470249653, 0.84454001695849, 0.7159264869987965, 0.7840563100762665, 1.0169936001766473, 0.9606764540076256, 0.8945034469943494, 0.6435837270691991, 0.6522717529442161, 0.7818898879922926, 0.803477318957448, 0.624903904972598, 0.6317798248492181, 0.7649879236705601, 0.9685729248449206, 0.9748548460192978, 0.8158182548359036, 0.6298624840565026, 0.6267549761105329, 0.6134953121654689, 0.6244870589580387, 0.6353291738778353, 0.7780246268957853, 0.9085066809784621, 0.642817476997152, 0.6335829580202699, 0.7374151989351958, 0.6460857801139355, 0.6347646480426192, 0.62514428794384, 0.7266388430725783, 0.9348539961501956, 0.6333922033663839, 0.6264024470001459, 0.6294617981184274, 0.6273941269610077, 0.6278655179776251, 0.7690861141309142, 0.8367532370612025, 0.6473613549023867, 0.7210415292065591, 0.9605834160465747, 0.9752987457904965, 0.9395052783656865, 0.6821023570373654, 0.6693917301017791, 0.672463129973039, 0.6672196821309626, 0.6438017759937793, 0.7811761300545186, 0.65726163610816, 0.6523983119986951, 0.6640765031334013, 0.6546896409709007, 0.7180525620933622, 0.8451667490880936, 0.6622978479135782, 0.660784485982731, 0.6603962890803814, 0.663222169270739, 0.6488298687618226, 0.7059223633259535, 0.6315931789577007, 0.6274301679804921, 0.6339715039357543, 0.7311528502032161, 0.6365128958132118, 0.6260150349698961, 0.627080884994939, 0.6341370330192149, 0.6983190840110183, 0.6856715788599104, 0.6248951982706785, 0.6466855420731008, 0.6712851559277624, 0.9132719561457634, 0.8554173540323973, 0.6219832149799913, 0.6363242308143526, 0.6147002000361681, 0.6106704641133547, 0.6605113479308784, 0.6785208913497627, 0.6165568807628006, 0.6241674930788577, 0.6070348920766264, 0.7372365060728043, 0.7273660879582167, 0.7796116629615426, 0.6472930882591754, 0.7236833027563989, 0.6046837540343404, 0.654614029917866, 0.8884070867206901, 0.7409915188327432, 0.6529636029154062, 0.6486990754492581, 0.6609371772501618, 0.6770505418535322, 0.944013532018289, 0.8982704905793071, 0.6916529380250722, 0.6704449937678874, 0.6732591676991433, 0.6417117628734559, 0.6699878079816699, 0.756645469693467, 0.662269450025633, 0.6340118439402431, 0.6394457397982478, 0.7280851497780532, 0.8455323830712587, 0.6738531398586929, 0.6607548319734633, 0.7478529543150216, 0.9887344441376626, 0.9373753329273313, 0.6623644949868321, 0.6798070000950247, 0.6282388218678534, 0.7964815399609506, 0.6726675757672638, 0.6559578031301498, 0.9836679662112147, 0.8720653459895402, 0.6210044471081346, 0.6202191468328238, 0.6270926909055561, 0.6314923437312245, 0.7179504260420799, 0.6561634079553187, 0.6425669039599597, 0.6331881759688258, 0.6297281361185014, 0.6382673790212721, 0.8985449972096831, 0.6346352130640298, 0.622109895106405, 0.6386636781971902, 0.6323602653574198, 0.6575891189277172, 0.7271947038825601, 0.6284665111452341, 0.6367264641448855, 0.9715116289444268, 0.8810337458271533, 0.629923433996737, 0.6237506028264761, 0.6312898530159146, 0.6264232338871807, 0.6125109840650111, 0.6758521611336619, 0.6822156540583819, 0.6295056899543852, 0.631033489946276]
Total Epoch List: [202, 267, 300]
Total Time List: [0.17763156397268176, 0.25727027910761535, 0.16961546102538705]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a28997f70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.8680;  Loss pred: 1.8680; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 1.8713;  Loss pred: 1.8713; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 1.8594;  Loss pred: 1.8594; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 1.8377;  Loss pred: 1.8377; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5039 time: 0.26s
Epoch 5/1000, LR 0.000105
Train loss: 1.8201;  Loss pred: 1.8201; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 1.7903;  Loss pred: 1.7903; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 1.7593;  Loss pred: 1.7593; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5039 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 1.7260;  Loss pred: 1.7260; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 1.6776;  Loss pred: 1.6776; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5039 time: 0.32s
Epoch 10/1000, LR 0.000255
Train loss: 1.6451;  Loss pred: 1.6451; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 0.27s
Epoch 11/1000, LR 0.000285
Train loss: 1.5950;  Loss pred: 1.5950; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.27s
Epoch 12/1000, LR 0.000285
Train loss: 1.5457;  Loss pred: 1.5457; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 1.5004;  Loss pred: 1.5004; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 1.4548;  Loss pred: 1.4548; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 1.4252;  Loss pred: 1.4252; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 1.3941;  Loss pred: 1.3941; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.19s
Epoch 17/1000, LR 0.000285
Train loss: 1.3540;  Loss pred: 1.3540; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 1.3283;  Loss pred: 1.3283; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5039 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 1.2950;  Loss pred: 1.2950; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 0.22s
Epoch 20/1000, LR 0.000285
Train loss: 1.2714;  Loss pred: 1.2714; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 1.2466;  Loss pred: 1.2466; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5039 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 1.2250;  Loss pred: 1.2250; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5039 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 1.2073;  Loss pred: 1.2073; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5039 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 1.1915;  Loss pred: 1.1915; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5039 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.1740;  Loss pred: 1.1740; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5039 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.1573;  Loss pred: 1.1573; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5039 time: 0.19s
Epoch 27/1000, LR 0.000285
Train loss: 1.1441;  Loss pred: 1.1441; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 0.18s
Test loss: 0.6893 score: 0.5116 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 1.1333;  Loss pred: 1.1333; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4961 time: 0.20s
Test loss: 0.6889 score: 0.5116 time: 0.27s
Epoch 29/1000, LR 0.000285
Train loss: 1.1178;  Loss pred: 1.1178; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.19s
Test loss: 0.6884 score: 0.5194 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 1.1068;  Loss pred: 1.1068; Loss self: 0.0000; time: 0.27s
Val loss: 0.6890 score: 0.5271 time: 0.20s
Test loss: 0.6880 score: 0.5504 time: 0.19s
Epoch 31/1000, LR 0.000285
Train loss: 1.0965;  Loss pred: 1.0965; Loss self: 0.0000; time: 0.27s
Val loss: 0.6886 score: 0.5581 time: 0.19s
Test loss: 0.6875 score: 0.6434 time: 0.19s
Epoch 32/1000, LR 0.000285
Train loss: 1.0866;  Loss pred: 1.0866; Loss self: 0.0000; time: 0.27s
Val loss: 0.6882 score: 0.6589 time: 0.19s
Test loss: 0.6870 score: 0.7054 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 1.0775;  Loss pred: 1.0775; Loss self: 0.0000; time: 0.28s
Val loss: 0.6877 score: 0.7907 time: 0.19s
Test loss: 0.6865 score: 0.7829 time: 0.19s
Epoch 34/1000, LR 0.000285
Train loss: 1.0709;  Loss pred: 1.0709; Loss self: 0.0000; time: 0.29s
Val loss: 0.6873 score: 0.8217 time: 0.27s
Test loss: 0.6859 score: 0.8527 time: 0.19s
Epoch 35/1000, LR 0.000285
Train loss: 1.0633;  Loss pred: 1.0633; Loss self: 0.0000; time: 0.28s
Val loss: 0.6868 score: 0.8372 time: 0.19s
Test loss: 0.6853 score: 0.8915 time: 0.19s
Epoch 36/1000, LR 0.000285
Train loss: 1.0554;  Loss pred: 1.0554; Loss self: 0.0000; time: 0.27s
Val loss: 0.6862 score: 0.8527 time: 0.19s
Test loss: 0.6846 score: 0.9070 time: 0.19s
Epoch 37/1000, LR 0.000285
Train loss: 1.0518;  Loss pred: 1.0518; Loss self: 0.0000; time: 0.28s
Val loss: 0.6856 score: 0.8682 time: 0.19s
Test loss: 0.6839 score: 0.9302 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 1.0429;  Loss pred: 1.0429; Loss self: 0.0000; time: 0.28s
Val loss: 0.6851 score: 0.8682 time: 0.19s
Test loss: 0.6832 score: 0.9302 time: 0.20s
Epoch 39/1000, LR 0.000284
Train loss: 1.0363;  Loss pred: 1.0363; Loss self: 0.0000; time: 0.28s
Val loss: 0.6844 score: 0.8605 time: 0.26s
Test loss: 0.6824 score: 0.9457 time: 0.26s
Epoch 40/1000, LR 0.000284
Train loss: 1.0350;  Loss pred: 1.0350; Loss self: 0.0000; time: 0.40s
Val loss: 0.6838 score: 0.8605 time: 0.27s
Test loss: 0.6816 score: 0.9535 time: 0.27s
Epoch 41/1000, LR 0.000284
Train loss: 1.0266;  Loss pred: 1.0266; Loss self: 0.0000; time: 0.28s
Val loss: 0.6830 score: 0.8605 time: 0.19s
Test loss: 0.6806 score: 0.9535 time: 0.19s
Epoch 42/1000, LR 0.000284
Train loss: 1.0250;  Loss pred: 1.0250; Loss self: 0.0000; time: 0.26s
Val loss: 0.6822 score: 0.8837 time: 0.19s
Test loss: 0.6797 score: 0.9535 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.0197;  Loss pred: 1.0197; Loss self: 0.0000; time: 0.28s
Val loss: 0.6814 score: 0.8760 time: 0.19s
Test loss: 0.6787 score: 0.9535 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 1.0147;  Loss pred: 1.0147; Loss self: 0.0000; time: 0.27s
Val loss: 0.6806 score: 0.8605 time: 0.19s
Test loss: 0.6776 score: 0.9535 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 1.0104;  Loss pred: 1.0104; Loss self: 0.0000; time: 0.26s
Val loss: 0.6797 score: 0.8605 time: 0.18s
Test loss: 0.6765 score: 0.9535 time: 0.20s
Epoch 46/1000, LR 0.000284
Train loss: 1.0074;  Loss pred: 1.0074; Loss self: 0.0000; time: 0.27s
Val loss: 0.6787 score: 0.8605 time: 0.23s
Test loss: 0.6753 score: 0.9535 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0042;  Loss pred: 1.0042; Loss self: 0.0000; time: 0.26s
Val loss: 0.6777 score: 0.8605 time: 0.18s
Test loss: 0.6740 score: 0.9535 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 1.0016;  Loss pred: 1.0016; Loss self: 0.0000; time: 0.26s
Val loss: 0.6766 score: 0.8605 time: 0.18s
Test loss: 0.6726 score: 0.9535 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.9974;  Loss pred: 0.9974; Loss self: 0.0000; time: 0.26s
Val loss: 0.6755 score: 0.8605 time: 0.19s
Test loss: 0.6712 score: 0.9535 time: 0.19s
Epoch 50/1000, LR 0.000284
Train loss: 0.9940;  Loss pred: 0.9940; Loss self: 0.0000; time: 0.31s
Val loss: 0.6742 score: 0.8605 time: 0.25s
Test loss: 0.6696 score: 0.9612 time: 0.25s
Epoch 51/1000, LR 0.000284
Train loss: 0.9908;  Loss pred: 0.9908; Loss self: 0.0000; time: 0.33s
Val loss: 0.6729 score: 0.8605 time: 0.17s
Test loss: 0.6680 score: 0.9535 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.9877;  Loss pred: 0.9877; Loss self: 0.0000; time: 0.27s
Val loss: 0.6715 score: 0.8605 time: 0.19s
Test loss: 0.6663 score: 0.9535 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 0.9862;  Loss pred: 0.9862; Loss self: 0.0000; time: 0.27s
Val loss: 0.6700 score: 0.8605 time: 0.19s
Test loss: 0.6645 score: 0.9535 time: 0.19s
Epoch 54/1000, LR 0.000284
Train loss: 0.9820;  Loss pred: 0.9820; Loss self: 0.0000; time: 0.25s
Val loss: 0.6685 score: 0.8605 time: 0.17s
Test loss: 0.6626 score: 0.9535 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.9806;  Loss pred: 0.9806; Loss self: 0.0000; time: 0.25s
Val loss: 0.6669 score: 0.8605 time: 0.18s
Test loss: 0.6606 score: 0.9535 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 0.9762;  Loss pred: 0.9762; Loss self: 0.0000; time: 0.26s
Val loss: 0.6652 score: 0.8605 time: 0.19s
Test loss: 0.6585 score: 0.9535 time: 0.18s
Epoch 57/1000, LR 0.000283
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.31s
Val loss: 0.6634 score: 0.8605 time: 0.19s
Test loss: 0.6562 score: 0.9535 time: 0.24s
Epoch 58/1000, LR 0.000283
Train loss: 0.9702;  Loss pred: 0.9702; Loss self: 0.0000; time: 0.28s
Val loss: 0.6615 score: 0.8605 time: 0.19s
Test loss: 0.6539 score: 0.9535 time: 0.19s
Epoch 59/1000, LR 0.000283
Train loss: 0.9680;  Loss pred: 0.9680; Loss self: 0.0000; time: 0.28s
Val loss: 0.6595 score: 0.8605 time: 0.18s
Test loss: 0.6514 score: 0.9535 time: 0.18s
Epoch 60/1000, LR 0.000283
Train loss: 0.9640;  Loss pred: 0.9640; Loss self: 0.0000; time: 0.26s
Val loss: 0.6575 score: 0.8605 time: 0.17s
Test loss: 0.6488 score: 0.9535 time: 0.17s
Epoch 61/1000, LR 0.000283
Train loss: 0.9617;  Loss pred: 0.9617; Loss self: 0.0000; time: 0.26s
Val loss: 0.6553 score: 0.8605 time: 0.20s
Test loss: 0.6461 score: 0.9535 time: 0.21s
Epoch 62/1000, LR 0.000283
Train loss: 0.9591;  Loss pred: 0.9591; Loss self: 0.0000; time: 0.32s
Val loss: 0.6530 score: 0.8682 time: 0.18s
Test loss: 0.6432 score: 0.9612 time: 0.18s
Epoch 63/1000, LR 0.000283
Train loss: 0.9534;  Loss pred: 0.9534; Loss self: 0.0000; time: 0.27s
Val loss: 0.6505 score: 0.8682 time: 0.18s
Test loss: 0.6402 score: 0.9612 time: 0.19s
Epoch 64/1000, LR 0.000283
Train loss: 0.9529;  Loss pred: 0.9529; Loss self: 0.0000; time: 0.26s
Val loss: 0.6479 score: 0.8682 time: 0.20s
Test loss: 0.6369 score: 0.9612 time: 0.24s
Epoch 65/1000, LR 0.000283
Train loss: 0.9479;  Loss pred: 0.9479; Loss self: 0.0000; time: 0.38s
Val loss: 0.6451 score: 0.8682 time: 0.25s
Test loss: 0.6335 score: 0.9612 time: 0.18s
Epoch 66/1000, LR 0.000283
Train loss: 0.9449;  Loss pred: 0.9449; Loss self: 0.0000; time: 0.27s
Val loss: 0.6422 score: 0.8682 time: 0.19s
Test loss: 0.6298 score: 0.9612 time: 0.18s
Epoch 67/1000, LR 0.000283
Train loss: 0.9421;  Loss pred: 0.9421; Loss self: 0.0000; time: 0.28s
Val loss: 0.6391 score: 0.8682 time: 0.19s
Test loss: 0.6260 score: 0.9612 time: 0.18s
Epoch 68/1000, LR 0.000283
Train loss: 0.9366;  Loss pred: 0.9366; Loss self: 0.0000; time: 0.27s
Val loss: 0.6359 score: 0.8760 time: 0.19s
Test loss: 0.6220 score: 0.9612 time: 0.18s
Epoch 69/1000, LR 0.000283
Train loss: 0.9337;  Loss pred: 0.9337; Loss self: 0.0000; time: 0.27s
Val loss: 0.6326 score: 0.8760 time: 0.18s
Test loss: 0.6178 score: 0.9612 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9304;  Loss pred: 0.9304; Loss self: 0.0000; time: 0.26s
Val loss: 0.6291 score: 0.8760 time: 0.21s
Test loss: 0.6135 score: 0.9612 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 0.9274;  Loss pred: 0.9274; Loss self: 0.0000; time: 0.37s
Val loss: 0.6255 score: 0.8760 time: 0.26s
Test loss: 0.6089 score: 0.9612 time: 0.26s
Epoch 72/1000, LR 0.000282
Train loss: 0.9221;  Loss pred: 0.9221; Loss self: 0.0000; time: 0.31s
Val loss: 0.6218 score: 0.8760 time: 0.18s
Test loss: 0.6043 score: 0.9612 time: 0.18s
Epoch 73/1000, LR 0.000282
Train loss: 0.9188;  Loss pred: 0.9188; Loss self: 0.0000; time: 0.26s
Val loss: 0.6179 score: 0.8760 time: 0.18s
Test loss: 0.5995 score: 0.9612 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 0.9155;  Loss pred: 0.9155; Loss self: 0.0000; time: 0.26s
Val loss: 0.6139 score: 0.8760 time: 0.18s
Test loss: 0.5945 score: 0.9612 time: 0.18s
Epoch 75/1000, LR 0.000282
Train loss: 0.9106;  Loss pred: 0.9106; Loss self: 0.0000; time: 0.26s
Val loss: 0.6098 score: 0.8760 time: 0.19s
Test loss: 0.5894 score: 0.9612 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9066;  Loss pred: 0.9066; Loss self: 0.0000; time: 0.25s
Val loss: 0.6055 score: 0.8760 time: 0.17s
Test loss: 0.5841 score: 0.9612 time: 0.17s
Epoch 77/1000, LR 0.000282
Train loss: 0.9018;  Loss pred: 0.9018; Loss self: 0.0000; time: 0.27s
Val loss: 0.6011 score: 0.8760 time: 0.18s
Test loss: 0.5786 score: 0.9612 time: 0.18s
Epoch 78/1000, LR 0.000282
Train loss: 0.8969;  Loss pred: 0.8969; Loss self: 0.0000; time: 0.29s
Val loss: 0.5966 score: 0.8760 time: 0.22s
Test loss: 0.5729 score: 0.9612 time: 0.17s
Epoch 79/1000, LR 0.000282
Train loss: 0.8928;  Loss pred: 0.8928; Loss self: 0.0000; time: 0.26s
Val loss: 0.5920 score: 0.8760 time: 0.17s
Test loss: 0.5670 score: 0.9612 time: 0.17s
Epoch 80/1000, LR 0.000282
Train loss: 0.8889;  Loss pred: 0.8889; Loss self: 0.0000; time: 0.25s
Val loss: 0.5873 score: 0.8760 time: 0.17s
Test loss: 0.5611 score: 0.9612 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.8827;  Loss pred: 0.8827; Loss self: 0.0000; time: 0.27s
Val loss: 0.5824 score: 0.8760 time: 0.17s
Test loss: 0.5549 score: 0.9612 time: 0.18s
Epoch 82/1000, LR 0.000281
Train loss: 0.8799;  Loss pred: 0.8799; Loss self: 0.0000; time: 0.26s
Val loss: 0.5775 score: 0.8760 time: 0.22s
Test loss: 0.5486 score: 0.9612 time: 0.21s
Epoch 83/1000, LR 0.000281
Train loss: 0.8744;  Loss pred: 0.8744; Loss self: 0.0000; time: 0.27s
Val loss: 0.5724 score: 0.8760 time: 0.19s
Test loss: 0.5422 score: 0.9612 time: 0.18s
Epoch 84/1000, LR 0.000281
Train loss: 0.8681;  Loss pred: 0.8681; Loss self: 0.0000; time: 0.26s
Val loss: 0.5673 score: 0.8837 time: 0.18s
Test loss: 0.5357 score: 0.9612 time: 0.18s
Epoch 85/1000, LR 0.000281
Train loss: 0.8639;  Loss pred: 0.8639; Loss self: 0.0000; time: 0.32s
Val loss: 0.5621 score: 0.8837 time: 0.26s
Test loss: 0.5291 score: 0.9612 time: 0.25s
Epoch 86/1000, LR 0.000281
Train loss: 0.8585;  Loss pred: 0.8585; Loss self: 0.0000; time: 0.27s
Val loss: 0.5568 score: 0.8837 time: 0.18s
Test loss: 0.5223 score: 0.9612 time: 0.18s
Epoch 87/1000, LR 0.000281
Train loss: 0.8527;  Loss pred: 0.8527; Loss self: 0.0000; time: 0.27s
Val loss: 0.5515 score: 0.8837 time: 0.18s
Test loss: 0.5154 score: 0.9612 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.8478;  Loss pred: 0.8478; Loss self: 0.0000; time: 0.26s
Val loss: 0.5462 score: 0.8837 time: 0.19s
Test loss: 0.5085 score: 0.9612 time: 0.19s
Epoch 89/1000, LR 0.000281
Train loss: 0.8446;  Loss pred: 0.8446; Loss self: 0.0000; time: 0.26s
Val loss: 0.5407 score: 0.8837 time: 0.17s
Test loss: 0.5016 score: 0.9612 time: 0.18s
Epoch 90/1000, LR 0.000281
Train loss: 0.8386;  Loss pred: 0.8386; Loss self: 0.0000; time: 0.25s
Val loss: 0.5353 score: 0.8837 time: 0.21s
Test loss: 0.4945 score: 0.9612 time: 0.19s
Epoch 91/1000, LR 0.000280
Train loss: 0.8334;  Loss pred: 0.8334; Loss self: 0.0000; time: 0.34s
Val loss: 0.5298 score: 0.8837 time: 0.18s
Test loss: 0.4874 score: 0.9612 time: 0.18s
Epoch 92/1000, LR 0.000280
Train loss: 0.8266;  Loss pred: 0.8266; Loss self: 0.0000; time: 0.26s
Val loss: 0.5243 score: 0.8837 time: 0.18s
Test loss: 0.4803 score: 0.9612 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.8223;  Loss pred: 0.8223; Loss self: 0.0000; time: 0.26s
Val loss: 0.5187 score: 0.8837 time: 0.18s
Test loss: 0.4730 score: 0.9612 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.8184;  Loss pred: 0.8184; Loss self: 0.0000; time: 0.26s
Val loss: 0.5131 score: 0.8837 time: 0.18s
Test loss: 0.4657 score: 0.9612 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.8120;  Loss pred: 0.8120; Loss self: 0.0000; time: 0.26s
Val loss: 0.5074 score: 0.8837 time: 0.18s
Test loss: 0.4582 score: 0.9612 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.8073;  Loss pred: 0.8073; Loss self: 0.0000; time: 0.33s
Val loss: 0.5017 score: 0.8837 time: 0.18s
Test loss: 0.4508 score: 0.9612 time: 0.18s
Epoch 97/1000, LR 0.000280
Train loss: 0.8006;  Loss pred: 0.8006; Loss self: 0.0000; time: 0.27s
Val loss: 0.4961 score: 0.8837 time: 0.18s
Test loss: 0.4433 score: 0.9612 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.7975;  Loss pred: 0.7975; Loss self: 0.0000; time: 0.26s
Val loss: 0.4905 score: 0.8837 time: 0.18s
Test loss: 0.4360 score: 0.9612 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.7934;  Loss pred: 0.7934; Loss self: 0.0000; time: 0.33s
Val loss: 0.4849 score: 0.8837 time: 0.18s
Test loss: 0.4287 score: 0.9612 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.7864;  Loss pred: 0.7864; Loss self: 0.0000; time: 0.26s
Val loss: 0.4794 score: 0.8837 time: 0.17s
Test loss: 0.4216 score: 0.9535 time: 0.18s
Epoch 101/1000, LR 0.000279
Train loss: 0.7813;  Loss pred: 0.7813; Loss self: 0.0000; time: 0.26s
Val loss: 0.4739 score: 0.8837 time: 0.18s
Test loss: 0.4145 score: 0.9535 time: 0.25s
Epoch 102/1000, LR 0.000279
Train loss: 0.7747;  Loss pred: 0.7747; Loss self: 0.0000; time: 0.31s
Val loss: 0.4685 score: 0.8837 time: 0.20s
Test loss: 0.4075 score: 0.9535 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.7707;  Loss pred: 0.7707; Loss self: 0.0000; time: 0.26s
Val loss: 0.4632 score: 0.8837 time: 0.18s
Test loss: 0.4006 score: 0.9535 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.7647;  Loss pred: 0.7647; Loss self: 0.0000; time: 0.30s
Val loss: 0.4579 score: 0.8837 time: 0.17s
Test loss: 0.3935 score: 0.9535 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.7615;  Loss pred: 0.7615; Loss self: 0.0000; time: 0.34s
Val loss: 0.4528 score: 0.8837 time: 0.18s
Test loss: 0.3865 score: 0.9535 time: 0.18s
Epoch 106/1000, LR 0.000279
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.26s
Val loss: 0.4478 score: 0.8837 time: 0.27s
Test loss: 0.3797 score: 0.9535 time: 0.18s
Epoch 107/1000, LR 0.000278
Train loss: 0.7491;  Loss pred: 0.7491; Loss self: 0.0000; time: 0.25s
Val loss: 0.4427 score: 0.8837 time: 0.17s
Test loss: 0.3730 score: 0.9535 time: 0.19s
Epoch 108/1000, LR 0.000278
Train loss: 0.7472;  Loss pred: 0.7472; Loss self: 0.0000; time: 0.33s
Val loss: 0.4378 score: 0.8837 time: 0.26s
Test loss: 0.3663 score: 0.9535 time: 0.25s
Epoch 109/1000, LR 0.000278
Train loss: 0.7406;  Loss pred: 0.7406; Loss self: 0.0000; time: 0.38s
Val loss: 0.4329 score: 0.8837 time: 0.26s
Test loss: 0.3598 score: 0.9535 time: 0.21s
Epoch 110/1000, LR 0.000278
Train loss: 0.7379;  Loss pred: 0.7379; Loss self: 0.0000; time: 0.32s
Val loss: 0.4279 score: 0.8837 time: 0.18s
Test loss: 0.3533 score: 0.9535 time: 0.18s
Epoch 111/1000, LR 0.000278
Train loss: 0.7320;  Loss pred: 0.7320; Loss self: 0.0000; time: 0.26s
Val loss: 0.4232 score: 0.8837 time: 0.18s
Test loss: 0.3470 score: 0.9535 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.7275;  Loss pred: 0.7275; Loss self: 0.0000; time: 0.26s
Val loss: 0.4184 score: 0.8837 time: 0.18s
Test loss: 0.3408 score: 0.9535 time: 0.18s
Epoch 113/1000, LR 0.000278
Train loss: 0.7239;  Loss pred: 0.7239; Loss self: 0.0000; time: 0.26s
Val loss: 0.4139 score: 0.8837 time: 0.18s
Test loss: 0.3347 score: 0.9535 time: 0.18s
Epoch 114/1000, LR 0.000277
Train loss: 0.7183;  Loss pred: 0.7183; Loss self: 0.0000; time: 0.26s
Val loss: 0.4093 score: 0.8837 time: 0.18s
Test loss: 0.3287 score: 0.9535 time: 0.18s
Epoch 115/1000, LR 0.000277
Train loss: 0.7152;  Loss pred: 0.7152; Loss self: 0.0000; time: 0.26s
Val loss: 0.4049 score: 0.8837 time: 0.20s
Test loss: 0.3228 score: 0.9535 time: 0.18s
Epoch 116/1000, LR 0.000277
Train loss: 0.7090;  Loss pred: 0.7090; Loss self: 0.0000; time: 0.34s
Val loss: 0.4006 score: 0.8837 time: 0.18s
Test loss: 0.3170 score: 0.9535 time: 0.18s
Epoch 117/1000, LR 0.000277
Train loss: 0.7046;  Loss pred: 0.7046; Loss self: 0.0000; time: 0.25s
Val loss: 0.3964 score: 0.8837 time: 0.17s
Test loss: 0.3113 score: 0.9535 time: 0.18s
Epoch 118/1000, LR 0.000277
Train loss: 0.7025;  Loss pred: 0.7025; Loss self: 0.0000; time: 0.25s
Val loss: 0.3924 score: 0.8837 time: 0.18s
Test loss: 0.3056 score: 0.9535 time: 0.18s
Epoch 119/1000, LR 0.000277
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.25s
Val loss: 0.3884 score: 0.8837 time: 0.17s
Test loss: 0.3001 score: 0.9535 time: 0.18s
Epoch 120/1000, LR 0.000277
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.28s
Val loss: 0.3842 score: 0.8837 time: 0.18s
Test loss: 0.2948 score: 0.9535 time: 0.23s
Epoch 121/1000, LR 0.000276
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.34s
Val loss: 0.3803 score: 0.8837 time: 0.18s
Test loss: 0.2896 score: 0.9535 time: 0.17s
Epoch 122/1000, LR 0.000276
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.25s
Val loss: 0.3765 score: 0.8837 time: 0.18s
Test loss: 0.2844 score: 0.9535 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.26s
Val loss: 0.3728 score: 0.8837 time: 0.18s
Test loss: 0.2794 score: 0.9535 time: 0.17s
Epoch 124/1000, LR 0.000276
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.25s
Val loss: 0.3692 score: 0.8837 time: 0.18s
Test loss: 0.2745 score: 0.9535 time: 0.18s
Epoch 125/1000, LR 0.000276
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.31s
Val loss: 0.3659 score: 0.8837 time: 0.26s
Test loss: 0.2696 score: 0.9535 time: 0.26s
Epoch 126/1000, LR 0.000276
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.39s
Val loss: 0.3623 score: 0.8837 time: 0.26s
Test loss: 0.2650 score: 0.9535 time: 0.21s
Epoch 127/1000, LR 0.000275
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.33s
Val loss: 0.3589 score: 0.8837 time: 0.18s
Test loss: 0.2605 score: 0.9535 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.26s
Val loss: 0.3557 score: 0.8837 time: 0.18s
Test loss: 0.2561 score: 0.9535 time: 0.17s
Epoch 129/1000, LR 0.000275
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.25s
Val loss: 0.3526 score: 0.8837 time: 0.18s
Test loss: 0.2517 score: 0.9535 time: 0.18s
Epoch 130/1000, LR 0.000275
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.26s
Val loss: 0.3496 score: 0.8837 time: 0.18s
Test loss: 0.2475 score: 0.9535 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 0.25s
Val loss: 0.3467 score: 0.8837 time: 0.19s
Test loss: 0.2433 score: 0.9535 time: 0.17s
Epoch 132/1000, LR 0.000275
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.26s
Val loss: 0.3441 score: 0.8837 time: 0.19s
Test loss: 0.2393 score: 0.9535 time: 0.22s
Epoch 133/1000, LR 0.000274
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.28s
Val loss: 0.3411 score: 0.8837 time: 0.28s
Test loss: 0.2355 score: 0.9535 time: 0.19s
Epoch 134/1000, LR 0.000274
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.28s
Val loss: 0.3382 score: 0.8837 time: 0.19s
Test loss: 0.2318 score: 0.9535 time: 0.19s
Epoch 135/1000, LR 0.000274
Train loss: 0.6463;  Loss pred: 0.6463; Loss self: 0.0000; time: 0.27s
Val loss: 0.3357 score: 0.8837 time: 0.19s
Test loss: 0.2281 score: 0.9535 time: 0.19s
Epoch 136/1000, LR 0.000274
Train loss: 0.6438;  Loss pred: 0.6438; Loss self: 0.0000; time: 0.27s
Val loss: 0.3330 score: 0.8837 time: 0.20s
Test loss: 0.2246 score: 0.9535 time: 0.22s
Epoch 137/1000, LR 0.000274
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.36s
Val loss: 0.3306 score: 0.8837 time: 0.28s
Test loss: 0.2210 score: 0.9535 time: 0.27s
Epoch 138/1000, LR 0.000274
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.36s
Val loss: 0.3282 score: 0.8837 time: 0.18s
Test loss: 0.2177 score: 0.9535 time: 0.18s
Epoch 139/1000, LR 0.000273
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.26s
Val loss: 0.3257 score: 0.8837 time: 0.18s
Test loss: 0.2145 score: 0.9535 time: 0.18s
Epoch 140/1000, LR 0.000273
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.27s
Val loss: 0.3233 score: 0.8837 time: 0.21s
Test loss: 0.2114 score: 0.9535 time: 0.18s
Epoch 141/1000, LR 0.000273
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.33s
Val loss: 0.3212 score: 0.8837 time: 0.19s
Test loss: 0.2083 score: 0.9535 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.6285;  Loss pred: 0.6285; Loss self: 0.0000; time: 0.26s
Val loss: 0.3188 score: 0.8915 time: 0.18s
Test loss: 0.2055 score: 0.9535 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.25s
Val loss: 0.3160 score: 0.8915 time: 0.17s
Test loss: 0.2028 score: 0.9535 time: 0.17s
Epoch 144/1000, LR 0.000272
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.26s
Val loss: 0.3140 score: 0.8915 time: 0.26s
Test loss: 0.2000 score: 0.9535 time: 0.26s
Epoch 145/1000, LR 0.000272
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.36s
Val loss: 0.3117 score: 0.8837 time: 0.18s
Test loss: 0.1975 score: 0.9612 time: 0.17s
Epoch 146/1000, LR 0.000272
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.26s
Val loss: 0.3098 score: 0.8837 time: 0.18s
Test loss: 0.1948 score: 0.9612 time: 0.17s
Epoch 147/1000, LR 0.000272
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 0.25s
Val loss: 0.3085 score: 0.8915 time: 0.17s
Test loss: 0.1920 score: 0.9612 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 0.25s
Val loss: 0.3069 score: 0.8915 time: 0.17s
Test loss: 0.1894 score: 0.9612 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.24s
Val loss: 0.3053 score: 0.8837 time: 0.17s
Test loss: 0.1871 score: 0.9612 time: 0.17s
Epoch 150/1000, LR 0.000271
Train loss: 0.6120;  Loss pred: 0.6120; Loss self: 0.0000; time: 0.24s
Val loss: 0.3036 score: 0.8837 time: 0.17s
Test loss: 0.1847 score: 0.9612 time: 0.18s
Epoch 151/1000, LR 0.000271
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.27s
Val loss: 0.3021 score: 0.8837 time: 0.24s
Test loss: 0.1825 score: 0.9612 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.25s
Val loss: 0.3005 score: 0.8837 time: 0.17s
Test loss: 0.1803 score: 0.9612 time: 0.17s
Epoch 153/1000, LR 0.000271
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.25s
Val loss: 0.2992 score: 0.8837 time: 0.17s
Test loss: 0.1781 score: 0.9612 time: 0.17s
Epoch 154/1000, LR 0.000271
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.25s
Val loss: 0.2982 score: 0.8837 time: 0.18s
Test loss: 0.1759 score: 0.9612 time: 0.17s
Epoch 155/1000, LR 0.000270
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.25s
Val loss: 0.2968 score: 0.8837 time: 0.18s
Test loss: 0.1738 score: 0.9612 time: 0.17s
Epoch 156/1000, LR 0.000270
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.25s
Val loss: 0.2957 score: 0.8837 time: 0.18s
Test loss: 0.1718 score: 0.9612 time: 0.17s
Epoch 157/1000, LR 0.000270
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 0.25s
Val loss: 0.2947 score: 0.8837 time: 0.17s
Test loss: 0.1698 score: 0.9612 time: 0.19s
Epoch 158/1000, LR 0.000270
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.26s
Val loss: 0.2925 score: 0.8837 time: 0.24s
Test loss: 0.1682 score: 0.9612 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.25s
Val loss: 0.2908 score: 0.8837 time: 0.17s
Test loss: 0.1666 score: 0.9612 time: 0.17s
Epoch 160/1000, LR 0.000269
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.25s
Val loss: 0.2889 score: 0.8837 time: 0.18s
Test loss: 0.1651 score: 0.9612 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.24s
Val loss: 0.2878 score: 0.8837 time: 0.17s
Test loss: 0.1635 score: 0.9612 time: 0.17s
Epoch 162/1000, LR 0.000269
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.25s
Val loss: 0.2868 score: 0.8915 time: 0.17s
Test loss: 0.1619 score: 0.9612 time: 0.18s
Epoch 163/1000, LR 0.000269
Train loss: 0.5919;  Loss pred: 0.5919; Loss self: 0.0000; time: 0.25s
Val loss: 0.2861 score: 0.8915 time: 0.25s
Test loss: 0.1602 score: 0.9612 time: 0.23s
Epoch 164/1000, LR 0.000269
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.35s
Val loss: 0.2858 score: 0.8837 time: 0.18s
Test loss: 0.1585 score: 0.9612 time: 0.16s
Epoch 165/1000, LR 0.000268
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.25s
Val loss: 0.2851 score: 0.8915 time: 0.17s
Test loss: 0.1570 score: 0.9612 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.25s
Val loss: 0.2845 score: 0.8915 time: 0.17s
Test loss: 0.1555 score: 0.9612 time: 0.17s
Epoch 167/1000, LR 0.000268
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.25s
Val loss: 0.2839 score: 0.8915 time: 0.17s
Test loss: 0.1541 score: 0.9612 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.25s
Val loss: 0.2831 score: 0.8915 time: 0.18s
Test loss: 0.1527 score: 0.9612 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.24s
Val loss: 0.2822 score: 0.8915 time: 0.20s
Test loss: 0.1515 score: 0.9612 time: 0.17s
Epoch 170/1000, LR 0.000267
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.34s
Val loss: 0.2816 score: 0.8915 time: 0.17s
Test loss: 0.1502 score: 0.9612 time: 0.17s
Epoch 171/1000, LR 0.000267
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.25s
Val loss: 0.2810 score: 0.8915 time: 0.17s
Test loss: 0.1490 score: 0.9612 time: 0.17s
Epoch 172/1000, LR 0.000267
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 0.25s
Val loss: 0.2797 score: 0.8915 time: 0.17s
Test loss: 0.1479 score: 0.9612 time: 0.17s
Epoch 173/1000, LR 0.000267
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.25s
Val loss: 0.2789 score: 0.8915 time: 0.17s
Test loss: 0.1468 score: 0.9612 time: 0.17s
Epoch 174/1000, LR 0.000266
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.25s
Val loss: 0.2787 score: 0.8915 time: 0.17s
Test loss: 0.1455 score: 0.9612 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.29s
Val loss: 0.2782 score: 0.8915 time: 0.18s
Test loss: 0.1444 score: 0.9612 time: 0.21s
Epoch 176/1000, LR 0.000266
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.38s
Val loss: 0.2776 score: 0.8915 time: 0.24s
Test loss: 0.1434 score: 0.9612 time: 0.21s
Epoch 177/1000, LR 0.000266
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.25s
Val loss: 0.2770 score: 0.8915 time: 0.17s
Test loss: 0.1424 score: 0.9612 time: 0.17s
Epoch 178/1000, LR 0.000265
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.25s
Val loss: 0.2765 score: 0.8915 time: 0.17s
Test loss: 0.1414 score: 0.9612 time: 0.17s
Epoch 179/1000, LR 0.000265
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.25s
Val loss: 0.2768 score: 0.8915 time: 0.17s
Test loss: 0.1403 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.26s
Val loss: 0.2775 score: 0.8915 time: 0.17s
Test loss: 0.1391 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 181/1000, LR 0.000265
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.26s
Val loss: 0.2772 score: 0.8915 time: 0.17s
Test loss: 0.1381 score: 0.9612 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 182/1000, LR 0.000265
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.27s
Val loss: 0.2769 score: 0.8915 time: 0.23s
Test loss: 0.1372 score: 0.9612 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 183/1000, LR 0.000264
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.36s
Val loss: 0.2769 score: 0.8915 time: 0.18s
Test loss: 0.1363 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.25s
Val loss: 0.2770 score: 0.8915 time: 0.17s
Test loss: 0.1354 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.5644;  Loss pred: 0.5644; Loss self: 0.0000; time: 0.25s
Val loss: 0.2761 score: 0.8915 time: 0.17s
Test loss: 0.1346 score: 0.9612 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.26s
Val loss: 0.2762 score: 0.8915 time: 0.21s
Test loss: 0.1337 score: 0.9612 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.25s
Val loss: 0.2757 score: 0.8915 time: 0.18s
Test loss: 0.1330 score: 0.9612 time: 0.17s
Epoch 188/1000, LR 0.000263
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 0.25s
Val loss: 0.2747 score: 0.8915 time: 0.19s
Test loss: 0.1323 score: 0.9612 time: 0.18s
Epoch 189/1000, LR 0.000263
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.25s
Val loss: 0.2749 score: 0.8915 time: 0.18s
Test loss: 0.1315 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.30s
Val loss: 0.2740 score: 0.8915 time: 0.19s
Test loss: 0.1309 score: 0.9612 time: 0.17s
Epoch 191/1000, LR 0.000262
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.25s
Val loss: 0.2740 score: 0.8915 time: 0.24s
Test loss: 0.1301 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 192/1000, LR 0.000262
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.26s
Val loss: 0.2739 score: 0.8915 time: 0.22s
Test loss: 0.1295 score: 0.9612 time: 0.26s
Epoch 193/1000, LR 0.000262
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.25s
Val loss: 0.2733 score: 0.8915 time: 0.18s
Test loss: 0.1289 score: 0.9612 time: 0.17s
Epoch 194/1000, LR 0.000262
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.25s
Val loss: 0.2730 score: 0.8915 time: 0.18s
Test loss: 0.1284 score: 0.9612 time: 0.17s
Epoch 195/1000, LR 0.000261
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.27s
Val loss: 0.2730 score: 0.8915 time: 0.32s
Test loss: 0.1277 score: 0.9612 time: 0.17s
Epoch 196/1000, LR 0.000261
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.26s
Val loss: 0.2726 score: 0.8915 time: 0.18s
Test loss: 0.1272 score: 0.9612 time: 0.17s
Epoch 197/1000, LR 0.000261
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.27s
Val loss: 0.2729 score: 0.8915 time: 0.18s
Test loss: 0.1266 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 198/1000, LR 0.000261
Train loss: 0.5569;  Loss pred: 0.5569; Loss self: 0.0000; time: 0.28s
Val loss: 0.2734 score: 0.8915 time: 0.17s
Test loss: 0.1259 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.26s
Val loss: 0.2732 score: 0.8915 time: 0.17s
Test loss: 0.1253 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 0.27s
Val loss: 0.2737 score: 0.8915 time: 0.18s
Test loss: 0.1246 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 0.34s
Val loss: 0.2746 score: 0.8915 time: 0.18s
Test loss: 0.1240 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 0.26s
Val loss: 0.2742 score: 0.8915 time: 0.18s
Test loss: 0.1235 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.26s
Val loss: 0.2738 score: 0.8915 time: 0.18s
Test loss: 0.1231 score: 0.9612 time: 0.25s
     INFO: Early stopping counter 7 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.39s
Val loss: 0.2738 score: 0.8915 time: 0.26s
Test loss: 0.1227 score: 0.9612 time: 0.25s
     INFO: Early stopping counter 8 of 20
Epoch 205/1000, LR 0.000259
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.39s
Val loss: 0.2738 score: 0.8915 time: 0.26s
Test loss: 0.1222 score: 0.9612 time: 0.25s
     INFO: Early stopping counter 9 of 20
Epoch 206/1000, LR 0.000259
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.40s
Val loss: 0.2742 score: 0.8915 time: 0.23s
Test loss: 0.1217 score: 0.9612 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.27s
Val loss: 0.2737 score: 0.8992 time: 0.18s
Test loss: 0.1213 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.27s
Val loss: 0.2740 score: 0.8992 time: 0.18s
Test loss: 0.1209 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5478;  Loss pred: 0.5478; Loss self: 0.0000; time: 0.27s
Val loss: 0.2742 score: 0.8992 time: 0.18s
Test loss: 0.1204 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.27s
Val loss: 0.2749 score: 0.8915 time: 0.18s
Test loss: 0.1199 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.27s
Val loss: 0.2754 score: 0.8915 time: 0.18s
Test loss: 0.1194 score: 0.9612 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.30s
Val loss: 0.2760 score: 0.8915 time: 0.20s
Test loss: 0.1190 score: 0.9612 time: 0.24s
     INFO: Early stopping counter 16 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.32s
Val loss: 0.2760 score: 0.8915 time: 0.19s
Test loss: 0.1186 score: 0.9612 time: 0.19s
     INFO: Early stopping counter 17 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.28s
Val loss: 0.2762 score: 0.8915 time: 0.19s
Test loss: 0.1182 score: 0.9612 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.26s
Val loss: 0.2763 score: 0.8915 time: 0.18s
Test loss: 0.1179 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.28s
Val loss: 0.2772 score: 0.8915 time: 0.19s
Test loss: 0.1175 score: 0.9612 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 195,   Train_Loss: 0.5571,   Val_Loss: 0.2726,   Val_Precision: 0.9630,   Val_Recall: 0.8125,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.2726,   Test_Precision: 0.9688,   Test_Recall: 0.9538,   Test_accuracy: 0.9612,   Test_Score: 0.9612,   Test_loss: 0.1272


[0.17553729191422462, 0.17636333708651364, 0.17716077016666532, 0.2639829209074378, 0.1783866120968014, 0.18214822001755238, 0.19493592693470418, 0.17595083895139396, 0.3294711129274219, 0.2708030780777335, 0.27246263809502125, 0.17976868501864374, 0.19387241709046066, 0.18678679503500462, 0.1849911310710013, 0.1952481889165938, 0.19207121594808996, 0.187116920016706, 0.22203910420648754, 0.18039479316212237, 0.17881705099716783, 0.17879667412489653, 0.18621564097702503, 0.17370856902562082, 0.17674741009250283, 0.19113518693484366, 0.18099136580713093, 0.2696086319629103, 0.19513565092347562, 0.1962961859535426, 0.19543089577928185, 0.1880191038362682, 0.1952838411089033, 0.19462593900971115, 0.19527766713872552, 0.19017368904314935, 0.18919846694916487, 0.20559936994686723, 0.26660167099907994, 0.27408914896659553, 0.193543063942343, 0.18811267614364624, 0.18419204582460225, 0.18594341399148107, 0.20090563781559467, 0.17734047188423574, 0.18097334099002182, 0.1793673629872501, 0.1910700029693544, 0.25825565797276795, 0.17306552990339696, 0.18774029309861362, 0.19375561783090234, 0.17612216505222023, 0.1730023710988462, 0.18816776596941054, 0.2434256940614432, 0.19006537809036672, 0.18164691887795925, 0.17743582697585225, 0.21789261605590582, 0.1867034260649234, 0.1904192331712693, 0.24072615103796124, 0.18368557188659906, 0.18431883794255555, 0.18012804491445422, 0.18502118694595993, 0.17942086583934724, 0.18469001003541052, 0.2635928839445114, 0.18782815895974636, 0.17860809504054487, 0.1817601011134684, 0.18327923491597176, 0.17584941000677645, 0.1824974159244448, 0.17552838777191937, 0.17139540892094374, 0.17944634798914194, 0.18365456815809011, 0.215807368978858, 0.1819220611359924, 0.1812916691415012, 0.2590104730334133, 0.18268886511214077, 0.17967040999792516, 0.1919750429224223, 0.180240822955966, 0.19123604404740036, 0.18294834694825113, 0.17649409407749772, 0.17618271405808628, 0.17568784696049988, 0.18142522894777358, 0.17980714910663664, 0.1787772229872644, 0.17695956793613732, 0.17462780186906457, 0.18039013701491058, 0.25538640399463475, 0.17624633479863405, 0.17694320296868682, 0.18305406998842955, 0.18163503101095557, 0.18172453599981964, 0.1969627591315657, 0.2581255428958684, 0.20949939009733498, 0.18680954002775252, 0.178399563068524, 0.1869447010103613, 0.18337008706294, 0.1823141749482602, 0.18597275693900883, 0.18686826596967876, 0.1832911951933056, 0.18077533296309412, 0.18270786199718714, 0.2378831380046904, 0.17629412305541337, 0.17856124509125948, 0.1787967428099364, 0.17993332794867456, 0.26006348710507154, 0.21152324811555445, 0.17604209599085152, 0.17809935589320958, 0.1814059680327773, 0.17561236815527081, 0.17786356480792165, 0.22196103795431554, 0.1958942108321935, 0.19468972203321755, 0.19518160610459745, 0.22323607420548797, 0.2710570970084518, 0.1835626158863306, 0.18445413000881672, 0.1864522930700332, 0.18714952003210783, 0.17613570601679385, 0.17557766288518906, 0.2644621569197625, 0.17550530191510916, 0.17424043989740312, 0.17179557611234486, 0.17350164707750082, 0.1776442441623658, 0.18168719997629523, 0.175346887903288, 0.17604778916575015, 0.1723386379890144, 0.17233277182094753, 0.17437032097950578, 0.17251294502057135, 0.19839427201077342, 0.1813806879799813, 0.1786529840901494, 0.17970847990363836, 0.17393021401949227, 0.1878040919546038, 0.2343284748494625, 0.16877628304064274, 0.17013867502100766, 0.17221621400676668, 0.171660162974149, 0.17358940397389233, 0.17681810702197254, 0.17388839810155332, 0.1751140400301665, 0.17318343208171427, 0.1724438068922609, 0.17320367507636547, 0.21645463490858674, 0.21509464597329497, 0.17782913986593485, 0.17481272295117378, 0.17871213401667774, 0.1794955290388316, 0.19261459703557193, 0.25579507695510983, 0.17671529413200915, 0.17869133688509464, 0.17184707685373724, 0.205432022921741, 0.17214757902547717, 0.18649212992750108, 0.1736732949502766, 0.17875031591393054, 0.18793575186282396, 0.2636923659592867, 0.17423190479166806, 0.17678671586327255, 0.17108807293698192, 0.1770331549923867, 0.18324562115594745, 0.17552830511704087, 0.18234013998880982, 0.17964884894900024, 0.18417781218886375, 0.1782823409885168, 0.2559902500361204, 0.25811415584757924, 0.2537865361664444, 0.1951947200577706, 0.18412874080240726, 0.18444849597290158, 0.18481083097867668, 0.17963643604889512, 0.17675286694429815, 0.24651980795897543, 0.19336487096734345, 0.19460616097785532, 0.18406679783947766, 0.18862420902587473]
[0.0013607542008854622, 0.0013671576518334391, 0.0013733393036175607, 0.0020463792318406032, 0.0013828419542387706, 0.0014120017055624216, 0.0015111312165480945, 0.001363959991871271, 0.002554039635096294, 0.002099248667269252, 0.002112113473604816, 0.0013935556978189438, 0.0015028869541896174, 0.0014479596514341442, 0.0014340397757441961, 0.0015135518520666186, 0.0014889241546363562, 0.0014505187598194264, 0.0017212333659417638, 0.001398409249318778, 0.0013861786899005259, 0.0013860207296503607, 0.0014435321005970933, 0.001346578054462177, 0.0013701349619573862, 0.0014816681157739818, 0.0014030338434661313, 0.00208998939506132, 0.0015126794645230668, 0.0015216758601049813, 0.0015149681843355182, 0.0014575124328392883, 0.0015138282256504131, 0.0015087282093776057, 0.001513780365416477, 0.0014742146437453439, 0.0014666547825516656, 0.001593793565479591, 0.0020666796201479066, 0.0021247220850123686, 0.0015003338290104108, 0.0014582377995631492, 0.001427845316469785, 0.00144142181388745, 0.0015574080450821291, 0.001374732340187874, 0.0014028941162017195, 0.0013904446743197682, 0.0014811628137159254, 0.002001981844750139, 0.0013415932550650928, 0.0014553511092915785, 0.0015019815335728863, 0.0013652881011800018, 0.0013411036519290402, 0.0014586648524760506, 0.0018870208841972342, 0.0014733750239563311, 0.0014081156502167385, 0.0013754715269445911, 0.0016890900469450063, 0.0014473133803482436, 0.0014761180865989869, 0.0018660941940927228, 0.0014239191619116207, 0.0014288282011050818, 0.0013963414334453815, 0.0014342727670229453, 0.001390859425111219, 0.0014317055041504691, 0.0020433556894923367, 0.0014560322399980337, 0.0013845588762832935, 0.001408993031887352, 0.0014207692629145097, 0.0013631737209827632, 0.0014147086505770915, 0.0013606851765265067, 0.0013286465807825095, 0.0013910569611561392, 0.0014236788229309311, 0.00167292534092138, 0.0014102485359379256, 0.0014053617762907068, 0.0020078331242900254, 0.0014161927528072928, 0.0013927938759529083, 0.0014881786273055991, 0.0013972156818291938, 0.0014824499538558166, 0.0014182042399089234, 0.0013681712719185869, 0.0013657574733184983, 0.001361921294267441, 0.0014063971236261518, 0.0013938538690436949, 0.0013858699456377084, 0.0013717795964041653, 0.001353703890457865, 0.0013983731551543456, 0.0019797395658498816, 0.0013662506573537524, 0.0013716527361913708, 0.001419023798359919, 0.0014080234962089578, 0.0014087173333319353, 0.001526843094043145, 0.002000973200743166, 0.0016240262798243022, 0.001448135969207384, 0.0013829423493684032, 0.0014491837287624908, 0.001421473543123566, 0.0014132881778934898, 0.0014416492785969677, 0.001448591209067277, 0.0014208619782426792, 0.0014013591702565435, 0.0014163400154820708, 0.001844055333369693, 0.0013666211089566928, 0.0013841956983818564, 0.0013860212620925303, 0.0013948319996021283, 0.002015996024070322, 0.0016397151016709648, 0.0013646674107817948, 0.001380615161962865, 0.001406247814207576, 0.0013613361872501614, 0.001378787324092416, 0.0017206282011962444, 0.0015185597738929728, 0.0015092226514202911, 0.0015130357062371896, 0.0017305122031433175, 0.0021012178062670686, 0.0014229660146227179, 0.0014298769768125326, 0.0014453666129459937, 0.001450771473117115, 0.0013653930698976267, 0.0013610671541487524, 0.002050094239688081, 0.001360506216396195, 0.0013507010844759932, 0.0013317486520336812, 0.0013449740083527196, 0.0013770871640493472, 0.0014084279067929863, 0.0013592782008006823, 0.0013647115439205438, 0.0013359584340233673, 0.0013359129598523064, 0.0013517079145698122, 0.0013373096513222586, 0.0015379400931067708, 0.0014060518448060567, 0.001384906853412011, 0.0013930889915010727, 0.0013482962327092424, 0.001455845674066696, 0.001816499805034593, 0.0013083432793848273, 0.0013189044575271911, 0.001335009410905168, 0.0013306989377841008, 0.0013456542943712584, 0.0013706830001703298, 0.001347972078306615, 0.0013574731785284226, 0.0013425072254396456, 0.0013367736968392318, 0.001342664147878802, 0.0016779429062681143, 0.0016674003563821316, 0.0013785204640770143, 0.0013551373872184014, 0.001385365379974246, 0.0013914382096033456, 0.0014931364111284645, 0.001982907573295425, 0.0013698860010233267, 0.0013852041618999584, 0.0013321478825871103, 0.0015924963017189225, 0.0013344773567866446, 0.001445675425794582, 0.0013463046120176481, 0.0013856613636738801, 0.0014568662935102632, 0.002044126867901447, 0.0013506349208656438, 0.001370439657854826, 0.0013262641312944334, 0.001372350038700672, 0.0014205086911313755, 0.0013606845357910144, 0.0014134894572775955, 0.0013926267360387615, 0.0014277349782082461, 0.0013820336510737736, 0.0019844205429156623, 0.002000884929051002, 0.0019673374896623597, 0.0015131373647889195, 0.0014273545798636222, 0.0014298333021155161, 0.0014326421006098967, 0.001392530512006939, 0.0013701772631340941, 0.0019110062632478717, 0.0014989524881189415, 0.001508574891301204, 0.001426874401856416, 0.001462203170743215]
[734.8865793317307, 731.4445401808205, 728.152174313999, 488.66797729400145, 723.1484385722748, 708.2144419943777, 661.7559011746968, 733.1593345550116, 391.53660196126805, 476.36090740065663, 473.4594104422174, 717.588828035436, 665.3860406548124, 690.6269791492748, 697.3307274416773, 660.6975496971512, 671.6258829478339, 689.4085259017878, 580.9787445369764, 715.0982450145697, 721.4077140889833, 721.4899305671, 692.745245905073, 742.6231228752647, 729.8551075372835, 674.9149754617133, 712.7411820156423, 478.47132734884525, 661.0785850228292, 657.170180731532, 660.0798685674122, 686.1004938750079, 660.5769287795861, 662.8099042520913, 660.5978138214762, 678.3272736047657, 681.8237065031853, 627.4338293611371, 483.86793494795904, 470.64978853183936, 666.5183312300431, 685.7592090258355, 700.3559758646737, 693.7594466556911, 642.0924838276827, 727.4143269687959, 712.8121705346234, 719.1943832567225, 675.1452242385228, 499.50502929001675, 745.3823997881392, 687.1194130513085, 665.7871469439563, 732.4461402217688, 745.6545201122989, 685.5584394883599, 529.9358413965908, 678.7138262428145, 710.168940914817, 727.023410089305, 592.034747826892, 690.9353658841918, 677.4525758328896, 535.8786299028118, 702.2870586680592, 699.8742040691678, 716.1572206108395, 697.2174491436901, 718.9799212958102, 698.4676646845538, 489.3910566536979, 686.7979791445761, 722.2516984502656, 709.7267178535978, 703.8440555425866, 733.5822167104735, 706.8593237145171, 734.9238583995991, 752.6455977563631, 718.8778230683503, 702.405615573671, 597.7553065513613, 709.0948683984428, 711.5605510770235, 498.0493587352297, 706.118568971433, 717.9813303787179, 671.9623448769291, 715.7091156397768, 674.5590280461232, 705.1170570920164, 730.9026439341178, 732.1944192406387, 734.2568210139386, 711.036721563873, 717.4353224603717, 721.5684293809039, 728.9800800517021, 738.7139883758248, 715.1167028014243, 505.11694429398864, 731.9301144468383, 729.0475013206853, 704.7098161114571, 710.215420902035, 709.8656177068353, 654.9461460063704, 499.7568181465887, 615.7535825763771, 690.5428918717732, 723.0959413866422, 690.0436294947469, 703.4953304882382, 707.5697763852401, 693.649984671177, 690.3258791994759, 703.7981276948512, 713.5929326504727, 706.0451509305385, 542.2830768167203, 731.7317092836514, 722.4411990074912, 721.489653405649, 716.9322185648501, 496.03272430120523, 609.8620418760198, 732.7792780125943, 724.3148036837925, 711.1122164221833, 734.5724071435694, 725.2750170576511, 581.1830814494165, 658.5186946157573, 662.5927586356628, 660.9229351810393, 577.8635933243297, 475.9144896913644, 702.7574725775428, 699.3608654565443, 691.8659882157974, 689.2884362079499, 732.3898312117383, 734.7176051907788, 487.7824544066562, 735.0205298207829, 740.3562575711944, 750.8924439104363, 743.508791835143, 726.1704459284072, 710.0114923716732, 735.6845709810916, 732.7555808073548, 748.5262823547614, 748.551762017906, 739.8047974870776, 747.7699716077385, 650.2203853596887, 711.2113281554954, 722.0702226552555, 717.8292313705575, 741.676773798157, 686.8859919792484, 550.5092801157533, 764.3253997301015, 758.2050347110784, 749.0583900243641, 751.4847811220279, 743.1329162199409, 729.5632906191538, 741.8551289699163, 736.662805436831, 744.8749481944271, 748.0697760320054, 744.7878917299181, 595.9678343431148, 599.7359879241995, 725.415418964816, 737.9325590393696, 721.8312327240269, 718.6808534495167, 669.7311729503884, 504.3099403458752, 729.987750260229, 721.9152436189557, 750.667409430506, 627.944943370111, 749.3570384798063, 691.7181977070497, 742.7739540320994, 721.6770462219175, 686.4047884521636, 489.20642632451944, 740.3925254346924, 729.6928356300766, 753.9976211405192, 728.6770662001004, 703.9731655591223, 734.9242044693808, 707.4690192072723, 718.0675008756737, 700.4101007982325, 723.5713828118787, 503.92544240180246, 499.77886558138516, 508.3011965433663, 660.8785317646945, 700.596764187036, 699.3822276488074, 698.0110381890113, 718.1171194294212, 729.8325748835125, 523.2845225218881, 667.1325528502343, 662.8772663301199, 700.8325320707718, 683.8994881208715]
Elapsed: 0.19268072257141788~0.02727101844239878
Time per graph: 0.001493649012181534~0.0002114032437395254
Speed: 680.118466880315~75.7392941840284
Total Time: 0.1894
best val loss: 0.27263852457205456 test_score: 0.9612

Testing...
Test loss: 0.1213 score: 0.9612 time: 0.18s
test Score 0.9612
Epoch Time List: [0.6235006421338767, 0.6146915510762483, 0.6186582711525261, 0.7454341158736497, 0.8299643038772047, 0.6174163997638971, 0.6398934538010508, 0.6017873729579151, 0.7764369966462255, 0.913911780808121, 0.7002250859513879, 0.6021345423068851, 0.6475065262056887, 0.6526677410583943, 0.6180973055306822, 0.6798804588615894, 0.7121924087405205, 0.6164275191258639, 0.6939948978833854, 0.5986697829794139, 0.6039261249825358, 0.6133825050201267, 0.6421189398970455, 0.6861946142744273, 0.6099444387946278, 0.6310881411191076, 0.6085848601069301, 0.7238331940025091, 0.657312777126208, 0.6615060546901077, 0.6508346970658749, 0.6506477380171418, 0.6556074689142406, 0.7503613929729909, 0.6585775210987777, 0.6486470720265061, 0.6539894971065223, 0.6660429406911135, 0.8100088543724269, 0.9367432778235525, 0.6586342190857977, 0.6339745579753071, 0.6452003046870232, 0.6384977586567402, 0.6373017968144268, 0.6701166052371264, 0.6165477188769728, 0.6084070210345089, 0.6332503659650683, 0.8136518748942763, 0.6682389099150896, 0.6409326691646129, 0.6468586919363588, 0.5927104472648352, 0.5988247729837894, 0.6339900111779571, 0.7352630514651537, 0.6477510158438236, 0.6376841228920966, 0.6037337007001042, 0.6786731530446559, 0.6883535429369658, 0.6350665029603988, 0.6942183070350438, 0.8082051840610802, 0.6320047189947218, 0.6467968949582428, 0.632416786858812, 0.6276483531109989, 0.6528174127452075, 0.890630123205483, 0.6741201225668192, 0.608288744231686, 0.6135257829446346, 0.6276011050213128, 0.5997566760051996, 0.6236874910537153, 0.6795925649348646, 0.597937302896753, 0.6028474820777774, 0.6197071908973157, 0.6913079309742898, 0.6315930229611695, 0.6184490947052836, 0.8280366808176041, 0.6282157099340111, 0.6188028787728399, 0.6371266692876816, 0.6109115472063422, 0.6547151419799775, 0.6994714620523155, 0.6096276151947677, 0.6067678739782423, 0.6106725470162928, 0.6158222921658307, 0.687823094194755, 0.6206376221962273, 0.6123947657179087, 0.6777550680562854, 0.6074421489611268, 0.6865707826800644, 0.6822121050208807, 0.6078663237858564, 0.651912814937532, 0.692917357897386, 0.7102800386492163, 0.6164888578932732, 0.837526666931808, 0.8382368320599198, 0.6893407788593322, 0.6144375139847398, 0.6243891450576484, 0.6195686429273337, 0.6143956538289785, 0.640046315966174, 0.704701819922775, 0.6060971189290285, 0.6122425277717412, 0.6008876080159098, 0.6905582451727241, 0.6918681988026947, 0.6056635591667145, 0.6144573737401515, 0.6067770416848361, 0.82668292010203, 0.8578346101567149, 0.6903238382656127, 0.6119623437989503, 0.6126190279610455, 0.6158417020924389, 0.6127806541044265, 0.6756256460212171, 0.7479287618771195, 0.6608702489174902, 0.6510320373345166, 0.6873140218667686, 0.9080599481239915, 0.7184558950830251, 0.6226705862209201, 0.6606602428946644, 0.7086483940947801, 0.6128713109064847, 0.5989487681072205, 0.7817212203517556, 0.7049247098620981, 0.6027338101994246, 0.5895942221395671, 0.5853080211672932, 0.5887725830543786, 0.5945484640542418, 0.6747376339044422, 0.5942409690469503, 0.5928731430321932, 0.5966036522295326, 0.5911836409941316, 0.5901763618458062, 0.6196397508028895, 0.6721270368434489, 0.5969715768005699, 0.6033627269789577, 0.5838549740146846, 0.5998533959500492, 0.7268601742107421, 0.6938464292325079, 0.5831978279165924, 0.5870572789572179, 0.5853270518127829, 0.5924624502658844, 0.6165005043148994, 0.6813445969019085, 0.590493094176054, 0.5926824316848069, 0.5922169322147965, 0.5958083621226251, 0.6860768420156091, 0.8308730139397085, 0.5927367201074958, 0.5922509080264717, 0.5950175758916885, 0.6102460348047316, 0.619573141913861, 0.7531872126273811, 0.7115334139671177, 0.60221958020702, 0.5910479461308569, 0.6711806657258421, 0.5907346699386835, 0.6224696952849627, 0.5985063470434397, 0.6676011632662266, 0.6743319192901254, 0.7359344938304275, 0.5933113181963563, 0.6041368159931153, 0.7641653560567647, 0.6059968480840325, 0.6222424721345305, 0.6280124010518193, 0.6089611370116472, 0.6273034710902721, 0.6925347321666777, 0.6145890569314361, 0.6918857980053872, 0.9040451559703797, 0.8973854200448841, 0.823693023994565, 0.638555099023506, 0.6273912338074297, 0.6295894628856331, 0.6267561689019203, 0.6246160231530666, 0.7355568169150501, 0.6980045633390546, 0.6630340351257473, 0.6217350631486624, 0.6486837919801474]
Total Epoch List: [216]
Total Time List: [0.18939930596388876]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a289b3b80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.0135;  Loss pred: 3.0135; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 3.0001;  Loss pred: 3.0001; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 2.9797;  Loss pred: 2.9797; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 2.9557;  Loss pred: 2.9557; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 2.8839;  Loss pred: 2.8839; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 2.8639;  Loss pred: 2.8639; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.15s
Epoch 7/1000, LR 0.000165
Train loss: 2.7867;  Loss pred: 2.7867; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 2.6955;  Loss pred: 2.6955; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.15s
Epoch 9/1000, LR 0.000225
Train loss: 2.6308;  Loss pred: 2.6308; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 2.5568;  Loss pred: 2.5568; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 2.4354;  Loss pred: 2.4354; Loss self: 0.0000; time: 0.27s
Val loss: 0.6927 score: 0.6512 time: 0.18s
Test loss: 0.6921 score: 0.6047 time: 0.15s
Epoch 12/1000, LR 0.000285
Train loss: 2.3619;  Loss pred: 2.3619; Loss self: 0.0000; time: 0.26s
Val loss: 0.6926 score: 0.4961 time: 0.19s
Test loss: 0.6921 score: 0.5736 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 2.2818;  Loss pred: 2.2818; Loss self: 0.0000; time: 0.32s
Val loss: 0.6926 score: 0.4806 time: 0.23s
Test loss: 0.6920 score: 0.5271 time: 0.15s
Epoch 14/1000, LR 0.000285
Train loss: 2.1920;  Loss pred: 2.1920; Loss self: 0.0000; time: 0.28s
Val loss: 0.6925 score: 0.5039 time: 0.19s
Test loss: 0.6920 score: 0.5194 time: 0.15s
Epoch 15/1000, LR 0.000285
Train loss: 2.1307;  Loss pred: 2.1307; Loss self: 0.0000; time: 0.26s
Val loss: 0.6925 score: 0.5116 time: 0.19s
Test loss: 0.6919 score: 0.5349 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 2.0556;  Loss pred: 2.0556; Loss self: 0.0000; time: 0.28s
Val loss: 0.6924 score: 0.5116 time: 0.19s
Test loss: 0.6918 score: 0.5349 time: 0.15s
Epoch 17/1000, LR 0.000285
Train loss: 1.9797;  Loss pred: 1.9797; Loss self: 0.0000; time: 0.27s
Val loss: 0.6923 score: 0.5116 time: 0.18s
Test loss: 0.6917 score: 0.5194 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 1.9189;  Loss pred: 1.9189; Loss self: 0.0000; time: 0.27s
Val loss: 0.6922 score: 0.5039 time: 0.26s
Test loss: 0.6916 score: 0.5194 time: 0.23s
Epoch 19/1000, LR 0.000285
Train loss: 1.8679;  Loss pred: 1.8679; Loss self: 0.0000; time: 0.40s
Val loss: 0.6921 score: 0.4884 time: 0.21s
Test loss: 0.6914 score: 0.5271 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 1.8034;  Loss pred: 1.8034; Loss self: 0.0000; time: 0.27s
Val loss: 0.6919 score: 0.4884 time: 0.18s
Test loss: 0.6913 score: 0.5194 time: 0.15s
Epoch 21/1000, LR 0.000285
Train loss: 1.7336;  Loss pred: 1.7336; Loss self: 0.0000; time: 0.27s
Val loss: 0.6918 score: 0.4884 time: 0.18s
Test loss: 0.6912 score: 0.5194 time: 0.15s
Epoch 22/1000, LR 0.000285
Train loss: 1.6898;  Loss pred: 1.6898; Loss self: 0.0000; time: 0.27s
Val loss: 0.6916 score: 0.5116 time: 0.19s
Test loss: 0.6909 score: 0.5814 time: 0.15s
Epoch 23/1000, LR 0.000285
Train loss: 1.6488;  Loss pred: 1.6488; Loss self: 0.0000; time: 0.27s
Val loss: 0.6913 score: 0.5194 time: 0.19s
Test loss: 0.6906 score: 0.5814 time: 0.20s
Epoch 24/1000, LR 0.000285
Train loss: 1.6025;  Loss pred: 1.6025; Loss self: 0.0000; time: 0.28s
Val loss: 0.6911 score: 0.5194 time: 0.23s
Test loss: 0.6904 score: 0.5814 time: 0.22s
Epoch 25/1000, LR 0.000285
Train loss: 1.5711;  Loss pred: 1.5711; Loss self: 0.0000; time: 0.27s
Val loss: 0.6909 score: 0.5194 time: 0.18s
Test loss: 0.6902 score: 0.5814 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.5214;  Loss pred: 1.5214; Loss self: 0.0000; time: 0.26s
Val loss: 0.6907 score: 0.5581 time: 0.19s
Test loss: 0.6899 score: 0.6279 time: 0.15s
Epoch 27/1000, LR 0.000285
Train loss: 1.4887;  Loss pred: 1.4887; Loss self: 0.0000; time: 0.27s
Val loss: 0.6905 score: 0.5581 time: 0.18s
Test loss: 0.6897 score: 0.6434 time: 0.15s
Epoch 28/1000, LR 0.000285
Train loss: 1.4506;  Loss pred: 1.4506; Loss self: 0.0000; time: 0.26s
Val loss: 0.6902 score: 0.5814 time: 0.19s
Test loss: 0.6894 score: 0.6667 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.4088;  Loss pred: 1.4088; Loss self: 0.0000; time: 0.26s
Val loss: 0.6900 score: 0.6279 time: 0.18s
Test loss: 0.6891 score: 0.7442 time: 0.15s
Epoch 30/1000, LR 0.000285
Train loss: 1.3909;  Loss pred: 1.3909; Loss self: 0.0000; time: 0.26s
Val loss: 0.6897 score: 0.7209 time: 0.21s
Test loss: 0.6887 score: 0.8140 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 1.3642;  Loss pred: 1.3642; Loss self: 0.0000; time: 0.34s
Val loss: 0.6894 score: 0.7984 time: 0.19s
Test loss: 0.6884 score: 0.8682 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 1.3298;  Loss pred: 1.3298; Loss self: 0.0000; time: 0.26s
Val loss: 0.6891 score: 0.8760 time: 0.18s
Test loss: 0.6880 score: 0.8760 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 1.3060;  Loss pred: 1.3060; Loss self: 0.0000; time: 0.26s
Val loss: 0.6887 score: 0.8760 time: 0.21s
Test loss: 0.6876 score: 0.8605 time: 0.23s
Epoch 34/1000, LR 0.000285
Train loss: 1.2890;  Loss pred: 1.2890; Loss self: 0.0000; time: 0.40s
Val loss: 0.6884 score: 0.8450 time: 0.28s
Test loss: 0.6872 score: 0.8450 time: 0.25s
Epoch 35/1000, LR 0.000285
Train loss: 1.2658;  Loss pred: 1.2658; Loss self: 0.0000; time: 0.39s
Val loss: 0.6880 score: 0.8450 time: 0.27s
Test loss: 0.6867 score: 0.8372 time: 0.25s
Epoch 36/1000, LR 0.000285
Train loss: 1.2440;  Loss pred: 1.2440; Loss self: 0.0000; time: 0.39s
Val loss: 0.6876 score: 0.8527 time: 0.28s
Test loss: 0.6862 score: 0.8062 time: 0.24s
Epoch 37/1000, LR 0.000285
Train loss: 1.2330;  Loss pred: 1.2330; Loss self: 0.0000; time: 0.29s
Val loss: 0.6871 score: 0.8527 time: 0.18s
Test loss: 0.6857 score: 0.8062 time: 0.15s
Epoch 38/1000, LR 0.000284
Train loss: 1.2127;  Loss pred: 1.2127; Loss self: 0.0000; time: 0.27s
Val loss: 0.6866 score: 0.8527 time: 0.18s
Test loss: 0.6852 score: 0.8062 time: 0.15s
Epoch 39/1000, LR 0.000284
Train loss: 1.1969;  Loss pred: 1.1969; Loss self: 0.0000; time: 0.28s
Val loss: 0.6861 score: 0.8527 time: 0.24s
Test loss: 0.6846 score: 0.7984 time: 0.24s
Epoch 40/1000, LR 0.000284
Train loss: 1.1799;  Loss pred: 1.1799; Loss self: 0.0000; time: 0.29s
Val loss: 0.6855 score: 0.8527 time: 0.19s
Test loss: 0.6840 score: 0.7907 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 1.1717;  Loss pred: 1.1717; Loss self: 0.0000; time: 0.27s
Val loss: 0.6850 score: 0.8450 time: 0.20s
Test loss: 0.6833 score: 0.7829 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 1.1545;  Loss pred: 1.1545; Loss self: 0.0000; time: 0.27s
Val loss: 0.6843 score: 0.8450 time: 0.19s
Test loss: 0.6826 score: 0.7829 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 1.1457;  Loss pred: 1.1457; Loss self: 0.0000; time: 0.26s
Val loss: 0.6836 score: 0.8450 time: 0.19s
Test loss: 0.6819 score: 0.7829 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 1.1328;  Loss pred: 1.1328; Loss self: 0.0000; time: 0.25s
Val loss: 0.6828 score: 0.8527 time: 0.18s
Test loss: 0.6811 score: 0.7829 time: 0.15s
Epoch 45/1000, LR 0.000284
Train loss: 1.1207;  Loss pred: 1.1207; Loss self: 0.0000; time: 0.25s
Val loss: 0.6820 score: 0.8527 time: 0.21s
Test loss: 0.6802 score: 0.7829 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 1.1117;  Loss pred: 1.1117; Loss self: 0.0000; time: 0.35s
Val loss: 0.6812 score: 0.8527 time: 0.19s
Test loss: 0.6793 score: 0.7984 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 1.1052;  Loss pred: 1.1052; Loss self: 0.0000; time: 0.27s
Val loss: 0.6803 score: 0.8527 time: 0.19s
Test loss: 0.6783 score: 0.7907 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 1.0904;  Loss pred: 1.0904; Loss self: 0.0000; time: 0.27s
Val loss: 0.6794 score: 0.8527 time: 0.19s
Test loss: 0.6773 score: 0.7907 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 1.0870;  Loss pred: 1.0870; Loss self: 0.0000; time: 0.27s
Val loss: 0.6784 score: 0.8527 time: 0.19s
Test loss: 0.6762 score: 0.7984 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.0769;  Loss pred: 1.0769; Loss self: 0.0000; time: 0.26s
Val loss: 0.6773 score: 0.8527 time: 0.18s
Test loss: 0.6751 score: 0.7984 time: 0.15s
Epoch 51/1000, LR 0.000284
Train loss: 1.0697;  Loss pred: 1.0697; Loss self: 0.0000; time: 0.26s
Val loss: 0.6762 score: 0.8605 time: 0.21s
Test loss: 0.6739 score: 0.8062 time: 0.20s
Epoch 52/1000, LR 0.000284
Train loss: 1.0627;  Loss pred: 1.0627; Loss self: 0.0000; time: 0.27s
Val loss: 0.6750 score: 0.8605 time: 0.24s
Test loss: 0.6727 score: 0.8140 time: 0.15s
Epoch 53/1000, LR 0.000284
Train loss: 1.0565;  Loss pred: 1.0565; Loss self: 0.0000; time: 0.26s
Val loss: 0.6737 score: 0.8605 time: 0.18s
Test loss: 0.6713 score: 0.8062 time: 0.15s
Epoch 54/1000, LR 0.000284
Train loss: 1.0512;  Loss pred: 1.0512; Loss self: 0.0000; time: 0.26s
Val loss: 0.6724 score: 0.8605 time: 0.18s
Test loss: 0.6699 score: 0.8140 time: 0.16s
Epoch 55/1000, LR 0.000284
Train loss: 1.0457;  Loss pred: 1.0457; Loss self: 0.0000; time: 0.28s
Val loss: 0.6710 score: 0.8605 time: 0.24s
Test loss: 0.6684 score: 0.8140 time: 0.24s
Epoch 56/1000, LR 0.000284
Train loss: 1.0395;  Loss pred: 1.0395; Loss self: 0.0000; time: 0.40s
Val loss: 0.6695 score: 0.8605 time: 0.25s
Test loss: 0.6669 score: 0.8217 time: 0.22s
Epoch 57/1000, LR 0.000283
Train loss: 1.0315;  Loss pred: 1.0315; Loss self: 0.0000; time: 0.35s
Val loss: 0.6679 score: 0.8605 time: 0.18s
Test loss: 0.6652 score: 0.8295 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 1.0283;  Loss pred: 1.0283; Loss self: 0.0000; time: 0.27s
Val loss: 0.6662 score: 0.8605 time: 0.29s
Test loss: 0.6635 score: 0.8372 time: 0.16s
Epoch 59/1000, LR 0.000283
Train loss: 1.0217;  Loss pred: 1.0217; Loss self: 0.0000; time: 0.27s
Val loss: 0.6644 score: 0.8605 time: 0.19s
Test loss: 0.6616 score: 0.8372 time: 0.17s
Epoch 60/1000, LR 0.000283
Train loss: 1.0193;  Loss pred: 1.0193; Loss self: 0.0000; time: 0.28s
Val loss: 0.6625 score: 0.8605 time: 0.20s
Test loss: 0.6596 score: 0.8527 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 1.0132;  Loss pred: 1.0132; Loss self: 0.0000; time: 0.28s
Val loss: 0.6605 score: 0.8605 time: 0.26s
Test loss: 0.6576 score: 0.8527 time: 0.16s
Epoch 62/1000, LR 0.000283
Train loss: 1.0082;  Loss pred: 1.0082; Loss self: 0.0000; time: 0.26s
Val loss: 0.6584 score: 0.8605 time: 0.18s
Test loss: 0.6554 score: 0.8527 time: 0.16s
Epoch 63/1000, LR 0.000283
Train loss: 1.0032;  Loss pred: 1.0032; Loss self: 0.0000; time: 0.27s
Val loss: 0.6562 score: 0.8605 time: 0.19s
Test loss: 0.6532 score: 0.8605 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9978;  Loss pred: 0.9978; Loss self: 0.0000; time: 0.27s
Val loss: 0.6539 score: 0.8682 time: 0.19s
Test loss: 0.6508 score: 0.8605 time: 0.18s
Epoch 65/1000, LR 0.000283
Train loss: 0.9949;  Loss pred: 0.9949; Loss self: 0.0000; time: 0.27s
Val loss: 0.6515 score: 0.8760 time: 0.20s
Test loss: 0.6483 score: 0.8682 time: 0.16s
Epoch 66/1000, LR 0.000283
Train loss: 0.9901;  Loss pred: 0.9901; Loss self: 0.0000; time: 0.28s
Val loss: 0.6490 score: 0.8682 time: 0.19s
Test loss: 0.6457 score: 0.8605 time: 0.18s
Epoch 67/1000, LR 0.000283
Train loss: 0.9873;  Loss pred: 0.9873; Loss self: 0.0000; time: 0.27s
Val loss: 0.6463 score: 0.8682 time: 0.25s
Test loss: 0.6430 score: 0.8605 time: 0.16s
Epoch 68/1000, LR 0.000283
Train loss: 0.9841;  Loss pred: 0.9841; Loss self: 0.0000; time: 0.27s
Val loss: 0.6435 score: 0.8682 time: 0.20s
Test loss: 0.6401 score: 0.8605 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 0.9771;  Loss pred: 0.9771; Loss self: 0.0000; time: 0.27s
Val loss: 0.6407 score: 0.8682 time: 0.19s
Test loss: 0.6372 score: 0.8605 time: 0.16s
Epoch 70/1000, LR 0.000283
Train loss: 0.9734;  Loss pred: 0.9734; Loss self: 0.0000; time: 0.27s
Val loss: 0.6376 score: 0.8682 time: 0.19s
Test loss: 0.6341 score: 0.8605 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9693;  Loss pred: 0.9693; Loss self: 0.0000; time: 0.27s
Val loss: 0.6344 score: 0.8682 time: 0.19s
Test loss: 0.6308 score: 0.8605 time: 0.16s
Epoch 72/1000, LR 0.000282
Train loss: 0.9635;  Loss pred: 0.9635; Loss self: 0.0000; time: 0.26s
Val loss: 0.6310 score: 0.8760 time: 0.18s
Test loss: 0.6274 score: 0.8682 time: 0.16s
Epoch 73/1000, LR 0.000282
Train loss: 0.9593;  Loss pred: 0.9593; Loss self: 0.0000; time: 0.34s
Val loss: 0.6275 score: 0.8760 time: 0.24s
Test loss: 0.6238 score: 0.8682 time: 0.22s
Epoch 74/1000, LR 0.000282
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.27s
Val loss: 0.6238 score: 0.8760 time: 0.18s
Test loss: 0.6201 score: 0.8682 time: 0.15s
Epoch 75/1000, LR 0.000282
Train loss: 0.9501;  Loss pred: 0.9501; Loss self: 0.0000; time: 0.27s
Val loss: 0.6200 score: 0.8760 time: 0.18s
Test loss: 0.6163 score: 0.8682 time: 0.15s
Epoch 76/1000, LR 0.000282
Train loss: 0.9449;  Loss pred: 0.9449; Loss self: 0.0000; time: 0.27s
Val loss: 0.6160 score: 0.8760 time: 0.19s
Test loss: 0.6123 score: 0.8682 time: 0.15s
Epoch 77/1000, LR 0.000282
Train loss: 0.9412;  Loss pred: 0.9412; Loss self: 0.0000; time: 0.27s
Val loss: 0.6119 score: 0.8760 time: 0.20s
Test loss: 0.6081 score: 0.8682 time: 0.16s
Epoch 78/1000, LR 0.000282
Train loss: 0.9379;  Loss pred: 0.9379; Loss self: 0.0000; time: 0.28s
Val loss: 0.6076 score: 0.8760 time: 0.20s
Test loss: 0.6038 score: 0.8682 time: 0.16s
Epoch 79/1000, LR 0.000282
Train loss: 0.9324;  Loss pred: 0.9324; Loss self: 0.0000; time: 0.26s
Val loss: 0.6032 score: 0.8760 time: 0.24s
Test loss: 0.5993 score: 0.8605 time: 0.16s
Epoch 80/1000, LR 0.000282
Train loss: 0.9266;  Loss pred: 0.9266; Loss self: 0.0000; time: 0.31s
Val loss: 0.5987 score: 0.8760 time: 0.21s
Test loss: 0.5949 score: 0.8682 time: 0.15s
Epoch 81/1000, LR 0.000281
Train loss: 0.9234;  Loss pred: 0.9234; Loss self: 0.0000; time: 0.26s
Val loss: 0.5940 score: 0.8760 time: 0.18s
Test loss: 0.5902 score: 0.8682 time: 0.15s
Epoch 82/1000, LR 0.000281
Train loss: 0.9190;  Loss pred: 0.9190; Loss self: 0.0000; time: 0.26s
Val loss: 0.5891 score: 0.8760 time: 0.18s
Test loss: 0.5853 score: 0.8682 time: 0.15s
Epoch 83/1000, LR 0.000281
Train loss: 0.9115;  Loss pred: 0.9115; Loss self: 0.0000; time: 0.26s
Val loss: 0.5842 score: 0.8760 time: 0.18s
Test loss: 0.5804 score: 0.8682 time: 0.15s
Epoch 84/1000, LR 0.000281
Train loss: 0.9055;  Loss pred: 0.9055; Loss self: 0.0000; time: 0.27s
Val loss: 0.5792 score: 0.8760 time: 0.18s
Test loss: 0.5755 score: 0.8682 time: 0.15s
Epoch 85/1000, LR 0.000281
Train loss: 0.9023;  Loss pred: 0.9023; Loss self: 0.0000; time: 0.26s
Val loss: 0.5738 score: 0.8760 time: 0.18s
Test loss: 0.5701 score: 0.8682 time: 0.15s
Epoch 86/1000, LR 0.000281
Train loss: 0.8969;  Loss pred: 0.8969; Loss self: 0.0000; time: 0.26s
Val loss: 0.5683 score: 0.8760 time: 0.18s
Test loss: 0.5646 score: 0.8760 time: 0.15s
Epoch 87/1000, LR 0.000281
Train loss: 0.8917;  Loss pred: 0.8917; Loss self: 0.0000; time: 0.26s
Val loss: 0.5626 score: 0.8760 time: 0.21s
Test loss: 0.5591 score: 0.8682 time: 0.16s
Epoch 88/1000, LR 0.000281
Train loss: 0.8862;  Loss pred: 0.8862; Loss self: 0.0000; time: 0.33s
Val loss: 0.5572 score: 0.8760 time: 0.19s
Test loss: 0.5538 score: 0.8760 time: 0.15s
Epoch 89/1000, LR 0.000281
Train loss: 0.8815;  Loss pred: 0.8815; Loss self: 0.0000; time: 0.27s
Val loss: 0.5514 score: 0.8760 time: 0.18s
Test loss: 0.5482 score: 0.8760 time: 0.15s
Epoch 90/1000, LR 0.000281
Train loss: 0.8756;  Loss pred: 0.8756; Loss self: 0.0000; time: 0.31s
Val loss: 0.5456 score: 0.8760 time: 0.26s
Test loss: 0.5425 score: 0.8682 time: 0.23s
Epoch 91/1000, LR 0.000280
Train loss: 0.8692;  Loss pred: 0.8692; Loss self: 0.0000; time: 0.38s
Val loss: 0.5397 score: 0.8760 time: 0.26s
Test loss: 0.5367 score: 0.8682 time: 0.23s
Epoch 92/1000, LR 0.000280
Train loss: 0.8649;  Loss pred: 0.8649; Loss self: 0.0000; time: 0.39s
Val loss: 0.5338 score: 0.8760 time: 0.26s
Test loss: 0.5310 score: 0.8760 time: 0.23s
Epoch 93/1000, LR 0.000280
Train loss: 0.8595;  Loss pred: 0.8595; Loss self: 0.0000; time: 0.40s
Val loss: 0.5279 score: 0.8760 time: 0.19s
Test loss: 0.5253 score: 0.8760 time: 0.15s
Epoch 94/1000, LR 0.000280
Train loss: 0.8520;  Loss pred: 0.8520; Loss self: 0.0000; time: 0.27s
Val loss: 0.5217 score: 0.8760 time: 0.18s
Test loss: 0.5193 score: 0.8682 time: 0.15s
Epoch 95/1000, LR 0.000280
Train loss: 0.8476;  Loss pred: 0.8476; Loss self: 0.0000; time: 0.27s
Val loss: 0.5158 score: 0.8760 time: 0.18s
Test loss: 0.5137 score: 0.8760 time: 0.15s
Epoch 96/1000, LR 0.000280
Train loss: 0.8432;  Loss pred: 0.8432; Loss self: 0.0000; time: 0.26s
Val loss: 0.5100 score: 0.8760 time: 0.18s
Test loss: 0.5083 score: 0.8760 time: 0.16s
Epoch 97/1000, LR 0.000280
Train loss: 0.8365;  Loss pred: 0.8365; Loss self: 0.0000; time: 0.30s
Val loss: 0.5038 score: 0.8760 time: 0.24s
Test loss: 0.5023 score: 0.8760 time: 0.23s
Epoch 98/1000, LR 0.000280
Train loss: 0.8326;  Loss pred: 0.8326; Loss self: 0.0000; time: 0.38s
Val loss: 0.4976 score: 0.8760 time: 0.26s
Test loss: 0.4965 score: 0.8760 time: 0.23s
Epoch 99/1000, LR 0.000279
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.39s
Val loss: 0.4913 score: 0.8760 time: 0.27s
Test loss: 0.4905 score: 0.8760 time: 0.21s
Epoch 100/1000, LR 0.000279
Train loss: 0.8209;  Loss pred: 0.8209; Loss self: 0.0000; time: 0.35s
Val loss: 0.4851 score: 0.8760 time: 0.18s
Test loss: 0.4847 score: 0.8760 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.8164;  Loss pred: 0.8164; Loss self: 0.0000; time: 0.26s
Val loss: 0.4788 score: 0.8837 time: 0.18s
Test loss: 0.4787 score: 0.8682 time: 0.16s
Epoch 102/1000, LR 0.000279
Train loss: 0.8088;  Loss pred: 0.8088; Loss self: 0.0000; time: 0.26s
Val loss: 0.4724 score: 0.8915 time: 0.18s
Test loss: 0.4727 score: 0.8682 time: 0.16s
Epoch 103/1000, LR 0.000279
Train loss: 0.8026;  Loss pred: 0.8026; Loss self: 0.0000; time: 0.26s
Val loss: 0.4662 score: 0.8915 time: 0.18s
Test loss: 0.4669 score: 0.8682 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.7975;  Loss pred: 0.7975; Loss self: 0.0000; time: 0.30s
Val loss: 0.4600 score: 0.8915 time: 0.26s
Test loss: 0.4611 score: 0.8682 time: 0.23s
Epoch 105/1000, LR 0.000279
Train loss: 0.7922;  Loss pred: 0.7922; Loss self: 0.0000; time: 0.40s
Val loss: 0.4537 score: 0.8915 time: 0.26s
Test loss: 0.4553 score: 0.8682 time: 0.24s
Epoch 106/1000, LR 0.000279
Train loss: 0.7868;  Loss pred: 0.7868; Loss self: 0.0000; time: 0.39s
Val loss: 0.4477 score: 0.8915 time: 0.26s
Test loss: 0.4497 score: 0.8682 time: 0.24s
Epoch 107/1000, LR 0.000278
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.27s
Val loss: 0.4418 score: 0.8915 time: 0.18s
Test loss: 0.4443 score: 0.8682 time: 0.16s
Epoch 108/1000, LR 0.000278
Train loss: 0.7786;  Loss pred: 0.7786; Loss self: 0.0000; time: 0.27s
Val loss: 0.4359 score: 0.8915 time: 0.18s
Test loss: 0.4389 score: 0.8682 time: 0.15s
Epoch 109/1000, LR 0.000278
Train loss: 0.7712;  Loss pred: 0.7712; Loss self: 0.0000; time: 0.27s
Val loss: 0.4299 score: 0.8915 time: 0.19s
Test loss: 0.4333 score: 0.8682 time: 0.21s
Epoch 110/1000, LR 0.000278
Train loss: 0.7652;  Loss pred: 0.7652; Loss self: 0.0000; time: 0.34s
Val loss: 0.4240 score: 0.8915 time: 0.19s
Test loss: 0.4280 score: 0.8682 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.7623;  Loss pred: 0.7623; Loss self: 0.0000; time: 0.27s
Val loss: 0.4181 score: 0.8837 time: 0.18s
Test loss: 0.4225 score: 0.8605 time: 0.15s
Epoch 112/1000, LR 0.000278
Train loss: 0.7570;  Loss pred: 0.7570; Loss self: 0.0000; time: 0.27s
Val loss: 0.4125 score: 0.8915 time: 0.18s
Test loss: 0.4177 score: 0.8682 time: 0.15s
Epoch 113/1000, LR 0.000278
Train loss: 0.7512;  Loss pred: 0.7512; Loss self: 0.0000; time: 0.27s
Val loss: 0.4069 score: 0.8837 time: 0.19s
Test loss: 0.4127 score: 0.8605 time: 0.23s
Epoch 114/1000, LR 0.000277
Train loss: 0.7481;  Loss pred: 0.7481; Loss self: 0.0000; time: 0.37s
Val loss: 0.4015 score: 0.8837 time: 0.19s
Test loss: 0.4079 score: 0.8605 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.7416;  Loss pred: 0.7416; Loss self: 0.0000; time: 0.26s
Val loss: 0.3966 score: 0.8915 time: 0.19s
Test loss: 0.4038 score: 0.8682 time: 0.16s
Epoch 116/1000, LR 0.000277
Train loss: 0.7360;  Loss pred: 0.7360; Loss self: 0.0000; time: 0.27s
Val loss: 0.3914 score: 0.8837 time: 0.18s
Test loss: 0.3993 score: 0.8682 time: 0.15s
Epoch 117/1000, LR 0.000277
Train loss: 0.7339;  Loss pred: 0.7339; Loss self: 0.0000; time: 0.27s
Val loss: 0.3864 score: 0.8915 time: 0.18s
Test loss: 0.3951 score: 0.8682 time: 0.16s
Epoch 118/1000, LR 0.000277
Train loss: 0.7302;  Loss pred: 0.7302; Loss self: 0.0000; time: 0.27s
Val loss: 0.3811 score: 0.8837 time: 0.18s
Test loss: 0.3904 score: 0.8760 time: 0.15s
Epoch 119/1000, LR 0.000277
Train loss: 0.7250;  Loss pred: 0.7250; Loss self: 0.0000; time: 0.31s
Val loss: 0.3754 score: 0.8915 time: 0.18s
Test loss: 0.3853 score: 0.8682 time: 0.22s
Epoch 120/1000, LR 0.000277
Train loss: 0.7179;  Loss pred: 0.7179; Loss self: 0.0000; time: 0.27s
Val loss: 0.3699 score: 0.8915 time: 0.18s
Test loss: 0.3804 score: 0.8682 time: 0.16s
Epoch 121/1000, LR 0.000276
Train loss: 0.7167;  Loss pred: 0.7167; Loss self: 0.0000; time: 0.28s
Val loss: 0.3648 score: 0.8915 time: 0.18s
Test loss: 0.3759 score: 0.8682 time: 0.15s
Epoch 122/1000, LR 0.000276
Train loss: 0.7106;  Loss pred: 0.7106; Loss self: 0.0000; time: 0.27s
Val loss: 0.3602 score: 0.8915 time: 0.18s
Test loss: 0.3721 score: 0.8682 time: 0.16s
Epoch 123/1000, LR 0.000276
Train loss: 0.7073;  Loss pred: 0.7073; Loss self: 0.0000; time: 0.28s
Val loss: 0.3556 score: 0.8915 time: 0.18s
Test loss: 0.3684 score: 0.8682 time: 0.15s
Epoch 124/1000, LR 0.000276
Train loss: 0.7038;  Loss pred: 0.7038; Loss self: 0.0000; time: 0.30s
Val loss: 0.3508 score: 0.8915 time: 0.24s
Test loss: 0.3644 score: 0.8682 time: 0.24s
Epoch 125/1000, LR 0.000276
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.42s
Val loss: 0.3466 score: 0.8915 time: 0.23s
Test loss: 0.3612 score: 0.8682 time: 0.17s
Epoch 126/1000, LR 0.000276
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.29s
Val loss: 0.3424 score: 0.8915 time: 0.19s
Test loss: 0.3579 score: 0.8682 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.28s
Val loss: 0.3389 score: 0.8915 time: 0.19s
Test loss: 0.3555 score: 0.8682 time: 0.16s
Epoch 128/1000, LR 0.000275
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.28s
Val loss: 0.3346 score: 0.8915 time: 0.20s
Test loss: 0.3520 score: 0.8682 time: 0.16s
Epoch 129/1000, LR 0.000275
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.28s
Val loss: 0.3306 score: 0.8915 time: 0.19s
Test loss: 0.3490 score: 0.8682 time: 0.16s
Epoch 130/1000, LR 0.000275
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.28s
Val loss: 0.3263 score: 0.8915 time: 0.20s
Test loss: 0.3455 score: 0.8682 time: 0.18s
Epoch 131/1000, LR 0.000275
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.28s
Val loss: 0.3217 score: 0.8992 time: 0.24s
Test loss: 0.3417 score: 0.8682 time: 0.16s
Epoch 132/1000, LR 0.000275
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.27s
Val loss: 0.3176 score: 0.8992 time: 0.19s
Test loss: 0.3384 score: 0.8682 time: 0.15s
Epoch 133/1000, LR 0.000274
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.28s
Val loss: 0.3139 score: 0.8992 time: 0.19s
Test loss: 0.3355 score: 0.8682 time: 0.15s
Epoch 134/1000, LR 0.000274
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.27s
Val loss: 0.3097 score: 0.8992 time: 0.19s
Test loss: 0.3320 score: 0.8682 time: 0.15s
Epoch 135/1000, LR 0.000274
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.28s
Val loss: 0.3060 score: 0.8992 time: 0.19s
Test loss: 0.3292 score: 0.8682 time: 0.15s
Epoch 136/1000, LR 0.000274
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.27s
Val loss: 0.3034 score: 0.8992 time: 0.19s
Test loss: 0.3278 score: 0.8682 time: 0.16s
Epoch 137/1000, LR 0.000274
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.27s
Val loss: 0.3006 score: 0.8992 time: 0.19s
Test loss: 0.3260 score: 0.8682 time: 0.16s
Epoch 138/1000, LR 0.000274
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 0.32s
Val loss: 0.2966 score: 0.8992 time: 0.19s
Test loss: 0.3228 score: 0.8682 time: 0.16s
Epoch 139/1000, LR 0.000273
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.35s
Val loss: 0.2935 score: 0.8992 time: 0.18s
Test loss: 0.3206 score: 0.8682 time: 0.16s
Epoch 140/1000, LR 0.000273
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.27s
Val loss: 0.2906 score: 0.8992 time: 0.18s
Test loss: 0.3186 score: 0.8682 time: 0.16s
Epoch 141/1000, LR 0.000273
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.27s
Val loss: 0.2870 score: 0.8992 time: 0.18s
Test loss: 0.3157 score: 0.8682 time: 0.16s
Epoch 142/1000, LR 0.000273
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.27s
Val loss: 0.2834 score: 0.8992 time: 0.20s
Test loss: 0.3129 score: 0.8682 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.26s
Val loss: 0.2803 score: 0.8992 time: 0.18s
Test loss: 0.3106 score: 0.8682 time: 0.16s
Epoch 144/1000, LR 0.000272
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.32s
Val loss: 0.2778 score: 0.8992 time: 0.28s
Test loss: 0.3090 score: 0.8682 time: 0.24s
Epoch 145/1000, LR 0.000272
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.38s
Val loss: 0.2757 score: 0.8992 time: 0.21s
Test loss: 0.3080 score: 0.8682 time: 0.23s
Epoch 146/1000, LR 0.000272
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.25s
Val loss: 0.2729 score: 0.8992 time: 0.18s
Test loss: 0.3061 score: 0.8682 time: 0.15s
Epoch 147/1000, LR 0.000272
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 0.26s
Val loss: 0.2702 score: 0.8992 time: 0.18s
Test loss: 0.3044 score: 0.8682 time: 0.19s
Epoch 148/1000, LR 0.000272
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.27s
Val loss: 0.2684 score: 0.8992 time: 0.20s
Test loss: 0.3036 score: 0.8682 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.28s
Val loss: 0.2664 score: 0.8992 time: 0.28s
Test loss: 0.3027 score: 0.8682 time: 0.25s
Epoch 150/1000, LR 0.000271
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.40s
Val loss: 0.2642 score: 0.8992 time: 0.27s
Test loss: 0.3014 score: 0.8682 time: 0.23s
Epoch 151/1000, LR 0.000271
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.28s
Val loss: 0.2612 score: 0.8992 time: 0.20s
Test loss: 0.2992 score: 0.8682 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.38s
Val loss: 0.2578 score: 0.9070 time: 0.18s
Test loss: 0.2965 score: 0.8760 time: 0.15s
Epoch 153/1000, LR 0.000271
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.26s
Val loss: 0.2557 score: 0.9070 time: 0.18s
Test loss: 0.2954 score: 0.8760 time: 0.15s
Epoch 154/1000, LR 0.000271
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.26s
Val loss: 0.2527 score: 0.9070 time: 0.18s
Test loss: 0.2931 score: 0.8760 time: 0.15s
Epoch 155/1000, LR 0.000270
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 0.26s
Val loss: 0.2497 score: 0.9070 time: 0.18s
Test loss: 0.2908 score: 0.8760 time: 0.15s
Epoch 156/1000, LR 0.000270
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.26s
Val loss: 0.2469 score: 0.9070 time: 0.21s
Test loss: 0.2890 score: 0.8760 time: 0.16s
Epoch 157/1000, LR 0.000270
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 0.35s
Val loss: 0.2450 score: 0.9070 time: 0.19s
Test loss: 0.2880 score: 0.8760 time: 0.15s
Epoch 158/1000, LR 0.000270
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.27s
Val loss: 0.2425 score: 0.9070 time: 0.18s
Test loss: 0.2864 score: 0.8760 time: 0.15s
Epoch 159/1000, LR 0.000270
Train loss: 0.6079;  Loss pred: 0.6079; Loss self: 0.0000; time: 0.27s
Val loss: 0.2419 score: 0.9070 time: 0.19s
Test loss: 0.2869 score: 0.8760 time: 0.16s
Epoch 160/1000, LR 0.000269
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.27s
Val loss: 0.2401 score: 0.9070 time: 0.21s
Test loss: 0.2861 score: 0.8760 time: 0.21s
Epoch 161/1000, LR 0.000269
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.39s
Val loss: 0.2377 score: 0.9070 time: 0.26s
Test loss: 0.2846 score: 0.8760 time: 0.22s
Epoch 162/1000, LR 0.000269
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.26s
Val loss: 0.2361 score: 0.9147 time: 0.18s
Test loss: 0.2838 score: 0.8760 time: 0.15s
Epoch 163/1000, LR 0.000269
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.26s
Val loss: 0.2353 score: 0.9147 time: 0.19s
Test loss: 0.2841 score: 0.8760 time: 0.16s
Epoch 164/1000, LR 0.000269
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.27s
Val loss: 0.2342 score: 0.9147 time: 0.19s
Test loss: 0.2839 score: 0.8837 time: 0.16s
Epoch 165/1000, LR 0.000268
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.27s
Val loss: 0.2332 score: 0.9147 time: 0.19s
Test loss: 0.2838 score: 0.8837 time: 0.16s
Epoch 166/1000, LR 0.000268
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.27s
Val loss: 0.2315 score: 0.9147 time: 0.22s
Test loss: 0.2829 score: 0.8837 time: 0.17s
Epoch 167/1000, LR 0.000268
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.34s
Val loss: 0.2290 score: 0.9147 time: 0.19s
Test loss: 0.2813 score: 0.8915 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.28s
Val loss: 0.2277 score: 0.9147 time: 0.19s
Test loss: 0.2808 score: 0.8915 time: 0.16s
Epoch 169/1000, LR 0.000267
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.28s
Val loss: 0.2254 score: 0.9147 time: 0.19s
Test loss: 0.2793 score: 0.8915 time: 0.16s
Epoch 170/1000, LR 0.000267
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.28s
Val loss: 0.2227 score: 0.9147 time: 0.19s
Test loss: 0.2773 score: 0.8837 time: 0.16s
Epoch 171/1000, LR 0.000267
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.27s
Val loss: 0.2194 score: 0.9147 time: 0.19s
Test loss: 0.2747 score: 0.8837 time: 0.16s
Epoch 172/1000, LR 0.000267
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.28s
Val loss: 0.2198 score: 0.9147 time: 0.22s
Test loss: 0.2760 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000267
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.28s
Val loss: 0.2188 score: 0.9147 time: 0.25s
Test loss: 0.2759 score: 0.8915 time: 0.18s
Epoch 174/1000, LR 0.000266
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.28s
Val loss: 0.2181 score: 0.9147 time: 0.19s
Test loss: 0.2760 score: 0.8915 time: 0.16s
Epoch 175/1000, LR 0.000266
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.27s
Val loss: 0.2188 score: 0.9147 time: 0.18s
Test loss: 0.2775 score: 0.8915 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 0.26s
Val loss: 0.2198 score: 0.9147 time: 0.18s
Test loss: 0.2794 score: 0.8915 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.26s
Val loss: 0.2193 score: 0.9147 time: 0.20s
Test loss: 0.2796 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 178/1000, LR 0.000265
Train loss: 0.5817;  Loss pred: 0.5817; Loss self: 0.0000; time: 0.28s
Val loss: 0.2180 score: 0.9147 time: 0.19s
Test loss: 0.2790 score: 0.8915 time: 0.24s
Epoch 179/1000, LR 0.000265
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.27s
Val loss: 0.2164 score: 0.9147 time: 0.18s
Test loss: 0.2780 score: 0.8915 time: 0.15s
Epoch 180/1000, LR 0.000265
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.27s
Val loss: 0.2147 score: 0.9147 time: 0.18s
Test loss: 0.2769 score: 0.8992 time: 0.15s
Epoch 181/1000, LR 0.000265
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 0.27s
Val loss: 0.2131 score: 0.9147 time: 0.18s
Test loss: 0.2757 score: 0.8992 time: 0.15s
Epoch 182/1000, LR 0.000265
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.27s
Val loss: 0.2117 score: 0.9147 time: 0.25s
Test loss: 0.2750 score: 0.8992 time: 0.23s
Epoch 183/1000, LR 0.000264
Train loss: 0.5771;  Loss pred: 0.5771; Loss self: 0.0000; time: 0.30s
Val loss: 0.2096 score: 0.9302 time: 0.18s
Test loss: 0.2733 score: 0.9070 time: 0.16s
Epoch 184/1000, LR 0.000264
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.26s
Val loss: 0.2085 score: 0.9302 time: 0.18s
Test loss: 0.2728 score: 0.9070 time: 0.16s
Epoch 185/1000, LR 0.000264
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.26s
Val loss: 0.2080 score: 0.9302 time: 0.18s
Test loss: 0.2730 score: 0.9070 time: 0.15s
Epoch 186/1000, LR 0.000264
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.27s
Val loss: 0.2086 score: 0.9302 time: 0.18s
Test loss: 0.2742 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.27s
Val loss: 0.2092 score: 0.9302 time: 0.18s
Test loss: 0.2755 score: 0.9070 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 0.27s
Val loss: 0.2074 score: 0.9302 time: 0.27s
Test loss: 0.2741 score: 0.9070 time: 0.15s
Epoch 189/1000, LR 0.000263
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.27s
Val loss: 0.2075 score: 0.9302 time: 0.18s
Test loss: 0.2748 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.27s
Val loss: 0.2072 score: 0.9302 time: 0.18s
Test loss: 0.2750 score: 0.9070 time: 0.15s
Epoch 191/1000, LR 0.000262
Train loss: 0.5679;  Loss pred: 0.5679; Loss self: 0.0000; time: 0.30s
Val loss: 0.2072 score: 0.9302 time: 0.26s
Test loss: 0.2756 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 192/1000, LR 0.000262
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.39s
Val loss: 0.2066 score: 0.9302 time: 0.26s
Test loss: 0.2756 score: 0.9070 time: 0.23s
Epoch 193/1000, LR 0.000262
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.39s
Val loss: 0.2066 score: 0.9225 time: 0.23s
Test loss: 0.2761 score: 0.9070 time: 0.21s
Epoch 194/1000, LR 0.000262
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.28s
Val loss: 0.2061 score: 0.9225 time: 0.19s
Test loss: 0.2762 score: 0.9070 time: 0.15s
Epoch 195/1000, LR 0.000261
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.26s
Val loss: 0.2062 score: 0.9225 time: 0.18s
Test loss: 0.2769 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.27s
Val loss: 0.2051 score: 0.9225 time: 0.19s
Test loss: 0.2763 score: 0.9070 time: 0.16s
Epoch 197/1000, LR 0.000261
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.28s
Val loss: 0.2050 score: 0.9225 time: 0.19s
Test loss: 0.2766 score: 0.9070 time: 0.16s
Epoch 198/1000, LR 0.000261
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.30s
Val loss: 0.2047 score: 0.9225 time: 0.27s
Test loss: 0.2769 score: 0.9070 time: 0.24s
Epoch 199/1000, LR 0.000260
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.42s
Val loss: 0.2042 score: 0.9225 time: 0.19s
Test loss: 0.2767 score: 0.9070 time: 0.17s
Epoch 200/1000, LR 0.000260
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.27s
Val loss: 0.2028 score: 0.9225 time: 0.19s
Test loss: 0.2758 score: 0.9070 time: 0.16s
Epoch 201/1000, LR 0.000260
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.27s
Val loss: 0.2029 score: 0.9225 time: 0.19s
Test loss: 0.2763 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.37s
Val loss: 0.2033 score: 0.9225 time: 0.26s
Test loss: 0.2773 score: 0.9070 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.39s
Val loss: 0.2032 score: 0.9225 time: 0.25s
Test loss: 0.2777 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.28s
Val loss: 0.2036 score: 0.9225 time: 0.19s
Test loss: 0.2785 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 205/1000, LR 0.000259
Train loss: 0.5554;  Loss pred: 0.5554; Loss self: 0.0000; time: 0.27s
Val loss: 0.2066 score: 0.9225 time: 0.19s
Test loss: 0.2822 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 206/1000, LR 0.000259
Train loss: 0.5581;  Loss pred: 0.5581; Loss self: 0.0000; time: 0.27s
Val loss: 0.2078 score: 0.9225 time: 0.21s
Test loss: 0.2839 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.34s
Val loss: 0.2075 score: 0.9225 time: 0.19s
Test loss: 0.2840 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.27s
Val loss: 0.2074 score: 0.9225 time: 0.19s
Test loss: 0.2843 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.26s
Val loss: 0.2062 score: 0.9225 time: 0.18s
Test loss: 0.2833 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.26s
Val loss: 0.2049 score: 0.9225 time: 0.18s
Test loss: 0.2822 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.26s
Val loss: 0.2040 score: 0.9225 time: 0.18s
Test loss: 0.2817 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.26s
Val loss: 0.2036 score: 0.9225 time: 0.20s
Test loss: 0.2815 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.27s
Val loss: 0.2021 score: 0.9225 time: 0.22s
Test loss: 0.2803 score: 0.9070 time: 0.20s
Epoch 214/1000, LR 0.000256
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.27s
Val loss: 0.2029 score: 0.9225 time: 0.18s
Test loss: 0.2815 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.27s
Val loss: 0.2029 score: 0.9225 time: 0.19s
Test loss: 0.2818 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.27s
Val loss: 0.2033 score: 0.9225 time: 0.18s
Test loss: 0.2827 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.26s
Val loss: 0.2048 score: 0.9225 time: 0.18s
Test loss: 0.2847 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.27s
Val loss: 0.2071 score: 0.9225 time: 0.18s
Test loss: 0.2875 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.29s
Val loss: 0.2077 score: 0.9225 time: 0.19s
Test loss: 0.2885 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.31s
Val loss: 0.2060 score: 0.9225 time: 0.18s
Test loss: 0.2869 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.26s
Val loss: 0.2060 score: 0.9225 time: 0.18s
Test loss: 0.2871 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.26s
Val loss: 0.2048 score: 0.9225 time: 0.18s
Test loss: 0.2861 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.26s
Val loss: 0.2036 score: 0.9225 time: 0.19s
Test loss: 0.2852 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.26s
Val loss: 0.2027 score: 0.9225 time: 0.18s
Test loss: 0.2845 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.28s
Val loss: 0.2039 score: 0.9225 time: 0.24s
Test loss: 0.2861 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.28s
Val loss: 0.2054 score: 0.9225 time: 0.18s
Test loss: 0.2880 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.27s
Val loss: 0.2063 score: 0.9225 time: 0.18s
Test loss: 0.2893 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.26s
Val loss: 0.2075 score: 0.9225 time: 0.18s
Test loss: 0.2909 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.27s
Val loss: 0.2087 score: 0.9225 time: 0.18s
Test loss: 0.2925 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.28s
Val loss: 0.2092 score: 0.9225 time: 0.18s
Test loss: 0.2934 score: 0.8992 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.38s
Val loss: 0.2093 score: 0.9225 time: 0.26s
Test loss: 0.2937 score: 0.8992 time: 0.24s
     INFO: Early stopping counter 18 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.39s
Val loss: 0.2085 score: 0.9225 time: 0.23s
Test loss: 0.2931 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.26s
Val loss: 0.2079 score: 0.9225 time: 0.19s
Test loss: 0.2927 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 212,   Train_Loss: 0.5521,   Val_Loss: 0.2021,   Val_Precision: 0.9825,   Val_Recall: 0.8615,   Val_accuracy: 0.9180,   Val_Score: 0.9225,   Val_Loss: 0.2021,   Test_Precision: 0.9815,   Test_Recall: 0.8281,   Test_accuracy: 0.8983,   Test_Score: 0.9070,   Test_loss: 0.2803


[0.17553729191422462, 0.17636333708651364, 0.17716077016666532, 0.2639829209074378, 0.1783866120968014, 0.18214822001755238, 0.19493592693470418, 0.17595083895139396, 0.3294711129274219, 0.2708030780777335, 0.27246263809502125, 0.17976868501864374, 0.19387241709046066, 0.18678679503500462, 0.1849911310710013, 0.1952481889165938, 0.19207121594808996, 0.187116920016706, 0.22203910420648754, 0.18039479316212237, 0.17881705099716783, 0.17879667412489653, 0.18621564097702503, 0.17370856902562082, 0.17674741009250283, 0.19113518693484366, 0.18099136580713093, 0.2696086319629103, 0.19513565092347562, 0.1962961859535426, 0.19543089577928185, 0.1880191038362682, 0.1952838411089033, 0.19462593900971115, 0.19527766713872552, 0.19017368904314935, 0.18919846694916487, 0.20559936994686723, 0.26660167099907994, 0.27408914896659553, 0.193543063942343, 0.18811267614364624, 0.18419204582460225, 0.18594341399148107, 0.20090563781559467, 0.17734047188423574, 0.18097334099002182, 0.1793673629872501, 0.1910700029693544, 0.25825565797276795, 0.17306552990339696, 0.18774029309861362, 0.19375561783090234, 0.17612216505222023, 0.1730023710988462, 0.18816776596941054, 0.2434256940614432, 0.19006537809036672, 0.18164691887795925, 0.17743582697585225, 0.21789261605590582, 0.1867034260649234, 0.1904192331712693, 0.24072615103796124, 0.18368557188659906, 0.18431883794255555, 0.18012804491445422, 0.18502118694595993, 0.17942086583934724, 0.18469001003541052, 0.2635928839445114, 0.18782815895974636, 0.17860809504054487, 0.1817601011134684, 0.18327923491597176, 0.17584941000677645, 0.1824974159244448, 0.17552838777191937, 0.17139540892094374, 0.17944634798914194, 0.18365456815809011, 0.215807368978858, 0.1819220611359924, 0.1812916691415012, 0.2590104730334133, 0.18268886511214077, 0.17967040999792516, 0.1919750429224223, 0.180240822955966, 0.19123604404740036, 0.18294834694825113, 0.17649409407749772, 0.17618271405808628, 0.17568784696049988, 0.18142522894777358, 0.17980714910663664, 0.1787772229872644, 0.17695956793613732, 0.17462780186906457, 0.18039013701491058, 0.25538640399463475, 0.17624633479863405, 0.17694320296868682, 0.18305406998842955, 0.18163503101095557, 0.18172453599981964, 0.1969627591315657, 0.2581255428958684, 0.20949939009733498, 0.18680954002775252, 0.178399563068524, 0.1869447010103613, 0.18337008706294, 0.1823141749482602, 0.18597275693900883, 0.18686826596967876, 0.1832911951933056, 0.18077533296309412, 0.18270786199718714, 0.2378831380046904, 0.17629412305541337, 0.17856124509125948, 0.1787967428099364, 0.17993332794867456, 0.26006348710507154, 0.21152324811555445, 0.17604209599085152, 0.17809935589320958, 0.1814059680327773, 0.17561236815527081, 0.17786356480792165, 0.22196103795431554, 0.1958942108321935, 0.19468972203321755, 0.19518160610459745, 0.22323607420548797, 0.2710570970084518, 0.1835626158863306, 0.18445413000881672, 0.1864522930700332, 0.18714952003210783, 0.17613570601679385, 0.17557766288518906, 0.2644621569197625, 0.17550530191510916, 0.17424043989740312, 0.17179557611234486, 0.17350164707750082, 0.1776442441623658, 0.18168719997629523, 0.175346887903288, 0.17604778916575015, 0.1723386379890144, 0.17233277182094753, 0.17437032097950578, 0.17251294502057135, 0.19839427201077342, 0.1813806879799813, 0.1786529840901494, 0.17970847990363836, 0.17393021401949227, 0.1878040919546038, 0.2343284748494625, 0.16877628304064274, 0.17013867502100766, 0.17221621400676668, 0.171660162974149, 0.17358940397389233, 0.17681810702197254, 0.17388839810155332, 0.1751140400301665, 0.17318343208171427, 0.1724438068922609, 0.17320367507636547, 0.21645463490858674, 0.21509464597329497, 0.17782913986593485, 0.17481272295117378, 0.17871213401667774, 0.1794955290388316, 0.19261459703557193, 0.25579507695510983, 0.17671529413200915, 0.17869133688509464, 0.17184707685373724, 0.205432022921741, 0.17214757902547717, 0.18649212992750108, 0.1736732949502766, 0.17875031591393054, 0.18793575186282396, 0.2636923659592867, 0.17423190479166806, 0.17678671586327255, 0.17108807293698192, 0.1770331549923867, 0.18324562115594745, 0.17552830511704087, 0.18234013998880982, 0.17964884894900024, 0.18417781218886375, 0.1782823409885168, 0.2559902500361204, 0.25811415584757924, 0.2537865361664444, 0.1951947200577706, 0.18412874080240726, 0.18444849597290158, 0.18481083097867668, 0.17963643604889512, 0.17675286694429815, 0.24651980795897543, 0.19336487096734345, 0.19460616097785532, 0.18406679783947766, 0.18862420902587473, 0.16133755585178733, 0.1689393250271678, 0.17049851804040372, 0.16401893901638687, 0.15933460392989218, 0.15879031200893223, 0.16645767888985574, 0.1564930488821119, 0.16197436209768057, 0.160867776023224, 0.15917440806515515, 0.1621789161581546, 0.15865458780899644, 0.15941856801509857, 0.16007544100284576, 0.15681927697733045, 0.16798237594775856, 0.23653064109385014, 0.15760661102831364, 0.15730691188946366, 0.15504202898591757, 0.1573575148358941, 0.20267578004859388, 0.22136965789832175, 0.1620777458883822, 0.15746038011275232, 0.155769880162552, 0.16113354312255979, 0.15675313514657319, 0.16039420012384653, 0.15655022207647562, 0.15953723597340286, 0.229766494827345, 0.2513717550318688, 0.251662079943344, 0.24937247508205473, 0.15840220707468688, 0.15636846981942654, 0.24024192988872528, 0.1599252219311893, 0.166559805162251, 0.16466115787625313, 0.16661451593972743, 0.15319592296145856, 0.16645220294594765, 0.1637984390836209, 0.1631423239596188, 0.1666318508796394, 0.16373152704909444, 0.15578425489366055, 0.2005819589830935, 0.15252652787603438, 0.15467819198966026, 0.16331128403544426, 0.24730439018458128, 0.22439561784267426, 0.1600506859831512, 0.16569499904289842, 0.1700539099983871, 0.1865876349620521, 0.1624334380030632, 0.1604957499075681, 0.17750479117967188, 0.1866347009781748, 0.1669779270887375, 0.18926703999750316, 0.16484184190630913, 0.17156295897439122, 0.16777538019232452, 0.17201139102689922, 0.16359737096354365, 0.16123100486584008, 0.2227923609316349, 0.15533105889335275, 0.15898267598822713, 0.15548861282877624, 0.1672408189624548, 0.16756803495809436, 0.15955345216207206, 0.15705781686119735, 0.15698520001024008, 0.15511055290699005, 0.15653513092547655, 0.15441629686392844, 0.1567270380910486, 0.15510329883545637, 0.16013717814348638, 0.15509692719206214, 0.15535202901810408, 0.23493556794710457, 0.23717625392600894, 0.23706437298096716, 0.15523187909275293, 0.1585094160400331, 0.15894272504374385, 0.16147640580311418, 0.23699160292744637, 0.23715394199825823, 0.21285675815306604, 0.16192506905645132, 0.16362693696282804, 0.16144760604947805, 0.171282951021567, 0.2370734300930053, 0.24093663389794528, 0.24425283796153963, 0.1604821030050516, 0.1570375661831349, 0.216685364022851, 0.16248639510013163, 0.15944701014086604, 0.1589030190370977, 0.23491631005890667, 0.15940516209229827, 0.16529361507855356, 0.15652871294878423, 0.1618442351464182, 0.15737491706386209, 0.2292263861745596, 0.15964548592455685, 0.15667878300882876, 0.16039849515073, 0.15879932302050292, 0.24114905018359423, 0.17533562192693353, 0.1607885940466076, 0.16197124402970076, 0.16007198602892458, 0.16208262904547155, 0.18558678589761257, 0.16015832219272852, 0.15836581215262413, 0.15833199489861727, 0.15802394086495042, 0.15872173383831978, 0.15991552406921983, 0.16454843082465231, 0.16526030912064016, 0.16553608002141118, 0.16337595996446908, 0.16181635903194547, 0.17822289001196623, 0.16126364096999168, 0.24656640086323023, 0.23025543801486492, 0.15649549313820899, 0.19768212107010186, 0.17216698895208538, 0.25141965597867966, 0.23378518992103636, 0.17139640101231635, 0.15784707106649876, 0.1580698899924755, 0.15532809495925903, 0.1543712019920349, 0.15987798292189837, 0.1561730750836432, 0.15837327390909195, 0.16809246595948935, 0.2134001690428704, 0.22238190099596977, 0.15430618496611714, 0.1681612899992615, 0.1668138150125742, 0.16777621116489172, 0.17118927789852023, 0.1661140089854598, 0.1667797730769962, 0.16642876085825264, 0.16570571484044194, 0.16489791101776063, 0.16882871510460973, 0.18910394911654294, 0.16397503693588078, 0.15536753204651177, 0.15390370110981166, 0.17356175789609551, 0.2418094000313431, 0.158978417981416, 0.1584360678680241, 0.15688430797308683, 0.23923370405100286, 0.161085237050429, 0.16252922010608017, 0.15724672097712755, 0.160638706991449, 0.20620269607752562, 0.15820085303857923, 0.16090363496914506, 0.1549569859635085, 0.2348957711365074, 0.23913690308108926, 0.2183021258097142, 0.15905963606201112, 0.15615182905457914, 0.1675257261376828, 0.1653585429303348, 0.24329606210812926, 0.1710131319705397, 0.16815753118135035, 0.18204326392151415, 0.24320298619568348, 0.17072186199948192, 0.16921365214511752, 0.16807392798364162, 0.16890555410645902, 0.1651548941154033, 0.16247564204968512, 0.15544680296443403, 0.155862077139318, 0.15429115411825478, 0.1612746410537511, 0.20173450792208314, 0.15507520409300923, 0.15677398489788175, 0.15418122499249876, 0.15676937997341156, 0.1571802720427513, 0.21666875714436173, 0.15445199911482632, 0.15631968807429075, 0.15395450312644243, 0.15545983286574483, 0.15587434195913374, 0.1880729158874601, 0.15778347803279757, 0.1537987389601767, 0.16042357287369668, 0.15723477094434202, 0.18851502682082355, 0.24116850504651666, 0.15739115606993437, 0.1574549530632794]
[0.0013607542008854622, 0.0013671576518334391, 0.0013733393036175607, 0.0020463792318406032, 0.0013828419542387706, 0.0014120017055624216, 0.0015111312165480945, 0.001363959991871271, 0.002554039635096294, 0.002099248667269252, 0.002112113473604816, 0.0013935556978189438, 0.0015028869541896174, 0.0014479596514341442, 0.0014340397757441961, 0.0015135518520666186, 0.0014889241546363562, 0.0014505187598194264, 0.0017212333659417638, 0.001398409249318778, 0.0013861786899005259, 0.0013860207296503607, 0.0014435321005970933, 0.001346578054462177, 0.0013701349619573862, 0.0014816681157739818, 0.0014030338434661313, 0.00208998939506132, 0.0015126794645230668, 0.0015216758601049813, 0.0015149681843355182, 0.0014575124328392883, 0.0015138282256504131, 0.0015087282093776057, 0.001513780365416477, 0.0014742146437453439, 0.0014666547825516656, 0.001593793565479591, 0.0020666796201479066, 0.0021247220850123686, 0.0015003338290104108, 0.0014582377995631492, 0.001427845316469785, 0.00144142181388745, 0.0015574080450821291, 0.001374732340187874, 0.0014028941162017195, 0.0013904446743197682, 0.0014811628137159254, 0.002001981844750139, 0.0013415932550650928, 0.0014553511092915785, 0.0015019815335728863, 0.0013652881011800018, 0.0013411036519290402, 0.0014586648524760506, 0.0018870208841972342, 0.0014733750239563311, 0.0014081156502167385, 0.0013754715269445911, 0.0016890900469450063, 0.0014473133803482436, 0.0014761180865989869, 0.0018660941940927228, 0.0014239191619116207, 0.0014288282011050818, 0.0013963414334453815, 0.0014342727670229453, 0.001390859425111219, 0.0014317055041504691, 0.0020433556894923367, 0.0014560322399980337, 0.0013845588762832935, 0.001408993031887352, 0.0014207692629145097, 0.0013631737209827632, 0.0014147086505770915, 0.0013606851765265067, 0.0013286465807825095, 0.0013910569611561392, 0.0014236788229309311, 0.00167292534092138, 0.0014102485359379256, 0.0014053617762907068, 0.0020078331242900254, 0.0014161927528072928, 0.0013927938759529083, 0.0014881786273055991, 0.0013972156818291938, 0.0014824499538558166, 0.0014182042399089234, 0.0013681712719185869, 0.0013657574733184983, 0.001361921294267441, 0.0014063971236261518, 0.0013938538690436949, 0.0013858699456377084, 0.0013717795964041653, 0.001353703890457865, 0.0013983731551543456, 0.0019797395658498816, 0.0013662506573537524, 0.0013716527361913708, 0.001419023798359919, 0.0014080234962089578, 0.0014087173333319353, 0.001526843094043145, 0.002000973200743166, 0.0016240262798243022, 0.001448135969207384, 0.0013829423493684032, 0.0014491837287624908, 0.001421473543123566, 0.0014132881778934898, 0.0014416492785969677, 0.001448591209067277, 0.0014208619782426792, 0.0014013591702565435, 0.0014163400154820708, 0.001844055333369693, 0.0013666211089566928, 0.0013841956983818564, 0.0013860212620925303, 0.0013948319996021283, 0.002015996024070322, 0.0016397151016709648, 0.0013646674107817948, 0.001380615161962865, 0.001406247814207576, 0.0013613361872501614, 0.001378787324092416, 0.0017206282011962444, 0.0015185597738929728, 0.0015092226514202911, 0.0015130357062371896, 0.0017305122031433175, 0.0021012178062670686, 0.0014229660146227179, 0.0014298769768125326, 0.0014453666129459937, 0.001450771473117115, 0.0013653930698976267, 0.0013610671541487524, 0.002050094239688081, 0.001360506216396195, 0.0013507010844759932, 0.0013317486520336812, 0.0013449740083527196, 0.0013770871640493472, 0.0014084279067929863, 0.0013592782008006823, 0.0013647115439205438, 0.0013359584340233673, 0.0013359129598523064, 0.0013517079145698122, 0.0013373096513222586, 0.0015379400931067708, 0.0014060518448060567, 0.001384906853412011, 0.0013930889915010727, 0.0013482962327092424, 0.001455845674066696, 0.001816499805034593, 0.0013083432793848273, 0.0013189044575271911, 0.001335009410905168, 0.0013306989377841008, 0.0013456542943712584, 0.0013706830001703298, 0.001347972078306615, 0.0013574731785284226, 0.0013425072254396456, 0.0013367736968392318, 0.001342664147878802, 0.0016779429062681143, 0.0016674003563821316, 0.0013785204640770143, 0.0013551373872184014, 0.001385365379974246, 0.0013914382096033456, 0.0014931364111284645, 0.001982907573295425, 0.0013698860010233267, 0.0013852041618999584, 0.0013321478825871103, 0.0015924963017189225, 0.0013344773567866446, 0.001445675425794582, 0.0013463046120176481, 0.0013856613636738801, 0.0014568662935102632, 0.002044126867901447, 0.0013506349208656438, 0.001370439657854826, 0.0013262641312944334, 0.001372350038700672, 0.0014205086911313755, 0.0013606845357910144, 0.0014134894572775955, 0.0013926267360387615, 0.0014277349782082461, 0.0013820336510737736, 0.0019844205429156623, 0.002000884929051002, 0.0019673374896623597, 0.0015131373647889195, 0.0014273545798636222, 0.0014298333021155161, 0.0014326421006098967, 0.001392530512006939, 0.0013701772631340941, 0.0019110062632478717, 0.0014989524881189415, 0.001508574891301204, 0.001426874401856416, 0.001462203170743215, 0.0012506787275332351, 0.0013096071707532388, 0.0013216939382977033, 0.0012714646435378826, 0.0012351519684487766, 0.0012309326512320328, 0.0012903696037973313, 0.001213124409938852, 0.0012556152100595393, 0.001247037023435845, 0.0012339101400399624, 0.001257200900450811, 0.0012298805256511353, 0.0012358028528302214, 0.0012408948914949284, 0.0012156533099017864, 0.0013021889608353376, 0.0018335708611926368, 0.0012217566746380902, 0.0012194334254997184, 0.0012018761936892835, 0.00121982569640228, 0.0015711300778960765, 0.0017160438596769128, 0.0012564166347936603, 0.0012206231016492427, 0.0012075184508724961, 0.0012490972335082154, 0.001215140582531575, 0.0012433658924329188, 0.001213567612995935, 0.001236722759483743, 0.001781135618816628, 0.0019486182560609986, 0.0019508688367701084, 0.0019331199618763933, 0.001227924085850286, 0.0012121586807707484, 0.0018623405417730642, 0.0012397304025673589, 0.0012911612803275271, 0.0012764430843120397, 0.001291585394881608, 0.0011875652942748725, 0.0012903271546197493, 0.0012697553417334955, 0.0012646691779815411, 0.0012917197742607705, 0.0012692366437914299, 0.0012076298828965933, 0.001554898906845686, 0.0011823761850855378, 0.0011990557518578315, 0.0012659789460111959, 0.001917088296004506, 0.0017395009135091027, 0.0012407029921174511, 0.001284457356921693, 0.001318247364328582, 0.0014464157748996288, 0.001259173938008242, 0.0012441531000586675, 0.001376006133175751, 0.0014467806277377892, 0.0012944025355716084, 0.001467186356569792, 0.0012778437357078226, 0.0013299454184061334, 0.001300584342576159, 0.0013334216358674357, 0.0012681966741359973, 0.0012498527508979851, 0.001727072565361511, 0.0012041167356073856, 0.0012324238448699776, 0.001205338083944002, 0.001296440457073293, 0.001298977015179026, 0.0012368484663726517, 0.0012175024562883516, 0.0012169395349631014, 0.001202407386875892, 0.0012134506273292756, 0.0011970255570847165, 0.0012149382797755706, 0.0012023511537632278, 0.0012413734739805146, 0.0012023017611787763, 0.0012042792947139851, 0.00182120595307833, 0.0018385756118295266, 0.001837708317681916, 0.0012033478999438213, 0.0012287551631010318, 0.0012321141476259213, 0.0012517550837450712, 0.0018371442087398943, 0.0018384026511492886, 0.0016500523887834576, 0.0012552330934608629, 0.0012684258679288994, 0.0012515318298409151, 0.0013277748141206744, 0.0018377785278527543, 0.0018677258441701186, 0.001893432852415036, 0.0012440473101166792, 0.0012173454742878675, 0.0016797315040531086, 0.0012595844581405552, 0.001236023334425318, 0.0012318063491247883, 0.0018210566671233075, 0.0012356989309480486, 0.0012813458533221206, 0.0012134008755719707, 0.001254606474003242, 0.0012199605973942796, 0.0017769487300353457, 0.0012375619063919137, 0.0012145642093707655, 0.0012433991872149613, 0.0012310025040349064, 0.0018693724820433662, 0.0013591908676506474, 0.001246423209663625, 0.0012555910389899284, 0.0012408681087513534, 0.0012564544887245857, 0.0014386572550202524, 0.0012415373813389808, 0.0012276419546715048, 0.0012273798054156376, 0.0012249917896507785, 0.001230401037506355, 0.0012396552253427894, 0.0012755692311988553, 0.0012810876676018618, 0.0012832254265225673, 0.0012664803098020858, 0.0012543903800926006, 0.001381572790790436, 0.0012501057439534239, 0.0019113674485521724, 0.0017849258760842242, 0.0012131433576605347, 0.001532419543179084, 0.001334627821333995, 0.0019489895812300749, 0.0018122882939615222, 0.001328654271413305, 0.001223620705941851, 0.0012253479844377946, 0.001204093759374101, 0.0011966759844343792, 0.0012393642086968866, 0.0012106439928964588, 0.0012276997977448988, 0.001303042371778987, 0.001654264876301321, 0.0017238907053951146, 0.0011961719764815282, 0.0013035758914671433, 0.0012931303489346836, 0.001300590784223967, 0.0013270486658800017, 0.0012877054960113163, 0.001292866457961211, 0.0012901454330097105, 0.0012845404251197049, 0.0012782783799826405, 0.0013087497294930986, 0.0014659220861747515, 0.001271124317332409, 0.0012043994732287733, 0.0011930519465876872, 0.001345439983690663, 0.0018744914731111868, 0.0012323908370652401, 0.001228186572620342, 0.0012161574261479598, 0.0018545248376046733, 0.0012487227678327829, 0.0012599164349308542, 0.0012189668292800586, 0.0012452612945073565, 0.0015984705122288807, 0.0012263632018494513, 0.0012473149997608145, 0.0012012169454535542, 0.0018208974506706, 0.001853774442489064, 0.001692264541160575, 0.0012330204345892336, 0.0012104792949967375, 0.0012986490398269985, 0.0012818491700025954, 0.0018860159853343354, 0.0013256831935700751, 0.0013035467533438012, 0.0014111880924148384, 0.0018852944666332052, 0.0013234252868176892, 0.0013117337375590505, 0.0013028986665398576, 0.001309345380670225, 0.0012802704970186303, 0.0012595011011603497, 0.0012050139764684807, 0.0012082331561187442, 0.0011960554582810447, 0.0012501910159205512, 0.0015638333947448305, 0.001202133365062087, 0.0012153022085107112, 0.0011952032945154942, 0.001215266511421795, 0.001218451721261638, 0.0016796027685609437, 0.0011973023187195838, 0.0012117805277076803, 0.0011934457606700964, 0.0012051149834553863, 0.0012083282322413469, 0.0014579295805229465, 0.0012231277366883534, 0.0011922382865129978, 0.0012435935881681913, 0.0012188741933669923, 0.0014613567970606478, 0.0018695232949342377, 0.001220086481162282, 0.0012205810314982898]
[734.8865793317307, 731.4445401808205, 728.152174313999, 488.66797729400145, 723.1484385722748, 708.2144419943777, 661.7559011746968, 733.1593345550116, 391.53660196126805, 476.36090740065663, 473.4594104422174, 717.588828035436, 665.3860406548124, 690.6269791492748, 697.3307274416773, 660.6975496971512, 671.6258829478339, 689.4085259017878, 580.9787445369764, 715.0982450145697, 721.4077140889833, 721.4899305671, 692.745245905073, 742.6231228752647, 729.8551075372835, 674.9149754617133, 712.7411820156423, 478.47132734884525, 661.0785850228292, 657.170180731532, 660.0798685674122, 686.1004938750079, 660.5769287795861, 662.8099042520913, 660.5978138214762, 678.3272736047657, 681.8237065031853, 627.4338293611371, 483.86793494795904, 470.64978853183936, 666.5183312300431, 685.7592090258355, 700.3559758646737, 693.7594466556911, 642.0924838276827, 727.4143269687959, 712.8121705346234, 719.1943832567225, 675.1452242385228, 499.50502929001675, 745.3823997881392, 687.1194130513085, 665.7871469439563, 732.4461402217688, 745.6545201122989, 685.5584394883599, 529.9358413965908, 678.7138262428145, 710.168940914817, 727.023410089305, 592.034747826892, 690.9353658841918, 677.4525758328896, 535.8786299028118, 702.2870586680592, 699.8742040691678, 716.1572206108395, 697.2174491436901, 718.9799212958102, 698.4676646845538, 489.3910566536979, 686.7979791445761, 722.2516984502656, 709.7267178535978, 703.8440555425866, 733.5822167104735, 706.8593237145171, 734.9238583995991, 752.6455977563631, 718.8778230683503, 702.405615573671, 597.7553065513613, 709.0948683984428, 711.5605510770235, 498.0493587352297, 706.118568971433, 717.9813303787179, 671.9623448769291, 715.7091156397768, 674.5590280461232, 705.1170570920164, 730.9026439341178, 732.1944192406387, 734.2568210139386, 711.036721563873, 717.4353224603717, 721.5684293809039, 728.9800800517021, 738.7139883758248, 715.1167028014243, 505.11694429398864, 731.9301144468383, 729.0475013206853, 704.7098161114571, 710.215420902035, 709.8656177068353, 654.9461460063704, 499.7568181465887, 615.7535825763771, 690.5428918717732, 723.0959413866422, 690.0436294947469, 703.4953304882382, 707.5697763852401, 693.649984671177, 690.3258791994759, 703.7981276948512, 713.5929326504727, 706.0451509305385, 542.2830768167203, 731.7317092836514, 722.4411990074912, 721.489653405649, 716.9322185648501, 496.03272430120523, 609.8620418760198, 732.7792780125943, 724.3148036837925, 711.1122164221833, 734.5724071435694, 725.2750170576511, 581.1830814494165, 658.5186946157573, 662.5927586356628, 660.9229351810393, 577.8635933243297, 475.9144896913644, 702.7574725775428, 699.3608654565443, 691.8659882157974, 689.2884362079499, 732.3898312117383, 734.7176051907788, 487.7824544066562, 735.0205298207829, 740.3562575711944, 750.8924439104363, 743.508791835143, 726.1704459284072, 710.0114923716732, 735.6845709810916, 732.7555808073548, 748.5262823547614, 748.551762017906, 739.8047974870776, 747.7699716077385, 650.2203853596887, 711.2113281554954, 722.0702226552555, 717.8292313705575, 741.676773798157, 686.8859919792484, 550.5092801157533, 764.3253997301015, 758.2050347110784, 749.0583900243641, 751.4847811220279, 743.1329162199409, 729.5632906191538, 741.8551289699163, 736.662805436831, 744.8749481944271, 748.0697760320054, 744.7878917299181, 595.9678343431148, 599.7359879241995, 725.415418964816, 737.9325590393696, 721.8312327240269, 718.6808534495167, 669.7311729503884, 504.3099403458752, 729.987750260229, 721.9152436189557, 750.667409430506, 627.944943370111, 749.3570384798063, 691.7181977070497, 742.7739540320994, 721.6770462219175, 686.4047884521636, 489.20642632451944, 740.3925254346924, 729.6928356300766, 753.9976211405192, 728.6770662001004, 703.9731655591223, 734.9242044693808, 707.4690192072723, 718.0675008756737, 700.4101007982325, 723.5713828118787, 503.92544240180246, 499.77886558138516, 508.3011965433663, 660.8785317646945, 700.596764187036, 699.3822276488074, 698.0110381890113, 718.1171194294212, 729.8325748835125, 523.2845225218881, 667.1325528502343, 662.8772663301199, 700.8325320707718, 683.8994881208715, 799.5658501143143, 763.5877554219836, 756.6048167611073, 786.4945400427923, 809.6169747078951, 812.3921312828173, 774.9717577484586, 824.317763130663, 796.4223370251955, 801.9008106469792, 810.431787170185, 795.4178203669891, 813.0871081730238, 809.1905579517085, 805.8700272311396, 822.6029509028284, 767.9377034178763, 545.3838851635956, 818.4935844907259, 820.0529681152577, 832.0324549655954, 819.7892559153102, 636.4845368749574, 582.7356884621087, 795.9143267505597, 819.2537062823502, 828.1446956586435, 800.5781881298378, 822.9500474065644, 804.2684829027118, 824.0167167376027, 808.5886609036285, 561.439561050602, 513.184148249454, 512.5921236486697, 517.2984707215711, 814.3825921514863, 824.9744986886967, 536.9587234824077, 806.6269875523735, 774.4965832202863, 783.4270186351213, 774.2422637812997, 842.0589628384181, 774.9972527662516, 787.5532924593016, 790.7206227608369, 774.1617182970534, 787.8751412446042, 828.0682799943828, 643.1286275894487, 845.7545175672293, 833.9895775910235, 789.9025518163368, 521.6243832295816, 574.8775365588602, 805.9946710480207, 778.5388861772582, 758.5829693726156, 691.364141178143, 794.1714562340748, 803.75960157383, 726.740946780558, 691.1897912011837, 772.5572011170387, 681.5766760113213, 782.5683000637636, 751.9105567493477, 768.8851597422967, 749.9503331138513, 788.521228918444, 800.094250527934, 579.0144664770817, 830.484263218529, 811.4091626533738, 829.6427478072272, 771.3427905956394, 769.8365623984341, 808.5064801290773, 821.3535790708593, 821.7335136789034, 831.6648840608099, 824.0961580784987, 835.4040513850344, 823.0870791104919, 831.7037804389419, 805.5593429054508, 831.7379482332015, 830.3721606685091, 549.0867182318012, 543.8993063793126, 544.1559960186714, 831.014871133016, 813.831778721712, 811.6131138716598, 798.878321315176, 544.323083208533, 543.9504775381192, 606.041363775895, 796.6647829869211, 788.3787498222594, 799.0208288406952, 753.1397563541339, 544.1352071777615, 535.4104849602952, 528.1412534511165, 803.8279508085669, 821.459496191899, 595.3332408108378, 793.9126221644848, 809.0462147018803, 811.8159162846586, 549.1317310733021, 809.2586106170567, 780.4294191199974, 824.1299475975916, 797.0626811841367, 819.6986051319242, 562.7624382725487, 808.0403855638054, 823.3405795137618, 804.2469468231347, 812.346032377887, 534.938868313138, 735.7318414951488, 802.2957148478263, 796.4376687527646, 805.8874210299986, 795.890347779401, 695.0925917277792, 805.4529932248306, 814.5697499134283, 814.7437293555288, 816.3320019353605, 812.7431378200826, 806.675904361618, 783.9637203071612, 780.5867040090667, 779.2863041296771, 789.5898517018958, 797.1999912229707, 723.8127492564995, 799.9323295943976, 523.1856390342332, 560.2473544693089, 824.3048883591405, 652.5628079145009, 749.2725567495469, 513.0863754381207, 551.7885886765164, 752.6412412284562, 817.2467130901283, 816.0947034640228, 830.5001103234761, 835.6480893804014, 806.8653209305093, 826.0066591562609, 814.5313714613708, 767.4347524361341, 604.498115341628, 580.08317863214, 836.0001903250092, 767.1206613636617, 773.3172458784434, 768.8813515595357, 753.5518671705028, 776.5750811016279, 773.4750900545077, 775.1064139080461, 778.4885399046987, 782.3022087047896, 764.0880280352128, 682.1644952559851, 786.7051132328327, 830.289303696874, 838.1864703042934, 743.2512874018431, 533.4780202228662, 811.4308950733152, 814.2085431421833, 822.2619691328828, 539.2216807900033, 800.8182646781936, 793.7034332399048, 820.3668680554795, 803.0443123951865, 625.5980278332546, 815.4191176740479, 801.7220992225383, 832.4890884905234, 549.1797463013198, 539.4399540093451, 590.9241585326772, 811.0165670799596, 826.1190456815664, 770.0309855334099, 780.1229843585851, 530.2181995147455, 754.3280361780799, 767.137808778123, 708.6227593437177, 530.4211186626029, 755.6150014366143, 762.349836225801, 767.5193978482812, 763.7404269056359, 781.0849366041809, 793.9651653172218, 829.865893282572, 827.6548238523268, 836.0816323995434, 799.8777684893788, 639.454307191828, 831.8544589670819, 822.8406012899848, 836.6777472826287, 822.8647713085214, 820.7136832344546, 595.3788709557699, 835.2109441075982, 825.2319435200824, 837.9098849356335, 829.7963378836541, 827.5897006437435, 685.9041845089039, 817.5760961054838, 838.7585026519764, 804.1212253860172, 820.429216929781, 684.2955820312916, 534.8957152390957, 819.6140318245125, 819.2819437579479]
Elapsed: 0.1834797645698121~0.029167846725522296
Time per graph: 0.0014223237563551327~0.00022610733895753716
Speed: 717.9867527315088~95.20175086234742
Total Time: 0.1588
best val loss: 0.20210935121994147 test_score: 0.9070

Testing...
Test loss: 0.2733 score: 0.9070 time: 0.16s
test Score 0.9070
Epoch Time List: [0.6235006421338767, 0.6146915510762483, 0.6186582711525261, 0.7454341158736497, 0.8299643038772047, 0.6174163997638971, 0.6398934538010508, 0.6017873729579151, 0.7764369966462255, 0.913911780808121, 0.7002250859513879, 0.6021345423068851, 0.6475065262056887, 0.6526677410583943, 0.6180973055306822, 0.6798804588615894, 0.7121924087405205, 0.6164275191258639, 0.6939948978833854, 0.5986697829794139, 0.6039261249825358, 0.6133825050201267, 0.6421189398970455, 0.6861946142744273, 0.6099444387946278, 0.6310881411191076, 0.6085848601069301, 0.7238331940025091, 0.657312777126208, 0.6615060546901077, 0.6508346970658749, 0.6506477380171418, 0.6556074689142406, 0.7503613929729909, 0.6585775210987777, 0.6486470720265061, 0.6539894971065223, 0.6660429406911135, 0.8100088543724269, 0.9367432778235525, 0.6586342190857977, 0.6339745579753071, 0.6452003046870232, 0.6384977586567402, 0.6373017968144268, 0.6701166052371264, 0.6165477188769728, 0.6084070210345089, 0.6332503659650683, 0.8136518748942763, 0.6682389099150896, 0.6409326691646129, 0.6468586919363588, 0.5927104472648352, 0.5988247729837894, 0.6339900111779571, 0.7352630514651537, 0.6477510158438236, 0.6376841228920966, 0.6037337007001042, 0.6786731530446559, 0.6883535429369658, 0.6350665029603988, 0.6942183070350438, 0.8082051840610802, 0.6320047189947218, 0.6467968949582428, 0.632416786858812, 0.6276483531109989, 0.6528174127452075, 0.890630123205483, 0.6741201225668192, 0.608288744231686, 0.6135257829446346, 0.6276011050213128, 0.5997566760051996, 0.6236874910537153, 0.6795925649348646, 0.597937302896753, 0.6028474820777774, 0.6197071908973157, 0.6913079309742898, 0.6315930229611695, 0.6184490947052836, 0.8280366808176041, 0.6282157099340111, 0.6188028787728399, 0.6371266692876816, 0.6109115472063422, 0.6547151419799775, 0.6994714620523155, 0.6096276151947677, 0.6067678739782423, 0.6106725470162928, 0.6158222921658307, 0.687823094194755, 0.6206376221962273, 0.6123947657179087, 0.6777550680562854, 0.6074421489611268, 0.6865707826800644, 0.6822121050208807, 0.6078663237858564, 0.651912814937532, 0.692917357897386, 0.7102800386492163, 0.6164888578932732, 0.837526666931808, 0.8382368320599198, 0.6893407788593322, 0.6144375139847398, 0.6243891450576484, 0.6195686429273337, 0.6143956538289785, 0.640046315966174, 0.704701819922775, 0.6060971189290285, 0.6122425277717412, 0.6008876080159098, 0.6905582451727241, 0.6918681988026947, 0.6056635591667145, 0.6144573737401515, 0.6067770416848361, 0.82668292010203, 0.8578346101567149, 0.6903238382656127, 0.6119623437989503, 0.6126190279610455, 0.6158417020924389, 0.6127806541044265, 0.6756256460212171, 0.7479287618771195, 0.6608702489174902, 0.6510320373345166, 0.6873140218667686, 0.9080599481239915, 0.7184558950830251, 0.6226705862209201, 0.6606602428946644, 0.7086483940947801, 0.6128713109064847, 0.5989487681072205, 0.7817212203517556, 0.7049247098620981, 0.6027338101994246, 0.5895942221395671, 0.5853080211672932, 0.5887725830543786, 0.5945484640542418, 0.6747376339044422, 0.5942409690469503, 0.5928731430321932, 0.5966036522295326, 0.5911836409941316, 0.5901763618458062, 0.6196397508028895, 0.6721270368434489, 0.5969715768005699, 0.6033627269789577, 0.5838549740146846, 0.5998533959500492, 0.7268601742107421, 0.6938464292325079, 0.5831978279165924, 0.5870572789572179, 0.5853270518127829, 0.5924624502658844, 0.6165005043148994, 0.6813445969019085, 0.590493094176054, 0.5926824316848069, 0.5922169322147965, 0.5958083621226251, 0.6860768420156091, 0.8308730139397085, 0.5927367201074958, 0.5922509080264717, 0.5950175758916885, 0.6102460348047316, 0.619573141913861, 0.7531872126273811, 0.7115334139671177, 0.60221958020702, 0.5910479461308569, 0.6711806657258421, 0.5907346699386835, 0.6224696952849627, 0.5985063470434397, 0.6676011632662266, 0.6743319192901254, 0.7359344938304275, 0.5933113181963563, 0.6041368159931153, 0.7641653560567647, 0.6059968480840325, 0.6222424721345305, 0.6280124010518193, 0.6089611370116472, 0.6273034710902721, 0.6925347321666777, 0.6145890569314361, 0.6918857980053872, 0.9040451559703797, 0.8973854200448841, 0.823693023994565, 0.638555099023506, 0.6273912338074297, 0.6295894628856331, 0.6267561689019203, 0.6246160231530666, 0.7355568169150501, 0.6980045633390546, 0.6630340351257473, 0.6217350631486624, 0.6486837919801474, 0.5939719900488853, 0.6092185210436583, 0.6247284789569676, 0.6186181840021163, 0.6333576769102365, 0.6616957748774439, 0.6132511089090258, 0.589972595218569, 0.6010067737661302, 0.6060711019672453, 0.6091995649039745, 0.6114883092232049, 0.6963420086540282, 0.6173499459400773, 0.6070462199859321, 0.6165604400448501, 0.6092802139464766, 0.7638276158832014, 0.7544745632912964, 0.6035440319683403, 0.6051480451133102, 0.6055568079464138, 0.6504656956531107, 0.7297453260980546, 0.6104348739609122, 0.6023319598753005, 0.6045183909591287, 0.5999978177715093, 0.5921923273708671, 0.6235901000909507, 0.6858647502958775, 0.6036543408408761, 0.6987545569427311, 0.9264617250300944, 0.9115451469551772, 0.9106481019407511, 0.6242021289654076, 0.5978200242388994, 0.7538684119936079, 0.6320381059776992, 0.6337655500974506, 0.6148798398207873, 0.6144663209561259, 0.5819570848252624, 0.6279230250511318, 0.6946501096244901, 0.6120328875258565, 0.6227959762327373, 0.6158549976535141, 0.5866395337507129, 0.6754252729006112, 0.6545249950140715, 0.5899167088791728, 0.6062093391083181, 0.7669536659959704, 0.8606033821124583, 0.6877281221095473, 0.7129298471845686, 0.6283112620003521, 0.6555009519215673, 0.7017977409996092, 0.5990356761030853, 0.6323350809980184, 0.6393874823115766, 0.6252719778567553, 0.6507099058944732, 0.6813490672502667, 0.6349102267995477, 0.629472822882235, 0.6354566090740263, 0.617288873065263, 0.6032623900100589, 0.7929566169623286, 0.6024511989671737, 0.6076609820593148, 0.6066108760423958, 0.6260699299164116, 0.6354528099764138, 0.6462868570815772, 0.6750761468429118, 0.5956166617106646, 0.5939083790872246, 0.5914509459398687, 0.5980812907218933, 0.5961053906939924, 0.5982807299587876, 0.6282486347481608, 0.6760020821820945, 0.5999497820157558, 0.8032606199849397, 0.8740347519051284, 0.8756359401158988, 0.7398124989122152, 0.6026295451447368, 0.6074273749254644, 0.6017636423930526, 0.7649506630841643, 0.8777492912486196, 0.8615368092432618, 0.6931262828875333, 0.5981891311239451, 0.5956229160074145, 0.6124339888338, 0.79548748023808, 0.8929946138523519, 0.895750735886395, 0.6117819179780781, 0.6068217249121517, 0.6720921550877392, 0.6872650671284646, 0.6060857872944325, 0.6039502972271293, 0.6911765090189874, 0.7114721580874175, 0.6099017059896141, 0.5982546019367874, 0.6085905830841511, 0.6094269659370184, 0.7163707339204848, 0.6085174463223666, 0.6102954319212586, 0.6026453238446265, 0.611514296149835, 0.7758088370319456, 0.8180061271414161, 0.6297018229961395, 0.6224452198948711, 0.6324994089081883, 0.625362945953384, 0.6603390832897276, 0.6782477071974427, 0.6101501437369734, 0.6219065629411489, 0.6147999749518931, 0.622829855652526, 0.6179460822604597, 0.6200018518138677, 0.6657079851720482, 0.691273839911446, 0.6142504664603621, 0.6086623789742589, 0.6456827477086335, 0.5956187709234655, 0.8408503753598779, 0.8167399996891618, 0.584955028956756, 0.6326788908336312, 0.6297886779066175, 0.8056065440177917, 0.9024268169887364, 0.6433181026950479, 0.714707454899326, 0.5954957939684391, 0.5887732733972371, 0.5943086622282863, 0.626327645033598, 0.690722624771297, 0.6035609720274806, 0.6276945821009576, 0.6859279768541455, 0.8671573919709772, 0.5918609562795609, 0.6129172882065177, 0.6240088290069252, 0.6263899309560657, 0.6627537480089813, 0.6998263539280742, 0.6316888330038637, 0.6278180310036987, 0.6317850486375391, 0.6252650448586792, 0.6626728258561343, 0.7170315640978515, 0.6256647799164057, 0.6040433926973492, 0.5921228351071477, 0.6244636839255691, 0.6997018931433558, 0.6041231590788811, 0.603819842915982, 0.5973065178841352, 0.7633056379854679, 0.6393606651108712, 0.6028211272787303, 0.5945935868658125, 0.6043457968626171, 0.6480360860005021, 0.6967923860065639, 0.6153295107651502, 0.5982003891840577, 0.7904536430723965, 0.8883168308530003, 0.835481742862612, 0.621599268168211, 0.5992377670481801, 0.6226061251945794, 0.630127104697749, 0.8037426439113915, 0.7809318399522454, 0.6198042880278081, 0.6342975632287562, 0.8746394168119878, 0.8071808679960668, 0.6278288068715483, 0.6193583179265261, 0.6391575608868152, 0.6946650361642241, 0.6187818981707096, 0.5884914952330291, 0.5920784671325237, 0.5898088733665645, 0.6171859868336469, 0.6778913061134517, 0.602272163843736, 0.6057696549687535, 0.5969514339230955, 0.5874466348905116, 0.6020193158183247, 0.6865210069809109, 0.6370706290472299, 0.596636489033699, 0.5945108709856868, 0.595938416197896, 0.5955529599450529, 0.7085508129093796, 0.6129334531724453, 0.5919570617843419, 0.5978135699406266, 0.5986958299763501, 0.6498600337654352, 0.8733738358132541, 0.7714004619047046, 0.6021387572400272]
Total Epoch List: [216, 233]
Total Time List: [0.18939930596388876, 0.15877319988794625]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a289b3d30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.9009;  Loss pred: 2.9009; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.28s
Epoch 2/1000, LR 0.000020
Train loss: 2.9044;  Loss pred: 2.9044; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 2.8668;  Loss pred: 2.8668; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 2.8087;  Loss pred: 2.8087; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.17s
Epoch 5/1000, LR 0.000110
Train loss: 2.7568;  Loss pred: 2.7568; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.22s
Epoch 6/1000, LR 0.000140
Train loss: 2.6646;  Loss pred: 2.6646; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 2.5452;  Loss pred: 2.5452; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000200
Train loss: 2.4898;  Loss pred: 2.4898; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000230
Train loss: 2.3900;  Loss pred: 2.3900; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.22s
Epoch 10/1000, LR 0.000260
Train loss: 2.2578;  Loss pred: 2.2578; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 2.1519;  Loss pred: 2.1519; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 2.0370;  Loss pred: 2.0370; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.18s
Epoch 13/1000, LR 0.000290
Train loss: 1.9309;  Loss pred: 1.9309; Loss self: 0.0000; time: 0.28s
Val loss: 0.6929 score: 0.5116 time: 0.17s
Test loss: 0.6931 score: 0.4844 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 1.8300;  Loss pred: 1.8300; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
Epoch 15/1000, LR 0.000290
Train loss: 1.7531;  Loss pred: 1.7531; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.23s
Epoch 16/1000, LR 0.000290
Train loss: 1.6671;  Loss pred: 1.6671; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 1.5913;  Loss pred: 1.5913; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 1.5374;  Loss pred: 1.5374; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 1.4680;  Loss pred: 1.4680; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 1.4183;  Loss pred: 1.4183; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.17s
Epoch 21/1000, LR 0.000290
Train loss: 1.3716;  Loss pred: 1.3716; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 1.3259;  Loss pred: 1.3259; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.21s
Epoch 23/1000, LR 0.000290
Train loss: 1.2852;  Loss pred: 1.2852; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000290
Train loss: 1.2520;  Loss pred: 1.2520; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000290
Train loss: 1.2220;  Loss pred: 1.2220; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.18s
Epoch 26/1000, LR 0.000290
Train loss: 1.1951;  Loss pred: 1.1951; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.18s
Epoch 27/1000, LR 0.000290
Train loss: 1.1732;  Loss pred: 1.1732; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.18s
Epoch 28/1000, LR 0.000290
Train loss: 1.1508;  Loss pred: 1.1508; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.18s
Epoch 29/1000, LR 0.000290
Train loss: 1.1315;  Loss pred: 1.1315; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 1.1169;  Loss pred: 1.1169; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 1.0987;  Loss pred: 1.0987; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.18s
Epoch 32/1000, LR 0.000290
Train loss: 1.0913;  Loss pred: 1.0913; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.17s
Epoch 33/1000, LR 0.000290
Train loss: 1.0836;  Loss pred: 1.0836; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.19s
Epoch 34/1000, LR 0.000290
Train loss: 1.0730;  Loss pred: 1.0730; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.20s
Epoch 35/1000, LR 0.000290
Train loss: 1.0589;  Loss pred: 1.0589; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 1.0571;  Loss pred: 1.0571; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 1.0468;  Loss pred: 1.0468; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 1.0415;  Loss pred: 1.0415; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 1.0349;  Loss pred: 1.0349; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 1.0291;  Loss pred: 1.0291; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 1.0241;  Loss pred: 1.0241; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5000 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 1.0206;  Loss pred: 1.0206; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5000 time: 0.19s
Epoch 43/1000, LR 0.000289
Train loss: 1.0163;  Loss pred: 1.0163; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 1.0116;  Loss pred: 1.0116; Loss self: 0.0000; time: 0.27s
Val loss: 0.6862 score: 0.5426 time: 0.17s
Test loss: 0.6863 score: 0.5391 time: 0.18s
Epoch 45/1000, LR 0.000289
Train loss: 1.0114;  Loss pred: 1.0114; Loss self: 0.0000; time: 0.27s
Val loss: 0.6856 score: 0.7597 time: 0.18s
Test loss: 0.6858 score: 0.7891 time: 0.18s
Epoch 46/1000, LR 0.000289
Train loss: 1.0068;  Loss pred: 1.0068; Loss self: 0.0000; time: 0.29s
Val loss: 0.6850 score: 0.8682 time: 0.20s
Test loss: 0.6853 score: 0.8672 time: 0.18s
Epoch 47/1000, LR 0.000289
Train loss: 1.0040;  Loss pred: 1.0040; Loss self: 0.0000; time: 0.49s
Val loss: 0.6843 score: 0.8605 time: 0.25s
Test loss: 0.6845 score: 0.9219 time: 0.22s
Epoch 48/1000, LR 0.000289
Train loss: 1.0005;  Loss pred: 1.0005; Loss self: 0.0000; time: 0.49s
Val loss: 0.6834 score: 0.8605 time: 0.23s
Test loss: 0.6836 score: 0.8672 time: 0.22s
Epoch 49/1000, LR 0.000289
Train loss: 0.9973;  Loss pred: 0.9973; Loss self: 0.0000; time: 0.28s
Val loss: 0.6826 score: 0.8217 time: 0.17s
Test loss: 0.6827 score: 0.8359 time: 0.17s
Epoch 50/1000, LR 0.000289
Train loss: 0.9942;  Loss pred: 0.9942; Loss self: 0.0000; time: 0.28s
Val loss: 0.6819 score: 0.7132 time: 0.25s
Test loss: 0.6820 score: 0.6797 time: 0.22s
Epoch 51/1000, LR 0.000289
Train loss: 0.9916;  Loss pred: 0.9916; Loss self: 0.0000; time: 0.49s
Val loss: 0.6811 score: 0.6899 time: 0.25s
Test loss: 0.6812 score: 0.6719 time: 0.22s
Epoch 52/1000, LR 0.000289
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.50s
Val loss: 0.6802 score: 0.7519 time: 0.25s
Test loss: 0.6803 score: 0.7812 time: 0.22s
Epoch 53/1000, LR 0.000289
Train loss: 0.9882;  Loss pred: 0.9882; Loss self: 0.0000; time: 0.49s
Val loss: 0.6793 score: 0.8217 time: 0.25s
Test loss: 0.6794 score: 0.8359 time: 0.21s
Epoch 54/1000, LR 0.000289
Train loss: 0.9842;  Loss pred: 0.9842; Loss self: 0.0000; time: 0.29s
Val loss: 0.6784 score: 0.8062 time: 0.17s
Test loss: 0.6785 score: 0.8359 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 0.9828;  Loss pred: 0.9828; Loss self: 0.0000; time: 0.28s
Val loss: 0.6776 score: 0.7442 time: 0.17s
Test loss: 0.6776 score: 0.7656 time: 0.17s
Epoch 56/1000, LR 0.000289
Train loss: 0.9794;  Loss pred: 0.9794; Loss self: 0.0000; time: 0.27s
Val loss: 0.6769 score: 0.6357 time: 0.17s
Test loss: 0.6768 score: 0.6016 time: 0.18s
Epoch 57/1000, LR 0.000288
Train loss: 0.9771;  Loss pred: 0.9771; Loss self: 0.0000; time: 0.42s
Val loss: 0.6765 score: 0.5271 time: 0.25s
Test loss: 0.6763 score: 0.5312 time: 0.22s
Epoch 58/1000, LR 0.000288
Train loss: 0.9762;  Loss pred: 0.9762; Loss self: 0.0000; time: 0.48s
Val loss: 0.6763 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.5000 time: 0.21s
Epoch 59/1000, LR 0.000288
Train loss: 0.9767;  Loss pred: 0.9767; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6759 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.5000 time: 0.21s
Epoch 60/1000, LR 0.000288
Train loss: 0.9734;  Loss pred: 0.9734; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.5000 time: 0.17s
Epoch 61/1000, LR 0.000288
Train loss: 0.9735;  Loss pred: 0.9735; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6743 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6738 score: 0.5000 time: 0.17s
Epoch 62/1000, LR 0.000288
Train loss: 0.9701;  Loss pred: 0.9701; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6721 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.5000 time: 0.16s
Epoch 63/1000, LR 0.000288
Train loss: 0.9676;  Loss pred: 0.9676; Loss self: 0.0000; time: 0.28s
Val loss: 0.6699 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6692 score: 0.5000 time: 0.17s
Epoch 64/1000, LR 0.000288
Train loss: 0.9655;  Loss pred: 0.9655; Loss self: 0.0000; time: 0.29s
Val loss: 0.6675 score: 0.5194 time: 0.24s
Test loss: 0.6667 score: 0.5156 time: 0.22s
Epoch 65/1000, LR 0.000288
Train loss: 0.9607;  Loss pred: 0.9607; Loss self: 0.0000; time: 0.48s
Val loss: 0.6643 score: 0.6202 time: 0.25s
Test loss: 0.6636 score: 0.5938 time: 0.23s
Epoch 66/1000, LR 0.000288
Train loss: 0.9584;  Loss pred: 0.9584; Loss self: 0.0000; time: 0.29s
Val loss: 0.6615 score: 0.7674 time: 0.17s
Test loss: 0.6608 score: 0.7891 time: 0.17s
Epoch 67/1000, LR 0.000288
Train loss: 0.9562;  Loss pred: 0.9562; Loss self: 0.0000; time: 0.27s
Val loss: 0.6595 score: 0.8527 time: 0.17s
Test loss: 0.6591 score: 0.8594 time: 0.17s
Epoch 68/1000, LR 0.000288
Train loss: 0.9554;  Loss pred: 0.9554; Loss self: 0.0000; time: 0.27s
Val loss: 0.6585 score: 0.8682 time: 0.17s
Test loss: 0.6581 score: 0.9141 time: 0.17s
Epoch 69/1000, LR 0.000288
Train loss: 0.9535;  Loss pred: 0.9535; Loss self: 0.0000; time: 0.27s
Val loss: 0.6565 score: 0.8682 time: 0.17s
Test loss: 0.6560 score: 0.9141 time: 0.17s
Epoch 70/1000, LR 0.000287
Train loss: 0.9512;  Loss pred: 0.9512; Loss self: 0.0000; time: 0.29s
Val loss: 0.6539 score: 0.8605 time: 0.17s
Test loss: 0.6533 score: 0.8984 time: 0.21s
Epoch 71/1000, LR 0.000287
Train loss: 0.9480;  Loss pred: 0.9480; Loss self: 0.0000; time: 0.31s
Val loss: 0.6513 score: 0.8605 time: 0.17s
Test loss: 0.6505 score: 0.8750 time: 0.17s
Epoch 72/1000, LR 0.000287
Train loss: 0.9455;  Loss pred: 0.9455; Loss self: 0.0000; time: 0.27s
Val loss: 0.6484 score: 0.8527 time: 0.17s
Test loss: 0.6474 score: 0.8594 time: 0.17s
Epoch 73/1000, LR 0.000287
Train loss: 0.9417;  Loss pred: 0.9417; Loss self: 0.0000; time: 0.27s
Val loss: 0.6457 score: 0.8450 time: 0.16s
Test loss: 0.6445 score: 0.8594 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9388;  Loss pred: 0.9388; Loss self: 0.0000; time: 0.27s
Val loss: 0.6429 score: 0.8450 time: 0.17s
Test loss: 0.6416 score: 0.8594 time: 0.17s
Epoch 75/1000, LR 0.000287
Train loss: 0.9370;  Loss pred: 0.9370; Loss self: 0.0000; time: 0.27s
Val loss: 0.6403 score: 0.8372 time: 0.16s
Test loss: 0.6388 score: 0.8594 time: 0.20s
Epoch 76/1000, LR 0.000287
Train loss: 0.9334;  Loss pred: 0.9334; Loss self: 0.0000; time: 0.28s
Val loss: 0.6384 score: 0.8140 time: 0.25s
Test loss: 0.6366 score: 0.8281 time: 0.17s
Epoch 77/1000, LR 0.000287
Train loss: 0.9310;  Loss pred: 0.9310; Loss self: 0.0000; time: 0.27s
Val loss: 0.6376 score: 0.7442 time: 0.17s
Test loss: 0.6356 score: 0.7812 time: 0.17s
Epoch 78/1000, LR 0.000287
Train loss: 0.9291;  Loss pred: 0.9291; Loss self: 0.0000; time: 0.27s
Val loss: 0.6371 score: 0.7209 time: 0.17s
Test loss: 0.6349 score: 0.6953 time: 0.17s
Epoch 79/1000, LR 0.000287
Train loss: 0.9289;  Loss pred: 0.9289; Loss self: 0.0000; time: 0.27s
Val loss: 0.6364 score: 0.6512 time: 0.18s
Test loss: 0.6340 score: 0.6641 time: 0.25s
Epoch 80/1000, LR 0.000287
Train loss: 0.9275;  Loss pred: 0.9275; Loss self: 0.0000; time: 0.42s
Val loss: 0.6332 score: 0.6589 time: 0.26s
Test loss: 0.6306 score: 0.6719 time: 0.22s
Epoch 81/1000, LR 0.000286
Train loss: 0.9230;  Loss pred: 0.9230; Loss self: 0.0000; time: 0.38s
Val loss: 0.6265 score: 0.7287 time: 0.27s
Test loss: 0.6238 score: 0.7344 time: 0.17s
Epoch 82/1000, LR 0.000286
Train loss: 0.9173;  Loss pred: 0.9173; Loss self: 0.0000; time: 0.28s
Val loss: 0.6207 score: 0.7519 time: 0.18s
Test loss: 0.6180 score: 0.7812 time: 0.17s
Epoch 83/1000, LR 0.000286
Train loss: 0.9125;  Loss pred: 0.9125; Loss self: 0.0000; time: 0.29s
Val loss: 0.6147 score: 0.8140 time: 0.18s
Test loss: 0.6120 score: 0.8359 time: 0.18s
Epoch 84/1000, LR 0.000286
Train loss: 0.9078;  Loss pred: 0.9078; Loss self: 0.0000; time: 0.36s
Val loss: 0.6088 score: 0.8450 time: 0.17s
Test loss: 0.6060 score: 0.8594 time: 0.17s
Epoch 85/1000, LR 0.000286
Train loss: 0.9043;  Loss pred: 0.9043; Loss self: 0.0000; time: 0.27s
Val loss: 0.6052 score: 0.8605 time: 0.21s
Test loss: 0.6025 score: 0.8750 time: 0.17s
Epoch 86/1000, LR 0.000286
Train loss: 0.9019;  Loss pred: 0.9019; Loss self: 0.0000; time: 0.38s
Val loss: 0.6031 score: 0.8760 time: 0.21s
Test loss: 0.6007 score: 0.9297 time: 0.17s
Epoch 87/1000, LR 0.000286
Train loss: 0.8994;  Loss pred: 0.8994; Loss self: 0.0000; time: 0.28s
Val loss: 0.5993 score: 0.8760 time: 0.18s
Test loss: 0.5968 score: 0.9297 time: 0.18s
Epoch 88/1000, LR 0.000286
Train loss: 0.8964;  Loss pred: 0.8964; Loss self: 0.0000; time: 0.27s
Val loss: 0.5945 score: 0.8760 time: 0.17s
Test loss: 0.5916 score: 0.9297 time: 0.17s
Epoch 89/1000, LR 0.000286
Train loss: 0.8910;  Loss pred: 0.8910; Loss self: 0.0000; time: 0.28s
Val loss: 0.5883 score: 0.8605 time: 0.17s
Test loss: 0.5847 score: 0.8906 time: 0.17s
Epoch 90/1000, LR 0.000285
Train loss: 0.8859;  Loss pred: 0.8859; Loss self: 0.0000; time: 0.27s
Val loss: 0.5836 score: 0.8527 time: 0.18s
Test loss: 0.5797 score: 0.8594 time: 0.18s
Epoch 91/1000, LR 0.000285
Train loss: 0.8815;  Loss pred: 0.8815; Loss self: 0.0000; time: 0.42s
Val loss: 0.5799 score: 0.8450 time: 0.26s
Test loss: 0.5756 score: 0.8594 time: 0.22s
Epoch 92/1000, LR 0.000285
Train loss: 0.8765;  Loss pred: 0.8765; Loss self: 0.0000; time: 0.42s
Val loss: 0.5751 score: 0.8527 time: 0.22s
Test loss: 0.5705 score: 0.8594 time: 0.17s
Epoch 93/1000, LR 0.000285
Train loss: 0.8723;  Loss pred: 0.8723; Loss self: 0.0000; time: 0.28s
Val loss: 0.5693 score: 0.8605 time: 0.17s
Test loss: 0.5645 score: 0.8672 time: 0.17s
Epoch 94/1000, LR 0.000285
Train loss: 0.8691;  Loss pred: 0.8691; Loss self: 0.0000; time: 0.28s
Val loss: 0.5653 score: 0.8682 time: 0.17s
Test loss: 0.5604 score: 0.8984 time: 0.17s
Epoch 95/1000, LR 0.000285
Train loss: 0.8657;  Loss pred: 0.8657; Loss self: 0.0000; time: 0.27s
Val loss: 0.5614 score: 0.8837 time: 0.17s
Test loss: 0.5562 score: 0.9062 time: 0.19s
Epoch 96/1000, LR 0.000285
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.41s
Val loss: 0.5573 score: 0.8760 time: 0.25s
Test loss: 0.5520 score: 0.9297 time: 0.22s
Epoch 97/1000, LR 0.000285
Train loss: 0.8598;  Loss pred: 0.8598; Loss self: 0.0000; time: 0.48s
Val loss: 0.5528 score: 0.8837 time: 0.26s
Test loss: 0.5473 score: 0.9297 time: 0.22s
Epoch 98/1000, LR 0.000285
Train loss: 0.8555;  Loss pred: 0.8555; Loss self: 0.0000; time: 0.49s
Val loss: 0.5468 score: 0.8760 time: 0.20s
Test loss: 0.5408 score: 0.9297 time: 0.17s
Epoch 99/1000, LR 0.000284
Train loss: 0.8500;  Loss pred: 0.8500; Loss self: 0.0000; time: 0.27s
Val loss: 0.5400 score: 0.8837 time: 0.17s
Test loss: 0.5334 score: 0.9062 time: 0.17s
Epoch 100/1000, LR 0.000284
Train loss: 0.8444;  Loss pred: 0.8444; Loss self: 0.0000; time: 0.27s
Val loss: 0.5341 score: 0.8682 time: 0.17s
Test loss: 0.5269 score: 0.8984 time: 0.17s
Epoch 101/1000, LR 0.000284
Train loss: 0.8387;  Loss pred: 0.8387; Loss self: 0.0000; time: 0.27s
Val loss: 0.5301 score: 0.8605 time: 0.16s
Test loss: 0.5220 score: 0.8672 time: 0.16s
Epoch 102/1000, LR 0.000284
Train loss: 0.8351;  Loss pred: 0.8351; Loss self: 0.0000; time: 0.27s
Val loss: 0.5309 score: 0.8527 time: 0.17s
Test loss: 0.5219 score: 0.8594 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000284
Train loss: 0.8332;  Loss pred: 0.8332; Loss self: 0.0000; time: 0.27s
Val loss: 0.5358 score: 0.8372 time: 0.17s
Test loss: 0.5261 score: 0.8516 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 104/1000, LR 0.000284
Train loss: 0.8342;  Loss pred: 0.8342; Loss self: 0.0000; time: 0.30s
Val loss: 0.5391 score: 0.8217 time: 0.17s
Test loss: 0.5289 score: 0.8359 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 105/1000, LR 0.000284
Train loss: 0.8339;  Loss pred: 0.8339; Loss self: 0.0000; time: 0.28s
Val loss: 0.5389 score: 0.8140 time: 0.18s
Test loss: 0.5281 score: 0.8281 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 106/1000, LR 0.000283
Train loss: 0.8322;  Loss pred: 0.8322; Loss self: 0.0000; time: 0.28s
Val loss: 0.5335 score: 0.8140 time: 0.17s
Test loss: 0.5221 score: 0.8281 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 107/1000, LR 0.000283
Train loss: 0.8271;  Loss pred: 0.8271; Loss self: 0.0000; time: 0.27s
Val loss: 0.5196 score: 0.8372 time: 0.17s
Test loss: 0.5075 score: 0.8516 time: 0.17s
Epoch 108/1000, LR 0.000283
Train loss: 0.8138;  Loss pred: 0.8138; Loss self: 0.0000; time: 0.28s
Val loss: 0.5016 score: 0.8527 time: 0.17s
Test loss: 0.4887 score: 0.8594 time: 0.17s
Epoch 109/1000, LR 0.000283
Train loss: 0.8046;  Loss pred: 0.8046; Loss self: 0.0000; time: 0.28s
Val loss: 0.4906 score: 0.8605 time: 0.17s
Test loss: 0.4774 score: 0.8672 time: 0.17s
Epoch 110/1000, LR 0.000283
Train loss: 0.7980;  Loss pred: 0.7980; Loss self: 0.0000; time: 0.28s
Val loss: 0.4827 score: 0.8682 time: 0.17s
Test loss: 0.4691 score: 0.8828 time: 0.17s
Epoch 111/1000, LR 0.000283
Train loss: 0.7925;  Loss pred: 0.7925; Loss self: 0.0000; time: 0.27s
Val loss: 0.4762 score: 0.8760 time: 0.19s
Test loss: 0.4624 score: 0.9062 time: 0.17s
Epoch 112/1000, LR 0.000283
Train loss: 0.7875;  Loss pred: 0.7875; Loss self: 0.0000; time: 0.41s
Val loss: 0.4713 score: 0.8837 time: 0.17s
Test loss: 0.4570 score: 0.9297 time: 0.17s
Epoch 113/1000, LR 0.000282
Train loss: 0.7837;  Loss pred: 0.7837; Loss self: 0.0000; time: 0.28s
Val loss: 0.4659 score: 0.8837 time: 0.17s
Test loss: 0.4510 score: 0.9297 time: 0.17s
Epoch 114/1000, LR 0.000282
Train loss: 0.7782;  Loss pred: 0.7782; Loss self: 0.0000; time: 0.28s
Val loss: 0.4606 score: 0.8837 time: 0.17s
Test loss: 0.4451 score: 0.9297 time: 0.17s
Epoch 115/1000, LR 0.000282
Train loss: 0.7738;  Loss pred: 0.7738; Loss self: 0.0000; time: 0.28s
Val loss: 0.4551 score: 0.8760 time: 0.18s
Test loss: 0.4385 score: 0.9062 time: 0.17s
Epoch 116/1000, LR 0.000282
Train loss: 0.7677;  Loss pred: 0.7677; Loss self: 0.0000; time: 0.29s
Val loss: 0.4501 score: 0.8760 time: 0.25s
Test loss: 0.4327 score: 0.9062 time: 0.21s
Epoch 117/1000, LR 0.000282
Train loss: 0.7615;  Loss pred: 0.7615; Loss self: 0.0000; time: 0.42s
Val loss: 0.4442 score: 0.8760 time: 0.23s
Test loss: 0.4262 score: 0.9062 time: 0.17s
Epoch 118/1000, LR 0.000282
Train loss: 0.7583;  Loss pred: 0.7583; Loss self: 0.0000; time: 0.28s
Val loss: 0.4390 score: 0.8760 time: 0.17s
Test loss: 0.4204 score: 0.9297 time: 0.17s
Epoch 119/1000, LR 0.000282
Train loss: 0.7539;  Loss pred: 0.7539; Loss self: 0.0000; time: 0.28s
Val loss: 0.4343 score: 0.8760 time: 0.17s
Test loss: 0.4150 score: 0.9297 time: 0.17s
Epoch 120/1000, LR 0.000281
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.28s
Val loss: 0.4284 score: 0.8760 time: 0.17s
Test loss: 0.4082 score: 0.9297 time: 0.17s
Epoch 121/1000, LR 0.000281
Train loss: 0.7428;  Loss pred: 0.7428; Loss self: 0.0000; time: 0.28s
Val loss: 0.4233 score: 0.8760 time: 0.17s
Test loss: 0.4023 score: 0.9297 time: 0.17s
Epoch 122/1000, LR 0.000281
Train loss: 0.7410;  Loss pred: 0.7410; Loss self: 0.0000; time: 0.28s
Val loss: 0.4191 score: 0.8760 time: 0.17s
Test loss: 0.3976 score: 0.9297 time: 0.17s
Epoch 123/1000, LR 0.000281
Train loss: 0.7342;  Loss pred: 0.7342; Loss self: 0.0000; time: 0.30s
Val loss: 0.4141 score: 0.8760 time: 0.17s
Test loss: 0.3919 score: 0.9297 time: 0.17s
Epoch 124/1000, LR 0.000281
Train loss: 0.7319;  Loss pred: 0.7319; Loss self: 0.0000; time: 0.37s
Val loss: 0.4094 score: 0.8760 time: 0.17s
Test loss: 0.3864 score: 0.9297 time: 0.17s
Epoch 125/1000, LR 0.000281
Train loss: 0.7250;  Loss pred: 0.7250; Loss self: 0.0000; time: 0.27s
Val loss: 0.4053 score: 0.8760 time: 0.17s
Test loss: 0.3819 score: 0.9297 time: 0.17s
Epoch 126/1000, LR 0.000280
Train loss: 0.7225;  Loss pred: 0.7225; Loss self: 0.0000; time: 0.27s
Val loss: 0.4022 score: 0.8837 time: 0.17s
Test loss: 0.3783 score: 0.9297 time: 0.17s
Epoch 127/1000, LR 0.000280
Train loss: 0.7192;  Loss pred: 0.7192; Loss self: 0.0000; time: 0.31s
Val loss: 0.3974 score: 0.8837 time: 0.21s
Test loss: 0.3725 score: 0.9297 time: 0.21s
Epoch 128/1000, LR 0.000280
Train loss: 0.7144;  Loss pred: 0.7144; Loss self: 0.0000; time: 0.36s
Val loss: 0.3929 score: 0.8837 time: 0.17s
Test loss: 0.3672 score: 0.9297 time: 0.17s
Epoch 129/1000, LR 0.000280
Train loss: 0.7102;  Loss pred: 0.7102; Loss self: 0.0000; time: 0.27s
Val loss: 0.3888 score: 0.8837 time: 0.17s
Test loss: 0.3621 score: 0.9297 time: 0.17s
Epoch 130/1000, LR 0.000280
Train loss: 0.7037;  Loss pred: 0.7037; Loss self: 0.0000; time: 0.27s
Val loss: 0.3847 score: 0.8837 time: 0.17s
Test loss: 0.3571 score: 0.9297 time: 0.17s
Epoch 131/1000, LR 0.000280
Train loss: 0.7030;  Loss pred: 0.7030; Loss self: 0.0000; time: 0.28s
Val loss: 0.3809 score: 0.8837 time: 0.17s
Test loss: 0.3523 score: 0.9297 time: 0.17s
Epoch 132/1000, LR 0.000279
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.28s
Val loss: 0.3771 score: 0.8760 time: 0.17s
Test loss: 0.3475 score: 0.9297 time: 0.17s
Epoch 133/1000, LR 0.000279
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.29s
Val loss: 0.3739 score: 0.8837 time: 0.18s
Test loss: 0.3432 score: 0.9219 time: 0.18s
Epoch 134/1000, LR 0.000279
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.42s
Val loss: 0.3720 score: 0.8837 time: 0.19s
Test loss: 0.3401 score: 0.9062 time: 0.19s
Epoch 135/1000, LR 0.000279
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.28s
Val loss: 0.3753 score: 0.8760 time: 0.18s
Test loss: 0.3422 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 136/1000, LR 0.000279
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.29s
Val loss: 0.3853 score: 0.8682 time: 0.18s
Test loss: 0.3521 score: 0.8672 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 137/1000, LR 0.000279
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.28s
Val loss: 0.3934 score: 0.8527 time: 0.17s
Test loss: 0.3603 score: 0.8672 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 138/1000, LR 0.000278
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.30s
Val loss: 0.3952 score: 0.8527 time: 0.17s
Test loss: 0.3617 score: 0.8672 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 139/1000, LR 0.000278
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.48s
Val loss: 0.3891 score: 0.8605 time: 0.25s
Test loss: 0.3544 score: 0.8672 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 140/1000, LR 0.000278
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.47s
Val loss: 0.3775 score: 0.8682 time: 0.18s
Test loss: 0.3409 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 141/1000, LR 0.000278
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.28s
Val loss: 0.3681 score: 0.8760 time: 0.17s
Test loss: 0.3300 score: 0.8828 time: 0.16s
Epoch 142/1000, LR 0.000278
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.27s
Val loss: 0.3630 score: 0.8760 time: 0.17s
Test loss: 0.3239 score: 0.8984 time: 0.17s
Epoch 143/1000, LR 0.000277
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.28s
Val loss: 0.3617 score: 0.8760 time: 0.17s
Test loss: 0.3217 score: 0.8984 time: 0.17s
Epoch 144/1000, LR 0.000277
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.30s
Val loss: 0.3569 score: 0.8760 time: 0.18s
Test loss: 0.3160 score: 0.9062 time: 0.21s
Epoch 145/1000, LR 0.000277
Train loss: 0.6566;  Loss pred: 0.6566; Loss self: 0.0000; time: 0.49s
Val loss: 0.3525 score: 0.8760 time: 0.25s
Test loss: 0.3106 score: 0.9062 time: 0.22s
Epoch 146/1000, LR 0.000277
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.33s
Val loss: 0.3488 score: 0.8760 time: 0.17s
Test loss: 0.3059 score: 0.9062 time: 0.17s
Epoch 147/1000, LR 0.000277
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.28s
Val loss: 0.3467 score: 0.8760 time: 0.17s
Test loss: 0.3029 score: 0.9062 time: 0.17s
Epoch 148/1000, LR 0.000277
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.27s
Val loss: 0.3463 score: 0.8760 time: 0.17s
Test loss: 0.3015 score: 0.9062 time: 0.17s
Epoch 149/1000, LR 0.000276
Train loss: 0.6466;  Loss pred: 0.6466; Loss self: 0.0000; time: 0.27s
Val loss: 0.3459 score: 0.8760 time: 0.17s
Test loss: 0.3001 score: 0.9062 time: 0.17s
Epoch 150/1000, LR 0.000276
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.27s
Val loss: 0.3427 score: 0.8760 time: 0.20s
Test loss: 0.2960 score: 0.9062 time: 0.17s
Epoch 151/1000, LR 0.000276
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.37s
Val loss: 0.3378 score: 0.8915 time: 0.17s
Test loss: 0.2901 score: 0.9219 time: 0.17s
Epoch 152/1000, LR 0.000276
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.27s
Val loss: 0.3350 score: 0.8837 time: 0.17s
Test loss: 0.2868 score: 0.9375 time: 0.17s
Epoch 153/1000, LR 0.000276
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.27s
Val loss: 0.3367 score: 0.8915 time: 0.17s
Test loss: 0.2891 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 154/1000, LR 0.000275
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.27s
Val loss: 0.3377 score: 0.8915 time: 0.17s
Test loss: 0.2899 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 155/1000, LR 0.000275
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 0.28s
Val loss: 0.3371 score: 0.8837 time: 0.18s
Test loss: 0.2889 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 156/1000, LR 0.000275
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.30s
Val loss: 0.3368 score: 0.8837 time: 0.19s
Test loss: 0.2881 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 157/1000, LR 0.000275
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.35s
Val loss: 0.3324 score: 0.8915 time: 0.17s
Test loss: 0.2821 score: 0.9297 time: 0.17s
Epoch 158/1000, LR 0.000275
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.28s
Val loss: 0.3300 score: 0.8837 time: 0.17s
Test loss: 0.2782 score: 0.9297 time: 0.17s
Epoch 159/1000, LR 0.000274
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.27s
Val loss: 0.3386 score: 0.8760 time: 0.17s
Test loss: 0.2869 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 160/1000, LR 0.000274
Train loss: 0.6231;  Loss pred: 0.6231; Loss self: 0.0000; time: 0.27s
Val loss: 0.3502 score: 0.8760 time: 0.18s
Test loss: 0.2993 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 161/1000, LR 0.000274
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.27s
Val loss: 0.3562 score: 0.8760 time: 0.17s
Test loss: 0.3053 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 162/1000, LR 0.000274
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.32s
Val loss: 0.3510 score: 0.8760 time: 0.17s
Test loss: 0.2992 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 163/1000, LR 0.000273
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.35s
Val loss: 0.3366 score: 0.8760 time: 0.17s
Test loss: 0.2825 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.28s
Val loss: 0.3262 score: 0.8837 time: 0.17s
Test loss: 0.2702 score: 0.9297 time: 0.17s
Epoch 165/1000, LR 0.000273
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.28s
Val loss: 0.3247 score: 0.8915 time: 0.19s
Test loss: 0.2683 score: 0.9375 time: 0.18s
Epoch 166/1000, LR 0.000273
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.28s
Val loss: 0.3252 score: 0.8915 time: 0.17s
Test loss: 0.2686 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 167/1000, LR 0.000273
Train loss: 0.6179;  Loss pred: 0.6179; Loss self: 0.0000; time: 0.27s
Val loss: 0.3235 score: 0.8915 time: 0.18s
Test loss: 0.2657 score: 0.9375 time: 0.20s
Epoch 168/1000, LR 0.000272
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.38s
Val loss: 0.3245 score: 0.8915 time: 0.23s
Test loss: 0.2658 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 169/1000, LR 0.000272
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.28s
Val loss: 0.3285 score: 0.8915 time: 0.17s
Test loss: 0.2698 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 170/1000, LR 0.000272
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.28s
Val loss: 0.3302 score: 0.8915 time: 0.24s
Test loss: 0.2711 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 171/1000, LR 0.000272
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.28s
Val loss: 0.3306 score: 0.8915 time: 0.18s
Test loss: 0.2710 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 172/1000, LR 0.000271
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.50s
Val loss: 0.3308 score: 0.8837 time: 0.33s
Test loss: 0.2705 score: 0.9141 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 173/1000, LR 0.000271
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.49s
Val loss: 0.3268 score: 0.8915 time: 0.26s
Test loss: 0.2651 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 174/1000, LR 0.000271
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.47s
Val loss: 0.3210 score: 0.8837 time: 0.24s
Test loss: 0.2578 score: 0.9375 time: 0.18s
Epoch 175/1000, LR 0.000271
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.28s
Val loss: 0.3199 score: 0.8915 time: 0.18s
Test loss: 0.2562 score: 0.9453 time: 0.17s
Epoch 176/1000, LR 0.000271
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.29s
Val loss: 0.3199 score: 0.8915 time: 0.18s
Test loss: 0.2557 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 177/1000, LR 0.000270
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 0.28s
Val loss: 0.3199 score: 0.8915 time: 0.18s
Test loss: 0.2549 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 178/1000, LR 0.000270
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.28s
Val loss: 0.3196 score: 0.8915 time: 0.18s
Test loss: 0.2536 score: 0.9375 time: 0.18s
Epoch 179/1000, LR 0.000270
Train loss: 0.5967;  Loss pred: 0.5967; Loss self: 0.0000; time: 0.28s
Val loss: 0.3193 score: 0.8915 time: 0.19s
Test loss: 0.2524 score: 0.9453 time: 0.17s
Epoch 180/1000, LR 0.000270
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.37s
Val loss: 0.3198 score: 0.8915 time: 0.20s
Test loss: 0.2521 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 181/1000, LR 0.000269
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.27s
Val loss: 0.3213 score: 0.8837 time: 0.17s
Test loss: 0.2532 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 182/1000, LR 0.000269
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.27s
Val loss: 0.3256 score: 0.8915 time: 0.17s
Test loss: 0.2576 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 183/1000, LR 0.000269
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.27s
Val loss: 0.3301 score: 0.8915 time: 0.17s
Test loss: 0.2624 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 184/1000, LR 0.000269
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.27s
Val loss: 0.3348 score: 0.8837 time: 0.17s
Test loss: 0.2674 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 185/1000, LR 0.000268
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.28s
Val loss: 0.3376 score: 0.8837 time: 0.19s
Test loss: 0.2699 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 186/1000, LR 0.000268
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.27s
Val loss: 0.3377 score: 0.8837 time: 0.16s
Test loss: 0.2692 score: 0.9062 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 187/1000, LR 0.000268
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.28s
Val loss: 0.3365 score: 0.8837 time: 0.23s
Test loss: 0.2672 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.28s
Val loss: 0.3335 score: 0.8915 time: 0.18s
Test loss: 0.2629 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.28s
Val loss: 0.3314 score: 0.8915 time: 0.18s
Test loss: 0.2596 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 0.27s
Val loss: 0.3309 score: 0.8915 time: 0.16s
Test loss: 0.2585 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 0.27s
Val loss: 0.3305 score: 0.8915 time: 0.17s
Test loss: 0.2573 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 192/1000, LR 0.000267
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.27s
Val loss: 0.3293 score: 0.8915 time: 0.23s
Test loss: 0.2551 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 193/1000, LR 0.000266
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.37s
Val loss: 0.3282 score: 0.8915 time: 0.18s
Test loss: 0.2529 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 194/1000, LR 0.000266
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 0.28s
Val loss: 0.3278 score: 0.8915 time: 0.18s
Test loss: 0.2517 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 195/1000, LR 0.000266
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.28s
Val loss: 0.3306 score: 0.8915 time: 0.18s
Test loss: 0.2545 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 196/1000, LR 0.000266
Train loss: 0.5796;  Loss pred: 0.5796; Loss self: 0.0000; time: 0.28s
Val loss: 0.3391 score: 0.8837 time: 0.18s
Test loss: 0.2645 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 197/1000, LR 0.000265
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.33s
Val loss: 0.3478 score: 0.8837 time: 0.18s
Test loss: 0.2743 score: 0.9062 time: 0.23s
     INFO: Early stopping counter 18 of 20
Epoch 198/1000, LR 0.000265
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.29s
Val loss: 0.3537 score: 0.8760 time: 0.18s
Test loss: 0.2809 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 199/1000, LR 0.000265
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 0.27s
Val loss: 0.3545 score: 0.8760 time: 0.17s
Test loss: 0.2814 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 178,   Train_Loss: 0.5967,   Val_Loss: 0.3193,   Val_Precision: 0.9322,   Val_Recall: 0.8462,   Val_accuracy: 0.8871,   Val_Score: 0.8915,   Val_Loss: 0.3193,   Test_Precision: 1.0000,   Test_Recall: 0.8906,   Test_accuracy: 0.9421,   Test_Score: 0.9453,   Test_loss: 0.2524


[0.17553729191422462, 0.17636333708651364, 0.17716077016666532, 0.2639829209074378, 0.1783866120968014, 0.18214822001755238, 0.19493592693470418, 0.17595083895139396, 0.3294711129274219, 0.2708030780777335, 0.27246263809502125, 0.17976868501864374, 0.19387241709046066, 0.18678679503500462, 0.1849911310710013, 0.1952481889165938, 0.19207121594808996, 0.187116920016706, 0.22203910420648754, 0.18039479316212237, 0.17881705099716783, 0.17879667412489653, 0.18621564097702503, 0.17370856902562082, 0.17674741009250283, 0.19113518693484366, 0.18099136580713093, 0.2696086319629103, 0.19513565092347562, 0.1962961859535426, 0.19543089577928185, 0.1880191038362682, 0.1952838411089033, 0.19462593900971115, 0.19527766713872552, 0.19017368904314935, 0.18919846694916487, 0.20559936994686723, 0.26660167099907994, 0.27408914896659553, 0.193543063942343, 0.18811267614364624, 0.18419204582460225, 0.18594341399148107, 0.20090563781559467, 0.17734047188423574, 0.18097334099002182, 0.1793673629872501, 0.1910700029693544, 0.25825565797276795, 0.17306552990339696, 0.18774029309861362, 0.19375561783090234, 0.17612216505222023, 0.1730023710988462, 0.18816776596941054, 0.2434256940614432, 0.19006537809036672, 0.18164691887795925, 0.17743582697585225, 0.21789261605590582, 0.1867034260649234, 0.1904192331712693, 0.24072615103796124, 0.18368557188659906, 0.18431883794255555, 0.18012804491445422, 0.18502118694595993, 0.17942086583934724, 0.18469001003541052, 0.2635928839445114, 0.18782815895974636, 0.17860809504054487, 0.1817601011134684, 0.18327923491597176, 0.17584941000677645, 0.1824974159244448, 0.17552838777191937, 0.17139540892094374, 0.17944634798914194, 0.18365456815809011, 0.215807368978858, 0.1819220611359924, 0.1812916691415012, 0.2590104730334133, 0.18268886511214077, 0.17967040999792516, 0.1919750429224223, 0.180240822955966, 0.19123604404740036, 0.18294834694825113, 0.17649409407749772, 0.17618271405808628, 0.17568784696049988, 0.18142522894777358, 0.17980714910663664, 0.1787772229872644, 0.17695956793613732, 0.17462780186906457, 0.18039013701491058, 0.25538640399463475, 0.17624633479863405, 0.17694320296868682, 0.18305406998842955, 0.18163503101095557, 0.18172453599981964, 0.1969627591315657, 0.2581255428958684, 0.20949939009733498, 0.18680954002775252, 0.178399563068524, 0.1869447010103613, 0.18337008706294, 0.1823141749482602, 0.18597275693900883, 0.18686826596967876, 0.1832911951933056, 0.18077533296309412, 0.18270786199718714, 0.2378831380046904, 0.17629412305541337, 0.17856124509125948, 0.1787967428099364, 0.17993332794867456, 0.26006348710507154, 0.21152324811555445, 0.17604209599085152, 0.17809935589320958, 0.1814059680327773, 0.17561236815527081, 0.17786356480792165, 0.22196103795431554, 0.1958942108321935, 0.19468972203321755, 0.19518160610459745, 0.22323607420548797, 0.2710570970084518, 0.1835626158863306, 0.18445413000881672, 0.1864522930700332, 0.18714952003210783, 0.17613570601679385, 0.17557766288518906, 0.2644621569197625, 0.17550530191510916, 0.17424043989740312, 0.17179557611234486, 0.17350164707750082, 0.1776442441623658, 0.18168719997629523, 0.175346887903288, 0.17604778916575015, 0.1723386379890144, 0.17233277182094753, 0.17437032097950578, 0.17251294502057135, 0.19839427201077342, 0.1813806879799813, 0.1786529840901494, 0.17970847990363836, 0.17393021401949227, 0.1878040919546038, 0.2343284748494625, 0.16877628304064274, 0.17013867502100766, 0.17221621400676668, 0.171660162974149, 0.17358940397389233, 0.17681810702197254, 0.17388839810155332, 0.1751140400301665, 0.17318343208171427, 0.1724438068922609, 0.17320367507636547, 0.21645463490858674, 0.21509464597329497, 0.17782913986593485, 0.17481272295117378, 0.17871213401667774, 0.1794955290388316, 0.19261459703557193, 0.25579507695510983, 0.17671529413200915, 0.17869133688509464, 0.17184707685373724, 0.205432022921741, 0.17214757902547717, 0.18649212992750108, 0.1736732949502766, 0.17875031591393054, 0.18793575186282396, 0.2636923659592867, 0.17423190479166806, 0.17678671586327255, 0.17108807293698192, 0.1770331549923867, 0.18324562115594745, 0.17552830511704087, 0.18234013998880982, 0.17964884894900024, 0.18417781218886375, 0.1782823409885168, 0.2559902500361204, 0.25811415584757924, 0.2537865361664444, 0.1951947200577706, 0.18412874080240726, 0.18444849597290158, 0.18481083097867668, 0.17963643604889512, 0.17675286694429815, 0.24651980795897543, 0.19336487096734345, 0.19460616097785532, 0.18406679783947766, 0.18862420902587473, 0.16133755585178733, 0.1689393250271678, 0.17049851804040372, 0.16401893901638687, 0.15933460392989218, 0.15879031200893223, 0.16645767888985574, 0.1564930488821119, 0.16197436209768057, 0.160867776023224, 0.15917440806515515, 0.1621789161581546, 0.15865458780899644, 0.15941856801509857, 0.16007544100284576, 0.15681927697733045, 0.16798237594775856, 0.23653064109385014, 0.15760661102831364, 0.15730691188946366, 0.15504202898591757, 0.1573575148358941, 0.20267578004859388, 0.22136965789832175, 0.1620777458883822, 0.15746038011275232, 0.155769880162552, 0.16113354312255979, 0.15675313514657319, 0.16039420012384653, 0.15655022207647562, 0.15953723597340286, 0.229766494827345, 0.2513717550318688, 0.251662079943344, 0.24937247508205473, 0.15840220707468688, 0.15636846981942654, 0.24024192988872528, 0.1599252219311893, 0.166559805162251, 0.16466115787625313, 0.16661451593972743, 0.15319592296145856, 0.16645220294594765, 0.1637984390836209, 0.1631423239596188, 0.1666318508796394, 0.16373152704909444, 0.15578425489366055, 0.2005819589830935, 0.15252652787603438, 0.15467819198966026, 0.16331128403544426, 0.24730439018458128, 0.22439561784267426, 0.1600506859831512, 0.16569499904289842, 0.1700539099983871, 0.1865876349620521, 0.1624334380030632, 0.1604957499075681, 0.17750479117967188, 0.1866347009781748, 0.1669779270887375, 0.18926703999750316, 0.16484184190630913, 0.17156295897439122, 0.16777538019232452, 0.17201139102689922, 0.16359737096354365, 0.16123100486584008, 0.2227923609316349, 0.15533105889335275, 0.15898267598822713, 0.15548861282877624, 0.1672408189624548, 0.16756803495809436, 0.15955345216207206, 0.15705781686119735, 0.15698520001024008, 0.15511055290699005, 0.15653513092547655, 0.15441629686392844, 0.1567270380910486, 0.15510329883545637, 0.16013717814348638, 0.15509692719206214, 0.15535202901810408, 0.23493556794710457, 0.23717625392600894, 0.23706437298096716, 0.15523187909275293, 0.1585094160400331, 0.15894272504374385, 0.16147640580311418, 0.23699160292744637, 0.23715394199825823, 0.21285675815306604, 0.16192506905645132, 0.16362693696282804, 0.16144760604947805, 0.171282951021567, 0.2370734300930053, 0.24093663389794528, 0.24425283796153963, 0.1604821030050516, 0.1570375661831349, 0.216685364022851, 0.16248639510013163, 0.15944701014086604, 0.1589030190370977, 0.23491631005890667, 0.15940516209229827, 0.16529361507855356, 0.15652871294878423, 0.1618442351464182, 0.15737491706386209, 0.2292263861745596, 0.15964548592455685, 0.15667878300882876, 0.16039849515073, 0.15879932302050292, 0.24114905018359423, 0.17533562192693353, 0.1607885940466076, 0.16197124402970076, 0.16007198602892458, 0.16208262904547155, 0.18558678589761257, 0.16015832219272852, 0.15836581215262413, 0.15833199489861727, 0.15802394086495042, 0.15872173383831978, 0.15991552406921983, 0.16454843082465231, 0.16526030912064016, 0.16553608002141118, 0.16337595996446908, 0.16181635903194547, 0.17822289001196623, 0.16126364096999168, 0.24656640086323023, 0.23025543801486492, 0.15649549313820899, 0.19768212107010186, 0.17216698895208538, 0.25141965597867966, 0.23378518992103636, 0.17139640101231635, 0.15784707106649876, 0.1580698899924755, 0.15532809495925903, 0.1543712019920349, 0.15987798292189837, 0.1561730750836432, 0.15837327390909195, 0.16809246595948935, 0.2134001690428704, 0.22238190099596977, 0.15430618496611714, 0.1681612899992615, 0.1668138150125742, 0.16777621116489172, 0.17118927789852023, 0.1661140089854598, 0.1667797730769962, 0.16642876085825264, 0.16570571484044194, 0.16489791101776063, 0.16882871510460973, 0.18910394911654294, 0.16397503693588078, 0.15536753204651177, 0.15390370110981166, 0.17356175789609551, 0.2418094000313431, 0.158978417981416, 0.1584360678680241, 0.15688430797308683, 0.23923370405100286, 0.161085237050429, 0.16252922010608017, 0.15724672097712755, 0.160638706991449, 0.20620269607752562, 0.15820085303857923, 0.16090363496914506, 0.1549569859635085, 0.2348957711365074, 0.23913690308108926, 0.2183021258097142, 0.15905963606201112, 0.15615182905457914, 0.1675257261376828, 0.1653585429303348, 0.24329606210812926, 0.1710131319705397, 0.16815753118135035, 0.18204326392151415, 0.24320298619568348, 0.17072186199948192, 0.16921365214511752, 0.16807392798364162, 0.16890555410645902, 0.1651548941154033, 0.16247564204968512, 0.15544680296443403, 0.155862077139318, 0.15429115411825478, 0.1612746410537511, 0.20173450792208314, 0.15507520409300923, 0.15677398489788175, 0.15418122499249876, 0.15676937997341156, 0.1571802720427513, 0.21666875714436173, 0.15445199911482632, 0.15631968807429075, 0.15395450312644243, 0.15545983286574483, 0.15587434195913374, 0.1880729158874601, 0.15778347803279757, 0.1537987389601767, 0.16042357287369668, 0.15723477094434202, 0.18851502682082355, 0.24116850504651666, 0.15739115606993437, 0.1574549530632794, 0.28667397098615766, 0.2118979631923139, 0.17111057904548943, 0.17167354188859463, 0.22383168200030923, 0.17515868111513555, 0.1752053420059383, 0.21834944002330303, 0.22398290899582207, 0.17487946106120944, 0.17399665317498147, 0.18655417999252677, 0.17637273482978344, 0.17306076106615365, 0.23267543385736644, 0.17051836079917848, 0.17384713818319142, 0.17678371910005808, 0.17618033988401294, 0.1736605539917946, 0.17478611692786217, 0.21765616489574313, 0.17049979511648417, 0.17040323000401258, 0.18498878204263747, 0.18438738980330527, 0.1832373570650816, 0.18172337301075459, 0.17660063109360635, 0.17316971393302083, 0.18590346910059452, 0.17209609085693955, 0.1907345198560506, 0.20252620009705424, 0.1671118838712573, 0.1663944039028138, 0.16764539992436767, 0.1673687540460378, 0.17297897418029606, 0.17524121701717377, 0.17048474191688, 0.19846137310378253, 0.17796027706936002, 0.180804232833907, 0.18444827501662076, 0.17994554806500673, 0.2259921538643539, 0.22293626982718706, 0.173878078116104, 0.22614391800016165, 0.22188629396259785, 0.2239155729766935, 0.21477731107734144, 0.17030662111938, 0.17720504780299962, 0.18373219203203917, 0.22181191015988588, 0.2178517309948802, 0.214651940157637, 0.17072084895335138, 0.1728464539628476, 0.16910799592733383, 0.17183819483034313, 0.22180439997464418, 0.2336835500318557, 0.17225745203904808, 0.17850692197680473, 0.1717123589478433, 0.176119094947353, 0.21752040507271886, 0.17355746496468782, 0.17359033110551536, 0.16906044399365783, 0.17773864814080298, 0.20218151807785034, 0.17314651585184038, 0.17135109496302903, 0.17726288177073002, 0.2596114119514823, 0.22820510109886527, 0.17895598290488124, 0.17945221718400717, 0.18443672289140522, 0.17245666100643575, 0.17705029901117086, 0.1788200919982046, 0.18291133898310363, 0.17074679606594145, 0.17529595596715808, 0.17999906186014414, 0.22171546798199415, 0.17636532499454916, 0.17422113195061684, 0.16953918989747763, 0.19556171307340264, 0.22392749809660017, 0.22373962099663913, 0.17241947911679745, 0.17183672613464296, 0.1720407521352172, 0.16715744300745428, 0.16950120986439288, 0.17560055502690375, 0.22530458797700703, 0.174966000020504, 0.1714917009230703, 0.174659508978948, 0.17188043310306966, 0.1728588999249041, 0.1710653689224273, 0.17550253705121577, 0.17397072399035096, 0.1759765581227839, 0.17330859508365393, 0.17696479614824057, 0.2176523911766708, 0.17484917701222003, 0.17035959591157734, 0.1698382031172514, 0.1774527868255973, 0.1759678991511464, 0.16975967097096145, 0.17194685409776866, 0.1760215440299362, 0.17445838288404047, 0.17330645280890167, 0.2176118448842317, 0.17069001193158329, 0.17309145908802748, 0.17234366992488503, 0.17150760488584638, 0.17037092498503625, 0.18687543412670493, 0.1943583630491048, 0.19105043495073915, 0.19224318210035563, 0.17232458293437958, 0.22337012784555554, 0.22400198807008564, 0.17230156087316573, 0.16925472393631935, 0.1729914171155542, 0.17121153115294874, 0.21583996806293726, 0.22323759994469583, 0.17477276106365025, 0.17591449501924217, 0.1743347360752523, 0.1735061570070684, 0.1753285068552941, 0.1773570489604026, 0.1728758260142058, 0.17293301108293235, 0.17423166893422604, 0.1829816959798336, 0.17809159099124372, 0.17197320400737226, 0.17434544302523136, 0.16969240293838084, 0.1735423058271408, 0.17038879613392055, 0.17529577598907053, 0.1740540110040456, 0.17344968090765178, 0.18912668712437153, 0.1716988869011402, 0.20534804300405085, 0.20288248802535236, 0.17410351196303964, 0.17400205391459167, 0.1788749429397285, 0.21955418586730957, 0.2218442028388381, 0.17994075594469905, 0.1782635550480336, 0.1813675828743726, 0.18256122316233814, 0.18164072511717677, 0.17657154495827854, 0.17653189017437398, 0.18106435192748904, 0.17116264207288623, 0.17229989892803133, 0.16989098908379674, 0.1892556690145284, 0.1963868490420282, 0.20295878499746323, 0.18595310393720865, 0.18538128002546728, 0.1763516899663955, 0.1846457419451326, 0.18637503404170275, 0.18269771593622863, 0.17869391501881182, 0.1844965680502355, 0.18066523293964565, 0.23506048088893294, 0.1823582979850471, 0.17779425205662847]
[0.0013607542008854622, 0.0013671576518334391, 0.0013733393036175607, 0.0020463792318406032, 0.0013828419542387706, 0.0014120017055624216, 0.0015111312165480945, 0.001363959991871271, 0.002554039635096294, 0.002099248667269252, 0.002112113473604816, 0.0013935556978189438, 0.0015028869541896174, 0.0014479596514341442, 0.0014340397757441961, 0.0015135518520666186, 0.0014889241546363562, 0.0014505187598194264, 0.0017212333659417638, 0.001398409249318778, 0.0013861786899005259, 0.0013860207296503607, 0.0014435321005970933, 0.001346578054462177, 0.0013701349619573862, 0.0014816681157739818, 0.0014030338434661313, 0.00208998939506132, 0.0015126794645230668, 0.0015216758601049813, 0.0015149681843355182, 0.0014575124328392883, 0.0015138282256504131, 0.0015087282093776057, 0.001513780365416477, 0.0014742146437453439, 0.0014666547825516656, 0.001593793565479591, 0.0020666796201479066, 0.0021247220850123686, 0.0015003338290104108, 0.0014582377995631492, 0.001427845316469785, 0.00144142181388745, 0.0015574080450821291, 0.001374732340187874, 0.0014028941162017195, 0.0013904446743197682, 0.0014811628137159254, 0.002001981844750139, 0.0013415932550650928, 0.0014553511092915785, 0.0015019815335728863, 0.0013652881011800018, 0.0013411036519290402, 0.0014586648524760506, 0.0018870208841972342, 0.0014733750239563311, 0.0014081156502167385, 0.0013754715269445911, 0.0016890900469450063, 0.0014473133803482436, 0.0014761180865989869, 0.0018660941940927228, 0.0014239191619116207, 0.0014288282011050818, 0.0013963414334453815, 0.0014342727670229453, 0.001390859425111219, 0.0014317055041504691, 0.0020433556894923367, 0.0014560322399980337, 0.0013845588762832935, 0.001408993031887352, 0.0014207692629145097, 0.0013631737209827632, 0.0014147086505770915, 0.0013606851765265067, 0.0013286465807825095, 0.0013910569611561392, 0.0014236788229309311, 0.00167292534092138, 0.0014102485359379256, 0.0014053617762907068, 0.0020078331242900254, 0.0014161927528072928, 0.0013927938759529083, 0.0014881786273055991, 0.0013972156818291938, 0.0014824499538558166, 0.0014182042399089234, 0.0013681712719185869, 0.0013657574733184983, 0.001361921294267441, 0.0014063971236261518, 0.0013938538690436949, 0.0013858699456377084, 0.0013717795964041653, 0.001353703890457865, 0.0013983731551543456, 0.0019797395658498816, 0.0013662506573537524, 0.0013716527361913708, 0.001419023798359919, 0.0014080234962089578, 0.0014087173333319353, 0.001526843094043145, 0.002000973200743166, 0.0016240262798243022, 0.001448135969207384, 0.0013829423493684032, 0.0014491837287624908, 0.001421473543123566, 0.0014132881778934898, 0.0014416492785969677, 0.001448591209067277, 0.0014208619782426792, 0.0014013591702565435, 0.0014163400154820708, 0.001844055333369693, 0.0013666211089566928, 0.0013841956983818564, 0.0013860212620925303, 0.0013948319996021283, 0.002015996024070322, 0.0016397151016709648, 0.0013646674107817948, 0.001380615161962865, 0.001406247814207576, 0.0013613361872501614, 0.001378787324092416, 0.0017206282011962444, 0.0015185597738929728, 0.0015092226514202911, 0.0015130357062371896, 0.0017305122031433175, 0.0021012178062670686, 0.0014229660146227179, 0.0014298769768125326, 0.0014453666129459937, 0.001450771473117115, 0.0013653930698976267, 0.0013610671541487524, 0.002050094239688081, 0.001360506216396195, 0.0013507010844759932, 0.0013317486520336812, 0.0013449740083527196, 0.0013770871640493472, 0.0014084279067929863, 0.0013592782008006823, 0.0013647115439205438, 0.0013359584340233673, 0.0013359129598523064, 0.0013517079145698122, 0.0013373096513222586, 0.0015379400931067708, 0.0014060518448060567, 0.001384906853412011, 0.0013930889915010727, 0.0013482962327092424, 0.001455845674066696, 0.001816499805034593, 0.0013083432793848273, 0.0013189044575271911, 0.001335009410905168, 0.0013306989377841008, 0.0013456542943712584, 0.0013706830001703298, 0.001347972078306615, 0.0013574731785284226, 0.0013425072254396456, 0.0013367736968392318, 0.001342664147878802, 0.0016779429062681143, 0.0016674003563821316, 0.0013785204640770143, 0.0013551373872184014, 0.001385365379974246, 0.0013914382096033456, 0.0014931364111284645, 0.001982907573295425, 0.0013698860010233267, 0.0013852041618999584, 0.0013321478825871103, 0.0015924963017189225, 0.0013344773567866446, 0.001445675425794582, 0.0013463046120176481, 0.0013856613636738801, 0.0014568662935102632, 0.002044126867901447, 0.0013506349208656438, 0.001370439657854826, 0.0013262641312944334, 0.001372350038700672, 0.0014205086911313755, 0.0013606845357910144, 0.0014134894572775955, 0.0013926267360387615, 0.0014277349782082461, 0.0013820336510737736, 0.0019844205429156623, 0.002000884929051002, 0.0019673374896623597, 0.0015131373647889195, 0.0014273545798636222, 0.0014298333021155161, 0.0014326421006098967, 0.001392530512006939, 0.0013701772631340941, 0.0019110062632478717, 0.0014989524881189415, 0.001508574891301204, 0.001426874401856416, 0.001462203170743215, 0.0012506787275332351, 0.0013096071707532388, 0.0013216939382977033, 0.0012714646435378826, 0.0012351519684487766, 0.0012309326512320328, 0.0012903696037973313, 0.001213124409938852, 0.0012556152100595393, 0.001247037023435845, 0.0012339101400399624, 0.001257200900450811, 0.0012298805256511353, 0.0012358028528302214, 0.0012408948914949284, 0.0012156533099017864, 0.0013021889608353376, 0.0018335708611926368, 0.0012217566746380902, 0.0012194334254997184, 0.0012018761936892835, 0.00121982569640228, 0.0015711300778960765, 0.0017160438596769128, 0.0012564166347936603, 0.0012206231016492427, 0.0012075184508724961, 0.0012490972335082154, 0.001215140582531575, 0.0012433658924329188, 0.001213567612995935, 0.001236722759483743, 0.001781135618816628, 0.0019486182560609986, 0.0019508688367701084, 0.0019331199618763933, 0.001227924085850286, 0.0012121586807707484, 0.0018623405417730642, 0.0012397304025673589, 0.0012911612803275271, 0.0012764430843120397, 0.001291585394881608, 0.0011875652942748725, 0.0012903271546197493, 0.0012697553417334955, 0.0012646691779815411, 0.0012917197742607705, 0.0012692366437914299, 0.0012076298828965933, 0.001554898906845686, 0.0011823761850855378, 0.0011990557518578315, 0.0012659789460111959, 0.001917088296004506, 0.0017395009135091027, 0.0012407029921174511, 0.001284457356921693, 0.001318247364328582, 0.0014464157748996288, 0.001259173938008242, 0.0012441531000586675, 0.001376006133175751, 0.0014467806277377892, 0.0012944025355716084, 0.001467186356569792, 0.0012778437357078226, 0.0013299454184061334, 0.001300584342576159, 0.0013334216358674357, 0.0012681966741359973, 0.0012498527508979851, 0.001727072565361511, 0.0012041167356073856, 0.0012324238448699776, 0.001205338083944002, 0.001296440457073293, 0.001298977015179026, 0.0012368484663726517, 0.0012175024562883516, 0.0012169395349631014, 0.001202407386875892, 0.0012134506273292756, 0.0011970255570847165, 0.0012149382797755706, 0.0012023511537632278, 0.0012413734739805146, 0.0012023017611787763, 0.0012042792947139851, 0.00182120595307833, 0.0018385756118295266, 0.001837708317681916, 0.0012033478999438213, 0.0012287551631010318, 0.0012321141476259213, 0.0012517550837450712, 0.0018371442087398943, 0.0018384026511492886, 0.0016500523887834576, 0.0012552330934608629, 0.0012684258679288994, 0.0012515318298409151, 0.0013277748141206744, 0.0018377785278527543, 0.0018677258441701186, 0.001893432852415036, 0.0012440473101166792, 0.0012173454742878675, 0.0016797315040531086, 0.0012595844581405552, 0.001236023334425318, 0.0012318063491247883, 0.0018210566671233075, 0.0012356989309480486, 0.0012813458533221206, 0.0012134008755719707, 0.001254606474003242, 0.0012199605973942796, 0.0017769487300353457, 0.0012375619063919137, 0.0012145642093707655, 0.0012433991872149613, 0.0012310025040349064, 0.0018693724820433662, 0.0013591908676506474, 0.001246423209663625, 0.0012555910389899284, 0.0012408681087513534, 0.0012564544887245857, 0.0014386572550202524, 0.0012415373813389808, 0.0012276419546715048, 0.0012273798054156376, 0.0012249917896507785, 0.001230401037506355, 0.0012396552253427894, 0.0012755692311988553, 0.0012810876676018618, 0.0012832254265225673, 0.0012664803098020858, 0.0012543903800926006, 0.001381572790790436, 0.0012501057439534239, 0.0019113674485521724, 0.0017849258760842242, 0.0012131433576605347, 0.001532419543179084, 0.001334627821333995, 0.0019489895812300749, 0.0018122882939615222, 0.001328654271413305, 0.001223620705941851, 0.0012253479844377946, 0.001204093759374101, 0.0011966759844343792, 0.0012393642086968866, 0.0012106439928964588, 0.0012276997977448988, 0.001303042371778987, 0.001654264876301321, 0.0017238907053951146, 0.0011961719764815282, 0.0013035758914671433, 0.0012931303489346836, 0.001300590784223967, 0.0013270486658800017, 0.0012877054960113163, 0.001292866457961211, 0.0012901454330097105, 0.0012845404251197049, 0.0012782783799826405, 0.0013087497294930986, 0.0014659220861747515, 0.001271124317332409, 0.0012043994732287733, 0.0011930519465876872, 0.001345439983690663, 0.0018744914731111868, 0.0012323908370652401, 0.001228186572620342, 0.0012161574261479598, 0.0018545248376046733, 0.0012487227678327829, 0.0012599164349308542, 0.0012189668292800586, 0.0012452612945073565, 0.0015984705122288807, 0.0012263632018494513, 0.0012473149997608145, 0.0012012169454535542, 0.0018208974506706, 0.001853774442489064, 0.001692264541160575, 0.0012330204345892336, 0.0012104792949967375, 0.0012986490398269985, 0.0012818491700025954, 0.0018860159853343354, 0.0013256831935700751, 0.0013035467533438012, 0.0014111880924148384, 0.0018852944666332052, 0.0013234252868176892, 0.0013117337375590505, 0.0013028986665398576, 0.001309345380670225, 0.0012802704970186303, 0.0012595011011603497, 0.0012050139764684807, 0.0012082331561187442, 0.0011960554582810447, 0.0012501910159205512, 0.0015638333947448305, 0.001202133365062087, 0.0012153022085107112, 0.0011952032945154942, 0.001215266511421795, 0.001218451721261638, 0.0016796027685609437, 0.0011973023187195838, 0.0012117805277076803, 0.0011934457606700964, 0.0012051149834553863, 0.0012083282322413469, 0.0014579295805229465, 0.0012231277366883534, 0.0011922382865129978, 0.0012435935881681913, 0.0012188741933669923, 0.0014613567970606478, 0.0018695232949342377, 0.001220086481162282, 0.0012205810314982898, 0.0022396403983293567, 0.0016554528374399524, 0.0013368013987928862, 0.0013411995460046455, 0.0017486850156274159, 0.0013684271962119965, 0.001368791734421393, 0.001705855000182055, 0.00174986647652986, 0.0013662457895406988, 0.0013593488529295428, 0.0014574545311916154, 0.0013779119908576831, 0.0013520371958293254, 0.0018177768270106753, 0.0013321746937435819, 0.001358180767056183, 0.0013811228054692037, 0.001376408905343851, 0.0013567230780608952, 0.0013655165384989232, 0.0017004387882479932, 0.0013320296493475325, 0.0013312752344063483, 0.0014452248597081052, 0.0014405264828383224, 0.00143154185207095, 0.0014197138516465202, 0.0013796924304187996, 0.0013528883901017252, 0.0014523708523483947, 0.0013445007098198403, 0.0014901134363753954, 0.0015822359382582363, 0.0013055615927441977, 0.0012999562804907328, 0.0013097296869091224, 0.0013075683909846703, 0.001351398235783563, 0.00136907200794667, 0.001331912046225625, 0.001550479477373301, 0.0013903146646043751, 0.0014125330690148985, 0.0014410021485673496, 0.001405824594257865, 0.0017655637020652648, 0.001741689608024899, 0.0013584224852820626, 0.0017667493593762629, 0.0017334866715827957, 0.001749340413880418, 0.00167794774279173, 0.0013305204774951562, 0.0013844144359609345, 0.001435407750250306, 0.0017329055481241085, 0.0017019666483975016, 0.001676968282481539, 0.0013337566324480576, 0.0013503629215847468, 0.0013211562181822956, 0.0013424858971120557, 0.0017328468748019077, 0.0018256527346238727, 0.001345761344055063, 0.001394585327943787, 0.0013415028042800259, 0.0013759304292761954, 0.001699378164630616, 0.0013559176950366236, 0.0013561744617618388, 0.0013207847187004518, 0.0013885831886000233, 0.0015795431099832058, 0.001352707155092503, 0.0013386804293986643, 0.0013848662638338283, 0.0020282141558709554, 0.001782852352334885, 0.0013980936164443847, 0.001401970446750056, 0.0014409118975891033, 0.0013473176641127793, 0.0013832054610247724, 0.0013970319687359734, 0.0014289948358054971, 0.0013339593442651676, 0.0013694996559934225, 0.001406242670782376, 0.0017321520936093293, 0.0013778541015199153, 0.001361102593364194, 0.001324524921074044, 0.0015278258833859582, 0.0017494335788796889, 0.0017479657890362432, 0.00134702718059998, 0.0013424744229268981, 0.0013440683760563843, 0.0013059175234957365, 0.0013242282020655693, 0.0013718793361476855, 0.0017601920935703674, 0.0013669218751601875, 0.0013397789134614868, 0.0013645274138980312, 0.0013428158836177317, 0.0013504601556633133, 0.0013364481947064633, 0.0013711135707126232, 0.0013591462811746169, 0.0013748168603342492, 0.0013539733990910463, 0.0013825374699081294, 0.0017004093060677405, 0.001366009195407969, 0.001330934343059198, 0.0013268609618535265, 0.0013863498970749788, 0.0013747492121183313, 0.0013262474294606363, 0.0013433347976388177, 0.0013751683127338765, 0.0013629561162815662, 0.0013539566625695443, 0.00170009253815806, 0.0013335157182154944, 0.0013522770241252147, 0.0013464349212881643, 0.0013399031631706748, 0.0013310228514455957, 0.0014599643291148823, 0.0015184247113211313, 0.0014925815230526496, 0.0015018998601590283, 0.0013462858041748405, 0.0017450791237934027, 0.001750015531797544, 0.0013461059443216072, 0.001322302530752495, 0.0013514954462152673, 0.001337590087132412, 0.0016862497504916973, 0.0017440437495679362, 0.0013654121958097676, 0.0013743319923378294, 0.0013619901255879086, 0.0013555168516177218, 0.0013697539598069852, 0.0013856019450031454, 0.001350592390735983, 0.001351039149085409, 0.001361184913548641, 0.00142954449984245, 0.0013913405546190916, 0.0013435406563075958, 0.00136207377363462, 0.0013257218979561003, 0.0013557992642745376, 0.0013311624697962543, 0.0013694982499146136, 0.0013597969609691063, 0.0013550756320910295, 0.0014775522431591526, 0.001341397553915158, 0.0016042815859691473, 0.0015850194376980653, 0.0013601836872112472, 0.0013593910462077474, 0.001397460491716629, 0.001715267077088356, 0.0017331578346784227, 0.0014057871558179613, 0.0013926840238127625, 0.001416934241206036, 0.0014262595559557667, 0.0014190681649779435, 0.001379465194986551, 0.0013791553919872968, 0.0014145652494335081, 0.0013372081411944237, 0.0013460929603752447, 0.001327273352217162, 0.001478559914176003, 0.0015342722581408452, 0.0015856155077926815, 0.0014527586245094426, 0.001448291250198963, 0.0013777475778624648, 0.0014425448589463485, 0.0014560549534508027, 0.0014273259057517862, 0.0013960462110844674, 0.001441379437892465, 0.0014114471323409816, 0.0018364100069447886, 0.0014246742030081805, 0.0013890175941924099]
[734.8865793317307, 731.4445401808205, 728.152174313999, 488.66797729400145, 723.1484385722748, 708.2144419943777, 661.7559011746968, 733.1593345550116, 391.53660196126805, 476.36090740065663, 473.4594104422174, 717.588828035436, 665.3860406548124, 690.6269791492748, 697.3307274416773, 660.6975496971512, 671.6258829478339, 689.4085259017878, 580.9787445369764, 715.0982450145697, 721.4077140889833, 721.4899305671, 692.745245905073, 742.6231228752647, 729.8551075372835, 674.9149754617133, 712.7411820156423, 478.47132734884525, 661.0785850228292, 657.170180731532, 660.0798685674122, 686.1004938750079, 660.5769287795861, 662.8099042520913, 660.5978138214762, 678.3272736047657, 681.8237065031853, 627.4338293611371, 483.86793494795904, 470.64978853183936, 666.5183312300431, 685.7592090258355, 700.3559758646737, 693.7594466556911, 642.0924838276827, 727.4143269687959, 712.8121705346234, 719.1943832567225, 675.1452242385228, 499.50502929001675, 745.3823997881392, 687.1194130513085, 665.7871469439563, 732.4461402217688, 745.6545201122989, 685.5584394883599, 529.9358413965908, 678.7138262428145, 710.168940914817, 727.023410089305, 592.034747826892, 690.9353658841918, 677.4525758328896, 535.8786299028118, 702.2870586680592, 699.8742040691678, 716.1572206108395, 697.2174491436901, 718.9799212958102, 698.4676646845538, 489.3910566536979, 686.7979791445761, 722.2516984502656, 709.7267178535978, 703.8440555425866, 733.5822167104735, 706.8593237145171, 734.9238583995991, 752.6455977563631, 718.8778230683503, 702.405615573671, 597.7553065513613, 709.0948683984428, 711.5605510770235, 498.0493587352297, 706.118568971433, 717.9813303787179, 671.9623448769291, 715.7091156397768, 674.5590280461232, 705.1170570920164, 730.9026439341178, 732.1944192406387, 734.2568210139386, 711.036721563873, 717.4353224603717, 721.5684293809039, 728.9800800517021, 738.7139883758248, 715.1167028014243, 505.11694429398864, 731.9301144468383, 729.0475013206853, 704.7098161114571, 710.215420902035, 709.8656177068353, 654.9461460063704, 499.7568181465887, 615.7535825763771, 690.5428918717732, 723.0959413866422, 690.0436294947469, 703.4953304882382, 707.5697763852401, 693.649984671177, 690.3258791994759, 703.7981276948512, 713.5929326504727, 706.0451509305385, 542.2830768167203, 731.7317092836514, 722.4411990074912, 721.489653405649, 716.9322185648501, 496.03272430120523, 609.8620418760198, 732.7792780125943, 724.3148036837925, 711.1122164221833, 734.5724071435694, 725.2750170576511, 581.1830814494165, 658.5186946157573, 662.5927586356628, 660.9229351810393, 577.8635933243297, 475.9144896913644, 702.7574725775428, 699.3608654565443, 691.8659882157974, 689.2884362079499, 732.3898312117383, 734.7176051907788, 487.7824544066562, 735.0205298207829, 740.3562575711944, 750.8924439104363, 743.508791835143, 726.1704459284072, 710.0114923716732, 735.6845709810916, 732.7555808073548, 748.5262823547614, 748.551762017906, 739.8047974870776, 747.7699716077385, 650.2203853596887, 711.2113281554954, 722.0702226552555, 717.8292313705575, 741.676773798157, 686.8859919792484, 550.5092801157533, 764.3253997301015, 758.2050347110784, 749.0583900243641, 751.4847811220279, 743.1329162199409, 729.5632906191538, 741.8551289699163, 736.662805436831, 744.8749481944271, 748.0697760320054, 744.7878917299181, 595.9678343431148, 599.7359879241995, 725.415418964816, 737.9325590393696, 721.8312327240269, 718.6808534495167, 669.7311729503884, 504.3099403458752, 729.987750260229, 721.9152436189557, 750.667409430506, 627.944943370111, 749.3570384798063, 691.7181977070497, 742.7739540320994, 721.6770462219175, 686.4047884521636, 489.20642632451944, 740.3925254346924, 729.6928356300766, 753.9976211405192, 728.6770662001004, 703.9731655591223, 734.9242044693808, 707.4690192072723, 718.0675008756737, 700.4101007982325, 723.5713828118787, 503.92544240180246, 499.77886558138516, 508.3011965433663, 660.8785317646945, 700.596764187036, 699.3822276488074, 698.0110381890113, 718.1171194294212, 729.8325748835125, 523.2845225218881, 667.1325528502343, 662.8772663301199, 700.8325320707718, 683.8994881208715, 799.5658501143143, 763.5877554219836, 756.6048167611073, 786.4945400427923, 809.6169747078951, 812.3921312828173, 774.9717577484586, 824.317763130663, 796.4223370251955, 801.9008106469792, 810.431787170185, 795.4178203669891, 813.0871081730238, 809.1905579517085, 805.8700272311396, 822.6029509028284, 767.9377034178763, 545.3838851635956, 818.4935844907259, 820.0529681152577, 832.0324549655954, 819.7892559153102, 636.4845368749574, 582.7356884621087, 795.9143267505597, 819.2537062823502, 828.1446956586435, 800.5781881298378, 822.9500474065644, 804.2684829027118, 824.0167167376027, 808.5886609036285, 561.439561050602, 513.184148249454, 512.5921236486697, 517.2984707215711, 814.3825921514863, 824.9744986886967, 536.9587234824077, 806.6269875523735, 774.4965832202863, 783.4270186351213, 774.2422637812997, 842.0589628384181, 774.9972527662516, 787.5532924593016, 790.7206227608369, 774.1617182970534, 787.8751412446042, 828.0682799943828, 643.1286275894487, 845.7545175672293, 833.9895775910235, 789.9025518163368, 521.6243832295816, 574.8775365588602, 805.9946710480207, 778.5388861772582, 758.5829693726156, 691.364141178143, 794.1714562340748, 803.75960157383, 726.740946780558, 691.1897912011837, 772.5572011170387, 681.5766760113213, 782.5683000637636, 751.9105567493477, 768.8851597422967, 749.9503331138513, 788.521228918444, 800.094250527934, 579.0144664770817, 830.484263218529, 811.4091626533738, 829.6427478072272, 771.3427905956394, 769.8365623984341, 808.5064801290773, 821.3535790708593, 821.7335136789034, 831.6648840608099, 824.0961580784987, 835.4040513850344, 823.0870791104919, 831.7037804389419, 805.5593429054508, 831.7379482332015, 830.3721606685091, 549.0867182318012, 543.8993063793126, 544.1559960186714, 831.014871133016, 813.831778721712, 811.6131138716598, 798.878321315176, 544.323083208533, 543.9504775381192, 606.041363775895, 796.6647829869211, 788.3787498222594, 799.0208288406952, 753.1397563541339, 544.1352071777615, 535.4104849602952, 528.1412534511165, 803.8279508085669, 821.459496191899, 595.3332408108378, 793.9126221644848, 809.0462147018803, 811.8159162846586, 549.1317310733021, 809.2586106170567, 780.4294191199974, 824.1299475975916, 797.0626811841367, 819.6986051319242, 562.7624382725487, 808.0403855638054, 823.3405795137618, 804.2469468231347, 812.346032377887, 534.938868313138, 735.7318414951488, 802.2957148478263, 796.4376687527646, 805.8874210299986, 795.890347779401, 695.0925917277792, 805.4529932248306, 814.5697499134283, 814.7437293555288, 816.3320019353605, 812.7431378200826, 806.675904361618, 783.9637203071612, 780.5867040090667, 779.2863041296771, 789.5898517018958, 797.1999912229707, 723.8127492564995, 799.9323295943976, 523.1856390342332, 560.2473544693089, 824.3048883591405, 652.5628079145009, 749.2725567495469, 513.0863754381207, 551.7885886765164, 752.6412412284562, 817.2467130901283, 816.0947034640228, 830.5001103234761, 835.6480893804014, 806.8653209305093, 826.0066591562609, 814.5313714613708, 767.4347524361341, 604.498115341628, 580.08317863214, 836.0001903250092, 767.1206613636617, 773.3172458784434, 768.8813515595357, 753.5518671705028, 776.5750811016279, 773.4750900545077, 775.1064139080461, 778.4885399046987, 782.3022087047896, 764.0880280352128, 682.1644952559851, 786.7051132328327, 830.289303696874, 838.1864703042934, 743.2512874018431, 533.4780202228662, 811.4308950733152, 814.2085431421833, 822.2619691328828, 539.2216807900033, 800.8182646781936, 793.7034332399048, 820.3668680554795, 803.0443123951865, 625.5980278332546, 815.4191176740479, 801.7220992225383, 832.4890884905234, 549.1797463013198, 539.4399540093451, 590.9241585326772, 811.0165670799596, 826.1190456815664, 770.0309855334099, 780.1229843585851, 530.2181995147455, 754.3280361780799, 767.137808778123, 708.6227593437177, 530.4211186626029, 755.6150014366143, 762.349836225801, 767.5193978482812, 763.7404269056359, 781.0849366041809, 793.9651653172218, 829.865893282572, 827.6548238523268, 836.0816323995434, 799.8777684893788, 639.454307191828, 831.8544589670819, 822.8406012899848, 836.6777472826287, 822.8647713085214, 820.7136832344546, 595.3788709557699, 835.2109441075982, 825.2319435200824, 837.9098849356335, 829.7963378836541, 827.5897006437435, 685.9041845089039, 817.5760961054838, 838.7585026519764, 804.1212253860172, 820.429216929781, 684.2955820312916, 534.8957152390957, 819.6140318245125, 819.2819437579479, 446.5002509983043, 604.0643245061776, 748.0542741075725, 745.60120675476, 571.8582769700278, 730.7659499666069, 730.5713315274465, 586.2162961642557, 571.4721742558829, 731.9327222491771, 735.6463337905443, 686.1277512255559, 725.7357557194554, 739.6246220775091, 550.12253712382, 750.6523016060842, 736.2790169437244, 724.0485755792539, 726.5282839405797, 737.070089077615, 732.3236092762919, 588.0835034528507, 750.7340399591177, 751.1594703749801, 691.9338491049565, 694.1906392652103, 698.5475126370515, 704.3672912257953, 724.799221879078, 739.1592738295351, 688.5293782803898, 743.770525888378, 671.0898483221756, 632.0169930540349, 765.9539048617929, 769.2566396329117, 763.5163270674085, 764.7783526236402, 739.9743269756332, 730.4217705099361, 750.8003271190481, 644.9617776909375, 719.2616358431045, 707.9480275087652, 693.9614913095058, 711.3262949620682, 566.3913450589474, 574.1551166134673, 736.1480031688081, 566.0112424509622, 576.872044298402, 571.6440276948625, 595.9661165229279, 751.5855764073654, 722.3270532468053, 696.6661562372227, 577.0654962023247, 587.5555792715192, 596.3142001232266, 749.761969816442, 740.5416603311568, 756.9127603818447, 744.8867821637393, 577.0850353493098, 547.7492959284086, 743.0738031059718, 717.0590282019001, 745.4326571733797, 726.7809321769614, 588.4505407996484, 737.5078912684223, 737.3682576951611, 757.1256585887224, 720.158509918448, 633.0944648991773, 739.2583060090464, 747.004272296115, 722.0913860892427, 493.04458166084544, 560.8989430282129, 715.2596852156355, 713.2817972861881, 694.0049573281852, 742.2154601220261, 722.958394958282, 715.8032331248614, 699.7925919279611, 749.6480341017182, 730.1936846961887, 711.114817361957, 577.3165091503453, 725.7662468739592, 734.6984752474328, 754.9876820657403, 654.5248453205978, 571.6135851470194, 572.0935765861636, 742.3755172888119, 744.8931487422857, 744.0097675194834, 765.745142406204, 755.1568516968383, 728.9270810128654, 568.1198112710548, 731.5707050798434, 746.3918038658891, 732.8544592177232, 744.7037320603191, 740.4883408121169, 748.2519741213309, 729.3341859932577, 735.7559770062194, 727.3696074376606, 738.5669472319937, 723.3077017915838, 588.0936998119217, 732.0594937147129, 751.3518643612921, 753.6584681812283, 721.3186238985355, 727.4053996067504, 754.0071164599234, 744.416061995641, 727.1837132517761, 733.6993378247661, 738.5760768015987, 588.2032757366461, 749.8974225352185, 739.4934485756704, 742.7020676523138, 746.3225906815935, 751.30190207773, 684.9482415822172, 658.5772692871502, 669.9801548894866, 665.8233524930985, 742.784330711201, 573.039919144886, 571.4234998662218, 742.8835777884977, 756.2565878406969, 739.9211020654221, 747.613196015714, 593.031963212097, 573.3801117361515, 732.3795723143829, 727.6262253772715, 734.2197136475904, 737.7259816479335, 730.0581194457084, 721.7079938479229, 740.4158403817651, 740.1710014672437, 734.654042993304, 699.5235196317498, 718.7312960009067, 744.3020018078678, 734.1746235459436, 754.3060136079263, 737.5723135054812, 751.2231021304698, 730.1944343940189, 735.4039086006748, 737.9661889845152, 676.79502002711, 745.4911462163357, 623.3319691168178, 630.9070893492051, 735.1948192014247, 735.6235005296453, 715.5837363041358, 582.9995884358063, 576.9814958517867, 711.3452387592396, 718.0379633151042, 705.7490537802526, 701.1346538042151, 704.6877836312697, 724.9186160218775, 725.0814562375368, 706.930981374292, 747.8267363125519, 742.8907433861285, 753.4243027855085, 676.333769374031, 651.7748037833586, 630.6699165626158, 688.3455951518946, 690.4688541497591, 725.822361126173, 693.2193434389358, 686.7872655699105, 700.6108387511473, 716.3086666186979, 693.7798429136503, 708.4927072978154, 544.5407050812618, 701.9148643868987, 719.9332853529555]
Elapsed: 0.18404963273802244~0.026746181142818747
Time per graph: 0.0014301883035256928~0.00020784890002932023
Speed: 711.744635163684~87.53127582092898
Total Time: 0.1794
best val loss: 0.3193182930523573 test_score: 0.9453

Testing...
Test loss: 0.2901 score: 0.9219 time: 0.21s
test Score 0.9219
Epoch Time List: [0.6235006421338767, 0.6146915510762483, 0.6186582711525261, 0.7454341158736497, 0.8299643038772047, 0.6174163997638971, 0.6398934538010508, 0.6017873729579151, 0.7764369966462255, 0.913911780808121, 0.7002250859513879, 0.6021345423068851, 0.6475065262056887, 0.6526677410583943, 0.6180973055306822, 0.6798804588615894, 0.7121924087405205, 0.6164275191258639, 0.6939948978833854, 0.5986697829794139, 0.6039261249825358, 0.6133825050201267, 0.6421189398970455, 0.6861946142744273, 0.6099444387946278, 0.6310881411191076, 0.6085848601069301, 0.7238331940025091, 0.657312777126208, 0.6615060546901077, 0.6508346970658749, 0.6506477380171418, 0.6556074689142406, 0.7503613929729909, 0.6585775210987777, 0.6486470720265061, 0.6539894971065223, 0.6660429406911135, 0.8100088543724269, 0.9367432778235525, 0.6586342190857977, 0.6339745579753071, 0.6452003046870232, 0.6384977586567402, 0.6373017968144268, 0.6701166052371264, 0.6165477188769728, 0.6084070210345089, 0.6332503659650683, 0.8136518748942763, 0.6682389099150896, 0.6409326691646129, 0.6468586919363588, 0.5927104472648352, 0.5988247729837894, 0.6339900111779571, 0.7352630514651537, 0.6477510158438236, 0.6376841228920966, 0.6037337007001042, 0.6786731530446559, 0.6883535429369658, 0.6350665029603988, 0.6942183070350438, 0.8082051840610802, 0.6320047189947218, 0.6467968949582428, 0.632416786858812, 0.6276483531109989, 0.6528174127452075, 0.890630123205483, 0.6741201225668192, 0.608288744231686, 0.6135257829446346, 0.6276011050213128, 0.5997566760051996, 0.6236874910537153, 0.6795925649348646, 0.597937302896753, 0.6028474820777774, 0.6197071908973157, 0.6913079309742898, 0.6315930229611695, 0.6184490947052836, 0.8280366808176041, 0.6282157099340111, 0.6188028787728399, 0.6371266692876816, 0.6109115472063422, 0.6547151419799775, 0.6994714620523155, 0.6096276151947677, 0.6067678739782423, 0.6106725470162928, 0.6158222921658307, 0.687823094194755, 0.6206376221962273, 0.6123947657179087, 0.6777550680562854, 0.6074421489611268, 0.6865707826800644, 0.6822121050208807, 0.6078663237858564, 0.651912814937532, 0.692917357897386, 0.7102800386492163, 0.6164888578932732, 0.837526666931808, 0.8382368320599198, 0.6893407788593322, 0.6144375139847398, 0.6243891450576484, 0.6195686429273337, 0.6143956538289785, 0.640046315966174, 0.704701819922775, 0.6060971189290285, 0.6122425277717412, 0.6008876080159098, 0.6905582451727241, 0.6918681988026947, 0.6056635591667145, 0.6144573737401515, 0.6067770416848361, 0.82668292010203, 0.8578346101567149, 0.6903238382656127, 0.6119623437989503, 0.6126190279610455, 0.6158417020924389, 0.6127806541044265, 0.6756256460212171, 0.7479287618771195, 0.6608702489174902, 0.6510320373345166, 0.6873140218667686, 0.9080599481239915, 0.7184558950830251, 0.6226705862209201, 0.6606602428946644, 0.7086483940947801, 0.6128713109064847, 0.5989487681072205, 0.7817212203517556, 0.7049247098620981, 0.6027338101994246, 0.5895942221395671, 0.5853080211672932, 0.5887725830543786, 0.5945484640542418, 0.6747376339044422, 0.5942409690469503, 0.5928731430321932, 0.5966036522295326, 0.5911836409941316, 0.5901763618458062, 0.6196397508028895, 0.6721270368434489, 0.5969715768005699, 0.6033627269789577, 0.5838549740146846, 0.5998533959500492, 0.7268601742107421, 0.6938464292325079, 0.5831978279165924, 0.5870572789572179, 0.5853270518127829, 0.5924624502658844, 0.6165005043148994, 0.6813445969019085, 0.590493094176054, 0.5926824316848069, 0.5922169322147965, 0.5958083621226251, 0.6860768420156091, 0.8308730139397085, 0.5927367201074958, 0.5922509080264717, 0.5950175758916885, 0.6102460348047316, 0.619573141913861, 0.7531872126273811, 0.7115334139671177, 0.60221958020702, 0.5910479461308569, 0.6711806657258421, 0.5907346699386835, 0.6224696952849627, 0.5985063470434397, 0.6676011632662266, 0.6743319192901254, 0.7359344938304275, 0.5933113181963563, 0.6041368159931153, 0.7641653560567647, 0.6059968480840325, 0.6222424721345305, 0.6280124010518193, 0.6089611370116472, 0.6273034710902721, 0.6925347321666777, 0.6145890569314361, 0.6918857980053872, 0.9040451559703797, 0.8973854200448841, 0.823693023994565, 0.638555099023506, 0.6273912338074297, 0.6295894628856331, 0.6267561689019203, 0.6246160231530666, 0.7355568169150501, 0.6980045633390546, 0.6630340351257473, 0.6217350631486624, 0.6486837919801474, 0.5939719900488853, 0.6092185210436583, 0.6247284789569676, 0.6186181840021163, 0.6333576769102365, 0.6616957748774439, 0.6132511089090258, 0.589972595218569, 0.6010067737661302, 0.6060711019672453, 0.6091995649039745, 0.6114883092232049, 0.6963420086540282, 0.6173499459400773, 0.6070462199859321, 0.6165604400448501, 0.6092802139464766, 0.7638276158832014, 0.7544745632912964, 0.6035440319683403, 0.6051480451133102, 0.6055568079464138, 0.6504656956531107, 0.7297453260980546, 0.6104348739609122, 0.6023319598753005, 0.6045183909591287, 0.5999978177715093, 0.5921923273708671, 0.6235901000909507, 0.6858647502958775, 0.6036543408408761, 0.6987545569427311, 0.9264617250300944, 0.9115451469551772, 0.9106481019407511, 0.6242021289654076, 0.5978200242388994, 0.7538684119936079, 0.6320381059776992, 0.6337655500974506, 0.6148798398207873, 0.6144663209561259, 0.5819570848252624, 0.6279230250511318, 0.6946501096244901, 0.6120328875258565, 0.6227959762327373, 0.6158549976535141, 0.5866395337507129, 0.6754252729006112, 0.6545249950140715, 0.5899167088791728, 0.6062093391083181, 0.7669536659959704, 0.8606033821124583, 0.6877281221095473, 0.7129298471845686, 0.6283112620003521, 0.6555009519215673, 0.7017977409996092, 0.5990356761030853, 0.6323350809980184, 0.6393874823115766, 0.6252719778567553, 0.6507099058944732, 0.6813490672502667, 0.6349102267995477, 0.629472822882235, 0.6354566090740263, 0.617288873065263, 0.6032623900100589, 0.7929566169623286, 0.6024511989671737, 0.6076609820593148, 0.6066108760423958, 0.6260699299164116, 0.6354528099764138, 0.6462868570815772, 0.6750761468429118, 0.5956166617106646, 0.5939083790872246, 0.5914509459398687, 0.5980812907218933, 0.5961053906939924, 0.5982807299587876, 0.6282486347481608, 0.6760020821820945, 0.5999497820157558, 0.8032606199849397, 0.8740347519051284, 0.8756359401158988, 0.7398124989122152, 0.6026295451447368, 0.6074273749254644, 0.6017636423930526, 0.7649506630841643, 0.8777492912486196, 0.8615368092432618, 0.6931262828875333, 0.5981891311239451, 0.5956229160074145, 0.6124339888338, 0.79548748023808, 0.8929946138523519, 0.895750735886395, 0.6117819179780781, 0.6068217249121517, 0.6720921550877392, 0.6872650671284646, 0.6060857872944325, 0.6039502972271293, 0.6911765090189874, 0.7114721580874175, 0.6099017059896141, 0.5982546019367874, 0.6085905830841511, 0.6094269659370184, 0.7163707339204848, 0.6085174463223666, 0.6102954319212586, 0.6026453238446265, 0.611514296149835, 0.7758088370319456, 0.8180061271414161, 0.6297018229961395, 0.6224452198948711, 0.6324994089081883, 0.625362945953384, 0.6603390832897276, 0.6782477071974427, 0.6101501437369734, 0.6219065629411489, 0.6147999749518931, 0.622829855652526, 0.6179460822604597, 0.6200018518138677, 0.6657079851720482, 0.691273839911446, 0.6142504664603621, 0.6086623789742589, 0.6456827477086335, 0.5956187709234655, 0.8408503753598779, 0.8167399996891618, 0.584955028956756, 0.6326788908336312, 0.6297886779066175, 0.8056065440177917, 0.9024268169887364, 0.6433181026950479, 0.714707454899326, 0.5954957939684391, 0.5887732733972371, 0.5943086622282863, 0.626327645033598, 0.690722624771297, 0.6035609720274806, 0.6276945821009576, 0.6859279768541455, 0.8671573919709772, 0.5918609562795609, 0.6129172882065177, 0.6240088290069252, 0.6263899309560657, 0.6627537480089813, 0.6998263539280742, 0.6316888330038637, 0.6278180310036987, 0.6317850486375391, 0.6252650448586792, 0.6626728258561343, 0.7170315640978515, 0.6256647799164057, 0.6040433926973492, 0.5921228351071477, 0.6244636839255691, 0.6997018931433558, 0.6041231590788811, 0.603819842915982, 0.5973065178841352, 0.7633056379854679, 0.6393606651108712, 0.6028211272787303, 0.5945935868658125, 0.6043457968626171, 0.6480360860005021, 0.6967923860065639, 0.6153295107651502, 0.5982003891840577, 0.7904536430723965, 0.8883168308530003, 0.835481742862612, 0.621599268168211, 0.5992377670481801, 0.6226061251945794, 0.630127104697749, 0.8037426439113915, 0.7809318399522454, 0.6198042880278081, 0.6342975632287562, 0.8746394168119878, 0.8071808679960668, 0.6278288068715483, 0.6193583179265261, 0.6391575608868152, 0.6946650361642241, 0.6187818981707096, 0.5884914952330291, 0.5920784671325237, 0.5898088733665645, 0.6171859868336469, 0.6778913061134517, 0.602272163843736, 0.6057696549687535, 0.5969514339230955, 0.5874466348905116, 0.6020193158183247, 0.6865210069809109, 0.6370706290472299, 0.596636489033699, 0.5945108709856868, 0.595938416197896, 0.5955529599450529, 0.7085508129093796, 0.6129334531724453, 0.5919570617843419, 0.5978135699406266, 0.5986958299763501, 0.6498600337654352, 0.8733738358132541, 0.7714004619047046, 0.6021387572400272, 1.0118782226927578, 0.9417681966442615, 0.7005803566426039, 0.614757786039263, 0.9407819660846144, 0.8130217760335654, 0.6167591142002493, 0.6649385569617152, 0.9628587877377868, 0.9236704288050532, 0.6165091812144965, 0.6510913870297372, 0.6258989495690912, 0.6168314558453858, 0.7061554579995573, 0.6294538881629705, 0.6099821431562304, 0.6049288571812212, 0.6041791571769863, 0.6055147259030491, 0.6140937160234898, 0.8746341061778367, 0.7902811549138278, 0.5997228678315878, 0.6370642916299403, 0.6480025800410658, 0.6485600997693837, 0.6562030457425863, 0.7585726936813444, 0.6042143970262259, 0.6321427628863603, 0.5946652600541711, 0.6341509798076004, 0.6747105699032545, 0.7485584581736475, 0.5959528470411897, 0.5933936461806297, 0.5854870791081339, 0.5947501841001213, 0.5994724179618061, 0.5901439562439919, 0.6467354700434953, 0.6465649721212685, 0.6206079754047096, 0.6354901059530675, 0.664431675337255, 0.9575556367635727, 0.9323050340171903, 0.6134502026252449, 0.7559411481488496, 0.955058712977916, 0.9680198477581143, 0.9527262200135738, 0.6251409377437085, 0.6249505281448364, 0.6214921239297837, 0.8889601181726903, 0.9436374099459499, 0.9366111040581018, 0.6875014819670469, 0.6084618682507426, 0.6007619737647474, 0.6135885259136558, 0.7495593389030546, 0.9623118739109486, 0.6259520829189569, 0.6148380159866065, 0.6014046261552721, 0.6070849669631571, 0.6724614824634045, 0.6498095418792218, 0.6136950112413615, 0.5989340161904693, 0.6144403829239309, 0.6361969797872007, 0.7015956190880388, 0.6055367740336806, 0.6114349609706551, 0.7008902388624847, 0.8980891110841185, 0.8231585850007832, 0.62913683289662, 0.649592692963779, 0.696258642943576, 0.6590756250079721, 0.7614069029223174, 0.6388475021813065, 0.6068881691899151, 0.6225334689952433, 0.6267826510593295, 0.8913947159890085, 0.8110986908432096, 0.6263258308172226, 0.6166882107499987, 0.6275526820681989, 0.8803150688763708, 0.9563674728851765, 0.8507739908527583, 0.6078287721611559, 0.6067757289856672, 0.5976846779230982, 0.6033693652134389, 0.6073185699060559, 0.6956384270451963, 0.6267469679005444, 0.6234712130390108, 0.609928339254111, 0.6153059122152627, 0.6121895781252533, 0.6186644402332604, 0.6339157498441637, 0.754556235158816, 0.6180758709087968, 0.6143565829843283, 0.6298830069135875, 0.750882372027263, 0.8200707202777267, 0.6128616703208536, 0.6097759797703475, 0.6198298162780702, 0.6202476320322603, 0.6103438870050013, 0.6390104601159692, 0.7116741680074483, 0.6171497830655426, 0.6086570208426565, 0.7326953520532697, 0.6942991588730365, 0.6076822958420962, 0.6101422251667827, 0.61127446196042, 0.6096111927181482, 0.6537138321436942, 0.7910089641809464, 0.651630993001163, 0.6577129070647061, 0.6176684780512005, 0.6953048850409687, 0.9522562809288502, 0.8161450850311667, 0.6161637650802732, 0.60840166779235, 0.6176462229341269, 0.6830451448913664, 0.9520441850181669, 0.6722188370767981, 0.6153476329054683, 0.6038318492937833, 0.611475613201037, 0.6374275221023709, 0.7144103632308543, 0.6084274749737233, 0.6116837484296411, 0.6142989308573306, 0.6360117802396417, 0.6615253612399101, 0.6872680317610502, 0.6197520399000496, 0.6097257740329951, 0.6177906170487404, 0.608234812039882, 0.6616974810604006, 0.6859873961657286, 0.617420124122873, 0.6533248811028898, 0.6096517296973616, 0.6515155697707087, 0.8093220088630915, 0.6161681332159787, 0.6876646911259741, 0.6321686480659992, 1.0453495411202312, 0.9656115719117224, 0.886935176094994, 0.6322975279763341, 0.6427299110218883, 0.6430023179855198, 0.6339574030134827, 0.6423711301758885, 0.7363382198382169, 0.618445327039808, 0.6125083421356976, 0.6104739198926836, 0.6105112340301275, 0.6567421928048134, 0.621630558045581, 0.7114725729916245, 0.6409114170819521, 0.631798243150115, 0.6029069791547954, 0.6262786779552698, 0.6830367548391223, 0.7307893701363355, 0.6309868339449167, 0.6373099370393902, 0.6380075770430267, 0.7443554617930204, 0.6462965358514339, 0.6052642615977675]
Total Epoch List: [216, 233, 199]
Total Time List: [0.18939930596388876, 0.15877319988794625, 0.17943641892634332]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a289b38e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8387;  Loss pred: 2.8387; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 2.8131;  Loss pred: 2.8131; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 2.8483;  Loss pred: 2.8483; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 2.7926;  Loss pred: 2.7926; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 2.7552;  Loss pred: 2.7552; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 2.7476;  Loss pred: 2.7476; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.23s
Epoch 7/1000, LR 0.000165
Train loss: 2.6462;  Loss pred: 2.6462; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 2.5873;  Loss pred: 2.5873; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 2.5371;  Loss pred: 2.5371; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 2.4633;  Loss pred: 2.4633; Loss self: 0.0000; time: 0.27s
Val loss: 0.6928 score: 0.5039 time: 0.19s
Test loss: 0.6928 score: 0.5271 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 2.3700;  Loss pred: 2.3700; Loss self: 0.0000; time: 0.39s
Val loss: 0.6927 score: 0.5271 time: 0.25s
Test loss: 0.6928 score: 0.5271 time: 0.24s
Epoch 12/1000, LR 0.000285
Train loss: 2.3064;  Loss pred: 2.3064; Loss self: 0.0000; time: 0.26s
Val loss: 0.6927 score: 0.5349 time: 0.17s
Test loss: 0.6927 score: 0.5271 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 2.2232;  Loss pred: 2.2232; Loss self: 0.0000; time: 0.26s
Val loss: 0.6926 score: 0.5426 time: 0.17s
Test loss: 0.6927 score: 0.5271 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 2.1631;  Loss pred: 2.1631; Loss self: 0.0000; time: 0.26s
Val loss: 0.6926 score: 0.5349 time: 0.17s
Test loss: 0.6926 score: 0.5194 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 2.0897;  Loss pred: 2.0897; Loss self: 0.0000; time: 0.26s
Val loss: 0.6925 score: 0.5426 time: 0.18s
Test loss: 0.6926 score: 0.5194 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 2.0102;  Loss pred: 2.0102; Loss self: 0.0000; time: 0.27s
Val loss: 0.6924 score: 0.5426 time: 0.23s
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 17/1000, LR 0.000285
Train loss: 1.9626;  Loss pred: 1.9626; Loss self: 0.0000; time: 0.28s
Val loss: 0.6923 score: 0.5426 time: 0.17s
Test loss: 0.6924 score: 0.5194 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 1.9083;  Loss pred: 1.9083; Loss self: 0.0000; time: 0.26s
Val loss: 0.6923 score: 0.5504 time: 0.18s
Test loss: 0.6923 score: 0.5194 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 1.8393;  Loss pred: 1.8393; Loss self: 0.0000; time: 0.26s
Val loss: 0.6922 score: 0.5504 time: 0.17s
Test loss: 0.6922 score: 0.5271 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 1.8020;  Loss pred: 1.8020; Loss self: 0.0000; time: 0.25s
Val loss: 0.6920 score: 0.5814 time: 0.17s
Test loss: 0.6921 score: 0.5271 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 1.7273;  Loss pred: 1.7273; Loss self: 0.0000; time: 0.25s
Val loss: 0.6919 score: 0.5814 time: 0.17s
Test loss: 0.6920 score: 0.5271 time: 0.20s
Epoch 22/1000, LR 0.000285
Train loss: 1.6807;  Loss pred: 1.6807; Loss self: 0.0000; time: 0.25s
Val loss: 0.6918 score: 0.5891 time: 0.23s
Test loss: 0.6919 score: 0.5349 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.6346;  Loss pred: 1.6346; Loss self: 0.0000; time: 0.25s
Val loss: 0.6917 score: 0.5969 time: 0.17s
Test loss: 0.6918 score: 0.5504 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.6129;  Loss pred: 1.6129; Loss self: 0.0000; time: 0.25s
Val loss: 0.6915 score: 0.6047 time: 0.17s
Test loss: 0.6917 score: 0.5581 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 1.5600;  Loss pred: 1.5600; Loss self: 0.0000; time: 0.25s
Val loss: 0.6914 score: 0.5969 time: 0.17s
Test loss: 0.6916 score: 0.5349 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.5278;  Loss pred: 1.5278; Loss self: 0.0000; time: 0.25s
Val loss: 0.6912 score: 0.6124 time: 0.17s
Test loss: 0.6915 score: 0.5659 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 1.5032;  Loss pred: 1.5032; Loss self: 0.0000; time: 0.26s
Val loss: 0.6910 score: 0.6357 time: 0.17s
Test loss: 0.6913 score: 0.5891 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.4617;  Loss pred: 1.4617; Loss self: 0.0000; time: 0.25s
Val loss: 0.6908 score: 0.6822 time: 0.17s
Test loss: 0.6911 score: 0.6047 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.4287;  Loss pred: 1.4287; Loss self: 0.0000; time: 0.26s
Val loss: 0.6907 score: 0.7054 time: 0.24s
Test loss: 0.6910 score: 0.6512 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 1.4096;  Loss pred: 1.4096; Loss self: 0.0000; time: 0.26s
Val loss: 0.6905 score: 0.8217 time: 0.17s
Test loss: 0.6908 score: 0.7674 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.3711;  Loss pred: 1.3711; Loss self: 0.0000; time: 0.25s
Val loss: 0.6902 score: 0.8760 time: 0.17s
Test loss: 0.6906 score: 0.8372 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.3450;  Loss pred: 1.3450; Loss self: 0.0000; time: 0.25s
Val loss: 0.6900 score: 0.8992 time: 0.18s
Test loss: 0.6904 score: 0.8837 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 1.3211;  Loss pred: 1.3211; Loss self: 0.0000; time: 0.25s
Val loss: 0.6898 score: 0.9070 time: 0.17s
Test loss: 0.6902 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 1.3093;  Loss pred: 1.3093; Loss self: 0.0000; time: 0.25s
Val loss: 0.6895 score: 0.9147 time: 0.17s
Test loss: 0.6900 score: 0.8915 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 1.2828;  Loss pred: 1.2828; Loss self: 0.0000; time: 0.25s
Val loss: 0.6892 score: 0.9147 time: 0.17s
Test loss: 0.6898 score: 0.8915 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 1.2678;  Loss pred: 1.2678; Loss self: 0.0000; time: 0.25s
Val loss: 0.6889 score: 0.9147 time: 0.17s
Test loss: 0.6895 score: 0.8915 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.2472;  Loss pred: 1.2472; Loss self: 0.0000; time: 0.29s
Val loss: 0.6886 score: 0.9225 time: 0.17s
Test loss: 0.6892 score: 0.8992 time: 0.21s
Epoch 38/1000, LR 0.000284
Train loss: 1.2271;  Loss pred: 1.2271; Loss self: 0.0000; time: 0.32s
Val loss: 0.6883 score: 0.9225 time: 0.17s
Test loss: 0.6889 score: 0.8992 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 1.2080;  Loss pred: 1.2080; Loss self: 0.0000; time: 0.26s
Val loss: 0.6879 score: 0.9147 time: 0.17s
Test loss: 0.6886 score: 0.8992 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 1.1961;  Loss pred: 1.1961; Loss self: 0.0000; time: 0.26s
Val loss: 0.6875 score: 0.9225 time: 0.23s
Test loss: 0.6883 score: 0.8915 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 1.1840;  Loss pred: 1.1840; Loss self: 0.0000; time: 0.26s
Val loss: 0.6871 score: 0.9302 time: 0.18s
Test loss: 0.6879 score: 0.8992 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 1.1716;  Loss pred: 1.1716; Loss self: 0.0000; time: 0.27s
Val loss: 0.6866 score: 0.9302 time: 0.19s
Test loss: 0.6875 score: 0.8992 time: 0.19s
Epoch 43/1000, LR 0.000284
Train loss: 1.1543;  Loss pred: 1.1543; Loss self: 0.0000; time: 0.26s
Val loss: 0.6861 score: 0.9380 time: 0.18s
Test loss: 0.6871 score: 0.8992 time: 0.19s
Epoch 44/1000, LR 0.000284
Train loss: 1.1472;  Loss pred: 1.1472; Loss self: 0.0000; time: 0.27s
Val loss: 0.6856 score: 0.9225 time: 0.18s
Test loss: 0.6866 score: 0.8992 time: 0.20s
Epoch 45/1000, LR 0.000284
Train loss: 1.1314;  Loss pred: 1.1314; Loss self: 0.0000; time: 0.34s
Val loss: 0.6850 score: 0.9225 time: 0.19s
Test loss: 0.6861 score: 0.8992 time: 0.19s
Epoch 46/1000, LR 0.000284
Train loss: 1.1253;  Loss pred: 1.1253; Loss self: 0.0000; time: 0.26s
Val loss: 0.6843 score: 0.9225 time: 0.18s
Test loss: 0.6854 score: 0.9070 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 1.1153;  Loss pred: 1.1153; Loss self: 0.0000; time: 0.29s
Val loss: 0.6835 score: 0.9380 time: 0.17s
Test loss: 0.6847 score: 0.9070 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 1.1063;  Loss pred: 1.1063; Loss self: 0.0000; time: 0.27s
Val loss: 0.6828 score: 0.9380 time: 0.18s
Test loss: 0.6841 score: 0.8992 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 1.0990;  Loss pred: 1.0990; Loss self: 0.0000; time: 0.26s
Val loss: 0.6820 score: 0.9380 time: 0.17s
Test loss: 0.6834 score: 0.8992 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 1.0937;  Loss pred: 1.0937; Loss self: 0.0000; time: 0.28s
Val loss: 0.6812 score: 0.9302 time: 0.24s
Test loss: 0.6827 score: 0.8992 time: 0.20s
Epoch 51/1000, LR 0.000284
Train loss: 1.0802;  Loss pred: 1.0802; Loss self: 0.0000; time: 0.27s
Val loss: 0.6804 score: 0.9225 time: 0.18s
Test loss: 0.6820 score: 0.8915 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 1.0748;  Loss pred: 1.0748; Loss self: 0.0000; time: 0.27s
Val loss: 0.6794 score: 0.9302 time: 0.17s
Test loss: 0.6812 score: 0.8992 time: 0.27s
Epoch 53/1000, LR 0.000284
Train loss: 1.0694;  Loss pred: 1.0694; Loss self: 0.0000; time: 0.29s
Val loss: 0.6784 score: 0.9380 time: 0.22s
Test loss: 0.6803 score: 0.8992 time: 0.26s
Epoch 54/1000, LR 0.000284
Train loss: 1.0621;  Loss pred: 1.0621; Loss self: 0.0000; time: 0.39s
Val loss: 0.6773 score: 0.9457 time: 0.21s
Test loss: 0.6793 score: 0.8992 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 1.0586;  Loss pred: 1.0586; Loss self: 0.0000; time: 0.36s
Val loss: 0.6762 score: 0.9457 time: 0.17s
Test loss: 0.6783 score: 0.8992 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 1.0537;  Loss pred: 1.0537; Loss self: 0.0000; time: 0.27s
Val loss: 0.6750 score: 0.9457 time: 0.18s
Test loss: 0.6773 score: 0.8992 time: 0.18s
Epoch 57/1000, LR 0.000283
Train loss: 1.0465;  Loss pred: 1.0465; Loss self: 0.0000; time: 0.28s
Val loss: 0.6737 score: 0.9535 time: 0.25s
Test loss: 0.6762 score: 0.8992 time: 0.18s
Epoch 58/1000, LR 0.000283
Train loss: 1.0394;  Loss pred: 1.0394; Loss self: 0.0000; time: 0.27s
Val loss: 0.6723 score: 0.9457 time: 0.19s
Test loss: 0.6749 score: 0.8992 time: 0.18s
Epoch 59/1000, LR 0.000283
Train loss: 1.0384;  Loss pred: 1.0384; Loss self: 0.0000; time: 0.31s
Val loss: 0.6708 score: 0.9457 time: 0.22s
Test loss: 0.6737 score: 0.8992 time: 0.28s
Epoch 60/1000, LR 0.000283
Train loss: 1.0336;  Loss pred: 1.0336; Loss self: 0.0000; time: 0.27s
Val loss: 0.6692 score: 0.9457 time: 0.19s
Test loss: 0.6722 score: 0.9147 time: 0.19s
Epoch 61/1000, LR 0.000283
Train loss: 1.0270;  Loss pred: 1.0270; Loss self: 0.0000; time: 0.30s
Val loss: 0.6675 score: 0.9457 time: 0.25s
Test loss: 0.6708 score: 0.9147 time: 0.25s
Epoch 62/1000, LR 0.000283
Train loss: 1.0268;  Loss pred: 1.0268; Loss self: 0.0000; time: 0.39s
Val loss: 0.6657 score: 0.9457 time: 0.26s
Test loss: 0.6692 score: 0.9147 time: 0.20s
Epoch 63/1000, LR 0.000283
Train loss: 1.0206;  Loss pred: 1.0206; Loss self: 0.0000; time: 0.27s
Val loss: 0.6638 score: 0.9457 time: 0.18s
Test loss: 0.6676 score: 0.9147 time: 0.18s
Epoch 64/1000, LR 0.000283
Train loss: 1.0130;  Loss pred: 1.0130; Loss self: 0.0000; time: 0.26s
Val loss: 0.6618 score: 0.9457 time: 0.18s
Test loss: 0.6658 score: 0.9147 time: 0.17s
Epoch 65/1000, LR 0.000283
Train loss: 1.0124;  Loss pred: 1.0124; Loss self: 0.0000; time: 0.27s
Val loss: 0.6596 score: 0.9457 time: 0.19s
Test loss: 0.6639 score: 0.9147 time: 0.19s
Epoch 66/1000, LR 0.000283
Train loss: 1.0059;  Loss pred: 1.0059; Loss self: 0.0000; time: 0.27s
Val loss: 0.6572 score: 0.9457 time: 0.18s
Test loss: 0.6619 score: 0.9147 time: 0.18s
Epoch 67/1000, LR 0.000283
Train loss: 1.0030;  Loss pred: 1.0030; Loss self: 0.0000; time: 0.29s
Val loss: 0.6548 score: 0.9457 time: 0.18s
Test loss: 0.6598 score: 0.9147 time: 0.27s
Epoch 68/1000, LR 0.000283
Train loss: 0.9998;  Loss pred: 0.9998; Loss self: 0.0000; time: 0.37s
Val loss: 0.6522 score: 0.9457 time: 0.22s
Test loss: 0.6575 score: 0.9147 time: 0.18s
Epoch 69/1000, LR 0.000283
Train loss: 0.9951;  Loss pred: 0.9951; Loss self: 0.0000; time: 0.27s
Val loss: 0.6494 score: 0.9457 time: 0.17s
Test loss: 0.6551 score: 0.9147 time: 0.18s
Epoch 70/1000, LR 0.000283
Train loss: 0.9906;  Loss pred: 0.9906; Loss self: 0.0000; time: 0.27s
Val loss: 0.6465 score: 0.9457 time: 0.17s
Test loss: 0.6526 score: 0.9147 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9879;  Loss pred: 0.9879; Loss self: 0.0000; time: 0.30s
Val loss: 0.6434 score: 0.9457 time: 0.27s
Test loss: 0.6500 score: 0.9147 time: 0.27s
Epoch 72/1000, LR 0.000282
Train loss: 0.9845;  Loss pred: 0.9845; Loss self: 0.0000; time: 0.38s
Val loss: 0.6402 score: 0.9457 time: 0.22s
Test loss: 0.6472 score: 0.9147 time: 0.19s
Epoch 73/1000, LR 0.000282
Train loss: 0.9800;  Loss pred: 0.9800; Loss self: 0.0000; time: 0.26s
Val loss: 0.6368 score: 0.9302 time: 0.17s
Test loss: 0.6442 score: 0.9147 time: 0.18s
Epoch 74/1000, LR 0.000282
Train loss: 0.9783;  Loss pred: 0.9783; Loss self: 0.0000; time: 0.27s
Val loss: 0.6331 score: 0.9302 time: 0.17s
Test loss: 0.6411 score: 0.9147 time: 0.18s
Epoch 75/1000, LR 0.000282
Train loss: 0.9721;  Loss pred: 0.9721; Loss self: 0.0000; time: 0.26s
Val loss: 0.6294 score: 0.9380 time: 0.17s
Test loss: 0.6379 score: 0.9147 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9678;  Loss pred: 0.9678; Loss self: 0.0000; time: 0.26s
Val loss: 0.6255 score: 0.9457 time: 0.17s
Test loss: 0.6345 score: 0.9147 time: 0.18s
Epoch 77/1000, LR 0.000282
Train loss: 0.9642;  Loss pred: 0.9642; Loss self: 0.0000; time: 0.27s
Val loss: 0.6215 score: 0.9535 time: 0.18s
Test loss: 0.6310 score: 0.9147 time: 0.18s
Epoch 78/1000, LR 0.000282
Train loss: 0.9597;  Loss pred: 0.9597; Loss self: 0.0000; time: 0.36s
Val loss: 0.6173 score: 0.9612 time: 0.17s
Test loss: 0.6273 score: 0.9070 time: 0.18s
Epoch 79/1000, LR 0.000282
Train loss: 0.9568;  Loss pred: 0.9568; Loss self: 0.0000; time: 0.26s
Val loss: 0.6129 score: 0.9612 time: 0.17s
Test loss: 0.6235 score: 0.8992 time: 0.18s
Epoch 80/1000, LR 0.000282
Train loss: 0.9531;  Loss pred: 0.9531; Loss self: 0.0000; time: 0.26s
Val loss: 0.6082 score: 0.9535 time: 0.18s
Test loss: 0.6196 score: 0.9147 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.9481;  Loss pred: 0.9481; Loss self: 0.0000; time: 0.29s
Val loss: 0.6034 score: 0.9535 time: 0.17s
Test loss: 0.6154 score: 0.9147 time: 0.18s
Epoch 82/1000, LR 0.000281
Train loss: 0.9447;  Loss pred: 0.9447; Loss self: 0.0000; time: 0.30s
Val loss: 0.5984 score: 0.9535 time: 0.17s
Test loss: 0.6111 score: 0.9147 time: 0.18s
Epoch 83/1000, LR 0.000281
Train loss: 0.9403;  Loss pred: 0.9403; Loss self: 0.0000; time: 0.30s
Val loss: 0.5931 score: 0.9612 time: 0.17s
Test loss: 0.6065 score: 0.9147 time: 0.24s
Epoch 84/1000, LR 0.000281
Train loss: 0.9362;  Loss pred: 0.9362; Loss self: 0.0000; time: 0.27s
Val loss: 0.5875 score: 0.9612 time: 0.18s
Test loss: 0.6017 score: 0.9070 time: 0.19s
Epoch 85/1000, LR 0.000281
Train loss: 0.9316;  Loss pred: 0.9316; Loss self: 0.0000; time: 0.28s
Val loss: 0.5819 score: 0.9612 time: 0.18s
Test loss: 0.5967 score: 0.9070 time: 0.19s
Epoch 86/1000, LR 0.000281
Train loss: 0.9275;  Loss pred: 0.9275; Loss self: 0.0000; time: 0.33s
Val loss: 0.5759 score: 0.9612 time: 0.26s
Test loss: 0.5916 score: 0.9070 time: 0.27s
Epoch 87/1000, LR 0.000281
Train loss: 0.9232;  Loss pred: 0.9232; Loss self: 0.0000; time: 0.41s
Val loss: 0.5698 score: 0.9612 time: 0.23s
Test loss: 0.5862 score: 0.9070 time: 0.22s
Epoch 88/1000, LR 0.000281
Train loss: 0.9158;  Loss pred: 0.9158; Loss self: 0.0000; time: 0.27s
Val loss: 0.5636 score: 0.9612 time: 0.18s
Test loss: 0.5808 score: 0.9070 time: 0.19s
Epoch 89/1000, LR 0.000281
Train loss: 0.9132;  Loss pred: 0.9132; Loss self: 0.0000; time: 0.27s
Val loss: 0.5570 score: 0.9612 time: 0.19s
Test loss: 0.5751 score: 0.9070 time: 0.19s
Epoch 90/1000, LR 0.000281
Train loss: 0.9074;  Loss pred: 0.9074; Loss self: 0.0000; time: 0.27s
Val loss: 0.5503 score: 0.9612 time: 0.19s
Test loss: 0.5693 score: 0.9070 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.9028;  Loss pred: 0.9028; Loss self: 0.0000; time: 0.26s
Val loss: 0.5435 score: 0.9612 time: 0.17s
Test loss: 0.5634 score: 0.9070 time: 0.18s
Epoch 92/1000, LR 0.000280
Train loss: 0.8975;  Loss pred: 0.8975; Loss self: 0.0000; time: 0.28s
Val loss: 0.5366 score: 0.9612 time: 0.25s
Test loss: 0.5574 score: 0.9070 time: 0.25s
Epoch 93/1000, LR 0.000280
Train loss: 0.8934;  Loss pred: 0.8934; Loss self: 0.0000; time: 0.38s
Val loss: 0.5296 score: 0.9612 time: 0.18s
Test loss: 0.5512 score: 0.9070 time: 0.18s
Epoch 94/1000, LR 0.000280
Train loss: 0.8882;  Loss pred: 0.8882; Loss self: 0.0000; time: 0.28s
Val loss: 0.5224 score: 0.9612 time: 0.18s
Test loss: 0.5449 score: 0.9070 time: 0.18s
Epoch 95/1000, LR 0.000280
Train loss: 0.8828;  Loss pred: 0.8828; Loss self: 0.0000; time: 0.28s
Val loss: 0.5152 score: 0.9535 time: 0.17s
Test loss: 0.5386 score: 0.9070 time: 0.18s
Epoch 96/1000, LR 0.000280
Train loss: 0.8775;  Loss pred: 0.8775; Loss self: 0.0000; time: 0.28s
Val loss: 0.5076 score: 0.9612 time: 0.18s
Test loss: 0.5321 score: 0.9147 time: 0.18s
Epoch 97/1000, LR 0.000280
Train loss: 0.8734;  Loss pred: 0.8734; Loss self: 0.0000; time: 0.27s
Val loss: 0.5001 score: 0.9612 time: 0.19s
Test loss: 0.5256 score: 0.9147 time: 0.18s
Epoch 98/1000, LR 0.000280
Train loss: 0.8664;  Loss pred: 0.8664; Loss self: 0.0000; time: 0.26s
Val loss: 0.4928 score: 0.9612 time: 0.18s
Test loss: 0.5191 score: 0.9147 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.8619;  Loss pred: 0.8619; Loss self: 0.0000; time: 0.28s
Val loss: 0.4853 score: 0.9535 time: 0.17s
Test loss: 0.5125 score: 0.9070 time: 0.19s
Epoch 100/1000, LR 0.000279
Train loss: 0.8584;  Loss pred: 0.8584; Loss self: 0.0000; time: 0.28s
Val loss: 0.4777 score: 0.9535 time: 0.18s
Test loss: 0.5059 score: 0.9070 time: 0.27s
Epoch 101/1000, LR 0.000279
Train loss: 0.8519;  Loss pred: 0.8519; Loss self: 0.0000; time: 0.28s
Val loss: 0.4701 score: 0.9535 time: 0.19s
Test loss: 0.4992 score: 0.9070 time: 0.20s
Epoch 102/1000, LR 0.000279
Train loss: 0.8460;  Loss pred: 0.8460; Loss self: 0.0000; time: 0.38s
Val loss: 0.4625 score: 0.9535 time: 0.26s
Test loss: 0.4926 score: 0.9070 time: 0.25s
Epoch 103/1000, LR 0.000279
Train loss: 0.8395;  Loss pred: 0.8395; Loss self: 0.0000; time: 0.39s
Val loss: 0.4551 score: 0.9535 time: 0.25s
Test loss: 0.4860 score: 0.9070 time: 0.25s
Epoch 104/1000, LR 0.000279
Train loss: 0.8362;  Loss pred: 0.8362; Loss self: 0.0000; time: 0.41s
Val loss: 0.4473 score: 0.9535 time: 0.26s
Test loss: 0.4792 score: 0.9070 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.8296;  Loss pred: 0.8296; Loss self: 0.0000; time: 0.28s
Val loss: 0.4393 score: 0.9535 time: 0.18s
Test loss: 0.4724 score: 0.9070 time: 0.19s
Epoch 106/1000, LR 0.000279
Train loss: 0.8245;  Loss pred: 0.8245; Loss self: 0.0000; time: 0.27s
Val loss: 0.4316 score: 0.9535 time: 0.18s
Test loss: 0.4656 score: 0.9070 time: 0.18s
Epoch 107/1000, LR 0.000278
Train loss: 0.8194;  Loss pred: 0.8194; Loss self: 0.0000; time: 0.27s
Val loss: 0.4237 score: 0.9535 time: 0.17s
Test loss: 0.4588 score: 0.9070 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.8163;  Loss pred: 0.8163; Loss self: 0.0000; time: 0.26s
Val loss: 0.4164 score: 0.9535 time: 0.18s
Test loss: 0.4523 score: 0.9070 time: 0.20s
Epoch 109/1000, LR 0.000278
Train loss: 0.8114;  Loss pred: 0.8114; Loss self: 0.0000; time: 0.30s
Val loss: 0.4087 score: 0.9535 time: 0.26s
Test loss: 0.4456 score: 0.9070 time: 0.24s
Epoch 110/1000, LR 0.000278
Train loss: 0.8060;  Loss pred: 0.8060; Loss self: 0.0000; time: 0.29s
Val loss: 0.4016 score: 0.9535 time: 0.18s
Test loss: 0.4391 score: 0.9070 time: 0.18s
Epoch 111/1000, LR 0.000278
Train loss: 0.8013;  Loss pred: 0.8013; Loss self: 0.0000; time: 0.27s
Val loss: 0.3941 score: 0.9535 time: 0.17s
Test loss: 0.4325 score: 0.9070 time: 0.18s
Epoch 112/1000, LR 0.000278
Train loss: 0.7947;  Loss pred: 0.7947; Loss self: 0.0000; time: 0.27s
Val loss: 0.3867 score: 0.9535 time: 0.18s
Test loss: 0.4260 score: 0.9070 time: 0.18s
Epoch 113/1000, LR 0.000278
Train loss: 0.7895;  Loss pred: 0.7895; Loss self: 0.0000; time: 0.29s
Val loss: 0.3796 score: 0.9535 time: 0.25s
Test loss: 0.4196 score: 0.9070 time: 0.26s
Epoch 114/1000, LR 0.000277
Train loss: 0.7857;  Loss pred: 0.7857; Loss self: 0.0000; time: 0.39s
Val loss: 0.3725 score: 0.9535 time: 0.25s
Test loss: 0.4132 score: 0.9070 time: 0.26s
Epoch 115/1000, LR 0.000277
Train loss: 0.7807;  Loss pred: 0.7807; Loss self: 0.0000; time: 0.41s
Val loss: 0.3655 score: 0.9535 time: 0.22s
Test loss: 0.4070 score: 0.9070 time: 0.18s
Epoch 116/1000, LR 0.000277
Train loss: 0.7757;  Loss pred: 0.7757; Loss self: 0.0000; time: 0.28s
Val loss: 0.3590 score: 0.9535 time: 0.18s
Test loss: 0.4009 score: 0.8992 time: 0.19s
Epoch 117/1000, LR 0.000277
Train loss: 0.7695;  Loss pred: 0.7695; Loss self: 0.0000; time: 0.28s
Val loss: 0.3521 score: 0.9535 time: 0.18s
Test loss: 0.3947 score: 0.8992 time: 0.19s
Epoch 118/1000, LR 0.000277
Train loss: 0.7662;  Loss pred: 0.7662; Loss self: 0.0000; time: 0.28s
Val loss: 0.3454 score: 0.9535 time: 0.18s
Test loss: 0.3887 score: 0.9070 time: 0.30s
Epoch 119/1000, LR 0.000277
Train loss: 0.7599;  Loss pred: 0.7599; Loss self: 0.0000; time: 0.28s
Val loss: 0.3387 score: 0.9535 time: 0.17s
Test loss: 0.3827 score: 0.9070 time: 0.18s
Epoch 120/1000, LR 0.000277
Train loss: 0.7566;  Loss pred: 0.7566; Loss self: 0.0000; time: 0.27s
Val loss: 0.3323 score: 0.9535 time: 0.19s
Test loss: 0.3769 score: 0.9070 time: 0.18s
Epoch 121/1000, LR 0.000276
Train loss: 0.7508;  Loss pred: 0.7508; Loss self: 0.0000; time: 0.27s
Val loss: 0.3263 score: 0.9535 time: 0.17s
Test loss: 0.3711 score: 0.9070 time: 0.26s
Epoch 122/1000, LR 0.000276
Train loss: 0.7449;  Loss pred: 0.7449; Loss self: 0.0000; time: 0.27s
Val loss: 0.3201 score: 0.9535 time: 0.18s
Test loss: 0.3655 score: 0.9070 time: 0.18s
Epoch 123/1000, LR 0.000276
Train loss: 0.7442;  Loss pred: 0.7442; Loss self: 0.0000; time: 0.26s
Val loss: 0.3140 score: 0.9535 time: 0.18s
Test loss: 0.3599 score: 0.9070 time: 0.18s
Epoch 124/1000, LR 0.000276
Train loss: 0.7391;  Loss pred: 0.7391; Loss self: 0.0000; time: 0.26s
Val loss: 0.3084 score: 0.9535 time: 0.18s
Test loss: 0.3544 score: 0.9070 time: 0.18s
Epoch 125/1000, LR 0.000276
Train loss: 0.7348;  Loss pred: 0.7348; Loss self: 0.0000; time: 0.27s
Val loss: 0.3024 score: 0.9535 time: 0.18s
Test loss: 0.3491 score: 0.9070 time: 0.18s
Epoch 126/1000, LR 0.000276
Train loss: 0.7318;  Loss pred: 0.7318; Loss self: 0.0000; time: 0.27s
Val loss: 0.2972 score: 0.9535 time: 0.19s
Test loss: 0.3439 score: 0.9070 time: 0.19s
Epoch 127/1000, LR 0.000275
Train loss: 0.7262;  Loss pred: 0.7262; Loss self: 0.0000; time: 0.30s
Val loss: 0.2915 score: 0.9535 time: 0.19s
Test loss: 0.3388 score: 0.9070 time: 0.25s
Epoch 128/1000, LR 0.000275
Train loss: 0.7201;  Loss pred: 0.7201; Loss self: 0.0000; time: 0.28s
Val loss: 0.2866 score: 0.9612 time: 0.18s
Test loss: 0.3337 score: 0.8992 time: 0.19s
Epoch 129/1000, LR 0.000275
Train loss: 0.7170;  Loss pred: 0.7170; Loss self: 0.0000; time: 0.28s
Val loss: 0.2814 score: 0.9612 time: 0.18s
Test loss: 0.3287 score: 0.9070 time: 0.19s
Epoch 130/1000, LR 0.000275
Train loss: 0.7147;  Loss pred: 0.7147; Loss self: 0.0000; time: 0.27s
Val loss: 0.2762 score: 0.9612 time: 0.19s
Test loss: 0.3239 score: 0.9070 time: 0.19s
Epoch 131/1000, LR 0.000275
Train loss: 0.7112;  Loss pred: 0.7112; Loss self: 0.0000; time: 0.26s
Val loss: 0.2715 score: 0.9612 time: 0.17s
Test loss: 0.3192 score: 0.9070 time: 0.18s
Epoch 132/1000, LR 0.000275
Train loss: 0.7073;  Loss pred: 0.7073; Loss self: 0.0000; time: 0.27s
Val loss: 0.2669 score: 0.9612 time: 0.19s
Test loss: 0.3146 score: 0.8992 time: 0.19s
Epoch 133/1000, LR 0.000274
Train loss: 0.7045;  Loss pred: 0.7045; Loss self: 0.0000; time: 0.27s
Val loss: 0.2625 score: 0.9612 time: 0.20s
Test loss: 0.3100 score: 0.8992 time: 0.19s
Epoch 134/1000, LR 0.000274
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.36s
Val loss: 0.2584 score: 0.9612 time: 0.18s
Test loss: 0.3055 score: 0.8992 time: 0.19s
Epoch 135/1000, LR 0.000274
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.28s
Val loss: 0.2544 score: 0.9612 time: 0.19s
Test loss: 0.3012 score: 0.8992 time: 0.18s
Epoch 136/1000, LR 0.000274
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.27s
Val loss: 0.2500 score: 0.9612 time: 0.18s
Test loss: 0.2970 score: 0.8992 time: 0.19s
Epoch 137/1000, LR 0.000274
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.26s
Val loss: 0.2460 score: 0.9612 time: 0.17s
Test loss: 0.2929 score: 0.8992 time: 0.18s
Epoch 138/1000, LR 0.000274
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.30s
Val loss: 0.2422 score: 0.9612 time: 0.27s
Test loss: 0.2889 score: 0.8992 time: 0.20s
Epoch 139/1000, LR 0.000273
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.37s
Val loss: 0.2380 score: 0.9612 time: 0.19s
Test loss: 0.2850 score: 0.8992 time: 0.19s
Epoch 140/1000, LR 0.000273
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.27s
Val loss: 0.2351 score: 0.9612 time: 0.18s
Test loss: 0.2810 score: 0.8992 time: 0.18s
Epoch 141/1000, LR 0.000273
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.28s
Val loss: 0.2324 score: 0.9612 time: 0.21s
Test loss: 0.2772 score: 0.8992 time: 0.17s
Epoch 142/1000, LR 0.000273
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.26s
Val loss: 0.2294 score: 0.9612 time: 0.17s
Test loss: 0.2734 score: 0.8992 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.28s
Val loss: 0.2261 score: 0.9612 time: 0.17s
Test loss: 0.2698 score: 0.8992 time: 0.18s
Epoch 144/1000, LR 0.000272
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.31s
Val loss: 0.2221 score: 0.9612 time: 0.17s
Test loss: 0.2664 score: 0.8992 time: 0.17s
Epoch 145/1000, LR 0.000272
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.32s
Val loss: 0.2182 score: 0.9612 time: 0.20s
Test loss: 0.2630 score: 0.8992 time: 0.18s
Epoch 146/1000, LR 0.000272
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.26s
Val loss: 0.2150 score: 0.9535 time: 0.18s
Test loss: 0.2598 score: 0.8992 time: 0.18s
Epoch 147/1000, LR 0.000272
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.35s
Val loss: 0.2117 score: 0.9535 time: 0.18s
Test loss: 0.2566 score: 0.9070 time: 0.18s
Epoch 148/1000, LR 0.000272
Train loss: 0.6586;  Loss pred: 0.6586; Loss self: 0.0000; time: 0.28s
Val loss: 0.2091 score: 0.9535 time: 0.21s
Test loss: 0.2533 score: 0.9070 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 0.26s
Val loss: 0.2066 score: 0.9535 time: 0.17s
Test loss: 0.2501 score: 0.9070 time: 0.17s
Epoch 150/1000, LR 0.000271
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.26s
Val loss: 0.2048 score: 0.9612 time: 0.17s
Test loss: 0.2468 score: 0.9070 time: 0.17s
Epoch 151/1000, LR 0.000271
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.26s
Val loss: 0.2020 score: 0.9535 time: 0.18s
Test loss: 0.2439 score: 0.9070 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.26s
Val loss: 0.2010 score: 0.9612 time: 0.17s
Test loss: 0.2408 score: 0.9147 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.26s
Val loss: 0.1994 score: 0.9612 time: 0.17s
Test loss: 0.2379 score: 0.9147 time: 0.23s
Epoch 154/1000, LR 0.000271
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.27s
Val loss: 0.1974 score: 0.9612 time: 0.23s
Test loss: 0.2351 score: 0.9147 time: 0.19s
Epoch 155/1000, LR 0.000270
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.26s
Val loss: 0.1953 score: 0.9612 time: 0.17s
Test loss: 0.2324 score: 0.9147 time: 0.18s
Epoch 156/1000, LR 0.000270
Train loss: 0.6412;  Loss pred: 0.6412; Loss self: 0.0000; time: 0.27s
Val loss: 0.1925 score: 0.9612 time: 0.18s
Test loss: 0.2297 score: 0.9147 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.27s
Val loss: 0.1900 score: 0.9612 time: 0.18s
Test loss: 0.2273 score: 0.9147 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.27s
Val loss: 0.1872 score: 0.9535 time: 0.18s
Test loss: 0.2249 score: 0.9147 time: 0.20s
Epoch 159/1000, LR 0.000270
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.34s
Val loss: 0.1854 score: 0.9535 time: 0.26s
Test loss: 0.2224 score: 0.9147 time: 0.26s
Epoch 160/1000, LR 0.000269
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.31s
Val loss: 0.1835 score: 0.9535 time: 0.18s
Test loss: 0.2202 score: 0.9147 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.6284;  Loss pred: 0.6284; Loss self: 0.0000; time: 0.27s
Val loss: 0.1821 score: 0.9535 time: 0.18s
Test loss: 0.2178 score: 0.9147 time: 0.18s
Epoch 162/1000, LR 0.000269
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.27s
Val loss: 0.1807 score: 0.9535 time: 0.18s
Test loss: 0.2156 score: 0.9225 time: 0.18s
Epoch 163/1000, LR 0.000269
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.27s
Val loss: 0.1796 score: 0.9535 time: 0.21s
Test loss: 0.2134 score: 0.9225 time: 0.19s
Epoch 164/1000, LR 0.000269
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.35s
Val loss: 0.1785 score: 0.9535 time: 0.19s
Test loss: 0.2113 score: 0.9225 time: 0.19s
Epoch 165/1000, LR 0.000268
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.28s
Val loss: 0.1765 score: 0.9535 time: 0.19s
Test loss: 0.2094 score: 0.9225 time: 0.19s
Epoch 166/1000, LR 0.000268
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.27s
Val loss: 0.1753 score: 0.9535 time: 0.19s
Test loss: 0.2074 score: 0.9225 time: 0.19s
Epoch 167/1000, LR 0.000268
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.26s
Val loss: 0.1736 score: 0.9535 time: 0.18s
Test loss: 0.2056 score: 0.9225 time: 0.18s
Epoch 168/1000, LR 0.000268
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.37s
Val loss: 0.1721 score: 0.9535 time: 0.25s
Test loss: 0.2038 score: 0.9225 time: 0.26s
Epoch 169/1000, LR 0.000267
Train loss: 0.6158;  Loss pred: 0.6158; Loss self: 0.0000; time: 0.38s
Val loss: 0.1711 score: 0.9535 time: 0.22s
Test loss: 0.2020 score: 0.9225 time: 0.18s
Epoch 170/1000, LR 0.000267
Train loss: 0.6167;  Loss pred: 0.6167; Loss self: 0.0000; time: 0.27s
Val loss: 0.1697 score: 0.9535 time: 0.17s
Test loss: 0.2004 score: 0.9225 time: 0.17s
Epoch 171/1000, LR 0.000267
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.26s
Val loss: 0.1692 score: 0.9535 time: 0.17s
Test loss: 0.1985 score: 0.9302 time: 0.18s
Epoch 172/1000, LR 0.000267
Train loss: 0.6141;  Loss pred: 0.6141; Loss self: 0.0000; time: 0.34s
Val loss: 0.1679 score: 0.9535 time: 0.25s
Test loss: 0.1969 score: 0.9302 time: 0.25s
Epoch 173/1000, LR 0.000267
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.38s
Val loss: 0.1667 score: 0.9535 time: 0.26s
Test loss: 0.1954 score: 0.9302 time: 0.26s
Epoch 174/1000, LR 0.000266
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.39s
Val loss: 0.1662 score: 0.9535 time: 0.25s
Test loss: 0.1938 score: 0.9225 time: 0.25s
Epoch 175/1000, LR 0.000266
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.39s
Val loss: 0.1652 score: 0.9535 time: 0.25s
Test loss: 0.1923 score: 0.9225 time: 0.25s
Epoch 176/1000, LR 0.000266
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.40s
Val loss: 0.1638 score: 0.9535 time: 0.18s
Test loss: 0.1910 score: 0.9225 time: 0.18s
Epoch 177/1000, LR 0.000266
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.27s
Val loss: 0.1626 score: 0.9535 time: 0.18s
Test loss: 0.1897 score: 0.9225 time: 0.18s
Epoch 178/1000, LR 0.000265
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.27s
Val loss: 0.1614 score: 0.9535 time: 0.18s
Test loss: 0.1885 score: 0.9225 time: 0.18s
Epoch 179/1000, LR 0.000265
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 0.27s
Val loss: 0.1600 score: 0.9535 time: 0.17s
Test loss: 0.1874 score: 0.9302 time: 0.18s
Epoch 180/1000, LR 0.000265
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.27s
Val loss: 0.1590 score: 0.9535 time: 0.18s
Test loss: 0.1862 score: 0.9302 time: 0.18s
Epoch 181/1000, LR 0.000265
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.34s
Val loss: 0.1583 score: 0.9535 time: 0.18s
Test loss: 0.1850 score: 0.9225 time: 0.18s
Epoch 182/1000, LR 0.000265
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.28s
Val loss: 0.1582 score: 0.9535 time: 0.18s
Test loss: 0.1837 score: 0.9225 time: 0.18s
Epoch 183/1000, LR 0.000264
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.27s
Val loss: 0.1580 score: 0.9535 time: 0.18s
Test loss: 0.1825 score: 0.9225 time: 0.19s
Epoch 184/1000, LR 0.000264
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.27s
Val loss: 0.1578 score: 0.9535 time: 0.19s
Test loss: 0.1814 score: 0.9225 time: 0.19s
Epoch 185/1000, LR 0.000264
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.26s
Val loss: 0.1572 score: 0.9535 time: 0.17s
Test loss: 0.1804 score: 0.9225 time: 0.18s
Epoch 186/1000, LR 0.000264
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.28s
Val loss: 0.1578 score: 0.9535 time: 0.18s
Test loss: 0.1793 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.36s
Val loss: 0.1574 score: 0.9535 time: 0.22s
Test loss: 0.1783 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.27s
Val loss: 0.1554 score: 0.9535 time: 0.17s
Test loss: 0.1774 score: 0.9225 time: 0.17s
Epoch 189/1000, LR 0.000263
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.27s
Val loss: 0.1542 score: 0.9535 time: 0.18s
Test loss: 0.1766 score: 0.9225 time: 0.17s
Epoch 190/1000, LR 0.000263
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.26s
Val loss: 0.1525 score: 0.9535 time: 0.17s
Test loss: 0.1760 score: 0.9225 time: 0.17s
Epoch 191/1000, LR 0.000262
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 0.26s
Val loss: 0.1515 score: 0.9535 time: 0.17s
Test loss: 0.1753 score: 0.9225 time: 0.17s
Epoch 192/1000, LR 0.000262
Train loss: 0.5867;  Loss pred: 0.5867; Loss self: 0.0000; time: 0.26s
Val loss: 0.1514 score: 0.9535 time: 0.18s
Test loss: 0.1744 score: 0.9225 time: 0.18s
Epoch 193/1000, LR 0.000262
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.27s
Val loss: 0.1519 score: 0.9535 time: 0.23s
Test loss: 0.1735 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 194/1000, LR 0.000262
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.26s
Val loss: 0.1523 score: 0.9535 time: 0.21s
Test loss: 0.1726 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.26s
Val loss: 0.1523 score: 0.9535 time: 0.17s
Test loss: 0.1718 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5834;  Loss pred: 0.5834; Loss self: 0.0000; time: 0.28s
Val loss: 0.1523 score: 0.9535 time: 0.25s
Test loss: 0.1711 score: 0.9302 time: 0.27s
     INFO: Early stopping counter 4 of 20
Epoch 197/1000, LR 0.000261
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.38s
Val loss: 0.1513 score: 0.9535 time: 0.27s
Test loss: 0.1705 score: 0.9302 time: 0.27s
Epoch 198/1000, LR 0.000261
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.39s
Val loss: 0.1509 score: 0.9535 time: 0.26s
Test loss: 0.1699 score: 0.9302 time: 0.27s
Epoch 199/1000, LR 0.000260
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.41s
Val loss: 0.1501 score: 0.9535 time: 0.20s
Test loss: 0.1694 score: 0.9302 time: 0.19s
Epoch 200/1000, LR 0.000260
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 0.26s
Val loss: 0.1490 score: 0.9535 time: 0.17s
Test loss: 0.1690 score: 0.9225 time: 0.18s
Epoch 201/1000, LR 0.000260
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.27s
Val loss: 0.1489 score: 0.9535 time: 0.17s
Test loss: 0.1684 score: 0.9302 time: 0.17s
Epoch 202/1000, LR 0.000260
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.27s
Val loss: 0.1480 score: 0.9535 time: 0.23s
Test loss: 0.1680 score: 0.9225 time: 0.26s
Epoch 203/1000, LR 0.000259
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.27s
Val loss: 0.1468 score: 0.9535 time: 0.17s
Test loss: 0.1678 score: 0.9225 time: 0.18s
Epoch 204/1000, LR 0.000259
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 0.26s
Val loss: 0.1458 score: 0.9535 time: 0.18s
Test loss: 0.1676 score: 0.9225 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5755;  Loss pred: 0.5755; Loss self: 0.0000; time: 0.26s
Val loss: 0.1461 score: 0.9535 time: 0.17s
Test loss: 0.1669 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 206/1000, LR 0.000259
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 0.26s
Val loss: 0.1467 score: 0.9535 time: 0.20s
Test loss: 0.1662 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.31s
Val loss: 0.1473 score: 0.9535 time: 0.26s
Test loss: 0.1656 score: 0.9302 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.39s
Val loss: 0.1475 score: 0.9535 time: 0.26s
Test loss: 0.1651 score: 0.9302 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 0.37s
Val loss: 0.1477 score: 0.9535 time: 0.18s
Test loss: 0.1647 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.27s
Val loss: 0.1475 score: 0.9535 time: 0.19s
Test loss: 0.1643 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.27s
Val loss: 0.1468 score: 0.9535 time: 0.19s
Test loss: 0.1640 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 0.27s
Val loss: 0.1458 score: 0.9535 time: 0.18s
Test loss: 0.1638 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.26s
Val loss: 0.1452 score: 0.9535 time: 0.18s
Test loss: 0.1636 score: 0.9302 time: 0.18s
Epoch 214/1000, LR 0.000256
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.26s
Val loss: 0.1452 score: 0.9535 time: 0.22s
Test loss: 0.1632 score: 0.9302 time: 0.18s
Epoch 215/1000, LR 0.000256
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.33s
Val loss: 0.1442 score: 0.9535 time: 0.18s
Test loss: 0.1633 score: 0.9302 time: 0.18s
Epoch 216/1000, LR 0.000256
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 0.27s
Val loss: 0.1437 score: 0.9535 time: 0.18s
Test loss: 0.1631 score: 0.9302 time: 0.18s
Epoch 217/1000, LR 0.000256
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.27s
Val loss: 0.1434 score: 0.9535 time: 0.18s
Test loss: 0.1629 score: 0.9302 time: 0.18s
Epoch 218/1000, LR 0.000255
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.26s
Val loss: 0.1431 score: 0.9535 time: 0.18s
Test loss: 0.1625 score: 0.9302 time: 0.18s
Epoch 219/1000, LR 0.000255
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.27s
Val loss: 0.1430 score: 0.9535 time: 0.18s
Test loss: 0.1622 score: 0.9302 time: 0.18s
Epoch 220/1000, LR 0.000255
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.28s
Val loss: 0.1429 score: 0.9535 time: 0.18s
Test loss: 0.1620 score: 0.9302 time: 0.18s
Epoch 221/1000, LR 0.000255
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.37s
Val loss: 0.1428 score: 0.9535 time: 0.25s
Test loss: 0.1617 score: 0.9302 time: 0.26s
Epoch 222/1000, LR 0.000254
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.39s
Val loss: 0.1430 score: 0.9535 time: 0.18s
Test loss: 0.1614 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.27s
Val loss: 0.1423 score: 0.9535 time: 0.18s
Test loss: 0.1614 score: 0.9302 time: 0.18s
Epoch 224/1000, LR 0.000254
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.26s
Val loss: 0.1423 score: 0.9535 time: 0.18s
Test loss: 0.1613 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.28s
Val loss: 0.1430 score: 0.9535 time: 0.18s
Test loss: 0.1609 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.27s
Val loss: 0.1433 score: 0.9535 time: 0.18s
Test loss: 0.1606 score: 0.9302 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.38s
Val loss: 0.1427 score: 0.9535 time: 0.22s
Test loss: 0.1606 score: 0.9302 time: 0.29s
     INFO: Early stopping counter 4 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.27s
Val loss: 0.1428 score: 0.9535 time: 0.19s
Test loss: 0.1604 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.28s
Val loss: 0.1423 score: 0.9535 time: 0.19s
Test loss: 0.1605 score: 0.9302 time: 0.20s
Epoch 230/1000, LR 0.000252
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.37s
Val loss: 0.1423 score: 0.9535 time: 0.23s
Test loss: 0.1603 score: 0.9302 time: 0.28s
     INFO: Early stopping counter 1 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5584;  Loss pred: 0.5584; Loss self: 0.0000; time: 0.27s
Val loss: 0.1415 score: 0.9457 time: 0.20s
Test loss: 0.1605 score: 0.9302 time: 0.19s
Epoch 232/1000, LR 0.000251
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.29s
Val loss: 0.1413 score: 0.9457 time: 0.20s
Test loss: 0.1605 score: 0.9302 time: 0.19s
Epoch 233/1000, LR 0.000251
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.27s
Val loss: 0.1414 score: 0.9457 time: 0.19s
Test loss: 0.1604 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5547;  Loss pred: 0.5547; Loss self: 0.0000; time: 0.28s
Val loss: 0.1415 score: 0.9457 time: 0.19s
Test loss: 0.1602 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5522;  Loss pred: 0.5522; Loss self: 0.0000; time: 0.27s
Val loss: 0.1415 score: 0.9457 time: 0.22s
Test loss: 0.1602 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.27s
Val loss: 0.1416 score: 0.9457 time: 0.28s
Test loss: 0.1601 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.27s
Val loss: 0.1423 score: 0.9535 time: 0.19s
Test loss: 0.1597 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.27s
Val loss: 0.1423 score: 0.9535 time: 0.17s
Test loss: 0.1596 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.26s
Val loss: 0.1428 score: 0.9535 time: 0.18s
Test loss: 0.1595 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.26s
Val loss: 0.1423 score: 0.9535 time: 0.18s
Test loss: 0.1597 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.26s
Val loss: 0.1416 score: 0.9457 time: 0.18s
Test loss: 0.1600 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.27s
Val loss: 0.1410 score: 0.9457 time: 0.26s
Test loss: 0.1605 score: 0.9302 time: 0.18s
Epoch 243/1000, LR 0.000248
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.27s
Val loss: 0.1408 score: 0.9457 time: 0.18s
Test loss: 0.1606 score: 0.9302 time: 0.18s
Epoch 244/1000, LR 0.000248
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.27s
Val loss: 0.1414 score: 0.9457 time: 0.18s
Test loss: 0.1603 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.26s
Val loss: 0.1418 score: 0.9457 time: 0.18s
Test loss: 0.1602 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.27s
Val loss: 0.1417 score: 0.9457 time: 0.17s
Test loss: 0.1603 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.26s
Val loss: 0.1416 score: 0.9457 time: 0.17s
Test loss: 0.1604 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.26s
Val loss: 0.1418 score: 0.9457 time: 0.17s
Test loss: 0.1603 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5476;  Loss pred: 0.5476; Loss self: 0.0000; time: 0.31s
Val loss: 0.1424 score: 0.9457 time: 0.18s
Test loss: 0.1602 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 6 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.29s
Val loss: 0.1425 score: 0.9457 time: 0.18s
Test loss: 0.1602 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.26s
Val loss: 0.1424 score: 0.9457 time: 0.18s
Test loss: 0.1604 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.26s
Val loss: 0.1421 score: 0.9457 time: 0.18s
Test loss: 0.1607 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.25s
Val loss: 0.1416 score: 0.9457 time: 0.21s
Test loss: 0.1611 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.38s
Val loss: 0.1412 score: 0.9457 time: 0.27s
Test loss: 0.1616 score: 0.9302 time: 0.27s
     INFO: Early stopping counter 11 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.39s
Val loss: 0.1419 score: 0.9457 time: 0.26s
Test loss: 0.1612 score: 0.9302 time: 0.27s
     INFO: Early stopping counter 12 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.28s
Val loss: 0.1422 score: 0.9457 time: 0.19s
Test loss: 0.1612 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.27s
Val loss: 0.1424 score: 0.9457 time: 0.19s
Test loss: 0.1611 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.27s
Val loss: 0.1428 score: 0.9457 time: 0.19s
Test loss: 0.1611 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 15 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.28s
Val loss: 0.1431 score: 0.9457 time: 0.20s
Test loss: 0.1610 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.39s
Val loss: 0.1432 score: 0.9457 time: 0.26s
Test loss: 0.1612 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 17 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.28s
Val loss: 0.1426 score: 0.9457 time: 0.19s
Test loss: 0.1616 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5399;  Loss pred: 0.5399; Loss self: 0.0000; time: 0.27s
Val loss: 0.1426 score: 0.9457 time: 0.19s
Test loss: 0.1618 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.27s
Val loss: 0.1424 score: 0.9457 time: 0.19s
Test loss: 0.1621 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 242,   Train_Loss: 0.5507,   Val_Loss: 0.1408,   Val_Precision: 0.9385,   Val_Recall: 0.9531,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1408,   Test_Precision: 0.9375,   Test_Recall: 0.9231,   Test_accuracy: 0.9302,   Test_Score: 0.9302,   Test_loss: 0.1606


[0.18168740393593907, 0.17993690306320786, 0.1788145659957081, 0.18228567112237215, 0.17690204712562263, 0.23888468882068992, 0.1784298368729651, 0.17704606195911765, 0.1796930010896176, 0.1843955221120268, 0.24733551195822656, 0.17931395303457975, 0.18048186507076025, 0.1778149229940027, 0.17880575102753937, 0.23898538784123957, 0.1790525179821998, 0.19404341815970838, 0.17513673100620508, 0.17378399590961635, 0.20219180290587246, 0.18160838400945067, 0.17443248187191784, 0.17985065700486302, 0.1760360209736973, 0.1748143839649856, 0.17443963303230703, 0.17921761516481638, 0.1787264528684318, 0.17682491592131555, 0.17457409808412194, 0.18594638397917151, 0.17429695511236787, 0.17443967098370194, 0.1782113970257342, 0.17641414608806372, 0.21121117984876037, 0.17939056386239827, 0.1752404139842838, 0.18276874697767198, 0.17966612498275936, 0.192024446092546, 0.19318971200846136, 0.2067842809483409, 0.19302397314459085, 0.1833941820077598, 0.17694903095252812, 0.18498046998865902, 0.17853786516934633, 0.20084986207075417, 0.18180940300226212, 0.2711539741139859, 0.2624143429566175, 0.17874474404379725, 0.17818864597938955, 0.1814644020050764, 0.18645196803845465, 0.18458584509789944, 0.2840159230399877, 0.19888689694926143, 0.25765787390992045, 0.20396828604862094, 0.18155831098556519, 0.17833115998655558, 0.19287341693416238, 0.1892456419300288, 0.27581674396060407, 0.17991724400781095, 0.1804673708975315, 0.1753166678827256, 0.2727769650518894, 0.19896838790737092, 0.1809561219997704, 0.1839779729489237, 0.18009839789010584, 0.18052981700748205, 0.1837447788566351, 0.18350780988112092, 0.18404719303362072, 0.17554380814544857, 0.1882991751190275, 0.18174860207363963, 0.24145855312235653, 0.1928718478884548, 0.19240265805274248, 0.2746984148398042, 0.22677652910351753, 0.19415098777972162, 0.19210935290902853, 0.17775634094141424, 0.18165841279551387, 0.25601488910615444, 0.1794003490358591, 0.18573276000097394, 0.1888417990412563, 0.18386748409830034, 0.18383482890203595, 0.17860724101774395, 0.19624479906633496, 0.27565432502888143, 0.2034821540582925, 0.2558202329091728, 0.25811801804229617, 0.18344635097309947, 0.19853700790554285, 0.1796335419639945, 0.17684747814200819, 0.20011176099069417, 0.24623317481018603, 0.17966424906626344, 0.1834364589303732, 0.1822256059385836, 0.26235823705792427, 0.25986084598116577, 0.1887125219218433, 0.1965301581658423, 0.19337897305376828, 0.3067359779961407, 0.18638653797097504, 0.18868466489948332, 0.26443788688629866, 0.18255002703517675, 0.18193056201562285, 0.18148718285374343, 0.1833625710569322, 0.19024271913804114, 0.252867846051231, 0.19650478102266788, 0.1942193869035691, 0.198158104903996, 0.1813597660511732, 0.1971800089813769, 0.1952816939447075, 0.19016393087804317, 0.18926978600211442, 0.19252808392047882, 0.18658543098717928, 0.2016567059326917, 0.1910969540476799, 0.18093420611694455, 0.1764968449715525, 0.17857850599102676, 0.1797091148328036, 0.17773758410476148, 0.18031929503194988, 0.1879357371944934, 0.18536841706372797, 0.17826207005418837, 0.17854330386035144, 0.17912889597937465, 0.17736377799883485, 0.18204319500364363, 0.23631161102093756, 0.1898831590078771, 0.18929122388362885, 0.18035321193747222, 0.18469136906787753, 0.20786970085464418, 0.26187392603605986, 0.1892113438807428, 0.18339773220941424, 0.18340318300761282, 0.19694175594486296, 0.1983655800577253, 0.19352237205021083, 0.1969647048972547, 0.18619839707389474, 0.26222395710647106, 0.18019392108544707, 0.17845997191034257, 0.17968398402445018, 0.25380648695863783, 0.26214759005233645, 0.25594697101041675, 0.25801608990877867, 0.18174890684895217, 0.18656321405433118, 0.1826921000611037, 0.18680506409145892, 0.185660783899948, 0.18310723593458533, 0.18395967315882444, 0.19128252402879298, 0.19214319507591426, 0.18352467892691493, 0.1824770870152861, 0.1797359159681946, 0.178780531976372, 0.17761349817737937, 0.1749681900255382, 0.17742997407913208, 0.179600786883384, 0.20665479800663888, 0.21230929694138467, 0.1791691528633237, 0.2767009718809277, 0.27262235805392265, 0.27480471320450306, 0.19470580713823438, 0.18040540697984397, 0.1784669968765229, 0.26820149295963347, 0.18159504095092416, 0.1793233088683337, 0.18320019193924963, 0.18140686000697315, 0.2581524520646781, 0.2580427611246705, 0.1858325768262148, 0.19660439412109554, 0.192632821155712, 0.18323963601142168, 0.18210981599986553, 0.18590736109763384, 0.18102639308199286, 0.18452469003386796, 0.18189326603896916, 0.18955694511532784, 0.1816487959586084, 0.18454485503025353, 0.2624757708981633, 0.18598547810688615, 0.18248570407740772, 0.18116677016951144, 0.2622371220495552, 0.258184903068468, 0.29449075693264604, 0.19812951982021332, 0.19960692385211587, 0.28071067901328206, 0.1972336838953197, 0.19629826000891626, 0.19859928591176867, 0.19713962194509804, 0.20224871509708464, 0.20172415091656148, 0.19808013108558953, 0.18747931299731135, 0.18480659509077668, 0.18180735502392054, 0.20587984914891422, 0.18432267010211945, 0.18113971618004143, 0.18134108209051192, 0.18504902301356196, 0.17904435703530908, 0.18304999195970595, 0.18289971607737243, 0.24081481690518558, 0.1796443669591099, 0.17653592000715435, 0.1805662871338427, 0.19790233811363578, 0.27462564199231565, 0.2764632028993219, 0.19782597105950117, 0.19928043615072966, 0.1963444270659238, 0.19771025399677455, 0.26518933195620775, 0.19867930398322642, 0.19360701693221927, 0.20502011198550463]
[0.0014084294878754966, 0.001394859713668278, 0.0013861594263233188, 0.001413067218002885, 0.0013713336986482375, 0.0018518192931836427, 0.0013831770300229852, 0.0013724500927063384, 0.0013929690006947101, 0.0014294226520312155, 0.0019173295500637718, 0.001390030643678913, 0.0013990842253547306, 0.0013784102557674628, 0.0013860910932367394, 0.0018525999057460432, 0.0013880040153658897, 0.0015042125438737084, 0.0013576490775674814, 0.001347162758989274, 0.0015673783170997864, 0.0014078169303058192, 0.0013521897819528516, 0.0013941911395725815, 0.0013646203176255606, 0.001355150263294462, 0.0013522452173047057, 0.0013892838384869488, 0.0013854763788250528, 0.0013707357823357794, 0.001353287582047457, 0.0014414448370478413, 0.0013511391869175803, 0.0013522455115015655, 0.0013814836978739084, 0.0013675515200625094, 0.001637295967819848, 0.0013906245260651027, 0.0013584528215835954, 0.0014168119920749767, 0.0013927606587810803, 0.0014885615976166357, 0.0014975946667322585, 0.001602978922080162, 0.0014963098693379136, 0.0014216603256415488, 0.0013716979143606832, 0.0014339571316950312, 0.0013840144586771033, 0.0015569756749670865, 0.0014093752170717994, 0.002101968791581286, 0.002034219712841996, 0.0013856181708821493, 0.0013813073331735623, 0.0014067007907370265, 0.0014453640933213539, 0.0014308980240147243, 0.0022016738220154083, 0.001541758891079546, 0.001997347859766825, 0.0015811495042528756, 0.0014074287673299627, 0.0013824120929190355, 0.0014951427669314913, 0.0014670204800777426, 0.0021381142942682486, 0.0013947073178900073, 0.001398971867422725, 0.0013590439370753922, 0.0021145501166813132, 0.001542390603933108, 0.0014027606356571349, 0.001426185836813362, 0.0013961116115512081, 0.0013994559457944345, 0.0014243781306715899, 0.0014225411618691544, 0.001426722426617215, 0.0013608047143058028, 0.0014596835280544767, 0.0014089038920437182, 0.0018717717296306707, 0.0014951306037864712, 0.0014914934732770736, 0.0021294450762775518, 0.0017579575899497482, 0.001505046416897067, 0.0014892197899924692, 0.0013779561313287925, 0.0014082047503528206, 0.0019846115434585614, 0.001390700380122939, 0.0014397888372168523, 0.0014638899150484985, 0.0014253293340953516, 0.0014250761930390385, 0.0013845522559515035, 0.0015212775121421316, 0.0021368552327820267, 0.0015773810392115699, 0.0019831025806912616, 0.0020009148685449316, 0.0014220647362255773, 0.001539046572911185, 0.0013925080772402676, 0.0013709106832713812, 0.0015512539611681718, 0.0019087843008541553, 0.0013927461167927398, 0.0014219880537238232, 0.0014126015964231289, 0.0020337847833947617, 0.0020144251626446957, 0.0014628877668359944, 0.001523489598184824, 0.0014990618066183588, 0.0023777982790398507, 0.0014448568834959305, 0.0014626718209262274, 0.002049906099893788, 0.001415116488644781, 0.0014103144342296344, 0.0014068773864631273, 0.0014214152795111025, 0.0014747497607600088, 0.001960215860862256, 0.0015232928761447122, 0.0015055766426633264, 0.0015361093403410543, 0.0014058896593114201, 0.0015285272014060223, 0.001513811580966725, 0.0014741389990545982, 0.0014672076434272434, 0.0014924657668254173, 0.0014463986898230951, 0.0015632302785479977, 0.0014813717368037202, 0.0014025907450925935, 0.0013681925966787015, 0.001384329503806409, 0.001393093913432586, 0.001377810729494275, 0.0013978239924957354, 0.0014568661798022744, 0.0014369644733622324, 0.001381876512047972, 0.0013840566190724919, 0.0013885960928633693, 0.0013749130077429059, 0.0014111875581677802, 0.0018318729536506788, 0.00147196247292928, 0.0014673738285552623, 0.0013980869142439707, 0.0014317160392858723, 0.0016113930298809625, 0.002030030434388061, 0.0014667546037266882, 0.0014216878465846065, 0.001421730100834208, 0.0015266802786423484, 0.0015377176748660876, 0.0015001734267458204, 0.0015268581774980985, 0.0014433984269294165, 0.0020327438535385355, 0.0013968521014375741, 0.001383410634963896, 0.00139289910096473, 0.001967492146966185, 0.0020321518608708254, 0.001984085046592378, 0.0020001247279750286, 0.00140890625464304, 0.001446226465537451, 0.0014162178299310364, 0.0014481012720268132, 0.0014392308829453333, 0.0014194359374774057, 0.0014260439779753833, 0.0014828102637890928, 0.0014894821323714284, 0.0014226719296660073, 0.0014145510621340006, 0.001393301674172051, 0.001385895596716062, 0.0013768488230804603, 0.0013563425583375055, 0.0013754261556521866, 0.0013922541618866975, 0.0016019751783460379, 0.0016458085034215867, 0.0013889081617311915, 0.002144968774270757, 0.002113351612821106, 0.0021302690946085508, 0.0015093473421568557, 0.0013984915269755346, 0.001383465092066069, 0.0020790813407723526, 0.0014077134957435982, 0.0013901031695219667, 0.0014201565266608498, 0.0014062547287362259, 0.002001181798950993, 0.0020003314815865932, 0.0014405626110559286, 0.001524065070706167, 0.0014932776833776124, 0.0014204622946621835, 0.0014117039999989576, 0.0014411423340901848, 0.001403305372728627, 0.0014304239537509143, 0.001410025318131544, 0.0014694336830645568, 0.0014081302012295225, 0.001430580271552353, 0.0020346958984353747, 0.001441747892301443, 0.0014146178610651761, 0.001404393567205515, 0.0020328459073608927, 0.002001433357119907, 0.002282874084749194, 0.0015358877505442894, 0.0015473404949776423, 0.00217605177529676, 0.0015289432860102303, 0.0015216919380536143, 0.0015395293481532456, 0.0015282141236054112, 0.0015678194968766252, 0.0015637531078803216, 0.001535504892136353, 0.0014533280077310958, 0.0014326092642695867, 0.0014093593412707018, 0.0015959678228598, 0.0014288579077683678, 0.0014041838463569104, 0.001405744822407069, 0.0014344885504927285, 0.0013879407522116984, 0.0014189921857341546, 0.0014178272564137398, 0.0018667815263967874, 0.0013925919919310845, 0.001368495503931429, 0.001399738659952269, 0.0015341266520436882, 0.0021288809456768655, 0.0021431256038707126, 0.001533534659375978, 0.0015448095825637959, 0.0015220498222164637, 0.0015326376278819733, 0.0020557312554744787, 0.0015401496432808249, 0.0015008295886218548, 0.0015893031936860824]
[710.0106953230722, 716.9179740449637, 721.4177395542612, 707.6804183549875, 729.2171124983863, 540.0094942745751, 722.9732552624752, 728.6239443709732, 717.8910654158663, 699.5831488881163, 521.5587481905441, 719.4086004847767, 714.7532520756248, 725.4734182482023, 721.4533048220112, 539.7819555633084, 720.4590108742527, 664.7996681537836, 736.5673623052239, 742.3008046557512, 638.0080603962671, 710.3196292594454, 739.5411600846339, 717.2617667808238, 732.804566284049, 737.9255475100837, 739.5108425624159, 719.7953163329729, 721.7734024798356, 729.5351977286, 738.9412370776724, 693.7483657356291, 740.1161994874478, 739.5106816731648, 723.8594284818499, 731.2338769908215, 610.7631238666987, 719.10136867037, 736.1315638729842, 705.8099490924416, 717.9984541458706, 671.7894654820594, 667.7374206880646, 623.8385210345217, 668.3107693745811, 703.402902904203, 729.0234894510844, 697.3709170914576, 722.5358042544152, 642.2707920733183, 709.5342587885565, 475.7444563426234, 491.588983081334, 721.6995424961505, 723.9518505287984, 710.8832287469322, 691.8671943081582, 698.8618218887901, 454.1998864684696, 648.6098480027547, 500.6639154567408, 632.4512623950257, 710.5155324465215, 723.3733017254267, 668.8324500624901, 681.6537421120437, 467.7018448830124, 716.9963096722379, 714.8106572309161, 735.8113838114459, 472.91383264514576, 648.3441985771907, 712.8799986118385, 701.1708952561033, 716.2751113350516, 714.563400873849, 702.0607649518622, 702.9673564496688, 700.9071851285175, 734.859300153246, 685.0800058919889, 709.7716215045917, 534.2531806468278, 668.8378911296876, 670.4689077873228, 469.6059133622191, 568.8419366411366, 664.4313349894458, 671.4925538325386, 725.7125080140821, 710.1240070021448, 503.87694422925233, 719.0621461623526, 694.5462932835519, 683.1114756104254, 701.5922398276426, 701.7168660066206, 722.2551519464115, 657.3422613681355, 467.9774205845823, 633.9622292530123, 504.26034928128837, 499.77138743898763, 703.202867299969, 649.7529169039044, 718.1286890499358, 729.4421235479153, 644.639772102145, 523.8936633921986, 718.0059509358618, 703.2407884027265, 707.9136838951025, 491.69411049030253, 496.41953374287743, 683.5794397015494, 656.3878094024793, 667.0839024681967, 420.55712160907007, 692.1100708469003, 683.6803619876648, 487.8272229405108, 706.6556061103303, 709.0617352620635, 710.793996422096, 703.5241666629272, 678.0811406842693, 510.14789746682385, 656.4725770469634, 664.1973391876123, 650.9953254876865, 711.2933745382139, 654.2245365866867, 660.5841919648923, 678.3620816227809, 681.5667874140191, 670.0321188117249, 691.3723076742465, 639.7010176445964, 675.0500061231414, 712.9663470964825, 730.8912520265845, 722.3713698583748, 717.8266952125274, 725.7890932283926, 715.3976504685377, 686.4048420258611, 695.9114289445056, 723.6536631757186, 722.5137947536695, 720.1518174647455, 727.3187426174889, 708.6230276139574, 545.889384963697, 679.3651457770858, 681.4895976334631, 715.2631140537925, 698.4625250819928, 620.5810633758745, 492.6034521750623, 681.7773044374488, 703.3892864754744, 703.3683815326443, 655.015993845996, 650.314434401676, 666.5895970235937, 654.9396759551005, 692.8094013011564, 491.94589778698975, 715.8954043673251, 722.851172837845, 717.9270912784667, 508.2612408603362, 492.0892081222101, 504.0106530299585, 499.9688199507551, 709.7704312863311, 691.4546399400712, 706.106065652835, 690.559437600911, 694.8155517296409, 704.5052006906213, 701.2406457616711, 674.3951160984375, 671.3742838981788, 702.9027417689787, 706.93807156837, 717.719657226591, 721.5550741120342, 726.2961504827186, 737.2768729057052, 727.0473924685759, 718.2596593174203, 624.2293972572357, 607.6041033455775, 719.9900090971886, 466.2072529890223, 473.1820270386069, 469.42426312754435, 662.5380202882931, 715.0561735348248, 722.8227193695204, 480.9816626166789, 710.3718214136811, 719.3710667848368, 704.1477338777966, 711.1087198964875, 499.7047247402479, 499.9171433360809, 694.1732294905271, 656.1399635887302, 669.6678127125838, 703.9961593896595, 708.3637929769544, 693.8939869747948, 712.6032718420878, 699.0934382619645, 709.2071235466342, 680.5342844152476, 710.1616023339606, 699.0170491550808, 491.47393513152144, 693.6025399029461, 706.9046896149198, 712.0511111353323, 491.9212009031382, 499.64191735018113, 438.04430856722615, 651.0892476651494, 646.2701669385632, 459.5478891413898, 654.046496786349, 657.1632371786717, 649.5491633203081, 654.3585643880639, 637.8285268120326, 639.4871383216671, 651.2515884001494, 688.0759158843834, 698.0270370580413, 709.5422513738642, 626.5790485726142, 699.8596533379791, 712.1574590068484, 711.3666606203046, 697.1125699515084, 720.4918498188697, 704.7255157945937, 705.3045393762605, 535.6813241719686, 718.0854161119485, 730.7294741759757, 714.4190759395757, 651.8366646377267, 469.73035388883886, 466.60820914737513, 652.0882941158418, 647.3289726364726, 657.0087164057245, 652.4699523278368, 486.4449072985429, 649.2875574543531, 666.2981644160252, 629.2065629596407]
Elapsed: 0.1987508925336622~0.030185325455454312
Time per graph: 0.001540704593284203~0.000233994770972514
Speed: 661.2941678881784~81.08950611794566
Total Time: 0.2062
best val loss: 0.14077794205310734 test_score: 0.9302

Testing...
Test loss: 0.6273 score: 0.9070 time: 0.20s
test Score 0.9070
Epoch Time List: [0.6481827420648187, 0.6002048871014267, 0.6042039359454066, 0.6065843638498336, 0.6033762781880796, 0.682153478031978, 0.6360694118775427, 0.6098216178361326, 0.6095646829344332, 0.6362422241363674, 0.882680713897571, 0.6083449230063707, 0.6114361039362848, 0.6095791701227427, 0.6107221541460603, 0.7330565981101245, 0.6252110020723194, 0.6340969728771597, 0.6013301422353834, 0.5911709719803184, 0.6187451761215925, 0.6612195211928338, 0.5870266172569245, 0.5894757478963584, 0.5854544590692967, 0.592657784698531, 0.593862900044769, 0.5936651821248233, 0.6759632320608944, 0.5971422391012311, 0.5823281379416585, 0.6151810972951353, 0.5928676691837609, 0.5886825113557279, 0.5997715478297323, 0.5953280460089445, 0.6681538720149547, 0.6616174920927733, 0.603164094965905, 0.6726443043444306, 0.6127253209706396, 0.6415551770478487, 0.6319464019034058, 0.6524375618901104, 0.715393346035853, 0.6269702578429133, 0.6320493817329407, 0.6270961260888726, 0.605056498432532, 0.7115374251734465, 0.6291808679234236, 0.7052385313436389, 0.7731248980853707, 0.7753542440477759, 0.7072891679126769, 0.6216274132020772, 0.7121247849427164, 0.6451224579941481, 0.8097315339837223, 0.6535383691079915, 0.8030074322596192, 0.8416529719252139, 0.6263104719109833, 0.6149992551654577, 0.6454315390437841, 0.634698050096631, 0.735549520002678, 0.7709165420383215, 0.6176349830348045, 0.6079736051615328, 0.8365920463111252, 0.802990835160017, 0.6072277883067727, 0.6164808529429138, 0.6097058970481157, 0.6121277550701052, 0.6244647779967636, 0.710300566162914, 0.6077039672527462, 0.6124712852761149, 0.6444336019922048, 0.650966206099838, 0.703047547955066, 0.645236881216988, 0.6476944389287382, 0.8658906309865415, 0.8535209749825299, 0.6405308519024402, 0.6447163298726082, 0.6243088277988136, 0.6085733957588673, 0.7818487307522446, 0.7323460718616843, 0.6390650228131562, 0.632952491287142, 0.6441728789359331, 0.6374857760965824, 0.6151000780519098, 0.6424551999662071, 0.7258461068850011, 0.6628649060148746, 0.8864745299797505, 0.899378992151469, 0.8519409550353885, 0.651549369096756, 0.6209665501955897, 0.612460358068347, 0.6361752280499786, 0.8010674465913326, 0.645163940731436, 0.621639811899513, 0.6256783329881728, 0.8002057757694274, 0.8946000749710947, 0.8098679869435728, 0.6524290291126817, 0.6486665159463882, 0.763941882411018, 0.6388484227936715, 0.6397047061473131, 0.7047948180697858, 0.6224633799865842, 0.6223167809657753, 0.619671787833795, 0.6242042179219425, 0.6417786350939423, 0.7367738622706383, 0.6554315851535648, 0.6489828543271869, 0.6541716582141817, 0.6137701829429716, 0.6479291240684688, 0.6676674820482731, 0.7298740979749709, 0.6503761929925531, 0.6425877041183412, 0.6195933721028268, 0.757266347296536, 0.7492962519172579, 0.627301107859239, 0.6585042721126229, 0.6060339119285345, 0.625022673048079, 0.6561586409807205, 0.693319687852636, 0.6234106591437012, 0.7124565420672297, 0.6707092167343944, 0.6062053302302957, 0.6062426341231912, 0.607752465410158, 0.6114050692413002, 0.6570089780725539, 0.6819819000083953, 0.618792254012078, 0.6243371551390737, 0.6291980659589171, 0.6523562781512737, 0.8523166121449322, 0.6721749487333, 0.6266973060555756, 0.6299425542820245, 0.6691390499472618, 0.7273296057246625, 0.6548047319520265, 0.6477365479804575, 0.6192102830391377, 0.8812252411153167, 0.7788043050095439, 0.6155605169478804, 0.6066699449438602, 0.8346381809096783, 0.8990426308009773, 0.8932295457925647, 0.8905169768258929, 0.7514091951306909, 0.6275368588976562, 0.6277242919895798, 0.6237345340196043, 0.6339434448163956, 0.6963936623651534, 0.6331913687754422, 0.6394660018850118, 0.6516352880280465, 0.6123517551459372, 0.6387786399573088, 0.7487797189969569, 0.6204007749911398, 0.6282875079195946, 0.6084526181221008, 0.6092228784691542, 0.6134092879947275, 0.7000927268527448, 0.6845485088415444, 0.6051796453539282, 0.8013759863097221, 0.9218486598692834, 0.9284981868695468, 0.794921533903107, 0.6153753807302564, 0.6143072419799864, 0.7664629297796637, 0.619035640032962, 0.6094994868617505, 0.6058941781520844, 0.6367112537845969, 0.8189896128606051, 0.9010511240921915, 0.7390868118964136, 0.6561266260687262, 0.6565959637518972, 0.626423382665962, 0.6174687102902681, 0.6558707340154797, 0.68850561324507, 0.6304966870229691, 0.6202601599507034, 0.6256621100474149, 0.6246884488500655, 0.6374530971515924, 0.8780030747875571, 0.7584795160219073, 0.6239053832832724, 0.614105385961011, 0.7145888768136501, 0.7016239592339844, 0.8841478517279029, 0.648579748114571, 0.658712127013132, 0.8758181370794773, 0.6624408019706607, 0.6791732322890311, 0.6536557867657393, 0.65975819574669, 0.6888496621977538, 0.7477934567723423, 0.6553528432268649, 0.6227110989857465, 0.6194629240781069, 0.6217008768580854, 0.6410483170766383, 0.7125802219379693, 0.6228088322095573, 0.6209909599274397, 0.623476912965998, 0.6211376700084656, 0.6143829617649317, 0.6152136470191181, 0.7248764638788998, 0.6420566907618195, 0.6097931996919215, 0.609866495244205, 0.6520504429936409, 0.9114905952010304, 0.9279870251193643, 0.6601345180533826, 0.6522978632710874, 0.6560092130675912, 0.672418205998838, 0.915079077007249, 0.6652315030805767, 0.6522672739811242, 0.6631804744247347]
Total Epoch List: [263]
Total Time List: [0.2061999780125916]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a289c3a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0607;  Loss pred: 2.0607; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7091 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7120 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 2.0675;  Loss pred: 2.0675; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7087 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7116 score: 0.4961 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 2.0446;  Loss pred: 2.0446; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7081 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7109 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 2.0361;  Loss pred: 2.0361; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7073 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7100 score: 0.4961 time: 0.24s
Epoch 5/1000, LR 0.000105
Train loss: 2.0078;  Loss pred: 2.0078; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7062 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7089 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 1.9803;  Loss pred: 1.9803; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7050 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7076 score: 0.4961 time: 0.19s
Epoch 7/1000, LR 0.000165
Train loss: 1.9310;  Loss pred: 1.9310; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7038 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7062 score: 0.4961 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 1.8982;  Loss pred: 1.8982; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7047 score: 0.4961 time: 0.20s
Epoch 9/1000, LR 0.000225
Train loss: 1.8548;  Loss pred: 1.8548; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7032 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 1.7875;  Loss pred: 1.7875; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 1.7434;  Loss pred: 1.7434; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.4961 time: 0.19s
Epoch 12/1000, LR 0.000285
Train loss: 1.6853;  Loss pred: 1.6853; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.4961 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 1.6267;  Loss pred: 1.6267; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.4961 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 1.5753;  Loss pred: 1.5753; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4961 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 1.5304;  Loss pred: 1.5304; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4961 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 1.4960;  Loss pred: 1.4960; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 1.4539;  Loss pred: 1.4539; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4961 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 1.4085;  Loss pred: 1.4085; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4961 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 1.3868;  Loss pred: 1.3868; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.19s
Epoch 20/1000, LR 0.000285
Train loss: 1.3631;  Loss pred: 1.3631; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.24s
Epoch 21/1000, LR 0.000285
Train loss: 1.3282;  Loss pred: 1.3282; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 1.2922;  Loss pred: 1.2922; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 1.2803;  Loss pred: 1.2803; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 1.2528;  Loss pred: 1.2528; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.2311;  Loss pred: 1.2311; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 1.2109;  Loss pred: 1.2109; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 1.2012;  Loss pred: 1.2012; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 1.1773;  Loss pred: 1.1773; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4961 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 1.1679;  Loss pred: 1.1679; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4961 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 1.1566;  Loss pred: 1.1566; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4961 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.1388;  Loss pred: 1.1388; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 1.1278;  Loss pred: 1.1278; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4961 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.1184;  Loss pred: 1.1184; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4961 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 1.1051;  Loss pred: 1.1051; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4961 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 1.1020;  Loss pred: 1.1020; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4961 time: 0.24s
Epoch 36/1000, LR 0.000285
Train loss: 1.0880;  Loss pred: 1.0880; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4961 time: 0.26s
Epoch 37/1000, LR 0.000285
Train loss: 1.0829;  Loss pred: 1.0829; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.4961 time: 0.26s
Epoch 38/1000, LR 0.000284
Train loss: 1.0759;  Loss pred: 1.0759; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4961 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 1.0669;  Loss pred: 1.0669; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.4961 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4961 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 1.0551;  Loss pred: 1.0551; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.4961 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 1.0498;  Loss pred: 1.0498; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6816 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.4961 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.0443;  Loss pred: 1.0443; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4961 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 1.0404;  Loss pred: 1.0404; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6815 score: 0.4961 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.0344;  Loss pred: 1.0344; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6786 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.4961 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 1.0302;  Loss pred: 1.0302; Loss self: 0.0000; time: 0.30s
Val loss: 0.6776 score: 0.5116 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.4961 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 1.0254;  Loss pred: 1.0254; Loss self: 0.0000; time: 0.46s
Val loss: 0.6765 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6786 score: 0.4961 time: 0.25s
Epoch 48/1000, LR 0.000284
Train loss: 1.0227;  Loss pred: 1.0227; Loss self: 0.0000; time: 0.28s
Val loss: 0.6753 score: 0.5194 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6775 score: 0.4961 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 1.0188;  Loss pred: 1.0188; Loss self: 0.0000; time: 0.32s
Val loss: 0.6740 score: 0.5271 time: 0.19s
Test loss: 0.6763 score: 0.5039 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 1.0127;  Loss pred: 1.0127; Loss self: 0.0000; time: 0.27s
Val loss: 0.6727 score: 0.5504 time: 0.17s
Test loss: 0.6751 score: 0.5349 time: 0.16s
Epoch 51/1000, LR 0.000284
Train loss: 1.0101;  Loss pred: 1.0101; Loss self: 0.0000; time: 0.27s
Val loss: 0.6713 score: 0.5891 time: 0.17s
Test loss: 0.6739 score: 0.5736 time: 0.16s
Epoch 52/1000, LR 0.000284
Train loss: 1.0076;  Loss pred: 1.0076; Loss self: 0.0000; time: 0.26s
Val loss: 0.6699 score: 0.6279 time: 0.17s
Test loss: 0.6725 score: 0.6124 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.26s
Val loss: 0.6683 score: 0.6589 time: 0.18s
Test loss: 0.6711 score: 0.6512 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 0.9991;  Loss pred: 0.9991; Loss self: 0.0000; time: 0.30s
Val loss: 0.6667 score: 0.6977 time: 0.22s
Test loss: 0.6696 score: 0.6899 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.9982;  Loss pred: 0.9982; Loss self: 0.0000; time: 0.26s
Val loss: 0.6649 score: 0.7287 time: 0.17s
Test loss: 0.6681 score: 0.7132 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 0.9942;  Loss pred: 0.9942; Loss self: 0.0000; time: 0.27s
Val loss: 0.6631 score: 0.7442 time: 0.18s
Test loss: 0.6664 score: 0.7287 time: 0.17s
Epoch 57/1000, LR 0.000283
Train loss: 0.9908;  Loss pred: 0.9908; Loss self: 0.0000; time: 0.26s
Val loss: 0.6613 score: 0.8062 time: 0.17s
Test loss: 0.6647 score: 0.7674 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 0.9875;  Loss pred: 0.9875; Loss self: 0.0000; time: 0.25s
Val loss: 0.6594 score: 0.8372 time: 0.18s
Test loss: 0.6630 score: 0.7907 time: 0.17s
Epoch 59/1000, LR 0.000283
Train loss: 0.9839;  Loss pred: 0.9839; Loss self: 0.0000; time: 0.26s
Val loss: 0.6573 score: 0.8527 time: 0.18s
Test loss: 0.6611 score: 0.8062 time: 0.16s
Epoch 60/1000, LR 0.000283
Train loss: 0.9794;  Loss pred: 0.9794; Loss self: 0.0000; time: 0.25s
Val loss: 0.6552 score: 0.8760 time: 0.18s
Test loss: 0.6592 score: 0.8217 time: 0.18s
Epoch 61/1000, LR 0.000283
Train loss: 0.9787;  Loss pred: 0.9787; Loss self: 0.0000; time: 0.26s
Val loss: 0.6529 score: 0.8837 time: 0.26s
Test loss: 0.6571 score: 0.8527 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 0.26s
Val loss: 0.6505 score: 0.8992 time: 0.18s
Test loss: 0.6550 score: 0.8682 time: 0.16s
Epoch 63/1000, LR 0.000283
Train loss: 0.9722;  Loss pred: 0.9722; Loss self: 0.0000; time: 0.26s
Val loss: 0.6480 score: 0.8992 time: 0.17s
Test loss: 0.6527 score: 0.8760 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 0.9696;  Loss pred: 0.9696; Loss self: 0.0000; time: 0.26s
Val loss: 0.6454 score: 0.8915 time: 0.19s
Test loss: 0.6503 score: 0.8837 time: 0.18s
Epoch 65/1000, LR 0.000283
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 0.27s
Val loss: 0.6428 score: 0.8915 time: 0.19s
Test loss: 0.6479 score: 0.8992 time: 0.18s
Epoch 66/1000, LR 0.000283
Train loss: 0.9627;  Loss pred: 0.9627; Loss self: 0.0000; time: 0.27s
Val loss: 0.6399 score: 0.8992 time: 0.19s
Test loss: 0.6453 score: 0.8992 time: 0.18s
Epoch 67/1000, LR 0.000283
Train loss: 0.9598;  Loss pred: 0.9598; Loss self: 0.0000; time: 0.33s
Val loss: 0.6369 score: 0.9070 time: 0.18s
Test loss: 0.6426 score: 0.8915 time: 0.16s
Epoch 68/1000, LR 0.000283
Train loss: 0.9566;  Loss pred: 0.9566; Loss self: 0.0000; time: 0.26s
Val loss: 0.6338 score: 0.9070 time: 0.18s
Test loss: 0.6398 score: 0.8915 time: 0.17s
Epoch 69/1000, LR 0.000283
Train loss: 0.9534;  Loss pred: 0.9534; Loss self: 0.0000; time: 0.27s
Val loss: 0.6307 score: 0.9070 time: 0.17s
Test loss: 0.6369 score: 0.8915 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 0.9502;  Loss pred: 0.9502; Loss self: 0.0000; time: 0.27s
Val loss: 0.6273 score: 0.9070 time: 0.17s
Test loss: 0.6339 score: 0.8992 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.9486;  Loss pred: 0.9486; Loss self: 0.0000; time: 0.27s
Val loss: 0.6239 score: 0.8992 time: 0.18s
Test loss: 0.6308 score: 0.8992 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 0.9443;  Loss pred: 0.9443; Loss self: 0.0000; time: 0.26s
Val loss: 0.6202 score: 0.8992 time: 0.19s
Test loss: 0.6275 score: 0.8992 time: 0.18s
Epoch 73/1000, LR 0.000282
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 0.39s
Val loss: 0.6165 score: 0.8915 time: 0.26s
Test loss: 0.6241 score: 0.8992 time: 0.25s
Epoch 74/1000, LR 0.000282
Train loss: 0.9389;  Loss pred: 0.9389; Loss self: 0.0000; time: 0.38s
Val loss: 0.6127 score: 0.8915 time: 0.22s
Test loss: 0.6206 score: 0.8992 time: 0.18s
Epoch 75/1000, LR 0.000282
Train loss: 0.9343;  Loss pred: 0.9343; Loss self: 0.0000; time: 0.28s
Val loss: 0.6086 score: 0.8915 time: 0.18s
Test loss: 0.6169 score: 0.8992 time: 0.18s
Epoch 76/1000, LR 0.000282
Train loss: 0.9310;  Loss pred: 0.9310; Loss self: 0.0000; time: 0.28s
Val loss: 0.6045 score: 0.8992 time: 0.19s
Test loss: 0.6131 score: 0.8992 time: 0.18s
Epoch 77/1000, LR 0.000282
Train loss: 0.9271;  Loss pred: 0.9271; Loss self: 0.0000; time: 0.27s
Val loss: 0.6001 score: 0.8915 time: 0.21s
Test loss: 0.6092 score: 0.8992 time: 0.24s
Epoch 78/1000, LR 0.000282
Train loss: 0.9245;  Loss pred: 0.9245; Loss self: 0.0000; time: 0.29s
Val loss: 0.5957 score: 0.8992 time: 0.18s
Test loss: 0.6052 score: 0.8992 time: 0.18s
Epoch 79/1000, LR 0.000282
Train loss: 0.9202;  Loss pred: 0.9202; Loss self: 0.0000; time: 0.28s
Val loss: 0.5911 score: 0.8992 time: 0.19s
Test loss: 0.6010 score: 0.8992 time: 0.18s
Epoch 80/1000, LR 0.000282
Train loss: 0.9164;  Loss pred: 0.9164; Loss self: 0.0000; time: 0.28s
Val loss: 0.5864 score: 0.8992 time: 0.19s
Test loss: 0.5967 score: 0.8992 time: 0.18s
Epoch 81/1000, LR 0.000281
Train loss: 0.9121;  Loss pred: 0.9121; Loss self: 0.0000; time: 0.28s
Val loss: 0.5816 score: 0.9070 time: 0.19s
Test loss: 0.5924 score: 0.9070 time: 0.19s
Epoch 82/1000, LR 0.000281
Train loss: 0.9091;  Loss pred: 0.9091; Loss self: 0.0000; time: 0.29s
Val loss: 0.5768 score: 0.9147 time: 0.26s
Test loss: 0.5881 score: 0.8992 time: 0.26s
Epoch 83/1000, LR 0.000281
Train loss: 0.9048;  Loss pred: 0.9048; Loss self: 0.0000; time: 0.32s
Val loss: 0.5718 score: 0.9147 time: 0.18s
Test loss: 0.5835 score: 0.8992 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9011;  Loss pred: 0.9011; Loss self: 0.0000; time: 0.26s
Val loss: 0.5667 score: 0.9147 time: 0.18s
Test loss: 0.5788 score: 0.9070 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.8973;  Loss pred: 0.8973; Loss self: 0.0000; time: 0.26s
Val loss: 0.5613 score: 0.9147 time: 0.18s
Test loss: 0.5740 score: 0.8992 time: 0.17s
Epoch 86/1000, LR 0.000281
Train loss: 0.8925;  Loss pred: 0.8925; Loss self: 0.0000; time: 0.27s
Val loss: 0.5561 score: 0.9147 time: 0.18s
Test loss: 0.5692 score: 0.9147 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.8887;  Loss pred: 0.8887; Loss self: 0.0000; time: 0.26s
Val loss: 0.5507 score: 0.9147 time: 0.18s
Test loss: 0.5643 score: 0.9147 time: 0.18s
Epoch 88/1000, LR 0.000281
Train loss: 0.8842;  Loss pred: 0.8842; Loss self: 0.0000; time: 0.27s
Val loss: 0.5451 score: 0.9147 time: 0.18s
Test loss: 0.5592 score: 0.9147 time: 0.18s
Epoch 89/1000, LR 0.000281
Train loss: 0.8801;  Loss pred: 0.8801; Loss self: 0.0000; time: 0.27s
Val loss: 0.5394 score: 0.9147 time: 0.26s
Test loss: 0.5541 score: 0.9147 time: 0.26s
Epoch 90/1000, LR 0.000281
Train loss: 0.8761;  Loss pred: 0.8761; Loss self: 0.0000; time: 0.39s
Val loss: 0.5338 score: 0.9070 time: 0.20s
Test loss: 0.5490 score: 0.9147 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8717;  Loss pred: 0.8717; Loss self: 0.0000; time: 0.27s
Val loss: 0.5281 score: 0.9070 time: 0.18s
Test loss: 0.5438 score: 0.9147 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.8696;  Loss pred: 0.8696; Loss self: 0.0000; time: 0.26s
Val loss: 0.5222 score: 0.9070 time: 0.18s
Test loss: 0.5384 score: 0.9147 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.8641;  Loss pred: 0.8641; Loss self: 0.0000; time: 0.26s
Val loss: 0.5162 score: 0.9070 time: 0.20s
Test loss: 0.5330 score: 0.9147 time: 0.18s
Epoch 94/1000, LR 0.000280
Train loss: 0.8586;  Loss pred: 0.8586; Loss self: 0.0000; time: 0.26s
Val loss: 0.5102 score: 0.9070 time: 0.17s
Test loss: 0.5275 score: 0.9147 time: 0.16s
Epoch 95/1000, LR 0.000280
Train loss: 0.8536;  Loss pred: 0.8536; Loss self: 0.0000; time: 0.25s
Val loss: 0.5042 score: 0.9070 time: 0.18s
Test loss: 0.5220 score: 0.9147 time: 0.16s
Epoch 96/1000, LR 0.000280
Train loss: 0.8501;  Loss pred: 0.8501; Loss self: 0.0000; time: 0.33s
Val loss: 0.4981 score: 0.9070 time: 0.17s
Test loss: 0.5164 score: 0.9147 time: 0.16s
Epoch 97/1000, LR 0.000280
Train loss: 0.8466;  Loss pred: 0.8466; Loss self: 0.0000; time: 0.26s
Val loss: 0.4919 score: 0.9070 time: 0.20s
Test loss: 0.5108 score: 0.9147 time: 0.18s
Epoch 98/1000, LR 0.000280
Train loss: 0.8402;  Loss pred: 0.8402; Loss self: 0.0000; time: 0.27s
Val loss: 0.4858 score: 0.9070 time: 0.19s
Test loss: 0.5052 score: 0.9147 time: 0.19s
Epoch 99/1000, LR 0.000279
Train loss: 0.8353;  Loss pred: 0.8353; Loss self: 0.0000; time: 0.27s
Val loss: 0.4797 score: 0.9070 time: 0.19s
Test loss: 0.4996 score: 0.9147 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8308;  Loss pred: 0.8308; Loss self: 0.0000; time: 0.26s
Val loss: 0.4736 score: 0.9070 time: 0.18s
Test loss: 0.4940 score: 0.9225 time: 0.17s
Epoch 101/1000, LR 0.000279
Train loss: 0.8264;  Loss pred: 0.8264; Loss self: 0.0000; time: 0.28s
Val loss: 0.4674 score: 0.9070 time: 0.26s
Test loss: 0.4883 score: 0.9225 time: 0.21s
Epoch 102/1000, LR 0.000279
Train loss: 0.8226;  Loss pred: 0.8226; Loss self: 0.0000; time: 0.28s
Val loss: 0.4611 score: 0.9070 time: 0.19s
Test loss: 0.4826 score: 0.9225 time: 0.18s
Epoch 103/1000, LR 0.000279
Train loss: 0.8168;  Loss pred: 0.8168; Loss self: 0.0000; time: 0.29s
Val loss: 0.4549 score: 0.9070 time: 0.18s
Test loss: 0.4769 score: 0.9225 time: 0.18s
Epoch 104/1000, LR 0.000279
Train loss: 0.8135;  Loss pred: 0.8135; Loss self: 0.0000; time: 0.29s
Val loss: 0.4487 score: 0.9070 time: 0.19s
Test loss: 0.4711 score: 0.9225 time: 0.18s
Epoch 105/1000, LR 0.000279
Train loss: 0.8081;  Loss pred: 0.8081; Loss self: 0.0000; time: 0.27s
Val loss: 0.4423 score: 0.9070 time: 0.18s
Test loss: 0.4654 score: 0.9225 time: 0.18s
Epoch 106/1000, LR 0.000279
Train loss: 0.8023;  Loss pred: 0.8023; Loss self: 0.0000; time: 0.27s
Val loss: 0.4362 score: 0.9070 time: 0.17s
Test loss: 0.4597 score: 0.9225 time: 0.16s
Epoch 107/1000, LR 0.000278
Train loss: 0.7999;  Loss pred: 0.7999; Loss self: 0.0000; time: 0.26s
Val loss: 0.4302 score: 0.9070 time: 0.17s
Test loss: 0.4541 score: 0.9147 time: 0.16s
Epoch 108/1000, LR 0.000278
Train loss: 0.7952;  Loss pred: 0.7952; Loss self: 0.0000; time: 0.27s
Val loss: 0.4241 score: 0.9070 time: 0.17s
Test loss: 0.4486 score: 0.9147 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.7904;  Loss pred: 0.7904; Loss self: 0.0000; time: 0.26s
Val loss: 0.4180 score: 0.9070 time: 0.26s
Test loss: 0.4429 score: 0.9147 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.7865;  Loss pred: 0.7865; Loss self: 0.0000; time: 0.26s
Val loss: 0.4119 score: 0.9070 time: 0.17s
Test loss: 0.4373 score: 0.9147 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.7816;  Loss pred: 0.7816; Loss self: 0.0000; time: 0.26s
Val loss: 0.4059 score: 0.9070 time: 0.23s
Test loss: 0.4318 score: 0.9147 time: 0.24s
Epoch 112/1000, LR 0.000278
Train loss: 0.7759;  Loss pred: 0.7759; Loss self: 0.0000; time: 0.38s
Val loss: 0.4000 score: 0.9070 time: 0.25s
Test loss: 0.4264 score: 0.9147 time: 0.21s
Epoch 113/1000, LR 0.000278
Train loss: 0.7740;  Loss pred: 0.7740; Loss self: 0.0000; time: 0.33s
Val loss: 0.3942 score: 0.9070 time: 0.17s
Test loss: 0.4211 score: 0.9147 time: 0.16s
Epoch 114/1000, LR 0.000277
Train loss: 0.7687;  Loss pred: 0.7687; Loss self: 0.0000; time: 0.26s
Val loss: 0.3886 score: 0.9070 time: 0.17s
Test loss: 0.4158 score: 0.9070 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.7628;  Loss pred: 0.7628; Loss self: 0.0000; time: 0.29s
Val loss: 0.3827 score: 0.9070 time: 0.23s
Test loss: 0.4105 score: 0.9070 time: 0.24s
Epoch 116/1000, LR 0.000277
Train loss: 0.7588;  Loss pred: 0.7588; Loss self: 0.0000; time: 0.36s
Val loss: 0.3771 score: 0.9070 time: 0.18s
Test loss: 0.4053 score: 0.9070 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7541;  Loss pred: 0.7541; Loss self: 0.0000; time: 0.27s
Val loss: 0.3714 score: 0.9070 time: 0.18s
Test loss: 0.4001 score: 0.9070 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.7500;  Loss pred: 0.7500; Loss self: 0.0000; time: 0.27s
Val loss: 0.3659 score: 0.9147 time: 0.18s
Test loss: 0.3950 score: 0.9070 time: 0.17s
Epoch 119/1000, LR 0.000277
Train loss: 0.7468;  Loss pred: 0.7468; Loss self: 0.0000; time: 0.27s
Val loss: 0.3605 score: 0.9147 time: 0.18s
Test loss: 0.3900 score: 0.9070 time: 0.20s
Epoch 120/1000, LR 0.000277
Train loss: 0.7421;  Loss pred: 0.7421; Loss self: 0.0000; time: 0.33s
Val loss: 0.3552 score: 0.9147 time: 0.26s
Test loss: 0.3852 score: 0.9070 time: 0.25s
Epoch 121/1000, LR 0.000276
Train loss: 0.7380;  Loss pred: 0.7380; Loss self: 0.0000; time: 0.40s
Val loss: 0.3499 score: 0.9147 time: 0.26s
Test loss: 0.3803 score: 0.9070 time: 0.25s
Epoch 122/1000, LR 0.000276
Train loss: 0.7330;  Loss pred: 0.7330; Loss self: 0.0000; time: 0.39s
Val loss: 0.3447 score: 0.9147 time: 0.22s
Test loss: 0.3755 score: 0.9070 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.7297;  Loss pred: 0.7297; Loss self: 0.0000; time: 0.27s
Val loss: 0.3396 score: 0.9147 time: 0.18s
Test loss: 0.3708 score: 0.9070 time: 0.17s
Epoch 124/1000, LR 0.000276
Train loss: 0.7248;  Loss pred: 0.7248; Loss self: 0.0000; time: 0.28s
Val loss: 0.3345 score: 0.9147 time: 0.19s
Test loss: 0.3661 score: 0.9070 time: 0.25s
Epoch 125/1000, LR 0.000276
Train loss: 0.7209;  Loss pred: 0.7209; Loss self: 0.0000; time: 0.39s
Val loss: 0.3295 score: 0.9147 time: 0.26s
Test loss: 0.3615 score: 0.9070 time: 0.25s
Epoch 126/1000, LR 0.000276
Train loss: 0.7174;  Loss pred: 0.7174; Loss self: 0.0000; time: 0.39s
Val loss: 0.3247 score: 0.9147 time: 0.26s
Test loss: 0.3571 score: 0.9070 time: 0.20s
Epoch 127/1000, LR 0.000275
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.27s
Val loss: 0.3199 score: 0.9147 time: 0.17s
Test loss: 0.3528 score: 0.9070 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7111;  Loss pred: 0.7111; Loss self: 0.0000; time: 0.27s
Val loss: 0.3154 score: 0.9147 time: 0.17s
Test loss: 0.3487 score: 0.9070 time: 0.17s
Epoch 129/1000, LR 0.000275
Train loss: 0.7055;  Loss pred: 0.7055; Loss self: 0.0000; time: 0.26s
Val loss: 0.3110 score: 0.9147 time: 0.17s
Test loss: 0.3446 score: 0.9070 time: 0.17s
Epoch 130/1000, LR 0.000275
Train loss: 0.7008;  Loss pred: 0.7008; Loss self: 0.0000; time: 0.27s
Val loss: 0.3066 score: 0.9225 time: 0.18s
Test loss: 0.3406 score: 0.9070 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.33s
Val loss: 0.3024 score: 0.9302 time: 0.18s
Test loss: 0.3368 score: 0.9070 time: 0.17s
Epoch 132/1000, LR 0.000275
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.27s
Val loss: 0.2982 score: 0.9302 time: 0.20s
Test loss: 0.3329 score: 0.9070 time: 0.18s
Epoch 133/1000, LR 0.000274
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.36s
Val loss: 0.2941 score: 0.9302 time: 0.18s
Test loss: 0.3292 score: 0.9070 time: 0.26s
Epoch 134/1000, LR 0.000274
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.27s
Val loss: 0.2899 score: 0.9302 time: 0.18s
Test loss: 0.3254 score: 0.9070 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.27s
Val loss: 0.2860 score: 0.9302 time: 0.18s
Test loss: 0.3218 score: 0.9070 time: 0.17s
Epoch 136/1000, LR 0.000274
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.27s
Val loss: 0.2823 score: 0.9302 time: 0.18s
Test loss: 0.3185 score: 0.9070 time: 0.17s
Epoch 137/1000, LR 0.000274
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.27s
Val loss: 0.2786 score: 0.9302 time: 0.23s
Test loss: 0.3152 score: 0.9070 time: 0.19s
Epoch 138/1000, LR 0.000274
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.36s
Val loss: 0.2750 score: 0.9302 time: 0.23s
Test loss: 0.3119 score: 0.9070 time: 0.28s
Epoch 139/1000, LR 0.000273
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.28s
Val loss: 0.2715 score: 0.9302 time: 0.18s
Test loss: 0.3088 score: 0.9070 time: 0.17s
Epoch 140/1000, LR 0.000273
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.28s
Val loss: 0.2681 score: 0.9302 time: 0.18s
Test loss: 0.3058 score: 0.9070 time: 0.18s
Epoch 141/1000, LR 0.000273
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.30s
Val loss: 0.2647 score: 0.9302 time: 0.26s
Test loss: 0.3027 score: 0.9070 time: 0.25s
Epoch 142/1000, LR 0.000273
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.39s
Val loss: 0.2615 score: 0.9302 time: 0.26s
Test loss: 0.2999 score: 0.9070 time: 0.26s
Epoch 143/1000, LR 0.000273
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.39s
Val loss: 0.2583 score: 0.9302 time: 0.26s
Test loss: 0.2970 score: 0.9070 time: 0.26s
Epoch 144/1000, LR 0.000272
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.32s
Val loss: 0.2554 score: 0.9302 time: 0.18s
Test loss: 0.2945 score: 0.9070 time: 0.17s
Epoch 145/1000, LR 0.000272
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.28s
Val loss: 0.2523 score: 0.9302 time: 0.18s
Test loss: 0.2917 score: 0.9070 time: 0.17s
Epoch 146/1000, LR 0.000272
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.28s
Val loss: 0.2494 score: 0.9302 time: 0.18s
Test loss: 0.2892 score: 0.9070 time: 0.18s
Epoch 147/1000, LR 0.000272
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.27s
Val loss: 0.2468 score: 0.9302 time: 0.18s
Test loss: 0.2870 score: 0.9070 time: 0.18s
Epoch 148/1000, LR 0.000272
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.27s
Val loss: 0.2444 score: 0.9302 time: 0.18s
Test loss: 0.2850 score: 0.9070 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.27s
Val loss: 0.2418 score: 0.9302 time: 0.18s
Test loss: 0.2828 score: 0.9070 time: 0.17s
Epoch 150/1000, LR 0.000271
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.32s
Val loss: 0.2392 score: 0.9302 time: 0.18s
Test loss: 0.2805 score: 0.9070 time: 0.26s
Epoch 151/1000, LR 0.000271
Train loss: 0.6373;  Loss pred: 0.6373; Loss self: 0.0000; time: 0.28s
Val loss: 0.2366 score: 0.9302 time: 0.18s
Test loss: 0.2782 score: 0.9070 time: 0.18s
Epoch 152/1000, LR 0.000271
Train loss: 0.6343;  Loss pred: 0.6343; Loss self: 0.0000; time: 0.28s
Val loss: 0.2343 score: 0.9302 time: 0.19s
Test loss: 0.2761 score: 0.9070 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.28s
Val loss: 0.2321 score: 0.9302 time: 0.18s
Test loss: 0.2744 score: 0.9070 time: 0.18s
Epoch 154/1000, LR 0.000271
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.29s
Val loss: 0.2302 score: 0.9302 time: 0.19s
Test loss: 0.2729 score: 0.9070 time: 0.25s
Epoch 155/1000, LR 0.000270
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.39s
Val loss: 0.2283 score: 0.9302 time: 0.20s
Test loss: 0.2713 score: 0.9070 time: 0.18s
Epoch 156/1000, LR 0.000270
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.29s
Val loss: 0.2262 score: 0.9302 time: 0.19s
Test loss: 0.2696 score: 0.9070 time: 0.18s
Epoch 157/1000, LR 0.000270
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.29s
Val loss: 0.2243 score: 0.9380 time: 0.19s
Test loss: 0.2681 score: 0.9070 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.29s
Val loss: 0.2220 score: 0.9302 time: 0.18s
Test loss: 0.2659 score: 0.9070 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.30s
Val loss: 0.2200 score: 0.9225 time: 0.19s
Test loss: 0.2641 score: 0.9147 time: 0.24s
Epoch 160/1000, LR 0.000269
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.32s
Val loss: 0.2183 score: 0.9302 time: 0.18s
Test loss: 0.2629 score: 0.9147 time: 0.18s
Epoch 161/1000, LR 0.000269
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.27s
Val loss: 0.2168 score: 0.9302 time: 0.18s
Test loss: 0.2617 score: 0.9147 time: 0.17s
Epoch 162/1000, LR 0.000269
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.29s
Val loss: 0.2153 score: 0.9302 time: 0.18s
Test loss: 0.2606 score: 0.9147 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.29s
Val loss: 0.2139 score: 0.9302 time: 0.19s
Test loss: 0.2596 score: 0.9147 time: 0.17s
Epoch 164/1000, LR 0.000269
Train loss: 0.6082;  Loss pred: 0.6082; Loss self: 0.0000; time: 0.28s
Val loss: 0.2122 score: 0.9302 time: 0.18s
Test loss: 0.2582 score: 0.9147 time: 0.17s
Epoch 165/1000, LR 0.000268
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.27s
Val loss: 0.2106 score: 0.9302 time: 0.18s
Test loss: 0.2568 score: 0.9147 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.29s
Val loss: 0.2093 score: 0.9302 time: 0.19s
Test loss: 0.2559 score: 0.9147 time: 0.18s
Epoch 167/1000, LR 0.000268
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.32s
Val loss: 0.2079 score: 0.9302 time: 0.22s
Test loss: 0.2547 score: 0.9147 time: 0.18s
Epoch 168/1000, LR 0.000268
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.28s
Val loss: 0.2066 score: 0.9302 time: 0.18s
Test loss: 0.2538 score: 0.9147 time: 0.18s
Epoch 169/1000, LR 0.000267
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.28s
Val loss: 0.2053 score: 0.9225 time: 0.19s
Test loss: 0.2528 score: 0.9147 time: 0.18s
Epoch 170/1000, LR 0.000267
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 0.28s
Val loss: 0.2043 score: 0.9302 time: 0.19s
Test loss: 0.2522 score: 0.9147 time: 0.18s
Epoch 171/1000, LR 0.000267
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.28s
Val loss: 0.2033 score: 0.9302 time: 0.18s
Test loss: 0.2515 score: 0.9147 time: 0.18s
Epoch 172/1000, LR 0.000267
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 0.28s
Val loss: 0.2021 score: 0.9302 time: 0.21s
Test loss: 0.2505 score: 0.9147 time: 0.18s
Epoch 173/1000, LR 0.000267
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.35s
Val loss: 0.2010 score: 0.9302 time: 0.19s
Test loss: 0.2498 score: 0.9147 time: 0.18s
Epoch 174/1000, LR 0.000266
Train loss: 0.5927;  Loss pred: 0.5927; Loss self: 0.0000; time: 0.29s
Val loss: 0.2000 score: 0.9225 time: 0.19s
Test loss: 0.2490 score: 0.9147 time: 0.18s
Epoch 175/1000, LR 0.000266
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.28s
Val loss: 0.1991 score: 0.9302 time: 0.18s
Test loss: 0.2484 score: 0.9147 time: 0.17s
Epoch 176/1000, LR 0.000266
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.28s
Val loss: 0.1983 score: 0.9302 time: 0.19s
Test loss: 0.2480 score: 0.9147 time: 0.20s
Epoch 177/1000, LR 0.000266
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.31s
Val loss: 0.1970 score: 0.9225 time: 0.26s
Test loss: 0.2470 score: 0.9147 time: 0.26s
Epoch 178/1000, LR 0.000265
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 0.38s
Val loss: 0.1961 score: 0.9225 time: 0.18s
Test loss: 0.2463 score: 0.9147 time: 0.18s
Epoch 179/1000, LR 0.000265
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.28s
Val loss: 0.1952 score: 0.9225 time: 0.19s
Test loss: 0.2457 score: 0.9147 time: 0.18s
Epoch 180/1000, LR 0.000265
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.28s
Val loss: 0.1943 score: 0.9225 time: 0.20s
Test loss: 0.2450 score: 0.9147 time: 0.18s
Epoch 181/1000, LR 0.000265
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.28s
Val loss: 0.1934 score: 0.9225 time: 0.19s
Test loss: 0.2444 score: 0.9147 time: 0.18s
Epoch 182/1000, LR 0.000265
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.29s
Val loss: 0.1929 score: 0.9225 time: 0.20s
Test loss: 0.2442 score: 0.9147 time: 0.20s
Epoch 183/1000, LR 0.000264
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.28s
Val loss: 0.1926 score: 0.9225 time: 0.25s
Test loss: 0.2444 score: 0.9147 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.27s
Val loss: 0.1922 score: 0.9380 time: 0.19s
Test loss: 0.2444 score: 0.9147 time: 0.20s
Epoch 185/1000, LR 0.000264
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.27s
Val loss: 0.1916 score: 0.9380 time: 0.20s
Test loss: 0.2442 score: 0.9147 time: 0.19s
Epoch 186/1000, LR 0.000264
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.30s
Val loss: 0.1911 score: 0.9380 time: 0.20s
Test loss: 0.2441 score: 0.9147 time: 0.19s
Epoch 187/1000, LR 0.000263
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.27s
Val loss: 0.1902 score: 0.9302 time: 0.18s
Test loss: 0.2433 score: 0.9147 time: 0.19s
Epoch 188/1000, LR 0.000263
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.35s
Val loss: 0.1893 score: 0.9225 time: 0.20s
Test loss: 0.2426 score: 0.9147 time: 0.20s
Epoch 189/1000, LR 0.000263
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 0.29s
Val loss: 0.1888 score: 0.9225 time: 0.20s
Test loss: 0.2424 score: 0.9147 time: 0.19s
Epoch 190/1000, LR 0.000263
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.28s
Val loss: 0.1884 score: 0.9225 time: 0.18s
Test loss: 0.2424 score: 0.9147 time: 0.18s
Epoch 191/1000, LR 0.000262
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.28s
Val loss: 0.1879 score: 0.9225 time: 0.19s
Test loss: 0.2422 score: 0.9147 time: 0.18s
Epoch 192/1000, LR 0.000262
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 0.28s
Val loss: 0.1877 score: 0.9380 time: 0.18s
Test loss: 0.2426 score: 0.9147 time: 0.18s
Epoch 193/1000, LR 0.000262
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.28s
Val loss: 0.1871 score: 0.9225 time: 0.19s
Test loss: 0.2422 score: 0.9147 time: 0.20s
Epoch 194/1000, LR 0.000262
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.28s
Val loss: 0.1865 score: 0.9225 time: 0.24s
Test loss: 0.2418 score: 0.9147 time: 0.18s
Epoch 195/1000, LR 0.000261
Train loss: 0.5641;  Loss pred: 0.5641; Loss self: 0.0000; time: 0.29s
Val loss: 0.1861 score: 0.9225 time: 0.19s
Test loss: 0.2418 score: 0.9147 time: 0.19s
Epoch 196/1000, LR 0.000261
Train loss: 0.5641;  Loss pred: 0.5641; Loss self: 0.0000; time: 0.29s
Val loss: 0.1855 score: 0.9225 time: 0.18s
Test loss: 0.2414 score: 0.9147 time: 0.17s
Epoch 197/1000, LR 0.000261
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.28s
Val loss: 0.1851 score: 0.9225 time: 0.18s
Test loss: 0.2412 score: 0.9147 time: 0.19s
Epoch 198/1000, LR 0.000261
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.29s
Val loss: 0.1849 score: 0.9225 time: 0.19s
Test loss: 0.2415 score: 0.9147 time: 0.23s
Epoch 199/1000, LR 0.000260
Train loss: 0.5647;  Loss pred: 0.5647; Loss self: 0.0000; time: 0.39s
Val loss: 0.1850 score: 0.9302 time: 0.28s
Test loss: 0.2421 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.27s
Val loss: 0.1846 score: 0.9302 time: 0.19s
Test loss: 0.2421 score: 0.9147 time: 0.18s
Epoch 201/1000, LR 0.000260
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.29s
Val loss: 0.1841 score: 0.9225 time: 0.21s
Test loss: 0.2417 score: 0.9147 time: 0.23s
Epoch 202/1000, LR 0.000260
Train loss: 0.5582;  Loss pred: 0.5582; Loss self: 0.0000; time: 0.41s
Val loss: 0.1836 score: 0.9225 time: 0.26s
Test loss: 0.2414 score: 0.9147 time: 0.26s
Epoch 203/1000, LR 0.000259
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.41s
Val loss: 0.1830 score: 0.9225 time: 0.27s
Test loss: 0.2409 score: 0.9147 time: 0.26s
Epoch 204/1000, LR 0.000259
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.41s
Val loss: 0.1828 score: 0.9225 time: 0.27s
Test loss: 0.2410 score: 0.9147 time: 0.25s
Epoch 205/1000, LR 0.000259
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.40s
Val loss: 0.1826 score: 0.9225 time: 0.23s
Test loss: 0.2412 score: 0.9147 time: 0.18s
Epoch 206/1000, LR 0.000259
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.28s
Val loss: 0.1825 score: 0.9225 time: 0.18s
Test loss: 0.2415 score: 0.9147 time: 0.18s
Epoch 207/1000, LR 0.000258
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 0.29s
Val loss: 0.1825 score: 0.9225 time: 0.20s
Test loss: 0.2420 score: 0.9147 time: 0.19s
Epoch 208/1000, LR 0.000258
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.27s
Val loss: 0.1825 score: 0.9225 time: 0.18s
Test loss: 0.2424 score: 0.9147 time: 0.18s
Epoch 209/1000, LR 0.000258
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.28s
Val loss: 0.1824 score: 0.9225 time: 0.18s
Test loss: 0.2427 score: 0.9147 time: 0.18s
Epoch 210/1000, LR 0.000258
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.29s
Val loss: 0.1823 score: 0.9302 time: 0.19s
Test loss: 0.2431 score: 0.9147 time: 0.27s
Epoch 211/1000, LR 0.000257
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.29s
Val loss: 0.1820 score: 0.9225 time: 0.20s
Test loss: 0.2430 score: 0.9147 time: 0.18s
Epoch 212/1000, LR 0.000257
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.29s
Val loss: 0.1818 score: 0.9225 time: 0.19s
Test loss: 0.2431 score: 0.9147 time: 0.18s
Epoch 213/1000, LR 0.000257
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.28s
Val loss: 0.1815 score: 0.9225 time: 0.19s
Test loss: 0.2430 score: 0.9147 time: 0.18s
Epoch 214/1000, LR 0.000256
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.28s
Val loss: 0.1811 score: 0.9225 time: 0.20s
Test loss: 0.2428 score: 0.9147 time: 0.19s
Epoch 215/1000, LR 0.000256
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.29s
Val loss: 0.1810 score: 0.9225 time: 0.19s
Test loss: 0.2429 score: 0.9147 time: 0.19s
Epoch 216/1000, LR 0.000256
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.29s
Val loss: 0.1810 score: 0.9225 time: 0.21s
Test loss: 0.2433 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.36s
Val loss: 0.1811 score: 0.9225 time: 0.19s
Test loss: 0.2439 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.28s
Val loss: 0.1812 score: 0.9225 time: 0.19s
Test loss: 0.2445 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.28s
Val loss: 0.1813 score: 0.9302 time: 0.24s
Test loss: 0.2452 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.28s
Val loss: 0.1814 score: 0.9302 time: 0.19s
Test loss: 0.2457 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.29s
Val loss: 0.1813 score: 0.9302 time: 0.19s
Test loss: 0.2458 score: 0.9147 time: 0.29s
     INFO: Early stopping counter 6 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.28s
Val loss: 0.1812 score: 0.9302 time: 0.20s
Test loss: 0.2460 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.29s
Val loss: 0.1808 score: 0.9225 time: 0.21s
Test loss: 0.2457 score: 0.9147 time: 0.19s
Epoch 224/1000, LR 0.000254
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.29s
Val loss: 0.1807 score: 0.9225 time: 0.27s
Test loss: 0.2458 score: 0.9147 time: 0.18s
Epoch 225/1000, LR 0.000253
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.28s
Val loss: 0.1805 score: 0.9225 time: 0.19s
Test loss: 0.2458 score: 0.9147 time: 0.19s
Epoch 226/1000, LR 0.000253
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.27s
Val loss: 0.1806 score: 0.9225 time: 0.20s
Test loss: 0.2464 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.32s
Val loss: 0.1807 score: 0.9225 time: 0.26s
Test loss: 0.2469 score: 0.9147 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.28s
Val loss: 0.1807 score: 0.9225 time: 0.20s
Test loss: 0.2471 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 0.29s
Val loss: 0.1807 score: 0.9225 time: 0.20s
Test loss: 0.2475 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.29s
Val loss: 0.1806 score: 0.9225 time: 0.20s
Test loss: 0.2476 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 0.30s
Val loss: 0.1805 score: 0.9225 time: 0.19s
Test loss: 0.2477 score: 0.9147 time: 0.18s
Epoch 232/1000, LR 0.000251
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.31s
Val loss: 0.1804 score: 0.9225 time: 0.19s
Test loss: 0.2478 score: 0.9147 time: 0.18s
Epoch 233/1000, LR 0.000251
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.32s
Val loss: 0.1805 score: 0.9225 time: 0.21s
Test loss: 0.2483 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5347;  Loss pred: 0.5347; Loss self: 0.0000; time: 0.28s
Val loss: 0.1807 score: 0.9225 time: 0.18s
Test loss: 0.2489 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.29s
Val loss: 0.1807 score: 0.9225 time: 0.26s
Test loss: 0.2494 score: 0.9147 time: 0.26s
     INFO: Early stopping counter 3 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.40s
Val loss: 0.1805 score: 0.9225 time: 0.26s
Test loss: 0.2492 score: 0.9147 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.39s
Val loss: 0.1807 score: 0.9225 time: 0.26s
Test loss: 0.2499 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.28s
Val loss: 0.1809 score: 0.9225 time: 0.18s
Test loss: 0.2505 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.29s
Val loss: 0.1807 score: 0.9225 time: 0.18s
Test loss: 0.2504 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.29s
Val loss: 0.1808 score: 0.9225 time: 0.18s
Test loss: 0.2508 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.29s
Val loss: 0.1808 score: 0.9225 time: 0.18s
Test loss: 0.2511 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.31s
Val loss: 0.1808 score: 0.9225 time: 0.19s
Test loss: 0.2513 score: 0.9147 time: 0.25s
     INFO: Early stopping counter 10 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.37s
Val loss: 0.1810 score: 0.9225 time: 0.19s
Test loss: 0.2519 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.31s
Val loss: 0.1813 score: 0.9225 time: 0.19s
Test loss: 0.2526 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.30s
Val loss: 0.1813 score: 0.9225 time: 0.20s
Test loss: 0.2529 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.30s
Val loss: 0.1815 score: 0.9225 time: 0.20s
Test loss: 0.2533 score: 0.9147 time: 0.27s
     INFO: Early stopping counter 14 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.29s
Val loss: 0.1816 score: 0.9302 time: 0.19s
Test loss: 0.2538 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.29s
Val loss: 0.1818 score: 0.9302 time: 0.19s
Test loss: 0.2544 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.29s
Val loss: 0.1819 score: 0.9302 time: 0.19s
Test loss: 0.2549 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.28s
Val loss: 0.1821 score: 0.9302 time: 0.19s
Test loss: 0.2555 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.28s
Val loss: 0.1821 score: 0.9302 time: 0.19s
Test loss: 0.2558 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.29s
Val loss: 0.1822 score: 0.9302 time: 0.19s
Test loss: 0.2561 score: 0.9147 time: 0.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 231,   Train_Loss: 0.5361,   Val_Loss: 0.1804,   Val_Precision: 0.9365,   Val_Recall: 0.9077,   Val_accuracy: 0.9219,   Val_Score: 0.9225,   Val_Loss: 0.1804,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2478


[0.18168740393593907, 0.17993690306320786, 0.1788145659957081, 0.18228567112237215, 0.17690204712562263, 0.23888468882068992, 0.1784298368729651, 0.17704606195911765, 0.1796930010896176, 0.1843955221120268, 0.24733551195822656, 0.17931395303457975, 0.18048186507076025, 0.1778149229940027, 0.17880575102753937, 0.23898538784123957, 0.1790525179821998, 0.19404341815970838, 0.17513673100620508, 0.17378399590961635, 0.20219180290587246, 0.18160838400945067, 0.17443248187191784, 0.17985065700486302, 0.1760360209736973, 0.1748143839649856, 0.17443963303230703, 0.17921761516481638, 0.1787264528684318, 0.17682491592131555, 0.17457409808412194, 0.18594638397917151, 0.17429695511236787, 0.17443967098370194, 0.1782113970257342, 0.17641414608806372, 0.21121117984876037, 0.17939056386239827, 0.1752404139842838, 0.18276874697767198, 0.17966612498275936, 0.192024446092546, 0.19318971200846136, 0.2067842809483409, 0.19302397314459085, 0.1833941820077598, 0.17694903095252812, 0.18498046998865902, 0.17853786516934633, 0.20084986207075417, 0.18180940300226212, 0.2711539741139859, 0.2624143429566175, 0.17874474404379725, 0.17818864597938955, 0.1814644020050764, 0.18645196803845465, 0.18458584509789944, 0.2840159230399877, 0.19888689694926143, 0.25765787390992045, 0.20396828604862094, 0.18155831098556519, 0.17833115998655558, 0.19287341693416238, 0.1892456419300288, 0.27581674396060407, 0.17991724400781095, 0.1804673708975315, 0.1753166678827256, 0.2727769650518894, 0.19896838790737092, 0.1809561219997704, 0.1839779729489237, 0.18009839789010584, 0.18052981700748205, 0.1837447788566351, 0.18350780988112092, 0.18404719303362072, 0.17554380814544857, 0.1882991751190275, 0.18174860207363963, 0.24145855312235653, 0.1928718478884548, 0.19240265805274248, 0.2746984148398042, 0.22677652910351753, 0.19415098777972162, 0.19210935290902853, 0.17775634094141424, 0.18165841279551387, 0.25601488910615444, 0.1794003490358591, 0.18573276000097394, 0.1888417990412563, 0.18386748409830034, 0.18383482890203595, 0.17860724101774395, 0.19624479906633496, 0.27565432502888143, 0.2034821540582925, 0.2558202329091728, 0.25811801804229617, 0.18344635097309947, 0.19853700790554285, 0.1796335419639945, 0.17684747814200819, 0.20011176099069417, 0.24623317481018603, 0.17966424906626344, 0.1834364589303732, 0.1822256059385836, 0.26235823705792427, 0.25986084598116577, 0.1887125219218433, 0.1965301581658423, 0.19337897305376828, 0.3067359779961407, 0.18638653797097504, 0.18868466489948332, 0.26443788688629866, 0.18255002703517675, 0.18193056201562285, 0.18148718285374343, 0.1833625710569322, 0.19024271913804114, 0.252867846051231, 0.19650478102266788, 0.1942193869035691, 0.198158104903996, 0.1813597660511732, 0.1971800089813769, 0.1952816939447075, 0.19016393087804317, 0.18926978600211442, 0.19252808392047882, 0.18658543098717928, 0.2016567059326917, 0.1910969540476799, 0.18093420611694455, 0.1764968449715525, 0.17857850599102676, 0.1797091148328036, 0.17773758410476148, 0.18031929503194988, 0.1879357371944934, 0.18536841706372797, 0.17826207005418837, 0.17854330386035144, 0.17912889597937465, 0.17736377799883485, 0.18204319500364363, 0.23631161102093756, 0.1898831590078771, 0.18929122388362885, 0.18035321193747222, 0.18469136906787753, 0.20786970085464418, 0.26187392603605986, 0.1892113438807428, 0.18339773220941424, 0.18340318300761282, 0.19694175594486296, 0.1983655800577253, 0.19352237205021083, 0.1969647048972547, 0.18619839707389474, 0.26222395710647106, 0.18019392108544707, 0.17845997191034257, 0.17968398402445018, 0.25380648695863783, 0.26214759005233645, 0.25594697101041675, 0.25801608990877867, 0.18174890684895217, 0.18656321405433118, 0.1826921000611037, 0.18680506409145892, 0.185660783899948, 0.18310723593458533, 0.18395967315882444, 0.19128252402879298, 0.19214319507591426, 0.18352467892691493, 0.1824770870152861, 0.1797359159681946, 0.178780531976372, 0.17761349817737937, 0.1749681900255382, 0.17742997407913208, 0.179600786883384, 0.20665479800663888, 0.21230929694138467, 0.1791691528633237, 0.2767009718809277, 0.27262235805392265, 0.27480471320450306, 0.19470580713823438, 0.18040540697984397, 0.1784669968765229, 0.26820149295963347, 0.18159504095092416, 0.1793233088683337, 0.18320019193924963, 0.18140686000697315, 0.2581524520646781, 0.2580427611246705, 0.1858325768262148, 0.19660439412109554, 0.192632821155712, 0.18323963601142168, 0.18210981599986553, 0.18590736109763384, 0.18102639308199286, 0.18452469003386796, 0.18189326603896916, 0.18955694511532784, 0.1816487959586084, 0.18454485503025353, 0.2624757708981633, 0.18598547810688615, 0.18248570407740772, 0.18116677016951144, 0.2622371220495552, 0.258184903068468, 0.29449075693264604, 0.19812951982021332, 0.19960692385211587, 0.28071067901328206, 0.1972336838953197, 0.19629826000891626, 0.19859928591176867, 0.19713962194509804, 0.20224871509708464, 0.20172415091656148, 0.19808013108558953, 0.18747931299731135, 0.18480659509077668, 0.18180735502392054, 0.20587984914891422, 0.18432267010211945, 0.18113971618004143, 0.18134108209051192, 0.18504902301356196, 0.17904435703530908, 0.18304999195970595, 0.18289971607737243, 0.24081481690518558, 0.1796443669591099, 0.17653592000715435, 0.1805662871338427, 0.19790233811363578, 0.27462564199231565, 0.2764632028993219, 0.19782597105950117, 0.19928043615072966, 0.1963444270659238, 0.19771025399677455, 0.26518933195620775, 0.19867930398322642, 0.19360701693221927, 0.20502011198550463, 0.1802920331247151, 0.1753831619862467, 0.1811112950090319, 0.2483785639051348, 0.18883854988962412, 0.18966347607783973, 0.1896190328989178, 0.20562671613879502, 0.18573754793033004, 0.1882908681873232, 0.19224560307338834, 0.19178669387474656, 0.19262000103481114, 0.18798826588317752, 0.18922094302251935, 0.18906449107453227, 0.1954092630185187, 0.19368316605687141, 0.1980601940304041, 0.2484677559696138, 0.1833145609125495, 0.18355662189424038, 0.18191270297393203, 0.1793722310103476, 0.17979942611418664, 0.17995452997274697, 0.17842229921370745, 0.17965890211053193, 0.1770287179388106, 0.1762514899019152, 0.18285556882619858, 0.17888285499066114, 0.17964874883182347, 0.1806462248787284, 0.24276216584257782, 0.2598722211550921, 0.2597331970464438, 0.19063687114976346, 0.18030333588831127, 0.17267546709626913, 0.17511430708691478, 0.18884316179901361, 0.1810079589486122, 0.1733140181750059, 0.18021191796287894, 0.18399698892608285, 0.2501429819967598, 0.18495635106228292, 0.1801813510246575, 0.16859115310944617, 0.16682780697010458, 0.17018711380660534, 0.172338942065835, 0.16985344304703176, 0.16961041488684714, 0.17212896002456546, 0.17206528410315514, 0.17062926501967013, 0.1665492309257388, 0.18750447710044682, 0.17274427507072687, 0.16915665799751878, 0.1733952770009637, 0.1822308418340981, 0.18455713614821434, 0.18430256005376577, 0.16885394416749477, 0.17235743696801364, 0.17034851689822972, 0.17260518996044993, 0.17121980106458068, 0.18521430692635477, 0.25401349109597504, 0.18040958605706692, 0.1800859358627349, 0.18210071604698896, 0.24440186494030058, 0.18106316099874675, 0.18409104901365936, 0.1832501501776278, 0.19813938392326236, 0.2616260398644954, 0.17228667088784277, 0.175611981889233, 0.17372494004666805, 0.17752021411433816, 0.18002100987359881, 0.18890480208210647, 0.26427693595178425, 0.17656637099571526, 0.17891383985988796, 0.17785048205405474, 0.18118969700299203, 0.16500380006618798, 0.16933613386936486, 0.1688686558045447, 0.183418198954314, 0.19245663611218333, 0.17916777916252613, 0.17800146201625466, 0.21648778882808983, 0.18142205104231834, 0.18766020704060793, 0.18813279806636274, 0.18126654205843806, 0.165900357067585, 0.16715226788073778, 0.17178493505343795, 0.17025723890401423, 0.16901552607305348, 0.24726916220970452, 0.21069943206384778, 0.1672998599242419, 0.16715987608768046, 0.24743621097877622, 0.17508694995194674, 0.17537454422563314, 0.17443449492566288, 0.20051067508757114, 0.2515318251680583, 0.25152816995978355, 0.1751227038912475, 0.1740553320851177, 0.2558939990121871, 0.253462360939011, 0.2089412328787148, 0.17703923396766186, 0.1747359309811145, 0.17650408297777176, 0.17595647601410747, 0.17481698794290423, 0.18239627894945443, 0.2639602078124881, 0.17551760817877948, 0.17632321896962821, 0.17330088117159903, 0.19051104807294905, 0.28318201401270926, 0.17840668093413115, 0.17991845402866602, 0.2515126448124647, 0.25994835793972015, 0.26130486908368766, 0.17823643703013659, 0.1790722890291363, 0.18021190795116127, 0.17964845383539796, 0.17801709100604057, 0.1783940449822694, 0.26226620585657656, 0.18499685800634325, 0.18485292606055737, 0.18414453114382923, 0.25323895714245737, 0.18577234004624188, 0.18057948886416852, 0.18118567089550197, 0.18056909809820354, 0.24151132395491004, 0.1840746260713786, 0.17680299212224782, 0.17888632300309837, 0.17788103315979242, 0.1753596479538828, 0.1764326300472021, 0.1825967978220433, 0.18272804887965322, 0.18073167488910258, 0.181890174979344, 0.18158667907118797, 0.18189655803143978, 0.18219552491791546, 0.18800539919175208, 0.1819156261626631, 0.17587476992048323, 0.20335526089183986, 0.2599627310410142, 0.1822793809697032, 0.18770882696844637, 0.1819497940596193, 0.18932873290032148, 0.20017106411978602, 0.1776065630838275, 0.2022371320053935, 0.19007672904990613, 0.19250879995524883, 0.19855448394082487, 0.2001423758920282, 0.195024506887421, 0.18564276094548404, 0.17956856288947165, 0.17965619009919465, 0.20392838586121798, 0.18067662185057998, 0.1912378049455583, 0.17795423907227814, 0.1966081541031599, 0.23317867796868086, 0.18154426896944642, 0.18756463890895247, 0.23639545310288668, 0.26415057899430394, 0.26190680102445185, 0.25557562289759517, 0.18164631188847125, 0.181515634059906, 0.1938612130470574, 0.18174590915441513, 0.17961207800544798, 0.27208637702278793, 0.18264919705688953, 0.18067347002215683, 0.18442333303391933, 0.18972592684440315, 0.19943136302754283, 0.19794633192941546, 0.22119683912023902, 0.19161030603572726, 0.18421697104349732, 0.1848851980175823, 0.2938428409397602, 0.19932278990745544, 0.19941812101751566, 0.1880698329769075, 0.19004999683238566, 0.18372177612036467, 0.2654381620232016, 0.19708245899528265, 0.19390313886106014, 0.1948069599457085, 0.18034358695149422, 0.18207829119637609, 0.18103137891739607, 0.18170549813658, 0.259817369049415, 0.26404107781127095, 0.236524673178792, 0.18146621505729854, 0.18263928592205048, 0.18136494513601065, 0.18339851102791727, 0.25560335582122207, 0.19794194004498422, 0.1953520851675421, 0.1920978759881109, 0.2786977009382099, 0.18681188696064055, 0.18286262499168515, 0.18567361496388912, 0.18161277007311583, 0.1832333360798657, 0.25822394993156195]
[0.0014084294878754966, 0.001394859713668278, 0.0013861594263233188, 0.001413067218002885, 0.0013713336986482375, 0.0018518192931836427, 0.0013831770300229852, 0.0013724500927063384, 0.0013929690006947101, 0.0014294226520312155, 0.0019173295500637718, 0.001390030643678913, 0.0013990842253547306, 0.0013784102557674628, 0.0013860910932367394, 0.0018525999057460432, 0.0013880040153658897, 0.0015042125438737084, 0.0013576490775674814, 0.001347162758989274, 0.0015673783170997864, 0.0014078169303058192, 0.0013521897819528516, 0.0013941911395725815, 0.0013646203176255606, 0.001355150263294462, 0.0013522452173047057, 0.0013892838384869488, 0.0013854763788250528, 0.0013707357823357794, 0.001353287582047457, 0.0014414448370478413, 0.0013511391869175803, 0.0013522455115015655, 0.0013814836978739084, 0.0013675515200625094, 0.001637295967819848, 0.0013906245260651027, 0.0013584528215835954, 0.0014168119920749767, 0.0013927606587810803, 0.0014885615976166357, 0.0014975946667322585, 0.001602978922080162, 0.0014963098693379136, 0.0014216603256415488, 0.0013716979143606832, 0.0014339571316950312, 0.0013840144586771033, 0.0015569756749670865, 0.0014093752170717994, 0.002101968791581286, 0.002034219712841996, 0.0013856181708821493, 0.0013813073331735623, 0.0014067007907370265, 0.0014453640933213539, 0.0014308980240147243, 0.0022016738220154083, 0.001541758891079546, 0.001997347859766825, 0.0015811495042528756, 0.0014074287673299627, 0.0013824120929190355, 0.0014951427669314913, 0.0014670204800777426, 0.0021381142942682486, 0.0013947073178900073, 0.001398971867422725, 0.0013590439370753922, 0.0021145501166813132, 0.001542390603933108, 0.0014027606356571349, 0.001426185836813362, 0.0013961116115512081, 0.0013994559457944345, 0.0014243781306715899, 0.0014225411618691544, 0.001426722426617215, 0.0013608047143058028, 0.0014596835280544767, 0.0014089038920437182, 0.0018717717296306707, 0.0014951306037864712, 0.0014914934732770736, 0.0021294450762775518, 0.0017579575899497482, 0.001505046416897067, 0.0014892197899924692, 0.0013779561313287925, 0.0014082047503528206, 0.0019846115434585614, 0.001390700380122939, 0.0014397888372168523, 0.0014638899150484985, 0.0014253293340953516, 0.0014250761930390385, 0.0013845522559515035, 0.0015212775121421316, 0.0021368552327820267, 0.0015773810392115699, 0.0019831025806912616, 0.0020009148685449316, 0.0014220647362255773, 0.001539046572911185, 0.0013925080772402676, 0.0013709106832713812, 0.0015512539611681718, 0.0019087843008541553, 0.0013927461167927398, 0.0014219880537238232, 0.0014126015964231289, 0.0020337847833947617, 0.0020144251626446957, 0.0014628877668359944, 0.001523489598184824, 0.0014990618066183588, 0.0023777982790398507, 0.0014448568834959305, 0.0014626718209262274, 0.002049906099893788, 0.001415116488644781, 0.0014103144342296344, 0.0014068773864631273, 0.0014214152795111025, 0.0014747497607600088, 0.001960215860862256, 0.0015232928761447122, 0.0015055766426633264, 0.0015361093403410543, 0.0014058896593114201, 0.0015285272014060223, 0.001513811580966725, 0.0014741389990545982, 0.0014672076434272434, 0.0014924657668254173, 0.0014463986898230951, 0.0015632302785479977, 0.0014813717368037202, 0.0014025907450925935, 0.0013681925966787015, 0.001384329503806409, 0.001393093913432586, 0.001377810729494275, 0.0013978239924957354, 0.0014568661798022744, 0.0014369644733622324, 0.001381876512047972, 0.0013840566190724919, 0.0013885960928633693, 0.0013749130077429059, 0.0014111875581677802, 0.0018318729536506788, 0.00147196247292928, 0.0014673738285552623, 0.0013980869142439707, 0.0014317160392858723, 0.0016113930298809625, 0.002030030434388061, 0.0014667546037266882, 0.0014216878465846065, 0.001421730100834208, 0.0015266802786423484, 0.0015377176748660876, 0.0015001734267458204, 0.0015268581774980985, 0.0014433984269294165, 0.0020327438535385355, 0.0013968521014375741, 0.001383410634963896, 0.00139289910096473, 0.001967492146966185, 0.0020321518608708254, 0.001984085046592378, 0.0020001247279750286, 0.00140890625464304, 0.001446226465537451, 0.0014162178299310364, 0.0014481012720268132, 0.0014392308829453333, 0.0014194359374774057, 0.0014260439779753833, 0.0014828102637890928, 0.0014894821323714284, 0.0014226719296660073, 0.0014145510621340006, 0.001393301674172051, 0.001385895596716062, 0.0013768488230804603, 0.0013563425583375055, 0.0013754261556521866, 0.0013922541618866975, 0.0016019751783460379, 0.0016458085034215867, 0.0013889081617311915, 0.002144968774270757, 0.002113351612821106, 0.0021302690946085508, 0.0015093473421568557, 0.0013984915269755346, 0.001383465092066069, 0.0020790813407723526, 0.0014077134957435982, 0.0013901031695219667, 0.0014201565266608498, 0.0014062547287362259, 0.002001181798950993, 0.0020003314815865932, 0.0014405626110559286, 0.001524065070706167, 0.0014932776833776124, 0.0014204622946621835, 0.0014117039999989576, 0.0014411423340901848, 0.001403305372728627, 0.0014304239537509143, 0.001410025318131544, 0.0014694336830645568, 0.0014081302012295225, 0.001430580271552353, 0.0020346958984353747, 0.001441747892301443, 0.0014146178610651761, 0.001404393567205515, 0.0020328459073608927, 0.002001433357119907, 0.002282874084749194, 0.0015358877505442894, 0.0015473404949776423, 0.00217605177529676, 0.0015289432860102303, 0.0015216919380536143, 0.0015395293481532456, 0.0015282141236054112, 0.0015678194968766252, 0.0015637531078803216, 0.001535504892136353, 0.0014533280077310958, 0.0014326092642695867, 0.0014093593412707018, 0.0015959678228598, 0.0014288579077683678, 0.0014041838463569104, 0.001405744822407069, 0.0014344885504927285, 0.0013879407522116984, 0.0014189921857341546, 0.0014178272564137398, 0.0018667815263967874, 0.0013925919919310845, 0.001368495503931429, 0.001399738659952269, 0.0015341266520436882, 0.0021288809456768655, 0.0021431256038707126, 0.001533534659375978, 0.0015448095825637959, 0.0015220498222164637, 0.0015326376278819733, 0.0020557312554744787, 0.0015401496432808249, 0.0015008295886218548, 0.0015893031936860824, 0.0013976126598815124, 0.0013595593952422226, 0.0014039635272017977, 0.0019254152240708125, 0.0014638647278265437, 0.0014702595044793778, 0.001469914983712541, 0.0015940055514635273, 0.001439825952948295, 0.0014596191332350637, 0.001490275992816964, 0.001486718557168578, 0.0014931783025954352, 0.0014572733789393606, 0.0014668290156784447, 0.0014656162098800952, 0.0015148004885156487, 0.0015014198919137318, 0.001535350341320962, 0.0019261066354233627, 0.001421043107849221, 0.001422919549567755, 0.0014101759920459847, 0.0013904824109329272, 0.0013937940008851679, 0.0013949963563778835, 0.0013831185985558718, 0.0013927046675235033, 0.0013723156429365161, 0.0013662906193946914, 0.001417485029660454, 0.0013866887983772181, 0.001392625959936616, 0.0014003583323932433, 0.0018818772545936265, 0.002014513342287536, 0.002013435636018944, 0.0014778052027113447, 0.0013977002782039633, 0.0013385695123741793, 0.0013574752487357734, 0.001463900479062121, 0.0014031624724698622, 0.00134351952073648, 0.0013969916121153406, 0.0014263332474890144, 0.0019390928836958122, 0.001433770163273511, 0.001396754659105872, 0.0013069081636391175, 0.001293238813721741, 0.0013192799519891887, 0.0013359607912080233, 0.0013166933569537347, 0.0013148094177274972, 0.001334333023446244, 0.0013338394116523655, 0.0013227074807726366, 0.0012910793095018513, 0.0014535230782980373, 0.0013391029075250146, 0.001311291922461386, 0.0013441494341159975, 0.0014126421847604504, 0.0014306754740171653, 0.001428702015920665, 0.0013089453036239905, 0.0013361041625427415, 0.0013205311387459668, 0.001338024728375581, 0.0013272852795703928, 0.0014357698211345331, 0.001969096830201357, 0.0013985239229229993, 0.0013960150066878675, 0.0014116334577285966, 0.0018945881003124077, 0.001403590395339122, 0.0014270623954547236, 0.0014205437998265722, 0.001535964216459398, 0.002028108836158879, 0.001335555588277851, 0.0013613331929397906, 0.001346704961602078, 0.0013761256908088229, 0.0013955117044465025, 0.0014643783107140037, 0.0020486584182308858, 0.0013687315581063198, 0.0013869289911619222, 0.0013786859073957733, 0.0014045712945968375, 0.0012790992253192866, 0.00131268320828965, 0.0013090593473220518, 0.001421846503521814, 0.0014919119078463823, 0.0013888975128877996, 0.0013798562946996486, 0.0016781999133960452, 0.0014063724887001421, 0.0014547302871364956, 0.0014583937834601762, 0.0014051669927010702, 0.001286049279593682, 0.0012957540145793625, 0.0013316661632049452, 0.0013198235573954591, 0.0013101978765352983, 0.001916815210927942, 0.001633328930727502, 0.0012968981389476116, 0.0012958129929277556, 0.0019181101626261723, 0.0013572631779220677, 0.001359492590896381, 0.0013522053870206425, 0.0015543463185083034, 0.001949859109829909, 0.0019498307748820431, 0.0013575403402422287, 0.0013492661401947109, 0.001983674410947187, 0.001964824503403186, 0.0016196994796799597, 0.0013723971625400145, 0.0013545421006287946, 0.0013682487052540447, 0.0013640036900318408, 0.0013551704491698003, 0.0014139246430190265, 0.0020462031613371168, 0.0013606016137889883, 0.0013668466586792885, 0.0013434176835007677, 0.0014768298300228608, 0.0021952094109512345, 0.0013829975266211718, 0.0013947166978966357, 0.0019497104249028272, 0.0020151035499203114, 0.0020256191401836253, 0.0013816778064351673, 0.0013881572792956301, 0.0013969915345051261, 0.00139262367314262, 0.0013799774496592292, 0.0013828995735059644, 0.0020330713632292757, 0.001434084170591808, 0.001432968419074088, 0.0014274769856110793, 0.0019630926910268014, 0.0014400956592731929, 0.0013998409989470428, 0.0014045400844612556, 0.0013997604503736708, 0.0018721808058520157, 0.0014269350858246402, 0.001370565830405022, 0.001386715682194561, 0.0013789227376728094, 0.0013593771159215722, 0.0013676948065674582, 0.0014154790528840566, 0.0014164965029430483, 0.0014010207355744386, 0.0014100013564290234, 0.0014076486749704494, 0.0014100508374530216, 0.001412368410216399, 0.0014574061952848999, 0.001410198652423745, 0.0013633703094611104, 0.0015763973712545727, 0.0020152149693101877, 0.001413018457129482, 0.0014551071858019099, 0.0014104635198420102, 0.0014676645961265232, 0.0015517136753471785, 0.0013767950626653294, 0.0015677297054681667, 0.0014734630158907453, 0.001492316278722859, 0.001539182046052906, 0.0015514912859847148, 0.0015118178828482248, 0.0014390911701200313, 0.0013920043634842764, 0.0013926836441798035, 0.001580840200474558, 0.0014005939678339533, 0.0014824636042291342, 0.0013794902253664973, 0.0015240942178539527, 0.0018075866509200066, 0.0014073199144918327, 0.0014539894489066084, 0.0018325228922704394, 0.0020476789069325887, 0.002030285279259317, 0.0019812063790511252, 0.0014081109448718702, 0.0014070979384488839, 0.0015028001011399797, 0.0014088830167008925, 0.001392341689964713, 0.0021091967211068832, 0.0014158852485030196, 0.0014005695350554793, 0.001429638240573018, 0.0014707436189488616, 0.0015459795583530453, 0.0015344676893753136, 0.001714704179226659, 0.001485351209579281, 0.0014280385352209094, 0.0014332185892835837, 0.00227785148015318, 0.0015451379062593445, 0.0015458769071125245, 0.001457905681991531, 0.0014732557893983385, 0.001424199814886548, 0.002057660170722493, 0.0015277709999634314, 0.0015031251074500787, 0.0015101314724473528, 0.0013980123019495675, 0.001411459621677334, 0.0014033440226154735, 0.001408569752996744, 0.0020140881321660076, 0.002046830060552488, 0.0018335245982852094, 0.001406714845405415, 0.0014158084180003912, 0.0014059298072558964, 0.0014216938839373432, 0.00198142136295521, 0.0015344336437595675, 0.0015143572493607914, 0.0014891308216132629, 0.0021604472940946503, 0.0014481541624855856, 0.0014175397286177143, 0.00143933034855728, 0.0014078509307993475, 0.001420413457983455, 0.0020017360459811005]
[710.0106953230722, 716.9179740449637, 721.4177395542612, 707.6804183549875, 729.2171124983863, 540.0094942745751, 722.9732552624752, 728.6239443709732, 717.8910654158663, 699.5831488881163, 521.5587481905441, 719.4086004847767, 714.7532520756248, 725.4734182482023, 721.4533048220112, 539.7819555633084, 720.4590108742527, 664.7996681537836, 736.5673623052239, 742.3008046557512, 638.0080603962671, 710.3196292594454, 739.5411600846339, 717.2617667808238, 732.804566284049, 737.9255475100837, 739.5108425624159, 719.7953163329729, 721.7734024798356, 729.5351977286, 738.9412370776724, 693.7483657356291, 740.1161994874478, 739.5106816731648, 723.8594284818499, 731.2338769908215, 610.7631238666987, 719.10136867037, 736.1315638729842, 705.8099490924416, 717.9984541458706, 671.7894654820594, 667.7374206880646, 623.8385210345217, 668.3107693745811, 703.402902904203, 729.0234894510844, 697.3709170914576, 722.5358042544152, 642.2707920733183, 709.5342587885565, 475.7444563426234, 491.588983081334, 721.6995424961505, 723.9518505287984, 710.8832287469322, 691.8671943081582, 698.8618218887901, 454.1998864684696, 648.6098480027547, 500.6639154567408, 632.4512623950257, 710.5155324465215, 723.3733017254267, 668.8324500624901, 681.6537421120437, 467.7018448830124, 716.9963096722379, 714.8106572309161, 735.8113838114459, 472.91383264514576, 648.3441985771907, 712.8799986118385, 701.1708952561033, 716.2751113350516, 714.563400873849, 702.0607649518622, 702.9673564496688, 700.9071851285175, 734.859300153246, 685.0800058919889, 709.7716215045917, 534.2531806468278, 668.8378911296876, 670.4689077873228, 469.6059133622191, 568.8419366411366, 664.4313349894458, 671.4925538325386, 725.7125080140821, 710.1240070021448, 503.87694422925233, 719.0621461623526, 694.5462932835519, 683.1114756104254, 701.5922398276426, 701.7168660066206, 722.2551519464115, 657.3422613681355, 467.9774205845823, 633.9622292530123, 504.26034928128837, 499.77138743898763, 703.202867299969, 649.7529169039044, 718.1286890499358, 729.4421235479153, 644.639772102145, 523.8936633921986, 718.0059509358618, 703.2407884027265, 707.9136838951025, 491.69411049030253, 496.41953374287743, 683.5794397015494, 656.3878094024793, 667.0839024681967, 420.55712160907007, 692.1100708469003, 683.6803619876648, 487.8272229405108, 706.6556061103303, 709.0617352620635, 710.793996422096, 703.5241666629272, 678.0811406842693, 510.14789746682385, 656.4725770469634, 664.1973391876123, 650.9953254876865, 711.2933745382139, 654.2245365866867, 660.5841919648923, 678.3620816227809, 681.5667874140191, 670.0321188117249, 691.3723076742465, 639.7010176445964, 675.0500061231414, 712.9663470964825, 730.8912520265845, 722.3713698583748, 717.8266952125274, 725.7890932283926, 715.3976504685377, 686.4048420258611, 695.9114289445056, 723.6536631757186, 722.5137947536695, 720.1518174647455, 727.3187426174889, 708.6230276139574, 545.889384963697, 679.3651457770858, 681.4895976334631, 715.2631140537925, 698.4625250819928, 620.5810633758745, 492.6034521750623, 681.7773044374488, 703.3892864754744, 703.3683815326443, 655.015993845996, 650.314434401676, 666.5895970235937, 654.9396759551005, 692.8094013011564, 491.94589778698975, 715.8954043673251, 722.851172837845, 717.9270912784667, 508.2612408603362, 492.0892081222101, 504.0106530299585, 499.9688199507551, 709.7704312863311, 691.4546399400712, 706.106065652835, 690.559437600911, 694.8155517296409, 704.5052006906213, 701.2406457616711, 674.3951160984375, 671.3742838981788, 702.9027417689787, 706.93807156837, 717.719657226591, 721.5550741120342, 726.2961504827186, 737.2768729057052, 727.0473924685759, 718.2596593174203, 624.2293972572357, 607.6041033455775, 719.9900090971886, 466.2072529890223, 473.1820270386069, 469.42426312754435, 662.5380202882931, 715.0561735348248, 722.8227193695204, 480.9816626166789, 710.3718214136811, 719.3710667848368, 704.1477338777966, 711.1087198964875, 499.7047247402479, 499.9171433360809, 694.1732294905271, 656.1399635887302, 669.6678127125838, 703.9961593896595, 708.3637929769544, 693.8939869747948, 712.6032718420878, 699.0934382619645, 709.2071235466342, 680.5342844152476, 710.1616023339606, 699.0170491550808, 491.47393513152144, 693.6025399029461, 706.9046896149198, 712.0511111353323, 491.9212009031382, 499.64191735018113, 438.04430856722615, 651.0892476651494, 646.2701669385632, 459.5478891413898, 654.046496786349, 657.1632371786717, 649.5491633203081, 654.3585643880639, 637.8285268120326, 639.4871383216671, 651.2515884001494, 688.0759158843834, 698.0270370580413, 709.5422513738642, 626.5790485726142, 699.8596533379791, 712.1574590068484, 711.3666606203046, 697.1125699515084, 720.4918498188697, 704.7255157945937, 705.3045393762605, 535.6813241719686, 718.0854161119485, 730.7294741759757, 714.4190759395757, 651.8366646377267, 469.73035388883886, 466.60820914737513, 652.0882941158418, 647.3289726364726, 657.0087164057245, 652.4699523278368, 486.4449072985429, 649.2875574543531, 666.2981644160252, 629.2065629596407, 715.5058255445243, 735.5324110881066, 712.2692154211965, 519.3684912731438, 683.123229210351, 680.1520391150964, 680.3114541184659, 627.3503872567167, 694.5283893183934, 685.1102299430844, 671.016647131093, 672.6222627532661, 669.7123834854852, 686.2130431064517, 681.7427180068942, 682.3068640062407, 660.1529426359631, 666.0362003898758, 651.3171444242719, 519.1820544142395, 703.7084198758203, 702.7804209336877, 709.131346470541, 719.1748648794933, 717.4661387299143, 716.8477504819483, 723.0037981154402, 718.0273200191052, 728.695329785919, 731.9087065408022, 705.4748227143824, 721.1423364566417, 718.0679010504113, 714.1029384178962, 531.3842853241459, 496.3978043771466, 496.6635049617206, 676.6791713585048, 715.4609722801187, 747.0661708306287, 736.6616819947968, 683.1065460410746, 712.6758444727993, 744.3137107913608, 715.8239114161813, 701.0984296695377, 515.7050538466476, 697.4618565899369, 715.9453476533572, 765.1647053879254, 773.2523872541027, 757.9892338182025, 748.5249616463402, 759.4782754228914, 760.5665022755842, 749.4380956091858, 749.7154389531767, 756.0250580996694, 774.5457561285209, 687.9835724183495, 746.7685973800486, 762.6066956341275, 743.9649004931262, 707.8933439677618, 698.9705339619192, 699.9360180475377, 763.9738629500912, 748.4446407957437, 757.2710484885976, 747.370342859091, 753.4175323059965, 696.4904717176801, 507.8470416803939, 715.0396096978732, 716.3246778933718, 708.399191394245, 527.8192129651322, 712.4585657758007, 700.7402081261883, 703.9557668845449, 651.0568340616248, 493.0701854708854, 748.7520615217991, 734.5740228668825, 742.5531415658943, 726.6778076152668, 716.5830260066697, 682.8836460384466, 488.12432131245566, 730.6034511131819, 721.0174467275602, 725.3283685831819, 711.9610117669648, 781.8001764095992, 761.798424543681, 763.9073064530681, 703.3107986854209, 670.2808622551506, 719.9955293467243, 724.7131486381838, 595.8765651324437, 711.0491765408913, 687.4126488205653, 685.6858629960738, 711.6591872669586, 777.5751799464035, 771.7514194425455, 750.9389572483254, 757.6770352344679, 763.2434900935812, 521.6986980794533, 612.2465482531981, 771.0705798463599, 771.7162935221103, 521.3464896254209, 736.7767845370802, 735.5685545447882, 739.5326254418584, 643.3572673557678, 512.8575674820077, 512.8650203300315, 736.6263604524399, 741.143626309107, 504.1149870570286, 508.9513074923201, 617.3984819687615, 728.6520457017073, 738.2568615148898, 730.8612799413017, 733.1358465581984, 737.9145557760771, 707.2512703822671, 488.7100259128412, 734.9689944988461, 731.6109628319588, 744.3701331920338, 677.1260843129911, 455.5374056849898, 723.0670921322032, 716.991487596079, 512.896678002755, 496.25241345019003, 493.67621985905436, 723.7577352277773, 720.379466300399, 715.823951183958, 718.069080172522, 724.6495225316467, 723.1183081970103, 491.8666496839673, 697.309140221056, 697.8520857048263, 700.5366882128167, 509.4002970776419, 694.3983155290487, 714.3668464862778, 711.9768321767574, 714.4079543989449, 534.1364449812887, 700.8027274219625, 729.6256610340896, 721.1283558987664, 725.2037932797361, 735.6310388689035, 731.1572692958657, 706.4746016286764, 705.9671505875974, 713.7653102543023, 709.2191758826425, 710.4045333051537, 709.1942882047448, 708.0305625405364, 686.1505071374531, 709.1199514914257, 733.4764392773544, 634.3578200743585, 496.2249761087782, 707.7048392074647, 687.2345967069772, 708.9867876285185, 681.354583764718, 644.4487896752352, 726.3245105368884, 637.8650583146109, 678.6732949625308, 670.0992371776653, 649.6957280423132, 644.5411643838597, 661.4553322494284, 694.8830072500496, 718.3885526744584, 718.0381590457555, 632.5750064426541, 713.9827979885706, 674.5528167755523, 724.905462620675, 656.127415408793, 553.2238244241461, 710.5704891279739, 687.7629000347933, 545.695775053064, 488.3578165572816, 492.5416197495247, 504.7429740655983, 710.1713140160232, 710.682584825866, 665.4244960733164, 709.7821381520005, 718.2145066886158, 474.1141449694702, 706.2719249722215, 713.9952533384128, 699.4776521920585, 679.9281581889157, 646.8390830893745, 651.6917931371386, 583.1909737637692, 673.2414485886105, 700.2612151816377, 697.7302746958267, 439.0101851296962, 647.1914228166987, 646.882035302446, 685.915428105046, 678.7687563803086, 702.1486659016736, 485.9888985696197, 654.548358375657, 665.2806177234397, 662.1939998239873, 715.3012878395074, 708.4864381820796, 712.5836458377871, 709.9399925864456, 496.50260285510524, 488.560344736229, 545.3976461157067, 710.8761262214448, 706.3102516457305, 711.2730627368993, 703.3862994687202, 504.6882095328479, 651.7062527056343, 660.3461636427593, 671.5326722716285, 462.867112163945, 690.5342165254133, 705.4476003823399, 694.7675361687155, 710.3024745895657, 704.0203641971182, 499.5663649099526]
Elapsed: 0.19655782496737478~0.029456315566803913
Time per graph: 0.0015237040695145333~0.0002283435315256117
Speed: 668.4053250141026~81.36465558223183
Total Time: 0.2597
best val loss: 0.18040499631171078 test_score: 0.9147

Testing...
Test loss: 0.2681 score: 0.9070 time: 0.26s
test Score 0.9070
Epoch Time List: [0.6481827420648187, 0.6002048871014267, 0.6042039359454066, 0.6065843638498336, 0.6033762781880796, 0.682153478031978, 0.6360694118775427, 0.6098216178361326, 0.6095646829344332, 0.6362422241363674, 0.882680713897571, 0.6083449230063707, 0.6114361039362848, 0.6095791701227427, 0.6107221541460603, 0.7330565981101245, 0.6252110020723194, 0.6340969728771597, 0.6013301422353834, 0.5911709719803184, 0.6187451761215925, 0.6612195211928338, 0.5870266172569245, 0.5894757478963584, 0.5854544590692967, 0.592657784698531, 0.593862900044769, 0.5936651821248233, 0.6759632320608944, 0.5971422391012311, 0.5823281379416585, 0.6151810972951353, 0.5928676691837609, 0.5886825113557279, 0.5997715478297323, 0.5953280460089445, 0.6681538720149547, 0.6616174920927733, 0.603164094965905, 0.6726443043444306, 0.6127253209706396, 0.6415551770478487, 0.6319464019034058, 0.6524375618901104, 0.715393346035853, 0.6269702578429133, 0.6320493817329407, 0.6270961260888726, 0.605056498432532, 0.7115374251734465, 0.6291808679234236, 0.7052385313436389, 0.7731248980853707, 0.7753542440477759, 0.7072891679126769, 0.6216274132020772, 0.7121247849427164, 0.6451224579941481, 0.8097315339837223, 0.6535383691079915, 0.8030074322596192, 0.8416529719252139, 0.6263104719109833, 0.6149992551654577, 0.6454315390437841, 0.634698050096631, 0.735549520002678, 0.7709165420383215, 0.6176349830348045, 0.6079736051615328, 0.8365920463111252, 0.802990835160017, 0.6072277883067727, 0.6164808529429138, 0.6097058970481157, 0.6121277550701052, 0.6244647779967636, 0.710300566162914, 0.6077039672527462, 0.6124712852761149, 0.6444336019922048, 0.650966206099838, 0.703047547955066, 0.645236881216988, 0.6476944389287382, 0.8658906309865415, 0.8535209749825299, 0.6405308519024402, 0.6447163298726082, 0.6243088277988136, 0.6085733957588673, 0.7818487307522446, 0.7323460718616843, 0.6390650228131562, 0.632952491287142, 0.6441728789359331, 0.6374857760965824, 0.6151000780519098, 0.6424551999662071, 0.7258461068850011, 0.6628649060148746, 0.8864745299797505, 0.899378992151469, 0.8519409550353885, 0.651549369096756, 0.6209665501955897, 0.612460358068347, 0.6361752280499786, 0.8010674465913326, 0.645163940731436, 0.621639811899513, 0.6256783329881728, 0.8002057757694274, 0.8946000749710947, 0.8098679869435728, 0.6524290291126817, 0.6486665159463882, 0.763941882411018, 0.6388484227936715, 0.6397047061473131, 0.7047948180697858, 0.6224633799865842, 0.6223167809657753, 0.619671787833795, 0.6242042179219425, 0.6417786350939423, 0.7367738622706383, 0.6554315851535648, 0.6489828543271869, 0.6541716582141817, 0.6137701829429716, 0.6479291240684688, 0.6676674820482731, 0.7298740979749709, 0.6503761929925531, 0.6425877041183412, 0.6195933721028268, 0.757266347296536, 0.7492962519172579, 0.627301107859239, 0.6585042721126229, 0.6060339119285345, 0.625022673048079, 0.6561586409807205, 0.693319687852636, 0.6234106591437012, 0.7124565420672297, 0.6707092167343944, 0.6062053302302957, 0.6062426341231912, 0.607752465410158, 0.6114050692413002, 0.6570089780725539, 0.6819819000083953, 0.618792254012078, 0.6243371551390737, 0.6291980659589171, 0.6523562781512737, 0.8523166121449322, 0.6721749487333, 0.6266973060555756, 0.6299425542820245, 0.6691390499472618, 0.7273296057246625, 0.6548047319520265, 0.6477365479804575, 0.6192102830391377, 0.8812252411153167, 0.7788043050095439, 0.6155605169478804, 0.6066699449438602, 0.8346381809096783, 0.8990426308009773, 0.8932295457925647, 0.8905169768258929, 0.7514091951306909, 0.6275368588976562, 0.6277242919895798, 0.6237345340196043, 0.6339434448163956, 0.6963936623651534, 0.6331913687754422, 0.6394660018850118, 0.6516352880280465, 0.6123517551459372, 0.6387786399573088, 0.7487797189969569, 0.6204007749911398, 0.6282875079195946, 0.6084526181221008, 0.6092228784691542, 0.6134092879947275, 0.7000927268527448, 0.6845485088415444, 0.6051796453539282, 0.8013759863097221, 0.9218486598692834, 0.9284981868695468, 0.794921533903107, 0.6153753807302564, 0.6143072419799864, 0.7664629297796637, 0.619035640032962, 0.6094994868617505, 0.6058941781520844, 0.6367112537845969, 0.8189896128606051, 0.9010511240921915, 0.7390868118964136, 0.6561266260687262, 0.6565959637518972, 0.626423382665962, 0.6174687102902681, 0.6558707340154797, 0.68850561324507, 0.6304966870229691, 0.6202601599507034, 0.6256621100474149, 0.6246884488500655, 0.6374530971515924, 0.8780030747875571, 0.7584795160219073, 0.6239053832832724, 0.614105385961011, 0.7145888768136501, 0.7016239592339844, 0.8841478517279029, 0.648579748114571, 0.658712127013132, 0.8758181370794773, 0.6624408019706607, 0.6791732322890311, 0.6536557867657393, 0.65975819574669, 0.6888496621977538, 0.7477934567723423, 0.6553528432268649, 0.6227110989857465, 0.6194629240781069, 0.6217008768580854, 0.6410483170766383, 0.7125802219379693, 0.6228088322095573, 0.6209909599274397, 0.623476912965998, 0.6211376700084656, 0.6143829617649317, 0.6152136470191181, 0.7248764638788998, 0.6420566907618195, 0.6097931996919215, 0.609866495244205, 0.6520504429936409, 0.9114905952010304, 0.9279870251193643, 0.6601345180533826, 0.6522978632710874, 0.6560092130675912, 0.672418205998838, 0.915079077007249, 0.6652315030805767, 0.6522672739811242, 0.6631804744247347, 0.6300811623223126, 0.6266099570784718, 0.6446814010851085, 0.7821668018586934, 0.7004260527901351, 0.6622496268246323, 0.6594036731403321, 0.6702683130279183, 0.7254338529892266, 0.6560155132319778, 0.6608421553391963, 0.6591705912724137, 0.6651226258836687, 0.7277887342497706, 0.6575194846373051, 0.6664009690284729, 0.6780176390893757, 0.6799167850986123, 0.681081481045112, 0.7082471228204668, 0.6360295570921153, 0.6404798289295286, 0.6403271981980652, 0.6322307400405407, 0.6406224579550326, 0.7113047321327031, 0.6348040131852031, 0.6281494561117142, 0.6309632041957229, 0.6210158639587462, 0.6379204478580505, 0.6906813122332096, 0.6228810758329928, 0.6432717670686543, 0.6914225108921528, 0.9155245670117438, 0.9213856072165072, 0.71743757231161, 0.642008483177051, 0.6158551229164004, 0.6254855296574533, 0.7490500658750534, 0.6985215242020786, 0.7113213778939098, 0.6235955432057381, 0.6658220789395273, 0.9486041758209467, 0.6533268082421273, 0.6804277440533042, 0.6037466658744961, 0.6039657392539084, 0.5927364670205861, 0.6069035141263157, 0.6831923890858889, 0.5944660841487348, 0.6124733968172222, 0.5994346991647035, 0.5953718209639192, 0.5960387650411576, 0.6116366449277848, 0.6843845029361546, 0.6029405246954411, 0.6007694688159972, 0.6262029260396957, 0.6375767278950661, 0.6443749349564314, 0.6714677331037819, 0.6074778151232749, 0.6070877930615097, 0.60988813592121, 0.6149081960320473, 0.6319334728177637, 0.9015421869698912, 0.7786546628922224, 0.6358109801076353, 0.6451775070745498, 0.7249937220476568, 0.6560680279508233, 0.6409334519412369, 0.6420060838572681, 0.6561789605766535, 0.808969029225409, 0.6652164550032467, 0.608201591996476, 0.6061249470803887, 0.6220684617292136, 0.6166898689698428, 0.6314873248338699, 0.7896061148494482, 0.7595184750389308, 0.6224434641189873, 0.6178623328451067, 0.6399882228579372, 0.5916546091903001, 0.5948464712128043, 0.6673916580621153, 0.6385632052551955, 0.6460254259873182, 0.6332538791466504, 0.6184064180124551, 0.7549727989826351, 0.6496446640230715, 0.6493108137510717, 0.6650717940647155, 0.6360070810187608, 0.6047101311851293, 0.5865778208244592, 0.6118156618904322, 0.6798435430973768, 0.593007052084431, 0.7300151528324932, 0.8352763908915222, 0.6657659457996488, 0.5883258490357548, 0.7671006410382688, 0.7108614922035486, 0.6232787971384823, 0.6124657713808119, 0.6441494401078671, 0.8326146441977471, 0.9081399678252637, 0.7804067810066044, 0.6195446187630296, 0.725023670354858, 0.8990082880482078, 0.8487813379615545, 0.6119508079718798, 0.610861643915996, 0.6057082067709416, 0.6188513289671391, 0.676976156886667, 0.6485416200011969, 0.8035540550481528, 0.6217726159375161, 0.6201651748269796, 0.6196700211148709, 0.6825826270505786, 0.8670445780735463, 0.6248268040362746, 0.6416013541165739, 0.8102344679646194, 0.9114591211546212, 0.9147259527817369, 0.6795459890272468, 0.632403994910419, 0.63498067506589, 0.6298889780882746, 0.625979125034064, 0.6268605708610266, 0.7569310651160777, 0.6429405589587986, 0.6419214981142431, 0.6374888499267399, 0.7246456518769264, 0.7717538690194488, 0.6522282240912318, 0.6561803549993783, 0.6462280219420791, 0.727073343237862, 0.6833857109304518, 0.6292352320160717, 0.6410356832202524, 0.6479443279094994, 0.6356390500441194, 0.6231363750994205, 0.655743231298402, 0.7278842316009104, 0.6355783801991493, 0.6441960860975087, 0.6447993379551917, 0.6371536410879344, 0.662103503011167, 0.7274305343162268, 0.656130651012063, 0.6312594630289823, 0.6710720949340612, 0.8301331039983779, 0.7386704229284078, 0.6449598311446607, 0.6487396692391485, 0.6532236160710454, 0.6838773840572685, 0.7049244460649788, 0.650913038989529, 0.6477536789607257, 0.6859746302943677, 0.6477929977700114, 0.745130875846371, 0.680921820923686, 0.6398131370078772, 0.6450145982671529, 0.6366068450734019, 0.6608086060732603, 0.696023179916665, 0.6655240564141423, 0.6516079013235867, 0.656639507971704, 0.7029062202200294, 0.8516024479176849, 0.6404284352902323, 0.7260415249038488, 0.9349619860295206, 0.9364824059884995, 0.9277273560874164, 0.8041563271544874, 0.6340577027294785, 0.6707456370349973, 0.6250444329343736, 0.6376004479825497, 0.7442717717494816, 0.6592188423965126, 0.6498323269188404, 0.6433666350785643, 0.6622902529779822, 0.6810363037511706, 0.692779169883579, 0.7687782540451735, 0.6518845448736101, 0.7055736179463565, 0.6435998952947557, 0.7672227048315108, 0.6781583919655532, 0.6917613411787897, 0.7435107608325779, 0.6628653861116618, 0.6498619408812374, 0.8418480609543622, 0.6783864106982946, 0.6749616637825966, 0.6829385806340724, 0.6737765569705516, 0.6744753601960838, 0.7115114920306951, 0.6420255741104484, 0.804022166877985, 0.9195517560001463, 0.8831113579217345, 0.6414601989090443, 0.6477979649789631, 0.6449379329569638, 0.6539131009485573, 0.7479383139871061, 0.759392635896802, 0.6904476552736014, 0.6841981180477887, 0.773674744181335, 0.6611391329206526, 0.6525052082724869, 0.6667572201695293, 0.6437691892497241, 0.6550855389796197, 0.7401760309003294]
Total Epoch List: [263, 252]
Total Time List: [0.2061999780125916, 0.2596967949066311]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x797a289b3ee0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.9829;  Loss pred: 2.9829; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000020
Train loss: 3.0074;  Loss pred: 3.0074; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 2.9428;  Loss pred: 2.9428; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 2.9114;  Loss pred: 2.9114; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 2.8506;  Loss pred: 2.8506; Loss self: 0.0000; time: 0.29s
Val loss: 0.6930 score: 0.4884 time: 0.18s
Test loss: 0.6928 score: 0.5078 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 2.7882;  Loss pred: 2.7882; Loss self: 0.0000; time: 0.47s
Val loss: 0.6929 score: 0.6357 time: 0.27s
Test loss: 0.6928 score: 0.7266 time: 0.21s
Epoch 7/1000, LR 0.000170
Train loss: 2.7301;  Loss pred: 2.7301; Loss self: 0.0000; time: 0.50s
Val loss: 0.6929 score: 0.6667 time: 0.26s
Test loss: 0.6927 score: 0.7266 time: 0.22s
Epoch 8/1000, LR 0.000200
Train loss: 2.6200;  Loss pred: 2.6200; Loss self: 0.0000; time: 0.51s
Val loss: 0.6928 score: 0.5039 time: 0.26s
Test loss: 0.6926 score: 0.5234 time: 0.21s
Epoch 9/1000, LR 0.000230
Train loss: 2.4916;  Loss pred: 2.4916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.19s
Test loss: 0.6926 score: 0.5078 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 2.4391;  Loss pred: 2.4391; Loss self: 0.0000; time: 0.29s
Val loss: 0.6928 score: 0.4961 time: 0.19s
Test loss: 0.6925 score: 0.5078 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 2.3254;  Loss pred: 2.3254; Loss self: 0.0000; time: 0.29s
Val loss: 0.6927 score: 0.4961 time: 0.19s
Test loss: 0.6925 score: 0.5156 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 2.1953;  Loss pred: 2.1953; Loss self: 0.0000; time: 0.29s
Val loss: 0.6926 score: 0.5039 time: 0.18s
Test loss: 0.6924 score: 0.5234 time: 0.16s
Epoch 13/1000, LR 0.000290
Train loss: 2.1210;  Loss pred: 2.1210; Loss self: 0.0000; time: 0.33s
Val loss: 0.6925 score: 0.5194 time: 0.19s
Test loss: 0.6923 score: 0.5547 time: 0.22s
Epoch 14/1000, LR 0.000290
Train loss: 2.0290;  Loss pred: 2.0290; Loss self: 0.0000; time: 0.31s
Val loss: 0.6925 score: 0.5116 time: 0.20s
Test loss: 0.6922 score: 0.5156 time: 0.17s
Epoch 15/1000, LR 0.000290
Train loss: 1.9373;  Loss pred: 1.9373; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
Test loss: 0.6922 score: 0.5078 time: 0.16s
Epoch 16/1000, LR 0.000290
Train loss: 1.8579;  Loss pred: 1.8579; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.16s
Epoch 17/1000, LR 0.000290
Train loss: 1.7972;  Loss pred: 1.7972; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 1.7272;  Loss pred: 1.7272; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 1.6701;  Loss pred: 1.6701; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.18s
Epoch 20/1000, LR 0.000290
Train loss: 1.6130;  Loss pred: 1.6130; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.17s
Epoch 21/1000, LR 0.000290
Train loss: 1.5639;  Loss pred: 1.5639; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.18s
Test loss: 0.6917 score: 0.5078 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 1.5153;  Loss pred: 1.5153; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.22s
Test loss: 0.6916 score: 0.5078 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 1.4607;  Loss pred: 1.4607; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 0.18s
Test loss: 0.6914 score: 0.5156 time: 0.16s
Epoch 24/1000, LR 0.000290
Train loss: 1.4295;  Loss pred: 1.4295; Loss self: 0.0000; time: 0.29s
Val loss: 0.6913 score: 0.5194 time: 0.18s
Test loss: 0.6910 score: 0.5469 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 1.3930;  Loss pred: 1.3930; Loss self: 0.0000; time: 0.28s
Val loss: 0.6911 score: 0.7597 time: 0.18s
Test loss: 0.6908 score: 0.7578 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 1.3590;  Loss pred: 1.3590; Loss self: 0.0000; time: 0.28s
Val loss: 0.6910 score: 0.7132 time: 0.17s
Test loss: 0.6907 score: 0.7422 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 1.3299;  Loss pred: 1.3299; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.18s
Test loss: 0.6905 score: 0.5078 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 1.3031;  Loss pred: 1.3031; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 1.2789;  Loss pred: 1.2789; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 1.2571;  Loss pred: 1.2571; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 1.2284;  Loss pred: 1.2284; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 1.2124;  Loss pred: 1.2124; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.21s
Epoch 33/1000, LR 0.000290
Train loss: 1.2003;  Loss pred: 1.2003; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.18s
Epoch 34/1000, LR 0.000290
Train loss: 1.1784;  Loss pred: 1.1784; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5000 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 1.1639;  Loss pred: 1.1639; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 1.1499;  Loss pred: 1.1499; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5000 time: 0.17s
Epoch 37/1000, LR 0.000290
Train loss: 1.1401;  Loss pred: 1.1401; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 1.1318;  Loss pred: 1.1318; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.22s
Epoch 39/1000, LR 0.000289
Train loss: 1.1167;  Loss pred: 1.1167; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5000 time: 0.18s
Epoch 40/1000, LR 0.000289
Train loss: 1.1030;  Loss pred: 1.1030; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 1.0996;  Loss pred: 1.0996; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 1.0920;  Loss pred: 1.0920; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 1.0847;  Loss pred: 1.0847; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.5000 time: 0.21s
Epoch 44/1000, LR 0.000289
Train loss: 1.0788;  Loss pred: 1.0788; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6831 score: 0.5000 time: 0.22s
Epoch 45/1000, LR 0.000289
Train loss: 1.0676;  Loss pred: 1.0676; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5000 time: 0.16s
Epoch 46/1000, LR 0.000289
Train loss: 1.0648;  Loss pred: 1.0648; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.5000 time: 0.18s
Epoch 47/1000, LR 0.000289
Train loss: 1.0582;  Loss pred: 1.0582; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.4961 time: 0.24s
Test loss: 0.6803 score: 0.5078 time: 0.17s
Epoch 48/1000, LR 0.000289
Train loss: 1.0516;  Loss pred: 1.0516; Loss self: 0.0000; time: 0.29s
Val loss: 0.6802 score: 0.5736 time: 0.19s
Test loss: 0.6791 score: 0.5625 time: 0.18s
Epoch 49/1000, LR 0.000289
Train loss: 1.0483;  Loss pred: 1.0483; Loss self: 0.0000; time: 0.32s
Val loss: 0.6789 score: 0.7209 time: 0.32s
Test loss: 0.6778 score: 0.7344 time: 0.18s
Epoch 50/1000, LR 0.000289
Train loss: 1.0429;  Loss pred: 1.0429; Loss self: 0.0000; time: 0.29s
Val loss: 0.6776 score: 0.8140 time: 0.18s
Test loss: 0.6764 score: 0.8281 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 1.0355;  Loss pred: 1.0355; Loss self: 0.0000; time: 0.29s
Val loss: 0.6761 score: 0.8450 time: 0.19s
Test loss: 0.6749 score: 0.8984 time: 0.17s
Epoch 52/1000, LR 0.000289
Train loss: 1.0328;  Loss pred: 1.0328; Loss self: 0.0000; time: 0.28s
Val loss: 0.6747 score: 0.8837 time: 0.17s
Test loss: 0.6734 score: 0.9062 time: 0.16s
Epoch 53/1000, LR 0.000289
Train loss: 1.0275;  Loss pred: 1.0275; Loss self: 0.0000; time: 0.28s
Val loss: 0.6733 score: 0.8837 time: 0.19s
Test loss: 0.6720 score: 0.9062 time: 0.20s
Epoch 54/1000, LR 0.000289
Train loss: 1.0242;  Loss pred: 1.0242; Loss self: 0.0000; time: 0.42s
Val loss: 0.6718 score: 0.8915 time: 0.20s
Test loss: 0.6704 score: 0.9062 time: 0.18s
Epoch 55/1000, LR 0.000289
Train loss: 1.0202;  Loss pred: 1.0202; Loss self: 0.0000; time: 0.28s
Val loss: 0.6702 score: 0.9147 time: 0.18s
Test loss: 0.6688 score: 0.9062 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 1.0159;  Loss pred: 1.0159; Loss self: 0.0000; time: 0.29s
Val loss: 0.6688 score: 0.8992 time: 0.18s
Test loss: 0.6673 score: 0.9062 time: 0.16s
Epoch 57/1000, LR 0.000288
Train loss: 1.0115;  Loss pred: 1.0115; Loss self: 0.0000; time: 0.33s
Val loss: 0.6674 score: 0.8605 time: 0.19s
Test loss: 0.6659 score: 0.9141 time: 0.21s
Epoch 58/1000, LR 0.000288
Train loss: 1.0075;  Loss pred: 1.0075; Loss self: 0.0000; time: 0.49s
Val loss: 0.6661 score: 0.8295 time: 0.25s
Test loss: 0.6645 score: 0.8828 time: 0.22s
Epoch 59/1000, LR 0.000288
Train loss: 1.0040;  Loss pred: 1.0040; Loss self: 0.0000; time: 0.47s
Val loss: 0.6646 score: 0.8217 time: 0.20s
Test loss: 0.6629 score: 0.8750 time: 0.16s
Epoch 60/1000, LR 0.000288
Train loss: 1.0002;  Loss pred: 1.0002; Loss self: 0.0000; time: 0.29s
Val loss: 0.6633 score: 0.8062 time: 0.18s
Test loss: 0.6615 score: 0.8125 time: 0.16s
Epoch 61/1000, LR 0.000288
Train loss: 0.9981;  Loss pred: 0.9981; Loss self: 0.0000; time: 0.29s
Val loss: 0.6620 score: 0.7364 time: 0.19s
Test loss: 0.6601 score: 0.7891 time: 0.16s
Epoch 62/1000, LR 0.000288
Train loss: 0.9950;  Loss pred: 0.9950; Loss self: 0.0000; time: 0.29s
Val loss: 0.6602 score: 0.7364 time: 0.18s
Test loss: 0.6582 score: 0.7812 time: 0.16s
Epoch 63/1000, LR 0.000288
Train loss: 0.9902;  Loss pred: 0.9902; Loss self: 0.0000; time: 0.28s
Val loss: 0.6575 score: 0.8062 time: 0.18s
Test loss: 0.6555 score: 0.8125 time: 0.17s
Epoch 64/1000, LR 0.000288
Train loss: 0.9879;  Loss pred: 0.9879; Loss self: 0.0000; time: 0.30s
Val loss: 0.6544 score: 0.8450 time: 0.25s
Test loss: 0.6524 score: 0.8984 time: 0.21s
Epoch 65/1000, LR 0.000288
Train loss: 0.9846;  Loss pred: 0.9846; Loss self: 0.0000; time: 0.47s
Val loss: 0.6516 score: 0.8915 time: 0.20s
Test loss: 0.6495 score: 0.9062 time: 0.16s
Epoch 66/1000, LR 0.000288
Train loss: 0.9805;  Loss pred: 0.9805; Loss self: 0.0000; time: 0.28s
Val loss: 0.6492 score: 0.9070 time: 0.18s
Test loss: 0.6470 score: 0.9062 time: 0.16s
Epoch 67/1000, LR 0.000288
Train loss: 0.9776;  Loss pred: 0.9776; Loss self: 0.0000; time: 0.29s
Val loss: 0.6465 score: 0.8915 time: 0.18s
Test loss: 0.6442 score: 0.9062 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.9734;  Loss pred: 0.9734; Loss self: 0.0000; time: 0.29s
Val loss: 0.6439 score: 0.8605 time: 0.20s
Test loss: 0.6415 score: 0.9062 time: 0.21s
Epoch 69/1000, LR 0.000288
Train loss: 0.9715;  Loss pred: 0.9715; Loss self: 0.0000; time: 0.49s
Val loss: 0.6416 score: 0.8372 time: 0.26s
Test loss: 0.6389 score: 0.8906 time: 0.21s
Epoch 70/1000, LR 0.000287
Train loss: 0.9680;  Loss pred: 0.9680; Loss self: 0.0000; time: 0.51s
Val loss: 0.6392 score: 0.8140 time: 0.23s
Test loss: 0.6363 score: 0.8828 time: 0.21s
Epoch 71/1000, LR 0.000287
Train loss: 0.9640;  Loss pred: 0.9640; Loss self: 0.0000; time: 0.29s
Val loss: 0.6356 score: 0.8372 time: 0.18s
Test loss: 0.6326 score: 0.8984 time: 0.16s
Epoch 72/1000, LR 0.000287
Train loss: 0.9605;  Loss pred: 0.9605; Loss self: 0.0000; time: 0.28s
Val loss: 0.6318 score: 0.8605 time: 0.18s
Test loss: 0.6287 score: 0.9062 time: 0.16s
Epoch 73/1000, LR 0.000287
Train loss: 0.9575;  Loss pred: 0.9575; Loss self: 0.0000; time: 0.28s
Val loss: 0.6283 score: 0.8682 time: 0.18s
Test loss: 0.6249 score: 0.9062 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9535;  Loss pred: 0.9535; Loss self: 0.0000; time: 0.28s
Val loss: 0.6250 score: 0.8450 time: 0.18s
Test loss: 0.6214 score: 0.9062 time: 0.16s
Epoch 75/1000, LR 0.000287
Train loss: 0.9507;  Loss pred: 0.9507; Loss self: 0.0000; time: 0.28s
Val loss: 0.6219 score: 0.8372 time: 0.18s
Test loss: 0.6181 score: 0.8984 time: 0.16s
Epoch 76/1000, LR 0.000287
Train loss: 0.9455;  Loss pred: 0.9455; Loss self: 0.0000; time: 0.33s
Val loss: 0.6180 score: 0.8450 time: 0.18s
Test loss: 0.6141 score: 0.8984 time: 0.16s
Epoch 77/1000, LR 0.000287
Train loss: 0.9437;  Loss pred: 0.9437; Loss self: 0.0000; time: 0.37s
Val loss: 0.6140 score: 0.8450 time: 0.18s
Test loss: 0.6098 score: 0.8984 time: 0.16s
Epoch 78/1000, LR 0.000287
Train loss: 0.9397;  Loss pred: 0.9397; Loss self: 0.0000; time: 0.28s
Val loss: 0.6088 score: 0.8760 time: 0.17s
Test loss: 0.6045 score: 0.9062 time: 0.16s
Epoch 79/1000, LR 0.000287
Train loss: 0.9365;  Loss pred: 0.9365; Loss self: 0.0000; time: 0.28s
Val loss: 0.6039 score: 0.8760 time: 0.18s
Test loss: 0.5995 score: 0.9062 time: 0.16s
Epoch 80/1000, LR 0.000287
Train loss: 0.9311;  Loss pred: 0.9311; Loss self: 0.0000; time: 0.28s
Val loss: 0.5994 score: 0.8915 time: 0.18s
Test loss: 0.5949 score: 0.9062 time: 0.16s
Epoch 81/1000, LR 0.000286
Train loss: 0.9281;  Loss pred: 0.9281; Loss self: 0.0000; time: 0.34s
Val loss: 0.5951 score: 0.9147 time: 0.18s
Test loss: 0.5906 score: 0.9062 time: 0.21s
Epoch 82/1000, LR 0.000286
Train loss: 0.9256;  Loss pred: 0.9256; Loss self: 0.0000; time: 0.48s
Val loss: 0.5908 score: 0.9070 time: 0.26s
Test loss: 0.5861 score: 0.9062 time: 0.22s
Epoch 83/1000, LR 0.000286
Train loss: 0.9216;  Loss pred: 0.9216; Loss self: 0.0000; time: 0.29s
Val loss: 0.5866 score: 0.8760 time: 0.18s
Test loss: 0.5817 score: 0.9062 time: 0.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.9174;  Loss pred: 0.9174; Loss self: 0.0000; time: 0.28s
Val loss: 0.5833 score: 0.8605 time: 0.18s
Test loss: 0.5782 score: 0.9141 time: 0.16s
Epoch 85/1000, LR 0.000286
Train loss: 0.9160;  Loss pred: 0.9160; Loss self: 0.0000; time: 0.28s
Val loss: 0.5812 score: 0.8217 time: 0.18s
Test loss: 0.5759 score: 0.8828 time: 0.18s
Epoch 86/1000, LR 0.000286
Train loss: 0.9128;  Loss pred: 0.9128; Loss self: 0.0000; time: 0.39s
Val loss: 0.5792 score: 0.8140 time: 0.18s
Test loss: 0.5737 score: 0.8750 time: 0.16s
Epoch 87/1000, LR 0.000286
Train loss: 0.9104;  Loss pred: 0.9104; Loss self: 0.0000; time: 0.29s
Val loss: 0.5770 score: 0.8062 time: 0.18s
Test loss: 0.5712 score: 0.8281 time: 0.16s
Epoch 88/1000, LR 0.000286
Train loss: 0.9077;  Loss pred: 0.9077; Loss self: 0.0000; time: 0.31s
Val loss: 0.5724 score: 0.8062 time: 0.23s
Test loss: 0.5665 score: 0.8281 time: 0.21s
Epoch 89/1000, LR 0.000286
Train loss: 0.9026;  Loss pred: 0.9026; Loss self: 0.0000; time: 0.50s
Val loss: 0.5625 score: 0.8140 time: 0.26s
Test loss: 0.5564 score: 0.8828 time: 0.22s
Epoch 90/1000, LR 0.000285
Train loss: 0.8956;  Loss pred: 0.8956; Loss self: 0.0000; time: 0.48s
Val loss: 0.5519 score: 0.8605 time: 0.23s
Test loss: 0.5457 score: 0.9141 time: 0.16s
Epoch 91/1000, LR 0.000285
Train loss: 0.8889;  Loss pred: 0.8889; Loss self: 0.0000; time: 0.29s
Val loss: 0.5439 score: 0.9070 time: 0.17s
Test loss: 0.5381 score: 0.9062 time: 0.16s
Epoch 92/1000, LR 0.000285
Train loss: 0.8844;  Loss pred: 0.8844; Loss self: 0.0000; time: 0.28s
Val loss: 0.5394 score: 0.9380 time: 0.18s
Test loss: 0.5344 score: 0.9297 time: 0.16s
Epoch 93/1000, LR 0.000285
Train loss: 0.8814;  Loss pred: 0.8814; Loss self: 0.0000; time: 0.29s
Val loss: 0.5388 score: 0.9380 time: 0.18s
Test loss: 0.5350 score: 0.9531 time: 0.18s
Epoch 94/1000, LR 0.000285
Train loss: 0.8821;  Loss pred: 0.8821; Loss self: 0.0000; time: 0.39s
Val loss: 0.5371 score: 0.9380 time: 0.22s
Test loss: 0.5338 score: 0.9375 time: 0.16s
Epoch 95/1000, LR 0.000285
Train loss: 0.8798;  Loss pred: 0.8798; Loss self: 0.0000; time: 0.28s
Val loss: 0.5306 score: 0.9302 time: 0.18s
Test loss: 0.5272 score: 0.9375 time: 0.16s
Epoch 96/1000, LR 0.000285
Train loss: 0.8743;  Loss pred: 0.8743; Loss self: 0.0000; time: 0.28s
Val loss: 0.5209 score: 0.9380 time: 0.18s
Test loss: 0.5170 score: 0.9453 time: 0.16s
Epoch 97/1000, LR 0.000285
Train loss: 0.8666;  Loss pred: 0.8666; Loss self: 0.0000; time: 0.29s
Val loss: 0.5121 score: 0.9302 time: 0.18s
Test loss: 0.5068 score: 0.9219 time: 0.17s
Epoch 98/1000, LR 0.000285
Train loss: 0.8603;  Loss pred: 0.8603; Loss self: 0.0000; time: 0.30s
Val loss: 0.5083 score: 0.8915 time: 0.23s
Test loss: 0.5018 score: 0.9062 time: 0.21s
Epoch 99/1000, LR 0.000284
Train loss: 0.8588;  Loss pred: 0.8588; Loss self: 0.0000; time: 0.35s
Val loss: 0.5072 score: 0.8605 time: 0.18s
Test loss: 0.5000 score: 0.9141 time: 0.16s
Epoch 100/1000, LR 0.000284
Train loss: 0.8557;  Loss pred: 0.8557; Loss self: 0.0000; time: 0.28s
Val loss: 0.5050 score: 0.8450 time: 0.18s
Test loss: 0.4976 score: 0.9062 time: 0.16s
Epoch 101/1000, LR 0.000284
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 0.28s
Val loss: 0.5008 score: 0.8450 time: 0.18s
Test loss: 0.4931 score: 0.9062 time: 0.16s
Epoch 102/1000, LR 0.000284
Train loss: 0.8490;  Loss pred: 0.8490; Loss self: 0.0000; time: 0.28s
Val loss: 0.4929 score: 0.8605 time: 0.18s
Test loss: 0.4851 score: 0.9141 time: 0.17s
Epoch 103/1000, LR 0.000284
Train loss: 0.8439;  Loss pred: 0.8439; Loss self: 0.0000; time: 0.28s
Val loss: 0.4858 score: 0.8605 time: 0.18s
Test loss: 0.4779 score: 0.9141 time: 0.16s
Epoch 104/1000, LR 0.000284
Train loss: 0.8382;  Loss pred: 0.8382; Loss self: 0.0000; time: 0.31s
Val loss: 0.4776 score: 0.8760 time: 0.23s
Test loss: 0.4698 score: 0.9062 time: 0.20s
Epoch 105/1000, LR 0.000284
Train loss: 0.8333;  Loss pred: 0.8333; Loss self: 0.0000; time: 0.29s
Val loss: 0.4676 score: 0.9147 time: 0.18s
Test loss: 0.4607 score: 0.9062 time: 0.16s
Epoch 106/1000, LR 0.000283
Train loss: 0.8275;  Loss pred: 0.8275; Loss self: 0.0000; time: 0.28s
Val loss: 0.4620 score: 0.9380 time: 0.18s
Test loss: 0.4570 score: 0.9297 time: 0.16s
Epoch 107/1000, LR 0.000283
Train loss: 0.8242;  Loss pred: 0.8242; Loss self: 0.0000; time: 0.29s
Val loss: 0.4603 score: 0.9457 time: 0.19s
Test loss: 0.4569 score: 0.9531 time: 0.17s
Epoch 108/1000, LR 0.000283
Train loss: 0.8244;  Loss pred: 0.8244; Loss self: 0.0000; time: 0.30s
Val loss: 0.4578 score: 0.9457 time: 0.18s
Test loss: 0.4551 score: 0.9453 time: 0.20s
Epoch 109/1000, LR 0.000283
Train loss: 0.8215;  Loss pred: 0.8215; Loss self: 0.0000; time: 0.48s
Val loss: 0.4510 score: 0.9457 time: 0.26s
Test loss: 0.4481 score: 0.9453 time: 0.21s
Epoch 110/1000, LR 0.000283
Train loss: 0.8155;  Loss pred: 0.8155; Loss self: 0.0000; time: 0.50s
Val loss: 0.4431 score: 0.9380 time: 0.26s
Test loss: 0.4397 score: 0.9531 time: 0.21s
Epoch 111/1000, LR 0.000283
Train loss: 0.8099;  Loss pred: 0.8099; Loss self: 0.0000; time: 0.39s
Val loss: 0.4344 score: 0.9380 time: 0.18s
Test loss: 0.4300 score: 0.9375 time: 0.16s
Epoch 112/1000, LR 0.000283
Train loss: 0.8030;  Loss pred: 0.8030; Loss self: 0.0000; time: 0.29s
Val loss: 0.4272 score: 0.9380 time: 0.18s
Test loss: 0.4214 score: 0.9297 time: 0.17s
Epoch 113/1000, LR 0.000282
Train loss: 0.7972;  Loss pred: 0.7972; Loss self: 0.0000; time: 0.28s
Val loss: 0.4225 score: 0.9225 time: 0.17s
Test loss: 0.4154 score: 0.9062 time: 0.16s
Epoch 114/1000, LR 0.000282
Train loss: 0.7961;  Loss pred: 0.7961; Loss self: 0.0000; time: 0.28s
Val loss: 0.4184 score: 0.9070 time: 0.19s
Test loss: 0.4107 score: 0.9062 time: 0.17s
Epoch 115/1000, LR 0.000282
Train loss: 0.7922;  Loss pred: 0.7922; Loss self: 0.0000; time: 0.28s
Val loss: 0.4142 score: 0.8992 time: 0.18s
Test loss: 0.4060 score: 0.9062 time: 0.16s
Epoch 116/1000, LR 0.000282
Train loss: 0.7893;  Loss pred: 0.7893; Loss self: 0.0000; time: 0.33s
Val loss: 0.4092 score: 0.8992 time: 0.19s
Test loss: 0.4010 score: 0.9062 time: 0.23s
Epoch 117/1000, LR 0.000282
Train loss: 0.7876;  Loss pred: 0.7876; Loss self: 0.0000; time: 0.30s
Val loss: 0.4049 score: 0.8915 time: 0.19s
Test loss: 0.3964 score: 0.9062 time: 0.18s
Epoch 118/1000, LR 0.000282
Train loss: 0.7820;  Loss pred: 0.7820; Loss self: 0.0000; time: 0.30s
Val loss: 0.3958 score: 0.9070 time: 0.18s
Test loss: 0.3881 score: 0.9062 time: 0.16s
Epoch 119/1000, LR 0.000282
Train loss: 0.7763;  Loss pred: 0.7763; Loss self: 0.0000; time: 0.29s
Val loss: 0.3860 score: 0.9380 time: 0.18s
Test loss: 0.3799 score: 0.9219 time: 0.16s
Epoch 120/1000, LR 0.000281
Train loss: 0.7706;  Loss pred: 0.7706; Loss self: 0.0000; time: 0.30s
Val loss: 0.3792 score: 0.9380 time: 0.18s
Test loss: 0.3747 score: 0.9375 time: 0.16s
Epoch 121/1000, LR 0.000281
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.32s
Val loss: 0.3734 score: 0.9380 time: 0.18s
Test loss: 0.3697 score: 0.9375 time: 0.21s
Epoch 122/1000, LR 0.000281
Train loss: 0.7611;  Loss pred: 0.7611; Loss self: 0.0000; time: 0.50s
Val loss: 0.3676 score: 0.9380 time: 0.19s
Test loss: 0.3642 score: 0.9375 time: 0.16s
Epoch 123/1000, LR 0.000281
Train loss: 0.7565;  Loss pred: 0.7565; Loss self: 0.0000; time: 0.29s
Val loss: 0.3620 score: 0.9380 time: 0.18s
Test loss: 0.3581 score: 0.9375 time: 0.16s
Epoch 124/1000, LR 0.000281
Train loss: 0.7523;  Loss pred: 0.7523; Loss self: 0.0000; time: 0.28s
Val loss: 0.3566 score: 0.9380 time: 0.18s
Test loss: 0.3525 score: 0.9375 time: 0.17s
Epoch 125/1000, LR 0.000281
Train loss: 0.7506;  Loss pred: 0.7506; Loss self: 0.0000; time: 0.30s
Val loss: 0.3524 score: 0.9380 time: 0.19s
Test loss: 0.3477 score: 0.9297 time: 0.17s
Epoch 126/1000, LR 0.000280
Train loss: 0.7458;  Loss pred: 0.7458; Loss self: 0.0000; time: 0.29s
Val loss: 0.3494 score: 0.9457 time: 0.19s
Test loss: 0.3441 score: 0.9141 time: 0.20s
Epoch 127/1000, LR 0.000280
Train loss: 0.7411;  Loss pred: 0.7411; Loss self: 0.0000; time: 0.50s
Val loss: 0.3429 score: 0.9457 time: 0.26s
Test loss: 0.3383 score: 0.9219 time: 0.21s
Epoch 128/1000, LR 0.000280
Train loss: 0.7391;  Loss pred: 0.7391; Loss self: 0.0000; time: 0.37s
Val loss: 0.3360 score: 0.9380 time: 0.19s
Test loss: 0.3328 score: 0.9375 time: 0.17s
Epoch 129/1000, LR 0.000280
Train loss: 0.7348;  Loss pred: 0.7348; Loss self: 0.0000; time: 0.31s
Val loss: 0.3305 score: 0.9380 time: 0.19s
Test loss: 0.3281 score: 0.9375 time: 0.17s
Epoch 130/1000, LR 0.000280
Train loss: 0.7303;  Loss pred: 0.7303; Loss self: 0.0000; time: 0.31s
Val loss: 0.3250 score: 0.9380 time: 0.27s
Test loss: 0.3236 score: 0.9375 time: 0.21s
Epoch 131/1000, LR 0.000280
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.51s
Val loss: 0.3202 score: 0.9380 time: 0.26s
Test loss: 0.3189 score: 0.9375 time: 0.21s
Epoch 132/1000, LR 0.000279
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 0.49s
Val loss: 0.3158 score: 0.9380 time: 0.26s
Test loss: 0.3145 score: 0.9375 time: 0.29s
Epoch 133/1000, LR 0.000279
Train loss: 0.7196;  Loss pred: 0.7196; Loss self: 0.0000; time: 0.49s
Val loss: 0.3114 score: 0.9380 time: 0.24s
Test loss: 0.3102 score: 0.9375 time: 0.17s
Epoch 134/1000, LR 0.000279
Train loss: 0.7181;  Loss pred: 0.7181; Loss self: 0.0000; time: 0.29s
Val loss: 0.3065 score: 0.9380 time: 0.18s
Test loss: 0.3058 score: 0.9375 time: 0.17s
Epoch 135/1000, LR 0.000279
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.29s
Val loss: 0.3017 score: 0.9380 time: 0.17s
Test loss: 0.3014 score: 0.9375 time: 0.16s
Epoch 136/1000, LR 0.000279
Train loss: 0.7087;  Loss pred: 0.7087; Loss self: 0.0000; time: 0.28s
Val loss: 0.2971 score: 0.9380 time: 0.18s
Test loss: 0.2971 score: 0.9375 time: 0.16s
Epoch 137/1000, LR 0.000279
Train loss: 0.7061;  Loss pred: 0.7061; Loss self: 0.0000; time: 0.30s
Val loss: 0.2929 score: 0.9380 time: 0.19s
Test loss: 0.2931 score: 0.9375 time: 0.17s
Epoch 138/1000, LR 0.000278
Train loss: 0.7052;  Loss pred: 0.7052; Loss self: 0.0000; time: 0.30s
Val loss: 0.2909 score: 0.9457 time: 0.19s
Test loss: 0.2906 score: 0.9375 time: 0.17s
Epoch 139/1000, LR 0.000278
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.29s
Val loss: 0.2882 score: 0.9457 time: 0.22s
Test loss: 0.2879 score: 0.9297 time: 0.17s
Epoch 140/1000, LR 0.000278
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.42s
Val loss: 0.2840 score: 0.9457 time: 0.18s
Test loss: 0.2842 score: 0.9375 time: 0.16s
Epoch 141/1000, LR 0.000278
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.29s
Val loss: 0.2788 score: 0.9457 time: 0.18s
Test loss: 0.2798 score: 0.9375 time: 0.16s
Epoch 142/1000, LR 0.000278
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.28s
Val loss: 0.2734 score: 0.9457 time: 0.18s
Test loss: 0.2754 score: 0.9375 time: 0.16s
Epoch 143/1000, LR 0.000277
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.28s
Val loss: 0.2677 score: 0.9380 time: 0.18s
Test loss: 0.2710 score: 0.9375 time: 0.16s
Epoch 144/1000, LR 0.000277
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.28s
Val loss: 0.2624 score: 0.9380 time: 0.18s
Test loss: 0.2669 score: 0.9375 time: 0.19s
Epoch 145/1000, LR 0.000277
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.29s
Val loss: 0.2570 score: 0.9535 time: 0.25s
Test loss: 0.2635 score: 0.9375 time: 0.21s
Epoch 146/1000, LR 0.000277
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.46s
Val loss: 0.2551 score: 0.9457 time: 0.21s
Test loss: 0.2642 score: 0.9453 time: 0.16s
Epoch 147/1000, LR 0.000277
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.28s
Val loss: 0.2545 score: 0.9535 time: 0.18s
Test loss: 0.2649 score: 0.9609 time: 0.16s
Epoch 148/1000, LR 0.000277
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.29s
Val loss: 0.2503 score: 0.9535 time: 0.18s
Test loss: 0.2611 score: 0.9609 time: 0.16s
Epoch 149/1000, LR 0.000276
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.28s
Val loss: 0.2441 score: 0.9535 time: 0.18s
Test loss: 0.2546 score: 0.9453 time: 0.16s
Epoch 150/1000, LR 0.000276
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.29s
Val loss: 0.2394 score: 0.9535 time: 0.18s
Test loss: 0.2495 score: 0.9375 time: 0.16s
Epoch 151/1000, LR 0.000276
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.28s
Val loss: 0.2362 score: 0.9535 time: 0.20s
Test loss: 0.2460 score: 0.9375 time: 0.17s
Epoch 152/1000, LR 0.000276
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.36s
Val loss: 0.2338 score: 0.9535 time: 0.18s
Test loss: 0.2434 score: 0.9375 time: 0.16s
Epoch 153/1000, LR 0.000276
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.29s
Val loss: 0.2313 score: 0.9457 time: 0.18s
Test loss: 0.2410 score: 0.9375 time: 0.16s
Epoch 154/1000, LR 0.000275
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.29s
Val loss: 0.2296 score: 0.9457 time: 0.18s
Test loss: 0.2392 score: 0.9375 time: 0.16s
Epoch 155/1000, LR 0.000275
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.29s
Val loss: 0.2265 score: 0.9457 time: 0.19s
Test loss: 0.2368 score: 0.9375 time: 0.17s
Epoch 156/1000, LR 0.000275
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.30s
Val loss: 0.2216 score: 0.9535 time: 0.19s
Test loss: 0.2335 score: 0.9375 time: 0.21s
Epoch 157/1000, LR 0.000275
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.29s
Val loss: 0.2182 score: 0.9535 time: 0.18s
Test loss: 0.2311 score: 0.9375 time: 0.16s
Epoch 158/1000, LR 0.000275
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.29s
Val loss: 0.2160 score: 0.9457 time: 0.18s
Test loss: 0.2316 score: 0.9453 time: 0.17s
Epoch 159/1000, LR 0.000274
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.29s
Val loss: 0.2268 score: 0.9457 time: 0.18s
Test loss: 0.2436 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 160/1000, LR 0.000274
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.29s
Val loss: 0.2371 score: 0.9302 time: 0.18s
Test loss: 0.2533 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 161/1000, LR 0.000274
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.30s
Val loss: 0.2389 score: 0.9302 time: 0.18s
Test loss: 0.2550 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 162/1000, LR 0.000274
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.28s
Val loss: 0.2325 score: 0.9302 time: 0.18s
Test loss: 0.2492 score: 0.9375 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 163/1000, LR 0.000273
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.29s
Val loss: 0.2217 score: 0.9302 time: 0.25s
Test loss: 0.2394 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.42s
Val loss: 0.2109 score: 0.9535 time: 0.18s
Test loss: 0.2292 score: 0.9453 time: 0.17s
Epoch 165/1000, LR 0.000273
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.28s
Val loss: 0.2035 score: 0.9535 time: 0.18s
Test loss: 0.2221 score: 0.9531 time: 0.16s
Epoch 166/1000, LR 0.000273
Train loss: 0.6343;  Loss pred: 0.6343; Loss self: 0.0000; time: 0.29s
Val loss: 0.1995 score: 0.9535 time: 0.18s
Test loss: 0.2180 score: 0.9375 time: 0.17s
Epoch 167/1000, LR 0.000273
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.29s
Val loss: 0.1979 score: 0.9535 time: 0.19s
Test loss: 0.2161 score: 0.9375 time: 0.17s
Epoch 168/1000, LR 0.000272
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.29s
Val loss: 0.1969 score: 0.9535 time: 0.19s
Test loss: 0.2150 score: 0.9375 time: 0.17s
Epoch 169/1000, LR 0.000272
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.36s
Val loss: 0.1951 score: 0.9535 time: 0.19s
Test loss: 0.2137 score: 0.9375 time: 0.20s
Epoch 170/1000, LR 0.000272
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.32s
Val loss: 0.1931 score: 0.9535 time: 0.19s
Test loss: 0.2123 score: 0.9375 time: 0.17s
Epoch 171/1000, LR 0.000272
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.27s
Val loss: 0.1909 score: 0.9535 time: 0.18s
Test loss: 0.2107 score: 0.9375 time: 0.16s
Epoch 172/1000, LR 0.000271
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.27s
Val loss: 0.1895 score: 0.9535 time: 0.18s
Test loss: 0.2096 score: 0.9375 time: 0.16s
Epoch 173/1000, LR 0.000271
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.28s
Val loss: 0.1887 score: 0.9535 time: 0.18s
Test loss: 0.2088 score: 0.9375 time: 0.16s
Epoch 174/1000, LR 0.000271
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 0.27s
Val loss: 0.1890 score: 0.9612 time: 0.18s
Test loss: 0.2089 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 175/1000, LR 0.000271
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.28s
Val loss: 0.1888 score: 0.9535 time: 0.18s
Test loss: 0.2088 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 176/1000, LR 0.000271
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 0.34s
Val loss: 0.1880 score: 0.9535 time: 0.18s
Test loss: 0.2084 score: 0.9375 time: 0.16s
Epoch 177/1000, LR 0.000270
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.36s
Val loss: 0.1871 score: 0.9535 time: 0.18s
Test loss: 0.2079 score: 0.9375 time: 0.16s
Epoch 178/1000, LR 0.000270
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 0.28s
Val loss: 0.1850 score: 0.9535 time: 0.18s
Test loss: 0.2067 score: 0.9375 time: 0.16s
Epoch 179/1000, LR 0.000270
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.28s
Val loss: 0.1825 score: 0.9535 time: 0.18s
Test loss: 0.2052 score: 0.9375 time: 0.16s
Epoch 180/1000, LR 0.000270
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.28s
Val loss: 0.1829 score: 0.9535 time: 0.18s
Test loss: 0.2056 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 181/1000, LR 0.000269
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 0.29s
Val loss: 0.1829 score: 0.9535 time: 0.21s
Test loss: 0.2058 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 182/1000, LR 0.000269
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.30s
Val loss: 0.1762 score: 0.9612 time: 0.18s
Test loss: 0.2013 score: 0.9375 time: 0.19s
Epoch 183/1000, LR 0.000269
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.35s
Val loss: 0.1684 score: 0.9612 time: 0.18s
Test loss: 0.1963 score: 0.9375 time: 0.17s
Epoch 184/1000, LR 0.000269
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.29s
Val loss: 0.1647 score: 0.9612 time: 0.18s
Test loss: 0.1944 score: 0.9453 time: 0.17s
Epoch 185/1000, LR 0.000268
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.28s
Val loss: 0.1638 score: 0.9612 time: 0.18s
Test loss: 0.1943 score: 0.9453 time: 0.17s
Epoch 186/1000, LR 0.000268
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 0.28s
Val loss: 0.1638 score: 0.9612 time: 0.18s
Test loss: 0.1947 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000268
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.27s
Val loss: 0.1627 score: 0.9535 time: 0.19s
Test loss: 0.1942 score: 0.9453 time: 0.16s
Epoch 188/1000, LR 0.000268
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.43s
Val loss: 0.1596 score: 0.9612 time: 0.23s
Test loss: 0.1919 score: 0.9453 time: 0.21s
Epoch 189/1000, LR 0.000267
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.28s
Val loss: 0.1579 score: 0.9612 time: 0.18s
Test loss: 0.1905 score: 0.9453 time: 0.16s
Epoch 190/1000, LR 0.000267
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.28s
Val loss: 0.1588 score: 0.9612 time: 0.18s
Test loss: 0.1914 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.5969;  Loss pred: 0.5969; Loss self: 0.0000; time: 0.28s
Val loss: 0.1601 score: 0.9690 time: 0.18s
Test loss: 0.1926 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 192/1000, LR 0.000267
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.48s
Val loss: 0.1603 score: 0.9690 time: 0.26s
Test loss: 0.1930 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 193/1000, LR 0.000266
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.36s
Val loss: 0.1596 score: 0.9690 time: 0.18s
Test loss: 0.1928 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 194/1000, LR 0.000266
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.28s
Val loss: 0.1583 score: 0.9690 time: 0.18s
Test loss: 0.1922 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 195/1000, LR 0.000266
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.28s
Val loss: 0.1554 score: 0.9690 time: 0.18s
Test loss: 0.1905 score: 0.9375 time: 0.21s
Epoch 196/1000, LR 0.000266
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.49s
Val loss: 0.1514 score: 0.9612 time: 0.26s
Test loss: 0.1880 score: 0.9453 time: 0.21s
Epoch 197/1000, LR 0.000265
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.49s
Val loss: 0.1493 score: 0.9612 time: 0.26s
Test loss: 0.1869 score: 0.9453 time: 0.22s
Epoch 198/1000, LR 0.000265
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 0.48s
Val loss: 0.1484 score: 0.9612 time: 0.26s
Test loss: 0.1865 score: 0.9453 time: 0.21s
Epoch 199/1000, LR 0.000265
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.49s
Val loss: 0.1475 score: 0.9612 time: 0.27s
Test loss: 0.1862 score: 0.9453 time: 0.16s
Epoch 200/1000, LR 0.000265
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.29s
Val loss: 0.1466 score: 0.9612 time: 0.18s
Test loss: 0.1858 score: 0.9453 time: 0.16s
Epoch 201/1000, LR 0.000264
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.28s
Val loss: 0.1454 score: 0.9690 time: 0.19s
Test loss: 0.1853 score: 0.9453 time: 0.21s
Epoch 202/1000, LR 0.000264
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.50s
Val loss: 0.1441 score: 0.9690 time: 0.27s
Test loss: 0.1846 score: 0.9453 time: 0.22s
Epoch 203/1000, LR 0.000264
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.49s
Val loss: 0.1429 score: 0.9690 time: 0.27s
Test loss: 0.1840 score: 0.9453 time: 0.22s
Epoch 204/1000, LR 0.000264
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.51s
Val loss: 0.1419 score: 0.9690 time: 0.19s
Test loss: 0.1836 score: 0.9453 time: 0.17s
Epoch 205/1000, LR 0.000263
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.28s
Val loss: 0.1409 score: 0.9690 time: 0.18s
Test loss: 0.1832 score: 0.9453 time: 0.16s
Epoch 206/1000, LR 0.000263
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.29s
Val loss: 0.1398 score: 0.9690 time: 0.19s
Test loss: 0.1827 score: 0.9453 time: 0.17s
Epoch 207/1000, LR 0.000263
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.29s
Val loss: 0.1388 score: 0.9690 time: 0.19s
Test loss: 0.1822 score: 0.9453 time: 0.17s
Epoch 208/1000, LR 0.000263
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.29s
Val loss: 0.1384 score: 0.9612 time: 0.19s
Test loss: 0.1821 score: 0.9453 time: 0.17s
Epoch 209/1000, LR 0.000262
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.32s
Val loss: 0.1428 score: 0.9535 time: 0.19s
Test loss: 0.1852 score: 0.9453 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.32s
Val loss: 0.1471 score: 0.9535 time: 0.19s
Test loss: 0.1883 score: 0.9531 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 211/1000, LR 0.000262
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.30s
Val loss: 0.1429 score: 0.9535 time: 0.25s
Test loss: 0.1855 score: 0.9531 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 212/1000, LR 0.000261
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.29s
Val loss: 0.1352 score: 0.9612 time: 0.19s
Test loss: 0.1810 score: 0.9453 time: 0.17s
Epoch 213/1000, LR 0.000261
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.52s
Val loss: 0.1353 score: 0.9690 time: 0.34s
Test loss: 0.1820 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 214/1000, LR 0.000261
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.50s
Val loss: 0.1381 score: 0.9690 time: 0.19s
Test loss: 0.1850 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 215/1000, LR 0.000261
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.29s
Val loss: 0.1401 score: 0.9690 time: 0.27s
Test loss: 0.1871 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 216/1000, LR 0.000260
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.30s
Val loss: 0.1404 score: 0.9690 time: 0.18s
Test loss: 0.1876 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 217/1000, LR 0.000260
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.31s
Val loss: 0.1349 score: 0.9767 time: 0.18s
Test loss: 0.1833 score: 0.9453 time: 0.25s
Epoch 218/1000, LR 0.000260
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.47s
Val loss: 0.1316 score: 0.9612 time: 0.26s
Test loss: 0.1796 score: 0.9453 time: 0.22s
Epoch 219/1000, LR 0.000260
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 0.48s
Val loss: 0.1357 score: 0.9535 time: 0.24s
Test loss: 0.1817 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 220/1000, LR 0.000259
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.29s
Val loss: 0.1396 score: 0.9535 time: 0.18s
Test loss: 0.1841 score: 0.9531 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 221/1000, LR 0.000259
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.30s
Val loss: 0.1392 score: 0.9535 time: 0.19s
Test loss: 0.1839 score: 0.9531 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 222/1000, LR 0.000259
Train loss: 0.5684;  Loss pred: 0.5684; Loss self: 0.0000; time: 0.29s
Val loss: 0.1358 score: 0.9535 time: 0.17s
Test loss: 0.1820 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 223/1000, LR 0.000258
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.28s
Val loss: 0.1315 score: 0.9535 time: 0.18s
Test loss: 0.1798 score: 0.9375 time: 0.16s
Epoch 224/1000, LR 0.000258
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.28s
Val loss: 0.1289 score: 0.9612 time: 0.20s
Test loss: 0.1789 score: 0.9453 time: 0.16s
Epoch 225/1000, LR 0.000258
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.48s
Val loss: 0.1281 score: 0.9612 time: 0.26s
Test loss: 0.1794 score: 0.9453 time: 0.16s
Epoch 226/1000, LR 0.000258
Train loss: 0.5647;  Loss pred: 0.5647; Loss self: 0.0000; time: 0.28s
Val loss: 0.1287 score: 0.9690 time: 0.18s
Test loss: 0.1810 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 227/1000, LR 0.000257
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.28s
Val loss: 0.1300 score: 0.9767 time: 0.17s
Test loss: 0.1829 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 228/1000, LR 0.000257
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.28s
Val loss: 0.1308 score: 0.9767 time: 0.17s
Test loss: 0.1842 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 229/1000, LR 0.000257
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.28s
Val loss: 0.1309 score: 0.9767 time: 0.17s
Test loss: 0.1847 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 230/1000, LR 0.000256
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.30s
Val loss: 0.1296 score: 0.9767 time: 0.25s
Test loss: 0.1839 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 231/1000, LR 0.000256
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.47s
Val loss: 0.1279 score: 0.9767 time: 0.23s
Test loss: 0.1826 score: 0.9453 time: 0.16s
Epoch 232/1000, LR 0.000256
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.27s
Val loss: 0.1260 score: 0.9767 time: 0.17s
Test loss: 0.1809 score: 0.9453 time: 0.16s
Epoch 233/1000, LR 0.000255
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.28s
Val loss: 0.1245 score: 0.9612 time: 0.18s
Test loss: 0.1793 score: 0.9453 time: 0.16s
Epoch 234/1000, LR 0.000255
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.28s
Val loss: 0.1238 score: 0.9612 time: 0.17s
Test loss: 0.1785 score: 0.9453 time: 0.17s
Epoch 235/1000, LR 0.000255
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.43s
Val loss: 0.1236 score: 0.9612 time: 0.26s
Test loss: 0.1781 score: 0.9453 time: 0.21s
Epoch 236/1000, LR 0.000255
Train loss: 0.5596;  Loss pred: 0.5596; Loss self: 0.0000; time: 0.41s
Val loss: 0.1234 score: 0.9612 time: 0.23s
Test loss: 0.1778 score: 0.9453 time: 0.16s
Epoch 237/1000, LR 0.000254
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 0.28s
Val loss: 0.1234 score: 0.9612 time: 0.18s
Test loss: 0.1777 score: 0.9453 time: 0.16s
Epoch 238/1000, LR 0.000254
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.28s
Val loss: 0.1230 score: 0.9612 time: 0.17s
Test loss: 0.1777 score: 0.9453 time: 0.16s
Epoch 239/1000, LR 0.000254
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.28s
Val loss: 0.1225 score: 0.9612 time: 0.18s
Test loss: 0.1777 score: 0.9453 time: 0.16s
Epoch 240/1000, LR 0.000253
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.28s
Val loss: 0.1219 score: 0.9612 time: 0.18s
Test loss: 0.1778 score: 0.9453 time: 0.19s
Epoch 241/1000, LR 0.000253
Train loss: 0.5535;  Loss pred: 0.5535; Loss self: 0.0000; time: 0.28s
Val loss: 0.1216 score: 0.9612 time: 0.24s
Test loss: 0.1778 score: 0.9453 time: 0.16s
Epoch 242/1000, LR 0.000253
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.28s
Val loss: 0.1209 score: 0.9612 time: 0.18s
Test loss: 0.1789 score: 0.9453 time: 0.16s
Epoch 243/1000, LR 0.000252
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.28s
Val loss: 0.1263 score: 0.9767 time: 0.18s
Test loss: 0.1861 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 244/1000, LR 0.000252
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.28s
Val loss: 0.1343 score: 0.9690 time: 0.18s
Test loss: 0.1935 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 245/1000, LR 0.000252
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 0.30s
Val loss: 0.1386 score: 0.9535 time: 0.23s
Test loss: 0.1975 score: 0.9375 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 246/1000, LR 0.000252
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.31s
Val loss: 0.1376 score: 0.9612 time: 0.18s
Test loss: 0.1970 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 247/1000, LR 0.000251
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.29s
Val loss: 0.1315 score: 0.9690 time: 0.18s
Test loss: 0.1923 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 248/1000, LR 0.000251
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.28s
Val loss: 0.1231 score: 0.9767 time: 0.17s
Test loss: 0.1850 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 249/1000, LR 0.000251
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.28s
Val loss: 0.1188 score: 0.9690 time: 0.18s
Test loss: 0.1803 score: 0.9453 time: 0.16s
Epoch 250/1000, LR 0.000250
Train loss: 0.5508;  Loss pred: 0.5508; Loss self: 0.0000; time: 0.33s
Val loss: 0.1175 score: 0.9612 time: 0.19s
Test loss: 0.1777 score: 0.9453 time: 0.20s
Epoch 251/1000, LR 0.000250
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.33s
Val loss: 0.1180 score: 0.9612 time: 0.19s
Test loss: 0.1768 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 252/1000, LR 0.000250
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.31s
Val loss: 0.1182 score: 0.9612 time: 0.19s
Test loss: 0.1770 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 253/1000, LR 0.000249
Train loss: 0.5522;  Loss pred: 0.5522; Loss self: 0.0000; time: 0.30s
Val loss: 0.1170 score: 0.9612 time: 0.19s
Test loss: 0.1778 score: 0.9453 time: 0.17s
Epoch 254/1000, LR 0.000249
Train loss: 0.5542;  Loss pred: 0.5542; Loss self: 0.0000; time: 0.29s
Val loss: 0.1165 score: 0.9612 time: 0.19s
Test loss: 0.1791 score: 0.9453 time: 0.17s
Epoch 255/1000, LR 0.000249
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.29s
Val loss: 0.1170 score: 0.9690 time: 0.19s
Test loss: 0.1811 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 256/1000, LR 0.000248
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 0.32s
Val loss: 0.1177 score: 0.9767 time: 0.19s
Test loss: 0.1828 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 257/1000, LR 0.000248
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.38s
Val loss: 0.1181 score: 0.9767 time: 0.19s
Test loss: 0.1837 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 258/1000, LR 0.000248
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 0.30s
Val loss: 0.1178 score: 0.9767 time: 0.19s
Test loss: 0.1838 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 259/1000, LR 0.000247
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.30s
Val loss: 0.1170 score: 0.9767 time: 0.19s
Test loss: 0.1831 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 260/1000, LR 0.000247
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 0.30s
Val loss: 0.1160 score: 0.9690 time: 0.19s
Test loss: 0.1818 score: 0.9453 time: 0.17s
Epoch 261/1000, LR 0.000247
Train loss: 0.5490;  Loss pred: 0.5490; Loss self: 0.0000; time: 0.29s
Val loss: 0.1152 score: 0.9690 time: 0.19s
Test loss: 0.1805 score: 0.9453 time: 0.17s
Epoch 262/1000, LR 0.000246
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.34s
Val loss: 0.1148 score: 0.9612 time: 0.19s
Test loss: 0.1795 score: 0.9453 time: 0.22s
Epoch 263/1000, LR 0.000246
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.50s
Val loss: 0.1158 score: 0.9612 time: 0.27s
Test loss: 0.1780 score: 0.9453 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 264/1000, LR 0.000246
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 0.29s
Val loss: 0.1213 score: 0.9457 time: 0.18s
Test loss: 0.1786 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 265/1000, LR 0.000245
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.29s
Val loss: 0.1223 score: 0.9457 time: 0.18s
Test loss: 0.1791 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 266/1000, LR 0.000245
Train loss: 0.5476;  Loss pred: 0.5476; Loss self: 0.0000; time: 0.28s
Val loss: 0.1211 score: 0.9457 time: 0.18s
Test loss: 0.1790 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 267/1000, LR 0.000245
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.28s
Val loss: 0.1176 score: 0.9535 time: 0.18s
Test loss: 0.1786 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 268/1000, LR 0.000244
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.32s
Val loss: 0.1144 score: 0.9612 time: 0.26s
Test loss: 0.1798 score: 0.9453 time: 0.22s
Epoch 269/1000, LR 0.000244
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.52s
Val loss: 0.1140 score: 0.9612 time: 0.24s
Test loss: 0.1824 score: 0.9453 time: 0.21s
Epoch 270/1000, LR 0.000244
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.30s
Val loss: 0.1148 score: 0.9690 time: 0.18s
Test loss: 0.1848 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 271/1000, LR 0.000243
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.28s
Val loss: 0.1155 score: 0.9690 time: 0.18s
Test loss: 0.1863 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 272/1000, LR 0.000243
Train loss: 0.5435;  Loss pred: 0.5435; Loss self: 0.0000; time: 0.27s
Val loss: 0.1157 score: 0.9767 time: 0.17s
Test loss: 0.1870 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 273/1000, LR 0.000243
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.27s
Val loss: 0.1154 score: 0.9767 time: 0.18s
Test loss: 0.1869 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 274/1000, LR 0.000242
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.28s
Val loss: 0.1151 score: 0.9767 time: 0.18s
Test loss: 0.1868 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 275/1000, LR 0.000242
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.27s
Val loss: 0.1142 score: 0.9690 time: 0.18s
Test loss: 0.1860 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 276/1000, LR 0.000242
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.30s
Val loss: 0.1132 score: 0.9690 time: 0.18s
Test loss: 0.1847 score: 0.9453 time: 0.16s
Epoch 277/1000, LR 0.000241
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.36s
Val loss: 0.1124 score: 0.9690 time: 0.18s
Test loss: 0.1833 score: 0.9453 time: 0.16s
Epoch 278/1000, LR 0.000241
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.28s
Val loss: 0.1118 score: 0.9612 time: 0.18s
Test loss: 0.1820 score: 0.9453 time: 0.16s
Epoch 279/1000, LR 0.000241
Train loss: 0.5406;  Loss pred: 0.5406; Loss self: 0.0000; time: 0.28s
Val loss: 0.1117 score: 0.9612 time: 0.18s
Test loss: 0.1812 score: 0.9453 time: 0.16s
Epoch 280/1000, LR 0.000240
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.28s
Val loss: 0.1116 score: 0.9612 time: 0.18s
Test loss: 0.1808 score: 0.9453 time: 0.18s
Epoch 281/1000, LR 0.000240
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.31s
Val loss: 0.1116 score: 0.9612 time: 0.25s
Test loss: 0.1805 score: 0.9453 time: 0.21s
Epoch 282/1000, LR 0.000240
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.46s
Val loss: 0.1116 score: 0.9612 time: 0.18s
Test loss: 0.1805 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 283/1000, LR 0.000239
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.29s
Val loss: 0.1112 score: 0.9612 time: 0.19s
Test loss: 0.1809 score: 0.9453 time: 0.16s
Epoch 284/1000, LR 0.000239
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.28s
Val loss: 0.1105 score: 0.9612 time: 0.18s
Test loss: 0.1826 score: 0.9453 time: 0.16s
Epoch 285/1000, LR 0.000239
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.29s
Val loss: 0.1107 score: 0.9612 time: 0.26s
Test loss: 0.1847 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 286/1000, LR 0.000238
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.47s
Val loss: 0.1112 score: 0.9690 time: 0.18s
Test loss: 0.1865 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 287/1000, LR 0.000238
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.27s
Val loss: 0.1118 score: 0.9690 time: 0.18s
Test loss: 0.1879 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 288/1000, LR 0.000237
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.28s
Val loss: 0.1119 score: 0.9690 time: 0.18s
Test loss: 0.1883 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 289/1000, LR 0.000237
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.41s
Val loss: 0.1114 score: 0.9690 time: 0.26s
Test loss: 0.1877 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 290/1000, LR 0.000237
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.45s
Val loss: 0.1108 score: 0.9690 time: 0.23s
Test loss: 0.1868 score: 0.9453 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 291/1000, LR 0.000236
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.29s
Val loss: 0.1096 score: 0.9612 time: 0.18s
Test loss: 0.1842 score: 0.9453 time: 0.16s
Epoch 292/1000, LR 0.000236
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 0.28s
Val loss: 0.1100 score: 0.9612 time: 0.19s
Test loss: 0.1812 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 293/1000, LR 0.000236
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.29s
Val loss: 0.1111 score: 0.9612 time: 0.19s
Test loss: 0.1804 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 294/1000, LR 0.000235
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.29s
Val loss: 0.1106 score: 0.9612 time: 0.19s
Test loss: 0.1808 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 295/1000, LR 0.000235
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.28s
Val loss: 0.1092 score: 0.9612 time: 0.18s
Test loss: 0.1826 score: 0.9453 time: 0.17s
Epoch 296/1000, LR 0.000235
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.30s
Val loss: 0.1090 score: 0.9612 time: 0.21s
Test loss: 0.1846 score: 0.9453 time: 0.26s
Epoch 297/1000, LR 0.000234
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.28s
Val loss: 0.1093 score: 0.9690 time: 0.18s
Test loss: 0.1864 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 298/1000, LR 0.000234
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.29s
Val loss: 0.1095 score: 0.9690 time: 0.18s
Test loss: 0.1876 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 299/1000, LR 0.000234
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.31s
Val loss: 0.1097 score: 0.9690 time: 0.18s
Test loss: 0.1884 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 300/1000, LR 0.000233
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.28s
Val loss: 0.1094 score: 0.9690 time: 0.21s
Test loss: 0.1882 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 301/1000, LR 0.000233
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.41s
Val loss: 0.1090 score: 0.9690 time: 0.34s
Test loss: 0.1879 score: 0.9453 time: 0.21s
Epoch 302/1000, LR 0.000232
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.50s
Val loss: 0.1092 score: 0.9690 time: 0.26s
Test loss: 0.1889 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 303/1000, LR 0.000232
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.37s
Val loss: 0.1088 score: 0.9690 time: 0.18s
Test loss: 0.1884 score: 0.9453 time: 0.16s
Epoch 304/1000, LR 0.000232
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.28s
Val loss: 0.1077 score: 0.9690 time: 0.18s
Test loss: 0.1861 score: 0.9453 time: 0.17s
Epoch 305/1000, LR 0.000231
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.29s
Val loss: 0.1073 score: 0.9612 time: 0.19s
Test loss: 0.1849 score: 0.9453 time: 0.17s
Epoch 306/1000, LR 0.000231
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.30s
Val loss: 0.1072 score: 0.9612 time: 0.19s
Test loss: 0.1841 score: 0.9453 time: 0.17s
Epoch 307/1000, LR 0.000231
Train loss: 0.5331;  Loss pred: 0.5331; Loss self: 0.0000; time: 0.29s
Val loss: 0.1071 score: 0.9612 time: 0.19s
Test loss: 0.1832 score: 0.9453 time: 0.17s
Epoch 308/1000, LR 0.000230
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.30s
Val loss: 0.1072 score: 0.9612 time: 0.27s
Test loss: 0.1827 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 309/1000, LR 0.000230
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.51s
Val loss: 0.1072 score: 0.9612 time: 0.27s
Test loss: 0.1824 score: 0.9453 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 310/1000, LR 0.000229
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 0.44s
Val loss: 0.1072 score: 0.9612 time: 0.19s
Test loss: 0.1821 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 311/1000, LR 0.000229
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.30s
Val loss: 0.1071 score: 0.9612 time: 0.18s
Test loss: 0.1820 score: 0.9453 time: 0.18s
Epoch 312/1000, LR 0.000229
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.30s
Val loss: 0.1070 score: 0.9612 time: 0.19s
Test loss: 0.1821 score: 0.9453 time: 0.17s
Epoch 313/1000, LR 0.000228
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.30s
Val loss: 0.1067 score: 0.9612 time: 0.19s
Test loss: 0.1826 score: 0.9453 time: 0.17s
Epoch 314/1000, LR 0.000228
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.30s
Val loss: 0.1065 score: 0.9612 time: 0.18s
Test loss: 0.1829 score: 0.9453 time: 0.17s
Epoch 315/1000, LR 0.000228
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.33s
Val loss: 0.1063 score: 0.9612 time: 0.24s
Test loss: 0.1833 score: 0.9453 time: 0.20s
Epoch 316/1000, LR 0.000227
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.29s
Val loss: 0.1061 score: 0.9612 time: 0.17s
Test loss: 0.1839 score: 0.9453 time: 0.17s
Epoch 317/1000, LR 0.000227
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.29s
Val loss: 0.1059 score: 0.9612 time: 0.17s
Test loss: 0.1853 score: 0.9453 time: 0.16s
Epoch 318/1000, LR 0.000226
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.31s
Val loss: 0.1059 score: 0.9612 time: 0.18s
Test loss: 0.1864 score: 0.9453 time: 0.16s
Epoch 319/1000, LR 0.000226
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.29s
Val loss: 0.1059 score: 0.9690 time: 0.17s
Test loss: 0.1873 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 320/1000, LR 0.000226
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.28s
Val loss: 0.1060 score: 0.9690 time: 0.18s
Test loss: 0.1881 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 321/1000, LR 0.000225
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.29s
Val loss: 0.1061 score: 0.9690 time: 0.21s
Test loss: 0.1887 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 322/1000, LR 0.000225
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.41s
Val loss: 0.1059 score: 0.9690 time: 0.25s
Test loss: 0.1885 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 323/1000, LR 0.000225
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.28s
Val loss: 0.1057 score: 0.9690 time: 0.18s
Test loss: 0.1881 score: 0.9453 time: 0.16s
Epoch 324/1000, LR 0.000224
Train loss: 0.5290;  Loss pred: 0.5290; Loss self: 0.0000; time: 0.29s
Val loss: 0.1055 score: 0.9612 time: 0.18s
Test loss: 0.1876 score: 0.9453 time: 0.18s
Epoch 325/1000, LR 0.000224
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.47s
Val loss: 0.1053 score: 0.9612 time: 0.27s
Test loss: 0.1870 score: 0.9453 time: 0.22s
Epoch 326/1000, LR 0.000223
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.50s
Val loss: 0.1052 score: 0.9612 time: 0.22s
Test loss: 0.1861 score: 0.9453 time: 0.22s
Epoch 327/1000, LR 0.000223
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.29s
Val loss: 0.1052 score: 0.9612 time: 0.19s
Test loss: 0.1852 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 328/1000, LR 0.000223
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.30s
Val loss: 0.1054 score: 0.9612 time: 0.18s
Test loss: 0.1844 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 329/1000, LR 0.000222
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.29s
Val loss: 0.1056 score: 0.9612 time: 0.18s
Test loss: 0.1837 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 330/1000, LR 0.000222
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.29s
Val loss: 0.1058 score: 0.9612 time: 0.18s
Test loss: 0.1836 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 331/1000, LR 0.000221
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.29s
Val loss: 0.1058 score: 0.9535 time: 0.18s
Test loss: 0.1837 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 332/1000, LR 0.000221
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.35s
Val loss: 0.1055 score: 0.9612 time: 0.19s
Test loss: 0.1842 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 333/1000, LR 0.000221
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.37s
Val loss: 0.1052 score: 0.9612 time: 0.18s
Test loss: 0.1850 score: 0.9453 time: 0.16s
Epoch 334/1000, LR 0.000220
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.30s
Val loss: 0.1049 score: 0.9612 time: 0.18s
Test loss: 0.1858 score: 0.9453 time: 0.16s
Epoch 335/1000, LR 0.000220
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.30s
Val loss: 0.1047 score: 0.9612 time: 0.18s
Test loss: 0.1865 score: 0.9453 time: 0.16s
Epoch 336/1000, LR 0.000219
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.30s
Val loss: 0.1046 score: 0.9612 time: 0.18s
Test loss: 0.1868 score: 0.9453 time: 0.16s
Epoch 337/1000, LR 0.000219
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.29s
Val loss: 0.1045 score: 0.9612 time: 0.18s
Test loss: 0.1871 score: 0.9453 time: 0.16s
Epoch 338/1000, LR 0.000219
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.43s
Val loss: 0.1044 score: 0.9612 time: 0.26s
Test loss: 0.1871 score: 0.9453 time: 0.21s
Epoch 339/1000, LR 0.000218
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.37s
Val loss: 0.1043 score: 0.9612 time: 0.18s
Test loss: 0.1872 score: 0.9453 time: 0.16s
Epoch 340/1000, LR 0.000218
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.29s
Val loss: 0.1042 score: 0.9612 time: 0.18s
Test loss: 0.1871 score: 0.9453 time: 0.16s
Epoch 341/1000, LR 0.000218
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.29s
Val loss: 0.1041 score: 0.9612 time: 0.18s
Test loss: 0.1873 score: 0.9453 time: 0.17s
Epoch 342/1000, LR 0.000217
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.29s
Val loss: 0.1040 score: 0.9612 time: 0.17s
Test loss: 0.1884 score: 0.9453 time: 0.16s
Epoch 343/1000, LR 0.000217
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.28s
Val loss: 0.1040 score: 0.9612 time: 0.17s
Test loss: 0.1902 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 344/1000, LR 0.000216
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.28s
Val loss: 0.1041 score: 0.9690 time: 0.19s
Test loss: 0.1916 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 345/1000, LR 0.000216
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 0.37s
Val loss: 0.1043 score: 0.9690 time: 0.17s
Test loss: 0.1928 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 346/1000, LR 0.000215
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.28s
Val loss: 0.1049 score: 0.9690 time: 0.17s
Test loss: 0.1947 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 347/1000, LR 0.000215
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.28s
Val loss: 0.1060 score: 0.9767 time: 0.17s
Test loss: 0.1971 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 348/1000, LR 0.000215
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.42s
Val loss: 0.1063 score: 0.9767 time: 0.25s
Test loss: 0.1979 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 349/1000, LR 0.000214
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.30s
Val loss: 0.1056 score: 0.9690 time: 0.18s
Test loss: 0.1969 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 350/1000, LR 0.000214
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.28s
Val loss: 0.1046 score: 0.9690 time: 0.17s
Test loss: 0.1948 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 351/1000, LR 0.000213
Train loss: 0.5235;  Loss pred: 0.5235; Loss self: 0.0000; time: 0.27s
Val loss: 0.1038 score: 0.9690 time: 0.17s
Test loss: 0.1928 score: 0.9453 time: 0.16s
Epoch 352/1000, LR 0.000213
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.29s
Val loss: 0.1033 score: 0.9612 time: 0.17s
Test loss: 0.1909 score: 0.9453 time: 0.18s
Epoch 353/1000, LR 0.000213
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.42s
Val loss: 0.1032 score: 0.9612 time: 0.25s
Test loss: 0.1895 score: 0.9453 time: 0.21s
Epoch 354/1000, LR 0.000212
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.36s
Val loss: 0.1032 score: 0.9612 time: 0.17s
Test loss: 0.1890 score: 0.9453 time: 0.16s
Epoch 355/1000, LR 0.000212
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.28s
Val loss: 0.1033 score: 0.9612 time: 0.17s
Test loss: 0.1879 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 356/1000, LR 0.000211
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.27s
Val loss: 0.1034 score: 0.9612 time: 0.17s
Test loss: 0.1872 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 357/1000, LR 0.000211
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.28s
Val loss: 0.1033 score: 0.9612 time: 0.18s
Test loss: 0.1873 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 358/1000, LR 0.000211
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.28s
Val loss: 0.1031 score: 0.9612 time: 0.17s
Test loss: 0.1877 score: 0.9453 time: 0.16s
Epoch 359/1000, LR 0.000210
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.27s
Val loss: 0.1029 score: 0.9612 time: 0.17s
Test loss: 0.1884 score: 0.9453 time: 0.16s
Epoch 360/1000, LR 0.000210
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.31s
Val loss: 0.1027 score: 0.9612 time: 0.17s
Test loss: 0.1893 score: 0.9453 time: 0.20s
Epoch 361/1000, LR 0.000209
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.37s
Val loss: 0.1026 score: 0.9612 time: 0.17s
Test loss: 0.1901 score: 0.9453 time: 0.16s
Epoch 362/1000, LR 0.000209
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.28s
Val loss: 0.1036 score: 0.9690 time: 0.17s
Test loss: 0.1955 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 363/1000, LR 0.000209
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.28s
Val loss: 0.1145 score: 0.9690 time: 0.17s
Test loss: 0.2123 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 364/1000, LR 0.000208
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.27s
Val loss: 0.1280 score: 0.9690 time: 0.20s
Test loss: 0.2260 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 365/1000, LR 0.000208
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.36s
Val loss: 0.1351 score: 0.9612 time: 0.17s
Test loss: 0.2323 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 366/1000, LR 0.000207
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.29s
Val loss: 0.1338 score: 0.9612 time: 0.17s
Test loss: 0.2315 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 367/1000, LR 0.000207
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.28s
Val loss: 0.1264 score: 0.9690 time: 0.18s
Test loss: 0.2251 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 368/1000, LR 0.000206
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.28s
Val loss: 0.1173 score: 0.9690 time: 0.17s
Test loss: 0.2162 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 369/1000, LR 0.000206
Train loss: 0.5280;  Loss pred: 0.5280; Loss self: 0.0000; time: 0.28s
Val loss: 0.1102 score: 0.9767 time: 0.17s
Test loss: 0.2080 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 370/1000, LR 0.000206
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.28s
Val loss: 0.1057 score: 0.9767 time: 0.19s
Test loss: 0.2017 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 371/1000, LR 0.000205
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.47s
Val loss: 0.1031 score: 0.9767 time: 0.25s
Test loss: 0.1973 score: 0.9453 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 372/1000, LR 0.000205
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.30s
Val loss: 0.1018 score: 0.9767 time: 0.17s
Test loss: 0.1943 score: 0.9453 time: 0.16s
Epoch 373/1000, LR 0.000204
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.29s
Val loss: 0.1011 score: 0.9612 time: 0.18s
Test loss: 0.1921 score: 0.9453 time: 0.16s
Epoch 374/1000, LR 0.000204
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.29s
Val loss: 0.1008 score: 0.9612 time: 0.17s
Test loss: 0.1910 score: 0.9453 time: 0.16s
Epoch 375/1000, LR 0.000204
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.43s
Val loss: 0.1006 score: 0.9612 time: 0.25s
Test loss: 0.1900 score: 0.9453 time: 0.21s
Epoch 376/1000, LR 0.000203
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.49s
Val loss: 0.1005 score: 0.9612 time: 0.25s
Test loss: 0.1890 score: 0.9375 time: 0.20s
Epoch 377/1000, LR 0.000203
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.50s
Val loss: 0.1004 score: 0.9612 time: 0.26s
Test loss: 0.1890 score: 0.9375 time: 0.21s
Epoch 378/1000, LR 0.000202
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.43s
Val loss: 0.1003 score: 0.9612 time: 0.18s
Test loss: 0.1892 score: 0.9375 time: 0.16s
Epoch 379/1000, LR 0.000202
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.29s
Val loss: 0.1002 score: 0.9612 time: 0.17s
Test loss: 0.1900 score: 0.9375 time: 0.16s
Epoch 380/1000, LR 0.000201
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.29s
Val loss: 0.1010 score: 0.9767 time: 0.17s
Test loss: 0.1954 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 381/1000, LR 0.000201
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.28s
Val loss: 0.1092 score: 0.9767 time: 0.17s
Test loss: 0.2100 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 382/1000, LR 0.000201
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.28s
Val loss: 0.1198 score: 0.9690 time: 0.28s
Test loss: 0.2222 score: 0.9297 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 383/1000, LR 0.000200
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.41s
Val loss: 0.1273 score: 0.9690 time: 0.26s
Test loss: 0.2296 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 384/1000, LR 0.000200
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.32s
Val loss: 0.1274 score: 0.9612 time: 0.23s
Test loss: 0.2301 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 385/1000, LR 0.000199
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.28s
Val loss: 0.1196 score: 0.9690 time: 0.17s
Test loss: 0.2230 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 386/1000, LR 0.000199
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.28s
Val loss: 0.1123 score: 0.9767 time: 0.17s
Test loss: 0.2155 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 387/1000, LR 0.000198
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.30s
Val loss: 0.1031 score: 0.9767 time: 0.17s
Test loss: 0.2027 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 388/1000, LR 0.000198
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.27s
Val loss: 0.1004 score: 0.9535 time: 0.17s
Test loss: 0.1873 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 389/1000, LR 0.000198
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.35s
Val loss: 0.1084 score: 0.9457 time: 0.18s
Test loss: 0.1828 score: 0.9531 time: 0.27s
     INFO: Early stopping counter 10 of 20
Epoch 390/1000, LR 0.000197
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.33s
Val loss: 0.1180 score: 0.9535 time: 0.17s
Test loss: 0.1835 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 391/1000, LR 0.000197
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.27s
Val loss: 0.1233 score: 0.9457 time: 0.18s
Test loss: 0.1848 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 392/1000, LR 0.000196
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.27s
Val loss: 0.1238 score: 0.9457 time: 0.17s
Test loss: 0.1848 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 393/1000, LR 0.000196
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.28s
Val loss: 0.1213 score: 0.9535 time: 0.18s
Test loss: 0.1840 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 394/1000, LR 0.000195
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.27s
Val loss: 0.1171 score: 0.9535 time: 0.17s
Test loss: 0.1830 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 395/1000, LR 0.000195
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.28s
Val loss: 0.1119 score: 0.9535 time: 0.19s
Test loss: 0.1825 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 396/1000, LR 0.000195
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.36s
Val loss: 0.1068 score: 0.9457 time: 0.18s
Test loss: 0.1829 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 397/1000, LR 0.000194
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.27s
Val loss: 0.1025 score: 0.9535 time: 0.18s
Test loss: 0.1848 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 398/1000, LR 0.000194
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.27s
Val loss: 0.1007 score: 0.9535 time: 0.17s
Test loss: 0.1871 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 399/1000, LR 0.000193
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.27s
Val loss: 0.0998 score: 0.9535 time: 0.18s
Test loss: 0.1897 score: 0.9375 time: 0.16s
Epoch 400/1000, LR 0.000193
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.27s
Val loss: 0.0997 score: 0.9535 time: 0.17s
Test loss: 0.1924 score: 0.9375 time: 0.17s
Epoch 401/1000, LR 0.000192
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.29s
Val loss: 0.0999 score: 0.9690 time: 0.26s
Test loss: 0.1942 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 402/1000, LR 0.000192
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.28s
Val loss: 0.0999 score: 0.9690 time: 0.18s
Test loss: 0.1946 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 403/1000, LR 0.000192
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.27s
Val loss: 0.0998 score: 0.9690 time: 0.17s
Test loss: 0.1946 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 404/1000, LR 0.000191
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.28s
Val loss: 0.0998 score: 0.9612 time: 0.18s
Test loss: 0.1947 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 405/1000, LR 0.000191
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.28s
Val loss: 0.0997 score: 0.9612 time: 0.18s
Test loss: 0.1948 score: 0.9375 time: 0.16s
Epoch 406/1000, LR 0.000190
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.28s
Val loss: 0.0997 score: 0.9535 time: 0.18s
Test loss: 0.1949 score: 0.9375 time: 0.16s
Epoch 407/1000, LR 0.000190
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.28s
Val loss: 0.0996 score: 0.9535 time: 0.18s
Test loss: 0.1949 score: 0.9375 time: 0.16s
Epoch 408/1000, LR 0.000189
Train loss: 0.5137;  Loss pred: 0.5137; Loss self: 0.0000; time: 0.29s
Val loss: 0.0995 score: 0.9535 time: 0.18s
Test loss: 0.1948 score: 0.9375 time: 0.22s
Epoch 409/1000, LR 0.000189
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.29s
Val loss: 0.0996 score: 0.9535 time: 0.17s
Test loss: 0.1951 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 410/1000, LR 0.000188
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.29s
Val loss: 0.0997 score: 0.9612 time: 0.17s
Test loss: 0.1956 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 411/1000, LR 0.000188
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.30s
Val loss: 0.0997 score: 0.9612 time: 0.19s
Test loss: 0.1959 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 412/1000, LR 0.000188
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.30s
Val loss: 0.0999 score: 0.9767 time: 0.18s
Test loss: 0.1974 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 413/1000, LR 0.000187
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.28s
Val loss: 0.1009 score: 0.9767 time: 0.18s
Test loss: 0.2008 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 414/1000, LR 0.000187
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.30s
Val loss: 0.1020 score: 0.9767 time: 0.23s
Test loss: 0.2038 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 415/1000, LR 0.000186
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.42s
Val loss: 0.1028 score: 0.9767 time: 0.21s
Test loss: 0.2058 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 416/1000, LR 0.000186
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.28s
Val loss: 0.1033 score: 0.9767 time: 0.18s
Test loss: 0.2070 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 417/1000, LR 0.000185
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.29s
Val loss: 0.1038 score: 0.9767 time: 0.18s
Test loss: 0.2080 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 418/1000, LR 0.000185
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.31s
Val loss: 0.1041 score: 0.9767 time: 0.23s
Test loss: 0.2086 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 10 of 20
Epoch 419/1000, LR 0.000185
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.37s
Val loss: 0.1039 score: 0.9767 time: 0.18s
Test loss: 0.2085 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 420/1000, LR 0.000184
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.31s
Val loss: 0.1035 score: 0.9767 time: 0.19s
Test loss: 0.2080 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 421/1000, LR 0.000184
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.30s
Val loss: 0.1030 score: 0.9767 time: 0.19s
Test loss: 0.2073 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 422/1000, LR 0.000183
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.34s
Val loss: 0.1021 score: 0.9767 time: 0.20s
Test loss: 0.2059 score: 0.9375 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 423/1000, LR 0.000183
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 0.52s
Val loss: 0.1011 score: 0.9767 time: 0.27s
Test loss: 0.2039 score: 0.9375 time: 0.23s
     INFO: Early stopping counter 15 of 20
Epoch 424/1000, LR 0.000182
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.37s
Val loss: 0.1003 score: 0.9767 time: 0.19s
Test loss: 0.2020 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 425/1000, LR 0.000182
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.31s
Val loss: 0.0997 score: 0.9767 time: 0.19s
Test loss: 0.2005 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 426/1000, LR 0.000181
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.31s
Val loss: 0.0993 score: 0.9690 time: 0.19s
Test loss: 0.1990 score: 0.9375 time: 0.17s
Epoch 427/1000, LR 0.000181
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.30s
Val loss: 0.0990 score: 0.9535 time: 0.19s
Test loss: 0.1970 score: 0.9375 time: 0.17s
Epoch 428/1000, LR 0.000181
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.32s
Val loss: 0.0989 score: 0.9535 time: 0.19s
Test loss: 0.1961 score: 0.9375 time: 0.20s
Epoch 429/1000, LR 0.000180
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.36s
Val loss: 0.0988 score: 0.9535 time: 0.18s
Test loss: 0.1966 score: 0.9375 time: 0.16s
Epoch 430/1000, LR 0.000180
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.29s
Val loss: 0.0988 score: 0.9535 time: 0.18s
Test loss: 0.1973 score: 0.9375 time: 0.17s
Epoch 431/1000, LR 0.000179
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.29s
Val loss: 0.0988 score: 0.9535 time: 0.18s
Test loss: 0.1981 score: 0.9375 time: 0.16s
Epoch 432/1000, LR 0.000179
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.30s
Val loss: 0.0988 score: 0.9535 time: 0.18s
Test loss: 0.1985 score: 0.9375 time: 0.16s
Epoch 433/1000, LR 0.000178
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.30s
Val loss: 0.0988 score: 0.9690 time: 0.18s
Test loss: 0.1993 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 434/1000, LR 0.000178
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.30s
Val loss: 0.0990 score: 0.9690 time: 0.20s
Test loss: 0.2005 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 435/1000, LR 0.000177
Train loss: 0.5121;  Loss pred: 0.5121; Loss self: 0.0000; time: 0.31s
Val loss: 0.0993 score: 0.9767 time: 0.24s
Test loss: 0.2018 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 436/1000, LR 0.000177
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.29s
Val loss: 0.0991 score: 0.9690 time: 0.19s
Test loss: 0.2008 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 437/1000, LR 0.000176
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.31s
Val loss: 0.0987 score: 0.9535 time: 0.18s
Test loss: 0.1979 score: 0.9375 time: 0.17s
Epoch 438/1000, LR 0.000176
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.30s
Val loss: 0.0987 score: 0.9535 time: 0.19s
Test loss: 0.1959 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 439/1000, LR 0.000176
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.29s
Val loss: 0.0990 score: 0.9535 time: 0.18s
Test loss: 0.1943 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 440/1000, LR 0.000175
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.29s
Val loss: 0.0992 score: 0.9535 time: 0.22s
Test loss: 0.1935 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 441/1000, LR 0.000175
Train loss: 0.5108;  Loss pred: 0.5108; Loss self: 0.0000; time: 0.38s
Val loss: 0.0993 score: 0.9535 time: 0.18s
Test loss: 0.1933 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 442/1000, LR 0.000174
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.29s
Val loss: 0.0992 score: 0.9535 time: 0.18s
Test loss: 0.1935 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 443/1000, LR 0.000174
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.30s
Val loss: 0.0990 score: 0.9535 time: 0.18s
Test loss: 0.1940 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 444/1000, LR 0.000173
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.47s
Val loss: 0.0989 score: 0.9535 time: 0.23s
Test loss: 0.1943 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 445/1000, LR 0.000173
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.31s
Val loss: 0.0988 score: 0.9535 time: 0.20s
Test loss: 0.1945 score: 0.9375 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 446/1000, LR 0.000172
Train loss: 0.5107;  Loss pred: 0.5107; Loss self: 0.0000; time: 0.31s
Val loss: 0.0986 score: 0.9535 time: 0.19s
Test loss: 0.1950 score: 0.9375 time: 0.17s
Epoch 447/1000, LR 0.000172
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.31s
Val loss: 0.0985 score: 0.9535 time: 0.19s
Test loss: 0.1956 score: 0.9375 time: 0.17s
Epoch 448/1000, LR 0.000172
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.31s
Val loss: 0.0984 score: 0.9535 time: 0.19s
Test loss: 0.1956 score: 0.9375 time: 0.17s
Epoch 449/1000, LR 0.000171
Train loss: 0.5121;  Loss pred: 0.5121; Loss self: 0.0000; time: 0.32s
Val loss: 0.0983 score: 0.9535 time: 0.19s
Test loss: 0.1952 score: 0.9375 time: 0.18s
Epoch 450/1000, LR 0.000171
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.32s
Val loss: 0.0983 score: 0.9535 time: 0.25s
Test loss: 0.1952 score: 0.9375 time: 0.20s
Epoch 451/1000, LR 0.000170
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.30s
Val loss: 0.0982 score: 0.9535 time: 0.19s
Test loss: 0.1953 score: 0.9375 time: 0.18s
Epoch 452/1000, LR 0.000170
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.29s
Val loss: 0.0980 score: 0.9535 time: 0.18s
Test loss: 0.1960 score: 0.9375 time: 0.16s
Epoch 453/1000, LR 0.000169
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.28s
Val loss: 0.0979 score: 0.9535 time: 0.18s
Test loss: 0.1971 score: 0.9375 time: 0.19s
Epoch 454/1000, LR 0.000169
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.44s
Val loss: 0.0978 score: 0.9535 time: 0.25s
Test loss: 0.1989 score: 0.9375 time: 0.21s
Epoch 455/1000, LR 0.000168
Train loss: 0.5088;  Loss pred: 0.5088; Loss self: 0.0000; time: 0.46s
Val loss: 0.0980 score: 0.9612 time: 0.18s
Test loss: 0.2010 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 456/1000, LR 0.000168
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 0.29s
Val loss: 0.0984 score: 0.9690 time: 0.18s
Test loss: 0.2035 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 457/1000, LR 0.000167
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.29s
Val loss: 0.0989 score: 0.9767 time: 0.18s
Test loss: 0.2054 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 458/1000, LR 0.000167
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.29s
Val loss: 0.0982 score: 0.9690 time: 0.17s
Test loss: 0.2025 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 459/1000, LR 0.000167
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.29s
Val loss: 0.0988 score: 0.9535 time: 0.18s
Test loss: 0.1957 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 460/1000, LR 0.000166
Train loss: 0.5102;  Loss pred: 0.5102; Loss self: 0.0000; time: 0.33s
Val loss: 0.1013 score: 0.9535 time: 0.18s
Test loss: 0.1926 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 461/1000, LR 0.000166
Train loss: 0.5101;  Loss pred: 0.5101; Loss self: 0.0000; time: 0.48s
Val loss: 0.1047 score: 0.9535 time: 0.26s
Test loss: 0.1910 score: 0.9531 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 462/1000, LR 0.000165
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.29s
Val loss: 0.1091 score: 0.9535 time: 0.18s
Test loss: 0.1904 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 463/1000, LR 0.000165
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.30s
Val loss: 0.1108 score: 0.9535 time: 0.19s
Test loss: 0.1906 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 464/1000, LR 0.000164
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.29s
Val loss: 0.1128 score: 0.9535 time: 0.18s
Test loss: 0.1908 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 465/1000, LR 0.000164
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.29s
Val loss: 0.1160 score: 0.9535 time: 0.18s
Test loss: 0.1913 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 466/1000, LR 0.000163
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.33s
Val loss: 0.1156 score: 0.9535 time: 0.18s
Test loss: 0.1916 score: 0.9453 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 467/1000, LR 0.000163
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.32s
Val loss: 0.1141 score: 0.9535 time: 0.18s
Test loss: 0.1917 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 468/1000, LR 0.000162
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.28s
Val loss: 0.1111 score: 0.9535 time: 0.17s
Test loss: 0.1918 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 469/1000, LR 0.000162
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.29s
Val loss: 0.1075 score: 0.9457 time: 0.19s
Test loss: 0.1924 score: 0.9531 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 470/1000, LR 0.000162
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.32s
Val loss: 0.1038 score: 0.9535 time: 0.19s
Test loss: 0.1938 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 471/1000, LR 0.000161
Train loss: 0.5101;  Loss pred: 0.5101; Loss self: 0.0000; time: 0.28s
Val loss: 0.1007 score: 0.9535 time: 0.18s
Test loss: 0.1962 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 472/1000, LR 0.000161
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.29s
Val loss: 0.0991 score: 0.9535 time: 0.23s
Test loss: 0.1991 score: 0.9375 time: 0.25s
     INFO: Early stopping counter 18 of 20
Epoch 473/1000, LR 0.000160
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.28s
Val loss: 0.0985 score: 0.9535 time: 0.22s
Test loss: 0.2010 score: 0.9375 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 474/1000, LR 0.000160
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.29s
Val loss: 0.0983 score: 0.9612 time: 0.17s
Test loss: 0.2026 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 453,   Train_Loss: 0.5111,   Val_Loss: 0.0978,   Val_Precision: 0.9538,   Val_Recall: 0.9538,   Val_accuracy: 0.9538,   Val_Score: 0.9535,   Val_Loss: 0.0978,   Test_Precision: 0.9667,   Test_Recall: 0.9062,   Test_accuracy: 0.9355,   Test_Score: 0.9375,   Test_loss: 0.1989


[0.18168740393593907, 0.17993690306320786, 0.1788145659957081, 0.18228567112237215, 0.17690204712562263, 0.23888468882068992, 0.1784298368729651, 0.17704606195911765, 0.1796930010896176, 0.1843955221120268, 0.24733551195822656, 0.17931395303457975, 0.18048186507076025, 0.1778149229940027, 0.17880575102753937, 0.23898538784123957, 0.1790525179821998, 0.19404341815970838, 0.17513673100620508, 0.17378399590961635, 0.20219180290587246, 0.18160838400945067, 0.17443248187191784, 0.17985065700486302, 0.1760360209736973, 0.1748143839649856, 0.17443963303230703, 0.17921761516481638, 0.1787264528684318, 0.17682491592131555, 0.17457409808412194, 0.18594638397917151, 0.17429695511236787, 0.17443967098370194, 0.1782113970257342, 0.17641414608806372, 0.21121117984876037, 0.17939056386239827, 0.1752404139842838, 0.18276874697767198, 0.17966612498275936, 0.192024446092546, 0.19318971200846136, 0.2067842809483409, 0.19302397314459085, 0.1833941820077598, 0.17694903095252812, 0.18498046998865902, 0.17853786516934633, 0.20084986207075417, 0.18180940300226212, 0.2711539741139859, 0.2624143429566175, 0.17874474404379725, 0.17818864597938955, 0.1814644020050764, 0.18645196803845465, 0.18458584509789944, 0.2840159230399877, 0.19888689694926143, 0.25765787390992045, 0.20396828604862094, 0.18155831098556519, 0.17833115998655558, 0.19287341693416238, 0.1892456419300288, 0.27581674396060407, 0.17991724400781095, 0.1804673708975315, 0.1753166678827256, 0.2727769650518894, 0.19896838790737092, 0.1809561219997704, 0.1839779729489237, 0.18009839789010584, 0.18052981700748205, 0.1837447788566351, 0.18350780988112092, 0.18404719303362072, 0.17554380814544857, 0.1882991751190275, 0.18174860207363963, 0.24145855312235653, 0.1928718478884548, 0.19240265805274248, 0.2746984148398042, 0.22677652910351753, 0.19415098777972162, 0.19210935290902853, 0.17775634094141424, 0.18165841279551387, 0.25601488910615444, 0.1794003490358591, 0.18573276000097394, 0.1888417990412563, 0.18386748409830034, 0.18383482890203595, 0.17860724101774395, 0.19624479906633496, 0.27565432502888143, 0.2034821540582925, 0.2558202329091728, 0.25811801804229617, 0.18344635097309947, 0.19853700790554285, 0.1796335419639945, 0.17684747814200819, 0.20011176099069417, 0.24623317481018603, 0.17966424906626344, 0.1834364589303732, 0.1822256059385836, 0.26235823705792427, 0.25986084598116577, 0.1887125219218433, 0.1965301581658423, 0.19337897305376828, 0.3067359779961407, 0.18638653797097504, 0.18868466489948332, 0.26443788688629866, 0.18255002703517675, 0.18193056201562285, 0.18148718285374343, 0.1833625710569322, 0.19024271913804114, 0.252867846051231, 0.19650478102266788, 0.1942193869035691, 0.198158104903996, 0.1813597660511732, 0.1971800089813769, 0.1952816939447075, 0.19016393087804317, 0.18926978600211442, 0.19252808392047882, 0.18658543098717928, 0.2016567059326917, 0.1910969540476799, 0.18093420611694455, 0.1764968449715525, 0.17857850599102676, 0.1797091148328036, 0.17773758410476148, 0.18031929503194988, 0.1879357371944934, 0.18536841706372797, 0.17826207005418837, 0.17854330386035144, 0.17912889597937465, 0.17736377799883485, 0.18204319500364363, 0.23631161102093756, 0.1898831590078771, 0.18929122388362885, 0.18035321193747222, 0.18469136906787753, 0.20786970085464418, 0.26187392603605986, 0.1892113438807428, 0.18339773220941424, 0.18340318300761282, 0.19694175594486296, 0.1983655800577253, 0.19352237205021083, 0.1969647048972547, 0.18619839707389474, 0.26222395710647106, 0.18019392108544707, 0.17845997191034257, 0.17968398402445018, 0.25380648695863783, 0.26214759005233645, 0.25594697101041675, 0.25801608990877867, 0.18174890684895217, 0.18656321405433118, 0.1826921000611037, 0.18680506409145892, 0.185660783899948, 0.18310723593458533, 0.18395967315882444, 0.19128252402879298, 0.19214319507591426, 0.18352467892691493, 0.1824770870152861, 0.1797359159681946, 0.178780531976372, 0.17761349817737937, 0.1749681900255382, 0.17742997407913208, 0.179600786883384, 0.20665479800663888, 0.21230929694138467, 0.1791691528633237, 0.2767009718809277, 0.27262235805392265, 0.27480471320450306, 0.19470580713823438, 0.18040540697984397, 0.1784669968765229, 0.26820149295963347, 0.18159504095092416, 0.1793233088683337, 0.18320019193924963, 0.18140686000697315, 0.2581524520646781, 0.2580427611246705, 0.1858325768262148, 0.19660439412109554, 0.192632821155712, 0.18323963601142168, 0.18210981599986553, 0.18590736109763384, 0.18102639308199286, 0.18452469003386796, 0.18189326603896916, 0.18955694511532784, 0.1816487959586084, 0.18454485503025353, 0.2624757708981633, 0.18598547810688615, 0.18248570407740772, 0.18116677016951144, 0.2622371220495552, 0.258184903068468, 0.29449075693264604, 0.19812951982021332, 0.19960692385211587, 0.28071067901328206, 0.1972336838953197, 0.19629826000891626, 0.19859928591176867, 0.19713962194509804, 0.20224871509708464, 0.20172415091656148, 0.19808013108558953, 0.18747931299731135, 0.18480659509077668, 0.18180735502392054, 0.20587984914891422, 0.18432267010211945, 0.18113971618004143, 0.18134108209051192, 0.18504902301356196, 0.17904435703530908, 0.18304999195970595, 0.18289971607737243, 0.24081481690518558, 0.1796443669591099, 0.17653592000715435, 0.1805662871338427, 0.19790233811363578, 0.27462564199231565, 0.2764632028993219, 0.19782597105950117, 0.19928043615072966, 0.1963444270659238, 0.19771025399677455, 0.26518933195620775, 0.19867930398322642, 0.19360701693221927, 0.20502011198550463, 0.1802920331247151, 0.1753831619862467, 0.1811112950090319, 0.2483785639051348, 0.18883854988962412, 0.18966347607783973, 0.1896190328989178, 0.20562671613879502, 0.18573754793033004, 0.1882908681873232, 0.19224560307338834, 0.19178669387474656, 0.19262000103481114, 0.18798826588317752, 0.18922094302251935, 0.18906449107453227, 0.1954092630185187, 0.19368316605687141, 0.1980601940304041, 0.2484677559696138, 0.1833145609125495, 0.18355662189424038, 0.18191270297393203, 0.1793722310103476, 0.17979942611418664, 0.17995452997274697, 0.17842229921370745, 0.17965890211053193, 0.1770287179388106, 0.1762514899019152, 0.18285556882619858, 0.17888285499066114, 0.17964874883182347, 0.1806462248787284, 0.24276216584257782, 0.2598722211550921, 0.2597331970464438, 0.19063687114976346, 0.18030333588831127, 0.17267546709626913, 0.17511430708691478, 0.18884316179901361, 0.1810079589486122, 0.1733140181750059, 0.18021191796287894, 0.18399698892608285, 0.2501429819967598, 0.18495635106228292, 0.1801813510246575, 0.16859115310944617, 0.16682780697010458, 0.17018711380660534, 0.172338942065835, 0.16985344304703176, 0.16961041488684714, 0.17212896002456546, 0.17206528410315514, 0.17062926501967013, 0.1665492309257388, 0.18750447710044682, 0.17274427507072687, 0.16915665799751878, 0.1733952770009637, 0.1822308418340981, 0.18455713614821434, 0.18430256005376577, 0.16885394416749477, 0.17235743696801364, 0.17034851689822972, 0.17260518996044993, 0.17121980106458068, 0.18521430692635477, 0.25401349109597504, 0.18040958605706692, 0.1800859358627349, 0.18210071604698896, 0.24440186494030058, 0.18106316099874675, 0.18409104901365936, 0.1832501501776278, 0.19813938392326236, 0.2616260398644954, 0.17228667088784277, 0.175611981889233, 0.17372494004666805, 0.17752021411433816, 0.18002100987359881, 0.18890480208210647, 0.26427693595178425, 0.17656637099571526, 0.17891383985988796, 0.17785048205405474, 0.18118969700299203, 0.16500380006618798, 0.16933613386936486, 0.1688686558045447, 0.183418198954314, 0.19245663611218333, 0.17916777916252613, 0.17800146201625466, 0.21648778882808983, 0.18142205104231834, 0.18766020704060793, 0.18813279806636274, 0.18126654205843806, 0.165900357067585, 0.16715226788073778, 0.17178493505343795, 0.17025723890401423, 0.16901552607305348, 0.24726916220970452, 0.21069943206384778, 0.1672998599242419, 0.16715987608768046, 0.24743621097877622, 0.17508694995194674, 0.17537454422563314, 0.17443449492566288, 0.20051067508757114, 0.2515318251680583, 0.25152816995978355, 0.1751227038912475, 0.1740553320851177, 0.2558939990121871, 0.253462360939011, 0.2089412328787148, 0.17703923396766186, 0.1747359309811145, 0.17650408297777176, 0.17595647601410747, 0.17481698794290423, 0.18239627894945443, 0.2639602078124881, 0.17551760817877948, 0.17632321896962821, 0.17330088117159903, 0.19051104807294905, 0.28318201401270926, 0.17840668093413115, 0.17991845402866602, 0.2515126448124647, 0.25994835793972015, 0.26130486908368766, 0.17823643703013659, 0.1790722890291363, 0.18021190795116127, 0.17964845383539796, 0.17801709100604057, 0.1783940449822694, 0.26226620585657656, 0.18499685800634325, 0.18485292606055737, 0.18414453114382923, 0.25323895714245737, 0.18577234004624188, 0.18057948886416852, 0.18118567089550197, 0.18056909809820354, 0.24151132395491004, 0.1840746260713786, 0.17680299212224782, 0.17888632300309837, 0.17788103315979242, 0.1753596479538828, 0.1764326300472021, 0.1825967978220433, 0.18272804887965322, 0.18073167488910258, 0.181890174979344, 0.18158667907118797, 0.18189655803143978, 0.18219552491791546, 0.18800539919175208, 0.1819156261626631, 0.17587476992048323, 0.20335526089183986, 0.2599627310410142, 0.1822793809697032, 0.18770882696844637, 0.1819497940596193, 0.18932873290032148, 0.20017106411978602, 0.1776065630838275, 0.2022371320053935, 0.19007672904990613, 0.19250879995524883, 0.19855448394082487, 0.2001423758920282, 0.195024506887421, 0.18564276094548404, 0.17956856288947165, 0.17965619009919465, 0.20392838586121798, 0.18067662185057998, 0.1912378049455583, 0.17795423907227814, 0.1966081541031599, 0.23317867796868086, 0.18154426896944642, 0.18756463890895247, 0.23639545310288668, 0.26415057899430394, 0.26190680102445185, 0.25557562289759517, 0.18164631188847125, 0.181515634059906, 0.1938612130470574, 0.18174590915441513, 0.17961207800544798, 0.27208637702278793, 0.18264919705688953, 0.18067347002215683, 0.18442333303391933, 0.18972592684440315, 0.19943136302754283, 0.19794633192941546, 0.22119683912023902, 0.19161030603572726, 0.18421697104349732, 0.1848851980175823, 0.2938428409397602, 0.19932278990745544, 0.19941812101751566, 0.1880698329769075, 0.19004999683238566, 0.18372177612036467, 0.2654381620232016, 0.19708245899528265, 0.19390313886106014, 0.1948069599457085, 0.18034358695149422, 0.18207829119637609, 0.18103137891739607, 0.18170549813658, 0.259817369049415, 0.26404107781127095, 0.236524673178792, 0.18146621505729854, 0.18263928592205048, 0.18136494513601065, 0.18339851102791727, 0.25560335582122207, 0.19794194004498422, 0.1953520851675421, 0.1920978759881109, 0.2786977009382099, 0.18681188696064055, 0.18286262499168515, 0.18567361496388912, 0.18161277007311583, 0.1832333360798657, 0.25822394993156195, 0.19161431398242712, 0.16968813491985202, 0.17717209295369685, 0.1691737819928676, 0.17465137992985547, 0.2174944079015404, 0.22392000211402774, 0.21763187600299716, 0.1745466140564531, 0.17287351889535785, 0.17299994197674096, 0.1688096751458943, 0.22450688085518777, 0.17006715689785779, 0.16212643682956696, 0.16161748999729753, 0.1733216338325292, 0.17471062391996384, 0.1821448360569775, 0.17609678395092487, 0.17565331398509443, 0.16514823585748672, 0.16376038501039147, 0.1622217500116676, 0.16553100012242794, 0.17303395317867398, 0.16416989592835307, 0.16817010520026088, 0.17024705512449145, 0.17460082983598113, 0.16943432809785008, 0.2173748358618468, 0.17977760103531182, 0.1786134319845587, 0.17899176012724638, 0.17070117499679327, 0.17905106488615274, 0.22150224191136658, 0.18034056690521538, 0.17635137005709112, 0.17836088011972606, 0.17678399803116918, 0.21440258203074336, 0.22313733282499015, 0.16382521693594754, 0.17983313486911356, 0.1727186089847237, 0.18234071787446737, 0.17999674892053008, 0.16666028392501175, 0.17411851696670055, 0.16532394802197814, 0.20406777784228325, 0.18131916993297637, 0.16686225682497025, 0.16641790000721812, 0.21592909307219088, 0.2239112970419228, 0.16370202507823706, 0.16425497992895544, 0.16243721800856292, 0.16190129308961332, 0.1732456050813198, 0.2133091720752418, 0.1636284210253507, 0.1648967950604856, 0.16626776522025466, 0.2096381860319525, 0.21342754596844316, 0.21657146606594324, 0.16734961094334722, 0.16605978715233505, 0.1665728590451181, 0.16643048799596727, 0.16591586405411363, 0.16546549508348107, 0.1642751768231392, 0.16225074999965727, 0.16601397190243006, 0.16342987585812807, 0.21249002404510975, 0.22546038287691772, 0.1657670889981091, 0.16154219419695437, 0.1848182580433786, 0.16498754709027708, 0.16243810299783945, 0.21352391596883535, 0.2197086571250111, 0.16622911603190005, 0.1643950310535729, 0.16487197787500918, 0.1877780039794743, 0.1646933970041573, 0.1653229440562427, 0.16891914512962103, 0.16954146116040647, 0.21554329292848706, 0.1638764648232609, 0.1653202809393406, 0.16479604016058147, 0.1696429569274187, 0.16782843507826328, 0.2042830679565668, 0.16651343600824475, 0.16395426401868463, 0.1781392248813063, 0.20888377400115132, 0.2153950361534953, 0.21548451902344823, 0.16929455706849694, 0.16991020389832556, 0.16566953784786165, 0.17750451690517366, 0.16901853797025979, 0.23438228899613023, 0.17958847084082663, 0.16781101212836802, 0.16672708303667605, 0.1670038769952953, 0.2139004480559379, 0.1687948419712484, 0.16673854691907763, 0.17095815297216177, 0.1787133750040084, 0.20071805897168815, 0.215710457181558, 0.17259045294485986, 0.1786667990963906, 0.2196892029605806, 0.2175811268389225, 0.29534882097505033, 0.17908660392276943, 0.17218120489269495, 0.16196845402009785, 0.16706218104809523, 0.17738340608775616, 0.177401120075956, 0.1767290618736297, 0.16365558211691678, 0.1625418639741838, 0.16110846982337534, 0.16354225086979568, 0.19151987601071596, 0.2133721869904548, 0.16715704300440848, 0.16754942503757775, 0.1641526820603758, 0.16509217116981745, 0.16526516806334257, 0.17070083389990032, 0.1668003760278225, 0.16775136394426227, 0.16882859892211854, 0.1701460590120405, 0.2149211319629103, 0.16865341598168015, 0.17269835621118546, 0.1707149469293654, 0.16988364607095718, 0.17113399389199913, 0.1992783669847995, 0.21535223489627242, 0.1701154219917953, 0.1655548179987818, 0.17424392909742892, 0.17682328401133418, 0.17914797505363822, 0.20608418295159936, 0.17389552388340235, 0.1632266330998391, 0.1632389819715172, 0.16560011287219822, 0.16470340616069734, 0.1658070390112698, 0.16644080309197307, 0.1643105740658939, 0.16316081397235394, 0.16659239307045937, 0.16552333999425173, 0.15842275298200548, 0.19023616891354322, 0.17443867889232934, 0.16996839083731174, 0.169719316996634, 0.1630523249041289, 0.16674696910195053, 0.21052531711757183, 0.16571549093350768, 0.1657265720423311, 0.21690210397355258, 0.21344226086512208, 0.16572348307818174, 0.1629080690909177, 0.21298435097560287, 0.21758697018958628, 0.22171010496094823, 0.21550023113377392, 0.1683425100054592, 0.1640487180557102, 0.2189724741037935, 0.22394572291523218, 0.22402465692721307, 0.17416207096539438, 0.16601126198656857, 0.17234893096610904, 0.17546222498640418, 0.1736619269941002, 0.23026442900300026, 0.1781828620005399, 0.17376277479343116, 0.176197747932747, 0.2196396819781512, 0.17517885798588395, 0.17214241111651063, 0.1722070099785924, 0.2526624428573996, 0.2239304599352181, 0.1743961088359356, 0.1754100420512259, 0.17666286998428404, 0.16411171318031847, 0.16698432411067188, 0.1641080309636891, 0.16851077903993428, 0.16054019005969167, 0.16148837399668992, 0.16233711200766265, 0.16806464991532266, 0.21333057805895805, 0.16362477815710008, 0.16318336897529662, 0.16426379699259996, 0.17673503491096199, 0.21548138791695237, 0.1630803078878671, 0.1661711318884045, 0.16577392909675837, 0.16850924002937973, 0.19015081506222486, 0.16433423198759556, 0.1653883708640933, 0.1629571421071887, 0.1649298700504005, 0.20651515619829297, 0.16431481204926968, 0.16642886213958263, 0.16416459600441158, 0.16564191598445177, 0.20691593503579497, 0.17856876295991242, 0.1826861440204084, 0.17670546215958893, 0.17808910901658237, 0.17521209688857198, 0.1794961029663682, 0.1793734251987189, 0.17900924989953637, 0.17691394314169884, 0.1747117741033435, 0.17630257015116513, 0.22877996694296598, 0.18977190693840384, 0.16347725689411163, 0.16308710793964565, 0.17113738087937236, 0.16064782696776092, 0.22197609092108905, 0.21764751779846847, 0.16893613408319652, 0.17137796408496797, 0.1667389888316393, 0.16755797411315143, 0.16697454499080777, 0.16311001009307802, 0.16659199190326035, 0.16691856691613793, 0.16850336105562747, 0.16619550390169024, 0.18004698492586613, 0.21767750498838723, 0.16401706682518125, 0.16262900503352284, 0.16499500977806747, 0.2174959189724177, 0.16311031393706799, 0.16487952391617, 0.18571417615748942, 0.2132327959407121, 0.20695698913186789, 0.16366010904312134, 0.1761183519847691, 0.17572851106524467, 0.1732179077807814, 0.1752855139784515, 0.2678813571110368, 0.1816824828274548, 0.16689829295501113, 0.1656579349655658, 0.1727572800591588, 0.21770547307096422, 0.22230360796675086, 0.1692264978773892, 0.17396430391818285, 0.17533522006124258, 0.17286061705090106, 0.17709833104163408, 0.22184959496371448, 0.22187071689404547, 0.1738046440295875, 0.18087676004506648, 0.17075849790126085, 0.1747018441092223, 0.1695545120164752, 0.20414626109413803, 0.17447134596295655, 0.16066002706065774, 0.1646667739842087, 0.16618676600046456, 0.17146300501190126, 0.17843348206952214, 0.1742910980246961, 0.16546265012584627, 0.18676998489536345, 0.22587746405042708, 0.2219922300428152, 0.17973170499317348, 0.17432327196002007, 0.17169767781160772, 0.16846672305837274, 0.17065028403885663, 0.1658805999904871, 0.1678581489250064, 0.16778579889796674, 0.16537443595007062, 0.16882260493002832, 0.16909463703632355, 0.2142646750435233, 0.1642722098622471, 0.16754923318512738, 0.16941332700662315, 0.15952554694376886, 0.16117962216958404, 0.16179603594355285, 0.16072355094365776, 0.16560735111124814, 0.1598543399013579, 0.21833906997926533, 0.16364491893909872, 0.1584267180878669, 0.15961170196533203, 0.1816907119937241, 0.21130263805389404, 0.16286281892098486, 0.16279350011609495, 0.1605414489749819, 0.16549190902151167, 0.16438547801226377, 0.16732606897130609, 0.20880125905387104, 0.1607405508402735, 0.16073736594989896, 0.15975409001111984, 0.1616618880070746, 0.16471476200968027, 0.15986858610995114, 0.16226842091418803, 0.15883258008398116, 0.16246861196123064, 0.1651918930001557, 0.21780424797907472, 0.16260506305843592, 0.1659499320667237, 0.16530993417836726, 0.2113725170493126, 0.20923910895362496, 0.21133174002170563, 0.1596828878391534, 0.16061462205834687, 0.16001877491362393, 0.15961993113160133, 0.19092539604753256, 0.20265389000996947, 0.1597590900491923, 0.1590270719025284, 0.15904163918457925, 0.15746513986960053, 0.16309534385800362, 0.2711501410230994, 0.16057530418038368, 0.1681013097986579, 0.16078341007232666, 0.1626339580398053, 0.16420466708950698, 0.17375945509411395, 0.16402050713077188, 0.16076732985675335, 0.1611111480742693, 0.16314505506306887, 0.17235687910579145, 0.16730002616532147, 0.16437955992296338, 0.16258972813375294, 0.16553799994289875, 0.16366659896448255, 0.16448572790250182, 0.1689299091231078, 0.22044048202224076, 0.1632136208936572, 0.17309524212032557, 0.17790022888220847, 0.16540533490478992, 0.1701155451592058, 0.1725894759874791, 0.16503971000202, 0.1674733180552721, 0.16584163811057806, 0.21765538305044174, 0.1734757269732654, 0.17923058406449854, 0.1817444320768118, 0.2288531749509275, 0.23021630314178765, 0.17853370890952647, 0.18334925989620388, 0.1788276620209217, 0.17507609794847667, 0.20227909297682345, 0.16775290411897004, 0.17223526909947395, 0.16855957615189254, 0.1678138650022447, 0.17211818788200617, 0.18435994302853942, 0.1893516629934311, 0.16912937490269542, 0.1701636598445475, 0.16795604210346937, 0.1717563329730183, 0.17210879805497825, 0.17135913809761405, 0.1706422020215541, 0.17861543805338442, 0.18278900883160532, 0.17824205802753568, 0.17793687200173736, 0.17774933413602412, 0.17852788395248353, 0.18267961801029742, 0.2075833808630705, 0.18073688098229468, 0.16223194915801287, 0.18951460300013423, 0.21552369906567037, 0.16931429994292557, 0.16463156905956566, 0.16659261588938534, 0.16334453620947897, 0.16604400984942913, 0.1661045339424163, 0.22505992278456688, 0.16579802706837654, 0.16938743786886334, 0.17541805491782725, 0.16372464294545352, 0.19356409506872296, 0.16611067089252174, 0.16771924402564764, 0.17204497195780277, 0.16634118393994868, 0.1650012480095029, 0.2585192658007145, 0.2033519211690873, 0.16241421294398606]
[0.0014084294878754966, 0.001394859713668278, 0.0013861594263233188, 0.001413067218002885, 0.0013713336986482375, 0.0018518192931836427, 0.0013831770300229852, 0.0013724500927063384, 0.0013929690006947101, 0.0014294226520312155, 0.0019173295500637718, 0.001390030643678913, 0.0013990842253547306, 0.0013784102557674628, 0.0013860910932367394, 0.0018525999057460432, 0.0013880040153658897, 0.0015042125438737084, 0.0013576490775674814, 0.001347162758989274, 0.0015673783170997864, 0.0014078169303058192, 0.0013521897819528516, 0.0013941911395725815, 0.0013646203176255606, 0.001355150263294462, 0.0013522452173047057, 0.0013892838384869488, 0.0013854763788250528, 0.0013707357823357794, 0.001353287582047457, 0.0014414448370478413, 0.0013511391869175803, 0.0013522455115015655, 0.0013814836978739084, 0.0013675515200625094, 0.001637295967819848, 0.0013906245260651027, 0.0013584528215835954, 0.0014168119920749767, 0.0013927606587810803, 0.0014885615976166357, 0.0014975946667322585, 0.001602978922080162, 0.0014963098693379136, 0.0014216603256415488, 0.0013716979143606832, 0.0014339571316950312, 0.0013840144586771033, 0.0015569756749670865, 0.0014093752170717994, 0.002101968791581286, 0.002034219712841996, 0.0013856181708821493, 0.0013813073331735623, 0.0014067007907370265, 0.0014453640933213539, 0.0014308980240147243, 0.0022016738220154083, 0.001541758891079546, 0.001997347859766825, 0.0015811495042528756, 0.0014074287673299627, 0.0013824120929190355, 0.0014951427669314913, 0.0014670204800777426, 0.0021381142942682486, 0.0013947073178900073, 0.001398971867422725, 0.0013590439370753922, 0.0021145501166813132, 0.001542390603933108, 0.0014027606356571349, 0.001426185836813362, 0.0013961116115512081, 0.0013994559457944345, 0.0014243781306715899, 0.0014225411618691544, 0.001426722426617215, 0.0013608047143058028, 0.0014596835280544767, 0.0014089038920437182, 0.0018717717296306707, 0.0014951306037864712, 0.0014914934732770736, 0.0021294450762775518, 0.0017579575899497482, 0.001505046416897067, 0.0014892197899924692, 0.0013779561313287925, 0.0014082047503528206, 0.0019846115434585614, 0.001390700380122939, 0.0014397888372168523, 0.0014638899150484985, 0.0014253293340953516, 0.0014250761930390385, 0.0013845522559515035, 0.0015212775121421316, 0.0021368552327820267, 0.0015773810392115699, 0.0019831025806912616, 0.0020009148685449316, 0.0014220647362255773, 0.001539046572911185, 0.0013925080772402676, 0.0013709106832713812, 0.0015512539611681718, 0.0019087843008541553, 0.0013927461167927398, 0.0014219880537238232, 0.0014126015964231289, 0.0020337847833947617, 0.0020144251626446957, 0.0014628877668359944, 0.001523489598184824, 0.0014990618066183588, 0.0023777982790398507, 0.0014448568834959305, 0.0014626718209262274, 0.002049906099893788, 0.001415116488644781, 0.0014103144342296344, 0.0014068773864631273, 0.0014214152795111025, 0.0014747497607600088, 0.001960215860862256, 0.0015232928761447122, 0.0015055766426633264, 0.0015361093403410543, 0.0014058896593114201, 0.0015285272014060223, 0.001513811580966725, 0.0014741389990545982, 0.0014672076434272434, 0.0014924657668254173, 0.0014463986898230951, 0.0015632302785479977, 0.0014813717368037202, 0.0014025907450925935, 0.0013681925966787015, 0.001384329503806409, 0.001393093913432586, 0.001377810729494275, 0.0013978239924957354, 0.0014568661798022744, 0.0014369644733622324, 0.001381876512047972, 0.0013840566190724919, 0.0013885960928633693, 0.0013749130077429059, 0.0014111875581677802, 0.0018318729536506788, 0.00147196247292928, 0.0014673738285552623, 0.0013980869142439707, 0.0014317160392858723, 0.0016113930298809625, 0.002030030434388061, 0.0014667546037266882, 0.0014216878465846065, 0.001421730100834208, 0.0015266802786423484, 0.0015377176748660876, 0.0015001734267458204, 0.0015268581774980985, 0.0014433984269294165, 0.0020327438535385355, 0.0013968521014375741, 0.001383410634963896, 0.00139289910096473, 0.001967492146966185, 0.0020321518608708254, 0.001984085046592378, 0.0020001247279750286, 0.00140890625464304, 0.001446226465537451, 0.0014162178299310364, 0.0014481012720268132, 0.0014392308829453333, 0.0014194359374774057, 0.0014260439779753833, 0.0014828102637890928, 0.0014894821323714284, 0.0014226719296660073, 0.0014145510621340006, 0.001393301674172051, 0.001385895596716062, 0.0013768488230804603, 0.0013563425583375055, 0.0013754261556521866, 0.0013922541618866975, 0.0016019751783460379, 0.0016458085034215867, 0.0013889081617311915, 0.002144968774270757, 0.002113351612821106, 0.0021302690946085508, 0.0015093473421568557, 0.0013984915269755346, 0.001383465092066069, 0.0020790813407723526, 0.0014077134957435982, 0.0013901031695219667, 0.0014201565266608498, 0.0014062547287362259, 0.002001181798950993, 0.0020003314815865932, 0.0014405626110559286, 0.001524065070706167, 0.0014932776833776124, 0.0014204622946621835, 0.0014117039999989576, 0.0014411423340901848, 0.001403305372728627, 0.0014304239537509143, 0.001410025318131544, 0.0014694336830645568, 0.0014081302012295225, 0.001430580271552353, 0.0020346958984353747, 0.001441747892301443, 0.0014146178610651761, 0.001404393567205515, 0.0020328459073608927, 0.002001433357119907, 0.002282874084749194, 0.0015358877505442894, 0.0015473404949776423, 0.00217605177529676, 0.0015289432860102303, 0.0015216919380536143, 0.0015395293481532456, 0.0015282141236054112, 0.0015678194968766252, 0.0015637531078803216, 0.001535504892136353, 0.0014533280077310958, 0.0014326092642695867, 0.0014093593412707018, 0.0015959678228598, 0.0014288579077683678, 0.0014041838463569104, 0.001405744822407069, 0.0014344885504927285, 0.0013879407522116984, 0.0014189921857341546, 0.0014178272564137398, 0.0018667815263967874, 0.0013925919919310845, 0.001368495503931429, 0.001399738659952269, 0.0015341266520436882, 0.0021288809456768655, 0.0021431256038707126, 0.001533534659375978, 0.0015448095825637959, 0.0015220498222164637, 0.0015326376278819733, 0.0020557312554744787, 0.0015401496432808249, 0.0015008295886218548, 0.0015893031936860824, 0.0013976126598815124, 0.0013595593952422226, 0.0014039635272017977, 0.0019254152240708125, 0.0014638647278265437, 0.0014702595044793778, 0.001469914983712541, 0.0015940055514635273, 0.001439825952948295, 0.0014596191332350637, 0.001490275992816964, 0.001486718557168578, 0.0014931783025954352, 0.0014572733789393606, 0.0014668290156784447, 0.0014656162098800952, 0.0015148004885156487, 0.0015014198919137318, 0.001535350341320962, 0.0019261066354233627, 0.001421043107849221, 0.001422919549567755, 0.0014101759920459847, 0.0013904824109329272, 0.0013937940008851679, 0.0013949963563778835, 0.0013831185985558718, 0.0013927046675235033, 0.0013723156429365161, 0.0013662906193946914, 0.001417485029660454, 0.0013866887983772181, 0.001392625959936616, 0.0014003583323932433, 0.0018818772545936265, 0.002014513342287536, 0.002013435636018944, 0.0014778052027113447, 0.0013977002782039633, 0.0013385695123741793, 0.0013574752487357734, 0.001463900479062121, 0.0014031624724698622, 0.00134351952073648, 0.0013969916121153406, 0.0014263332474890144, 0.0019390928836958122, 0.001433770163273511, 0.001396754659105872, 0.0013069081636391175, 0.001293238813721741, 0.0013192799519891887, 0.0013359607912080233, 0.0013166933569537347, 0.0013148094177274972, 0.001334333023446244, 0.0013338394116523655, 0.0013227074807726366, 0.0012910793095018513, 0.0014535230782980373, 0.0013391029075250146, 0.001311291922461386, 0.0013441494341159975, 0.0014126421847604504, 0.0014306754740171653, 0.001428702015920665, 0.0013089453036239905, 0.0013361041625427415, 0.0013205311387459668, 0.001338024728375581, 0.0013272852795703928, 0.0014357698211345331, 0.001969096830201357, 0.0013985239229229993, 0.0013960150066878675, 0.0014116334577285966, 0.0018945881003124077, 0.001403590395339122, 0.0014270623954547236, 0.0014205437998265722, 0.001535964216459398, 0.002028108836158879, 0.001335555588277851, 0.0013613331929397906, 0.001346704961602078, 0.0013761256908088229, 0.0013955117044465025, 0.0014643783107140037, 0.0020486584182308858, 0.0013687315581063198, 0.0013869289911619222, 0.0013786859073957733, 0.0014045712945968375, 0.0012790992253192866, 0.00131268320828965, 0.0013090593473220518, 0.001421846503521814, 0.0014919119078463823, 0.0013888975128877996, 0.0013798562946996486, 0.0016781999133960452, 0.0014063724887001421, 0.0014547302871364956, 0.0014583937834601762, 0.0014051669927010702, 0.001286049279593682, 0.0012957540145793625, 0.0013316661632049452, 0.0013198235573954591, 0.0013101978765352983, 0.001916815210927942, 0.001633328930727502, 0.0012968981389476116, 0.0012958129929277556, 0.0019181101626261723, 0.0013572631779220677, 0.001359492590896381, 0.0013522053870206425, 0.0015543463185083034, 0.001949859109829909, 0.0019498307748820431, 0.0013575403402422287, 0.0013492661401947109, 0.001983674410947187, 0.001964824503403186, 0.0016196994796799597, 0.0013723971625400145, 0.0013545421006287946, 0.0013682487052540447, 0.0013640036900318408, 0.0013551704491698003, 0.0014139246430190265, 0.0020462031613371168, 0.0013606016137889883, 0.0013668466586792885, 0.0013434176835007677, 0.0014768298300228608, 0.0021952094109512345, 0.0013829975266211718, 0.0013947166978966357, 0.0019497104249028272, 0.0020151035499203114, 0.0020256191401836253, 0.0013816778064351673, 0.0013881572792956301, 0.0013969915345051261, 0.00139262367314262, 0.0013799774496592292, 0.0013828995735059644, 0.0020330713632292757, 0.001434084170591808, 0.001432968419074088, 0.0014274769856110793, 0.0019630926910268014, 0.0014400956592731929, 0.0013998409989470428, 0.0014045400844612556, 0.0013997604503736708, 0.0018721808058520157, 0.0014269350858246402, 0.001370565830405022, 0.001386715682194561, 0.0013789227376728094, 0.0013593771159215722, 0.0013676948065674582, 0.0014154790528840566, 0.0014164965029430483, 0.0014010207355744386, 0.0014100013564290234, 0.0014076486749704494, 0.0014100508374530216, 0.001412368410216399, 0.0014574061952848999, 0.001410198652423745, 0.0013633703094611104, 0.0015763973712545727, 0.0020152149693101877, 0.001413018457129482, 0.0014551071858019099, 0.0014104635198420102, 0.0014676645961265232, 0.0015517136753471785, 0.0013767950626653294, 0.0015677297054681667, 0.0014734630158907453, 0.001492316278722859, 0.001539182046052906, 0.0015514912859847148, 0.0015118178828482248, 0.0014390911701200313, 0.0013920043634842764, 0.0013926836441798035, 0.001580840200474558, 0.0014005939678339533, 0.0014824636042291342, 0.0013794902253664973, 0.0015240942178539527, 0.0018075866509200066, 0.0014073199144918327, 0.0014539894489066084, 0.0018325228922704394, 0.0020476789069325887, 0.002030285279259317, 0.0019812063790511252, 0.0014081109448718702, 0.0014070979384488839, 0.0015028001011399797, 0.0014088830167008925, 0.001392341689964713, 0.0021091967211068832, 0.0014158852485030196, 0.0014005695350554793, 0.001429638240573018, 0.0014707436189488616, 0.0015459795583530453, 0.0015344676893753136, 0.001714704179226659, 0.001485351209579281, 0.0014280385352209094, 0.0014332185892835837, 0.00227785148015318, 0.0015451379062593445, 0.0015458769071125245, 0.001457905681991531, 0.0014732557893983385, 0.001424199814886548, 0.002057660170722493, 0.0015277709999634314, 0.0015031251074500787, 0.0015101314724473528, 0.0013980123019495675, 0.001411459621677334, 0.0014033440226154735, 0.001408569752996744, 0.0020140881321660076, 0.002046830060552488, 0.0018335245982852094, 0.001406714845405415, 0.0014158084180003912, 0.0014059298072558964, 0.0014216938839373432, 0.00198142136295521, 0.0015344336437595675, 0.0015143572493607914, 0.0014891308216132629, 0.0021604472940946503, 0.0014481541624855856, 0.0014175397286177143, 0.00143933034855728, 0.0014078509307993475, 0.001420413457983455, 0.0020017360459811005, 0.0014969868279877119, 0.001325688554061344, 0.0013841569762007566, 0.001321670171819278, 0.0013644639057019958, 0.0016991750617307844, 0.0017493750165158417, 0.0017002490312734153, 0.00136364542231604, 0.0013505743663699832, 0.0013515620466932887, 0.0013188255870772991, 0.0017539600066811545, 0.001328649663264514, 0.0012666127877309918, 0.001262636640603887, 0.0013540752643166343, 0.0013649267493747175, 0.0014230065316951368, 0.0013757561246166006, 0.0013722915155085502, 0.001290220592636615, 0.0012793780078936834, 0.0012673574219661532, 0.0012932109384564683, 0.0013518277592083905, 0.0012825773119402584, 0.001313828946877038, 0.0013300551181600895, 0.0013640689830936026, 0.0013237056882644538, 0.0016982409051706782, 0.0014045125080883736, 0.0013954174373793649, 0.0013983731259941123, 0.0013336029296624474, 0.0013988364444230683, 0.0017304862649325514, 0.0014089106789469952, 0.0013777450785710244, 0.0013934443759353599, 0.0013811249846185092, 0.0016750201721151825, 0.0017432604126952356, 0.0012798845073120901, 0.0014049463661649497, 0.0013493641326931538, 0.0014245368583942764, 0.0014062246009416413, 0.0013020334681641543, 0.001360300913802348, 0.0012915933439217042, 0.0015942795143928379, 0.0014165560151013779, 0.00130361138144508, 0.0013001398438063916, 0.0016869460396264913, 0.001749307008140022, 0.001278922070923727, 0.0012832420306949643, 0.0012690407656918978, 0.001264853852262604, 0.001353481289697811, 0.0016664779068378266, 0.0012783470392605523, 0.0012882562114100438, 0.0012989669157832395, 0.001637798328374629, 0.0016674027028784622, 0.0016919645786401816, 0.0013074188354949001, 0.0012973420871276176, 0.001301350461289985, 0.0013002381874684943, 0.0012962176879227627, 0.0012926991803396959, 0.001283399818930775, 0.0012675839843723224, 0.0012969841554877348, 0.0012767959051416256, 0.00166007831285242, 0.0017614092412259197, 0.0012950553827977274, 0.001262048392163706, 0.0014438926409638952, 0.0012889652116427897, 0.0012690476796706207, 0.0016681555935065262, 0.0017164738837891491, 0.0012986649689992191, 0.0012843361801060382, 0.0012880623271485092, 0.001467015656089643, 0.001286667164094979, 0.0012915855004393961, 0.0013196808213251643, 0.0013245426653156755, 0.0016839319760038052, 0.0012802848814317258, 0.0012915646948385984, 0.0012874690637545427, 0.0013253356009954587, 0.001311159649048932, 0.0015959614684106782, 0.0013008862188144121, 0.0012808926876459736, 0.0013917126943852054, 0.0016319044843839947, 0.0016827737199491821, 0.0016834728048706893, 0.0013226137270976324, 0.0013274234679556685, 0.0012942932644364191, 0.0013867540383216692, 0.0013204573278926546, 0.0018311116327822674, 0.001403034928443958, 0.0013110235322528752, 0.0013025553362240316, 0.0013047177890257444, 0.0016710972504370147, 0.001318709702900378, 0.001302644897805294, 0.0013356105700950138, 0.0013961982422188157, 0.0015681098357163137, 0.001685237946730922, 0.0013483629136317177, 0.0013958343679405516, 0.0017163218981295358, 0.001699852553429082, 0.0023074126638675807, 0.0013991140931466362, 0.0013451656632241793, 0.0012653785470320145, 0.001305173289438244, 0.001385807860060595, 0.0013859462505934061, 0.001380695795887732, 0.0012785592352884123, 0.001269858312298311, 0.0012586599204951199, 0.0012776738349202788, 0.0014962490313337184, 0.001666970210862928, 0.0013059143984719412, 0.0013089798831060762, 0.001282442828596686, 0.0012897825872641988, 0.0012911341254948638, 0.0013336002648429712, 0.0013031279377173632, 0.001310557530814549, 0.0013189734290790511, 0.0013292660860315664, 0.0016790713434602367, 0.0013176048123568762, 0.0013492059078998864, 0.0013337105228856672, 0.001327215984929353, 0.0013369843272812432, 0.0015568622420687461, 0.0016824393351271283, 0.0013290267343109008, 0.0012933970156154828, 0.0013612806960736634, 0.0013814319063385483, 0.0013995935551065486, 0.00161003267930937, 0.0013585587803390808, 0.001275208071092493, 0.0012753045466524782, 0.0012937508818140486, 0.001286745360630448, 0.0012953674922755454, 0.0013003187741560396, 0.001283676359889796, 0.0012746938591590151, 0.0013015030708629638, 0.0012931510937050916, 0.0012376777576719178, 0.0014862200696370564, 0.001362802178846323, 0.001327878053416498, 0.0013259321640362032, 0.001273846288313507, 0.0013027106961089885, 0.00164472903998103, 0.0012946522729180288, 0.0012947388440807117, 0.0016945476872933796, 0.0016675176630087662, 0.0012947147115482949, 0.0012727192897727946, 0.0016639402419968974, 0.0016998982046061428, 0.001732110195007408, 0.0016835955557326088, 0.00131517585941765, 0.001281630609810236, 0.0017107224539358867, 0.0017495759602752514, 0.0017501926322438521, 0.0013606411794171436, 0.001296962984270067, 0.0013464760231727269, 0.0013707986327062827, 0.001356733804641408, 0.0017989408515859395, 0.001392053609379218, 0.001357521678073681, 0.001376544905724586, 0.0017159350154543063, 0.0013685848280147184, 0.0013448625868477393, 0.001345367265457753, 0.0019739253348234342, 0.0017494567182438914, 0.0013624696002807468, 0.0013703909535252023, 0.001380178671752219, 0.001282122759221238, 0.001304565032114624, 0.001282093991903821, 0.0013164904612494865, 0.0012542202348413412, 0.00126162792184914, 0.0012682586875598645, 0.0013130050774634583, 0.0016666451410856098, 0.0012783185793523444, 0.0012748700701195048, 0.0012833109140046872, 0.0013807424602418905, 0.0016834483431011904, 0.0012740649053739617, 0.00129821196787816, 0.0012951088210684247, 0.001316478437729529, 0.0014855532426736318, 0.0012838611874030903, 0.001292096647375729, 0.0012731026727124117, 0.0012885146097687539, 0.0016133996577991638, 0.0012837094691349193, 0.0013002254854654893, 0.0012825359062844655, 0.0012940774686285295, 0.0016165307424671482, 0.0013950684606243158, 0.0014272355001594406, 0.0013805114231217885, 0.0013913211641920498, 0.0013688445069419686, 0.0014023133044247515, 0.0014013548843649914, 0.001398509764840128, 0.0013821401807945222, 0.001364935735182371, 0.0013773638293059776, 0.0017873434917419218, 0.00148259302295628, 0.0012771660694852471, 0.0012741180307784816, 0.0013370107881200965, 0.0012550611481856322, 0.0017341882103210082, 0.001700371232800535, 0.0013198135475249728, 0.0013388903444138123, 0.001302648350247182, 0.0013090466727589956, 0.0013044886327406857, 0.001274296953852172, 0.0013014999367442215, 0.0013040513040323276, 0.0013164325082470896, 0.001298402374231955, 0.0014066170697333291, 0.0017006055077217752, 0.0012813833345717285, 0.0012705391018243972, 0.001289023513891152, 0.0016991868669720134, 0.0012742993276333436, 0.0012881212805950781, 0.001450892001230386, 0.0016658812182868132, 0.0016168514775927179, 0.0012785946018993855, 0.0013759246248810086, 0.001372878992697224, 0.0013532649045373546, 0.0013694180779566523, 0.002092823102429975, 0.0014193943970894907, 0.0013038929137110244, 0.0012942026169184828, 0.0013496662504621781, 0.001700824008366908, 0.001736746937240241, 0.001322082014667103, 0.0013590961243608035, 0.0013698064067284577, 0.0013504735707101645, 0.0013835807112627663, 0.0017331999606540194, 0.0017333649757347303, 0.0013578487814811524, 0.0014130996878520818, 0.0013340507648536004, 0.0013648581571032992, 0.0013246446251287125, 0.0015948926647979533, 0.001363057390335598, 0.0012551564614113886, 0.0012864591717516305, 0.0012983341093786294, 0.0013395547266554786, 0.0013940115786681417, 0.0013616492033179384, 0.001292676954108174, 0.001459140506995027, 0.0017646676878939616, 0.0017343142972094938, 0.0014041539452591678, 0.0013619005621876568, 0.0013413881079031853, 0.001316146273893537, 0.0013332053440535674, 0.0012959421874256805, 0.0013113917884766124, 0.0013108265538903652, 0.0012919877808599267, 0.0013189266010158462, 0.0013210518518462777, 0.0016739427737775259, 0.0012833766395488055, 0.0013089783842588076, 0.0013235416172392434, 0.0012462933354981942, 0.0012592157981998753, 0.0012640315308090067, 0.0012556527417473262, 0.001293807430556626, 0.0012488620304793585, 0.0017057739842130104, 0.0012784759292117087, 0.0012377087350614602, 0.0012469664216041565, 0.0014194586874509696, 0.0016508018597960472, 0.0012723657728201943, 0.0012718242196569918, 0.0012542300701170461, 0.00129290553923056, 0.0012842615469708107, 0.0013072349138383288, 0.0016312598363583675, 0.0012557855534396367, 0.0012557606714835856, 0.0012480788282118738, 0.0012629835000552703, 0.0012868340782006271, 0.0012489733289839933, 0.001267722038392094, 0.0012408795319061028, 0.0012692860309471143, 0.0012905616640637163, 0.0017015956873365212, 0.0012703520551440306, 0.001296483844271279, 0.0012914838607684942, 0.0016513477894477546, 0.001634680538700195, 0.0016510292189195752, 0.001247522561243386, 0.001254801734830835, 0.001250146679012687, 0.0012470307119656354, 0.0014916046566213481, 0.0015832335157028865, 0.001248117891009315, 0.0012423989992385032, 0.0012425128061295254, 0.0012301964052312542, 0.0012741823738906533, 0.0021183604767429642, 0.0012544945639092475, 0.0013132914828020148, 0.001256120391190052, 0.0012705777971859789, 0.0012828489616367733, 0.0013574957429227652, 0.0012814102119591553, 0.0012559947645058855, 0.0012586808443302289, 0.0012745707426802255, 0.0013465381180139957, 0.001307031454416574, 0.0012842153118981514, 0.0012702322510449449, 0.0012932656245538965, 0.0012786453044100199, 0.0012850447492382955, 0.0013197649150242796, 0.001722191265798756, 0.001275106413231697, 0.0013523065790650435, 0.0013898455381422536, 0.0012922291789436713, 0.0013290276965562953, 0.0013483552811521804, 0.0012893727343907813, 0.0013083852973068133, 0.001295637797738891, 0.0017004326800815761, 0.001355279116978636, 0.0014002389380038949, 0.0014198783756000921, 0.001787915429304121, 0.001798564868295216, 0.0013947946008556755, 0.0014324160929390928, 0.0013970911095384508, 0.001367782015222474, 0.0015803054138814332, 0.0013105695634294534, 0.0013455880398396403, 0.0013168716886866605, 0.0013110458203300368, 0.0013446733428281732, 0.0014403120549104642, 0.0014793098671361804, 0.001321323241427308, 0.0013294035925355274, 0.0013121565789333545, 0.0013418463513517054, 0.0013445999848045176, 0.0013387432663876098, 0.0013331422032933915, 0.0013954331097920658, 0.0014280391314969165, 0.0013925160783401225, 0.001390131812513573, 0.0013886666729376884, 0.0013947490933787776, 0.0014271845157054486, 0.0016217451629927382, 0.0014120068826741772, 0.0012674371027969755, 0.0014805828359385487, 0.0016837788989505498, 0.001322767968304106, 0.0012861841332778567, 0.001301504811635823, 0.0012761291891365545, 0.001297218826948665, 0.0012976916714251274, 0.0017582806467544287, 0.0012952970864716917, 0.0013233393583504949, 0.0013704535540455254, 0.0012790987730113557, 0.0015122194927243982, 0.001297739616347826, 0.0013103065939503722, 0.0013441013434203342, 0.001299540499530849, 0.0012890722500742413, 0.002019681764068082, 0.0015886868841334945, 0.001268861038624891]
[710.0106953230722, 716.9179740449637, 721.4177395542612, 707.6804183549875, 729.2171124983863, 540.0094942745751, 722.9732552624752, 728.6239443709732, 717.8910654158663, 699.5831488881163, 521.5587481905441, 719.4086004847767, 714.7532520756248, 725.4734182482023, 721.4533048220112, 539.7819555633084, 720.4590108742527, 664.7996681537836, 736.5673623052239, 742.3008046557512, 638.0080603962671, 710.3196292594454, 739.5411600846339, 717.2617667808238, 732.804566284049, 737.9255475100837, 739.5108425624159, 719.7953163329729, 721.7734024798356, 729.5351977286, 738.9412370776724, 693.7483657356291, 740.1161994874478, 739.5106816731648, 723.8594284818499, 731.2338769908215, 610.7631238666987, 719.10136867037, 736.1315638729842, 705.8099490924416, 717.9984541458706, 671.7894654820594, 667.7374206880646, 623.8385210345217, 668.3107693745811, 703.402902904203, 729.0234894510844, 697.3709170914576, 722.5358042544152, 642.2707920733183, 709.5342587885565, 475.7444563426234, 491.588983081334, 721.6995424961505, 723.9518505287984, 710.8832287469322, 691.8671943081582, 698.8618218887901, 454.1998864684696, 648.6098480027547, 500.6639154567408, 632.4512623950257, 710.5155324465215, 723.3733017254267, 668.8324500624901, 681.6537421120437, 467.7018448830124, 716.9963096722379, 714.8106572309161, 735.8113838114459, 472.91383264514576, 648.3441985771907, 712.8799986118385, 701.1708952561033, 716.2751113350516, 714.563400873849, 702.0607649518622, 702.9673564496688, 700.9071851285175, 734.859300153246, 685.0800058919889, 709.7716215045917, 534.2531806468278, 668.8378911296876, 670.4689077873228, 469.6059133622191, 568.8419366411366, 664.4313349894458, 671.4925538325386, 725.7125080140821, 710.1240070021448, 503.87694422925233, 719.0621461623526, 694.5462932835519, 683.1114756104254, 701.5922398276426, 701.7168660066206, 722.2551519464115, 657.3422613681355, 467.9774205845823, 633.9622292530123, 504.26034928128837, 499.77138743898763, 703.202867299969, 649.7529169039044, 718.1286890499358, 729.4421235479153, 644.639772102145, 523.8936633921986, 718.0059509358618, 703.2407884027265, 707.9136838951025, 491.69411049030253, 496.41953374287743, 683.5794397015494, 656.3878094024793, 667.0839024681967, 420.55712160907007, 692.1100708469003, 683.6803619876648, 487.8272229405108, 706.6556061103303, 709.0617352620635, 710.793996422096, 703.5241666629272, 678.0811406842693, 510.14789746682385, 656.4725770469634, 664.1973391876123, 650.9953254876865, 711.2933745382139, 654.2245365866867, 660.5841919648923, 678.3620816227809, 681.5667874140191, 670.0321188117249, 691.3723076742465, 639.7010176445964, 675.0500061231414, 712.9663470964825, 730.8912520265845, 722.3713698583748, 717.8266952125274, 725.7890932283926, 715.3976504685377, 686.4048420258611, 695.9114289445056, 723.6536631757186, 722.5137947536695, 720.1518174647455, 727.3187426174889, 708.6230276139574, 545.889384963697, 679.3651457770858, 681.4895976334631, 715.2631140537925, 698.4625250819928, 620.5810633758745, 492.6034521750623, 681.7773044374488, 703.3892864754744, 703.3683815326443, 655.015993845996, 650.314434401676, 666.5895970235937, 654.9396759551005, 692.8094013011564, 491.94589778698975, 715.8954043673251, 722.851172837845, 717.9270912784667, 508.2612408603362, 492.0892081222101, 504.0106530299585, 499.9688199507551, 709.7704312863311, 691.4546399400712, 706.106065652835, 690.559437600911, 694.8155517296409, 704.5052006906213, 701.2406457616711, 674.3951160984375, 671.3742838981788, 702.9027417689787, 706.93807156837, 717.719657226591, 721.5550741120342, 726.2961504827186, 737.2768729057052, 727.0473924685759, 718.2596593174203, 624.2293972572357, 607.6041033455775, 719.9900090971886, 466.2072529890223, 473.1820270386069, 469.42426312754435, 662.5380202882931, 715.0561735348248, 722.8227193695204, 480.9816626166789, 710.3718214136811, 719.3710667848368, 704.1477338777966, 711.1087198964875, 499.7047247402479, 499.9171433360809, 694.1732294905271, 656.1399635887302, 669.6678127125838, 703.9961593896595, 708.3637929769544, 693.8939869747948, 712.6032718420878, 699.0934382619645, 709.2071235466342, 680.5342844152476, 710.1616023339606, 699.0170491550808, 491.47393513152144, 693.6025399029461, 706.9046896149198, 712.0511111353323, 491.9212009031382, 499.64191735018113, 438.04430856722615, 651.0892476651494, 646.2701669385632, 459.5478891413898, 654.046496786349, 657.1632371786717, 649.5491633203081, 654.3585643880639, 637.8285268120326, 639.4871383216671, 651.2515884001494, 688.0759158843834, 698.0270370580413, 709.5422513738642, 626.5790485726142, 699.8596533379791, 712.1574590068484, 711.3666606203046, 697.1125699515084, 720.4918498188697, 704.7255157945937, 705.3045393762605, 535.6813241719686, 718.0854161119485, 730.7294741759757, 714.4190759395757, 651.8366646377267, 469.73035388883886, 466.60820914737513, 652.0882941158418, 647.3289726364726, 657.0087164057245, 652.4699523278368, 486.4449072985429, 649.2875574543531, 666.2981644160252, 629.2065629596407, 715.5058255445243, 735.5324110881066, 712.2692154211965, 519.3684912731438, 683.123229210351, 680.1520391150964, 680.3114541184659, 627.3503872567167, 694.5283893183934, 685.1102299430844, 671.016647131093, 672.6222627532661, 669.7123834854852, 686.2130431064517, 681.7427180068942, 682.3068640062407, 660.1529426359631, 666.0362003898758, 651.3171444242719, 519.1820544142395, 703.7084198758203, 702.7804209336877, 709.131346470541, 719.1748648794933, 717.4661387299143, 716.8477504819483, 723.0037981154402, 718.0273200191052, 728.695329785919, 731.9087065408022, 705.4748227143824, 721.1423364566417, 718.0679010504113, 714.1029384178962, 531.3842853241459, 496.3978043771466, 496.6635049617206, 676.6791713585048, 715.4609722801187, 747.0661708306287, 736.6616819947968, 683.1065460410746, 712.6758444727993, 744.3137107913608, 715.8239114161813, 701.0984296695377, 515.7050538466476, 697.4618565899369, 715.9453476533572, 765.1647053879254, 773.2523872541027, 757.9892338182025, 748.5249616463402, 759.4782754228914, 760.5665022755842, 749.4380956091858, 749.7154389531767, 756.0250580996694, 774.5457561285209, 687.9835724183495, 746.7685973800486, 762.6066956341275, 743.9649004931262, 707.8933439677618, 698.9705339619192, 699.9360180475377, 763.9738629500912, 748.4446407957437, 757.2710484885976, 747.370342859091, 753.4175323059965, 696.4904717176801, 507.8470416803939, 715.0396096978732, 716.3246778933718, 708.399191394245, 527.8192129651322, 712.4585657758007, 700.7402081261883, 703.9557668845449, 651.0568340616248, 493.0701854708854, 748.7520615217991, 734.5740228668825, 742.5531415658943, 726.6778076152668, 716.5830260066697, 682.8836460384466, 488.12432131245566, 730.6034511131819, 721.0174467275602, 725.3283685831819, 711.9610117669648, 781.8001764095992, 761.798424543681, 763.9073064530681, 703.3107986854209, 670.2808622551506, 719.9955293467243, 724.7131486381838, 595.8765651324437, 711.0491765408913, 687.4126488205653, 685.6858629960738, 711.6591872669586, 777.5751799464035, 771.7514194425455, 750.9389572483254, 757.6770352344679, 763.2434900935812, 521.6986980794533, 612.2465482531981, 771.0705798463599, 771.7162935221103, 521.3464896254209, 736.7767845370802, 735.5685545447882, 739.5326254418584, 643.3572673557678, 512.8575674820077, 512.8650203300315, 736.6263604524399, 741.143626309107, 504.1149870570286, 508.9513074923201, 617.3984819687615, 728.6520457017073, 738.2568615148898, 730.8612799413017, 733.1358465581984, 737.9145557760771, 707.2512703822671, 488.7100259128412, 734.9689944988461, 731.6109628319588, 744.3701331920338, 677.1260843129911, 455.5374056849898, 723.0670921322032, 716.991487596079, 512.896678002755, 496.25241345019003, 493.67621985905436, 723.7577352277773, 720.379466300399, 715.823951183958, 718.069080172522, 724.6495225316467, 723.1183081970103, 491.8666496839673, 697.309140221056, 697.8520857048263, 700.5366882128167, 509.4002970776419, 694.3983155290487, 714.3668464862778, 711.9768321767574, 714.4079543989449, 534.1364449812887, 700.8027274219625, 729.6256610340896, 721.1283558987664, 725.2037932797361, 735.6310388689035, 731.1572692958657, 706.4746016286764, 705.9671505875974, 713.7653102543023, 709.2191758826425, 710.4045333051537, 709.1942882047448, 708.0305625405364, 686.1505071374531, 709.1199514914257, 733.4764392773544, 634.3578200743585, 496.2249761087782, 707.7048392074647, 687.2345967069772, 708.9867876285185, 681.354583764718, 644.4487896752352, 726.3245105368884, 637.8650583146109, 678.6732949625308, 670.0992371776653, 649.6957280423132, 644.5411643838597, 661.4553322494284, 694.8830072500496, 718.3885526744584, 718.0381590457555, 632.5750064426541, 713.9827979885706, 674.5528167755523, 724.905462620675, 656.127415408793, 553.2238244241461, 710.5704891279739, 687.7629000347933, 545.695775053064, 488.3578165572816, 492.5416197495247, 504.7429740655983, 710.1713140160232, 710.682584825866, 665.4244960733164, 709.7821381520005, 718.2145066886158, 474.1141449694702, 706.2719249722215, 713.9952533384128, 699.4776521920585, 679.9281581889157, 646.8390830893745, 651.6917931371386, 583.1909737637692, 673.2414485886105, 700.2612151816377, 697.7302746958267, 439.0101851296962, 647.1914228166987, 646.882035302446, 685.915428105046, 678.7687563803086, 702.1486659016736, 485.9888985696197, 654.548358375657, 665.2806177234397, 662.1939998239873, 715.3012878395074, 708.4864381820796, 712.5836458377871, 709.9399925864456, 496.50260285510524, 488.560344736229, 545.3976461157067, 710.8761262214448, 706.3102516457305, 711.2730627368993, 703.3862994687202, 504.6882095328479, 651.7062527056343, 660.3461636427593, 671.5326722716285, 462.867112163945, 690.5342165254133, 705.4476003823399, 694.7675361687155, 710.3024745895657, 704.0203641971182, 499.5663649099526, 668.008549777439, 754.3249860130622, 722.4614095034269, 756.6184221465032, 732.8885695114927, 588.5208784675766, 571.63272057678, 588.1491367479515, 733.3284618090692, 740.4257217525591, 739.8846412168682, 758.2503780626057, 570.1384274389479, 752.6438516102007, 789.5072666930818, 791.9934903217488, 738.5113858531885, 732.6400486019539, 702.7374630591216, 726.8730133974019, 728.7081416002309, 775.0612613897766, 781.629818419624, 789.0433927065499, 773.2690547711925, 739.7392109965137, 779.6800946737621, 761.1340900784633, 751.8485409712449, 733.1007539897855, 755.4549390137674, 588.8446079441816, 711.9908112182358, 716.6314345892291, 715.1167177137315, 749.8483827214696, 714.879858890463, 577.8722549057485, 709.7682024437414, 725.8236778005293, 717.6461560072985, 724.0474331700092, 597.0077355768317, 573.6377610123728, 781.3204975034186, 711.7709430643088, 741.0898035388944, 701.9825384702154, 711.1239551138391, 768.0294127999516, 735.1314623503224, 774.2374987499312, 627.24258260374, 705.9374915918405, 767.0997769990927, 769.1480303167399, 592.7871885110275, 571.6549441274266, 781.9084702148662, 779.2762207597196, 787.9967508016078, 790.605174037438, 738.8354812228457, 600.0679612353931, 782.2601917069723, 776.243103773172, 769.8425478350455, 610.5757849883827, 599.7351439299487, 591.0289214232204, 764.865835531173, 770.8067208503593, 768.4325089559145, 769.0898557186321, 771.4753542690328, 773.5751791358139, 779.1804122530726, 788.9023625485268, 771.0194421179701, 783.2105319049228, 602.3812203664993, 567.7272360079206, 772.1677491812629, 792.362643310024, 692.5722672375648, 775.8161282921649, 787.9924576667981, 599.4644647613248, 582.5896970785718, 770.0215404829337, 778.612341137534, 776.3599469707209, 681.6559835942844, 777.2017720708557, 774.2422005045744, 757.7589852339029, 754.9775678699425, 593.8482161097345, 781.0761608632862, 774.2546726433753, 776.7176922168368, 754.5258719745405, 762.6836295071802, 626.5815433475594, 768.7067366363289, 780.7055264229834, 718.5391094257094, 612.7809621023722, 594.2569628614112, 594.0101895954368, 756.0786490507838, 753.3390994963158, 772.6224245132229, 721.1084102630474, 757.3133783852909, 546.116349269519, 712.7406308473405, 762.7628150057617, 767.721702249436, 766.4492723339945, 598.4092186965698, 758.3170107875858, 767.6689185861838, 748.7212383538273, 716.23066822575, 637.710431516552, 593.3880149920857, 741.6400954744242, 716.4173794312177, 582.6412872141349, 588.2863181178379, 433.3858505932984, 714.7379937764615, 743.4028590970245, 790.2773461313468, 766.18178451262, 721.600756367679, 721.528702553826, 724.2725030223187, 782.1303639282884, 787.4894311555944, 794.4957837432603, 782.6723633754272, 668.337943122092, 599.8907439877629, 765.7469748171138, 763.9536809588714, 779.7618558125126, 775.3244693131837, 774.5128722523089, 749.8498810794313, 767.3843611638468, 763.0340343612932, 758.1653867722199, 752.2948268284133, 595.5673080211089, 758.9529050150037, 741.1767130167369, 749.78789088082, 753.4568686295844, 747.9519240390046, 642.3175878883208, 594.3750714342625, 752.4303117337208, 773.1578068657709, 734.6023512155106, 723.886566838083, 714.4931443499479, 621.1054054064008, 736.0741503951793, 784.1857518540348, 784.126428957601, 772.9463330667166, 777.1545409031392, 771.9817009174135, 769.0421917110604, 779.0125542904368, 784.5020926512931, 768.3424053213707, 773.3048402989281, 807.9647499531771, 672.8478644782437, 733.7822139722052, 753.0812015659868, 754.1863958982264, 785.0240717221366, 767.6301445799576, 608.0028841781342, 772.4081754756362, 772.356529327749, 590.1279778070173, 599.6937976630853, 772.3709254868527, 785.719213997707, 600.9831211245294, 588.2705195466069, 577.330474055505, 593.9668803442847, 760.3545889618082, 780.2560209981756, 584.5483571571086, 571.567066938137, 571.3656780270752, 734.9476225821484, 771.0320279979322, 742.6793962834043, 729.5017489372324, 737.0642616694477, 555.8826456792083, 718.3631386480489, 736.6364870275935, 726.4565041367973, 582.7726522238039, 730.6817813044216, 743.5703913393321, 743.2914607594203, 506.6047749417275, 571.6060246427829, 733.9613300685334, 729.7187692516458, 724.5438728091924, 779.956515714143, 766.5390190468759, 779.9740161913318, 759.5953251730331, 797.3081379336062, 792.6267187669105, 788.4826729821222, 761.6116777947727, 600.0077493092655, 782.2776075950069, 784.3936597446838, 779.2343921391661, 724.2480250986207, 594.018821009877, 784.8893692794101, 770.2902336006289, 772.1358882993561, 759.6022626277532, 673.1498887244493, 778.9004059097183, 773.9359141833683, 785.4826020194017, 776.0874361986995, 619.8092302586072, 778.9924621136364, 769.0973690167235, 779.7052660280068, 772.7512643117151, 618.6087116869801, 716.8107001376003, 700.6552176485851, 724.369232482461, 718.7413127440689, 730.5431661000152, 713.1074039194215, 713.5951150968722, 715.0468485390347, 723.515612884614, 732.6352253986438, 726.0245831371058, 559.4895467045413, 674.4939336123457, 782.9835319717216, 784.8566426683433, 747.9371212898361, 796.7739272669231, 576.6386797283636, 588.1068679061256, 757.682781689342, 746.8871548534589, 767.666884013822, 763.9147028213766, 766.5839125780916, 784.7464415394086, 768.3442555529881, 766.8409953717663, 759.6287646615178, 770.1772731211546, 710.9255400900141, 588.0258504746671, 780.4065910801204, 787.0674728263588, 775.7810383003161, 588.5167896701208, 784.7449797036476, 776.3244153050762, 689.231175822859, 600.2828947362746, 618.4859981628432, 782.1087297838377, 726.7839981325147, 728.3963155670057, 738.953619980172, 730.2371832947674, 477.82347148160824, 704.5258189341376, 766.9341473402817, 772.6765399231042, 740.9239133434367, 587.9503082509854, 575.7891253800275, 756.3827273240665, 735.7831297402234, 730.0301671010027, 740.4809851066813, 722.7623165455379, 576.9674721332512, 576.9125452509647, 736.4590325803377, 707.6641574523343, 749.5966617955057, 732.6768681386978, 754.9194561543874, 627.0014415839598, 733.6448245614883, 796.7134223852282, 777.3274286182045, 770.2177681202496, 746.51671940029, 717.354156380404, 734.403543558278, 773.5884799539158, 685.3349593175321, 566.6789315972843, 576.5967573518807, 712.1726242170888, 734.2679985341027, 745.4963959410434, 759.7939680683926, 750.072000881374, 771.6393599211754, 762.5486210811622, 762.877435639466, 774.001128195203, 758.192305189533, 756.972558346153, 597.3919871486027, 779.1944852226453, 763.9545557249498, 755.5485879513828, 802.379320756184, 794.1450555413617, 791.1195058243358, 796.3985318173494, 772.9125497213884, 800.7289641243747, 586.2441385875445, 782.1813279007816, 807.9445282014137, 801.9462133659965, 704.4939094323178, 605.766218438564, 785.9375199817765, 786.2721786110488, 797.3018856952448, 773.4517098558683, 778.6575891481772, 764.9734484705433, 613.0231234236754, 796.3143048277376, 796.3300831985574, 801.231442594618, 791.7759812034269, 777.1009619191111, 800.6576095691919, 788.8164516476678, 805.8800022786337, 787.8444854969539, 774.8564271242982, 587.6836709461123, 787.1833606681744, 771.3169773913187, 774.3031333004444, 605.5659543011354, 611.7403225434757, 605.6827998806676, 801.5887095487202, 796.9386495427616, 799.9061364461311, 801.904869226314, 670.4189314245721, 631.6187663296425, 801.2063661641213, 804.8944023722851, 804.8206787622881, 812.8783304418927, 784.8170093159813, 472.06318800732475, 797.1337850072516, 761.4455839357294, 796.102035293446, 787.0435027392712, 779.5149935064145, 736.6505605733565, 780.390222168664, 796.1816627423641, 794.4825763453335, 784.5778712110983, 742.6451480444508, 765.0925282791874, 778.685622835268, 787.2576051957107, 773.236356873665, 782.0777165888161, 778.1830170449282, 757.7107018196518, 580.6555983990535, 784.248271064332, 739.4772867934867, 719.5044143802172, 773.856538990589, 752.429766957563, 741.644293591146, 775.5709216795966, 764.3008539291942, 771.8206444310057, 588.0856159221936, 737.8553889543651, 714.1638279432124, 704.2856748750496, 559.3105711880413, 555.9988508770651, 716.9514417295006, 698.1211708869852, 715.7729321821856, 731.1106513104334, 632.7890743244823, 763.027028785282, 743.169506856775, 759.3754263160731, 762.7498478644053, 743.6750385017361, 694.2939876054597, 675.9908942782325, 756.8170820334537, 752.2170134148149, 762.1041696204389, 745.2418072998095, 743.7156115581715, 746.9692099354825, 750.1075260610626, 716.6233859457517, 700.2609227884168, 718.124562835927, 719.356244492992, 720.1152151830114, 716.9748342173153, 700.6802477153461, 616.6196902074422, 708.211845331884, 788.9937873786429, 675.4096938899708, 593.9022045134731, 755.9904865870616, 777.4936528345185, 768.3413776573976, 783.619721665181, 770.879961981598, 770.5990737397568, 568.7374207558263, 772.0236619414759, 755.6640658269786, 729.6854366556524, 781.8004528655131, 661.2796652940975, 770.5706039970158, 763.1801630373807, 743.9915188651627, 769.5027591375666, 775.7517082090683, 495.1275085960972, 629.45065512102, 788.1083661326184]
Elapsed: 0.18805744340059444~0.027337330772124458
Time per graph: 0.0014630000626215552~0.00021071200620758268
Speed: 695.252784809071~82.4064903202803
Total Time: 0.1648
best val loss: 0.09784344875058809 test_score: 0.9375

Testing...
Test loss: 0.1833 score: 0.9453 time: 0.25s
test Score 0.9453
Epoch Time List: [0.6481827420648187, 0.6002048871014267, 0.6042039359454066, 0.6065843638498336, 0.6033762781880796, 0.682153478031978, 0.6360694118775427, 0.6098216178361326, 0.6095646829344332, 0.6362422241363674, 0.882680713897571, 0.6083449230063707, 0.6114361039362848, 0.6095791701227427, 0.6107221541460603, 0.7330565981101245, 0.6252110020723194, 0.6340969728771597, 0.6013301422353834, 0.5911709719803184, 0.6187451761215925, 0.6612195211928338, 0.5870266172569245, 0.5894757478963584, 0.5854544590692967, 0.592657784698531, 0.593862900044769, 0.5936651821248233, 0.6759632320608944, 0.5971422391012311, 0.5823281379416585, 0.6151810972951353, 0.5928676691837609, 0.5886825113557279, 0.5997715478297323, 0.5953280460089445, 0.6681538720149547, 0.6616174920927733, 0.603164094965905, 0.6726443043444306, 0.6127253209706396, 0.6415551770478487, 0.6319464019034058, 0.6524375618901104, 0.715393346035853, 0.6269702578429133, 0.6320493817329407, 0.6270961260888726, 0.605056498432532, 0.7115374251734465, 0.6291808679234236, 0.7052385313436389, 0.7731248980853707, 0.7753542440477759, 0.7072891679126769, 0.6216274132020772, 0.7121247849427164, 0.6451224579941481, 0.8097315339837223, 0.6535383691079915, 0.8030074322596192, 0.8416529719252139, 0.6263104719109833, 0.6149992551654577, 0.6454315390437841, 0.634698050096631, 0.735549520002678, 0.7709165420383215, 0.6176349830348045, 0.6079736051615328, 0.8365920463111252, 0.802990835160017, 0.6072277883067727, 0.6164808529429138, 0.6097058970481157, 0.6121277550701052, 0.6244647779967636, 0.710300566162914, 0.6077039672527462, 0.6124712852761149, 0.6444336019922048, 0.650966206099838, 0.703047547955066, 0.645236881216988, 0.6476944389287382, 0.8658906309865415, 0.8535209749825299, 0.6405308519024402, 0.6447163298726082, 0.6243088277988136, 0.6085733957588673, 0.7818487307522446, 0.7323460718616843, 0.6390650228131562, 0.632952491287142, 0.6441728789359331, 0.6374857760965824, 0.6151000780519098, 0.6424551999662071, 0.7258461068850011, 0.6628649060148746, 0.8864745299797505, 0.899378992151469, 0.8519409550353885, 0.651549369096756, 0.6209665501955897, 0.612460358068347, 0.6361752280499786, 0.8010674465913326, 0.645163940731436, 0.621639811899513, 0.6256783329881728, 0.8002057757694274, 0.8946000749710947, 0.8098679869435728, 0.6524290291126817, 0.6486665159463882, 0.763941882411018, 0.6388484227936715, 0.6397047061473131, 0.7047948180697858, 0.6224633799865842, 0.6223167809657753, 0.619671787833795, 0.6242042179219425, 0.6417786350939423, 0.7367738622706383, 0.6554315851535648, 0.6489828543271869, 0.6541716582141817, 0.6137701829429716, 0.6479291240684688, 0.6676674820482731, 0.7298740979749709, 0.6503761929925531, 0.6425877041183412, 0.6195933721028268, 0.757266347296536, 0.7492962519172579, 0.627301107859239, 0.6585042721126229, 0.6060339119285345, 0.625022673048079, 0.6561586409807205, 0.693319687852636, 0.6234106591437012, 0.7124565420672297, 0.6707092167343944, 0.6062053302302957, 0.6062426341231912, 0.607752465410158, 0.6114050692413002, 0.6570089780725539, 0.6819819000083953, 0.618792254012078, 0.6243371551390737, 0.6291980659589171, 0.6523562781512737, 0.8523166121449322, 0.6721749487333, 0.6266973060555756, 0.6299425542820245, 0.6691390499472618, 0.7273296057246625, 0.6548047319520265, 0.6477365479804575, 0.6192102830391377, 0.8812252411153167, 0.7788043050095439, 0.6155605169478804, 0.6066699449438602, 0.8346381809096783, 0.8990426308009773, 0.8932295457925647, 0.8905169768258929, 0.7514091951306909, 0.6275368588976562, 0.6277242919895798, 0.6237345340196043, 0.6339434448163956, 0.6963936623651534, 0.6331913687754422, 0.6394660018850118, 0.6516352880280465, 0.6123517551459372, 0.6387786399573088, 0.7487797189969569, 0.6204007749911398, 0.6282875079195946, 0.6084526181221008, 0.6092228784691542, 0.6134092879947275, 0.7000927268527448, 0.6845485088415444, 0.6051796453539282, 0.8013759863097221, 0.9218486598692834, 0.9284981868695468, 0.794921533903107, 0.6153753807302564, 0.6143072419799864, 0.7664629297796637, 0.619035640032962, 0.6094994868617505, 0.6058941781520844, 0.6367112537845969, 0.8189896128606051, 0.9010511240921915, 0.7390868118964136, 0.6561266260687262, 0.6565959637518972, 0.626423382665962, 0.6174687102902681, 0.6558707340154797, 0.68850561324507, 0.6304966870229691, 0.6202601599507034, 0.6256621100474149, 0.6246884488500655, 0.6374530971515924, 0.8780030747875571, 0.7584795160219073, 0.6239053832832724, 0.614105385961011, 0.7145888768136501, 0.7016239592339844, 0.8841478517279029, 0.648579748114571, 0.658712127013132, 0.8758181370794773, 0.6624408019706607, 0.6791732322890311, 0.6536557867657393, 0.65975819574669, 0.6888496621977538, 0.7477934567723423, 0.6553528432268649, 0.6227110989857465, 0.6194629240781069, 0.6217008768580854, 0.6410483170766383, 0.7125802219379693, 0.6228088322095573, 0.6209909599274397, 0.623476912965998, 0.6211376700084656, 0.6143829617649317, 0.6152136470191181, 0.7248764638788998, 0.6420566907618195, 0.6097931996919215, 0.609866495244205, 0.6520504429936409, 0.9114905952010304, 0.9279870251193643, 0.6601345180533826, 0.6522978632710874, 0.6560092130675912, 0.672418205998838, 0.915079077007249, 0.6652315030805767, 0.6522672739811242, 0.6631804744247347, 0.6300811623223126, 0.6266099570784718, 0.6446814010851085, 0.7821668018586934, 0.7004260527901351, 0.6622496268246323, 0.6594036731403321, 0.6702683130279183, 0.7254338529892266, 0.6560155132319778, 0.6608421553391963, 0.6591705912724137, 0.6651226258836687, 0.7277887342497706, 0.6575194846373051, 0.6664009690284729, 0.6780176390893757, 0.6799167850986123, 0.681081481045112, 0.7082471228204668, 0.6360295570921153, 0.6404798289295286, 0.6403271981980652, 0.6322307400405407, 0.6406224579550326, 0.7113047321327031, 0.6348040131852031, 0.6281494561117142, 0.6309632041957229, 0.6210158639587462, 0.6379204478580505, 0.6906813122332096, 0.6228810758329928, 0.6432717670686543, 0.6914225108921528, 0.9155245670117438, 0.9213856072165072, 0.71743757231161, 0.642008483177051, 0.6158551229164004, 0.6254855296574533, 0.7490500658750534, 0.6985215242020786, 0.7113213778939098, 0.6235955432057381, 0.6658220789395273, 0.9486041758209467, 0.6533268082421273, 0.6804277440533042, 0.6037466658744961, 0.6039657392539084, 0.5927364670205861, 0.6069035141263157, 0.6831923890858889, 0.5944660841487348, 0.6124733968172222, 0.5994346991647035, 0.5953718209639192, 0.5960387650411576, 0.6116366449277848, 0.6843845029361546, 0.6029405246954411, 0.6007694688159972, 0.6262029260396957, 0.6375767278950661, 0.6443749349564314, 0.6714677331037819, 0.6074778151232749, 0.6070877930615097, 0.60988813592121, 0.6149081960320473, 0.6319334728177637, 0.9015421869698912, 0.7786546628922224, 0.6358109801076353, 0.6451775070745498, 0.7249937220476568, 0.6560680279508233, 0.6409334519412369, 0.6420060838572681, 0.6561789605766535, 0.808969029225409, 0.6652164550032467, 0.608201591996476, 0.6061249470803887, 0.6220684617292136, 0.6166898689698428, 0.6314873248338699, 0.7896061148494482, 0.7595184750389308, 0.6224434641189873, 0.6178623328451067, 0.6399882228579372, 0.5916546091903001, 0.5948464712128043, 0.6673916580621153, 0.6385632052551955, 0.6460254259873182, 0.6332538791466504, 0.6184064180124551, 0.7549727989826351, 0.6496446640230715, 0.6493108137510717, 0.6650717940647155, 0.6360070810187608, 0.6047101311851293, 0.5865778208244592, 0.6118156618904322, 0.6798435430973768, 0.593007052084431, 0.7300151528324932, 0.8352763908915222, 0.6657659457996488, 0.5883258490357548, 0.7671006410382688, 0.7108614922035486, 0.6232787971384823, 0.6124657713808119, 0.6441494401078671, 0.8326146441977471, 0.9081399678252637, 0.7804067810066044, 0.6195446187630296, 0.725023670354858, 0.8990082880482078, 0.8487813379615545, 0.6119508079718798, 0.610861643915996, 0.6057082067709416, 0.6188513289671391, 0.676976156886667, 0.6485416200011969, 0.8035540550481528, 0.6217726159375161, 0.6201651748269796, 0.6196700211148709, 0.6825826270505786, 0.8670445780735463, 0.6248268040362746, 0.6416013541165739, 0.8102344679646194, 0.9114591211546212, 0.9147259527817369, 0.6795459890272468, 0.632403994910419, 0.63498067506589, 0.6298889780882746, 0.625979125034064, 0.6268605708610266, 0.7569310651160777, 0.6429405589587986, 0.6419214981142431, 0.6374888499267399, 0.7246456518769264, 0.7717538690194488, 0.6522282240912318, 0.6561803549993783, 0.6462280219420791, 0.727073343237862, 0.6833857109304518, 0.6292352320160717, 0.6410356832202524, 0.6479443279094994, 0.6356390500441194, 0.6231363750994205, 0.655743231298402, 0.7278842316009104, 0.6355783801991493, 0.6441960860975087, 0.6447993379551917, 0.6371536410879344, 0.662103503011167, 0.7274305343162268, 0.656130651012063, 0.6312594630289823, 0.6710720949340612, 0.8301331039983779, 0.7386704229284078, 0.6449598311446607, 0.6487396692391485, 0.6532236160710454, 0.6838773840572685, 0.7049244460649788, 0.650913038989529, 0.6477536789607257, 0.6859746302943677, 0.6477929977700114, 0.745130875846371, 0.680921820923686, 0.6398131370078772, 0.6450145982671529, 0.6366068450734019, 0.6608086060732603, 0.696023179916665, 0.6655240564141423, 0.6516079013235867, 0.656639507971704, 0.7029062202200294, 0.8516024479176849, 0.6404284352902323, 0.7260415249038488, 0.9349619860295206, 0.9364824059884995, 0.9277273560874164, 0.8041563271544874, 0.6340577027294785, 0.6707456370349973, 0.6250444329343736, 0.6376004479825497, 0.7442717717494816, 0.6592188423965126, 0.6498323269188404, 0.6433666350785643, 0.6622902529779822, 0.6810363037511706, 0.692779169883579, 0.7687782540451735, 0.6518845448736101, 0.7055736179463565, 0.6435998952947557, 0.7672227048315108, 0.6781583919655532, 0.6917613411787897, 0.7435107608325779, 0.6628653861116618, 0.6498619408812374, 0.8418480609543622, 0.6783864106982946, 0.6749616637825966, 0.6829385806340724, 0.6737765569705516, 0.6744753601960838, 0.7115114920306951, 0.6420255741104484, 0.804022166877985, 0.9195517560001463, 0.8831113579217345, 0.6414601989090443, 0.6477979649789631, 0.6449379329569638, 0.6539131009485573, 0.7479383139871061, 0.759392635896802, 0.6904476552736014, 0.6841981180477887, 0.773674744181335, 0.6611391329206526, 0.6525052082724869, 0.6667572201695293, 0.6437691892497241, 0.6550855389796197, 0.7401760309003294, 0.6701259263791144, 0.6359668106306344, 0.6563514496665448, 0.7498871092684567, 0.6409683211240917, 0.9494680648203939, 0.979031991912052, 0.9813031500671059, 0.7529160745907575, 0.640730252256617, 0.6432091740425676, 0.6366965339984745, 0.7458946846891195, 0.669613495003432, 0.617114543216303, 0.616333229932934, 0.6209035569336265, 0.7141421567648649, 0.670501910848543, 0.6524167938623577, 0.6364954779855907, 0.6766221250873059, 0.6096308364067227, 0.6208197388332337, 0.6161351171322167, 0.6205184718128294, 0.6271947219502181, 0.6737695597112179, 0.7103870019782335, 0.6473920999560505, 0.6488260049372911, 0.9802514270413667, 0.7134222520980984, 0.6521028170827776, 0.6671148950699717, 0.6273795322049409, 0.6628127389121801, 0.7938879653811455, 0.7039356110617518, 0.6625501799862832, 0.6762542959768325, 0.7365729629527777, 0.7396309520117939, 0.6990283459890634, 0.6177410900127143, 0.6695099102798849, 0.7142682971898466, 0.6545017631724477, 0.8210531859658659, 0.6267183730378747, 0.6530221602879465, 0.6115315218921751, 0.6752968251239508, 0.7992586540058255, 0.6221797410398722, 0.6224120638798922, 0.7255252273753285, 0.9589386258739978, 0.8302754417527467, 0.6256462766323239, 0.6336751787457615, 0.6290492909029126, 0.6293578487820923, 0.7665007058531046, 0.8343221500981599, 0.6171287959441543, 0.6315458687022328, 0.6951634099241346, 0.9594090930186212, 0.9515072361100465, 0.6298055099323392, 0.6199433840811253, 0.6197240999899805, 0.62728659552522, 0.6238512319978327, 0.671128444140777, 0.706746316049248, 0.6137257369700819, 0.6203501601703465, 0.622674552956596, 0.7301358189433813, 0.964246419724077, 0.6302072776015848, 0.6214792046230286, 0.6392758360598236, 0.7266017347574234, 0.6232446550857276, 0.7484365419950336, 0.9761169119738042, 0.869880890706554, 0.6246407353319228, 0.6198841938748956, 0.6496891039423645, 0.766893537947908, 0.6184218169655651, 0.623768372926861, 0.6359287591185421, 0.7441284589003772, 0.684210849693045, 0.6203024110291153, 0.6204007030464709, 0.6250987129751593, 0.6250583732035011, 0.7446573507040739, 0.6373821310698986, 0.6153265421744436, 0.6546669888775796, 0.6849437381606549, 0.9501378717832267, 0.9675979153253138, 0.7368554559070617, 0.629529946949333, 0.618276837747544, 0.6427095178514719, 0.6239811726845801, 0.7488383296877146, 0.6716945290099829, 0.6427274593152106, 0.6361272856593132, 0.6451279688626528, 0.7138512539677322, 0.8530646748840809, 0.632678896188736, 0.6268079779110849, 0.6591861897613853, 0.6744359959848225, 0.9712428960483521, 0.7239184498321265, 0.6796190678142011, 0.7955108280293643, 0.9815827480051666, 1.0471320690121502, 0.9118175611365587, 0.6366685619577765, 0.6152497630100697, 0.6193412779830396, 0.6684622741304338, 0.6626297882758081, 0.6863320400007069, 0.7609165338799357, 0.6261132024228573, 0.6163158318959177, 0.6134651938918978, 0.6458937420975417, 0.7527331549208611, 0.8273989018052816, 0.6160139839630574, 0.6282819693442434, 0.621599012054503, 0.6286080048885196, 0.6516880588606, 0.7071890239603817, 0.6307970720808953, 0.6264310779515654, 0.6466800039634109, 0.7018871130421758, 0.639308077050373, 0.6379158389754593, 0.6347484292928129, 0.6374287758953869, 0.6424053800292313, 0.6470983438193798, 0.7569614828098565, 0.7683622429613024, 0.6211565118283033, 0.6451844568364322, 0.6516035560052842, 0.6565261499490589, 0.7540728331077844, 0.6831123563461006, 0.6071336821187288, 0.6095029972493649, 0.6187334202695638, 0.6092588431201875, 0.6180942920036614, 0.6843022708781064, 0.6982638949993998, 0.6124873450025916, 0.6132631490472704, 0.6170708916615695, 0.6480995093006641, 0.663908313959837, 0.7014949950389564, 0.6358202309347689, 0.6301251910626888, 0.6187923720572144, 0.6238914197310805, 0.8608452463522553, 0.6234016779344529, 0.6118023477029055, 0.6729379268363118, 0.9414855379145592, 0.6972960596904159, 0.622039572102949, 0.6674552799668163, 0.9590456644073129, 0.9654238140210509, 0.9528311321046203, 0.9169599488377571, 0.6319908890873194, 0.6874553009402007, 0.9840673366561532, 0.9777194201014936, 0.8650160918477923, 0.6175051257014275, 0.6526646320708096, 0.6559497339185327, 0.6509655900299549, 0.7426483619492501, 0.6789468990173191, 0.7195227730553597, 0.65095568401739, 1.0728302916977555, 0.8630058520939201, 0.7339382590726018, 0.6501897836569697, 0.7418590541929007, 0.953918072860688, 0.8870912611018866, 0.6424669919069856, 0.6613476350903511, 0.6224015101324767, 0.6183657220099121, 0.6396323929075152, 0.9002014740835875, 0.6158747877925634, 0.6089140588883311, 0.6084135277196765, 0.6157251838594675, 0.7590801450423896, 0.8584825342986733, 0.6081363307312131, 0.6140538223553449, 0.6242244897875935, 0.8955314112827182, 0.7985937667544931, 0.6131950470153242, 0.6173225450329483, 0.6201708531007171, 0.6452343647833914, 0.6823160990606993, 0.617299848003313, 0.6169851829763502, 0.6187801158521324, 0.7346034089569002, 0.6521116129588336, 0.6351236049085855, 0.6142859500832856, 0.6215506587177515, 0.7222191069740802, 0.6961834318935871, 0.6748592089861631, 0.6649232148192823, 0.6554473978467286, 0.6538724512793124, 0.6924368280451745, 0.7437627739273012, 0.6662988611496985, 0.6642212751321495, 0.655587597982958, 0.6525916040409356, 0.7594525450840592, 0.96087660593912, 0.6204395599197596, 0.6260748810600489, 0.6328690419904888, 0.6131785269826651, 0.7980776270851493, 0.9688831169623882, 0.6449668069835752, 0.6230090097524226, 0.6088376780971885, 0.6117612388916314, 0.6148649891838431, 0.606378389056772, 0.6390225838404149, 0.6936204710509628, 0.6202056552283466, 0.6169633681420237, 0.6362398730125278, 0.7766132832039148, 0.807301684981212, 0.6344405459240079, 0.6139260660856962, 0.7593587918672711, 0.8157852359581739, 0.6124468322377652, 0.6385867821518332, 0.8790495740249753, 0.8821527040563524, 0.6265000991988927, 0.6407270731870085, 0.652561544906348, 0.6509401011280715, 0.6265415921807289, 0.7705767408479005, 0.6381137690041214, 0.6367251551710069, 0.6471025187056512, 0.6574790177401155, 0.9656966712791473, 0.9751788501162082, 0.7171642931643873, 0.6327656311914325, 0.659749167971313, 0.6503040469251573, 0.6547982508782297, 0.7938442761078477, 0.9991448172368109, 0.7913098367862403, 0.6582561680115759, 0.6545534341130406, 0.6645258222706616, 0.6453892451245338, 0.7695887058507651, 0.6304268650710583, 0.6160404360853136, 0.6463404879905283, 0.6231681401841342, 0.6317009620834142, 0.6782012977637351, 0.8298096859361976, 0.6188158101867884, 0.6560388319194317, 0.9653054100926965, 0.9426576648838818, 0.6540185241028666, 0.6474181360099465, 0.642988603329286, 0.6359117082320154, 0.6374850489664823, 0.7036385119426996, 0.7139558903872967, 0.6399980203714222, 0.6370807667262852, 0.641736961202696, 0.6335469370242208, 0.8997139479033649, 0.7105411009397358, 0.6361714792437851, 0.6302927988581359, 0.6194352498278022, 0.6033775771502405, 0.6276054352056235, 0.6964243380352855, 0.6117231820244342, 0.6024551738519222, 0.884560695849359, 0.6386256378609687, 0.6012903251685202, 0.5967372888699174, 0.6319375827442855, 0.8789799148216844, 0.6942406198941171, 0.6098577750381082, 0.5981598456855863, 0.6149402696173638, 0.6093919770792127, 0.6097841190639883, 0.6902389430906624, 0.6961998636834323, 0.6083186839241534, 0.6138151008635759, 0.626279370393604, 0.6921243160031736, 0.6121678040362895, 0.6137550109997392, 0.6078313810285181, 0.6047978282440454, 0.6349962488748133, 0.9360978119075298, 0.6339853019453585, 0.6229159792419523, 0.6243383081164211, 0.8889549297746271, 0.9528727382421494, 0.9593052540440112, 0.763424479868263, 0.6214327423367649, 0.6215341149363667, 0.6124507717322558, 0.7413286629598588, 0.870032258098945, 0.7094922158867121, 0.6034273181576282, 0.6054662701208144, 0.6203402481041849, 0.598357121925801, 0.7908014231361449, 0.660624000011012, 0.6179692649748176, 0.6017223610542715, 0.6185976420529187, 0.6062145056203008, 0.6361057800240815, 0.6973243490792811, 0.6033741938881576, 0.6033477110322565, 0.6076118981000036, 0.6130930709186941, 0.7107168869115412, 0.6169999251142144, 0.6050711020361632, 0.6232952130958438, 0.6135479600634426, 0.6183775668032467, 0.6207446600310504, 0.6839016620069742, 0.6182716279290617, 0.6304303109645844, 0.6559111371170729, 0.6451926201116294, 0.6219965440686792, 0.6951989401131868, 0.7882149899378419, 0.6248379750177264, 0.6305987860541791, 0.7557950541377068, 0.7200400547590107, 0.6709419169928879, 0.6725749380420893, 0.7666021280456334, 1.0132103529758751, 0.7368219359777868, 0.6811954099684954, 0.6745193158276379, 0.6547667121049017, 0.6986762250307947, 0.7048271910753101, 0.6410279769916087, 0.6408149071503431, 0.6383137460798025, 0.651382806012407, 0.6736494170036167, 0.7270454389508814, 0.6507399748079479, 0.6552144698798656, 0.6490804159548134, 0.6373203578405082, 0.6765145550016314, 0.7305739528965205, 0.6439268360845745, 0.661018201848492, 0.8830359671264887, 0.6790957178454846, 0.6721800591330975, 0.6777096302248538, 0.6696067666634917, 0.6849903059192002, 0.7669455660507083, 0.6736335349269211, 0.62530183698982, 0.6486696349456906, 0.9009975150693208, 0.8044017523061484, 0.627108110813424, 0.625679005170241, 0.6275196569040418, 0.6278391238301992, 0.6766697212588042, 0.9603867258410901, 0.626183777814731, 0.6509166238829494, 0.6478186431340873, 0.6248491278383881, 0.6953232812229544, 0.6592019817326218, 0.6198967797681689, 0.6474144731182605, 0.6720421568024904, 0.6146613769233227, 0.7652226309292018, 0.697352098301053, 0.6229319081176072]
Total Epoch List: [263, 252, 474]
Total Time List: [0.2061999780125916, 0.2596967949066311, 0.16476355190388858]
T-times Epoch Time: 0.6779234042750906 ~ 0.008491966542310943
T-times Total Epoch: 267.3333333333333 ~ 47.05158713424928
T-times Total Time: 0.1958651726341082 ~ 0.01457968627168271
T-times Inference Elapsed: 0.18681439891444654 ~ 0.0019582924966662836
T-times Time Per Graph: 0.001452446634193698 ~ 1.5746035185188528e-05
T-times Speed: 700.954923473335 ~ 7.633602535349765
T-times cross validation test micro f1 score:0.9270099465596039 ~ 0.005610102977339166
T-times cross validation test precision:0.9651969196078438 ~ 0.013506499027298018
T-times cross validation test recall:0.892815170940171 ~ 0.00639833738773939
T-times cross validation test f1_score:0.9270099465596039 ~ 0.005102537858689002
