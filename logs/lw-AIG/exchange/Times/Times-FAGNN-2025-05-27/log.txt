Namespace(seed=15, model='FAGNN', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71389351ead0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 2.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 1.58s
Epoch 2/1000, LR 0.000015
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 1.30s
Epoch 3/1000, LR 0.000045
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 1.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 1.10s
Epoch 5/1000, LR 0.000105
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 1.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 1.09s
Epoch 7/1000, LR 0.000165
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.96s
Epoch 8/1000, LR 0.000195
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 1.23s
Epoch 9/1000, LR 0.000225
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 0.96s
Epoch 10/1000, LR 0.000255
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.12s
Val loss: 0.6922 score: 0.5116 time: 0.93s
Test loss: 0.6921 score: 0.5039 time: 1.05s
Epoch 11/1000, LR 0.000285
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.03s
Val loss: 0.6918 score: 0.5659 time: 0.95s
Test loss: 0.6917 score: 0.5426 time: 1.03s
Epoch 12/1000, LR 0.000285
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 1.00s
Val loss: 0.6914 score: 0.7364 time: 0.92s
Test loss: 0.6912 score: 0.7674 time: 1.04s
Epoch 13/1000, LR 0.000285
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 1.01s
Val loss: 0.6908 score: 0.7674 time: 1.07s
Test loss: 0.6904 score: 0.7519 time: 1.03s
Epoch 14/1000, LR 0.000285
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 1.09s
Val loss: 0.6899 score: 0.6822 time: 1.06s
Test loss: 0.6894 score: 0.7287 time: 0.92s
Epoch 15/1000, LR 0.000285
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 1.06s
Val loss: 0.6889 score: 0.6124 time: 1.02s
Test loss: 0.6882 score: 0.6202 time: 0.93s
Epoch 16/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 1.07s
Val loss: 0.6878 score: 0.5814 time: 1.04s
Test loss: 0.6869 score: 0.6047 time: 1.10s
Epoch 17/1000, LR 0.000285
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 1.10s
Val loss: 0.6864 score: 0.5581 time: 0.97s
Test loss: 0.6852 score: 0.6124 time: 1.01s
Epoch 18/1000, LR 0.000285
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 1.15s
Val loss: 0.6847 score: 0.5581 time: 1.09s
Test loss: 0.6832 score: 0.6124 time: 1.22s
Epoch 19/1000, LR 0.000285
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 1.34s
Val loss: 0.6827 score: 0.5659 time: 0.95s
Test loss: 0.6808 score: 0.6124 time: 1.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 1.22s
Val loss: 0.6803 score: 0.5891 time: 0.95s
Test loss: 0.6780 score: 0.6124 time: 1.00s
Epoch 21/1000, LR 0.000285
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 1.19s
Val loss: 0.6774 score: 0.5891 time: 1.25s
Test loss: 0.6746 score: 0.6124 time: 1.02s
Epoch 22/1000, LR 0.000285
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 1.13s
Val loss: 0.6740 score: 0.5814 time: 0.97s
Test loss: 0.6706 score: 0.6279 time: 0.95s
Epoch 23/1000, LR 0.000285
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 1.12s
Val loss: 0.6698 score: 0.6202 time: 0.98s
Test loss: 0.6659 score: 0.6589 time: 1.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 1.06s
Val loss: 0.6650 score: 0.6512 time: 0.96s
Test loss: 0.6602 score: 0.6589 time: 1.13s
Epoch 25/1000, LR 0.000285
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 1.06s
Val loss: 0.6590 score: 0.7054 time: 1.02s
Test loss: 0.6533 score: 0.7287 time: 0.97s
Epoch 26/1000, LR 0.000285
Train loss: 0.6504;  Loss pred: 0.6504; Loss self: 0.0000; time: 1.13s
Val loss: 0.6515 score: 0.7674 time: 1.12s
Test loss: 0.6448 score: 0.7752 time: 0.96s
Epoch 27/1000, LR 0.000285
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 1.02s
Val loss: 0.6430 score: 0.7674 time: 1.06s
Test loss: 0.6350 score: 0.8527 time: 0.94s
Epoch 28/1000, LR 0.000285
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 1.30s
Val loss: 0.6329 score: 0.8140 time: 1.25s
Test loss: 0.6235 score: 0.8605 time: 1.25s
Epoch 29/1000, LR 0.000285
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 1.19s
Val loss: 0.6211 score: 0.8140 time: 0.96s
Test loss: 0.6099 score: 0.9070 time: 1.08s
Epoch 30/1000, LR 0.000285
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 1.23s
Val loss: 0.6074 score: 0.8450 time: 0.98s
Test loss: 0.5942 score: 0.9147 time: 1.28s
Epoch 31/1000, LR 0.000285
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 1.12s
Val loss: 0.5920 score: 0.8760 time: 0.99s
Test loss: 0.5763 score: 0.9147 time: 1.09s
Epoch 32/1000, LR 0.000285
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 1.12s
Val loss: 0.5748 score: 0.8837 time: 0.95s
Test loss: 0.5564 score: 0.9225 time: 1.08s
Epoch 33/1000, LR 0.000285
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 1.07s
Val loss: 0.5560 score: 0.8915 time: 1.01s
Test loss: 0.5344 score: 0.9225 time: 1.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 1.27s
Val loss: 0.5358 score: 0.8915 time: 1.19s
Test loss: 0.5109 score: 0.9535 time: 0.94s
Epoch 35/1000, LR 0.000285
Train loss: 0.5033;  Loss pred: 0.5033; Loss self: 0.0000; time: 1.01s
Val loss: 0.5144 score: 0.8915 time: 1.06s
Test loss: 0.4859 score: 0.9612 time: 0.92s
Epoch 36/1000, LR 0.000285
Train loss: 0.4793;  Loss pred: 0.4793; Loss self: 0.0000; time: 0.99s
Val loss: 0.4922 score: 0.8915 time: 1.01s
Test loss: 0.4597 score: 0.9535 time: 0.91s
Epoch 37/1000, LR 0.000285
Train loss: 0.4499;  Loss pred: 0.4499; Loss self: 0.0000; time: 1.00s
Val loss: 0.4703 score: 0.8915 time: 1.00s
Test loss: 0.4339 score: 0.9457 time: 0.92s
Epoch 38/1000, LR 0.000284
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 1.08s
Val loss: 0.4485 score: 0.8915 time: 0.93s
Test loss: 0.4082 score: 0.9457 time: 5.11s
Epoch 39/1000, LR 0.000284
Train loss: 0.3959;  Loss pred: 0.3959; Loss self: 0.0000; time: 2.27s
Val loss: 0.4280 score: 0.8915 time: 0.92s
Test loss: 0.3838 score: 0.9457 time: 1.05s
Epoch 40/1000, LR 0.000284
Train loss: 0.3752;  Loss pred: 0.3752; Loss self: 0.0000; time: 0.98s
Val loss: 0.4085 score: 0.8915 time: 0.90s
Test loss: 0.3601 score: 0.9457 time: 1.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3446;  Loss pred: 0.3446; Loss self: 0.0000; time: 0.98s
Val loss: 0.3913 score: 0.8915 time: 0.93s
Test loss: 0.3385 score: 0.9457 time: 1.00s
Epoch 42/1000, LR 0.000284
Train loss: 0.3197;  Loss pred: 0.3197; Loss self: 0.0000; time: 1.06s
Val loss: 0.3763 score: 0.8837 time: 0.96s
Test loss: 0.3186 score: 0.9302 time: 1.02s
Epoch 43/1000, LR 0.000284
Train loss: 0.2953;  Loss pred: 0.2953; Loss self: 0.0000; time: 1.03s
Val loss: 0.3627 score: 0.8837 time: 0.90s
Test loss: 0.2993 score: 0.9302 time: 1.02s
Epoch 44/1000, LR 0.000284
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 1.07s
Val loss: 0.3502 score: 0.8837 time: 0.92s
Test loss: 0.2802 score: 0.9302 time: 1.13s
Epoch 45/1000, LR 0.000284
Train loss: 0.2485;  Loss pred: 0.2485; Loss self: 0.0000; time: 0.97s
Val loss: 0.3410 score: 0.8837 time: 0.89s
Test loss: 0.2636 score: 0.9302 time: 1.21s
Epoch 46/1000, LR 0.000284
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 1.05s
Val loss: 0.3354 score: 0.8837 time: 1.02s
Test loss: 0.2497 score: 0.9302 time: 0.93s
Epoch 47/1000, LR 0.000284
Train loss: 0.2047;  Loss pred: 0.2047; Loss self: 0.0000; time: 1.19s
Val loss: 0.3303 score: 0.8837 time: 1.02s
Test loss: 0.2354 score: 0.9302 time: 0.94s
Epoch 48/1000, LR 0.000284
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 1.01s
Val loss: 0.3281 score: 0.8837 time: 1.00s
Test loss: 0.2226 score: 0.9380 time: 0.93s
Epoch 49/1000, LR 0.000284
Train loss: 0.1692;  Loss pred: 0.1692; Loss self: 0.0000; time: 1.03s
Val loss: 0.3288 score: 0.8915 time: 1.01s
Test loss: 0.2114 score: 0.9380 time: 0.95s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.1540;  Loss pred: 0.1540; Loss self: 0.0000; time: 1.13s
Val loss: 0.3336 score: 0.8915 time: 0.92s
Test loss: 0.2034 score: 0.9380 time: 1.06s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 1.02s
Val loss: 0.3420 score: 0.8992 time: 1.01s
Test loss: 0.1984 score: 0.9302 time: 1.02s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 1.01s
Val loss: 0.3560 score: 0.8992 time: 1.07s
Test loss: 0.1995 score: 0.9380 time: 0.97s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 1.03s
Val loss: 0.3743 score: 0.8992 time: 1.05s
Test loss: 0.2047 score: 0.9457 time: 0.91s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 1.08s
Val loss: 0.3942 score: 0.8992 time: 1.02s
Test loss: 0.2108 score: 0.9302 time: 0.93s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 1.05s
Val loss: 0.4118 score: 0.8992 time: 1.11s
Test loss: 0.2137 score: 0.9302 time: 0.95s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 1.00s
Val loss: 0.4263 score: 0.8992 time: 1.08s
Test loss: 0.2128 score: 0.9457 time: 0.90s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 1.09s
Val loss: 0.4456 score: 0.8992 time: 0.96s
Test loss: 0.2167 score: 0.9457 time: 0.93s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 1.11s
Val loss: 0.4682 score: 0.8992 time: 0.95s
Test loss: 0.2237 score: 0.9457 time: 1.04s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 1.02s
Val loss: 0.4893 score: 0.8992 time: 0.95s
Test loss: 0.2289 score: 0.9457 time: 1.02s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 1.01s
Val loss: 0.5096 score: 0.8992 time: 1.13s
Test loss: 0.2327 score: 0.9380 time: 1.18s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 1.27s
Val loss: 0.5368 score: 0.8992 time: 1.05s
Test loss: 0.2442 score: 0.9380 time: 1.05s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 1.00s
Val loss: 0.5543 score: 0.8992 time: 1.13s
Test loss: 0.2456 score: 0.9457 time: 0.91s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 1.00s
Val loss: 0.5837 score: 0.8992 time: 1.05s
Test loss: 0.2593 score: 0.9225 time: 0.90s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 1.05s
Val loss: 0.6153 score: 0.8915 time: 1.00s
Test loss: 0.2751 score: 0.9225 time: 0.91s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 1.10s
Val loss: 0.6462 score: 0.8915 time: 0.98s
Test loss: 0.2904 score: 0.9147 time: 0.99s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 1.09s
Val loss: 0.6677 score: 0.8915 time: 0.90s
Test loss: 0.2976 score: 0.9147 time: 0.89s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 1.09s
Val loss: 0.6874 score: 0.8915 time: 0.90s
Test loss: 0.3033 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.99s
Val loss: 0.6820 score: 0.8915 time: 0.91s
Test loss: 0.2862 score: 0.9225 time: 1.00s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.1810,   Val_Loss: 0.3281,   Val_Precision: 0.9455,   Val_Recall: 0.8125,   Val_accuracy: 0.8739,   Val_Score: 0.8837,   Val_Loss: 0.3281,   Test_Precision: 0.9831,   Test_Recall: 0.8923,   Test_accuracy: 0.9355,   Test_Score: 0.9380,   Test_loss: 0.2226


[1.591574443038553, 1.3046091960277408, 1.1257583780679852, 1.1057114228606224, 1.0662058137822896, 1.0948089859448373, 0.9656337210908532, 1.2427639099769294, 0.9660250039305538, 1.0541200011502951, 1.0382186118513346, 1.044285950018093, 1.0359894479624927, 0.9265783170703799, 0.9383665770292282, 1.1075081781018525, 1.0147762440610677, 1.2383585250936449, 1.1719125730451196, 1.0025742698926479, 1.0232970130164176, 0.9593146038241684, 1.0652705451939255, 1.1353493349161, 0.972297488944605, 0.9627713339868933, 0.9453859231434762, 1.2598694718908519, 1.092100433073938, 1.2894688011147082, 1.094902864890173, 1.0846578530035913, 1.1746602521743625, 0.9411124300677329, 0.9273792090825737, 0.9175965110771358, 0.9254553199280053, 5.1169169140048325, 1.053045239066705, 1.0654243661556393, 1.0062620309181511, 1.033222194062546, 1.0243710859213024, 1.1378193648997694, 1.2103543519042432, 0.9335646869149059, 0.9410089689772576, 0.9317908538505435, 0.9582162350416183, 1.0677358990069479, 1.02981415996328, 0.9702250889968127, 0.9157468029297888, 0.9359962109010667, 0.9523978598881513, 0.9095096420496702, 0.9389564958401024, 1.0450404770672321, 1.0222833959851414, 1.1837362558580935, 1.0630543378647417, 0.9153309038374573, 0.9062267849221826, 0.9187495170626789, 0.9944039369001985, 0.9064482781104743, 1.0204656668938696, 1.0076769678853452]
[0.012337786380143823, 0.010113249581610395, 0.008726809132309962, 0.008571406378764515, 0.008265161347149531, 0.008486891363913467, 0.007485532721634521, 0.009633828759511081, 0.007488565921942278, 0.008171472877134071, 0.008048206293421198, 0.008095239922620876, 0.008030925953197618, 0.007182777651708371, 0.007274159511854482, 0.008585334713967848, 0.0078664825121013, 0.009599678489098023, 0.009084593589497052, 0.007771893565059286, 0.007932534984623393, 0.0074365473164664225, 0.008257911203053686, 0.008801157635008527, 0.007537189836779884, 0.007463343674317002, 0.007328573047623847, 0.009766430014657766, 0.008465894830030527, 0.009995882179183785, 0.00848761910767576, 0.008408200410880553, 0.009105893427708237, 0.0072954451943235115, 0.007188986116919175, 0.0071131512486599675, 0.007174072247503918, 0.039666022589184745, 0.008163141388113993, 0.008259103613609606, 0.007800480859830629, 0.008009474372577876, 0.007940861131172887, 0.00882030515426178, 0.009382591875226692, 0.007236935557479891, 0.007294643170366338, 0.007223184913570105, 0.007428032829779987, 0.008277022472922077, 0.007983055503591317, 0.007521124720905525, 0.007098812425812316, 0.007255784580628423, 0.007382929146419777, 0.007050462341470312, 0.007278732525892267, 0.008101088969513427, 0.00792467748825691, 0.009176250045411578, 0.008240731301277067, 0.007095588401840754, 0.007025013836606067, 0.007122089279555651, 0.007708557650389135, 0.0070267308380656925, 0.007910586565068756, 0.007811449363452289]
[81.05181668645028, 98.88018603025161, 114.58942035269456, 116.66696873426508, 120.98977358075139, 117.8287734719961, 133.5910264756204, 103.80089006800556, 133.53691620312722, 122.37695884645997, 124.25128824262676, 123.52938387973619, 124.51864278512454, 139.22190668983848, 137.47292706055313, 116.47769519958926, 127.12161991864383, 104.17015540006477, 110.07647069166941, 128.668771854491, 126.06310617456121, 134.47100615977305, 121.09599817811203, 113.62141680343068, 132.6754429243924, 133.98820202280362, 136.45221156992238, 102.3915595052817, 118.12100434472255, 100.04119517159567, 117.81867062055743, 118.93151341944221, 109.81898788284843, 137.07182678557942, 139.1016735512278, 140.58466705433656, 139.39084602164846, 25.210493382632666, 122.50185957284262, 121.07851490713459, 128.19722501334525, 124.85213804088205, 125.93092656845106, 113.37476226849384, 106.58035789026997, 138.18003380815358, 137.08689741842161, 138.44308459019413, 134.62514543431539, 120.81639300502772, 125.26531972001604, 132.95883755529618, 140.8686326692975, 137.82107074537623, 135.44759541474573, 141.8346700638442, 137.38655685488519, 123.44019473965395, 126.18810058602867, 108.97697807396088, 121.3484536069062, 140.9326391790958, 142.348474075479, 140.40823707034326, 129.72595462777903, 142.31369082514598, 126.41287618490378, 128.01721594442336]
Elapsed: 1.1018009401912636~0.50514030132038
Time per graph: 0.008541092559622198~0.003915816289305272
Speed: 123.8093272088168~17.479186716905765
Total Time: 1.0081
best val loss: 0.3281079661245494 test_score: 0.9380

Testing...
Test loss: 0.1984 score: 0.9302 time: 0.88s
test Score 0.9302
Epoch Time List: [5.460292680887505, 3.7824660879559815, 3.2989012498874217, 3.442309334874153, 3.2114545102231205, 3.359965565847233, 3.1925992879550904, 3.7858298420906067, 3.2037536310963333, 3.1010942701250315, 3.008759702090174, 2.96369590703398, 3.114729384193197, 3.0649560808669776, 3.013786273309961, 3.2057537250220776, 3.0761802489869297, 3.4621189220342785, 3.4511459439527243, 3.1717854391317815, 3.4578350400552154, 3.0600644561927766, 3.1610042660031468, 3.1486532150302082, 3.0498577018734068, 3.204407633980736, 3.0250618890859187, 3.8027085312642157, 3.233878026017919, 3.4855758021585643, 3.203221410047263, 3.1444422961212695, 3.2370991432107985, 3.395851780893281, 2.9904941259883344, 2.9114032650832087, 2.9221801720559597, 7.113861073041335, 4.234130458906293, 2.9456743223126978, 2.913202774943784, 3.0480846802238375, 2.947356719058007, 3.115980404894799, 3.068543071858585, 2.999263959703967, 3.1489693778567016, 2.937886595958844, 2.9956519051920623, 3.1167925829067826, 3.0501450789161026, 3.0498694707639515, 2.9937610160559416, 3.0338518258649856, 3.1066161890048534, 2.97718976973556, 2.990113138919696, 3.1012895219027996, 2.9867492550984025, 3.3168449837248772, 3.37742482800968, 3.0332591203041375, 2.9511483090464026, 2.9603495330084115, 3.074644629145041, 2.8823389201425016, 3.0127715307753533, 2.90601352811791]
Total Epoch List: [68]
Total Time List: [1.0081280339509249]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71389351ef50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 1.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.97s
Epoch 2/1000, LR 0.000015
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.94s
Epoch 3/1000, LR 0.000045
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.95s
Epoch 4/1000, LR 0.000075
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.97s
Epoch 5/1000, LR 0.000105
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 1.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 1.60s
Epoch 6/1000, LR 0.000135
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.08s
Epoch 7/1000, LR 0.000165
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.96s
Epoch 8/1000, LR 0.000195
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.94s
Epoch 9/1000, LR 0.000225
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.99s
Epoch 10/1000, LR 0.000255
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.98s
Test loss: 0.6916 score: 0.5116 time: 0.97s
Epoch 11/1000, LR 0.000285
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 1.04s
Test loss: 0.6909 score: 0.5116 time: 0.95s
Epoch 12/1000, LR 0.000285
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 1.07s
Val loss: 0.6904 score: 0.5116 time: 1.05s
Test loss: 0.6901 score: 0.5349 time: 1.06s
Epoch 13/1000, LR 0.000285
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.99s
Val loss: 0.6893 score: 0.7132 time: 1.02s
Test loss: 0.6891 score: 0.6744 time: 1.07s
Epoch 14/1000, LR 0.000285
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.95s
Val loss: 0.6880 score: 0.7674 time: 1.06s
Test loss: 0.6879 score: 0.7752 time: 1.13s
Epoch 15/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.96s
Val loss: 0.6863 score: 0.6822 time: 1.08s
Test loss: 0.6863 score: 0.6357 time: 0.94s
Epoch 16/1000, LR 0.000285
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.95s
Val loss: 0.6841 score: 0.6124 time: 1.08s
Test loss: 0.6843 score: 0.5581 time: 0.95s
Epoch 17/1000, LR 0.000285
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.94s
Val loss: 0.6816 score: 0.5891 time: 1.14s
Test loss: 0.6819 score: 0.5271 time: 0.94s
Epoch 18/1000, LR 0.000285
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.93s
Val loss: 0.6787 score: 0.5736 time: 1.12s
Test loss: 0.6792 score: 0.5349 time: 0.96s
Epoch 19/1000, LR 0.000285
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 1.04s
Val loss: 0.6750 score: 0.5581 time: 0.96s
Test loss: 0.6758 score: 0.5349 time: 0.93s
Epoch 20/1000, LR 0.000285
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 1.07s
Val loss: 0.6706 score: 0.5581 time: 0.97s
Test loss: 0.6718 score: 0.5349 time: 0.96s
Epoch 21/1000, LR 0.000285
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 1.07s
Val loss: 0.6653 score: 0.5581 time: 0.96s
Test loss: 0.6671 score: 0.5271 time: 1.07s
Epoch 22/1000, LR 0.000285
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.95s
Val loss: 0.6589 score: 0.5581 time: 1.01s
Test loss: 0.6614 score: 0.5271 time: 1.03s
Epoch 23/1000, LR 0.000285
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.92s
Val loss: 0.6512 score: 0.5581 time: 1.04s
Test loss: 0.6548 score: 0.5271 time: 1.19s
Epoch 24/1000, LR 0.000285
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.97s
Val loss: 0.6423 score: 0.5581 time: 1.14s
Test loss: 0.6471 score: 0.5349 time: 1.08s
Epoch 25/1000, LR 0.000285
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 1.07s
Val loss: 0.6318 score: 0.5504 time: 1.15s
Test loss: 0.6382 score: 0.5349 time: 0.99s
Epoch 26/1000, LR 0.000285
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.90s
Val loss: 0.6195 score: 0.5581 time: 1.18s
Test loss: 0.6280 score: 0.5349 time: 0.93s
Epoch 27/1000, LR 0.000285
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 1.00s
Val loss: 0.6053 score: 0.5891 time: 1.05s
Test loss: 0.6163 score: 0.5426 time: 0.91s
Epoch 28/1000, LR 0.000285
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 1.00s
Val loss: 0.5887 score: 0.6047 time: 5.24s
Test loss: 0.6026 score: 0.5426 time: 1.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 1.02s
Val loss: 0.5700 score: 0.6279 time: 0.99s
Test loss: 0.5871 score: 0.5736 time: 0.94s
Epoch 30/1000, LR 0.000285
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 1.02s
Val loss: 0.5491 score: 0.6589 time: 0.98s
Test loss: 0.5694 score: 0.5891 time: 1.02s
Epoch 31/1000, LR 0.000285
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.93s
Val loss: 0.5260 score: 0.7209 time: 0.96s
Test loss: 0.5490 score: 0.6667 time: 1.03s
Epoch 32/1000, LR 0.000285
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 0.91s
Val loss: 0.5015 score: 0.8062 time: 0.94s
Test loss: 0.5270 score: 0.7674 time: 1.02s
Epoch 33/1000, LR 0.000285
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.90s
Val loss: 0.4774 score: 0.8372 time: 0.95s
Test loss: 0.5050 score: 0.7829 time: 1.01s
Epoch 34/1000, LR 0.000285
Train loss: 0.4901;  Loss pred: 0.4901; Loss self: 0.0000; time: 0.91s
Val loss: 0.4548 score: 0.8527 time: 1.05s
Test loss: 0.4844 score: 0.8217 time: 0.92s
Epoch 35/1000, LR 0.000285
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.90s
Val loss: 0.4344 score: 0.8527 time: 1.05s
Test loss: 0.4664 score: 0.8295 time: 0.91s
Epoch 36/1000, LR 0.000285
Train loss: 0.4512;  Loss pred: 0.4512; Loss self: 0.0000; time: 0.92s
Val loss: 0.4172 score: 0.8527 time: 1.07s
Test loss: 0.4515 score: 0.8295 time: 1.07s
Epoch 37/1000, LR 0.000285
Train loss: 0.4336;  Loss pred: 0.4336; Loss self: 0.0000; time: 0.92s
Val loss: 0.4026 score: 0.8527 time: 1.09s
Test loss: 0.4387 score: 0.8372 time: 1.00s
Epoch 38/1000, LR 0.000284
Train loss: 0.4230;  Loss pred: 0.4230; Loss self: 0.0000; time: 1.05s
Val loss: 0.3901 score: 0.8605 time: 1.13s
Test loss: 0.4275 score: 0.8372 time: 0.99s
Epoch 39/1000, LR 0.000284
Train loss: 0.4078;  Loss pred: 0.4078; Loss self: 0.0000; time: 1.03s
Val loss: 0.3787 score: 0.8760 time: 1.08s
Test loss: 0.4168 score: 0.8372 time: 0.94s
Epoch 40/1000, LR 0.000284
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 1.07s
Val loss: 0.3671 score: 0.8760 time: 0.99s
Test loss: 0.4056 score: 0.8372 time: 0.95s
Epoch 41/1000, LR 0.000284
Train loss: 0.3809;  Loss pred: 0.3809; Loss self: 0.0000; time: 1.09s
Val loss: 0.3559 score: 0.8760 time: 1.02s
Test loss: 0.3945 score: 0.8372 time: 1.14s
Epoch 42/1000, LR 0.000284
Train loss: 0.3689;  Loss pred: 0.3689; Loss self: 0.0000; time: 0.93s
Val loss: 0.3426 score: 0.8992 time: 0.99s
Test loss: 0.3805 score: 0.8682 time: 1.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3585;  Loss pred: 0.3585; Loss self: 0.0000; time: 0.93s
Val loss: 0.3302 score: 0.9302 time: 1.37s
Test loss: 0.3671 score: 0.8760 time: 1.40s
Epoch 44/1000, LR 0.000284
Train loss: 0.3419;  Loss pred: 0.3419; Loss self: 0.0000; time: 1.09s
Val loss: 0.3199 score: 0.9380 time: 1.10s
Test loss: 0.3565 score: 0.8837 time: 1.24s
Epoch 45/1000, LR 0.000284
Train loss: 0.3435;  Loss pred: 0.3435; Loss self: 0.0000; time: 1.11s
Val loss: 0.3117 score: 0.9225 time: 1.11s
Test loss: 0.3486 score: 0.8837 time: 1.00s
Epoch 46/1000, LR 0.000284
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.97s
Val loss: 0.3059 score: 0.9225 time: 1.20s
Test loss: 0.3441 score: 0.8837 time: 0.92s
Epoch 47/1000, LR 0.000284
Train loss: 0.3199;  Loss pred: 0.3199; Loss self: 0.0000; time: 0.94s
Val loss: 0.2966 score: 0.9225 time: 1.12s
Test loss: 0.3347 score: 0.8837 time: 0.94s
Epoch 48/1000, LR 0.000284
Train loss: 0.2932;  Loss pred: 0.2932; Loss self: 0.0000; time: 0.98s
Val loss: 0.2839 score: 0.9302 time: 1.07s
Test loss: 0.3214 score: 0.8915 time: 0.93s
Epoch 49/1000, LR 0.000284
Train loss: 0.2858;  Loss pred: 0.2858; Loss self: 0.0000; time: 0.95s
Val loss: 0.2718 score: 0.9302 time: 1.10s
Test loss: 0.3095 score: 0.8915 time: 1.00s
Epoch 50/1000, LR 0.000284
Train loss: 0.2726;  Loss pred: 0.2726; Loss self: 0.0000; time: 0.92s
Val loss: 0.2614 score: 0.9302 time: 1.12s
Test loss: 0.2995 score: 0.8992 time: 0.95s
Epoch 51/1000, LR 0.000284
Train loss: 0.2487;  Loss pred: 0.2487; Loss self: 0.0000; time: 1.04s
Val loss: 0.2478 score: 0.9380 time: 0.98s
Test loss: 0.2860 score: 0.9070 time: 1.00s
Epoch 52/1000, LR 0.000284
Train loss: 0.2409;  Loss pred: 0.2409; Loss self: 0.0000; time: 1.01s
Val loss: 0.2380 score: 0.9380 time: 0.96s
Test loss: 0.2764 score: 0.9070 time: 0.92s
Epoch 53/1000, LR 0.000284
Train loss: 0.2396;  Loss pred: 0.2396; Loss self: 0.0000; time: 1.04s
Val loss: 0.2299 score: 0.9302 time: 0.97s
Test loss: 0.2689 score: 0.9147 time: 1.08s
Epoch 54/1000, LR 0.000284
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.94s
Val loss: 0.2241 score: 0.9302 time: 0.99s
Test loss: 0.2636 score: 0.9070 time: 1.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2078;  Loss pred: 0.2078; Loss self: 0.0000; time: 1.01s
Val loss: 0.2191 score: 0.9302 time: 1.01s
Test loss: 0.2587 score: 0.9070 time: 1.07s
Epoch 56/1000, LR 0.000284
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.98s
Val loss: 0.2121 score: 0.9302 time: 1.04s
Test loss: 0.2513 score: 0.9147 time: 1.27s
Epoch 57/1000, LR 0.000283
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.95s
Val loss: 0.2082 score: 0.9302 time: 0.99s
Test loss: 0.2472 score: 0.9147 time: 1.09s
Epoch 58/1000, LR 0.000283
Train loss: 0.1662;  Loss pred: 0.1662; Loss self: 0.0000; time: 0.94s
Val loss: 0.2014 score: 0.9302 time: 0.98s
Test loss: 0.2390 score: 0.9225 time: 1.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.91s
Val loss: 0.1941 score: 0.9302 time: 1.07s
Test loss: 0.2305 score: 0.9225 time: 0.97s
Epoch 60/1000, LR 0.000283
Train loss: 0.1513;  Loss pred: 0.1513; Loss self: 0.0000; time: 0.98s
Val loss: 0.1866 score: 0.9535 time: 1.09s
Test loss: 0.2221 score: 0.9302 time: 0.91s
Epoch 61/1000, LR 0.000283
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.94s
Val loss: 0.1862 score: 0.9535 time: 1.10s
Test loss: 0.2206 score: 0.9380 time: 0.96s
Epoch 62/1000, LR 0.000283
Train loss: 0.1279;  Loss pred: 0.1279; Loss self: 0.0000; time: 1.08s
Val loss: 0.1909 score: 0.9380 time: 1.06s
Test loss: 0.2244 score: 0.9380 time: 0.98s
     INFO: Early stopping counter 1 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.93s
Val loss: 0.1932 score: 0.9380 time: 1.08s
Test loss: 0.2259 score: 0.9380 time: 0.96s
     INFO: Early stopping counter 2 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 1.03s
Val loss: 0.1968 score: 0.9380 time: 1.13s
Test loss: 0.2290 score: 0.9380 time: 0.93s
     INFO: Early stopping counter 3 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 1.06s
Val loss: 0.2025 score: 0.9380 time: 1.02s
Test loss: 0.2351 score: 0.9380 time: 1.07s
     INFO: Early stopping counter 4 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 1.00s
Val loss: 0.2029 score: 0.9457 time: 1.00s
Test loss: 0.2344 score: 0.9380 time: 1.09s
     INFO: Early stopping counter 5 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1263;  Loss pred: 0.1263; Loss self: 0.0000; time: 0.95s
Val loss: 0.2043 score: 0.9457 time: 1.00s
Test loss: 0.2351 score: 0.9380 time: 1.05s
     INFO: Early stopping counter 6 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 1.00s
Val loss: 0.2124 score: 0.9380 time: 1.16s
Test loss: 0.2453 score: 0.9380 time: 0.94s
     INFO: Early stopping counter 7 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.96s
Val loss: 0.2200 score: 0.9380 time: 1.13s
Test loss: 0.2557 score: 0.9380 time: 0.96s
     INFO: Early stopping counter 8 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.97s
Val loss: 0.2230 score: 0.9380 time: 1.08s
Test loss: 0.2592 score: 0.9302 time: 1.05s
     INFO: Early stopping counter 9 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.95s
Val loss: 0.2268 score: 0.9380 time: 1.07s
Test loss: 0.2651 score: 0.9302 time: 0.93s
     INFO: Early stopping counter 10 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 1.03s
Val loss: 0.2338 score: 0.9380 time: 0.97s
Test loss: 0.2771 score: 0.9225 time: 0.97s
     INFO: Early stopping counter 11 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.96s
Val loss: 0.2396 score: 0.9380 time: 1.10s
Test loss: 0.2867 score: 0.9225 time: 0.97s
     INFO: Early stopping counter 12 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 1.03s
Val loss: 0.2421 score: 0.9380 time: 0.96s
Test loss: 0.2906 score: 0.9225 time: 1.02s
     INFO: Early stopping counter 13 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.95s
Val loss: 0.2435 score: 0.9380 time: 0.96s
Test loss: 0.2921 score: 0.9225 time: 1.12s
     INFO: Early stopping counter 14 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.96s
Val loss: 0.2456 score: 0.9380 time: 1.12s
Test loss: 0.2943 score: 0.9225 time: 1.07s
     INFO: Early stopping counter 15 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.97s
Val loss: 0.2572 score: 0.9380 time: 1.09s
Test loss: 0.3141 score: 0.9225 time: 1.06s
     INFO: Early stopping counter 16 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.0755;  Loss pred: 0.0755; Loss self: 0.0000; time: 1.05s
Val loss: 0.2662 score: 0.9380 time: 1.09s
Test loss: 0.3287 score: 0.9225 time: 1.03s
     INFO: Early stopping counter 17 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 1.09s
Val loss: 0.2703 score: 0.9380 time: 1.10s
Test loss: 0.3329 score: 0.9147 time: 0.95s
     INFO: Early stopping counter 18 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.94s
Val loss: 0.2755 score: 0.9380 time: 1.07s
Test loss: 0.3388 score: 0.9147 time: 0.97s
     INFO: Early stopping counter 19 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.94s
Val loss: 0.2787 score: 0.9380 time: 1.16s
Test loss: 0.3405 score: 0.9225 time: 0.93s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 060,   Train_Loss: 0.1280,   Val_Loss: 0.1862,   Val_Precision: 0.9683,   Val_Recall: 0.9385,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.1862,   Test_Precision: 0.9667,   Test_Recall: 0.9062,   Test_accuracy: 0.9355,   Test_Score: 0.9380,   Test_loss: 0.2206


[1.591574443038553, 1.3046091960277408, 1.1257583780679852, 1.1057114228606224, 1.0662058137822896, 1.0948089859448373, 0.9656337210908532, 1.2427639099769294, 0.9660250039305538, 1.0541200011502951, 1.0382186118513346, 1.044285950018093, 1.0359894479624927, 0.9265783170703799, 0.9383665770292282, 1.1075081781018525, 1.0147762440610677, 1.2383585250936449, 1.1719125730451196, 1.0025742698926479, 1.0232970130164176, 0.9593146038241684, 1.0652705451939255, 1.1353493349161, 0.972297488944605, 0.9627713339868933, 0.9453859231434762, 1.2598694718908519, 1.092100433073938, 1.2894688011147082, 1.094902864890173, 1.0846578530035913, 1.1746602521743625, 0.9411124300677329, 0.9273792090825737, 0.9175965110771358, 0.9254553199280053, 5.1169169140048325, 1.053045239066705, 1.0654243661556393, 1.0062620309181511, 1.033222194062546, 1.0243710859213024, 1.1378193648997694, 1.2103543519042432, 0.9335646869149059, 0.9410089689772576, 0.9317908538505435, 0.9582162350416183, 1.0677358990069479, 1.02981415996328, 0.9702250889968127, 0.9157468029297888, 0.9359962109010667, 0.9523978598881513, 0.9095096420496702, 0.9389564958401024, 1.0450404770672321, 1.0222833959851414, 1.1837362558580935, 1.0630543378647417, 0.9153309038374573, 0.9062267849221826, 0.9187495170626789, 0.9944039369001985, 0.9064482781104743, 1.0204656668938696, 1.0076769678853452, 0.980054748011753, 0.9508741700556129, 0.9576431110035628, 0.9707367229275405, 1.6041485290043056, 1.084830524167046, 0.9650675228331238, 0.9469699009787291, 0.9978566148784012, 0.9720934771467, 0.9576006201095879, 1.0677173340227455, 1.0790453769732267, 1.1414619709830731, 0.9424952559638768, 0.9503262080252171, 0.943746532779187, 0.965190518181771, 0.9321661130525172, 0.9713619479443878, 1.075898976996541, 1.0319170409347862, 1.1912668670993298, 1.083729743026197, 0.990277815843001, 0.935744411079213, 0.9113332540728152, 1.226555695058778, 0.9405594479758292, 1.0236791530624032, 1.0378840730991215, 1.023774344008416, 1.014870150014758, 0.9241385799832642, 0.9165124548599124, 1.0768631740938872, 1.0059144098777324, 0.9942065561190248, 0.95054029696621, 0.9552834948990494, 1.1468128110282123, 1.0672801309265196, 1.4072679409291595, 1.2501652408391237, 1.0068607989232987, 0.9259075880981982, 0.9481218450237066, 0.9327199719846249, 1.0016286680474877, 0.9598964299075305, 1.0082713270094246, 0.9271092910785228, 1.084901344962418, 1.0637465219479054, 1.0799192991107702, 1.276496275793761, 1.0998962000012398, 1.0682499669492245, 0.9794897141400725, 0.9153995639644563, 0.961650688899681, 0.9855980358552188, 0.9683415058534592, 0.9329992739949375, 1.0742661689873785, 1.0959308051969856, 1.0579434069804847, 0.9442764699924737, 0.9640111299231648, 1.0506846881471574, 0.9336461839266121, 0.9777939640916884, 0.9791233560536057, 1.0270990820135921, 1.1240323500242084, 1.076739378971979, 1.064490769058466, 1.035449534887448, 0.9552641920745373, 0.9790447012055665, 0.9361885630059987]
[0.012337786380143823, 0.010113249581610395, 0.008726809132309962, 0.008571406378764515, 0.008265161347149531, 0.008486891363913467, 0.007485532721634521, 0.009633828759511081, 0.007488565921942278, 0.008171472877134071, 0.008048206293421198, 0.008095239922620876, 0.008030925953197618, 0.007182777651708371, 0.007274159511854482, 0.008585334713967848, 0.0078664825121013, 0.009599678489098023, 0.009084593589497052, 0.007771893565059286, 0.007932534984623393, 0.0074365473164664225, 0.008257911203053686, 0.008801157635008527, 0.007537189836779884, 0.007463343674317002, 0.007328573047623847, 0.009766430014657766, 0.008465894830030527, 0.009995882179183785, 0.00848761910767576, 0.008408200410880553, 0.009105893427708237, 0.0072954451943235115, 0.007188986116919175, 0.0071131512486599675, 0.007174072247503918, 0.039666022589184745, 0.008163141388113993, 0.008259103613609606, 0.007800480859830629, 0.008009474372577876, 0.007940861131172887, 0.00882030515426178, 0.009382591875226692, 0.007236935557479891, 0.007294643170366338, 0.007223184913570105, 0.007428032829779987, 0.008277022472922077, 0.007983055503591317, 0.007521124720905525, 0.007098812425812316, 0.007255784580628423, 0.007382929146419777, 0.007050462341470312, 0.007278732525892267, 0.008101088969513427, 0.00792467748825691, 0.009176250045411578, 0.008240731301277067, 0.007095588401840754, 0.007025013836606067, 0.007122089279555651, 0.007708557650389135, 0.0070267308380656925, 0.007910586565068756, 0.007811449363452289, 0.007597323627998085, 0.007371117597330333, 0.007423590007779556, 0.007525090875407291, 0.012435259914762058, 0.008409538947031364, 0.007481143587853673, 0.007340851945571544, 0.0077353225959565985, 0.007535608349974418, 0.007423260621004558, 0.008276878558315857, 0.008364692844753696, 0.008848542410721498, 0.007306164774913774, 0.007366869829652845, 0.007315864595187496, 0.007482097040168768, 0.007226093899631916, 0.007529937580964247, 0.008340302147260007, 0.007999356906471211, 0.009234626876738991, 0.008401005759892999, 0.0076765722158372175, 0.007253832644024907, 0.007064598868781513, 0.009508183682626185, 0.0072911585114405365, 0.007935497310561266, 0.008045612969760632, 0.00793623522487144, 0.007867210465230683, 0.0071638649611105755, 0.007104747712092344, 0.008347776543363467, 0.0077977861230831965, 0.007707027566814145, 0.007368529433846589, 0.00740529841007015, 0.008890021790916374, 0.008273489387027284, 0.010909053805652399, 0.009691203417357548, 0.007805122472273634, 0.007177578202311614, 0.007349781744369818, 0.007230387379725774, 0.00776456331819758, 0.007441057596182407, 0.007816056798522672, 0.007186893729290875, 0.0084100879454451, 0.008246097069363608, 0.00837146743496721, 0.009895319967393497, 0.008526327131792556, 0.008281007495730422, 0.007592943520465678, 0.007096120650887258, 0.007454656503098302, 0.007640294851590844, 0.007506523301189606, 0.0072325525115886635, 0.00832764472083239, 0.008495587637185935, 0.008201111682019261, 0.007319972635600571, 0.0074729544955284095, 0.008144842543776414, 0.007237567317260559, 0.007579798171253398, 0.007590103535299268, 0.007962008387702264, 0.008713429069955104, 0.00834681689125565, 0.008251866426809813, 0.008026740580522853, 0.007405148775771607, 0.00758949380779509, 0.007257275682217045]
[81.05181668645028, 98.88018603025161, 114.58942035269456, 116.66696873426508, 120.98977358075139, 117.8287734719961, 133.5910264756204, 103.80089006800556, 133.53691620312722, 122.37695884645997, 124.25128824262676, 123.52938387973619, 124.51864278512454, 139.22190668983848, 137.47292706055313, 116.47769519958926, 127.12161991864383, 104.17015540006477, 110.07647069166941, 128.668771854491, 126.06310617456121, 134.47100615977305, 121.09599817811203, 113.62141680343068, 132.6754429243924, 133.98820202280362, 136.45221156992238, 102.3915595052817, 118.12100434472255, 100.04119517159567, 117.81867062055743, 118.93151341944221, 109.81898788284843, 137.07182678557942, 139.1016735512278, 140.58466705433656, 139.39084602164846, 25.210493382632666, 122.50185957284262, 121.07851490713459, 128.19722501334525, 124.85213804088205, 125.93092656845106, 113.37476226849384, 106.58035789026997, 138.18003380815358, 137.08689741842161, 138.44308459019413, 134.62514543431539, 120.81639300502772, 125.26531972001604, 132.95883755529618, 140.8686326692975, 137.82107074537623, 135.44759541474573, 141.8346700638442, 137.38655685488519, 123.44019473965395, 126.18810058602867, 108.97697807396088, 121.3484536069062, 140.9326391790958, 142.348474075479, 140.40823707034326, 129.72595462777903, 142.31369082514598, 126.41287618490378, 128.01721594442336, 131.6252997719807, 135.66463793254084, 134.70571501821212, 132.88876062189422, 80.41649365228683, 118.91258323418649, 133.66940338153546, 136.22397065278804, 129.27709059253962, 132.70328732031234, 134.71169221385554, 120.8184937056121, 119.5501160125917, 113.01296344450266, 136.870715458481, 135.74286272506646, 136.6892439012359, 133.6523697342268, 138.38735198984034, 132.8032256904771, 119.89973292856354, 125.01004914420479, 108.28807848413345, 119.03336678735197, 130.26647465608954, 137.85815707007183, 141.55085356919605, 105.17255801728447, 137.15241527541923, 126.01604674090319, 124.29133787052541, 126.00432971871734, 127.10985735280923, 139.58945421620223, 140.7509514092937, 119.7923776235968, 128.2415270457054, 129.7517092459772, 135.71228953861566, 135.03844742301573, 112.48566353591818, 120.86798607223525, 91.66697843967565, 103.18635951949351, 128.1209876657707, 139.32275926689863, 136.05846197623947, 138.30517612431532, 128.79024344567188, 134.38949868000543, 127.94175193161493, 139.1421715231998, 118.90482079222483, 121.26949168658949, 119.45337036408321, 101.0578741561813, 117.28379459793987, 120.7582532095988, 131.70122987279507, 140.92206843678252, 134.14434314771987, 130.88500109282856, 133.21746431420837, 138.26377318349333, 120.08197197683101, 117.70816130751356, 121.9346887072, 136.61253255736446, 133.81588240613132, 122.77708189265287, 138.1679722156288, 131.92963419428878, 131.7505084547666, 125.59645146123583, 114.76538019321393, 119.80615041976382, 121.1847051657381, 124.58357037556846, 135.04117611678927, 131.76109307486496, 137.79275361556986]
Elapsed: 1.05966118289214~0.35295214690513704
Time per graph: 0.008214427774357673~0.0027360631543033877
Speed: 125.61973005732878~14.545627132255907
Total Time: 0.9368
best val loss: 0.18616018696984118 test_score: 0.9380

Testing...
Test loss: 0.2221 score: 0.9302 time: 1.04s
test Score 0.9302
Epoch Time List: [5.460292680887505, 3.7824660879559815, 3.2989012498874217, 3.442309334874153, 3.2114545102231205, 3.359965565847233, 3.1925992879550904, 3.7858298420906067, 3.2037536310963333, 3.1010942701250315, 3.008759702090174, 2.96369590703398, 3.114729384193197, 3.0649560808669776, 3.013786273309961, 3.2057537250220776, 3.0761802489869297, 3.4621189220342785, 3.4511459439527243, 3.1717854391317815, 3.4578350400552154, 3.0600644561927766, 3.1610042660031468, 3.1486532150302082, 3.0498577018734068, 3.204407633980736, 3.0250618890859187, 3.8027085312642157, 3.233878026017919, 3.4855758021585643, 3.203221410047263, 3.1444422961212695, 3.2370991432107985, 3.395851780893281, 2.9904941259883344, 2.9114032650832087, 2.9221801720559597, 7.113861073041335, 4.234130458906293, 2.9456743223126978, 2.913202774943784, 3.0480846802238375, 2.947356719058007, 3.115980404894799, 3.068543071858585, 2.999263959703967, 3.1489693778567016, 2.937886595958844, 2.9956519051920623, 3.1167925829067826, 3.0501450789161026, 3.0498694707639515, 2.9937610160559416, 3.0338518258649856, 3.1066161890048534, 2.97718976973556, 2.990113138919696, 3.1012895219027996, 2.9867492550984025, 3.3168449837248772, 3.37742482800968, 3.0332591203041375, 2.9511483090464026, 2.9603495330084115, 3.074644629145041, 2.8823389201425016, 3.0127715307753533, 2.90601352811791, 3.2660880752373487, 3.036463968222961, 3.054431786062196, 3.117948374012485, 4.110639719991013, 3.25362038705498, 2.99383406015113, 3.0026860137004405, 3.038901256863028, 2.9610763790551573, 3.047266853041947, 3.1737492957618088, 3.086267138598487, 3.1414313009008765, 2.9790844791568816, 2.9769791211001575, 3.017546733142808, 3.0087035060860217, 2.928344320738688, 3.0103720847982913, 3.101353476056829, 2.9891032660380006, 3.1402138511184603, 3.193071232875809, 3.208869265858084, 3.0103896469809115, 2.959674126235768, 7.455201967153698, 2.9485629501286894, 3.0213504740968347, 2.9178467069286853, 2.8647170530166477, 2.861177642829716, 2.8699913800228387, 2.86377359717153, 3.0571727198548615, 3.009768303250894, 3.1617866759188473, 3.049594890791923, 3.007920235162601, 3.252883919980377, 2.980200698133558, 3.6993720100726932, 3.438953216187656, 3.2248892101924866, 3.0878220291342586, 3.0063461752142757, 2.976022647926584, 3.0439277107361704, 2.9945005329791456, 3.0266422340646386, 2.8882114260923117, 3.080686065601185, 2.9893160769715905, 3.091349651804194, 3.285513855982572, 3.038478735135868, 2.9786459628958255, 2.9571213112212718, 2.978413392789662, 2.999915729975328, 3.118635216029361, 2.9742779179941863, 3.0846605168189853, 3.1453649608884007, 3.096292518079281, 3.0001324710901827, 3.0970098450779915, 3.049516585888341, 3.0964221090544015, 2.944278731243685, 2.975443699164316, 3.034302191110328, 3.0141411758959293, 3.029596976004541, 3.1573145291768014, 3.118987030116841, 3.174129315884784, 3.135308442171663, 2.9886262577492744, 3.0393106408882886]
Total Epoch List: [68, 81]
Total Time List: [1.0081280339509249, 0.936827037949115]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x713893537e20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 1.05s
Epoch 2/1000, LR 0.000020
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.86s
Epoch 3/1000, LR 0.000050
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.88s
Epoch 4/1000, LR 0.000080
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 5.06s
Epoch 5/1000, LR 0.000110
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.86s
Epoch 6/1000, LR 0.000140
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.85s
Epoch 7/1000, LR 0.000170
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.95s
Epoch 8/1000, LR 0.000200
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.95s
Epoch 9/1000, LR 0.000230
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.98s
Epoch 10/1000, LR 0.000260
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.87s
Epoch 11/1000, LR 0.000290
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.88s
Epoch 12/1000, LR 0.000290
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.89s
Epoch 13/1000, LR 0.000290
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4961 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.92s
Epoch 14/1000, LR 0.000290
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 1.10s
Epoch 15/1000, LR 0.000290
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 1.02s
Epoch 16/1000, LR 0.000290
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.89s
Epoch 17/1000, LR 0.000290
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.91s
Epoch 18/1000, LR 0.000290
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.87s
Epoch 19/1000, LR 0.000290
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.87s
Epoch 20/1000, LR 0.000290
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 1.24s
Val loss: 0.6847 score: 0.5039 time: 0.91s
Test loss: 0.6857 score: 0.5078 time: 0.99s
Epoch 21/1000, LR 0.000290
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 1.14s
Val loss: 0.6812 score: 0.5504 time: 0.92s
Test loss: 0.6827 score: 0.5312 time: 1.14s
Epoch 22/1000, LR 0.000290
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 1.11s
Val loss: 0.6763 score: 0.6434 time: 0.99s
Test loss: 0.6785 score: 0.6719 time: 1.13s
Epoch 23/1000, LR 0.000290
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 1.11s
Val loss: 0.6698 score: 0.8450 time: 1.04s
Test loss: 0.6729 score: 0.8047 time: 0.88s
Epoch 24/1000, LR 0.000290
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 1.24s
Val loss: 0.6631 score: 0.8605 time: 0.98s
Test loss: 0.6669 score: 0.8438 time: 0.90s
Epoch 25/1000, LR 0.000290
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 1.08s
Val loss: 0.6562 score: 0.8837 time: 0.96s
Test loss: 0.6605 score: 0.8438 time: 0.96s
Epoch 26/1000, LR 0.000290
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 1.12s
Val loss: 0.6471 score: 0.8992 time: 1.00s
Test loss: 0.6521 score: 0.8750 time: 0.89s
Epoch 27/1000, LR 0.000290
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 1.07s
Val loss: 0.6330 score: 0.9070 time: 0.98s
Test loss: 0.6397 score: 0.8828 time: 0.87s
Epoch 28/1000, LR 0.000290
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 1.18s
Val loss: 0.6164 score: 0.8992 time: 0.87s
Test loss: 0.6251 score: 0.8125 time: 0.86s
Epoch 29/1000, LR 0.000290
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 1.17s
Val loss: 0.5975 score: 0.8915 time: 0.87s
Test loss: 0.6083 score: 0.7656 time: 0.87s
Epoch 30/1000, LR 0.000290
Train loss: 0.5998;  Loss pred: 0.5998; Loss self: 0.0000; time: 1.25s
Val loss: 0.5760 score: 0.8605 time: 0.89s
Test loss: 0.5892 score: 0.7734 time: 0.92s
Epoch 31/1000, LR 0.000290
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 1.45s
Val loss: 0.5539 score: 0.8760 time: 1.19s
Test loss: 0.5691 score: 0.7812 time: 0.91s
Epoch 32/1000, LR 0.000290
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 1.21s
Val loss: 0.5302 score: 0.8760 time: 0.93s
Test loss: 0.5475 score: 0.7969 time: 0.87s
Epoch 33/1000, LR 0.000290
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 1.31s
Val loss: 0.5087 score: 0.8915 time: 0.88s
Test loss: 0.5272 score: 0.8047 time: 0.88s
Epoch 34/1000, LR 0.000290
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 1.25s
Val loss: 0.4875 score: 0.8992 time: 0.90s
Test loss: 0.5075 score: 0.8438 time: 1.09s
Epoch 35/1000, LR 0.000290
Train loss: 0.4897;  Loss pred: 0.4897; Loss self: 0.0000; time: 1.08s
Val loss: 0.4655 score: 0.8915 time: 0.90s
Test loss: 0.4883 score: 0.8281 time: 1.01s
Epoch 36/1000, LR 0.000290
Train loss: 0.4678;  Loss pred: 0.4678; Loss self: 0.0000; time: 1.09s
Val loss: 0.4473 score: 0.8915 time: 0.96s
Test loss: 0.4731 score: 0.8125 time: 0.99s
Epoch 37/1000, LR 0.000290
Train loss: 0.4474;  Loss pred: 0.4474; Loss self: 0.0000; time: 1.08s
Val loss: 0.4314 score: 0.8837 time: 0.90s
Test loss: 0.4595 score: 0.8203 time: 0.97s
Epoch 38/1000, LR 0.000289
Train loss: 0.4291;  Loss pred: 0.4291; Loss self: 0.0000; time: 1.12s
Val loss: 0.4180 score: 0.8837 time: 0.97s
Test loss: 0.4481 score: 0.8281 time: 0.86s
Epoch 39/1000, LR 0.000289
Train loss: 0.4173;  Loss pred: 0.4173; Loss self: 0.0000; time: 1.17s
Val loss: 0.4078 score: 0.8837 time: 0.95s
Test loss: 0.4401 score: 0.8281 time: 0.87s
Epoch 40/1000, LR 0.000289
Train loss: 0.4027;  Loss pred: 0.4027; Loss self: 0.0000; time: 1.10s
Val loss: 0.3959 score: 0.8837 time: 0.88s
Test loss: 0.4286 score: 0.8438 time: 1.05s
Epoch 41/1000, LR 0.000289
Train loss: 0.3845;  Loss pred: 0.3845; Loss self: 0.0000; time: 1.13s
Val loss: 0.3824 score: 0.8915 time: 0.97s
Test loss: 0.4142 score: 0.8906 time: 0.90s
Epoch 42/1000, LR 0.000289
Train loss: 0.3729;  Loss pred: 0.3729; Loss self: 0.0000; time: 1.12s
Val loss: 0.3699 score: 0.9147 time: 1.00s
Test loss: 0.4016 score: 0.8906 time: 0.93s
Epoch 43/1000, LR 0.000289
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 1.07s
Val loss: 0.3570 score: 0.9380 time: 1.01s
Test loss: 0.3894 score: 0.8906 time: 0.90s
Epoch 44/1000, LR 0.000289
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 1.09s
Val loss: 0.3426 score: 0.9380 time: 1.14s
Test loss: 0.3764 score: 0.8906 time: 0.87s
Epoch 45/1000, LR 0.000289
Train loss: 0.3246;  Loss pred: 0.3246; Loss self: 0.0000; time: 1.08s
Val loss: 0.3275 score: 0.9380 time: 0.99s
Test loss: 0.3627 score: 0.9141 time: 0.86s
Epoch 46/1000, LR 0.000289
Train loss: 0.3105;  Loss pred: 0.3105; Loss self: 0.0000; time: 1.06s
Val loss: 0.3128 score: 0.9380 time: 0.96s
Test loss: 0.3493 score: 0.9141 time: 0.85s
Epoch 47/1000, LR 0.000289
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 1.05s
Val loss: 0.2982 score: 0.9380 time: 0.99s
Test loss: 0.3361 score: 0.9141 time: 0.89s
Epoch 48/1000, LR 0.000289
Train loss: 0.2742;  Loss pred: 0.2742; Loss self: 0.0000; time: 1.08s
Val loss: 0.2858 score: 0.9380 time: 0.97s
Test loss: 0.3250 score: 0.9141 time: 0.86s
Epoch 49/1000, LR 0.000289
Train loss: 0.2551;  Loss pred: 0.2551; Loss self: 0.0000; time: 1.07s
Val loss: 0.2768 score: 0.9380 time: 1.15s
Test loss: 0.3172 score: 0.9219 time: 0.87s
Epoch 50/1000, LR 0.000289
Train loss: 0.2388;  Loss pred: 0.2388; Loss self: 0.0000; time: 1.09s
Val loss: 0.2638 score: 0.9380 time: 1.14s
Test loss: 0.3056 score: 0.9219 time: 0.90s
Epoch 51/1000, LR 0.000289
Train loss: 0.2184;  Loss pred: 0.2184; Loss self: 0.0000; time: 1.15s
Val loss: 0.2512 score: 0.9380 time: 0.98s
Test loss: 0.2945 score: 0.9297 time: 0.86s
Epoch 52/1000, LR 0.000289
Train loss: 0.1981;  Loss pred: 0.1981; Loss self: 0.0000; time: 1.18s
Val loss: 0.2388 score: 0.9380 time: 0.88s
Test loss: 0.2840 score: 0.9297 time: 0.92s
Epoch 53/1000, LR 0.000289
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 1.18s
Val loss: 0.2260 score: 0.9380 time: 0.88s
Test loss: 0.2735 score: 0.9297 time: 0.97s
Epoch 54/1000, LR 0.000289
Train loss: 0.1720;  Loss pred: 0.1720; Loss self: 0.0000; time: 1.06s
Val loss: 0.2100 score: 0.9302 time: 0.89s
Test loss: 0.2604 score: 0.9297 time: 1.03s
Epoch 55/1000, LR 0.000289
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 1.10s
Val loss: 0.1971 score: 0.9302 time: 0.91s
Test loss: 0.2513 score: 0.9141 time: 0.98s
Epoch 56/1000, LR 0.000289
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 1.09s
Val loss: 0.1900 score: 0.9225 time: 1.02s
Test loss: 0.2473 score: 0.9062 time: 0.85s
Epoch 57/1000, LR 0.000288
Train loss: 0.1521;  Loss pred: 0.1521; Loss self: 0.0000; time: 1.10s
Val loss: 0.1851 score: 0.9302 time: 1.00s
Test loss: 0.2456 score: 0.9062 time: 0.87s
Epoch 58/1000, LR 0.000288
Train loss: 0.1414;  Loss pred: 0.1414; Loss self: 0.0000; time: 1.11s
Val loss: 0.1823 score: 0.9225 time: 0.99s
Test loss: 0.2451 score: 0.9141 time: 0.87s
Epoch 59/1000, LR 0.000288
Train loss: 0.1484;  Loss pred: 0.1484; Loss self: 0.0000; time: 1.23s
Val loss: 0.1808 score: 0.9225 time: 0.87s
Test loss: 0.2452 score: 0.9141 time: 0.86s
Epoch 60/1000, LR 0.000288
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 1.22s
Val loss: 0.1810 score: 0.9302 time: 0.86s
Test loss: 0.2463 score: 0.9219 time: 0.86s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1142;  Loss pred: 0.1142; Loss self: 0.0000; time: 1.21s
Val loss: 0.1838 score: 0.9302 time: 0.87s
Test loss: 0.2500 score: 0.9297 time: 0.89s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.1117;  Loss pred: 0.1117; Loss self: 0.0000; time: 1.23s
Val loss: 0.1907 score: 0.9535 time: 0.89s
Test loss: 0.2578 score: 0.9219 time: 0.88s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.1071;  Loss pred: 0.1071; Loss self: 0.0000; time: 1.20s
Val loss: 0.2010 score: 0.9535 time: 0.89s
Test loss: 0.2694 score: 0.9141 time: 1.09s
     INFO: Early stopping counter 4 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 1.11s
Val loss: 0.2053 score: 0.9535 time: 0.88s
Test loss: 0.2751 score: 0.9219 time: 0.95s
     INFO: Early stopping counter 5 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 1.09s
Val loss: 0.2024 score: 0.9535 time: 0.87s
Test loss: 0.2746 score: 0.9141 time: 0.95s
     INFO: Early stopping counter 6 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 1.09s
Val loss: 0.1918 score: 0.9457 time: 5.01s
Test loss: 0.2672 score: 0.9219 time: 0.96s
     INFO: Early stopping counter 7 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 1.08s
Val loss: 0.1851 score: 0.9380 time: 0.87s
Test loss: 0.2640 score: 0.9219 time: 0.95s
     INFO: Early stopping counter 8 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.1187;  Loss pred: 0.1187; Loss self: 0.0000; time: 1.21s
Val loss: 0.1854 score: 0.9380 time: 0.90s
Test loss: 0.2672 score: 0.9219 time: 1.09s
     INFO: Early stopping counter 9 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 1.12s
Val loss: 0.1863 score: 0.9380 time: 1.00s
Test loss: 0.2708 score: 0.9219 time: 0.99s
     INFO: Early stopping counter 10 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 1.09s
Val loss: 0.1925 score: 0.9457 time: 1.01s
Test loss: 0.2781 score: 0.9141 time: 0.87s
     INFO: Early stopping counter 11 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 1.28s
Val loss: 0.1957 score: 0.9457 time: 0.88s
Test loss: 0.2830 score: 0.9141 time: 0.89s
     INFO: Early stopping counter 12 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 1.21s
Val loss: 0.1926 score: 0.9457 time: 0.86s
Test loss: 0.2840 score: 0.9141 time: 0.87s
     INFO: Early stopping counter 13 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 1.21s
Val loss: 0.1941 score: 0.9457 time: 0.89s
Test loss: 0.2881 score: 0.9062 time: 0.89s
     INFO: Early stopping counter 14 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 1.16s
Val loss: 0.2012 score: 0.9457 time: 0.91s
Test loss: 0.2949 score: 0.8984 time: 1.00s
     INFO: Early stopping counter 15 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0984;  Loss pred: 0.0984; Loss self: 0.0000; time: 1.09s
Val loss: 0.2088 score: 0.9380 time: 1.00s
Test loss: 0.3019 score: 0.8984 time: 0.86s
     INFO: Early stopping counter 16 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 1.15s
Val loss: 0.2184 score: 0.9380 time: 0.97s
Test loss: 0.3109 score: 0.9062 time: 0.88s
     INFO: Early stopping counter 17 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 1.10s
Val loss: 0.2315 score: 0.9457 time: 0.89s
Test loss: 0.3243 score: 0.9141 time: 1.10s
     INFO: Early stopping counter 18 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0791;  Loss pred: 0.0791; Loss self: 0.0000; time: 1.07s
Val loss: 0.2345 score: 0.9457 time: 0.88s
Test loss: 0.3295 score: 0.9062 time: 0.98s
     INFO: Early stopping counter 19 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 1.10s
Val loss: 0.2207 score: 0.9380 time: 1.02s
Test loss: 0.3235 score: 0.8984 time: 0.85s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 058,   Train_Loss: 0.1484,   Val_Loss: 0.1808,   Val_Precision: 0.9365,   Val_Recall: 0.9077,   Val_accuracy: 0.9219,   Val_Score: 0.9225,   Val_Loss: 0.1808,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9141,   Test_loss: 0.2452


[1.591574443038553, 1.3046091960277408, 1.1257583780679852, 1.1057114228606224, 1.0662058137822896, 1.0948089859448373, 0.9656337210908532, 1.2427639099769294, 0.9660250039305538, 1.0541200011502951, 1.0382186118513346, 1.044285950018093, 1.0359894479624927, 0.9265783170703799, 0.9383665770292282, 1.1075081781018525, 1.0147762440610677, 1.2383585250936449, 1.1719125730451196, 1.0025742698926479, 1.0232970130164176, 0.9593146038241684, 1.0652705451939255, 1.1353493349161, 0.972297488944605, 0.9627713339868933, 0.9453859231434762, 1.2598694718908519, 1.092100433073938, 1.2894688011147082, 1.094902864890173, 1.0846578530035913, 1.1746602521743625, 0.9411124300677329, 0.9273792090825737, 0.9175965110771358, 0.9254553199280053, 5.1169169140048325, 1.053045239066705, 1.0654243661556393, 1.0062620309181511, 1.033222194062546, 1.0243710859213024, 1.1378193648997694, 1.2103543519042432, 0.9335646869149059, 0.9410089689772576, 0.9317908538505435, 0.9582162350416183, 1.0677358990069479, 1.02981415996328, 0.9702250889968127, 0.9157468029297888, 0.9359962109010667, 0.9523978598881513, 0.9095096420496702, 0.9389564958401024, 1.0450404770672321, 1.0222833959851414, 1.1837362558580935, 1.0630543378647417, 0.9153309038374573, 0.9062267849221826, 0.9187495170626789, 0.9944039369001985, 0.9064482781104743, 1.0204656668938696, 1.0076769678853452, 0.980054748011753, 0.9508741700556129, 0.9576431110035628, 0.9707367229275405, 1.6041485290043056, 1.084830524167046, 0.9650675228331238, 0.9469699009787291, 0.9978566148784012, 0.9720934771467, 0.9576006201095879, 1.0677173340227455, 1.0790453769732267, 1.1414619709830731, 0.9424952559638768, 0.9503262080252171, 0.943746532779187, 0.965190518181771, 0.9321661130525172, 0.9713619479443878, 1.075898976996541, 1.0319170409347862, 1.1912668670993298, 1.083729743026197, 0.990277815843001, 0.935744411079213, 0.9113332540728152, 1.226555695058778, 0.9405594479758292, 1.0236791530624032, 1.0378840730991215, 1.023774344008416, 1.014870150014758, 0.9241385799832642, 0.9165124548599124, 1.0768631740938872, 1.0059144098777324, 0.9942065561190248, 0.95054029696621, 0.9552834948990494, 1.1468128110282123, 1.0672801309265196, 1.4072679409291595, 1.2501652408391237, 1.0068607989232987, 0.9259075880981982, 0.9481218450237066, 0.9327199719846249, 1.0016286680474877, 0.9598964299075305, 1.0082713270094246, 0.9271092910785228, 1.084901344962418, 1.0637465219479054, 1.0799192991107702, 1.276496275793761, 1.0998962000012398, 1.0682499669492245, 0.9794897141400725, 0.9153995639644563, 0.961650688899681, 0.9855980358552188, 0.9683415058534592, 0.9329992739949375, 1.0742661689873785, 1.0959308051969856, 1.0579434069804847, 0.9442764699924737, 0.9640111299231648, 1.0506846881471574, 0.9336461839266121, 0.9777939640916884, 0.9791233560536057, 1.0270990820135921, 1.1240323500242084, 1.076739378971979, 1.064490769058466, 1.035449534887448, 0.9552641920745373, 0.9790447012055665, 0.9361885630059987, 1.0525379690807313, 0.8616784510668367, 0.8829292128793895, 5.068715254077688, 0.8644057079218328, 0.8553566378541291, 0.9530466520227492, 0.9594662499148399, 0.983355016913265, 0.8720861179754138, 0.8876248400192708, 0.8925856398418546, 0.9235924498643726, 1.1045655040070415, 1.0275747389532626, 0.897150939097628, 0.9162938098888844, 0.8767622550949454, 0.8750483458861709, 0.99426438100636, 1.1420208481140435, 1.1358023840002716, 0.8852855050936341, 0.9081933361012489, 0.9699516289401799, 0.891085762064904, 0.8733197781257331, 0.8625530318822712, 0.8814767319709063, 0.92163967480883, 0.9116449400316924, 0.8797032439615577, 0.8866129550151527, 1.0953278259839863, 1.010071326047182, 0.994164276169613, 0.9760659348685294, 0.8648109720088542, 0.8762673500459641, 1.0516329109668732, 0.9087601120118052, 0.9326222189702094, 0.9049700829200447, 0.8757068018894643, 0.8656042490620166, 0.8582754309754819, 0.8998362880665809, 0.8648271900601685, 0.8744696038775146, 0.9060785400215536, 0.8648242889903486, 0.9267721869982779, 0.975919954944402, 1.0350434021092951, 0.9874872239306569, 0.8546886020340025, 0.8711075249593705, 0.8792135789990425, 0.867821856867522, 0.8628919089678675, 0.8902764930389822, 0.8840856899041682, 1.0985874878242612, 0.9527043059933931, 0.9538640058599412, 0.9674829528667033, 0.9576367139816284, 1.0958978931885213, 0.9978448720648885, 0.8792865800205618, 0.8967288671992719, 0.876133891986683, 0.8994029629975557, 1.008804464014247, 0.8628178669605404, 0.888317849021405, 1.1098129490856081, 0.9848725700285286, 0.8566919709555805]
[0.012337786380143823, 0.010113249581610395, 0.008726809132309962, 0.008571406378764515, 0.008265161347149531, 0.008486891363913467, 0.007485532721634521, 0.009633828759511081, 0.007488565921942278, 0.008171472877134071, 0.008048206293421198, 0.008095239922620876, 0.008030925953197618, 0.007182777651708371, 0.007274159511854482, 0.008585334713967848, 0.0078664825121013, 0.009599678489098023, 0.009084593589497052, 0.007771893565059286, 0.007932534984623393, 0.0074365473164664225, 0.008257911203053686, 0.008801157635008527, 0.007537189836779884, 0.007463343674317002, 0.007328573047623847, 0.009766430014657766, 0.008465894830030527, 0.009995882179183785, 0.00848761910767576, 0.008408200410880553, 0.009105893427708237, 0.0072954451943235115, 0.007188986116919175, 0.0071131512486599675, 0.007174072247503918, 0.039666022589184745, 0.008163141388113993, 0.008259103613609606, 0.007800480859830629, 0.008009474372577876, 0.007940861131172887, 0.00882030515426178, 0.009382591875226692, 0.007236935557479891, 0.007294643170366338, 0.007223184913570105, 0.007428032829779987, 0.008277022472922077, 0.007983055503591317, 0.007521124720905525, 0.007098812425812316, 0.007255784580628423, 0.007382929146419777, 0.007050462341470312, 0.007278732525892267, 0.008101088969513427, 0.00792467748825691, 0.009176250045411578, 0.008240731301277067, 0.007095588401840754, 0.007025013836606067, 0.007122089279555651, 0.007708557650389135, 0.0070267308380656925, 0.007910586565068756, 0.007811449363452289, 0.007597323627998085, 0.007371117597330333, 0.007423590007779556, 0.007525090875407291, 0.012435259914762058, 0.008409538947031364, 0.007481143587853673, 0.007340851945571544, 0.0077353225959565985, 0.007535608349974418, 0.007423260621004558, 0.008276878558315857, 0.008364692844753696, 0.008848542410721498, 0.007306164774913774, 0.007366869829652845, 0.007315864595187496, 0.007482097040168768, 0.007226093899631916, 0.007529937580964247, 0.008340302147260007, 0.007999356906471211, 0.009234626876738991, 0.008401005759892999, 0.0076765722158372175, 0.007253832644024907, 0.007064598868781513, 0.009508183682626185, 0.0072911585114405365, 0.007935497310561266, 0.008045612969760632, 0.00793623522487144, 0.007867210465230683, 0.0071638649611105755, 0.007104747712092344, 0.008347776543363467, 0.0077977861230831965, 0.007707027566814145, 0.007368529433846589, 0.00740529841007015, 0.008890021790916374, 0.008273489387027284, 0.010909053805652399, 0.009691203417357548, 0.007805122472273634, 0.007177578202311614, 0.007349781744369818, 0.007230387379725774, 0.00776456331819758, 0.007441057596182407, 0.007816056798522672, 0.007186893729290875, 0.0084100879454451, 0.008246097069363608, 0.00837146743496721, 0.009895319967393497, 0.008526327131792556, 0.008281007495730422, 0.007592943520465678, 0.007096120650887258, 0.007454656503098302, 0.007640294851590844, 0.007506523301189606, 0.0072325525115886635, 0.00832764472083239, 0.008495587637185935, 0.008201111682019261, 0.007319972635600571, 0.0074729544955284095, 0.008144842543776414, 0.007237567317260559, 0.007579798171253398, 0.007590103535299268, 0.007962008387702264, 0.008713429069955104, 0.00834681689125565, 0.008251866426809813, 0.008026740580522853, 0.007405148775771607, 0.00758949380779509, 0.007257275682217045, 0.008222952883443213, 0.006731862898959662, 0.006897884475620231, 0.039599337922481936, 0.006753169593139319, 0.006682473733235383, 0.007445676968927728, 0.0074958300774596864, 0.007682461069634883, 0.00681317279668292, 0.006934569062650553, 0.006973325311264489, 0.007215566014565411, 0.008629418000055011, 0.008027927648072364, 0.007008991711700219, 0.00715854538975691, 0.006849705117929261, 0.00683631520223571, 0.007767690476612188, 0.008922037875890965, 0.008873456125002122, 0.006916293008544017, 0.007095260438291007, 0.007577747101095156, 0.006961607516132062, 0.00682281076660729, 0.0067386955615802435, 0.006886536968522705, 0.0072003099594439846, 0.007122226093997597, 0.00687268159344967, 0.00692666371105588, 0.008557248640499893, 0.00789118223474361, 0.007766908407575102, 0.007625515116160386, 0.006756335718819173, 0.006845838672234095, 0.008215882116928697, 0.007099688375092228, 0.007286111085704761, 0.007070078772812849, 0.0068414593897614395, 0.006762533195797005, 0.006705276804495952, 0.007029971000520163, 0.0067564624223450664, 0.006831793780293083, 0.007078738593918388, 0.006756439757737098, 0.007240407710924046, 0.0076243746480031405, 0.008086276578978868, 0.007714743936958257, 0.006677254703390645, 0.006805527538745082, 0.00686885608593002, 0.006779858256777516, 0.006741343038811465, 0.006955285101867048, 0.0069069194523763144, 0.00858271474862704, 0.007443002390573383, 0.007452062545780791, 0.007558460569271119, 0.007481536827981472, 0.008561702290535322, 0.007795663063006941, 0.006869426406410639, 0.007005694274994312, 0.006844796031145961, 0.007026585648418404, 0.007881284875111305, 0.006740764585629222, 0.006939983195479726, 0.008670413664731313, 0.0076943169533478795, 0.0066929060230904724]
[81.05181668645028, 98.88018603025161, 114.58942035269456, 116.66696873426508, 120.98977358075139, 117.8287734719961, 133.5910264756204, 103.80089006800556, 133.53691620312722, 122.37695884645997, 124.25128824262676, 123.52938387973619, 124.51864278512454, 139.22190668983848, 137.47292706055313, 116.47769519958926, 127.12161991864383, 104.17015540006477, 110.07647069166941, 128.668771854491, 126.06310617456121, 134.47100615977305, 121.09599817811203, 113.62141680343068, 132.6754429243924, 133.98820202280362, 136.45221156992238, 102.3915595052817, 118.12100434472255, 100.04119517159567, 117.81867062055743, 118.93151341944221, 109.81898788284843, 137.07182678557942, 139.1016735512278, 140.58466705433656, 139.39084602164846, 25.210493382632666, 122.50185957284262, 121.07851490713459, 128.19722501334525, 124.85213804088205, 125.93092656845106, 113.37476226849384, 106.58035789026997, 138.18003380815358, 137.08689741842161, 138.44308459019413, 134.62514543431539, 120.81639300502772, 125.26531972001604, 132.95883755529618, 140.8686326692975, 137.82107074537623, 135.44759541474573, 141.8346700638442, 137.38655685488519, 123.44019473965395, 126.18810058602867, 108.97697807396088, 121.3484536069062, 140.9326391790958, 142.348474075479, 140.40823707034326, 129.72595462777903, 142.31369082514598, 126.41287618490378, 128.01721594442336, 131.6252997719807, 135.66463793254084, 134.70571501821212, 132.88876062189422, 80.41649365228683, 118.91258323418649, 133.66940338153546, 136.22397065278804, 129.27709059253962, 132.70328732031234, 134.71169221385554, 120.8184937056121, 119.5501160125917, 113.01296344450266, 136.870715458481, 135.74286272506646, 136.6892439012359, 133.6523697342268, 138.38735198984034, 132.8032256904771, 119.89973292856354, 125.01004914420479, 108.28807848413345, 119.03336678735197, 130.26647465608954, 137.85815707007183, 141.55085356919605, 105.17255801728447, 137.15241527541923, 126.01604674090319, 124.29133787052541, 126.00432971871734, 127.10985735280923, 139.58945421620223, 140.7509514092937, 119.7923776235968, 128.2415270457054, 129.7517092459772, 135.71228953861566, 135.03844742301573, 112.48566353591818, 120.86798607223525, 91.66697843967565, 103.18635951949351, 128.1209876657707, 139.32275926689863, 136.05846197623947, 138.30517612431532, 128.79024344567188, 134.38949868000543, 127.94175193161493, 139.1421715231998, 118.90482079222483, 121.26949168658949, 119.45337036408321, 101.0578741561813, 117.28379459793987, 120.7582532095988, 131.70122987279507, 140.92206843678252, 134.14434314771987, 130.88500109282856, 133.21746431420837, 138.26377318349333, 120.08197197683101, 117.70816130751356, 121.9346887072, 136.61253255736446, 133.81588240613132, 122.77708189265287, 138.1679722156288, 131.92963419428878, 131.7505084547666, 125.59645146123583, 114.76538019321393, 119.80615041976382, 121.1847051657381, 124.58357037556846, 135.04117611678927, 131.76109307486496, 137.79275361556986, 121.61081477354496, 148.54729144209688, 144.97198431408697, 25.252947459817626, 148.07861496858013, 149.64518229626327, 134.3061220857682, 133.40750652913638, 130.16662120847246, 146.7744955018406, 144.205067534186, 143.40360665300264, 138.58926631415892, 115.88267018628893, 124.56514854616512, 142.67387395118251, 139.69318423696663, 145.99168617966882, 146.27763208942818, 128.73839438001673, 112.08201690133924, 112.69566062116084, 144.58612420911805, 140.93915349509905, 131.96534361188662, 143.64498396134948, 146.56716039880143, 148.39667274796685, 145.21086644431668, 138.88290999033896, 140.40553989752877, 145.50361258596595, 144.3696477430926, 116.85999110358706, 126.72372405710773, 128.7513573643659, 131.13868175026604, 148.00922298970266, 146.07414049295096, 121.71547568087858, 140.85125250120709, 137.2474270893268, 141.44113978551152, 146.16764392353863, 147.87358095654332, 149.13627418475707, 142.24809745673315, 148.00644738181123, 146.37444164146012, 141.26810684309459, 148.0069438723043, 138.1137692689928, 131.15829771847646, 123.66631171132656, 129.62193018609463, 149.76214693326133, 146.93938042375433, 145.58464866491718, 147.49570892582386, 148.3383940325792, 143.77555849314706, 144.7823457179527, 116.5132512600361, 134.3543838258748, 134.1910368916831, 132.3020727349554, 133.6623775291635, 116.79920254941203, 128.27645216547882, 145.5725617886796, 142.74102761939494, 146.09639139715594, 142.31663143892473, 126.88286438648461, 148.35112357015421, 144.09256792600561, 115.33475087442542, 129.96605235567912, 149.41192907087117]
Elapsed: 1.0339226240094082~0.3983958751907999
Time per graph: 0.008035581432831885~0.003097544618645116
Speed: 129.42705570323017~16.05235078214186
Total Time: 0.8575
best val loss: 0.18084500005194384 test_score: 0.9141

Testing...
Test loss: 0.2578 score: 0.9219 time: 0.86s
test Score 0.9219
Epoch Time List: [5.460292680887505, 3.7824660879559815, 3.2989012498874217, 3.442309334874153, 3.2114545102231205, 3.359965565847233, 3.1925992879550904, 3.7858298420906067, 3.2037536310963333, 3.1010942701250315, 3.008759702090174, 2.96369590703398, 3.114729384193197, 3.0649560808669776, 3.013786273309961, 3.2057537250220776, 3.0761802489869297, 3.4621189220342785, 3.4511459439527243, 3.1717854391317815, 3.4578350400552154, 3.0600644561927766, 3.1610042660031468, 3.1486532150302082, 3.0498577018734068, 3.204407633980736, 3.0250618890859187, 3.8027085312642157, 3.233878026017919, 3.4855758021585643, 3.203221410047263, 3.1444422961212695, 3.2370991432107985, 3.395851780893281, 2.9904941259883344, 2.9114032650832087, 2.9221801720559597, 7.113861073041335, 4.234130458906293, 2.9456743223126978, 2.913202774943784, 3.0480846802238375, 2.947356719058007, 3.115980404894799, 3.068543071858585, 2.999263959703967, 3.1489693778567016, 2.937886595958844, 2.9956519051920623, 3.1167925829067826, 3.0501450789161026, 3.0498694707639515, 2.9937610160559416, 3.0338518258649856, 3.1066161890048534, 2.97718976973556, 2.990113138919696, 3.1012895219027996, 2.9867492550984025, 3.3168449837248772, 3.37742482800968, 3.0332591203041375, 2.9511483090464026, 2.9603495330084115, 3.074644629145041, 2.8823389201425016, 3.0127715307753533, 2.90601352811791, 3.2660880752373487, 3.036463968222961, 3.054431786062196, 3.117948374012485, 4.110639719991013, 3.25362038705498, 2.99383406015113, 3.0026860137004405, 3.038901256863028, 2.9610763790551573, 3.047266853041947, 3.1737492957618088, 3.086267138598487, 3.1414313009008765, 2.9790844791568816, 2.9769791211001575, 3.017546733142808, 3.0087035060860217, 2.928344320738688, 3.0103720847982913, 3.101353476056829, 2.9891032660380006, 3.1402138511184603, 3.193071232875809, 3.208869265858084, 3.0103896469809115, 2.959674126235768, 7.455201967153698, 2.9485629501286894, 3.0213504740968347, 2.9178467069286853, 2.8647170530166477, 2.861177642829716, 2.8699913800228387, 2.86377359717153, 3.0571727198548615, 3.009768303250894, 3.1617866759188473, 3.049594890791923, 3.007920235162601, 3.252883919980377, 2.980200698133558, 3.6993720100726932, 3.438953216187656, 3.2248892101924866, 3.0878220291342586, 3.0063461752142757, 2.976022647926584, 3.0439277107361704, 2.9945005329791456, 3.0266422340646386, 2.8882114260923117, 3.080686065601185, 2.9893160769715905, 3.091349651804194, 3.285513855982572, 3.038478735135868, 2.9786459628958255, 2.9571213112212718, 2.978413392789662, 2.999915729975328, 3.118635216029361, 2.9742779179941863, 3.0846605168189853, 3.1453649608884007, 3.096292518079281, 3.0001324710901827, 3.0970098450779915, 3.049516585888341, 3.0964221090544015, 2.944278731243685, 2.975443699164316, 3.034302191110328, 3.0141411758959293, 3.029596976004541, 3.1573145291768014, 3.118987030116841, 3.174129315884784, 3.135308442171663, 2.9886262577492744, 3.0393106408882886, 3.1505591690074652, 3.131274773972109, 3.0114728431217372, 7.295902655925602, 3.0954979902599007, 2.931389491073787, 2.9823904037475586, 2.895233831834048, 2.9816300130914897, 2.970668349182233, 3.0029401739593595, 3.101101784966886, 3.0560433291830122, 3.411838927073404, 3.505057671107352, 3.0122567312791944, 2.984517561038956, 2.9975359349045902, 3.043039011070505, 3.143748729256913, 3.1899137918371707, 3.2390538637991995, 3.0371994259767234, 3.11889143101871, 3.0116949039511383, 3.0037323988508433, 2.9132216742727906, 2.9091265748720616, 2.9180010808631778, 3.0647217659279704, 3.5391769020352513, 3.0180491299834102, 3.0730879581533372, 3.2410784238018095, 2.9916916680522263, 3.0371395882684737, 2.9516501610632986, 2.9490308361127973, 2.992510166950524, 3.0260743349790573, 3.003177986945957, 3.0427459119819105, 2.98258533119224, 3.107627034885809, 2.9315436019096524, 2.8751084918621927, 2.933645980199799, 2.9110938939265907, 3.091468649916351, 3.132340047741309, 2.9945727332960814, 2.989601492881775, 3.025264122057706, 2.9786844672635198, 2.9979497781023383, 2.9574040330480784, 2.964085196144879, 2.977313117356971, 2.9540892459917814, 2.9431687619071454, 2.9734632580075413, 2.9988692249171436, 3.1769967088475823, 2.9390452848747373, 2.906561190262437, 7.061268551973626, 2.898845663992688, 3.2030757069587708, 3.113020504824817, 2.974149770103395, 3.0430657791439444, 2.9376265262253582, 2.992965109180659, 3.0715572230983526, 2.9465482379309833, 3.0012509843800217, 3.0913050977978855, 2.922886378131807, 2.9703893740661442]
Total Epoch List: [68, 81, 79]
Total Time List: [1.0081280339509249, 0.936827037949115, 0.8575064928736538]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71389351fe20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 1.00s
Epoch 2/1000, LR 0.000015
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.92s
Epoch 3/1000, LR 0.000045
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.91s
Epoch 4/1000, LR 0.000075
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 1.02s
Epoch 5/1000, LR 0.000105
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.90s
Epoch 6/1000, LR 0.000135
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.95s
Epoch 7/1000, LR 0.000165
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.96s
Epoch 8/1000, LR 0.000195
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.95s
Epoch 9/1000, LR 0.000225
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 1.04s
Epoch 10/1000, LR 0.000255
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 1.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.05s
Val loss: 0.6917 score: 0.5116 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 1.05s
Epoch 12/1000, LR 0.000285
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 1.09s
Val loss: 0.6913 score: 0.5426 time: 1.00s
Test loss: 0.6905 score: 0.5194 time: 1.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 1.04s
Val loss: 0.6907 score: 0.6434 time: 1.10s
Test loss: 0.6897 score: 0.6512 time: 1.15s
Epoch 14/1000, LR 0.000285
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 1.03s
Val loss: 0.6900 score: 0.7364 time: 1.09s
Test loss: 0.6887 score: 0.7907 time: 1.05s
Epoch 15/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.99s
Val loss: 0.6891 score: 0.6357 time: 1.08s
Test loss: 0.6875 score: 0.7907 time: 0.98s
Epoch 16/1000, LR 0.000285
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 1.01s
Val loss: 0.6881 score: 0.6357 time: 1.09s
Test loss: 0.6860 score: 0.7674 time: 0.91s
Epoch 17/1000, LR 0.000285
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 1.07s
Val loss: 0.6868 score: 0.6047 time: 0.97s
Test loss: 0.6842 score: 0.7054 time: 0.94s
Epoch 18/1000, LR 0.000285
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 1.08s
Val loss: 0.6852 score: 0.5814 time: 0.95s
Test loss: 0.6819 score: 0.6589 time: 0.93s
Epoch 19/1000, LR 0.000285
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 1.10s
Val loss: 0.6833 score: 0.5581 time: 0.96s
Test loss: 0.6792 score: 0.6357 time: 0.95s
Epoch 20/1000, LR 0.000285
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 1.07s
Val loss: 0.6810 score: 0.5504 time: 0.96s
Test loss: 0.6759 score: 0.6357 time: 1.10s
Epoch 21/1000, LR 0.000285
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 1.09s
Val loss: 0.6784 score: 0.5349 time: 0.93s
Test loss: 0.6722 score: 0.6279 time: 0.90s
Epoch 22/1000, LR 0.000285
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 1.07s
Val loss: 0.6751 score: 0.5504 time: 0.96s
Test loss: 0.6678 score: 0.6279 time: 0.91s
Epoch 23/1000, LR 0.000285
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 1.08s
Val loss: 0.6711 score: 0.5659 time: 0.99s
Test loss: 0.6626 score: 0.6512 time: 0.93s
Epoch 24/1000, LR 0.000285
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 1.12s
Val loss: 0.6664 score: 0.5736 time: 0.98s
Test loss: 0.6564 score: 0.6667 time: 1.05s
Epoch 25/1000, LR 0.000285
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.99s
Val loss: 0.6607 score: 0.5736 time: 0.97s
Test loss: 0.6489 score: 0.6744 time: 1.03s
Epoch 26/1000, LR 0.000285
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.99s
Val loss: 0.6539 score: 0.5736 time: 1.00s
Test loss: 0.6399 score: 0.6744 time: 1.02s
Epoch 27/1000, LR 0.000285
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 1.03s
Val loss: 0.6460 score: 0.5814 time: 0.96s
Test loss: 0.6296 score: 0.6744 time: 1.04s
Epoch 28/1000, LR 0.000285
Train loss: 0.6179;  Loss pred: 0.6179; Loss self: 0.0000; time: 1.02s
Val loss: 0.6369 score: 0.5969 time: 1.07s
Test loss: 0.6179 score: 0.6822 time: 0.93s
Epoch 29/1000, LR 0.000285
Train loss: 0.6076;  Loss pred: 0.6076; Loss self: 0.0000; time: 1.03s
Val loss: 0.6270 score: 0.6279 time: 1.27s
Test loss: 0.6048 score: 0.7132 time: 1.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 1.22s
Val loss: 0.6150 score: 0.6589 time: 1.10s
Test loss: 0.5896 score: 0.7442 time: 0.92s
Epoch 31/1000, LR 0.000285
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 1.08s
Val loss: 0.6013 score: 0.6899 time: 1.02s
Test loss: 0.5726 score: 0.7752 time: 1.03s
Epoch 32/1000, LR 0.000285
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 1.37s
Val loss: 0.5862 score: 0.7054 time: 1.21s
Test loss: 0.5535 score: 0.8062 time: 1.30s
Epoch 33/1000, LR 0.000285
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 1.22s
Val loss: 0.5691 score: 0.7364 time: 1.13s
Test loss: 0.5326 score: 0.8450 time: 1.03s
Epoch 34/1000, LR 0.000285
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.98s
Val loss: 0.5509 score: 0.7752 time: 0.99s
Test loss: 0.5100 score: 0.8760 time: 1.04s
Epoch 35/1000, LR 0.000285
Train loss: 0.4950;  Loss pred: 0.4950; Loss self: 0.0000; time: 0.99s
Val loss: 0.5311 score: 0.7907 time: 1.09s
Test loss: 0.4857 score: 0.8915 time: 0.91s
Epoch 36/1000, LR 0.000285
Train loss: 0.4688;  Loss pred: 0.4688; Loss self: 0.0000; time: 1.04s
Val loss: 0.5108 score: 0.8062 time: 1.06s
Test loss: 0.4606 score: 0.8915 time: 0.94s
Epoch 37/1000, LR 0.000285
Train loss: 0.4452;  Loss pred: 0.4452; Loss self: 0.0000; time: 1.12s
Val loss: 0.4902 score: 0.8140 time: 1.06s
Test loss: 0.4355 score: 0.8992 time: 0.92s
Epoch 38/1000, LR 0.000284
Train loss: 0.4170;  Loss pred: 0.4170; Loss self: 0.0000; time: 1.09s
Val loss: 0.4699 score: 0.8217 time: 0.97s
Test loss: 0.4108 score: 0.9070 time: 1.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3910;  Loss pred: 0.3910; Loss self: 0.0000; time: 1.08s
Val loss: 0.4505 score: 0.8217 time: 1.08s
Test loss: 0.3871 score: 0.9225 time: 1.00s
Epoch 40/1000, LR 0.000284
Train loss: 0.3682;  Loss pred: 0.3682; Loss self: 0.0000; time: 1.09s
Val loss: 0.4306 score: 0.8295 time: 5.20s
Test loss: 0.3639 score: 0.9225 time: 1.52s
Epoch 41/1000, LR 0.000284
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 1.07s
Val loss: 0.4112 score: 0.8450 time: 0.94s
Test loss: 0.3420 score: 0.9225 time: 0.96s
Epoch 42/1000, LR 0.000284
Train loss: 0.3212;  Loss pred: 0.3212; Loss self: 0.0000; time: 1.06s
Val loss: 0.3949 score: 0.8450 time: 0.94s
Test loss: 0.3222 score: 0.9225 time: 1.00s
Epoch 43/1000, LR 0.000284
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 0.96s
Val loss: 0.3778 score: 0.8527 time: 0.94s
Test loss: 0.3030 score: 0.9225 time: 1.04s
Epoch 44/1000, LR 0.000284
Train loss: 0.2741;  Loss pred: 0.2741; Loss self: 0.0000; time: 0.96s
Val loss: 0.3610 score: 0.8450 time: 0.99s
Test loss: 0.2848 score: 0.9380 time: 1.04s
Epoch 45/1000, LR 0.000284
Train loss: 0.2610;  Loss pred: 0.2610; Loss self: 0.0000; time: 0.98s
Val loss: 0.3456 score: 0.8605 time: 1.03s
Test loss: 0.2682 score: 0.9380 time: 1.04s
Epoch 46/1000, LR 0.000284
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 1.00s
Val loss: 0.3323 score: 0.8605 time: 1.05s
Test loss: 0.2532 score: 0.9380 time: 0.93s
Epoch 47/1000, LR 0.000284
Train loss: 0.2246;  Loss pred: 0.2246; Loss self: 0.0000; time: 1.00s
Val loss: 0.3206 score: 0.8682 time: 1.08s
Test loss: 0.2397 score: 0.9380 time: 0.94s
Epoch 48/1000, LR 0.000284
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 0.97s
Val loss: 0.3115 score: 0.8682 time: 1.09s
Test loss: 0.2279 score: 0.9380 time: 0.91s
Epoch 49/1000, LR 0.000284
Train loss: 0.1818;  Loss pred: 0.1818; Loss self: 0.0000; time: 0.99s
Val loss: 0.3049 score: 0.8682 time: 1.10s
Test loss: 0.2178 score: 0.9380 time: 0.93s
Epoch 50/1000, LR 0.000284
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 1.09s
Val loss: 0.3003 score: 0.8682 time: 0.95s
Test loss: 0.2094 score: 0.9380 time: 0.92s
Epoch 51/1000, LR 0.000284
Train loss: 0.1516;  Loss pred: 0.1516; Loss self: 0.0000; time: 1.26s
Val loss: 0.2970 score: 0.8682 time: 1.00s
Test loss: 0.2026 score: 0.9457 time: 0.95s
Epoch 52/1000, LR 0.000284
Train loss: 0.1346;  Loss pred: 0.1346; Loss self: 0.0000; time: 1.22s
Val loss: 0.2952 score: 0.8682 time: 1.18s
Test loss: 0.1973 score: 0.9457 time: 0.97s
Epoch 53/1000, LR 0.000284
Train loss: 0.1267;  Loss pred: 0.1267; Loss self: 0.0000; time: 1.17s
Val loss: 0.2955 score: 0.8605 time: 0.99s
Test loss: 0.1938 score: 0.9457 time: 0.93s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 1.09s
Val loss: 0.2966 score: 0.8605 time: 0.96s
Test loss: 0.1915 score: 0.9457 time: 1.02s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1103;  Loss pred: 0.1103; Loss self: 0.0000; time: 1.01s
Val loss: 0.2959 score: 0.8605 time: 0.95s
Test loss: 0.1901 score: 0.9457 time: 1.02s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0956;  Loss pred: 0.0956; Loss self: 0.0000; time: 1.03s
Val loss: 0.2975 score: 0.8605 time: 0.95s
Test loss: 0.1897 score: 0.9457 time: 1.06s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0856;  Loss pred: 0.0856; Loss self: 0.0000; time: 0.97s
Val loss: 0.3003 score: 0.8682 time: 0.96s
Test loss: 0.1898 score: 0.9457 time: 1.07s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0831;  Loss pred: 0.0831; Loss self: 0.0000; time: 0.95s
Val loss: 0.3034 score: 0.8682 time: 0.95s
Test loss: 0.1905 score: 0.9457 time: 1.01s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.95s
Val loss: 0.3082 score: 0.8682 time: 0.94s
Test loss: 0.1913 score: 0.9457 time: 1.00s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.95s
Val loss: 0.3116 score: 0.8682 time: 1.05s
Test loss: 0.1927 score: 0.9457 time: 0.92s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.97s
Val loss: 0.3137 score: 0.8682 time: 1.15s
Test loss: 0.1947 score: 0.9535 time: 0.91s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0622;  Loss pred: 0.0622; Loss self: 0.0000; time: 0.97s
Val loss: 0.3115 score: 0.8682 time: 1.24s
Test loss: 0.1981 score: 0.9535 time: 0.91s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.96s
Val loss: 0.3116 score: 0.8605 time: 1.07s
Test loss: 0.2023 score: 0.9535 time: 0.90s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.97s
Val loss: 0.3140 score: 0.8682 time: 1.09s
Test loss: 0.2071 score: 0.9535 time: 0.91s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 1.07s
Val loss: 0.3169 score: 0.8682 time: 0.99s
Test loss: 0.2109 score: 0.9535 time: 0.91s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 1.09s
Val loss: 0.3183 score: 0.8837 time: 0.94s
Test loss: 0.2136 score: 0.9535 time: 0.93s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 1.12s
Val loss: 0.3202 score: 0.9070 time: 0.96s
Test loss: 0.2160 score: 0.9535 time: 0.96s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 1.13s
Val loss: 0.3237 score: 0.8992 time: 0.98s
Test loss: 0.2190 score: 0.9535 time: 1.07s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.99s
Val loss: 0.3280 score: 0.8992 time: 0.97s
Test loss: 0.2223 score: 0.9535 time: 1.04s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.99s
Val loss: 0.3318 score: 0.8992 time: 0.99s
Test loss: 0.2264 score: 0.9535 time: 1.04s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.98s
Val loss: 0.3344 score: 0.8915 time: 1.31s
Test loss: 0.2328 score: 0.9535 time: 0.92s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.97s
Val loss: 0.3449 score: 0.8915 time: 1.16s
Test loss: 0.2399 score: 0.9535 time: 1.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.1346,   Val_Loss: 0.2952,   Val_Precision: 0.9123,   Val_Recall: 0.8125,   Val_accuracy: 0.8595,   Val_Score: 0.8682,   Val_Loss: 0.2952,   Test_Precision: 0.9677,   Test_Recall: 0.9231,   Test_accuracy: 0.9449,   Test_Score: 0.9457,   Test_loss: 0.1973


[1.000802458031103, 0.9254461990203708, 0.9143370180390775, 1.0290164640173316, 0.9055836119223386, 0.9506509460043162, 0.9686355371959507, 0.9504107148386538, 1.0405861740000546, 1.0637568500824273, 1.0583128929138184, 1.1917587998323143, 1.1532152350991964, 1.0576077247969806, 0.9854708269704133, 0.9155615710187703, 0.9467514159623533, 0.9329841618891805, 0.9582161090802401, 1.107795518124476, 0.9060090139973909, 0.9126447509042919, 0.9358366611413658, 1.0540203040000051, 1.037081859074533, 1.0224305188748986, 1.046191070927307, 0.9306777799502015, 1.2011076679918915, 0.9266112269833684, 1.0411044310312718, 1.3066127069760114, 1.0313366858754307, 1.04039369802922, 0.9139482299797237, 0.9414774398319423, 0.9280127577949315, 1.067389033967629, 1.0059906269889325, 1.5223205911461264, 0.9634214409161359, 1.005177672021091, 1.041308545973152, 1.0447143248748034, 1.0412509059533477, 0.9358627130277455, 0.9456453928723931, 0.9147601609583944, 0.9315294148400426, 0.9225593069568276, 0.9515979089774191, 0.9730778990779072, 0.9309431069996208, 1.0237465559039265, 1.0283818100579083, 1.067288015037775, 1.0723670371808112, 1.0127135929651558, 1.003797739977017, 0.9201732149813324, 0.9137983380351216, 0.911943320883438, 0.9047713167965412, 0.9179190329741687, 0.9108253170270473, 0.9321907709818333, 0.9626299000810832, 1.070626461878419, 1.049307068809867, 1.0422393400222063, 0.9283342729322612, 1.065684932982549]
[0.007758158589388395, 0.007174001542793572, 0.0070878838607680425, 0.007976871814087842, 0.007020027999397974, 0.007369387178328033, 0.007508802613922099, 0.007367524921229875, 0.008066559488372517, 0.008246177132421918, 0.008203975914060607, 0.009238440308777629, 0.00893965298526509, 0.008198509494550237, 0.007639308736204754, 0.007097376519525351, 0.007339158263274057, 0.007232435363482019, 0.007428031853335195, 0.00858756215600369, 0.007023325689902255, 0.007074765510885983, 0.007254547760785781, 0.008170700031007791, 0.008039394256391729, 0.007925817975774407, 0.00811000830176207, 0.007214556433722492, 0.009310912154975903, 0.0071830327673129335, 0.008070576984738542, 0.010128780674232646, 0.007994858030042098, 0.0080650674265831, 0.007084869999842819, 0.007298274727379398, 0.007193897347247531, 0.008274333596648288, 0.0077983769534025775, 0.011800934815086251, 0.007468383262915782, 0.007792074976907682, 0.008072159271109705, 0.008098560657944212, 0.008071712449250756, 0.007254749713393376, 0.007330584440871264, 0.007091164038437166, 0.007221158254573974, 0.0071516225345490516, 0.00737672797656914, 0.007543239527735715, 0.0072166132325552, 0.007936019813208732, 0.007971952015952778, 0.008273550504168798, 0.008312922768843498, 0.007850492968722138, 0.007781377829279202, 0.007133125697529709, 0.007083708046783888, 0.0070693280688638605, 0.007013731137957684, 0.007115651418404408, 0.007060661372302692, 0.007226285046370801, 0.007462247287450257, 0.008299429937042007, 0.008134163324107496, 0.008079374728854312, 0.007196389712653187, 0.008261123511492628]
[128.8965659154995, 139.39221981413147, 141.0858331828874, 125.36242568595799, 142.44957428741856, 135.69649358915623, 133.17702587439126, 135.73079299921366, 123.96858926552798, 121.26831426749843, 121.89211797734838, 108.24337946415908, 111.86116526539278, 121.97339048820106, 130.90189630126204, 140.89713251775976, 136.2554075177951, 138.2660127250076, 134.62516313133457, 116.44748321278645, 142.38268936292448, 141.34744090970847, 137.84456770764774, 122.38853417761047, 124.38748096039063, 126.16994271840984, 123.30443604881748, 138.6086600315122, 107.40086291820332, 139.21696202620626, 123.90687826793545, 98.72856685938258, 125.08039495414708, 123.99152382829692, 141.14585024456136, 137.01868418963608, 139.0067096776981, 120.85565421305631, 128.23181105187297, 84.73904954729565, 133.897788155235, 128.33552076482383, 123.88259032239424, 123.47873186812014, 123.88944803067449, 137.84073048775855, 136.41477130043762, 141.02057075249832, 138.48193942662692, 139.8284088916971, 135.56145803075856, 132.56903699307213, 138.56915533298272, 126.00774992214578, 125.43979165941879, 120.86709321422882, 120.29463376563055, 127.38053571720792, 128.5119450487641, 140.19099654255533, 141.16900264601043, 141.45615966026514, 142.5774641671235, 140.53527093999173, 141.62979178165432, 138.3836914241602, 134.00788817086837, 120.49020325321393, 122.93827406148412, 123.77195433560537, 138.95856671599248, 121.04891043074588]
Elapsed: 1.0023428349619887~0.0999735425457734
Time per graph: 0.00777009949582937~0.00077498870190522
Speed: 129.77191329155818~10.940268878888162
Total Time: 1.0662
best val loss: 0.2952202139494493 test_score: 0.9457

Testing...
Test loss: 0.2160 score: 0.9535 time: 1.05s
test Score 0.9535
Epoch Time List: [3.000502449925989, 2.921781881712377, 2.9060520550701767, 2.941703285789117, 2.914627072168514, 3.0355552840046585, 3.013128597289324, 3.0044197398237884, 3.1015319048892707, 3.1157271361444145, 3.1016855610068887, 3.279422693187371, 3.288023378001526, 3.172935737995431, 3.0535828080028296, 3.005910911830142, 2.985895082121715, 2.959226841107011, 3.021194730652496, 3.1303899090271443, 2.923301527975127, 2.9398232728708535, 2.9970009790267795, 3.145837256219238, 2.995490266941488, 3.0136252290103585, 3.0292423798236996, 3.00872243498452, 3.493579346453771, 3.244630339089781, 3.12884495803155, 3.8716683220118284, 3.378297003218904, 3.005811606068164, 2.9835051079280674, 3.036473515909165, 3.101095614954829, 3.122124592307955, 3.161435002926737, 7.803203632123768, 2.9701363760977983, 3.0090711081866175, 2.9380107577890158, 2.9832899270113558, 3.045563711086288, 2.9795174601022154, 3.026310984045267, 2.9706010590307415, 3.0135401859879494, 2.963608935009688, 3.2102142099756747, 3.3651157738640904, 3.0852102991193533, 3.064302099868655, 2.981659283163026, 3.036275235703215, 2.996517940191552, 2.907128264894709, 2.8920116322115064, 2.9075958388857543, 3.0324710747227073, 3.1153642449062318, 2.930597990984097, 2.969655022956431, 2.9635789450258017, 2.964834503363818, 3.0305791050195694, 3.172378211747855, 3.0060180141590536, 3.0262716717552394, 3.2154631323646754, 3.1903589949943125]
Total Epoch List: [72]
Total Time List: [1.0661689219996333]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71389351e1a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5039 time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.4961 time: 1.01s
Epoch 2/1000, LR 0.000015
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.4961 time: 1.10s
Epoch 3/1000, LR 0.000045
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.4961 time: 1.00s
Epoch 4/1000, LR 0.000075
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5039 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.4961 time: 0.85s
Epoch 5/1000, LR 0.000105
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5039 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.4961 time: 0.85s
Epoch 6/1000, LR 0.000135
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5039 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.4961 time: 0.92s
Epoch 7/1000, LR 0.000165
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4961 time: 0.90s
Epoch 8/1000, LR 0.000195
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5039 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.4961 time: 0.92s
Epoch 9/1000, LR 0.000225
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4961 time: 0.87s
Epoch 10/1000, LR 0.000255
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4961 time: 0.86s
Epoch 11/1000, LR 0.000285
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4961 time: 0.92s
Epoch 12/1000, LR 0.000285
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.91s
Epoch 13/1000, LR 0.000285
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.94s
Epoch 14/1000, LR 0.000285
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 1.00s
Epoch 15/1000, LR 0.000285
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5039 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4961 time: 1.00s
Epoch 16/1000, LR 0.000285
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5039 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4961 time: 1.01s
Epoch 17/1000, LR 0.000285
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4961 time: 0.98s
Epoch 18/1000, LR 0.000285
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.4961 time: 1.06s
Epoch 19/1000, LR 0.000285
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6805 score: 0.4961 time: 1.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.4961 time: 0.86s
Epoch 21/1000, LR 0.000285
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6706 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6718 score: 0.4961 time: 0.89s
Epoch 22/1000, LR 0.000285
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 2.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6646 score: 0.5039 time: 3.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6662 score: 0.4961 time: 0.85s
Epoch 23/1000, LR 0.000285
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6576 score: 0.5039 time: 0.98s
Test loss: 0.6596 score: 0.4961 time: 0.84s
Epoch 24/1000, LR 0.000285
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 1.09s
Val loss: 0.6491 score: 0.6124 time: 0.98s
Test loss: 0.6519 score: 0.6512 time: 0.86s
Epoch 25/1000, LR 0.000285
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 1.08s
Val loss: 0.6392 score: 0.7442 time: 0.98s
Test loss: 0.6430 score: 0.7752 time: 0.95s
Epoch 26/1000, LR 0.000285
Train loss: 0.6229;  Loss pred: 0.6229; Loss self: 0.0000; time: 0.98s
Val loss: 0.6280 score: 0.8062 time: 1.00s
Test loss: 0.6331 score: 0.8217 time: 1.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.99s
Val loss: 0.6153 score: 0.8295 time: 1.10s
Test loss: 0.6215 score: 0.8450 time: 0.89s
Epoch 28/1000, LR 0.000285
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 1.00s
Val loss: 0.6008 score: 0.8527 time: 1.18s
Test loss: 0.6083 score: 0.8605 time: 0.87s
Epoch 29/1000, LR 0.000285
Train loss: 0.5762;  Loss pred: 0.5762; Loss self: 0.0000; time: 1.06s
Val loss: 0.5843 score: 0.8760 time: 1.18s
Test loss: 0.5933 score: 0.8682 time: 0.89s
Epoch 30/1000, LR 0.000285
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 1.06s
Val loss: 0.5655 score: 0.8760 time: 1.18s
Test loss: 0.5770 score: 0.8837 time: 0.90s
Epoch 31/1000, LR 0.000285
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 1.18s
Val loss: 0.5449 score: 0.8760 time: 1.03s
Test loss: 0.5590 score: 0.8837 time: 0.94s
Epoch 32/1000, LR 0.000285
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 1.11s
Val loss: 0.5220 score: 0.8837 time: 1.06s
Test loss: 0.5394 score: 0.8837 time: 0.89s
Epoch 33/1000, LR 0.000285
Train loss: 0.4839;  Loss pred: 0.4839; Loss self: 0.0000; time: 1.13s
Val loss: 0.4970 score: 0.8837 time: 1.07s
Test loss: 0.5183 score: 0.8992 time: 0.90s
Epoch 34/1000, LR 0.000285
Train loss: 0.4573;  Loss pred: 0.4573; Loss self: 0.0000; time: 1.14s
Val loss: 0.4701 score: 0.8837 time: 1.01s
Test loss: 0.4958 score: 0.8915 time: 1.01s
Epoch 35/1000, LR 0.000285
Train loss: 0.4246;  Loss pred: 0.4246; Loss self: 0.0000; time: 1.09s
Val loss: 0.4418 score: 0.8837 time: 1.07s
Test loss: 0.4724 score: 0.8837 time: 1.05s
Epoch 36/1000, LR 0.000285
Train loss: 0.3938;  Loss pred: 0.3938; Loss self: 0.0000; time: 1.03s
Val loss: 0.4127 score: 0.8760 time: 1.04s
Test loss: 0.4484 score: 0.8837 time: 1.13s
Epoch 37/1000, LR 0.000285
Train loss: 0.3626;  Loss pred: 0.3626; Loss self: 0.0000; time: 0.97s
Val loss: 0.3838 score: 0.8760 time: 1.03s
Test loss: 0.4246 score: 0.8837 time: 1.00s
Epoch 38/1000, LR 0.000284
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.99s
Val loss: 0.3559 score: 0.8760 time: 1.05s
Test loss: 0.4017 score: 0.8837 time: 0.95s
Epoch 39/1000, LR 0.000284
Train loss: 0.2953;  Loss pred: 0.2953; Loss self: 0.0000; time: 0.98s
Val loss: 0.3310 score: 0.8682 time: 1.12s
Test loss: 0.3814 score: 0.8837 time: 0.85s
Epoch 40/1000, LR 0.000284
Train loss: 0.2618;  Loss pred: 0.2618; Loss self: 0.0000; time: 1.02s
Val loss: 0.3096 score: 0.8605 time: 1.08s
Test loss: 0.3645 score: 0.8837 time: 0.86s
Epoch 41/1000, LR 0.000284
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 1.01s
Val loss: 0.2918 score: 0.8682 time: 1.08s
Test loss: 0.3508 score: 0.8837 time: 0.85s
Epoch 42/1000, LR 0.000284
Train loss: 0.2154;  Loss pred: 0.2154; Loss self: 0.0000; time: 0.99s
Val loss: 0.2792 score: 0.8605 time: 1.26s
Test loss: 0.3418 score: 0.8837 time: 0.87s
Epoch 43/1000, LR 0.000284
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.97s
Val loss: 0.2678 score: 0.8682 time: 1.07s
Test loss: 0.3338 score: 0.8837 time: 0.84s
Epoch 44/1000, LR 0.000284
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 1.08s
Val loss: 0.2555 score: 0.8760 time: 0.98s
Test loss: 0.3256 score: 0.8837 time: 0.85s
Epoch 45/1000, LR 0.000284
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 1.07s
Val loss: 0.2476 score: 0.8837 time: 0.96s
Test loss: 0.3227 score: 0.9070 time: 0.84s
Epoch 46/1000, LR 0.000284
Train loss: 0.1464;  Loss pred: 0.1464; Loss self: 0.0000; time: 1.10s
Val loss: 0.2428 score: 0.8837 time: 1.01s
Test loss: 0.3234 score: 0.8992 time: 0.97s
Epoch 47/1000, LR 0.000284
Train loss: 0.1470;  Loss pred: 0.1470; Loss self: 0.0000; time: 1.02s
Val loss: 0.2404 score: 0.8837 time: 1.13s
Test loss: 0.3267 score: 0.8992 time: 0.97s
Epoch 48/1000, LR 0.000284
Train loss: 0.1289;  Loss pred: 0.1289; Loss self: 0.0000; time: 1.03s
Val loss: 0.2425 score: 0.8837 time: 1.01s
Test loss: 0.3336 score: 0.8992 time: 1.01s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 0.98s
Val loss: 0.2508 score: 0.8837 time: 0.99s
Test loss: 0.3447 score: 0.9070 time: 1.08s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.98s
Val loss: 0.2581 score: 0.8837 time: 1.10s
Test loss: 0.3552 score: 0.9070 time: 0.90s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1226;  Loss pred: 0.1226; Loss self: 0.0000; time: 0.99s
Val loss: 0.2587 score: 0.8837 time: 1.14s
Test loss: 0.3610 score: 0.9070 time: 0.94s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 0.99s
Val loss: 0.2551 score: 0.8915 time: 1.12s
Test loss: 0.3636 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 1.00s
Val loss: 0.2545 score: 0.8915 time: 1.13s
Test loss: 0.3679 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1090;  Loss pred: 0.1090; Loss self: 0.0000; time: 1.01s
Val loss: 0.2478 score: 0.8992 time: 1.16s
Test loss: 0.3680 score: 0.9070 time: 0.87s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 1.16s
Val loss: 0.2406 score: 0.8992 time: 1.00s
Test loss: 0.3678 score: 0.9070 time: 0.87s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 1.13s
Val loss: 0.2405 score: 0.9070 time: 0.99s
Test loss: 0.3712 score: 0.9070 time: 0.86s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 1.16s
Val loss: 0.2415 score: 0.9147 time: 1.02s
Test loss: 0.3749 score: 0.9070 time: 0.92s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0790;  Loss pred: 0.0790; Loss self: 0.0000; time: 1.10s
Val loss: 0.2459 score: 0.9147 time: 1.01s
Test loss: 0.3806 score: 0.9070 time: 1.02s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 1.04s
Val loss: 0.2513 score: 0.9070 time: 1.17s
Test loss: 0.3871 score: 0.9070 time: 1.01s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 1.03s
Val loss: 0.2566 score: 0.9070 time: 1.03s
Test loss: 0.3940 score: 0.9070 time: 0.96s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 1.05s
Val loss: 0.2593 score: 0.9070 time: 1.09s
Test loss: 0.3995 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.99s
Val loss: 0.2567 score: 0.9147 time: 1.09s
Test loss: 0.4014 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 1.00s
Val loss: 0.2611 score: 0.9147 time: 1.08s
Test loss: 0.4085 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 1.10s
Val loss: 0.2600 score: 0.9147 time: 0.99s
Test loss: 0.4116 score: 0.9070 time: 0.87s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 1.00s
Val loss: 0.2589 score: 0.9147 time: 1.09s
Test loss: 0.4146 score: 0.9070 time: 0.85s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 1.15s
Val loss: 0.2638 score: 0.9147 time: 1.01s
Test loss: 0.4220 score: 0.9070 time: 0.87s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 1.18s
Val loss: 0.2707 score: 0.9147 time: 0.99s
Test loss: 0.4313 score: 0.9147 time: 1.00s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.1470,   Val_Loss: 0.2404,   Val_Precision: 0.9630,   Val_Recall: 0.8000,   Val_accuracy: 0.8739,   Val_Score: 0.8837,   Val_Loss: 0.2404,   Test_Precision: 0.9474,   Test_Recall: 0.8438,   Test_accuracy: 0.8926,   Test_Score: 0.8992,   Test_loss: 0.3267


[1.000802458031103, 0.9254461990203708, 0.9143370180390775, 1.0290164640173316, 0.9055836119223386, 0.9506509460043162, 0.9686355371959507, 0.9504107148386538, 1.0405861740000546, 1.0637568500824273, 1.0583128929138184, 1.1917587998323143, 1.1532152350991964, 1.0576077247969806, 0.9854708269704133, 0.9155615710187703, 0.9467514159623533, 0.9329841618891805, 0.9582161090802401, 1.107795518124476, 0.9060090139973909, 0.9126447509042919, 0.9358366611413658, 1.0540203040000051, 1.037081859074533, 1.0224305188748986, 1.046191070927307, 0.9306777799502015, 1.2011076679918915, 0.9266112269833684, 1.0411044310312718, 1.3066127069760114, 1.0313366858754307, 1.04039369802922, 0.9139482299797237, 0.9414774398319423, 0.9280127577949315, 1.067389033967629, 1.0059906269889325, 1.5223205911461264, 0.9634214409161359, 1.005177672021091, 1.041308545973152, 1.0447143248748034, 1.0412509059533477, 0.9358627130277455, 0.9456453928723931, 0.9147601609583944, 0.9315294148400426, 0.9225593069568276, 0.9515979089774191, 0.9730778990779072, 0.9309431069996208, 1.0237465559039265, 1.0283818100579083, 1.067288015037775, 1.0723670371808112, 1.0127135929651558, 1.003797739977017, 0.9201732149813324, 0.9137983380351216, 0.911943320883438, 0.9047713167965412, 0.9179190329741687, 0.9108253170270473, 0.9321907709818333, 0.9626299000810832, 1.070626461878419, 1.049307068809867, 1.0422393400222063, 0.9283342729322612, 1.065684932982549, 1.019566647009924, 1.1073736869730055, 1.0022497700992972, 0.8581821711268276, 0.8576368228532374, 0.9240333139896393, 0.9007276729680598, 0.9213243869598955, 0.8771479718852788, 0.8656506619881839, 0.9207649030722678, 0.9123175551649183, 0.9500419360119849, 1.0037739949766546, 1.0076383880805224, 1.0158368251286447, 0.9812615311238915, 1.066944170044735, 1.1818509188015014, 0.864510394167155, 0.8941818261519074, 0.853090382181108, 0.8433343770448118, 0.8665965001564473, 0.9589137129951268, 1.0683484319597483, 0.8940194591414183, 0.8766177049838006, 0.899468949995935, 0.9091282049193978, 0.9441021960228682, 0.8912201980128884, 0.9027299399022013, 1.015712572960183, 1.054238713113591, 1.1390127260237932, 1.0085338070057333, 0.9598486500326544, 0.859894918045029, 0.867806961061433, 0.8584823391865939, 0.8708519621286541, 0.8488943199627101, 0.8592984119895846, 0.8404842761810869, 0.9780039051547647, 0.9721506719943136, 1.010139724938199, 1.0807291599921882, 0.903684631921351, 0.9468810830730945, 0.8538680591154844, 0.8528009699657559, 0.8743683551438153, 0.8741525472141802, 0.8653620211407542, 0.9230540369171649, 1.0235654858406633, 1.019795310916379, 0.9633469698019326, 0.8591164348181337, 0.8579212189652026, 0.8529587318189442, 0.8786070770584047, 0.8539167549461126, 0.8796916590072215, 1.0015743421390653]
[0.007758158589388395, 0.007174001542793572, 0.0070878838607680425, 0.007976871814087842, 0.007020027999397974, 0.007369387178328033, 0.007508802613922099, 0.007367524921229875, 0.008066559488372517, 0.008246177132421918, 0.008203975914060607, 0.009238440308777629, 0.00893965298526509, 0.008198509494550237, 0.007639308736204754, 0.007097376519525351, 0.007339158263274057, 0.007232435363482019, 0.007428031853335195, 0.00858756215600369, 0.007023325689902255, 0.007074765510885983, 0.007254547760785781, 0.008170700031007791, 0.008039394256391729, 0.007925817975774407, 0.00811000830176207, 0.007214556433722492, 0.009310912154975903, 0.0071830327673129335, 0.008070576984738542, 0.010128780674232646, 0.007994858030042098, 0.0080650674265831, 0.007084869999842819, 0.007298274727379398, 0.007193897347247531, 0.008274333596648288, 0.0077983769534025775, 0.011800934815086251, 0.007468383262915782, 0.007792074976907682, 0.008072159271109705, 0.008098560657944212, 0.008071712449250756, 0.007254749713393376, 0.007330584440871264, 0.007091164038437166, 0.007221158254573974, 0.0071516225345490516, 0.00737672797656914, 0.007543239527735715, 0.0072166132325552, 0.007936019813208732, 0.007971952015952778, 0.008273550504168798, 0.008312922768843498, 0.007850492968722138, 0.007781377829279202, 0.007133125697529709, 0.007083708046783888, 0.0070693280688638605, 0.007013731137957684, 0.007115651418404408, 0.007060661372302692, 0.007226285046370801, 0.007462247287450257, 0.008299429937042007, 0.008134163324107496, 0.008079374728854312, 0.007196389712653187, 0.008261123511492628, 0.007903617418681582, 0.008584292147077562, 0.00776937806278525, 0.006652574969975408, 0.0066483474639785845, 0.0071630489456561185, 0.006982385061767905, 0.007142049511317019, 0.006799596681281231, 0.006710470247970417, 0.007137712426916804, 0.0070722291098055685, 0.007364666170635542, 0.007781193759508951, 0.007811150295197848, 0.007874704070764687, 0.00760667853584412, 0.008270885039106474, 0.009161635029469003, 0.006701630962536086, 0.006931642063193081, 0.006613103737838046, 0.006537475791045053, 0.006717802326794165, 0.007433439635621138, 0.008281770790385645, 0.006930383404197041, 0.006795486085145741, 0.006972627519348333, 0.007047505464491456, 0.007318621674595877, 0.006908683705526267, 0.006997906510869778, 0.007873740875660333, 0.008172393124911559, 0.00882955601568832, 0.007818091527176227, 0.00744068720955546, 0.0066658520778684425, 0.006727185744662271, 0.006654901854159643, 0.006750790404098094, 0.006580576123741939, 0.00666122799991926, 0.006515381985899899, 0.007581425621354765, 0.007536051720886152, 0.00783054050339689, 0.008377745426296032, 0.007005307224196519, 0.007340163434675151, 0.0066191322412053055, 0.006610860232292681, 0.006778049264680739, 0.006776376334993645, 0.00670823272202135, 0.007155457650520659, 0.007934616169307468, 0.007905390007103713, 0.007467805967456841, 0.0066598173241715795, 0.0066505520850015706, 0.006612083192394916, 0.00681090757409616, 0.006619509728264439, 0.006819315186102492, 0.007764142187124537]
[128.8965659154995, 139.39221981413147, 141.0858331828874, 125.36242568595799, 142.44957428741856, 135.69649358915623, 133.17702587439126, 135.73079299921366, 123.96858926552798, 121.26831426749843, 121.89211797734838, 108.24337946415908, 111.86116526539278, 121.97339048820106, 130.90189630126204, 140.89713251775976, 136.2554075177951, 138.2660127250076, 134.62516313133457, 116.44748321278645, 142.38268936292448, 141.34744090970847, 137.84456770764774, 122.38853417761047, 124.38748096039063, 126.16994271840984, 123.30443604881748, 138.6086600315122, 107.40086291820332, 139.21696202620626, 123.90687826793545, 98.72856685938258, 125.08039495414708, 123.99152382829692, 141.14585024456136, 137.01868418963608, 139.0067096776981, 120.85565421305631, 128.23181105187297, 84.73904954729565, 133.897788155235, 128.33552076482383, 123.88259032239424, 123.47873186812014, 123.88944803067449, 137.84073048775855, 136.41477130043762, 141.02057075249832, 138.48193942662692, 139.8284088916971, 135.56145803075856, 132.56903699307213, 138.56915533298272, 126.00774992214578, 125.43979165941879, 120.86709321422882, 120.29463376563055, 127.38053571720792, 128.5119450487641, 140.19099654255533, 141.16900264601043, 141.45615966026514, 142.5774641671235, 140.53527093999173, 141.62979178165432, 138.3836914241602, 134.00788817086837, 120.49020325321393, 122.93827406148412, 123.77195433560537, 138.95856671599248, 121.04891043074588, 126.52434284538181, 116.49184147820972, 128.71043112059724, 150.31773478889434, 150.41331780838783, 139.60535626472708, 143.21753829869775, 140.01583136821415, 147.06754633740607, 149.0208529428247, 140.10090911325193, 141.3981340923346, 135.7834797709105, 128.51498509183855, 128.02211738452678, 126.98889901305118, 131.46342326520167, 120.90604515378837, 109.15082261882667, 149.21740776092687, 144.26596048719617, 151.21492715717167, 152.96423756854085, 148.85820560862123, 134.52722414102715, 120.7471234486356, 144.29216129578052, 147.15650763907837, 143.41795789680455, 141.89417873295108, 136.63775017516784, 144.74537301513723, 142.8998799064706, 127.00443357124509, 122.36317865714787, 113.2559777890535, 127.90845393967707, 134.3961883945051, 150.01833048773153, 148.65057067785833, 150.26517624372636, 148.13080248987526, 151.9623785510388, 150.12246991277297, 153.48294269839053, 131.90131380874837, 132.69547994588493, 127.70510535846148, 119.36385615886634, 142.74891421549268, 136.23674852741973, 151.07720522258455, 151.2662444616825, 147.53507402355865, 147.5714970014188, 149.0705587355765, 139.7534649551362, 126.03004085669338, 126.49597288703136, 133.90813906491329, 150.1542687020159, 150.36345663019702, 151.2382665042962, 146.82331086143168, 151.06858982775282, 146.64229071534373, 128.79722909484116]
Elapsed: 0.969194377933314~0.09738953343430082
Time per graph: 0.007513134712661348~0.0007549576235217117
Speed: 134.28876396802258~12.02269813042903
Total Time: 1.0022
best val loss: 0.24036142484162204 test_score: 0.8992

Testing...
Test loss: 0.3749 score: 0.9070 time: 0.85s
test Score 0.9070
Epoch Time List: [3.000502449925989, 2.921781881712377, 2.9060520550701767, 2.941703285789117, 2.914627072168514, 3.0355552840046585, 3.013128597289324, 3.0044197398237884, 3.1015319048892707, 3.1157271361444145, 3.1016855610068887, 3.279422693187371, 3.288023378001526, 3.172935737995431, 3.0535828080028296, 3.005910911830142, 2.985895082121715, 2.959226841107011, 3.021194730652496, 3.1303899090271443, 2.923301527975127, 2.9398232728708535, 2.9970009790267795, 3.145837256219238, 2.995490266941488, 3.0136252290103585, 3.0292423798236996, 3.00872243498452, 3.493579346453771, 3.244630339089781, 3.12884495803155, 3.8716683220118284, 3.378297003218904, 3.005811606068164, 2.9835051079280674, 3.036473515909165, 3.101095614954829, 3.122124592307955, 3.161435002926737, 7.803203632123768, 2.9701363760977983, 3.0090711081866175, 2.9380107577890158, 2.9832899270113558, 3.045563711086288, 2.9795174601022154, 3.026310984045267, 2.9706010590307415, 3.0135401859879494, 2.963608935009688, 3.2102142099756747, 3.3651157738640904, 3.0852102991193533, 3.064302099868655, 2.981659283163026, 3.036275235703215, 2.996517940191552, 2.907128264894709, 2.8920116322115064, 2.9075958388857543, 3.0324710747227073, 3.1153642449062318, 2.930597990984097, 2.969655022956431, 2.9635789450258017, 2.964834503363818, 3.0305791050195694, 3.172378211747855, 3.0060180141590536, 3.0262716717552394, 3.2154631323646754, 3.1903589949943125, 3.2754861211869866, 3.191912275040522, 2.9884126831311733, 2.9675883329473436, 2.9350609737448394, 3.032634736970067, 3.001762261148542, 3.042859059991315, 3.0226604777853936, 3.0311488481238484, 3.120587724260986, 3.090357026318088, 3.1312793891411275, 3.182019982021302, 3.080349266761914, 3.1584672210738063, 3.0766546921804547, 3.179620797978714, 3.326943000080064, 2.9993845680728555, 3.065438989084214, 7.105928106699139, 2.901249841786921, 2.9259294702205807, 3.011840513907373, 3.040756380185485, 2.982063320931047, 3.056091670645401, 3.1309447770472616, 3.1371481551323086, 3.1433732400182635, 3.0557554999832064, 3.0999214330222458, 3.1640990951564163, 3.212272967910394, 3.2006980881560594, 3.003583276644349, 2.991273042978719, 2.9592210692353547, 2.9688806429039687, 2.951951610855758, 3.114611769793555, 2.8880480588413775, 2.906735597876832, 2.862488731741905, 3.081519036088139, 3.116051367018372, 3.0380614451132715, 3.0435849300120026, 2.9757831830065697, 3.0769640496000648, 2.961423244094476, 2.97253317409195, 3.0394611759111285, 3.023630306823179, 2.9863916342146695, 3.0979495339561254, 3.13393364707008, 3.2303365152329206, 3.016736441059038, 2.9920532451942563, 2.9380078620743006, 2.9325828081928194, 2.964792924700305, 2.93398585007526, 3.033588487887755, 3.1673204600811005]
Total Epoch List: [72, 67]
Total Time List: [1.0661689219996333, 1.0022054079454392]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7138937cbaf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 1.01s
Epoch 2/1000, LR 0.000020
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 1.00s
Epoch 3/1000, LR 0.000050
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 1.01s
Epoch 4/1000, LR 0.000080
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.98s
Epoch 5/1000, LR 0.000110
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 1.05s
Epoch 6/1000, LR 0.000140
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 1.06s
Epoch 7/1000, LR 0.000170
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.07s
Epoch 8/1000, LR 0.000200
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.97s
Epoch 9/1000, LR 0.000230
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.04s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.99s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 1.09s
Epoch 12/1000, LR 0.000290
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 1.09s
Epoch 13/1000, LR 0.000290
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 1.06s
Epoch 14/1000, LR 0.000290
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 1.06s
Epoch 15/1000, LR 0.000290
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 1.06s
Epoch 16/1000, LR 0.000290
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 5.49s
Epoch 17/1000, LR 0.000290
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4961 time: 1.00s
Test loss: 0.6899 score: 0.5078 time: 0.99s
Epoch 18/1000, LR 0.000290
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 1.02s
Val loss: 0.6892 score: 0.5039 time: 1.03s
Test loss: 0.6886 score: 0.5078 time: 1.00s
Epoch 19/1000, LR 0.000290
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.98s
Val loss: 0.6875 score: 0.5194 time: 1.01s
Test loss: 0.6868 score: 0.5391 time: 0.98s
Epoch 20/1000, LR 0.000290
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 1.02s
Val loss: 0.6854 score: 0.5891 time: 1.02s
Test loss: 0.6846 score: 0.6016 time: 0.95s
Epoch 21/1000, LR 0.000290
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 1.01s
Val loss: 0.6828 score: 0.7287 time: 1.07s
Test loss: 0.6820 score: 0.7500 time: 0.99s
Epoch 22/1000, LR 0.000290
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 1.12s
Val loss: 0.6806 score: 0.7674 time: 0.92s
Test loss: 0.6796 score: 0.7969 time: 0.98s
Epoch 23/1000, LR 0.000290
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 1.10s
Val loss: 0.6776 score: 0.7829 time: 0.93s
Test loss: 0.6764 score: 0.8281 time: 1.00s
Epoch 24/1000, LR 0.000290
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 1.07s
Val loss: 0.6733 score: 0.8140 time: 0.90s
Test loss: 0.6718 score: 0.8359 time: 1.11s
Epoch 25/1000, LR 0.000290
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 1.00s
Val loss: 0.6681 score: 0.8450 time: 0.96s
Test loss: 0.6663 score: 0.8594 time: 1.06s
Epoch 26/1000, LR 0.000290
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.99s
Val loss: 0.6612 score: 0.8605 time: 0.97s
Test loss: 0.6593 score: 0.8125 time: 1.33s
Epoch 27/1000, LR 0.000290
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 1.15s
Val loss: 0.6528 score: 0.8372 time: 1.04s
Test loss: 0.6508 score: 0.7734 time: 0.99s
Epoch 28/1000, LR 0.000290
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 1.02s
Val loss: 0.6425 score: 0.6822 time: 1.02s
Test loss: 0.6409 score: 0.6875 time: 0.99s
Epoch 29/1000, LR 0.000290
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.97s
Val loss: 0.6311 score: 0.6202 time: 1.01s
Test loss: 0.6300 score: 0.6484 time: 1.02s
Epoch 30/1000, LR 0.000290
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 1.14s
Val loss: 0.6180 score: 0.6357 time: 0.99s
Test loss: 0.6168 score: 0.6484 time: 0.97s
Epoch 31/1000, LR 0.000290
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.96s
Val loss: 0.6037 score: 0.7054 time: 0.99s
Test loss: 0.6018 score: 0.6875 time: 0.95s
Epoch 32/1000, LR 0.000290
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.98s
Val loss: 0.5876 score: 0.6899 time: 1.01s
Test loss: 0.5858 score: 0.6953 time: 0.99s
Epoch 33/1000, LR 0.000290
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 1.06s
Val loss: 0.5721 score: 0.6589 time: 0.94s
Test loss: 0.5706 score: 0.7031 time: 0.98s
Epoch 34/1000, LR 0.000290
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 1.07s
Val loss: 0.5561 score: 0.6744 time: 0.92s
Test loss: 0.5546 score: 0.6953 time: 1.01s
Epoch 35/1000, LR 0.000290
Train loss: 0.5277;  Loss pred: 0.5277; Loss self: 0.0000; time: 1.08s
Val loss: 0.5409 score: 0.6667 time: 0.94s
Test loss: 0.5399 score: 0.6953 time: 1.10s
Epoch 36/1000, LR 0.000290
Train loss: 0.5051;  Loss pred: 0.5051; Loss self: 0.0000; time: 1.02s
Val loss: 0.5230 score: 0.7287 time: 0.92s
Test loss: 0.5210 score: 0.7109 time: 1.07s
Epoch 37/1000, LR 0.000290
Train loss: 0.4791;  Loss pred: 0.4791; Loss self: 0.0000; time: 1.00s
Val loss: 0.4999 score: 0.7597 time: 0.91s
Test loss: 0.4936 score: 0.7422 time: 1.09s
Epoch 38/1000, LR 0.000289
Train loss: 0.4468;  Loss pred: 0.4468; Loss self: 0.0000; time: 1.04s
Val loss: 0.4781 score: 0.8295 time: 0.93s
Test loss: 0.4672 score: 0.7891 time: 1.08s
Epoch 39/1000, LR 0.000289
Train loss: 0.4173;  Loss pred: 0.4173; Loss self: 0.0000; time: 0.96s
Val loss: 0.4578 score: 0.8760 time: 1.01s
Test loss: 0.4432 score: 0.8516 time: 1.01s
Epoch 40/1000, LR 0.000289
Train loss: 0.3931;  Loss pred: 0.3931; Loss self: 0.0000; time: 0.98s
Val loss: 0.4388 score: 0.8760 time: 1.13s
Test loss: 0.4210 score: 0.8750 time: 0.98s
Epoch 41/1000, LR 0.000289
Train loss: 0.3685;  Loss pred: 0.3685; Loss self: 0.0000; time: 1.10s
Val loss: 0.4228 score: 0.8760 time: 0.94s
Test loss: 0.4033 score: 0.8750 time: 0.97s
Epoch 42/1000, LR 0.000289
Train loss: 0.3483;  Loss pred: 0.3483; Loss self: 0.0000; time: 1.10s
Val loss: 0.4090 score: 0.8760 time: 0.94s
Test loss: 0.3871 score: 0.8828 time: 1.07s
Epoch 43/1000, LR 0.000289
Train loss: 0.3244;  Loss pred: 0.3244; Loss self: 0.0000; time: 1.03s
Val loss: 0.3959 score: 0.8760 time: 0.91s
Test loss: 0.3702 score: 0.8984 time: 1.10s
Epoch 44/1000, LR 0.000289
Train loss: 0.3079;  Loss pred: 0.3079; Loss self: 0.0000; time: 1.01s
Val loss: 0.3859 score: 0.8760 time: 0.91s
Test loss: 0.3578 score: 0.8984 time: 1.10s
Epoch 45/1000, LR 0.000289
Train loss: 0.2928;  Loss pred: 0.2928; Loss self: 0.0000; time: 0.99s
Val loss: 0.3758 score: 0.8760 time: 0.93s
Test loss: 0.3446 score: 0.8984 time: 1.11s
Epoch 46/1000, LR 0.000289
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.98s
Val loss: 0.3656 score: 0.8682 time: 1.00s
Test loss: 0.3310 score: 0.8984 time: 1.06s
Epoch 47/1000, LR 0.000289
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 1.02s
Val loss: 0.3559 score: 0.8682 time: 1.11s
Test loss: 0.3178 score: 0.8906 time: 0.96s
Epoch 48/1000, LR 0.000289
Train loss: 0.2441;  Loss pred: 0.2441; Loss self: 0.0000; time: 1.01s
Val loss: 0.3476 score: 0.8837 time: 1.00s
Test loss: 0.3065 score: 0.8828 time: 0.97s
Epoch 49/1000, LR 0.000289
Train loss: 0.2256;  Loss pred: 0.2256; Loss self: 0.0000; time: 1.03s
Val loss: 0.3401 score: 0.8837 time: 0.99s
Test loss: 0.2957 score: 0.8828 time: 0.97s
Epoch 50/1000, LR 0.000289
Train loss: 0.2067;  Loss pred: 0.2067; Loss self: 0.0000; time: 1.08s
Val loss: 0.3338 score: 0.8682 time: 0.90s
Test loss: 0.2854 score: 0.8906 time: 0.95s
Epoch 51/1000, LR 0.000289
Train loss: 0.1986;  Loss pred: 0.1986; Loss self: 0.0000; time: 1.09s
Val loss: 0.3293 score: 0.8682 time: 0.89s
Test loss: 0.2764 score: 0.8906 time: 0.96s
Epoch 52/1000, LR 0.000289
Train loss: 0.1770;  Loss pred: 0.1770; Loss self: 0.0000; time: 1.08s
Val loss: 0.3251 score: 0.8682 time: 0.90s
Test loss: 0.2674 score: 0.8984 time: 1.06s
Epoch 53/1000, LR 0.000289
Train loss: 0.1653;  Loss pred: 0.1653; Loss self: 0.0000; time: 0.98s
Val loss: 0.3220 score: 0.8682 time: 0.91s
Test loss: 0.2595 score: 0.8906 time: 0.95s
Epoch 54/1000, LR 0.000289
Train loss: 0.1485;  Loss pred: 0.1485; Loss self: 0.0000; time: 1.08s
Val loss: 0.3208 score: 0.8760 time: 0.96s
Test loss: 0.2541 score: 0.8906 time: 1.11s
Epoch 55/1000, LR 0.000289
Train loss: 0.1307;  Loss pred: 0.1307; Loss self: 0.0000; time: 1.04s
Val loss: 0.3222 score: 0.8605 time: 1.08s
Test loss: 0.2523 score: 0.8828 time: 0.96s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 1.01s
Val loss: 0.3263 score: 0.8682 time: 1.01s
Test loss: 0.2527 score: 0.8828 time: 0.96s
     INFO: Early stopping counter 2 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 1.00s
Val loss: 0.3311 score: 0.8682 time: 1.01s
Test loss: 0.2530 score: 0.8828 time: 0.98s
     INFO: Early stopping counter 3 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 1.10s
Val loss: 0.3353 score: 0.8605 time: 0.90s
Test loss: 0.2522 score: 0.8906 time: 1.06s
     INFO: Early stopping counter 4 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.1052;  Loss pred: 0.1052; Loss self: 0.0000; time: 1.14s
Val loss: 0.3395 score: 0.8760 time: 0.91s
Test loss: 0.2510 score: 0.8828 time: 0.98s
     INFO: Early stopping counter 5 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 1.10s
Val loss: 0.3444 score: 0.8760 time: 0.95s
Test loss: 0.2506 score: 0.8828 time: 0.97s
     INFO: Early stopping counter 6 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 1.10s
Val loss: 0.3500 score: 0.8760 time: 0.97s
Test loss: 0.2509 score: 0.8906 time: 1.10s
     INFO: Early stopping counter 7 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 1.04s
Val loss: 0.3563 score: 0.8760 time: 0.91s
Test loss: 0.2522 score: 0.8906 time: 1.06s
     INFO: Early stopping counter 8 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 1.03s
Val loss: 0.3649 score: 0.8760 time: 0.90s
Test loss: 0.2564 score: 0.8906 time: 1.27s
     INFO: Early stopping counter 9 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0693;  Loss pred: 0.0693; Loss self: 0.0000; time: 1.21s
Val loss: 0.3709 score: 0.8760 time: 1.27s
Test loss: 0.2558 score: 0.8984 time: 1.33s
     INFO: Early stopping counter 10 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 1.49s
Val loss: 0.3875 score: 0.8837 time: 1.16s
Test loss: 0.2735 score: 0.9062 time: 1.14s
     INFO: Early stopping counter 11 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 1.01s
Val loss: 0.4031 score: 0.8837 time: 1.06s
Test loss: 0.2949 score: 0.8906 time: 1.06s
     INFO: Early stopping counter 12 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1161;  Loss pred: 0.1161; Loss self: 0.0000; time: 1.08s
Val loss: 0.4031 score: 0.8837 time: 1.13s
Test loss: 0.2938 score: 0.8906 time: 1.05s
     INFO: Early stopping counter 13 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 1.05s
Val loss: 0.3896 score: 0.8837 time: 1.09s
Test loss: 0.2735 score: 0.9062 time: 1.05s
     INFO: Early stopping counter 14 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0829;  Loss pred: 0.0829; Loss self: 0.0000; time: 1.19s
Val loss: 0.3840 score: 0.8760 time: 0.99s
Test loss: 0.2649 score: 0.9062 time: 1.05s
     INFO: Early stopping counter 15 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 1.07s
Val loss: 0.3834 score: 0.8760 time: 1.04s
Test loss: 0.2649 score: 0.8906 time: 1.14s
     INFO: Early stopping counter 16 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 1.33s
Val loss: 0.3998 score: 0.8682 time: 1.14s
Test loss: 0.2876 score: 0.8906 time: 1.17s
     INFO: Early stopping counter 17 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0744;  Loss pred: 0.0744; Loss self: 0.0000; time: 0.98s
Val loss: 0.4247 score: 0.8605 time: 0.92s
Test loss: 0.3201 score: 0.8750 time: 1.10s
     INFO: Early stopping counter 18 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 1.04s
Val loss: 0.4481 score: 0.8527 time: 0.91s
Test loss: 0.3522 score: 0.8594 time: 1.08s
     INFO: Early stopping counter 19 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.1039;  Loss pred: 0.1039; Loss self: 0.0000; time: 1.00s
Val loss: 0.4620 score: 0.8527 time: 0.91s
Test loss: 0.3717 score: 0.8594 time: 1.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 053,   Train_Loss: 0.1485,   Val_Loss: 0.3208,   Val_Precision: 0.9298,   Val_Recall: 0.8154,   Val_accuracy: 0.8689,   Val_Score: 0.8760,   Val_Loss: 0.3208,   Test_Precision: 0.9310,   Test_Recall: 0.8438,   Test_accuracy: 0.8852,   Test_Score: 0.8906,   Test_loss: 0.2541


[1.000802458031103, 0.9254461990203708, 0.9143370180390775, 1.0290164640173316, 0.9055836119223386, 0.9506509460043162, 0.9686355371959507, 0.9504107148386538, 1.0405861740000546, 1.0637568500824273, 1.0583128929138184, 1.1917587998323143, 1.1532152350991964, 1.0576077247969806, 0.9854708269704133, 0.9155615710187703, 0.9467514159623533, 0.9329841618891805, 0.9582161090802401, 1.107795518124476, 0.9060090139973909, 0.9126447509042919, 0.9358366611413658, 1.0540203040000051, 1.037081859074533, 1.0224305188748986, 1.046191070927307, 0.9306777799502015, 1.2011076679918915, 0.9266112269833684, 1.0411044310312718, 1.3066127069760114, 1.0313366858754307, 1.04039369802922, 0.9139482299797237, 0.9414774398319423, 0.9280127577949315, 1.067389033967629, 1.0059906269889325, 1.5223205911461264, 0.9634214409161359, 1.005177672021091, 1.041308545973152, 1.0447143248748034, 1.0412509059533477, 0.9358627130277455, 0.9456453928723931, 0.9147601609583944, 0.9315294148400426, 0.9225593069568276, 0.9515979089774191, 0.9730778990779072, 0.9309431069996208, 1.0237465559039265, 1.0283818100579083, 1.067288015037775, 1.0723670371808112, 1.0127135929651558, 1.003797739977017, 0.9201732149813324, 0.9137983380351216, 0.911943320883438, 0.9047713167965412, 0.9179190329741687, 0.9108253170270473, 0.9321907709818333, 0.9626299000810832, 1.070626461878419, 1.049307068809867, 1.0422393400222063, 0.9283342729322612, 1.065684932982549, 1.019566647009924, 1.1073736869730055, 1.0022497700992972, 0.8581821711268276, 0.8576368228532374, 0.9240333139896393, 0.9007276729680598, 0.9213243869598955, 0.8771479718852788, 0.8656506619881839, 0.9207649030722678, 0.9123175551649183, 0.9500419360119849, 1.0037739949766546, 1.0076383880805224, 1.0158368251286447, 0.9812615311238915, 1.066944170044735, 1.1818509188015014, 0.864510394167155, 0.8941818261519074, 0.853090382181108, 0.8433343770448118, 0.8665965001564473, 0.9589137129951268, 1.0683484319597483, 0.8940194591414183, 0.8766177049838006, 0.899468949995935, 0.9091282049193978, 0.9441021960228682, 0.8912201980128884, 0.9027299399022013, 1.015712572960183, 1.054238713113591, 1.1390127260237932, 1.0085338070057333, 0.9598486500326544, 0.859894918045029, 0.867806961061433, 0.8584823391865939, 0.8708519621286541, 0.8488943199627101, 0.8592984119895846, 0.8404842761810869, 0.9780039051547647, 0.9721506719943136, 1.010139724938199, 1.0807291599921882, 0.903684631921351, 0.9468810830730945, 0.8538680591154844, 0.8528009699657559, 0.8743683551438153, 0.8741525472141802, 0.8653620211407542, 0.9230540369171649, 1.0235654858406633, 1.019795310916379, 0.9633469698019326, 0.8591164348181337, 0.8579212189652026, 0.8529587318189442, 0.8786070770584047, 0.8539167549461126, 0.8796916590072215, 1.0015743421390653, 1.0104417540133, 1.0022282970603555, 1.0106999499257654, 0.9818081399425864, 1.059176677139476, 1.0635091632138938, 1.078125674976036, 0.9717332250438631, 1.0462027040775865, 0.9973750719800591, 1.0996928419917822, 1.095613817917183, 1.0690415150020272, 1.0614180041011423, 1.060985007090494, 5.497916856082156, 1.0000295338686556, 1.0077306509483606, 0.9895846771541983, 0.9503874289803207, 0.9934135510120541, 0.9891438281629235, 1.0087672078516334, 1.1102129938080907, 1.065784557024017, 1.334793555084616, 0.9941775330808014, 0.9990195969585329, 1.0277930770535022, 0.9735595129895955, 0.9567242830526084, 0.9931159880943596, 0.9820705989841372, 1.0155848909635097, 1.1008950651157647, 1.071222685975954, 1.0907921958714724, 1.0888828171882778, 1.012405761051923, 0.9843748300336301, 0.9722513451706618, 1.0798592341598123, 1.1004162260796875, 1.1035883671138436, 1.1145957929547876, 1.0620235269889235, 0.9666109071113169, 0.97572957794182, 0.9755703490227461, 0.9563911489676684, 0.9685830499511212, 1.0687611252069473, 0.9594099631067365, 1.112050972878933, 0.9629296178463846, 0.9643936459906399, 0.9893473410047591, 1.0693624848499894, 0.9895173101685941, 0.9722835440188646, 1.1097586008254439, 1.069985412992537, 1.275521274888888, 1.3352251679170877, 1.1424536500126123, 1.0629860199987888, 1.0551942789461464, 1.057304193964228, 1.0607670461758971, 1.1442677420563996, 1.17328338092193, 1.108993544941768, 1.0802929010242224, 1.0701141271274537]
[0.007758158589388395, 0.007174001542793572, 0.0070878838607680425, 0.007976871814087842, 0.007020027999397974, 0.007369387178328033, 0.007508802613922099, 0.007367524921229875, 0.008066559488372517, 0.008246177132421918, 0.008203975914060607, 0.009238440308777629, 0.00893965298526509, 0.008198509494550237, 0.007639308736204754, 0.007097376519525351, 0.007339158263274057, 0.007232435363482019, 0.007428031853335195, 0.00858756215600369, 0.007023325689902255, 0.007074765510885983, 0.007254547760785781, 0.008170700031007791, 0.008039394256391729, 0.007925817975774407, 0.00811000830176207, 0.007214556433722492, 0.009310912154975903, 0.0071830327673129335, 0.008070576984738542, 0.010128780674232646, 0.007994858030042098, 0.0080650674265831, 0.007084869999842819, 0.007298274727379398, 0.007193897347247531, 0.008274333596648288, 0.0077983769534025775, 0.011800934815086251, 0.007468383262915782, 0.007792074976907682, 0.008072159271109705, 0.008098560657944212, 0.008071712449250756, 0.007254749713393376, 0.007330584440871264, 0.007091164038437166, 0.007221158254573974, 0.0071516225345490516, 0.00737672797656914, 0.007543239527735715, 0.0072166132325552, 0.007936019813208732, 0.007971952015952778, 0.008273550504168798, 0.008312922768843498, 0.007850492968722138, 0.007781377829279202, 0.007133125697529709, 0.007083708046783888, 0.0070693280688638605, 0.007013731137957684, 0.007115651418404408, 0.007060661372302692, 0.007226285046370801, 0.007462247287450257, 0.008299429937042007, 0.008134163324107496, 0.008079374728854312, 0.007196389712653187, 0.008261123511492628, 0.007903617418681582, 0.008584292147077562, 0.00776937806278525, 0.006652574969975408, 0.0066483474639785845, 0.0071630489456561185, 0.006982385061767905, 0.007142049511317019, 0.006799596681281231, 0.006710470247970417, 0.007137712426916804, 0.0070722291098055685, 0.007364666170635542, 0.007781193759508951, 0.007811150295197848, 0.007874704070764687, 0.00760667853584412, 0.008270885039106474, 0.009161635029469003, 0.006701630962536086, 0.006931642063193081, 0.006613103737838046, 0.006537475791045053, 0.006717802326794165, 0.007433439635621138, 0.008281770790385645, 0.006930383404197041, 0.006795486085145741, 0.006972627519348333, 0.007047505464491456, 0.007318621674595877, 0.006908683705526267, 0.006997906510869778, 0.007873740875660333, 0.008172393124911559, 0.00882955601568832, 0.007818091527176227, 0.00744068720955546, 0.0066658520778684425, 0.006727185744662271, 0.006654901854159643, 0.006750790404098094, 0.006580576123741939, 0.00666122799991926, 0.006515381985899899, 0.007581425621354765, 0.007536051720886152, 0.00783054050339689, 0.008377745426296032, 0.007005307224196519, 0.007340163434675151, 0.0066191322412053055, 0.006610860232292681, 0.006778049264680739, 0.006776376334993645, 0.00670823272202135, 0.007155457650520659, 0.007934616169307468, 0.007905390007103713, 0.007467805967456841, 0.0066598173241715795, 0.0066505520850015706, 0.006612083192394916, 0.00681090757409616, 0.006619509728264439, 0.006819315186102492, 0.007764142187124537, 0.007894076203228906, 0.007829908570784028, 0.007896093358795042, 0.007670376093301456, 0.008274817790152156, 0.008308665337608545, 0.008422856835750281, 0.00759166582065518, 0.008173458625606145, 0.007791992749844212, 0.008591350328060798, 0.008559482952477993, 0.008351886835953337, 0.008292328157040174, 0.008288945367894485, 0.042952475438141846, 0.007812730733348872, 0.007872895710534067, 0.007731130290267174, 0.007424901788908755, 0.0077610433672816725, 0.0077276861575228395, 0.007880993811340886, 0.008673539014125708, 0.008326441851750133, 0.010428074649098562, 0.007767011977193761, 0.0078048406012385385, 0.008029633414480486, 0.007605933695231215, 0.007474408461348503, 0.007758718656987185, 0.007672426554563572, 0.00793425696065242, 0.008600742696216912, 0.00836892723418714, 0.008521814030245878, 0.00850689700928342, 0.007909420008218149, 0.007690428359637735, 0.007595713634145795, 0.008436400266873534, 0.008597001766247558, 0.008621784118076903, 0.008707779632459278, 0.008297058804600965, 0.007551647711807163, 0.007622887327670469, 0.007621643351740204, 0.0074718058513099095, 0.0075670550777431345, 0.008349696290679276, 0.007495390336771379, 0.008687898225616664, 0.00752288763942488, 0.007534325359301874, 0.00772927610159968, 0.008354394412890542, 0.007730603985692142, 0.00759596518764738, 0.00866998906894878, 0.008359261039004195, 0.009965009960069438, 0.010431446624352247, 0.008925419140723534, 0.008304578281240538, 0.008243705304266769, 0.008260189015345532, 0.008287242548249196, 0.008939591734815622, 0.009166276413452579, 0.008664012069857563, 0.008439788289251737, 0.008360266618183232]
[128.8965659154995, 139.39221981413147, 141.0858331828874, 125.36242568595799, 142.44957428741856, 135.69649358915623, 133.17702587439126, 135.73079299921366, 123.96858926552798, 121.26831426749843, 121.89211797734838, 108.24337946415908, 111.86116526539278, 121.97339048820106, 130.90189630126204, 140.89713251775976, 136.2554075177951, 138.2660127250076, 134.62516313133457, 116.44748321278645, 142.38268936292448, 141.34744090970847, 137.84456770764774, 122.38853417761047, 124.38748096039063, 126.16994271840984, 123.30443604881748, 138.6086600315122, 107.40086291820332, 139.21696202620626, 123.90687826793545, 98.72856685938258, 125.08039495414708, 123.99152382829692, 141.14585024456136, 137.01868418963608, 139.0067096776981, 120.85565421305631, 128.23181105187297, 84.73904954729565, 133.897788155235, 128.33552076482383, 123.88259032239424, 123.47873186812014, 123.88944803067449, 137.84073048775855, 136.41477130043762, 141.02057075249832, 138.48193942662692, 139.8284088916971, 135.56145803075856, 132.56903699307213, 138.56915533298272, 126.00774992214578, 125.43979165941879, 120.86709321422882, 120.29463376563055, 127.38053571720792, 128.5119450487641, 140.19099654255533, 141.16900264601043, 141.45615966026514, 142.5774641671235, 140.53527093999173, 141.62979178165432, 138.3836914241602, 134.00788817086837, 120.49020325321393, 122.93827406148412, 123.77195433560537, 138.95856671599248, 121.04891043074588, 126.52434284538181, 116.49184147820972, 128.71043112059724, 150.31773478889434, 150.41331780838783, 139.60535626472708, 143.21753829869775, 140.01583136821415, 147.06754633740607, 149.0208529428247, 140.10090911325193, 141.3981340923346, 135.7834797709105, 128.51498509183855, 128.02211738452678, 126.98889901305118, 131.46342326520167, 120.90604515378837, 109.15082261882667, 149.21740776092687, 144.26596048719617, 151.21492715717167, 152.96423756854085, 148.85820560862123, 134.52722414102715, 120.7471234486356, 144.29216129578052, 147.15650763907837, 143.41795789680455, 141.89417873295108, 136.63775017516784, 144.74537301513723, 142.8998799064706, 127.00443357124509, 122.36317865714787, 113.2559777890535, 127.90845393967707, 134.3961883945051, 150.01833048773153, 148.65057067785833, 150.26517624372636, 148.13080248987526, 151.9623785510388, 150.12246991277297, 153.48294269839053, 131.90131380874837, 132.69547994588493, 127.70510535846148, 119.36385615886634, 142.74891421549268, 136.23674852741973, 151.07720522258455, 151.2662444616825, 147.53507402355865, 147.5714970014188, 149.0705587355765, 139.7534649551362, 126.03004085669338, 126.49597288703136, 133.90813906491329, 150.1542687020159, 150.36345663019702, 151.2382665042962, 146.82331086143168, 151.06858982775282, 146.64229071534373, 128.79722909484116, 126.67726713747342, 127.7154121226051, 126.64490584906177, 130.37170379080897, 120.84858245339227, 120.35627376560416, 118.72456335189784, 131.72339557929823, 122.34722726400781, 128.3368750593349, 116.39613818724531, 116.82948672857599, 119.73342307455411, 120.59339440769737, 120.64260959823588, 23.28154523806558, 127.99621977645671, 127.0180676548253, 129.34719277191766, 134.68191612901737, 128.8486550939417, 129.40484119253577, 126.88755047123406, 115.29319212969493, 120.09931946979373, 95.89497904931505, 128.74964052280274, 128.12561474238328, 124.53868668482664, 131.47629733177692, 133.7898517549821, 128.88726144225384, 130.33686186349982, 126.03574663124748, 116.26902877118461, 119.48962776435567, 117.345907391404, 117.55167588237148, 126.43152076397094, 130.03176848358365, 131.65319918125886, 118.53396808667453, 116.31962249049096, 115.98527477663764, 114.83983773227123, 120.52463692862709, 132.4214314760047, 131.18388833717643, 131.205299677487, 133.83645398450582, 132.15180671028614, 119.76483517326193, 133.4153333008068, 115.1026374884842, 132.9276798924049, 132.72588484188574, 129.37822208124203, 119.69748500945018, 129.3559988133925, 131.6488392582694, 115.34039916860566, 119.62779907625972, 100.35112900108248, 95.86398090419182, 112.03955626434991, 120.41550649946068, 121.3046758819024, 121.0626050011967, 120.66739861635459, 111.86193169263618, 109.09555362441215, 115.41996847846498, 118.4863844598475, 119.6134101542936]
Elapsed: 1.0172501264080982~0.3230209206665719
Time per graph: 0.00790896241196688~0.002528293886307311
Speed: 129.83903780749318~14.185118990219266
Total Time: 1.0709
best val loss: 0.32077195668636366 test_score: 0.8906

Testing...
Test loss: 0.3065 score: 0.8828 time: 0.98s
test Score 0.8828
Epoch Time List: [3.000502449925989, 2.921781881712377, 2.9060520550701767, 2.941703285789117, 2.914627072168514, 3.0355552840046585, 3.013128597289324, 3.0044197398237884, 3.1015319048892707, 3.1157271361444145, 3.1016855610068887, 3.279422693187371, 3.288023378001526, 3.172935737995431, 3.0535828080028296, 3.005910911830142, 2.985895082121715, 2.959226841107011, 3.021194730652496, 3.1303899090271443, 2.923301527975127, 2.9398232728708535, 2.9970009790267795, 3.145837256219238, 2.995490266941488, 3.0136252290103585, 3.0292423798236996, 3.00872243498452, 3.493579346453771, 3.244630339089781, 3.12884495803155, 3.8716683220118284, 3.378297003218904, 3.005811606068164, 2.9835051079280674, 3.036473515909165, 3.101095614954829, 3.122124592307955, 3.161435002926737, 7.803203632123768, 2.9701363760977983, 3.0090711081866175, 2.9380107577890158, 2.9832899270113558, 3.045563711086288, 2.9795174601022154, 3.026310984045267, 2.9706010590307415, 3.0135401859879494, 2.963608935009688, 3.2102142099756747, 3.3651157738640904, 3.0852102991193533, 3.064302099868655, 2.981659283163026, 3.036275235703215, 2.996517940191552, 2.907128264894709, 2.8920116322115064, 2.9075958388857543, 3.0324710747227073, 3.1153642449062318, 2.930597990984097, 2.969655022956431, 2.9635789450258017, 2.964834503363818, 3.0305791050195694, 3.172378211747855, 3.0060180141590536, 3.0262716717552394, 3.2154631323646754, 3.1903589949943125, 3.2754861211869866, 3.191912275040522, 2.9884126831311733, 2.9675883329473436, 2.9350609737448394, 3.032634736970067, 3.001762261148542, 3.042859059991315, 3.0226604777853936, 3.0311488481238484, 3.120587724260986, 3.090357026318088, 3.1312793891411275, 3.182019982021302, 3.080349266761914, 3.1584672210738063, 3.0766546921804547, 3.179620797978714, 3.326943000080064, 2.9993845680728555, 3.065438989084214, 7.105928106699139, 2.901249841786921, 2.9259294702205807, 3.011840513907373, 3.040756380185485, 2.982063320931047, 3.056091670645401, 3.1309447770472616, 3.1371481551323086, 3.1433732400182635, 3.0557554999832064, 3.0999214330222458, 3.1640990951564163, 3.212272967910394, 3.2006980881560594, 3.003583276644349, 2.991273042978719, 2.9592210692353547, 2.9688806429039687, 2.951951610855758, 3.114611769793555, 2.8880480588413775, 2.906735597876832, 2.862488731741905, 3.081519036088139, 3.116051367018372, 3.0380614451132715, 3.0435849300120026, 2.9757831830065697, 3.0769640496000648, 2.961423244094476, 2.97253317409195, 3.0394611759111285, 3.023630306823179, 2.9863916342146695, 3.0979495339561254, 3.13393364707008, 3.2303365152329206, 3.016736441059038, 2.9920532451942563, 2.9380078620743006, 2.9325828081928194, 2.964792924700305, 2.93398585007526, 3.033588487887755, 3.1673204600811005, 3.0826363060623407, 3.0538426036946476, 3.023182389093563, 3.1125404979102314, 3.0891017583198845, 3.001559474039823, 3.013477062806487, 3.0129014819394797, 3.0889072120189667, 3.158853300148621, 3.16206566314213, 3.029047606047243, 2.973867309046909, 2.9536556261591613, 2.9361111749894917, 7.499805999919772, 3.0568777439184487, 3.047303694533184, 2.9740937964525074, 2.989753426751122, 3.062031422974542, 3.02405973803252, 3.031913649989292, 3.075382866896689, 3.02152093895711, 3.285837887087837, 3.1879620330873877, 3.0343003559391946, 3.0019354440737516, 3.105839810334146, 2.9001835740637034, 2.9753332508262247, 2.9724061882589012, 3.0006770468316972, 3.118171541020274, 3.0075557462405413, 2.9963191729038954, 3.052247619954869, 2.983620878076181, 3.0882422861177474, 3.0090548750013113, 3.1162746141199023, 3.030766423093155, 3.014244255144149, 3.0287062891293317, 3.0368852007668465, 3.0957647040486336, 2.9766445960849524, 2.991565020987764, 2.9351547681726515, 2.9488681596703827, 3.0503930333070457, 2.8450934970751405, 3.1475211868528277, 3.0830860510468483, 2.9798678988590837, 2.9955301301088184, 3.061235912842676, 3.0353772616945207, 3.0198534475639462, 3.1700047391932458, 3.016477149212733, 3.197940426878631, 3.812185092829168, 3.7926682149991393, 3.1311225399840623, 3.2622735288459808, 3.1978439970407635, 3.232835839036852, 3.2474759607575834, 3.6394139018375427, 3.0060072978958488, 3.026052897097543, 2.975961167132482]
Total Epoch List: [72, 67, 74]
Total Time List: [1.0661689219996333, 1.0022054079454392, 1.0709356041625142]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7138937c82b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5039 time: 0.96s
Epoch 2/1000, LR 0.000015
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5039 time: 0.97s
Epoch 3/1000, LR 0.000045
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4961 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5039 time: 1.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5039 time: 1.10s
Epoch 5/1000, LR 0.000105
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4961 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5039 time: 1.10s
Epoch 6/1000, LR 0.000135
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5039 time: 1.07s
Epoch 7/1000, LR 0.000165
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5039 time: 1.11s
Epoch 8/1000, LR 0.000195
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 1.08s
Epoch 9/1000, LR 0.000225
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 1.09s
Epoch 10/1000, LR 0.000255
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 1.09s
Epoch 11/1000, LR 0.000285
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.98s
Epoch 12/1000, LR 0.000285
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 1.10s
Epoch 13/1000, LR 0.000285
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5039 time: 0.96s
Epoch 14/1000, LR 0.000285
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5039 time: 1.02s
Epoch 15/1000, LR 0.000285
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5039 time: 1.12s
Epoch 16/1000, LR 0.000285
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5039 time: 1.12s
Epoch 17/1000, LR 0.000285
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.5039 time: 0.97s
Epoch 18/1000, LR 0.000285
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5039 time: 1.07s
Epoch 19/1000, LR 0.000285
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.4961 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6797 score: 0.5039 time: 1.09s
Epoch 20/1000, LR 0.000285
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6771 score: 0.4961 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6757 score: 0.5039 time: 1.00s
Epoch 21/1000, LR 0.000285
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6719 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6704 score: 0.5039 time: 1.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6654 score: 0.4961 time: 1.11s
Test loss: 0.6636 score: 0.5116 time: 0.99s
Epoch 23/1000, LR 0.000285
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 1.19s
Val loss: 0.6573 score: 0.5426 time: 0.98s
Test loss: 0.6553 score: 0.5194 time: 1.08s
Epoch 24/1000, LR 0.000285
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 1.00s
Val loss: 0.6473 score: 0.6202 time: 0.92s
Test loss: 0.6451 score: 0.5814 time: 1.09s
Epoch 25/1000, LR 0.000285
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 1.01s
Val loss: 0.6354 score: 0.7132 time: 0.91s
Test loss: 0.6330 score: 0.6667 time: 1.24s
Epoch 26/1000, LR 0.000285
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.96s
Val loss: 0.6214 score: 0.7907 time: 0.91s
Test loss: 0.6189 score: 0.7519 time: 1.10s
Epoch 27/1000, LR 0.000285
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 1.00s
Val loss: 0.6053 score: 0.8062 time: 1.03s
Test loss: 0.6029 score: 0.7752 time: 0.99s
Epoch 28/1000, LR 0.000285
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 0.98s
Val loss: 0.5866 score: 0.8295 time: 1.05s
Test loss: 0.5846 score: 0.8062 time: 1.00s
Epoch 29/1000, LR 0.000285
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 1.05s
Val loss: 0.5659 score: 0.8372 time: 1.00s
Test loss: 0.5646 score: 0.8140 time: 1.01s
Epoch 30/1000, LR 0.000285
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 1.01s
Val loss: 0.5429 score: 0.8527 time: 1.00s
Test loss: 0.5426 score: 0.8295 time: 0.99s
Epoch 31/1000, LR 0.000285
Train loss: 0.5359;  Loss pred: 0.5359; Loss self: 0.0000; time: 1.11s
Val loss: 0.5181 score: 0.8682 time: 0.94s
Test loss: 0.5190 score: 0.8527 time: 1.10s
Epoch 32/1000, LR 0.000285
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 1.17s
Val loss: 0.4920 score: 0.8682 time: 1.01s
Test loss: 0.4944 score: 0.8527 time: 1.09s
Epoch 33/1000, LR 0.000285
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.98s
Val loss: 0.4649 score: 0.8760 time: 0.98s
Test loss: 0.4691 score: 0.8527 time: 1.08s
Epoch 34/1000, LR 0.000285
Train loss: 0.4521;  Loss pred: 0.4521; Loss self: 0.0000; time: 0.98s
Val loss: 0.4379 score: 0.8915 time: 0.94s
Test loss: 0.4440 score: 0.8682 time: 1.07s
Epoch 35/1000, LR 0.000285
Train loss: 0.4274;  Loss pred: 0.4274; Loss self: 0.0000; time: 0.99s
Val loss: 0.4117 score: 0.9147 time: 0.89s
Test loss: 0.4196 score: 0.8682 time: 1.10s
Epoch 36/1000, LR 0.000285
Train loss: 0.3988;  Loss pred: 0.3988; Loss self: 0.0000; time: 1.01s
Val loss: 0.3852 score: 0.9302 time: 1.02s
Test loss: 0.3948 score: 0.8760 time: 1.00s
Epoch 37/1000, LR 0.000285
Train loss: 0.3717;  Loss pred: 0.3717; Loss self: 0.0000; time: 1.01s
Val loss: 0.3602 score: 0.9380 time: 1.03s
Test loss: 0.3716 score: 0.8915 time: 0.99s
Epoch 38/1000, LR 0.000284
Train loss: 0.3409;  Loss pred: 0.3409; Loss self: 0.0000; time: 1.00s
Val loss: 0.3375 score: 0.9380 time: 1.02s
Test loss: 0.3505 score: 0.8915 time: 1.10s
Epoch 39/1000, LR 0.000284
Train loss: 0.3132;  Loss pred: 0.3132; Loss self: 0.0000; time: 1.01s
Val loss: 0.3172 score: 0.9457 time: 1.02s
Test loss: 0.3312 score: 0.8915 time: 1.02s
Epoch 40/1000, LR 0.000284
Train loss: 0.2942;  Loss pred: 0.2942; Loss self: 0.0000; time: 1.06s
Val loss: 0.2967 score: 0.9457 time: 0.92s
Test loss: 0.3120 score: 0.8837 time: 1.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.2731;  Loss pred: 0.2731; Loss self: 0.0000; time: 0.97s
Val loss: 0.2789 score: 0.9457 time: 0.96s
Test loss: 0.2951 score: 0.8915 time: 1.23s
Epoch 42/1000, LR 0.000284
Train loss: 0.2595;  Loss pred: 0.2595; Loss self: 0.0000; time: 1.26s
Val loss: 0.2611 score: 0.9457 time: 1.11s
Test loss: 0.2784 score: 0.8915 time: 1.13s
Epoch 43/1000, LR 0.000284
Train loss: 0.2378;  Loss pred: 0.2378; Loss self: 0.0000; time: 0.96s
Val loss: 0.2485 score: 0.9457 time: 1.01s
Test loss: 0.2658 score: 0.8915 time: 1.03s
Epoch 44/1000, LR 0.000284
Train loss: 0.2121;  Loss pred: 0.2121; Loss self: 0.0000; time: 0.97s
Val loss: 0.2389 score: 0.9457 time: 1.05s
Test loss: 0.2556 score: 0.8915 time: 0.99s
Epoch 45/1000, LR 0.000284
Train loss: 0.2029;  Loss pred: 0.2029; Loss self: 0.0000; time: 1.08s
Val loss: 0.2340 score: 0.9457 time: 0.95s
Test loss: 0.2488 score: 0.8915 time: 1.10s
Epoch 46/1000, LR 0.000284
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 1.09s
Val loss: 0.2306 score: 0.9457 time: 0.90s
Test loss: 0.2433 score: 0.8915 time: 1.09s
Epoch 47/1000, LR 0.000284
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 1.04s
Val loss: 0.2284 score: 0.9457 time: 0.93s
Test loss: 0.2386 score: 0.8915 time: 0.98s
Epoch 48/1000, LR 0.000284
Train loss: 0.1801;  Loss pred: 0.1801; Loss self: 0.0000; time: 1.09s
Val loss: 0.2241 score: 0.9457 time: 0.97s
Test loss: 0.2322 score: 0.8915 time: 1.14s
Epoch 49/1000, LR 0.000284
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.95s
Val loss: 0.2212 score: 0.9457 time: 0.92s
Test loss: 0.2265 score: 0.8992 time: 1.10s
Epoch 50/1000, LR 0.000284
Train loss: 0.1574;  Loss pred: 0.1574; Loss self: 0.0000; time: 0.94s
Val loss: 0.2234 score: 0.9457 time: 0.89s
Test loss: 0.2240 score: 0.9070 time: 1.07s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1459;  Loss pred: 0.1459; Loss self: 0.0000; time: 0.95s
Val loss: 0.2250 score: 0.9457 time: 1.00s
Test loss: 0.2207 score: 0.9070 time: 0.96s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1363;  Loss pred: 0.1363; Loss self: 0.0000; time: 0.94s
Val loss: 0.2315 score: 0.9457 time: 0.98s
Test loss: 0.2203 score: 0.9070 time: 0.98s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1290;  Loss pred: 0.1290; Loss self: 0.0000; time: 0.95s
Val loss: 0.2366 score: 0.9535 time: 1.00s
Test loss: 0.2188 score: 0.8992 time: 1.02s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1289;  Loss pred: 0.1289; Loss self: 0.0000; time: 1.38s
Val loss: 0.2411 score: 0.9535 time: 1.05s
Test loss: 0.2162 score: 0.9070 time: 1.01s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 1.07s
Val loss: 0.2441 score: 0.9535 time: 0.95s
Test loss: 0.2119 score: 0.9070 time: 1.05s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1105;  Loss pred: 0.1105; Loss self: 0.0000; time: 0.96s
Val loss: 0.2437 score: 0.9535 time: 1.01s
Test loss: 0.2049 score: 0.9070 time: 1.10s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1082;  Loss pred: 0.1082; Loss self: 0.0000; time: 2.72s
Val loss: 0.2477 score: 0.9535 time: 3.60s
Test loss: 0.2013 score: 0.9070 time: 1.02s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 1.09s
Val loss: 0.2554 score: 0.9535 time: 0.96s
Test loss: 0.2007 score: 0.9225 time: 1.09s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 1.01s
Val loss: 0.2669 score: 0.9457 time: 0.90s
Test loss: 0.2038 score: 0.9225 time: 1.13s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.98s
Val loss: 0.2737 score: 0.9457 time: 1.03s
Test loss: 0.2035 score: 0.9147 time: 1.04s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0944;  Loss pred: 0.0944; Loss self: 0.0000; time: 0.97s
Val loss: 0.2704 score: 0.9535 time: 1.15s
Test loss: 0.1946 score: 0.9302 time: 0.98s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.98s
Val loss: 0.2688 score: 0.9612 time: 1.06s
Test loss: 0.1878 score: 0.9302 time: 1.00s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0820;  Loss pred: 0.0820; Loss self: 0.0000; time: 1.18s
Val loss: 0.2718 score: 0.9612 time: 0.95s
Test loss: 0.1852 score: 0.9302 time: 1.03s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 1.09s
Val loss: 0.2764 score: 0.9612 time: 1.00s
Test loss: 0.1844 score: 0.9380 time: 1.15s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 1.01s
Val loss: 0.2800 score: 0.9612 time: 0.95s
Test loss: 0.1831 score: 0.9380 time: 1.13s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0776;  Loss pred: 0.0776; Loss self: 0.0000; time: 1.00s
Val loss: 0.2866 score: 0.9612 time: 0.93s
Test loss: 0.1852 score: 0.9457 time: 1.10s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.99s
Val loss: 0.2931 score: 0.9612 time: 1.03s
Test loss: 0.1881 score: 0.9380 time: 0.99s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.97s
Val loss: 0.2955 score: 0.9612 time: 1.02s
Test loss: 0.1873 score: 0.9380 time: 1.03s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.97s
Val loss: 0.2962 score: 0.9612 time: 1.03s
Test loss: 0.1847 score: 0.9380 time: 0.97s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.1572,   Val_Loss: 0.2212,   Val_Precision: 0.9254,   Val_Recall: 0.9688,   Val_accuracy: 0.9466,   Val_Score: 0.9457,   Val_Loss: 0.2212,   Test_Precision: 0.9333,   Test_Recall: 0.8615,   Test_accuracy: 0.8960,   Test_Score: 0.8992,   Test_loss: 0.2265


[0.9637760268524289, 0.9761781608685851, 1.1240670389961451, 1.100541983032599, 1.1013713988941163, 1.0713056640233845, 1.114577128086239, 1.0827825812157243, 1.0964749529957771, 1.0987144690006971, 0.9834696392063051, 1.1064946600235999, 0.9682277520187199, 1.023572813021019, 1.1258170760702342, 1.126716820988804, 0.9726898579392582, 1.0797252899501473, 1.0967197900172323, 1.0038856351748109, 1.0647708270698786, 0.9938502539880574, 1.0878181839361787, 1.0954082319512963, 1.2491978430189192, 1.100653411122039, 0.9940285610500723, 1.006910493830219, 1.0106097091920674, 0.9967217440716922, 1.1020638290792704, 1.0954450289718807, 1.088455606950447, 1.0799323271494359, 1.1031788710970432, 1.0011288397945464, 0.9936848818324506, 1.1075573631096631, 1.020720822038129, 1.1713787191547453, 1.2366481029894203, 1.1395299318246543, 1.0356491161510348, 0.9946530479937792, 1.1081010620109737, 1.0910091609694064, 0.9856091709807515, 1.1495546619407833, 1.1022630319930613, 1.071163880173117, 0.9643036620691419, 0.9868234780151397, 1.0261700439732522, 1.0131932699587196, 1.0582857369445264, 1.1025965961162, 1.0282378040719777, 1.098047369159758, 1.1340656550601125, 1.0451302339788526, 0.987706562038511, 1.0096238940022886, 1.032642001984641, 1.1588029190897942, 1.135090515948832, 1.108585350913927, 0.9936184010002762, 1.0361043692100793, 0.9761980560142547]
[0.0074711319911041005, 0.007567272564872753, 0.008713697976714303, 0.008531333201803094, 0.008537762782124932, 0.008304695069948716, 0.008640132775862319, 0.008393663420276932, 0.008499805837176567, 0.008517166426361994, 0.007623795652762055, 0.00857747798467907, 0.007505641488517209, 0.007934672969155187, 0.008727264155583211, 0.00873423892239383, 0.007540231456893475, 0.008369963487985637, 0.008501703798583195, 0.007782059187401635, 0.008254037419146345, 0.007704265534791143, 0.008432699100280454, 0.008491536681792995, 0.009683704209448986, 0.008532196985442162, 0.00770564776007808, 0.007805507704110224, 0.00783418379218657, 0.007726525147842576, 0.00854313045797884, 0.00849182193001458, 0.008437640363956953, 0.008371568427515007, 0.008551774194550723, 0.007760688680577879, 0.007702983580096517, 0.008585715993098163, 0.00791256451192348, 0.009080455187246088, 0.009586419403018761, 0.008833565362981816, 0.008028287722101046, 0.007710488744137823, 0.008589930713263362, 0.008457435356351987, 0.007640381170393422, 0.008911276449153358, 0.008544674666612878, 0.00830359597033424, 0.007475222186582495, 0.007649794403218137, 0.00795480654242831, 0.007854211395028834, 0.008203765402670747, 0.008547260435009304, 0.007970835690480448, 0.008511995109765566, 0.008791206628372964, 0.00810178475952599, 0.007656640015802411, 0.00782654181397123, 0.00800497675957086, 0.008982968365037165, 0.008799151286425055, 0.008593684890805635, 0.007702468224808343, 0.00803181681558201, 0.007567426790808176]
[133.84852538955317, 132.14800860246476, 114.76183850671774, 117.21497406626322, 117.12670233631317, 120.41381309936226, 115.73896211336823, 119.13749097733144, 117.64974625963647, 117.40994010693979, 131.1682586399999, 116.58438550191343, 133.23311558777328, 126.0291386787263, 114.58344587407228, 114.49194473442749, 132.6219235731516, 119.47483420153671, 117.6234815622075, 128.5006931865667, 121.15283093827101, 129.79822612345998, 118.58599341778269, 117.76431492595863, 103.26626860661837, 117.20310744187269, 129.77494315025206, 128.1146644020888, 127.64571607285382, 129.42428593262562, 117.0531112592401, 117.76035911274498, 118.5165469094532, 119.45192930792746, 116.93479940539241, 128.85454386317912, 129.81982755147845, 116.47252259495589, 126.38127606960997, 110.12663785891984, 104.31423433081805, 113.20457356784019, 124.55956171664145, 129.6934647314395, 116.41537439364217, 118.23915381735036, 130.8835223921829, 112.21736927429828, 117.0319572151015, 120.42975158866594, 133.77528788307197, 130.72246746648736, 125.71015959550118, 127.32022983656003, 121.89524577024699, 116.9965520067731, 125.45735965857355, 117.48127050175685, 113.75002798507482, 123.42959356260495, 130.60559173947277, 127.77035167880801, 124.9222864768953, 111.32177687412604, 113.64732432123951, 116.36451798109304, 129.82851351196356, 124.50483159177192, 132.1453153950107]
Elapsed: 1.0636236430921902~0.06407068105148835
Time per graph: 0.008245144520094495~0.0004966719461355685
Speed: 121.71740282330467~7.2224994293349365
Total Time: 0.9767
best val loss: 0.221178747078245 test_score: 0.8992

Testing...
Test loss: 0.1878 score: 0.9302 time: 1.02s
test Score 0.9302
Epoch Time List: [2.909240392036736, 2.934292125981301, 3.0839556800201535, 2.951469091232866, 2.946334370179102, 2.976989096030593, 2.9960957439616323, 2.9796733430121094, 2.9709932790137827, 3.149184667970985, 3.21980641502887, 3.2747408931609243, 2.916105980053544, 3.0043148330878466, 3.0951598291285336, 2.983654929790646, 2.9773641189094633, 2.9888130391482264, 3.003319649025798, 2.9939547800458968, 3.1902023346628994, 3.0694199518766254, 3.253309397958219, 3.010480142896995, 3.1597953368909657, 2.9722759379073977, 3.015066066524014, 3.0332930120639503, 3.0554769460577518, 2.9996400051750243, 3.1508818240836263, 3.26415589498356, 3.0392821941059083, 2.990817334735766, 2.9807805272284895, 3.0240983061958104, 3.027376594254747, 3.1171056423336267, 3.050524576101452, 3.1457512269262224, 3.1659049049485475, 3.506551507860422, 3.005423729075119, 3.0091167059727013, 3.1335303757805377, 3.074242863804102, 2.945005143294111, 3.200382764916867, 2.9675183489453048, 2.9024988189339638, 2.914166749920696, 2.899786193855107, 2.9760694270953536, 3.4385116810444742, 3.069464805070311, 3.067209405824542, 7.338786747772247, 3.139812321867794, 3.039477188838646, 3.0526075221132487, 3.105636761058122, 3.039047881960869, 3.1614181341137737, 3.242652312386781, 3.095849942183122, 3.0359953728038818, 3.0040191339794546, 3.0169911871198565, 2.9674827640410513]
Total Epoch List: [69]
Total Time List: [0.9767056340351701]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x713893536d70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.88s
Epoch 2/1000, LR 0.000015
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.96s
Epoch 3/1000, LR 0.000045
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.00s
Epoch 4/1000, LR 0.000075
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.02s
Epoch 5/1000, LR 0.000105
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.91s
Epoch 6/1000, LR 0.000135
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 1.04s
Epoch 7/1000, LR 0.000165
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.96s
Epoch 8/1000, LR 0.000195
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.05s
Val loss: 0.6925 score: 0.5039 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.93s
Epoch 9/1000, LR 0.000225
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.14s
Val loss: 0.6921 score: 0.5194 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.99s
Epoch 10/1000, LR 0.000255
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.09s
Val loss: 0.6917 score: 0.5659 time: 1.07s
Test loss: 0.6921 score: 0.5891 time: 0.98s
Epoch 11/1000, LR 0.000285
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.12s
Val loss: 0.6911 score: 0.8372 time: 0.98s
Test loss: 0.6918 score: 0.6899 time: 1.13s
Epoch 12/1000, LR 0.000285
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 1.12s
Val loss: 0.6905 score: 0.7054 time: 1.03s
Test loss: 0.6913 score: 0.6512 time: 0.90s
Epoch 13/1000, LR 0.000285
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 1.16s
Val loss: 0.6897 score: 0.6357 time: 0.97s
Test loss: 0.6908 score: 0.6047 time: 1.01s
Epoch 14/1000, LR 0.000285
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 1.04s
Val loss: 0.6888 score: 0.5659 time: 1.00s
Test loss: 0.6902 score: 0.5814 time: 0.99s
Epoch 15/1000, LR 0.000285
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.00s
Val loss: 0.6877 score: 0.5659 time: 1.05s
Test loss: 0.6895 score: 0.5504 time: 0.98s
Epoch 16/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 1.00s
Val loss: 0.6863 score: 0.5736 time: 1.06s
Test loss: 0.6885 score: 0.5581 time: 0.87s
Epoch 17/1000, LR 0.000285
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 1.02s
Val loss: 0.6846 score: 0.5736 time: 1.08s
Test loss: 0.6873 score: 0.5504 time: 0.89s
Epoch 18/1000, LR 0.000285
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 1.03s
Val loss: 0.6824 score: 0.5814 time: 1.06s
Test loss: 0.6858 score: 0.5504 time: 0.89s
Epoch 19/1000, LR 0.000285
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 1.11s
Val loss: 0.6797 score: 0.5814 time: 0.96s
Test loss: 0.6838 score: 0.5814 time: 0.88s
Epoch 20/1000, LR 0.000285
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 1.14s
Val loss: 0.6765 score: 0.5814 time: 0.97s
Test loss: 0.6815 score: 0.6047 time: 0.93s
Epoch 21/1000, LR 0.000285
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 1.13s
Val loss: 0.6725 score: 0.5969 time: 1.03s
Test loss: 0.6787 score: 0.6047 time: 0.87s
Epoch 22/1000, LR 0.000285
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 1.12s
Val loss: 0.6679 score: 0.6434 time: 1.00s
Test loss: 0.6753 score: 0.6124 time: 0.91s
Epoch 23/1000, LR 0.000285
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 1.12s
Val loss: 0.6622 score: 0.6512 time: 1.01s
Test loss: 0.6711 score: 0.6434 time: 0.98s
Epoch 24/1000, LR 0.000285
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 1.03s
Val loss: 0.6554 score: 0.6512 time: 0.96s
Test loss: 0.6658 score: 0.6434 time: 0.98s
Epoch 25/1000, LR 0.000285
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 1.24s
Val loss: 0.6472 score: 0.6744 time: 1.26s
Test loss: 0.6595 score: 0.6589 time: 1.25s
Epoch 26/1000, LR 0.000285
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 1.12s
Val loss: 0.6379 score: 0.7209 time: 1.08s
Test loss: 0.6521 score: 0.6899 time: 0.93s
Epoch 27/1000, LR 0.000285
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 1.12s
Val loss: 0.6271 score: 0.7907 time: 1.11s
Test loss: 0.6433 score: 0.7519 time: 0.89s
Epoch 28/1000, LR 0.000285
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 1.04s
Val loss: 0.6145 score: 0.8295 time: 1.10s
Test loss: 0.6328 score: 0.7752 time: 0.90s
Epoch 29/1000, LR 0.000285
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 1.05s
Val loss: 0.5998 score: 0.8527 time: 1.12s
Test loss: 0.6204 score: 0.8217 time: 0.89s
Epoch 30/1000, LR 0.000285
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 1.08s
Val loss: 0.5827 score: 0.8760 time: 1.07s
Test loss: 0.6058 score: 0.8605 time: 0.89s
Epoch 31/1000, LR 0.000285
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 1.06s
Val loss: 0.5630 score: 0.8837 time: 1.07s
Test loss: 0.5887 score: 0.8760 time: 0.90s
Epoch 32/1000, LR 0.000285
Train loss: 0.5601;  Loss pred: 0.5601; Loss self: 0.0000; time: 1.23s
Val loss: 0.5411 score: 0.8837 time: 0.99s
Test loss: 0.5693 score: 0.8837 time: 0.92s
Epoch 33/1000, LR 0.000285
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 1.12s
Val loss: 0.5171 score: 0.8992 time: 1.02s
Test loss: 0.5480 score: 0.8915 time: 0.87s
Epoch 34/1000, LR 0.000285
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 1.13s
Val loss: 0.4920 score: 0.9070 time: 0.99s
Test loss: 0.5253 score: 0.8915 time: 0.89s
Epoch 35/1000, LR 0.000285
Train loss: 0.4868;  Loss pred: 0.4868; Loss self: 0.0000; time: 1.10s
Val loss: 0.4666 score: 0.9070 time: 0.96s
Test loss: 0.5020 score: 0.8837 time: 0.88s
Epoch 36/1000, LR 0.000285
Train loss: 0.4588;  Loss pred: 0.4588; Loss self: 0.0000; time: 1.10s
Val loss: 0.4413 score: 0.9070 time: 0.96s
Test loss: 0.4777 score: 0.8837 time: 0.99s
Epoch 37/1000, LR 0.000285
Train loss: 0.4303;  Loss pred: 0.4303; Loss self: 0.0000; time: 1.02s
Val loss: 0.4168 score: 0.8992 time: 0.98s
Test loss: 0.4542 score: 0.8837 time: 0.99s
Epoch 38/1000, LR 0.000284
Train loss: 0.4054;  Loss pred: 0.4054; Loss self: 0.0000; time: 1.01s
Val loss: 0.3935 score: 0.9070 time: 1.07s
Test loss: 0.4312 score: 0.8837 time: 0.88s
Epoch 39/1000, LR 0.000284
Train loss: 0.3809;  Loss pred: 0.3809; Loss self: 0.0000; time: 1.04s
Val loss: 0.3715 score: 0.9070 time: 1.09s
Test loss: 0.4090 score: 0.8915 time: 0.87s
Epoch 40/1000, LR 0.000284
Train loss: 0.3499;  Loss pred: 0.3499; Loss self: 0.0000; time: 1.03s
Val loss: 0.3511 score: 0.9147 time: 1.13s
Test loss: 0.3881 score: 0.9070 time: 0.92s
Epoch 41/1000, LR 0.000284
Train loss: 0.3277;  Loss pred: 0.3277; Loss self: 0.0000; time: 1.15s
Val loss: 0.3322 score: 0.9147 time: 0.99s
Test loss: 0.3686 score: 0.9070 time: 0.87s
Epoch 42/1000, LR 0.000284
Train loss: 0.3034;  Loss pred: 0.3034; Loss self: 0.0000; time: 1.17s
Val loss: 0.3151 score: 0.9147 time: 0.97s
Test loss: 0.3504 score: 0.9070 time: 0.92s
Epoch 43/1000, LR 0.000284
Train loss: 0.2852;  Loss pred: 0.2852; Loss self: 0.0000; time: 1.15s
Val loss: 0.3001 score: 0.9147 time: 0.97s
Test loss: 0.3342 score: 0.9070 time: 0.90s
Epoch 44/1000, LR 0.000284
Train loss: 0.2584;  Loss pred: 0.2584; Loss self: 0.0000; time: 1.14s
Val loss: 0.2872 score: 0.9225 time: 0.98s
Test loss: 0.3206 score: 0.9070 time: 1.03s
Epoch 45/1000, LR 0.000284
Train loss: 0.2400;  Loss pred: 0.2400; Loss self: 0.0000; time: 1.03s
Val loss: 0.2767 score: 0.9225 time: 1.04s
Test loss: 0.3097 score: 0.9070 time: 0.98s
Epoch 46/1000, LR 0.000284
Train loss: 0.2251;  Loss pred: 0.2251; Loss self: 0.0000; time: 1.08s
Val loss: 0.2689 score: 0.9147 time: 1.25s
Test loss: 0.3014 score: 0.9147 time: 0.98s
Epoch 47/1000, LR 0.000284
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 1.02s
Val loss: 0.2639 score: 0.9147 time: 0.99s
Test loss: 0.2965 score: 0.9147 time: 0.98s
Epoch 48/1000, LR 0.000284
Train loss: 0.2001;  Loss pred: 0.2001; Loss self: 0.0000; time: 1.04s
Val loss: 0.2612 score: 0.9147 time: 0.94s
Test loss: 0.2947 score: 0.9147 time: 0.98s
Epoch 49/1000, LR 0.000284
Train loss: 0.1762;  Loss pred: 0.1762; Loss self: 0.0000; time: 1.08s
Val loss: 0.2605 score: 0.9147 time: 5.37s
Test loss: 0.2958 score: 0.9147 time: 0.88s
Epoch 50/1000, LR 0.000284
Train loss: 0.1652;  Loss pred: 0.1652; Loss self: 0.0000; time: 1.03s
Val loss: 0.2616 score: 0.9147 time: 1.11s
Test loss: 0.2994 score: 0.9225 time: 0.89s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 1.05s
Val loss: 0.2640 score: 0.9225 time: 1.04s
Test loss: 0.3054 score: 0.9225 time: 0.92s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1318;  Loss pred: 0.1318; Loss self: 0.0000; time: 1.08s
Val loss: 0.2681 score: 0.9225 time: 1.05s
Test loss: 0.3137 score: 0.9225 time: 0.92s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 1.00s
Val loss: 0.2739 score: 0.9225 time: 1.08s
Test loss: 0.3234 score: 0.9225 time: 0.89s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1162;  Loss pred: 0.1162; Loss self: 0.0000; time: 1.11s
Val loss: 0.2807 score: 0.9225 time: 0.97s
Test loss: 0.3345 score: 0.9225 time: 0.87s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1142;  Loss pred: 0.1142; Loss self: 0.0000; time: 1.10s
Val loss: 0.2891 score: 0.9302 time: 0.97s
Test loss: 0.3479 score: 0.9225 time: 0.98s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1193;  Loss pred: 0.1193; Loss self: 0.0000; time: 1.00s
Val loss: 0.2981 score: 0.9457 time: 0.96s
Test loss: 0.3618 score: 0.9225 time: 0.97s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.99s
Val loss: 0.3086 score: 0.9457 time: 0.97s
Test loss: 0.3773 score: 0.9302 time: 1.01s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 1.01s
Val loss: 0.3188 score: 0.9380 time: 1.00s
Test loss: 0.3925 score: 0.9225 time: 1.03s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 1.02s
Val loss: 0.3292 score: 0.9457 time: 1.10s
Test loss: 0.4079 score: 0.9147 time: 0.90s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0796;  Loss pred: 0.0796; Loss self: 0.0000; time: 1.11s
Val loss: 0.3375 score: 0.9380 time: 1.06s
Test loss: 0.4199 score: 0.9147 time: 0.88s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 1.04s
Val loss: 0.3438 score: 0.9457 time: 1.06s
Test loss: 0.4286 score: 0.9147 time: 0.88s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0796;  Loss pred: 0.0796; Loss self: 0.0000; time: 1.04s
Val loss: 0.3508 score: 0.9457 time: 1.08s
Test loss: 0.4385 score: 0.9147 time: 0.92s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 1.01s
Val loss: 0.3580 score: 0.9457 time: 1.08s
Test loss: 0.4486 score: 0.9225 time: 0.95s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 1.12s
Val loss: 0.3584 score: 0.9457 time: 1.00s
Test loss: 0.4451 score: 0.9225 time: 0.89s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 1.12s
Val loss: 0.3637 score: 0.9457 time: 1.01s
Test loss: 0.4493 score: 0.9225 time: 0.90s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 1.17s
Val loss: 0.3695 score: 0.9457 time: 0.97s
Test loss: 0.4581 score: 0.9225 time: 1.11s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 1.08s
Val loss: 0.3762 score: 0.9380 time: 1.00s
Test loss: 0.4702 score: 0.9225 time: 1.04s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 1.08s
Val loss: 0.3845 score: 0.9380 time: 1.05s
Test loss: 0.4856 score: 0.9302 time: 1.07s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 1.26s
Val loss: 0.3934 score: 0.9457 time: 1.35s
Test loss: 0.5009 score: 0.9225 time: 0.92s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.1762,   Val_Loss: 0.2605,   Val_Precision: 0.9500,   Val_Recall: 0.8769,   Val_accuracy: 0.9120,   Val_Score: 0.9147,   Val_Loss: 0.2605,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2958


[0.9637760268524289, 0.9761781608685851, 1.1240670389961451, 1.100541983032599, 1.1013713988941163, 1.0713056640233845, 1.114577128086239, 1.0827825812157243, 1.0964749529957771, 1.0987144690006971, 0.9834696392063051, 1.1064946600235999, 0.9682277520187199, 1.023572813021019, 1.1258170760702342, 1.126716820988804, 0.9726898579392582, 1.0797252899501473, 1.0967197900172323, 1.0038856351748109, 1.0647708270698786, 0.9938502539880574, 1.0878181839361787, 1.0954082319512963, 1.2491978430189192, 1.100653411122039, 0.9940285610500723, 1.006910493830219, 1.0106097091920674, 0.9967217440716922, 1.1020638290792704, 1.0954450289718807, 1.088455606950447, 1.0799323271494359, 1.1031788710970432, 1.0011288397945464, 0.9936848818324506, 1.1075573631096631, 1.020720822038129, 1.1713787191547453, 1.2366481029894203, 1.1395299318246543, 1.0356491161510348, 0.9946530479937792, 1.1081010620109737, 1.0910091609694064, 0.9856091709807515, 1.1495546619407833, 1.1022630319930613, 1.071163880173117, 0.9643036620691419, 0.9868234780151397, 1.0261700439732522, 1.0131932699587196, 1.0582857369445264, 1.1025965961162, 1.0282378040719777, 1.098047369159758, 1.1340656550601125, 1.0451302339788526, 0.987706562038511, 1.0096238940022886, 1.032642001984641, 1.1588029190897942, 1.135090515948832, 1.108585350913927, 0.9936184010002762, 1.0361043692100793, 0.9761980560142547, 0.884632650995627, 0.9631265311036259, 1.0065412500407547, 1.0224952609278262, 0.9144253579434007, 1.0444052759557962, 0.9608418450225145, 0.9388588960282505, 0.9946650960482657, 0.9842329320963472, 1.1330781830474734, 0.9046641399618238, 1.0167175428941846, 0.9924674578942358, 0.9810194249730557, 0.8784757398534566, 0.8998384170699865, 0.8936462341807783, 0.8895393249113113, 0.933447573101148, 0.8714342780876905, 0.9134311689995229, 0.9857536158524454, 0.9817497711628675, 1.256291237194091, 0.9334021890535951, 0.8906535930000246, 0.9065381961409003, 0.8922830699011683, 0.890635872958228, 0.9052440219093114, 0.9295939800795168, 0.8759913449175656, 0.8966779480688274, 0.8825386760290712, 0.999086385127157, 0.9974465570412576, 0.8884468800388277, 0.8749147239141166, 0.9224917020183057, 0.87734295707196, 0.922664649086073, 0.9026024299673736, 1.0383363380096853, 0.9899145450908691, 0.9813540149480104, 0.9831679358612746, 0.9808865680824965, 0.8801054200157523, 0.8989011361263692, 0.925942812114954, 0.9206830249167979, 0.8904868869576603, 0.8731046461034566, 0.9880308229476213, 0.9726652770768851, 1.019775178981945, 1.0312004541046917, 0.9004152088891715, 0.8880199079867452, 0.8846541119273752, 0.928207929013297, 0.9545434101019055, 0.8933042308781296, 0.9069063391070813, 1.1122702788561583, 1.048025059979409, 1.0763640841469169, 0.9215175660792738]
[0.0074711319911041005, 0.007567272564872753, 0.008713697976714303, 0.008531333201803094, 0.008537762782124932, 0.008304695069948716, 0.008640132775862319, 0.008393663420276932, 0.008499805837176567, 0.008517166426361994, 0.007623795652762055, 0.00857747798467907, 0.007505641488517209, 0.007934672969155187, 0.008727264155583211, 0.00873423892239383, 0.007540231456893475, 0.008369963487985637, 0.008501703798583195, 0.007782059187401635, 0.008254037419146345, 0.007704265534791143, 0.008432699100280454, 0.008491536681792995, 0.009683704209448986, 0.008532196985442162, 0.00770564776007808, 0.007805507704110224, 0.00783418379218657, 0.007726525147842576, 0.00854313045797884, 0.00849182193001458, 0.008437640363956953, 0.008371568427515007, 0.008551774194550723, 0.007760688680577879, 0.007702983580096517, 0.008585715993098163, 0.00791256451192348, 0.009080455187246088, 0.009586419403018761, 0.008833565362981816, 0.008028287722101046, 0.007710488744137823, 0.008589930713263362, 0.008457435356351987, 0.007640381170393422, 0.008911276449153358, 0.008544674666612878, 0.00830359597033424, 0.007475222186582495, 0.007649794403218137, 0.00795480654242831, 0.007854211395028834, 0.008203765402670747, 0.008547260435009304, 0.007970835690480448, 0.008511995109765566, 0.008791206628372964, 0.00810178475952599, 0.007656640015802411, 0.00782654181397123, 0.00800497675957086, 0.008982968365037165, 0.008799151286425055, 0.008593684890805635, 0.007702468224808343, 0.00803181681558201, 0.007567426790808176, 0.0068576174495785045, 0.007466097140338185, 0.007802645349153137, 0.00792631985215369, 0.007088568666227913, 0.008096164929889894, 0.007448386395523368, 0.007277975938203492, 0.007710582139909036, 0.007629712651909668, 0.008783551806569562, 0.00701290030978158, 0.007881531340264997, 0.007693546185226634, 0.007604801743977176, 0.006809889456228346, 0.006975491605193693, 0.006927490187447894, 0.006895653681483033, 0.007236027698458511, 0.006755304481299926, 0.0070808617751901, 0.007641500898080971, 0.00761046334234781, 0.009738691761194504, 0.007235675884136396, 0.006904291418604842, 0.007027427877061242, 0.006916923022489677, 0.006904154053939752, 0.0070173955186768324, 0.00720615488433734, 0.006790630580756322, 0.006950991845494786, 0.006841385085496676, 0.007744855698660132, 0.007732143853033005, 0.0068871851165800595, 0.00678228468150478, 0.007151098465258183, 0.006801108194356279, 0.007152439140202116, 0.006996918061762586, 0.008049118899299885, 0.007673756163495109, 0.0076073954647132595, 0.007621456867141664, 0.007603771845600748, 0.006822522635781026, 0.006968225861444723, 0.007177851256705069, 0.0071370777125333166, 0.006902999123702793, 0.0067682530705694315, 0.007659153666260631, 0.007540040907572753, 0.007905233945596472, 0.007993801969803812, 0.00697996285960598, 0.006883875255711203, 0.006857783813390505, 0.007195410302428659, 0.007399561318619422, 0.006924838999055268, 0.007030281698504506, 0.008622250223691149, 0.00812422527115821, 0.008343907629045867, 0.00714354702387034]
[133.84852538955317, 132.14800860246476, 114.76183850671774, 117.21497406626322, 117.12670233631317, 120.41381309936226, 115.73896211336823, 119.13749097733144, 117.64974625963647, 117.40994010693979, 131.1682586399999, 116.58438550191343, 133.23311558777328, 126.0291386787263, 114.58344587407228, 114.49194473442749, 132.6219235731516, 119.47483420153671, 117.6234815622075, 128.5006931865667, 121.15283093827101, 129.79822612345998, 118.58599341778269, 117.76431492595863, 103.26626860661837, 117.20310744187269, 129.77494315025206, 128.1146644020888, 127.64571607285382, 129.42428593262562, 117.0531112592401, 117.76035911274498, 118.5165469094532, 119.45192930792746, 116.93479940539241, 128.85454386317912, 129.81982755147845, 116.47252259495589, 126.38127606960997, 110.12663785891984, 104.31423433081805, 113.20457356784019, 124.55956171664145, 129.6934647314395, 116.41537439364217, 118.23915381735036, 130.8835223921829, 112.21736927429828, 117.0319572151015, 120.42975158866594, 133.77528788307197, 130.72246746648736, 125.71015959550118, 127.32022983656003, 121.89524577024699, 116.9965520067731, 125.45735965857355, 117.48127050175685, 113.75002798507482, 123.42959356260495, 130.60559173947277, 127.77035167880801, 124.9222864768953, 111.32177687412604, 113.64732432123951, 116.36451798109304, 129.82851351196356, 124.50483159177192, 132.1453153950107, 145.82324070314885, 133.93878772312678, 128.16166251981906, 126.16195392724231, 141.07220330167678, 123.5152703359762, 134.25726686266174, 137.40083898200436, 129.69189379672406, 131.0665349565565, 113.84916056988028, 142.594355520098, 126.87889660366147, 129.97907283902816, 131.4958671726027, 146.84526179575454, 143.35907153202461, 144.3524238853383, 145.01888380579638, 138.19737039052927, 148.0318174803528, 141.22574790314317, 130.864343711735, 131.39804437866226, 102.68319652385676, 138.20408984769688, 144.83745534050345, 142.2995749645721, 144.57295487438574, 144.84033701845993, 142.5030123125446, 138.7702618179234, 147.26172895251543, 143.86436097578502, 146.16923144992083, 129.11796409234597, 129.33023738400067, 145.1972007537044, 147.44294097931183, 139.8386562369752, 147.03486129360854, 139.81244445398266, 142.92006726002606, 124.23720068130442, 130.31427878267863, 131.45103401531819, 131.20851005682303, 131.51367772542542, 146.57335026716586, 143.50855151424008, 139.31745925577195, 140.11336856314662, 144.86457003395887, 147.74861246668297, 130.5627284128146, 132.62527514879417, 126.49847011258149, 125.09691931041701, 143.26723796585503, 145.26701354304038, 145.8197031594085, 138.97748119554365, 135.14314659217874, 144.40768949811346, 142.2418109096143, 115.97900479068957, 123.08865973351298, 119.84792311445457, 139.98647963798308]
Elapsed: 1.0066170213285133~0.08860339774838029
Time per graph: 0.007803232723476845~0.0006868480445610875
Speed: 129.13073549655942~11.171458068610475
Total Time: 0.9222
best val loss: 0.26047385715005933 test_score: 0.9147

Testing...
Test loss: 0.3618 score: 0.9225 time: 0.92s
test Score 0.9225
Epoch Time List: [2.909240392036736, 2.934292125981301, 3.0839556800201535, 2.951469091232866, 2.946334370179102, 2.976989096030593, 2.9960957439616323, 2.9796733430121094, 2.9709932790137827, 3.149184667970985, 3.21980641502887, 3.2747408931609243, 2.916105980053544, 3.0043148330878466, 3.0951598291285336, 2.983654929790646, 2.9773641189094633, 2.9888130391482264, 3.003319649025798, 2.9939547800458968, 3.1902023346628994, 3.0694199518766254, 3.253309397958219, 3.010480142896995, 3.1597953368909657, 2.9722759379073977, 3.015066066524014, 3.0332930120639503, 3.0554769460577518, 2.9996400051750243, 3.1508818240836263, 3.26415589498356, 3.0392821941059083, 2.990817334735766, 2.9807805272284895, 3.0240983061958104, 3.027376594254747, 3.1171056423336267, 3.050524576101452, 3.1457512269262224, 3.1659049049485475, 3.506551507860422, 3.005423729075119, 3.0091167059727013, 3.1335303757805377, 3.074242863804102, 2.945005143294111, 3.200382764916867, 2.9675183489453048, 2.9024988189339638, 2.914166749920696, 2.899786193855107, 2.9760694270953536, 3.4385116810444742, 3.069464805070311, 3.067209405824542, 7.338786747772247, 3.139812321867794, 3.039477188838646, 3.0526075221132487, 3.105636761058122, 3.039047881960869, 3.1614181341137737, 3.242652312386781, 3.095849942183122, 3.0359953728038818, 3.0040191339794546, 3.0169911871198565, 2.9674827640410513, 3.024623428005725, 3.044505547732115, 2.9934829589910805, 3.0002699429169297, 3.0018154338467866, 3.1212566981557757, 3.1691188768018037, 3.144173668231815, 3.302564633078873, 3.1435353776905686, 3.2262532140593976, 3.0550734270364046, 3.149890741100535, 3.0290436421055347, 3.0244528220500797, 2.9305955269373953, 2.9884560108184814, 2.9717959268018603, 2.9512693830765784, 3.0420830729417503, 3.0255811160895973, 3.030543126165867, 3.107161482097581, 2.9744363802019507, 3.746645870152861, 3.128938800888136, 3.1074421810917556, 3.0464791080448776, 3.0527613589074463, 3.0371368161868304, 3.031647790921852, 3.14588377205655, 3.0164540850091726, 3.012375903315842, 2.938176886178553, 3.0601072020363063, 2.985830174991861, 2.9544338788837194, 2.9990536288823932, 3.0798296029679477, 3.0132088840473443, 3.0570041011087596, 3.011946131940931, 3.1589493271894753, 3.0546242480631918, 3.30428093415685, 2.988624998833984, 2.9534630058333278, 7.316587320063263, 3.0335291081573814, 3.0160765680484474, 3.0411354689858854, 2.960126883117482, 2.9447408991400152, 3.0542384500149637, 2.927998567931354, 2.978385127382353, 3.0358472508378327, 3.016768520930782, 3.0580262932926416, 2.9829222038388252, 3.047097429865971, 3.0349009199999273, 3.01086701778695, 3.0330316161271185, 3.246414363849908, 3.1158339930698276, 3.1977083187084645, 3.5300576786976308]
Total Epoch List: [69, 69]
Total Time List: [0.9767056340351701, 0.9221587581560016]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x713893534550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.92s
Epoch 2/1000, LR 0.000020
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 1.03s
Epoch 3/1000, LR 0.000050
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 1.00s
Epoch 4/1000, LR 0.000080
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 1.03s
Epoch 5/1000, LR 0.000110
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 1.02s
Epoch 6/1000, LR 0.000140
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 1.07s
Epoch 7/1000, LR 0.000170
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.90s
Epoch 8/1000, LR 0.000200
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.90s
Epoch 9/1000, LR 0.000230
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.89s
Epoch 10/1000, LR 0.000260
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.91s
Epoch 11/1000, LR 0.000290
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 1.00s
Epoch 12/1000, LR 0.000290
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.91s
Epoch 13/1000, LR 0.000290
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 1.00s
Epoch 14/1000, LR 0.000290
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 1.01s
Epoch 15/1000, LR 0.000290
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 1.12s
Epoch 16/1000, LR 0.000290
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.88s
Epoch 17/1000, LR 0.000290
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.92s
Epoch 18/1000, LR 0.000290
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5039 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.91s
Epoch 19/1000, LR 0.000290
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5039 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.88s
Epoch 20/1000, LR 0.000290
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5039 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.87s
Epoch 21/1000, LR 0.000290
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5039 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 0.89s
Epoch 22/1000, LR 0.000290
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5039 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5000 time: 0.97s
Epoch 23/1000, LR 0.000290
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5039 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.5000 time: 0.98s
Epoch 24/1000, LR 0.000290
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6758 score: 0.5039 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6757 score: 0.5000 time: 0.94s
Epoch 25/1000, LR 0.000290
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6707 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.5000 time: 0.89s
Epoch 26/1000, LR 0.000290
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6646 score: 0.5039 time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6643 score: 0.5000 time: 0.90s
Epoch 27/1000, LR 0.000290
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6580 score: 0.5039 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6575 score: 0.5000 time: 0.93s
Epoch 28/1000, LR 0.000290
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6498 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6490 score: 0.5000 time: 0.92s
Epoch 29/1000, LR 0.000290
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6414 score: 0.5039 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6401 score: 0.5000 time: 0.88s
Epoch 30/1000, LR 0.000290
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 1.14s
Val loss: 0.6306 score: 0.5116 time: 1.01s
Test loss: 0.6285 score: 0.5078 time: 0.92s
Epoch 31/1000, LR 0.000290
Train loss: 0.6322;  Loss pred: 0.6322; Loss self: 0.0000; time: 1.17s
Val loss: 0.6156 score: 0.5504 time: 0.98s
Test loss: 0.6129 score: 0.5156 time: 1.00s
Epoch 32/1000, LR 0.000290
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 1.17s
Val loss: 0.6001 score: 0.6434 time: 1.30s
Test loss: 0.5967 score: 0.5703 time: 1.24s
Epoch 33/1000, LR 0.000290
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 1.00s
Val loss: 0.5829 score: 0.6899 time: 0.98s
Test loss: 0.5784 score: 0.6797 time: 1.01s
Epoch 34/1000, LR 0.000290
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 1.00s
Val loss: 0.5618 score: 0.7597 time: 1.01s
Test loss: 0.5563 score: 0.7266 time: 0.98s
Epoch 35/1000, LR 0.000290
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 1.00s
Val loss: 0.5384 score: 0.7907 time: 1.03s
Test loss: 0.5319 score: 0.7969 time: 2.93s
Epoch 36/1000, LR 0.000290
Train loss: 0.5354;  Loss pred: 0.5354; Loss self: 0.0000; time: 3.71s
Val loss: 0.5128 score: 0.8295 time: 1.10s
Test loss: 0.5056 score: 0.8125 time: 0.87s
Epoch 37/1000, LR 0.000290
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.99s
Val loss: 0.4866 score: 0.8682 time: 1.11s
Test loss: 0.4787 score: 0.8750 time: 0.89s
Epoch 38/1000, LR 0.000289
Train loss: 0.4815;  Loss pred: 0.4815; Loss self: 0.0000; time: 0.99s
Val loss: 0.4602 score: 0.8915 time: 1.07s
Test loss: 0.4522 score: 0.9062 time: 0.87s
Epoch 39/1000, LR 0.000289
Train loss: 0.4554;  Loss pred: 0.4554; Loss self: 0.0000; time: 0.97s
Val loss: 0.4353 score: 0.8915 time: 1.05s
Test loss: 0.4282 score: 0.8984 time: 0.87s
Epoch 40/1000, LR 0.000289
Train loss: 0.4318;  Loss pred: 0.4318; Loss self: 0.0000; time: 1.20s
Val loss: 0.4139 score: 0.8760 time: 0.98s
Test loss: 0.4082 score: 0.8828 time: 0.87s
Epoch 41/1000, LR 0.000289
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 1.08s
Val loss: 0.3993 score: 0.8682 time: 0.97s
Test loss: 0.3951 score: 0.8906 time: 1.00s
Epoch 42/1000, LR 0.000289
Train loss: 0.3926;  Loss pred: 0.3926; Loss self: 0.0000; time: 1.00s
Val loss: 0.3993 score: 0.8527 time: 0.97s
Test loss: 0.3957 score: 0.8594 time: 1.01s
Epoch 43/1000, LR 0.000289
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.99s
Val loss: 0.3842 score: 0.8605 time: 0.98s
Test loss: 0.3808 score: 0.8828 time: 1.03s
Epoch 44/1000, LR 0.000289
Train loss: 0.3689;  Loss pred: 0.3689; Loss self: 0.0000; time: 1.00s
Val loss: 0.3594 score: 0.8837 time: 1.01s
Test loss: 0.3565 score: 0.8906 time: 0.98s
Epoch 45/1000, LR 0.000289
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 1.04s
Val loss: 0.3291 score: 0.9147 time: 1.05s
Test loss: 0.3282 score: 0.9375 time: 0.88s
Epoch 46/1000, LR 0.000289
Train loss: 0.3078;  Loss pred: 0.3078; Loss self: 0.0000; time: 1.03s
Val loss: 0.2984 score: 0.9147 time: 1.08s
Test loss: 0.3004 score: 0.9453 time: 0.91s
Epoch 47/1000, LR 0.000289
Train loss: 0.2800;  Loss pred: 0.2800; Loss self: 0.0000; time: 1.07s
Val loss: 0.2727 score: 0.9302 time: 1.08s
Test loss: 0.2769 score: 0.9453 time: 0.93s
Epoch 48/1000, LR 0.000289
Train loss: 0.2565;  Loss pred: 0.2565; Loss self: 0.0000; time: 1.09s
Val loss: 0.2532 score: 0.9380 time: 1.02s
Test loss: 0.2586 score: 0.9375 time: 0.89s
Epoch 49/1000, LR 0.000289
Train loss: 0.2460;  Loss pred: 0.2460; Loss self: 0.0000; time: 1.11s
Val loss: 0.2468 score: 0.8992 time: 1.26s
Test loss: 0.2505 score: 0.9219 time: 0.89s
Epoch 50/1000, LR 0.000289
Train loss: 0.2365;  Loss pred: 0.2365; Loss self: 0.0000; time: 1.13s
Val loss: 0.2418 score: 0.8992 time: 0.97s
Test loss: 0.2445 score: 0.9141 time: 1.00s
Epoch 51/1000, LR 0.000289
Train loss: 0.2271;  Loss pred: 0.2271; Loss self: 0.0000; time: 1.17s
Val loss: 0.2333 score: 0.8992 time: 0.97s
Test loss: 0.2369 score: 0.9141 time: 1.08s
Epoch 52/1000, LR 0.000289
Train loss: 0.2140;  Loss pred: 0.2140; Loss self: 0.0000; time: 1.01s
Val loss: 0.2212 score: 0.8992 time: 1.09s
Test loss: 0.2273 score: 0.9141 time: 1.02s
Epoch 53/1000, LR 0.000289
Train loss: 0.2013;  Loss pred: 0.2013; Loss self: 0.0000; time: 1.01s
Val loss: 0.2067 score: 0.9070 time: 1.14s
Test loss: 0.2155 score: 0.9219 time: 0.88s
Epoch 54/1000, LR 0.000289
Train loss: 0.1852;  Loss pred: 0.1852; Loss self: 0.0000; time: 1.00s
Val loss: 0.1945 score: 0.9147 time: 1.12s
Test loss: 0.2037 score: 0.9219 time: 0.90s
Epoch 55/1000, LR 0.000289
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 1.06s
Val loss: 0.1874 score: 0.9225 time: 1.08s
Test loss: 0.1942 score: 0.9375 time: 0.90s
Epoch 56/1000, LR 0.000289
Train loss: 0.1481;  Loss pred: 0.1481; Loss self: 0.0000; time: 1.01s
Val loss: 0.1987 score: 0.9225 time: 1.08s
Test loss: 0.2003 score: 0.9453 time: 1.01s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.1410;  Loss pred: 0.1410; Loss self: 0.0000; time: 1.02s
Val loss: 0.2279 score: 0.9302 time: 1.08s
Test loss: 0.2228 score: 0.9375 time: 0.89s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 1.00s
Val loss: 0.2694 score: 0.9147 time: 1.07s
Test loss: 0.2547 score: 0.9297 time: 0.89s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.1861;  Loss pred: 0.1861; Loss self: 0.0000; time: 1.10s
Val loss: 0.2768 score: 0.9070 time: 0.98s
Test loss: 0.2602 score: 0.9141 time: 0.90s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.1875;  Loss pred: 0.1875; Loss self: 0.0000; time: 1.10s
Val loss: 0.2584 score: 0.9380 time: 0.96s
Test loss: 0.2450 score: 0.9453 time: 0.99s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1506;  Loss pred: 0.1506; Loss self: 0.0000; time: 1.01s
Val loss: 0.2267 score: 0.9225 time: 0.99s
Test loss: 0.2194 score: 0.9453 time: 0.89s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 1.11s
Val loss: 0.1978 score: 0.9380 time: 1.01s
Test loss: 0.1957 score: 0.9453 time: 1.05s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 1.33s
Val loss: 0.1821 score: 0.9457 time: 1.26s
Test loss: 0.1831 score: 0.9531 time: 1.11s
Epoch 64/1000, LR 0.000288
Train loss: 0.0935;  Loss pred: 0.0935; Loss self: 0.0000; time: 1.08s
Val loss: 0.1753 score: 0.9457 time: 1.08s
Test loss: 0.1800 score: 0.9375 time: 1.22s
Epoch 65/1000, LR 0.000288
Train loss: 0.0932;  Loss pred: 0.0932; Loss self: 0.0000; time: 1.05s
Val loss: 0.1742 score: 0.9457 time: 1.46s
Test loss: 0.1817 score: 0.9375 time: 1.03s
Epoch 66/1000, LR 0.000288
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 1.06s
Val loss: 0.1753 score: 0.9380 time: 1.09s
Test loss: 0.1839 score: 0.9297 time: 0.93s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 1.01s
Val loss: 0.1760 score: 0.9380 time: 1.09s
Test loss: 0.1845 score: 0.9297 time: 0.93s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 1.02s
Val loss: 0.1766 score: 0.9457 time: 1.12s
Test loss: 0.1838 score: 0.9375 time: 0.88s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 1.11s
Val loss: 0.1786 score: 0.9457 time: 1.00s
Test loss: 0.1837 score: 0.9375 time: 0.90s
     INFO: Early stopping counter 4 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 1.16s
Val loss: 0.1814 score: 0.9535 time: 1.03s
Test loss: 0.1849 score: 0.9453 time: 1.01s
     INFO: Early stopping counter 5 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 1.04s
Val loss: 0.1833 score: 0.9457 time: 0.98s
Test loss: 0.1864 score: 0.9453 time: 1.03s
     INFO: Early stopping counter 6 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 1.02s
Val loss: 0.1830 score: 0.9457 time: 1.02s
Test loss: 0.1878 score: 0.9375 time: 1.04s
     INFO: Early stopping counter 7 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 1.04s
Val loss: 0.1845 score: 0.9457 time: 1.10s
Test loss: 0.1898 score: 0.9375 time: 0.91s
     INFO: Early stopping counter 8 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0561;  Loss pred: 0.0561; Loss self: 0.0000; time: 1.17s
Val loss: 0.1867 score: 0.9457 time: 0.97s
Test loss: 0.1915 score: 0.9375 time: 0.89s
     INFO: Early stopping counter 9 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 1.16s
Val loss: 0.1890 score: 0.9457 time: 0.98s
Test loss: 0.1931 score: 0.9375 time: 0.89s
     INFO: Early stopping counter 10 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 1.10s
Val loss: 0.1913 score: 0.9457 time: 1.01s
Test loss: 0.1948 score: 0.9375 time: 0.93s
     INFO: Early stopping counter 11 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 1.11s
Val loss: 0.1943 score: 0.9457 time: 0.94s
Test loss: 0.1968 score: 0.9375 time: 1.04s
     INFO: Early stopping counter 12 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 1.29s
Val loss: 0.1989 score: 0.9457 time: 1.27s
Test loss: 0.1991 score: 0.9375 time: 1.15s
     INFO: Early stopping counter 13 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 1.04s
Val loss: 0.2041 score: 0.9380 time: 1.01s
Test loss: 0.2009 score: 0.9375 time: 1.03s
     INFO: Early stopping counter 14 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 1.02s
Val loss: 0.2175 score: 0.9302 time: 1.09s
Test loss: 0.2070 score: 0.9531 time: 0.88s
     INFO: Early stopping counter 15 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 1.06s
Val loss: 0.2312 score: 0.9302 time: 1.29s
Test loss: 0.2148 score: 0.9531 time: 0.91s
     INFO: Early stopping counter 16 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 1.04s
Val loss: 0.2383 score: 0.9225 time: 1.09s
Test loss: 0.2175 score: 0.9375 time: 0.90s
     INFO: Early stopping counter 17 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 1.20s
Val loss: 0.2428 score: 0.9225 time: 0.99s
Test loss: 0.2167 score: 0.9375 time: 0.92s
     INFO: Early stopping counter 18 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 1.10s
Val loss: 0.2487 score: 0.9225 time: 1.01s
Test loss: 0.2164 score: 0.9453 time: 1.02s
     INFO: Early stopping counter 19 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.99s
Val loss: 0.2551 score: 0.9225 time: 1.00s
Test loss: 0.2169 score: 0.9531 time: 1.00s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 064,   Train_Loss: 0.0932,   Val_Loss: 0.1742,   Val_Precision: 0.9394,   Val_Recall: 0.9538,   Val_accuracy: 0.9466,   Val_Score: 0.9457,   Val_Loss: 0.1742,   Test_Precision: 0.9667,   Test_Recall: 0.9062,   Test_accuracy: 0.9355,   Test_Score: 0.9375,   Test_loss: 0.1817


[0.9637760268524289, 0.9761781608685851, 1.1240670389961451, 1.100541983032599, 1.1013713988941163, 1.0713056640233845, 1.114577128086239, 1.0827825812157243, 1.0964749529957771, 1.0987144690006971, 0.9834696392063051, 1.1064946600235999, 0.9682277520187199, 1.023572813021019, 1.1258170760702342, 1.126716820988804, 0.9726898579392582, 1.0797252899501473, 1.0967197900172323, 1.0038856351748109, 1.0647708270698786, 0.9938502539880574, 1.0878181839361787, 1.0954082319512963, 1.2491978430189192, 1.100653411122039, 0.9940285610500723, 1.006910493830219, 1.0106097091920674, 0.9967217440716922, 1.1020638290792704, 1.0954450289718807, 1.088455606950447, 1.0799323271494359, 1.1031788710970432, 1.0011288397945464, 0.9936848818324506, 1.1075573631096631, 1.020720822038129, 1.1713787191547453, 1.2366481029894203, 1.1395299318246543, 1.0356491161510348, 0.9946530479937792, 1.1081010620109737, 1.0910091609694064, 0.9856091709807515, 1.1495546619407833, 1.1022630319930613, 1.071163880173117, 0.9643036620691419, 0.9868234780151397, 1.0261700439732522, 1.0131932699587196, 1.0582857369445264, 1.1025965961162, 1.0282378040719777, 1.098047369159758, 1.1340656550601125, 1.0451302339788526, 0.987706562038511, 1.0096238940022886, 1.032642001984641, 1.1588029190897942, 1.135090515948832, 1.108585350913927, 0.9936184010002762, 1.0361043692100793, 0.9761980560142547, 0.884632650995627, 0.9631265311036259, 1.0065412500407547, 1.0224952609278262, 0.9144253579434007, 1.0444052759557962, 0.9608418450225145, 0.9388588960282505, 0.9946650960482657, 0.9842329320963472, 1.1330781830474734, 0.9046641399618238, 1.0167175428941846, 0.9924674578942358, 0.9810194249730557, 0.8784757398534566, 0.8998384170699865, 0.8936462341807783, 0.8895393249113113, 0.933447573101148, 0.8714342780876905, 0.9134311689995229, 0.9857536158524454, 0.9817497711628675, 1.256291237194091, 0.9334021890535951, 0.8906535930000246, 0.9065381961409003, 0.8922830699011683, 0.890635872958228, 0.9052440219093114, 0.9295939800795168, 0.8759913449175656, 0.8966779480688274, 0.8825386760290712, 0.999086385127157, 0.9974465570412576, 0.8884468800388277, 0.8749147239141166, 0.9224917020183057, 0.87734295707196, 0.922664649086073, 0.9026024299673736, 1.0383363380096853, 0.9899145450908691, 0.9813540149480104, 0.9831679358612746, 0.9808865680824965, 0.8801054200157523, 0.8989011361263692, 0.925942812114954, 0.9206830249167979, 0.8904868869576603, 0.8731046461034566, 0.9880308229476213, 0.9726652770768851, 1.019775178981945, 1.0312004541046917, 0.9004152088891715, 0.8880199079867452, 0.8846541119273752, 0.928207929013297, 0.9545434101019055, 0.8933042308781296, 0.9069063391070813, 1.1122702788561583, 1.048025059979409, 1.0763640841469169, 0.9215175660792738, 0.9272453999146819, 1.0314787060488015, 1.0033445078879595, 1.0400316519662738, 1.0264913979917765, 1.0715041800867766, 0.9058784570079297, 0.9028149012010545, 0.8925416278652847, 0.9116702261380851, 1.0039115690160543, 0.9108105970080942, 1.0037695821374655, 1.0146637989673764, 1.1228083609603345, 0.883418588899076, 0.9267721869982779, 0.9120749998837709, 0.8860257552005351, 0.8777897809632123, 0.8931389839854091, 0.977261119056493, 0.9859578220639378, 0.9470739639364183, 0.8947295539546758, 0.9085017710458487, 0.9397029811516404, 0.921053766971454, 0.8848265148699284, 0.923929895972833, 1.0045215759892017, 1.2462875600904226, 1.0131504910532385, 0.9839085589628667, 2.931481835898012, 0.8795324671082199, 0.8926829500123858, 0.8797906639520079, 0.879814354935661, 0.8725232800934464, 1.0033586751669645, 1.0177858939860016, 1.0314254141412675, 0.9891537649091333, 0.8883757709991187, 0.9140544799156487, 0.9304671189747751, 0.8916329529602081, 0.8920136028900743, 1.0013327510096133, 1.0856398879550397, 1.0279634138569236, 0.8864687189925462, 0.9027841549832374, 0.90692398394458, 1.018732015043497, 0.8935191568452865, 0.8971071359701455, 0.9069084688089788, 0.9946466030087322, 0.899894017027691, 1.0549918320029974, 1.1127040309365839, 1.225615720031783, 1.0394907069858164, 0.9355494019109756, 0.9349401011131704, 0.8827660349197686, 0.9046715230215341, 1.0143777229823172, 1.0309960800223053, 1.0471407261211425, 0.9194165419321507, 0.89202039106749, 0.8922815411351621, 0.9317300480324775, 1.047268816968426, 1.1558021889068186, 1.031960038933903, 0.8862672268878669, 0.9124108678661287, 0.9079630381893367, 0.9280220030341297, 1.0284681550692767, 1.0011876979842782]
[0.0074711319911041005, 0.007567272564872753, 0.008713697976714303, 0.008531333201803094, 0.008537762782124932, 0.008304695069948716, 0.008640132775862319, 0.008393663420276932, 0.008499805837176567, 0.008517166426361994, 0.007623795652762055, 0.00857747798467907, 0.007505641488517209, 0.007934672969155187, 0.008727264155583211, 0.00873423892239383, 0.007540231456893475, 0.008369963487985637, 0.008501703798583195, 0.007782059187401635, 0.008254037419146345, 0.007704265534791143, 0.008432699100280454, 0.008491536681792995, 0.009683704209448986, 0.008532196985442162, 0.00770564776007808, 0.007805507704110224, 0.00783418379218657, 0.007726525147842576, 0.00854313045797884, 0.00849182193001458, 0.008437640363956953, 0.008371568427515007, 0.008551774194550723, 0.007760688680577879, 0.007702983580096517, 0.008585715993098163, 0.00791256451192348, 0.009080455187246088, 0.009586419403018761, 0.008833565362981816, 0.008028287722101046, 0.007710488744137823, 0.008589930713263362, 0.008457435356351987, 0.007640381170393422, 0.008911276449153358, 0.008544674666612878, 0.00830359597033424, 0.007475222186582495, 0.007649794403218137, 0.00795480654242831, 0.007854211395028834, 0.008203765402670747, 0.008547260435009304, 0.007970835690480448, 0.008511995109765566, 0.008791206628372964, 0.00810178475952599, 0.007656640015802411, 0.00782654181397123, 0.00800497675957086, 0.008982968365037165, 0.008799151286425055, 0.008593684890805635, 0.007702468224808343, 0.00803181681558201, 0.007567426790808176, 0.0068576174495785045, 0.007466097140338185, 0.007802645349153137, 0.00792631985215369, 0.007088568666227913, 0.008096164929889894, 0.007448386395523368, 0.007277975938203492, 0.007710582139909036, 0.007629712651909668, 0.008783551806569562, 0.00701290030978158, 0.007881531340264997, 0.007693546185226634, 0.007604801743977176, 0.006809889456228346, 0.006975491605193693, 0.006927490187447894, 0.006895653681483033, 0.007236027698458511, 0.006755304481299926, 0.0070808617751901, 0.007641500898080971, 0.00761046334234781, 0.009738691761194504, 0.007235675884136396, 0.006904291418604842, 0.007027427877061242, 0.006916923022489677, 0.006904154053939752, 0.0070173955186768324, 0.00720615488433734, 0.006790630580756322, 0.006950991845494786, 0.006841385085496676, 0.007744855698660132, 0.007732143853033005, 0.0068871851165800595, 0.00678228468150478, 0.007151098465258183, 0.006801108194356279, 0.007152439140202116, 0.006996918061762586, 0.008049118899299885, 0.007673756163495109, 0.0076073954647132595, 0.007621456867141664, 0.007603771845600748, 0.006822522635781026, 0.006968225861444723, 0.007177851256705069, 0.0071370777125333166, 0.006902999123702793, 0.0067682530705694315, 0.007659153666260631, 0.007540040907572753, 0.007905233945596472, 0.007993801969803812, 0.00697996285960598, 0.006883875255711203, 0.006857783813390505, 0.007195410302428659, 0.007399561318619422, 0.006924838999055268, 0.007030281698504506, 0.008622250223691149, 0.00812422527115821, 0.008343907629045867, 0.00714354702387034, 0.007244104686833452, 0.008058427391006262, 0.007838628967874683, 0.008125247280986514, 0.008019464046810754, 0.008371126406927942, 0.007077175445374451, 0.007053241415633238, 0.006972981467697537, 0.00712242364170379, 0.007843059132937924, 0.007115707789125736, 0.007841949860448949, 0.007927060929432628, 0.008771940320002614, 0.006901707725774031, 0.007240407710924046, 0.0071255859365919605, 0.00692207621250418, 0.006857732663775096, 0.006977648312386009, 0.007634852492628852, 0.007702795484874514, 0.007399015343253268, 0.006990074640270905, 0.007097670086295693, 0.007341429540247191, 0.0071957325544644846, 0.006912707147421315, 0.007218202312287758, 0.007847824812415638, 0.009736621563206427, 0.007915238211353426, 0.007686785616897396, 0.022902201842953218, 0.006871347399282968, 0.006974085546971764, 0.006873364562125062, 0.006873549647934851, 0.00681658812573005, 0.00783873964974191, 0.007951452296765638, 0.008058011047978653, 0.007727763788352604, 0.006940435710930615, 0.0071410506243410055, 0.00726927436699043, 0.0069658824450016255, 0.006968856272578705, 0.007822912117262604, 0.008481561624648748, 0.008030964170757215, 0.006925536867129267, 0.007053001210806542, 0.007085343624567031, 0.007958843867527321, 0.006980618412853801, 0.0070086494997667614, 0.007085222412570147, 0.00777067658600572, 0.007030422008028836, 0.008242123687523417, 0.008693000241692062, 0.009575122812748305, 0.00812102114832669, 0.007308979702429497, 0.007304219539946644, 0.006896609647810692, 0.007067746273605735, 0.007924825960799353, 0.00805465687517426, 0.008180786922821426, 0.0071829417338449275, 0.0069689093052147655, 0.006970949540118454, 0.0072791410002537305, 0.008181787632565829, 0.00902970460083452, 0.008062187804171117, 0.00692396271006146, 0.00712820990520413, 0.007093461235854193, 0.007250171898704139, 0.008034907461478724, 0.007821778890502173]
[133.84852538955317, 132.14800860246476, 114.76183850671774, 117.21497406626322, 117.12670233631317, 120.41381309936226, 115.73896211336823, 119.13749097733144, 117.64974625963647, 117.40994010693979, 131.1682586399999, 116.58438550191343, 133.23311558777328, 126.0291386787263, 114.58344587407228, 114.49194473442749, 132.6219235731516, 119.47483420153671, 117.6234815622075, 128.5006931865667, 121.15283093827101, 129.79822612345998, 118.58599341778269, 117.76431492595863, 103.26626860661837, 117.20310744187269, 129.77494315025206, 128.1146644020888, 127.64571607285382, 129.42428593262562, 117.0531112592401, 117.76035911274498, 118.5165469094532, 119.45192930792746, 116.93479940539241, 128.85454386317912, 129.81982755147845, 116.47252259495589, 126.38127606960997, 110.12663785891984, 104.31423433081805, 113.20457356784019, 124.55956171664145, 129.6934647314395, 116.41537439364217, 118.23915381735036, 130.8835223921829, 112.21736927429828, 117.0319572151015, 120.42975158866594, 133.77528788307197, 130.72246746648736, 125.71015959550118, 127.32022983656003, 121.89524577024699, 116.9965520067731, 125.45735965857355, 117.48127050175685, 113.75002798507482, 123.42959356260495, 130.60559173947277, 127.77035167880801, 124.9222864768953, 111.32177687412604, 113.64732432123951, 116.36451798109304, 129.82851351196356, 124.50483159177192, 132.1453153950107, 145.82324070314885, 133.93878772312678, 128.16166251981906, 126.16195392724231, 141.07220330167678, 123.5152703359762, 134.25726686266174, 137.40083898200436, 129.69189379672406, 131.0665349565565, 113.84916056988028, 142.594355520098, 126.87889660366147, 129.97907283902816, 131.4958671726027, 146.84526179575454, 143.35907153202461, 144.3524238853383, 145.01888380579638, 138.19737039052927, 148.0318174803528, 141.22574790314317, 130.864343711735, 131.39804437866226, 102.68319652385676, 138.20408984769688, 144.83745534050345, 142.2995749645721, 144.57295487438574, 144.84033701845993, 142.5030123125446, 138.7702618179234, 147.26172895251543, 143.86436097578502, 146.16923144992083, 129.11796409234597, 129.33023738400067, 145.1972007537044, 147.44294097931183, 139.8386562369752, 147.03486129360854, 139.81244445398266, 142.92006726002606, 124.23720068130442, 130.31427878267863, 131.45103401531819, 131.20851005682303, 131.51367772542542, 146.57335026716586, 143.50855151424008, 139.31745925577195, 140.11336856314662, 144.86457003395887, 147.74861246668297, 130.5627284128146, 132.62527514879417, 126.49847011258149, 125.09691931041701, 143.26723796585503, 145.26701354304038, 145.8197031594085, 138.97748119554365, 135.14314659217874, 144.40768949811346, 142.2418109096143, 115.97900479068957, 123.08865973351298, 119.84792311445457, 139.98647963798308, 138.04328391575476, 124.09369117305272, 127.57332999155, 123.07317739609601, 124.69661241235794, 119.45823672814213, 141.29930898542113, 141.77878525234348, 143.41067800517155, 140.4016456062399, 127.5012699828276, 140.53415761791197, 127.5193055037909, 126.1501594225256, 113.99986360140923, 144.89167605077762, 138.1137692689928, 140.33933614704, 144.46532648594356, 145.82079078152816, 143.31476096680052, 130.97829996918216, 129.82299763295492, 135.15311884193372, 143.0599888360054, 140.8913048707093, 136.21325308889763, 138.97125725991094, 144.66112605002158, 138.53864947753965, 127.42384340918922, 102.70502899885572, 126.33858556090254, 130.0933901163785, 43.66392396928814, 145.53186469721368, 143.38797441826802, 145.48915468712264, 145.48523706385808, 146.70095677709747, 127.57152867462882, 125.7631892486815, 124.10010287226517, 129.40354122976873, 144.08317311045505, 140.03541672025082, 137.56531250780296, 143.5568297190474, 143.49556955778158, 127.82963492499522, 117.90281604437598, 124.518050228795, 144.3931379163265, 141.7836138277999, 141.13641525199955, 125.64638993360265, 143.25378367031843, 142.68084030072822, 141.13882977418805, 128.68892289262286, 142.23897212115952, 121.32795356053177, 115.03508250281074, 104.43730274337594, 123.13722396918602, 136.8180020622579, 136.90716640306582, 144.99878216500872, 141.4878182221208, 126.18573643718645, 124.1517814473463, 122.23762939117299, 139.2187264012114, 143.49447757222353, 143.4524800738993, 137.37884730700267, 122.22267857695513, 110.74559403721585, 124.03581066204289, 144.42596557414498, 140.28767576975036, 140.97490163835093, 137.92776419256145, 124.456940517889, 127.84815500400805]
Elapsed: 0.998772626672767~0.15667511447573076
Time per graph: 0.00776518531502898~0.001220618200244787
Speed: 130.5004985932965~12.49045079529609
Total Time: 1.0020
best val loss: 0.17417913187898032 test_score: 0.9375

Testing...
Test loss: 0.1849 score: 0.9453 time: 0.93s
test Score 0.9453
Epoch Time List: [2.909240392036736, 2.934292125981301, 3.0839556800201535, 2.951469091232866, 2.946334370179102, 2.976989096030593, 2.9960957439616323, 2.9796733430121094, 2.9709932790137827, 3.149184667970985, 3.21980641502887, 3.2747408931609243, 2.916105980053544, 3.0043148330878466, 3.0951598291285336, 2.983654929790646, 2.9773641189094633, 2.9888130391482264, 3.003319649025798, 2.9939547800458968, 3.1902023346628994, 3.0694199518766254, 3.253309397958219, 3.010480142896995, 3.1597953368909657, 2.9722759379073977, 3.015066066524014, 3.0332930120639503, 3.0554769460577518, 2.9996400051750243, 3.1508818240836263, 3.26415589498356, 3.0392821941059083, 2.990817334735766, 2.9807805272284895, 3.0240983061958104, 3.027376594254747, 3.1171056423336267, 3.050524576101452, 3.1457512269262224, 3.1659049049485475, 3.506551507860422, 3.005423729075119, 3.0091167059727013, 3.1335303757805377, 3.074242863804102, 2.945005143294111, 3.200382764916867, 2.9675183489453048, 2.9024988189339638, 2.914166749920696, 2.899786193855107, 2.9760694270953536, 3.4385116810444742, 3.069464805070311, 3.067209405824542, 7.338786747772247, 3.139812321867794, 3.039477188838646, 3.0526075221132487, 3.105636761058122, 3.039047881960869, 3.1614181341137737, 3.242652312386781, 3.095849942183122, 3.0359953728038818, 3.0040191339794546, 3.0169911871198565, 2.9674827640410513, 3.024623428005725, 3.044505547732115, 2.9934829589910805, 3.0002699429169297, 3.0018154338467866, 3.1212566981557757, 3.1691188768018037, 3.144173668231815, 3.302564633078873, 3.1435353776905686, 3.2262532140593976, 3.0550734270364046, 3.149890741100535, 3.0290436421055347, 3.0244528220500797, 2.9305955269373953, 2.9884560108184814, 2.9717959268018603, 2.9512693830765784, 3.0420830729417503, 3.0255811160895973, 3.030543126165867, 3.107161482097581, 2.9744363802019507, 3.746645870152861, 3.128938800888136, 3.1074421810917556, 3.0464791080448776, 3.0527613589074463, 3.0371368161868304, 3.031647790921852, 3.14588377205655, 3.0164540850091726, 3.012375903315842, 2.938176886178553, 3.0601072020363063, 2.985830174991861, 2.9544338788837194, 2.9990536288823932, 3.0798296029679477, 3.0132088840473443, 3.0570041011087596, 3.011946131940931, 3.1589493271894753, 3.0546242480631918, 3.30428093415685, 2.988624998833984, 2.9534630058333278, 7.316587320063263, 3.0335291081573814, 3.0160765680484474, 3.0411354689858854, 2.960126883117482, 2.9447408991400152, 3.0542384500149637, 2.927998567931354, 2.978385127382353, 3.0358472508378327, 3.016768520930782, 3.0580262932926416, 2.9829222038388252, 3.047097429865971, 3.0349009199999273, 3.01086701778695, 3.0330316161271185, 3.246414363849908, 3.1158339930698276, 3.1977083187084645, 3.5300576786976308, 3.069817777024582, 3.170150263933465, 3.139978569932282, 3.064844483975321, 3.122434535063803, 3.0665294791106135, 3.041051587089896, 3.0345918838866055, 2.997962702997029, 3.0539955208078027, 3.1292110269423574, 3.021235174033791, 3.124297465197742, 3.0411607830319554, 3.41069264896214, 3.0142911639995873, 3.068811703938991, 3.0139750756789, 3.0820624430198222, 2.9342321939766407, 2.933033180888742, 3.056538665900007, 2.9503573330584913, 3.031753450864926, 2.996367634739727, 3.1347008089069277, 3.1804571342654526, 3.0096395248547196, 3.037586063845083, 3.0658052668441087, 3.150862810900435, 3.7069335628766567, 2.9951984109357, 2.9867796876933426, 4.95173089299351, 5.685149020049721, 2.98482909030281, 2.9327173477504402, 2.898323317989707, 3.0441509161610156, 3.044572825077921, 2.9840449339244515, 2.999917015200481, 2.992405579891056, 2.9800581999588758, 3.0198447378352284, 3.0723843071609735, 2.9882086731959134, 3.253496647113934, 3.101840680697933, 3.216835386119783, 3.11393726663664, 3.0373137178830802, 3.0249923712108284, 3.0369867677800357, 3.104191977996379, 2.981280328705907, 2.958187618292868, 2.9764521680772305, 3.0504893329925835, 2.888022110098973, 3.165416653966531, 3.6972081859130412, 3.3758004480041564, 3.54299808293581, 3.078515537781641, 3.038791330996901, 3.0152020109817386, 3.010293377796188, 3.1957815859932452, 3.0383960290346295, 3.085542873945087, 3.0551114208064973, 3.0324054178781807, 3.0203067229595035, 3.0370164760388434, 3.088660222943872, 3.711708361050114, 3.0691912958864123, 2.9931624841410667, 3.255502640036866, 3.031160392332822, 3.1137357491534203, 3.1362464718986303, 2.9938714678864926]
Total Epoch List: [69, 69, 85]
Total Time List: [0.9767056340351701, 0.9221587581560016, 1.001996787963435]
T-times Epoch Time: 3.142167601679858 ~ 0.018429632667915637
T-times Total Epoch: 73.77777777777777 ~ 2.078698548207745
T-times Total Time: 0.9825147421150985 ~ 0.047141383943887075
T-times Inference Elapsed: 1.016648459030091 ~ 0.014356231653113533
T-times Time Per Graph: 0.007903243053275914 ~ 0.00011046280951402451
T-times Speed: 129.92219736800664 ~ 0.4421587527401443
T-times cross validation test micro f1 score:0.9162532887955043 ~ 0.007620891852401919
T-times cross validation test precision:0.9549074931294644 ~ 0.008059061319529583
T-times cross validation test recall:0.8807692307692307 ~ 0.008571346225197393
T-times cross validation test f1_score:0.9162532887955043 ~ 0.008162612464177463
