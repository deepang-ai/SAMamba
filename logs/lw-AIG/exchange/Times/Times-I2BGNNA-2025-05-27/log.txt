Namespace(seed=15, model='I2BGNNA', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae234df90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.20s
Epoch 14/1000, LR 0.000285
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4961 time: 0.21s
Epoch 15/1000, LR 0.000285
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4961 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.4961 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.4961 time: 0.27s
Epoch 20/1000, LR 0.000285
Train loss: 0.4827;  Loss pred: 0.4827; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4961 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4513;  Loss pred: 0.4513; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6826 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4234;  Loss pred: 0.4234; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6780 score: 0.4961 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.3862;  Loss pred: 0.3862; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6747 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.4961 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6693 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.4961 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3261;  Loss pred: 0.3261; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6634 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6599 score: 0.4961 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2814;  Loss pred: 0.2814; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6574 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6525 score: 0.4961 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2533;  Loss pred: 0.2533; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6494 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6426 score: 0.4961 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2148;  Loss pred: 0.2148; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6400 score: 0.5039 time: 0.17s
Test loss: 0.6309 score: 0.5116 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1900;  Loss pred: 0.1900; Loss self: 0.0000; time: 0.25s
Val loss: 0.6285 score: 0.5271 time: 0.17s
Test loss: 0.6166 score: 0.5194 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.25s
Val loss: 0.6153 score: 0.5504 time: 0.17s
Test loss: 0.5998 score: 0.5426 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1398;  Loss pred: 0.1398; Loss self: 0.0000; time: 0.25s
Val loss: 0.5989 score: 0.5891 time: 0.17s
Test loss: 0.5794 score: 0.6047 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.26s
Val loss: 0.5819 score: 0.6202 time: 0.17s
Test loss: 0.5577 score: 0.6279 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.26s
Val loss: 0.5598 score: 0.6667 time: 0.15s
Test loss: 0.5305 score: 0.6667 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.25s
Val loss: 0.5333 score: 0.6977 time: 0.16s
Test loss: 0.4986 score: 0.7132 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.25s
Val loss: 0.5097 score: 0.7209 time: 0.18s
Test loss: 0.4688 score: 0.8062 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.26s
Val loss: 0.4840 score: 0.7442 time: 0.17s
Test loss: 0.4369 score: 0.8217 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.26s
Val loss: 0.4587 score: 0.7752 time: 0.17s
Test loss: 0.4051 score: 0.8372 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.27s
Val loss: 0.4340 score: 0.7984 time: 0.20s
Test loss: 0.3740 score: 0.8527 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.28s
Val loss: 0.4125 score: 0.7984 time: 0.19s
Test loss: 0.3460 score: 0.8605 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.26s
Val loss: 0.3964 score: 0.8140 time: 0.18s
Test loss: 0.3232 score: 0.8837 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0367;  Loss pred: 0.0367; Loss self: 0.0000; time: 0.26s
Val loss: 0.3890 score: 0.8295 time: 0.19s
Test loss: 0.3080 score: 0.8837 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.27s
Val loss: 0.3770 score: 0.8140 time: 0.18s
Test loss: 0.2898 score: 0.8992 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.27s
Val loss: 0.3657 score: 0.8217 time: 0.18s
Test loss: 0.2727 score: 0.9147 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.26s
Val loss: 0.3575 score: 0.8450 time: 0.18s
Test loss: 0.2590 score: 0.9147 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.25s
Val loss: 0.3470 score: 0.8682 time: 0.17s
Test loss: 0.2451 score: 0.9147 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.26s
Val loss: 0.3406 score: 0.8682 time: 0.17s
Test loss: 0.2348 score: 0.9147 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.25s
Val loss: 0.3331 score: 0.8682 time: 0.17s
Test loss: 0.2252 score: 0.9070 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.25s
Val loss: 0.3353 score: 0.8682 time: 0.17s
Test loss: 0.2222 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.25s
Val loss: 0.3431 score: 0.8605 time: 0.17s
Test loss: 0.2230 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.26s
Val loss: 0.3528 score: 0.8605 time: 0.17s
Test loss: 0.2255 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.26s
Val loss: 0.3696 score: 0.8605 time: 0.17s
Test loss: 0.2328 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.26s
Val loss: 0.3885 score: 0.8527 time: 0.18s
Test loss: 0.2413 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.26s
Val loss: 0.4006 score: 0.8527 time: 0.17s
Test loss: 0.2455 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.25s
Val loss: 0.4080 score: 0.8527 time: 0.16s
Test loss: 0.2474 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.25s
Val loss: 0.3987 score: 0.8605 time: 0.18s
Test loss: 0.2393 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.25s
Val loss: 0.3959 score: 0.8682 time: 0.18s
Test loss: 0.2347 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.26s
Val loss: 0.3981 score: 0.8682 time: 0.18s
Test loss: 0.2337 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.26s
Val loss: 0.4093 score: 0.8682 time: 0.17s
Test loss: 0.2389 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.26s
Val loss: 0.4214 score: 0.8682 time: 0.17s
Test loss: 0.2450 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.26s
Val loss: 0.4305 score: 0.8682 time: 0.18s
Test loss: 0.2487 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.25s
Val loss: 0.4381 score: 0.8682 time: 0.18s
Test loss: 0.2516 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.26s
Val loss: 0.4451 score: 0.8682 time: 0.17s
Test loss: 0.2543 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.25s
Val loss: 0.4563 score: 0.8682 time: 0.17s
Test loss: 0.2597 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.26s
Val loss: 0.4594 score: 0.8682 time: 0.17s
Test loss: 0.2607 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.26s
Val loss: 0.4565 score: 0.8682 time: 0.18s
Test loss: 0.2577 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.26s
Val loss: 0.4515 score: 0.8682 time: 0.17s
Test loss: 0.2529 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.26s
Val loss: 0.4500 score: 0.8682 time: 0.17s
Test loss: 0.2505 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0198,   Val_Loss: 0.3331,   Val_Precision: 0.9608,   Val_Recall: 0.7656,   Val_accuracy: 0.8522,   Val_Score: 0.8682,   Val_Loss: 0.3331,   Test_Precision: 0.9818,   Test_Recall: 0.8308,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.2252


[0.16367715480737388, 0.17422513384371996, 0.17644050810486078, 0.176322469022125, 0.1723570900503546, 0.1770383450202644, 0.17389672598801553, 0.1722163069061935, 0.18582536396570504, 0.1778099020011723, 0.18829623493365943, 0.18170448415912688, 0.20380340702831745, 0.2185075469315052, 0.16887984098866582, 0.17605099291540682, 0.17476033605635166, 0.18417769810184836, 0.2821021690033376, 0.17529537994414568, 0.17929648095741868, 0.1646286027971655, 0.1755997168365866, 0.18836336187087, 0.17575143720023334, 0.1838474590331316, 0.17240848415531218, 0.1737592499703169, 0.16717457701452076, 0.17363573890179396, 0.16737650404684246, 0.1682164480444044, 0.18484922498464584, 0.1808662200346589, 0.1704419320449233, 0.16531789489090443, 0.18427271000109613, 0.18947842880152166, 0.18656683014705777, 0.18923211889341474, 0.1763035252224654, 0.1690766920801252, 0.179607406957075, 0.17024595709517598, 0.17599866003729403, 0.17756173992529511, 0.17310004215687513, 0.1627454669214785, 0.16908056801185012, 0.17314590699970722, 0.17616091901436448, 0.17417139885947108, 0.17316618585027754, 0.1730995220132172, 0.18650483805686235, 0.17716010101139545, 0.17677689692936838, 0.17647565808147192, 0.17117892787791789, 0.18255332787521183, 0.1756176589988172, 0.17637306288816035, 0.1772969427984208, 0.17704555089585483, 0.18793924478814006, 0.17364569590426981, 0.17279988201335073]
[0.0012688151535455339, 0.001350582432897054, 0.0013677558767818665, 0.001366840845132752, 0.001336101473258563, 0.0013723902714749177, 0.0013480366355660119, 0.0013350101310557634, 0.0014405066974085663, 0.001378371333342421, 0.0014596607359198405, 0.00140856189270641, 0.0015798713723125385, 0.0016938569529574047, 0.001309146054175704, 0.0013647363791892, 0.00135473128725854, 0.001427734093812778, 0.002186838519405718, 0.001358878914295703, 0.0013898952012203, 0.0012761907193578723, 0.0013612381150122992, 0.001460181099774186, 0.0013624142418622739, 0.001425174101032028, 0.0013364998771729626, 0.0013469709300024564, 0.0012959269536009363, 0.001346013479858868, 0.0012974922794328873, 0.001304003473212437, 0.001432939728563146, 0.0014020637211989062, 0.001321255287169948, 0.0012815340689217398, 0.0014284706201635358, 0.0014688250294691602, 0.0014462544972640137, 0.0014669156503365483, 0.0013666939939726, 0.0013106720316288776, 0.001392305480287403, 0.0013197361015129922, 0.0013643306979635196, 0.0013764475963201172, 0.0013418607919137608, 0.0012615927668331668, 0.0013107020776112412, 0.0013422163333310637, 0.0013655885194911975, 0.0013501658826315587, 0.0013423735337230818, 0.0013418567597923816, 0.0014457739384252895, 0.0013733341163674065, 0.0013703635420881269, 0.0013680283572207125, 0.0013269684331621541, 0.0014151420765520298, 0.0013613772015412186, 0.0013672330456446539, 0.0013743949054141147, 0.0013724461309756188, 0.0014568933704506982, 0.0013460906659245723, 0.001339533969095742]
[788.1368670650204, 740.4212994648238, 731.1246231694918, 731.6140745727253, 748.4461472534277, 728.6557044194818, 741.8196016461535, 749.0579859563851, 694.2001740075029, 725.4939041536028, 685.0907031967437, 709.9439543111596, 632.9629218714489, 590.3686248440525, 763.856711640661, 732.7422462308122, 738.1537648131084, 700.4105346601972, 457.2811349014256, 735.9007410298162, 719.4787053887374, 783.5819402472695, 734.6253304044199, 684.8465578376873, 733.9911528178885, 701.6686587806067, 748.2230392083944, 742.4065194919806, 771.6484306629654, 742.934610212709, 770.717495472947, 766.8691230833007, 697.866058192644, 713.2343451158546, 756.8560063376865, 780.314799466379, 700.0493996057943, 680.8162850829173, 691.441237964531, 681.702454923413, 731.6926864464207, 762.9673754136796, 718.233185287454, 757.7272447526174, 732.9601257910996, 726.5078617402245, 745.2337873094875, 792.6488057712843, 762.9498854709251, 745.0363813694893, 732.2850080583487, 740.649732646878, 744.9491329186843, 745.2360266491669, 691.6710651799282, 728.1549246334102, 729.7333658455508, 730.9789996105058, 753.5974293050881, 706.6428287091028, 734.5502766374356, 731.4042058780824, 727.5929182076626, 728.6260476315662, 686.3920313472526, 742.8920096649973, 746.5282875021484]
Elapsed: 0.17879555656213258~0.015442317995027929
Time per graph: 0.0013860120663731208~0.00011970789143432505
Speed: 725.4745596908158~47.012463917030104
Total Time: 0.1733
best val loss: 0.3331008238010397 test_score: 0.9070

Testing...
Test loss: 0.2451 score: 0.9147 time: 0.16s
test Score 0.9147
Epoch Time List: [0.7355788538698107, 0.5761964002158493, 0.5847231498919427, 0.5850930558517575, 0.5984998540952802, 0.5883450489491224, 0.5884611047804356, 0.5928767358418554, 0.6100106269586831, 0.6126978620886803, 0.6365901741664857, 0.6856405176222324, 0.6656457500066608, 0.7957151159644127, 0.5950423679314554, 0.6082313389051706, 0.6691571641713381, 0.636045107152313, 0.7082949560135603, 0.5977205652743578, 0.5999750189948827, 0.5992610356770456, 0.6076365530025214, 0.6331185370218009, 0.6201958889141679, 0.6261784452944994, 0.607981787994504, 0.5919536810833961, 0.5852300385013223, 0.5879270450677723, 0.587035161210224, 0.5980533070396632, 0.5935527689289302, 0.5918538991827518, 0.5939101872500032, 0.5914561559911817, 0.6146295219659805, 0.6466403130907565, 0.6462541988585144, 0.6336487773805857, 0.6233505480922759, 0.6071122467983514, 0.6205560930538923, 0.6053324639797211, 0.59228326799348, 0.5976065883878618, 0.5908271500375122, 0.5812313158530742, 0.5887017780914903, 0.6014713668264449, 0.5964144382160157, 0.604856233112514, 0.596102928975597, 0.5861369906924665, 0.6082421990577132, 0.6007783869281411, 0.6071245840284973, 0.600632979767397, 0.593400300713256, 0.6118422949220985, 0.6000126758590341, 0.6027024898212403, 0.5997032602317631, 0.6020743080880493, 0.6246934360824525, 0.5983229987323284, 0.600915860151872]
Total Epoch List: [67]
Total Time List: [0.17328483704477549]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae234e080>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4961 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4961 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4619;  Loss pred: 0.4619; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.4961 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4384;  Loss pred: 0.4384; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6759 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6812 score: 0.4961 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6782 score: 0.4961 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6676 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6741 score: 0.4961 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3268;  Loss pred: 0.3268; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6627 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.4961 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.2984;  Loss pred: 0.2984; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6576 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6651 score: 0.4961 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6509 score: 0.5039 time: 0.18s
Test loss: 0.6584 score: 0.5116 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2552;  Loss pred: 0.2552; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6441 score: 0.5039 time: 0.18s
Test loss: 0.6517 score: 0.5116 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2256;  Loss pred: 0.2256; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6340 score: 0.5039 time: 0.17s
Test loss: 0.6415 score: 0.5116 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.2031;  Loss pred: 0.2031; Loss self: 0.0000; time: 0.24s
Val loss: 0.6209 score: 0.5194 time: 0.17s
Test loss: 0.6286 score: 0.5116 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1865;  Loss pred: 0.1865; Loss self: 0.0000; time: 0.24s
Val loss: 0.6054 score: 0.5581 time: 0.17s
Test loss: 0.6132 score: 0.5271 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.24s
Val loss: 0.5861 score: 0.5736 time: 0.18s
Test loss: 0.5938 score: 0.5349 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1535;  Loss pred: 0.1535; Loss self: 0.0000; time: 0.23s
Val loss: 0.5600 score: 0.6047 time: 0.18s
Test loss: 0.5675 score: 0.5349 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1385;  Loss pred: 0.1385; Loss self: 0.0000; time: 0.24s
Val loss: 0.5330 score: 0.6357 time: 0.18s
Test loss: 0.5400 score: 0.5969 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1127;  Loss pred: 0.1127; Loss self: 0.0000; time: 0.24s
Val loss: 0.4986 score: 0.7054 time: 0.17s
Test loss: 0.5043 score: 0.6899 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.1287;  Loss pred: 0.1287; Loss self: 0.0000; time: 0.32s
Val loss: 0.4724 score: 0.7209 time: 0.18s
Test loss: 0.4773 score: 0.7287 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.23s
Val loss: 0.4382 score: 0.7364 time: 0.18s
Test loss: 0.4419 score: 0.8062 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0835;  Loss pred: 0.0835; Loss self: 0.0000; time: 0.24s
Val loss: 0.4044 score: 0.7907 time: 0.17s
Test loss: 0.4061 score: 0.8295 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.28s
Val loss: 0.3721 score: 0.8450 time: 0.17s
Test loss: 0.3716 score: 0.8527 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 0.23s
Val loss: 0.3348 score: 0.8682 time: 0.17s
Test loss: 0.3305 score: 0.8837 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.24s
Val loss: 0.3072 score: 0.8837 time: 0.18s
Test loss: 0.3000 score: 0.9070 time: 0.25s
Epoch 42/1000, LR 0.000284
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.24s
Val loss: 0.2796 score: 0.8837 time: 0.18s
Test loss: 0.2694 score: 0.9225 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.24s
Val loss: 0.2654 score: 0.8837 time: 0.18s
Test loss: 0.2535 score: 0.9225 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.24s
Val loss: 0.2499 score: 0.8837 time: 0.27s
Test loss: 0.2366 score: 0.9225 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.24s
Val loss: 0.2351 score: 0.9225 time: 0.18s
Test loss: 0.2214 score: 0.9147 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.25s
Val loss: 0.2235 score: 0.9225 time: 0.18s
Test loss: 0.2102 score: 0.9147 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.24s
Val loss: 0.2127 score: 0.9380 time: 0.18s
Test loss: 0.2012 score: 0.9225 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.24s
Val loss: 0.2051 score: 0.9380 time: 0.18s
Test loss: 0.1951 score: 0.9302 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.24s
Val loss: 0.2010 score: 0.9380 time: 0.18s
Test loss: 0.1919 score: 0.9302 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.24s
Val loss: 0.2006 score: 0.9380 time: 0.18s
Test loss: 0.1917 score: 0.9302 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.24s
Val loss: 0.1998 score: 0.9380 time: 0.18s
Test loss: 0.1918 score: 0.9302 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.24s
Val loss: 0.2002 score: 0.9380 time: 0.18s
Test loss: 0.1943 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.24s
Val loss: 0.2016 score: 0.9380 time: 0.18s
Test loss: 0.2001 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.24s
Val loss: 0.2058 score: 0.9380 time: 0.32s
Test loss: 0.2110 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.23s
Val loss: 0.2114 score: 0.9457 time: 0.18s
Test loss: 0.2227 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.24s
Val loss: 0.2187 score: 0.9457 time: 0.18s
Test loss: 0.2373 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.24s
Val loss: 0.2239 score: 0.9457 time: 0.17s
Test loss: 0.2465 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.24s
Val loss: 0.2242 score: 0.9380 time: 0.17s
Test loss: 0.2439 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.24s
Val loss: 0.2248 score: 0.9302 time: 0.18s
Test loss: 0.2406 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.24s
Val loss: 0.2261 score: 0.9302 time: 0.18s
Test loss: 0.2395 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.24s
Val loss: 0.2285 score: 0.9302 time: 0.17s
Test loss: 0.2398 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.24s
Val loss: 0.2329 score: 0.9302 time: 0.18s
Test loss: 0.2484 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.24s
Val loss: 0.2384 score: 0.9302 time: 0.18s
Test loss: 0.2624 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.24s
Val loss: 0.2441 score: 0.9302 time: 0.18s
Test loss: 0.2761 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.24s
Val loss: 0.2493 score: 0.9380 time: 0.17s
Test loss: 0.2891 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.24s
Val loss: 0.2521 score: 0.9380 time: 0.17s
Test loss: 0.2960 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.24s
Val loss: 0.2558 score: 0.9380 time: 0.18s
Test loss: 0.3046 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.24s
Val loss: 0.2583 score: 0.9380 time: 0.18s
Test loss: 0.3093 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.24s
Val loss: 0.2597 score: 0.9302 time: 0.17s
Test loss: 0.3111 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.24s
Val loss: 0.2609 score: 0.9380 time: 0.18s
Test loss: 0.3125 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.24s
Val loss: 0.2614 score: 0.9302 time: 0.18s
Test loss: 0.3127 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0244,   Val_Loss: 0.1998,   Val_Precision: 0.9385,   Val_Recall: 0.9385,   Val_accuracy: 0.9385,   Val_Score: 0.9380,   Val_Loss: 0.1998,   Test_Precision: 0.9365,   Test_Recall: 0.9219,   Test_accuracy: 0.9291,   Test_Score: 0.9302,   Test_loss: 0.1918


[0.16367715480737388, 0.17422513384371996, 0.17644050810486078, 0.176322469022125, 0.1723570900503546, 0.1770383450202644, 0.17389672598801553, 0.1722163069061935, 0.18582536396570504, 0.1778099020011723, 0.18829623493365943, 0.18170448415912688, 0.20380340702831745, 0.2185075469315052, 0.16887984098866582, 0.17605099291540682, 0.17476033605635166, 0.18417769810184836, 0.2821021690033376, 0.17529537994414568, 0.17929648095741868, 0.1646286027971655, 0.1755997168365866, 0.18836336187087, 0.17575143720023334, 0.1838474590331316, 0.17240848415531218, 0.1737592499703169, 0.16717457701452076, 0.17363573890179396, 0.16737650404684246, 0.1682164480444044, 0.18484922498464584, 0.1808662200346589, 0.1704419320449233, 0.16531789489090443, 0.18427271000109613, 0.18947842880152166, 0.18656683014705777, 0.18923211889341474, 0.1763035252224654, 0.1690766920801252, 0.179607406957075, 0.17024595709517598, 0.17599866003729403, 0.17756173992529511, 0.17310004215687513, 0.1627454669214785, 0.16908056801185012, 0.17314590699970722, 0.17616091901436448, 0.17417139885947108, 0.17316618585027754, 0.1730995220132172, 0.18650483805686235, 0.17716010101139545, 0.17677689692936838, 0.17647565808147192, 0.17117892787791789, 0.18255332787521183, 0.1756176589988172, 0.17637306288816035, 0.1772969427984208, 0.17704555089585483, 0.18793924478814006, 0.17364569590426981, 0.17279988201335073, 0.1838614868465811, 0.17619177396409214, 0.17226611799560487, 0.1731519610621035, 0.17912466102279723, 0.17626335797831416, 0.17296778899617493, 0.17360842297784984, 0.1784362441394478, 0.177690870128572, 0.17513952706940472, 0.17225796496495605, 0.17114563193172216, 0.17412557313218713, 0.17942539905197918, 0.1752725199330598, 0.17369563109241426, 0.17266709893010557, 0.17469727899879217, 0.17968683899380267, 0.1780314848292619, 0.17330173798836768, 0.17073995200917125, 0.1719913629349321, 0.17956707091070712, 0.17617801018059254, 0.1775020370259881, 0.17325422703288496, 0.17344216303899884, 0.17505908594466746, 0.17714482313022017, 0.1751193660311401, 0.1732337619177997, 0.17398847988806665, 0.17893862398341298, 0.17355130705982447, 0.1737898129504174, 0.17119188490323722, 0.17291788407601416, 0.17776678618974984, 0.2568941281642765, 0.1746897241100669, 0.18104757508262992, 0.17551816999912262, 0.1749403530266136, 0.17623046995140612, 0.17647275212220848, 0.17397335497662425, 0.17410086910240352, 0.17789575108326972, 0.17846283107064664, 0.17470021802000701, 0.17411198699846864, 0.17820460489019752, 0.17363814497366548, 0.17173203290440142, 0.17787064099684358, 0.17468596994876862, 0.1728360909037292, 0.1732337900903076, 0.1770753909368068, 0.17713674600236118, 0.17526662000454962, 0.1742504690773785, 0.17725263512693346, 0.17714155418798327, 0.17594421585090458, 0.17458833009004593, 0.17686080490238965, 0.17853870685212314, 0.17365251411683857]
[0.0012688151535455339, 0.001350582432897054, 0.0013677558767818665, 0.001366840845132752, 0.001336101473258563, 0.0013723902714749177, 0.0013480366355660119, 0.0013350101310557634, 0.0014405066974085663, 0.001378371333342421, 0.0014596607359198405, 0.00140856189270641, 0.0015798713723125385, 0.0016938569529574047, 0.001309146054175704, 0.0013647363791892, 0.00135473128725854, 0.001427734093812778, 0.002186838519405718, 0.001358878914295703, 0.0013898952012203, 0.0012761907193578723, 0.0013612381150122992, 0.001460181099774186, 0.0013624142418622739, 0.001425174101032028, 0.0013364998771729626, 0.0013469709300024564, 0.0012959269536009363, 0.001346013479858868, 0.0012974922794328873, 0.001304003473212437, 0.001432939728563146, 0.0014020637211989062, 0.001321255287169948, 0.0012815340689217398, 0.0014284706201635358, 0.0014688250294691602, 0.0014462544972640137, 0.0014669156503365483, 0.0013666939939726, 0.0013106720316288776, 0.001392305480287403, 0.0013197361015129922, 0.0013643306979635196, 0.0013764475963201172, 0.0013418607919137608, 0.0012615927668331668, 0.0013107020776112412, 0.0013422163333310637, 0.0013655885194911975, 0.0013501658826315587, 0.0013423735337230818, 0.0013418567597923816, 0.0014457739384252895, 0.0013733341163674065, 0.0013703635420881269, 0.0013680283572207125, 0.0013269684331621541, 0.0014151420765520298, 0.0013613772015412186, 0.0013672330456446539, 0.0013743949054141147, 0.0013724461309756188, 0.0014568933704506982, 0.0013460906659245723, 0.001339533969095742, 0.0014252828437719466, 0.001365827705148001, 0.0013353962635318207, 0.001342263264047314, 0.0013885632637426141, 0.0013663826199869316, 0.0013408355736137592, 0.001345801728510464, 0.0013832266987554094, 0.001377448605647845, 0.0013576707524760056, 0.0013353330617438452, 0.0013267103250521098, 0.001349810644435559, 0.0013908945662944123, 0.0013587017049074405, 0.0013464777604063121, 0.0013385046428690355, 0.0013542424728588541, 0.0013929212325100983, 0.0013800890296842008, 0.0013434243254912223, 0.0013235655194509398, 0.001333266379340559, 0.0013919927977574195, 0.00136572100915188, 0.0013759847831471947, 0.0013430560235107361, 0.001344512891775185, 0.0013570471778656392, 0.0013732156831800014, 0.0013575144653576751, 0.0013428973792077496, 0.0013487479061090439, 0.0013871211161504882, 0.0013453589694560036, 0.0013472078523288171, 0.0013270688752188933, 0.0013404487137675517, 0.0013780371022461227, 0.0019914273501106705, 0.0013541839078299758, 0.001403469574283953, 0.0013606059689854468, 0.0013561267676481673, 0.001366127674041908, 0.0013680058304047168, 0.0013486306587335213, 0.0013496191403287094, 0.001379036830102866, 0.001383432798997261, 0.0013542652559690466, 0.0013497053255695243, 0.001381431045660446, 0.0013460321315788022, 0.0013312560690263676, 0.0013788421782701052, 0.0013541548058044078, 0.0013398146581684435, 0.0013428975975992837, 0.0013726774491225333, 0.0013731530697857455, 0.0013586559690275165, 0.0013507788300571979, 0.0013740514350925074, 0.001373190342542506, 0.0013639086500070122, 0.0013533979076747746, 0.0013710139914913926, 0.0013840209833497917, 0.0013461435202855704]
[788.1368670650204, 740.4212994648238, 731.1246231694918, 731.6140745727253, 748.4461472534277, 728.6557044194818, 741.8196016461535, 749.0579859563851, 694.2001740075029, 725.4939041536028, 685.0907031967437, 709.9439543111596, 632.9629218714489, 590.3686248440525, 763.856711640661, 732.7422462308122, 738.1537648131084, 700.4105346601972, 457.2811349014256, 735.9007410298162, 719.4787053887374, 783.5819402472695, 734.6253304044199, 684.8465578376873, 733.9911528178885, 701.6686587806067, 748.2230392083944, 742.4065194919806, 771.6484306629654, 742.934610212709, 770.717495472947, 766.8691230833007, 697.866058192644, 713.2343451158546, 756.8560063376865, 780.314799466379, 700.0493996057943, 680.8162850829173, 691.441237964531, 681.702454923413, 731.6926864464207, 762.9673754136796, 718.233185287454, 757.7272447526174, 732.9601257910996, 726.5078617402245, 745.2337873094875, 792.6488057712843, 762.9498854709251, 745.0363813694893, 732.2850080583487, 740.649732646878, 744.9491329186843, 745.2360266491669, 691.6710651799282, 728.1549246334102, 729.7333658455508, 730.9789996105058, 753.5974293050881, 706.6428287091028, 734.5502766374356, 731.4042058780824, 727.5929182076626, 728.6260476315662, 686.3920313472526, 742.8920096649973, 746.5282875021484, 701.6151245836547, 732.1567692841902, 748.8413943552803, 745.0103320154269, 720.1688436612429, 731.8594260292657, 745.8036016339016, 743.0515051476432, 722.9472948286593, 725.9798992860991, 736.5556031727752, 748.876837284381, 753.7440397629547, 740.8446541167727, 718.9617561481858, 735.9967212730653, 742.6784380740464, 747.1023767661626, 738.4202017301706, 717.9156844339009, 724.5909347086291, 744.3664529703603, 755.5349435325529, 750.0376635122275, 718.3945215888024, 732.2139685183618, 726.7522230244214, 744.5705782146073, 743.7637869575813, 736.8940566773794, 728.2177244613655, 736.6404009083778, 744.658538681456, 741.4283984950646, 720.917581281713, 743.2960441809448, 742.2759585845458, 753.5403916658471, 746.0188440849299, 725.669866486219, 502.1523883080277, 738.4521365362102, 712.5198994856713, 734.9666419188671, 737.3941904665946, 731.9960052059699, 730.9910365690149, 741.4928568649661, 740.9497762134899, 725.143794691406, 722.8395920096875, 738.4077791203824, 740.9024629713438, 723.8870178438127, 742.9243155043182, 751.1702844152018, 725.2461635998106, 738.468006548166, 746.3718909949995, 744.6584175798018, 728.5032624665301, 728.25092992439, 736.0214968295239, 740.3136455415545, 727.7747939127734, 728.2311628761297, 733.1869330067367, 738.8810004280742, 729.3871588518199, 722.5323980129745, 742.8628410942842]
Elapsed: 0.17765678077146574~0.012951929197180594
Time per graph: 0.0013771843470656256~0.00010040255191612864
Speed: 728.9274576898576~39.12908950625349
Total Time: 0.1743
best val loss: 0.19979292747759542 test_score: 0.9302

Testing...
Test loss: 0.2227 score: 0.9225 time: 0.17s
test Score 0.9225
Epoch Time List: [0.7355788538698107, 0.5761964002158493, 0.5847231498919427, 0.5850930558517575, 0.5984998540952802, 0.5883450489491224, 0.5884611047804356, 0.5928767358418554, 0.6100106269586831, 0.6126978620886803, 0.6365901741664857, 0.6856405176222324, 0.6656457500066608, 0.7957151159644127, 0.5950423679314554, 0.6082313389051706, 0.6691571641713381, 0.636045107152313, 0.7082949560135603, 0.5977205652743578, 0.5999750189948827, 0.5992610356770456, 0.6076365530025214, 0.6331185370218009, 0.6201958889141679, 0.6261784452944994, 0.607981787994504, 0.5919536810833961, 0.5852300385013223, 0.5879270450677723, 0.587035161210224, 0.5980533070396632, 0.5935527689289302, 0.5918538991827518, 0.5939101872500032, 0.5914561559911817, 0.6146295219659805, 0.6466403130907565, 0.6462541988585144, 0.6336487773805857, 0.6233505480922759, 0.6071122467983514, 0.6205560930538923, 0.6053324639797211, 0.59228326799348, 0.5976065883878618, 0.5908271500375122, 0.5812313158530742, 0.5887017780914903, 0.6014713668264449, 0.5964144382160157, 0.604856233112514, 0.596102928975597, 0.5861369906924665, 0.6082421990577132, 0.6007783869281411, 0.6071245840284973, 0.600632979767397, 0.593400300713256, 0.6118422949220985, 0.6000126758590341, 0.6027024898212403, 0.5997032602317631, 0.6020743080880493, 0.6246934360824525, 0.5983229987323284, 0.600915860151872, 0.5832505198195577, 0.5958667339291424, 0.60274987667799, 0.5869217454455793, 0.6077811850700527, 0.5845521311275661, 0.5850287510547787, 0.5812229430302978, 0.5864684840198606, 0.5860187890939415, 0.5888702541124076, 0.5831452468410134, 0.5807782129850239, 0.5850423432420939, 0.5864634199533612, 0.587118525756523, 0.5815147841349244, 0.5812619389034808, 0.5878323209472001, 0.5903903648722917, 0.586510359775275, 0.584799821022898, 0.5776085839606822, 0.5819746619090438, 0.5885324887931347, 0.5828083981759846, 0.5880082692019641, 0.5835265289060771, 0.5814440702088177, 0.5821210930589586, 0.5830806519370526, 0.5860690618865192, 0.5786605237517506, 0.5882829187903553, 0.587056343909353, 0.660180636914447, 0.5838895828928798, 0.5791701020207256, 0.6156350499950349, 0.5784120340831578, 0.6650710948742926, 0.5921579122077674, 0.5967311998829246, 0.6822053890209645, 0.5872353608720005, 0.5954865838866681, 0.5864470440428704, 0.5833011029753834, 0.5918463992420584, 0.5938139311037958, 0.5885407677851617, 0.5882212398573756, 0.5887607489712536, 0.7321967561729252, 0.578000953886658, 0.5821100999601185, 0.5829480392858386, 0.5812181681394577, 0.5846625019330531, 0.5865747970528901, 0.5880061031784862, 0.5902438862249255, 0.5880087669938803, 0.5853040618821979, 0.5841779937036335, 0.5830957000143826, 0.5852565381210297, 0.5849502691999078, 0.5905407669488341, 0.5867508361116052, 0.583723190240562]
Total Epoch List: [67, 71]
Total Time List: [0.17328483704477549, 0.17430002498440444]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae2310a90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.14s
Epoch 2/1000, LR 0.000020
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.16s
Epoch 7/1000, LR 0.000170
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.16s
Epoch 8/1000, LR 0.000200
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.16s
Epoch 9/1000, LR 0.000230
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.16s
Epoch 10/1000, LR 0.000260
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.16s
Epoch 11/1000, LR 0.000290
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.16s
Epoch 12/1000, LR 0.000290
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.15s
Epoch 13/1000, LR 0.000290
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.14s
Epoch 14/1000, LR 0.000290
Train loss: 0.6482;  Loss pred: 0.6482; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.14s
Epoch 15/1000, LR 0.000290
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.28s
Val loss: 0.6903 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.28s
Val loss: 0.6891 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.14s
Epoch 18/1000, LR 0.000290
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.28s
Val loss: 0.6873 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.14s
Epoch 19/1000, LR 0.000290
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 0.28s
Val loss: 0.6847 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5000 time: 0.15s
Epoch 20/1000, LR 0.000290
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.5000 time: 0.15s
Epoch 21/1000, LR 0.000290
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.29s
Val loss: 0.6749 score: 0.5504 time: 0.16s
Test loss: 0.6764 score: 0.5234 time: 0.14s
Epoch 22/1000, LR 0.000290
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.28s
Val loss: 0.6671 score: 0.6434 time: 0.16s
Test loss: 0.6693 score: 0.5859 time: 0.14s
Epoch 23/1000, LR 0.000290
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.28s
Val loss: 0.6590 score: 0.8992 time: 0.16s
Test loss: 0.6619 score: 0.8359 time: 0.15s
Epoch 24/1000, LR 0.000290
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.28s
Val loss: 0.6448 score: 0.9457 time: 0.16s
Test loss: 0.6488 score: 0.8828 time: 0.15s
Epoch 25/1000, LR 0.000290
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.28s
Val loss: 0.6275 score: 0.9535 time: 0.16s
Test loss: 0.6325 score: 0.9531 time: 0.14s
Epoch 26/1000, LR 0.000290
Train loss: 0.4607;  Loss pred: 0.4607; Loss self: 0.0000; time: 0.29s
Val loss: 0.6064 score: 0.9535 time: 0.16s
Test loss: 0.6125 score: 0.9531 time: 0.14s
Epoch 27/1000, LR 0.000290
Train loss: 0.4321;  Loss pred: 0.4321; Loss self: 0.0000; time: 0.29s
Val loss: 0.5846 score: 0.9302 time: 0.16s
Test loss: 0.5921 score: 0.9219 time: 0.15s
Epoch 28/1000, LR 0.000290
Train loss: 0.4103;  Loss pred: 0.4103; Loss self: 0.0000; time: 0.28s
Val loss: 0.5568 score: 0.9225 time: 0.16s
Test loss: 0.5660 score: 0.9219 time: 0.15s
Epoch 29/1000, LR 0.000290
Train loss: 0.3839;  Loss pred: 0.3839; Loss self: 0.0000; time: 0.28s
Val loss: 0.5344 score: 0.8992 time: 0.16s
Test loss: 0.5445 score: 0.8672 time: 0.15s
Epoch 30/1000, LR 0.000290
Train loss: 0.3591;  Loss pred: 0.3591; Loss self: 0.0000; time: 0.28s
Val loss: 0.4970 score: 0.9147 time: 0.16s
Test loss: 0.5097 score: 0.8828 time: 0.15s
Epoch 31/1000, LR 0.000290
Train loss: 0.3291;  Loss pred: 0.3291; Loss self: 0.0000; time: 0.29s
Val loss: 0.4594 score: 0.9147 time: 0.16s
Test loss: 0.4751 score: 0.8828 time: 0.15s
Epoch 32/1000, LR 0.000290
Train loss: 0.3112;  Loss pred: 0.3112; Loss self: 0.0000; time: 0.28s
Val loss: 0.4175 score: 0.9147 time: 0.16s
Test loss: 0.4359 score: 0.9141 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 0.2786;  Loss pred: 0.2786; Loss self: 0.0000; time: 0.29s
Val loss: 0.3854 score: 0.9147 time: 0.16s
Test loss: 0.4066 score: 0.8828 time: 0.15s
Epoch 34/1000, LR 0.000290
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.28s
Val loss: 0.3473 score: 0.9225 time: 0.16s
Test loss: 0.3713 score: 0.8984 time: 0.14s
Epoch 35/1000, LR 0.000290
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 0.28s
Val loss: 0.3175 score: 0.9380 time: 0.16s
Test loss: 0.3421 score: 0.9375 time: 0.14s
Epoch 36/1000, LR 0.000290
Train loss: 0.2235;  Loss pred: 0.2235; Loss self: 0.0000; time: 0.28s
Val loss: 0.2944 score: 0.9225 time: 0.15s
Test loss: 0.3198 score: 0.8984 time: 0.14s
Epoch 37/1000, LR 0.000290
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.28s
Val loss: 0.2777 score: 0.9225 time: 0.15s
Test loss: 0.3032 score: 0.8984 time: 0.14s
Epoch 38/1000, LR 0.000289
Train loss: 0.1910;  Loss pred: 0.1910; Loss self: 0.0000; time: 0.28s
Val loss: 0.2520 score: 0.9302 time: 0.16s
Test loss: 0.2778 score: 0.9297 time: 0.14s
Epoch 39/1000, LR 0.000289
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.28s
Val loss: 0.2308 score: 0.9380 time: 0.15s
Test loss: 0.2575 score: 0.9219 time: 0.14s
Epoch 40/1000, LR 0.000289
Train loss: 0.1597;  Loss pred: 0.1597; Loss self: 0.0000; time: 0.28s
Val loss: 0.2340 score: 0.9225 time: 0.15s
Test loss: 0.2628 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.1671;  Loss pred: 0.1671; Loss self: 0.0000; time: 0.28s
Val loss: 0.2125 score: 0.9302 time: 0.15s
Test loss: 0.2399 score: 0.8984 time: 0.14s
Epoch 42/1000, LR 0.000289
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.28s
Val loss: 0.2028 score: 0.9302 time: 0.16s
Test loss: 0.2288 score: 0.8984 time: 0.14s
Epoch 43/1000, LR 0.000289
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.28s
Val loss: 0.2200 score: 0.9225 time: 0.16s
Test loss: 0.2488 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.1184;  Loss pred: 0.1184; Loss self: 0.0000; time: 0.28s
Val loss: 0.2022 score: 0.9302 time: 0.15s
Test loss: 0.2287 score: 0.8984 time: 0.14s
Epoch 45/1000, LR 0.000289
Train loss: 0.1199;  Loss pred: 0.1199; Loss self: 0.0000; time: 0.28s
Val loss: 0.2451 score: 0.8992 time: 0.15s
Test loss: 0.2767 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 0.28s
Val loss: 0.2062 score: 0.9302 time: 0.15s
Test loss: 0.2329 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1027;  Loss pred: 0.1027; Loss self: 0.0000; time: 0.28s
Val loss: 0.2261 score: 0.9225 time: 0.15s
Test loss: 0.2545 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0920;  Loss pred: 0.0920; Loss self: 0.0000; time: 0.28s
Val loss: 0.2339 score: 0.9225 time: 0.15s
Test loss: 0.2628 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0805;  Loss pred: 0.0805; Loss self: 0.0000; time: 0.28s
Val loss: 0.2158 score: 0.9302 time: 0.15s
Test loss: 0.2453 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0753;  Loss pred: 0.0753; Loss self: 0.0000; time: 0.27s
Val loss: 0.2024 score: 0.9302 time: 0.15s
Test loss: 0.2330 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.28s
Val loss: 0.1863 score: 0.9225 time: 0.15s
Test loss: 0.2125 score: 0.8906 time: 0.14s
Epoch 52/1000, LR 0.000289
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.28s
Val loss: 0.1959 score: 0.9302 time: 0.15s
Test loss: 0.2244 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.28s
Val loss: 0.2071 score: 0.9302 time: 0.15s
Test loss: 0.2427 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.28s
Val loss: 0.2085 score: 0.9302 time: 0.15s
Test loss: 0.2481 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.28s
Val loss: 0.1882 score: 0.9225 time: 0.15s
Test loss: 0.2268 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.28s
Val loss: 0.1803 score: 0.9302 time: 0.15s
Test loss: 0.2187 score: 0.9062 time: 0.24s
Epoch 57/1000, LR 0.000288
Train loss: 0.0625;  Loss pred: 0.0625; Loss self: 0.0000; time: 0.29s
Val loss: 0.1980 score: 0.9302 time: 0.16s
Test loss: 0.2329 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.29s
Val loss: 0.1906 score: 0.9225 time: 0.16s
Test loss: 0.2250 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.32s
Val loss: 0.2033 score: 0.9302 time: 0.16s
Test loss: 0.2428 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.30s
Val loss: 0.2175 score: 0.9302 time: 0.17s
Test loss: 0.2600 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0566;  Loss pred: 0.0566; Loss self: 0.0000; time: 0.29s
Val loss: 0.2040 score: 0.9302 time: 0.17s
Test loss: 0.2419 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.32s
Val loss: 0.1949 score: 0.9225 time: 0.17s
Test loss: 0.2266 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.30s
Val loss: 0.2126 score: 0.9302 time: 0.17s
Test loss: 0.2556 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.29s
Val loss: 0.2310 score: 0.9302 time: 0.17s
Test loss: 0.2790 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.33s
Val loss: 0.2166 score: 0.9302 time: 0.17s
Test loss: 0.2527 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.30s
Val loss: 0.2120 score: 0.9302 time: 0.18s
Test loss: 0.2391 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.30s
Val loss: 0.2108 score: 0.9302 time: 0.17s
Test loss: 0.2319 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.29s
Val loss: 0.2439 score: 0.9302 time: 0.18s
Test loss: 0.2756 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.29s
Val loss: 0.2590 score: 0.9225 time: 0.17s
Test loss: 0.2935 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.29s
Val loss: 0.2492 score: 0.9302 time: 0.17s
Test loss: 0.2743 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.29s
Val loss: 0.2454 score: 0.9302 time: 0.17s
Test loss: 0.2626 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.28s
Val loss: 0.2401 score: 0.9225 time: 0.16s
Test loss: 0.2468 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.28s
Val loss: 0.2591 score: 0.9302 time: 0.16s
Test loss: 0.2805 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.28s
Val loss: 0.2607 score: 0.9302 time: 0.16s
Test loss: 0.2832 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.28s
Val loss: 0.2478 score: 0.9147 time: 0.16s
Test loss: 0.2582 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.28s
Val loss: 0.2585 score: 0.9147 time: 0.16s
Test loss: 0.2698 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 055,   Train_Loss: 0.0629,   Val_Loss: 0.1803,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.1803,   Test_Precision: 0.9333,   Test_Recall: 0.8750,   Test_accuracy: 0.9032,   Test_Score: 0.9062,   Test_loss: 0.2187


[0.16367715480737388, 0.17422513384371996, 0.17644050810486078, 0.176322469022125, 0.1723570900503546, 0.1770383450202644, 0.17389672598801553, 0.1722163069061935, 0.18582536396570504, 0.1778099020011723, 0.18829623493365943, 0.18170448415912688, 0.20380340702831745, 0.2185075469315052, 0.16887984098866582, 0.17605099291540682, 0.17476033605635166, 0.18417769810184836, 0.2821021690033376, 0.17529537994414568, 0.17929648095741868, 0.1646286027971655, 0.1755997168365866, 0.18836336187087, 0.17575143720023334, 0.1838474590331316, 0.17240848415531218, 0.1737592499703169, 0.16717457701452076, 0.17363573890179396, 0.16737650404684246, 0.1682164480444044, 0.18484922498464584, 0.1808662200346589, 0.1704419320449233, 0.16531789489090443, 0.18427271000109613, 0.18947842880152166, 0.18656683014705777, 0.18923211889341474, 0.1763035252224654, 0.1690766920801252, 0.179607406957075, 0.17024595709517598, 0.17599866003729403, 0.17756173992529511, 0.17310004215687513, 0.1627454669214785, 0.16908056801185012, 0.17314590699970722, 0.17616091901436448, 0.17417139885947108, 0.17316618585027754, 0.1730995220132172, 0.18650483805686235, 0.17716010101139545, 0.17677689692936838, 0.17647565808147192, 0.17117892787791789, 0.18255332787521183, 0.1756176589988172, 0.17637306288816035, 0.1772969427984208, 0.17704555089585483, 0.18793924478814006, 0.17364569590426981, 0.17279988201335073, 0.1838614868465811, 0.17619177396409214, 0.17226611799560487, 0.1731519610621035, 0.17912466102279723, 0.17626335797831416, 0.17296778899617493, 0.17360842297784984, 0.1784362441394478, 0.177690870128572, 0.17513952706940472, 0.17225796496495605, 0.17114563193172216, 0.17412557313218713, 0.17942539905197918, 0.1752725199330598, 0.17369563109241426, 0.17266709893010557, 0.17469727899879217, 0.17968683899380267, 0.1780314848292619, 0.17330173798836768, 0.17073995200917125, 0.1719913629349321, 0.17956707091070712, 0.17617801018059254, 0.1775020370259881, 0.17325422703288496, 0.17344216303899884, 0.17505908594466746, 0.17714482313022017, 0.1751193660311401, 0.1732337619177997, 0.17398847988806665, 0.17893862398341298, 0.17355130705982447, 0.1737898129504174, 0.17119188490323722, 0.17291788407601416, 0.17776678618974984, 0.2568941281642765, 0.1746897241100669, 0.18104757508262992, 0.17551816999912262, 0.1749403530266136, 0.17623046995140612, 0.17647275212220848, 0.17397335497662425, 0.17410086910240352, 0.17789575108326972, 0.17846283107064664, 0.17470021802000701, 0.17411198699846864, 0.17820460489019752, 0.17363814497366548, 0.17173203290440142, 0.17787064099684358, 0.17468596994876862, 0.1728360909037292, 0.1732337900903076, 0.1770753909368068, 0.17713674600236118, 0.17526662000454962, 0.1742504690773785, 0.17725263512693346, 0.17714155418798327, 0.17594421585090458, 0.17458833009004593, 0.17686080490238965, 0.17853870685212314, 0.17365251411683857, 0.14747703191824257, 0.16634376090951264, 0.1937222508713603, 0.15992554184049368, 0.16278876783326268, 0.1603477420285344, 0.16023721010424197, 0.16837424412369728, 0.16017074789851904, 0.16885482799261808, 0.1631125130224973, 0.1538492611143738, 0.14941447507590055, 0.14907775190658867, 0.15284639783203602, 0.15101032122038305, 0.14813393517397344, 0.1478364779613912, 0.15204841690137982, 0.1513792050536722, 0.14924112800508738, 0.14845404215157032, 0.1525546871125698, 0.15177915804088116, 0.14860894391313195, 0.14944790094159544, 0.1536130360327661, 0.15344806807115674, 0.15099360491149127, 0.14954512589611113, 0.15471683186478913, 0.15144668985158205, 0.15744859213009477, 0.14745648205280304, 0.14786261087283492, 0.14749866677448153, 0.146740750875324, 0.14763967297039926, 0.14698860584758222, 0.14723345590755343, 0.14762019482441247, 0.1477545639500022, 0.14750329591333866, 0.14662660006433725, 0.14733244688250124, 0.1468088950496167, 0.14715445297770202, 0.14643205306492746, 0.14687925204634666, 0.147538878954947, 0.1470124360639602, 0.1463359829504043, 0.1463797721080482, 0.14748437702655792, 0.14793903497047722, 0.2469731019809842, 0.15524826501496136, 0.1542798310983926, 0.15463975607417524, 0.16456181299872696, 0.16332639893516898, 0.16243379306979477, 0.16124430997297168, 0.15706277498975396, 0.16688834503293037, 0.16325592389330268, 0.16713489685207605, 0.16451781312935054, 0.1616659068968147, 0.16336302203126252, 0.15933757089078426, 0.15014538005925715, 0.14897888712584972, 0.15311025897972286, 0.15325924986973405, 0.1499614908825606]
[0.0012688151535455339, 0.001350582432897054, 0.0013677558767818665, 0.001366840845132752, 0.001336101473258563, 0.0013723902714749177, 0.0013480366355660119, 0.0013350101310557634, 0.0014405066974085663, 0.001378371333342421, 0.0014596607359198405, 0.00140856189270641, 0.0015798713723125385, 0.0016938569529574047, 0.001309146054175704, 0.0013647363791892, 0.00135473128725854, 0.001427734093812778, 0.002186838519405718, 0.001358878914295703, 0.0013898952012203, 0.0012761907193578723, 0.0013612381150122992, 0.001460181099774186, 0.0013624142418622739, 0.001425174101032028, 0.0013364998771729626, 0.0013469709300024564, 0.0012959269536009363, 0.001346013479858868, 0.0012974922794328873, 0.001304003473212437, 0.001432939728563146, 0.0014020637211989062, 0.001321255287169948, 0.0012815340689217398, 0.0014284706201635358, 0.0014688250294691602, 0.0014462544972640137, 0.0014669156503365483, 0.0013666939939726, 0.0013106720316288776, 0.001392305480287403, 0.0013197361015129922, 0.0013643306979635196, 0.0013764475963201172, 0.0013418607919137608, 0.0012615927668331668, 0.0013107020776112412, 0.0013422163333310637, 0.0013655885194911975, 0.0013501658826315587, 0.0013423735337230818, 0.0013418567597923816, 0.0014457739384252895, 0.0013733341163674065, 0.0013703635420881269, 0.0013680283572207125, 0.0013269684331621541, 0.0014151420765520298, 0.0013613772015412186, 0.0013672330456446539, 0.0013743949054141147, 0.0013724461309756188, 0.0014568933704506982, 0.0013460906659245723, 0.001339533969095742, 0.0014252828437719466, 0.001365827705148001, 0.0013353962635318207, 0.001342263264047314, 0.0013885632637426141, 0.0013663826199869316, 0.0013408355736137592, 0.001345801728510464, 0.0013832266987554094, 0.001377448605647845, 0.0013576707524760056, 0.0013353330617438452, 0.0013267103250521098, 0.001349810644435559, 0.0013908945662944123, 0.0013587017049074405, 0.0013464777604063121, 0.0013385046428690355, 0.0013542424728588541, 0.0013929212325100983, 0.0013800890296842008, 0.0013434243254912223, 0.0013235655194509398, 0.001333266379340559, 0.0013919927977574195, 0.00136572100915188, 0.0013759847831471947, 0.0013430560235107361, 0.001344512891775185, 0.0013570471778656392, 0.0013732156831800014, 0.0013575144653576751, 0.0013428973792077496, 0.0013487479061090439, 0.0013871211161504882, 0.0013453589694560036, 0.0013472078523288171, 0.0013270688752188933, 0.0013404487137675517, 0.0013780371022461227, 0.0019914273501106705, 0.0013541839078299758, 0.001403469574283953, 0.0013606059689854468, 0.0013561267676481673, 0.001366127674041908, 0.0013680058304047168, 0.0013486306587335213, 0.0013496191403287094, 0.001379036830102866, 0.001383432798997261, 0.0013542652559690466, 0.0013497053255695243, 0.001381431045660446, 0.0013460321315788022, 0.0013312560690263676, 0.0013788421782701052, 0.0013541548058044078, 0.0013398146581684435, 0.0013428975975992837, 0.0013726774491225333, 0.0013731530697857455, 0.0013586559690275165, 0.0013507788300571979, 0.0013740514350925074, 0.001373190342542506, 0.0013639086500070122, 0.0013533979076747746, 0.0013710139914913926, 0.0013840209833497917, 0.0013461435202855704, 0.00115216431186127, 0.0012995606321055675, 0.0015134550849325024, 0.0012494182956288569, 0.0012717872486973647, 0.001252716734597925, 0.0012518532039393904, 0.001315423782216385, 0.00125133396795718, 0.0013191783436923288, 0.0012743165079882601, 0.0012019473524560453, 0.001167300586530473, 0.001164669936770224, 0.0011941124830627814, 0.0011797681345342426, 0.0011572963685466675, 0.0011549724840733688, 0.0011878782570420299, 0.001182650039481814, 0.0011659463125397451, 0.0011597972043091431, 0.0011918334930669516, 0.001185774672194384, 0.0011610073743213434, 0.0011675617261062143, 0.0012001018440059852, 0.001198813031805912, 0.0011796375383710256, 0.0011683212960633682, 0.001208725248943665, 0.0011831772644654848, 0.0012300671260163654, 0.0011520037660375237, 0.0011551766474440228, 0.001152333334175637, 0.0011464121162134688, 0.0011534349450812442, 0.001148348483184236, 0.0011502613742777612, 0.0011532827720657224, 0.0011543325308593921, 0.0011523694993229583, 0.0011455203130026348, 0.001151034741269541, 0.0011469444925751304, 0.001149644163888297, 0.0011440004145697458, 0.0011474941566120833, 0.0011526474918355234, 0.001148534656749689, 0.0011432498668000335, 0.0011435919695941266, 0.0011522216955199838, 0.0011557737107068533, 0.0019294773592264391, 0.0012128770704293856, 0.0012053111804561922, 0.001208123094329494, 0.0012856391640525544, 0.0012759874916810077, 0.0012690140083577717, 0.0012597211716638412, 0.0012270529296074528, 0.0013038151955697685, 0.0012754369054164272, 0.0013057413816568442, 0.0012852954150730511, 0.0012630148976313649, 0.0012762736096192384, 0.001244824772584252, 0.0011730107817129465, 0.001163897555670701, 0.0011961738982790848, 0.0011973378896072973, 0.0011715741475200048]
[788.1368670650204, 740.4212994648238, 731.1246231694918, 731.6140745727253, 748.4461472534277, 728.6557044194818, 741.8196016461535, 749.0579859563851, 694.2001740075029, 725.4939041536028, 685.0907031967437, 709.9439543111596, 632.9629218714489, 590.3686248440525, 763.856711640661, 732.7422462308122, 738.1537648131084, 700.4105346601972, 457.2811349014256, 735.9007410298162, 719.4787053887374, 783.5819402472695, 734.6253304044199, 684.8465578376873, 733.9911528178885, 701.6686587806067, 748.2230392083944, 742.4065194919806, 771.6484306629654, 742.934610212709, 770.717495472947, 766.8691230833007, 697.866058192644, 713.2343451158546, 756.8560063376865, 780.314799466379, 700.0493996057943, 680.8162850829173, 691.441237964531, 681.702454923413, 731.6926864464207, 762.9673754136796, 718.233185287454, 757.7272447526174, 732.9601257910996, 726.5078617402245, 745.2337873094875, 792.6488057712843, 762.9498854709251, 745.0363813694893, 732.2850080583487, 740.649732646878, 744.9491329186843, 745.2360266491669, 691.6710651799282, 728.1549246334102, 729.7333658455508, 730.9789996105058, 753.5974293050881, 706.6428287091028, 734.5502766374356, 731.4042058780824, 727.5929182076626, 728.6260476315662, 686.3920313472526, 742.8920096649973, 746.5282875021484, 701.6151245836547, 732.1567692841902, 748.8413943552803, 745.0103320154269, 720.1688436612429, 731.8594260292657, 745.8036016339016, 743.0515051476432, 722.9472948286593, 725.9798992860991, 736.5556031727752, 748.876837284381, 753.7440397629547, 740.8446541167727, 718.9617561481858, 735.9967212730653, 742.6784380740464, 747.1023767661626, 738.4202017301706, 717.9156844339009, 724.5909347086291, 744.3664529703603, 755.5349435325529, 750.0376635122275, 718.3945215888024, 732.2139685183618, 726.7522230244214, 744.5705782146073, 743.7637869575813, 736.8940566773794, 728.2177244613655, 736.6404009083778, 744.658538681456, 741.4283984950646, 720.917581281713, 743.2960441809448, 742.2759585845458, 753.5403916658471, 746.0188440849299, 725.669866486219, 502.1523883080277, 738.4521365362102, 712.5198994856713, 734.9666419188671, 737.3941904665946, 731.9960052059699, 730.9910365690149, 741.4928568649661, 740.9497762134899, 725.143794691406, 722.8395920096875, 738.4077791203824, 740.9024629713438, 723.8870178438127, 742.9243155043182, 751.1702844152018, 725.2461635998106, 738.468006548166, 746.3718909949995, 744.6584175798018, 728.5032624665301, 728.25092992439, 736.0214968295239, 740.3136455415545, 727.7747939127734, 728.2311628761297, 733.1869330067367, 738.8810004280742, 729.3871588518199, 722.5323980129745, 742.8628410942842, 867.9317608653791, 769.4908381302572, 660.7397933085, 800.3724641287411, 786.2950356077682, 798.2650605533439, 798.8157052705167, 760.2112821125061, 799.1471706250521, 758.0476171258534, 784.7343997596653, 831.9831962328562, 856.6773730254563, 858.6123573972602, 837.4420451874836, 847.6241820133473, 864.0828980184218, 865.8214925373717, 841.8371108922635, 845.558674684657, 857.6724238886528, 862.219702103585, 839.0433779694297, 843.3305445371075, 861.3209718711229, 856.4857665683954, 833.262614331099, 834.1584329406089, 847.7180214024987, 855.9289327083885, 827.3178713474587, 845.1818928854779, 812.9637633992794, 868.0527177785523, 865.6684691580538, 867.8044540952084, 872.2866636327437, 866.9756402512699, 870.8157973328069, 869.3676257953903, 867.0900356976916, 866.3014974164397, 867.7772195355061, 872.9657507153257, 868.7835076959047, 871.881774988771, 869.8343638938031, 874.1255573548863, 871.4641327259111, 867.5679312914295, 870.6746410508529, 874.6994240191857, 874.4377597849966, 867.8885355901167, 865.2212718944918, 518.2750630465637, 824.4858645451823, 829.6612660819383, 827.730224423032, 777.8232244013391, 783.7067420485313, 788.0133658209951, 793.8264613582693, 814.9607697199424, 766.9798629421549, 784.0450560535586, 765.8484398580587, 778.031251238194, 791.7562982632919, 783.5310488777861, 803.3259154410972, 852.5070831315812, 859.1821463391129, 835.9988471899304, 835.1861314002014, 853.5524636804304]
Elapsed: 0.1696939051780136~0.016918215926342968
Time per graph: 0.0013187954245108006~0.00012857348534977474
Speed: 764.4971786550965~65.70009854793554
Total Time: 0.1507
best val loss: 0.18032054920760235 test_score: 0.9062

Testing...
Test loss: 0.6325 score: 0.9531 time: 0.14s
test Score 0.9531
Epoch Time List: [0.7355788538698107, 0.5761964002158493, 0.5847231498919427, 0.5850930558517575, 0.5984998540952802, 0.5883450489491224, 0.5884611047804356, 0.5928767358418554, 0.6100106269586831, 0.6126978620886803, 0.6365901741664857, 0.6856405176222324, 0.6656457500066608, 0.7957151159644127, 0.5950423679314554, 0.6082313389051706, 0.6691571641713381, 0.636045107152313, 0.7082949560135603, 0.5977205652743578, 0.5999750189948827, 0.5992610356770456, 0.6076365530025214, 0.6331185370218009, 0.6201958889141679, 0.6261784452944994, 0.607981787994504, 0.5919536810833961, 0.5852300385013223, 0.5879270450677723, 0.587035161210224, 0.5980533070396632, 0.5935527689289302, 0.5918538991827518, 0.5939101872500032, 0.5914561559911817, 0.6146295219659805, 0.6466403130907565, 0.6462541988585144, 0.6336487773805857, 0.6233505480922759, 0.6071122467983514, 0.6205560930538923, 0.6053324639797211, 0.59228326799348, 0.5976065883878618, 0.5908271500375122, 0.5812313158530742, 0.5887017780914903, 0.6014713668264449, 0.5964144382160157, 0.604856233112514, 0.596102928975597, 0.5861369906924665, 0.6082421990577132, 0.6007783869281411, 0.6071245840284973, 0.600632979767397, 0.593400300713256, 0.6118422949220985, 0.6000126758590341, 0.6027024898212403, 0.5997032602317631, 0.6020743080880493, 0.6246934360824525, 0.5983229987323284, 0.600915860151872, 0.5832505198195577, 0.5958667339291424, 0.60274987667799, 0.5869217454455793, 0.6077811850700527, 0.5845521311275661, 0.5850287510547787, 0.5812229430302978, 0.5864684840198606, 0.5860187890939415, 0.5888702541124076, 0.5831452468410134, 0.5807782129850239, 0.5850423432420939, 0.5864634199533612, 0.587118525756523, 0.5815147841349244, 0.5812619389034808, 0.5878323209472001, 0.5903903648722917, 0.586510359775275, 0.584799821022898, 0.5776085839606822, 0.5819746619090438, 0.5885324887931347, 0.5828083981759846, 0.5880082692019641, 0.5835265289060771, 0.5814440702088177, 0.5821210930589586, 0.5830806519370526, 0.5860690618865192, 0.5786605237517506, 0.5882829187903553, 0.587056343909353, 0.660180636914447, 0.5838895828928798, 0.5791701020207256, 0.6156350499950349, 0.5784120340831578, 0.6650710948742926, 0.5921579122077674, 0.5967311998829246, 0.6822053890209645, 0.5872353608720005, 0.5954865838866681, 0.5864470440428704, 0.5833011029753834, 0.5918463992420584, 0.5938139311037958, 0.5885407677851617, 0.5882212398573756, 0.5887607489712536, 0.7321967561729252, 0.578000953886658, 0.5821100999601185, 0.5829480392858386, 0.5812181681394577, 0.5846625019330531, 0.5865747970528901, 0.5880061031784862, 0.5902438862249255, 0.5880087669938803, 0.5853040618821979, 0.5841779937036335, 0.5830957000143826, 0.5852565381210297, 0.5849502691999078, 0.5905407669488341, 0.5867508361116052, 0.583723190240562, 0.5843335511162877, 0.6126822400838137, 0.6358356238342822, 0.617108941078186, 0.6146530299447477, 0.6100604010280222, 0.602839971659705, 0.6921962038613856, 0.6001332842279226, 0.6441783872433007, 0.6189706779550761, 0.5916590539272875, 0.5873968238011003, 0.5822081151418388, 0.5888888218905777, 0.5867836778052151, 0.5779705820605159, 0.5826930811163038, 0.5863898799289018, 0.583324977895245, 0.5892871231772006, 0.5868420617189258, 0.5865142932161689, 0.5832520029507577, 0.5863104327581823, 0.5908477792982012, 0.5916422219015658, 0.5884683169424534, 0.5873905648477376, 0.5847838541958481, 0.5939689229708165, 0.5885969747323543, 0.5992931621149182, 0.5836426408495754, 0.5812097461894155, 0.5779820929747075, 0.5750138780567795, 0.5748265522997826, 0.5737781638745219, 0.5749010068830103, 0.5755362450145185, 0.5752184449229389, 0.5802586139179766, 0.573438033927232, 0.5769972389098257, 0.5735092719551176, 0.5746766498778015, 0.5704935742542148, 0.5719791799783707, 0.5708152493461967, 0.5718503901734948, 0.5725401237141341, 0.5752872300799936, 0.5771421750541776, 0.5741870356723666, 0.6763583859428763, 0.601733657065779, 0.6033369596116245, 0.6283217666205019, 0.6234769651200622, 0.6182419809047133, 0.6560157861094922, 0.6267122330609709, 0.610861360328272, 0.6658075549639761, 0.6297801150940359, 0.6285160237457603, 0.627120794961229, 0.6209430797025561, 0.620215036906302, 0.6152031258679926, 0.5851811682805419, 0.5799863548018038, 0.5890177912078798, 0.5873495561536402, 0.5784577669110149]
Total Epoch List: [67, 71, 76]
Total Time List: [0.17328483704477549, 0.17430002498440444, 0.15070676594041288]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae4d4dfc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.24s
Val loss: 0.6931 score: 0.5271 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.25s
Val loss: 0.6931 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.25s
Val loss: 0.6931 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.25s
Val loss: 0.6931 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.24s
Val loss: 0.6931 score: 0.5194 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.25s
Val loss: 0.6930 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.5982;  Loss pred: 0.5982; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4961 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4961 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4961 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.4961 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 0.4877;  Loss pred: 0.4877; Loss self: 0.0000; time: 0.25s
Val loss: 0.6854 score: 0.5349 time: 0.18s
Test loss: 0.6834 score: 0.5659 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4586;  Loss pred: 0.4586; Loss self: 0.0000; time: 0.26s
Val loss: 0.6830 score: 0.6744 time: 0.19s
Test loss: 0.6803 score: 0.6899 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4257;  Loss pred: 0.4257; Loss self: 0.0000; time: 0.25s
Val loss: 0.6801 score: 0.7519 time: 0.18s
Test loss: 0.6766 score: 0.7674 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4105;  Loss pred: 0.4105; Loss self: 0.0000; time: 0.27s
Val loss: 0.6767 score: 0.8372 time: 0.19s
Test loss: 0.6722 score: 0.8760 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3715;  Loss pred: 0.3715; Loss self: 0.0000; time: 0.26s
Val loss: 0.6726 score: 0.8372 time: 0.18s
Test loss: 0.6669 score: 0.9302 time: 0.21s
Epoch 24/1000, LR 0.000285
Train loss: 0.3419;  Loss pred: 0.3419; Loss self: 0.0000; time: 0.24s
Val loss: 0.6678 score: 0.8527 time: 0.22s
Test loss: 0.6607 score: 0.9535 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3234;  Loss pred: 0.3234; Loss self: 0.0000; time: 0.26s
Val loss: 0.6621 score: 0.8682 time: 0.19s
Test loss: 0.6534 score: 0.9535 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.26s
Val loss: 0.6555 score: 0.8682 time: 0.18s
Test loss: 0.6448 score: 0.9690 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2617;  Loss pred: 0.2617; Loss self: 0.0000; time: 0.26s
Val loss: 0.6476 score: 0.8682 time: 0.19s
Test loss: 0.6346 score: 0.9612 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.28s
Val loss: 0.6382 score: 0.8682 time: 0.19s
Test loss: 0.6226 score: 0.9612 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2052;  Loss pred: 0.2052; Loss self: 0.0000; time: 0.26s
Val loss: 0.6270 score: 0.8682 time: 0.19s
Test loss: 0.6084 score: 0.9612 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.1819;  Loss pred: 0.1819; Loss self: 0.0000; time: 0.25s
Val loss: 0.6137 score: 0.8760 time: 0.18s
Test loss: 0.5916 score: 0.9612 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.1566;  Loss pred: 0.1566; Loss self: 0.0000; time: 0.26s
Val loss: 0.5981 score: 0.8682 time: 0.19s
Test loss: 0.5722 score: 0.9612 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1314;  Loss pred: 0.1314; Loss self: 0.0000; time: 0.26s
Val loss: 0.5802 score: 0.8682 time: 0.19s
Test loss: 0.5497 score: 0.9612 time: 0.19s
Epoch 33/1000, LR 0.000285
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.25s
Val loss: 0.5596 score: 0.8682 time: 0.18s
Test loss: 0.5239 score: 0.9612 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.24s
Val loss: 0.5362 score: 0.8682 time: 0.18s
Test loss: 0.4942 score: 0.9612 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.24s
Val loss: 0.5102 score: 0.8682 time: 0.17s
Test loss: 0.4613 score: 0.9612 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.24s
Val loss: 0.4829 score: 0.8682 time: 0.18s
Test loss: 0.4261 score: 0.9612 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.24s
Val loss: 0.4546 score: 0.8682 time: 0.17s
Test loss: 0.3894 score: 0.9612 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.24s
Val loss: 0.4260 score: 0.8682 time: 0.17s
Test loss: 0.3517 score: 0.9612 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.24s
Val loss: 0.3989 score: 0.8682 time: 0.18s
Test loss: 0.3147 score: 0.9612 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.24s
Val loss: 0.3743 score: 0.8682 time: 0.17s
Test loss: 0.2793 score: 0.9612 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.25s
Val loss: 0.3540 score: 0.8682 time: 0.17s
Test loss: 0.2472 score: 0.9612 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.25s
Val loss: 0.3377 score: 0.8682 time: 0.17s
Test loss: 0.2188 score: 0.9535 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.24s
Val loss: 0.3261 score: 0.8682 time: 0.17s
Test loss: 0.1942 score: 0.9535 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.24s
Val loss: 0.3194 score: 0.8682 time: 0.18s
Test loss: 0.1738 score: 0.9535 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.24s
Val loss: 0.3188 score: 0.8682 time: 0.18s
Test loss: 0.1579 score: 0.9535 time: 0.21s
Epoch 46/1000, LR 0.000284
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.28s
Val loss: 0.3221 score: 0.8682 time: 0.20s
Test loss: 0.1462 score: 0.9535 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.26s
Val loss: 0.3278 score: 0.8682 time: 0.20s
Test loss: 0.1378 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.27s
Val loss: 0.3343 score: 0.8682 time: 0.20s
Test loss: 0.1314 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.27s
Val loss: 0.3443 score: 0.8605 time: 0.23s
Test loss: 0.1275 score: 0.9535 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.31s
Val loss: 0.3533 score: 0.8605 time: 0.21s
Test loss: 0.1249 score: 0.9535 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.30s
Val loss: 0.3647 score: 0.8605 time: 0.19s
Test loss: 0.1242 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.27s
Val loss: 0.3781 score: 0.8605 time: 0.20s
Test loss: 0.1248 score: 0.9535 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.25s
Val loss: 0.3898 score: 0.8605 time: 0.19s
Test loss: 0.1259 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.26s
Val loss: 0.4042 score: 0.8605 time: 0.18s
Test loss: 0.1282 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.26s
Val loss: 0.4164 score: 0.8605 time: 0.19s
Test loss: 0.1306 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.25s
Val loss: 0.4269 score: 0.8605 time: 0.18s
Test loss: 0.1332 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.26s
Val loss: 0.4372 score: 0.8605 time: 0.19s
Test loss: 0.1359 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.26s
Val loss: 0.4460 score: 0.8605 time: 0.19s
Test loss: 0.1383 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.26s
Val loss: 0.4537 score: 0.8605 time: 0.19s
Test loss: 0.1408 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.26s
Val loss: 0.4616 score: 0.8605 time: 0.18s
Test loss: 0.1425 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.27s
Val loss: 0.4615 score: 0.8682 time: 0.18s
Test loss: 0.1424 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.27s
Val loss: 0.4637 score: 0.8682 time: 0.18s
Test loss: 0.1427 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.27s
Val loss: 0.4651 score: 0.8682 time: 0.18s
Test loss: 0.1432 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.27s
Val loss: 0.4683 score: 0.8682 time: 0.18s
Test loss: 0.1439 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.26s
Val loss: 0.4702 score: 0.8682 time: 0.18s
Test loss: 0.1449 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.0157,   Val_Loss: 0.3188,   Val_Precision: 0.9608,   Val_Recall: 0.7656,   Val_accuracy: 0.8522,   Val_Score: 0.8682,   Val_Loss: 0.3188,   Test_Precision: 0.9683,   Test_Recall: 0.9385,   Test_accuracy: 0.9531,   Test_Score: 0.9535,   Test_loss: 0.1579


[0.17784391692839563, 0.18035966297611594, 0.18314624903723598, 0.17992463102564216, 0.17628968483768404, 0.18057957291603088, 0.17864365899004042, 0.1736993209924549, 0.17915833508595824, 0.17913055699318647, 0.1726560911629349, 0.17651046509854496, 0.18021810799837112, 0.18454003892838955, 0.1813419449608773, 0.1798144921194762, 0.18934265221469104, 0.19044404989108443, 0.17773487395606935, 0.18604349601082504, 0.1813361111562699, 0.1782723229844123, 0.21084306482225657, 0.17852375213988125, 0.18699319195002317, 0.1826836250256747, 0.18659889698028564, 0.18157036695629358, 0.18215126497671008, 0.18781408900395036, 0.185586930019781, 0.194993152981624, 0.18704273202456534, 0.1830208469182253, 0.17055930197238922, 0.17516069510020316, 0.1761520649306476, 0.1780277790967375, 0.1747009630780667, 0.17223708890378475, 0.1715893168002367, 0.17463875096291304, 0.17800498916767538, 0.17614954710006714, 0.21161105413921177, 0.19978339690715075, 0.19669431913644075, 0.19238697597756982, 0.21077235788106918, 0.20749434712342918, 0.19899726705625653, 0.20121278194710612, 0.18064657505601645, 0.17942591616883874, 0.18742602784186602, 0.18117123004049063, 0.18927396088838577, 0.18824015115387738, 0.19223285606130958, 0.1819424128625542, 0.1793572879396379, 0.18637097906321287, 0.1803191681392491, 0.18253246415406466, 0.18508880911394954]
[0.0013786350149488033, 0.0013981369222954723, 0.0014197383646297362, 0.0013947645815941252, 0.0013665867041680933, 0.0013998416505118672, 0.001384834565814267, 0.001346506364282596, 0.0013888243029919244, 0.00138860896893943, 0.0013384193113405807, 0.001368298179058488, 0.001397039596886598, 0.0014305429374293765, 0.0014057515113246302, 0.0013939107916238465, 0.0014677724977883028, 0.0014763104642719724, 0.0013777897205896849, 0.0014421976434947678, 0.0014057062880331, 0.0013819559921272272, 0.0016344423629632292, 0.0013839050553479167, 0.0014495596275195595, 0.0014161521319819743, 0.0014465030773665555, 0.0014075222244673922, 0.0014120253098969773, 0.0014559231705732587, 0.0014386583722463644, 0.0015115748293149148, 0.0014499436591051576, 0.001418766255180041, 0.0013221651315689087, 0.001357834845738009, 0.0013655198831833148, 0.0013800603030754844, 0.0013542710316129202, 0.0013351712318122849, 0.001330149742637494, 0.0013537887671543646, 0.0013798836369587238, 0.0013655003651167996, 0.0016403957685210216, 0.0015487085031562072, 0.001524762163848378, 0.0014913719068028668, 0.001633894247140071, 0.0016084833110343348, 0.0015426144733043142, 0.001559789007341908, 0.001400361046945864, 0.0013908985749522384, 0.0014529149445105893, 0.0014044281398487646, 0.0014672400068867113, 0.0014592259779370338, 0.0014901771787698417, 0.00141040630126011, 0.0013903665731754876, 0.0014447362718078517, 0.0013978230088313882, 0.001414980342279571, 0.0014347969698755778]
[725.3551441511412, 715.2375307836031, 704.3551297289886, 716.966872543512, 731.7501311479156, 714.366513979877, 722.1079143211676, 742.6626613330485, 720.0334828860024, 720.1451397536084, 747.1500086160481, 730.8348540579727, 715.7993246781059, 699.0352920108478, 711.363275759673, 717.4060248396834, 681.3044947407308, 677.364297145411, 725.8001602537771, 693.3862390572045, 711.3861611867906, 723.6120438688592, 611.8294671382655, 722.5929236515419, 689.8646878784616, 706.1388232353617, 691.3224144815227, 710.4683553955257, 708.2026030205938, 686.8494301153663, 695.0920519362564, 661.561690897715, 689.6819705512949, 704.8377393731431, 756.3351779012498, 736.4665910134903, 732.3218155335747, 724.6060174120547, 738.4046299868146, 748.9676051832373, 751.7950558086379, 738.6676742059094, 724.6987885181458, 732.3322831293889, 609.6089853374826, 645.699302329676, 655.8399884976683, 670.5235598434687, 612.0347150682339, 621.7036839238017, 648.2501086988883, 641.1123525637199, 714.1015541534545, 718.9596840548484, 688.2715356313219, 712.0335826564146, 681.5517538414641, 685.2948173344199, 671.0611424243602, 709.0155504173247, 719.2347826056252, 692.16785064077, 715.3981539022044, 706.7235989928831, 696.9627208557024]
Elapsed: 0.18423158439735954~0.009436176602530795
Time per graph: 0.0014281518170337947~7.314865583357206e-05
Speed: 701.9386141074655~33.90375898227345
Total Time: 0.1856
best val loss: 0.3187693952070188 test_score: 0.9535

Testing...
Test loss: 0.5916 score: 0.9612 time: 0.18s
test Score 0.9612
Epoch Time List: [0.5996897041331977, 0.5954353336710483, 0.5980404962319881, 0.5997032499872148, 0.59503329009749, 0.5995223459322006, 0.5961276197340339, 0.5873305902350694, 0.5949691568966955, 0.5919328178279102, 0.5850181363057345, 0.5959222230594605, 0.5984742299187928, 0.6116181560792029, 0.6075543067418039, 0.6063216968905181, 0.6273905641864985, 0.6307576578110456, 0.6007346799597144, 0.6229497308377177, 0.605084290727973, 0.62949986057356, 0.6424768101423979, 0.6352134176995605, 0.6352104181423783, 0.6126998451072723, 0.6321634168270975, 0.643944603158161, 0.6250184739474207, 0.6200141997542232, 0.6284278356470168, 0.6436166679486632, 0.6171019312459975, 0.5983942591119558, 0.5821269350126386, 0.5912530981004238, 0.5827502058818936, 0.5862940798979253, 0.5945157681126148, 0.5868980940431356, 0.5869281708728522, 0.5895099961198866, 0.5888552758842707, 0.5925982082262635, 0.627073701005429, 0.6794025192502886, 0.6519582769833505, 0.6533312667161226, 0.7090703598223627, 0.7271223380230367, 0.6860457370057702, 0.6695488977711648, 0.6205281121656299, 0.6143112319987267, 0.6291306428611279, 0.6063253441825509, 0.6374963717535138, 0.6360186981037259, 0.6410940480418503, 0.6260202068369836, 0.624841890996322, 0.6292872438207269, 0.6286251780111343, 0.6227864630054682, 0.6229611788876355]
Total Epoch List: [65]
Total Time List: [0.18556979508139193]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae232cb50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5039 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.4982;  Loss pred: 0.4982; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4663;  Loss pred: 0.4663; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.5039 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.4377;  Loss pred: 0.4377; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.5039 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4112;  Loss pred: 0.4112; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.5039 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.3760;  Loss pred: 0.3760; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6734 score: 0.4961 time: 0.18s
Test loss: 0.6709 score: 0.5116 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.3452;  Loss pred: 0.3452; Loss self: 0.0000; time: 0.25s
Val loss: 0.6663 score: 0.5039 time: 0.18s
Test loss: 0.6635 score: 0.5349 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3127;  Loss pred: 0.3127; Loss self: 0.0000; time: 0.26s
Val loss: 0.6575 score: 0.5736 time: 0.18s
Test loss: 0.6544 score: 0.6124 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.27s
Val loss: 0.6468 score: 0.7132 time: 0.19s
Test loss: 0.6435 score: 0.6512 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.28s
Val loss: 0.6342 score: 0.7597 time: 0.19s
Test loss: 0.6307 score: 0.7519 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2307;  Loss pred: 0.2307; Loss self: 0.0000; time: 0.25s
Val loss: 0.6189 score: 0.8372 time: 0.18s
Test loss: 0.6155 score: 0.7984 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 0.2047;  Loss pred: 0.2047; Loss self: 0.0000; time: 0.25s
Val loss: 0.6014 score: 0.8682 time: 0.19s
Test loss: 0.5983 score: 0.8295 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1663;  Loss pred: 0.1663; Loss self: 0.0000; time: 0.26s
Val loss: 0.5814 score: 0.8837 time: 0.18s
Test loss: 0.5790 score: 0.8527 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.1487;  Loss pred: 0.1487; Loss self: 0.0000; time: 0.25s
Val loss: 0.5589 score: 0.8915 time: 0.18s
Test loss: 0.5576 score: 0.8527 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 0.1267;  Loss pred: 0.1267; Loss self: 0.0000; time: 0.25s
Val loss: 0.5338 score: 0.9070 time: 0.18s
Test loss: 0.5342 score: 0.8760 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.25s
Val loss: 0.5060 score: 0.9070 time: 0.18s
Test loss: 0.5085 score: 0.8837 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.0929;  Loss pred: 0.0929; Loss self: 0.0000; time: 0.26s
Val loss: 0.4762 score: 0.9070 time: 0.18s
Test loss: 0.4815 score: 0.8837 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.26s
Val loss: 0.4448 score: 0.9070 time: 0.18s
Test loss: 0.4536 score: 0.9070 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.25s
Val loss: 0.4122 score: 0.9070 time: 0.18s
Test loss: 0.4250 score: 0.9070 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.26s
Val loss: 0.3795 score: 0.9070 time: 0.18s
Test loss: 0.3970 score: 0.9070 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.26s
Val loss: 0.3478 score: 0.9147 time: 0.18s
Test loss: 0.3704 score: 0.9070 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.26s
Val loss: 0.3182 score: 0.9147 time: 0.17s
Test loss: 0.3458 score: 0.9070 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.25s
Val loss: 0.2927 score: 0.9147 time: 0.18s
Test loss: 0.3254 score: 0.8915 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.25s
Val loss: 0.2722 score: 0.9147 time: 0.18s
Test loss: 0.3096 score: 0.8837 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.25s
Val loss: 0.2554 score: 0.9147 time: 0.18s
Test loss: 0.2967 score: 0.8837 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.26s
Val loss: 0.2428 score: 0.9147 time: 0.18s
Test loss: 0.2875 score: 0.8837 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.26s
Val loss: 0.2329 score: 0.9147 time: 0.17s
Test loss: 0.2812 score: 0.8837 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.25s
Val loss: 0.2280 score: 0.9147 time: 0.18s
Test loss: 0.2793 score: 0.8837 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.26s
Val loss: 0.2251 score: 0.9147 time: 0.18s
Test loss: 0.2779 score: 0.8837 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.25s
Val loss: 0.2251 score: 0.9147 time: 0.19s
Test loss: 0.2784 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.27s
Val loss: 0.2282 score: 0.9147 time: 0.18s
Test loss: 0.2840 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.27s
Val loss: 0.2355 score: 0.9147 time: 0.18s
Test loss: 0.2929 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.25s
Val loss: 0.2442 score: 0.9147 time: 0.18s
Test loss: 0.3031 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.25s
Val loss: 0.2472 score: 0.9147 time: 0.18s
Test loss: 0.3091 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.26s
Val loss: 0.2484 score: 0.9147 time: 0.18s
Test loss: 0.3127 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.26s
Val loss: 0.2559 score: 0.9147 time: 0.18s
Test loss: 0.3225 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.26s
Val loss: 0.2645 score: 0.9147 time: 0.17s
Test loss: 0.3351 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.25s
Val loss: 0.2725 score: 0.9147 time: 0.18s
Test loss: 0.3503 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.25s
Val loss: 0.2786 score: 0.9147 time: 0.18s
Test loss: 0.3610 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.26s
Val loss: 0.2876 score: 0.9147 time: 0.18s
Test loss: 0.3724 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.26s
Val loss: 0.2966 score: 0.9147 time: 0.18s
Test loss: 0.3817 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.25s
Val loss: 0.3024 score: 0.9147 time: 0.18s
Test loss: 0.3868 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.25s
Val loss: 0.3063 score: 0.9147 time: 0.18s
Test loss: 0.3890 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.25s
Val loss: 0.3067 score: 0.9147 time: 0.18s
Test loss: 0.3895 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.26s
Val loss: 0.3094 score: 0.9147 time: 0.18s
Test loss: 0.3918 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.26s
Val loss: 0.3160 score: 0.9147 time: 0.18s
Test loss: 0.3952 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.25s
Val loss: 0.3169 score: 0.9147 time: 0.18s
Test loss: 0.3942 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.25s
Val loss: 0.3162 score: 0.9147 time: 0.18s
Test loss: 0.3922 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.25s
Val loss: 0.3179 score: 0.9147 time: 0.18s
Test loss: 0.3920 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.0159,   Val_Loss: 0.2251,   Val_Precision: 1.0000,   Val_Recall: 0.8308,   Val_accuracy: 0.9076,   Val_Score: 0.9147,   Val_Loss: 0.2251,   Test_Precision: 0.9804,   Test_Recall: 0.7812,   Test_accuracy: 0.8696,   Test_Score: 0.8837,   Test_loss: 0.2779


[0.17784391692839563, 0.18035966297611594, 0.18314624903723598, 0.17992463102564216, 0.17628968483768404, 0.18057957291603088, 0.17864365899004042, 0.1736993209924549, 0.17915833508595824, 0.17913055699318647, 0.1726560911629349, 0.17651046509854496, 0.18021810799837112, 0.18454003892838955, 0.1813419449608773, 0.1798144921194762, 0.18934265221469104, 0.19044404989108443, 0.17773487395606935, 0.18604349601082504, 0.1813361111562699, 0.1782723229844123, 0.21084306482225657, 0.17852375213988125, 0.18699319195002317, 0.1826836250256747, 0.18659889698028564, 0.18157036695629358, 0.18215126497671008, 0.18781408900395036, 0.185586930019781, 0.194993152981624, 0.18704273202456534, 0.1830208469182253, 0.17055930197238922, 0.17516069510020316, 0.1761520649306476, 0.1780277790967375, 0.1747009630780667, 0.17223708890378475, 0.1715893168002367, 0.17463875096291304, 0.17800498916767538, 0.17614954710006714, 0.21161105413921177, 0.19978339690715075, 0.19669431913644075, 0.19238697597756982, 0.21077235788106918, 0.20749434712342918, 0.19899726705625653, 0.20121278194710612, 0.18064657505601645, 0.17942591616883874, 0.18742602784186602, 0.18117123004049063, 0.18927396088838577, 0.18824015115387738, 0.19223285606130958, 0.1819424128625542, 0.1793572879396379, 0.18637097906321287, 0.1803191681392491, 0.18253246415406466, 0.18508880911394954, 0.16709276102483273, 0.18749175686389208, 0.18319460307247937, 0.17458910890854895, 0.19192822999320924, 0.20280281198211014, 0.17812716215848923, 0.1660676260944456, 0.1637227658648044, 0.17596594989299774, 0.17626660619862378, 0.1676885150372982, 0.16669926210306585, 0.16460060700774193, 0.16806594002991915, 0.17667131102643907, 0.17514435201883316, 0.1786932849790901, 0.16621206607669592, 0.16250308393500745, 0.16451509296894073, 0.16593537596054375, 0.17346712108701468, 0.16697002993896604, 0.178157381946221, 0.17033151583746076, 0.1659468561410904, 0.17115343804471195, 0.16554027702659369, 0.1648035270627588, 0.16255407314747572, 0.1623921839054674, 0.16270965500734746, 0.16494411788880825, 0.16417867178097367, 0.16323084803298116, 0.16332301101647317, 0.16398364189080894, 0.16640201304107904, 0.16398007585667074, 0.1621461878530681, 0.16380490106530488, 0.1657887890469283, 0.16460557305254042, 0.16580579103901982, 0.1724544269964099, 0.16983913513831794, 0.16560115781612694, 0.16345739807002246, 0.16481076297350228, 0.16120697115547955, 0.16184419207274914, 0.16451678308658302, 0.16455284622497857, 0.16305928211659193, 0.16119650704786181, 0.16251342999748886, 0.16615871293470263, 0.16322688502259552, 0.161727761849761, 0.16068165586329997, 0.1636184509843588, 0.16436842922121286, 0.16026329202577472, 0.1615319261327386]
[0.0013786350149488033, 0.0013981369222954723, 0.0014197383646297362, 0.0013947645815941252, 0.0013665867041680933, 0.0013998416505118672, 0.001384834565814267, 0.001346506364282596, 0.0013888243029919244, 0.00138860896893943, 0.0013384193113405807, 0.001368298179058488, 0.001397039596886598, 0.0014305429374293765, 0.0014057515113246302, 0.0013939107916238465, 0.0014677724977883028, 0.0014763104642719724, 0.0013777897205896849, 0.0014421976434947678, 0.0014057062880331, 0.0013819559921272272, 0.0016344423629632292, 0.0013839050553479167, 0.0014495596275195595, 0.0014161521319819743, 0.0014465030773665555, 0.0014075222244673922, 0.0014120253098969773, 0.0014559231705732587, 0.0014386583722463644, 0.0015115748293149148, 0.0014499436591051576, 0.001418766255180041, 0.0013221651315689087, 0.001357834845738009, 0.0013655198831833148, 0.0013800603030754844, 0.0013542710316129202, 0.0013351712318122849, 0.001330149742637494, 0.0013537887671543646, 0.0013798836369587238, 0.0013655003651167996, 0.0016403957685210216, 0.0015487085031562072, 0.001524762163848378, 0.0014913719068028668, 0.001633894247140071, 0.0016084833110343348, 0.0015426144733043142, 0.001559789007341908, 0.001400361046945864, 0.0013908985749522384, 0.0014529149445105893, 0.0014044281398487646, 0.0014672400068867113, 0.0014592259779370338, 0.0014901771787698417, 0.00141040630126011, 0.0013903665731754876, 0.0014447362718078517, 0.0013978230088313882, 0.001414980342279571, 0.0014347969698755778, 0.0012952927211227344, 0.0014534244718131168, 0.0014201132021122431, 0.0013534039450275113, 0.001487815736381467, 0.0015721148215667453, 0.0013808307144068932, 0.0012873459387166325, 0.0012691687276341426, 0.001364077130953471, 0.0013664077999893315, 0.0012999109692813813, 0.0012922423418842313, 0.0012759736977344335, 0.001302836744417978, 0.001369545046716582, 0.001357708155184753, 0.0013852192634037992, 0.0012884656285015188, 0.0012597138289535462, 0.001275310798208843, 0.001286320743880184, 0.0013447063650156175, 0.001294341317356326, 0.0013810649763272946, 0.0013203993475772152, 0.0012864097375278325, 0.0013267708375559065, 0.0012832579614464627, 0.001277546721416735, 0.0012601090941664784, 0.0012588541388020727, 0.0012613151550957167, 0.0012786365727814593, 0.0012727028820230517, 0.001265355411108381, 0.0012660698528408772, 0.0012711910224093715, 0.0012899380855897601, 0.0012711633787338818, 0.0012569471926594427, 0.0012698054346147664, 0.0012851844112164983, 0.0012760121942057398, 0.0012853162096048048, 0.0013368560232279838, 0.0013165824429326972, 0.0012837299055513717, 0.0012671116129459106, 0.0012776028137480796, 0.001249666443065733, 0.0012546061400988304, 0.0012753238998959924, 0.001275603459108361, 0.0012640254427642786, 0.001249585325952417, 0.0012597940309882858, 0.001288052038253509, 0.0012653246900976398, 0.0012537035802307056, 0.0012455942314984494, 0.001268360085150068, 0.0012741738699318826, 0.0012423511009749977, 0.0012521854738971985]
[725.3551441511412, 715.2375307836031, 704.3551297289886, 716.966872543512, 731.7501311479156, 714.366513979877, 722.1079143211676, 742.6626613330485, 720.0334828860024, 720.1451397536084, 747.1500086160481, 730.8348540579727, 715.7993246781059, 699.0352920108478, 711.363275759673, 717.4060248396834, 681.3044947407308, 677.364297145411, 725.8001602537771, 693.3862390572045, 711.3861611867906, 723.6120438688592, 611.8294671382655, 722.5929236515419, 689.8646878784616, 706.1388232353617, 691.3224144815227, 710.4683553955257, 708.2026030205938, 686.8494301153663, 695.0920519362564, 661.561690897715, 689.6819705512949, 704.8377393731431, 756.3351779012498, 736.4665910134903, 732.3218155335747, 724.6060174120547, 738.4046299868146, 748.9676051832373, 751.7950558086379, 738.6676742059094, 724.6987885181458, 732.3322831293889, 609.6089853374826, 645.699302329676, 655.8399884976683, 670.5235598434687, 612.0347150682339, 621.7036839238017, 648.2501086988883, 641.1123525637199, 714.1015541534545, 718.9596840548484, 688.2715356313219, 712.0335826564146, 681.5517538414641, 685.2948173344199, 671.0611424243602, 709.0155504173247, 719.2347826056252, 692.16785064077, 715.3981539022044, 706.7235989928831, 696.9627208557024, 772.0262637878639, 688.030248143903, 704.1692158854825, 738.8777043794361, 672.1262422133744, 636.0858547236489, 724.2017356410912, 776.7919794712753, 787.917302267682, 733.0963750568958, 731.8459394097484, 769.2834537374674, 773.8486563920279, 783.7152143304826, 767.5558770387111, 730.1694839446513, 736.5353122327845, 721.9073733805667, 776.1169393109824, 793.8310884708693, 784.1225851804021, 777.4110809901906, 743.6567759448255, 772.5937406081466, 724.0788935646812, 757.3466329219929, 777.357299799174, 753.7096623574699, 779.2665465896039, 782.7502378081722, 793.5820831937316, 794.3732074882014, 792.8232654305287, 782.0830572870818, 785.7293435294409, 790.2917956655795, 789.8458349325235, 786.6638312978599, 775.2310061787189, 786.6809386815652, 795.578371024645, 787.5222240668547, 778.098451298086, 783.6915701440104, 778.0186638332907, 748.0237083312769, 759.5422568240334, 778.9800608956699, 789.1964605036629, 782.7158716614897, 800.2135334183728, 797.0628933166435, 784.1145297140231, 783.942684428744, 791.1233161676831, 800.2654794604069, 793.7805509488865, 776.3661484949914, 790.3109832803739, 797.6367107574032, 802.8296653212664, 788.4196386404602, 784.8222472600697, 804.9254346981296, 798.6037379012894]
Elapsed: 0.17609139189589768~0.011883418127098231
Time per graph: 0.0013650495495806023~9.211952036510257e-05
Speed: 735.765578374199~47.450204589753525
Total Time: 0.1622
best val loss: 0.22507188951437787 test_score: 0.8837

Testing...
Test loss: 0.3704 score: 0.9070 time: 0.16s
test Score 0.9070
Epoch Time List: [0.5996897041331977, 0.5954353336710483, 0.5980404962319881, 0.5997032499872148, 0.59503329009749, 0.5995223459322006, 0.5961276197340339, 0.5873305902350694, 0.5949691568966955, 0.5919328178279102, 0.5850181363057345, 0.5959222230594605, 0.5984742299187928, 0.6116181560792029, 0.6075543067418039, 0.6063216968905181, 0.6273905641864985, 0.6307576578110456, 0.6007346799597144, 0.6229497308377177, 0.605084290727973, 0.62949986057356, 0.6424768101423979, 0.6352134176995605, 0.6352104181423783, 0.6126998451072723, 0.6321634168270975, 0.643944603158161, 0.6250184739474207, 0.6200141997542232, 0.6284278356470168, 0.6436166679486632, 0.6171019312459975, 0.5983942591119558, 0.5821269350126386, 0.5912530981004238, 0.5827502058818936, 0.5862940798979253, 0.5945157681126148, 0.5868980940431356, 0.5869281708728522, 0.5895099961198866, 0.5888552758842707, 0.5925982082262635, 0.627073701005429, 0.6794025192502886, 0.6519582769833505, 0.6533312667161226, 0.7090703598223627, 0.7271223380230367, 0.6860457370057702, 0.6695488977711648, 0.6205281121656299, 0.6143112319987267, 0.6291306428611279, 0.6063253441825509, 0.6374963717535138, 0.6360186981037259, 0.6410940480418503, 0.6260202068369836, 0.624841890996322, 0.6292872438207269, 0.6286251780111343, 0.6227864630054682, 0.6229611788876355, 0.6342402952723205, 0.7843682221136987, 0.6518122800625861, 0.7144578660372645, 0.6607075338251889, 0.6944175220560282, 0.6426137562375516, 0.6155738108791411, 0.5974395470693707, 0.6201960609760135, 0.6274824042338878, 0.596312900306657, 0.5967795150354505, 0.6015739280264825, 0.6021320261061192, 0.6365226521156728, 0.6287920738104731, 0.6354603611398488, 0.6030996178742498, 0.5953323447611183, 0.5951460469514132, 0.5902245652396232, 0.5998921820428222, 0.5997316278517246, 0.633446735329926, 0.6435682468581945, 0.5990368470083922, 0.6054925573989749, 0.6035095697734505, 0.5913663303945214, 0.5908223309088498, 0.5912732880096883, 0.6003552440088242, 0.59416344598867, 0.5917610342148691, 0.5943864460568875, 0.5898227132856846, 0.589955790201202, 0.5902895552571863, 0.5909111499786377, 0.5921963769942522, 0.5955982289742678, 0.5917229927144945, 0.5913517619483173, 0.5957089180592448, 0.60684132296592, 0.6230390979908407, 0.6090041960123926, 0.5880472229328007, 0.5909401027020067, 0.5983757600188255, 0.5942786908708513, 0.5914579930249602, 0.5899614377412945, 0.589093271875754, 0.5919756339862943, 0.590890770079568, 0.591750101884827, 0.5892289471812546, 0.5903620889876038, 0.5905734298285097, 0.5904562268406153, 0.58772785612382, 0.5852924291975796, 0.5865055387839675]
Total Epoch List: [65, 65]
Total Time List: [0.18556979508139193, 0.16220792708918452]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae2311030>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.6374;  Loss pred: 0.6374; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6843 score: 0.5000 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6791 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.5000 time: 0.26s
Epoch 22/1000, LR 0.000290
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6741 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6754 score: 0.5000 time: 0.18s
Epoch 23/1000, LR 0.000290
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6664 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6679 score: 0.5000 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.5359;  Loss pred: 0.5359; Loss self: 0.0000; time: 0.36s
Val loss: 0.6553 score: 0.5504 time: 0.18s
Test loss: 0.6571 score: 0.5469 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 0.5176;  Loss pred: 0.5176; Loss self: 0.0000; time: 0.27s
Val loss: 0.6425 score: 0.7597 time: 0.18s
Test loss: 0.6445 score: 0.7422 time: 0.18s
Epoch 26/1000, LR 0.000290
Train loss: 0.4991;  Loss pred: 0.4991; Loss self: 0.0000; time: 0.27s
Val loss: 0.6277 score: 0.8372 time: 0.27s
Test loss: 0.6299 score: 0.7891 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.28s
Val loss: 0.6091 score: 0.8682 time: 0.18s
Test loss: 0.6115 score: 0.8594 time: 0.18s
Epoch 28/1000, LR 0.000290
Train loss: 0.4639;  Loss pred: 0.4639; Loss self: 0.0000; time: 0.28s
Val loss: 0.5880 score: 0.8837 time: 0.18s
Test loss: 0.5900 score: 0.8984 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 0.4369;  Loss pred: 0.4369; Loss self: 0.0000; time: 0.36s
Val loss: 0.5670 score: 0.8760 time: 0.18s
Test loss: 0.5684 score: 0.9297 time: 0.18s
Epoch 30/1000, LR 0.000290
Train loss: 0.4197;  Loss pred: 0.4197; Loss self: 0.0000; time: 0.26s
Val loss: 0.5496 score: 0.8915 time: 0.17s
Test loss: 0.5503 score: 0.8984 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.3982;  Loss pred: 0.3982; Loss self: 0.0000; time: 0.26s
Val loss: 0.5267 score: 0.8915 time: 0.17s
Test loss: 0.5270 score: 0.8906 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.3758;  Loss pred: 0.3758; Loss self: 0.0000; time: 0.26s
Val loss: 0.5074 score: 0.8837 time: 0.17s
Test loss: 0.5064 score: 0.8672 time: 0.17s
Epoch 33/1000, LR 0.000290
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.26s
Val loss: 0.4790 score: 0.8837 time: 0.17s
Test loss: 0.4774 score: 0.8828 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.3352;  Loss pred: 0.3352; Loss self: 0.0000; time: 0.27s
Val loss: 0.4518 score: 0.8837 time: 0.17s
Test loss: 0.4479 score: 0.8828 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 0.3098;  Loss pred: 0.3098; Loss self: 0.0000; time: 0.27s
Val loss: 0.4336 score: 0.8915 time: 0.17s
Test loss: 0.4253 score: 0.8672 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 0.2836;  Loss pred: 0.2836; Loss self: 0.0000; time: 0.26s
Val loss: 0.4078 score: 0.8915 time: 0.17s
Test loss: 0.3959 score: 0.8750 time: 0.17s
Epoch 37/1000, LR 0.000290
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.27s
Val loss: 0.3835 score: 0.8915 time: 0.17s
Test loss: 0.3664 score: 0.8750 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 0.2494;  Loss pred: 0.2494; Loss self: 0.0000; time: 0.27s
Val loss: 0.3648 score: 0.8837 time: 0.17s
Test loss: 0.3427 score: 0.8828 time: 0.17s
Epoch 39/1000, LR 0.000289
Train loss: 0.2314;  Loss pred: 0.2314; Loss self: 0.0000; time: 0.27s
Val loss: 0.3646 score: 0.8915 time: 0.17s
Test loss: 0.3383 score: 0.8750 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.26s
Val loss: 0.3504 score: 0.8837 time: 0.17s
Test loss: 0.3190 score: 0.8750 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.26s
Val loss: 0.3403 score: 0.8915 time: 0.17s
Test loss: 0.3005 score: 0.8984 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 0.1846;  Loss pred: 0.1846; Loss self: 0.0000; time: 0.26s
Val loss: 0.3376 score: 0.8915 time: 0.17s
Test loss: 0.2894 score: 0.9062 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 0.26s
Val loss: 0.3441 score: 0.8915 time: 0.17s
Test loss: 0.2873 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 0.26s
Val loss: 0.3757 score: 0.8837 time: 0.17s
Test loss: 0.3122 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1562;  Loss pred: 0.1562; Loss self: 0.0000; time: 0.26s
Val loss: 0.3707 score: 0.8760 time: 0.17s
Test loss: 0.3014 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.26s
Val loss: 0.3942 score: 0.8760 time: 0.17s
Test loss: 0.3175 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.27s
Val loss: 0.3777 score: 0.8915 time: 0.16s
Test loss: 0.2880 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.26s
Val loss: 0.3672 score: 0.8915 time: 0.17s
Test loss: 0.2714 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.1217;  Loss pred: 0.1217; Loss self: 0.0000; time: 0.26s
Val loss: 0.3652 score: 0.8915 time: 0.17s
Test loss: 0.2653 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1110;  Loss pred: 0.1110; Loss self: 0.0000; time: 0.26s
Val loss: 0.4006 score: 0.8760 time: 0.17s
Test loss: 0.2909 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 0.26s
Val loss: 0.4088 score: 0.8760 time: 0.16s
Test loss: 0.2949 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.26s
Val loss: 0.4313 score: 0.8682 time: 0.17s
Test loss: 0.3090 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.26s
Val loss: 0.4413 score: 0.8605 time: 0.17s
Test loss: 0.3109 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.26s
Val loss: 0.4455 score: 0.8605 time: 0.17s
Test loss: 0.3052 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.26s
Val loss: 0.4321 score: 0.8605 time: 0.17s
Test loss: 0.2868 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.26s
Val loss: 0.4168 score: 0.8837 time: 0.17s
Test loss: 0.2666 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.26s
Val loss: 0.4037 score: 0.8760 time: 0.17s
Test loss: 0.2548 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.26s
Val loss: 0.4402 score: 0.8605 time: 0.17s
Test loss: 0.2870 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.26s
Val loss: 0.4328 score: 0.8760 time: 0.17s
Test loss: 0.2743 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.27s
Val loss: 0.4664 score: 0.8605 time: 0.17s
Test loss: 0.3010 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.27s
Val loss: 0.5008 score: 0.8682 time: 0.17s
Test loss: 0.3257 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.26s
Val loss: 0.4735 score: 0.8837 time: 0.17s
Test loss: 0.2957 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.1846,   Val_Loss: 0.3376,   Val_Precision: 0.9474,   Val_Recall: 0.8308,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.3376,   Test_Precision: 0.9815,   Test_Recall: 0.8281,   Test_accuracy: 0.8983,   Test_Score: 0.9062,   Test_loss: 0.2894


[0.17784391692839563, 0.18035966297611594, 0.18314624903723598, 0.17992463102564216, 0.17628968483768404, 0.18057957291603088, 0.17864365899004042, 0.1736993209924549, 0.17915833508595824, 0.17913055699318647, 0.1726560911629349, 0.17651046509854496, 0.18021810799837112, 0.18454003892838955, 0.1813419449608773, 0.1798144921194762, 0.18934265221469104, 0.19044404989108443, 0.17773487395606935, 0.18604349601082504, 0.1813361111562699, 0.1782723229844123, 0.21084306482225657, 0.17852375213988125, 0.18699319195002317, 0.1826836250256747, 0.18659889698028564, 0.18157036695629358, 0.18215126497671008, 0.18781408900395036, 0.185586930019781, 0.194993152981624, 0.18704273202456534, 0.1830208469182253, 0.17055930197238922, 0.17516069510020316, 0.1761520649306476, 0.1780277790967375, 0.1747009630780667, 0.17223708890378475, 0.1715893168002367, 0.17463875096291304, 0.17800498916767538, 0.17614954710006714, 0.21161105413921177, 0.19978339690715075, 0.19669431913644075, 0.19238697597756982, 0.21077235788106918, 0.20749434712342918, 0.19899726705625653, 0.20121278194710612, 0.18064657505601645, 0.17942591616883874, 0.18742602784186602, 0.18117123004049063, 0.18927396088838577, 0.18824015115387738, 0.19223285606130958, 0.1819424128625542, 0.1793572879396379, 0.18637097906321287, 0.1803191681392491, 0.18253246415406466, 0.18508880911394954, 0.16709276102483273, 0.18749175686389208, 0.18319460307247937, 0.17458910890854895, 0.19192822999320924, 0.20280281198211014, 0.17812716215848923, 0.1660676260944456, 0.1637227658648044, 0.17596594989299774, 0.17626660619862378, 0.1676885150372982, 0.16669926210306585, 0.16460060700774193, 0.16806594002991915, 0.17667131102643907, 0.17514435201883316, 0.1786932849790901, 0.16621206607669592, 0.16250308393500745, 0.16451509296894073, 0.16593537596054375, 0.17346712108701468, 0.16697002993896604, 0.178157381946221, 0.17033151583746076, 0.1659468561410904, 0.17115343804471195, 0.16554027702659369, 0.1648035270627588, 0.16255407314747572, 0.1623921839054674, 0.16270965500734746, 0.16494411788880825, 0.16417867178097367, 0.16323084803298116, 0.16332301101647317, 0.16398364189080894, 0.16640201304107904, 0.16398007585667074, 0.1621461878530681, 0.16380490106530488, 0.1657887890469283, 0.16460557305254042, 0.16580579103901982, 0.1724544269964099, 0.16983913513831794, 0.16560115781612694, 0.16345739807002246, 0.16481076297350228, 0.16120697115547955, 0.16184419207274914, 0.16451678308658302, 0.16455284622497857, 0.16305928211659193, 0.16119650704786181, 0.16251342999748886, 0.16615871293470263, 0.16322688502259552, 0.161727761849761, 0.16068165586329997, 0.1636184509843588, 0.16436842922121286, 0.16026329202577472, 0.1615319261327386, 0.17116732290014625, 0.1731670170556754, 0.17028431198559701, 0.1700808450113982, 0.1714905728586018, 0.16981053887866437, 0.16897612088359892, 0.17212543613277376, 0.17247353796847165, 0.17135803494602442, 0.17021029302850366, 0.1728195259347558, 0.17303916905075312, 0.16882295277900994, 0.17056981986388564, 0.17269699089229107, 0.17236623913049698, 0.1718820659443736, 0.1718393221963197, 0.18259302387014031, 0.2666449211537838, 0.18394520902074873, 0.1703761089593172, 0.1831559520214796, 0.18492484209127724, 0.17724377196282148, 0.1827084079850465, 0.17207954078912735, 0.18815712304785848, 0.16972472914494574, 0.17350007221102715, 0.17250334192067385, 0.16843891888856888, 0.16956533794291317, 0.1716738340910524, 0.17023476003669202, 0.16969219618476927, 0.16958155808970332, 0.17266469192691147, 0.1732253609225154, 0.17123358813114464, 0.16963024297729135, 0.17264580889604986, 0.17299121711403131, 0.1709866328164935, 0.16963119083084166, 0.17360502597875893, 0.1729605218861252, 0.17010440514422953, 0.1700078679714352, 0.17427031183615327, 0.17351751099340618, 0.17150884098373353, 0.16845990996807814, 0.16889712098054588, 0.17134679201990366, 0.1735210409387946, 0.17171234404668212, 0.17403683112934232, 0.17526123393326998, 0.1754927469883114, 0.17563640279695392]
[0.0013786350149488033, 0.0013981369222954723, 0.0014197383646297362, 0.0013947645815941252, 0.0013665867041680933, 0.0013998416505118672, 0.001384834565814267, 0.001346506364282596, 0.0013888243029919244, 0.00138860896893943, 0.0013384193113405807, 0.001368298179058488, 0.001397039596886598, 0.0014305429374293765, 0.0014057515113246302, 0.0013939107916238465, 0.0014677724977883028, 0.0014763104642719724, 0.0013777897205896849, 0.0014421976434947678, 0.0014057062880331, 0.0013819559921272272, 0.0016344423629632292, 0.0013839050553479167, 0.0014495596275195595, 0.0014161521319819743, 0.0014465030773665555, 0.0014075222244673922, 0.0014120253098969773, 0.0014559231705732587, 0.0014386583722463644, 0.0015115748293149148, 0.0014499436591051576, 0.001418766255180041, 0.0013221651315689087, 0.001357834845738009, 0.0013655198831833148, 0.0013800603030754844, 0.0013542710316129202, 0.0013351712318122849, 0.001330149742637494, 0.0013537887671543646, 0.0013798836369587238, 0.0013655003651167996, 0.0016403957685210216, 0.0015487085031562072, 0.001524762163848378, 0.0014913719068028668, 0.001633894247140071, 0.0016084833110343348, 0.0015426144733043142, 0.001559789007341908, 0.001400361046945864, 0.0013908985749522384, 0.0014529149445105893, 0.0014044281398487646, 0.0014672400068867113, 0.0014592259779370338, 0.0014901771787698417, 0.00141040630126011, 0.0013903665731754876, 0.0014447362718078517, 0.0013978230088313882, 0.001414980342279571, 0.0014347969698755778, 0.0012952927211227344, 0.0014534244718131168, 0.0014201132021122431, 0.0013534039450275113, 0.001487815736381467, 0.0015721148215667453, 0.0013808307144068932, 0.0012873459387166325, 0.0012691687276341426, 0.001364077130953471, 0.0013664077999893315, 0.0012999109692813813, 0.0012922423418842313, 0.0012759736977344335, 0.001302836744417978, 0.001369545046716582, 0.001357708155184753, 0.0013852192634037992, 0.0012884656285015188, 0.0012597138289535462, 0.001275310798208843, 0.001286320743880184, 0.0013447063650156175, 0.001294341317356326, 0.0013810649763272946, 0.0013203993475772152, 0.0012864097375278325, 0.0013267708375559065, 0.0012832579614464627, 0.001277546721416735, 0.0012601090941664784, 0.0012588541388020727, 0.0012613151550957167, 0.0012786365727814593, 0.0012727028820230517, 0.001265355411108381, 0.0012660698528408772, 0.0012711910224093715, 0.0012899380855897601, 0.0012711633787338818, 0.0012569471926594427, 0.0012698054346147664, 0.0012851844112164983, 0.0012760121942057398, 0.0012853162096048048, 0.0013368560232279838, 0.0013165824429326972, 0.0012837299055513717, 0.0012671116129459106, 0.0012776028137480796, 0.001249666443065733, 0.0012546061400988304, 0.0012753238998959924, 0.001275603459108361, 0.0012640254427642786, 0.001249585325952417, 0.0012597940309882858, 0.001288052038253509, 0.0012653246900976398, 0.0012537035802307056, 0.0012455942314984494, 0.001268360085150068, 0.0012741738699318826, 0.0012423511009749977, 0.0012521854738971985, 0.0013372447101573925, 0.001352867320747464, 0.0013303461873874767, 0.0013287566016515484, 0.0013397701004578266, 0.0013266448349895654, 0.0013201259444031166, 0.001344729969787295, 0.0013474495153786847, 0.0013387346480158158, 0.0013297679142851848, 0.0013501525463652797, 0.0013518685082090087, 0.0013189293185860151, 0.0013325767176866066, 0.001349195241346024, 0.0013466112432070076, 0.0013428286401904188, 0.0013424947046587477, 0.0014265079989854712, 0.002083163446513936, 0.0014370719454745995, 0.0013310633512446657, 0.0014309058751678094, 0.0014447253288381035, 0.0013847169684595428, 0.0014274094373831758, 0.0013443714124150574, 0.0014699775238113943, 0.0013259744464448886, 0.0013554693141486496, 0.0013476823587552644, 0.0013159290538169444, 0.0013247292026790092, 0.001341201828836347, 0.0013299590627866564, 0.00132572028269351, 0.0013248559225758072, 0.0013489429056789959, 0.0013533231322071515, 0.0013377624072745675, 0.0013252362732600886, 0.0013487953820003895, 0.0013514938837033696, 0.0013358330688788556, 0.0013252436783659505, 0.0013562892654590541, 0.0013512540772353532, 0.0013289406651892932, 0.0013281864685268374, 0.0013614868112199474, 0.0013556055546359858, 0.0013399128201854182, 0.0013160930466256104, 0.0013195087576605147, 0.0013386468126554973, 0.001355633132334333, 0.001341502687864704, 0.0013596627431979869, 0.0013692283901036717, 0.0013710370858461829, 0.0013721593968512025]
[725.3551441511412, 715.2375307836031, 704.3551297289886, 716.966872543512, 731.7501311479156, 714.366513979877, 722.1079143211676, 742.6626613330485, 720.0334828860024, 720.1451397536084, 747.1500086160481, 730.8348540579727, 715.7993246781059, 699.0352920108478, 711.363275759673, 717.4060248396834, 681.3044947407308, 677.364297145411, 725.8001602537771, 693.3862390572045, 711.3861611867906, 723.6120438688592, 611.8294671382655, 722.5929236515419, 689.8646878784616, 706.1388232353617, 691.3224144815227, 710.4683553955257, 708.2026030205938, 686.8494301153663, 695.0920519362564, 661.561690897715, 689.6819705512949, 704.8377393731431, 756.3351779012498, 736.4665910134903, 732.3218155335747, 724.6060174120547, 738.4046299868146, 748.9676051832373, 751.7950558086379, 738.6676742059094, 724.6987885181458, 732.3322831293889, 609.6089853374826, 645.699302329676, 655.8399884976683, 670.5235598434687, 612.0347150682339, 621.7036839238017, 648.2501086988883, 641.1123525637199, 714.1015541534545, 718.9596840548484, 688.2715356313219, 712.0335826564146, 681.5517538414641, 685.2948173344199, 671.0611424243602, 709.0155504173247, 719.2347826056252, 692.16785064077, 715.3981539022044, 706.7235989928831, 696.9627208557024, 772.0262637878639, 688.030248143903, 704.1692158854825, 738.8777043794361, 672.1262422133744, 636.0858547236489, 724.2017356410912, 776.7919794712753, 787.917302267682, 733.0963750568958, 731.8459394097484, 769.2834537374674, 773.8486563920279, 783.7152143304826, 767.5558770387111, 730.1694839446513, 736.5353122327845, 721.9073733805667, 776.1169393109824, 793.8310884708693, 784.1225851804021, 777.4110809901906, 743.6567759448255, 772.5937406081466, 724.0788935646812, 757.3466329219929, 777.357299799174, 753.7096623574699, 779.2665465896039, 782.7502378081722, 793.5820831937316, 794.3732074882014, 792.8232654305287, 782.0830572870818, 785.7293435294409, 790.2917956655795, 789.8458349325235, 786.6638312978599, 775.2310061787189, 786.6809386815652, 795.578371024645, 787.5222240668547, 778.098451298086, 783.6915701440104, 778.0186638332907, 748.0237083312769, 759.5422568240334, 778.9800608956699, 789.1964605036629, 782.7158716614897, 800.2135334183728, 797.0628933166435, 784.1145297140231, 783.942684428744, 791.1233161676831, 800.2654794604069, 793.7805509488865, 776.3661484949914, 790.3109832803739, 797.6367107574032, 802.8296653212664, 788.4196386404602, 784.8222472600697, 804.9254346981296, 798.6037379012894, 747.8062858684264, 739.1707853860321, 751.6840424549885, 752.5832788014542, 746.3967136289127, 753.7812484739861, 757.5034823303479, 743.6437221356617, 742.1428325045349, 746.974018699026, 752.0109255588023, 740.6570484883957, 739.7169132409383, 758.190742982399, 750.4258379480247, 741.1825726589062, 742.6048200952642, 744.6966575409024, 744.8818952728701, 701.012543014969, 480.03914511530394, 695.8593848756441, 751.2790424775116, 698.8579873450622, 692.1730934171643, 722.1692394745987, 700.5698391859228, 743.84205939308, 680.2825103115694, 754.1623465528548, 737.7518543295724, 742.014610121937, 759.9193870668255, 754.8712581995572, 745.5999376824736, 751.9028427120945, 754.306932657217, 754.7990562292865, 741.3212195935349, 738.9218259862946, 747.5168943021107, 754.5824244155303, 741.4023011532755, 739.9219575154832, 748.5965299835591, 754.5782080115396, 737.3058428369532, 740.0532711405289, 752.4790430410682, 752.9063303205862, 734.4911399501251, 737.6777091095095, 746.3172117881663, 759.8246967142213, 757.857796846303, 747.0230314269993, 737.662702502741, 745.4327218618693, 735.4765032745958, 730.3383476618422, 729.3748727320643, 728.7783054175596]
Elapsed: 0.17555808517984892~0.012118229661009155
Time per graph: 0.001364326819380107~9.40088953319175e-05
Speed: 735.9730883878377~44.35074507791931
Total Time: 0.1764
best val loss: 0.3376303899080254 test_score: 0.9062

Testing...
Test loss: 0.5503 score: 0.8984 time: 0.17s
test Score 0.8984
Epoch Time List: [0.5996897041331977, 0.5954353336710483, 0.5980404962319881, 0.5997032499872148, 0.59503329009749, 0.5995223459322006, 0.5961276197340339, 0.5873305902350694, 0.5949691568966955, 0.5919328178279102, 0.5850181363057345, 0.5959222230594605, 0.5984742299187928, 0.6116181560792029, 0.6075543067418039, 0.6063216968905181, 0.6273905641864985, 0.6307576578110456, 0.6007346799597144, 0.6229497308377177, 0.605084290727973, 0.62949986057356, 0.6424768101423979, 0.6352134176995605, 0.6352104181423783, 0.6126998451072723, 0.6321634168270975, 0.643944603158161, 0.6250184739474207, 0.6200141997542232, 0.6284278356470168, 0.6436166679486632, 0.6171019312459975, 0.5983942591119558, 0.5821269350126386, 0.5912530981004238, 0.5827502058818936, 0.5862940798979253, 0.5945157681126148, 0.5868980940431356, 0.5869281708728522, 0.5895099961198866, 0.5888552758842707, 0.5925982082262635, 0.627073701005429, 0.6794025192502886, 0.6519582769833505, 0.6533312667161226, 0.7090703598223627, 0.7271223380230367, 0.6860457370057702, 0.6695488977711648, 0.6205281121656299, 0.6143112319987267, 0.6291306428611279, 0.6063253441825509, 0.6374963717535138, 0.6360186981037259, 0.6410940480418503, 0.6260202068369836, 0.624841890996322, 0.6292872438207269, 0.6286251780111343, 0.6227864630054682, 0.6229611788876355, 0.6342402952723205, 0.7843682221136987, 0.6518122800625861, 0.7144578660372645, 0.6607075338251889, 0.6944175220560282, 0.6426137562375516, 0.6155738108791411, 0.5974395470693707, 0.6201960609760135, 0.6274824042338878, 0.596312900306657, 0.5967795150354505, 0.6015739280264825, 0.6021320261061192, 0.6365226521156728, 0.6287920738104731, 0.6354603611398488, 0.6030996178742498, 0.5953323447611183, 0.5951460469514132, 0.5902245652396232, 0.5998921820428222, 0.5997316278517246, 0.633446735329926, 0.6435682468581945, 0.5990368470083922, 0.6054925573989749, 0.6035095697734505, 0.5913663303945214, 0.5908223309088498, 0.5912732880096883, 0.6003552440088242, 0.59416344598867, 0.5917610342148691, 0.5943864460568875, 0.5898227132856846, 0.589955790201202, 0.5902895552571863, 0.5909111499786377, 0.5921963769942522, 0.5955982289742678, 0.5917229927144945, 0.5913517619483173, 0.5957089180592448, 0.60684132296592, 0.6230390979908407, 0.6090041960123926, 0.5880472229328007, 0.5909401027020067, 0.5983757600188255, 0.5942786908708513, 0.5914579930249602, 0.5899614377412945, 0.589093271875754, 0.5919756339862943, 0.590890770079568, 0.591750101884827, 0.5892289471812546, 0.5903620889876038, 0.5905734298285097, 0.5904562268406153, 0.58772785612382, 0.5852924291975796, 0.5865055387839675, 0.6025454041082412, 0.6018155177589506, 0.597700617974624, 0.6005359559785575, 0.600289419060573, 0.5935035969596356, 0.59516039211303, 0.5999452618416399, 0.5944929306861013, 0.5988102781120688, 0.5982990697957575, 0.6069707872811705, 0.6055879699997604, 0.6040572638157755, 0.5984333059750497, 0.6045469967648387, 0.5987429667729884, 0.6026771229226142, 0.6007486151065677, 0.6292541730217636, 0.7081882189959288, 0.6298202939797193, 0.619593993993476, 0.7129814359359443, 0.6302579019684345, 0.7171757339965552, 0.636512644123286, 0.6224277089349926, 0.7263384161051363, 0.5934782009571791, 0.5991107563022524, 0.5988671188242733, 0.5941403270699084, 0.5989482528530061, 0.6012421150226146, 0.5985201399307698, 0.5998722310177982, 0.6007745401002467, 0.6012037158943713, 0.5985694399569184, 0.5951923131942749, 0.5991187039762735, 0.5966184709686786, 0.5926948848646134, 0.5940944510512054, 0.5908622469287366, 0.5988646158948541, 0.5958571871742606, 0.5925724681001157, 0.5917987069115043, 0.5947122720535845, 0.5977710820734501, 0.5944535059388727, 0.5922063288744539, 0.591556467814371, 0.5983622502535582, 0.5988568542525172, 0.5976939627435058, 0.6013224739581347, 0.6138188950717449, 0.6079185756389052, 0.6043962191324681]
Total Epoch List: [65, 65, 62]
Total Time List: [0.18556979508139193, 0.16220792708918452, 0.1763546469155699]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae2310f40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.19s
Epoch 5/1000, LR 0.000105
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.21s
Epoch 15/1000, LR 0.000285
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 0.19s
Epoch 16/1000, LR 0.000285
Train loss: 0.6230;  Loss pred: 0.6230; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5039 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5039 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5039 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5039 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5039 time: 0.19s
Epoch 21/1000, LR 0.000285
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.5039 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.5129;  Loss pred: 0.5129; Loss self: 0.0000; time: 0.26s
Val loss: 0.6803 score: 0.5039 time: 0.19s
Test loss: 0.6809 score: 0.5349 time: 0.19s
Epoch 23/1000, LR 0.000285
Train loss: 0.4836;  Loss pred: 0.4836; Loss self: 0.0000; time: 0.26s
Val loss: 0.6768 score: 0.5039 time: 0.19s
Test loss: 0.6775 score: 0.5349 time: 0.19s
Epoch 24/1000, LR 0.000285
Train loss: 0.4563;  Loss pred: 0.4563; Loss self: 0.0000; time: 0.26s
Val loss: 0.6725 score: 0.5349 time: 0.20s
Test loss: 0.6734 score: 0.5349 time: 0.19s
Epoch 25/1000, LR 0.000285
Train loss: 0.4356;  Loss pred: 0.4356; Loss self: 0.0000; time: 0.26s
Val loss: 0.6667 score: 0.5426 time: 0.18s
Test loss: 0.6679 score: 0.5426 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.26s
Val loss: 0.6599 score: 0.5581 time: 0.18s
Test loss: 0.6614 score: 0.5581 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.3649;  Loss pred: 0.3649; Loss self: 0.0000; time: 0.25s
Val loss: 0.6515 score: 0.5659 time: 0.18s
Test loss: 0.6535 score: 0.5581 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.26s
Val loss: 0.6408 score: 0.5969 time: 0.18s
Test loss: 0.6433 score: 0.5659 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.26s
Val loss: 0.6276 score: 0.6047 time: 0.18s
Test loss: 0.6307 score: 0.5969 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.26s
Val loss: 0.6115 score: 0.6202 time: 0.18s
Test loss: 0.6155 score: 0.6047 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.2407;  Loss pred: 0.2407; Loss self: 0.0000; time: 0.26s
Val loss: 0.5922 score: 0.6434 time: 0.18s
Test loss: 0.5973 score: 0.6434 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.2152;  Loss pred: 0.2152; Loss self: 0.0000; time: 0.26s
Val loss: 0.5688 score: 0.7132 time: 0.17s
Test loss: 0.5752 score: 0.7132 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1927;  Loss pred: 0.1927; Loss self: 0.0000; time: 0.26s
Val loss: 0.5421 score: 0.7674 time: 0.18s
Test loss: 0.5499 score: 0.7752 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1692;  Loss pred: 0.1692; Loss self: 0.0000; time: 0.25s
Val loss: 0.5130 score: 0.8140 time: 0.18s
Test loss: 0.5224 score: 0.7984 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1511;  Loss pred: 0.1511; Loss self: 0.0000; time: 0.26s
Val loss: 0.4813 score: 0.8760 time: 0.18s
Test loss: 0.4925 score: 0.8372 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1327;  Loss pred: 0.1327; Loss self: 0.0000; time: 0.26s
Val loss: 0.4458 score: 0.8915 time: 0.17s
Test loss: 0.4592 score: 0.8992 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.1106;  Loss pred: 0.1106; Loss self: 0.0000; time: 0.26s
Val loss: 0.4093 score: 0.9225 time: 0.18s
Test loss: 0.4254 score: 0.9147 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.26s
Val loss: 0.3747 score: 0.9302 time: 0.18s
Test loss: 0.3926 score: 0.9147 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 0.0960;  Loss pred: 0.0960; Loss self: 0.0000; time: 0.26s
Val loss: 0.3439 score: 0.9302 time: 0.18s
Test loss: 0.3620 score: 0.9225 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0763;  Loss pred: 0.0763; Loss self: 0.0000; time: 0.26s
Val loss: 0.3132 score: 0.9380 time: 0.18s
Test loss: 0.3308 score: 0.9302 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.26s
Val loss: 0.2809 score: 0.9535 time: 0.18s
Test loss: 0.2988 score: 0.9380 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.25s
Val loss: 0.2510 score: 0.9380 time: 0.18s
Test loss: 0.2695 score: 0.9380 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 0.25s
Val loss: 0.2298 score: 0.9302 time: 0.18s
Test loss: 0.2490 score: 0.9380 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.26s
Val loss: 0.2062 score: 0.9302 time: 0.18s
Test loss: 0.2281 score: 0.9457 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.26s
Val loss: 0.1828 score: 0.9302 time: 0.18s
Test loss: 0.2096 score: 0.9457 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0353;  Loss pred: 0.0353; Loss self: 0.0000; time: 0.26s
Val loss: 0.1656 score: 0.9380 time: 0.18s
Test loss: 0.1982 score: 0.9457 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.26s
Val loss: 0.1532 score: 0.9380 time: 0.18s
Test loss: 0.1888 score: 0.9380 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.25s
Val loss: 0.1449 score: 0.9380 time: 0.29s
Test loss: 0.1826 score: 0.9380 time: 0.19s
Epoch 49/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.26s
Val loss: 0.1397 score: 0.9380 time: 0.19s
Test loss: 0.1772 score: 0.9380 time: 0.19s
Epoch 50/1000, LR 0.000284
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.27s
Val loss: 0.1385 score: 0.9302 time: 0.19s
Test loss: 0.1737 score: 0.9380 time: 0.28s
Epoch 51/1000, LR 0.000284
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.28s
Val loss: 0.1378 score: 0.9302 time: 0.20s
Test loss: 0.1719 score: 0.9380 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.28s
Val loss: 0.1405 score: 0.9302 time: 0.18s
Test loss: 0.1732 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.26s
Val loss: 0.1419 score: 0.9302 time: 0.22s
Test loss: 0.1752 score: 0.9380 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.26s
Val loss: 0.1423 score: 0.9380 time: 0.20s
Test loss: 0.1795 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.29s
Val loss: 0.1430 score: 0.9380 time: 0.18s
Test loss: 0.1861 score: 0.9380 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.27s
Val loss: 0.1457 score: 0.9380 time: 0.18s
Test loss: 0.1940 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.26s
Val loss: 0.1460 score: 0.9380 time: 0.18s
Test loss: 0.2028 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.26s
Val loss: 0.1468 score: 0.9380 time: 0.18s
Test loss: 0.2079 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.26s
Val loss: 0.1474 score: 0.9380 time: 0.18s
Test loss: 0.2117 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.26s
Val loss: 0.1481 score: 0.9380 time: 0.18s
Test loss: 0.2174 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.26s
Val loss: 0.1482 score: 0.9380 time: 0.18s
Test loss: 0.2197 score: 0.9457 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.26s
Val loss: 0.1477 score: 0.9380 time: 0.18s
Test loss: 0.2228 score: 0.9457 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.27s
Val loss: 0.1472 score: 0.9302 time: 0.18s
Test loss: 0.2276 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.27s
Val loss: 0.1471 score: 0.9457 time: 0.18s
Test loss: 0.2337 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.26s
Val loss: 0.1482 score: 0.9457 time: 0.18s
Test loss: 0.2405 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.27s
Val loss: 0.1502 score: 0.9457 time: 0.18s
Test loss: 0.2470 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.26s
Val loss: 0.1522 score: 0.9457 time: 0.18s
Test loss: 0.2532 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.26s
Val loss: 0.1536 score: 0.9457 time: 0.18s
Test loss: 0.2549 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.26s
Val loss: 0.1558 score: 0.9457 time: 0.19s
Test loss: 0.2618 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.26s
Val loss: 0.1592 score: 0.9535 time: 0.18s
Test loss: 0.2705 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.27s
Val loss: 0.1619 score: 0.9457 time: 0.18s
Test loss: 0.2769 score: 0.9380 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0250,   Val_Loss: 0.1378,   Val_Precision: 0.9231,   Val_Recall: 0.9375,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1378,   Test_Precision: 0.9672,   Test_Recall: 0.9077,   Test_accuracy: 0.9365,   Test_Score: 0.9380,   Test_loss: 0.1719


[0.1814970918931067, 0.17870643106289208, 0.1778492860030383, 0.19110383186489344, 0.1916478129569441, 0.18656853609718382, 0.18104628007858992, 0.1840810279827565, 0.1891629728488624, 0.18253488698974252, 0.17896579601801932, 0.18576225102879107, 0.1976375039666891, 0.21441768086515367, 0.19361927220597863, 0.18931001308374107, 0.19766568788327277, 0.18668085895478725, 0.18610644899308681, 0.1902207180391997, 0.18674948601983488, 0.1903451110702008, 0.1902148500084877, 0.19214443303644657, 0.18853458994999528, 0.18486099014990032, 0.1828388690482825, 0.18513948912732303, 0.18650964903645217, 0.1856012309435755, 0.1818454118911177, 0.1856296609621495, 0.18653418705798686, 0.1821519338991493, 0.18120165704749525, 0.18383599794469774, 0.18744214018806815, 0.18452483299188316, 0.1811096789315343, 0.18569244001992047, 0.18879041518084705, 0.18357037799432874, 0.18352947593666613, 0.1858683859463781, 0.182005604961887, 0.18477549799717963, 0.18476615380495787, 0.1989070950075984, 0.19956260197795928, 0.2877028309740126, 0.18902584118768573, 0.1868742119986564, 0.19509237702004611, 0.18273692089132965, 0.2594421610701829, 0.1843875190243125, 0.18780427891761065, 0.18708447692915797, 0.18277085199952126, 0.18502971692942083, 0.20833548810333014, 0.19649400608614087, 0.18760059890337288, 0.18935800017789006, 0.18722235201857984, 0.18330262112431228, 0.18626634497195482, 0.18943613185547292, 0.1861100171227008, 0.1833981571253389, 0.18772807507775724]
[0.0014069542007217573, 0.001385321171030171, 0.0013786766356824673, 0.0014814250532162283, 0.0014856419609065436, 0.0014462677216835955, 0.001403459535492945, 0.001426984713044624, 0.0014663796344873054, 0.0014149991239514924, 0.0013873317520776691, 0.0014400174498355896, 0.0015320736741603806, 0.0016621525648461526, 0.0015009245907440204, 0.0014675194812693107, 0.0015322921541338975, 0.0014471384415099787, 0.0014426856511092002, 0.0014745792096061993, 0.001447670434262286, 0.0014755434966682232, 0.0014745337209960286, 0.001489491728964702, 0.0014615084492247696, 0.0014330309313945761, 0.0014173555740176938, 0.0014351898381963026, 0.0014458112328407146, 0.0014387692321207403, 0.0014096543557450983, 0.001438989619861624, 0.0014460014500619137, 0.0014120304953422426, 0.0014046640081201182, 0.0014250852553852537, 0.0014530398464191329, 0.0014304250619525825, 0.0014039509994692582, 0.0014394762792241896, 0.001463491590549202, 0.0014230261860025484, 0.0014227091157881094, 0.001440840201134714, 0.0014108961624952482, 0.0014323682015285242, 0.001432295765929906, 0.0015419154651751814, 0.001546996914557824, 0.0022302545036745162, 0.0014653165983541528, 0.0014486373023151658, 0.0015123440079073342, 0.0014165652782273616, 0.0020111795431797127, 0.0014293606125915697, 0.0014558471233923306, 0.0014502672630167285, 0.0014168283100738082, 0.001434338890925743, 0.0016150037837467453, 0.001523209349504968, 0.0014542682085532783, 0.00146789147424721, 0.0014513360621595337, 0.001420950551351258, 0.0014439251548213552, 0.0014684971461664568, 0.0014427133110286885, 0.0014216911405065032, 0.0014552563959516066]
[710.7551898185508, 721.854268101863, 725.3332464758741, 675.0257111076685, 673.1096901637032, 691.4349155465512, 712.5249960617955, 700.778355127851, 681.9516423178046, 706.7142184564922, 720.8081257438239, 694.4360293093479, 652.7101254109258, 601.6294900658287, 666.2559905853044, 681.4219591381941, 652.617059548434, 691.0188903257771, 693.1516919372949, 678.1595681571149, 690.7649533573483, 677.7163819690844, 678.1804890325009, 671.3699583246884, 684.2245766902214, 697.8216436869473, 705.5392579896936, 696.7719345454436, 691.653223661301, 695.0384937868068, 709.3937573593577, 694.9320455113234, 691.562238721948, 708.2000022652653, 711.9140194517507, 701.7124036762717, 688.212372471682, 699.0928966491704, 712.2755711403279, 694.697102295394, 683.2974008581298, 702.7277571111465, 702.8843696176433, 694.0394911333426, 708.7693811792957, 698.1445126559422, 698.1798199694869, 648.5439847938662, 646.4137003698089, 448.37932099337667, 682.4463744717028, 690.303914169428, 661.2252204336256, 705.9328753641087, 497.22065013598, 699.6135133365, 686.8853081701724, 689.5280790658413, 705.8018200863773, 697.1853069915616, 619.1935957450448, 656.508575347829, 687.6310670332338, 681.2492732222167, 689.0202938332818, 703.7542573519158, 692.5566721106965, 680.9682964727043, 693.1384027274113, 703.3876567900197, 687.1641332633279]
Elapsed: 0.1898939382599693~0.015686858477687148
Time per graph: 0.0014720460330230178~0.00012160355409059809
Speed: 682.7445001519965~41.80221699055934
Total Time: 0.1883
best val loss: 0.13776985131377398 test_score: 0.9380

Testing...
Test loss: 0.2988 score: 0.9380 time: 0.34s
test Score 0.9380
Epoch Time List: [0.5988706911448389, 0.5934263800736517, 0.6002791267819703, 0.626824997831136, 0.6354825361631811, 0.622906408039853, 0.6089932322502136, 0.6141225469764322, 0.6138905079569668, 0.6123495791107416, 0.6094213102478534, 0.6007844363339245, 0.6583832169417292, 0.6595323979854584, 0.6479438543319702, 0.6365097146481276, 0.6711158468388021, 0.6527666919864714, 0.6161734731867909, 0.622545403894037, 0.6197332690935582, 0.6327971979044378, 0.6325262980535626, 0.646787955192849, 0.6174309938214719, 0.6171446512453258, 0.610823969822377, 0.6141448239795864, 0.616976265097037, 0.616399047197774, 0.6129905597772449, 0.6156390672549605, 0.6141770388931036, 0.6051008210051805, 0.6122001921758056, 0.6100234480109066, 0.6173967241775244, 0.6193468158598989, 0.6107344648335129, 0.6186481958720833, 0.6157727437093854, 0.6129474528133869, 0.612153829773888, 0.6151915297377855, 0.6124452948570251, 0.6129253781400621, 0.613207925343886, 0.7391917807981372, 0.6466600648127496, 0.739249428967014, 0.6668684100732207, 0.646207488141954, 0.6752834680955857, 0.6378959272988141, 0.719531541923061, 0.6274449911434203, 0.6282913768664002, 0.6137142018415034, 0.6151414159685373, 0.617775629973039, 0.6434823640156537, 0.6365919359959662, 0.6356882876716554, 0.6328748620580882, 0.6220405618660152, 0.6255800169892609, 0.6211573441978544, 0.6228172392584383, 0.6341111708898097, 0.6197075711097568, 0.6339805710595101]
Total Epoch List: [71]
Total Time List: [0.1882500839419663]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebadf968220>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000075
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.27s
Val loss: 0.6926 score: 0.8372 time: 0.19s
Test loss: 0.6927 score: 0.7752 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6515;  Loss pred: 0.6515; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5961;  Loss pred: 0.5961; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5039 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5039 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4936;  Loss pred: 0.4936; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5039 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4674;  Loss pred: 0.4674; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6843 score: 0.5039 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4339;  Loss pred: 0.4339; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.5039 time: 0.19s
Epoch 22/1000, LR 0.000285
Train loss: 0.4086;  Loss pred: 0.4086; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6799 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5039 time: 0.19s
Epoch 23/1000, LR 0.000285
Train loss: 0.3770;  Loss pred: 0.3770; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5039 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3489;  Loss pred: 0.3489; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6720 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6712 score: 0.5039 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3158;  Loss pred: 0.3158; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6671 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6662 score: 0.5039 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.2898;  Loss pred: 0.2898; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6611 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6601 score: 0.5039 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2569;  Loss pred: 0.2569; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6539 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6526 score: 0.5039 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6457 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6440 score: 0.5039 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6355 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6332 score: 0.5039 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6241 score: 0.4961 time: 0.18s
Test loss: 0.6210 score: 0.5116 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 0.26s
Val loss: 0.6128 score: 0.5116 time: 0.18s
Test loss: 0.6086 score: 0.5116 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.25s
Val loss: 0.5998 score: 0.5116 time: 0.18s
Test loss: 0.5944 score: 0.5349 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1168;  Loss pred: 0.1168; Loss self: 0.0000; time: 0.25s
Val loss: 0.5841 score: 0.5116 time: 0.18s
Test loss: 0.5772 score: 0.5426 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1185;  Loss pred: 0.1185; Loss self: 0.0000; time: 0.25s
Val loss: 0.5667 score: 0.5504 time: 0.17s
Test loss: 0.5584 score: 0.5814 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 0.25s
Val loss: 0.5460 score: 0.6124 time: 0.18s
Test loss: 0.5358 score: 0.6512 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.25s
Val loss: 0.5202 score: 0.6667 time: 0.18s
Test loss: 0.5077 score: 0.6977 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.25s
Val loss: 0.4891 score: 0.7519 time: 0.17s
Test loss: 0.4743 score: 0.7364 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.26s
Val loss: 0.4593 score: 0.7984 time: 0.17s
Test loss: 0.4432 score: 0.8062 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.25s
Val loss: 0.4264 score: 0.8295 time: 0.18s
Test loss: 0.4094 score: 0.8295 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.25s
Val loss: 0.3953 score: 0.8372 time: 0.18s
Test loss: 0.3785 score: 0.8372 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0393;  Loss pred: 0.0393; Loss self: 0.0000; time: 0.26s
Val loss: 0.3633 score: 0.8527 time: 0.17s
Test loss: 0.3470 score: 0.8605 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.26s
Val loss: 0.3373 score: 0.8760 time: 0.17s
Test loss: 0.3207 score: 0.8915 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.27s
Val loss: 0.3140 score: 0.9070 time: 0.19s
Test loss: 0.2965 score: 0.9070 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0311;  Loss pred: 0.0311; Loss self: 0.0000; time: 0.26s
Val loss: 0.2934 score: 0.8992 time: 0.19s
Test loss: 0.2758 score: 0.9070 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.27s
Val loss: 0.2779 score: 0.8992 time: 0.19s
Test loss: 0.2603 score: 0.9225 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.27s
Val loss: 0.2662 score: 0.9070 time: 0.19s
Test loss: 0.2490 score: 0.9225 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.27s
Val loss: 0.2568 score: 0.9147 time: 0.19s
Test loss: 0.2413 score: 0.9147 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.26s
Val loss: 0.2541 score: 0.9147 time: 0.17s
Test loss: 0.2397 score: 0.9147 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.26s
Val loss: 0.2546 score: 0.9147 time: 0.17s
Test loss: 0.2415 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.25s
Val loss: 0.2580 score: 0.9147 time: 0.18s
Test loss: 0.2443 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.26s
Val loss: 0.2639 score: 0.9147 time: 0.18s
Test loss: 0.2485 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.26s
Val loss: 0.2710 score: 0.9147 time: 0.18s
Test loss: 0.2544 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.26s
Val loss: 0.2789 score: 0.9147 time: 0.18s
Test loss: 0.2603 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.26s
Val loss: 0.2867 score: 0.9147 time: 0.18s
Test loss: 0.2664 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.26s
Val loss: 0.2965 score: 0.9147 time: 0.18s
Test loss: 0.2738 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.27s
Val loss: 0.3067 score: 0.9147 time: 0.18s
Test loss: 0.2811 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.26s
Val loss: 0.3142 score: 0.9147 time: 0.18s
Test loss: 0.2873 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.26s
Val loss: 0.3218 score: 0.9147 time: 0.18s
Test loss: 0.2936 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.27s
Val loss: 0.3278 score: 0.9147 time: 0.18s
Test loss: 0.2993 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.26s
Val loss: 0.3358 score: 0.9147 time: 0.18s
Test loss: 0.3064 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.26s
Val loss: 0.3407 score: 0.9147 time: 0.18s
Test loss: 0.3117 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.26s
Val loss: 0.3464 score: 0.9147 time: 0.17s
Test loss: 0.3154 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.26s
Val loss: 0.3524 score: 0.9147 time: 0.18s
Test loss: 0.3181 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.26s
Val loss: 0.3579 score: 0.9147 time: 0.19s
Test loss: 0.3209 score: 0.9070 time: 0.25s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.25s
Val loss: 0.3640 score: 0.9147 time: 0.17s
Test loss: 0.3243 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.25s
Val loss: 0.3693 score: 0.9147 time: 0.18s
Test loss: 0.3274 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.34s
Val loss: 0.3772 score: 0.9147 time: 0.19s
Test loss: 0.3316 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.27s
Val loss: 0.3834 score: 0.9147 time: 0.19s
Test loss: 0.3356 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.0192,   Val_Loss: 0.2541,   Val_Precision: 0.9655,   Val_Recall: 0.8615,   Val_accuracy: 0.9106,   Val_Score: 0.9147,   Val_Loss: 0.2541,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2397


[0.1814970918931067, 0.17870643106289208, 0.1778492860030383, 0.19110383186489344, 0.1916478129569441, 0.18656853609718382, 0.18104628007858992, 0.1840810279827565, 0.1891629728488624, 0.18253488698974252, 0.17896579601801932, 0.18576225102879107, 0.1976375039666891, 0.21441768086515367, 0.19361927220597863, 0.18931001308374107, 0.19766568788327277, 0.18668085895478725, 0.18610644899308681, 0.1902207180391997, 0.18674948601983488, 0.1903451110702008, 0.1902148500084877, 0.19214443303644657, 0.18853458994999528, 0.18486099014990032, 0.1828388690482825, 0.18513948912732303, 0.18650964903645217, 0.1856012309435755, 0.1818454118911177, 0.1856296609621495, 0.18653418705798686, 0.1821519338991493, 0.18120165704749525, 0.18383599794469774, 0.18744214018806815, 0.18452483299188316, 0.1811096789315343, 0.18569244001992047, 0.18879041518084705, 0.18357037799432874, 0.18352947593666613, 0.1858683859463781, 0.182005604961887, 0.18477549799717963, 0.18476615380495787, 0.1989070950075984, 0.19956260197795928, 0.2877028309740126, 0.18902584118768573, 0.1868742119986564, 0.19509237702004611, 0.18273692089132965, 0.2594421610701829, 0.1843875190243125, 0.18780427891761065, 0.18708447692915797, 0.18277085199952126, 0.18502971692942083, 0.20833548810333014, 0.19649400608614087, 0.18760059890337288, 0.18935800017789006, 0.18722235201857984, 0.18330262112431228, 0.18626634497195482, 0.18943613185547292, 0.1861100171227008, 0.1833981571253389, 0.18772807507775724, 0.18043200415559113, 0.18609786499291658, 0.18151106708683074, 0.17951917299069464, 0.18430500803515315, 0.1809377169702202, 0.18308119382709265, 0.1833957659546286, 0.17850429704412818, 0.1794667209032923, 0.1813490518834442, 0.17852881806902587, 0.1760934970807284, 0.1818276911508292, 0.17789852316491306, 0.17390524502843618, 0.17801717389374971, 0.17946161911822855, 0.18010959099046886, 0.18033240106888115, 0.18985660606995225, 0.19497443502768874, 0.18121864600107074, 0.17671182402409613, 0.1824694888200611, 0.17157219001092017, 0.17688775318674743, 0.1761078650597483, 0.1730822930112481, 0.17598796589300036, 0.17706182110123336, 0.17419285490177572, 0.17193642584607005, 0.17242169007658958, 0.17206281889230013, 0.17072295118123293, 0.1717474590986967, 0.17549060098826885, 0.17437303601764143, 0.1721768770366907, 0.1703635728918016, 0.17454232997260988, 0.18696692190133035, 0.1845918430481106, 0.18580391514115036, 0.18760096793994308, 0.18144687404856086, 0.1716845480259508, 0.17534579313360155, 0.1770238948520273, 0.17823301092721522, 0.17441676603630185, 0.1785960071720183, 0.17949854582548141, 0.1732383519411087, 0.1792292189784348, 0.1789925480261445, 0.17447116691619158, 0.17989789508283138, 0.1746791850309819, 0.17418642295524478, 0.17366341804154217, 0.189129285980016, 0.2540531209670007, 0.17225540918298066, 0.17487339489161968, 0.18428311916068196, 0.1878384151495993]
[0.0014069542007217573, 0.001385321171030171, 0.0013786766356824673, 0.0014814250532162283, 0.0014856419609065436, 0.0014462677216835955, 0.001403459535492945, 0.001426984713044624, 0.0014663796344873054, 0.0014149991239514924, 0.0013873317520776691, 0.0014400174498355896, 0.0015320736741603806, 0.0016621525648461526, 0.0015009245907440204, 0.0014675194812693107, 0.0015322921541338975, 0.0014471384415099787, 0.0014426856511092002, 0.0014745792096061993, 0.001447670434262286, 0.0014755434966682232, 0.0014745337209960286, 0.001489491728964702, 0.0014615084492247696, 0.0014330309313945761, 0.0014173555740176938, 0.0014351898381963026, 0.0014458112328407146, 0.0014387692321207403, 0.0014096543557450983, 0.001438989619861624, 0.0014460014500619137, 0.0014120304953422426, 0.0014046640081201182, 0.0014250852553852537, 0.0014530398464191329, 0.0014304250619525825, 0.0014039509994692582, 0.0014394762792241896, 0.001463491590549202, 0.0014230261860025484, 0.0014227091157881094, 0.001440840201134714, 0.0014108961624952482, 0.0014323682015285242, 0.001432295765929906, 0.0015419154651751814, 0.001546996914557824, 0.0022302545036745162, 0.0014653165983541528, 0.0014486373023151658, 0.0015123440079073342, 0.0014165652782273616, 0.0020111795431797127, 0.0014293606125915697, 0.0014558471233923306, 0.0014502672630167285, 0.0014168283100738082, 0.001434338890925743, 0.0016150037837467453, 0.001523209349504968, 0.0014542682085532783, 0.00146789147424721, 0.0014513360621595337, 0.001420950551351258, 0.0014439251548213552, 0.0014684971461664568, 0.0014427133110286885, 0.0014216911405065032, 0.0014552563959516066, 0.0013986977066324895, 0.0014426191084722217, 0.0014070625355568273, 0.0013916214960518964, 0.0014287209925205671, 0.001402617961009459, 0.001419234060675137, 0.0014216726042994463, 0.0013837542406521565, 0.0013912148907231961, 0.0014058066037476295, 0.0013839443261164795, 0.001365065868842856, 0.0014095169856653425, 0.0013790583191078533, 0.0013481026746390403, 0.0013799780921996102, 0.0013911753420017717, 0.0013961983797710765, 0.0013979255896812493, 0.0014717566362011801, 0.001511429728896812, 0.0014047957054346568, 0.0013698591009619855, 0.0014144921613958223, 0.0013300169768288386, 0.0013712228929205227, 0.0013651772485251807, 0.0013417232016375822, 0.001364247797620158, 0.0013725722565987083, 0.001350332208540897, 0.0013328405104346516, 0.0013366022486557333, 0.0013338203014906986, 0.001323433730087077, 0.0013313756519278816, 0.0013603922557230144, 0.0013517289613770654, 0.0013347044731526411, 0.001320647851874431, 0.0013530413176171309, 0.001449355983731243, 0.0014309445197527955, 0.001440340427450778, 0.0014542710693018844, 0.0014065649151051229, 0.001330887969193417, 0.0013592697142139655, 0.0013722782546668783, 0.0013816512474977925, 0.0013520679537697819, 0.0013844651718761109, 0.0013914615955463675, 0.0013429329607837885, 0.0013893737905305023, 0.0013875391319856164, 0.001352489666016989, 0.001394557326223499, 0.0013541022095424954, 0.0013502823484902696, 0.0013462280468336603, 0.0014661184959691162, 0.0019694040385038816, 0.0013353132494804702, 0.001355607712338137, 0.001428551311323116, 0.001456111745345731]
[710.7551898185508, 721.854268101863, 725.3332464758741, 675.0257111076685, 673.1096901637032, 691.4349155465512, 712.5249960617955, 700.778355127851, 681.9516423178046, 706.7142184564922, 720.8081257438239, 694.4360293093479, 652.7101254109258, 601.6294900658287, 666.2559905853044, 681.4219591381941, 652.617059548434, 691.0188903257771, 693.1516919372949, 678.1595681571149, 690.7649533573483, 677.7163819690844, 678.1804890325009, 671.3699583246884, 684.2245766902214, 697.8216436869473, 705.5392579896936, 696.7719345454436, 691.653223661301, 695.0384937868068, 709.3937573593577, 694.9320455113234, 691.562238721948, 708.2000022652653, 711.9140194517507, 701.7124036762717, 688.212372471682, 699.0928966491704, 712.2755711403279, 694.697102295394, 683.2974008581298, 702.7277571111465, 702.8843696176433, 694.0394911333426, 708.7693811792957, 698.1445126559422, 698.1798199694869, 648.5439847938662, 646.4137003698089, 448.37932099337667, 682.4463744717028, 690.303914169428, 661.2252204336256, 705.9328753641087, 497.22065013598, 699.6135133365, 686.8853081701724, 689.5280790658413, 705.8018200863773, 697.1853069915616, 619.1935957450448, 656.508575347829, 687.6310670332338, 681.2492732222167, 689.0202938332818, 703.7542573519158, 692.5566721106965, 680.9682964727043, 693.1384027274113, 703.3876567900197, 687.1641332633279, 714.9507683169112, 693.1836644386549, 710.7004662051232, 718.5861980696998, 699.9267213368145, 712.9525129425148, 704.6054119672797, 703.3968277758065, 722.671678699756, 718.7962166507357, 711.335398008644, 722.5724193733463, 732.565382246121, 709.4628941473622, 725.1324952282835, 741.7832623674261, 724.6491851229713, 718.8166507904698, 716.2305976633221, 715.345657438045, 679.4601603300031, 661.625202204999, 711.8472786693142, 730.0020851033136, 706.9675091116793, 751.870101977421, 729.2760390472571, 732.5056149890526, 745.3102091247235, 733.0046650941533, 728.5590942061164, 740.5585038074083, 750.2773153810359, 748.1657321807847, 749.7261804175452, 755.610180748683, 751.1028150108971, 735.0821028222666, 739.7932785143971, 749.2295261721482, 757.20412415821, 739.0757303414214, 689.9616182806832, 698.839113743387, 694.2803110580425, 687.6297143695812, 710.9519008052769, 751.3780446945124, 735.6891642202718, 728.7151833814862, 723.771647737464, 739.6077964956125, 722.3005824298809, 718.668774762226, 744.6388086389367, 719.7487147200118, 720.7003946396565, 739.3771835203352, 717.0734262377213, 738.4966902445759, 740.585849410003, 742.8161984531584, 682.073108517052, 507.7678223711173, 748.8879484937858, 737.6765349580456, 700.0098575904885, 686.7604791983637]
Elapsed: 0.18485759413908337~0.014350032196023342
Time per graph: 0.0014330046057293283~0.00011124055965909568
Speed: 701.058879409824~42.21632560853361
Total Time: 0.1885
best val loss: 0.2540678747012749 test_score: 0.9147

Testing...
Test loss: 0.2413 score: 0.9147 time: 0.17s
test Score 0.9147
Epoch Time List: [0.5988706911448389, 0.5934263800736517, 0.6002791267819703, 0.626824997831136, 0.6354825361631811, 0.622906408039853, 0.6089932322502136, 0.6141225469764322, 0.6138905079569668, 0.6123495791107416, 0.6094213102478534, 0.6007844363339245, 0.6583832169417292, 0.6595323979854584, 0.6479438543319702, 0.6365097146481276, 0.6711158468388021, 0.6527666919864714, 0.6161734731867909, 0.622545403894037, 0.6197332690935582, 0.6327971979044378, 0.6325262980535626, 0.646787955192849, 0.6174309938214719, 0.6171446512453258, 0.610823969822377, 0.6141448239795864, 0.616976265097037, 0.616399047197774, 0.6129905597772449, 0.6156390672549605, 0.6141770388931036, 0.6051008210051805, 0.6122001921758056, 0.6100234480109066, 0.6173967241775244, 0.6193468158598989, 0.6107344648335129, 0.6186481958720833, 0.6157727437093854, 0.6129474528133869, 0.612153829773888, 0.6151915297377855, 0.6124452948570251, 0.6129253781400621, 0.613207925343886, 0.7391917807981372, 0.6466600648127496, 0.739249428967014, 0.6668684100732207, 0.646207488141954, 0.6752834680955857, 0.6378959272988141, 0.719531541923061, 0.6274449911434203, 0.6282913768664002, 0.6137142018415034, 0.6151414159685373, 0.617775629973039, 0.6434823640156537, 0.6365919359959662, 0.6356882876716554, 0.6328748620580882, 0.6220405618660152, 0.6255800169892609, 0.6211573441978544, 0.6228172392584383, 0.6341111708898097, 0.6197075711097568, 0.6339805710595101, 0.6175221777521074, 0.6338715751189739, 0.6263286869507283, 0.6222768442239612, 0.6423102740664035, 0.6269348622299731, 0.640795580111444, 0.6446676750201732, 0.6301875666249543, 0.6425770854111761, 0.6373618089128286, 0.6200850657187402, 0.6155826998874545, 0.6204804580193013, 0.6183979269117117, 0.6115171920973808, 0.6142752498853952, 0.6152228051796556, 0.6203430208843201, 0.6258801869116724, 0.6407326401676983, 0.6503575288224965, 0.6191258721519262, 0.6007558221463114, 0.6281683768611401, 0.6027427131775767, 0.6074401459190995, 0.6083032358437777, 0.6039602810051292, 0.6075710621662438, 0.6086828492116183, 0.6024747262708843, 0.5962847294285893, 0.5960456619504839, 0.5972668169997633, 0.5969399071764201, 0.5959968790411949, 0.6017403290607035, 0.5981946138199419, 0.5962877182755619, 0.5975130219012499, 0.6007914049550891, 0.6421454609371722, 0.6345936662983149, 0.6359848461579531, 0.636368955951184, 0.63481933507137, 0.5995934060774744, 0.601478359894827, 0.6036972599104047, 0.6091253571212292, 0.6060226080007851, 0.6148860780522227, 0.6137352958321571, 0.6099820639938116, 0.619008278939873, 0.6129513247869909, 0.6093400300014764, 0.6184789380058646, 0.608260725159198, 0.6048665139824152, 0.5990991769358516, 0.6297688388731331, 0.6942047998309135, 0.5901709699537605, 0.6023404030129313, 0.7154229308944196, 0.6363058488350362]
Total Epoch List: [71, 68]
Total Time List: [0.1882500839419663, 0.18849629699252546]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ebae4dcf2e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.16s
Epoch 3/1000, LR 0.000050
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 8/1000, LR 0.000200
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.16s
Epoch 10/1000, LR 0.000260
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.31s
Val loss: 0.6922 score: 0.7984 time: 0.18s
Test loss: 0.6921 score: 0.7734 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.19s
Epoch 12/1000, LR 0.000290
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.17s
Epoch 15/1000, LR 0.000290
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.16s
Epoch 16/1000, LR 0.000290
Train loss: 0.5750;  Loss pred: 0.5750; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5000 time: 0.17s
Epoch 17/1000, LR 0.000290
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6776 score: 0.5000 time: 0.16s
Epoch 19/1000, LR 0.000290
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6718 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 0.4946;  Loss pred: 0.4946; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6638 score: 0.5039 time: 0.19s
Test loss: 0.6633 score: 0.5078 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.4671;  Loss pred: 0.4671; Loss self: 0.0000; time: 0.28s
Val loss: 0.6519 score: 0.5116 time: 0.18s
Test loss: 0.6513 score: 0.5391 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 0.4422;  Loss pred: 0.4422; Loss self: 0.0000; time: 0.28s
Val loss: 0.6382 score: 0.5426 time: 0.18s
Test loss: 0.6371 score: 0.5469 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.4124;  Loss pred: 0.4124; Loss self: 0.0000; time: 0.28s
Val loss: 0.6220 score: 0.5504 time: 0.19s
Test loss: 0.6203 score: 0.6016 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.3902;  Loss pred: 0.3902; Loss self: 0.0000; time: 0.28s
Val loss: 0.6000 score: 0.7209 time: 0.18s
Test loss: 0.5976 score: 0.7344 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.28s
Val loss: 0.5755 score: 0.8062 time: 0.18s
Test loss: 0.5720 score: 0.7969 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.3516;  Loss pred: 0.3516; Loss self: 0.0000; time: 0.28s
Val loss: 0.5489 score: 0.8605 time: 0.18s
Test loss: 0.5446 score: 0.8594 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.3206;  Loss pred: 0.3206; Loss self: 0.0000; time: 0.28s
Val loss: 0.5145 score: 0.9147 time: 0.18s
Test loss: 0.5109 score: 0.9375 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.3108;  Loss pred: 0.3108; Loss self: 0.0000; time: 0.28s
Val loss: 0.4836 score: 0.9380 time: 0.18s
Test loss: 0.4811 score: 0.9141 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 0.28s
Val loss: 0.4431 score: 0.9457 time: 0.18s
Test loss: 0.4410 score: 0.9141 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.2633;  Loss pred: 0.2633; Loss self: 0.0000; time: 0.28s
Val loss: 0.4082 score: 0.9225 time: 0.19s
Test loss: 0.4046 score: 0.9219 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.28s
Val loss: 0.3730 score: 0.9225 time: 0.19s
Test loss: 0.3700 score: 0.9219 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.2124;  Loss pred: 0.2124; Loss self: 0.0000; time: 0.28s
Val loss: 0.3396 score: 0.9612 time: 0.18s
Test loss: 0.3421 score: 0.9219 time: 0.17s
Epoch 33/1000, LR 0.000290
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.29s
Val loss: 0.3110 score: 0.9457 time: 0.18s
Test loss: 0.3148 score: 0.9141 time: 0.17s
Epoch 34/1000, LR 0.000290
Train loss: 0.1819;  Loss pred: 0.1819; Loss self: 0.0000; time: 0.28s
Val loss: 0.2779 score: 0.9690 time: 0.18s
Test loss: 0.2883 score: 0.9219 time: 0.17s
Epoch 35/1000, LR 0.000290
Train loss: 0.1692;  Loss pred: 0.1692; Loss self: 0.0000; time: 0.28s
Val loss: 0.2488 score: 0.9690 time: 0.18s
Test loss: 0.2673 score: 0.9219 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 0.1531;  Loss pred: 0.1531; Loss self: 0.0000; time: 0.29s
Val loss: 0.2382 score: 0.9535 time: 0.18s
Test loss: 0.2555 score: 0.9141 time: 0.17s
Epoch 37/1000, LR 0.000290
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 0.28s
Val loss: 0.2251 score: 0.9380 time: 0.19s
Test loss: 0.2447 score: 0.9219 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 0.1397;  Loss pred: 0.1397; Loss self: 0.0000; time: 0.28s
Val loss: 0.2173 score: 0.9302 time: 0.18s
Test loss: 0.2385 score: 0.9219 time: 0.17s
Epoch 39/1000, LR 0.000289
Train loss: 0.1205;  Loss pred: 0.1205; Loss self: 0.0000; time: 0.28s
Val loss: 0.1910 score: 0.9690 time: 0.19s
Test loss: 0.2264 score: 0.9219 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.28s
Val loss: 0.1785 score: 0.9690 time: 0.19s
Test loss: 0.2176 score: 0.9219 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 0.1079;  Loss pred: 0.1079; Loss self: 0.0000; time: 0.27s
Val loss: 0.1872 score: 0.9535 time: 0.18s
Test loss: 0.2191 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.28s
Val loss: 0.1781 score: 0.9690 time: 0.18s
Test loss: 0.2151 score: 0.9141 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 0.0991;  Loss pred: 0.0991; Loss self: 0.0000; time: 0.27s
Val loss: 0.1800 score: 0.9612 time: 0.18s
Test loss: 0.2162 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.27s
Val loss: 0.1843 score: 0.9535 time: 0.18s
Test loss: 0.2216 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.27s
Val loss: 0.1984 score: 0.9457 time: 0.18s
Test loss: 0.2373 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.27s
Val loss: 0.1822 score: 0.9457 time: 0.18s
Test loss: 0.2241 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 0.27s
Val loss: 0.1756 score: 0.9535 time: 0.18s
Test loss: 0.2203 score: 0.9219 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 0.28s
Val loss: 0.1713 score: 0.9535 time: 0.19s
Test loss: 0.2200 score: 0.9219 time: 0.17s
Epoch 49/1000, LR 0.000289
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.27s
Val loss: 0.1679 score: 0.9457 time: 0.18s
Test loss: 0.2184 score: 0.9219 time: 0.16s
Epoch 50/1000, LR 0.000289
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.27s
Val loss: 0.1696 score: 0.9535 time: 0.18s
Test loss: 0.2183 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.28s
Val loss: 0.1797 score: 0.9535 time: 0.18s
Test loss: 0.2289 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0694;  Loss pred: 0.0694; Loss self: 0.0000; time: 0.28s
Val loss: 0.1932 score: 0.9535 time: 0.18s
Test loss: 0.2391 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.28s
Val loss: 0.1992 score: 0.9535 time: 0.18s
Test loss: 0.2433 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.27s
Val loss: 0.1897 score: 0.9535 time: 0.18s
Test loss: 0.2397 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.27s
Val loss: 0.1754 score: 0.9457 time: 0.18s
Test loss: 0.2265 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.27s
Val loss: 0.1711 score: 0.9457 time: 0.18s
Test loss: 0.2230 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.28s
Val loss: 0.1723 score: 0.9380 time: 0.18s
Test loss: 0.2215 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.27s
Val loss: 0.1707 score: 0.9457 time: 0.18s
Test loss: 0.2194 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.27s
Val loss: 0.1683 score: 0.9535 time: 0.18s
Test loss: 0.2191 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.27s
Val loss: 0.1828 score: 0.9535 time: 0.18s
Test loss: 0.2338 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.27s
Val loss: 0.1746 score: 0.9535 time: 0.18s
Test loss: 0.2315 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.27s
Val loss: 0.1966 score: 0.9457 time: 0.18s
Test loss: 0.2510 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0424;  Loss pred: 0.0424; Loss self: 0.0000; time: 0.27s
Val loss: 0.2118 score: 0.9457 time: 0.18s
Test loss: 0.2660 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.27s
Val loss: 0.2065 score: 0.9457 time: 0.18s
Test loss: 0.2649 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.27s
Val loss: 0.2066 score: 0.9457 time: 0.18s
Test loss: 0.2720 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.27s
Val loss: 0.2063 score: 0.9457 time: 0.18s
Test loss: 0.2712 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.27s
Val loss: 0.1993 score: 0.9457 time: 0.18s
Test loss: 0.2724 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.28s
Val loss: 0.1597 score: 0.9380 time: 0.19s
Test loss: 0.2473 score: 0.9219 time: 0.18s
Epoch 69/1000, LR 0.000288
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.28s
Val loss: 0.1596 score: 0.9380 time: 0.20s
Test loss: 0.2429 score: 0.9219 time: 0.18s
Epoch 70/1000, LR 0.000287
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.27s
Val loss: 0.1435 score: 0.9612 time: 0.18s
Test loss: 0.2306 score: 0.9141 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.27s
Val loss: 0.1506 score: 0.9457 time: 0.18s
Test loss: 0.2354 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.27s
Val loss: 0.1579 score: 0.9380 time: 0.18s
Test loss: 0.2460 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.27s
Val loss: 0.1451 score: 0.9612 time: 0.18s
Test loss: 0.2407 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.27s
Val loss: 0.1427 score: 0.9690 time: 0.18s
Test loss: 0.2390 score: 0.9219 time: 0.16s
Epoch 75/1000, LR 0.000287
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.27s
Val loss: 0.1411 score: 0.9690 time: 0.18s
Test loss: 0.2345 score: 0.9219 time: 0.16s
Epoch 76/1000, LR 0.000287
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.27s
Val loss: 0.1544 score: 0.9457 time: 0.18s
Test loss: 0.2501 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.27s
Val loss: 0.1472 score: 0.9690 time: 0.18s
Test loss: 0.2509 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.27s
Val loss: 0.1687 score: 0.9380 time: 0.18s
Test loss: 0.2725 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.27s
Val loss: 0.1593 score: 0.9535 time: 0.18s
Test loss: 0.2696 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.27s
Val loss: 0.1517 score: 0.9535 time: 0.18s
Test loss: 0.2674 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.27s
Val loss: 0.1521 score: 0.9612 time: 0.18s
Test loss: 0.2670 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.27s
Val loss: 0.1841 score: 0.9535 time: 0.18s
Test loss: 0.2898 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.30s
Val loss: 0.2064 score: 0.9457 time: 0.22s
Test loss: 0.3096 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.27s
Val loss: 0.2299 score: 0.9380 time: 0.18s
Test loss: 0.3276 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.27s
Val loss: 0.1968 score: 0.9457 time: 0.18s
Test loss: 0.3020 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.37s
Val loss: 0.2211 score: 0.9457 time: 0.18s
Test loss: 0.3248 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.27s
Val loss: 0.2280 score: 0.9457 time: 0.18s
Test loss: 0.3340 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.27s
Val loss: 0.1896 score: 0.9535 time: 0.18s
Test loss: 0.2993 score: 0.9219 time: 0.26s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.27s
Val loss: 0.2050 score: 0.9457 time: 0.18s
Test loss: 0.3229 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.27s
Val loss: 0.2094 score: 0.9535 time: 0.18s
Test loss: 0.3200 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.29s
Val loss: 0.1723 score: 0.9457 time: 0.18s
Test loss: 0.2890 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.28s
Val loss: 0.1994 score: 0.9535 time: 0.18s
Test loss: 0.3130 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000285
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.27s
Val loss: 0.2125 score: 0.9535 time: 0.18s
Test loss: 0.3246 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000285
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.27s
Val loss: 0.2225 score: 0.9535 time: 0.18s
Test loss: 0.3353 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000285
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.27s
Val loss: 0.1885 score: 0.9457 time: 0.20s
Test loss: 0.3104 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0236,   Val_Loss: 0.1411,   Val_Precision: 0.9841,   Val_Recall: 0.9538,   Val_accuracy: 0.9688,   Val_Score: 0.9690,   Val_Loss: 0.1411,   Test_Precision: 0.9500,   Test_Recall: 0.8906,   Test_accuracy: 0.9194,   Test_Score: 0.9219,   Test_loss: 0.2345


[0.1814970918931067, 0.17870643106289208, 0.1778492860030383, 0.19110383186489344, 0.1916478129569441, 0.18656853609718382, 0.18104628007858992, 0.1840810279827565, 0.1891629728488624, 0.18253488698974252, 0.17896579601801932, 0.18576225102879107, 0.1976375039666891, 0.21441768086515367, 0.19361927220597863, 0.18931001308374107, 0.19766568788327277, 0.18668085895478725, 0.18610644899308681, 0.1902207180391997, 0.18674948601983488, 0.1903451110702008, 0.1902148500084877, 0.19214443303644657, 0.18853458994999528, 0.18486099014990032, 0.1828388690482825, 0.18513948912732303, 0.18650964903645217, 0.1856012309435755, 0.1818454118911177, 0.1856296609621495, 0.18653418705798686, 0.1821519338991493, 0.18120165704749525, 0.18383599794469774, 0.18744214018806815, 0.18452483299188316, 0.1811096789315343, 0.18569244001992047, 0.18879041518084705, 0.18357037799432874, 0.18352947593666613, 0.1858683859463781, 0.182005604961887, 0.18477549799717963, 0.18476615380495787, 0.1989070950075984, 0.19956260197795928, 0.2877028309740126, 0.18902584118768573, 0.1868742119986564, 0.19509237702004611, 0.18273692089132965, 0.2594421610701829, 0.1843875190243125, 0.18780427891761065, 0.18708447692915797, 0.18277085199952126, 0.18502971692942083, 0.20833548810333014, 0.19649400608614087, 0.18760059890337288, 0.18935800017789006, 0.18722235201857984, 0.18330262112431228, 0.18626634497195482, 0.18943613185547292, 0.1861100171227008, 0.1833981571253389, 0.18772807507775724, 0.18043200415559113, 0.18609786499291658, 0.18151106708683074, 0.17951917299069464, 0.18430500803515315, 0.1809377169702202, 0.18308119382709265, 0.1833957659546286, 0.17850429704412818, 0.1794667209032923, 0.1813490518834442, 0.17852881806902587, 0.1760934970807284, 0.1818276911508292, 0.17789852316491306, 0.17390524502843618, 0.17801717389374971, 0.17946161911822855, 0.18010959099046886, 0.18033240106888115, 0.18985660606995225, 0.19497443502768874, 0.18121864600107074, 0.17671182402409613, 0.1824694888200611, 0.17157219001092017, 0.17688775318674743, 0.1761078650597483, 0.1730822930112481, 0.17598796589300036, 0.17706182110123336, 0.17419285490177572, 0.17193642584607005, 0.17242169007658958, 0.17206281889230013, 0.17072295118123293, 0.1717474590986967, 0.17549060098826885, 0.17437303601764143, 0.1721768770366907, 0.1703635728918016, 0.17454232997260988, 0.18696692190133035, 0.1845918430481106, 0.18580391514115036, 0.18760096793994308, 0.18144687404856086, 0.1716845480259508, 0.17534579313360155, 0.1770238948520273, 0.17823301092721522, 0.17441676603630185, 0.1785960071720183, 0.17949854582548141, 0.1732383519411087, 0.1792292189784348, 0.1789925480261445, 0.17447116691619158, 0.17989789508283138, 0.1746791850309819, 0.17418642295524478, 0.17366341804154217, 0.189129285980016, 0.2540531209670007, 0.17225540918298066, 0.17487339489161968, 0.18428311916068196, 0.1878384151495993, 0.17088191909715533, 0.16868255706503987, 0.17215020605362952, 0.16832175897434354, 0.16679309005849063, 0.17038159910589457, 0.17902565002441406, 0.17550349910743535, 0.16907667485065758, 0.17317889211699367, 0.19061906612478197, 0.17163743707351387, 0.17372881900519133, 0.17326950677670538, 0.16933650779537857, 0.1716670070309192, 0.17425869102589786, 0.16755816992372274, 0.17212986503727734, 0.16716019809246063, 0.17489875596947968, 0.1714781909249723, 0.17053595697507262, 0.16850004298612475, 0.16935265017673373, 0.17301863594911993, 0.1710747960023582, 0.16999709699302912, 0.1738501579966396, 0.1739487659651786, 0.1706978059373796, 0.1703135611023754, 0.17384432791732252, 0.17098572687245905, 0.1726100870873779, 0.17352329986169934, 0.17202055314555764, 0.17322299093939364, 0.1718755669426173, 0.17061629402451217, 0.16564021608792245, 0.16952895699068904, 0.16571967885829508, 0.16678952099755406, 0.16955754510127008, 0.16243309597484767, 0.16633476899005473, 0.17638188414275646, 0.16641803598031402, 0.16530603799037635, 0.16990286181680858, 0.1673533720895648, 0.16556931613013148, 0.17214141809381545, 0.16808139393106103, 0.1645229309797287, 0.16935867187567055, 0.16898748907260597, 0.16581977205350995, 0.16414369107224047, 0.1685618490446359, 0.16967451199889183, 0.1652066728565842, 0.1636942469049245, 0.16491517215035856, 0.169580880086869, 0.1805418310686946, 0.1857300589326769, 0.17959426902234554, 0.16504958202131093, 0.16567251412197948, 0.1696770149283111, 0.17042061081156135, 0.16488587204366922, 0.16895115398801863, 0.1699162619188428, 0.1678513609804213, 0.16603566706180573, 0.16950218193233013, 0.16717904899269342, 0.16438674204982817, 0.16667995508760214, 0.1697956679854542, 0.1686325678601861, 0.16474069911055267, 0.17205765401013196, 0.17288499092683196, 0.2653092099353671, 0.17097572796046734, 0.17039335682056844, 0.16539486218243837, 0.17016355716623366, 0.16953877918422222, 0.1651928350329399, 0.1673356289975345]
[0.0014069542007217573, 0.001385321171030171, 0.0013786766356824673, 0.0014814250532162283, 0.0014856419609065436, 0.0014462677216835955, 0.001403459535492945, 0.001426984713044624, 0.0014663796344873054, 0.0014149991239514924, 0.0013873317520776691, 0.0014400174498355896, 0.0015320736741603806, 0.0016621525648461526, 0.0015009245907440204, 0.0014675194812693107, 0.0015322921541338975, 0.0014471384415099787, 0.0014426856511092002, 0.0014745792096061993, 0.001447670434262286, 0.0014755434966682232, 0.0014745337209960286, 0.001489491728964702, 0.0014615084492247696, 0.0014330309313945761, 0.0014173555740176938, 0.0014351898381963026, 0.0014458112328407146, 0.0014387692321207403, 0.0014096543557450983, 0.001438989619861624, 0.0014460014500619137, 0.0014120304953422426, 0.0014046640081201182, 0.0014250852553852537, 0.0014530398464191329, 0.0014304250619525825, 0.0014039509994692582, 0.0014394762792241896, 0.001463491590549202, 0.0014230261860025484, 0.0014227091157881094, 0.001440840201134714, 0.0014108961624952482, 0.0014323682015285242, 0.001432295765929906, 0.0015419154651751814, 0.001546996914557824, 0.0022302545036745162, 0.0014653165983541528, 0.0014486373023151658, 0.0015123440079073342, 0.0014165652782273616, 0.0020111795431797127, 0.0014293606125915697, 0.0014558471233923306, 0.0014502672630167285, 0.0014168283100738082, 0.001434338890925743, 0.0016150037837467453, 0.001523209349504968, 0.0014542682085532783, 0.00146789147424721, 0.0014513360621595337, 0.001420950551351258, 0.0014439251548213552, 0.0014684971461664568, 0.0014427133110286885, 0.0014216911405065032, 0.0014552563959516066, 0.0013986977066324895, 0.0014426191084722217, 0.0014070625355568273, 0.0013916214960518964, 0.0014287209925205671, 0.001402617961009459, 0.001419234060675137, 0.0014216726042994463, 0.0013837542406521565, 0.0013912148907231961, 0.0014058066037476295, 0.0013839443261164795, 0.001365065868842856, 0.0014095169856653425, 0.0013790583191078533, 0.0013481026746390403, 0.0013799780921996102, 0.0013911753420017717, 0.0013961983797710765, 0.0013979255896812493, 0.0014717566362011801, 0.001511429728896812, 0.0014047957054346568, 0.0013698591009619855, 0.0014144921613958223, 0.0013300169768288386, 0.0013712228929205227, 0.0013651772485251807, 0.0013417232016375822, 0.001364247797620158, 0.0013725722565987083, 0.001350332208540897, 0.0013328405104346516, 0.0013366022486557333, 0.0013338203014906986, 0.001323433730087077, 0.0013313756519278816, 0.0013603922557230144, 0.0013517289613770654, 0.0013347044731526411, 0.001320647851874431, 0.0013530413176171309, 0.001449355983731243, 0.0014309445197527955, 0.001440340427450778, 0.0014542710693018844, 0.0014065649151051229, 0.001330887969193417, 0.0013592697142139655, 0.0013722782546668783, 0.0013816512474977925, 0.0013520679537697819, 0.0013844651718761109, 0.0013914615955463675, 0.0013429329607837885, 0.0013893737905305023, 0.0013875391319856164, 0.001352489666016989, 0.001394557326223499, 0.0013541022095424954, 0.0013502823484902696, 0.0013462280468336603, 0.0014661184959691162, 0.0019694040385038816, 0.0013353132494804702, 0.001355607712338137, 0.001428551311323116, 0.001456111745345731, 0.001335014992946526, 0.001317832477070624, 0.0013449234847939806, 0.001315013741987059, 0.001303071016081958, 0.0013311062430148013, 0.0013986378908157349, 0.0013711210867768386, 0.0013209115222707624, 0.001352960094664013, 0.0014892114540998591, 0.001340917477136827, 0.0013572563984780572, 0.0013536680216930108, 0.001322941467151395, 0.0013411484924290562, 0.001361396023639827, 0.001309048202529084, 0.0013447645706037292, 0.0013059390475973487, 0.00136639653101156, 0.001339673366601346, 0.0013323121638677549, 0.0013164065858290996, 0.0013230675795057323, 0.0013517080933524994, 0.0013365218437684234, 0.00132810232025804, 0.001358204359348747, 0.0013589747341029579, 0.0013335766088857781, 0.0013305746961123077, 0.0013581588118540822, 0.0013358259911910864, 0.0013485163053701399, 0.0013556507801695261, 0.001343910571449669, 0.0013533046167140128, 0.0013427778667391976, 0.0013329397970665013, 0.0012940641881868942, 0.0013244449764897581, 0.0012946849910804303, 0.0013030431327933911, 0.0013246683211036725, 0.0012690085623034975, 0.0012994903827348026, 0.0013779834698652849, 0.0013001409060962033, 0.0012914534217998153, 0.001327366107943817, 0.001307448219449725, 0.0012935102822666522, 0.0013448548288579332, 0.0013131358900864143, 0.0012853353982791305, 0.0013231146240286762, 0.0013202147583797341, 0.0012954669691680465, 0.0012823725865018787, 0.001316889445661218, 0.0013255821249913424, 0.001290677131692064, 0.0012788613039447227, 0.0012883997824246762, 0.001324850625678664, 0.0014104830552241765, 0.0014510160854115384, 0.0014030802267370746, 0.0012894498595414916, 0.0012943165165779646, 0.0013256016791274305, 0.001331411021965323, 0.0012881708753411658, 0.0013199308905313956, 0.0013274707962409593, 0.0013113387576595414, 0.0012971536489203572, 0.0013242357963463292, 0.0013060863202554174, 0.0012842714222642826, 0.0013021871491218917, 0.001326528656136361, 0.001317441936407704, 0.0012870367118011927, 0.001344200421954156, 0.0013506639916158747, 0.0020727282026200555, 0.001335747874691151, 0.001331198100160691, 0.0012921473608002998, 0.0013294027903612005, 0.001324521712376736, 0.001290569023694843, 0.0013073096015432384]
[710.7551898185508, 721.854268101863, 725.3332464758741, 675.0257111076685, 673.1096901637032, 691.4349155465512, 712.5249960617955, 700.778355127851, 681.9516423178046, 706.7142184564922, 720.8081257438239, 694.4360293093479, 652.7101254109258, 601.6294900658287, 666.2559905853044, 681.4219591381941, 652.617059548434, 691.0188903257771, 693.1516919372949, 678.1595681571149, 690.7649533573483, 677.7163819690844, 678.1804890325009, 671.3699583246884, 684.2245766902214, 697.8216436869473, 705.5392579896936, 696.7719345454436, 691.653223661301, 695.0384937868068, 709.3937573593577, 694.9320455113234, 691.562238721948, 708.2000022652653, 711.9140194517507, 701.7124036762717, 688.212372471682, 699.0928966491704, 712.2755711403279, 694.697102295394, 683.2974008581298, 702.7277571111465, 702.8843696176433, 694.0394911333426, 708.7693811792957, 698.1445126559422, 698.1798199694869, 648.5439847938662, 646.4137003698089, 448.37932099337667, 682.4463744717028, 690.303914169428, 661.2252204336256, 705.9328753641087, 497.22065013598, 699.6135133365, 686.8853081701724, 689.5280790658413, 705.8018200863773, 697.1853069915616, 619.1935957450448, 656.508575347829, 687.6310670332338, 681.2492732222167, 689.0202938332818, 703.7542573519158, 692.5566721106965, 680.9682964727043, 693.1384027274113, 703.3876567900197, 687.1641332633279, 714.9507683169112, 693.1836644386549, 710.7004662051232, 718.5861980696998, 699.9267213368145, 712.9525129425148, 704.6054119672797, 703.3968277758065, 722.671678699756, 718.7962166507357, 711.335398008644, 722.5724193733463, 732.565382246121, 709.4628941473622, 725.1324952282835, 741.7832623674261, 724.6491851229713, 718.8166507904698, 716.2305976633221, 715.345657438045, 679.4601603300031, 661.625202204999, 711.8472786693142, 730.0020851033136, 706.9675091116793, 751.870101977421, 729.2760390472571, 732.5056149890526, 745.3102091247235, 733.0046650941533, 728.5590942061164, 740.5585038074083, 750.2773153810359, 748.1657321807847, 749.7261804175452, 755.610180748683, 751.1028150108971, 735.0821028222666, 739.7932785143971, 749.2295261721482, 757.20412415821, 739.0757303414214, 689.9616182806832, 698.839113743387, 694.2803110580425, 687.6297143695812, 710.9519008052769, 751.3780446945124, 735.6891642202718, 728.7151833814862, 723.771647737464, 739.6077964956125, 722.3005824298809, 718.668774762226, 744.6388086389367, 719.7487147200118, 720.7003946396565, 739.3771835203352, 717.0734262377213, 738.4966902445759, 740.585849410003, 742.8161984531584, 682.073108517052, 507.7678223711173, 748.8879484937858, 737.6765349580456, 700.0098575904885, 686.7604791983637, 749.0552580184057, 758.8217906291658, 743.5367225765881, 760.4483269421538, 767.4178825700347, 751.254834276125, 714.9813447544774, 729.3301880075004, 757.0529767814521, 739.120099656993, 671.4963125263102, 745.7580477921985, 736.7804647090541, 738.7335624205084, 755.8913412497651, 745.6295895981092, 734.5401210489811, 763.9138101011084, 743.6245881694014, 765.7325216209656, 731.8519751068804, 746.4506087307889, 750.5748480873735, 759.6437231208317, 755.8192910853254, 739.8046996373346, 748.2107416818757, 752.9540342988843, 736.2662276238722, 735.848853481509, 749.8631824650208, 751.5549505952687, 736.2909192002783, 748.6004963179015, 741.5557350087217, 737.6530996241883, 744.0971306009636, 738.9319356850499, 744.7248161964424, 750.2214295054987, 772.759194736, 755.0332537410119, 772.3886558424439, 767.434304232321, 754.9059519796103, 788.0167476449532, 769.5324361658459, 725.6981102231673, 769.1474018786126, 774.3213832724718, 753.3716538454262, 764.8486457237115, 773.090105049396, 743.5746807327985, 761.5358071845804, 778.0070488518784, 755.7924172549443, 757.4525232752848, 771.9224216439911, 779.8045673510933, 759.365186876332, 754.3855496742846, 774.787106275766, 781.9456237478148, 776.156604216489, 754.8020739981482, 708.9769680650747, 689.1722359620701, 712.7176200932889, 775.5245328853536, 772.6085445033907, 754.3744216273504, 751.0828613420066, 776.2945267142101, 757.6154230297667, 753.3122407149983, 762.579458708889, 770.9186963566859, 755.152520992922, 765.6461785806322, 778.6516017283268, 767.9387718380829, 753.8472654730232, 759.0467347097821, 776.9786136096396, 743.9366806225456, 740.3765897420899, 482.45592390547813, 748.6442755757466, 751.202995166, 773.9055392108246, 752.2174673097375, 754.9895110481724, 774.8520084086966, 764.9297448894516]
Elapsed: 0.1792437846873863~0.014653606411087839
Time per graph: 0.0013936918394982644~0.00011152381721791709
Speed: 721.1164620841944~45.8407503906404
Total Time: 0.1682
best val loss: 0.14105058064352172 test_score: 0.9219

Testing...
Test loss: 0.2883 score: 0.9219 time: 0.17s
test Score 0.9219
Epoch Time List: [0.5988706911448389, 0.5934263800736517, 0.6002791267819703, 0.626824997831136, 0.6354825361631811, 0.622906408039853, 0.6089932322502136, 0.6141225469764322, 0.6138905079569668, 0.6123495791107416, 0.6094213102478534, 0.6007844363339245, 0.6583832169417292, 0.6595323979854584, 0.6479438543319702, 0.6365097146481276, 0.6711158468388021, 0.6527666919864714, 0.6161734731867909, 0.622545403894037, 0.6197332690935582, 0.6327971979044378, 0.6325262980535626, 0.646787955192849, 0.6174309938214719, 0.6171446512453258, 0.610823969822377, 0.6141448239795864, 0.616976265097037, 0.616399047197774, 0.6129905597772449, 0.6156390672549605, 0.6141770388931036, 0.6051008210051805, 0.6122001921758056, 0.6100234480109066, 0.6173967241775244, 0.6193468158598989, 0.6107344648335129, 0.6186481958720833, 0.6157727437093854, 0.6129474528133869, 0.612153829773888, 0.6151915297377855, 0.6124452948570251, 0.6129253781400621, 0.613207925343886, 0.7391917807981372, 0.6466600648127496, 0.739249428967014, 0.6668684100732207, 0.646207488141954, 0.6752834680955857, 0.6378959272988141, 0.719531541923061, 0.6274449911434203, 0.6282913768664002, 0.6137142018415034, 0.6151414159685373, 0.617775629973039, 0.6434823640156537, 0.6365919359959662, 0.6356882876716554, 0.6328748620580882, 0.6220405618660152, 0.6255800169892609, 0.6211573441978544, 0.6228172392584383, 0.6341111708898097, 0.6197075711097568, 0.6339805710595101, 0.6175221777521074, 0.6338715751189739, 0.6263286869507283, 0.6222768442239612, 0.6423102740664035, 0.6269348622299731, 0.640795580111444, 0.6446676750201732, 0.6301875666249543, 0.6425770854111761, 0.6373618089128286, 0.6200850657187402, 0.6155826998874545, 0.6204804580193013, 0.6183979269117117, 0.6115171920973808, 0.6142752498853952, 0.6152228051796556, 0.6203430208843201, 0.6258801869116724, 0.6407326401676983, 0.6503575288224965, 0.6191258721519262, 0.6007558221463114, 0.6281683768611401, 0.6027427131775767, 0.6074401459190995, 0.6083032358437777, 0.6039602810051292, 0.6075710621662438, 0.6086828492116183, 0.6024747262708843, 0.5962847294285893, 0.5960456619504839, 0.5972668169997633, 0.5969399071764201, 0.5959968790411949, 0.6017403290607035, 0.5981946138199419, 0.5962877182755619, 0.5975130219012499, 0.6007914049550891, 0.6421454609371722, 0.6345936662983149, 0.6359848461579531, 0.636368955951184, 0.63481933507137, 0.5995934060774744, 0.601478359894827, 0.6036972599104047, 0.6091253571212292, 0.6060226080007851, 0.6148860780522227, 0.6137352958321571, 0.6099820639938116, 0.619008278939873, 0.6129513247869909, 0.6093400300014764, 0.6184789380058646, 0.608260725159198, 0.6048665139824152, 0.5990991769358516, 0.6297688388731331, 0.6942047998309135, 0.5901709699537605, 0.6023404030129313, 0.7154229308944196, 0.6363058488350362, 0.6283921177964658, 0.6236267150379717, 0.6292343370150775, 0.6264832681044936, 0.6434639552608132, 0.6185289260465652, 0.6293909878004342, 0.6565990473609418, 0.6245955871418118, 0.6572957101743668, 0.6531429409515113, 0.6293269961606711, 0.6386505530681461, 0.6345442808233202, 0.6313722657505423, 0.6315022432245314, 0.6318235627841204, 0.6290963790379465, 0.6296163070946932, 0.6324784359894693, 0.6339731242042035, 0.6264241749886423, 0.6281757790129632, 0.6234475581441075, 0.628290964756161, 0.6279674000106752, 0.6299130071420223, 0.6320671238936484, 0.6332101768348366, 0.6389441210776567, 0.6328903518151492, 0.6321312442887574, 0.6375867361202836, 0.6328463521786034, 0.6307510901242495, 0.6386443488299847, 0.6374018180649728, 0.6356768109835684, 0.6327775209210813, 0.6372147127985954, 0.6122525939717889, 0.6187898742500693, 0.6135694701224566, 0.6106876251287758, 0.6163129699416459, 0.6068661389872432, 0.6082533008884639, 0.6409046549815685, 0.6162598861847073, 0.6126770139671862, 0.6222176719456911, 0.6220838287845254, 0.6166943577118218, 0.6189534328877926, 0.620304775191471, 0.6140507101081312, 0.619534231023863, 0.614835006184876, 0.6084807983133942, 0.6112106242217124, 0.6153311908710748, 0.6137055950239301, 0.6105900858528912, 0.6110318778082728, 0.6084349930752069, 0.6163991852663457, 0.6278612080495805, 0.6551364702172577, 0.6532774469815195, 0.6159097717609257, 0.6128374021500349, 0.6165269089397043, 0.6189361407887191, 0.6113455276936293, 0.6165716021787375, 0.6128650298342109, 0.6174631440080702, 0.612577660009265, 0.6163085082080215, 0.613638139795512, 0.6106717139482498, 0.6148039032705128, 0.6916032971348614, 0.6105416549835354, 0.6129365779925138, 0.7131457789801061, 0.6198568008840084, 0.7174775646999478, 0.6149273351766169, 0.6194794222246855, 0.6393050870392472, 0.6188141771126539, 0.6182804917916656, 0.6093007437884808, 0.6293372500222176]
Total Epoch List: [71, 68, 95]
Total Time List: [0.1882500839419663, 0.18849629699252546, 0.1682286688592285]
T-times Epoch Time: 0.6126716894262422 ~ 0.010096447775914154
T-times Total Epoch: 71.1111111111111 ~ 5.717635702341603
T-times Total Time: 0.17415544964993993 ~ 0.006364933944688453
T-times Inference Elapsed: 0.17483192501508293 ~ 0.003932389527971834
T-times Time Per Graph: 0.0013589380277963907 ~ 3.0812849598213026e-05
T-times Speed: 740.528909709043 ~ 18.000708754274015
T-times cross validation test micro f1 score:0.9133096497516742 ~ 0.004891430952975483
T-times cross validation test precision:0.9609058572649585 ~ 0.011352440331071438
T-times cross validation test recall:0.8720886752136753 ~ 0.017285089968891342
T-times cross validation test f1_score:0.9133096497516742 ~ 0.006435383806239628
