Namespace(seed=15, model='SGFormer', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.8, epochs=1000, lr=0.001, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.8_0.001_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=5, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e25bdc0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7247;  Loss pred: 0.7247; Loss self: 0.0000; time: 0.52s
Val loss: 0.6918 score: 0.5039 time: 0.23s
Test loss: 0.6873 score: 0.5736 time: 0.21s
Epoch 2/1000, LR 0.000050
Train loss: 0.7046;  Loss pred: 0.7046; Loss self: 0.0000; time: 0.30s
Val loss: 0.6857 score: 0.5426 time: 0.20s
Test loss: 0.6804 score: 0.5891 time: 0.35s
Epoch 3/1000, LR 0.000150
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.29s
Val loss: 0.6762 score: 0.7829 time: 0.21s
Test loss: 0.6698 score: 0.8372 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.29s
Val loss: 0.6676 score: 0.5426 time: 0.20s
Test loss: 0.6607 score: 0.5581 time: 0.21s
Epoch 5/1000, LR 0.000350
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6633 score: 0.5039 time: 0.19s
Test loss: 0.6558 score: 0.5039 time: 0.20s
Epoch 6/1000, LR 0.000450
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6586 score: 0.5039 time: 0.19s
Test loss: 0.6495 score: 0.5039 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.4840;  Loss pred: 0.4840; Loss self: 0.0000; time: 0.29s
Val loss: 0.6459 score: 0.5116 time: 0.19s
Test loss: 0.6334 score: 0.5116 time: 0.21s
Epoch 8/1000, LR 0.000650
Train loss: 0.4430;  Loss pred: 0.4430; Loss self: 0.0000; time: 0.30s
Val loss: 0.6182 score: 0.6047 time: 0.21s
Test loss: 0.5987 score: 0.6202 time: 0.18s
Epoch 9/1000, LR 0.000750
Train loss: 0.3937;  Loss pred: 0.3937; Loss self: 0.0000; time: 0.33s
Val loss: 0.5667 score: 0.7364 time: 0.21s
Test loss: 0.5369 score: 0.8062 time: 0.21s
Epoch 10/1000, LR 0.000850
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 0.30s
Val loss: 0.5057 score: 0.8217 time: 0.20s
Test loss: 0.4698 score: 0.8915 time: 0.19s
Epoch 11/1000, LR 0.000950
Train loss: 0.3018;  Loss pred: 0.3018; Loss self: 0.0000; time: 0.30s
Val loss: 0.4680 score: 0.8682 time: 0.21s
Test loss: 0.4326 score: 0.9225 time: 0.33s
Epoch 12/1000, LR 0.000950
Train loss: 0.2599;  Loss pred: 0.2599; Loss self: 0.0000; time: 0.30s
Val loss: 0.4462 score: 0.8527 time: 0.21s
Test loss: 0.4128 score: 0.9147 time: 0.21s
Epoch 13/1000, LR 0.000950
Train loss: 0.2316;  Loss pred: 0.2316; Loss self: 0.0000; time: 0.30s
Val loss: 0.4408 score: 0.8372 time: 0.21s
Test loss: 0.4082 score: 0.8915 time: 0.21s
Epoch 14/1000, LR 0.000950
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.30s
Val loss: 0.4396 score: 0.8217 time: 0.32s
Test loss: 0.4054 score: 0.8837 time: 0.18s
Epoch 15/1000, LR 0.000950
Train loss: 0.1954;  Loss pred: 0.1954; Loss self: 0.0000; time: 0.29s
Val loss: 0.4398 score: 0.8140 time: 0.21s
Test loss: 0.4015 score: 0.8682 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 16/1000, LR 0.000950
Train loss: 0.1645;  Loss pred: 0.1645; Loss self: 0.0000; time: 0.30s
Val loss: 0.4312 score: 0.8140 time: 0.20s
Test loss: 0.3919 score: 0.8760 time: 0.22s
Epoch 17/1000, LR 0.000950
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 0.43s
Val loss: 0.4196 score: 0.8217 time: 0.19s
Test loss: 0.3805 score: 0.8837 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1328;  Loss pred: 0.1328; Loss self: 0.0000; time: 0.30s
Val loss: 0.4069 score: 0.8450 time: 0.20s
Test loss: 0.3683 score: 0.8915 time: 0.20s
Epoch 19/1000, LR 0.000950
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.29s
Val loss: 0.3979 score: 0.8450 time: 0.32s
Test loss: 0.3579 score: 0.8837 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.29s
Val loss: 0.3903 score: 0.8450 time: 0.19s
Test loss: 0.3487 score: 0.8837 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.31s
Val loss: 0.3828 score: 0.8527 time: 0.21s
Test loss: 0.3403 score: 0.8837 time: 0.18s
Epoch 22/1000, LR 0.000950
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.31s
Val loss: 0.3784 score: 0.8372 time: 0.27s
Test loss: 0.3336 score: 0.8915 time: 0.20s
Epoch 23/1000, LR 0.000950
Train loss: 0.0842;  Loss pred: 0.0842; Loss self: 0.0000; time: 0.31s
Val loss: 0.3732 score: 0.8450 time: 0.20s
Test loss: 0.3271 score: 0.8992 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.29s
Val loss: 0.3684 score: 0.8450 time: 0.21s
Test loss: 0.3204 score: 0.8992 time: 0.21s
Epoch 25/1000, LR 0.000950
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 0.37s
Val loss: 0.3624 score: 0.8450 time: 0.21s
Test loss: 0.3128 score: 0.8992 time: 0.20s
Epoch 26/1000, LR 0.000949
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.30s
Val loss: 0.3588 score: 0.8450 time: 0.22s
Test loss: 0.3062 score: 0.8992 time: 0.20s
Epoch 27/1000, LR 0.000949
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.29s
Val loss: 0.3548 score: 0.8450 time: 0.20s
Test loss: 0.2992 score: 0.8992 time: 0.30s
Epoch 28/1000, LR 0.000949
Train loss: 0.0553;  Loss pred: 0.0553; Loss self: 0.0000; time: 0.29s
Val loss: 0.3480 score: 0.8527 time: 0.21s
Test loss: 0.2910 score: 0.8992 time: 0.20s
Epoch 29/1000, LR 0.000949
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.30s
Val loss: 0.3407 score: 0.8527 time: 0.20s
Test loss: 0.2823 score: 0.8992 time: 0.20s
Epoch 30/1000, LR 0.000949
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.30s
Val loss: 0.3341 score: 0.8527 time: 0.32s
Test loss: 0.2743 score: 0.8992 time: 0.19s
Epoch 31/1000, LR 0.000949
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.28s
Val loss: 0.3284 score: 0.8527 time: 0.20s
Test loss: 0.2665 score: 0.8992 time: 0.21s
Epoch 32/1000, LR 0.000949
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.31s
Val loss: 0.3220 score: 0.8527 time: 0.20s
Test loss: 0.2581 score: 0.8992 time: 0.21s
Epoch 33/1000, LR 0.000949
Train loss: 0.0443;  Loss pred: 0.0443; Loss self: 0.0000; time: 0.42s
Val loss: 0.3196 score: 0.8605 time: 0.20s
Test loss: 0.2526 score: 0.8992 time: 0.20s
Epoch 34/1000, LR 0.000949
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.31s
Val loss: 0.3167 score: 0.8605 time: 0.21s
Test loss: 0.2465 score: 0.8992 time: 0.20s
Epoch 35/1000, LR 0.000949
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.31s
Val loss: 0.3160 score: 0.8605 time: 0.21s
Test loss: 0.2421 score: 0.8992 time: 0.30s
Epoch 36/1000, LR 0.000949
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.29s
Val loss: 0.3152 score: 0.8682 time: 0.19s
Test loss: 0.2371 score: 0.8992 time: 0.19s
Epoch 37/1000, LR 0.000948
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.31s
Val loss: 0.3161 score: 0.8682 time: 0.20s
Test loss: 0.2331 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 38/1000, LR 0.000948
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.30s
Val loss: 0.3148 score: 0.8605 time: 0.21s
Test loss: 0.2271 score: 0.8992 time: 0.33s
Epoch 39/1000, LR 0.000948
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.31s
Val loss: 0.3147 score: 0.8605 time: 0.21s
Test loss: 0.2224 score: 0.8992 time: 0.21s
Epoch 40/1000, LR 0.000948
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.30s
Val loss: 0.3139 score: 0.8605 time: 0.20s
Test loss: 0.2178 score: 0.8992 time: 0.22s
Epoch 41/1000, LR 0.000948
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.39s
Val loss: 0.3100 score: 0.8605 time: 0.20s
Test loss: 0.2118 score: 0.9070 time: 0.20s
Epoch 42/1000, LR 0.000948
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.30s
Val loss: 0.3042 score: 0.8760 time: 0.21s
Test loss: 0.2054 score: 0.9070 time: 0.20s
Epoch 43/1000, LR 0.000948
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.30s
Val loss: 0.2973 score: 0.8760 time: 0.21s
Test loss: 0.1981 score: 0.9070 time: 0.19s
Epoch 44/1000, LR 0.000947
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.34s
Val loss: 0.2874 score: 0.8915 time: 0.19s
Test loss: 0.1888 score: 0.9070 time: 0.19s
Epoch 45/1000, LR 0.000947
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.29s
Val loss: 0.2830 score: 0.8992 time: 0.19s
Test loss: 0.1839 score: 0.9225 time: 0.20s
Epoch 46/1000, LR 0.000947
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.29s
Val loss: 0.2887 score: 0.8915 time: 0.20s
Test loss: 0.1865 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 47/1000, LR 0.000947
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.43s
Val loss: 0.2939 score: 0.8837 time: 0.21s
Test loss: 0.1880 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 48/1000, LR 0.000947
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.29s
Val loss: 0.2970 score: 0.8837 time: 0.20s
Test loss: 0.1863 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 3 of 5
Epoch 49/1000, LR 0.000947
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.31s
Val loss: 0.3001 score: 0.8837 time: 0.21s
Test loss: 0.1850 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 4 of 5
Epoch 50/1000, LR 0.000946
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.29s
Val loss: 0.3021 score: 0.8837 time: 0.20s
Test loss: 0.1834 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.0250,   Val_Loss: 0.2830,   Val_Precision: 0.9474,   Val_Recall: 0.8438,   Val_accuracy: 0.8926,   Val_Score: 0.8992,   Val_Loss: 0.2830,   Test_Precision: 0.9825,   Test_Recall: 0.8615,   Test_accuracy: 0.9180,   Test_Score: 0.9225,   Test_loss: 0.1839


[0.21934950887225568, 0.3556077100802213, 0.19749797694385052, 0.2203993801958859, 0.20924732903949916, 0.1957195249851793, 0.21020821295678616, 0.19802475394681096, 0.2101310680154711, 0.19698996003717184, 0.3300489392131567, 0.21392571297474205, 0.21581591898575425, 0.1863535949960351, 0.20047892187722027, 0.2241068088915199, 0.20861862297169864, 0.20065868203528225, 0.19481684104539454, 0.19778676191344857, 0.18738014600239694, 0.2091626888141036, 0.1943671042099595, 0.21234081406146288, 0.20372195006348193, 0.2073023021221161, 0.30001906608231366, 0.20419084909372032, 0.20504064811393619, 0.19254050101153553, 0.21229621209204197, 0.21679881401360035, 0.20065336814150214, 0.2074086619541049, 0.3082633160520345, 0.190815893933177, 0.2069251360371709, 0.33045397186651826, 0.2162710700649768, 0.22289029089733958, 0.20470430701971054, 0.2004683050327003, 0.1995248650200665, 0.1948091508820653, 0.2000659469049424, 0.18638528604060411, 0.19242495205253363, 0.2240958649199456, 0.21554544800892472, 0.1967960421461612]
[0.0017003837897074084, 0.0027566489153505527, 0.0015309920693321744, 0.0017085223270998907, 0.0016220723181356524, 0.0015172056200401496, 0.0016295210306727609, 0.0015350756119907826, 0.001628923007871869, 0.0015270539537765259, 0.0025585189086291217, 0.0016583388602693183, 0.0016729916200446066, 0.0014446015115971713, 0.001554100169590855, 0.0017372620844303868, 0.0016171986276875864, 0.0015554936591882346, 0.0015102080701193375, 0.0015332307125073533, 0.0014525592713364103, 0.0016214161923573923, 0.0015067217380617015, 0.0016460528221818828, 0.001579239922972728, 0.0016069945900939232, 0.00232572919443654, 0.0015828747991761265, 0.0015894623884801255, 0.0014925620233452366, 0.0016457070704809455, 0.001680610961345739, 0.001555452466213195, 0.001607819084915542, 0.0023896381089305, 0.0014791929762261783, 0.0016040708219935728, 0.0025616586966396766, 0.0016765199229843164, 0.0017278317123824774, 0.0015868550931760507, 0.001554017868470545, 0.0015467043800005155, 0.0015101484564501186, 0.0015508988132166078, 0.0014448471786093343, 0.0014916662949808809, 0.0017371772474414388, 0.001670894945805618, 0.0015255507143113272]
[588.1025248847343, 362.7592887986005, 653.1712476056159, 585.3011015064914, 616.495324418927, 659.106443313555, 613.6772592539919, 651.4337093162056, 613.9025571911253, 654.8557092740047, 390.85112743443017, 603.01306563943, 597.731625202843, 692.2324197863993, 643.4591666399909, 575.6183876699754, 618.3532331027812, 642.8827234961987, 662.1604133799784, 652.2175637642036, 688.4400655678315, 616.7447967483848, 663.69255499455, 607.5139184625168, 633.2160081905864, 622.2796306623245, 429.9726736853696, 631.7619059451144, 629.1435439099753, 669.98890790396, 607.641552945238, 595.021705201337, 642.8997489293492, 621.9605236571313, 418.47340660613975, 676.0443134007239, 623.4138706900603, 390.3720668611226, 596.4736751949442, 578.7600683755927, 630.1772633810722, 643.4932443757509, 646.5359592501229, 662.1865524073598, 644.7873913359775, 692.114719677482, 670.3912284971334, 575.6464986361218, 598.4816714601122, 655.5009876885186]
Elapsed: 0.21658898405265062~0.03786183096670385
Time per graph: 0.0016789843725011676~0.00029350256563336313
Speed: 608.8091069264277~76.47693569938383
Total Time: 0.1972
best val loss: 0.28304713824402916 test_score: 0.9225

Testing...
Test loss: 0.1839 score: 0.9225 time: 0.22s
test Score 0.9225
Epoch Time List: [0.9633196871727705, 0.8566829261835665, 0.6898946990258992, 0.7035902021452785, 0.6939213220030069, 0.7361981570720673, 0.6923042249400169, 0.6863790040370077, 0.7415371900424361, 0.6919508019927889, 0.8353804799262434, 0.7134221431333572, 0.7157405321486294, 0.8023197455331683, 0.6884121710900217, 0.7141581922769547, 0.8175767259672284, 0.697363682789728, 0.7931031358893961, 0.6696588890627027, 0.6994294810574502, 0.7811993830837309, 0.6942601772025228, 0.7072781440801919, 0.7806083792820573, 0.7202223972417414, 0.7870884470175952, 0.7057592400815338, 0.695019745035097, 0.8071021719370037, 0.6872150218114257, 0.7218198331538588, 0.8232038340065628, 0.7143342043273151, 0.8224438750185072, 0.6637586050201207, 0.7104510699864477, 0.8436439340002835, 0.733891072217375, 0.7243493020068854, 0.7918908349238336, 0.7106469301506877, 0.6991014308296144, 0.724667162867263, 0.6764928109478205, 0.6711688619107008, 0.8253422968555242, 0.7058475448284298, 0.7306763210799545, 0.6856995769776404]
Total Epoch List: [50]
Total Time List: [0.1971973991021514]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e25b580>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7658;  Loss pred: 0.7658; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7874 score: 0.5039 time: 0.21s
Test loss: 0.7959 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000050
Train loss: 0.7240;  Loss pred: 0.7240; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7535 score: 0.5039 time: 0.23s
Test loss: 0.7616 score: 0.4884 time: 0.22s
Epoch 3/1000, LR 0.000150
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.5039 time: 0.22s
Test loss: 0.7087 score: 0.4806 time: 0.21s
Epoch 4/1000, LR 0.000250
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.29s
Val loss: 0.6489 score: 0.6434 time: 0.22s
Test loss: 0.6569 score: 0.5736 time: 0.21s
Epoch 5/1000, LR 0.000350
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.29s
Val loss: 0.6175 score: 0.7442 time: 0.21s
Test loss: 0.6271 score: 0.6744 time: 0.21s
Epoch 6/1000, LR 0.000450
Train loss: 0.5060;  Loss pred: 0.5060; Loss self: 0.0000; time: 0.30s
Val loss: 0.6076 score: 0.5271 time: 0.23s
Test loss: 0.6203 score: 0.5736 time: 0.22s
Epoch 7/1000, LR 0.000550
Train loss: 0.4673;  Loss pred: 0.4673; Loss self: 0.0000; time: 0.29s
Val loss: 0.5845 score: 0.5736 time: 0.21s
Test loss: 0.6038 score: 0.5659 time: 0.20s
Epoch 8/1000, LR 0.000650
Train loss: 0.4358;  Loss pred: 0.4358; Loss self: 0.0000; time: 0.28s
Val loss: 0.5422 score: 0.7519 time: 0.20s
Test loss: 0.5668 score: 0.6822 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.3749;  Loss pred: 0.3749; Loss self: 0.0000; time: 0.28s
Val loss: 0.4982 score: 0.8450 time: 0.20s
Test loss: 0.5225 score: 0.7364 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3287;  Loss pred: 0.3287; Loss self: 0.0000; time: 0.28s
Val loss: 0.4673 score: 0.9070 time: 0.20s
Test loss: 0.4912 score: 0.8915 time: 0.19s
Epoch 11/1000, LR 0.000950
Train loss: 0.3009;  Loss pred: 0.3009; Loss self: 0.0000; time: 0.28s
Val loss: 0.4619 score: 0.9457 time: 0.20s
Test loss: 0.4861 score: 0.9070 time: 0.19s
Epoch 12/1000, LR 0.000950
Train loss: 0.2927;  Loss pred: 0.2927; Loss self: 0.0000; time: 0.28s
Val loss: 0.4586 score: 0.9147 time: 0.20s
Test loss: 0.4832 score: 0.8605 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 0.28s
Val loss: 0.4521 score: 0.8682 time: 0.20s
Test loss: 0.4770 score: 0.8217 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.28s
Val loss: 0.4498 score: 0.8140 time: 0.20s
Test loss: 0.4766 score: 0.8217 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 0.28s
Val loss: 0.4338 score: 0.8217 time: 0.20s
Test loss: 0.4609 score: 0.8295 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.2103;  Loss pred: 0.2103; Loss self: 0.0000; time: 0.28s
Val loss: 0.4142 score: 0.8760 time: 0.20s
Test loss: 0.4408 score: 0.8372 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.1894;  Loss pred: 0.1894; Loss self: 0.0000; time: 0.28s
Val loss: 0.3942 score: 0.8915 time: 0.19s
Test loss: 0.4212 score: 0.8605 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1751;  Loss pred: 0.1751; Loss self: 0.0000; time: 0.28s
Val loss: 0.3847 score: 0.8915 time: 0.20s
Test loss: 0.4129 score: 0.8605 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.28s
Val loss: 0.3706 score: 0.8915 time: 0.21s
Test loss: 0.3989 score: 0.8760 time: 0.20s
Epoch 20/1000, LR 0.000950
Train loss: 0.1590;  Loss pred: 0.1590; Loss self: 0.0000; time: 0.29s
Val loss: 0.3572 score: 0.9147 time: 0.24s
Test loss: 0.3850 score: 0.8992 time: 0.21s
Epoch 21/1000, LR 0.000950
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.28s
Val loss: 0.3444 score: 0.9302 time: 0.20s
Test loss: 0.3716 score: 0.9070 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.28s
Val loss: 0.3288 score: 0.9457 time: 0.20s
Test loss: 0.3535 score: 0.9225 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1087;  Loss pred: 0.1087; Loss self: 0.0000; time: 0.28s
Val loss: 0.3226 score: 0.9380 time: 0.20s
Test loss: 0.3483 score: 0.9225 time: 0.21s
Epoch 24/1000, LR 0.000950
Train loss: 0.1183;  Loss pred: 0.1183; Loss self: 0.0000; time: 0.35s
Val loss: 0.3182 score: 0.9302 time: 0.20s
Test loss: 0.3453 score: 0.9225 time: 0.33s
Epoch 25/1000, LR 0.000950
Train loss: 0.1168;  Loss pred: 0.1168; Loss self: 0.0000; time: 0.29s
Val loss: 0.3106 score: 0.9302 time: 0.21s
Test loss: 0.3376 score: 0.9147 time: 0.20s
Epoch 26/1000, LR 0.000949
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.30s
Val loss: 0.3084 score: 0.9302 time: 0.21s
Test loss: 0.3380 score: 0.9070 time: 0.21s
Epoch 27/1000, LR 0.000949
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.30s
Val loss: 0.3047 score: 0.9147 time: 0.32s
Test loss: 0.3360 score: 0.8992 time: 0.22s
Epoch 28/1000, LR 0.000949
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.30s
Val loss: 0.2946 score: 0.9302 time: 0.24s
Test loss: 0.3260 score: 0.9147 time: 0.22s
Epoch 29/1000, LR 0.000949
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.29s
Val loss: 0.2849 score: 0.9302 time: 0.21s
Test loss: 0.3162 score: 0.9225 time: 0.21s
Epoch 30/1000, LR 0.000949
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 0.41s
Val loss: 0.2908 score: 0.9147 time: 0.21s
Test loss: 0.3254 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 31/1000, LR 0.000949
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.29s
Val loss: 0.2771 score: 0.9225 time: 0.22s
Test loss: 0.3104 score: 0.9070 time: 0.21s
Epoch 32/1000, LR 0.000949
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 0.29s
Val loss: 0.2696 score: 0.9302 time: 0.21s
Test loss: 0.3034 score: 0.9147 time: 0.33s
Epoch 33/1000, LR 0.000949
Train loss: 0.0974;  Loss pred: 0.0974; Loss self: 0.0000; time: 0.29s
Val loss: 0.2699 score: 0.9147 time: 0.21s
Test loss: 0.3063 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 1 of 5
Epoch 34/1000, LR 0.000949
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.30s
Val loss: 0.2623 score: 0.9147 time: 0.24s
Test loss: 0.2997 score: 0.8915 time: 0.22s
Epoch 35/1000, LR 0.000949
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.29s
Val loss: 0.2476 score: 0.9302 time: 0.33s
Test loss: 0.2833 score: 0.9225 time: 0.20s
Epoch 36/1000, LR 0.000949
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.29s
Val loss: 0.2363 score: 0.9457 time: 0.21s
Test loss: 0.2703 score: 0.9302 time: 0.20s
Epoch 37/1000, LR 0.000948
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.30s
Val loss: 0.2303 score: 0.9457 time: 0.22s
Test loss: 0.2657 score: 0.9302 time: 0.21s
Epoch 38/1000, LR 0.000948
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.39s
Val loss: 0.2258 score: 0.9535 time: 0.22s
Test loss: 0.2638 score: 0.9302 time: 0.21s
Epoch 39/1000, LR 0.000948
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.30s
Val loss: 0.2194 score: 0.9535 time: 0.22s
Test loss: 0.2601 score: 0.9302 time: 0.21s
Epoch 40/1000, LR 0.000948
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.30s
Val loss: 0.2144 score: 0.9380 time: 0.33s
Test loss: 0.2597 score: 0.9302 time: 0.20s
Epoch 41/1000, LR 0.000948
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.30s
Val loss: 0.2088 score: 0.9380 time: 0.22s
Test loss: 0.2587 score: 0.9225 time: 0.22s
Epoch 42/1000, LR 0.000948
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.30s
Val loss: 0.1994 score: 0.9380 time: 0.21s
Test loss: 0.2521 score: 0.9225 time: 0.33s
Epoch 43/1000, LR 0.000948
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.40s
Val loss: 0.1905 score: 0.9457 time: 0.19s
Test loss: 0.2460 score: 0.9302 time: 0.19s
Epoch 44/1000, LR 0.000947
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.29s
Val loss: 0.1835 score: 0.9457 time: 0.20s
Test loss: 0.2418 score: 0.9302 time: 0.19s
Epoch 45/1000, LR 0.000947
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.29s
Val loss: 0.1770 score: 0.9457 time: 0.22s
Test loss: 0.2395 score: 0.9380 time: 0.33s
Epoch 46/1000, LR 0.000947
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.28s
Val loss: 0.1740 score: 0.9380 time: 0.21s
Test loss: 0.2399 score: 0.9302 time: 0.20s
Epoch 47/1000, LR 0.000947
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.30s
Val loss: 0.1727 score: 0.9380 time: 0.21s
Test loss: 0.2416 score: 0.9302 time: 0.20s
Epoch 48/1000, LR 0.000947
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.30s
Val loss: 0.1718 score: 0.9302 time: 0.28s
Test loss: 0.2429 score: 0.9225 time: 0.21s
Epoch 49/1000, LR 0.000947
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.28s
Val loss: 0.1606 score: 0.9457 time: 0.20s
Test loss: 0.2351 score: 0.9302 time: 0.19s
Epoch 50/1000, LR 0.000946
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.29s
Val loss: 0.1693 score: 0.9457 time: 0.20s
Test loss: 0.2463 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 51/1000, LR 0.000946
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.40s
Val loss: 0.1694 score: 0.9535 time: 0.21s
Test loss: 0.2516 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 2 of 5
Epoch 52/1000, LR 0.000946
Train loss: 0.0443;  Loss pred: 0.0443; Loss self: 0.0000; time: 0.31s
Val loss: 0.1690 score: 0.9535 time: 0.21s
Test loss: 0.2569 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 5
Epoch 53/1000, LR 0.000946
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.31s
Val loss: 0.1680 score: 0.9535 time: 0.20s
Test loss: 0.2597 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 4 of 5
Epoch 54/1000, LR 0.000946
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.42s
Val loss: 0.1645 score: 0.9535 time: 0.22s
Test loss: 0.2582 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0548,   Val_Loss: 0.1606,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.1606,   Test_Precision: 0.9825,   Test_Recall: 0.8750,   Test_accuracy: 0.9256,   Test_Score: 0.9302,   Test_loss: 0.2351


[0.21934950887225568, 0.3556077100802213, 0.19749797694385052, 0.2203993801958859, 0.20924732903949916, 0.1957195249851793, 0.21020821295678616, 0.19802475394681096, 0.2101310680154711, 0.19698996003717184, 0.3300489392131567, 0.21392571297474205, 0.21581591898575425, 0.1863535949960351, 0.20047892187722027, 0.2241068088915199, 0.20861862297169864, 0.20065868203528225, 0.19481684104539454, 0.19778676191344857, 0.18738014600239694, 0.2091626888141036, 0.1943671042099595, 0.21234081406146288, 0.20372195006348193, 0.2073023021221161, 0.30001906608231366, 0.20419084909372032, 0.20504064811393619, 0.19254050101153553, 0.21229621209204197, 0.21679881401360035, 0.20065336814150214, 0.2074086619541049, 0.3082633160520345, 0.190815893933177, 0.2069251360371709, 0.33045397186651826, 0.2162710700649768, 0.22289029089733958, 0.20470430701971054, 0.2004683050327003, 0.1995248650200665, 0.1948091508820653, 0.2000659469049424, 0.18638528604060411, 0.19242495205253363, 0.2240958649199456, 0.21554544800892472, 0.1967960421461612, 0.21186610986478627, 0.22885323292575777, 0.2158371009863913, 0.2127767838537693, 0.21315228915773332, 0.22986257006414235, 0.2052972218953073, 0.1956378559116274, 0.19646922289393842, 0.1969327221158892, 0.19593358295969665, 0.19614487304352224, 0.19587385398335755, 0.19617709587328136, 0.19637904688715935, 0.19629303994588554, 0.1958190279547125, 0.1963834159541875, 0.20771174412220716, 0.21080852090381086, 0.1954146830830723, 0.19375924114137888, 0.21183163300156593, 0.3308905081357807, 0.20950887398794293, 0.21149985399097204, 0.22464631008915603, 0.22143791406415403, 0.21353389509022236, 0.20935157896019518, 0.21185293490998447, 0.33847026992589235, 0.21536736702546477, 0.22373202885501087, 0.2063499588984996, 0.2060990110039711, 0.2172898189164698, 0.21376057504676282, 0.21162016806192696, 0.20259895594790578, 0.22335140989162028, 0.3337556840851903, 0.19216337287798524, 0.19803481712006032, 0.3322154579218477, 0.20209807599894702, 0.20599201088771224, 0.21489711105823517, 0.194691542070359, 0.2013651761226356, 0.21184167405590415, 0.20609449897892773, 0.20856399298645556, 0.21578336600214243]
[0.0017003837897074084, 0.0027566489153505527, 0.0015309920693321744, 0.0017085223270998907, 0.0016220723181356524, 0.0015172056200401496, 0.0016295210306727609, 0.0015350756119907826, 0.001628923007871869, 0.0015270539537765259, 0.0025585189086291217, 0.0016583388602693183, 0.0016729916200446066, 0.0014446015115971713, 0.001554100169590855, 0.0017372620844303868, 0.0016171986276875864, 0.0015554936591882346, 0.0015102080701193375, 0.0015332307125073533, 0.0014525592713364103, 0.0016214161923573923, 0.0015067217380617015, 0.0016460528221818828, 0.001579239922972728, 0.0016069945900939232, 0.00232572919443654, 0.0015828747991761265, 0.0015894623884801255, 0.0014925620233452366, 0.0016457070704809455, 0.001680610961345739, 0.001555452466213195, 0.001607819084915542, 0.0023896381089305, 0.0014791929762261783, 0.0016040708219935728, 0.0025616586966396766, 0.0016765199229843164, 0.0017278317123824774, 0.0015868550931760507, 0.001554017868470545, 0.0015467043800005155, 0.0015101484564501186, 0.0015508988132166078, 0.0014448471786093343, 0.0014916662949808809, 0.0017371772474414388, 0.001670894945805618, 0.0015255507143113272, 0.0016423729446882656, 0.0017740560691919206, 0.0016731558215999326, 0.0016494324329749557, 0.0016523433268041342, 0.0017818803880941267, 0.001591451332521762, 0.0015165725264467242, 0.0015230172317359568, 0.0015266102489603812, 0.001518864984183695, 0.00152050289181025, 0.0015184019688632368, 0.0015207526811882276, 0.0015223181929237158, 0.001521651472448725, 0.0015179769608892443, 0.0015223520616603683, 0.0016101685590868772, 0.0016341745806496967, 0.0015148425045199403, 0.001502009621250999, 0.0016421056821826817, 0.002565042698726982, 0.0016240997983561468, 0.0016395337518680002, 0.0017414442642570236, 0.001716572977241504, 0.001655301512327305, 0.0016228804570557766, 0.0016422708132556936, 0.002623800542061181, 0.0016695144730656183, 0.0017343568128295416, 0.001599612084484493, 0.0015976667519687683, 0.0016844172009028667, 0.001657058721292735, 0.001640466419084705, 0.0015705345422318277, 0.0017314062782296145, 0.002587253365001475, 0.001489638549441746, 0.0015351536210857389, 0.002575313627301145, 0.0015666517519298219, 0.0015968372937031957, 0.0016658690779708153, 0.001509236760235341, 0.001560970357539811, 0.0016421835198132105, 0.0015976317750304476, 0.001616775139429888, 0.0016727392713344375]
[588.1025248847343, 362.7592887986005, 653.1712476056159, 585.3011015064914, 616.495324418927, 659.106443313555, 613.6772592539919, 651.4337093162056, 613.9025571911253, 654.8557092740047, 390.85112743443017, 603.01306563943, 597.731625202843, 692.2324197863993, 643.4591666399909, 575.6183876699754, 618.3532331027812, 642.8827234961987, 662.1604133799784, 652.2175637642036, 688.4400655678315, 616.7447967483848, 663.69255499455, 607.5139184625168, 633.2160081905864, 622.2796306623245, 429.9726736853696, 631.7619059451144, 629.1435439099753, 669.98890790396, 607.641552945238, 595.021705201337, 642.8997489293492, 621.9605236571313, 418.47340660613975, 676.0443134007239, 623.4138706900603, 390.3720668611226, 596.4736751949442, 578.7600683755927, 630.1772633810722, 643.4932443757509, 646.5359592501229, 662.1865524073598, 644.7873913359775, 692.114719677482, 670.3912284971334, 575.6464986361218, 598.4816714601122, 655.5009876885186, 608.8751055198412, 563.680042229724, 597.6729645202821, 606.2691505322083, 605.2011006296987, 561.2048971870584, 628.3572608000727, 659.3815874687937, 656.5913892255741, 655.0460411758653, 658.3863677240832, 657.6771444409685, 658.5871333851454, 657.5691184832621, 656.8928918069565, 657.1807132619831, 658.7715266865389, 656.8782774920935, 621.0529912266437, 611.9297239358792, 660.1346324890085, 665.7746966807819, 608.9742035791528, 389.8570579336925, 615.7257091049224, 609.9294990790227, 574.2360065865466, 582.5560656366492, 604.1195471355726, 616.1883308485926, 608.9129709475661, 381.12653152149636, 598.9765384685563, 576.5826227929047, 625.1515662450562, 625.9127560661339, 593.6771480747101, 603.4789154725077, 609.5827310856798, 636.7258873395663, 577.565192279719, 386.51027128896203, 671.3037873346914, 651.4006066003668, 388.3022205136123, 638.3039490226128, 626.2378790521096, 600.2872694042042, 662.5865645122944, 640.6271555188683, 608.9453388947325, 625.9264591685666, 618.5152007920055, 597.8217987327124]
Elapsed: 0.21666848350119275~0.0361416170832196
Time per graph: 0.0016796006472960677~0.000280167574288524
Speed: 607.3040181178619~72.33222107176675
Total Time: 0.2165
best val loss: 0.160581700846755 test_score: 0.9302

Testing...
Test loss: 0.2638 score: 0.9302 time: 0.21s
test Score 0.9302
Epoch Time List: [0.9633196871727705, 0.8566829261835665, 0.6898946990258992, 0.7035902021452785, 0.6939213220030069, 0.7361981570720673, 0.6923042249400169, 0.6863790040370077, 0.7415371900424361, 0.6919508019927889, 0.8353804799262434, 0.7134221431333572, 0.7157405321486294, 0.8023197455331683, 0.6884121710900217, 0.7141581922769547, 0.8175767259672284, 0.697363682789728, 0.7931031358893961, 0.6696588890627027, 0.6994294810574502, 0.7811993830837309, 0.6942601772025228, 0.7072781440801919, 0.7806083792820573, 0.7202223972417414, 0.7870884470175952, 0.7057592400815338, 0.695019745035097, 0.8071021719370037, 0.6872150218114257, 0.7218198331538588, 0.8232038340065628, 0.7143342043273151, 0.8224438750185072, 0.6637586050201207, 0.7104510699864477, 0.8436439340002835, 0.733891072217375, 0.7243493020068854, 0.7918908349238336, 0.7106469301506877, 0.6991014308296144, 0.724667162867263, 0.6764928109478205, 0.6711688619107008, 0.8253422968555242, 0.7058475448284298, 0.7306763210799545, 0.6856995769776404, 0.705704147927463, 0.7498937181662768, 0.7309182421304286, 0.714408848900348, 0.711545562138781, 0.7547602518461645, 0.7062202817760408, 0.667845078278333, 0.6691687528509647, 0.6663108959328383, 0.6687328252010047, 0.6660393220372498, 0.6626829337328672, 0.6675861971452832, 0.6668112480547279, 0.6682074989657849, 0.665198682108894, 0.6707297849934548, 0.6935054678469896, 0.7310971710830927, 0.6718706851825118, 0.6665065148845315, 0.6926947920583189, 0.8758322519715875, 0.7111909841187298, 0.7169025491457433, 0.8337544500827789, 0.7522669089958072, 0.7065001998562366, 0.8238765017595142, 0.7223708191886544, 0.8372035040520132, 0.7124143368564546, 0.7557153250090778, 0.8276831139810383, 0.6982606078963727, 0.7255656949710101, 0.8140221161302179, 0.7207674537785351, 0.8258533908519894, 0.7356021236628294, 0.8362179910764098, 0.7771339493338019, 0.6792977491859347, 0.8285343451425433, 0.6880570261273533, 0.7114529912360013, 0.7894552785437554, 0.6693937790114433, 0.6870378674939275, 0.8243623971939087, 0.7208647709339857, 0.7066925142426044, 0.8479917661752552]
Total Epoch List: [50, 54]
Total Time List: [0.1971973991021514, 0.216513680992648]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e40e860>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.8030;  Loss pred: 0.8030; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7444 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7391 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000067
Train loss: 0.7948;  Loss pred: 0.7948; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7281 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7236 score: 0.5000 time: 0.18s
Epoch 3/1000, LR 0.000167
Train loss: 0.7656;  Loss pred: 0.7656; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.17s
Epoch 4/1000, LR 0.000267
Train loss: 0.7117;  Loss pred: 0.7117; Loss self: 0.0000; time: 0.38s
Val loss: 0.6524 score: 0.5039 time: 0.30s
Test loss: 0.6534 score: 0.5156 time: 0.20s
Epoch 5/1000, LR 0.000367
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.38s
Val loss: 0.6238 score: 0.8837 time: 0.19s
Test loss: 0.6300 score: 0.8984 time: 0.17s
Epoch 6/1000, LR 0.000467
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.38s
Val loss: 0.6275 score: 0.5969 time: 0.18s
Test loss: 0.6395 score: 0.5859 time: 0.16s
     INFO: Early stopping counter 1 of 5
Epoch 7/1000, LR 0.000567
Train loss: 0.5288;  Loss pred: 0.5288; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6611 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 5
Epoch 8/1000, LR 0.000667
Train loss: 0.5007;  Loss pred: 0.5007; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7057 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 5
Epoch 9/1000, LR 0.000767
Train loss: 0.4548;  Loss pred: 0.4548; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7285 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7404 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 4 of 5
Epoch 10/1000, LR 0.000867
Train loss: 0.4290;  Loss pred: 0.4290; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7515 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7681 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 004,   Train_Loss: 0.6359,   Val_Loss: 0.6238,   Val_Precision: 1.0000,   Val_Recall: 0.7692,   Val_accuracy: 0.8696,   Val_Score: 0.8837,   Val_Loss: 0.6238,   Test_Precision: 0.9636,   Test_Recall: 0.8281,   Test_accuracy: 0.8908,   Test_Score: 0.8984,   Test_loss: 0.6300


[0.21934950887225568, 0.3556077100802213, 0.19749797694385052, 0.2203993801958859, 0.20924732903949916, 0.1957195249851793, 0.21020821295678616, 0.19802475394681096, 0.2101310680154711, 0.19698996003717184, 0.3300489392131567, 0.21392571297474205, 0.21581591898575425, 0.1863535949960351, 0.20047892187722027, 0.2241068088915199, 0.20861862297169864, 0.20065868203528225, 0.19481684104539454, 0.19778676191344857, 0.18738014600239694, 0.2091626888141036, 0.1943671042099595, 0.21234081406146288, 0.20372195006348193, 0.2073023021221161, 0.30001906608231366, 0.20419084909372032, 0.20504064811393619, 0.19254050101153553, 0.21229621209204197, 0.21679881401360035, 0.20065336814150214, 0.2074086619541049, 0.3082633160520345, 0.190815893933177, 0.2069251360371709, 0.33045397186651826, 0.2162710700649768, 0.22289029089733958, 0.20470430701971054, 0.2004683050327003, 0.1995248650200665, 0.1948091508820653, 0.2000659469049424, 0.18638528604060411, 0.19242495205253363, 0.2240958649199456, 0.21554544800892472, 0.1967960421461612, 0.21186610986478627, 0.22885323292575777, 0.2158371009863913, 0.2127767838537693, 0.21315228915773332, 0.22986257006414235, 0.2052972218953073, 0.1956378559116274, 0.19646922289393842, 0.1969327221158892, 0.19593358295969665, 0.19614487304352224, 0.19587385398335755, 0.19617709587328136, 0.19637904688715935, 0.19629303994588554, 0.1958190279547125, 0.1963834159541875, 0.20771174412220716, 0.21080852090381086, 0.1954146830830723, 0.19375924114137888, 0.21183163300156593, 0.3308905081357807, 0.20950887398794293, 0.21149985399097204, 0.22464631008915603, 0.22143791406415403, 0.21353389509022236, 0.20935157896019518, 0.21185293490998447, 0.33847026992589235, 0.21536736702546477, 0.22373202885501087, 0.2063499588984996, 0.2060990110039711, 0.2172898189164698, 0.21376057504676282, 0.21162016806192696, 0.20259895594790578, 0.22335140989162028, 0.3337556840851903, 0.19216337287798524, 0.19803481712006032, 0.3322154579218477, 0.20209807599894702, 0.20599201088771224, 0.21489711105823517, 0.194691542070359, 0.2013651761226356, 0.21184167405590415, 0.20609449897892773, 0.20856399298645556, 0.21578336600214243, 0.17423837818205357, 0.18444953090511262, 0.17472240002825856, 0.20136659196577966, 0.17321412288583815, 0.16993157914839685, 0.17206250596791506, 0.17206767806783319, 0.18914765282534063, 0.18369747581891716]
[0.0017003837897074084, 0.0027566489153505527, 0.0015309920693321744, 0.0017085223270998907, 0.0016220723181356524, 0.0015172056200401496, 0.0016295210306727609, 0.0015350756119907826, 0.001628923007871869, 0.0015270539537765259, 0.0025585189086291217, 0.0016583388602693183, 0.0016729916200446066, 0.0014446015115971713, 0.001554100169590855, 0.0017372620844303868, 0.0016171986276875864, 0.0015554936591882346, 0.0015102080701193375, 0.0015332307125073533, 0.0014525592713364103, 0.0016214161923573923, 0.0015067217380617015, 0.0016460528221818828, 0.001579239922972728, 0.0016069945900939232, 0.00232572919443654, 0.0015828747991761265, 0.0015894623884801255, 0.0014925620233452366, 0.0016457070704809455, 0.001680610961345739, 0.001555452466213195, 0.001607819084915542, 0.0023896381089305, 0.0014791929762261783, 0.0016040708219935728, 0.0025616586966396766, 0.0016765199229843164, 0.0017278317123824774, 0.0015868550931760507, 0.001554017868470545, 0.0015467043800005155, 0.0015101484564501186, 0.0015508988132166078, 0.0014448471786093343, 0.0014916662949808809, 0.0017371772474414388, 0.001670894945805618, 0.0015255507143113272, 0.0016423729446882656, 0.0017740560691919206, 0.0016731558215999326, 0.0016494324329749557, 0.0016523433268041342, 0.0017818803880941267, 0.001591451332521762, 0.0015165725264467242, 0.0015230172317359568, 0.0015266102489603812, 0.001518864984183695, 0.00152050289181025, 0.0015184019688632368, 0.0015207526811882276, 0.0015223181929237158, 0.001521651472448725, 0.0015179769608892443, 0.0015223520616603683, 0.0016101685590868772, 0.0016341745806496967, 0.0015148425045199403, 0.001502009621250999, 0.0016421056821826817, 0.002565042698726982, 0.0016240997983561468, 0.0016395337518680002, 0.0017414442642570236, 0.001716572977241504, 0.001655301512327305, 0.0016228804570557766, 0.0016422708132556936, 0.002623800542061181, 0.0016695144730656183, 0.0017343568128295416, 0.001599612084484493, 0.0015976667519687683, 0.0016844172009028667, 0.001657058721292735, 0.001640466419084705, 0.0015705345422318277, 0.0017314062782296145, 0.002587253365001475, 0.001489638549441746, 0.0015351536210857389, 0.002575313627301145, 0.0015666517519298219, 0.0015968372937031957, 0.0016658690779708153, 0.001509236760235341, 0.001560970357539811, 0.0016421835198132105, 0.0015976317750304476, 0.001616775139429888, 0.0016727392713344375, 0.0013612373295472935, 0.0014410119601961924, 0.00136501875022077, 0.0015731764997326536, 0.0013532353350456106, 0.0013275904620968504, 0.0013442383278743364, 0.0013442787349049468, 0.0014777160376979737, 0.0014351365298352903]
[588.1025248847343, 362.7592887986005, 653.1712476056159, 585.3011015064914, 616.495324418927, 659.106443313555, 613.6772592539919, 651.4337093162056, 613.9025571911253, 654.8557092740047, 390.85112743443017, 603.01306563943, 597.731625202843, 692.2324197863993, 643.4591666399909, 575.6183876699754, 618.3532331027812, 642.8827234961987, 662.1604133799784, 652.2175637642036, 688.4400655678315, 616.7447967483848, 663.69255499455, 607.5139184625168, 633.2160081905864, 622.2796306623245, 429.9726736853696, 631.7619059451144, 629.1435439099753, 669.98890790396, 607.641552945238, 595.021705201337, 642.8997489293492, 621.9605236571313, 418.47340660613975, 676.0443134007239, 623.4138706900603, 390.3720668611226, 596.4736751949442, 578.7600683755927, 630.1772633810722, 643.4932443757509, 646.5359592501229, 662.1865524073598, 644.7873913359775, 692.114719677482, 670.3912284971334, 575.6464986361218, 598.4816714601122, 655.5009876885186, 608.8751055198412, 563.680042229724, 597.6729645202821, 606.2691505322083, 605.2011006296987, 561.2048971870584, 628.3572608000727, 659.3815874687937, 656.5913892255741, 655.0460411758653, 658.3863677240832, 657.6771444409685, 658.5871333851454, 657.5691184832621, 656.8928918069565, 657.1807132619831, 658.7715266865389, 656.8782774920935, 621.0529912266437, 611.9297239358792, 660.1346324890085, 665.7746966807819, 608.9742035791528, 389.8570579336925, 615.7257091049224, 609.9294990790227, 574.2360065865466, 582.5560656366492, 604.1195471355726, 616.1883308485926, 608.9129709475661, 381.12653152149636, 598.9765384685563, 576.5826227929047, 625.1515662450562, 625.9127560661339, 593.6771480747101, 603.4789154725077, 609.5827310856798, 636.7258873395663, 577.565192279719, 386.51027128896203, 671.3037873346914, 651.4006066003668, 388.3022205136123, 638.3039490226128, 626.2378790521096, 600.2872694042042, 662.5865645122944, 640.6271555188683, 608.9453388947325, 625.9264591685666, 618.5152007920055, 597.8217987327124, 734.6257543000014, 693.9567662324266, 732.5906694236002, 635.656584095898, 738.9697668264738, 753.2443389360898, 743.9157024940023, 743.8933414882215, 676.7200020092001, 696.7978162431482]
Elapsed: 0.2134071947361359~0.03619659220603777
Time per graph: 0.0016552728709293243~0.00027972910607859767
Speed: 616.7542861956729~76.25807152537053
Total Time: 0.1843
best val loss: 0.6237807976183041 test_score: 0.8984

Testing...
Test loss: 0.6300 score: 0.8984 time: 0.16s
test Score 0.8984
Epoch Time List: [0.9633196871727705, 0.8566829261835665, 0.6898946990258992, 0.7035902021452785, 0.6939213220030069, 0.7361981570720673, 0.6923042249400169, 0.6863790040370077, 0.7415371900424361, 0.6919508019927889, 0.8353804799262434, 0.7134221431333572, 0.7157405321486294, 0.8023197455331683, 0.6884121710900217, 0.7141581922769547, 0.8175767259672284, 0.697363682789728, 0.7931031358893961, 0.6696588890627027, 0.6994294810574502, 0.7811993830837309, 0.6942601772025228, 0.7072781440801919, 0.7806083792820573, 0.7202223972417414, 0.7870884470175952, 0.7057592400815338, 0.695019745035097, 0.8071021719370037, 0.6872150218114257, 0.7218198331538588, 0.8232038340065628, 0.7143342043273151, 0.8224438750185072, 0.6637586050201207, 0.7104510699864477, 0.8436439340002835, 0.733891072217375, 0.7243493020068854, 0.7918908349238336, 0.7106469301506877, 0.6991014308296144, 0.724667162867263, 0.6764928109478205, 0.6711688619107008, 0.8253422968555242, 0.7058475448284298, 0.7306763210799545, 0.6856995769776404, 0.705704147927463, 0.7498937181662768, 0.7309182421304286, 0.714408848900348, 0.711545562138781, 0.7547602518461645, 0.7062202817760408, 0.667845078278333, 0.6691687528509647, 0.6663108959328383, 0.6687328252010047, 0.6660393220372498, 0.6626829337328672, 0.6675861971452832, 0.6668112480547279, 0.6682074989657849, 0.665198682108894, 0.6707297849934548, 0.6935054678469896, 0.7310971710830927, 0.6718706851825118, 0.6665065148845315, 0.6926947920583189, 0.8758322519715875, 0.7111909841187298, 0.7169025491457433, 0.8337544500827789, 0.7522669089958072, 0.7065001998562366, 0.8238765017595142, 0.7223708191886544, 0.8372035040520132, 0.7124143368564546, 0.7557153250090778, 0.8276831139810383, 0.6982606078963727, 0.7255656949710101, 0.8140221161302179, 0.7207674537785351, 0.8258533908519894, 0.7356021236628294, 0.8362179910764098, 0.7771339493338019, 0.6792977491859347, 0.8285343451425433, 0.6880570261273533, 0.7114529912360013, 0.7894552785437554, 0.6693937790114433, 0.6870378674939275, 0.8243623971939087, 0.7208647709339857, 0.7066925142426044, 0.8479917661752552, 0.7526494131889194, 0.8743653879500926, 0.7738604680635035, 0.8767312997952104, 0.7351112426258624, 0.7278798900078982, 0.7705562498886138, 0.7465498461388052, 0.7616064350586385, 0.7612569641787559]
Total Epoch List: [50, 54, 10]
Total Time List: [0.1971973991021514, 0.216513680992648, 0.1842750059440732]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f89e593ca0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7010 score: 0.5039 time: 0.21s
Test loss: 0.7005 score: 0.5039 time: 0.20s
Epoch 2/1000, LR 0.000050
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.20s
Test loss: 0.6899 score: 0.5039 time: 0.21s
Epoch 3/1000, LR 0.000150
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6720 score: 0.5039 time: 0.25s
Test loss: 0.6672 score: 0.5194 time: 0.22s
Epoch 4/1000, LR 0.000250
Train loss: 0.6109;  Loss pred: 0.6109; Loss self: 0.0000; time: 0.36s
Val loss: 0.6377 score: 0.6822 time: 0.22s
Test loss: 0.6281 score: 0.7752 time: 0.21s
Epoch 5/1000, LR 0.000350
Train loss: 0.5542;  Loss pred: 0.5542; Loss self: 0.0000; time: 0.36s
Val loss: 0.5964 score: 0.8682 time: 0.21s
Test loss: 0.5792 score: 0.9380 time: 0.20s
Epoch 6/1000, LR 0.000450
Train loss: 0.4836;  Loss pred: 0.4836; Loss self: 0.0000; time: 0.33s
Val loss: 0.5643 score: 0.8450 time: 0.21s
Test loss: 0.5392 score: 0.9225 time: 0.22s
Epoch 7/1000, LR 0.000550
Train loss: 0.4389;  Loss pred: 0.4389; Loss self: 0.0000; time: 0.33s
Val loss: 0.5514 score: 0.7674 time: 0.26s
Test loss: 0.5187 score: 0.8372 time: 0.23s
Epoch 8/1000, LR 0.000650
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.32s
Val loss: 0.5630 score: 0.6744 time: 0.20s
Test loss: 0.5230 score: 0.6977 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 9/1000, LR 0.000750
Train loss: 0.3381;  Loss pred: 0.3381; Loss self: 0.0000; time: 0.32s
Val loss: 0.5456 score: 0.6977 time: 0.20s
Test loss: 0.4998 score: 0.7829 time: 0.20s
Epoch 10/1000, LR 0.000850
Train loss: 0.3046;  Loss pred: 0.3046; Loss self: 0.0000; time: 0.32s
Val loss: 0.5119 score: 0.7674 time: 0.20s
Test loss: 0.4633 score: 0.8450 time: 0.20s
Epoch 11/1000, LR 0.000950
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.32s
Val loss: 0.4778 score: 0.8450 time: 0.20s
Test loss: 0.4273 score: 0.9225 time: 0.20s
Epoch 12/1000, LR 0.000950
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 0.32s
Val loss: 0.4743 score: 0.8527 time: 0.20s
Test loss: 0.4206 score: 0.9225 time: 0.20s
Epoch 13/1000, LR 0.000950
Train loss: 0.2062;  Loss pred: 0.2062; Loss self: 0.0000; time: 0.32s
Val loss: 0.4832 score: 0.8217 time: 0.20s
Test loss: 0.4250 score: 0.8760 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 14/1000, LR 0.000950
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.32s
Val loss: 0.4863 score: 0.7752 time: 0.20s
Test loss: 0.4251 score: 0.8527 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 15/1000, LR 0.000950
Train loss: 0.1756;  Loss pred: 0.1756; Loss self: 0.0000; time: 0.32s
Val loss: 0.4674 score: 0.8217 time: 0.20s
Test loss: 0.4045 score: 0.8915 time: 0.20s
Epoch 16/1000, LR 0.000950
Train loss: 0.1476;  Loss pred: 0.1476; Loss self: 0.0000; time: 0.32s
Val loss: 0.4282 score: 0.8837 time: 0.20s
Test loss: 0.3667 score: 0.9302 time: 0.20s
Epoch 17/1000, LR 0.000950
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 0.32s
Val loss: 0.4069 score: 0.8837 time: 0.21s
Test loss: 0.3454 score: 0.9457 time: 0.20s
Epoch 18/1000, LR 0.000950
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.32s
Val loss: 0.3889 score: 0.8915 time: 0.20s
Test loss: 0.3269 score: 0.9690 time: 0.20s
Epoch 19/1000, LR 0.000950
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.32s
Val loss: 0.3779 score: 0.8915 time: 0.20s
Test loss: 0.3149 score: 0.9690 time: 0.20s
Epoch 20/1000, LR 0.000950
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.32s
Val loss: 0.3695 score: 0.8915 time: 0.20s
Test loss: 0.3054 score: 0.9690 time: 0.20s
Epoch 21/1000, LR 0.000950
Train loss: 0.0829;  Loss pred: 0.0829; Loss self: 0.0000; time: 0.32s
Val loss: 0.3661 score: 0.8915 time: 0.20s
Test loss: 0.3008 score: 0.9535 time: 0.20s
Epoch 22/1000, LR 0.000950
Train loss: 0.0800;  Loss pred: 0.0800; Loss self: 0.0000; time: 0.32s
Val loss: 0.3610 score: 0.8915 time: 0.20s
Test loss: 0.2945 score: 0.9535 time: 0.22s
Epoch 23/1000, LR 0.000950
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 0.32s
Val loss: 0.3541 score: 0.8992 time: 0.24s
Test loss: 0.2865 score: 0.9535 time: 0.22s
Epoch 24/1000, LR 0.000950
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.37s
Val loss: 0.3455 score: 0.8915 time: 0.34s
Test loss: 0.2767 score: 0.9612 time: 0.21s
Epoch 25/1000, LR 0.000950
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.33s
Val loss: 0.3383 score: 0.8915 time: 0.21s
Test loss: 0.2681 score: 0.9612 time: 0.20s
Epoch 26/1000, LR 0.000949
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.33s
Val loss: 0.3338 score: 0.8915 time: 0.22s
Test loss: 0.2623 score: 0.9612 time: 0.20s
Epoch 27/1000, LR 0.000949
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.35s
Val loss: 0.3288 score: 0.8992 time: 0.22s
Test loss: 0.2560 score: 0.9612 time: 0.22s
Epoch 28/1000, LR 0.000949
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.34s
Val loss: 0.3195 score: 0.8915 time: 0.22s
Test loss: 0.2445 score: 0.9612 time: 0.22s
Epoch 29/1000, LR 0.000949
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.34s
Val loss: 0.3145 score: 0.8992 time: 0.27s
Test loss: 0.2357 score: 0.9690 time: 0.20s
Epoch 30/1000, LR 0.000949
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.32s
Val loss: 0.3107 score: 0.8837 time: 0.21s
Test loss: 0.2284 score: 0.9535 time: 0.20s
Epoch 31/1000, LR 0.000949
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.33s
Val loss: 0.3067 score: 0.8837 time: 0.20s
Test loss: 0.2207 score: 0.9612 time: 0.20s
Epoch 32/1000, LR 0.000949
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.39s
Val loss: 0.3013 score: 0.8760 time: 0.22s
Test loss: 0.2124 score: 0.9612 time: 0.23s
Epoch 33/1000, LR 0.000949
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.34s
Val loss: 0.2958 score: 0.8760 time: 0.24s
Test loss: 0.2043 score: 0.9612 time: 0.22s
Epoch 34/1000, LR 0.000949
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.31s
Val loss: 0.2894 score: 0.8760 time: 0.30s
Test loss: 0.1952 score: 0.9612 time: 0.19s
Epoch 35/1000, LR 0.000949
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.29s
Val loss: 0.2823 score: 0.8837 time: 0.19s
Test loss: 0.1864 score: 0.9612 time: 0.21s
Epoch 36/1000, LR 0.000949
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.33s
Val loss: 0.2764 score: 0.8837 time: 0.21s
Test loss: 0.1780 score: 0.9612 time: 0.20s
Epoch 37/1000, LR 0.000948
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.38s
Val loss: 0.2712 score: 0.8837 time: 0.23s
Test loss: 0.1707 score: 0.9612 time: 0.23s
Epoch 38/1000, LR 0.000948
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.31s
Val loss: 0.2675 score: 0.8837 time: 0.23s
Test loss: 0.1645 score: 0.9612 time: 0.22s
Epoch 39/1000, LR 0.000948
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.31s
Val loss: 0.2644 score: 0.8837 time: 0.34s
Test loss: 0.1584 score: 0.9612 time: 0.20s
Epoch 40/1000, LR 0.000948
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.31s
Val loss: 0.2626 score: 0.8837 time: 0.21s
Test loss: 0.1529 score: 0.9535 time: 0.21s
Epoch 41/1000, LR 0.000948
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.31s
Val loss: 0.2625 score: 0.8915 time: 0.19s
Test loss: 0.1475 score: 0.9612 time: 0.19s
Epoch 42/1000, LR 0.000948
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.42s
Val loss: 0.2654 score: 0.8837 time: 0.20s
Test loss: 0.1424 score: 0.9612 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 43/1000, LR 0.000948
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.31s
Val loss: 0.2685 score: 0.8760 time: 0.20s
Test loss: 0.1381 score: 0.9612 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 44/1000, LR 0.000947
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.30s
Val loss: 0.2730 score: 0.8760 time: 0.25s
Test loss: 0.1350 score: 0.9612 time: 0.24s
     INFO: Early stopping counter 3 of 5
Epoch 45/1000, LR 0.000947
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.30s
Val loss: 0.2755 score: 0.8760 time: 0.20s
Test loss: 0.1326 score: 0.9612 time: 0.20s
     INFO: Early stopping counter 4 of 5
Epoch 46/1000, LR 0.000947
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.31s
Val loss: 0.2768 score: 0.8760 time: 0.20s
Test loss: 0.1303 score: 0.9612 time: 0.20s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 040,   Train_Loss: 0.0163,   Val_Loss: 0.2625,   Val_Precision: 0.9630,   Val_Recall: 0.8125,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.2625,   Test_Precision: 0.9688,   Test_Recall: 0.9538,   Test_accuracy: 0.9612,   Test_Score: 0.9612,   Test_loss: 0.1475


[0.20976876188069582, 0.2135372159536928, 0.22073019691742957, 0.21542047406546772, 0.2090913539286703, 0.2228641959372908, 0.23869828786700964, 0.20088337105698884, 0.20093045104295015, 0.2023111949674785, 0.20125023811124265, 0.20058216387405992, 0.2008342540357262, 0.2006916960235685, 0.20046817301772535, 0.2004460208117962, 0.20269259787164629, 0.2009336689952761, 0.20106379315257072, 0.20196046005003154, 0.2014124719426036, 0.2210318148136139, 0.224467562045902, 0.21109928679652512, 0.2088657058775425, 0.2082689858507365, 0.2219439740292728, 0.22373870899900794, 0.205861893016845, 0.2050199070945382, 0.20565594593062997, 0.2304959879256785, 0.22673256509006023, 0.19928125594742596, 0.21366594498977065, 0.20499944989569485, 0.2335040809120983, 0.22950646001845598, 0.20829920400865376, 0.21020146599039435, 0.19856894295662642, 0.20076181483455002, 0.20403376989997923, 0.25017180596478283, 0.20192450401373208, 0.2026169339660555]
[0.0016261144331836885, 0.001655327255454983, 0.0017110867978095314, 0.0016699261555462614, 0.001620863208749382, 0.0017276294258704713, 0.0018503743245504624, 0.001557235434550301, 0.0015576003956817841, 0.0015683038369571978, 0.0015600793652034313, 0.0015549004951477513, 0.0015568546824474899, 0.0015557495815780504, 0.001554016845098646, 0.0015538451225720635, 0.001571260448617413, 0.001557625341048652, 0.0015586340554462846, 0.0015655849616281515, 0.0015613369918031285, 0.0017134249210357666, 0.0017400586205108682, 0.0016364285798180242, 0.001619113999050717, 0.00161448826240881, 0.001720495922707541, 0.0017344085968915345, 0.001595828628037558, 0.0015893016053840172, 0.0015942321389971316, 0.001786790604075027, 0.0017576167836438777, 0.0015448159375769455, 0.0016563251549594623, 0.0015891430224472469, 0.001810109154357351, 0.00177911984510431, 0.0016147225116949905, 0.0016294687286077083, 0.0015392941314467164, 0.0015562931382523259, 0.001581657131007591, 0.0019393163253083941, 0.0015653062326645897, 0.0015706739067136086]
[614.9628707508301, 604.1101520588086, 584.4238885369007, 598.8288743659344, 616.9552091762113, 578.8278348501427, 540.4311909931761, 642.1636560619238, 642.0131907852307, 637.6315459000513, 640.9930304216298, 643.1279706454637, 642.320706790647, 642.776968633773, 643.4936681375045, 643.5647835639568, 636.4317264397015, 642.002908945204, 641.587418487189, 638.7388896224683, 640.4767229943986, 583.6263892995669, 574.6932822909192, 611.0868584996255, 617.6217366944501, 619.391310103428, 581.2277650889762, 576.5654078238737, 626.6337014079841, 629.2071917704844, 627.261222213887, 559.6626698838462, 568.9522365204135, 647.3263096757709, 603.7461889688408, 629.2699812884185, 552.4528714706341, 562.0756818332101, 619.3014544339819, 613.6969568323323, 649.6484197338834, 642.5524699819549, 632.2482796021362, 515.6456360160728, 638.8526277683824, 636.6693912247799]
Elapsed: 0.21081063070374986~0.01208324180521853
Time per graph: 0.0016341909356879832~9.366854112572502e-05
Speed: 613.8104184475869~32.981187568816445
Total Time: 0.2035
best val loss: 0.2625084405317325 test_score: 0.9612

Testing...
Test loss: 0.2865 score: 0.9535 time: 0.20s
test Score 0.9535
Epoch Time List: [0.74394485889934, 0.7335607898421586, 0.7920819618739188, 0.7917306579183787, 0.7695630108937621, 0.7589730517938733, 0.8269050463568419, 0.7172369328327477, 0.7142091782297939, 0.7150966690387577, 0.7152995229698718, 0.7127560710068792, 0.7121328399516642, 0.7115761931054294, 0.7119976391550153, 0.7109059651847929, 0.7311053790617734, 0.7130616898648441, 0.7127799929585308, 0.7166074810083956, 0.7162425532005727, 0.7325491979718208, 0.7830471210181713, 0.9173911269754171, 0.7388032050803304, 0.7524096132256091, 0.7832214999943972, 0.7764312038198113, 0.8098035620059818, 0.7300136908888817, 0.738835024414584, 0.839144088095054, 0.805502570932731, 0.8032388670835644, 0.6974907091353089, 0.7350284939166158, 0.8397310809232295, 0.7659410270862281, 0.847139413934201, 0.7225354826077819, 0.7002160800620914, 0.8154383301734924, 0.7051741450559348, 0.7969984360970557, 0.6971530169248581, 0.7039294228889048]
Total Epoch List: [46]
Total Time List: [0.20348850404843688]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e3da290>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7653;  Loss pred: 0.7653; Loss self: 0.0000; time: 0.32s
Val loss: 0.6980 score: 0.4961 time: 0.21s
Test loss: 0.7003 score: 0.4884 time: 0.29s
Epoch 2/1000, LR 0.000050
Train loss: 0.7417;  Loss pred: 0.7417; Loss self: 0.0000; time: 0.32s
Val loss: 0.6911 score: 0.4961 time: 0.21s
Test loss: 0.6930 score: 0.4884 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.34s
Val loss: 0.6806 score: 0.5349 time: 0.21s
Test loss: 0.6816 score: 0.6047 time: 0.18s
Epoch 4/1000, LR 0.000250
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.32s
Val loss: 0.6705 score: 0.6434 time: 0.22s
Test loss: 0.6706 score: 0.6357 time: 0.19s
Epoch 5/1000, LR 0.000350
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.34s
Val loss: 0.6657 score: 0.5039 time: 0.21s
Test loss: 0.6648 score: 0.5116 time: 0.18s
Epoch 6/1000, LR 0.000450
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6642 score: 0.4961 time: 0.21s
Test loss: 0.6624 score: 0.5116 time: 0.18s
Epoch 7/1000, LR 0.000550
Train loss: 0.4750;  Loss pred: 0.4750; Loss self: 0.0000; time: 0.44s
Val loss: 0.6590 score: 0.5116 time: 0.21s
Test loss: 0.6573 score: 0.5116 time: 0.17s
Epoch 8/1000, LR 0.000650
Train loss: 0.4428;  Loss pred: 0.4428; Loss self: 0.0000; time: 0.33s
Val loss: 0.6400 score: 0.5271 time: 0.21s
Test loss: 0.6390 score: 0.5504 time: 0.17s
Epoch 9/1000, LR 0.000750
Train loss: 0.4080;  Loss pred: 0.4080; Loss self: 0.0000; time: 0.32s
Val loss: 0.6013 score: 0.6434 time: 0.34s
Test loss: 0.6019 score: 0.6279 time: 0.18s
Epoch 10/1000, LR 0.000850
Train loss: 0.3589;  Loss pred: 0.3589; Loss self: 0.0000; time: 0.33s
Val loss: 0.5489 score: 0.7519 time: 0.21s
Test loss: 0.5493 score: 0.7442 time: 0.18s
Epoch 11/1000, LR 0.000950
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 0.32s
Val loss: 0.5067 score: 0.8450 time: 0.21s
Test loss: 0.5071 score: 0.8295 time: 0.18s
Epoch 12/1000, LR 0.000950
Train loss: 0.2882;  Loss pred: 0.2882; Loss self: 0.0000; time: 0.44s
Val loss: 0.4791 score: 0.8992 time: 0.21s
Test loss: 0.4800 score: 0.8527 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.2594;  Loss pred: 0.2594; Loss self: 0.0000; time: 0.33s
Val loss: 0.4589 score: 0.8837 time: 0.23s
Test loss: 0.4616 score: 0.8372 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2305;  Loss pred: 0.2305; Loss self: 0.0000; time: 0.33s
Val loss: 0.4479 score: 0.8605 time: 0.30s
Test loss: 0.4555 score: 0.8140 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2023;  Loss pred: 0.2023; Loss self: 0.0000; time: 0.33s
Val loss: 0.4404 score: 0.8527 time: 0.22s
Test loss: 0.4513 score: 0.8062 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.1898;  Loss pred: 0.1898; Loss self: 0.0000; time: 0.34s
Val loss: 0.4305 score: 0.8527 time: 0.21s
Test loss: 0.4430 score: 0.8140 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.1672;  Loss pred: 0.1672; Loss self: 0.0000; time: 0.31s
Val loss: 0.4194 score: 0.8527 time: 0.22s
Test loss: 0.4333 score: 0.8217 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1590;  Loss pred: 0.1590; Loss self: 0.0000; time: 0.31s
Val loss: 0.4074 score: 0.8527 time: 0.21s
Test loss: 0.4221 score: 0.8295 time: 0.18s
Epoch 19/1000, LR 0.000950
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.32s
Val loss: 0.4067 score: 0.8527 time: 0.22s
Test loss: 0.4243 score: 0.8062 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.32s
Val loss: 0.3954 score: 0.8605 time: 0.23s
Test loss: 0.4151 score: 0.8140 time: 0.18s
Epoch 21/1000, LR 0.000950
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.31s
Val loss: 0.3786 score: 0.8682 time: 0.21s
Test loss: 0.3996 score: 0.8372 time: 0.17s
Epoch 22/1000, LR 0.000950
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.31s
Val loss: 0.3636 score: 0.8760 time: 0.21s
Test loss: 0.3845 score: 0.8527 time: 0.18s
Epoch 23/1000, LR 0.000950
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.31s
Val loss: 0.3604 score: 0.8682 time: 0.20s
Test loss: 0.3834 score: 0.8372 time: 0.18s
Epoch 24/1000, LR 0.000950
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.31s
Val loss: 0.3591 score: 0.8682 time: 0.21s
Test loss: 0.3844 score: 0.8140 time: 0.18s
Epoch 25/1000, LR 0.000950
Train loss: 0.0831;  Loss pred: 0.0831; Loss self: 0.0000; time: 0.32s
Val loss: 0.3597 score: 0.8682 time: 0.21s
Test loss: 0.3870 score: 0.8140 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 26/1000, LR 0.000949
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.32s
Val loss: 0.3508 score: 0.8682 time: 0.20s
Test loss: 0.3786 score: 0.8140 time: 0.18s
Epoch 27/1000, LR 0.000949
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 0.31s
Val loss: 0.3512 score: 0.8682 time: 0.20s
Test loss: 0.3818 score: 0.8140 time: 0.17s
     INFO: Early stopping counter 1 of 5
Epoch 28/1000, LR 0.000949
Train loss: 0.0601;  Loss pred: 0.0601; Loss self: 0.0000; time: 0.31s
Val loss: 0.3478 score: 0.8682 time: 0.21s
Test loss: 0.3805 score: 0.8140 time: 0.17s
Epoch 29/1000, LR 0.000949
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.31s
Val loss: 0.3441 score: 0.8682 time: 0.21s
Test loss: 0.3785 score: 0.8140 time: 0.17s
Epoch 30/1000, LR 0.000949
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.31s
Val loss: 0.3352 score: 0.8682 time: 0.20s
Test loss: 0.3698 score: 0.8140 time: 0.17s
Epoch 31/1000, LR 0.000949
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.31s
Val loss: 0.3251 score: 0.8760 time: 0.20s
Test loss: 0.3600 score: 0.8372 time: 0.17s
Epoch 32/1000, LR 0.000949
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.31s
Val loss: 0.3114 score: 0.8837 time: 0.20s
Test loss: 0.3461 score: 0.8450 time: 0.17s
Epoch 33/1000, LR 0.000949
Train loss: 0.0527;  Loss pred: 0.0527; Loss self: 0.0000; time: 0.31s
Val loss: 0.3056 score: 0.8992 time: 0.20s
Test loss: 0.3430 score: 0.8372 time: 0.17s
Epoch 34/1000, LR 0.000949
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.31s
Val loss: 0.3114 score: 0.8760 time: 0.20s
Test loss: 0.3559 score: 0.8372 time: 0.17s
     INFO: Early stopping counter 1 of 5
Epoch 35/1000, LR 0.000949
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.31s
Val loss: 0.3137 score: 0.8760 time: 0.20s
Test loss: 0.3638 score: 0.8372 time: 0.17s
     INFO: Early stopping counter 2 of 5
Epoch 36/1000, LR 0.000949
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.31s
Val loss: 0.3115 score: 0.8760 time: 0.21s
Test loss: 0.3657 score: 0.8372 time: 0.17s
     INFO: Early stopping counter 3 of 5
Epoch 37/1000, LR 0.000948
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.32s
Val loss: 0.3051 score: 0.8760 time: 0.20s
Test loss: 0.3629 score: 0.8372 time: 0.17s
Epoch 38/1000, LR 0.000948
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.32s
Val loss: 0.2946 score: 0.8992 time: 0.21s
Test loss: 0.3542 score: 0.8372 time: 0.18s
Epoch 39/1000, LR 0.000948
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.32s
Val loss: 0.2844 score: 0.8992 time: 0.21s
Test loss: 0.3454 score: 0.8372 time: 0.18s
Epoch 40/1000, LR 0.000948
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.32s
Val loss: 0.2743 score: 0.9147 time: 0.21s
Test loss: 0.3362 score: 0.8450 time: 0.18s
Epoch 41/1000, LR 0.000948
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.32s
Val loss: 0.2704 score: 0.9147 time: 0.21s
Test loss: 0.3359 score: 0.8450 time: 0.17s
Epoch 42/1000, LR 0.000948
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.32s
Val loss: 0.2707 score: 0.9147 time: 0.21s
Test loss: 0.3424 score: 0.8450 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 43/1000, LR 0.000948
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.32s
Val loss: 0.2701 score: 0.9147 time: 0.21s
Test loss: 0.3468 score: 0.8450 time: 0.18s
Epoch 44/1000, LR 0.000947
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.32s
Val loss: 0.2701 score: 0.9147 time: 0.21s
Test loss: 0.3515 score: 0.8450 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 45/1000, LR 0.000947
Train loss: 0.0353;  Loss pred: 0.0353; Loss self: 0.0000; time: 0.32s
Val loss: 0.2675 score: 0.9147 time: 0.21s
Test loss: 0.3516 score: 0.8450 time: 0.18s
Epoch 46/1000, LR 0.000947
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.32s
Val loss: 0.2623 score: 0.9147 time: 0.20s
Test loss: 0.3488 score: 0.8527 time: 0.17s
Epoch 47/1000, LR 0.000947
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.32s
Val loss: 0.2566 score: 0.9147 time: 0.21s
Test loss: 0.3443 score: 0.8682 time: 0.18s
Epoch 48/1000, LR 0.000947
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.32s
Val loss: 0.2499 score: 0.9147 time: 0.21s
Test loss: 0.3375 score: 0.8682 time: 0.18s
Epoch 49/1000, LR 0.000947
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.32s
Val loss: 0.2424 score: 0.9070 time: 0.20s
Test loss: 0.3286 score: 0.8760 time: 0.18s
Epoch 50/1000, LR 0.000946
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.32s
Val loss: 0.2407 score: 0.9070 time: 0.21s
Test loss: 0.3284 score: 0.8837 time: 0.18s
Epoch 51/1000, LR 0.000946
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.33s
Val loss: 0.2397 score: 0.9147 time: 0.27s
Test loss: 0.3292 score: 0.8837 time: 0.18s
Epoch 52/1000, LR 0.000946
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.29s
Val loss: 0.2356 score: 0.9070 time: 0.20s
Test loss: 0.3243 score: 0.8915 time: 0.17s
Epoch 53/1000, LR 0.000946
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.31s
Val loss: 0.2317 score: 0.9070 time: 0.20s
Test loss: 0.3214 score: 0.8915 time: 0.17s
Epoch 54/1000, LR 0.000946
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.42s
Val loss: 0.2284 score: 0.9070 time: 0.21s
Test loss: 0.3180 score: 0.8915 time: 0.17s
Epoch 55/1000, LR 0.000945
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.30s
Val loss: 0.2268 score: 0.9070 time: 0.20s
Test loss: 0.3164 score: 0.8915 time: 0.17s
Epoch 56/1000, LR 0.000945
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.31s
Val loss: 0.2211 score: 0.9147 time: 0.33s
Test loss: 0.3100 score: 0.8915 time: 0.18s
Epoch 57/1000, LR 0.000945
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.32s
Val loss: 0.2235 score: 0.9225 time: 0.21s
Test loss: 0.3159 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 58/1000, LR 0.000945
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.31s
Val loss: 0.2224 score: 0.9225 time: 0.22s
Test loss: 0.3157 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 59/1000, LR 0.000945
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.36s
Val loss: 0.2218 score: 0.9225 time: 0.21s
Test loss: 0.3168 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 3 of 5
Epoch 60/1000, LR 0.000944
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.30s
Val loss: 0.2229 score: 0.9225 time: 0.20s
Test loss: 0.3183 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 4 of 5
Epoch 61/1000, LR 0.000944
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.31s
Val loss: 0.2249 score: 0.9147 time: 0.20s
Test loss: 0.3194 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 055,   Train_Loss: 0.0178,   Val_Loss: 0.2211,   Val_Precision: 0.9821,   Val_Recall: 0.8462,   Val_accuracy: 0.9091,   Val_Score: 0.9147,   Val_Loss: 0.2211,   Test_Precision: 0.9808,   Test_Recall: 0.7969,   Test_accuracy: 0.8793,   Test_Score: 0.8915,   Test_loss: 0.3100


[0.20976876188069582, 0.2135372159536928, 0.22073019691742957, 0.21542047406546772, 0.2090913539286703, 0.2228641959372908, 0.23869828786700964, 0.20088337105698884, 0.20093045104295015, 0.2023111949674785, 0.20125023811124265, 0.20058216387405992, 0.2008342540357262, 0.2006916960235685, 0.20046817301772535, 0.2004460208117962, 0.20269259787164629, 0.2009336689952761, 0.20106379315257072, 0.20196046005003154, 0.2014124719426036, 0.2210318148136139, 0.224467562045902, 0.21109928679652512, 0.2088657058775425, 0.2082689858507365, 0.2219439740292728, 0.22373870899900794, 0.205861893016845, 0.2050199070945382, 0.20565594593062997, 0.2304959879256785, 0.22673256509006023, 0.19928125594742596, 0.21366594498977065, 0.20499944989569485, 0.2335040809120983, 0.22950646001845598, 0.20829920400865376, 0.21020146599039435, 0.19856894295662642, 0.20076181483455002, 0.20403376989997923, 0.25017180596478283, 0.20192450401373208, 0.2026169339660555, 0.29581013903953135, 0.19547391310334206, 0.18280357006005943, 0.19190420885570347, 0.18206214089877903, 0.18180695897899568, 0.17928843502886593, 0.18011141591705382, 0.18212678213603795, 0.18229567096568644, 0.18246095115318894, 0.1927579299081117, 0.19256651611067355, 0.1952154659666121, 0.19498436897993088, 0.1906112958677113, 0.19053489505313337, 0.18070389702916145, 0.19900334812700748, 0.1885368910152465, 0.1800970290787518, 0.18020699406042695, 0.18029805598780513, 0.1803305521607399, 0.18045595986768603, 0.1817368541378528, 0.18001143005676568, 0.1800303419586271, 0.18004152202047408, 0.17846091277897358, 0.17891867901198566, 0.17865252890624106, 0.17876925901509821, 0.17912898305803537, 0.17882400890812278, 0.1798055311664939, 0.17986321519128978, 0.18048511212691665, 0.18026057397946715, 0.18076807307079434, 0.17994905286468565, 0.18115949886851013, 0.18045832100324333, 0.1804692221339792, 0.18027866398915648, 0.17988392990082502, 0.18043701606802642, 0.18010130594484508, 0.1805273280479014, 0.181728919968009, 0.1889766079839319, 0.17861685785464942, 0.1744096449110657, 0.17315155593678355, 0.17367478692904115, 0.18983190110884607, 0.1909743989817798, 0.18965153908357024, 0.1881347820162773, 0.1773666930384934, 0.17706650402396917]
[0.0016261144331836885, 0.001655327255454983, 0.0017110867978095314, 0.0016699261555462614, 0.001620863208749382, 0.0017276294258704713, 0.0018503743245504624, 0.001557235434550301, 0.0015576003956817841, 0.0015683038369571978, 0.0015600793652034313, 0.0015549004951477513, 0.0015568546824474899, 0.0015557495815780504, 0.001554016845098646, 0.0015538451225720635, 0.001571260448617413, 0.001557625341048652, 0.0015586340554462846, 0.0015655849616281515, 0.0015613369918031285, 0.0017134249210357666, 0.0017400586205108682, 0.0016364285798180242, 0.001619113999050717, 0.00161448826240881, 0.001720495922707541, 0.0017344085968915345, 0.001595828628037558, 0.0015893016053840172, 0.0015942321389971316, 0.001786790604075027, 0.0017576167836438777, 0.0015448159375769455, 0.0016563251549594623, 0.0015891430224472469, 0.001810109154357351, 0.00177911984510431, 0.0016147225116949905, 0.0016294687286077083, 0.0015392941314467164, 0.0015562931382523259, 0.001581657131007591, 0.0019393163253083941, 0.0015653062326645897, 0.0015706739067136086, 0.002293101853019623, 0.0015153016519638919, 0.001417081938450073, 0.0014876295260132053, 0.0014113344255719305, 0.0014093562711550052, 0.0013898328296811312, 0.0013962125264887894, 0.0014118355204344028, 0.001413144736168112, 0.0014144259779316972, 0.0014942475186675325, 0.001492763690780415, 0.0015132981857876907, 0.001511506736278534, 0.0014776069447109404, 0.0014770146903343672, 0.001400805403326833, 0.001542661613387655, 0.0014615262869398953, 0.0013961010006104792, 0.001396953442328891, 0.0013976593487426755, 0.0013979112570599993, 0.0013988834098270235, 0.0014088128227740526, 0.0013954374423005092, 0.0013955840461909077, 0.0013956707133370083, 0.001383417928519175, 0.001386966503968881, 0.0013849033248545819, 0.001385808209419366, 0.0013885967678917471, 0.0013862326271947502, 0.0013938413268720458, 0.0013942884898549596, 0.001399109396332687, 0.001397368790538505, 0.0014013028920216615, 0.0013949538981758577, 0.0014043372005310862, 0.0013989017132034366, 0.001398986218092862, 0.0013975090231717557, 0.0013944490689986437, 0.0013987365586668715, 0.0013961341546112021, 0.0013994366515341194, 0.001408751317581465, 0.0014649349456118752, 0.0013846268050748017, 0.0013520127512485713, 0.0013422601235409577, 0.001346316177744505, 0.0014715651248747756, 0.0014804216975331768, 0.0014701669696400793, 0.0014584091629168785, 0.0013749356049495612, 0.0013726085583253423]
[614.9628707508301, 604.1101520588086, 584.4238885369007, 598.8288743659344, 616.9552091762113, 578.8278348501427, 540.4311909931761, 642.1636560619238, 642.0131907852307, 637.6315459000513, 640.9930304216298, 643.1279706454637, 642.320706790647, 642.776968633773, 643.4936681375045, 643.5647835639568, 636.4317264397015, 642.002908945204, 641.587418487189, 638.7388896224683, 640.4767229943986, 583.6263892995669, 574.6932822909192, 611.0868584996255, 617.6217366944501, 619.391310103428, 581.2277650889762, 576.5654078238737, 626.6337014079841, 629.2071917704844, 627.261222213887, 559.6626698838462, 568.9522365204135, 647.3263096757709, 603.7461889688408, 629.2699812884185, 552.4528714706341, 562.0756818332101, 619.3014544339819, 613.6969568323323, 649.6484197338834, 642.5524699819549, 632.2482796021362, 515.6456360160728, 638.8526277683824, 636.6693912247799, 436.09052894147334, 659.9346068843519, 705.6754961493232, 672.2103739631767, 708.5492863215316, 709.5437970276126, 719.5109934404338, 716.2233406649137, 708.2978048974951, 707.6415984901896, 707.0005893573103, 669.2331675355442, 669.8983946194465, 660.8082989800768, 661.5914941021634, 676.7699648268957, 677.0413365175261, 713.8750304825046, 648.2302997116908, 684.2162258290771, 716.2805553199415, 715.8434702969618, 715.4819240465089, 715.3529917937268, 714.8558578757134, 709.8175029603499, 716.6211609969462, 716.5458810806768, 716.5013856377547, 722.8473618745207, 720.9979456161666, 722.0720623982922, 721.6005744539397, 720.1514673826164, 721.3796446442399, 717.4417781427999, 717.2116870189645, 714.7403931537996, 715.6306958985604, 713.6215915156635, 716.8695691719074, 712.0796911324604, 714.846504626858, 714.803324769867, 715.5588861461674, 717.1290958070643, 714.930909472399, 716.2635458040791, 714.5732526754672, 709.8484931441224, 682.6241690768863, 722.2162653033263, 739.6379945947324, 745.0120751273931, 742.7675731233569, 679.548586125331, 675.4832097275375, 680.1948490550133, 685.678632188486, 727.3067890599025, 728.53982581899]
Elapsed: 0.19590973788567723~0.01913334066213696
Time per graph: 0.0015186801386486608~0.00014832047024912373
Speed: 664.0049630971145~57.4354346214528
Total Time: 0.1777
best val loss: 0.2210703020655485 test_score: 0.8915

Testing...
Test loss: 0.3159 score: 0.8915 time: 0.29s
test Score 0.8915
Epoch Time List: [0.74394485889934, 0.7335607898421586, 0.7920819618739188, 0.7917306579183787, 0.7695630108937621, 0.7589730517938733, 0.8269050463568419, 0.7172369328327477, 0.7142091782297939, 0.7150966690387577, 0.7152995229698718, 0.7127560710068792, 0.7121328399516642, 0.7115761931054294, 0.7119976391550153, 0.7109059651847929, 0.7311053790617734, 0.7130616898648441, 0.7127799929585308, 0.7166074810083956, 0.7162425532005727, 0.7325491979718208, 0.7830471210181713, 0.9173911269754171, 0.7388032050803304, 0.7524096132256091, 0.7832214999943972, 0.7764312038198113, 0.8098035620059818, 0.7300136908888817, 0.738835024414584, 0.839144088095054, 0.805502570932731, 0.8032388670835644, 0.6974907091353089, 0.7350284939166158, 0.8397310809232295, 0.7659410270862281, 0.847139413934201, 0.7225354826077819, 0.7002160800620914, 0.8154383301734924, 0.7051741450559348, 0.7969984360970557, 0.6971530169248581, 0.7039294228889048, 0.8167677810415626, 0.7162398297805339, 0.7281244758050889, 0.7324486952275038, 0.7206020941957831, 0.7061080010607839, 0.8168762258719653, 0.7093837549909949, 0.8379231940489262, 0.7134282281622291, 0.707250518258661, 0.8382564261555672, 0.7510312970262021, 0.8204847450833768, 0.7424337512347847, 0.7349222889170051, 0.7164125789422542, 0.6991260200738907, 0.7381661317776889, 0.7315873659681529, 0.6940028730314225, 0.6951905672904104, 0.6948454871308059, 0.6945809049066156, 0.6962904778774828, 0.6959937771316618, 0.6939829459879547, 0.6934393199626356, 0.69395886012353, 0.6914153969846666, 0.6922193202190101, 0.6905417300295085, 0.691706907004118, 0.6921200018841773, 0.6908422480337322, 0.69280467601493, 0.6968000989872962, 0.6993377150502056, 0.7010037761647254, 0.7006095279939473, 0.6972203457262367, 0.6993346062954515, 0.7001368727069348, 0.6988400579430163, 0.699548427015543, 0.6958622676320374, 0.6963456289377064, 0.695933704264462, 0.6990023471880704, 0.7003216978628188, 0.7820561672560871, 0.6684395882766694, 0.6812784168869257, 0.798909141914919, 0.6657627706881613, 0.8213643569033593, 0.7220444229897112, 0.7127150357700884, 0.7600444231647998, 0.6767179081216455, 0.6850479929707944]
Total Epoch List: [46, 61]
Total Time List: [0.20348850404843688, 0.1777210719883442]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e40cb20>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.35s
Val loss: 0.6946 score: 0.4729 time: 0.19s
Test loss: 0.6986 score: 0.4609 time: 0.19s
Epoch 2/1000, LR 0.000067
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.33s
Val loss: 0.6821 score: 0.4961 time: 0.18s
Test loss: 0.6859 score: 0.4844 time: 0.19s
Epoch 3/1000, LR 0.000167
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6625 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6658 score: 0.5000 time: 0.21s
Epoch 4/1000, LR 0.000267
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.42s
Val loss: 0.6358 score: 0.5426 time: 0.20s
Test loss: 0.6384 score: 0.5547 time: 0.20s
Epoch 5/1000, LR 0.000367
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.38s
Val loss: 0.6061 score: 0.8837 time: 0.20s
Test loss: 0.6073 score: 0.8516 time: 0.19s
Epoch 6/1000, LR 0.000467
Train loss: 0.4790;  Loss pred: 0.4790; Loss self: 0.0000; time: 0.35s
Val loss: 0.5810 score: 0.8682 time: 0.31s
Test loss: 0.5798 score: 0.9062 time: 0.19s
Epoch 7/1000, LR 0.000567
Train loss: 0.4240;  Loss pred: 0.4240; Loss self: 0.0000; time: 0.34s
Val loss: 0.5571 score: 0.8760 time: 0.18s
Test loss: 0.5524 score: 0.8984 time: 0.19s
Epoch 8/1000, LR 0.000667
Train loss: 0.3799;  Loss pred: 0.3799; Loss self: 0.0000; time: 0.34s
Val loss: 0.5191 score: 0.8760 time: 0.19s
Test loss: 0.5128 score: 0.8984 time: 0.19s
Epoch 9/1000, LR 0.000767
Train loss: 0.3380;  Loss pred: 0.3380; Loss self: 0.0000; time: 0.45s
Val loss: 0.4880 score: 0.8837 time: 0.20s
Test loss: 0.4849 score: 0.8828 time: 0.19s
Epoch 10/1000, LR 0.000867
Train loss: 0.3079;  Loss pred: 0.3079; Loss self: 0.0000; time: 0.34s
Val loss: 0.4706 score: 0.8682 time: 0.19s
Test loss: 0.4723 score: 0.8516 time: 0.19s
Epoch 11/1000, LR 0.000967
Train loss: 0.2710;  Loss pred: 0.2710; Loss self: 0.0000; time: 0.35s
Val loss: 0.4471 score: 0.8760 time: 0.19s
Test loss: 0.4481 score: 0.8594 time: 0.19s
Epoch 12/1000, LR 0.000967
Train loss: 0.2456;  Loss pred: 0.2456; Loss self: 0.0000; time: 0.47s
Val loss: 0.4237 score: 0.8605 time: 0.21s
Test loss: 0.4135 score: 0.8906 time: 0.19s
Epoch 13/1000, LR 0.000967
Train loss: 0.2266;  Loss pred: 0.2266; Loss self: 0.0000; time: 0.34s
Val loss: 0.4455 score: 0.8295 time: 0.18s
Test loss: 0.4269 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 14/1000, LR 0.000967
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.35s
Val loss: 0.4679 score: 0.8062 time: 0.28s
Test loss: 0.4444 score: 0.8438 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 15/1000, LR 0.000967
Train loss: 0.2132;  Loss pred: 0.2132; Loss self: 0.0000; time: 0.34s
Val loss: 0.4693 score: 0.8062 time: 0.18s
Test loss: 0.4436 score: 0.8359 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 16/1000, LR 0.000967
Train loss: 0.2092;  Loss pred: 0.2092; Loss self: 0.0000; time: 0.34s
Val loss: 0.4727 score: 0.7907 time: 0.18s
Test loss: 0.4446 score: 0.8203 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 17/1000, LR 0.000967
Train loss: 0.1984;  Loss pred: 0.1984; Loss self: 0.0000; time: 0.47s
Val loss: 0.4538 score: 0.8140 time: 0.19s
Test loss: 0.4263 score: 0.8438 time: 0.19s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 011,   Train_Loss: 0.2456,   Val_Loss: 0.4237,   Val_Precision: 0.9434,   Val_Recall: 0.7692,   Val_accuracy: 0.8475,   Val_Score: 0.8605,   Val_Loss: 0.4237,   Test_Precision: 0.9808,   Test_Recall: 0.7969,   Test_accuracy: 0.8793,   Test_Score: 0.8906,   Test_loss: 0.4135


[0.20976876188069582, 0.2135372159536928, 0.22073019691742957, 0.21542047406546772, 0.2090913539286703, 0.2228641959372908, 0.23869828786700964, 0.20088337105698884, 0.20093045104295015, 0.2023111949674785, 0.20125023811124265, 0.20058216387405992, 0.2008342540357262, 0.2006916960235685, 0.20046817301772535, 0.2004460208117962, 0.20269259787164629, 0.2009336689952761, 0.20106379315257072, 0.20196046005003154, 0.2014124719426036, 0.2210318148136139, 0.224467562045902, 0.21109928679652512, 0.2088657058775425, 0.2082689858507365, 0.2219439740292728, 0.22373870899900794, 0.205861893016845, 0.2050199070945382, 0.20565594593062997, 0.2304959879256785, 0.22673256509006023, 0.19928125594742596, 0.21366594498977065, 0.20499944989569485, 0.2335040809120983, 0.22950646001845598, 0.20829920400865376, 0.21020146599039435, 0.19856894295662642, 0.20076181483455002, 0.20403376989997923, 0.25017180596478283, 0.20192450401373208, 0.2026169339660555, 0.29581013903953135, 0.19547391310334206, 0.18280357006005943, 0.19190420885570347, 0.18206214089877903, 0.18180695897899568, 0.17928843502886593, 0.18011141591705382, 0.18212678213603795, 0.18229567096568644, 0.18246095115318894, 0.1927579299081117, 0.19256651611067355, 0.1952154659666121, 0.19498436897993088, 0.1906112958677113, 0.19053489505313337, 0.18070389702916145, 0.19900334812700748, 0.1885368910152465, 0.1800970290787518, 0.18020699406042695, 0.18029805598780513, 0.1803305521607399, 0.18045595986768603, 0.1817368541378528, 0.18001143005676568, 0.1800303419586271, 0.18004152202047408, 0.17846091277897358, 0.17891867901198566, 0.17865252890624106, 0.17876925901509821, 0.17912898305803537, 0.17882400890812278, 0.1798055311664939, 0.17986321519128978, 0.18048511212691665, 0.18026057397946715, 0.18076807307079434, 0.17994905286468565, 0.18115949886851013, 0.18045832100324333, 0.1804692221339792, 0.18027866398915648, 0.17988392990082502, 0.18043701606802642, 0.18010130594484508, 0.1805273280479014, 0.181728919968009, 0.1889766079839319, 0.17861685785464942, 0.1744096449110657, 0.17315155593678355, 0.17367478692904115, 0.18983190110884607, 0.1909743989817798, 0.18965153908357024, 0.1881347820162773, 0.1773666930384934, 0.17706650402396917, 0.1923597059212625, 0.19603126193396747, 0.21165408892557025, 0.20345409703440964, 0.19838131801225245, 0.19649541098624468, 0.19464017497375607, 0.19516162713989615, 0.19761602790094912, 0.19756133598275483, 0.19870744622312486, 0.19439087994396687, 0.19490284100174904, 0.20156991994008422, 0.19384264783002436, 0.1939972119871527, 0.19915024214424193]
[0.0016261144331836885, 0.001655327255454983, 0.0017110867978095314, 0.0016699261555462614, 0.001620863208749382, 0.0017276294258704713, 0.0018503743245504624, 0.001557235434550301, 0.0015576003956817841, 0.0015683038369571978, 0.0015600793652034313, 0.0015549004951477513, 0.0015568546824474899, 0.0015557495815780504, 0.001554016845098646, 0.0015538451225720635, 0.001571260448617413, 0.001557625341048652, 0.0015586340554462846, 0.0015655849616281515, 0.0015613369918031285, 0.0017134249210357666, 0.0017400586205108682, 0.0016364285798180242, 0.001619113999050717, 0.00161448826240881, 0.001720495922707541, 0.0017344085968915345, 0.001595828628037558, 0.0015893016053840172, 0.0015942321389971316, 0.001786790604075027, 0.0017576167836438777, 0.0015448159375769455, 0.0016563251549594623, 0.0015891430224472469, 0.001810109154357351, 0.00177911984510431, 0.0016147225116949905, 0.0016294687286077083, 0.0015392941314467164, 0.0015562931382523259, 0.001581657131007591, 0.0019393163253083941, 0.0015653062326645897, 0.0015706739067136086, 0.002293101853019623, 0.0015153016519638919, 0.001417081938450073, 0.0014876295260132053, 0.0014113344255719305, 0.0014093562711550052, 0.0013898328296811312, 0.0013962125264887894, 0.0014118355204344028, 0.001413144736168112, 0.0014144259779316972, 0.0014942475186675325, 0.001492763690780415, 0.0015132981857876907, 0.001511506736278534, 0.0014776069447109404, 0.0014770146903343672, 0.001400805403326833, 0.001542661613387655, 0.0014615262869398953, 0.0013961010006104792, 0.001396953442328891, 0.0013976593487426755, 0.0013979112570599993, 0.0013988834098270235, 0.0014088128227740526, 0.0013954374423005092, 0.0013955840461909077, 0.0013956707133370083, 0.001383417928519175, 0.001386966503968881, 0.0013849033248545819, 0.001385808209419366, 0.0013885967678917471, 0.0013862326271947502, 0.0013938413268720458, 0.0013942884898549596, 0.001399109396332687, 0.001397368790538505, 0.0014013028920216615, 0.0013949538981758577, 0.0014043372005310862, 0.0013989017132034366, 0.001398986218092862, 0.0013975090231717557, 0.0013944490689986437, 0.0013987365586668715, 0.0013961341546112021, 0.0013994366515341194, 0.001408751317581465, 0.0014649349456118752, 0.0013846268050748017, 0.0013520127512485713, 0.0013422601235409577, 0.001346316177744505, 0.0014715651248747756, 0.0014804216975331768, 0.0014701669696400793, 0.0014584091629168785, 0.0013749356049495612, 0.0013726085583253423, 0.0015028102025098633, 0.0015314942338591209, 0.0016535475697310176, 0.0015894851330813253, 0.0015498540469707223, 0.0015351203983300366, 0.0015206263669824693, 0.0015247002120304387, 0.001543875217976165, 0.001543447937365272, 0.001552401923618163, 0.0015186787495622411, 0.0015226784453261644, 0.001574764999531908, 0.0015143956861720653, 0.0015156032186496304, 0.00155586126675189]
[614.9628707508301, 604.1101520588086, 584.4238885369007, 598.8288743659344, 616.9552091762113, 578.8278348501427, 540.4311909931761, 642.1636560619238, 642.0131907852307, 637.6315459000513, 640.9930304216298, 643.1279706454637, 642.320706790647, 642.776968633773, 643.4936681375045, 643.5647835639568, 636.4317264397015, 642.002908945204, 641.587418487189, 638.7388896224683, 640.4767229943986, 583.6263892995669, 574.6932822909192, 611.0868584996255, 617.6217366944501, 619.391310103428, 581.2277650889762, 576.5654078238737, 626.6337014079841, 629.2071917704844, 627.261222213887, 559.6626698838462, 568.9522365204135, 647.3263096757709, 603.7461889688408, 629.2699812884185, 552.4528714706341, 562.0756818332101, 619.3014544339819, 613.6969568323323, 649.6484197338834, 642.5524699819549, 632.2482796021362, 515.6456360160728, 638.8526277683824, 636.6693912247799, 436.09052894147334, 659.9346068843519, 705.6754961493232, 672.2103739631767, 708.5492863215316, 709.5437970276126, 719.5109934404338, 716.2233406649137, 708.2978048974951, 707.6415984901896, 707.0005893573103, 669.2331675355442, 669.8983946194465, 660.8082989800768, 661.5914941021634, 676.7699648268957, 677.0413365175261, 713.8750304825046, 648.2302997116908, 684.2162258290771, 716.2805553199415, 715.8434702969618, 715.4819240465089, 715.3529917937268, 714.8558578757134, 709.8175029603499, 716.6211609969462, 716.5458810806768, 716.5013856377547, 722.8473618745207, 720.9979456161666, 722.0720623982922, 721.6005744539397, 720.1514673826164, 721.3796446442399, 717.4417781427999, 717.2116870189645, 714.7403931537996, 715.6306958985604, 713.6215915156635, 716.8695691719074, 712.0796911324604, 714.846504626858, 714.803324769867, 715.5588861461674, 717.1290958070643, 714.930909472399, 716.2635458040791, 714.5732526754672, 709.8484931441224, 682.6241690768863, 722.2162653033263, 739.6379945947324, 745.0120751273931, 742.7675731233569, 679.548586125331, 675.4832097275375, 680.1948490550133, 685.678632188486, 727.3067890599025, 728.53982581899, 665.4200233202348, 652.9570780558277, 604.7603457593118, 629.1345412343882, 645.2220465240302, 651.4147040765264, 657.6237409222357, 655.8666366736468, 647.7207408710659, 647.9000527267802, 644.163077091088, 658.4671052309449, 656.7374766940998, 635.0153834364147, 660.3294034254005, 659.8032965982867, 642.7308278505193]
Elapsed: 0.1961472434810393~0.017860880340267053
Time per graph: 0.0015221622616439934~0.00013866422964390143
Speed: 661.8048188055003~53.89420276986293
Total Time: 0.1997
best val loss: 0.4236732975457066 test_score: 0.8906

Testing...
Test loss: 0.6073 score: 0.8516 time: 0.18s
test Score 0.8516
Epoch Time List: [0.74394485889934, 0.7335607898421586, 0.7920819618739188, 0.7917306579183787, 0.7695630108937621, 0.7589730517938733, 0.8269050463568419, 0.7172369328327477, 0.7142091782297939, 0.7150966690387577, 0.7152995229698718, 0.7127560710068792, 0.7121328399516642, 0.7115761931054294, 0.7119976391550153, 0.7109059651847929, 0.7311053790617734, 0.7130616898648441, 0.7127799929585308, 0.7166074810083956, 0.7162425532005727, 0.7325491979718208, 0.7830471210181713, 0.9173911269754171, 0.7388032050803304, 0.7524096132256091, 0.7832214999943972, 0.7764312038198113, 0.8098035620059818, 0.7300136908888817, 0.738835024414584, 0.839144088095054, 0.805502570932731, 0.8032388670835644, 0.6974907091353089, 0.7350284939166158, 0.8397310809232295, 0.7659410270862281, 0.847139413934201, 0.7225354826077819, 0.7002160800620914, 0.8154383301734924, 0.7051741450559348, 0.7969984360970557, 0.6971530169248581, 0.7039294228889048, 0.8167677810415626, 0.7162398297805339, 0.7281244758050889, 0.7324486952275038, 0.7206020941957831, 0.7061080010607839, 0.8168762258719653, 0.7093837549909949, 0.8379231940489262, 0.7134282281622291, 0.707250518258661, 0.8382564261555672, 0.7510312970262021, 0.8204847450833768, 0.7424337512347847, 0.7349222889170051, 0.7164125789422542, 0.6991260200738907, 0.7381661317776889, 0.7315873659681529, 0.6940028730314225, 0.6951905672904104, 0.6948454871308059, 0.6945809049066156, 0.6962904778774828, 0.6959937771316618, 0.6939829459879547, 0.6934393199626356, 0.69395886012353, 0.6914153969846666, 0.6922193202190101, 0.6905417300295085, 0.691706907004118, 0.6921200018841773, 0.6908422480337322, 0.69280467601493, 0.6968000989872962, 0.6993377150502056, 0.7010037761647254, 0.7006095279939473, 0.6972203457262367, 0.6993346062954515, 0.7001368727069348, 0.6988400579430163, 0.699548427015543, 0.6958622676320374, 0.6963456289377064, 0.695933704264462, 0.6990023471880704, 0.7003216978628188, 0.7820561672560871, 0.6684395882766694, 0.6812784168869257, 0.798909141914919, 0.6657627706881613, 0.8213643569033593, 0.7220444229897112, 0.7127150357700884, 0.7600444231647998, 0.6767179081216455, 0.6850479929707944, 0.7191649880260229, 0.7043575292918831, 0.7521390770561993, 0.8183579419273883, 0.77002262766473, 0.8501144191250205, 0.7080379559192806, 0.7239342569373548, 0.8400691768620163, 0.7183414930477738, 0.7300054559018463, 0.8668031820561737, 0.7168424651026726, 0.8282475902233273, 0.7077833148650825, 0.714252911740914, 0.8531271424144506]
Total Epoch List: [46, 61, 17]
Total Time List: [0.20348850404843688, 0.1777210719883442, 0.19974155421368778]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e25dc00>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7093;  Loss pred: 0.7093; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5039 time: 0.21s
Test loss: 0.6918 score: 0.5116 time: 0.21s
Epoch 2/1000, LR 0.000050
Train loss: 0.7110;  Loss pred: 0.7110; Loss self: 0.0000; time: 0.32s
Val loss: 0.6785 score: 0.6202 time: 0.26s
Test loss: 0.6809 score: 0.6822 time: 0.21s
Epoch 3/1000, LR 0.000150
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.31s
Val loss: 0.6664 score: 0.6202 time: 0.21s
Test loss: 0.6689 score: 0.5969 time: 0.21s
Epoch 4/1000, LR 0.000250
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.31s
Val loss: 0.6586 score: 0.5194 time: 0.20s
Test loss: 0.6611 score: 0.5349 time: 0.21s
Epoch 5/1000, LR 0.000350
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.31s
Val loss: 0.6611 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6640 score: 0.5039 time: 0.21s
     INFO: Early stopping counter 1 of 5
Epoch 6/1000, LR 0.000450
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.32s
Val loss: 0.6637 score: 0.5039 time: 0.20s
Test loss: 0.6679 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 2 of 5
Epoch 7/1000, LR 0.000550
Train loss: 0.5036;  Loss pred: 0.5036; Loss self: 0.0000; time: 0.31s
Val loss: 0.6685 score: 0.5116 time: 0.21s
Test loss: 0.6741 score: 0.5194 time: 0.21s
     INFO: Early stopping counter 3 of 5
Epoch 8/1000, LR 0.000650
Train loss: 0.4680;  Loss pred: 0.4680; Loss self: 0.0000; time: 0.32s
Val loss: 0.6611 score: 0.5116 time: 0.20s
Test loss: 0.6685 score: 0.5271 time: 0.21s
     INFO: Early stopping counter 4 of 5
Epoch 9/1000, LR 0.000750
Train loss: 0.4272;  Loss pred: 0.4272; Loss self: 0.0000; time: 0.31s
Val loss: 0.6328 score: 0.5504 time: 0.20s
Test loss: 0.6419 score: 0.5426 time: 0.21s
Epoch 10/1000, LR 0.000850
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.33s
Val loss: 0.5899 score: 0.5736 time: 0.21s
Test loss: 0.6009 score: 0.5504 time: 0.21s
Epoch 11/1000, LR 0.000950
Train loss: 0.3452;  Loss pred: 0.3452; Loss self: 0.0000; time: 0.31s
Val loss: 0.5332 score: 0.6512 time: 0.20s
Test loss: 0.5430 score: 0.6202 time: 0.21s
Epoch 12/1000, LR 0.000950
Train loss: 0.3052;  Loss pred: 0.3052; Loss self: 0.0000; time: 0.33s
Val loss: 0.4856 score: 0.7287 time: 0.20s
Test loss: 0.4939 score: 0.7132 time: 0.20s
Epoch 13/1000, LR 0.000950
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.33s
Val loss: 0.4696 score: 0.7519 time: 0.22s
Test loss: 0.4767 score: 0.7442 time: 0.21s
Epoch 14/1000, LR 0.000950
Train loss: 0.2622;  Loss pred: 0.2622; Loss self: 0.0000; time: 0.34s
Val loss: 0.4949 score: 0.6977 time: 0.21s
Test loss: 0.5007 score: 0.6667 time: 0.21s
     INFO: Early stopping counter 1 of 5
Epoch 15/1000, LR 0.000950
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.32s
Val loss: 0.5357 score: 0.6434 time: 0.20s
Test loss: 0.5395 score: 0.6202 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 16/1000, LR 0.000950
Train loss: 0.2218;  Loss pred: 0.2218; Loss self: 0.0000; time: 0.32s
Val loss: 0.5741 score: 0.6124 time: 0.20s
Test loss: 0.5767 score: 0.5969 time: 0.20s
     INFO: Early stopping counter 3 of 5
Epoch 17/1000, LR 0.000950
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.31s
Val loss: 0.5793 score: 0.6124 time: 0.20s
Test loss: 0.5806 score: 0.5969 time: 0.20s
     INFO: Early stopping counter 4 of 5
Epoch 18/1000, LR 0.000950
Train loss: 0.1951;  Loss pred: 0.1951; Loss self: 0.0000; time: 0.32s
Val loss: 0.5312 score: 0.6589 time: 0.19s
Test loss: 0.5298 score: 0.6357 time: 0.20s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 012,   Train_Loss: 0.2873,   Val_Loss: 0.4696,   Val_Precision: 0.6667,   Val_Recall: 1.0000,   Val_accuracy: 0.8000,   Val_Score: 0.7519,   Val_Loss: 0.4696,   Test_Precision: 0.6667,   Test_Recall: 0.9846,   Test_accuracy: 0.7950,   Test_Score: 0.7442,   Test_loss: 0.4767


[0.21445385785773396, 0.2136310078203678, 0.21438535186462104, 0.21282021701335907, 0.21259720902889967, 0.21179628209210932, 0.2124520381912589, 0.21174292406067252, 0.2142084559891373, 0.21298064407892525, 0.21488667093217373, 0.20628816401585937, 0.21975385793484747, 0.21042691287584603, 0.20709026604890823, 0.2037930937949568, 0.20524711022153497, 0.20402446086518466]
[0.0016624330066491006, 0.0016560543241888979, 0.0016619019524389228, 0.0016497691241345665, 0.0016480403800689897, 0.0016418316441248785, 0.0016469150247384412, 0.0016414180159742056, 0.0016605306665824597, 0.0016510127447978702, 0.0016657881467610366, 0.0015991330543865068, 0.001703518278564709, 0.0016312163788825274, 0.0016053508996039397, 0.001579791424767107, 0.0015910628699343796, 0.001581584967947168]
[601.527999023347, 603.8449254916683, 601.7202149214945, 606.1454208173393, 606.7812488660857, 609.0758474405062, 607.1958692336403, 609.2293311441969, 602.21712258895, 605.6888434997683, 600.3164339621476, 625.3388342245487, 587.0204109829365, 613.0394550630094, 622.9167718077789, 632.99495384172, 628.5106760371092, 632.2771272276055]
Elapsed: 0.21125436248257756~0.0041884008669781595
Time per graph: 0.0016376307169192058~3.2468223775024486e-05
Speed: 610.8800825652139~12.19935973374213
Total Time: 0.2044
best val loss: 0.46956688997357393 test_score: 0.7442

Testing...
Test loss: 0.4767 score: 0.7442 time: 0.20s
test Score 0.7442
Epoch Time List: [0.7334908852353692, 0.7831565821543336, 0.7272145219612867, 0.7274080459028482, 0.72348140203394, 0.7245048100594431, 0.7289620540104806, 0.7258917558938265, 0.7269605239853263, 0.7460497198626399, 0.7267897259443998, 0.7235786626115441, 0.7558491001836956, 0.754764816025272, 0.7198657398112118, 0.7142253771889955, 0.712319347076118, 0.7095127070788294]
Total Epoch List: [18]
Total Time List: [0.20441348594613373]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84e3d8af0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.32s
Val loss: 0.7251 score: 0.4884 time: 0.22s
Test loss: 0.7251 score: 0.4806 time: 0.21s
Epoch 2/1000, LR 0.000050
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.32s
Val loss: 0.7024 score: 0.4961 time: 0.20s
Test loss: 0.7039 score: 0.4729 time: 0.20s
Epoch 3/1000, LR 0.000150
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.31s
Val loss: 0.6681 score: 0.5504 time: 0.19s
Test loss: 0.6722 score: 0.5116 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.31s
Val loss: 0.6247 score: 0.7209 time: 0.19s
Test loss: 0.6318 score: 0.7209 time: 0.19s
Epoch 5/1000, LR 0.000350
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.31s
Val loss: 0.5807 score: 0.8140 time: 0.19s
Test loss: 0.5910 score: 0.8062 time: 0.19s
Epoch 6/1000, LR 0.000450
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.31s
Val loss: 0.5426 score: 0.8372 time: 0.18s
Test loss: 0.5561 score: 0.8140 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.4820;  Loss pred: 0.4820; Loss self: 0.0000; time: 0.31s
Val loss: 0.4964 score: 0.8915 time: 0.19s
Test loss: 0.5131 score: 0.8837 time: 0.19s
Epoch 8/1000, LR 0.000650
Train loss: 0.4175;  Loss pred: 0.4175; Loss self: 0.0000; time: 0.31s
Val loss: 0.4601 score: 0.9070 time: 0.19s
Test loss: 0.4800 score: 0.8992 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.3923;  Loss pred: 0.3923; Loss self: 0.0000; time: 0.32s
Val loss: 0.4325 score: 0.9070 time: 0.19s
Test loss: 0.4546 score: 0.9070 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3335;  Loss pred: 0.3335; Loss self: 0.0000; time: 0.31s
Val loss: 0.4167 score: 0.9225 time: 0.19s
Test loss: 0.4396 score: 0.8837 time: 0.19s
Epoch 11/1000, LR 0.000950
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.32s
Val loss: 0.4050 score: 0.9302 time: 0.19s
Test loss: 0.4277 score: 0.8837 time: 0.19s
Epoch 12/1000, LR 0.000950
Train loss: 0.2890;  Loss pred: 0.2890; Loss self: 0.0000; time: 0.32s
Val loss: 0.3909 score: 0.9302 time: 0.19s
Test loss: 0.4136 score: 0.8760 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.2638;  Loss pred: 0.2638; Loss self: 0.0000; time: 0.32s
Val loss: 0.3791 score: 0.9302 time: 0.19s
Test loss: 0.4019 score: 0.8760 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2296;  Loss pred: 0.2296; Loss self: 0.0000; time: 0.32s
Val loss: 0.3666 score: 0.9302 time: 0.19s
Test loss: 0.3910 score: 0.9070 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2106;  Loss pred: 0.2106; Loss self: 0.0000; time: 0.33s
Val loss: 0.3561 score: 0.9380 time: 0.19s
Test loss: 0.3828 score: 0.9147 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.1893;  Loss pred: 0.1893; Loss self: 0.0000; time: 0.43s
Val loss: 0.3459 score: 0.9380 time: 0.18s
Test loss: 0.3739 score: 0.9147 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.31s
Val loss: 0.3350 score: 0.9380 time: 0.18s
Test loss: 0.3633 score: 0.9147 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1579;  Loss pred: 0.1579; Loss self: 0.0000; time: 0.32s
Val loss: 0.3308 score: 0.9225 time: 0.18s
Test loss: 0.3631 score: 0.9070 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.43s
Val loss: 0.3255 score: 0.9302 time: 0.19s
Test loss: 0.3595 score: 0.9147 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.32s
Val loss: 0.3117 score: 0.9225 time: 0.19s
Test loss: 0.3442 score: 0.9225 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 0.31s
Val loss: 0.3055 score: 0.9302 time: 0.29s
Test loss: 0.3386 score: 0.9147 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.31s
Val loss: 0.2931 score: 0.9302 time: 0.18s
Test loss: 0.3246 score: 0.9147 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1049;  Loss pred: 0.1049; Loss self: 0.0000; time: 0.32s
Val loss: 0.2874 score: 0.9380 time: 0.19s
Test loss: 0.3198 score: 0.9225 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.32s
Val loss: 0.2852 score: 0.9302 time: 0.22s
Test loss: 0.3198 score: 0.9147 time: 0.19s
Epoch 25/1000, LR 0.000950
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.32s
Val loss: 0.2757 score: 0.9225 time: 0.19s
Test loss: 0.3096 score: 0.9225 time: 0.20s
Epoch 26/1000, LR 0.000949
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.32s
Val loss: 0.2687 score: 0.9225 time: 0.19s
Test loss: 0.3029 score: 0.9225 time: 0.24s
Epoch 27/1000, LR 0.000949
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.38s
Val loss: 0.2658 score: 0.9302 time: 0.19s
Test loss: 0.3014 score: 0.9225 time: 0.19s
Epoch 28/1000, LR 0.000949
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.33s
Val loss: 0.2566 score: 0.9225 time: 0.19s
Test loss: 0.2911 score: 0.9225 time: 0.19s
Epoch 29/1000, LR 0.000949
Train loss: 0.0614;  Loss pred: 0.0614; Loss self: 0.0000; time: 0.32s
Val loss: 0.2490 score: 0.9225 time: 0.26s
Test loss: 0.2828 score: 0.9225 time: 0.19s
Epoch 30/1000, LR 0.000949
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.31s
Val loss: 0.2452 score: 0.9225 time: 0.19s
Test loss: 0.2797 score: 0.9225 time: 0.19s
Epoch 31/1000, LR 0.000949
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.32s
Val loss: 0.2409 score: 0.9225 time: 0.19s
Test loss: 0.2762 score: 0.9225 time: 0.19s
Epoch 32/1000, LR 0.000949
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.34s
Val loss: 0.2406 score: 0.9225 time: 0.19s
Test loss: 0.2780 score: 0.9147 time: 0.19s
Epoch 33/1000, LR 0.000949
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.32s
Val loss: 0.2398 score: 0.9147 time: 0.19s
Test loss: 0.2789 score: 0.9070 time: 0.19s
Epoch 34/1000, LR 0.000949
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.31s
Val loss: 0.2268 score: 0.9147 time: 0.18s
Test loss: 0.2628 score: 0.9225 time: 0.23s
Epoch 35/1000, LR 0.000949
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.33s
Val loss: 0.2191 score: 0.9147 time: 0.20s
Test loss: 0.2544 score: 0.9147 time: 0.20s
Epoch 36/1000, LR 0.000949
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.32s
Val loss: 0.2120 score: 0.9225 time: 0.20s
Test loss: 0.2473 score: 0.9147 time: 0.20s
Epoch 37/1000, LR 0.000948
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.31s
Val loss: 0.2074 score: 0.9380 time: 0.25s
Test loss: 0.2433 score: 0.9147 time: 0.21s
Epoch 38/1000, LR 0.000948
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.33s
Val loss: 0.2060 score: 0.9147 time: 0.20s
Test loss: 0.2442 score: 0.9147 time: 0.21s
Epoch 39/1000, LR 0.000948
Train loss: 0.0319;  Loss pred: 0.0319; Loss self: 0.0000; time: 0.34s
Val loss: 0.2115 score: 0.9070 time: 0.20s
Test loss: 0.2539 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 1 of 5
Epoch 40/1000, LR 0.000948
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.40s
Val loss: 0.2203 score: 0.9070 time: 0.20s
Test loss: 0.2676 score: 0.9147 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 41/1000, LR 0.000948
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.32s
Val loss: 0.2147 score: 0.9147 time: 0.19s
Test loss: 0.2625 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 42/1000, LR 0.000948
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.31s
Val loss: 0.2049 score: 0.9070 time: 0.30s
Test loss: 0.2518 score: 0.9147 time: 0.19s
Epoch 43/1000, LR 0.000948
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.31s
Val loss: 0.1984 score: 0.9070 time: 0.18s
Test loss: 0.2442 score: 0.9225 time: 0.19s
Epoch 44/1000, LR 0.000947
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.32s
Val loss: 0.1974 score: 0.9302 time: 0.19s
Test loss: 0.2438 score: 0.9147 time: 0.19s
Epoch 45/1000, LR 0.000947
Train loss: 0.0274;  Loss pred: 0.0274; Loss self: 0.0000; time: 0.33s
Val loss: 0.1986 score: 0.9302 time: 0.18s
Test loss: 0.2469 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 46/1000, LR 0.000947
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.31s
Val loss: 0.2029 score: 0.9070 time: 0.18s
Test loss: 0.2531 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 47/1000, LR 0.000947
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.32s
Val loss: 0.2193 score: 0.9147 time: 0.18s
Test loss: 0.2762 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 48/1000, LR 0.000947
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.42s
Val loss: 0.2217 score: 0.9147 time: 0.20s
Test loss: 0.2796 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 4 of 5
Epoch 49/1000, LR 0.000947
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.33s
Val loss: 0.2232 score: 0.9147 time: 0.20s
Test loss: 0.2806 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 043,   Train_Loss: 0.0368,   Val_Loss: 0.1974,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.1974,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2438


[0.21445385785773396, 0.2136310078203678, 0.21438535186462104, 0.21282021701335907, 0.21259720902889967, 0.21179628209210932, 0.2124520381912589, 0.21174292406067252, 0.2142084559891373, 0.21298064407892525, 0.21488667093217373, 0.20628816401585937, 0.21975385793484747, 0.21042691287584603, 0.20709026604890823, 0.2037930937949568, 0.20524711022153497, 0.20402446086518466, 0.2185888651292771, 0.2030803789384663, 0.1960931308567524, 0.19697525002993643, 0.19555600103922188, 0.19383319607004523, 0.19626410212367773, 0.1964410301297903, 0.1956142841372639, 0.19689962500706315, 0.19687107298523188, 0.19442052789963782, 0.19632504903711379, 0.19790835003368556, 0.1977998181246221, 0.19424049509689212, 0.19789624493569136, 0.2009008601307869, 0.19719042791984975, 0.19787745410576463, 0.1942445281893015, 0.19599503697827458, 0.19702795892953873, 0.19647802715189755, 0.20037270500324667, 0.2495176389347762, 0.19724041782319546, 0.19811009988188744, 0.19510304904542863, 0.19816524698399007, 0.19798173592425883, 0.19830141006968915, 0.1981414631009102, 0.2412329250946641, 0.2079277748707682, 0.20212354697287083, 0.21115369186736643, 0.21253502904437482, 0.2349014279898256, 0.20860409806482494, 0.1977841821499169, 0.19695473089814186, 0.19555128598585725, 0.19713037996552885, 0.19519494380801916, 0.1976205729879439, 0.19497432396747172, 0.2129155199509114, 0.2152250581420958]
[0.0016624330066491006, 0.0016560543241888979, 0.0016619019524389228, 0.0016497691241345665, 0.0016480403800689897, 0.0016418316441248785, 0.0016469150247384412, 0.0016414180159742056, 0.0016605306665824597, 0.0016510127447978702, 0.0016657881467610366, 0.0015991330543865068, 0.001703518278564709, 0.0016312163788825274, 0.0016053508996039397, 0.001579791424767107, 0.0015910628699343796, 0.001581584967947168, 0.0016944873265835436, 0.0015742665033989637, 0.001520101789587228, 0.0015269399227126855, 0.0015159379925521075, 0.0015025829152716685, 0.0015214271482455637, 0.001522798683176669, 0.0015163897995136735, 0.0015263536822252958, 0.0015261323487227277, 0.0015071358751909908, 0.0015218996049388665, 0.001534173256075082, 0.001533331923446683, 0.0015057402720689312, 0.0015340794181061346, 0.0015573710087657898, 0.0015286079683709283, 0.0015339337527578653, 0.0015057715363511743, 0.0015193413719246092, 0.0015273485188336336, 0.0015230854817976554, 0.0015532767829709044, 0.0019342452630602806, 0.001528995487001515, 0.0015357372083867242, 0.0015124267367862684, 0.0015361647053022487, 0.0015347421389477428, 0.0015372202330983656, 0.0015359803341155828, 0.0018700226751524349, 0.0016118432160524667, 0.0015668492013400839, 0.001636850324553228, 0.0016475583646850761, 0.00182094130224671, 0.0016170860315102707, 0.001533210714340441, 0.0015267808596755182, 0.0015159014417508313, 0.0015281424803529368, 0.0015131390992869703, 0.0015319424262631308, 0.001511428867964897, 0.0016505079065962124, 0.0016684113034270993]
[601.527999023347, 603.8449254916683, 601.7202149214945, 606.1454208173393, 606.7812488660857, 609.0758474405062, 607.1958692336403, 609.2293311441969, 602.21712258895, 605.6888434997683, 600.3164339621476, 625.3388342245487, 587.0204109829365, 613.0394550630094, 622.9167718077789, 632.99495384172, 628.5106760371092, 632.2771272276055, 590.1489992351954, 635.2164629310998, 657.8506826648381, 654.9046135511669, 659.6575881817454, 665.5206776520542, 657.2776101393692, 656.6856217093169, 659.4610438033237, 655.1561486994834, 655.2511653638259, 663.5101827652239, 657.0735656641223, 651.8168636040025, 652.1745127122647, 664.1251605935802, 651.856734532381, 642.1077536254485, 654.1899693652142, 651.9186361223855, 664.1113713858788, 658.1799314351987, 654.7294135353301, 656.5619671062243, 643.8002621061077, 516.9975178937973, 654.0241671746733, 651.1530713320994, 661.1890517916154, 650.9718629443739, 651.5752546454643, 650.5248750105482, 651.0500022617802, 534.7528740091267, 620.4077357158101, 638.2235119657507, 610.9294081442334, 606.958770890733, 549.1665210548973, 618.3962884560038, 652.2260708503994, 654.972842803732, 659.6734935781992, 654.3892424016915, 660.8777742054418, 652.766045809764, 661.6255790763585, 605.8741045732199, 599.3725875303594]
Elapsed: 0.20450542534618124~0.011096780873688537
Time per graph: 0.001585313374776599~8.602155716037628e-05
Speed: 632.4959265489358~31.53974615770994
Total Time: 0.2157
best val loss: 0.19739204349155112 test_score: 0.9147

Testing...
Test loss: 0.3828 score: 0.9147 time: 0.20s
test Score 0.9147
Epoch Time List: [0.7334908852353692, 0.7831565821543336, 0.7272145219612867, 0.7274080459028482, 0.72348140203394, 0.7245048100594431, 0.7289620540104806, 0.7258917558938265, 0.7269605239853263, 0.7460497198626399, 0.7267897259443998, 0.7235786626115441, 0.7558491001836956, 0.754764816025272, 0.7198657398112118, 0.7142253771889955, 0.712319347076118, 0.7095127070788294, 0.7526217270642519, 0.7135401603300124, 0.6898416087497026, 0.6895122609566897, 0.6873944222461432, 0.6838083032052964, 0.689288706984371, 0.6911316991318017, 0.6925805809441954, 0.691867504036054, 0.6930211558938026, 0.6910576082300395, 0.7089786911383271, 0.6963223742786795, 0.7080103859771043, 0.8050252050161362, 0.6864933981560171, 0.6955126549582928, 0.8130172821693122, 0.7022668379358947, 0.7921418629121035, 0.6876957069616765, 0.6984178158454597, 0.7381714289076626, 0.6972884668502957, 0.7487772349268198, 0.755674592917785, 0.7084511597640812, 0.7745181557256728, 0.6906549041159451, 0.7034589541144669, 0.7207307771313936, 0.6989921561907977, 0.732268736930564, 0.7260997579433024, 0.7200339040718973, 0.7712792153470218, 0.7360864309594035, 0.768457303987816, 0.8027662329841405, 0.7036931610200554, 0.8055879066232592, 0.6864374850410968, 0.6947399717755616, 0.7035448867827654, 0.6914687152020633, 0.6925276711117476, 0.834298200905323, 0.7388717208523303]
Total Epoch List: [18, 49]
Total Time List: [0.20441348594613373, 0.2156830991152674]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f84df709d0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7101 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7106 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000067
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.18s
Epoch 3/1000, LR 0.000167
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.35s
Val loss: 0.6744 score: 0.4884 time: 0.20s
Test loss: 0.6754 score: 0.5234 time: 0.18s
Epoch 4/1000, LR 0.000267
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.34s
Val loss: 0.6569 score: 0.7597 time: 0.20s
Test loss: 0.6585 score: 0.7500 time: 0.18s
Epoch 5/1000, LR 0.000367
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.35s
Val loss: 0.6450 score: 0.5814 time: 0.20s
Test loss: 0.6466 score: 0.5625 time: 0.18s
Epoch 6/1000, LR 0.000467
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.35s
Val loss: 0.6269 score: 0.6434 time: 0.20s
Test loss: 0.6292 score: 0.5859 time: 0.18s
Epoch 7/1000, LR 0.000567
Train loss: 0.4734;  Loss pred: 0.4734; Loss self: 0.0000; time: 0.35s
Val loss: 0.6007 score: 0.7597 time: 0.20s
Test loss: 0.6037 score: 0.7344 time: 0.18s
Epoch 8/1000, LR 0.000667
Train loss: 0.4184;  Loss pred: 0.4184; Loss self: 0.0000; time: 0.35s
Val loss: 0.5669 score: 0.8682 time: 0.20s
Test loss: 0.5711 score: 0.8984 time: 0.18s
Epoch 9/1000, LR 0.000767
Train loss: 0.3753;  Loss pred: 0.3753; Loss self: 0.0000; time: 0.35s
Val loss: 0.5413 score: 0.9070 time: 0.20s
Test loss: 0.5464 score: 0.9219 time: 0.18s
Epoch 10/1000, LR 0.000867
Train loss: 0.3528;  Loss pred: 0.3528; Loss self: 0.0000; time: 0.35s
Val loss: 0.5192 score: 0.9380 time: 0.20s
Test loss: 0.5257 score: 0.9297 time: 0.18s
Epoch 11/1000, LR 0.000967
Train loss: 0.3052;  Loss pred: 0.3052; Loss self: 0.0000; time: 0.35s
Val loss: 0.5175 score: 0.8837 time: 0.20s
Test loss: 0.5246 score: 0.8828 time: 0.18s
Epoch 12/1000, LR 0.000967
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.35s
Val loss: 0.5238 score: 0.7442 time: 0.20s
Test loss: 0.5301 score: 0.7109 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 13/1000, LR 0.000967
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.35s
Val loss: 0.5283 score: 0.6899 time: 0.20s
Test loss: 0.5322 score: 0.6562 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 14/1000, LR 0.000967
Train loss: 0.2704;  Loss pred: 0.2704; Loss self: 0.0000; time: 0.35s
Val loss: 0.5128 score: 0.7209 time: 0.20s
Test loss: 0.5151 score: 0.6875 time: 0.18s
Epoch 15/1000, LR 0.000967
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.35s
Val loss: 0.4490 score: 0.8992 time: 0.20s
Test loss: 0.4515 score: 0.8984 time: 0.18s
Epoch 16/1000, LR 0.000967
Train loss: 0.2291;  Loss pred: 0.2291; Loss self: 0.0000; time: 0.35s
Val loss: 0.4138 score: 0.9535 time: 0.32s
Test loss: 0.4166 score: 0.9453 time: 0.18s
Epoch 17/1000, LR 0.000967
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.35s
Val loss: 0.3934 score: 0.9225 time: 0.20s
Test loss: 0.3965 score: 0.9297 time: 0.18s
Epoch 18/1000, LR 0.000967
Train loss: 0.1925;  Loss pred: 0.1925; Loss self: 0.0000; time: 0.35s
Val loss: 0.3875 score: 0.8915 time: 0.20s
Test loss: 0.3915 score: 0.9141 time: 0.18s
Epoch 19/1000, LR 0.000967
Train loss: 0.1798;  Loss pred: 0.1798; Loss self: 0.0000; time: 0.35s
Val loss: 0.3736 score: 0.8915 time: 0.20s
Test loss: 0.3777 score: 0.9062 time: 0.18s
Epoch 20/1000, LR 0.000966
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 0.35s
Val loss: 0.3600 score: 0.8915 time: 0.20s
Test loss: 0.3643 score: 0.9062 time: 0.18s
Epoch 21/1000, LR 0.000966
Train loss: 0.1663;  Loss pred: 0.1663; Loss self: 0.0000; time: 0.35s
Val loss: 0.3378 score: 0.9070 time: 0.20s
Test loss: 0.3421 score: 0.9297 time: 0.18s
Epoch 22/1000, LR 0.000966
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 0.35s
Val loss: 0.3247 score: 0.8992 time: 0.20s
Test loss: 0.3302 score: 0.9219 time: 0.18s
Epoch 23/1000, LR 0.000966
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.35s
Val loss: 0.3087 score: 0.9380 time: 0.20s
Test loss: 0.3152 score: 0.9219 time: 0.18s
Epoch 24/1000, LR 0.000966
Train loss: 0.1356;  Loss pred: 0.1356; Loss self: 0.0000; time: 0.35s
Val loss: 0.2883 score: 0.9380 time: 0.20s
Test loss: 0.2956 score: 0.9297 time: 0.18s
Epoch 25/1000, LR 0.000966
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.35s
Val loss: 0.2731 score: 0.9380 time: 0.20s
Test loss: 0.2821 score: 0.9219 time: 0.18s
Epoch 26/1000, LR 0.000966
Train loss: 0.1241;  Loss pred: 0.1241; Loss self: 0.0000; time: 0.35s
Val loss: 0.2620 score: 0.9147 time: 0.20s
Test loss: 0.2725 score: 0.9219 time: 0.18s
Epoch 27/1000, LR 0.000966
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.35s
Val loss: 0.2596 score: 0.9070 time: 0.20s
Test loss: 0.2700 score: 0.9219 time: 0.18s
Epoch 28/1000, LR 0.000966
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 0.35s
Val loss: 0.2578 score: 0.8992 time: 0.20s
Test loss: 0.2683 score: 0.9219 time: 0.18s
Epoch 29/1000, LR 0.000966
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.35s
Val loss: 0.2535 score: 0.8915 time: 0.20s
Test loss: 0.2638 score: 0.9219 time: 0.18s
Epoch 30/1000, LR 0.000966
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.35s
Val loss: 0.2547 score: 0.8837 time: 0.20s
Test loss: 0.2658 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 31/1000, LR 0.000966
Train loss: 0.1160;  Loss pred: 0.1160; Loss self: 0.0000; time: 0.35s
Val loss: 0.2248 score: 0.9225 time: 0.20s
Test loss: 0.2379 score: 0.9219 time: 0.18s
Epoch 32/1000, LR 0.000966
Train loss: 0.1074;  Loss pred: 0.1074; Loss self: 0.0000; time: 0.35s
Val loss: 0.2070 score: 0.9380 time: 0.20s
Test loss: 0.2229 score: 0.9219 time: 0.18s
Epoch 33/1000, LR 0.000965
Train loss: 0.1074;  Loss pred: 0.1074; Loss self: 0.0000; time: 0.35s
Val loss: 0.2099 score: 0.9302 time: 0.20s
Test loss: 0.2308 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 34/1000, LR 0.000965
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.35s
Val loss: 0.1892 score: 0.9380 time: 0.20s
Test loss: 0.2140 score: 0.9219 time: 0.18s
Epoch 35/1000, LR 0.000965
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.35s
Val loss: 0.1735 score: 0.9612 time: 0.20s
Test loss: 0.2024 score: 0.9297 time: 0.18s
Epoch 36/1000, LR 0.000965
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 0.35s
Val loss: 0.1763 score: 0.9380 time: 0.20s
Test loss: 0.2124 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 37/1000, LR 0.000965
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.35s
Val loss: 0.1750 score: 0.9380 time: 0.25s
Test loss: 0.2178 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 38/1000, LR 0.000965
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.34s
Val loss: 0.1552 score: 0.9612 time: 0.20s
Test loss: 0.1960 score: 0.9297 time: 0.18s
Epoch 39/1000, LR 0.000965
Train loss: 0.0829;  Loss pred: 0.0829; Loss self: 0.0000; time: 0.35s
Val loss: 0.1459 score: 0.9457 time: 0.20s
Test loss: 0.1834 score: 0.9219 time: 0.18s
Epoch 40/1000, LR 0.000965
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.41s
Val loss: 0.1475 score: 0.9535 time: 0.21s
Test loss: 0.1883 score: 0.9219 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 41/1000, LR 0.000964
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.36s
Val loss: 0.1462 score: 0.9457 time: 0.21s
Test loss: 0.1838 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 42/1000, LR 0.000964
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.34s
Val loss: 0.1481 score: 0.9380 time: 0.31s
Test loss: 0.1747 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 3 of 5
Epoch 43/1000, LR 0.000964
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.34s
Val loss: 0.1456 score: 0.9457 time: 0.20s
Test loss: 0.1733 score: 0.9297 time: 0.18s
Epoch 44/1000, LR 0.000964
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.35s
Val loss: 0.1468 score: 0.9457 time: 0.20s
Test loss: 0.1760 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 45/1000, LR 0.000964
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.35s
Val loss: 0.1539 score: 0.9380 time: 0.20s
Test loss: 0.1761 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 46/1000, LR 0.000964
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.34s
Val loss: 0.1502 score: 0.9535 time: 0.20s
Test loss: 0.1724 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 3 of 5
Epoch 47/1000, LR 0.000964
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.35s
Val loss: 0.1618 score: 0.9302 time: 0.20s
Test loss: 0.1802 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 4 of 5
Epoch 48/1000, LR 0.000963
Train loss: 0.0528;  Loss pred: 0.0528; Loss self: 0.0000; time: 0.39s
Val loss: 0.1432 score: 0.9457 time: 0.20s
Test loss: 0.1701 score: 0.9297 time: 0.18s
Epoch 49/1000, LR 0.000963
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.35s
Val loss: 0.1407 score: 0.9457 time: 0.20s
Test loss: 0.1679 score: 0.9375 time: 0.18s
Epoch 50/1000, LR 0.000963
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.35s
Val loss: 0.1473 score: 0.9612 time: 0.30s
Test loss: 0.1765 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 51/1000, LR 0.000963
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.34s
Val loss: 0.1424 score: 0.9457 time: 0.20s
Test loss: 0.1705 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 52/1000, LR 0.000963
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.34s
Val loss: 0.1390 score: 0.9457 time: 0.20s
Test loss: 0.1684 score: 0.9297 time: 0.18s
Epoch 53/1000, LR 0.000962
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.38s
Val loss: 0.1562 score: 0.9612 time: 0.20s
Test loss: 0.1927 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 54/1000, LR 0.000962
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.33s
Val loss: 0.1907 score: 0.9302 time: 0.20s
Test loss: 0.2304 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 55/1000, LR 0.000962
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.34s
Val loss: 0.1830 score: 0.9302 time: 0.30s
Test loss: 0.2235 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 3 of 5
Epoch 56/1000, LR 0.000962
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.34s
Val loss: 0.1562 score: 0.9612 time: 0.20s
Test loss: 0.1961 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 4 of 5
Epoch 57/1000, LR 0.000962
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.35s
Val loss: 0.1694 score: 0.9380 time: 0.20s
Test loss: 0.2100 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.0516,   Val_Loss: 0.1390,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.1390,   Test_Precision: 0.9661,   Test_Recall: 0.8906,   Test_accuracy: 0.9268,   Test_Score: 0.9297,   Test_loss: 0.1684


[0.21445385785773396, 0.2136310078203678, 0.21438535186462104, 0.21282021701335907, 0.21259720902889967, 0.21179628209210932, 0.2124520381912589, 0.21174292406067252, 0.2142084559891373, 0.21298064407892525, 0.21488667093217373, 0.20628816401585937, 0.21975385793484747, 0.21042691287584603, 0.20709026604890823, 0.2037930937949568, 0.20524711022153497, 0.20402446086518466, 0.2185888651292771, 0.2030803789384663, 0.1960931308567524, 0.19697525002993643, 0.19555600103922188, 0.19383319607004523, 0.19626410212367773, 0.1964410301297903, 0.1956142841372639, 0.19689962500706315, 0.19687107298523188, 0.19442052789963782, 0.19632504903711379, 0.19790835003368556, 0.1977998181246221, 0.19424049509689212, 0.19789624493569136, 0.2009008601307869, 0.19719042791984975, 0.19787745410576463, 0.1942445281893015, 0.19599503697827458, 0.19702795892953873, 0.19647802715189755, 0.20037270500324667, 0.2495176389347762, 0.19724041782319546, 0.19811009988188744, 0.19510304904542863, 0.19816524698399007, 0.19798173592425883, 0.19830141006968915, 0.1981414631009102, 0.2412329250946641, 0.2079277748707682, 0.20212354697287083, 0.21115369186736643, 0.21253502904437482, 0.2349014279898256, 0.20860409806482494, 0.1977841821499169, 0.19695473089814186, 0.19555128598585725, 0.19713037996552885, 0.19519494380801916, 0.1976205729879439, 0.19497432396747172, 0.2129155199509114, 0.2152250581420958, 0.19388173101469874, 0.18301299400627613, 0.1865041039418429, 0.18549783318303525, 0.18407659605145454, 0.1834269210230559, 0.1834337890613824, 0.1838353299535811, 0.18275852501392365, 0.1831095649395138, 0.18343873997218907, 0.18349500792101026, 0.18374927784316242, 0.18376374198123813, 0.18385321903042495, 0.18403399898670614, 0.18251226004213095, 0.18311137007549405, 0.183244532905519, 0.18307479610666633, 0.18325573788024485, 0.1834706380032003, 0.18246786599047482, 0.1831907220184803, 0.18345878506079316, 0.183537659002468, 0.18323053000494838, 0.18290165695361793, 0.18330128584057093, 0.1831506621092558, 0.18294141092337668, 0.18276156904175878, 0.1827775079291314, 0.1850728839635849, 0.18377153505571187, 0.18541652499698102, 0.1852372579742223, 0.18502268381416798, 0.1850702071096748, 0.19380965898744762, 0.18954495107755065, 0.18120003794319928, 0.18390026804991066, 0.18204287299886346, 0.18300765380263329, 0.18412686814554036, 0.18337197811342776, 0.1830154729541391, 0.18351509398780763, 0.1845185849815607, 0.18444224284030497, 0.18337579513899982, 0.18128545605577528, 0.18337592110037804, 0.18113220483064651, 0.1824348419904709, 0.1814767699688673]
[0.0016624330066491006, 0.0016560543241888979, 0.0016619019524389228, 0.0016497691241345665, 0.0016480403800689897, 0.0016418316441248785, 0.0016469150247384412, 0.0016414180159742056, 0.0016605306665824597, 0.0016510127447978702, 0.0016657881467610366, 0.0015991330543865068, 0.001703518278564709, 0.0016312163788825274, 0.0016053508996039397, 0.001579791424767107, 0.0015910628699343796, 0.001581584967947168, 0.0016944873265835436, 0.0015742665033989637, 0.001520101789587228, 0.0015269399227126855, 0.0015159379925521075, 0.0015025829152716685, 0.0015214271482455637, 0.001522798683176669, 0.0015163897995136735, 0.0015263536822252958, 0.0015261323487227277, 0.0015071358751909908, 0.0015218996049388665, 0.001534173256075082, 0.001533331923446683, 0.0015057402720689312, 0.0015340794181061346, 0.0015573710087657898, 0.0015286079683709283, 0.0015339337527578653, 0.0015057715363511743, 0.0015193413719246092, 0.0015273485188336336, 0.0015230854817976554, 0.0015532767829709044, 0.0019342452630602806, 0.001528995487001515, 0.0015357372083867242, 0.0015124267367862684, 0.0015361647053022487, 0.0015347421389477428, 0.0015372202330983656, 0.0015359803341155828, 0.0018700226751524349, 0.0016118432160524667, 0.0015668492013400839, 0.001636850324553228, 0.0016475583646850761, 0.00182094130224671, 0.0016170860315102707, 0.001533210714340441, 0.0015267808596755182, 0.0015159014417508313, 0.0015281424803529368, 0.0015131390992869703, 0.0015319424262631308, 0.001511428867964897, 0.0016505079065962124, 0.0016684113034270993, 0.001514701023552334, 0.0014297890156740323, 0.0014570633120456478, 0.001449201821742463, 0.0014380984066519886, 0.0014330228204926243, 0.00143307647704205, 0.0014362135152623523, 0.0014278009766712785, 0.0014305434760899516, 0.001433115156032727, 0.0014335547493828926, 0.0014355412331497064, 0.0014356542342284229, 0.001436353273675195, 0.0014377656170836417, 0.001425877031579148, 0.0014305575787147973, 0.0014315979133243673, 0.0014302718445833307, 0.001431685452189413, 0.0014333643594000023, 0.0014255302030505845, 0.0014311775157693774, 0.0014332717582874466, 0.0014338879609567812, 0.0014314885156636592, 0.00142891919495014, 0.0014320412956294604, 0.0014308645477285609, 0.0014292297728388803, 0.0014278247581387404, 0.001427949280696339, 0.001445881905965507, 0.001435715117622749, 0.0014485666015389143, 0.0014471660779236117, 0.0014454897172981873, 0.0014458609930443345, 0.0015141379608394345, 0.0014808199302933645, 0.0014156252964312444, 0.001436720844139927, 0.0014222099453036208, 0.0014297472953330725, 0.001438491157387034, 0.0014325935790111544, 0.0014298083824542118, 0.0014337116717797471, 0.001441551445168443, 0.0014409550221898826, 0.001432623399523436, 0.0014162926254357444, 0.0014326243835967034, 0.001415095350239426, 0.0014252722030505538, 0.0014177872653817758]
[601.527999023347, 603.8449254916683, 601.7202149214945, 606.1454208173393, 606.7812488660857, 609.0758474405062, 607.1958692336403, 609.2293311441969, 602.21712258895, 605.6888434997683, 600.3164339621476, 625.3388342245487, 587.0204109829365, 613.0394550630094, 622.9167718077789, 632.99495384172, 628.5106760371092, 632.2771272276055, 590.1489992351954, 635.2164629310998, 657.8506826648381, 654.9046135511669, 659.6575881817454, 665.5206776520542, 657.2776101393692, 656.6856217093169, 659.4610438033237, 655.1561486994834, 655.2511653638259, 663.5101827652239, 657.0735656641223, 651.8168636040025, 652.1745127122647, 664.1251605935802, 651.856734532381, 642.1077536254485, 654.1899693652142, 651.9186361223855, 664.1113713858788, 658.1799314351987, 654.7294135353301, 656.5619671062243, 643.8002621061077, 516.9975178937973, 654.0241671746733, 651.1530713320994, 661.1890517916154, 650.9718629443739, 651.5752546454643, 650.5248750105482, 651.0500022617802, 534.7528740091267, 620.4077357158101, 638.2235119657507, 610.9294081442334, 606.958770890733, 549.1665210548973, 618.3962884560038, 652.2260708503994, 654.972842803732, 659.6734935781992, 654.3892424016915, 660.8777742054418, 652.766045809764, 661.6255790763585, 605.8741045732199, 599.3725875303594, 660.196292503165, 699.4038903904848, 686.3119754185886, 690.0350144451513, 695.3627063172139, 697.8255933539384, 697.7994657089451, 696.2753026435144, 700.3777251444122, 699.0350287942739, 697.7806324847518, 697.5666610783254, 696.6013771725039, 696.5465473219875, 696.2075544558071, 695.5236570675519, 701.3227493344973, 699.0281376149801, 698.5201575754344, 699.1677867303064, 698.4774473127072, 697.6593170061756, 701.4933796983291, 698.7253425808723, 697.7043915208767, 697.4045582562368, 698.5735401002398, 699.8296359472541, 698.3038848474306, 698.8781723521344, 699.6775599025615, 700.3660598402587, 700.3049852809551, 691.619416408864, 696.5170093463917, 690.337606111883, 691.0056939938753, 691.8070658220475, 691.6294199862524, 660.4417997984823, 675.301553918099, 706.4016180842309, 696.0294368100688, 703.1310695739191, 699.4242991500404, 695.1728516819409, 698.0346796543989, 699.3944169522478, 697.4903111157934, 693.6970604494462, 693.9841872928525, 698.020149840252, 706.0687756475004, 698.0196703684676, 706.6661619874632, 701.6203626645278, 705.3244336559366]
Elapsed: 0.1950426744184487~0.013198830293418609
Time per graph: 0.0015170788578488916~9.807553008103096e-05
Speed: 661.7149408007706~39.66482779792703
Total Time: 0.1821
best val loss: 0.1389824659888481 test_score: 0.9297

Testing...
Test loss: 0.2024 score: 0.9297 time: 0.29s
test Score 0.9297
Epoch Time List: [0.7334908852353692, 0.7831565821543336, 0.7272145219612867, 0.7274080459028482, 0.72348140203394, 0.7245048100594431, 0.7289620540104806, 0.7258917558938265, 0.7269605239853263, 0.7460497198626399, 0.7267897259443998, 0.7235786626115441, 0.7558491001836956, 0.754764816025272, 0.7198657398112118, 0.7142253771889955, 0.712319347076118, 0.7095127070788294, 0.7526217270642519, 0.7135401603300124, 0.6898416087497026, 0.6895122609566897, 0.6873944222461432, 0.6838083032052964, 0.689288706984371, 0.6911316991318017, 0.6925805809441954, 0.691867504036054, 0.6930211558938026, 0.6910576082300395, 0.7089786911383271, 0.6963223742786795, 0.7080103859771043, 0.8050252050161362, 0.6864933981560171, 0.6955126549582928, 0.8130172821693122, 0.7022668379358947, 0.7921418629121035, 0.6876957069616765, 0.6984178158454597, 0.7381714289076626, 0.6972884668502957, 0.7487772349268198, 0.755674592917785, 0.7084511597640812, 0.7745181557256728, 0.6906549041159451, 0.7034589541144669, 0.7207307771313936, 0.6989921561907977, 0.732268736930564, 0.7260997579433024, 0.7200339040718973, 0.7712792153470218, 0.7360864309594035, 0.768457303987816, 0.8027662329841405, 0.7036931610200554, 0.8055879066232592, 0.6864374850410968, 0.6947399717755616, 0.7035448867827654, 0.6914687152020633, 0.6925276711117476, 0.834298200905323, 0.7388717208523303, 0.7512053758837283, 0.8295304372441024, 0.7280683212447912, 0.7246320527046919, 0.7274670852348208, 0.7251591628883034, 0.7259782620240003, 0.7314941419754177, 0.7246893842238933, 0.7270014060195535, 0.7265899330377579, 0.7280985657125711, 0.7281903447583318, 0.7264006452169269, 0.7271474788431078, 0.8531157253310084, 0.7273567761294544, 0.7247362993657589, 0.7266979787964374, 0.7236303419340402, 0.7279290060978383, 0.724972483003512, 0.7239339461084455, 0.7249906517099589, 0.7262792270630598, 0.725586726795882, 0.7271332470700145, 0.7242804900743067, 0.7270485481712967, 0.7244750941172242, 0.7227262679953128, 0.7237520241178572, 0.7258779960684478, 0.7306213930714875, 0.7265853048302233, 0.7279676592443138, 0.7872293677646667, 0.7240900758188218, 0.7311825477518141, 0.8044798520859331, 0.7510724002495408, 0.8290219996124506, 0.7153518251143396, 0.7245061891153455, 0.7318888751324266, 0.7158957489300519, 0.7259306057821959, 0.7635992388240993, 0.7302423319779336, 0.8225681190378964, 0.7195809341501445, 0.7203613589517772, 0.7540270518511534, 0.7065475943963975, 0.8121158299036324, 0.7178314998745918, 0.7225885360967368]
Total Epoch List: [18, 49, 57]
Total Time List: [0.20441348594613373, 0.2156830991152674, 0.1820502809714526]
T-times Epoch Time: 0.7358435338356002 ~ 0.003221454118110248
T-times Total Epoch: 40.22222222222223 ~ 1.5713484026367734
T-times Total Time: 0.1978982313691328 ~ 0.003056587753389562
T-times Inference Elapsed: 0.20153237087854128 ~ 0.00840886831229116
T-times Time Per Graph: 0.0015648379968074032 ~ 6.398077884084568e-05
T-times Speed: 646.7580152673146 ~ 21.21587201707395
T-times cross validation test micro f1 score:0.8985221504148946 ~ 0.024950877104278624
T-times cross validation test precision:0.9378620010923758 ~ 0.05460450084826459
T-times cross validation test recall:0.8736111111111112 ~ 0.030589833628429618
T-times cross validation test f1_score:0.8985221504148946 ~ 0.015012644363664602
