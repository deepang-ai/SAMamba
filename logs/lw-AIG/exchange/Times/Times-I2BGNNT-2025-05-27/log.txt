Namespace(seed=15, model='I2BGNNT', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef8159f30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.23s
Epoch 5/1000, LR 0.000105
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.19s
Epoch 9/1000, LR 0.000225
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.32s
Epoch 10/1000, LR 0.000255
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6515;  Loss pred: 0.6515; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5967;  Loss pred: 0.5967; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4961 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4961 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5084;  Loss pred: 0.5084; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4961 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4825;  Loss pred: 0.4825; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4961 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4507;  Loss pred: 0.4507; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4230;  Loss pred: 0.4230; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.4961 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.3860;  Loss pred: 0.3860; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6772 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.4961 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3535;  Loss pred: 0.3535; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6733 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.4961 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3243;  Loss pred: 0.3243; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6695 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6665 score: 0.4961 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2811;  Loss pred: 0.2811; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6654 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6612 score: 0.4961 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6593 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6532 score: 0.4961 time: 0.21s
Epoch 28/1000, LR 0.000285
Train loss: 0.2161;  Loss pred: 0.2161; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6523 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6439 score: 0.4961 time: 0.19s
Epoch 29/1000, LR 0.000285
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6435 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6324 score: 0.4961 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.26s
Val loss: 0.6328 score: 0.5194 time: 0.18s
Test loss: 0.6182 score: 0.5116 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1422;  Loss pred: 0.1422; Loss self: 0.0000; time: 0.26s
Val loss: 0.6182 score: 0.5504 time: 0.17s
Test loss: 0.5993 score: 0.5426 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1283;  Loss pred: 0.1283; Loss self: 0.0000; time: 0.26s
Val loss: 0.6022 score: 0.5814 time: 0.17s
Test loss: 0.5784 score: 0.5969 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1011;  Loss pred: 0.1011; Loss self: 0.0000; time: 0.26s
Val loss: 0.5809 score: 0.6202 time: 0.18s
Test loss: 0.5515 score: 0.6279 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.27s
Val loss: 0.5571 score: 0.6744 time: 0.17s
Test loss: 0.5220 score: 0.6667 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.26s
Val loss: 0.5332 score: 0.6977 time: 0.18s
Test loss: 0.4914 score: 0.7132 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.26s
Val loss: 0.5072 score: 0.7364 time: 0.18s
Test loss: 0.4585 score: 0.7984 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.26s
Val loss: 0.4805 score: 0.7519 time: 0.17s
Test loss: 0.4247 score: 0.8295 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.26s
Val loss: 0.4568 score: 0.7829 time: 0.17s
Test loss: 0.3931 score: 0.8372 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.26s
Val loss: 0.4341 score: 0.7984 time: 0.17s
Test loss: 0.3624 score: 0.8450 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.25s
Val loss: 0.4165 score: 0.8062 time: 0.17s
Test loss: 0.3371 score: 0.8527 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.26s
Val loss: 0.4065 score: 0.8062 time: 0.18s
Test loss: 0.3199 score: 0.8682 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.27s
Val loss: 0.3945 score: 0.8140 time: 0.18s
Test loss: 0.3022 score: 0.8837 time: 0.19s
Epoch 43/1000, LR 0.000284
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.26s
Val loss: 0.3819 score: 0.8295 time: 0.18s
Test loss: 0.2841 score: 0.8992 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.27s
Val loss: 0.3727 score: 0.8217 time: 0.18s
Test loss: 0.2689 score: 0.9147 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.27s
Val loss: 0.3659 score: 0.8450 time: 0.18s
Test loss: 0.2565 score: 0.9147 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.27s
Val loss: 0.3625 score: 0.8527 time: 0.19s
Test loss: 0.2470 score: 0.9147 time: 0.19s
Epoch 47/1000, LR 0.000284
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.27s
Val loss: 0.3616 score: 0.8527 time: 0.18s
Test loss: 0.2409 score: 0.9147 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.27s
Val loss: 0.3639 score: 0.8527 time: 0.18s
Test loss: 0.2369 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.27s
Val loss: 0.3695 score: 0.8450 time: 0.17s
Test loss: 0.2357 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.27s
Val loss: 0.3766 score: 0.8450 time: 0.18s
Test loss: 0.2361 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.27s
Val loss: 0.3882 score: 0.8450 time: 0.18s
Test loss: 0.2393 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.27s
Val loss: 0.4014 score: 0.8450 time: 0.18s
Test loss: 0.2434 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.27s
Val loss: 0.4114 score: 0.8450 time: 0.18s
Test loss: 0.2465 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.26s
Val loss: 0.4215 score: 0.8450 time: 0.19s
Test loss: 0.2511 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.25s
Val loss: 0.4239 score: 0.8605 time: 0.17s
Test loss: 0.2543 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.26s
Val loss: 0.4260 score: 0.8605 time: 0.17s
Test loss: 0.2550 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.25s
Val loss: 0.4289 score: 0.8605 time: 0.18s
Test loss: 0.2547 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.26s
Val loss: 0.4342 score: 0.8605 time: 0.17s
Test loss: 0.2554 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.26s
Val loss: 0.4394 score: 0.8682 time: 0.17s
Test loss: 0.2554 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.25s
Val loss: 0.4440 score: 0.8682 time: 0.17s
Test loss: 0.2549 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.25s
Val loss: 0.4506 score: 0.8682 time: 0.16s
Test loss: 0.2563 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.25s
Val loss: 0.4563 score: 0.8682 time: 0.17s
Test loss: 0.2575 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.25s
Val loss: 0.4640 score: 0.8682 time: 0.17s
Test loss: 0.2591 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.25s
Val loss: 0.4720 score: 0.8605 time: 0.17s
Test loss: 0.2632 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.26s
Val loss: 0.4779 score: 0.8682 time: 0.17s
Test loss: 0.2676 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.25s
Val loss: 0.4810 score: 0.8682 time: 0.17s
Test loss: 0.2705 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.25s
Val loss: 0.4859 score: 0.8682 time: 0.17s
Test loss: 0.2747 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0217,   Val_Loss: 0.3616,   Val_Precision: 0.9592,   Val_Recall: 0.7344,   Val_accuracy: 0.8319,   Val_Score: 0.8527,   Val_Loss: 0.3616,   Test_Precision: 1.0000,   Test_Recall: 0.8308,   Test_accuracy: 0.9076,   Test_Score: 0.9147,   Test_loss: 0.2409


[0.18253785907290876, 0.1839626079890877, 0.18847330007702112, 0.23407855699770153, 0.19301296188496053, 0.1819267189130187, 0.1922175141517073, 0.19861392909660935, 0.3284006279427558, 0.1876483429223299, 0.17940224194899201, 0.1678359170909971, 0.17659638193435967, 0.17469651205465198, 0.1729864990338683, 0.17239712388254702, 0.17892897105775774, 0.17798338504508138, 0.18010637001134455, 0.1744397608563304, 0.1784680208656937, 0.1801996862050146, 0.18736211908981204, 0.17953693005256355, 0.17868106486275792, 0.1872842621523887, 0.22421239991672337, 0.1944086840376258, 0.17885235999710858, 0.17465620208531618, 0.17618028703145683, 0.18661550991237164, 0.18173951492644846, 0.17880620807409286, 0.1834795840550214, 0.17658571992069483, 0.1728559830226004, 0.17607563105411828, 0.17936754412949085, 0.17771658999845386, 0.18398700817488134, 0.19805452600121498, 0.18580170208588243, 0.18019313807599247, 0.18682111497037113, 0.19027028791606426, 0.18196199997328222, 0.18826145981438458, 0.18642704095691442, 0.1835658960044384, 0.18105687201023102, 0.18389299884438515, 0.1902521310839802, 0.18326080311089754, 0.1772156918887049, 0.1740894301328808, 0.16755001596175134, 0.17403832799755037, 0.17645484395325184, 0.1792945151682943, 0.17285337205976248, 0.17647420521825552, 0.1752832669299096, 0.17372872401028872, 0.17518768110312521, 0.17514205304905772, 0.1758794980123639]
[0.001415022163355882, 0.0014260667285975791, 0.0014610333339303964, 0.0018145624573465235, 0.001496224510736128, 0.0014102846427365792, 0.001490058249238041, 0.001539642861214026, 0.002545741301881828, 0.0014546383172273636, 0.0013907150538681552, 0.0013010536208604426, 0.0013689642010415479, 0.0013542365275554416, 0.001340980612665646, 0.0013364118130430001, 0.0013870462872694399, 0.0013797161631401657, 0.0013961734109406555, 0.0013522462081886077, 0.0013834730299666178, 0.0013968967922869347, 0.0014524195278280002, 0.001391759147694291, 0.0013851245338198288, 0.0014518159856774318, 0.0017380806195094836, 0.0015070440623071767, 0.001386452403078361, 0.0013539240471729936, 0.0013657386591585802, 0.0014466318597858266, 0.001408833449042236, 0.0013860946362332781, 0.0014223223570156698, 0.0013688815497728281, 0.0013399688606403133, 0.0013649273725125448, 0.001390446078523185, 0.0013776479844841384, 0.0014262558773246615, 0.0015353064031101936, 0.0014403232719835846, 0.0013968460315968408, 0.0014482256974447374, 0.0014749634722175525, 0.001410558139327769, 0.0014593911613518184, 0.0014451708601311195, 0.0014229914418948714, 0.001403541643490163, 0.001425527122824691, 0.0014748227215812418, 0.001420626380704632, 0.0013737650534008132, 0.0013495304661463629, 0.001298837333036832, 0.001349134325562406, 0.001367867007389549, 0.0013898799625449171, 0.0013399486206183138, 0.0013680170947151591, 0.0013587850149605395, 0.001346734294653401, 0.0013580440395591102, 0.0013576903337136257, 0.0013634069613361543]
[706.702711728832, 701.2294585846056, 684.4470805534954, 551.0970404746073, 668.3488960543826, 709.0767138040698, 671.114703409321, 649.5012740886472, 392.8128907916895, 687.4561106750342, 719.0545591770115, 768.6078298130841, 730.479291744204, 738.4234435066666, 745.7229363011944, 748.2723440785869, 720.9564736073913, 724.7867544901768, 716.2434065595489, 739.5103006719045, 722.8185720571144, 715.8725007613814, 688.506303337463, 718.5151264546647, 721.956745103813, 688.7925259573376, 575.347304823074, 663.5506054607796, 721.2652939110532, 738.5938687535756, 732.2045058137927, 691.2608714065302, 709.8071107552335, 721.4514607151984, 703.0754983688855, 730.5233971236988, 746.285999155341, 732.6397141257486, 719.1936569464931, 725.874832513511, 701.1364621864187, 651.3357841628352, 694.2885805231904, 715.8985152120338, 690.500107658916, 677.982891668861, 708.9392291739005, 685.2172511951565, 691.9597035808421, 702.7449150842318, 712.4833129377745, 701.4948954590872, 678.047595393596, 703.9148460019438, 727.9265093579561, 740.9984621210807, 769.919353689876, 741.216038353435, 731.0652238834314, 719.4865937695556, 746.2972718599867, 730.9850175579966, 735.9515957195341, 742.5369681087409, 736.3531453108476, 736.5449802273745, 733.4567215499535]
Elapsed: 0.18442281325167018~0.020614547890828387
Time per graph: 0.0014296342112532572~0.00015980269682812703
Speed: 705.2248072448909~52.76633760106535
Total Time: 0.1763
best val loss: 0.36160068148963675 test_score: 0.9147

Testing...
Test loss: 0.2554 score: 0.9070 time: 0.16s
test Score 0.9070
Epoch Time List: [0.7719869841821492, 0.6159643109422177, 0.6237466800957918, 0.6865689009428024, 0.6117218032013625, 0.6132300838362426, 0.7145833899267018, 0.6388231781311333, 0.8140990789979696, 0.6475675131659955, 0.6060097913723439, 0.6908481540158391, 0.5989389738533646, 0.5992735049221665, 0.58851980837062, 0.5895599129144102, 0.6158415540121496, 0.6024873836431652, 0.6148860261309892, 0.6046392603311688, 0.6110838456079364, 0.6086903971154243, 0.6294969217851758, 0.6161369860637933, 0.6179792149923742, 0.6357025997713208, 0.6888952220324427, 0.6438161020632833, 0.6133277660701424, 0.6114459717646241, 0.6017520639579743, 0.6116145940031856, 0.6153802992776036, 0.6137169147841632, 0.6186074300203472, 0.6122774889227003, 0.5938350271899253, 0.5957225172314793, 0.5958779340144247, 0.5970747519750148, 0.6230628781486303, 0.6480078937020153, 0.6252357009798288, 0.6207190807908773, 0.6332080690190196, 0.6520367730408907, 0.6246327741537243, 0.6375171898398548, 0.6231589110102504, 0.6255539059638977, 0.6232531568966806, 0.624034927925095, 0.6354674710892141, 0.6243943371810019, 0.5977087751962245, 0.5974333418998867, 0.5916446181945503, 0.5959108890965581, 0.5940762951504439, 0.5969376869034022, 0.5861434801481664, 0.592534925090149, 0.5889586419798434, 0.5914933481253684, 0.6008269181475043, 0.5953667759895325, 0.5948077959474176]
Total Epoch List: [67]
Total Time List: [0.1763010141439736]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef815a080>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4961 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4961 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.19s
Epoch 9/1000, LR 0.000225
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.19s
Epoch 10/1000, LR 0.000255
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.19s
Epoch 11/1000, LR 0.000285
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 0.20s
Epoch 13/1000, LR 0.000285
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.19s
Epoch 16/1000, LR 0.000285
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4961 time: 0.19s
Epoch 17/1000, LR 0.000285
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4961 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4961 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.4934;  Loss pred: 0.4934; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4961 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4656;  Loss pred: 0.4656; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6842 score: 0.4961 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6772 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4134;  Loss pred: 0.4134; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6735 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.4961 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6694 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.4961 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3564;  Loss pred: 0.3564; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6643 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.4961 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3288;  Loss pred: 0.3288; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6588 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6656 score: 0.4961 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3033;  Loss pred: 0.3033; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6526 score: 0.5039 time: 0.24s
Test loss: 0.6600 score: 0.5039 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2760;  Loss pred: 0.2760; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6445 score: 0.5039 time: 0.18s
Test loss: 0.6521 score: 0.5116 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2584;  Loss pred: 0.2584; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6364 score: 0.5039 time: 0.18s
Test loss: 0.6440 score: 0.5116 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2287;  Loss pred: 0.2287; Loss self: 0.0000; time: 0.28s
Val loss: 0.6245 score: 0.5194 time: 0.20s
Test loss: 0.6319 score: 0.5116 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.2070;  Loss pred: 0.2070; Loss self: 0.0000; time: 0.26s
Val loss: 0.6093 score: 0.5504 time: 0.20s
Test loss: 0.6167 score: 0.5194 time: 0.19s
Epoch 31/1000, LR 0.000285
Train loss: 0.1896;  Loss pred: 0.1896; Loss self: 0.0000; time: 0.25s
Val loss: 0.5921 score: 0.5581 time: 0.18s
Test loss: 0.5995 score: 0.5271 time: 0.27s
Epoch 32/1000, LR 0.000285
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 0.24s
Val loss: 0.5720 score: 0.5891 time: 0.19s
Test loss: 0.5794 score: 0.5349 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.25s
Val loss: 0.5485 score: 0.6279 time: 0.18s
Test loss: 0.5556 score: 0.5504 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 0.24s
Val loss: 0.5218 score: 0.6977 time: 0.27s
Test loss: 0.5284 score: 0.6357 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 0.24s
Val loss: 0.4874 score: 0.7209 time: 0.18s
Test loss: 0.4931 score: 0.7209 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1300;  Loss pred: 0.1300; Loss self: 0.0000; time: 0.25s
Val loss: 0.4594 score: 0.7364 time: 0.18s
Test loss: 0.4649 score: 0.7752 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.27s
Val loss: 0.4239 score: 0.7752 time: 0.18s
Test loss: 0.4287 score: 0.7984 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0876;  Loss pred: 0.0876; Loss self: 0.0000; time: 0.24s
Val loss: 0.3884 score: 0.8295 time: 0.18s
Test loss: 0.3918 score: 0.8605 time: 0.21s
Epoch 39/1000, LR 0.000284
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.25s
Val loss: 0.3566 score: 0.8760 time: 0.18s
Test loss: 0.3584 score: 0.8605 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.27s
Val loss: 0.3209 score: 0.8915 time: 0.18s
Test loss: 0.3197 score: 0.8992 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.25s
Val loss: 0.2942 score: 0.8915 time: 0.21s
Test loss: 0.2906 score: 0.9070 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.25s
Val loss: 0.2663 score: 0.9070 time: 0.18s
Test loss: 0.2597 score: 0.9225 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.26s
Val loss: 0.2505 score: 0.9147 time: 0.18s
Test loss: 0.2424 score: 0.9147 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.24s
Val loss: 0.2328 score: 0.9225 time: 0.18s
Test loss: 0.2232 score: 0.9225 time: 0.20s
Epoch 45/1000, LR 0.000284
Train loss: 0.0509;  Loss pred: 0.0509; Loss self: 0.0000; time: 0.24s
Val loss: 0.2172 score: 0.9302 time: 0.18s
Test loss: 0.2065 score: 0.9225 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.24s
Val loss: 0.2057 score: 0.9380 time: 0.20s
Test loss: 0.1947 score: 0.9302 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.24s
Val loss: 0.1961 score: 0.9380 time: 0.19s
Test loss: 0.1848 score: 0.9302 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.26s
Val loss: 0.1876 score: 0.9380 time: 0.18s
Test loss: 0.1767 score: 0.9302 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.26s
Val loss: 0.1822 score: 0.9457 time: 0.19s
Test loss: 0.1724 score: 0.9302 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.24s
Val loss: 0.1809 score: 0.9457 time: 0.18s
Test loss: 0.1718 score: 0.9302 time: 0.19s
Epoch 51/1000, LR 0.000284
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.25s
Val loss: 0.1800 score: 0.9535 time: 0.19s
Test loss: 0.1720 score: 0.9302 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.24s
Val loss: 0.1809 score: 0.9535 time: 0.18s
Test loss: 0.1737 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0316;  Loss pred: 0.0316; Loss self: 0.0000; time: 0.25s
Val loss: 0.1833 score: 0.9535 time: 0.19s
Test loss: 0.1764 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.24s
Val loss: 0.1880 score: 0.9457 time: 0.35s
Test loss: 0.1806 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.25s
Val loss: 0.1910 score: 0.9457 time: 0.18s
Test loss: 0.1848 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.25s
Val loss: 0.1960 score: 0.9457 time: 0.18s
Test loss: 0.1893 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.25s
Val loss: 0.2013 score: 0.9457 time: 0.19s
Test loss: 0.1941 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.25s
Val loss: 0.2044 score: 0.9457 time: 0.18s
Test loss: 0.1986 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.25s
Val loss: 0.2073 score: 0.9457 time: 0.20s
Test loss: 0.2027 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.25s
Val loss: 0.2088 score: 0.9380 time: 0.23s
Test loss: 0.2080 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.26s
Val loss: 0.2103 score: 0.9380 time: 0.20s
Test loss: 0.2116 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.29s
Val loss: 0.2132 score: 0.9380 time: 0.20s
Test loss: 0.2165 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.25s
Val loss: 0.2172 score: 0.9380 time: 0.20s
Test loss: 0.2209 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.25s
Val loss: 0.2210 score: 0.9380 time: 0.20s
Test loss: 0.2247 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.25s
Val loss: 0.2253 score: 0.9380 time: 0.18s
Test loss: 0.2294 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0311;  Loss pred: 0.0311; Loss self: 0.0000; time: 0.25s
Val loss: 0.2289 score: 0.9380 time: 0.18s
Test loss: 0.2361 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.24s
Val loss: 0.2334 score: 0.9380 time: 0.18s
Test loss: 0.2425 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.24s
Val loss: 0.2337 score: 0.9380 time: 0.18s
Test loss: 0.2419 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.25s
Val loss: 0.2339 score: 0.9380 time: 0.18s
Test loss: 0.2403 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.25s
Val loss: 0.2349 score: 0.9380 time: 0.18s
Test loss: 0.2395 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.25s
Val loss: 0.2370 score: 0.9380 time: 0.18s
Test loss: 0.2390 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0338,   Val_Loss: 0.1800,   Val_Precision: 0.9683,   Val_Recall: 0.9385,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.1800,   Test_Precision: 0.9365,   Test_Recall: 0.9219,   Test_accuracy: 0.9291,   Test_Score: 0.9302,   Test_loss: 0.1720


[0.18253785907290876, 0.1839626079890877, 0.18847330007702112, 0.23407855699770153, 0.19301296188496053, 0.1819267189130187, 0.1922175141517073, 0.19861392909660935, 0.3284006279427558, 0.1876483429223299, 0.17940224194899201, 0.1678359170909971, 0.17659638193435967, 0.17469651205465198, 0.1729864990338683, 0.17239712388254702, 0.17892897105775774, 0.17798338504508138, 0.18010637001134455, 0.1744397608563304, 0.1784680208656937, 0.1801996862050146, 0.18736211908981204, 0.17953693005256355, 0.17868106486275792, 0.1872842621523887, 0.22421239991672337, 0.1944086840376258, 0.17885235999710858, 0.17465620208531618, 0.17618028703145683, 0.18661550991237164, 0.18173951492644846, 0.17880620807409286, 0.1834795840550214, 0.17658571992069483, 0.1728559830226004, 0.17607563105411828, 0.17936754412949085, 0.17771658999845386, 0.18398700817488134, 0.19805452600121498, 0.18580170208588243, 0.18019313807599247, 0.18682111497037113, 0.19027028791606426, 0.18196199997328222, 0.18826145981438458, 0.18642704095691442, 0.1835658960044384, 0.18105687201023102, 0.18389299884438515, 0.1902521310839802, 0.18326080311089754, 0.1772156918887049, 0.1740894301328808, 0.16755001596175134, 0.17403832799755037, 0.17645484395325184, 0.1792945151682943, 0.17285337205976248, 0.17647420521825552, 0.1752832669299096, 0.17372872401028872, 0.17518768110312521, 0.17514205304905772, 0.1758794980123639, 0.17822269396856427, 0.17459705285727978, 0.17530617001466453, 0.17392966710031033, 0.17536031804047525, 0.18256420991383493, 0.19538038782775402, 0.19218465988524258, 0.19667000719346106, 0.19637999590486288, 0.1869225688278675, 0.20639769290573895, 0.18325063702650368, 0.17994738789275289, 0.19174756086431444, 0.19489842210896313, 0.18631190503947437, 0.18232848588377237, 0.18332257913425565, 0.18141483678482473, 0.1777221590746194, 0.18209024495445192, 0.18386575882323086, 0.18175130221061409, 0.1776424809359014, 0.17848971486091614, 0.18335698288865387, 0.1843132919166237, 0.19942716299556196, 0.19889992102980614, 0.27715128380805254, 0.17918031592853367, 0.18326123896986246, 0.18042206205427647, 0.18551692203618586, 0.1805558861233294, 0.18115337705239654, 0.21037676092237234, 0.17992931208573282, 0.1848947680555284, 0.1788562450092286, 0.17821334395557642, 0.18328054016456008, 0.2031301020178944, 0.1811746470630169, 0.18384995707310736, 0.17986601102165878, 0.17764997808262706, 0.18114085914567113, 0.19865722698159516, 0.18371627293527126, 0.17931505385786295, 0.18336410308256745, 0.18217508308589458, 0.1800120568368584, 0.18574499292299151, 0.184714529896155, 0.18019341793842614, 0.18685230892151594, 0.19780222000554204, 0.18443264788948, 0.1934593408368528, 0.2096774058882147, 0.1763452140148729, 0.1792906851042062, 0.18091987096704543, 0.17909321305342019, 0.17670270800590515, 0.1787396939471364, 0.1823250891175121, 0.1832976141013205]
[0.001415022163355882, 0.0014260667285975791, 0.0014610333339303964, 0.0018145624573465235, 0.001496224510736128, 0.0014102846427365792, 0.001490058249238041, 0.001539642861214026, 0.002545741301881828, 0.0014546383172273636, 0.0013907150538681552, 0.0013010536208604426, 0.0013689642010415479, 0.0013542365275554416, 0.001340980612665646, 0.0013364118130430001, 0.0013870462872694399, 0.0013797161631401657, 0.0013961734109406555, 0.0013522462081886077, 0.0013834730299666178, 0.0013968967922869347, 0.0014524195278280002, 0.001391759147694291, 0.0013851245338198288, 0.0014518159856774318, 0.0017380806195094836, 0.0015070440623071767, 0.001386452403078361, 0.0013539240471729936, 0.0013657386591585802, 0.0014466318597858266, 0.001408833449042236, 0.0013860946362332781, 0.0014223223570156698, 0.0013688815497728281, 0.0013399688606403133, 0.0013649273725125448, 0.001390446078523185, 0.0013776479844841384, 0.0014262558773246615, 0.0015353064031101936, 0.0014403232719835846, 0.0013968460315968408, 0.0014482256974447374, 0.0014749634722175525, 0.001410558139327769, 0.0014593911613518184, 0.0014451708601311195, 0.0014229914418948714, 0.001403541643490163, 0.001425527122824691, 0.0014748227215812418, 0.001420626380704632, 0.0013737650534008132, 0.0013495304661463629, 0.001298837333036832, 0.001349134325562406, 0.001367867007389549, 0.0013898799625449171, 0.0013399486206183138, 0.0013680170947151591, 0.0013587850149605395, 0.001346734294653401, 0.0013580440395591102, 0.0013576903337136257, 0.0013634069613361543, 0.0013815712710741417, 0.0013534655260254247, 0.0013589625582532134, 0.0013482919930256614, 0.001359382310391281, 0.0014152264334405809, 0.0015145766498275506, 0.0014898035650018804, 0.0015245736991741168, 0.0015223255496501, 0.0014490121614563372, 0.0015999821155483639, 0.0014205475738488656, 0.001394940991416689, 0.0014864152004985616, 0.0015108404814648305, 0.0014442783336393362, 0.0014133991153780804, 0.001421105264606633, 0.0014063165642234476, 0.0013776911556172048, 0.0014115522864686196, 0.0014253159598700067, 0.0014089248233380937, 0.0013770734956271428, 0.0013836412004722182, 0.0014213719603771617, 0.0014287852086559978, 0.0015459469999655965, 0.0015418598529442337, 0.002148459564403508, 0.001388994697120416, 0.001420629759456298, 0.0013986206360796626, 0.0014381156746991152, 0.0013996580319637938, 0.0014042897445922212, 0.001630827604049398, 0.0013948008688816496, 0.001433292775624251, 0.0013864825194513843, 0.0013814987903533055, 0.0014207793811206208, 0.0015746519536270884, 0.0014044546283954798, 0.0014251934656830027, 0.0013943101629585951, 0.0013771316130436206, 0.0014041927065555901, 0.0015399785037332958, 0.0014241571545369866, 0.001390039177192736, 0.0014214271556788176, 0.0014122099464022836, 0.0013954423010609177, 0.0014398836660697017, 0.0014318955805903489, 0.0013968482010730708, 0.0014484675110195033, 0.0015333505426786205, 0.0014297104487556588, 0.0014996848126887813, 0.001625406247195463, 0.0013670171629059915, 0.0013898502721256295, 0.0014024796198995769, 0.001388319481034265, 0.0013697884341543035, 0.0013855790228460185, 0.001413372783856683, 0.0014209117372195388]
[706.702711728832, 701.2294585846056, 684.4470805534954, 551.0970404746073, 668.3488960543826, 709.0767138040698, 671.114703409321, 649.5012740886472, 392.8128907916895, 687.4561106750342, 719.0545591770115, 768.6078298130841, 730.479291744204, 738.4234435066666, 745.7229363011944, 748.2723440785869, 720.9564736073913, 724.7867544901768, 716.2434065595489, 739.5103006719045, 722.8185720571144, 715.8725007613814, 688.506303337463, 718.5151264546647, 721.956745103813, 688.7925259573376, 575.347304823074, 663.5506054607796, 721.2652939110532, 738.5938687535756, 732.2045058137927, 691.2608714065302, 709.8071107552335, 721.4514607151984, 703.0754983688855, 730.5233971236988, 746.285999155341, 732.6397141257486, 719.1936569464931, 725.874832513511, 701.1364621864187, 651.3357841628352, 694.2885805231904, 715.8985152120338, 690.500107658916, 677.982891668861, 708.9392291739005, 685.2172511951565, 691.9597035808421, 702.7449150842318, 712.4833129377745, 701.4948954590872, 678.047595393596, 703.9148460019438, 727.9265093579561, 740.9984621210807, 769.919353689876, 741.216038353435, 731.0652238834314, 719.4865937695556, 746.2972718599867, 730.9850175579966, 735.9515957195341, 742.5369681087409, 736.3531453108476, 736.5449802273745, 733.4567215499535, 723.8135454441824, 738.8440863629468, 735.8554464409839, 741.6791059894454, 735.6282278766469, 706.6007081063933, 660.2505063800237, 671.2294315114877, 655.9210620921207, 656.889717334013, 690.125332692132, 625.0069861920105, 703.95389665872, 716.8762020423598, 672.7595356025611, 661.8832446364247, 692.3873167024321, 707.5142393396098, 703.6776408514704, 711.0774525735526, 725.8520866035469, 708.4399278625172, 701.5988231067048, 709.761076982625, 726.1776536804109, 722.7307192491186, 703.5456079594038, 699.8952634319758, 646.852705831606, 648.5673766590822, 465.44976529620516, 719.9451531911119, 703.913171847617, 714.9901654554467, 695.3543568108466, 714.4602304013841, 712.1037548347124, 613.1855982305963, 716.9482198572182, 697.694160611717, 721.2496270026461, 723.8515205245017, 703.8390430548502, 635.0609718525911, 712.0201534331164, 701.6591249390586, 717.2005387080403, 726.1470076849693, 712.1529654237748, 649.3597135127198, 702.1696986278976, 719.4041840026102, 703.5182886473275, 708.1100105176139, 716.6186658092037, 694.500551374122, 698.3749468573087, 715.8974033340139, 690.3848325159536, 652.1665934608098, 699.4423247520817, 666.8067793572586, 615.2308087442372, 731.5197110431371, 719.5019636687954, 713.0228388428233, 720.2953020979176, 730.0397456030443, 721.7199333358639, 707.5274205233321, 703.7734813541725]
Elapsed: 0.18534390657019895~0.017402185070152645
Time per graph: 0.001436774469536426~0.0001349006594585476
Speed: 700.3048386865713~47.01316551600015
Total Time: 0.1839
best val loss: 0.17997251468342404 test_score: 0.9302

Testing...
Test loss: 0.1720 score: 0.9302 time: 0.17s
test Score 0.9302
Epoch Time List: [0.7719869841821492, 0.6159643109422177, 0.6237466800957918, 0.6865689009428024, 0.6117218032013625, 0.6132300838362426, 0.7145833899267018, 0.6388231781311333, 0.8140990789979696, 0.6475675131659955, 0.6060097913723439, 0.6908481540158391, 0.5989389738533646, 0.5992735049221665, 0.58851980837062, 0.5895599129144102, 0.6158415540121496, 0.6024873836431652, 0.6148860261309892, 0.6046392603311688, 0.6110838456079364, 0.6086903971154243, 0.6294969217851758, 0.6161369860637933, 0.6179792149923742, 0.6357025997713208, 0.6888952220324427, 0.6438161020632833, 0.6133277660701424, 0.6114459717646241, 0.6017520639579743, 0.6116145940031856, 0.6153802992776036, 0.6137169147841632, 0.6186074300203472, 0.6122774889227003, 0.5938350271899253, 0.5957225172314793, 0.5958779340144247, 0.5970747519750148, 0.6230628781486303, 0.6480078937020153, 0.6252357009798288, 0.6207190807908773, 0.6332080690190196, 0.6520367730408907, 0.6246327741537243, 0.6375171898398548, 0.6231589110102504, 0.6255539059638977, 0.6232531568966806, 0.624034927925095, 0.6354674710892141, 0.6243943371810019, 0.5977087751962245, 0.5974333418998867, 0.5916446181945503, 0.5959108890965581, 0.5940762951504439, 0.5969376869034022, 0.5861434801481664, 0.592534925090149, 0.5889586419798434, 0.5914933481253684, 0.6008269181475043, 0.5953667759895325, 0.5948077959474176, 0.5897319160867482, 0.5900017337407917, 0.5848983370233327, 0.5833733549807221, 0.5840912330895662, 0.5959742900449783, 0.6328770641703159, 0.6406940701417625, 0.6438490690197796, 0.6436179580632597, 0.6165338878054172, 0.6568071150686592, 0.6153250320348889, 0.6112113751005381, 0.6181609029881656, 0.6408966151066124, 0.6265943751204759, 0.6050513978116214, 0.6058086489792913, 0.604473217856139, 0.5994405187666416, 0.603880925104022, 0.6031010057777166, 0.6041176142171025, 0.6109689930453897, 0.6622590289916843, 0.6032962221652269, 0.6114441000390798, 0.67644322803244, 0.6540391601156443, 0.6983961849473417, 0.607317958958447, 0.6090989180374891, 0.6906284128781408, 0.6050695909652859, 0.6012029289267957, 0.6246694638393819, 0.6268305070698261, 0.6078582860063761, 0.6342883610632271, 0.6251006321981549, 0.5990364220924675, 0.6220947462134063, 0.6263199199456722, 0.6027688009198755, 0.6251535892952234, 0.6084929490461946, 0.6151938089169562, 0.6217727772891521, 0.6163109322078526, 0.6168739588465542, 0.6010280507616699, 0.6129858738277107, 0.7740433381404728, 0.6079184191767126, 0.6156265172176063, 0.6151136599946767, 0.6095519592054188, 0.6308194098528475, 0.6755987929645926, 0.6334258620627224, 0.6778765709605068, 0.6580574908293784, 0.6133732460439205, 0.5996281360276043, 0.6004149236250669, 0.5990795427933335, 0.5958851410541683, 0.6010532979853451, 0.605830840067938, 0.6069397570099682]
Total Epoch List: [67, 71]
Total Time List: [0.1763010141439736, 0.18394593801349401]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef811c8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.14s
Epoch 2/1000, LR 0.000020
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.15s
Epoch 5/1000, LR 0.000110
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.15s
Epoch 6/1000, LR 0.000140
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.15s
Epoch 7/1000, LR 0.000170
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.14s
Epoch 8/1000, LR 0.000200
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.15s
Epoch 9/1000, LR 0.000230
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.15s
Epoch 10/1000, LR 0.000260
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.15s
Epoch 11/1000, LR 0.000290
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.15s
Epoch 12/1000, LR 0.000290
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.15s
Epoch 13/1000, LR 0.000290
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.15s
Epoch 14/1000, LR 0.000290
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.15s
Epoch 15/1000, LR 0.000290
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.26s
Val loss: 0.6911 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.25s
Val loss: 0.6903 score: 0.5659 time: 0.16s
Test loss: 0.6905 score: 0.5234 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.25s
Val loss: 0.6891 score: 0.5969 time: 0.16s
Test loss: 0.6894 score: 0.5547 time: 0.15s
Epoch 18/1000, LR 0.000290
Train loss: 0.6158;  Loss pred: 0.6158; Loss self: 0.0000; time: 0.25s
Val loss: 0.6873 score: 0.5504 time: 0.16s
Test loss: 0.6877 score: 0.5391 time: 0.15s
Epoch 19/1000, LR 0.000290
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.25s
Val loss: 0.6848 score: 0.5504 time: 0.16s
Test loss: 0.6855 score: 0.5391 time: 0.15s
Epoch 20/1000, LR 0.000290
Train loss: 0.5885;  Loss pred: 0.5885; Loss self: 0.0000; time: 0.25s
Val loss: 0.6811 score: 0.5349 time: 0.16s
Test loss: 0.6821 score: 0.5156 time: 0.14s
Epoch 21/1000, LR 0.000290
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.25s
Val loss: 0.6756 score: 0.5659 time: 0.16s
Test loss: 0.6771 score: 0.5547 time: 0.15s
Epoch 22/1000, LR 0.000290
Train loss: 0.5542;  Loss pred: 0.5542; Loss self: 0.0000; time: 0.25s
Val loss: 0.6682 score: 0.6744 time: 0.16s
Test loss: 0.6702 score: 0.6250 time: 0.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.25s
Val loss: 0.6605 score: 0.8915 time: 0.16s
Test loss: 0.6632 score: 0.8438 time: 0.15s
Epoch 24/1000, LR 0.000290
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.25s
Val loss: 0.6473 score: 0.9457 time: 0.16s
Test loss: 0.6511 score: 0.8828 time: 0.15s
Epoch 25/1000, LR 0.000290
Train loss: 0.4896;  Loss pred: 0.4896; Loss self: 0.0000; time: 0.25s
Val loss: 0.6312 score: 0.9457 time: 0.16s
Test loss: 0.6357 score: 0.9375 time: 0.15s
Epoch 26/1000, LR 0.000290
Train loss: 0.4700;  Loss pred: 0.4700; Loss self: 0.0000; time: 0.26s
Val loss: 0.6115 score: 0.9457 time: 0.16s
Test loss: 0.6172 score: 0.9141 time: 0.14s
Epoch 27/1000, LR 0.000290
Train loss: 0.4420;  Loss pred: 0.4420; Loss self: 0.0000; time: 0.25s
Val loss: 0.5898 score: 0.9225 time: 0.16s
Test loss: 0.5967 score: 0.8984 time: 0.15s
Epoch 28/1000, LR 0.000290
Train loss: 0.4223;  Loss pred: 0.4223; Loss self: 0.0000; time: 0.27s
Val loss: 0.5624 score: 0.9225 time: 0.17s
Test loss: 0.5710 score: 0.8906 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.26s
Val loss: 0.5402 score: 0.9070 time: 0.17s
Test loss: 0.5494 score: 0.8672 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.3704;  Loss pred: 0.3704; Loss self: 0.0000; time: 0.25s
Val loss: 0.5036 score: 0.9225 time: 0.16s
Test loss: 0.5149 score: 0.8750 time: 0.15s
Epoch 31/1000, LR 0.000290
Train loss: 0.3406;  Loss pred: 0.3406; Loss self: 0.0000; time: 0.25s
Val loss: 0.4677 score: 0.9147 time: 0.16s
Test loss: 0.4810 score: 0.8750 time: 0.14s
Epoch 32/1000, LR 0.000290
Train loss: 0.3229;  Loss pred: 0.3229; Loss self: 0.0000; time: 0.25s
Val loss: 0.4277 score: 0.9147 time: 0.16s
Test loss: 0.4432 score: 0.9062 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 0.2921;  Loss pred: 0.2921; Loss self: 0.0000; time: 0.25s
Val loss: 0.3965 score: 0.9225 time: 0.16s
Test loss: 0.4133 score: 0.8906 time: 0.15s
Epoch 34/1000, LR 0.000290
Train loss: 0.2721;  Loss pred: 0.2721; Loss self: 0.0000; time: 0.26s
Val loss: 0.3608 score: 0.9225 time: 0.16s
Test loss: 0.3789 score: 0.9062 time: 0.15s
Epoch 35/1000, LR 0.000290
Train loss: 0.2488;  Loss pred: 0.2488; Loss self: 0.0000; time: 0.25s
Val loss: 0.3301 score: 0.9225 time: 0.16s
Test loss: 0.3486 score: 0.8984 time: 0.15s
Epoch 36/1000, LR 0.000290
Train loss: 0.2334;  Loss pred: 0.2334; Loss self: 0.0000; time: 0.25s
Val loss: 0.3099 score: 0.9147 time: 0.16s
Test loss: 0.3293 score: 0.8906 time: 0.15s
Epoch 37/1000, LR 0.000290
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.25s
Val loss: 0.2958 score: 0.9225 time: 0.16s
Test loss: 0.3154 score: 0.8906 time: 0.15s
Epoch 38/1000, LR 0.000289
Train loss: 0.1978;  Loss pred: 0.1978; Loss self: 0.0000; time: 0.25s
Val loss: 0.2689 score: 0.9225 time: 0.16s
Test loss: 0.2887 score: 0.8984 time: 0.15s
Epoch 39/1000, LR 0.000289
Train loss: 0.1778;  Loss pred: 0.1778; Loss self: 0.0000; time: 0.25s
Val loss: 0.2489 score: 0.9147 time: 0.16s
Test loss: 0.2687 score: 0.9062 time: 0.14s
Epoch 40/1000, LR 0.000289
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.26s
Val loss: 0.2581 score: 0.9225 time: 0.16s
Test loss: 0.2776 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.1718;  Loss pred: 0.1718; Loss self: 0.0000; time: 0.26s
Val loss: 0.2347 score: 0.9225 time: 0.16s
Test loss: 0.2513 score: 0.9062 time: 0.15s
Epoch 42/1000, LR 0.000289
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.25s
Val loss: 0.2253 score: 0.9302 time: 0.16s
Test loss: 0.2390 score: 0.9062 time: 0.15s
Epoch 43/1000, LR 0.000289
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.25s
Val loss: 0.2427 score: 0.9225 time: 0.16s
Test loss: 0.2556 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 0.25s
Val loss: 0.2272 score: 0.9302 time: 0.16s
Test loss: 0.2358 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 0.27s
Val loss: 0.2683 score: 0.9070 time: 0.16s
Test loss: 0.2778 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.26s
Val loss: 0.2331 score: 0.9302 time: 0.16s
Test loss: 0.2419 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1071;  Loss pred: 0.1071; Loss self: 0.0000; time: 0.31s
Val loss: 0.2587 score: 0.9225 time: 0.17s
Test loss: 0.2678 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.25s
Val loss: 0.2725 score: 0.9070 time: 0.16s
Test loss: 0.2814 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0860;  Loss pred: 0.0860; Loss self: 0.0000; time: 0.25s
Val loss: 0.2549 score: 0.9225 time: 0.25s
Test loss: 0.2622 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.26s
Val loss: 0.2403 score: 0.9302 time: 0.16s
Test loss: 0.2464 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.26s
Val loss: 0.2195 score: 0.9225 time: 0.16s
Test loss: 0.2220 score: 0.9062 time: 0.15s
Epoch 52/1000, LR 0.000289
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.28s
Val loss: 0.2300 score: 0.9302 time: 0.18s
Test loss: 0.2325 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0750;  Loss pred: 0.0750; Loss self: 0.0000; time: 0.26s
Val loss: 0.2442 score: 0.9302 time: 0.16s
Test loss: 0.2481 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.25s
Val loss: 0.2425 score: 0.9302 time: 0.16s
Test loss: 0.2445 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 0.28s
Val loss: 0.2198 score: 0.9302 time: 0.18s
Test loss: 0.2222 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0627;  Loss pred: 0.0627; Loss self: 0.0000; time: 0.26s
Val loss: 0.2174 score: 0.9302 time: 0.17s
Test loss: 0.2170 score: 0.8984 time: 0.15s
Epoch 57/1000, LR 0.000288
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.28s
Val loss: 0.2418 score: 0.9302 time: 0.17s
Test loss: 0.2415 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0644;  Loss pred: 0.0644; Loss self: 0.0000; time: 0.26s
Val loss: 0.2292 score: 0.9302 time: 0.16s
Test loss: 0.2261 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.25s
Val loss: 0.2457 score: 0.9302 time: 0.16s
Test loss: 0.2437 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.25s
Val loss: 0.2611 score: 0.9302 time: 0.16s
Test loss: 0.2618 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.26s
Val loss: 0.2422 score: 0.9302 time: 0.16s
Test loss: 0.2444 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.25s
Val loss: 0.2307 score: 0.9302 time: 0.16s
Test loss: 0.2307 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.25s
Val loss: 0.2451 score: 0.9302 time: 0.16s
Test loss: 0.2486 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.25s
Val loss: 0.2558 score: 0.9302 time: 0.16s
Test loss: 0.2544 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.25s
Val loss: 0.2385 score: 0.9302 time: 0.16s
Test loss: 0.2242 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.25s
Val loss: 0.2380 score: 0.9302 time: 0.16s
Test loss: 0.2187 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.26s
Val loss: 0.2347 score: 0.9302 time: 0.16s
Test loss: 0.2175 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.25s
Val loss: 0.2695 score: 0.9302 time: 0.16s
Test loss: 0.2590 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.24s
Val loss: 0.2878 score: 0.9302 time: 0.16s
Test loss: 0.2832 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0550;  Loss pred: 0.0550; Loss self: 0.0000; time: 0.26s
Val loss: 0.2736 score: 0.9302 time: 0.16s
Test loss: 0.2643 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.25s
Val loss: 0.2661 score: 0.9302 time: 0.16s
Test loss: 0.2483 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.25s
Val loss: 0.2631 score: 0.9302 time: 0.16s
Test loss: 0.2321 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.25s
Val loss: 0.2769 score: 0.9302 time: 0.16s
Test loss: 0.2541 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.27s
Val loss: 0.2745 score: 0.9302 time: 0.16s
Test loss: 0.2469 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 18 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.25s
Val loss: 0.2661 score: 0.9302 time: 0.15s
Test loss: 0.2285 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.25s
Val loss: 0.2751 score: 0.9302 time: 0.16s
Test loss: 0.2414 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 055,   Train_Loss: 0.0627,   Val_Loss: 0.2174,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2174,   Test_Precision: 0.9474,   Test_Recall: 0.8438,   Test_accuracy: 0.8926,   Test_Score: 0.8984,   Test_loss: 0.2170


[0.18253785907290876, 0.1839626079890877, 0.18847330007702112, 0.23407855699770153, 0.19301296188496053, 0.1819267189130187, 0.1922175141517073, 0.19861392909660935, 0.3284006279427558, 0.1876483429223299, 0.17940224194899201, 0.1678359170909971, 0.17659638193435967, 0.17469651205465198, 0.1729864990338683, 0.17239712388254702, 0.17892897105775774, 0.17798338504508138, 0.18010637001134455, 0.1744397608563304, 0.1784680208656937, 0.1801996862050146, 0.18736211908981204, 0.17953693005256355, 0.17868106486275792, 0.1872842621523887, 0.22421239991672337, 0.1944086840376258, 0.17885235999710858, 0.17465620208531618, 0.17618028703145683, 0.18661550991237164, 0.18173951492644846, 0.17880620807409286, 0.1834795840550214, 0.17658571992069483, 0.1728559830226004, 0.17607563105411828, 0.17936754412949085, 0.17771658999845386, 0.18398700817488134, 0.19805452600121498, 0.18580170208588243, 0.18019313807599247, 0.18682111497037113, 0.19027028791606426, 0.18196199997328222, 0.18826145981438458, 0.18642704095691442, 0.1835658960044384, 0.18105687201023102, 0.18389299884438515, 0.1902521310839802, 0.18326080311089754, 0.1772156918887049, 0.1740894301328808, 0.16755001596175134, 0.17403832799755037, 0.17645484395325184, 0.1792945151682943, 0.17285337205976248, 0.17647420521825552, 0.1752832669299096, 0.17372872401028872, 0.17518768110312521, 0.17514205304905772, 0.1758794980123639, 0.17822269396856427, 0.17459705285727978, 0.17530617001466453, 0.17392966710031033, 0.17536031804047525, 0.18256420991383493, 0.19538038782775402, 0.19218465988524258, 0.19667000719346106, 0.19637999590486288, 0.1869225688278675, 0.20639769290573895, 0.18325063702650368, 0.17994738789275289, 0.19174756086431444, 0.19489842210896313, 0.18631190503947437, 0.18232848588377237, 0.18332257913425565, 0.18141483678482473, 0.1777221590746194, 0.18209024495445192, 0.18386575882323086, 0.18175130221061409, 0.1776424809359014, 0.17848971486091614, 0.18335698288865387, 0.1843132919166237, 0.19942716299556196, 0.19889992102980614, 0.27715128380805254, 0.17918031592853367, 0.18326123896986246, 0.18042206205427647, 0.18551692203618586, 0.1805558861233294, 0.18115337705239654, 0.21037676092237234, 0.17992931208573282, 0.1848947680555284, 0.1788562450092286, 0.17821334395557642, 0.18328054016456008, 0.2031301020178944, 0.1811746470630169, 0.18384995707310736, 0.17986601102165878, 0.17764997808262706, 0.18114085914567113, 0.19865722698159516, 0.18371627293527126, 0.17931505385786295, 0.18336410308256745, 0.18217508308589458, 0.1800120568368584, 0.18574499292299151, 0.184714529896155, 0.18019341793842614, 0.18685230892151594, 0.19780222000554204, 0.18443264788948, 0.1934593408368528, 0.2096774058882147, 0.1763452140148729, 0.1792906851042062, 0.18091987096704543, 0.17909321305342019, 0.17670270800590515, 0.1787396939471364, 0.1823250891175121, 0.1832976141013205, 0.14775802404619753, 0.14971664501354098, 0.1516298248898238, 0.15195858594961464, 0.1517622631508857, 0.15177283994853497, 0.14918949105776846, 0.15227866801433265, 0.15249835886061192, 0.15246679983101785, 0.1514689028263092, 0.15273413388058543, 0.1524553031194955, 0.15570635185576975, 0.15558851091191173, 0.15197504893876612, 0.15420912997797132, 0.15367432194761932, 0.14973058504983783, 0.148254020139575, 0.15076275006867945, 0.15184420812875032, 0.15278765303082764, 0.15368690597824752, 0.15271706692874432, 0.14921956090256572, 0.15838732803240418, 0.16528227808885276, 0.17354772589169443, 0.14980557188391685, 0.14888110011816025, 0.15052028698846698, 0.1519577440340072, 0.15370125602930784, 0.15275240503251553, 0.15388243505731225, 0.1524495508056134, 0.14978654216974974, 0.14830539701506495, 0.15115002798847854, 0.15296048996970057, 0.15292951790615916, 0.15287377801723778, 0.15245850384235382, 0.15182740986347198, 0.15044576092623174, 0.15559442597441375, 0.15630591404624283, 0.15318783395923674, 0.15571463108062744, 0.15145866619423032, 0.15154242096468806, 0.15318212402053177, 0.1537062767893076, 0.16027505788952112, 0.15912049589678645, 0.1473606899380684, 0.14929646207019687, 0.14984351419843733, 0.14839473087340593, 0.14500398794189095, 0.14912195084616542, 0.14962832699529827, 0.14919729810208082, 0.1473941041622311, 0.1488596450071782, 0.1513164930511266, 0.1503051151521504, 0.14949393412098289, 0.15044252621009946, 0.15110660693608224, 0.15017729508690536, 0.14880720619112253, 0.14890243182890117, 0.14980071899481118, 0.15141300903633237]
[0.001415022163355882, 0.0014260667285975791, 0.0014610333339303964, 0.0018145624573465235, 0.001496224510736128, 0.0014102846427365792, 0.001490058249238041, 0.001539642861214026, 0.002545741301881828, 0.0014546383172273636, 0.0013907150538681552, 0.0013010536208604426, 0.0013689642010415479, 0.0013542365275554416, 0.001340980612665646, 0.0013364118130430001, 0.0013870462872694399, 0.0013797161631401657, 0.0013961734109406555, 0.0013522462081886077, 0.0013834730299666178, 0.0013968967922869347, 0.0014524195278280002, 0.001391759147694291, 0.0013851245338198288, 0.0014518159856774318, 0.0017380806195094836, 0.0015070440623071767, 0.001386452403078361, 0.0013539240471729936, 0.0013657386591585802, 0.0014466318597858266, 0.001408833449042236, 0.0013860946362332781, 0.0014223223570156698, 0.0013688815497728281, 0.0013399688606403133, 0.0013649273725125448, 0.001390446078523185, 0.0013776479844841384, 0.0014262558773246615, 0.0015353064031101936, 0.0014403232719835846, 0.0013968460315968408, 0.0014482256974447374, 0.0014749634722175525, 0.001410558139327769, 0.0014593911613518184, 0.0014451708601311195, 0.0014229914418948714, 0.001403541643490163, 0.001425527122824691, 0.0014748227215812418, 0.001420626380704632, 0.0013737650534008132, 0.0013495304661463629, 0.001298837333036832, 0.001349134325562406, 0.001367867007389549, 0.0013898799625449171, 0.0013399486206183138, 0.0013680170947151591, 0.0013587850149605395, 0.001346734294653401, 0.0013580440395591102, 0.0013576903337136257, 0.0013634069613361543, 0.0013815712710741417, 0.0013534655260254247, 0.0013589625582532134, 0.0013482919930256614, 0.001359382310391281, 0.0014152264334405809, 0.0015145766498275506, 0.0014898035650018804, 0.0015245736991741168, 0.0015223255496501, 0.0014490121614563372, 0.0015999821155483639, 0.0014205475738488656, 0.001394940991416689, 0.0014864152004985616, 0.0015108404814648305, 0.0014442783336393362, 0.0014133991153780804, 0.001421105264606633, 0.0014063165642234476, 0.0013776911556172048, 0.0014115522864686196, 0.0014253159598700067, 0.0014089248233380937, 0.0013770734956271428, 0.0013836412004722182, 0.0014213719603771617, 0.0014287852086559978, 0.0015459469999655965, 0.0015418598529442337, 0.002148459564403508, 0.001388994697120416, 0.001420629759456298, 0.0013986206360796626, 0.0014381156746991152, 0.0013996580319637938, 0.0014042897445922212, 0.001630827604049398, 0.0013948008688816496, 0.001433292775624251, 0.0013864825194513843, 0.0013814987903533055, 0.0014207793811206208, 0.0015746519536270884, 0.0014044546283954798, 0.0014251934656830027, 0.0013943101629585951, 0.0013771316130436206, 0.0014041927065555901, 0.0015399785037332958, 0.0014241571545369866, 0.001390039177192736, 0.0014214271556788176, 0.0014122099464022836, 0.0013954423010609177, 0.0014398836660697017, 0.0014318955805903489, 0.0013968482010730708, 0.0014484675110195033, 0.0015333505426786205, 0.0014297104487556588, 0.0014996848126887813, 0.001625406247195463, 0.0013670171629059915, 0.0013898502721256295, 0.0014024796198995769, 0.001388319481034265, 0.0013697884341543035, 0.0013855790228460185, 0.001413372783856683, 0.0014209117372195388, 0.0011543595628609182, 0.001169661289168289, 0.0011846080069517484, 0.0011871764527313644, 0.0011856426808662945, 0.0011857253120979294, 0.0011655428988888161, 0.0011896770938619738, 0.0011913934285985306, 0.001191146873679827, 0.0011833508033305407, 0.0011932354209420737, 0.0011910570556210587, 0.0012164558738732012, 0.0012155352414993104, 0.0011873050698341103, 0.001204758827952901, 0.001200580640215776, 0.001169770195701858, 0.0011582345323404297, 0.0011778339849115582, 0.001186282876005862, 0.001193653539303341, 0.0012006789529550588, 0.001193102085380815, 0.0011657778195512947, 0.0012374010002531577, 0.0012912677975691622, 0.0013558416085288627, 0.0011703560303431004, 0.001163133594673127, 0.0011759397420973983, 0.0011871698752656812, 0.0012007910627289675, 0.0011933781643165275, 0.001202206523885252, 0.0011910121156688547, 0.0011702073607011698, 0.001158635914180195, 0.0011808595936599886, 0.0011950038278882857, 0.0011947618586418685, 0.0011943263907596702, 0.0011910820612683892, 0.0011861516395583749, 0.0011753575072361855, 0.0012155814529251074, 0.0012211399534862721, 0.001196779952806537, 0.0012165205553174019, 0.0011832708296424244, 0.0011839251637866255, 0.0011967353439104045, 0.0012008302874164656, 0.0012521488897618838, 0.0012431288741936442, 0.0011512553901411593, 0.001166378609923413, 0.0011706524546752917, 0.0011593338349484839, 0.001132843655796023, 0.0011650152409856673, 0.0011689713046507677, 0.0011656038914225064, 0.0011515164387674304, 0.0011629659766185796, 0.0011821601019619266, 0.001174258712126175, 0.0011679213603201788, 0.001175332236016402, 0.0011805203666881425, 0.0011732601178664481, 0.0011625562983681448, 0.0011633002486632904, 0.0011703181171469623, 0.0011829141330963466]
[706.702711728832, 701.2294585846056, 684.4470805534954, 551.0970404746073, 668.3488960543826, 709.0767138040698, 671.114703409321, 649.5012740886472, 392.8128907916895, 687.4561106750342, 719.0545591770115, 768.6078298130841, 730.479291744204, 738.4234435066666, 745.7229363011944, 748.2723440785869, 720.9564736073913, 724.7867544901768, 716.2434065595489, 739.5103006719045, 722.8185720571144, 715.8725007613814, 688.506303337463, 718.5151264546647, 721.956745103813, 688.7925259573376, 575.347304823074, 663.5506054607796, 721.2652939110532, 738.5938687535756, 732.2045058137927, 691.2608714065302, 709.8071107552335, 721.4514607151984, 703.0754983688855, 730.5233971236988, 746.285999155341, 732.6397141257486, 719.1936569464931, 725.874832513511, 701.1364621864187, 651.3357841628352, 694.2885805231904, 715.8985152120338, 690.500107658916, 677.982891668861, 708.9392291739005, 685.2172511951565, 691.9597035808421, 702.7449150842318, 712.4833129377745, 701.4948954590872, 678.047595393596, 703.9148460019438, 727.9265093579561, 740.9984621210807, 769.919353689876, 741.216038353435, 731.0652238834314, 719.4865937695556, 746.2972718599867, 730.9850175579966, 735.9515957195341, 742.5369681087409, 736.3531453108476, 736.5449802273745, 733.4567215499535, 723.8135454441824, 738.8440863629468, 735.8554464409839, 741.6791059894454, 735.6282278766469, 706.6007081063933, 660.2505063800237, 671.2294315114877, 655.9210620921207, 656.889717334013, 690.125332692132, 625.0069861920105, 703.95389665872, 716.8762020423598, 672.7595356025611, 661.8832446364247, 692.3873167024321, 707.5142393396098, 703.6776408514704, 711.0774525735526, 725.8520866035469, 708.4399278625172, 701.5988231067048, 709.761076982625, 726.1776536804109, 722.7307192491186, 703.5456079594038, 699.8952634319758, 646.852705831606, 648.5673766590822, 465.44976529620516, 719.9451531911119, 703.913171847617, 714.9901654554467, 695.3543568108466, 714.4602304013841, 712.1037548347124, 613.1855982305963, 716.9482198572182, 697.694160611717, 721.2496270026461, 723.8515205245017, 703.8390430548502, 635.0609718525911, 712.0201534331164, 701.6591249390586, 717.2005387080403, 726.1470076849693, 712.1529654237748, 649.3597135127198, 702.1696986278976, 719.4041840026102, 703.5182886473275, 708.1100105176139, 716.6186658092037, 694.500551374122, 698.3749468573087, 715.8974033340139, 690.3848325159536, 652.1665934608098, 699.4423247520817, 666.8067793572586, 615.2308087442372, 731.5197110431371, 719.5019636687954, 713.0228388428233, 720.2953020979176, 730.0397456030443, 721.7199333358639, 707.5274205233321, 703.7734813541725, 866.2812109613752, 854.9483592049712, 844.1611015049741, 842.3347664109044, 843.4244280657525, 843.3656512153548, 857.9692784824665, 840.5642212995485, 839.3532950541182, 839.5270323891174, 845.0579466253795, 838.0575890133131, 839.5903414371406, 822.0602337312864, 822.6828526719991, 842.2435188790355, 830.0416455127186, 832.9303059728447, 854.8687628342278, 863.3829954796053, 849.016086146545, 842.9692615701708, 837.7640304100601, 832.8621048439664, 838.1512464466269, 857.7963855796277, 808.1454595522484, 774.432694660643, 737.5492784035712, 854.4408488302837, 859.746468144124, 850.3837094717172, 842.3394333319031, 832.7843461187649, 837.9573465488374, 831.8038374706473, 839.6220213414159, 854.5494017409152, 863.0838969872261, 846.84072972687, 836.8174031434855, 836.9868796587951, 837.2920566244326, 839.5727150277917, 843.062528137058, 850.8049626121563, 822.6515776410176, 818.9069542316321, 835.5754937696996, 822.0165254331362, 845.1150615300747, 844.647981635625, 835.6066402555139, 832.7571435189703, 798.6270707712451, 804.4218268589814, 868.6169972045791, 857.3545429349584, 854.2244933637232, 862.5643191414628, 882.7343428050741, 858.3578693390695, 855.4529918925185, 857.9243835395891, 868.4200818447614, 859.8703832313163, 845.9091102299836, 851.6010906909493, 856.2220317007096, 850.823256060208, 847.0840725987827, 852.3259120223754, 860.1733966808132, 859.6233011632781, 854.4685289823855, 845.3698979675235]
Elapsed: 0.17352880403904297~0.021314833768979753
Time per graph: 0.001348455358774794~0.0001619818060384345
Speed: 750.7062415283309~78.66831378641773
Total Time: 0.1522
best val loss: 0.2173797738297965 test_score: 0.8984

Testing...
Test loss: 0.6511 score: 0.8828 time: 0.14s
test Score 0.8828
Epoch Time List: [0.7719869841821492, 0.6159643109422177, 0.6237466800957918, 0.6865689009428024, 0.6117218032013625, 0.6132300838362426, 0.7145833899267018, 0.6388231781311333, 0.8140990789979696, 0.6475675131659955, 0.6060097913723439, 0.6908481540158391, 0.5989389738533646, 0.5992735049221665, 0.58851980837062, 0.5895599129144102, 0.6158415540121496, 0.6024873836431652, 0.6148860261309892, 0.6046392603311688, 0.6110838456079364, 0.6086903971154243, 0.6294969217851758, 0.6161369860637933, 0.6179792149923742, 0.6357025997713208, 0.6888952220324427, 0.6438161020632833, 0.6133277660701424, 0.6114459717646241, 0.6017520639579743, 0.6116145940031856, 0.6153802992776036, 0.6137169147841632, 0.6186074300203472, 0.6122774889227003, 0.5938350271899253, 0.5957225172314793, 0.5958779340144247, 0.5970747519750148, 0.6230628781486303, 0.6480078937020153, 0.6252357009798288, 0.6207190807908773, 0.6332080690190196, 0.6520367730408907, 0.6246327741537243, 0.6375171898398548, 0.6231589110102504, 0.6255539059638977, 0.6232531568966806, 0.624034927925095, 0.6354674710892141, 0.6243943371810019, 0.5977087751962245, 0.5974333418998867, 0.5916446181945503, 0.5959108890965581, 0.5940762951504439, 0.5969376869034022, 0.5861434801481664, 0.592534925090149, 0.5889586419798434, 0.5914933481253684, 0.6008269181475043, 0.5953667759895325, 0.5948077959474176, 0.5897319160867482, 0.5900017337407917, 0.5848983370233327, 0.5833733549807221, 0.5840912330895662, 0.5959742900449783, 0.6328770641703159, 0.6406940701417625, 0.6438490690197796, 0.6436179580632597, 0.6165338878054172, 0.6568071150686592, 0.6153250320348889, 0.6112113751005381, 0.6181609029881656, 0.6408966151066124, 0.6265943751204759, 0.6050513978116214, 0.6058086489792913, 0.604473217856139, 0.5994405187666416, 0.603880925104022, 0.6031010057777166, 0.6041176142171025, 0.6109689930453897, 0.6622590289916843, 0.6032962221652269, 0.6114441000390798, 0.67644322803244, 0.6540391601156443, 0.6983961849473417, 0.607317958958447, 0.6090989180374891, 0.6906284128781408, 0.6050695909652859, 0.6012029289267957, 0.6246694638393819, 0.6268305070698261, 0.6078582860063761, 0.6342883610632271, 0.6251006321981549, 0.5990364220924675, 0.6220947462134063, 0.6263199199456722, 0.6027688009198755, 0.6251535892952234, 0.6084929490461946, 0.6151938089169562, 0.6217727772891521, 0.6163109322078526, 0.6168739588465542, 0.6010280507616699, 0.6129858738277107, 0.7740433381404728, 0.6079184191767126, 0.6156265172176063, 0.6151136599946767, 0.6095519592054188, 0.6308194098528475, 0.6755987929645926, 0.6334258620627224, 0.6778765709605068, 0.6580574908293784, 0.6133732460439205, 0.5996281360276043, 0.6004149236250669, 0.5990795427933335, 0.5958851410541683, 0.6010532979853451, 0.605830840067938, 0.6069397570099682, 0.5617470860015601, 0.5552393198013306, 0.5568308418150991, 0.5529897117521614, 0.5554260041099042, 0.5558143241796643, 0.5580551598686725, 0.5605660101864487, 0.5565431548748165, 0.5536171607673168, 0.5574384550563991, 0.555066104978323, 0.5549024848733097, 0.5649602098856121, 0.5679752419237047, 0.5599640002474189, 0.5610603550449014, 0.5566386189311743, 0.5542076299898326, 0.5569760790094733, 0.5592658701352775, 0.5565792848356068, 0.559417150914669, 0.5672753909602761, 0.5639046560972929, 0.5593311269767582, 0.5726800509728491, 0.5997096691280603, 0.597707083215937, 0.5586789997760206, 0.554247051011771, 0.5563210030086339, 0.5579229060094804, 0.563190828775987, 0.5605504983104765, 0.5623475280590355, 0.5538875260390341, 0.5566982300952077, 0.5534032138530165, 0.5602992859203368, 0.5602087790612131, 0.5542059177532792, 0.5542525607161224, 0.5569624011404812, 0.5764619952533394, 0.5634627889376134, 0.634167199023068, 0.5632503582164645, 0.6537385787814856, 0.5651674659457058, 0.5666387211531401, 0.6046004730742425, 0.5635951969306916, 0.5613528529647738, 0.6147803100757301, 0.5811446409206837, 0.5959117319434881, 0.5572603140026331, 0.5565513230394572, 0.5495289650280029, 0.5521980612538755, 0.5522862488869578, 0.5554866739548743, 0.5551701481454074, 0.5536832502111793, 0.552280608098954, 0.5579638578929007, 0.5558542290236801, 0.5472661948297173, 0.5646388998720795, 0.5547127709724009, 0.5567626757547259, 0.5518870721571147, 0.5710089490748942, 0.5538895181380212, 0.5543873768765479]
Total Epoch List: [67, 71, 76]
Total Time List: [0.1763010141439736, 0.18394593801349401, 0.15223694499582052]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798efab4dfc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.25s
Val loss: 0.6931 score: 0.5194 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.26s
Val loss: 0.6931 score: 0.5116 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.26s
Val loss: 0.6931 score: 0.5116 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.19s
Epoch 7/1000, LR 0.000165
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.26s
Val loss: 0.6931 score: 0.5116 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.19s
Epoch 8/1000, LR 0.000195
Train loss: 0.6738;  Loss pred: 0.6738; Loss self: 0.0000; time: 0.26s
Val loss: 0.6931 score: 0.5194 time: 0.18s
Test loss: 0.6931 score: 0.5116 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.25s
Val loss: 0.6930 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.19s
Epoch 11/1000, LR 0.000285
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4961 time: 0.20s
Epoch 16/1000, LR 0.000285
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4961 time: 0.20s
Epoch 17/1000, LR 0.000285
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4961 time: 0.19s
Epoch 18/1000, LR 0.000285
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4961 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.4906;  Loss pred: 0.4906; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.4961 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4617;  Loss pred: 0.4617; Loss self: 0.0000; time: 0.25s
Val loss: 0.6832 score: 0.5349 time: 0.18s
Test loss: 0.6807 score: 0.5426 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.4302;  Loss pred: 0.4302; Loss self: 0.0000; time: 0.25s
Val loss: 0.6804 score: 0.6589 time: 0.18s
Test loss: 0.6770 score: 0.6589 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.4119;  Loss pred: 0.4119; Loss self: 0.0000; time: 0.25s
Val loss: 0.6770 score: 0.7519 time: 0.18s
Test loss: 0.6726 score: 0.7674 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.3736;  Loss pred: 0.3736; Loss self: 0.0000; time: 0.25s
Val loss: 0.6728 score: 0.8140 time: 0.18s
Test loss: 0.6673 score: 0.8295 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3448;  Loss pred: 0.3448; Loss self: 0.0000; time: 0.25s
Val loss: 0.6680 score: 0.8372 time: 0.18s
Test loss: 0.6611 score: 0.9147 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.25s
Val loss: 0.6623 score: 0.8372 time: 0.18s
Test loss: 0.6538 score: 0.9380 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2886;  Loss pred: 0.2886; Loss self: 0.0000; time: 0.25s
Val loss: 0.6557 score: 0.8527 time: 0.18s
Test loss: 0.6453 score: 0.9535 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2644;  Loss pred: 0.2644; Loss self: 0.0000; time: 0.25s
Val loss: 0.6479 score: 0.8527 time: 0.18s
Test loss: 0.6353 score: 0.9535 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2394;  Loss pred: 0.2394; Loss self: 0.0000; time: 0.25s
Val loss: 0.6386 score: 0.8682 time: 0.18s
Test loss: 0.6235 score: 0.9535 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.2062;  Loss pred: 0.2062; Loss self: 0.0000; time: 0.25s
Val loss: 0.6274 score: 0.8682 time: 0.18s
Test loss: 0.6093 score: 0.9690 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 0.25s
Val loss: 0.6139 score: 0.8682 time: 0.18s
Test loss: 0.5924 score: 0.9690 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 0.25s
Val loss: 0.5982 score: 0.8682 time: 0.18s
Test loss: 0.5728 score: 0.9690 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.25s
Val loss: 0.5798 score: 0.8760 time: 0.18s
Test loss: 0.5500 score: 0.9690 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.25s
Val loss: 0.5584 score: 0.8760 time: 0.18s
Test loss: 0.5234 score: 0.9690 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.0946;  Loss pred: 0.0946; Loss self: 0.0000; time: 0.25s
Val loss: 0.5340 score: 0.8760 time: 0.18s
Test loss: 0.4932 score: 0.9612 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.25s
Val loss: 0.5074 score: 0.8760 time: 0.18s
Test loss: 0.4599 score: 0.9612 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.25s
Val loss: 0.4789 score: 0.8682 time: 0.18s
Test loss: 0.4239 score: 0.9612 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.25s
Val loss: 0.4498 score: 0.8760 time: 0.18s
Test loss: 0.3866 score: 0.9612 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.25s
Val loss: 0.4206 score: 0.8760 time: 0.18s
Test loss: 0.3484 score: 0.9612 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.25s
Val loss: 0.3931 score: 0.8760 time: 0.18s
Test loss: 0.3112 score: 0.9612 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.25s
Val loss: 0.3678 score: 0.8760 time: 0.18s
Test loss: 0.2754 score: 0.9612 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.25s
Val loss: 0.3474 score: 0.8760 time: 0.18s
Test loss: 0.2432 score: 0.9612 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.25s
Val loss: 0.3316 score: 0.8760 time: 0.18s
Test loss: 0.2151 score: 0.9612 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.26s
Val loss: 0.3207 score: 0.8760 time: 0.18s
Test loss: 0.1909 score: 0.9535 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.25s
Val loss: 0.3149 score: 0.8760 time: 0.18s
Test loss: 0.1714 score: 0.9535 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.25s
Val loss: 0.3154 score: 0.8760 time: 0.18s
Test loss: 0.1564 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.25s
Val loss: 0.3201 score: 0.8760 time: 0.18s
Test loss: 0.1455 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.25s
Val loss: 0.3265 score: 0.8682 time: 0.18s
Test loss: 0.1381 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.25s
Val loss: 0.3331 score: 0.8682 time: 0.18s
Test loss: 0.1322 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.25s
Val loss: 0.3427 score: 0.8682 time: 0.18s
Test loss: 0.1286 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.25s
Val loss: 0.3514 score: 0.8682 time: 0.18s
Test loss: 0.1262 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.25s
Val loss: 0.3623 score: 0.8682 time: 0.18s
Test loss: 0.1254 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.25s
Val loss: 0.3749 score: 0.8682 time: 0.18s
Test loss: 0.1260 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.25s
Val loss: 0.3856 score: 0.8682 time: 0.18s
Test loss: 0.1270 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.25s
Val loss: 0.3989 score: 0.8682 time: 0.18s
Test loss: 0.1291 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.25s
Val loss: 0.4102 score: 0.8682 time: 0.18s
Test loss: 0.1314 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.25s
Val loss: 0.4203 score: 0.8682 time: 0.18s
Test loss: 0.1340 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.24s
Val loss: 0.4305 score: 0.8682 time: 0.18s
Test loss: 0.1367 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.25s
Val loss: 0.4387 score: 0.8682 time: 0.18s
Test loss: 0.1391 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.25s
Val loss: 0.4462 score: 0.8682 time: 0.18s
Test loss: 0.1418 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.25s
Val loss: 0.4537 score: 0.8682 time: 0.18s
Test loss: 0.1433 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.24s
Val loss: 0.4536 score: 0.8682 time: 0.18s
Test loss: 0.1429 score: 0.9535 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.25s
Val loss: 0.4555 score: 0.8682 time: 0.29s
Test loss: 0.1430 score: 0.9535 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.26s
Val loss: 0.4567 score: 0.8682 time: 0.20s
Test loss: 0.1435 score: 0.9612 time: 0.19s
     INFO: Early stopping counter 19 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.26s
Val loss: 0.4590 score: 0.8682 time: 0.20s
Test loss: 0.1442 score: 0.9612 time: 0.26s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 043,   Train_Loss: 0.0168,   Val_Loss: 0.3149,   Val_Precision: 0.9615,   Val_Recall: 0.7812,   Val_accuracy: 0.8621,   Val_Score: 0.8760,   Val_Loss: 0.3149,   Test_Precision: 0.9683,   Test_Recall: 0.9385,   Test_accuracy: 0.9531,   Test_Score: 0.9535,   Test_loss: 0.1714


[0.17451642197556794, 0.179863756056875, 0.17808882193639874, 0.1790466441307217, 0.19296179106459022, 0.19152409583330154, 0.19463782291859388, 0.18407362280413508, 0.18418230512179434, 0.19024873804301023, 0.1871804699767381, 0.19182427204214036, 0.18096355092711747, 0.1821223150473088, 0.20508531108498573, 0.200457549886778, 0.19364653807133436, 0.18512640404514968, 0.18503513000905514, 0.18114893999882042, 0.18109959503635764, 0.18492800113745034, 0.18176604295149446, 0.17883265181444585, 0.17943953606300056, 0.18103378103114665, 0.18358638999052346, 0.17957824980840087, 0.18200621288269758, 0.18440873199142516, 0.18024207511916757, 0.180279437918216, 0.1831976098474115, 0.18119415803812444, 0.17954375105910003, 0.18239440605975688, 0.18396882200613618, 0.1817703910637647, 0.1814649999141693, 0.18274153606034815, 0.18486682302318513, 0.1811650451272726, 0.1834986270405352, 0.18485054606571794, 0.17951377597637475, 0.17695045401342213, 0.18113895505666733, 0.18464089091867208, 0.18458618293516338, 0.17969229095615447, 0.1815526019781828, 0.185175052145496, 0.18093399098142982, 0.1815995890647173, 0.1786027168855071, 0.18295401404611766, 0.18190860096365213, 0.1779758450575173, 0.1832274969201535, 0.18556160503067076, 0.18255026102997363, 0.1960034449584782, 0.19705787696875632, 0.26706946501508355]
[0.001352840480430759, 0.001394292682611434, 0.001380533503382936, 0.0013879584816335014, 0.0014958278377100018, 0.001484682913436446, 0.0015088203327022782, 0.0014269273085591866, 0.0014277698071456927, 0.001474796418938064, 0.0014510113951685124, 0.0014870098607917858, 0.0014028182242412208, 0.0014118008918396032, 0.001589808613061905, 0.001553934495246341, 0.0015011359540413516, 0.0014350884034507727, 0.001434380852783373, 0.0014042553488280653, 0.0014038728297392066, 0.0014335503964143436, 0.0014090390926472438, 0.0013862996264685725, 0.001391004155527136, 0.0014033626436522997, 0.0014231503100040578, 0.0013920794558790765, 0.001410900875059671, 0.0014295250541970941, 0.0013972253885206787, 0.0013975150226218293, 0.0014201365104450504, 0.0014046058762645305, 0.0013918120237139536, 0.0014139101244942393, 0.0014261148992723734, 0.0014090727989439123, 0.001406705425691235, 0.001416601054731381, 0.0014330761474665515, 0.001404380194785059, 0.0014224699770584125, 0.0014329499695016894, 0.001391579658731587, 0.0013717089458404816, 0.0014041779461757156, 0.0014313247358036596, 0.0014309006429082433, 0.0013929634957841432, 0.0014073845114587814, 0.001435465520507721, 0.001402589077375425, 0.0014077487524396688, 0.0013845171851589698, 0.001418248170900137, 0.0014101441935166833, 0.001379657713624165, 0.0014203681931794845, 0.0014384620545013238, 0.001415118302557935, 0.0015194065500657226, 0.001527580441618266, 0.002070305930349485]
[739.1854505134183, 717.2095303025292, 724.3576469166046, 720.4826464427744, 668.5261330146949, 673.5444928677738, 662.7694353833452, 700.8065470480983, 700.3930150331005, 678.0596882111065, 689.1744636394573, 672.4904967795786, 712.8507334162247, 708.3151779972183, 629.0065305873779, 643.5277697091555, 666.1621802527642, 696.8211836953238, 697.1649112992061, 712.1211970704328, 712.3152317049737, 697.5687792359738, 709.7035172539051, 721.3447806715326, 718.9051132784281, 712.5741906578513, 702.6664667607378, 718.349800923192, 708.7670138114465, 699.5330351601701, 715.704143523155, 715.5558142938132, 704.1576585384839, 711.9434831494818, 718.4878295070116, 707.2585326862296, 701.2057727678296, 709.6865405034368, 710.8808864575284, 705.9150469075587, 697.7996261872333, 712.057891241517, 703.0025351170809, 697.8610707167617, 718.6078020941261, 729.0176265397715, 712.160451403972, 698.6534746348252, 698.8605428029884, 717.8939024795251, 710.5378749432738, 696.638118933224, 712.9671948331695, 710.3540303388452, 722.2734471765913, 705.0952157162436, 709.1473372706329, 724.8174602475435, 704.0428001710646, 695.186916381102, 706.6547003119267, 658.151697422092, 654.6300101490135, 483.02040067633465]
Elapsed: 0.18528573488947586~0.011752685801223623
Time per graph: 0.0014363235262750066~9.11060914823537e-05
Speed: 698.3582030587779~34.056480476539114
Total Time: 0.2675
best val loss: 0.314890644635803 test_score: 0.9535

Testing...
Test loss: 0.5500 score: 0.9690 time: 0.18s
test Score 0.9690
Epoch Time List: [0.588912706123665, 0.5955892235506326, 0.5965950670652092, 0.5970035803038627, 0.6358401698525995, 0.6369635872542858, 0.6396311521530151, 0.6189721378032118, 0.6084533839020878, 0.6168984577525407, 0.6557779179420322, 0.6529392627999187, 0.6278037161100656, 0.6125560810323805, 0.6481367601081729, 0.6388881087768823, 0.64777901628986, 0.6102613287512213, 0.6138935112394392, 0.6053586821071804, 0.6072488988284022, 0.6108133578673005, 0.6046584260184318, 0.6022037158254534, 0.6084476809483021, 0.6017999220639467, 0.6057962798513472, 0.605717534897849, 0.60483343526721, 0.615804738830775, 0.6090344318654388, 0.6024090691935271, 0.609383292729035, 0.6065553987864405, 0.6060683999676257, 0.6081097330898046, 0.6092247099149972, 0.6073566928971559, 0.6081551900133491, 0.6092559020034969, 0.6098223552107811, 0.6088505340740085, 0.612507738173008, 0.6094164859969169, 0.6051619409117848, 0.6002596127800643, 0.6034225968178362, 0.6101926048286259, 0.6092910980805755, 0.6077058869414032, 0.6082843742333353, 0.6116961138322949, 0.6060195751488209, 0.6060581589117646, 0.6006856446620077, 0.6059450032189488, 0.6008624457754195, 0.6030993303284049, 0.6087455695960671, 0.6123132421635091, 0.6060293931514025, 0.7288052248768508, 0.6438003210350871, 0.7164462979417294]
Total Epoch List: [64]
Total Time List: [0.26753174886107445]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef8133a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.15s
Epoch 2/1000, LR 0.000015
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5039 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5039 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5039 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6581;  Loss pred: 0.6581; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5039 time: 0.15s
Epoch 11/1000, LR 0.000285
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5039 time: 0.15s
Epoch 12/1000, LR 0.000285
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5039 time: 0.15s
Epoch 13/1000, LR 0.000285
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 0.15s
Epoch 14/1000, LR 0.000285
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5039 time: 0.15s
Epoch 17/1000, LR 0.000285
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5039 time: 0.15s
Epoch 18/1000, LR 0.000285
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5039 time: 0.15s
Epoch 19/1000, LR 0.000285
Train loss: 0.4792;  Loss pred: 0.4792; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5039 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 0.4468;  Loss pred: 0.4468; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.5039 time: 0.15s
Epoch 21/1000, LR 0.000285
Train loss: 0.4177;  Loss pred: 0.4177; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.5039 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.3876;  Loss pred: 0.3876; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6730 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6699 score: 0.5039 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6659 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6627 score: 0.5039 time: 0.15s
Epoch 24/1000, LR 0.000285
Train loss: 0.3274;  Loss pred: 0.3274; Loss self: 0.0000; time: 0.23s
Val loss: 0.6574 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6544 score: 0.5039 time: 0.15s
Epoch 25/1000, LR 0.000285
Train loss: 0.2866;  Loss pred: 0.2866; Loss self: 0.0000; time: 0.22s
Val loss: 0.6472 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6446 score: 0.5039 time: 0.15s
Epoch 26/1000, LR 0.000285
Train loss: 0.2575;  Loss pred: 0.2575; Loss self: 0.0000; time: 0.24s
Val loss: 0.6353 score: 0.5116 time: 0.18s
Test loss: 0.6333 score: 0.5271 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.24s
Val loss: 0.6215 score: 0.5349 time: 0.18s
Test loss: 0.6204 score: 0.5736 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.23s
Val loss: 0.6051 score: 0.5581 time: 0.19s
Test loss: 0.6052 score: 0.6202 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.23s
Val loss: 0.5866 score: 0.6357 time: 0.17s
Test loss: 0.5884 score: 0.6357 time: 0.15s
Epoch 30/1000, LR 0.000285
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.22s
Val loss: 0.5661 score: 0.6977 time: 0.17s
Test loss: 0.5702 score: 0.6512 time: 0.15s
Epoch 31/1000, LR 0.000285
Train loss: 0.1364;  Loss pred: 0.1364; Loss self: 0.0000; time: 0.22s
Val loss: 0.5440 score: 0.7597 time: 0.17s
Test loss: 0.5512 score: 0.6822 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.22s
Val loss: 0.5192 score: 0.7984 time: 0.17s
Test loss: 0.5301 score: 0.7287 time: 0.15s
Epoch 33/1000, LR 0.000285
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 0.23s
Val loss: 0.4930 score: 0.8295 time: 0.17s
Test loss: 0.5084 score: 0.7519 time: 0.15s
Epoch 34/1000, LR 0.000285
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.22s
Val loss: 0.4652 score: 0.8450 time: 0.17s
Test loss: 0.4857 score: 0.7674 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.24s
Val loss: 0.4388 score: 0.8605 time: 0.17s
Test loss: 0.4651 score: 0.7829 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.25s
Val loss: 0.4131 score: 0.8682 time: 0.17s
Test loss: 0.4458 score: 0.7984 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.24s
Val loss: 0.3872 score: 0.8682 time: 0.17s
Test loss: 0.4274 score: 0.8217 time: 0.15s
Epoch 38/1000, LR 0.000284
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.23s
Val loss: 0.3616 score: 0.8682 time: 0.17s
Test loss: 0.4101 score: 0.8295 time: 0.15s
Epoch 39/1000, LR 0.000284
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.23s
Val loss: 0.3379 score: 0.8760 time: 0.17s
Test loss: 0.3941 score: 0.8372 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.24s
Val loss: 0.3163 score: 0.8760 time: 0.17s
Test loss: 0.3800 score: 0.8450 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.22s
Val loss: 0.2979 score: 0.8915 time: 0.17s
Test loss: 0.3679 score: 0.8450 time: 0.15s
Epoch 42/1000, LR 0.000284
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.24s
Val loss: 0.2806 score: 0.9070 time: 0.17s
Test loss: 0.3556 score: 0.8527 time: 0.15s
Epoch 43/1000, LR 0.000284
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.24s
Val loss: 0.2663 score: 0.9070 time: 0.17s
Test loss: 0.3448 score: 0.8605 time: 0.15s
Epoch 44/1000, LR 0.000284
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.24s
Val loss: 0.2544 score: 0.9070 time: 0.17s
Test loss: 0.3362 score: 0.8682 time: 0.15s
Epoch 45/1000, LR 0.000284
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.23s
Val loss: 0.2473 score: 0.9147 time: 0.17s
Test loss: 0.3327 score: 0.8760 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.24s
Val loss: 0.2411 score: 0.9147 time: 0.17s
Test loss: 0.3291 score: 0.8837 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.24s
Val loss: 0.2367 score: 0.9147 time: 0.17s
Test loss: 0.3271 score: 0.8837 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.24s
Val loss: 0.2352 score: 0.9225 time: 0.17s
Test loss: 0.3290 score: 0.8915 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.24s
Val loss: 0.2340 score: 0.9225 time: 0.17s
Test loss: 0.3306 score: 0.8915 time: 0.15s
Epoch 50/1000, LR 0.000284
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.24s
Val loss: 0.2354 score: 0.9302 time: 0.17s
Test loss: 0.3354 score: 0.8915 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.24s
Val loss: 0.2352 score: 0.9302 time: 0.17s
Test loss: 0.3374 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.23s
Val loss: 0.2354 score: 0.9302 time: 0.17s
Test loss: 0.3397 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.22s
Val loss: 0.2386 score: 0.9302 time: 0.17s
Test loss: 0.3451 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.24s
Val loss: 0.2422 score: 0.9302 time: 0.17s
Test loss: 0.3505 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.24s
Val loss: 0.2491 score: 0.9302 time: 0.17s
Test loss: 0.3588 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.22s
Val loss: 0.2533 score: 0.9302 time: 0.17s
Test loss: 0.3646 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.23s
Val loss: 0.2580 score: 0.9302 time: 0.17s
Test loss: 0.3719 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.24s
Val loss: 0.2636 score: 0.9302 time: 0.16s
Test loss: 0.3806 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.24s
Val loss: 0.2678 score: 0.9302 time: 0.17s
Test loss: 0.3891 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.22s
Val loss: 0.2705 score: 0.9302 time: 0.17s
Test loss: 0.3944 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.24s
Val loss: 0.2725 score: 0.9302 time: 0.17s
Test loss: 0.3989 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.2749 score: 0.9302 time: 0.17s
Test loss: 0.4034 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.24s
Val loss: 0.2803 score: 0.9302 time: 0.17s
Test loss: 0.4098 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.22s
Val loss: 0.2852 score: 0.9302 time: 0.17s
Test loss: 0.4163 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.24s
Val loss: 0.2883 score: 0.9302 time: 0.17s
Test loss: 0.4200 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.23s
Val loss: 0.2904 score: 0.9302 time: 0.17s
Test loss: 0.4223 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.24s
Val loss: 0.2906 score: 0.9302 time: 0.17s
Test loss: 0.4235 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.22s
Val loss: 0.2946 score: 0.9302 time: 0.17s
Test loss: 0.4286 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.23s
Val loss: 0.2959 score: 0.9302 time: 0.17s
Test loss: 0.4318 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0112,   Val_Loss: 0.2340,   Val_Precision: 1.0000,   Val_Recall: 0.8462,   Val_accuracy: 0.9167,   Val_Score: 0.9225,   Val_Loss: 0.2340,   Test_Precision: 0.9808,   Test_Recall: 0.7969,   Test_accuracy: 0.8793,   Test_Score: 0.8915,   Test_loss: 0.3306


[0.17451642197556794, 0.179863756056875, 0.17808882193639874, 0.1790466441307217, 0.19296179106459022, 0.19152409583330154, 0.19463782291859388, 0.18407362280413508, 0.18418230512179434, 0.19024873804301023, 0.1871804699767381, 0.19182427204214036, 0.18096355092711747, 0.1821223150473088, 0.20508531108498573, 0.200457549886778, 0.19364653807133436, 0.18512640404514968, 0.18503513000905514, 0.18114893999882042, 0.18109959503635764, 0.18492800113745034, 0.18176604295149446, 0.17883265181444585, 0.17943953606300056, 0.18103378103114665, 0.18358638999052346, 0.17957824980840087, 0.18200621288269758, 0.18440873199142516, 0.18024207511916757, 0.180279437918216, 0.1831976098474115, 0.18119415803812444, 0.17954375105910003, 0.18239440605975688, 0.18396882200613618, 0.1817703910637647, 0.1814649999141693, 0.18274153606034815, 0.18486682302318513, 0.1811650451272726, 0.1834986270405352, 0.18485054606571794, 0.17951377597637475, 0.17695045401342213, 0.18113895505666733, 0.18464089091867208, 0.18458618293516338, 0.17969229095615447, 0.1815526019781828, 0.185175052145496, 0.18093399098142982, 0.1815995890647173, 0.1786027168855071, 0.18295401404611766, 0.18190860096365213, 0.1779758450575173, 0.1832274969201535, 0.18556160503067076, 0.18255026102997363, 0.1960034449584782, 0.19705787696875632, 0.26706946501508355, 0.15796702587977052, 0.16594073898158967, 0.1710662660188973, 0.15598975913599133, 0.15528180892579257, 0.15718317101709545, 0.16549369716085494, 0.16487835394218564, 0.16156957298517227, 0.1584637959022075, 0.15782025386579335, 0.15647092508152127, 0.15722347097471356, 0.16168965701945126, 0.15951010305434465, 0.15554758091457188, 0.1560237759258598, 0.1555711671244353, 0.15448727109469473, 0.1557130308356136, 0.1607805211097002, 0.16012784512713552, 0.1565850479528308, 0.15318537899293005, 0.1522907610051334, 0.17425592499785125, 0.1771918369922787, 0.17551540001295507, 0.1579660198185593, 0.15618073800578713, 0.1548688360489905, 0.15559869189746678, 0.1593288208823651, 0.1617071230430156, 0.16774908010847867, 0.15940295602194965, 0.15551081509329379, 0.15678612212650478, 0.15988347702659667, 0.16105286800302565, 0.15943467197939754, 0.1561484111007303, 0.15534866391681135, 0.156112315133214, 0.15806486899964511, 0.16074619814753532, 0.15772494906559587, 0.1573774740099907, 0.15488479984924197, 0.15661081997677684, 0.16000869404524565, 0.1615475290454924, 0.16065761097706854, 0.15956430207006633, 0.1554167449939996, 0.1542647979222238, 0.15778423799201846, 0.15921690594404936, 0.15784626896493137, 0.15823598392307758, 0.15680100210011005, 0.1611438118852675, 0.15812596888281405, 0.15873547899536788, 0.1557157461065799, 0.15462416782975197, 0.15764277707785368, 0.15862553589977324, 0.15946911298669875]
[0.001352840480430759, 0.001394292682611434, 0.001380533503382936, 0.0013879584816335014, 0.0014958278377100018, 0.001484682913436446, 0.0015088203327022782, 0.0014269273085591866, 0.0014277698071456927, 0.001474796418938064, 0.0014510113951685124, 0.0014870098607917858, 0.0014028182242412208, 0.0014118008918396032, 0.001589808613061905, 0.001553934495246341, 0.0015011359540413516, 0.0014350884034507727, 0.001434380852783373, 0.0014042553488280653, 0.0014038728297392066, 0.0014335503964143436, 0.0014090390926472438, 0.0013862996264685725, 0.001391004155527136, 0.0014033626436522997, 0.0014231503100040578, 0.0013920794558790765, 0.001410900875059671, 0.0014295250541970941, 0.0013972253885206787, 0.0013975150226218293, 0.0014201365104450504, 0.0014046058762645305, 0.0013918120237139536, 0.0014139101244942393, 0.0014261148992723734, 0.0014090727989439123, 0.001406705425691235, 0.001416601054731381, 0.0014330761474665515, 0.001404380194785059, 0.0014224699770584125, 0.0014329499695016894, 0.001391579658731587, 0.0013717089458404816, 0.0014041779461757156, 0.0014313247358036596, 0.0014309006429082433, 0.0013929634957841432, 0.0014073845114587814, 0.001435465520507721, 0.001402589077375425, 0.0014077487524396688, 0.0013845171851589698, 0.001418248170900137, 0.0014101441935166833, 0.001379657713624165, 0.0014203681931794845, 0.0014384620545013238, 0.001415118302557935, 0.0015194065500657226, 0.001527580441618266, 0.002070305930349485, 0.0012245505882152753, 0.0012863623176867416, 0.0013260950854178085, 0.0012092229390386925, 0.0012037349529131207, 0.0012184741939309724, 0.0012828968772159298, 0.0012781267747456252, 0.0012524773099625757, 0.0012284015186217634, 0.0012234128206650647, 0.001212952907608692, 0.0012187865967032059, 0.0012534081939492346, 0.0012365124267778656, 0.0012057952008881542, 0.0012094866350841845, 0.0012059780397243047, 0.0011975757449201143, 0.0012070777584156094, 0.001246360628757366, 0.0012413011250165544, 0.001213837581029696, 0.001187483558084729, 0.0011805485349235146, 0.0013508211240143508, 0.0013735801317230907, 0.001360584496224458, 0.0012245427892911573, 0.0012107033953936987, 0.001200533612782872, 0.001206191410057882, 0.0012351071386229853, 0.001253543589480741, 0.001300380465957199, 0.0012356818296275166, 0.0012055101945216572, 0.0012153962955543005, 0.0012394067986557882, 0.001248471844984695, 0.0012359276897627716, 0.0012104527992304675, 0.0012042532086574524, 0.0012101729855287908, 0.0012253090620127527, 0.0012460945592832197, 0.001222674023764309, 0.0012199804186820984, 0.0012006573631724183, 0.0012140373641610607, 0.0012403774732189586, 0.0012523064267092436, 0.0012454078370315392, 0.0012369325741865608, 0.0012047809689457332, 0.0011958511466839054, 0.0012231336278451044, 0.001234239580961623, 0.0012236144881002432, 0.0012266355342874231, 0.0012155116441868996, 0.0012491768363199031, 0.0012257827045179385, 0.0012305075891113789, 0.0012070988070277512, 0.0011986369599205579, 0.0012220370316112688, 0.0012296553170525056, 0.0012361946743154942]
[739.1854505134183, 717.2095303025292, 724.3576469166046, 720.4826464427744, 668.5261330146949, 673.5444928677738, 662.7694353833452, 700.8065470480983, 700.3930150331005, 678.0596882111065, 689.1744636394573, 672.4904967795786, 712.8507334162247, 708.3151779972183, 629.0065305873779, 643.5277697091555, 666.1621802527642, 696.8211836953238, 697.1649112992061, 712.1211970704328, 712.3152317049737, 697.5687792359738, 709.7035172539051, 721.3447806715326, 718.9051132784281, 712.5741906578513, 702.6664667607378, 718.349800923192, 708.7670138114465, 699.5330351601701, 715.704143523155, 715.5558142938132, 704.1576585384839, 711.9434831494818, 718.4878295070116, 707.2585326862296, 701.2057727678296, 709.6865405034368, 710.8808864575284, 705.9150469075587, 697.7996261872333, 712.057891241517, 703.0025351170809, 697.8610707167617, 718.6078020941261, 729.0176265397715, 712.160451403972, 698.6534746348252, 698.8605428029884, 717.8939024795251, 710.5378749432738, 696.638118933224, 712.9671948331695, 710.3540303388452, 722.2734471765913, 705.0952157162436, 709.1473372706329, 724.8174602475435, 704.0428001710646, 695.186916381102, 706.6547003119267, 658.151697422092, 654.6300101490135, 483.02040067633465, 816.6261235948225, 777.3859559243733, 754.0937380707759, 826.9773651457352, 830.7476638274329, 820.6985465764003, 779.4858789976506, 782.395001621824, 798.4176575860525, 814.0660727300105, 817.385581635793, 824.4343154026287, 820.4881828410163, 797.8246869834185, 808.7262031048281, 829.3282302528893, 826.7970649633482, 829.2024954522449, 835.0202517392386, 828.4470433061279, 802.3359988489127, 805.6062947551616, 823.8334482540093, 842.1169229600804, 847.06386092359, 740.2904664595519, 728.0245082939193, 734.9782411713064, 816.6313245606249, 825.9661315931291, 832.9629336091397, 829.0558129177961, 809.6463608127915, 797.7385137554194, 769.0057073134427, 809.2698104183012, 829.5242997897641, 822.7769030215237, 806.8375944722594, 800.9792163252661, 809.1088243131308, 826.137128713932, 830.3901478617096, 826.3281464368876, 816.1206270337632, 802.5073157973031, 817.8794842808951, 819.6852873099918, 832.877081066463, 823.6978774463275, 806.2061925430295, 798.5266055271765, 802.9498211473642, 808.4515040422687, 830.0263913324171, 836.2244772461852, 817.5721582945787, 810.2154682325762, 817.2508659590799, 815.2380817672299, 822.6988238100645, 800.5271719142804, 815.8052779780968, 812.6727611019109, 828.4325973797521, 834.2809653277144, 818.30580754291, 813.236023243496, 808.9340787313455]
Elapsed: 0.17169944791620442~0.01580405600799307
Time per graph: 0.0013310034722186388~0.00012251206207746567
Speed: 757.0707097680922~63.53679374361028
Total Time: 0.1601
best val loss: 0.23399660735961425 test_score: 0.8915

Testing...
Test loss: 0.3354 score: 0.8915 time: 0.15s
test Score 0.8915
Epoch Time List: [0.588912706123665, 0.5955892235506326, 0.5965950670652092, 0.5970035803038627, 0.6358401698525995, 0.6369635872542858, 0.6396311521530151, 0.6189721378032118, 0.6084533839020878, 0.6168984577525407, 0.6557779179420322, 0.6529392627999187, 0.6278037161100656, 0.6125560810323805, 0.6481367601081729, 0.6388881087768823, 0.64777901628986, 0.6102613287512213, 0.6138935112394392, 0.6053586821071804, 0.6072488988284022, 0.6108133578673005, 0.6046584260184318, 0.6022037158254534, 0.6084476809483021, 0.6017999220639467, 0.6057962798513472, 0.605717534897849, 0.60483343526721, 0.615804738830775, 0.6090344318654388, 0.6024090691935271, 0.609383292729035, 0.6065553987864405, 0.6060683999676257, 0.6081097330898046, 0.6092247099149972, 0.6073566928971559, 0.6081551900133491, 0.6092559020034969, 0.6098223552107811, 0.6088505340740085, 0.612507738173008, 0.6094164859969169, 0.6051619409117848, 0.6002596127800643, 0.6034225968178362, 0.6101926048286259, 0.6092910980805755, 0.6077058869414032, 0.6082843742333353, 0.6116961138322949, 0.6060195751488209, 0.6060581589117646, 0.6006856446620077, 0.6059450032189488, 0.6008624457754195, 0.6030993303284049, 0.6087455695960671, 0.6123132421635091, 0.6060293931514025, 0.7288052248768508, 0.6438003210350871, 0.7164462979417294, 0.5563340708613396, 0.5427145569119602, 0.5839294260367751, 0.6287456799764186, 0.5641445126384497, 0.5590395268518478, 0.5747090289369226, 0.5978401789907366, 0.565180208068341, 0.5558299680706114, 0.5591249049175531, 0.5411375318653882, 0.5456883709412068, 0.5608925498090684, 0.556386805139482, 0.553242176072672, 0.538220820017159, 0.5356370320077986, 0.5548570167738944, 0.5556859709322453, 0.5597795457579195, 0.5447920758742839, 0.5547930302564055, 0.552608918165788, 0.5348728951066732, 0.5934050851501524, 0.5927790247369558, 0.5915535059757531, 0.5577225591987371, 0.5411263599526137, 0.5441223138477653, 0.5404210400301963, 0.5469326977618039, 0.5438895020633936, 0.5726743801496923, 0.5762496141251177, 0.5619000380393118, 0.552438922226429, 0.5488932901062071, 0.5700034371111542, 0.5488840297330171, 0.566444902215153, 0.563719434896484, 0.5638873078860343, 0.5482067910488695, 0.5668409448117018, 0.5624394190963358, 0.5652089850045741, 0.5618555922992527, 0.5631499157752842, 0.5671647402923554, 0.5500542137306184, 0.5478356981184334, 0.567951092030853, 0.5565533440094441, 0.5456446660682559, 0.5471822412218899, 0.5600385912694037, 0.5605959598906338, 0.547013231087476, 0.5644367809873074, 0.5553689538501203, 0.5662452802062035, 0.5486398090142757, 0.56211356096901, 0.54408247792162, 0.5619007498025894, 0.5451739409472793, 0.5568194051738828]
Total Epoch List: [64, 69]
Total Time List: [0.26753174886107445, 0.1601415821351111]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef811dc30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.17s
Epoch 13/1000, LR 0.000290
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.18s
Epoch 15/1000, LR 0.000290
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.18s
Epoch 16/1000, LR 0.000290
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 0.20s
Epoch 17/1000, LR 0.000290
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.19s
Epoch 18/1000, LR 0.000290
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.5000 time: 0.19s
Epoch 19/1000, LR 0.000290
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.5000 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.5236;  Loss pred: 0.5236; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6740 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6741 score: 0.5000 time: 0.19s
Epoch 22/1000, LR 0.000290
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6675 score: 0.4961 time: 0.19s
Test loss: 0.6680 score: 0.5078 time: 0.18s
Epoch 23/1000, LR 0.000290
Train loss: 0.4832;  Loss pred: 0.4832; Loss self: 0.0000; time: 0.37s
Val loss: 0.6573 score: 0.5194 time: 0.17s
Test loss: 0.6584 score: 0.5156 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.4597;  Loss pred: 0.4597; Loss self: 0.0000; time: 0.27s
Val loss: 0.6456 score: 0.6124 time: 0.18s
Test loss: 0.6474 score: 0.5859 time: 0.17s
Epoch 25/1000, LR 0.000290
Train loss: 0.4398;  Loss pred: 0.4398; Loss self: 0.0000; time: 0.28s
Val loss: 0.6303 score: 0.7752 time: 0.17s
Test loss: 0.6326 score: 0.8125 time: 0.17s
Epoch 26/1000, LR 0.000290
Train loss: 0.4129;  Loss pred: 0.4129; Loss self: 0.0000; time: 0.27s
Val loss: 0.6094 score: 0.8140 time: 0.17s
Test loss: 0.6121 score: 0.8516 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.27s
Val loss: 0.5906 score: 0.7907 time: 0.17s
Test loss: 0.5930 score: 0.8359 time: 0.18s
Epoch 28/1000, LR 0.000290
Train loss: 0.3745;  Loss pred: 0.3745; Loss self: 0.0000; time: 0.27s
Val loss: 0.5671 score: 0.8295 time: 0.18s
Test loss: 0.5692 score: 0.8594 time: 0.17s
Epoch 29/1000, LR 0.000290
Train loss: 0.3460;  Loss pred: 0.3460; Loss self: 0.0000; time: 0.27s
Val loss: 0.5461 score: 0.8217 time: 0.18s
Test loss: 0.5464 score: 0.8516 time: 0.17s
Epoch 30/1000, LR 0.000290
Train loss: 0.3158;  Loss pred: 0.3158; Loss self: 0.0000; time: 0.28s
Val loss: 0.5157 score: 0.8527 time: 0.17s
Test loss: 0.5149 score: 0.8672 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.2992;  Loss pred: 0.2992; Loss self: 0.0000; time: 0.27s
Val loss: 0.4843 score: 0.8605 time: 0.18s
Test loss: 0.4815 score: 0.8750 time: 0.18s
Epoch 32/1000, LR 0.000290
Train loss: 0.2685;  Loss pred: 0.2685; Loss self: 0.0000; time: 0.27s
Val loss: 0.4638 score: 0.8527 time: 0.18s
Test loss: 0.4564 score: 0.8672 time: 0.17s
Epoch 33/1000, LR 0.000290
Train loss: 0.2528;  Loss pred: 0.2528; Loss self: 0.0000; time: 0.27s
Val loss: 0.4314 score: 0.8682 time: 0.18s
Test loss: 0.4207 score: 0.8750 time: 0.19s
Epoch 34/1000, LR 0.000290
Train loss: 0.2286;  Loss pred: 0.2286; Loss self: 0.0000; time: 0.29s
Val loss: 0.4022 score: 0.8682 time: 0.18s
Test loss: 0.3863 score: 0.8750 time: 0.18s
Epoch 35/1000, LR 0.000290
Train loss: 0.2176;  Loss pred: 0.2176; Loss self: 0.0000; time: 0.28s
Val loss: 0.3746 score: 0.8837 time: 0.19s
Test loss: 0.3541 score: 0.8906 time: 0.20s
Epoch 36/1000, LR 0.000290
Train loss: 0.1876;  Loss pred: 0.1876; Loss self: 0.0000; time: 0.28s
Val loss: 0.3780 score: 0.8682 time: 0.19s
Test loss: 0.3511 score: 0.8828 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.1763;  Loss pred: 0.1763; Loss self: 0.0000; time: 0.28s
Val loss: 0.3564 score: 0.8760 time: 0.19s
Test loss: 0.3230 score: 0.8906 time: 0.19s
Epoch 38/1000, LR 0.000289
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.34s
Val loss: 0.3407 score: 0.8837 time: 0.19s
Test loss: 0.2996 score: 0.8828 time: 0.18s
Epoch 39/1000, LR 0.000289
Train loss: 0.1492;  Loss pred: 0.1492; Loss self: 0.0000; time: 0.28s
Val loss: 0.3323 score: 0.8837 time: 0.18s
Test loss: 0.2854 score: 0.8828 time: 0.18s
Epoch 40/1000, LR 0.000289
Train loss: 0.1383;  Loss pred: 0.1383; Loss self: 0.0000; time: 0.28s
Val loss: 0.3346 score: 0.8760 time: 0.19s
Test loss: 0.2804 score: 0.8828 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.1289;  Loss pred: 0.1289; Loss self: 0.0000; time: 0.28s
Val loss: 0.3744 score: 0.8760 time: 0.20s
Test loss: 0.3085 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.1261;  Loss pred: 0.1261; Loss self: 0.0000; time: 0.28s
Val loss: 0.3679 score: 0.8760 time: 0.17s
Test loss: 0.2964 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.1207;  Loss pred: 0.1207; Loss self: 0.0000; time: 0.28s
Val loss: 0.4043 score: 0.8682 time: 0.17s
Test loss: 0.3199 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.1076;  Loss pred: 0.1076; Loss self: 0.0000; time: 0.28s
Val loss: 0.3476 score: 0.8837 time: 0.17s
Test loss: 0.2655 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0928;  Loss pred: 0.0928; Loss self: 0.0000; time: 0.27s
Val loss: 0.3400 score: 0.8915 time: 0.17s
Test loss: 0.2539 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.27s
Val loss: 0.3470 score: 0.8837 time: 0.18s
Test loss: 0.2526 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.28s
Val loss: 0.4015 score: 0.8682 time: 0.17s
Test loss: 0.2871 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.27s
Val loss: 0.4182 score: 0.8682 time: 0.17s
Test loss: 0.2968 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.27s
Val loss: 0.4373 score: 0.8605 time: 0.17s
Test loss: 0.3137 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.27s
Val loss: 0.4489 score: 0.8605 time: 0.17s
Test loss: 0.3195 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.27s
Val loss: 0.4516 score: 0.8605 time: 0.17s
Test loss: 0.3151 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.28s
Val loss: 0.4379 score: 0.8682 time: 0.17s
Test loss: 0.3028 score: 0.8672 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.27s
Val loss: 0.4161 score: 0.8760 time: 0.18s
Test loss: 0.2779 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.28s
Val loss: 0.3981 score: 0.8605 time: 0.17s
Test loss: 0.2601 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.27s
Val loss: 0.4495 score: 0.8682 time: 0.17s
Test loss: 0.2997 score: 0.8906 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.28s
Val loss: 0.4378 score: 0.8682 time: 0.18s
Test loss: 0.2826 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.28s
Val loss: 0.4928 score: 0.8682 time: 0.17s
Test loss: 0.3235 score: 0.8750 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.27s
Val loss: 0.5395 score: 0.8527 time: 0.18s
Test loss: 0.3589 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.28s
Val loss: 0.5009 score: 0.8682 time: 0.17s
Test loss: 0.3213 score: 0.8828 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.1492,   Val_Loss: 0.3323,   Val_Precision: 0.9630,   Val_Recall: 0.8000,   Val_accuracy: 0.8739,   Val_Score: 0.8837,   Val_Loss: 0.3323,   Test_Precision: 0.9623,   Test_Recall: 0.7969,   Test_accuracy: 0.8718,   Test_Score: 0.8828,   Test_loss: 0.2854


[0.17451642197556794, 0.179863756056875, 0.17808882193639874, 0.1790466441307217, 0.19296179106459022, 0.19152409583330154, 0.19463782291859388, 0.18407362280413508, 0.18418230512179434, 0.19024873804301023, 0.1871804699767381, 0.19182427204214036, 0.18096355092711747, 0.1821223150473088, 0.20508531108498573, 0.200457549886778, 0.19364653807133436, 0.18512640404514968, 0.18503513000905514, 0.18114893999882042, 0.18109959503635764, 0.18492800113745034, 0.18176604295149446, 0.17883265181444585, 0.17943953606300056, 0.18103378103114665, 0.18358638999052346, 0.17957824980840087, 0.18200621288269758, 0.18440873199142516, 0.18024207511916757, 0.180279437918216, 0.1831976098474115, 0.18119415803812444, 0.17954375105910003, 0.18239440605975688, 0.18396882200613618, 0.1817703910637647, 0.1814649999141693, 0.18274153606034815, 0.18486682302318513, 0.1811650451272726, 0.1834986270405352, 0.18485054606571794, 0.17951377597637475, 0.17695045401342213, 0.18113895505666733, 0.18464089091867208, 0.18458618293516338, 0.17969229095615447, 0.1815526019781828, 0.185175052145496, 0.18093399098142982, 0.1815995890647173, 0.1786027168855071, 0.18295401404611766, 0.18190860096365213, 0.1779758450575173, 0.1832274969201535, 0.18556160503067076, 0.18255026102997363, 0.1960034449584782, 0.19705787696875632, 0.26706946501508355, 0.15796702587977052, 0.16594073898158967, 0.1710662660188973, 0.15598975913599133, 0.15528180892579257, 0.15718317101709545, 0.16549369716085494, 0.16487835394218564, 0.16156957298517227, 0.1584637959022075, 0.15782025386579335, 0.15647092508152127, 0.15722347097471356, 0.16168965701945126, 0.15951010305434465, 0.15554758091457188, 0.1560237759258598, 0.1555711671244353, 0.15448727109469473, 0.1557130308356136, 0.1607805211097002, 0.16012784512713552, 0.1565850479528308, 0.15318537899293005, 0.1522907610051334, 0.17425592499785125, 0.1771918369922787, 0.17551540001295507, 0.1579660198185593, 0.15618073800578713, 0.1548688360489905, 0.15559869189746678, 0.1593288208823651, 0.1617071230430156, 0.16774908010847867, 0.15940295602194965, 0.15551081509329379, 0.15678612212650478, 0.15988347702659667, 0.16105286800302565, 0.15943467197939754, 0.1561484111007303, 0.15534866391681135, 0.156112315133214, 0.15806486899964511, 0.16074619814753532, 0.15772494906559587, 0.1573774740099907, 0.15488479984924197, 0.15661081997677684, 0.16000869404524565, 0.1615475290454924, 0.16065761097706854, 0.15956430207006633, 0.1554167449939996, 0.1542647979222238, 0.15778423799201846, 0.15921690594404936, 0.15784626896493137, 0.15823598392307758, 0.15680100210011005, 0.1611438118852675, 0.15812596888281405, 0.15873547899536788, 0.1557157461065799, 0.15462416782975197, 0.15764277707785368, 0.15862553589977324, 0.15946911298669875, 0.17779123387299478, 0.17428476200439036, 0.19804076105356216, 0.19063124200329185, 0.18303817813284695, 0.1733324450906366, 0.17601669812574983, 0.17802416998893023, 0.1742531768977642, 0.1736551399808377, 0.17881784099154174, 0.17331322701647878, 0.17862471495755017, 0.1819610500242561, 0.18832326191477478, 0.20221220282837749, 0.19191297702491283, 0.19737695204094052, 0.1776494779624045, 0.18521018302999437, 0.196363037917763, 0.1847631549462676, 0.17881666007451713, 0.17912892205640674, 0.1739826190751046, 0.17893488495610654, 0.18270437000319362, 0.1774642770178616, 0.1757243589963764, 0.17935572611168027, 0.1836743929889053, 0.17904329299926758, 0.1907086989376694, 0.1859419089742005, 0.2083554759155959, 0.18464448885060847, 0.19486762792803347, 0.1816872649360448, 0.18114077695645392, 0.19019147008657455, 0.1961451480165124, 0.17880561598576605, 0.17767464509233832, 0.1810242289211601, 0.1790236399974674, 0.17587926308624446, 0.17544308700598776, 0.17592288507148623, 0.17892860318534076, 0.17291667987592518, 0.175782066071406, 0.18228113208897412, 0.1749225070234388, 0.1785372761078179, 0.18016889202408493, 0.1770443948917091, 0.1800916399806738, 0.17767974408343434, 0.17606095992960036]
[0.001352840480430759, 0.001394292682611434, 0.001380533503382936, 0.0013879584816335014, 0.0014958278377100018, 0.001484682913436446, 0.0015088203327022782, 0.0014269273085591866, 0.0014277698071456927, 0.001474796418938064, 0.0014510113951685124, 0.0014870098607917858, 0.0014028182242412208, 0.0014118008918396032, 0.001589808613061905, 0.001553934495246341, 0.0015011359540413516, 0.0014350884034507727, 0.001434380852783373, 0.0014042553488280653, 0.0014038728297392066, 0.0014335503964143436, 0.0014090390926472438, 0.0013862996264685725, 0.001391004155527136, 0.0014033626436522997, 0.0014231503100040578, 0.0013920794558790765, 0.001410900875059671, 0.0014295250541970941, 0.0013972253885206787, 0.0013975150226218293, 0.0014201365104450504, 0.0014046058762645305, 0.0013918120237139536, 0.0014139101244942393, 0.0014261148992723734, 0.0014090727989439123, 0.001406705425691235, 0.001416601054731381, 0.0014330761474665515, 0.001404380194785059, 0.0014224699770584125, 0.0014329499695016894, 0.001391579658731587, 0.0013717089458404816, 0.0014041779461757156, 0.0014313247358036596, 0.0014309006429082433, 0.0013929634957841432, 0.0014073845114587814, 0.001435465520507721, 0.001402589077375425, 0.0014077487524396688, 0.0013845171851589698, 0.001418248170900137, 0.0014101441935166833, 0.001379657713624165, 0.0014203681931794845, 0.0014384620545013238, 0.001415118302557935, 0.0015194065500657226, 0.001527580441618266, 0.002070305930349485, 0.0012245505882152753, 0.0012863623176867416, 0.0013260950854178085, 0.0012092229390386925, 0.0012037349529131207, 0.0012184741939309724, 0.0012828968772159298, 0.0012781267747456252, 0.0012524773099625757, 0.0012284015186217634, 0.0012234128206650647, 0.001212952907608692, 0.0012187865967032059, 0.0012534081939492346, 0.0012365124267778656, 0.0012057952008881542, 0.0012094866350841845, 0.0012059780397243047, 0.0011975757449201143, 0.0012070777584156094, 0.001246360628757366, 0.0012413011250165544, 0.001213837581029696, 0.001187483558084729, 0.0011805485349235146, 0.0013508211240143508, 0.0013735801317230907, 0.001360584496224458, 0.0012245427892911573, 0.0012107033953936987, 0.001200533612782872, 0.001206191410057882, 0.0012351071386229853, 0.001253543589480741, 0.001300380465957199, 0.0012356818296275166, 0.0012055101945216572, 0.0012153962955543005, 0.0012394067986557882, 0.001248471844984695, 0.0012359276897627716, 0.0012104527992304675, 0.0012042532086574524, 0.0012101729855287908, 0.0012253090620127527, 0.0012460945592832197, 0.001222674023764309, 0.0012199804186820984, 0.0012006573631724183, 0.0012140373641610607, 0.0012403774732189586, 0.0012523064267092436, 0.0012454078370315392, 0.0012369325741865608, 0.0012047809689457332, 0.0011958511466839054, 0.0012231336278451044, 0.001234239580961623, 0.0012236144881002432, 0.0012266355342874231, 0.0012155116441868996, 0.0012491768363199031, 0.0012257827045179385, 0.0012305075891113789, 0.0012070988070277512, 0.0011986369599205579, 0.0012220370316112688, 0.0012296553170525056, 0.0012361946743154942, 0.0013889940146327717, 0.0013615997031592997, 0.0015471934457309544, 0.0014893065781507175, 0.0014299857666628668, 0.0013541597272705985, 0.0013751304541074205, 0.0013908138280385174, 0.0013613529445137829, 0.0013566807811002946, 0.0013970143827464199, 0.0013540095860662404, 0.0013955055856058607, 0.0014215707033145009, 0.001471275483709178, 0.001579782834596699, 0.0014993201330071315, 0.0015420074378198478, 0.001387886546581285, 0.001446954554921831, 0.0015340862337325234, 0.0014434621480177157, 0.001397005156832165, 0.0013994447035656776, 0.0013592392115242546, 0.0013979287887195824, 0.0014273778906499501, 0.0013864396642020438, 0.0013728465546591906, 0.001401216610247502, 0.0014349561952258227, 0.001398775726556778, 0.0014899117104505422, 0.0014526711638609413, 0.0016277771555905929, 0.0014425350691453787, 0.0015224033431877615, 0.00141943175731285, 0.0014151623199722962, 0.0014858708600513637, 0.001532383968879003, 0.0013969188748887973, 0.0013880831647838932, 0.0014142517884465633, 0.001398622187480214, 0.0013740567428612849, 0.0013706491172342794, 0.0013743975396209862, 0.0013978797123854747, 0.0013509115615306655, 0.0013732973911828594, 0.0014240713444451103, 0.0013665820861206157, 0.0013948224695923273, 0.0014075694689381635, 0.0013831593350914773, 0.001406965937349014, 0.0013881230006518308, 0.0013754762494500028]
[739.1854505134183, 717.2095303025292, 724.3576469166046, 720.4826464427744, 668.5261330146949, 673.5444928677738, 662.7694353833452, 700.8065470480983, 700.3930150331005, 678.0596882111065, 689.1744636394573, 672.4904967795786, 712.8507334162247, 708.3151779972183, 629.0065305873779, 643.5277697091555, 666.1621802527642, 696.8211836953238, 697.1649112992061, 712.1211970704328, 712.3152317049737, 697.5687792359738, 709.7035172539051, 721.3447806715326, 718.9051132784281, 712.5741906578513, 702.6664667607378, 718.349800923192, 708.7670138114465, 699.5330351601701, 715.704143523155, 715.5558142938132, 704.1576585384839, 711.9434831494818, 718.4878295070116, 707.2585326862296, 701.2057727678296, 709.6865405034368, 710.8808864575284, 705.9150469075587, 697.7996261872333, 712.057891241517, 703.0025351170809, 697.8610707167617, 718.6078020941261, 729.0176265397715, 712.160451403972, 698.6534746348252, 698.8605428029884, 717.8939024795251, 710.5378749432738, 696.638118933224, 712.9671948331695, 710.3540303388452, 722.2734471765913, 705.0952157162436, 709.1473372706329, 724.8174602475435, 704.0428001710646, 695.186916381102, 706.6547003119267, 658.151697422092, 654.6300101490135, 483.02040067633465, 816.6261235948225, 777.3859559243733, 754.0937380707759, 826.9773651457352, 830.7476638274329, 820.6985465764003, 779.4858789976506, 782.395001621824, 798.4176575860525, 814.0660727300105, 817.385581635793, 824.4343154026287, 820.4881828410163, 797.8246869834185, 808.7262031048281, 829.3282302528893, 826.7970649633482, 829.2024954522449, 835.0202517392386, 828.4470433061279, 802.3359988489127, 805.6062947551616, 823.8334482540093, 842.1169229600804, 847.06386092359, 740.2904664595519, 728.0245082939193, 734.9782411713064, 816.6313245606249, 825.9661315931291, 832.9629336091397, 829.0558129177961, 809.6463608127915, 797.7385137554194, 769.0057073134427, 809.2698104183012, 829.5242997897641, 822.7769030215237, 806.8375944722594, 800.9792163252661, 809.1088243131308, 826.137128713932, 830.3901478617096, 826.3281464368876, 816.1206270337632, 802.5073157973031, 817.8794842808951, 819.6852873099918, 832.877081066463, 823.6978774463275, 806.2061925430295, 798.5266055271765, 802.9498211473642, 808.4515040422687, 830.0263913324171, 836.2244772461852, 817.5721582945787, 810.2154682325762, 817.2508659590799, 815.2380817672299, 822.6988238100645, 800.5271719142804, 815.8052779780968, 812.6727611019109, 828.4325973797521, 834.2809653277144, 818.30580754291, 813.236023243496, 808.9340787313455, 719.9455069389801, 734.430242368381, 646.3315901183649, 671.453423137167, 699.3076597773996, 738.4653227101712, 727.2037332989524, 719.0034926603461, 734.5633650920389, 737.0930685617736, 715.8122438468235, 738.5472084472217, 716.5861679914728, 703.4472486443506, 679.6823647730045, 632.9983957923488, 666.9689667905257, 648.5053025514846, 720.5199895216597, 691.1067086374549, 651.8538384683502, 692.7788174932641, 715.8169711181239, 714.5691412115655, 735.7056738221944, 715.344020431784, 700.5853226048321, 721.2719210363499, 728.4135263377998, 713.6655337131397, 696.8853846041116, 714.9108902980444, 671.1807102298732, 688.3870382214913, 614.3347058075516, 693.2240479896576, 656.8561508188095, 704.50727542768, 706.6327204214824, 673.0059972812388, 652.5779571627455, 715.8611841933954, 720.417929826047, 707.0876686664232, 714.9893723633973, 727.7719826312536, 729.5813256844451, 727.5915236837278, 715.369134511227, 740.240907307758, 728.1743971993365, 702.2120091810781, 731.752603927913, 716.9371169452667, 710.4445088272453, 722.9825043503491, 710.7492608415143, 720.3972555244909, 727.020913956064]
Elapsed: 0.1748558441977366~0.014640312640710739
Time per graph: 0.0013588581796940245~0.00011531424787141262
Speed: 740.9607585673812~60.29608466163544
Total Time: 0.1769
best val loss: 0.3323149271598158 test_score: 0.8828

Testing...
Test loss: 0.2539 score: 0.9062 time: 0.17s
test Score 0.9062
Epoch Time List: [0.588912706123665, 0.5955892235506326, 0.5965950670652092, 0.5970035803038627, 0.6358401698525995, 0.6369635872542858, 0.6396311521530151, 0.6189721378032118, 0.6084533839020878, 0.6168984577525407, 0.6557779179420322, 0.6529392627999187, 0.6278037161100656, 0.6125560810323805, 0.6481367601081729, 0.6388881087768823, 0.64777901628986, 0.6102613287512213, 0.6138935112394392, 0.6053586821071804, 0.6072488988284022, 0.6108133578673005, 0.6046584260184318, 0.6022037158254534, 0.6084476809483021, 0.6017999220639467, 0.6057962798513472, 0.605717534897849, 0.60483343526721, 0.615804738830775, 0.6090344318654388, 0.6024090691935271, 0.609383292729035, 0.6065553987864405, 0.6060683999676257, 0.6081097330898046, 0.6092247099149972, 0.6073566928971559, 0.6081551900133491, 0.6092559020034969, 0.6098223552107811, 0.6088505340740085, 0.612507738173008, 0.6094164859969169, 0.6051619409117848, 0.6002596127800643, 0.6034225968178362, 0.6101926048286259, 0.6092910980805755, 0.6077058869414032, 0.6082843742333353, 0.6116961138322949, 0.6060195751488209, 0.6060581589117646, 0.6006856446620077, 0.6059450032189488, 0.6008624457754195, 0.6030993303284049, 0.6087455695960671, 0.6123132421635091, 0.6060293931514025, 0.7288052248768508, 0.6438003210350871, 0.7164462979417294, 0.5563340708613396, 0.5427145569119602, 0.5839294260367751, 0.6287456799764186, 0.5641445126384497, 0.5590395268518478, 0.5747090289369226, 0.5978401789907366, 0.565180208068341, 0.5558299680706114, 0.5591249049175531, 0.5411375318653882, 0.5456883709412068, 0.5608925498090684, 0.556386805139482, 0.553242176072672, 0.538220820017159, 0.5356370320077986, 0.5548570167738944, 0.5556859709322453, 0.5597795457579195, 0.5447920758742839, 0.5547930302564055, 0.552608918165788, 0.5348728951066732, 0.5934050851501524, 0.5927790247369558, 0.5915535059757531, 0.5577225591987371, 0.5411263599526137, 0.5441223138477653, 0.5404210400301963, 0.5469326977618039, 0.5438895020633936, 0.5726743801496923, 0.5762496141251177, 0.5619000380393118, 0.552438922226429, 0.5488932901062071, 0.5700034371111542, 0.5488840297330171, 0.566444902215153, 0.563719434896484, 0.5638873078860343, 0.5482067910488695, 0.5668409448117018, 0.5624394190963358, 0.5652089850045741, 0.5618555922992527, 0.5631499157752842, 0.5671647402923554, 0.5500542137306184, 0.5478356981184334, 0.567951092030853, 0.5565533440094441, 0.5456446660682559, 0.5471822412218899, 0.5600385912694037, 0.5605959598906338, 0.547013231087476, 0.5644367809873074, 0.5553689538501203, 0.5662452802062035, 0.5486398090142757, 0.56211356096901, 0.54408247792162, 0.5619007498025894, 0.5451739409472793, 0.5568194051738828, 0.6095858828630298, 0.599761649267748, 0.6320492797531188, 0.662002480821684, 0.6193663431331515, 0.6044261301867664, 0.6107599777169526, 0.6132532840128988, 0.6069790630135685, 0.6121467880439013, 0.6080418487545103, 0.6108769397251308, 0.6163521618582308, 0.6162962522357702, 0.6604844192042947, 0.746453191852197, 0.6942231717985123, 0.7415569599252194, 0.6211062741931528, 0.6471006381325424, 0.7296166059095412, 0.6474900359753519, 0.7162995550315827, 0.6202394801657647, 0.6189134621527046, 0.6190185649320483, 0.6233266261406243, 0.6238616821356118, 0.619010629830882, 0.6245157471857965, 0.6286868639290333, 0.6239364380016923, 0.637369860894978, 0.6503426649142057, 0.6731095269788057, 0.6559045021422207, 0.6555658902507275, 0.7052753448951989, 0.6361990822479129, 0.6524823282379657, 0.6698015562724322, 0.6260854681022465, 0.6239035308826715, 0.6270267199724913, 0.6177026142831892, 0.6125218118540943, 0.619250098709017, 0.6147088787984103, 0.6184345139190555, 0.6152559919282794, 0.6178691650275141, 0.6271078810095787, 0.6208949061110616, 0.6228323790710419, 0.6231331168673933, 0.6236125540453941, 0.6268392659258097, 0.6247068101074547, 0.619008210953325]
Total Epoch List: [64, 69, 59]
Total Time List: [0.26753174886107445, 0.1601415821351111, 0.17687923298217356]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef811ce20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 3/1000, LR 0.000045
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.18s
Epoch 14/1000, LR 0.000285
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.26s
Val loss: 0.6905 score: 0.5039 time: 0.17s
Test loss: 0.6907 score: 0.5271 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.26s
Val loss: 0.6895 score: 0.5814 time: 0.17s
Test loss: 0.6898 score: 0.5736 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.26s
Val loss: 0.6883 score: 0.6512 time: 0.18s
Test loss: 0.6887 score: 0.5969 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.26s
Val loss: 0.6867 score: 0.6744 time: 0.17s
Test loss: 0.6873 score: 0.6357 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.26s
Val loss: 0.6847 score: 0.6977 time: 0.17s
Test loss: 0.6855 score: 0.6899 time: 0.18s
Epoch 21/1000, LR 0.000285
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.26s
Val loss: 0.6822 score: 0.7054 time: 0.17s
Test loss: 0.6832 score: 0.7054 time: 0.18s
Epoch 22/1000, LR 0.000285
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.26s
Val loss: 0.6790 score: 0.7674 time: 0.17s
Test loss: 0.6803 score: 0.7597 time: 0.18s
Epoch 23/1000, LR 0.000285
Train loss: 0.4847;  Loss pred: 0.4847; Loss self: 0.0000; time: 0.26s
Val loss: 0.6751 score: 0.7752 time: 0.17s
Test loss: 0.6768 score: 0.7597 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.4570;  Loss pred: 0.4570; Loss self: 0.0000; time: 0.26s
Val loss: 0.6703 score: 0.7907 time: 0.17s
Test loss: 0.6723 score: 0.7752 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.4324;  Loss pred: 0.4324; Loss self: 0.0000; time: 0.26s
Val loss: 0.6638 score: 0.8217 time: 0.18s
Test loss: 0.6665 score: 0.8062 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3968;  Loss pred: 0.3968; Loss self: 0.0000; time: 0.26s
Val loss: 0.6560 score: 0.8062 time: 0.17s
Test loss: 0.6594 score: 0.7752 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.3651;  Loss pred: 0.3651; Loss self: 0.0000; time: 0.26s
Val loss: 0.6462 score: 0.7907 time: 0.17s
Test loss: 0.6504 score: 0.7907 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.3327;  Loss pred: 0.3327; Loss self: 0.0000; time: 0.26s
Val loss: 0.6334 score: 0.8217 time: 0.17s
Test loss: 0.6386 score: 0.8140 time: 0.18s
Epoch 29/1000, LR 0.000285
Train loss: 0.3057;  Loss pred: 0.3057; Loss self: 0.0000; time: 0.26s
Val loss: 0.6180 score: 0.8450 time: 0.18s
Test loss: 0.6243 score: 0.8372 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.26s
Val loss: 0.5994 score: 0.8605 time: 0.17s
Test loss: 0.6070 score: 0.8372 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.2408;  Loss pred: 0.2408; Loss self: 0.0000; time: 0.26s
Val loss: 0.5774 score: 0.8837 time: 0.18s
Test loss: 0.5867 score: 0.8682 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.2084;  Loss pred: 0.2084; Loss self: 0.0000; time: 0.26s
Val loss: 0.5515 score: 0.8915 time: 0.17s
Test loss: 0.5625 score: 0.8915 time: 0.18s
Epoch 33/1000, LR 0.000285
Train loss: 0.1866;  Loss pred: 0.1866; Loss self: 0.0000; time: 0.26s
Val loss: 0.5218 score: 0.9147 time: 0.18s
Test loss: 0.5346 score: 0.9147 time: 0.18s
Epoch 34/1000, LR 0.000285
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 0.26s
Val loss: 0.4877 score: 0.9302 time: 0.17s
Test loss: 0.5025 score: 0.9147 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.28s
Val loss: 0.4518 score: 0.9535 time: 0.19s
Test loss: 0.4687 score: 0.9380 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1234;  Loss pred: 0.1234; Loss self: 0.0000; time: 0.26s
Val loss: 0.4135 score: 0.9535 time: 0.18s
Test loss: 0.4327 score: 0.9380 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.1115;  Loss pred: 0.1115; Loss self: 0.0000; time: 0.26s
Val loss: 0.3756 score: 0.9535 time: 0.18s
Test loss: 0.3971 score: 0.9612 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.26s
Val loss: 0.3396 score: 0.9535 time: 0.17s
Test loss: 0.3629 score: 0.9612 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 0.0901;  Loss pred: 0.0901; Loss self: 0.0000; time: 0.25s
Val loss: 0.3047 score: 0.9457 time: 0.17s
Test loss: 0.3295 score: 0.9612 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.26s
Val loss: 0.2715 score: 0.9380 time: 0.18s
Test loss: 0.2980 score: 0.9612 time: 0.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.26s
Val loss: 0.2404 score: 0.9380 time: 0.17s
Test loss: 0.2693 score: 0.9535 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.26s
Val loss: 0.2146 score: 0.9380 time: 0.17s
Test loss: 0.2453 score: 0.9380 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.26s
Val loss: 0.1956 score: 0.9380 time: 0.17s
Test loss: 0.2265 score: 0.9457 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.26s
Val loss: 0.1789 score: 0.9380 time: 0.26s
Test loss: 0.2101 score: 0.9457 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.25s
Val loss: 0.1630 score: 0.9380 time: 0.18s
Test loss: 0.1963 score: 0.9457 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.26s
Val loss: 0.1508 score: 0.9380 time: 0.17s
Test loss: 0.1851 score: 0.9380 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.34s
Val loss: 0.1409 score: 0.9380 time: 0.18s
Test loss: 0.1750 score: 0.9380 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.26s
Val loss: 0.1340 score: 0.9380 time: 0.18s
Test loss: 0.1688 score: 0.9380 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.26s
Val loss: 0.1287 score: 0.9380 time: 0.18s
Test loss: 0.1645 score: 0.9380 time: 0.28s
Epoch 50/1000, LR 0.000284
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.26s
Val loss: 0.1261 score: 0.9380 time: 0.19s
Test loss: 0.1615 score: 0.9380 time: 0.19s
Epoch 51/1000, LR 0.000284
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.26s
Val loss: 0.1242 score: 0.9380 time: 0.18s
Test loss: 0.1599 score: 0.9380 time: 0.19s
Epoch 52/1000, LR 0.000284
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.30s
Val loss: 0.1241 score: 0.9380 time: 0.19s
Test loss: 0.1603 score: 0.9380 time: 0.20s
Epoch 53/1000, LR 0.000284
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.27s
Val loss: 0.1246 score: 0.9380 time: 0.18s
Test loss: 0.1625 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.25s
Val loss: 0.1257 score: 0.9380 time: 0.19s
Test loss: 0.1671 score: 0.9380 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.25s
Val loss: 0.1275 score: 0.9380 time: 0.18s
Test loss: 0.1733 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.25s
Val loss: 0.1314 score: 0.9380 time: 0.18s
Test loss: 0.1798 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.27s
Val loss: 0.1333 score: 0.9380 time: 0.17s
Test loss: 0.1840 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.25s
Val loss: 0.1350 score: 0.9302 time: 0.18s
Test loss: 0.1873 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.25s
Val loss: 0.1365 score: 0.9302 time: 0.18s
Test loss: 0.1907 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.25s
Val loss: 0.1389 score: 0.9380 time: 0.18s
Test loss: 0.1945 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.26s
Val loss: 0.1400 score: 0.9380 time: 0.17s
Test loss: 0.1979 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.26s
Val loss: 0.1408 score: 0.9380 time: 0.18s
Test loss: 0.2004 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.25s
Val loss: 0.1412 score: 0.9380 time: 0.18s
Test loss: 0.2030 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.25s
Val loss: 0.1409 score: 0.9302 time: 0.18s
Test loss: 0.2060 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.26s
Val loss: 0.1404 score: 0.9380 time: 0.18s
Test loss: 0.2075 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.26s
Val loss: 0.1408 score: 0.9457 time: 0.18s
Test loss: 0.2108 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.26s
Val loss: 0.1426 score: 0.9380 time: 0.18s
Test loss: 0.2157 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.26s
Val loss: 0.1453 score: 0.9380 time: 0.17s
Test loss: 0.2233 score: 0.9457 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.26s
Val loss: 0.1497 score: 0.9457 time: 0.18s
Test loss: 0.2361 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.26s
Val loss: 0.1562 score: 0.9457 time: 0.18s
Test loss: 0.2518 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.26s
Val loss: 0.1612 score: 0.9457 time: 0.18s
Test loss: 0.2621 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.26s
Val loss: 0.1643 score: 0.9457 time: 0.33s
Test loss: 0.2669 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.0261,   Val_Loss: 0.1241,   Val_Precision: 0.9375,   Val_Recall: 0.9375,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1241,   Test_Precision: 0.9672,   Test_Recall: 0.9077,   Test_accuracy: 0.9365,   Test_Score: 0.9380,   Test_loss: 0.1603


[0.18450635997578502, 0.17939962400123477, 0.1795061961747706, 0.1827832970302552, 0.18488640198484063, 0.1830220241099596, 0.18262070487253368, 0.18395877303555608, 0.1834778089541942, 0.18098828895017505, 0.18002124316990376, 0.18043924984522164, 0.18803949002176523, 0.18340450106188655, 0.18053008196875453, 0.18219820107333362, 0.18366206996142864, 0.18051027203910053, 0.18216186808422208, 0.1832731121685356, 0.17969010304659605, 0.1802746111061424, 0.18517198809422553, 0.18285745405592024, 0.182247051037848, 0.17990595195442438, 0.18412649794481695, 0.18326487392187119, 0.18137691193260252, 0.18230848107486963, 0.18205580208450556, 0.1839195501524955, 0.18134915293194354, 0.1800272420514375, 0.18376755388453603, 0.18280128599144518, 0.1793301801662892, 0.1855954509228468, 0.18263004394248128, 0.21305262204259634, 0.17939424817450345, 0.17914125206880271, 0.18478482798673213, 0.18303436506539583, 0.17796863708645105, 0.18725996697321534, 0.1849202960729599, 0.18499023700132966, 0.2863333038985729, 0.1911860469263047, 0.19705500407144427, 0.20509080588817596, 0.20832051197066903, 0.20592051814310253, 0.1822737290058285, 0.18352568382397294, 0.18568614195100963, 0.1853063760790974, 0.1847513869870454, 0.1831509410403669, 0.18558786110952497, 0.18875489104539156, 0.18328541796654463, 0.18065477604977787, 0.18247709283605218, 0.18519145413301885, 0.18255652906373143, 0.1818565430585295, 0.18436914985068142, 0.1840295661240816, 0.1868585478514433, 0.1802611551247537]
[0.0014302818602774032, 0.0013906947596994943, 0.001391520900579617, 0.0014169247831802728, 0.0014332279223631056, 0.0014187753806973612, 0.0014156643788568503, 0.0014260370002756285, 0.001422308596544141, 0.001403009991861822, 0.0013955135129449904, 0.00139875387476916, 0.0014576704652850018, 0.0014217403183091981, 0.001399457999757787, 0.0014123891556072374, 0.0014237369764451833, 0.001399304434411632, 0.0014121075045288533, 0.00142072179975609, 0.0013929465352449306, 0.001397477605473972, 0.001435441768172291, 0.0014174996438443429, 0.0014127678375026977, 0.001394619782592437, 0.001427337193370674, 0.0014206579373788465, 0.0014060225731209498, 0.0014132440393400747, 0.0014112852874767874, 0.00142573294691857, 0.0014058073870693297, 0.0013955600159026164, 0.001424554681275473, 0.001417064232491823, 0.0013901564353975908, 0.0014387244257585022, 0.001415736774747917, 0.0016515707135084988, 0.0013906530866240577, 0.0013886918765023467, 0.0014324405270289312, 0.0014188710470185723, 0.0013796018378794655, 0.0014516276509551577, 0.0014334906672322473, 0.0014340328449715478, 0.0022196380147176194, 0.0014820623792736799, 0.0015275581710964673, 0.0015898512084354726, 0.0016148876896951087, 0.0015962830863806398, 0.001412974643456035, 0.0014226797195656816, 0.0014394274569845707, 0.001436483535496879, 0.0014321812944732201, 0.00141977473674703, 0.0014386655899963176, 0.0014632162096541981, 0.0014208171935391057, 0.0014004246205409137, 0.0014145511072562184, 0.0014355926676978205, 0.0014151668919669103, 0.0014097406438645698, 0.0014292182158967552, 0.0014265857839076094, 0.0014485158748173898, 0.0013973732955407264]
[699.16289073683, 719.065052216119, 718.6381459189474, 705.7537646815031, 697.7257311253052, 704.8332058796203, 706.382116365392, 701.2440769816752, 703.0823004443292, 712.7532988364396, 716.5820973597545, 714.9220588683143, 686.0261107125343, 703.3633267074033, 714.5623521199466, 708.0201628778888, 702.3769253340748, 714.6407710917258, 708.1613806263623, 703.8675694085081, 717.9026435671227, 715.5749731394353, 696.6496462432418, 705.4675493871313, 707.8303833471085, 717.0413129671197, 700.6052982046155, 703.8992101399356, 711.2261347129719, 707.5918752623629, 708.5739565725105, 701.3936250553061, 711.3350016496131, 716.5582193562797, 701.973755829893, 705.6843134355029, 719.3435030310064, 695.0601394515112, 706.345994422627, 605.4842168251212, 719.0866001150545, 720.1021457104421, 698.1092625703146, 704.7856830268455, 724.8468163372866, 688.8818901610266, 697.5978447985143, 697.3340976857756, 450.52391127263115, 674.735432182061, 654.639554107592, 628.9896782127628, 619.2381094866109, 626.4553001481506, 707.7267837971045, 702.8988930166811, 694.7206649058098, 696.1444216303533, 698.2356241203502, 704.3370853965097, 695.0885646764928, 683.4259991121407, 703.8203116821142, 714.0691368406178, 706.9380490180264, 696.5764192733333, 706.6304374956944, 709.350336426895, 699.6832176341633, 700.974320142786, 690.3617815897717, 715.6283887714061]
Elapsed: 0.18598777279516476~0.013538356873951347
Time per graph: 0.0014417656805826723~0.00010494850289884763
Speed: 696.2311368366446~36.68917510247903
Total Time: 0.1808
best val loss: 0.12407751270938058 test_score: 0.9380

Testing...
Test loss: 0.4687 score: 0.9380 time: 0.18s
test Score 0.9380
Epoch Time List: [0.6070799229200929, 0.6081994269043207, 0.6046663529705256, 0.6083417749032378, 0.610097715863958, 0.6099450089968741, 0.6077162793371826, 0.6072495749685913, 0.6018266561441123, 0.6011047151405364, 0.6071971550118178, 0.6071761879138649, 0.6099002480041236, 0.606961689889431, 0.6068990859203041, 0.6105907957535237, 0.6137319980189204, 0.6089680190198123, 0.609665225027129, 0.6108894369099289, 0.6070711950305849, 0.6096108320634812, 0.6109745728317648, 0.61077816109173, 0.6123034399934113, 0.6087668219115585, 0.6118094208650291, 0.6093572438694537, 0.6112055070698261, 0.611761957174167, 0.614716200158, 0.615535870892927, 0.6144425203092396, 0.6055479638744146, 0.643765069078654, 0.6099897660315037, 0.6098469221033156, 0.6165041069034487, 0.6042886099312454, 0.6471237998921424, 0.6120979278348386, 0.6096078110858798, 0.6164852250367403, 0.693265455076471, 0.6025981183629483, 0.6145459490362555, 0.701060833176598, 0.6160020036622882, 0.7210256110411137, 0.6303398290183395, 0.6398461889475584, 0.687737017404288, 0.6458690899889916, 0.6394637608900666, 0.6049849370028824, 0.6066462630406022, 0.6227783598005772, 0.6084233080036938, 0.6102943120058626, 0.610499300993979, 0.6137616771738976, 0.6212786599062383, 0.6143931159749627, 0.6077606801409274, 0.6136672997381538, 0.6141655682586133, 0.6107872689608485, 0.6143119849730283, 0.6231962288729846, 0.6126815220341086, 0.6170713065657765, 0.7588496841490269]
Total Epoch List: [72]
Total Time List: [0.18077090312726796]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798ef5774220>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.18s
Epoch 3/1000, LR 0.000045
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.19s
Epoch 10/1000, LR 0.000255
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4961 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4961 time: 0.18s
Epoch 17/1000, LR 0.000285
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4961 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4961 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.4841;  Loss pred: 0.4841; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.4961 time: 0.19s
Epoch 21/1000, LR 0.000285
Train loss: 0.4563;  Loss pred: 0.4563; Loss self: 0.0000; time: 0.27s
Val loss: 0.6778 score: 0.5116 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4231;  Loss pred: 0.4231; Loss self: 0.0000; time: 0.26s
Val loss: 0.6733 score: 0.5426 time: 0.18s
Test loss: 0.6765 score: 0.5194 time: 0.20s
Epoch 23/1000, LR 0.000285
Train loss: 0.3925;  Loss pred: 0.3925; Loss self: 0.0000; time: 0.28s
Val loss: 0.6677 score: 0.5891 time: 0.18s
Test loss: 0.6715 score: 0.5426 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3632;  Loss pred: 0.3632; Loss self: 0.0000; time: 0.27s
Val loss: 0.6606 score: 0.6202 time: 0.19s
Test loss: 0.6648 score: 0.5814 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.28s
Val loss: 0.6521 score: 0.7132 time: 0.19s
Test loss: 0.6569 score: 0.6977 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2984;  Loss pred: 0.2984; Loss self: 0.0000; time: 0.27s
Val loss: 0.6417 score: 0.8372 time: 0.18s
Test loss: 0.6471 score: 0.8372 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2640;  Loss pred: 0.2640; Loss self: 0.0000; time: 0.27s
Val loss: 0.6297 score: 0.9225 time: 0.18s
Test loss: 0.6356 score: 0.9302 time: 0.18s
Epoch 28/1000, LR 0.000285
Train loss: 0.2377;  Loss pred: 0.2377; Loss self: 0.0000; time: 0.26s
Val loss: 0.6156 score: 0.9225 time: 0.18s
Test loss: 0.6222 score: 0.9302 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 0.27s
Val loss: 0.5998 score: 0.9302 time: 0.18s
Test loss: 0.6068 score: 0.9147 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.1872;  Loss pred: 0.1872; Loss self: 0.0000; time: 0.27s
Val loss: 0.5818 score: 0.9147 time: 0.18s
Test loss: 0.5893 score: 0.8992 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.26s
Val loss: 0.5610 score: 0.9070 time: 0.20s
Test loss: 0.5691 score: 0.8992 time: 0.19s
Epoch 32/1000, LR 0.000285
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.26s
Val loss: 0.5390 score: 0.9070 time: 0.20s
Test loss: 0.5475 score: 0.8915 time: 0.19s
Epoch 33/1000, LR 0.000285
Train loss: 0.1414;  Loss pred: 0.1414; Loss self: 0.0000; time: 0.27s
Val loss: 0.5150 score: 0.9070 time: 0.19s
Test loss: 0.5238 score: 0.8915 time: 0.19s
Epoch 34/1000, LR 0.000285
Train loss: 0.1088;  Loss pred: 0.1088; Loss self: 0.0000; time: 0.26s
Val loss: 0.4911 score: 0.9147 time: 0.18s
Test loss: 0.4999 score: 0.8837 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.26s
Val loss: 0.4663 score: 0.9147 time: 0.18s
Test loss: 0.4751 score: 0.8837 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.27s
Val loss: 0.4398 score: 0.9147 time: 0.18s
Test loss: 0.4486 score: 0.8915 time: 0.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.26s
Val loss: 0.4124 score: 0.9147 time: 0.18s
Test loss: 0.4210 score: 0.8915 time: 0.18s
Epoch 38/1000, LR 0.000284
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.26s
Val loss: 0.3846 score: 0.9147 time: 0.18s
Test loss: 0.3926 score: 0.8837 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.27s
Val loss: 0.3579 score: 0.9225 time: 0.18s
Test loss: 0.3648 score: 0.8915 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.26s
Val loss: 0.3339 score: 0.9225 time: 0.18s
Test loss: 0.3386 score: 0.8915 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.26s
Val loss: 0.3136 score: 0.9225 time: 0.18s
Test loss: 0.3150 score: 0.8915 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.27s
Val loss: 0.2964 score: 0.9147 time: 0.18s
Test loss: 0.2939 score: 0.8992 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.27s
Val loss: 0.2810 score: 0.9070 time: 0.18s
Test loss: 0.2746 score: 0.8992 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.26s
Val loss: 0.2692 score: 0.9070 time: 0.18s
Test loss: 0.2589 score: 0.9147 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.26s
Val loss: 0.2594 score: 0.9070 time: 0.18s
Test loss: 0.2454 score: 0.9147 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.26s
Val loss: 0.2513 score: 0.9147 time: 0.18s
Test loss: 0.2344 score: 0.9070 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.26s
Val loss: 0.2481 score: 0.9147 time: 0.18s
Test loss: 0.2280 score: 0.9070 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.27s
Val loss: 0.2475 score: 0.9147 time: 0.18s
Test loss: 0.2246 score: 0.9070 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.26s
Val loss: 0.2508 score: 0.9147 time: 0.18s
Test loss: 0.2244 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.26s
Val loss: 0.2548 score: 0.9147 time: 0.18s
Test loss: 0.2255 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.27s
Val loss: 0.2612 score: 0.9147 time: 0.18s
Test loss: 0.2284 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.27s
Val loss: 0.2687 score: 0.9147 time: 0.18s
Test loss: 0.2325 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.26s
Val loss: 0.2804 score: 0.9147 time: 0.18s
Test loss: 0.2374 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.27s
Val loss: 0.2924 score: 0.9147 time: 0.18s
Test loss: 0.2426 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.26s
Val loss: 0.3036 score: 0.9147 time: 0.18s
Test loss: 0.2471 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.26s
Val loss: 0.3114 score: 0.9147 time: 0.18s
Test loss: 0.2508 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.27s
Val loss: 0.3201 score: 0.9147 time: 0.18s
Test loss: 0.2553 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.27s
Val loss: 0.3276 score: 0.9147 time: 0.18s
Test loss: 0.2595 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.33s
Val loss: 0.3377 score: 0.9147 time: 0.18s
Test loss: 0.2641 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.26s
Val loss: 0.3482 score: 0.9147 time: 0.18s
Test loss: 0.2688 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.27s
Val loss: 0.3565 score: 0.9147 time: 0.18s
Test loss: 0.2726 score: 0.9147 time: 0.28s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.27s
Val loss: 0.3652 score: 0.9147 time: 0.19s
Test loss: 0.2770 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.28s
Val loss: 0.3757 score: 0.9147 time: 0.19s
Test loss: 0.2818 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.35s
Val loss: 0.3894 score: 0.9147 time: 0.19s
Test loss: 0.2869 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.26s
Val loss: 0.3998 score: 0.9147 time: 0.19s
Test loss: 0.2908 score: 0.9147 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.27s
Val loss: 0.4093 score: 0.9147 time: 0.27s
Test loss: 0.2951 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.26s
Val loss: 0.4147 score: 0.9147 time: 0.18s
Test loss: 0.2983 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.26s
Val loss: 0.4210 score: 0.9147 time: 0.18s
Test loss: 0.3024 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.0244,   Val_Loss: 0.2475,   Val_Precision: 0.9655,   Val_Recall: 0.8615,   Val_accuracy: 0.9106,   Val_Score: 0.9147,   Val_Loss: 0.2475,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9070,   Test_loss: 0.2246


[0.18450635997578502, 0.17939962400123477, 0.1795061961747706, 0.1827832970302552, 0.18488640198484063, 0.1830220241099596, 0.18262070487253368, 0.18395877303555608, 0.1834778089541942, 0.18098828895017505, 0.18002124316990376, 0.18043924984522164, 0.18803949002176523, 0.18340450106188655, 0.18053008196875453, 0.18219820107333362, 0.18366206996142864, 0.18051027203910053, 0.18216186808422208, 0.1832731121685356, 0.17969010304659605, 0.1802746111061424, 0.18517198809422553, 0.18285745405592024, 0.182247051037848, 0.17990595195442438, 0.18412649794481695, 0.18326487392187119, 0.18137691193260252, 0.18230848107486963, 0.18205580208450556, 0.1839195501524955, 0.18134915293194354, 0.1800272420514375, 0.18376755388453603, 0.18280128599144518, 0.1793301801662892, 0.1855954509228468, 0.18263004394248128, 0.21305262204259634, 0.17939424817450345, 0.17914125206880271, 0.18478482798673213, 0.18303436506539583, 0.17796863708645105, 0.18725996697321534, 0.1849202960729599, 0.18499023700132966, 0.2863333038985729, 0.1911860469263047, 0.19705500407144427, 0.20509080588817596, 0.20832051197066903, 0.20592051814310253, 0.1822737290058285, 0.18352568382397294, 0.18568614195100963, 0.1853063760790974, 0.1847513869870454, 0.1831509410403669, 0.18558786110952497, 0.18875489104539156, 0.18328541796654463, 0.18065477604977787, 0.18247709283605218, 0.18519145413301885, 0.18255652906373143, 0.1818565430585295, 0.18436914985068142, 0.1840295661240816, 0.1868585478514433, 0.1802611551247537, 0.1823274048510939, 0.18066206388175488, 0.17738885316066444, 0.18418748187832534, 0.1807165949139744, 0.17296003713272512, 0.17710208497010171, 0.18026979500427842, 0.1945194979198277, 0.1781005880329758, 0.1879179677926004, 0.17582501494325697, 0.17686272505670786, 0.18013029801659286, 0.1768194588366896, 0.18737604515627027, 0.18427732586860657, 0.17890972481109202, 0.183290203101933, 0.19483181601390243, 0.17780692782253027, 0.2043680700007826, 0.18003505794331431, 0.1828110609203577, 0.17682688497006893, 0.1818722530733794, 0.1820431030355394, 0.17928721709176898, 0.1796325019095093, 0.18813816690817475, 0.19328633998520672, 0.1971150920726359, 0.19286188296973705, 0.18075118795968592, 0.17944134818390012, 0.18000274687074125, 0.18225062196142972, 0.1783659770153463, 0.17935762903653085, 0.18203932489268482, 0.17770769889466465, 0.17637359607033432, 0.1798474509268999, 0.18284182995557785, 0.18026018096134067, 0.17693236120976508, 0.17621955298818648, 0.17777508008293808, 0.18041790300048888, 0.17626796988770366, 0.17582795419730246, 0.17850746703334153, 0.18134044110774994, 0.17709486396051943, 0.17910169693641365, 0.17629565601237118, 0.1779124888125807, 0.1804992922116071, 0.17400894407182932, 0.17642895597964525, 0.2821852269116789, 0.1890750490128994, 0.19225984904915094, 0.1860274530481547, 0.18498460482805967, 0.17787529597990215, 0.17557231592945755, 0.17707828618586063]
[0.0014302818602774032, 0.0013906947596994943, 0.001391520900579617, 0.0014169247831802728, 0.0014332279223631056, 0.0014187753806973612, 0.0014156643788568503, 0.0014260370002756285, 0.001422308596544141, 0.001403009991861822, 0.0013955135129449904, 0.00139875387476916, 0.0014576704652850018, 0.0014217403183091981, 0.001399457999757787, 0.0014123891556072374, 0.0014237369764451833, 0.001399304434411632, 0.0014121075045288533, 0.00142072179975609, 0.0013929465352449306, 0.001397477605473972, 0.001435441768172291, 0.0014174996438443429, 0.0014127678375026977, 0.001394619782592437, 0.001427337193370674, 0.0014206579373788465, 0.0014060225731209498, 0.0014132440393400747, 0.0014112852874767874, 0.00142573294691857, 0.0014058073870693297, 0.0013955600159026164, 0.001424554681275473, 0.001417064232491823, 0.0013901564353975908, 0.0014387244257585022, 0.001415736774747917, 0.0016515707135084988, 0.0013906530866240577, 0.0013886918765023467, 0.0014324405270289312, 0.0014188710470185723, 0.0013796018378794655, 0.0014516276509551577, 0.0014334906672322473, 0.0014340328449715478, 0.0022196380147176194, 0.0014820623792736799, 0.0015275581710964673, 0.0015898512084354726, 0.0016148876896951087, 0.0015962830863806398, 0.001412974643456035, 0.0014226797195656816, 0.0014394274569845707, 0.001436483535496879, 0.0014321812944732201, 0.00141977473674703, 0.0014386655899963176, 0.0014632162096541981, 0.0014208171935391057, 0.0014004246205409137, 0.0014145511072562184, 0.0014355926676978205, 0.0014151668919669103, 0.0014097406438645698, 0.0014292182158967552, 0.0014265857839076094, 0.0014485158748173898, 0.0013973732955407264, 0.0014133907352797976, 0.0014004811153624408, 0.00137510738884236, 0.0014278099370412817, 0.0014009038365424373, 0.0013407754816490319, 0.0013728843796131916, 0.0013974402713509955, 0.0015079030846498272, 0.0013806247134339209, 0.0014567284325007783, 0.0013629846119632323, 0.0013710288764085881, 0.001396358899353433, 0.001370693479354183, 0.0014525274818315524, 0.0014285064020822214, 0.001386897091558853, 0.0014208542876118838, 0.0015103241551465303, 0.001378348277694033, 0.0015842486046572296, 0.0013956206042117388, 0.0014171400071345559, 0.0013707510462796042, 0.0014098624269254217, 0.0014111868452367394, 0.0013898233883082867, 0.0013925000148023977, 0.0014584354023889515, 0.001498343720815556, 0.001528023969555317, 0.0014950533563545508, 0.0014011719996874878, 0.00139101820297597, 0.0013953701307809398, 0.0014127955190808505, 0.0013826819923670255, 0.0013903692173374484, 0.0014111575573076342, 0.0013775790611989507, 0.001367237178839801, 0.001394166286255038, 0.0014173785267874253, 0.0013973657438863618, 0.0013715686915485666, 0.0013660430464200503, 0.001378101395991768, 0.001398588395352627, 0.001366418371222509, 0.0013630073968783137, 0.0013837788142119498, 0.0014057398535484492, 0.0013728284027947242, 0.001388385247569098, 0.0013666329923439626, 0.001379166579942486, 0.0013992193194698225, 0.0013489065431924754, 0.0013676663254236067, 0.002187482379160302, 0.0014656980543635613, 0.0014903864267376042, 0.0014420732794430597, 0.001433989184713641, 0.0013788782634100941, 0.001361025704879516, 0.0013726998929136484]
[699.16289073683, 719.065052216119, 718.6381459189474, 705.7537646815031, 697.7257311253052, 704.8332058796203, 706.382116365392, 701.2440769816752, 703.0823004443292, 712.7532988364396, 716.5820973597545, 714.9220588683143, 686.0261107125343, 703.3633267074033, 714.5623521199466, 708.0201628778888, 702.3769253340748, 714.6407710917258, 708.1613806263623, 703.8675694085081, 717.9026435671227, 715.5749731394353, 696.6496462432418, 705.4675493871313, 707.8303833471085, 717.0413129671197, 700.6052982046155, 703.8992101399356, 711.2261347129719, 707.5918752623629, 708.5739565725105, 701.3936250553061, 711.3350016496131, 716.5582193562797, 701.973755829893, 705.6843134355029, 719.3435030310064, 695.0601394515112, 706.345994422627, 605.4842168251212, 719.0866001150545, 720.1021457104421, 698.1092625703146, 704.7856830268455, 724.8468163372866, 688.8818901610266, 697.5978447985143, 697.3340976857756, 450.52391127263115, 674.735432182061, 654.639554107592, 628.9896782127628, 619.2381094866109, 626.4553001481506, 707.7267837971045, 702.8988930166811, 694.7206649058098, 696.1444216303533, 698.2356241203502, 704.3370853965097, 695.0885646764928, 683.4259991121407, 703.8203116821142, 714.0691368406178, 706.9380490180264, 696.5764192733333, 706.6304374956944, 709.350336426895, 699.6832176341633, 700.974320142786, 690.3617815897717, 715.6283887714061, 707.5184342439022, 714.0403315907638, 727.2159309985632, 700.3733298510356, 713.824870712107, 745.8370276655797, 728.3934574896604, 715.5940904960722, 663.1725939019649, 724.3097927117192, 686.4697480252318, 733.6839985006197, 729.3792400780801, 716.1482627876242, 729.5577129842048, 688.4551325246241, 700.0318644301339, 721.0340306330977, 703.8019371295003, 662.1095190674355, 725.5060394989523, 631.2140639166677, 716.5271112952724, 705.646580412327, 729.5270740183854, 709.289063175309, 708.623385610033, 719.5158812352515, 718.1328469442815, 685.6662957865507, 667.4036044651324, 654.439995657279, 668.8724490999709, 713.6882554197746, 718.8978532851559, 716.6557302185729, 707.8165144879488, 723.2320992971719, 719.2334147867543, 708.6380927640092, 725.9111496146495, 731.4019948232917, 717.274553156902, 705.5278326154418, 715.6322561756768, 729.0921746478132, 732.0413530310566, 725.6360111879412, 715.0066476476586, 731.8402775171404, 733.6717337633626, 722.6588452790351, 711.3691750829591, 728.4231575951212, 720.2611823705879, 731.7253466015503, 725.0755742948048, 714.6842429097602, 741.3412033967056, 731.1724953747549, 457.14653956840795, 682.2687640355928, 670.9669264695061, 693.446036519176, 697.35532921728, 725.2271839624965, 734.7399806005313, 728.4913513597152]
Elapsed: 0.18450433916046416~0.013606751655393278
Time per graph: 0.0014302661950423578~0.00010547869500304866
Speed: 701.9107773446718~37.70474233667904
Total Time: 0.1778
best val loss: 0.24745985822275626 test_score: 0.9070

Testing...
Test loss: 0.6068 score: 0.9147 time: 0.18s
test Score 0.9147
Epoch Time List: [0.6070799229200929, 0.6081994269043207, 0.6046663529705256, 0.6083417749032378, 0.610097715863958, 0.6099450089968741, 0.6077162793371826, 0.6072495749685913, 0.6018266561441123, 0.6011047151405364, 0.6071971550118178, 0.6071761879138649, 0.6099002480041236, 0.606961689889431, 0.6068990859203041, 0.6105907957535237, 0.6137319980189204, 0.6089680190198123, 0.609665225027129, 0.6108894369099289, 0.6070711950305849, 0.6096108320634812, 0.6109745728317648, 0.61077816109173, 0.6123034399934113, 0.6087668219115585, 0.6118094208650291, 0.6093572438694537, 0.6112055070698261, 0.611761957174167, 0.614716200158, 0.615535870892927, 0.6144425203092396, 0.6055479638744146, 0.643765069078654, 0.6099897660315037, 0.6098469221033156, 0.6165041069034487, 0.6042886099312454, 0.6471237998921424, 0.6120979278348386, 0.6096078110858798, 0.6164852250367403, 0.693265455076471, 0.6025981183629483, 0.6145459490362555, 0.701060833176598, 0.6160020036622882, 0.7210256110411137, 0.6303398290183395, 0.6398461889475584, 0.687737017404288, 0.6458690899889916, 0.6394637608900666, 0.6049849370028824, 0.6066462630406022, 0.6227783598005772, 0.6084233080036938, 0.6102943120058626, 0.610499300993979, 0.6137616771738976, 0.6212786599062383, 0.6143931159749627, 0.6077606801409274, 0.6136672997381538, 0.6141655682586133, 0.6107872689608485, 0.6143119849730283, 0.6231962288729846, 0.6126815220341086, 0.6170713065657765, 0.7588496841490269, 0.6216927021741867, 0.6252488638274372, 0.6221464748959988, 0.6280852712225169, 0.6254296181723475, 0.6487467051483691, 0.6154690482653677, 0.6647685901261866, 0.637059231987223, 0.6101232680957764, 0.6183331480715424, 0.6158694017212838, 0.6111560938879848, 0.6181659770663828, 0.6195296372752637, 0.6318653579801321, 0.6298103360459208, 0.6229403668548912, 0.6244337346870452, 0.671588544966653, 0.6266703337896615, 0.645643234020099, 0.6437051901593804, 0.6312155199702829, 0.6407200419344008, 0.6258743640501052, 0.62664958788082, 0.6203541450668126, 0.6241766680032015, 0.6324845340568572, 0.6474049710668623, 0.6557928638067096, 0.6457795519381762, 0.6207136672455817, 0.62188500771299, 0.6224337590392679, 0.6199534079059958, 0.6202000700868666, 0.6241106127854437, 0.6208496221806854, 0.619803864043206, 0.6226850277744234, 0.6202820732723922, 0.618191690184176, 0.6185160847380757, 0.6157325650565326, 0.6147019187919796, 0.6238796608522534, 0.6191225969232619, 0.6161727218423039, 0.6188092750962824, 0.6219257300253958, 0.6213192481081933, 0.617243560962379, 0.6164883731398731, 0.6144833345897496, 0.6151848959270865, 0.6226598750799894, 0.6772064501419663, 0.6088900349568576, 0.7202644341159612, 0.6473145810887218, 0.6557547301054001, 0.7257214928977191, 0.6340720569714904, 0.7077086558565497, 0.6076391430106014, 0.6149723527487367]
Total Epoch List: [72, 68]
Total Time List: [0.18077090312726796, 0.17775630112737417]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x798efabd6650>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.16s
Epoch 2/1000, LR 0.000020
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
Epoch 3/1000, LR 0.000050
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.17s
Epoch 5/1000, LR 0.000110
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.16s
Epoch 12/1000, LR 0.000290
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.16s
Epoch 13/1000, LR 0.000290
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.16s
Epoch 14/1000, LR 0.000290
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5000 time: 0.16s
Epoch 17/1000, LR 0.000290
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.27s
Val loss: 0.6779 score: 0.5116 time: 0.18s
Test loss: 0.6768 score: 0.5234 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.28s
Val loss: 0.6722 score: 0.5349 time: 0.18s
Test loss: 0.6706 score: 0.5547 time: 0.16s
Epoch 20/1000, LR 0.000290
Train loss: 0.5007;  Loss pred: 0.5007; Loss self: 0.0000; time: 0.28s
Val loss: 0.6645 score: 0.5969 time: 0.18s
Test loss: 0.6623 score: 0.6406 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.4711;  Loss pred: 0.4711; Loss self: 0.0000; time: 0.27s
Val loss: 0.6552 score: 0.6899 time: 0.18s
Test loss: 0.6520 score: 0.7031 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.28s
Val loss: 0.6447 score: 0.8217 time: 0.18s
Test loss: 0.6406 score: 0.8281 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 0.4160;  Loss pred: 0.4160; Loss self: 0.0000; time: 0.28s
Val loss: 0.6313 score: 0.9225 time: 0.18s
Test loss: 0.6262 score: 0.9375 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.27s
Val loss: 0.6132 score: 0.9225 time: 0.18s
Test loss: 0.6070 score: 0.9453 time: 0.17s
Epoch 25/1000, LR 0.000290
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.27s
Val loss: 0.5903 score: 0.9457 time: 0.18s
Test loss: 0.5831 score: 0.9219 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.3419;  Loss pred: 0.3419; Loss self: 0.0000; time: 0.28s
Val loss: 0.5614 score: 0.9457 time: 0.18s
Test loss: 0.5540 score: 0.9219 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.3278;  Loss pred: 0.3278; Loss self: 0.0000; time: 0.28s
Val loss: 0.5335 score: 0.9147 time: 0.18s
Test loss: 0.5260 score: 0.9219 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.2974;  Loss pred: 0.2974; Loss self: 0.0000; time: 0.28s
Val loss: 0.4929 score: 0.9147 time: 0.18s
Test loss: 0.4857 score: 0.9219 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.2685;  Loss pred: 0.2685; Loss self: 0.0000; time: 0.28s
Val loss: 0.4468 score: 0.9690 time: 0.18s
Test loss: 0.4401 score: 0.9219 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.2443;  Loss pred: 0.2443; Loss self: 0.0000; time: 0.28s
Val loss: 0.3996 score: 0.9690 time: 0.18s
Test loss: 0.3945 score: 0.9219 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.28s
Val loss: 0.3630 score: 0.9535 time: 0.18s
Test loss: 0.3613 score: 0.9219 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.28s
Val loss: 0.3258 score: 0.9690 time: 0.18s
Test loss: 0.3265 score: 0.9219 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.28s
Val loss: 0.2905 score: 0.9612 time: 0.18s
Test loss: 0.2960 score: 0.9219 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.1696;  Loss pred: 0.1696; Loss self: 0.0000; time: 0.28s
Val loss: 0.2633 score: 0.9380 time: 0.18s
Test loss: 0.2745 score: 0.9219 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.28s
Val loss: 0.2419 score: 0.9690 time: 0.18s
Test loss: 0.2545 score: 0.9062 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 0.1393;  Loss pred: 0.1393; Loss self: 0.0000; time: 0.28s
Val loss: 0.2266 score: 0.9612 time: 0.18s
Test loss: 0.2410 score: 0.9141 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.1433;  Loss pred: 0.1433; Loss self: 0.0000; time: 0.28s
Val loss: 0.2129 score: 0.9535 time: 0.18s
Test loss: 0.2305 score: 0.9297 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.28s
Val loss: 0.1943 score: 0.9535 time: 0.18s
Test loss: 0.2255 score: 0.9219 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 0.1195;  Loss pred: 0.1195; Loss self: 0.0000; time: 0.27s
Val loss: 0.1772 score: 0.9535 time: 0.18s
Test loss: 0.2165 score: 0.9219 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.28s
Val loss: 0.1714 score: 0.9767 time: 0.18s
Test loss: 0.2092 score: 0.9062 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.28s
Val loss: 0.1585 score: 0.9612 time: 0.18s
Test loss: 0.2069 score: 0.9141 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.28s
Val loss: 0.1573 score: 0.9612 time: 0.18s
Test loss: 0.2074 score: 0.9141 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.27s
Val loss: 0.1649 score: 0.9457 time: 0.18s
Test loss: 0.2214 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.28s
Val loss: 0.1813 score: 0.9302 time: 0.18s
Test loss: 0.2431 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.28s
Val loss: 0.1634 score: 0.9380 time: 0.18s
Test loss: 0.2271 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0744;  Loss pred: 0.0744; Loss self: 0.0000; time: 0.28s
Val loss: 0.1565 score: 0.9380 time: 0.18s
Test loss: 0.2230 score: 0.9219 time: 0.17s
Epoch 47/1000, LR 0.000289
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.28s
Val loss: 0.1518 score: 0.9380 time: 0.18s
Test loss: 0.2250 score: 0.9219 time: 0.17s
Epoch 48/1000, LR 0.000289
Train loss: 0.0750;  Loss pred: 0.0750; Loss self: 0.0000; time: 0.28s
Val loss: 0.1488 score: 0.9457 time: 0.18s
Test loss: 0.2236 score: 0.9219 time: 0.17s
Epoch 49/1000, LR 0.000289
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.27s
Val loss: 0.1493 score: 0.9457 time: 0.18s
Test loss: 0.2216 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.28s
Val loss: 0.1813 score: 0.9302 time: 0.18s
Test loss: 0.2576 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.28s
Val loss: 0.2128 score: 0.9225 time: 0.18s
Test loss: 0.2825 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.28s
Val loss: 0.2336 score: 0.9225 time: 0.18s
Test loss: 0.3051 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.28s
Val loss: 0.2026 score: 0.9302 time: 0.18s
Test loss: 0.2943 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.28s
Val loss: 0.1498 score: 0.9380 time: 0.18s
Test loss: 0.2450 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.28s
Val loss: 0.1333 score: 0.9612 time: 0.18s
Test loss: 0.2249 score: 0.9062 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.28s
Val loss: 0.1304 score: 0.9612 time: 0.18s
Test loss: 0.2126 score: 0.9062 time: 0.17s
Epoch 57/1000, LR 0.000288
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.28s
Val loss: 0.1276 score: 0.9612 time: 0.18s
Test loss: 0.2116 score: 0.9062 time: 0.16s
Epoch 58/1000, LR 0.000288
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.28s
Val loss: 0.1276 score: 0.9690 time: 0.18s
Test loss: 0.2130 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.28s
Val loss: 0.1472 score: 0.9457 time: 0.18s
Test loss: 0.2414 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.27s
Val loss: 0.1407 score: 0.9380 time: 0.18s
Test loss: 0.2405 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.28s
Val loss: 0.1716 score: 0.9302 time: 0.18s
Test loss: 0.2720 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.28s
Val loss: 0.2027 score: 0.9225 time: 0.19s
Test loss: 0.2993 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.28s
Val loss: 0.2035 score: 0.9302 time: 0.18s
Test loss: 0.3004 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.28s
Val loss: 0.1999 score: 0.9225 time: 0.18s
Test loss: 0.3095 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.28s
Val loss: 0.2094 score: 0.9225 time: 0.18s
Test loss: 0.3120 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.28s
Val loss: 0.1908 score: 0.9302 time: 0.18s
Test loss: 0.3091 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.27s
Val loss: 0.1389 score: 0.9380 time: 0.18s
Test loss: 0.2695 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.27s
Val loss: 0.1462 score: 0.9380 time: 0.18s
Test loss: 0.2648 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.28s
Val loss: 0.1264 score: 0.9612 time: 0.18s
Test loss: 0.2440 score: 0.9141 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.28s
Val loss: 0.1403 score: 0.9380 time: 0.18s
Test loss: 0.2545 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.27s
Val loss: 0.1550 score: 0.9380 time: 0.18s
Test loss: 0.2719 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.27s
Val loss: 0.1356 score: 0.9535 time: 0.18s
Test loss: 0.2610 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.28s
Val loss: 0.1264 score: 0.9690 time: 0.18s
Test loss: 0.2506 score: 0.9062 time: 0.17s
Epoch 74/1000, LR 0.000287
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.27s
Val loss: 0.1172 score: 0.9612 time: 0.18s
Test loss: 0.2434 score: 0.9062 time: 0.16s
Epoch 75/1000, LR 0.000287
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.28s
Val loss: 0.1384 score: 0.9380 time: 0.18s
Test loss: 0.2786 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.28s
Val loss: 0.1293 score: 0.9535 time: 0.18s
Test loss: 0.2808 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.28s
Val loss: 0.1615 score: 0.9302 time: 0.18s
Test loss: 0.3227 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.38s
Val loss: 0.1457 score: 0.9380 time: 0.18s
Test loss: 0.3128 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.27s
Val loss: 0.1319 score: 0.9380 time: 0.18s
Test loss: 0.3067 score: 0.9141 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.30s
Val loss: 0.1253 score: 0.9612 time: 0.22s
Test loss: 0.2984 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.28s
Val loss: 0.1656 score: 0.9302 time: 0.19s
Test loss: 0.3391 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.28s
Val loss: 0.1957 score: 0.9380 time: 0.19s
Test loss: 0.3660 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.37s
Val loss: 0.2349 score: 0.9302 time: 0.18s
Test loss: 0.3885 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.27s
Val loss: 0.1869 score: 0.9380 time: 0.18s
Test loss: 0.3618 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.30s
Val loss: 0.2244 score: 0.9302 time: 0.28s
Test loss: 0.3890 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.26s
Val loss: 0.2377 score: 0.9302 time: 0.17s
Test loss: 0.4040 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.27s
Val loss: 0.1804 score: 0.9380 time: 0.17s
Test loss: 0.3600 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.26s
Val loss: 0.1837 score: 0.9380 time: 0.17s
Test loss: 0.3855 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.26s
Val loss: 0.2100 score: 0.9380 time: 0.18s
Test loss: 0.3959 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.27s
Val loss: 0.1383 score: 0.9457 time: 0.18s
Test loss: 0.3436 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.28s
Val loss: 0.1832 score: 0.9380 time: 0.17s
Test loss: 0.3795 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.27s
Val loss: 0.2051 score: 0.9302 time: 0.18s
Test loss: 0.3945 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 93/1000, LR 0.000285
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.27s
Val loss: 0.2175 score: 0.9302 time: 0.17s
Test loss: 0.4052 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 94/1000, LR 0.000285
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.26s
Val loss: 0.1662 score: 0.9457 time: 0.18s
Test loss: 0.3711 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 073,   Train_Loss: 0.0166,   Val_Loss: 0.1172,   Val_Precision: 0.9545,   Val_Recall: 0.9692,   Val_accuracy: 0.9618,   Val_Score: 0.9612,   Val_Loss: 0.1172,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9062,   Test_loss: 0.2434


[0.18450635997578502, 0.17939962400123477, 0.1795061961747706, 0.1827832970302552, 0.18488640198484063, 0.1830220241099596, 0.18262070487253368, 0.18395877303555608, 0.1834778089541942, 0.18098828895017505, 0.18002124316990376, 0.18043924984522164, 0.18803949002176523, 0.18340450106188655, 0.18053008196875453, 0.18219820107333362, 0.18366206996142864, 0.18051027203910053, 0.18216186808422208, 0.1832731121685356, 0.17969010304659605, 0.1802746111061424, 0.18517198809422553, 0.18285745405592024, 0.182247051037848, 0.17990595195442438, 0.18412649794481695, 0.18326487392187119, 0.18137691193260252, 0.18230848107486963, 0.18205580208450556, 0.1839195501524955, 0.18134915293194354, 0.1800272420514375, 0.18376755388453603, 0.18280128599144518, 0.1793301801662892, 0.1855954509228468, 0.18263004394248128, 0.21305262204259634, 0.17939424817450345, 0.17914125206880271, 0.18478482798673213, 0.18303436506539583, 0.17796863708645105, 0.18725996697321534, 0.1849202960729599, 0.18499023700132966, 0.2863333038985729, 0.1911860469263047, 0.19705500407144427, 0.20509080588817596, 0.20832051197066903, 0.20592051814310253, 0.1822737290058285, 0.18352568382397294, 0.18568614195100963, 0.1853063760790974, 0.1847513869870454, 0.1831509410403669, 0.18558786110952497, 0.18875489104539156, 0.18328541796654463, 0.18065477604977787, 0.18247709283605218, 0.18519145413301885, 0.18255652906373143, 0.1818565430585295, 0.18436914985068142, 0.1840295661240816, 0.1868585478514433, 0.1802611551247537, 0.1823274048510939, 0.18066206388175488, 0.17738885316066444, 0.18418748187832534, 0.1807165949139744, 0.17296003713272512, 0.17710208497010171, 0.18026979500427842, 0.1945194979198277, 0.1781005880329758, 0.1879179677926004, 0.17582501494325697, 0.17686272505670786, 0.18013029801659286, 0.1768194588366896, 0.18737604515627027, 0.18427732586860657, 0.17890972481109202, 0.183290203101933, 0.19483181601390243, 0.17780692782253027, 0.2043680700007826, 0.18003505794331431, 0.1828110609203577, 0.17682688497006893, 0.1818722530733794, 0.1820431030355394, 0.17928721709176898, 0.1796325019095093, 0.18813816690817475, 0.19328633998520672, 0.1971150920726359, 0.19286188296973705, 0.18075118795968592, 0.17944134818390012, 0.18000274687074125, 0.18225062196142972, 0.1783659770153463, 0.17935762903653085, 0.18203932489268482, 0.17770769889466465, 0.17637359607033432, 0.1798474509268999, 0.18284182995557785, 0.18026018096134067, 0.17693236120976508, 0.17621955298818648, 0.17777508008293808, 0.18041790300048888, 0.17626796988770366, 0.17582795419730246, 0.17850746703334153, 0.18134044110774994, 0.17709486396051943, 0.17910169693641365, 0.17629565601237118, 0.1779124888125807, 0.1804992922116071, 0.17400894407182932, 0.17642895597964525, 0.2821852269116789, 0.1890750490128994, 0.19225984904915094, 0.1860274530481547, 0.18498460482805967, 0.17787529597990215, 0.17557231592945755, 0.17707828618586063, 0.16833375301212072, 0.17285372782498598, 0.16726021212525666, 0.17062752391211689, 0.16946706012822688, 0.16909946012310684, 0.17333643487654626, 0.170849195914343, 0.1748862350359559, 0.17023306502960622, 0.16753829387016594, 0.1670451839454472, 0.16958597605116665, 0.16689480422064662, 0.17007416998967528, 0.1691500931046903, 0.17091813497245312, 0.178416890790686, 0.1649269920308143, 0.16900180792436004, 0.16808994812890887, 0.16603200300596654, 0.17734636389650404, 0.17473506182432175, 0.16467760293744504, 0.16990388580597937, 0.16829360206611454, 0.1692404211498797, 0.16578638111241162, 0.16945556201972067, 0.17029817216098309, 0.1689978849608451, 0.16425098991021514, 0.1685840650461614, 0.16947700292803347, 0.16506044287234545, 0.1670994171872735, 0.1689250459894538, 0.16727396892383695, 0.1670532829593867, 0.16967786592431366, 0.16931477701291442, 0.16764870681799948, 0.1692141629755497, 0.169509869068861, 0.16997820395044982, 0.17031371290795505, 0.1716245668940246, 0.16877326811663806, 0.16898070508614182, 0.1692409690003842, 0.16982476809062064, 0.16933419718407094, 0.16784881497733295, 0.165773888817057, 0.17092711804434657, 0.1679521461483091, 0.16693127993494272, 0.17137218406423926, 0.16926548606716096, 0.16399949905462563, 0.1777853302191943, 0.17258220189251006, 0.16963478596881032, 0.16782818012870848, 0.17358807194978, 0.16788977989926934, 0.16872098995372653, 0.1670111301355064, 0.17037966009229422, 0.16836020001210272, 0.16861752700060606, 0.17166375182569027, 0.16904767090454698, 0.16873823595233262, 0.16836296999827027, 0.17109316098503768, 0.17071162187494338, 0.1821538619697094, 0.17207311489619315, 0.17630948103033006, 0.17817499698139727, 0.1641471169423312, 0.1807061731815338, 0.17350216512568295, 0.16126823099330068, 0.16801152587868273, 0.16434107697568834, 0.1648331810720265, 0.16229097195900977, 0.16338124102912843, 0.16835474502295256, 0.16717325593344867, 0.1650477279908955]
[0.0014302818602774032, 0.0013906947596994943, 0.001391520900579617, 0.0014169247831802728, 0.0014332279223631056, 0.0014187753806973612, 0.0014156643788568503, 0.0014260370002756285, 0.001422308596544141, 0.001403009991861822, 0.0013955135129449904, 0.00139875387476916, 0.0014576704652850018, 0.0014217403183091981, 0.001399457999757787, 0.0014123891556072374, 0.0014237369764451833, 0.001399304434411632, 0.0014121075045288533, 0.00142072179975609, 0.0013929465352449306, 0.001397477605473972, 0.001435441768172291, 0.0014174996438443429, 0.0014127678375026977, 0.001394619782592437, 0.001427337193370674, 0.0014206579373788465, 0.0014060225731209498, 0.0014132440393400747, 0.0014112852874767874, 0.00142573294691857, 0.0014058073870693297, 0.0013955600159026164, 0.001424554681275473, 0.001417064232491823, 0.0013901564353975908, 0.0014387244257585022, 0.001415736774747917, 0.0016515707135084988, 0.0013906530866240577, 0.0013886918765023467, 0.0014324405270289312, 0.0014188710470185723, 0.0013796018378794655, 0.0014516276509551577, 0.0014334906672322473, 0.0014340328449715478, 0.0022196380147176194, 0.0014820623792736799, 0.0015275581710964673, 0.0015898512084354726, 0.0016148876896951087, 0.0015962830863806398, 0.001412974643456035, 0.0014226797195656816, 0.0014394274569845707, 0.001436483535496879, 0.0014321812944732201, 0.00141977473674703, 0.0014386655899963176, 0.0014632162096541981, 0.0014208171935391057, 0.0014004246205409137, 0.0014145511072562184, 0.0014355926676978205, 0.0014151668919669103, 0.0014097406438645698, 0.0014292182158967552, 0.0014265857839076094, 0.0014485158748173898, 0.0013973732955407264, 0.0014133907352797976, 0.0014004811153624408, 0.00137510738884236, 0.0014278099370412817, 0.0014009038365424373, 0.0013407754816490319, 0.0013728843796131916, 0.0013974402713509955, 0.0015079030846498272, 0.0013806247134339209, 0.0014567284325007783, 0.0013629846119632323, 0.0013710288764085881, 0.001396358899353433, 0.001370693479354183, 0.0014525274818315524, 0.0014285064020822214, 0.001386897091558853, 0.0014208542876118838, 0.0015103241551465303, 0.001378348277694033, 0.0015842486046572296, 0.0013956206042117388, 0.0014171400071345559, 0.0013707510462796042, 0.0014098624269254217, 0.0014111868452367394, 0.0013898233883082867, 0.0013925000148023977, 0.0014584354023889515, 0.001498343720815556, 0.001528023969555317, 0.0014950533563545508, 0.0014011719996874878, 0.00139101820297597, 0.0013953701307809398, 0.0014127955190808505, 0.0013826819923670255, 0.0013903692173374484, 0.0014111575573076342, 0.0013775790611989507, 0.001367237178839801, 0.001394166286255038, 0.0014173785267874253, 0.0013973657438863618, 0.0013715686915485666, 0.0013660430464200503, 0.001378101395991768, 0.001398588395352627, 0.001366418371222509, 0.0013630073968783137, 0.0013837788142119498, 0.0014057398535484492, 0.0013728284027947242, 0.001388385247569098, 0.0013666329923439626, 0.001379166579942486, 0.0013992193194698225, 0.0013489065431924754, 0.0013676663254236067, 0.002187482379160302, 0.0014656980543635613, 0.0014903864267376042, 0.0014420732794430597, 0.001433989184713641, 0.0013788782634100941, 0.001361025704879516, 0.0013726998929136484, 0.0013151074454071932, 0.001350419748632703, 0.0013067204072285676, 0.0013330275305634132, 0.0013239614072517725, 0.0013210895322117722, 0.0013541908974730177, 0.0013347593430808047, 0.0013662987112184055, 0.0013299458205437986, 0.0013088929208606714, 0.0013050404995738063, 0.0013248904378997395, 0.0013038656579738017, 0.0013287044530443382, 0.001321485102380393, 0.00133529792947229, 0.0013938819593022345, 0.0012884921252407366, 0.0013203266244090628, 0.0013132027197571006, 0.0012971250234841136, 0.0013855184679414378, 0.0013651176705025136, 0.0012865437729487894, 0.0013273741078592138, 0.0013147937661415199, 0.0013221907902334351, 0.0012952061024407158, 0.0013238715782790678, 0.0013304544700076804, 0.0013202959762566024, 0.0012832108586735558, 0.001317063008173136, 0.0013240390853752615, 0.0012895347099401988, 0.0013054641967755742, 0.0013197269217926078, 0.0013068278822174761, 0.0013051037731202086, 0.0013256083275337005, 0.001322771695413394, 0.001309755522015621, 0.001321985648246482, 0.0013242958521004766, 0.0013279547183628893, 0.0013305758820933988, 0.0013408169288595673, 0.0013185411571612349, 0.001320161758485483, 0.0013221950703155017, 0.0013267560007079737, 0.0013229234155005543, 0.0013113188670104137, 0.001295108506383258, 0.0013353681097214576, 0.001312126141783665, 0.00130415062449174, 0.0013388451880018692, 0.001322386609899695, 0.0012812460863642627, 0.0013889478923374554, 0.0013482984522852348, 0.0013252717653813306, 0.001311157657255535, 0.0013561568121076562, 0.0013116389054630417, 0.0013181327340134885, 0.0013047744541836437, 0.0013310910944710486, 0.0013153140625945525, 0.0013173244296922348, 0.0013411230611382052, 0.0013206849289417733, 0.0013182674683775986, 0.0013153357031114865, 0.001336665320195607, 0.0013336845458979951, 0.0014230770466383547, 0.001344321210126509, 0.0013774178205494536, 0.0013919921639171662, 0.0012823993511119625, 0.001411766977980733, 0.001355485665044398, 0.0012599080546351615, 0.0013125900459272088, 0.0012839146638725651, 0.001287759227125207, 0.0012678982184297638, 0.0012764159455400659, 0.0013152714454918168, 0.0013060410619800678, 0.0012894353749288712]
[699.16289073683, 719.065052216119, 718.6381459189474, 705.7537646815031, 697.7257311253052, 704.8332058796203, 706.382116365392, 701.2440769816752, 703.0823004443292, 712.7532988364396, 716.5820973597545, 714.9220588683143, 686.0261107125343, 703.3633267074033, 714.5623521199466, 708.0201628778888, 702.3769253340748, 714.6407710917258, 708.1613806263623, 703.8675694085081, 717.9026435671227, 715.5749731394353, 696.6496462432418, 705.4675493871313, 707.8303833471085, 717.0413129671197, 700.6052982046155, 703.8992101399356, 711.2261347129719, 707.5918752623629, 708.5739565725105, 701.3936250553061, 711.3350016496131, 716.5582193562797, 701.973755829893, 705.6843134355029, 719.3435030310064, 695.0601394515112, 706.345994422627, 605.4842168251212, 719.0866001150545, 720.1021457104421, 698.1092625703146, 704.7856830268455, 724.8468163372866, 688.8818901610266, 697.5978447985143, 697.3340976857756, 450.52391127263115, 674.735432182061, 654.639554107592, 628.9896782127628, 619.2381094866109, 626.4553001481506, 707.7267837971045, 702.8988930166811, 694.7206649058098, 696.1444216303533, 698.2356241203502, 704.3370853965097, 695.0885646764928, 683.4259991121407, 703.8203116821142, 714.0691368406178, 706.9380490180264, 696.5764192733333, 706.6304374956944, 709.350336426895, 699.6832176341633, 700.974320142786, 690.3617815897717, 715.6283887714061, 707.5184342439022, 714.0403315907638, 727.2159309985632, 700.3733298510356, 713.824870712107, 745.8370276655797, 728.3934574896604, 715.5940904960722, 663.1725939019649, 724.3097927117192, 686.4697480252318, 733.6839985006197, 729.3792400780801, 716.1482627876242, 729.5577129842048, 688.4551325246241, 700.0318644301339, 721.0340306330977, 703.8019371295003, 662.1095190674355, 725.5060394989523, 631.2140639166677, 716.5271112952724, 705.646580412327, 729.5270740183854, 709.289063175309, 708.623385610033, 719.5158812352515, 718.1328469442815, 685.6662957865507, 667.4036044651324, 654.439995657279, 668.8724490999709, 713.6882554197746, 718.8978532851559, 716.6557302185729, 707.8165144879488, 723.2320992971719, 719.2334147867543, 708.6380927640092, 725.9111496146495, 731.4019948232917, 717.274553156902, 705.5278326154418, 715.6322561756768, 729.0921746478132, 732.0413530310566, 725.6360111879412, 715.0066476476586, 731.8402775171404, 733.6717337633626, 722.6588452790351, 711.3691750829591, 728.4231575951212, 720.2611823705879, 731.7253466015503, 725.0755742948048, 714.6842429097602, 741.3412033967056, 731.1724953747549, 457.14653956840795, 682.2687640355928, 670.9669264695061, 693.446036519176, 697.35532921728, 725.2271839624965, 734.7399806005313, 728.4913513597152, 760.3941438339076, 740.5104975786217, 765.274648247751, 750.1720535189121, 755.3090252651405, 756.9509678316782, 738.4483250227466, 749.1987264849296, 731.9043718545586, 751.9103293930517, 764.0044376910857, 766.2597446796287, 754.7793926154628, 766.9501791725945, 752.6128159718229, 756.7243839515849, 748.896540560952, 717.4208643180887, 776.1009791295108, 757.3883473322891, 761.4970521725444, 770.9357092764832, 721.7514765326588, 732.5375838347253, 777.2763127273748, 753.3671133700188, 760.5755562217682, 756.320500329191, 772.0778941016239, 755.3602754278658, 751.6228646247679, 757.4059286579601, 779.2951510975302, 759.2651177615824, 755.2647131384178, 775.4735039635918, 766.0110499161492, 757.7325153310375, 765.2117112034381, 766.2225951651535, 754.3706381661798, 755.988356469541, 763.5012666036108, 756.4378640013433, 755.1182754321036, 753.0377249856879, 751.5542807124214, 745.8139724194493, 758.4139445089139, 757.4829323546084, 756.3180520415799, 753.7180909424095, 755.9016555932909, 762.5910258424267, 772.1360759127566, 748.8571823155104, 762.1218480112209, 766.7825949090236, 746.9123457749648, 756.2085040137025, 780.4901889204261, 719.9694139116361, 741.6755528459572, 754.5622159333202, 762.684788107909, 737.3778541478998, 762.404954469519, 758.648938908581, 766.4159861450296, 751.2633839665059, 760.274696696717, 759.1144424715702, 745.6437287353067, 757.1828663186693, 758.5714007118049, 760.262188302541, 748.1304294284081, 749.8024949570672, 702.7026416891742, 743.8698374073067, 725.9961248367607, 718.3948487080042, 779.7882922608349, 708.3321933413628, 737.7429550073815, 793.7087125691687, 761.8524939320286, 778.8679638440947, 776.5426788922332, 788.7068421300063, 783.4436756247892, 760.299330930941, 765.6727105378419, 775.5332445840202]
Elapsed: 0.1784315296074646~0.013082479950571706
Time per graph: 0.0013873110530462337~9.867872348423135e-05
Speed: 723.6458159395206~40.727329280865995
Total Time: 0.1659
best val loss: 0.11715458122681277 test_score: 0.9062

Testing...
Test loss: 0.2092 score: 0.9062 time: 0.16s
test Score 0.9062
Epoch Time List: [0.6070799229200929, 0.6081994269043207, 0.6046663529705256, 0.6083417749032378, 0.610097715863958, 0.6099450089968741, 0.6077162793371826, 0.6072495749685913, 0.6018266561441123, 0.6011047151405364, 0.6071971550118178, 0.6071761879138649, 0.6099002480041236, 0.606961689889431, 0.6068990859203041, 0.6105907957535237, 0.6137319980189204, 0.6089680190198123, 0.609665225027129, 0.6108894369099289, 0.6070711950305849, 0.6096108320634812, 0.6109745728317648, 0.61077816109173, 0.6123034399934113, 0.6087668219115585, 0.6118094208650291, 0.6093572438694537, 0.6112055070698261, 0.611761957174167, 0.614716200158, 0.615535870892927, 0.6144425203092396, 0.6055479638744146, 0.643765069078654, 0.6099897660315037, 0.6098469221033156, 0.6165041069034487, 0.6042886099312454, 0.6471237998921424, 0.6120979278348386, 0.6096078110858798, 0.6164852250367403, 0.693265455076471, 0.6025981183629483, 0.6145459490362555, 0.701060833176598, 0.6160020036622882, 0.7210256110411137, 0.6303398290183395, 0.6398461889475584, 0.687737017404288, 0.6458690899889916, 0.6394637608900666, 0.6049849370028824, 0.6066462630406022, 0.6227783598005772, 0.6084233080036938, 0.6102943120058626, 0.610499300993979, 0.6137616771738976, 0.6212786599062383, 0.6143931159749627, 0.6077606801409274, 0.6136672997381538, 0.6141655682586133, 0.6107872689608485, 0.6143119849730283, 0.6231962288729846, 0.6126815220341086, 0.6170713065657765, 0.7588496841490269, 0.6216927021741867, 0.6252488638274372, 0.6221464748959988, 0.6280852712225169, 0.6254296181723475, 0.6487467051483691, 0.6154690482653677, 0.6647685901261866, 0.637059231987223, 0.6101232680957764, 0.6183331480715424, 0.6158694017212838, 0.6111560938879848, 0.6181659770663828, 0.6195296372752637, 0.6318653579801321, 0.6298103360459208, 0.6229403668548912, 0.6244337346870452, 0.671588544966653, 0.6266703337896615, 0.645643234020099, 0.6437051901593804, 0.6312155199702829, 0.6407200419344008, 0.6258743640501052, 0.62664958788082, 0.6203541450668126, 0.6241766680032015, 0.6324845340568572, 0.6474049710668623, 0.6557928638067096, 0.6457795519381762, 0.6207136672455817, 0.62188500771299, 0.6224337590392679, 0.6199534079059958, 0.6202000700868666, 0.6241106127854437, 0.6208496221806854, 0.619803864043206, 0.6226850277744234, 0.6202820732723922, 0.618191690184176, 0.6185160847380757, 0.6157325650565326, 0.6147019187919796, 0.6238796608522534, 0.6191225969232619, 0.6161727218423039, 0.6188092750962824, 0.6219257300253958, 0.6213192481081933, 0.617243560962379, 0.6164883731398731, 0.6144833345897496, 0.6151848959270865, 0.6226598750799894, 0.6772064501419663, 0.6088900349568576, 0.7202644341159612, 0.6473145810887218, 0.6557547301054001, 0.7257214928977191, 0.6340720569714904, 0.7077086558565497, 0.6076391430106014, 0.6149723527487367, 0.6358596549835056, 0.6334481160156429, 0.6258198281284422, 0.6334516541101038, 0.6298554628156126, 0.627501233946532, 0.6291386499069631, 0.6311382800340652, 0.636165204225108, 0.6220789509825408, 0.6228845906443894, 0.6228249529376626, 0.6192699561361223, 0.6215393429156393, 0.623038575751707, 0.62056850688532, 0.6206082212738693, 0.6269776429980993, 0.6197566241025925, 0.6261087849270552, 0.6171639102976769, 0.6264950500335544, 0.6315246359445155, 0.6236401819624007, 0.6160446291323751, 0.6203636080026627, 0.6215765860397369, 0.6241760686971247, 0.6254756341222674, 0.6202722089365125, 0.6260973983444273, 0.621968005085364, 0.6181307558435947, 0.6195643632672727, 0.6211343198083341, 0.6202536390628666, 0.6171426849905401, 0.6198181912768632, 0.6194995953701437, 0.6174510731361806, 0.6229497059248388, 0.6192564480006695, 0.6152778109535575, 0.6197221071925014, 0.6208079031202942, 0.625536508159712, 0.627830897923559, 0.6294201829005033, 0.6218040119856596, 0.626731735188514, 0.6260849349200726, 0.6265449470374733, 0.6233845131937414, 0.6257273710798472, 0.6199078231584281, 0.6293977301102132, 0.6234762859530747, 0.6222423899453133, 0.625642933184281, 0.6172817810438573, 0.616549605038017, 0.6429345014039427, 0.6289410111494362, 0.6238306048326194, 0.6249055089429021, 0.6290170431602746, 0.6174756023101509, 0.6209997981786728, 0.6210647239349782, 0.6238793658558279, 0.6196507499553263, 0.6181138940155506, 0.6226668858435005, 0.6223932940047234, 0.6204220138024539, 0.6219677734188735, 0.6286192191764712, 0.7236192070413381, 0.6359274019487202, 0.688775414833799, 0.6386548429727554, 0.6474641510285437, 0.7089492019731551, 0.6257810790557414, 0.7505287737585604, 0.5922879776917398, 0.6077849147841334, 0.5988616209942847, 0.6007931320928037, 0.6080108170863241, 0.6085350920911878, 0.6139715830795467, 0.6017493528779596, 0.6038718780037016]
Total Epoch List: [72, 68, 94]
Total Time List: [0.18077090312726796, 0.17775630112737417, 0.16586673283018172]
T-times Epoch Time: 0.6097249005108888 ~ 0.011432833153112491
T-times Total Epoch: 71.1111111111111 ~ 5.717635702341603
T-times Total Time: 0.18238115535738567 ~ 0.013628172493977356
T-times Inference Elapsed: 0.17560539261474806 ~ 0.00207051468994179
T-times Time Per Graph: 0.0013648748638383508 ~ 1.6423390035725172e-05
T-times Speed: 738.4376053450775 ~ 11.190513390049045
T-times cross validation test micro f1 score:0.9081417454004289 ~ 0.0032479664815745855
T-times cross validation test precision:0.9621031718243548 ~ 0.006492374203509876
T-times cross validation test recall:0.8616720085470085 ~ 0.013100628007028381
T-times cross validation test f1_score:0.9081417454004289 ~ 0.004970934291860526
