Namespace(seed=15, model='GPSTransformer', dataset='exchange/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 188], edge_attr=[188, 2], x=[48, 14887], y=[1, 1], num_nodes=48)
Data(edge_index=[2, 174], edge_attr=[174, 2], x=[44, 14887], y=[1, 1], num_nodes=48)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda88cf850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.7436;  Loss pred: 1.7436; Loss self: 0.0000; time: 1.25s
Val loss: 0.7255 score: 0.4264 time: 0.52s
Test loss: 0.7266 score: 0.4651 time: 0.26s
Epoch 2/1000, LR 0.000015
Train loss: 1.6711;  Loss pred: 1.6711; Loss self: 0.0000; time: 0.37s
Val loss: 0.6885 score: 0.4574 time: 0.31s
Test loss: 0.6806 score: 0.5581 time: 0.25s
Epoch 3/1000, LR 0.000045
Train loss: 1.4541;  Loss pred: 1.4541; Loss self: 0.0000; time: 0.40s
Val loss: 0.6661 score: 0.5426 time: 0.26s
Test loss: 0.6541 score: 0.5659 time: 0.25s
Epoch 4/1000, LR 0.000075
Train loss: 1.1866;  Loss pred: 1.1866; Loss self: 0.0000; time: 0.42s
Val loss: 0.6550 score: 0.5659 time: 0.24s
Test loss: 0.6386 score: 0.5659 time: 0.25s
Epoch 5/1000, LR 0.000105
Train loss: 0.8790;  Loss pred: 0.8790; Loss self: 0.0000; time: 0.38s
Val loss: 0.6503 score: 0.5814 time: 0.26s
Test loss: 0.6297 score: 0.5891 time: 0.26s
Epoch 6/1000, LR 0.000135
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.39s
Val loss: 0.6479 score: 0.5736 time: 0.25s
Test loss: 0.6302 score: 0.5969 time: 0.25s
Epoch 7/1000, LR 0.000165
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.38s
Val loss: 0.6494 score: 0.5736 time: 0.25s
Test loss: 0.6356 score: 0.5969 time: 0.25s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.4766;  Loss pred: 0.4766; Loss self: 0.0000; time: 0.37s
Val loss: 0.6524 score: 0.6124 time: 0.25s
Test loss: 0.6411 score: 0.6434 time: 0.25s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.38s
Val loss: 0.6533 score: 0.6202 time: 0.27s
Test loss: 0.6446 score: 0.6279 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.3411;  Loss pred: 0.3411; Loss self: 0.0000; time: 0.33s
Val loss: 0.6546 score: 0.6202 time: 0.23s
Test loss: 0.6468 score: 0.6202 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.2898;  Loss pred: 0.2898; Loss self: 0.0000; time: 0.33s
Val loss: 0.6543 score: 0.6357 time: 0.21s
Test loss: 0.6467 score: 0.6357 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.33s
Val loss: 0.6527 score: 0.6279 time: 0.22s
Test loss: 0.6442 score: 0.6357 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.2239;  Loss pred: 0.2239; Loss self: 0.0000; time: 0.32s
Val loss: 0.6509 score: 0.6124 time: 0.21s
Test loss: 0.6391 score: 0.6667 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.2092;  Loss pred: 0.2092; Loss self: 0.0000; time: 0.36s
Val loss: 0.6482 score: 0.7054 time: 0.23s
Test loss: 0.6327 score: 0.6512 time: 0.26s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.1834;  Loss pred: 0.1834; Loss self: 0.0000; time: 0.34s
Val loss: 0.6502 score: 0.6977 time: 0.24s
Test loss: 0.6298 score: 0.7209 time: 0.33s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.31s
Val loss: 0.6578 score: 0.6822 time: 0.22s
Test loss: 0.6327 score: 0.7054 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.1624;  Loss pred: 0.1624; Loss self: 0.0000; time: 0.33s
Val loss: 0.6659 score: 0.6822 time: 0.21s
Test loss: 0.6388 score: 0.6744 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.37s
Val loss: 0.6714 score: 0.6822 time: 0.23s
Test loss: 0.6423 score: 0.6744 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.43s
Val loss: 0.6792 score: 0.6744 time: 0.22s
Test loss: 0.6485 score: 0.6667 time: 0.25s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.32s
Val loss: 0.6860 score: 0.6667 time: 0.21s
Test loss: 0.6536 score: 0.6512 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.44s
Val loss: 0.6865 score: 0.6589 time: 0.30s
Test loss: 0.6516 score: 0.6434 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.33s
Val loss: 0.6854 score: 0.6667 time: 0.31s
Test loss: 0.6465 score: 0.6667 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.31s
Val loss: 0.6812 score: 0.6589 time: 0.21s
Test loss: 0.6396 score: 0.6822 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.29s
Val loss: 0.6788 score: 0.6589 time: 0.20s
Test loss: 0.6348 score: 0.6589 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0960;  Loss pred: 0.0960; Loss self: 0.0000; time: 0.30s
Val loss: 0.6762 score: 0.6589 time: 0.22s
Test loss: 0.6298 score: 0.6589 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0875;  Loss pred: 0.0875; Loss self: 0.0000; time: 0.31s
Val loss: 0.6660 score: 0.6589 time: 0.32s
Test loss: 0.6152 score: 0.6667 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6887,   Val_Loss: 0.6479,   Val_Precision: 0.5421,   Val_Recall: 0.9062,   Val_accuracy: 0.6784,   Val_Score: 0.5736,   Val_Loss: 0.6479,   Test_Precision: 0.5596,   Test_Recall: 0.9385,   Test_accuracy: 0.7011,   Test_Score: 0.5969,   Test_loss: 0.6302


[0.27242274512536824, 0.25756121799349785, 0.2561691338196397, 0.253836358897388, 0.26785339205525815, 0.25946853985078633, 0.2589219370856881, 0.2593787021469325, 0.22190398490056396, 0.26328471791930497, 0.21723285992629826, 0.22316992096602917, 0.21401724591851234, 0.26998375286348164, 0.33641365985386074, 0.22522922698408365, 0.21691164397634566, 0.21061254502274096, 0.25640793493948877, 0.22071818984113634, 0.21305282320827246, 0.21906174416653812, 0.20899241603910923, 0.21148687694221735, 0.21010892908088863, 0.20666628307662904]
[0.0021118042257780483, 0.0019965985890968824, 0.0019858072389119354, 0.0019677237123828526, 0.002076382884149288, 0.0020113840298510567, 0.0020071467991138615, 0.0020106876135421122, 0.001720185929461736, 0.0020409668055760073, 0.0016839756583433975, 0.0017299993873335594, 0.001659048417972964, 0.0020928973090192375, 0.0026078578283245018, 0.0017459629998766175, 0.0016814856122197338, 0.0016326553877731857, 0.001987658410383634, 0.0017109937196987312, 0.0016515722729323447, 0.001698153055554559, 0.001620096248365188, 0.0016394331545908321, 0.001628751388223943, 0.0016020642098963493]
[473.5287427657134, 500.85180138904536, 503.57354953944093, 508.20142772433786, 481.60674393620263, 497.1701003681779, 498.2196620802682, 497.34229885584153, 581.3325076510249, 489.9638726450414, 593.8328116831243, 578.0348867876165, 602.75516324099, 477.80652958487235, 383.4564864459964, 572.7498234903417, 594.7121954138503, 612.4991271819596, 503.104554975818, 584.455681214352, 605.4836451235121, 588.8750703177541, 617.2472783694693, 609.9669249702217, 613.9672433927691, 624.1947069429248]
Elapsed: 0.2396487224076946~0.030024476831085584
Time per graph: 0.0018577420341681754~0.00023274788241151614
Speed: 545.9589552342565~61.95623613484192
Total Time: 0.2071
best val loss: 0.6479340653086818 test_score: 0.5969

Testing...
Test loss: 0.6327 score: 0.6512 time: 0.20s
test Score 0.6512
Epoch Time List: [2.0323011099826545, 0.9299877700395882, 0.9119203230366111, 0.91039874474518, 0.9014273907523602, 0.8875278199557215, 0.8839556358288974, 0.8733970378525555, 0.8683556928299367, 0.8128523188643157, 0.7530937578994781, 0.7656967688817531, 0.7427907718811184, 0.8493299859110266, 0.9073417368344963, 0.7424444740172476, 0.7549234430771321, 0.797627632971853, 0.8976777328643948, 0.7452014707960188, 0.941228544106707, 0.8531863291282207, 0.721020613797009, 0.6944866222329438, 0.7253013080917299, 0.8331563668325543]
Total Epoch List: [26]
Total Time List: [0.2070711450651288]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda88cf4f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1064;  Loss pred: 1.1064; Loss self: 0.0000; time: 0.40s
Val loss: 0.7921 score: 0.4961 time: 0.38s
Test loss: 0.7418 score: 0.6047 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 1.0754;  Loss pred: 1.0754; Loss self: 0.0000; time: 0.30s
Val loss: 0.7605 score: 0.4729 time: 0.22s
Test loss: 0.7186 score: 0.5891 time: 0.21s
Epoch 3/1000, LR 0.000045
Train loss: 0.8782;  Loss pred: 0.8782; Loss self: 0.0000; time: 0.30s
Val loss: 0.7662 score: 0.4496 time: 0.22s
Test loss: 0.7293 score: 0.5116 time: 0.40s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6305;  Loss pred: 0.6305; Loss self: 0.0000; time: 0.33s
Val loss: 0.7336 score: 0.4961 time: 0.22s
Test loss: 0.7133 score: 0.5039 time: 0.22s
Epoch 5/1000, LR 0.000105
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.31s
Val loss: 0.7015 score: 0.5814 time: 0.22s
Test loss: 0.6843 score: 0.5271 time: 0.22s
Epoch 6/1000, LR 0.000135
Train loss: 0.4011;  Loss pred: 0.4011; Loss self: 0.0000; time: 0.32s
Val loss: 0.6833 score: 0.5814 time: 0.23s
Test loss: 0.6590 score: 0.5891 time: 0.22s
Epoch 7/1000, LR 0.000165
Train loss: 0.3161;  Loss pred: 0.3161; Loss self: 0.0000; time: 0.32s
Val loss: 0.6760 score: 0.5736 time: 0.33s
Test loss: 0.6501 score: 0.6202 time: 0.22s
Epoch 8/1000, LR 0.000195
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 0.32s
Val loss: 0.6796 score: 0.5736 time: 0.22s
Test loss: 0.6510 score: 0.6047 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.2354;  Loss pred: 0.2354; Loss self: 0.0000; time: 0.32s
Val loss: 0.6830 score: 0.5736 time: 0.23s
Test loss: 0.6508 score: 0.6202 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.2060;  Loss pred: 0.2060; Loss self: 0.0000; time: 0.32s
Val loss: 0.6804 score: 0.5969 time: 0.23s
Test loss: 0.6479 score: 0.6667 time: 0.38s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.34s
Val loss: 0.6765 score: 0.5116 time: 0.25s
Test loss: 0.6487 score: 0.6124 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.1590;  Loss pred: 0.1590; Loss self: 0.0000; time: 0.32s
Val loss: 0.6763 score: 0.5116 time: 0.22s
Test loss: 0.6502 score: 0.5814 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 0.33s
Val loss: 0.6712 score: 0.4884 time: 0.23s
Test loss: 0.6462 score: 0.5659 time: 0.22s
Epoch 14/1000, LR 0.000285
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.52s
Val loss: 0.6569 score: 0.5116 time: 0.37s
Test loss: 0.6347 score: 0.5659 time: 0.25s
Epoch 15/1000, LR 0.000285
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.32s
Val loss: 0.6384 score: 0.5659 time: 0.25s
Test loss: 0.6203 score: 0.5969 time: 0.22s
Epoch 16/1000, LR 0.000285
Train loss: 0.1039;  Loss pred: 0.1039; Loss self: 0.0000; time: 0.34s
Val loss: 0.6233 score: 0.5581 time: 0.21s
Test loss: 0.6098 score: 0.5814 time: 0.20s
Epoch 17/1000, LR 0.000285
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.31s
Val loss: 0.6141 score: 0.5736 time: 0.23s
Test loss: 0.6031 score: 0.5891 time: 0.23s
Epoch 18/1000, LR 0.000285
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.32s
Val loss: 0.6075 score: 0.5349 time: 0.35s
Test loss: 0.5986 score: 0.5736 time: 0.28s
Epoch 19/1000, LR 0.000285
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.44s
Val loss: 0.6036 score: 0.5194 time: 0.27s
Test loss: 0.5968 score: 0.5659 time: 0.23s
Epoch 20/1000, LR 0.000285
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.34s
Val loss: 0.5990 score: 0.5426 time: 0.27s
Test loss: 0.5914 score: 0.5736 time: 0.26s
Epoch 21/1000, LR 0.000285
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.51s
Val loss: 0.5933 score: 0.5504 time: 0.22s
Test loss: 0.5865 score: 0.5736 time: 0.23s
Epoch 22/1000, LR 0.000285
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.31s
Val loss: 0.5894 score: 0.5581 time: 0.22s
Test loss: 0.5826 score: 0.5814 time: 0.21s
Epoch 23/1000, LR 0.000285
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.31s
Val loss: 0.5846 score: 0.5659 time: 0.22s
Test loss: 0.5772 score: 0.5814 time: 0.21s
Epoch 24/1000, LR 0.000285
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.32s
Val loss: 0.5813 score: 0.5736 time: 0.36s
Test loss: 0.5736 score: 0.5736 time: 0.22s
Epoch 25/1000, LR 0.000285
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.30s
Val loss: 0.5729 score: 0.5814 time: 0.21s
Test loss: 0.5658 score: 0.5659 time: 0.22s
Epoch 26/1000, LR 0.000285
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.31s
Val loss: 0.5458 score: 0.6047 time: 0.24s
Test loss: 0.5397 score: 0.6124 time: 0.23s
Epoch 27/1000, LR 0.000285
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.37s
Val loss: 0.5131 score: 0.6822 time: 0.37s
Test loss: 0.5098 score: 0.6434 time: 0.22s
Epoch 28/1000, LR 0.000285
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.31s
Val loss: 0.4870 score: 0.7287 time: 0.22s
Test loss: 0.4881 score: 0.6977 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.34s
Val loss: 0.4883 score: 0.7287 time: 0.22s
Test loss: 0.4902 score: 0.7829 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.31s
Val loss: 0.4624 score: 0.7752 time: 0.22s
Test loss: 0.4695 score: 0.8295 time: 0.22s
Epoch 31/1000, LR 0.000285
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.33s
Val loss: 0.4257 score: 0.8450 time: 0.31s
Test loss: 0.4403 score: 0.8837 time: 0.24s
Epoch 32/1000, LR 0.000285
Train loss: 0.0268;  Loss pred: 0.0268; Loss self: 0.0000; time: 0.33s
Val loss: 0.4048 score: 0.8527 time: 0.25s
Test loss: 0.4282 score: 0.8915 time: 0.22s
Epoch 33/1000, LR 0.000285
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.34s
Val loss: 0.4990 score: 0.8140 time: 0.22s
Test loss: 0.4987 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.32s
Val loss: 0.5811 score: 0.7674 time: 0.35s
Test loss: 0.5677 score: 0.8217 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.31s
Val loss: 0.4358 score: 0.8372 time: 0.21s
Test loss: 0.4534 score: 0.8837 time: 7.64s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 4.06s
Val loss: 0.3236 score: 0.8992 time: 1.76s
Test loss: 0.3722 score: 0.9302 time: 0.22s
Epoch 37/1000, LR 0.000285
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.31s
Val loss: 0.2993 score: 0.9070 time: 0.21s
Test loss: 0.3599 score: 0.9147 time: 0.20s
Epoch 38/1000, LR 0.000284
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.32s
Val loss: 0.3884 score: 0.8217 time: 0.20s
Test loss: 0.4341 score: 0.8372 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.35s
Val loss: 0.4959 score: 0.7597 time: 0.21s
Test loss: 0.5171 score: 0.7829 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.30s
Val loss: 0.4868 score: 0.7829 time: 0.20s
Test loss: 0.5048 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.30s
Val loss: 0.2984 score: 0.8837 time: 0.21s
Test loss: 0.3738 score: 0.8760 time: 0.21s
Epoch 42/1000, LR 0.000284
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.31s
Val loss: 0.2663 score: 0.9147 time: 0.20s
Test loss: 0.3591 score: 0.9147 time: 0.33s
Epoch 43/1000, LR 0.000284
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.31s
Val loss: 0.2596 score: 0.9225 time: 0.22s
Test loss: 0.3599 score: 0.9147 time: 0.21s
Epoch 44/1000, LR 0.000284
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.29s
Val loss: 0.3077 score: 0.8915 time: 0.21s
Test loss: 0.3921 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.31s
Val loss: 0.3511 score: 0.8837 time: 0.22s
Test loss: 0.4234 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.47s
Val loss: 0.4093 score: 0.8605 time: 0.26s
Test loss: 0.4825 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.32s
Val loss: 0.3904 score: 0.8682 time: 0.22s
Test loss: 0.4764 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.33s
Val loss: 0.3458 score: 0.8992 time: 0.22s
Test loss: 0.4480 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.34s
Val loss: 0.2945 score: 0.9147 time: 0.26s
Test loss: 0.3966 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.31s
Val loss: 0.2558 score: 0.9302 time: 0.21s
Test loss: 0.3957 score: 0.9380 time: 0.23s
Epoch 51/1000, LR 0.000284
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.32s
Val loss: 0.2651 score: 0.9147 time: 0.26s
Test loss: 0.4199 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.33s
Val loss: 0.2999 score: 0.9225 time: 0.34s
Test loss: 0.4438 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.35s
Val loss: 0.3050 score: 0.9225 time: 0.23s
Test loss: 0.4540 score: 0.9302 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.37s
Val loss: 0.3135 score: 0.9225 time: 0.61s
Test loss: 0.4665 score: 0.9302 time: 0.28s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.44s
Val loss: 0.3751 score: 0.9070 time: 0.27s
Test loss: 0.5356 score: 0.9225 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.35s
Val loss: 0.4509 score: 0.8992 time: 0.24s
Test loss: 0.6029 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.34s
Val loss: 0.6323 score: 0.8915 time: 0.21s
Test loss: 0.7987 score: 0.8682 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.35s
Val loss: 0.7503 score: 0.8760 time: 0.21s
Test loss: 0.9431 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.31s
Val loss: 0.7528 score: 0.8760 time: 0.23s
Test loss: 0.9584 score: 0.8527 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.35s
Val loss: 0.7701 score: 0.8605 time: 0.27s
Test loss: 0.9805 score: 0.8450 time: 0.24s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.38s
Val loss: 0.7552 score: 0.8605 time: 0.24s
Test loss: 0.9756 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.34s
Val loss: 0.7950 score: 0.8527 time: 0.22s
Test loss: 1.0076 score: 0.8295 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.36s
Val loss: 0.8302 score: 0.8605 time: 0.25s
Test loss: 1.0340 score: 0.8372 time: 0.23s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.35s
Val loss: 0.7708 score: 0.8682 time: 0.22s
Test loss: 0.9931 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.32s
Val loss: 0.8001 score: 0.8682 time: 0.23s
Test loss: 1.0387 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.34s
Val loss: 0.7761 score: 0.8760 time: 0.26s
Test loss: 1.0214 score: 0.8450 time: 0.23s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.35s
Val loss: 0.6700 score: 0.8837 time: 0.35s
Test loss: 0.8748 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.33s
Val loss: 0.5406 score: 0.8992 time: 0.22s
Test loss: 0.7188 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.30s
Val loss: 0.4465 score: 0.9070 time: 0.22s
Test loss: 0.6697 score: 0.9070 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.33s
Val loss: 0.3893 score: 0.9147 time: 0.36s
Test loss: 0.6538 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0115,   Val_Loss: 0.2558,   Val_Precision: 0.9242,   Val_Recall: 0.9385,   Val_accuracy: 0.9313,   Val_Score: 0.9302,   Val_Loss: 0.2558,   Test_Precision: 0.9516,   Test_Recall: 0.9219,   Test_accuracy: 0.9365,   Test_Score: 0.9380,   Test_loss: 0.3957


[0.27242274512536824, 0.25756121799349785, 0.2561691338196397, 0.253836358897388, 0.26785339205525815, 0.25946853985078633, 0.2589219370856881, 0.2593787021469325, 0.22190398490056396, 0.26328471791930497, 0.21723285992629826, 0.22316992096602917, 0.21401724591851234, 0.26998375286348164, 0.33641365985386074, 0.22522922698408365, 0.21691164397634566, 0.21061254502274096, 0.25640793493948877, 0.22071818984113634, 0.21305282320827246, 0.21906174416653812, 0.20899241603910923, 0.21148687694221735, 0.21010892908088863, 0.20666628307662904, 0.22045873291790485, 0.21669109910726547, 0.40310830110684037, 0.22601828491315246, 0.22541654692031443, 0.221641510957852, 0.22713258606381714, 0.2236248068511486, 0.23044914891943336, 0.38933639507740736, 0.2246322468854487, 0.2237163761164993, 0.22328511299565434, 0.2581586940214038, 0.22146765887737274, 0.20649052411317825, 0.23318072990514338, 0.2829320461023599, 0.23613937408663332, 0.26446269918233156, 0.23412431799806654, 0.21540508698672056, 0.21880221692845225, 0.22325515397824347, 0.22373816487379372, 0.23788026208058, 0.22469548811204731, 0.2262164109852165, 0.21957886894233525, 0.22876080381684005, 0.241299401037395, 0.22698229411616921, 0.22289609187282622, 0.20419163210317492, 7.644421289907768, 0.22195874503813684, 0.20613672002218664, 0.20188080798834562, 0.20117902988567948, 0.20404873904772103, 0.22167517291381955, 0.3380167910363525, 0.21610942482948303, 0.21703130891546607, 0.22277365997433662, 0.22211842285469174, 0.22257919004186988, 0.21216077590361238, 0.2270398149266839, 0.23653898504562676, 0.22643706109374762, 0.22094096196815372, 0.2561359938699752, 0.28357339394278824, 0.24548080400563776, 0.2288933340460062, 0.21438812697306275, 0.23352345498278737, 0.23529727198183537, 0.24526579887606204, 0.21689535304903984, 0.2522525880485773, 0.23923931503668427, 0.2260641900356859, 0.22725038486532867, 0.23402117704972625, 0.2229472459293902, 0.22128501790575683, 0.20842769788578153, 0.2248064959421754]
[0.0021118042257780483, 0.0019965985890968824, 0.0019858072389119354, 0.0019677237123828526, 0.002076382884149288, 0.0020113840298510567, 0.0020071467991138615, 0.0020106876135421122, 0.001720185929461736, 0.0020409668055760073, 0.0016839756583433975, 0.0017299993873335594, 0.001659048417972964, 0.0020928973090192375, 0.0026078578283245018, 0.0017459629998766175, 0.0016814856122197338, 0.0016326553877731857, 0.001987658410383634, 0.0017109937196987312, 0.0016515722729323447, 0.001698153055554559, 0.001620096248365188, 0.0016394331545908321, 0.001628751388223943, 0.0016020642098963493, 0.0017089824257201928, 0.0016797759620718253, 0.003124870551215817, 0.0017520797280089339, 0.0017474150924055383, 0.001718151247735287, 0.001760717721424939, 0.001733525634505028, 0.0017864275110033594, 0.003018111589747344, 0.001741335247174021, 0.0017342354737713125, 0.001730892348803522, 0.0020012301862124323, 0.001716803557188936, 0.0016007017373114592, 0.0018076025574042121, 0.002193271675212092, 0.0018305377836173127, 0.0020500984432738883, 0.0018149171937834616, 0.001669806875866051, 0.0016961412164996298, 0.0017306601083584765, 0.001734404378866618, 0.001844033039384341, 0.0017418254892406768, 0.001753615589032686, 0.001702161774746785, 0.0017733395644716284, 0.0018705379925379457, 0.0017595526675672033, 0.0017278766811846993, 0.0015828808690168598, 0.05925907976672689, 0.0017206104266522236, 0.0015979590699394313, 0.001564967503785625, 0.0015595273634548797, 0.001581773170912566, 0.0017184121931303842, 0.00262028520183219, 0.0016752668591432792, 0.0016824132474067138, 0.0017269275967002839, 0.001721848239183657, 0.0017254200778439525, 0.0016446571775473827, 0.001759998565323131, 0.0018336355429893548, 0.0017553260549902916, 0.0017127206354120442, 0.0019855503400773274, 0.0021982433638975834, 0.0019029519690359517, 0.0017743669305891954, 0.0016619234649074633, 0.0018102593409518402, 0.0018240098603243052, 0.001901285262605132, 0.001681359325961549, 0.0019554464189812195, 0.0018545683336177075, 0.0017524355816719837, 0.0017616308904289044, 0.0018141176515482655, 0.0017282732242588387, 0.0017153877357035413, 0.0016157185882618723, 0.0017426860150556232]
[473.5287427657134, 500.85180138904536, 503.57354953944093, 508.20142772433786, 481.60674393620263, 497.1701003681779, 498.2196620802682, 497.34229885584153, 581.3325076510249, 489.9638726450414, 593.8328116831243, 578.0348867876165, 602.75516324099, 477.80652958487235, 383.4564864459964, 572.7498234903417, 594.7121954138503, 612.4991271819596, 503.104554975818, 584.455681214352, 605.4836451235121, 588.8750703177541, 617.2472783694693, 609.9669249702217, 613.9672433927691, 624.1947069429248, 585.1435245617484, 595.3174843427371, 320.01325610461606, 570.7502826577428, 572.2738714722747, 582.020937515315, 567.9502102078609, 576.8590784557559, 559.7764218478382, 331.3330108128021, 574.2719568922071, 576.6229644843872, 577.73668055743, 499.69264250037105, 582.4778238678526, 624.7260040334568, 553.2189561825134, 455.93986887342567, 546.2875494565903, 487.7814542422939, 550.9893252569574, 598.8716506400458, 589.5735509946074, 577.8142080991834, 576.5668100154766, 542.2896329091075, 574.1103263082545, 570.2504050797194, 587.488225171054, 563.9077929770054, 534.6055541182567, 568.326267484009, 578.7450058729661, 631.7594833407192, 16.87505111345798, 581.1890852862557, 625.7982565460227, 638.9909040162304, 641.2199127975942, 632.2018974585806, 581.9325561106078, 381.6378458729481, 596.9198247683319, 594.3842878920555, 579.0630724245439, 580.7712766103641, 579.5690063196542, 608.0294505455925, 568.1822813397594, 545.3646466569423, 569.6947283138977, 583.866381547637, 503.6387039983358, 454.90868591863074, 525.4993380135636, 563.581287928952, 601.7124260627011, 552.4070376978124, 548.2426503013543, 525.9600017252565, 594.7568640201952, 511.3921763813894, 539.2090341849521, 570.634384772026, 567.655804307865, 551.2321646540104, 578.6122159179114, 582.9585808422867, 618.9196604315615, 573.8268347600655]
Elapsed: 0.3126648999726361~0.7529644516540991
Time per graph: 0.002423758914516559~0.005836933733752706
Speed: 550.5339934475686~80.98619285599565
Total Time: 0.2255
best val loss: 0.25582343356886 test_score: 0.9380

Testing...
Test loss: 0.3957 score: 0.9380 time: 0.23s
test Score 0.9380
Epoch Time List: [2.0323011099826545, 0.9299877700395882, 0.9119203230366111, 0.91039874474518, 0.9014273907523602, 0.8875278199557215, 0.8839556358288974, 0.8733970378525555, 0.8683556928299367, 0.8128523188643157, 0.7530937578994781, 0.7656967688817531, 0.7427907718811184, 0.8493299859110266, 0.9073417368344963, 0.7424444740172476, 0.7549234430771321, 0.797627632971853, 0.8976777328643948, 0.7452014707960188, 0.941228544106707, 0.8531863291282207, 0.721020613797009, 0.6944866222329438, 0.7253013080917299, 0.8331563668325543, 0.994717386784032, 0.7292981371283531, 0.9205382321961224, 0.7718512022402138, 0.7477948903106153, 0.762831524014473, 0.8723282720893621, 0.7658566529862583, 0.7706198419909924, 0.9374788128770888, 0.8164192417170852, 0.7555926681961864, 0.776078175753355, 1.1445298150647432, 0.7894083689898252, 0.7453201108146459, 0.7709815020207316, 0.9469973968807608, 0.938990059774369, 0.8624746771529317, 0.9531015842221677, 0.7466629657428712, 0.7392747648991644, 0.8946361993439496, 0.730068864999339, 0.7809813690837473, 0.9566236671525985, 0.7545840907841921, 0.7719112581107765, 0.7631265060044825, 0.8743038561660796, 0.8063396450597793, 0.7767401561141014, 0.8713087379001081, 8.165334895718843, 6.035003174794838, 0.7147473979275674, 0.7171353287994862, 0.7628885039594024, 0.6989228541497141, 0.7253499520011246, 0.8463503569364548, 0.7404154960531741, 0.7160563159268349, 0.7476406490895897, 0.9440492107532918, 0.7567572470288724, 0.7557674727868289, 0.817480118945241, 0.7549518218729645, 0.7960529362317175, 0.8825275467243046, 0.8336890439968556, 1.2617106980178505, 0.9450999901164323, 0.8133851978927851, 0.7641775791998953, 0.7957948429975659, 0.7683739967178553, 0.8658697260543704, 0.8271942059509456, 0.7955220539588481, 0.848821999039501, 0.782620359910652, 0.7730932901613414, 0.8315759657416493, 0.912630002014339, 0.7690659309737384, 0.7223448492586613, 0.9100594501942396]
Total Epoch List: [26, 70]
Total Time List: [0.2070711450651288, 0.22545717284083366]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bdaa2ec1c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.4574;  Loss pred: 1.4574; Loss self: 0.0000; time: 0.40s
Val loss: 0.5582 score: 0.7209 time: 0.21s
Test loss: 0.6575 score: 0.6562 time: 0.20s
Epoch 2/1000, LR 0.000020
Train loss: 1.2934;  Loss pred: 1.2934; Loss self: 0.0000; time: 0.53s
Val loss: 0.5823 score: 0.6744 time: 0.20s
Test loss: 0.6378 score: 0.6250 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 1.0202;  Loss pred: 1.0202; Loss self: 0.0000; time: 0.41s
Val loss: 0.6467 score: 0.6667 time: 0.20s
Test loss: 0.6607 score: 0.6094 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.7217;  Loss pred: 0.7217; Loss self: 0.0000; time: 0.40s
Val loss: 0.7613 score: 0.6202 time: 0.20s
Test loss: 0.7417 score: 0.5703 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.4865;  Loss pred: 0.4865; Loss self: 0.0000; time: 0.40s
Val loss: 0.8514 score: 0.6047 time: 0.33s
Test loss: 0.8201 score: 0.5859 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.3820;  Loss pred: 0.3820; Loss self: 0.0000; time: 0.40s
Val loss: 0.8843 score: 0.6047 time: 0.22s
Test loss: 0.8513 score: 0.5781 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 0.41s
Val loss: 0.7741 score: 0.6124 time: 0.21s
Test loss: 0.7592 score: 0.5859 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.2154;  Loss pred: 0.2154; Loss self: 0.0000; time: 0.55s
Val loss: 0.8620 score: 0.5659 time: 0.19s
Test loss: 0.8290 score: 0.5703 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 0.42s
Val loss: 0.8179 score: 0.5814 time: 0.21s
Test loss: 0.7886 score: 0.5781 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.40s
Val loss: 0.8022 score: 0.6047 time: 0.20s
Test loss: 0.7765 score: 0.5781 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.1300;  Loss pred: 0.1300; Loss self: 0.0000; time: 0.52s
Val loss: 0.7254 score: 0.6434 time: 0.19s
Test loss: 0.7157 score: 0.5938 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.41s
Val loss: 0.7518 score: 0.6357 time: 0.19s
Test loss: 0.7392 score: 0.6016 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.1098;  Loss pred: 0.1098; Loss self: 0.0000; time: 0.39s
Val loss: 0.7239 score: 0.6434 time: 0.20s
Test loss: 0.7151 score: 0.6328 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.38s
Val loss: 0.6381 score: 0.7132 time: 0.36s
Test loss: 0.6468 score: 0.6562 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.42s
Val loss: 0.5880 score: 0.7442 time: 0.22s
Test loss: 0.6097 score: 0.6875 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.0687;  Loss pred: 0.0687; Loss self: 0.0000; time: 0.41s
Val loss: 0.6111 score: 0.7287 time: 0.20s
Test loss: 0.6242 score: 0.6719 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.40s
Val loss: 0.6063 score: 0.7287 time: 0.20s
Test loss: 0.6209 score: 0.6719 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.41s
Val loss: 0.5152 score: 0.7829 time: 0.19s
Test loss: 0.5336 score: 0.7266 time: 0.19s
Epoch 19/1000, LR 0.000290
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.38s
Val loss: 0.4454 score: 0.8605 time: 0.22s
Test loss: 0.4692 score: 0.7891 time: 0.18s
Epoch 20/1000, LR 0.000290
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.39s
Val loss: 0.4126 score: 0.8682 time: 0.20s
Test loss: 0.4315 score: 0.8359 time: 0.19s
Epoch 21/1000, LR 0.000290
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.38s
Val loss: 0.4207 score: 0.8450 time: 0.19s
Test loss: 0.4325 score: 0.8047 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000290
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.41s
Val loss: 0.4119 score: 0.8527 time: 0.20s
Test loss: 0.4220 score: 0.8203 time: 0.20s
Epoch 23/1000, LR 0.000290
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.39s
Val loss: 0.4380 score: 0.8140 time: 0.20s
Test loss: 0.4608 score: 0.7891 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000290
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.40s
Val loss: 0.3906 score: 0.8682 time: 0.19s
Test loss: 0.4277 score: 0.8047 time: 0.18s
Epoch 25/1000, LR 0.000290
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.40s
Val loss: 0.3602 score: 0.8837 time: 0.18s
Test loss: 0.3863 score: 0.8438 time: 0.19s
Epoch 26/1000, LR 0.000290
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.39s
Val loss: 0.3459 score: 0.8837 time: 0.20s
Test loss: 0.3558 score: 0.8438 time: 0.20s
Epoch 27/1000, LR 0.000290
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.40s
Val loss: 0.3280 score: 0.8915 time: 0.23s
Test loss: 0.3568 score: 0.8594 time: 0.20s
Epoch 28/1000, LR 0.000290
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.37s
Val loss: 0.3342 score: 0.8760 time: 0.20s
Test loss: 0.3686 score: 0.8438 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.43s
Val loss: 0.3165 score: 0.8992 time: 0.26s
Test loss: 0.3288 score: 0.8594 time: 0.20s
Epoch 30/1000, LR 0.000290
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.41s
Val loss: 0.3120 score: 0.8760 time: 0.20s
Test loss: 0.3550 score: 0.8359 time: 0.20s
Epoch 31/1000, LR 0.000290
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.40s
Val loss: 0.2959 score: 0.9070 time: 0.20s
Test loss: 0.3162 score: 0.8672 time: 0.20s
Epoch 32/1000, LR 0.000290
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.38s
Val loss: 0.2992 score: 0.9225 time: 0.19s
Test loss: 0.2764 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.39s
Val loss: 0.3356 score: 0.9070 time: 0.19s
Test loss: 0.2847 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.37s
Val loss: 0.3923 score: 0.8915 time: 0.19s
Test loss: 0.3018 score: 0.9062 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.38s
Val loss: 0.4540 score: 0.8837 time: 0.18s
Test loss: 0.3595 score: 0.8750 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.50s
Val loss: 0.3491 score: 0.9147 time: 0.19s
Test loss: 0.2959 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.38s
Val loss: 0.3649 score: 0.9147 time: 0.18s
Test loss: 0.3038 score: 0.9062 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.38s
Val loss: 0.3928 score: 0.9070 time: 0.19s
Test loss: 0.3303 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.41s
Val loss: 0.3765 score: 0.9147 time: 0.20s
Test loss: 0.3125 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.39s
Val loss: 0.3719 score: 0.9147 time: 0.20s
Test loss: 0.3071 score: 0.9219 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.38s
Val loss: 0.3405 score: 0.9225 time: 0.19s
Test loss: 0.2916 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.38s
Val loss: 0.3283 score: 0.9225 time: 0.31s
Test loss: 0.2947 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.38s
Val loss: 0.3017 score: 0.9457 time: 0.21s
Test loss: 0.3136 score: 0.8750 time: 0.23s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.39s
Val loss: 0.3025 score: 0.9302 time: 0.22s
Test loss: 0.2782 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.41s
Val loss: 0.3296 score: 0.9302 time: 0.33s
Test loss: 0.2860 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.38s
Val loss: 0.3690 score: 0.9225 time: 0.20s
Test loss: 0.3091 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.40s
Val loss: 0.4098 score: 0.9147 time: 0.23s
Test loss: 0.3594 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.41s
Val loss: 0.4188 score: 0.9147 time: 0.28s
Test loss: 0.3751 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.40s
Val loss: 0.4657 score: 0.8992 time: 0.23s
Test loss: 0.4144 score: 0.9062 time: 0.22s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.42s
Val loss: 0.3544 score: 0.9225 time: 0.22s
Test loss: 0.3886 score: 0.8828 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.38s
Val loss: 0.3710 score: 0.9302 time: 0.31s
Test loss: 0.3481 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.0197,   Val_Loss: 0.2959,   Val_Precision: 0.9649,   Val_Recall: 0.8462,   Val_accuracy: 0.9016,   Val_Score: 0.9070,   Val_Loss: 0.2959,   Test_Precision: 0.8852,   Test_Recall: 0.8438,   Test_accuracy: 0.8640,   Test_Score: 0.8672,   Test_loss: 0.3162


[0.27242274512536824, 0.25756121799349785, 0.2561691338196397, 0.253836358897388, 0.26785339205525815, 0.25946853985078633, 0.2589219370856881, 0.2593787021469325, 0.22190398490056396, 0.26328471791930497, 0.21723285992629826, 0.22316992096602917, 0.21401724591851234, 0.26998375286348164, 0.33641365985386074, 0.22522922698408365, 0.21691164397634566, 0.21061254502274096, 0.25640793493948877, 0.22071818984113634, 0.21305282320827246, 0.21906174416653812, 0.20899241603910923, 0.21148687694221735, 0.21010892908088863, 0.20666628307662904, 0.22045873291790485, 0.21669109910726547, 0.40310830110684037, 0.22601828491315246, 0.22541654692031443, 0.221641510957852, 0.22713258606381714, 0.2236248068511486, 0.23044914891943336, 0.38933639507740736, 0.2246322468854487, 0.2237163761164993, 0.22328511299565434, 0.2581586940214038, 0.22146765887737274, 0.20649052411317825, 0.23318072990514338, 0.2829320461023599, 0.23613937408663332, 0.26446269918233156, 0.23412431799806654, 0.21540508698672056, 0.21880221692845225, 0.22325515397824347, 0.22373816487379372, 0.23788026208058, 0.22469548811204731, 0.2262164109852165, 0.21957886894233525, 0.22876080381684005, 0.241299401037395, 0.22698229411616921, 0.22289609187282622, 0.20419163210317492, 7.644421289907768, 0.22195874503813684, 0.20613672002218664, 0.20188080798834562, 0.20117902988567948, 0.20404873904772103, 0.22167517291381955, 0.3380167910363525, 0.21610942482948303, 0.21703130891546607, 0.22277365997433662, 0.22211842285469174, 0.22257919004186988, 0.21216077590361238, 0.2270398149266839, 0.23653898504562676, 0.22643706109374762, 0.22094096196815372, 0.2561359938699752, 0.28357339394278824, 0.24548080400563776, 0.2288933340460062, 0.21438812697306275, 0.23352345498278737, 0.23529727198183537, 0.24526579887606204, 0.21689535304903984, 0.2522525880485773, 0.23923931503668427, 0.2260641900356859, 0.22725038486532867, 0.23402117704972625, 0.2229472459293902, 0.22128501790575683, 0.20842769788578153, 0.2248064959421754, 0.20782114006578922, 0.20622015302069485, 0.2041568080894649, 0.20008356706239283, 0.21319578704424202, 0.24625112488865852, 0.19934541289694607, 0.19827577215619385, 0.2101796690840274, 0.2051792349666357, 0.20075288019143045, 0.20092129008844495, 0.20529519906267524, 0.21936060907319188, 0.21884376602247357, 0.20895123900845647, 0.194508251035586, 0.19777272595092654, 0.19000819488428533, 0.1959478750359267, 0.22837821999564767, 0.20583338616415858, 0.206902583129704, 0.18916648300364614, 0.19284533290192485, 0.20186443510465324, 0.20800748490728438, 0.218197925016284, 0.20297081489115953, 0.20535560604184866, 0.2032729850616306, 0.19236816186457872, 0.19627907196991146, 0.19198215496726334, 0.19203223916701972, 0.21981887612491846, 0.18860372388735414, 0.1966786307748407, 0.20960366213694215, 0.2233010099735111, 0.2032901500351727, 0.20304923807270825, 0.23382108891382813, 0.2191788398195058, 0.21744985692203045, 0.210637103067711, 0.20489013101905584, 0.20713462796993554, 0.2230698401108384, 0.21878956397995353, 0.19572507007978857]
[0.0021118042257780483, 0.0019965985890968824, 0.0019858072389119354, 0.0019677237123828526, 0.002076382884149288, 0.0020113840298510567, 0.0020071467991138615, 0.0020106876135421122, 0.001720185929461736, 0.0020409668055760073, 0.0016839756583433975, 0.0017299993873335594, 0.001659048417972964, 0.0020928973090192375, 0.0026078578283245018, 0.0017459629998766175, 0.0016814856122197338, 0.0016326553877731857, 0.001987658410383634, 0.0017109937196987312, 0.0016515722729323447, 0.001698153055554559, 0.001620096248365188, 0.0016394331545908321, 0.001628751388223943, 0.0016020642098963493, 0.0017089824257201928, 0.0016797759620718253, 0.003124870551215817, 0.0017520797280089339, 0.0017474150924055383, 0.001718151247735287, 0.001760717721424939, 0.001733525634505028, 0.0017864275110033594, 0.003018111589747344, 0.001741335247174021, 0.0017342354737713125, 0.001730892348803522, 0.0020012301862124323, 0.001716803557188936, 0.0016007017373114592, 0.0018076025574042121, 0.002193271675212092, 0.0018305377836173127, 0.0020500984432738883, 0.0018149171937834616, 0.001669806875866051, 0.0016961412164996298, 0.0017306601083584765, 0.001734404378866618, 0.001844033039384341, 0.0017418254892406768, 0.001753615589032686, 0.001702161774746785, 0.0017733395644716284, 0.0018705379925379457, 0.0017595526675672033, 0.0017278766811846993, 0.0015828808690168598, 0.05925907976672689, 0.0017206104266522236, 0.0015979590699394313, 0.001564967503785625, 0.0015595273634548797, 0.001581773170912566, 0.0017184121931303842, 0.00262028520183219, 0.0016752668591432792, 0.0016824132474067138, 0.0017269275967002839, 0.001721848239183657, 0.0017254200778439525, 0.0016446571775473827, 0.001759998565323131, 0.0018336355429893548, 0.0017553260549902916, 0.0017127206354120442, 0.0019855503400773274, 0.0021982433638975834, 0.0019029519690359517, 0.0017743669305891954, 0.0016619234649074633, 0.0018102593409518402, 0.0018240098603243052, 0.001901285262605132, 0.001681359325961549, 0.0019554464189812195, 0.0018545683336177075, 0.0017524355816719837, 0.0017616308904289044, 0.0018141176515482655, 0.0017282732242588387, 0.0017153877357035413, 0.0016157185882618723, 0.0017426860150556232, 0.0016236026567639783, 0.0016110949454741785, 0.0015949750631989446, 0.001563152867674944, 0.0016655920862831408, 0.0019238369131926447, 0.0015573860382573912, 0.0015490294699702645, 0.0016420286647189641, 0.0016029627731768414, 0.0015683818764955504, 0.0015696975788159762, 0.0016038687426771503, 0.0017137547583843116, 0.0017097169220505748, 0.0016324315547535662, 0.0015195957112155156, 0.0015450994214916136, 0.0014844390225334791, 0.0015308427737181773, 0.0017842048437159974, 0.001608073329407489, 0.0016164264307008125, 0.0014778631484659854, 0.0015066041632962879, 0.0015770658992551034, 0.0016250584758381592, 0.0017046712891897187, 0.0015857094913371839, 0.0016043406722019427, 0.0015880701957939891, 0.0015028762645670213, 0.0015334302497649333, 0.0014998605856817449, 0.0015002518684923416, 0.0017173349697259255, 0.0014734665928699542, 0.001536551802928443, 0.0016375286104448605, 0.0017445391404180555, 0.0015882042971497867, 0.0015863221724430332, 0.0018267272571392823, 0.0017123346860898891, 0.0016988270072033629, 0.0016456023677164922, 0.0016007041485863738, 0.0016182392810151214, 0.001742733125865925, 0.001709293468593387, 0.0015291021099983482]
[473.5287427657134, 500.85180138904536, 503.57354953944093, 508.20142772433786, 481.60674393620263, 497.1701003681779, 498.2196620802682, 497.34229885584153, 581.3325076510249, 489.9638726450414, 593.8328116831243, 578.0348867876165, 602.75516324099, 477.80652958487235, 383.4564864459964, 572.7498234903417, 594.7121954138503, 612.4991271819596, 503.104554975818, 584.455681214352, 605.4836451235121, 588.8750703177541, 617.2472783694693, 609.9669249702217, 613.9672433927691, 624.1947069429248, 585.1435245617484, 595.3174843427371, 320.01325610461606, 570.7502826577428, 572.2738714722747, 582.020937515315, 567.9502102078609, 576.8590784557559, 559.7764218478382, 331.3330108128021, 574.2719568922071, 576.6229644843872, 577.73668055743, 499.69264250037105, 582.4778238678526, 624.7260040334568, 553.2189561825134, 455.93986887342567, 546.2875494565903, 487.7814542422939, 550.9893252569574, 598.8716506400458, 589.5735509946074, 577.8142080991834, 576.5668100154766, 542.2896329091075, 574.1103263082545, 570.2504050797194, 587.488225171054, 563.9077929770054, 534.6055541182567, 568.326267484009, 578.7450058729661, 631.7594833407192, 16.87505111345798, 581.1890852862557, 625.7982565460227, 638.9909040162304, 641.2199127975942, 632.2018974585806, 581.9325561106078, 381.6378458729481, 596.9198247683319, 594.3842878920555, 579.0630724245439, 580.7712766103641, 579.5690063196542, 608.0294505455925, 568.1822813397594, 545.3646466569423, 569.6947283138977, 583.866381547637, 503.6387039983358, 454.90868591863074, 525.4993380135636, 563.581287928952, 601.7124260627011, 552.4070376978124, 548.2426503013543, 525.9600017252565, 594.7568640201952, 511.3921763813894, 539.2090341849521, 570.634384772026, 567.655804307865, 551.2321646540104, 578.6122159179114, 582.9585808422867, 618.9196604315615, 573.8268347600655, 615.9142422155874, 620.6958831378366, 626.9690499075018, 639.7326970889382, 600.3870985191544, 519.7945798536948, 642.1015569902835, 645.5655101379035, 609.002766812936, 623.8448058392173, 637.5998186324599, 637.0653898531847, 623.4924176717959, 583.5140618035553, 584.8921462394107, 612.5831108128519, 658.0697698864298, 647.2075428224654, 673.6551551260817, 653.2349482051358, 560.4737614753253, 621.8621885660278, 618.6486319494561, 676.6526393448507, 663.7443492868807, 634.0889118662262, 615.3624714853589, 586.6233603754364, 630.6325373361601, 623.3090124353135, 629.6950869353914, 665.3907733968372, 652.1326941041464, 666.7286343453457, 666.5547439077259, 582.2975817930226, 678.6716474190592, 650.8078660896081, 610.6763531467913, 573.2172909347069, 629.6419181050031, 630.3889697639029, 547.4270973358302, 583.9979813079048, 588.6414542268294, 607.6802146241698, 624.7250629562794, 617.9555840300083, 573.8113226619953, 585.0370450563535, 653.9785626226625]
Elapsed: 0.27584625438147153~0.6106203113181801
Time per graph: 0.002142682745444448~0.004733006806882987
Speed: 575.2206916422289~76.39340215580245
Total Time: 0.1964
best val loss: 0.29594195218280306 test_score: 0.8672

Testing...
Test loss: 0.3136 score: 0.8750 time: 0.19s
test Score 0.8750
Epoch Time List: [2.0323011099826545, 0.9299877700395882, 0.9119203230366111, 0.91039874474518, 0.9014273907523602, 0.8875278199557215, 0.8839556358288974, 0.8733970378525555, 0.8683556928299367, 0.8128523188643157, 0.7530937578994781, 0.7656967688817531, 0.7427907718811184, 0.8493299859110266, 0.9073417368344963, 0.7424444740172476, 0.7549234430771321, 0.797627632971853, 0.8976777328643948, 0.7452014707960188, 0.941228544106707, 0.8531863291282207, 0.721020613797009, 0.6944866222329438, 0.7253013080917299, 0.8331563668325543, 0.994717386784032, 0.7292981371283531, 0.9205382321961224, 0.7718512022402138, 0.7477948903106153, 0.762831524014473, 0.8723282720893621, 0.7658566529862583, 0.7706198419909924, 0.9374788128770888, 0.8164192417170852, 0.7555926681961864, 0.776078175753355, 1.1445298150647432, 0.7894083689898252, 0.7453201108146459, 0.7709815020207316, 0.9469973968807608, 0.938990059774369, 0.8624746771529317, 0.9531015842221677, 0.7466629657428712, 0.7392747648991644, 0.8946361993439496, 0.730068864999339, 0.7809813690837473, 0.9566236671525985, 0.7545840907841921, 0.7719112581107765, 0.7631265060044825, 0.8743038561660796, 0.8063396450597793, 0.7767401561141014, 0.8713087379001081, 8.165334895718843, 6.035003174794838, 0.7147473979275674, 0.7171353287994862, 0.7628885039594024, 0.6989228541497141, 0.7253499520011246, 0.8463503569364548, 0.7404154960531741, 0.7160563159268349, 0.7476406490895897, 0.9440492107532918, 0.7567572470288724, 0.7557674727868289, 0.817480118945241, 0.7549518218729645, 0.7960529362317175, 0.8825275467243046, 0.8336890439968556, 1.2617106980178505, 0.9450999901164323, 0.8133851978927851, 0.7641775791998953, 0.7957948429975659, 0.7683739967178553, 0.8658697260543704, 0.8271942059509456, 0.7955220539588481, 0.848821999039501, 0.782620359910652, 0.7730932901613414, 0.8315759657416493, 0.912630002014339, 0.7690659309737384, 0.7223448492586613, 0.9100594501942396, 0.8143583990167826, 0.9268205089028925, 0.8150819002185017, 0.7952401938382536, 0.9407605081796646, 0.8622451960109174, 0.8123066201806068, 0.9343355789314955, 0.8365701921284199, 0.792474176036194, 0.9098017569631338, 0.7953549621161073, 0.7827481511048973, 0.954305297229439, 0.8458274058066308, 0.8107203592080623, 0.7870336829219013, 0.7871436248533428, 0.786525989882648, 0.7774758487939835, 0.7958591170608997, 0.8068836741149426, 0.7905834240373224, 0.7689497440587729, 0.7691675829701126, 0.7815954813268036, 0.8247706817928702, 0.7901567039079964, 0.8844678110908717, 0.8099645359907299, 0.7918780690524727, 0.7567032310180366, 0.7701942860148847, 0.754236192908138, 0.7470284891314805, 0.8993502028752118, 0.750625868793577, 0.7603134182281792, 0.8141247876919806, 0.8071408150717616, 0.770828242180869, 0.8898557471111417, 0.8205050840042531, 0.8213279929477721, 0.9490666328929365, 0.782280667219311, 0.8278488758951426, 0.8904958539642394, 0.8482658138964325, 0.8539114352315664, 0.8797224762383848]
Total Epoch List: [26, 70, 51]
Total Time List: [0.2070711450651288, 0.22545717284083366, 0.19643850391730666]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bdaa26db70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9359;  Loss pred: 0.9359; Loss self: 0.0000; time: 0.34s
Val loss: 0.7149 score: 0.4574 time: 0.24s
Test loss: 0.6877 score: 0.5504 time: 0.24s
Epoch 2/1000, LR 0.000015
Train loss: 0.8757;  Loss pred: 0.8757; Loss self: 0.0000; time: 0.35s
Val loss: 0.7846 score: 0.5349 time: 0.25s
Test loss: 0.7670 score: 0.5194 time: 0.39s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.7533;  Loss pred: 0.7533; Loss self: 0.0000; time: 0.41s
Val loss: 0.8266 score: 0.5194 time: 0.26s
Test loss: 0.8196 score: 0.5116 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 0.35s
Val loss: 0.8085 score: 0.4574 time: 0.22s
Test loss: 0.8233 score: 0.4729 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.4843;  Loss pred: 0.4843; Loss self: 0.0000; time: 0.35s
Val loss: 0.7806 score: 0.4341 time: 0.23s
Test loss: 0.7972 score: 0.3876 time: 0.28s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.3709;  Loss pred: 0.3709; Loss self: 0.0000; time: 0.34s
Val loss: 0.7470 score: 0.4109 time: 0.22s
Test loss: 0.7612 score: 0.4419 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.3270;  Loss pred: 0.3270; Loss self: 0.0000; time: 0.33s
Val loss: 0.7220 score: 0.3953 time: 0.25s
Test loss: 0.7253 score: 0.4264 time: 0.25s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.3036;  Loss pred: 0.3036; Loss self: 0.0000; time: 0.38s
Val loss: 0.7036 score: 0.4264 time: 0.26s
Test loss: 0.7043 score: 0.4109 time: 0.34s
Epoch 9/1000, LR 0.000225
Train loss: 0.2356;  Loss pred: 0.2356; Loss self: 0.0000; time: 0.34s
Val loss: 0.6890 score: 0.4419 time: 0.23s
Test loss: 0.6854 score: 0.4109 time: 0.23s
Epoch 10/1000, LR 0.000255
Train loss: 0.2023;  Loss pred: 0.2023; Loss self: 0.0000; time: 0.33s
Val loss: 0.6773 score: 0.4651 time: 0.26s
Test loss: 0.6677 score: 0.4264 time: 0.28s
Epoch 11/1000, LR 0.000285
Train loss: 0.1709;  Loss pred: 0.1709; Loss self: 0.0000; time: 0.38s
Val loss: 0.6680 score: 0.4884 time: 0.26s
Test loss: 0.6552 score: 0.4496 time: 0.33s
Epoch 12/1000, LR 0.000285
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 0.34s
Val loss: 0.6627 score: 0.5116 time: 0.23s
Test loss: 0.6549 score: 0.4729 time: 0.22s
Epoch 13/1000, LR 0.000285
Train loss: 0.1208;  Loss pred: 0.1208; Loss self: 0.0000; time: 0.36s
Val loss: 0.6611 score: 0.4806 time: 0.23s
Test loss: 0.6590 score: 0.5116 time: 0.23s
Epoch 14/1000, LR 0.000285
Train loss: 0.1051;  Loss pred: 0.1051; Loss self: 0.0000; time: 0.38s
Val loss: 0.6607 score: 0.5271 time: 0.26s
Test loss: 0.6636 score: 0.5426 time: 0.24s
Epoch 15/1000, LR 0.000285
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.37s
Val loss: 0.6562 score: 0.5271 time: 0.28s
Test loss: 0.6598 score: 0.5426 time: 0.23s
Epoch 16/1000, LR 0.000285
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.34s
Val loss: 0.6512 score: 0.5194 time: 0.22s
Test loss: 0.6495 score: 0.5581 time: 0.23s
Epoch 17/1000, LR 0.000285
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.34s
Val loss: 0.6439 score: 0.5349 time: 0.22s
Test loss: 0.6386 score: 0.5814 time: 0.22s
Epoch 18/1000, LR 0.000285
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 0.34s
Val loss: 0.6369 score: 0.5581 time: 0.28s
Test loss: 0.6306 score: 0.5891 time: 0.22s
Epoch 19/1000, LR 0.000285
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.34s
Val loss: 0.6292 score: 0.5736 time: 0.22s
Test loss: 0.6214 score: 0.6279 time: 0.22s
Epoch 20/1000, LR 0.000285
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.34s
Val loss: 0.6191 score: 0.6667 time: 0.23s
Test loss: 0.6099 score: 0.6589 time: 0.24s
Epoch 21/1000, LR 0.000285
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.41s
Val loss: 0.6061 score: 0.6822 time: 0.31s
Test loss: 0.5944 score: 0.6977 time: 0.23s
Epoch 22/1000, LR 0.000285
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.34s
Val loss: 0.6000 score: 0.7287 time: 0.22s
Test loss: 0.5858 score: 0.7442 time: 0.22s
Epoch 23/1000, LR 0.000285
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.47s
Val loss: 0.6003 score: 0.7209 time: 0.23s
Test loss: 0.5820 score: 0.7519 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.36s
Val loss: 0.5860 score: 0.7907 time: 0.30s
Test loss: 0.5638 score: 0.8372 time: 0.21s
Epoch 25/1000, LR 0.000285
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.34s
Val loss: 0.5737 score: 0.8062 time: 0.22s
Test loss: 0.5494 score: 0.8527 time: 0.21s
Epoch 26/1000, LR 0.000285
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.33s
Val loss: 0.5739 score: 0.7907 time: 0.22s
Test loss: 0.5496 score: 0.8295 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.32s
Val loss: 0.5449 score: 0.7984 time: 0.21s
Test loss: 0.5156 score: 0.8450 time: 0.31s
Epoch 28/1000, LR 0.000285
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.32s
Val loss: 0.4772 score: 0.8682 time: 0.23s
Test loss: 0.4310 score: 0.9225 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.34s
Val loss: 0.4398 score: 0.8760 time: 0.23s
Test loss: 0.3831 score: 0.9612 time: 0.22s
Epoch 30/1000, LR 0.000285
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.37s
Val loss: 0.4252 score: 0.8760 time: 0.24s
Test loss: 0.3576 score: 0.9612 time: 0.23s
Epoch 31/1000, LR 0.000285
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.34s
Val loss: 0.4095 score: 0.8760 time: 0.22s
Test loss: 0.3376 score: 0.9612 time: 0.24s
Epoch 32/1000, LR 0.000285
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.34s
Val loss: 0.3968 score: 0.8760 time: 0.23s
Test loss: 0.3189 score: 0.9612 time: 0.21s
Epoch 33/1000, LR 0.000285
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.33s
Val loss: 0.3874 score: 0.8527 time: 0.22s
Test loss: 0.2975 score: 0.9612 time: 0.22s
Epoch 34/1000, LR 0.000285
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.33s
Val loss: 0.3824 score: 0.8450 time: 0.21s
Test loss: 0.2730 score: 0.9612 time: 0.22s
Epoch 35/1000, LR 0.000285
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.34s
Val loss: 0.3795 score: 0.8527 time: 0.21s
Test loss: 0.2513 score: 0.9612 time: 0.23s
Epoch 36/1000, LR 0.000285
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.33s
Val loss: 0.3846 score: 0.8527 time: 0.22s
Test loss: 0.2353 score: 0.9535 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.34s
Val loss: 0.3805 score: 0.8605 time: 0.22s
Test loss: 0.2215 score: 0.9535 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.34s
Val loss: 0.3827 score: 0.8605 time: 0.22s
Test loss: 0.2128 score: 0.9535 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.35s
Val loss: 0.3794 score: 0.8605 time: 0.26s
Test loss: 0.2026 score: 0.9535 time: 0.28s
Epoch 40/1000, LR 0.000284
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.36s
Val loss: 0.3785 score: 0.8682 time: 0.25s
Test loss: 0.1873 score: 0.9535 time: 0.24s
Epoch 41/1000, LR 0.000284
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.34s
Val loss: 0.3793 score: 0.8682 time: 0.22s
Test loss: 0.1735 score: 0.9535 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.35s
Val loss: 0.3996 score: 0.8682 time: 0.22s
Test loss: 0.1641 score: 0.9535 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.34s
Val loss: 0.4239 score: 0.8682 time: 0.24s
Test loss: 0.1551 score: 0.9457 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.34s
Val loss: 0.4399 score: 0.8682 time: 0.23s
Test loss: 0.1509 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.36s
Val loss: 0.4598 score: 0.8682 time: 0.26s
Test loss: 0.1494 score: 0.9457 time: 0.37s
     INFO: Early stopping counter 5 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.49s
Val loss: 0.4853 score: 0.8682 time: 0.24s
Test loss: 0.1470 score: 0.9457 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.37s
Val loss: 0.5091 score: 0.8682 time: 0.23s
Test loss: 0.1427 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.33s
Val loss: 0.5358 score: 0.8682 time: 0.22s
Test loss: 0.1376 score: 0.9457 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.40s
Val loss: 0.5649 score: 0.8682 time: 0.22s
Test loss: 0.1323 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.33s
Val loss: 0.5928 score: 0.8605 time: 0.22s
Test loss: 0.1281 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.34s
Val loss: 0.6099 score: 0.8605 time: 0.23s
Test loss: 0.1228 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 11 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.34s
Val loss: 0.6504 score: 0.8605 time: 0.48s
Test loss: 0.1248 score: 0.9457 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.33s
Val loss: 0.6899 score: 0.8605 time: 0.22s
Test loss: 0.1292 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.35s
Val loss: 0.7184 score: 0.8605 time: 0.23s
Test loss: 0.1368 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.33s
Val loss: 0.7451 score: 0.8605 time: 0.37s
Test loss: 0.1445 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.33s
Val loss: 0.7661 score: 0.8605 time: 0.22s
Test loss: 0.1447 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 16 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.34s
Val loss: 0.7799 score: 0.8605 time: 0.24s
Test loss: 0.1393 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 17 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.35s
Val loss: 0.7981 score: 0.8605 time: 0.23s
Test loss: 0.1368 score: 0.9457 time: 0.23s
     INFO: Early stopping counter 18 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.42s
Val loss: 0.8180 score: 0.8605 time: 0.22s
Test loss: 0.1390 score: 0.9457 time: 0.22s
     INFO: Early stopping counter 19 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.34s
Val loss: 0.8368 score: 0.8605 time: 0.23s
Test loss: 0.1428 score: 0.9457 time: 0.24s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 039,   Train_Loss: 0.0072,   Val_Loss: 0.3785,   Val_Precision: 0.9796,   Val_Recall: 0.7500,   Val_accuracy: 0.8496,   Val_Score: 0.8682,   Val_Loss: 0.3785,   Test_Precision: 0.9836,   Test_Recall: 0.9231,   Test_accuracy: 0.9524,   Test_Score: 0.9535,   Test_loss: 0.1873


[0.24175775004550815, 0.3962318520061672, 0.26595542300492525, 0.22068424592725933, 0.28852650709450245, 0.22019024984911084, 0.25912417308427393, 0.34263991098850965, 0.2386893981602043, 0.29103748104535043, 0.3364168528933078, 0.22809053398668766, 0.23870774894021451, 0.2515620819758624, 0.23558568698354065, 0.23073945892974734, 0.2267840711865574, 0.22886933805420995, 0.22946594492532313, 0.24158045183867216, 0.23558970703743398, 0.22394154500216246, 0.22853033617138863, 0.21829661703668535, 0.21920659090392292, 0.22100309189409018, 0.3108275579288602, 0.22758914693258703, 0.22514442191459239, 0.23099229508079588, 0.244700439972803, 0.2145808341447264, 0.223513247910887, 0.22397002694197, 0.2359674379695207, 0.22329358803108335, 0.22867268999107182, 0.24155767099000514, 0.29074389999732375, 0.24177584913559258, 0.24167596409097314, 0.22330868686549366, 0.2368428730405867, 0.22204985190182924, 0.3795196700375527, 0.2185595971532166, 0.22543214494362473, 0.21822175686247647, 0.2225071748252958, 0.2215015939436853, 0.22316017816774547, 0.21537434100173414, 0.2221019510179758, 0.22891715983860195, 0.22323997179046273, 0.2266382658854127, 0.22139574098400772, 0.23599584586918354, 0.2222070440184325, 0.24338874593377113]
[0.0018740910856240942, 0.003071564744233854, 0.0020616699457746144, 0.0017107305885834057, 0.002236639589879864, 0.0017069011616210143, 0.002008714520033131, 0.002656123340996199, 0.001850305412094607, 0.0022561045042275227, 0.0026078825805682776, 0.0017681436743154082, 0.0018504476662032133, 0.0019500936587276154, 0.0018262456355313229, 0.0017886779761995919, 0.0017580160557097474, 0.0017741809151489143, 0.001778805774614908, 0.0018727166809199393, 0.0018262767987397982, 0.0017359809690090113, 0.001771552993576656, 0.001692221837493685, 0.0016992758984800227, 0.0017132022627448851, 0.0024095159529369005, 0.001764256952965791, 0.0017453055962371503, 0.0017906379463627589, 0.0018969026354480852, 0.0016634173189513674, 0.001732660836518504, 0.0017362017592400774, 0.0018292049455001604, 0.0017309580467525841, 0.0017726565115586963, 0.0018725400851938383, 0.0022538286821497965, 0.0018742313886480045, 0.001873457085976536, 0.001731075091980571, 0.0018359912638805172, 0.001721316681409529, 0.0029420129460275405, 0.0016942604430481907, 0.0017475360073149204, 0.001691641526065709, 0.0017248618203511302, 0.0017170666197184907, 0.0017299238617654688, 0.0016695685348971639, 0.0017217205505269443, 0.001774551626655829, 0.0017305424169803312, 0.0017568857820574628, 0.0017162460541395948, 0.0018294251617766166, 0.0017225352249490892, 0.001886734464602877]
[533.5919943650915, 325.5669612295383, 485.0436909406846, 584.545577587447, 447.09930224105204, 585.8570036066513, 497.83082166574184, 376.48854048507496, 540.4513187193064, 443.2418791444212, 383.4528469384125, 565.5648998021504, 540.4097712483924, 512.7958831743874, 547.5714660416219, 559.0721266243253, 568.8230188525094, 563.6403770672202, 562.1749233507459, 533.9836026391176, 547.5621223957062, 576.0431812630137, 564.4764811585239, 590.9390706605462, 588.4859550438428, 583.7022409705462, 415.0211160798185, 566.8108595626944, 572.9655609630676, 558.4601856736334, 527.1751861759528, 601.1720502167241, 577.1469977986777, 575.9699266965912, 546.6855982759054, 577.7147527498313, 564.1250820333491, 534.0339616262387, 443.6894462830946, 533.5520502200958, 533.77256809635, 577.6756910388401, 544.6649010117937, 580.9506239032862, 339.9033309320587, 590.2280278708936, 572.2342748957113, 591.1417901437569, 579.7565858327306, 582.3885855773889, 578.0601228191932, 598.9571431768715, 580.8143485851657, 563.522630155605, 577.8535043046943, 569.1889650498025, 582.66703517715, 546.6197912293205, 580.5396519711553, 530.0162893936862]
Elapsed: 0.243242911901325~0.038051657131964095
Time per graph: 0.0018856039682273252~0.0002949740862942953
Speed: 540.0648948789534~63.010827946933006
Total Time: 0.2439
best val loss: 0.3785333690457335 test_score: 0.9535

Testing...
Test loss: 0.3831 score: 0.9612 time: 0.23s
test Score 0.9612
Epoch Time List: [0.81945462198928, 0.9947179439477623, 0.932217397261411, 0.7808945011347532, 0.8554523580241948, 0.7745037549175322, 0.8322925930842757, 0.9746150439605117, 0.7989953809883446, 0.8757224429864436, 0.9614329908508807, 0.7838078939821571, 0.8215428199619055, 0.8864213530905545, 0.8838268469553441, 0.792216117028147, 0.7868550340645015, 0.8464460917748511, 0.7929419942665845, 0.8037890740670264, 0.9461682178080082, 0.7798121289815754, 0.9236926422454417, 0.8638444659300148, 0.7724564992822707, 0.7694621689151973, 0.8313060970976949, 0.7728160498663783, 0.7926631451118737, 0.8380305389873683, 0.8018924701027572, 0.783775727963075, 0.7714064468163997, 0.7625457993708551, 0.7768294350244105, 0.7710058148950338, 0.7807118538767099, 0.7987805148586631, 0.8983670899178833, 0.8510310589335859, 0.8004368410911411, 0.7906228792853653, 0.8150229789316654, 0.7936645748559386, 0.990773974917829, 0.9406256428919733, 0.8144506830722094, 0.7652375688776374, 0.8359228046610951, 0.7687615919858217, 0.7853150689043105, 1.0283140242099762, 0.7704542439896613, 0.8000994401518255, 0.9186927306000143, 0.7706028609536588, 0.7979612553026527, 0.8149969051592052, 0.8601992991752923, 0.806622157106176]
Total Epoch List: [60]
Total Time List: [0.2439081040211022]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda88de560>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.35s
Val loss: 1.0138 score: 0.3643 time: 0.25s
Test loss: 0.9686 score: 0.3256 time: 0.21s
Epoch 2/1000, LR 0.000015
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.36s
Val loss: 0.8611 score: 0.3256 time: 0.39s
Test loss: 0.8506 score: 0.3256 time: 0.21s
Epoch 3/1000, LR 0.000045
Train loss: 0.5303;  Loss pred: 0.5303; Loss self: 0.0000; time: 0.35s
Val loss: 0.8069 score: 0.3643 time: 0.22s
Test loss: 0.8033 score: 0.3256 time: 0.19s
Epoch 4/1000, LR 0.000075
Train loss: 0.4413;  Loss pred: 0.4413; Loss self: 0.0000; time: 0.34s
Val loss: 0.7852 score: 0.3876 time: 0.23s
Test loss: 0.7766 score: 0.3566 time: 0.20s
Epoch 5/1000, LR 0.000105
Train loss: 0.3483;  Loss pred: 0.3483; Loss self: 0.0000; time: 0.35s
Val loss: 0.7650 score: 0.3953 time: 0.23s
Test loss: 0.7660 score: 0.4031 time: 0.34s
Epoch 6/1000, LR 0.000135
Train loss: 0.2858;  Loss pred: 0.2858; Loss self: 0.0000; time: 0.33s
Val loss: 0.7502 score: 0.4419 time: 0.22s
Test loss: 0.7532 score: 0.3953 time: 0.20s
Epoch 7/1000, LR 0.000165
Train loss: 0.2375;  Loss pred: 0.2375; Loss self: 0.0000; time: 0.34s
Val loss: 0.7370 score: 0.4729 time: 3.22s
Test loss: 0.7433 score: 0.4031 time: 7.64s
Epoch 8/1000, LR 0.000195
Train loss: 0.2023;  Loss pred: 0.2023; Loss self: 0.0000; time: 0.42s
Val loss: 0.7293 score: 0.5039 time: 0.22s
Test loss: 0.7382 score: 0.4729 time: 0.21s
Epoch 9/1000, LR 0.000225
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.34s
Val loss: 0.7313 score: 0.5194 time: 0.22s
Test loss: 0.7466 score: 0.4574 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.35s
Val loss: 0.7407 score: 0.5581 time: 0.22s
Test loss: 0.7631 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.1038;  Loss pred: 0.1038; Loss self: 0.0000; time: 0.35s
Val loss: 0.7456 score: 0.5271 time: 0.26s
Test loss: 0.7796 score: 0.4651 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 0.33s
Val loss: 0.7452 score: 0.5736 time: 0.22s
Test loss: 0.7882 score: 0.4729 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.35s
Val loss: 0.7389 score: 0.5736 time: 0.23s
Test loss: 0.7951 score: 0.4884 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.34s
Val loss: 0.7186 score: 0.5891 time: 0.33s
Test loss: 0.7885 score: 0.4961 time: 0.20s
Epoch 15/1000, LR 0.000285
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.33s
Val loss: 0.6691 score: 0.6279 time: 0.21s
Test loss: 0.7496 score: 0.5039 time: 0.20s
Epoch 16/1000, LR 0.000285
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.33s
Val loss: 0.6109 score: 0.6977 time: 0.23s
Test loss: 0.6935 score: 0.5736 time: 0.21s
Epoch 17/1000, LR 0.000285
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.34s
Val loss: 0.5761 score: 0.7287 time: 0.23s
Test loss: 0.6473 score: 0.6047 time: 0.38s
Epoch 18/1000, LR 0.000285
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.34s
Val loss: 0.5621 score: 0.6279 time: 0.24s
Test loss: 0.6207 score: 0.5581 time: 0.19s
Epoch 19/1000, LR 0.000285
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.32s
Val loss: 0.5590 score: 0.5969 time: 0.21s
Test loss: 0.6087 score: 0.5504 time: 0.20s
Epoch 20/1000, LR 0.000285
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.33s
Val loss: 0.5499 score: 0.6047 time: 0.31s
Test loss: 0.5950 score: 0.5814 time: 0.23s
Epoch 21/1000, LR 0.000285
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.33s
Val loss: 0.5410 score: 0.6124 time: 0.23s
Test loss: 0.5839 score: 0.5969 time: 0.21s
Epoch 22/1000, LR 0.000285
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.32s
Val loss: 0.5307 score: 0.6279 time: 0.22s
Test loss: 0.5720 score: 0.5814 time: 0.23s
Epoch 23/1000, LR 0.000285
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.32s
Val loss: 0.5120 score: 0.6589 time: 0.21s
Test loss: 0.5536 score: 0.6124 time: 0.35s
Epoch 24/1000, LR 0.000285
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.32s
Val loss: 0.4962 score: 0.6977 time: 0.22s
Test loss: 0.5369 score: 0.6202 time: 0.20s
Epoch 25/1000, LR 0.000285
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.32s
Val loss: 0.4860 score: 0.7364 time: 0.21s
Test loss: 0.5223 score: 0.6357 time: 0.19s
Epoch 26/1000, LR 0.000285
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.32s
Val loss: 0.4611 score: 0.7519 time: 0.22s
Test loss: 0.4992 score: 0.6667 time: 0.20s
Epoch 27/1000, LR 0.000285
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.35s
Val loss: 0.4205 score: 0.8295 time: 0.22s
Test loss: 0.4675 score: 0.7597 time: 0.20s
Epoch 28/1000, LR 0.000285
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.34s
Val loss: 0.3908 score: 0.8682 time: 0.23s
Test loss: 0.4437 score: 0.7984 time: 0.21s
Epoch 29/1000, LR 0.000285
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.31s
Val loss: 0.3601 score: 0.9070 time: 0.22s
Test loss: 0.4201 score: 0.8372 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.32s
Val loss: 0.3381 score: 0.9070 time: 0.21s
Test loss: 0.4021 score: 0.8372 time: 0.19s
Epoch 31/1000, LR 0.000285
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.32s
Val loss: 0.3561 score: 0.8915 time: 0.21s
Test loss: 0.4082 score: 0.8295 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.33s
Val loss: 0.3828 score: 0.8837 time: 0.21s
Test loss: 0.4177 score: 0.8140 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.32s
Val loss: 0.4029 score: 0.8295 time: 0.21s
Test loss: 0.4243 score: 0.8062 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.31s
Val loss: 0.4020 score: 0.8295 time: 0.21s
Test loss: 0.4221 score: 0.7984 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.31s
Val loss: 0.3982 score: 0.8450 time: 0.21s
Test loss: 0.4177 score: 0.7984 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.32s
Val loss: 0.3399 score: 0.8760 time: 0.21s
Test loss: 0.3820 score: 0.8527 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.33s
Val loss: 0.3026 score: 0.8915 time: 0.21s
Test loss: 0.3590 score: 0.9147 time: 0.22s
Epoch 38/1000, LR 0.000284
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.35s
Val loss: 0.2712 score: 0.9147 time: 0.24s
Test loss: 0.3404 score: 0.9147 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.34s
Val loss: 0.2439 score: 0.9302 time: 0.28s
Test loss: 0.3255 score: 0.9070 time: 0.20s
Epoch 40/1000, LR 0.000284
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.34s
Val loss: 0.1932 score: 0.9302 time: 0.23s
Test loss: 0.2997 score: 0.9302 time: 0.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.34s
Val loss: 0.1661 score: 0.9535 time: 0.23s
Test loss: 0.2954 score: 0.9147 time: 0.21s
Epoch 42/1000, LR 0.000284
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.33s
Val loss: 0.1560 score: 0.9457 time: 0.22s
Test loss: 0.3163 score: 0.8992 time: 0.34s
Epoch 43/1000, LR 0.000284
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.34s
Val loss: 0.1622 score: 0.9457 time: 0.22s
Test loss: 0.3474 score: 0.8915 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.36s
Val loss: 0.2068 score: 0.9070 time: 0.23s
Test loss: 0.4116 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.38s
Val loss: 0.3077 score: 0.8992 time: 0.23s
Test loss: 0.5311 score: 0.8295 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.46s
Val loss: 0.3798 score: 0.8837 time: 0.23s
Test loss: 0.6586 score: 0.8062 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.34s
Val loss: 0.4116 score: 0.8837 time: 0.22s
Test loss: 0.7215 score: 0.7984 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.35s
Val loss: 0.4195 score: 0.8837 time: 0.22s
Test loss: 0.7498 score: 0.8217 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.47s
Val loss: 0.4761 score: 0.8760 time: 0.24s
Test loss: 0.8489 score: 0.7984 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.34s
Val loss: 0.4574 score: 0.8837 time: 0.22s
Test loss: 0.8511 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.34s
Val loss: 0.3983 score: 0.9070 time: 0.21s
Test loss: 0.7927 score: 0.8295 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.33s
Val loss: 0.3226 score: 0.9380 time: 0.34s
Test loss: 0.6720 score: 0.8450 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.35s
Val loss: 0.2666 score: 0.9380 time: 0.51s
Test loss: 0.5929 score: 0.8605 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.38s
Val loss: 0.2747 score: 0.9380 time: 0.23s
Test loss: 0.6041 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.33s
Val loss: 0.2990 score: 0.9380 time: 0.34s
Test loss: 0.6391 score: 0.8527 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.34s
Val loss: 0.3173 score: 0.9380 time: 0.22s
Test loss: 0.6656 score: 0.8450 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.32s
Val loss: 0.3707 score: 0.9380 time: 0.21s
Test loss: 0.7389 score: 0.8450 time: 0.19s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.33s
Val loss: 0.4388 score: 0.9147 time: 0.34s
Test loss: 0.8453 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.36s
Val loss: 0.5303 score: 0.8992 time: 0.24s
Test loss: 1.0113 score: 0.8140 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.35s
Val loss: 0.5574 score: 0.8992 time: 0.22s
Test loss: 1.0761 score: 0.8140 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.34s
Val loss: 0.6252 score: 0.8915 time: 0.23s
Test loss: 1.1932 score: 0.8140 time: 0.33s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.34s
Val loss: 0.5811 score: 0.8992 time: 0.21s
Test loss: 1.1279 score: 0.8295 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0098,   Val_Loss: 0.1560,   Val_Precision: 0.9833,   Val_Recall: 0.9077,   Val_accuracy: 0.9440,   Val_Score: 0.9457,   Val_Loss: 0.1560,   Test_Precision: 0.9636,   Test_Recall: 0.8281,   Test_accuracy: 0.8908,   Test_Score: 0.8992,   Test_loss: 0.3163


[0.24175775004550815, 0.3962318520061672, 0.26595542300492525, 0.22068424592725933, 0.28852650709450245, 0.22019024984911084, 0.25912417308427393, 0.34263991098850965, 0.2386893981602043, 0.29103748104535043, 0.3364168528933078, 0.22809053398668766, 0.23870774894021451, 0.2515620819758624, 0.23558568698354065, 0.23073945892974734, 0.2267840711865574, 0.22886933805420995, 0.22946594492532313, 0.24158045183867216, 0.23558970703743398, 0.22394154500216246, 0.22853033617138863, 0.21829661703668535, 0.21920659090392292, 0.22100309189409018, 0.3108275579288602, 0.22758914693258703, 0.22514442191459239, 0.23099229508079588, 0.244700439972803, 0.2145808341447264, 0.223513247910887, 0.22397002694197, 0.2359674379695207, 0.22329358803108335, 0.22867268999107182, 0.24155767099000514, 0.29074389999732375, 0.24177584913559258, 0.24167596409097314, 0.22330868686549366, 0.2368428730405867, 0.22204985190182924, 0.3795196700375527, 0.2185595971532166, 0.22543214494362473, 0.21822175686247647, 0.2225071748252958, 0.2215015939436853, 0.22316017816774547, 0.21537434100173414, 0.2221019510179758, 0.22891715983860195, 0.22323997179046273, 0.2266382658854127, 0.22139574098400772, 0.23599584586918354, 0.2222070440184325, 0.24338874593377113, 0.21709725796245039, 0.21554411202669144, 0.1997258448973298, 0.20613726996816695, 0.34779972303658724, 0.20379540510475636, 7.642889303853735, 0.21159911481663585, 0.2086191310081631, 0.20125566003844142, 0.21017237519845366, 0.21127200010232627, 0.2150911760982126, 0.20773148606531322, 0.20292019098997116, 0.21369990915991366, 0.3879655769560486, 0.1947427170816809, 0.20380831300280988, 0.24166762689128518, 0.21709597599692643, 0.23731210990808904, 0.3558476238977164, 0.20608443394303322, 0.20006645401008427, 0.20113692502491176, 0.20773210912011564, 0.2106448591221124, 0.19257665611803532, 0.19783968990668654, 0.1955079440958798, 0.19720545806922019, 0.19474387005902827, 0.19325175392441452, 0.1951389980968088, 0.20314372912980616, 0.22432057210244238, 0.1978278299793601, 0.20460762199945748, 0.2159815328195691, 0.2134121060371399, 0.34681936888955534, 0.20751182292588055, 0.21576079679653049, 0.21769254095852375, 0.21105021005496383, 0.208315015072003, 0.2084923298098147, 0.22402901505120099, 0.2079262330662459, 0.1976304880809039, 0.19797818013466895, 0.217074410058558, 0.20915505988523364, 0.21556004998274148, 0.2008664000313729, 0.197916405973956, 0.21073263720609248, 0.2176309118513018, 0.2137942831031978, 0.3312655668705702, 0.19968290999531746]
[0.0018740910856240942, 0.003071564744233854, 0.0020616699457746144, 0.0017107305885834057, 0.002236639589879864, 0.0017069011616210143, 0.002008714520033131, 0.002656123340996199, 0.001850305412094607, 0.0022561045042275227, 0.0026078825805682776, 0.0017681436743154082, 0.0018504476662032133, 0.0019500936587276154, 0.0018262456355313229, 0.0017886779761995919, 0.0017580160557097474, 0.0017741809151489143, 0.001778805774614908, 0.0018727166809199393, 0.0018262767987397982, 0.0017359809690090113, 0.001771552993576656, 0.001692221837493685, 0.0016992758984800227, 0.0017132022627448851, 0.0024095159529369005, 0.001764256952965791, 0.0017453055962371503, 0.0017906379463627589, 0.0018969026354480852, 0.0016634173189513674, 0.001732660836518504, 0.0017362017592400774, 0.0018292049455001604, 0.0017309580467525841, 0.0017726565115586963, 0.0018725400851938383, 0.0022538286821497965, 0.0018742313886480045, 0.001873457085976536, 0.001731075091980571, 0.0018359912638805172, 0.001721316681409529, 0.0029420129460275405, 0.0016942604430481907, 0.0017475360073149204, 0.001691641526065709, 0.0017248618203511302, 0.0017170666197184907, 0.0017299238617654688, 0.0016695685348971639, 0.0017217205505269443, 0.001774551626655829, 0.0017305424169803312, 0.0017568857820574628, 0.0017162460541395948, 0.0018294251617766166, 0.0017225352249490892, 0.001886734464602877, 0.0016829244803290727, 0.0016708845893541973, 0.0015482623635451923, 0.0015979633330865655, 0.0026961218840045524, 0.0015798093418973361, 0.05924720390584291, 0.001640303215632836, 0.0016172025659547528, 0.0015601213956468326, 0.001629243218592664, 0.001637767442653692, 0.001667373458125679, 0.0016103215974055288, 0.0015730247363563655, 0.001656588443100106, 0.0030074850926825472, 0.0015096334657494642, 0.0015799094031225573, 0.0018733924565215905, 0.0016829145426118328, 0.0018396287589774343, 0.0027585087123853983, 0.0015975537514963814, 0.001550902744264219, 0.0015592009691853626, 0.001610326427287718, 0.0016329058846675378, 0.0014928422954886458, 0.0015336410070285779, 0.0015155654581075952, 0.0015287244811567457, 0.0015096424035583587, 0.0014980756118171668, 0.001512705411603169, 0.001574757590153536, 0.0017389191635848247, 0.0015335490696074426, 0.0015861055968950193, 0.001674275448213714, 0.0016543574111406193, 0.0026885222394539172, 0.001608618782371167, 0.0016725643162521743, 0.0016875390771978586, 0.00163604813996096, 0.0016148450780775426, 0.001616219610928796, 0.0017366590314046588, 0.0016118312640794256, 0.0015320192874488674, 0.0015347145746873563, 0.0016827473647950232, 0.0016213570533739042, 0.0016710081394010967, 0.001557103876212193, 0.001534235705224465, 0.0016335863349309495, 0.001687061332180634, 0.001657320024055797, 0.002567950130779614, 0.0015479295348474222]
[533.5919943650915, 325.5669612295383, 485.0436909406846, 584.545577587447, 447.09930224105204, 585.8570036066513, 497.83082166574184, 376.48854048507496, 540.4513187193064, 443.2418791444212, 383.4528469384125, 565.5648998021504, 540.4097712483924, 512.7958831743874, 547.5714660416219, 559.0721266243253, 568.8230188525094, 563.6403770672202, 562.1749233507459, 533.9836026391176, 547.5621223957062, 576.0431812630137, 564.4764811585239, 590.9390706605462, 588.4859550438428, 583.7022409705462, 415.0211160798185, 566.8108595626944, 572.9655609630676, 558.4601856736334, 527.1751861759528, 601.1720502167241, 577.1469977986777, 575.9699266965912, 546.6855982759054, 577.7147527498313, 564.1250820333491, 534.0339616262387, 443.6894462830946, 533.5520502200958, 533.77256809635, 577.6756910388401, 544.6649010117937, 580.9506239032862, 339.9033309320587, 590.2280278708936, 572.2342748957113, 591.1417901437569, 579.7565858327306, 582.3885855773889, 578.0601228191932, 598.9571431768715, 580.8143485851657, 563.522630155605, 577.8535043046943, 569.1889650498025, 582.66703517715, 546.6197912293205, 580.5396519711553, 530.0162893936862, 594.203727908494, 598.4853809600959, 645.8853638411853, 625.7965870020546, 370.90311307243246, 632.9877748406079, 16.878433648771413, 609.6433820707933, 618.3517272677756, 640.9757617517936, 613.7819010618914, 610.5872994884361, 599.7456629327156, 620.9939689135083, 635.7179114146187, 603.6502332037403, 332.50372626387417, 662.4124482452073, 632.9476854961333, 533.7909825134791, 594.2072367192395, 543.5879359463016, 362.5147151104185, 625.957029028494, 644.7857570040093, 641.3541421299085, 620.9921063546759, 612.4051663905917, 669.8631215246177, 652.043076193884, 659.8197356969629, 654.1401098275905, 662.408526445013, 667.5230489781483, 661.0672456973613, 635.0183712418251, 575.069860026429, 652.0821666671413, 630.4750465275533, 597.2732868220106, 604.4643033397097, 371.9515447278265, 621.6513265659878, 597.8843326280967, 592.5788703278444, 611.2289581062464, 619.2544495912204, 618.7277974094919, 575.8182705508817, 620.412336133172, 652.7332966317997, 651.5869572710056, 594.2662701015793, 616.7672925091276, 598.4411304892916, 642.2179119048868, 651.7903322121524, 612.1500765628462, 592.74667786229, 603.3837674589835, 389.41566193748713, 646.0242391450778]
Elapsed: 0.2922661622745733~0.6695149673247346
Time per graph: 0.002265629164919173~0.005190038506393292
Speed: 566.0018053478195~90.6212053126547
Total Time: 0.2003
best val loss: 0.15602770205153976 test_score: 0.8992

Testing...
Test loss: 0.2954 score: 0.9147 time: 0.20s
test Score 0.9147
Epoch Time List: [0.81945462198928, 0.9947179439477623, 0.932217397261411, 0.7808945011347532, 0.8554523580241948, 0.7745037549175322, 0.8322925930842757, 0.9746150439605117, 0.7989953809883446, 0.8757224429864436, 0.9614329908508807, 0.7838078939821571, 0.8215428199619055, 0.8864213530905545, 0.8838268469553441, 0.792216117028147, 0.7868550340645015, 0.8464460917748511, 0.7929419942665845, 0.8037890740670264, 0.9461682178080082, 0.7798121289815754, 0.9236926422454417, 0.8638444659300148, 0.7724564992822707, 0.7694621689151973, 0.8313060970976949, 0.7728160498663783, 0.7926631451118737, 0.8380305389873683, 0.8018924701027572, 0.783775727963075, 0.7714064468163997, 0.7625457993708551, 0.7768294350244105, 0.7710058148950338, 0.7807118538767099, 0.7987805148586631, 0.8983670899178833, 0.8510310589335859, 0.8004368410911411, 0.7906228792853653, 0.8150229789316654, 0.7936645748559386, 0.990773974917829, 0.9406256428919733, 0.8144506830722094, 0.7652375688776374, 0.8359228046610951, 0.7687615919858217, 0.7853150689043105, 1.0283140242099762, 0.7704542439896613, 0.8000994401518255, 0.9186927306000143, 0.7706028609536588, 0.7979612553026527, 0.8149969051592052, 0.8601992991752923, 0.806622157106176, 0.8077224779408425, 0.9674452298786491, 0.7655753458384424, 0.7638507417868823, 0.9147166390903294, 0.751895149005577, 11.196250804234296, 0.8467442921828479, 0.7639824231155217, 0.7687225409317762, 0.8118573939427733, 0.7587325260974467, 0.7895413029473275, 0.8756481751333922, 0.73265281599015, 0.7690913709811866, 0.9564482558052987, 0.769211343023926, 0.7366627848241478, 0.8752417860087007, 0.7693089921958745, 0.7661231171805412, 0.883639156119898, 0.7364788760896772, 0.7238840381614864, 0.7430941890925169, 0.7760806400328875, 0.7739087881054729, 0.7152152399066836, 0.7245297390036285, 0.7243432791437954, 0.7328273057937622, 0.7223857347853482, 0.7119435202330351, 0.7186387467663735, 0.725269417045638, 0.7640491770580411, 0.7834269409067929, 0.8181001718621701, 0.7795703238807619, 0.7732905230950564, 0.892006458947435, 0.7655853908509016, 0.7962524697650224, 0.8218729989603162, 0.8934257721994072, 0.760281759314239, 0.7649964920710772, 0.9299333060625941, 0.7639335710555315, 0.7502931249327958, 0.8620608646888286, 1.0660538419615477, 0.8067153259180486, 0.8838286011014134, 0.7500715630594641, 0.7283046999946237, 0.8783615590073168, 0.8030854861717671, 0.7802541179116815, 0.8937968011014163, 0.7429365357384086]
Total Epoch List: [60, 62]
Total Time List: [0.2439081040211022, 0.20028333505615592]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda89ada80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.4036;  Loss pred: 1.4036; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8002 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8280 score: 0.5000 time: 0.24s
Epoch 2/1000, LR 0.000020
Train loss: 1.3224;  Loss pred: 1.3224; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8082 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8413 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 1.1118;  Loss pred: 1.1118; Loss self: 0.0000; time: 0.38s
Val loss: 0.8695 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8925 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 0.48s
Val loss: 0.9458 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9600 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.34s
Val loss: 0.9289 score: 0.5194 time: 0.22s
Test loss: 0.9399 score: 0.5312 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.4741;  Loss pred: 0.4741; Loss self: 0.0000; time: 0.37s
Val loss: 0.9524 score: 0.5426 time: 0.21s
Test loss: 0.9477 score: 0.5547 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.3803;  Loss pred: 0.3803; Loss self: 0.0000; time: 0.41s
Val loss: 0.9824 score: 0.5736 time: 0.21s
Test loss: 0.9706 score: 0.5547 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.3004;  Loss pred: 0.3004; Loss self: 0.0000; time: 0.36s
Val loss: 0.8568 score: 0.5814 time: 0.20s
Test loss: 0.8598 score: 0.5938 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.2538;  Loss pred: 0.2538; Loss self: 0.0000; time: 0.38s
Val loss: 0.9520 score: 0.5814 time: 0.23s
Test loss: 0.9353 score: 0.5781 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.2035;  Loss pred: 0.2035; Loss self: 0.0000; time: 0.38s
Val loss: 0.9000 score: 0.5814 time: 0.36s
Test loss: 0.8839 score: 0.5938 time: 0.24s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.1530;  Loss pred: 0.1530; Loss self: 0.0000; time: 0.38s
Val loss: 0.7256 score: 0.6047 time: 0.23s
Test loss: 0.7326 score: 0.6250 time: 0.22s
Epoch 12/1000, LR 0.000290
Train loss: 0.1285;  Loss pred: 0.1285; Loss self: 0.0000; time: 0.37s
Val loss: 0.7738 score: 0.6124 time: 0.21s
Test loss: 0.7537 score: 0.6094 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.1052;  Loss pred: 0.1052; Loss self: 0.0000; time: 0.37s
Val loss: 0.7916 score: 0.5969 time: 0.34s
Test loss: 0.7588 score: 0.6016 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.37s
Val loss: 0.6662 score: 0.6279 time: 0.21s
Test loss: 0.6600 score: 0.6406 time: 0.21s
Epoch 15/1000, LR 0.000290
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.37s
Val loss: 0.5971 score: 0.6744 time: 0.21s
Test loss: 0.6056 score: 0.6641 time: 0.21s
Epoch 16/1000, LR 0.000290
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.37s
Val loss: 0.5500 score: 0.7287 time: 0.21s
Test loss: 0.5601 score: 0.7109 time: 0.32s
Epoch 17/1000, LR 0.000290
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.37s
Val loss: 0.5095 score: 0.8062 time: 0.24s
Test loss: 0.5391 score: 0.7812 time: 0.23s
Epoch 18/1000, LR 0.000290
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.36s
Val loss: 0.4885 score: 0.8062 time: 0.21s
Test loss: 0.5321 score: 0.7969 time: 0.21s
Epoch 19/1000, LR 0.000290
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.37s
Val loss: 0.4733 score: 0.8527 time: 0.22s
Test loss: 0.5429 score: 0.8359 time: 0.21s
Epoch 20/1000, LR 0.000290
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.50s
Val loss: 0.4624 score: 0.8372 time: 0.21s
Test loss: 0.5065 score: 0.8203 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.35s
Val loss: 0.4486 score: 0.8527 time: 0.21s
Test loss: 0.4848 score: 0.8359 time: 0.21s
Epoch 22/1000, LR 0.000290
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.36s
Val loss: 0.4336 score: 0.8295 time: 0.21s
Test loss: 0.4755 score: 0.8438 time: 0.21s
Epoch 23/1000, LR 0.000290
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.43s
Val loss: 0.4304 score: 0.8682 time: 0.26s
Test loss: 0.5198 score: 0.8281 time: 0.22s
Epoch 24/1000, LR 0.000290
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.35s
Val loss: 0.4147 score: 0.8527 time: 0.21s
Test loss: 0.4621 score: 0.8281 time: 0.20s
Epoch 25/1000, LR 0.000290
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.35s
Val loss: 0.4213 score: 0.8682 time: 0.20s
Test loss: 0.5147 score: 0.8281 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000290
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.40s
Val loss: 0.4017 score: 0.8605 time: 0.23s
Test loss: 0.4391 score: 0.8281 time: 0.21s
Epoch 27/1000, LR 0.000290
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.37s
Val loss: 0.3895 score: 0.8605 time: 0.24s
Test loss: 0.4110 score: 0.8516 time: 0.21s
Epoch 28/1000, LR 0.000290
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.40s
Val loss: 0.3830 score: 0.8682 time: 0.21s
Test loss: 0.4230 score: 0.8438 time: 0.21s
Epoch 29/1000, LR 0.000290
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.38s
Val loss: 0.3806 score: 0.8605 time: 0.22s
Test loss: 0.3772 score: 0.8594 time: 0.21s
Epoch 30/1000, LR 0.000290
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.38s
Val loss: 0.3989 score: 0.8682 time: 0.23s
Test loss: 0.3351 score: 0.8516 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000290
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.38s
Val loss: 0.3922 score: 0.8605 time: 0.22s
Test loss: 0.3560 score: 0.8516 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000290
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.37s
Val loss: 0.3857 score: 0.8605 time: 0.21s
Test loss: 0.3865 score: 0.8516 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.38s
Val loss: 0.3919 score: 0.8682 time: 0.23s
Test loss: 0.3814 score: 0.8438 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.38s
Val loss: 0.4134 score: 0.8760 time: 0.22s
Test loss: 0.3394 score: 0.8594 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.39s
Val loss: 0.4797 score: 0.8760 time: 0.26s
Test loss: 0.3498 score: 0.8828 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.39s
Val loss: 0.6007 score: 0.8605 time: 0.22s
Test loss: 0.4646 score: 0.8750 time: 0.23s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.51s
Val loss: 0.4929 score: 0.8837 time: 0.21s
Test loss: 0.3557 score: 0.8906 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.38s
Val loss: 0.4963 score: 0.8760 time: 0.22s
Test loss: 0.3624 score: 0.8906 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.37s
Val loss: 0.6252 score: 0.8605 time: 0.21s
Test loss: 0.5192 score: 0.8750 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.38s
Val loss: 0.7924 score: 0.8372 time: 0.22s
Test loss: 0.6870 score: 0.8750 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.44s
Val loss: 0.8520 score: 0.8372 time: 0.21s
Test loss: 0.7450 score: 0.8750 time: 0.21s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.35s
Val loss: 0.6502 score: 0.8605 time: 0.20s
Test loss: 0.5236 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.36s
Val loss: 0.8004 score: 0.8372 time: 0.20s
Test loss: 0.6861 score: 0.8750 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.34s
Val loss: 0.4546 score: 0.8837 time: 0.19s
Test loss: 0.3399 score: 0.9141 time: 0.32s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.36s
Val loss: 0.4634 score: 0.8837 time: 0.21s
Test loss: 0.3458 score: 0.9141 time: 0.21s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.38s
Val loss: 0.5313 score: 0.8682 time: 0.22s
Test loss: 0.4245 score: 0.8906 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.36s
Val loss: 0.6165 score: 0.8605 time: 0.22s
Test loss: 0.4975 score: 0.8906 time: 0.21s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.37s
Val loss: 0.4874 score: 0.8837 time: 0.42s
Test loss: 0.3371 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.37s
Val loss: 0.5461 score: 0.8760 time: 0.22s
Test loss: 0.3823 score: 0.8984 time: 0.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0284,   Val_Loss: 0.3806,   Val_Precision: 0.9434,   Val_Recall: 0.7692,   Val_accuracy: 0.8475,   Val_Score: 0.8605,   Val_Loss: 0.3806,   Test_Precision: 0.8833,   Test_Recall: 0.8281,   Test_accuracy: 0.8548,   Test_Score: 0.8594,   Test_loss: 0.3772


[0.24175775004550815, 0.3962318520061672, 0.26595542300492525, 0.22068424592725933, 0.28852650709450245, 0.22019024984911084, 0.25912417308427393, 0.34263991098850965, 0.2386893981602043, 0.29103748104535043, 0.3364168528933078, 0.22809053398668766, 0.23870774894021451, 0.2515620819758624, 0.23558568698354065, 0.23073945892974734, 0.2267840711865574, 0.22886933805420995, 0.22946594492532313, 0.24158045183867216, 0.23558970703743398, 0.22394154500216246, 0.22853033617138863, 0.21829661703668535, 0.21920659090392292, 0.22100309189409018, 0.3108275579288602, 0.22758914693258703, 0.22514442191459239, 0.23099229508079588, 0.244700439972803, 0.2145808341447264, 0.223513247910887, 0.22397002694197, 0.2359674379695207, 0.22329358803108335, 0.22867268999107182, 0.24155767099000514, 0.29074389999732375, 0.24177584913559258, 0.24167596409097314, 0.22330868686549366, 0.2368428730405867, 0.22204985190182924, 0.3795196700375527, 0.2185595971532166, 0.22543214494362473, 0.21822175686247647, 0.2225071748252958, 0.2215015939436853, 0.22316017816774547, 0.21537434100173414, 0.2221019510179758, 0.22891715983860195, 0.22323997179046273, 0.2266382658854127, 0.22139574098400772, 0.23599584586918354, 0.2222070440184325, 0.24338874593377113, 0.21709725796245039, 0.21554411202669144, 0.1997258448973298, 0.20613726996816695, 0.34779972303658724, 0.20379540510475636, 7.642889303853735, 0.21159911481663585, 0.2086191310081631, 0.20125566003844142, 0.21017237519845366, 0.21127200010232627, 0.2150911760982126, 0.20773148606531322, 0.20292019098997116, 0.21369990915991366, 0.3879655769560486, 0.1947427170816809, 0.20380831300280988, 0.24166762689128518, 0.21709597599692643, 0.23731210990808904, 0.3558476238977164, 0.20608443394303322, 0.20006645401008427, 0.20113692502491176, 0.20773210912011564, 0.2106448591221124, 0.19257665611803532, 0.19783968990668654, 0.1955079440958798, 0.19720545806922019, 0.19474387005902827, 0.19325175392441452, 0.1951389980968088, 0.20314372912980616, 0.22432057210244238, 0.1978278299793601, 0.20460762199945748, 0.2159815328195691, 0.2134121060371399, 0.34681936888955534, 0.20751182292588055, 0.21576079679653049, 0.21769254095852375, 0.21105021005496383, 0.208315015072003, 0.2084923298098147, 0.22402901505120099, 0.2079262330662459, 0.1976304880809039, 0.19797818013466895, 0.217074410058558, 0.20915505988523364, 0.21556004998274148, 0.2008664000313729, 0.197916405973956, 0.21073263720609248, 0.2176309118513018, 0.2137942831031978, 0.3312655668705702, 0.19968290999531746, 0.24064852902665734, 0.21773998602293432, 0.20548898191191256, 0.19900103099644184, 0.2139937800820917, 0.21018822607584298, 0.2045154480729252, 0.21119109215214849, 0.21638877619989216, 0.24758021300658584, 0.2237272730562836, 0.21184149687178433, 0.21657776413485408, 0.21327737998217344, 0.21307695284485817, 0.3222125871106982, 0.23889947310090065, 0.21254621911793947, 0.215204220963642, 0.21259096218273044, 0.21201535291038454, 0.21296675689518452, 0.2204853028524667, 0.208455178886652, 0.22465572599321604, 0.21626447094604373, 0.21509961294941604, 0.2164703570306301, 0.21780152805149555, 0.22214346495456994, 0.2297455919906497, 0.21014008903875947, 0.22304635494947433, 0.23996251705102623, 0.227123836055398, 0.23310745204798877, 0.21341015794314444, 0.22444704989902675, 0.22868845099583268, 0.2161675130482763, 0.2137113360222429, 0.20193669898435473, 0.2108613010495901, 0.32083542202599347, 0.21742053097113967, 0.2159550180658698, 0.21604266413487494, 0.21899356390349567, 0.22164367884397507]
[0.0018740910856240942, 0.003071564744233854, 0.0020616699457746144, 0.0017107305885834057, 0.002236639589879864, 0.0017069011616210143, 0.002008714520033131, 0.002656123340996199, 0.001850305412094607, 0.0022561045042275227, 0.0026078825805682776, 0.0017681436743154082, 0.0018504476662032133, 0.0019500936587276154, 0.0018262456355313229, 0.0017886779761995919, 0.0017580160557097474, 0.0017741809151489143, 0.001778805774614908, 0.0018727166809199393, 0.0018262767987397982, 0.0017359809690090113, 0.001771552993576656, 0.001692221837493685, 0.0016992758984800227, 0.0017132022627448851, 0.0024095159529369005, 0.001764256952965791, 0.0017453055962371503, 0.0017906379463627589, 0.0018969026354480852, 0.0016634173189513674, 0.001732660836518504, 0.0017362017592400774, 0.0018292049455001604, 0.0017309580467525841, 0.0017726565115586963, 0.0018725400851938383, 0.0022538286821497965, 0.0018742313886480045, 0.001873457085976536, 0.001731075091980571, 0.0018359912638805172, 0.001721316681409529, 0.0029420129460275405, 0.0016942604430481907, 0.0017475360073149204, 0.001691641526065709, 0.0017248618203511302, 0.0017170666197184907, 0.0017299238617654688, 0.0016695685348971639, 0.0017217205505269443, 0.001774551626655829, 0.0017305424169803312, 0.0017568857820574628, 0.0017162460541395948, 0.0018294251617766166, 0.0017225352249490892, 0.001886734464602877, 0.0016829244803290727, 0.0016708845893541973, 0.0015482623635451923, 0.0015979633330865655, 0.0026961218840045524, 0.0015798093418973361, 0.05924720390584291, 0.001640303215632836, 0.0016172025659547528, 0.0015601213956468326, 0.001629243218592664, 0.001637767442653692, 0.001667373458125679, 0.0016103215974055288, 0.0015730247363563655, 0.001656588443100106, 0.0030074850926825472, 0.0015096334657494642, 0.0015799094031225573, 0.0018733924565215905, 0.0016829145426118328, 0.0018396287589774343, 0.0027585087123853983, 0.0015975537514963814, 0.001550902744264219, 0.0015592009691853626, 0.001610326427287718, 0.0016329058846675378, 0.0014928422954886458, 0.0015336410070285779, 0.0015155654581075952, 0.0015287244811567457, 0.0015096424035583587, 0.0014980756118171668, 0.001512705411603169, 0.001574757590153536, 0.0017389191635848247, 0.0015335490696074426, 0.0015861055968950193, 0.001674275448213714, 0.0016543574111406193, 0.0026885222394539172, 0.001608618782371167, 0.0016725643162521743, 0.0016875390771978586, 0.00163604813996096, 0.0016148450780775426, 0.001616219610928796, 0.0017366590314046588, 0.0016118312640794256, 0.0015320192874488674, 0.0015347145746873563, 0.0016827473647950232, 0.0016213570533739042, 0.0016710081394010967, 0.001557103876212193, 0.001534235705224465, 0.0016335863349309495, 0.001687061332180634, 0.001657320024055797, 0.002567950130779614, 0.0015479295348474222, 0.0018800666330207605, 0.0017010936408041744, 0.0016053826711868169, 0.0015546955546597019, 0.0016718264068913413, 0.0016420955162175233, 0.0015977769380697282, 0.00164993040743866, 0.0016905373140616575, 0.0019342204141139518, 0.0017478693207522156, 0.001655011694310815, 0.0016920137823035475, 0.00166622953111073, 0.0016646636941004544, 0.00251728583680233, 0.0018664021336007863, 0.0016605173368589021, 0.0016812829762784531, 0.0016608668920525815, 0.0016563699446123792, 0.001663802788243629, 0.0017225414285348961, 0.0016285560850519687, 0.0017551228593220003, 0.0016895661792659666, 0.0016804657261673128, 0.0016911746643017977, 0.001701574437902309, 0.0017354958199575776, 0.0017948874374269508, 0.0016417194456153084, 0.0017425496480427682, 0.0018747071644611424, 0.0017744049691827968, 0.0018211519691249123, 0.001667266858930816, 0.0017534925773361465, 0.0017866285234049428, 0.0016888086956896586, 0.0016696198126737727, 0.0015776304608152714, 0.0016473539144499227, 0.002506526734578074, 0.0016985978982120287, 0.0016871485786396079, 0.0016878333135537105, 0.00171088721799606, 0.0017315912409685552]
[533.5919943650915, 325.5669612295383, 485.0436909406846, 584.545577587447, 447.09930224105204, 585.8570036066513, 497.83082166574184, 376.48854048507496, 540.4513187193064, 443.2418791444212, 383.4528469384125, 565.5648998021504, 540.4097712483924, 512.7958831743874, 547.5714660416219, 559.0721266243253, 568.8230188525094, 563.6403770672202, 562.1749233507459, 533.9836026391176, 547.5621223957062, 576.0431812630137, 564.4764811585239, 590.9390706605462, 588.4859550438428, 583.7022409705462, 415.0211160798185, 566.8108595626944, 572.9655609630676, 558.4601856736334, 527.1751861759528, 601.1720502167241, 577.1469977986777, 575.9699266965912, 546.6855982759054, 577.7147527498313, 564.1250820333491, 534.0339616262387, 443.6894462830946, 533.5520502200958, 533.77256809635, 577.6756910388401, 544.6649010117937, 580.9506239032862, 339.9033309320587, 590.2280278708936, 572.2342748957113, 591.1417901437569, 579.7565858327306, 582.3885855773889, 578.0601228191932, 598.9571431768715, 580.8143485851657, 563.522630155605, 577.8535043046943, 569.1889650498025, 582.66703517715, 546.6197912293205, 580.5396519711553, 530.0162893936862, 594.203727908494, 598.4853809600959, 645.8853638411853, 625.7965870020546, 370.90311307243246, 632.9877748406079, 16.878433648771413, 609.6433820707933, 618.3517272677756, 640.9757617517936, 613.7819010618914, 610.5872994884361, 599.7456629327156, 620.9939689135083, 635.7179114146187, 603.6502332037403, 332.50372626387417, 662.4124482452073, 632.9476854961333, 533.7909825134791, 594.2072367192395, 543.5879359463016, 362.5147151104185, 625.957029028494, 644.7857570040093, 641.3541421299085, 620.9921063546759, 612.4051663905917, 669.8631215246177, 652.043076193884, 659.8197356969629, 654.1401098275905, 662.408526445013, 667.5230489781483, 661.0672456973613, 635.0183712418251, 575.069860026429, 652.0821666671413, 630.4750465275533, 597.2732868220106, 604.4643033397097, 371.9515447278265, 621.6513265659878, 597.8843326280967, 592.5788703278444, 611.2289581062464, 619.2544495912204, 618.7277974094919, 575.8182705508817, 620.412336133172, 652.7332966317997, 651.5869572710056, 594.2662701015793, 616.7672925091276, 598.4411304892916, 642.2179119048868, 651.7903322121524, 612.1500765628462, 592.74667786229, 603.3837674589835, 389.41566193748713, 646.0242391450778, 531.8960415744784, 587.8571149835469, 622.9044438736382, 643.2127479896758, 598.1482263217978, 608.9779736464083, 625.8695917892634, 606.08616914479, 591.5279075369334, 517.004159765365, 572.1251515357216, 604.2253377650138, 591.0117343362159, 600.1574100858644, 600.7219377367251, 397.2532580051715, 535.7902147650967, 602.2219568581188, 594.7838728573319, 602.0952099082127, 603.7298631580871, 601.0327708704206, 580.5375612071913, 614.0408728804014, 569.7606835263359, 591.8679080297705, 595.0731302808116, 591.304979378254, 587.691009999418, 576.2042112117815, 557.1380016083591, 609.1174729463017, 573.8717408271265, 533.4166417865243, 563.5692062227207, 549.102994672385, 599.7840085667386, 570.290409508987, 559.7134417703171, 592.1333793177978, 598.9387478569591, 633.8620005366976, 607.0340994903428, 398.9584416574479, 588.7208509162857, 592.7160255241575, 592.4755673263217, 584.4920632297943, 577.5034987129271]
Elapsed: 0.2722383577128796~0.5665243832345207
Time per graph: 0.00211423393689612~0.004391342185847684
Speed: 570.0594871107253~80.53891768537048
Total Time: 0.2225
best val loss: 0.3805703694621722 test_score: 0.8594

Testing...
Test loss: 0.3557 score: 0.8906 time: 0.21s
test Score 0.8906
Epoch Time List: [0.81945462198928, 0.9947179439477623, 0.932217397261411, 0.7808945011347532, 0.8554523580241948, 0.7745037549175322, 0.8322925930842757, 0.9746150439605117, 0.7989953809883446, 0.8757224429864436, 0.9614329908508807, 0.7838078939821571, 0.8215428199619055, 0.8864213530905545, 0.8838268469553441, 0.792216117028147, 0.7868550340645015, 0.8464460917748511, 0.7929419942665845, 0.8037890740670264, 0.9461682178080082, 0.7798121289815754, 0.9236926422454417, 0.8638444659300148, 0.7724564992822707, 0.7694621689151973, 0.8313060970976949, 0.7728160498663783, 0.7926631451118737, 0.8380305389873683, 0.8018924701027572, 0.783775727963075, 0.7714064468163997, 0.7625457993708551, 0.7768294350244105, 0.7710058148950338, 0.7807118538767099, 0.7987805148586631, 0.8983670899178833, 0.8510310589335859, 0.8004368410911411, 0.7906228792853653, 0.8150229789316654, 0.7936645748559386, 0.990773974917829, 0.9406256428919733, 0.8144506830722094, 0.7652375688776374, 0.8359228046610951, 0.7687615919858217, 0.7853150689043105, 1.0283140242099762, 0.7704542439896613, 0.8000994401518255, 0.9186927306000143, 0.7706028609536588, 0.7979612553026527, 0.8149969051592052, 0.8601992991752923, 0.806622157106176, 0.8077224779408425, 0.9674452298786491, 0.7655753458384424, 0.7638507417868823, 0.9147166390903294, 0.751895149005577, 11.196250804234296, 0.8467442921828479, 0.7639824231155217, 0.7687225409317762, 0.8118573939427733, 0.7587325260974467, 0.7895413029473275, 0.8756481751333922, 0.73265281599015, 0.7690913709811866, 0.9564482558052987, 0.769211343023926, 0.7366627848241478, 0.8752417860087007, 0.7693089921958745, 0.7661231171805412, 0.883639156119898, 0.7364788760896772, 0.7238840381614864, 0.7430941890925169, 0.7760806400328875, 0.7739087881054729, 0.7152152399066836, 0.7245297390036285, 0.7243432791437954, 0.7328273057937622, 0.7223857347853482, 0.7119435202330351, 0.7186387467663735, 0.725269417045638, 0.7640491770580411, 0.7834269409067929, 0.8181001718621701, 0.7795703238807619, 0.7732905230950564, 0.892006458947435, 0.7655853908509016, 0.7962524697650224, 0.8218729989603162, 0.8934257721994072, 0.760281759314239, 0.7649964920710772, 0.9299333060625941, 0.7639335710555315, 0.7502931249327958, 0.8620608646888286, 1.0660538419615477, 0.8067153259180486, 0.8838286011014134, 0.7500715630594641, 0.7283046999946237, 0.8783615590073168, 0.8030854861717671, 0.7802541179116815, 0.8937968011014163, 0.7429365357384086, 0.8721796153113246, 0.7821106982883066, 0.7903581280261278, 0.9016342679969966, 0.7700178271625191, 0.7844137800857425, 0.820265609305352, 0.7684732049237937, 0.8237272808328271, 0.9829769788775593, 0.8273857270833105, 0.7763266221154481, 0.9193490978796035, 0.784225539304316, 0.7908020301256329, 0.9015742528717965, 0.8436904710251838, 0.7791401678696275, 0.8038944590371102, 0.9167306870222092, 0.7617364809848368, 0.7765425341203809, 0.901365090161562, 0.7537086713127792, 0.7740359392482787, 0.842050647130236, 0.8163382110651582, 0.8237430171575397, 0.8172307871282101, 0.8305837302468717, 0.8195646030362695, 0.7857261372264475, 0.8253757101483643, 0.8281989311799407, 0.8680571450386196, 0.8342350209131837, 0.9216759731061757, 0.8228059741668403, 0.8050808431580663, 0.813203050987795, 0.8645382928662002, 0.7450313521549106, 0.7597667381633073, 0.8490727131720632, 0.7774844111409038, 0.8055163640528917, 0.7923028809018433, 1.0091946630273014, 0.8044710690155625]
Total Epoch List: [60, 62, 49]
Total Time List: [0.2439081040211022, 0.20028333505615592, 0.222545871976763]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda88ced70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7513;  Loss pred: 0.7513; Loss self: 0.0000; time: 0.38s
Val loss: 0.7099 score: 0.5736 time: 0.22s
Test loss: 0.6690 score: 0.6279 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 0.7232;  Loss pred: 0.7232; Loss self: 0.0000; time: 0.33s
Val loss: 0.6565 score: 0.6357 time: 0.22s
Test loss: 0.6253 score: 0.6977 time: 0.24s
Epoch 3/1000, LR 0.000045
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.34s
Val loss: 0.6482 score: 0.6202 time: 0.23s
Test loss: 0.6161 score: 0.6589 time: 0.36s
Epoch 4/1000, LR 0.000075
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.35s
Val loss: 0.6611 score: 0.6667 time: 0.22s
Test loss: 0.6326 score: 0.6357 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 0.35s
Val loss: 0.6671 score: 0.6512 time: 0.22s
Test loss: 0.6426 score: 0.6434 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.4469;  Loss pred: 0.4469; Loss self: 0.0000; time: 0.37s
Val loss: 0.6679 score: 0.6279 time: 0.22s
Test loss: 0.6446 score: 0.6279 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.3940;  Loss pred: 0.3940; Loss self: 0.0000; time: 0.48s
Val loss: 0.6615 score: 0.6279 time: 0.22s
Test loss: 0.6416 score: 0.6279 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.3519;  Loss pred: 0.3519; Loss self: 0.0000; time: 0.33s
Val loss: 0.6582 score: 0.6279 time: 0.21s
Test loss: 0.6393 score: 0.6279 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.3022;  Loss pred: 0.3022; Loss self: 0.0000; time: 0.33s
Val loss: 0.6535 score: 0.6434 time: 0.21s
Test loss: 0.6370 score: 0.6357 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.2684;  Loss pred: 0.2684; Loss self: 0.0000; time: 0.34s
Val loss: 0.6481 score: 0.6512 time: 0.26s
Test loss: 0.6348 score: 0.6357 time: 0.22s
Epoch 11/1000, LR 0.000285
Train loss: 0.2312;  Loss pred: 0.2312; Loss self: 0.0000; time: 0.35s
Val loss: 0.6542 score: 0.6357 time: 0.20s
Test loss: 0.6396 score: 0.6357 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.2073;  Loss pred: 0.2073; Loss self: 0.0000; time: 0.36s
Val loss: 0.6668 score: 0.6434 time: 0.23s
Test loss: 0.6476 score: 0.6512 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 0.37s
Val loss: 0.6883 score: 0.6434 time: 0.52s
Test loss: 0.6609 score: 0.6512 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 0.37s
Val loss: 0.7134 score: 0.6434 time: 0.23s
Test loss: 0.6797 score: 0.6434 time: 0.24s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.37s
Val loss: 0.7489 score: 0.5891 time: 0.23s
Test loss: 0.7087 score: 0.6124 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.1282;  Loss pred: 0.1282; Loss self: 0.0000; time: 0.35s
Val loss: 0.7973 score: 0.5736 time: 0.26s
Test loss: 0.7446 score: 0.5814 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.36s
Val loss: 0.8076 score: 0.5659 time: 0.22s
Test loss: 0.7473 score: 0.5814 time: 0.37s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.34s
Val loss: 0.7995 score: 0.5736 time: 0.22s
Test loss: 0.7338 score: 0.5969 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.0910;  Loss pred: 0.0910; Loss self: 0.0000; time: 0.47s
Val loss: 0.7847 score: 0.5659 time: 0.22s
Test loss: 0.7178 score: 0.5891 time: 0.24s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.38s
Val loss: 0.7509 score: 0.5736 time: 0.22s
Test loss: 0.6830 score: 0.5969 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.35s
Val loss: 0.7029 score: 0.6124 time: 0.23s
Test loss: 0.6434 score: 0.6047 time: 0.23s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.41s
Val loss: 0.6461 score: 0.6202 time: 0.22s
Test loss: 0.5999 score: 0.6434 time: 0.22s
Epoch 23/1000, LR 0.000285
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.35s
Val loss: 0.5782 score: 0.6899 time: 0.22s
Test loss: 0.5467 score: 0.6977 time: 0.21s
Epoch 24/1000, LR 0.000285
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.34s
Val loss: 0.5445 score: 0.7442 time: 0.22s
Test loss: 0.5228 score: 0.7364 time: 0.24s
Epoch 25/1000, LR 0.000285
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.49s
Val loss: 0.5132 score: 0.7752 time: 0.24s
Test loss: 0.4993 score: 0.7442 time: 0.22s
Epoch 26/1000, LR 0.000285
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.34s
Val loss: 0.4882 score: 0.8062 time: 0.22s
Test loss: 0.4824 score: 0.7519 time: 0.22s
Epoch 27/1000, LR 0.000285
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.34s
Val loss: 0.4698 score: 0.8372 time: 0.24s
Test loss: 0.4648 score: 0.7519 time: 0.27s
Epoch 28/1000, LR 0.000285
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.46s
Val loss: 0.4415 score: 0.8682 time: 0.22s
Test loss: 0.4406 score: 0.7674 time: 0.22s
Epoch 29/1000, LR 0.000285
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.34s
Val loss: 0.4102 score: 0.8760 time: 0.22s
Test loss: 0.4157 score: 0.8295 time: 0.23s
Epoch 30/1000, LR 0.000285
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.35s
Val loss: 0.3796 score: 0.8915 time: 0.22s
Test loss: 0.3882 score: 0.8760 time: 0.24s
Epoch 31/1000, LR 0.000285
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.49s
Val loss: 0.3605 score: 0.9147 time: 0.22s
Test loss: 0.3670 score: 0.8837 time: 0.23s
Epoch 32/1000, LR 0.000285
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.36s
Val loss: 0.3542 score: 0.9380 time: 0.22s
Test loss: 0.3555 score: 0.8915 time: 0.22s
Epoch 33/1000, LR 0.000285
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.35s
Val loss: 0.3307 score: 0.9535 time: 0.22s
Test loss: 0.3351 score: 0.9070 time: 0.22s
Epoch 34/1000, LR 0.000285
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.49s
Val loss: 0.2988 score: 0.9302 time: 0.23s
Test loss: 0.3024 score: 0.9147 time: 0.28s
Epoch 35/1000, LR 0.000285
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.34s
Val loss: 0.2648 score: 0.9457 time: 0.31s
Test loss: 0.2652 score: 0.9070 time: 0.23s
Epoch 36/1000, LR 0.000285
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.34s
Val loss: 0.2451 score: 0.9457 time: 0.21s
Test loss: 0.2417 score: 0.8992 time: 0.36s
Epoch 37/1000, LR 0.000285
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.34s
Val loss: 0.2402 score: 0.9302 time: 0.22s
Test loss: 0.2375 score: 0.8992 time: 0.23s
Epoch 38/1000, LR 0.000284
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.33s
Val loss: 0.2331 score: 0.9302 time: 0.21s
Test loss: 0.2265 score: 0.9070 time: 0.21s
Epoch 39/1000, LR 0.000284
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.33s
Val loss: 0.2097 score: 0.9457 time: 0.21s
Test loss: 0.1957 score: 0.9070 time: 0.21s
Epoch 40/1000, LR 0.000284
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.34s
Val loss: 0.1983 score: 0.9457 time: 0.22s
Test loss: 0.1842 score: 0.9070 time: 0.22s
Epoch 41/1000, LR 0.000284
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.34s
Val loss: 0.1989 score: 0.9070 time: 0.22s
Test loss: 0.1884 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.35s
Val loss: 0.2033 score: 0.9147 time: 0.34s
Test loss: 0.1877 score: 0.9380 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.34s
Val loss: 0.2174 score: 0.8992 time: 0.23s
Test loss: 0.2045 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.35s
Val loss: 0.2080 score: 0.8992 time: 0.22s
Test loss: 0.1897 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.36s
Val loss: 0.1998 score: 0.8992 time: 0.21s
Test loss: 0.1826 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.35s
Val loss: 0.2040 score: 0.8992 time: 0.22s
Test loss: 0.1797 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.34s
Val loss: 0.1924 score: 0.9380 time: 0.22s
Test loss: 0.1517 score: 0.9225 time: 0.22s
Epoch 48/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.36s
Val loss: 0.2085 score: 0.9380 time: 0.21s
Test loss: 0.1486 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.33s
Val loss: 0.2476 score: 0.9457 time: 0.20s
Test loss: 0.1781 score: 0.9147 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.36s
Val loss: 0.3043 score: 0.9380 time: 0.23s
Test loss: 0.2306 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.37s
Val loss: 0.3702 score: 0.9225 time: 0.23s
Test loss: 0.2959 score: 0.8992 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.40s
Val loss: 0.4575 score: 0.9225 time: 0.21s
Test loss: 0.3987 score: 0.8682 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.34s
Val loss: 0.5655 score: 0.9070 time: 0.23s
Test loss: 0.5586 score: 0.8450 time: 0.24s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.34s
Val loss: 0.6576 score: 0.8992 time: 0.21s
Test loss: 0.6895 score: 0.8140 time: 0.21s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.34s
Val loss: 0.5784 score: 0.9070 time: 0.22s
Test loss: 0.5534 score: 0.8450 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.45s
Val loss: 0.4970 score: 0.9225 time: 0.21s
Test loss: 0.4375 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.35s
Val loss: 0.5224 score: 0.9225 time: 0.21s
Test loss: 0.4682 score: 0.8682 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.34s
Val loss: 0.4691 score: 0.9225 time: 0.22s
Test loss: 0.3973 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.35s
Val loss: 0.3569 score: 0.9380 time: 0.44s
Test loss: 0.2907 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.33s
Val loss: 0.3229 score: 0.9380 time: 0.21s
Test loss: 0.2626 score: 0.9225 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.34s
Val loss: 0.2511 score: 0.9225 time: 0.21s
Test loss: 0.1766 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.47s
Val loss: 0.2397 score: 0.9225 time: 0.21s
Test loss: 0.1776 score: 0.9225 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.34s
Val loss: 0.2509 score: 0.9147 time: 0.23s
Test loss: 0.1933 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.33s
Val loss: 0.2434 score: 0.9225 time: 0.21s
Test loss: 0.1865 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.34s
Val loss: 0.2362 score: 0.9225 time: 0.22s
Test loss: 0.1746 score: 0.9225 time: 0.35s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.34s
Val loss: 0.2321 score: 0.9302 time: 0.22s
Test loss: 0.1673 score: 0.9302 time: 0.29s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.35s
Val loss: 0.2345 score: 0.9302 time: 0.22s
Test loss: 0.1689 score: 0.9302 time: 0.23s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0126,   Val_Loss: 0.1924,   Val_Precision: 0.9667,   Val_Recall: 0.9062,   Val_accuracy: 0.9355,   Val_Score: 0.9380,   Val_Loss: 0.1924,   Test_Precision: 0.9661,   Test_Recall: 0.8769,   Test_accuracy: 0.9194,   Test_Score: 0.9225,   Test_loss: 0.1517


[0.22357910708524287, 0.24239516886882484, 0.3642447730526328, 0.23412813991308212, 0.22797992918640375, 0.22992742201313376, 0.2312316989991814, 0.21932963514700532, 0.21858455706387758, 0.23075828002765775, 0.21616878313943744, 0.24013428203761578, 0.24599635787308216, 0.24710278795100749, 0.22794162994250655, 0.22769634402357042, 0.3776605378370732, 0.23835298884660006, 0.2478047979529947, 0.22232565400190651, 0.23350003897212446, 0.22905854205600917, 0.2189835598692298, 0.24208540888503194, 0.22870979784056544, 0.22269868501462042, 0.2771703649777919, 0.22945774486288428, 0.233440559823066, 0.24295867700129747, 0.23149742395617068, 0.22545669600367546, 0.2208383148536086, 0.28655872610397637, 0.2315491798799485, 0.36736226407811046, 0.23636411898769438, 0.21508719795383513, 0.2136987680569291, 0.22428431711159647, 0.23135620588436723, 0.22881575813516974, 0.23515666206367314, 0.22804595902562141, 0.24307246785610914, 0.22970413090661168, 0.22572798002511263, 0.22194186178967357, 0.23193041211925447, 0.22827008296735585, 0.2632522680796683, 0.21958617097698152, 0.24423585389740765, 0.2207943699322641, 0.23634692002087831, 0.22525568399578333, 0.22752502607181668, 0.22082905401475728, 0.2219722520094365, 0.22247107001021504, 0.21851889207027853, 0.21625497401691973, 0.22657870198599994, 0.22330913902260363, 0.3538089918438345, 0.2996434529777616, 0.2318804159294814]
[0.0017331713727538206, 0.0018790323168125956, 0.0028236028918808744, 0.0018149468210316444, 0.0017672862727628198, 0.001782383116380882, 0.0017924937906913286, 0.0017002297298217466, 0.0016944539307277332, 0.0017888238761833933, 0.0016757270010809104, 0.0018615060623070991, 0.0019069485106440478, 0.001915525487992306, 0.0017669893793992755, 0.0017650879381672126, 0.0029276010685044436, 0.00184769758795814, 0.001920967425992207, 0.0017234546821853218, 0.0018100778214893368, 0.0017756476128372805, 0.0016975469757304635, 0.0018766310766281545, 0.0017729441693067089, 0.0017263463954621738, 0.0021486074804479994, 0.001778742208239413, 0.001809616742814465, 0.0018834005969092828, 0.0017945536740788424, 0.0017477263256098874, 0.0017119249213458031, 0.0022213854736742355, 0.0017949548827902984, 0.0028477694889776006, 0.0018322799921526697, 0.0016673426197971715, 0.0016565795973405358, 0.0017386381171441586, 0.001793458960343932, 0.0017737655669393003, 0.0018229198609587066, 0.0017677981319815614, 0.0018842826965589855, 0.001780652177570633, 0.001749829302520253, 0.0017204795487571595, 0.001797910171467089, 0.0017695355268787275, 0.0020407152564315373, 0.0017022183796665234, 0.0018933011930031601, 0.001711584263040807, 0.001832146666828514, 0.0017461680929905684, 0.0017637598920295866, 0.0017118531318973433, 0.0017207151318560968, 0.0017245819380636825, 0.001693944899769601, 0.0016763951474179823, 0.0017564240464031002, 0.0017310785970744468, 0.0027427053631305003, 0.0023228174649438885, 0.0017975226041045068]
[576.9769889581714, 532.1888245627946, 354.15744999959054, 550.9803308901272, 565.8392844508932, 561.046607101224, 557.8819883188104, 588.1558135704643, 590.1606304342075, 559.0265276051572, 596.7559151072701, 537.1994323567379, 524.3980078215445, 522.049957710621, 565.9343579869001, 566.5440108544137, 341.5765934635511, 541.2141069605896, 520.5710344013178, 580.229936032905, 552.4624345583094, 563.1748060653291, 589.0853179893268, 532.8697858914041, 564.0335535162618, 579.258022971851, 465.4177224550541, 562.1950136269568, 552.6031984235058, 530.9544881959952, 557.2416219388408, 572.171961563284, 584.1377665171586, 450.1695054059984, 557.1170671685503, 351.15201699805334, 545.7681163811332, 599.7567555261364, 603.6534565591625, 575.162818610335, 557.5817579947466, 563.7723601352449, 548.5704673128538, 565.6754478403478, 530.7059295434633, 561.5919900563135, 571.4843148184311, 581.233296683114, 556.2013140979136, 565.1200469333858, 490.0242681326514, 587.4686890620386, 528.1779801838063, 584.2540280333008, 545.8078319303016, 572.6825521633222, 566.970597596073, 584.1622633196597, 581.1537200357635, 579.8506744902912, 590.3379738833378, 596.5180712555868, 569.3385956812956, 577.6745213591212, 364.60350916388944, 430.51165883331976, 556.3212377505438]
Elapsed: 0.24030429878925433~0.03510740561259801
Time per graph: 0.0018628240216221267~0.00027215043110541095
Speed: 545.2099750334337~58.06371697399626
Total Time: 0.2324
best val loss: 0.1923563392334726 test_score: 0.9225

Testing...
Test loss: 0.3351 score: 0.9070 time: 0.22s
test Score 0.9070
Epoch Time List: [0.8176490690093488, 0.7852276710327715, 0.9247142921667546, 0.7980746089015156, 0.7957909472752362, 0.81363948690705, 0.9245832308661193, 0.750636235345155, 0.7583490870893002, 0.821612891042605, 0.7662683839444071, 0.8237649509683251, 1.130693505750969, 0.8393322110641748, 0.8252688201609999, 0.8340627460274845, 0.9559119797777385, 0.7895017270930111, 0.9323012290988117, 0.812568096909672, 0.8110674489289522, 0.8557941229082644, 0.7893569369334728, 0.7954422407783568, 0.9490186341572553, 0.7751659147907048, 0.8507713361177593, 0.901724579744041, 0.7956568161025643, 0.8012104779481888, 0.9331615341361612, 0.8033235969487578, 0.7899528350681067, 1.0006292047910392, 0.876933928579092, 0.9079464951064438, 0.7928618520963937, 0.7474387530237436, 0.7444294777233154, 0.7792467726394534, 0.787847087951377, 0.9177205178420991, 0.7925688473042101, 0.7882598089054227, 0.8090140928979963, 0.7913336891215295, 0.7805726390797645, 0.7869484659750015, 0.7572306888177991, 0.812423107912764, 0.8518422411289066, 0.8193880489561707, 0.8075771320145577, 0.7675648897420615, 0.7841835527215153, 0.8837106982246041, 0.7837793598882854, 0.7684130559209734, 1.0051145339384675, 0.7569330099504441, 0.7600822180975229, 0.888088540174067, 0.7871246400754899, 0.7663860430475324, 0.9111245272215456, 0.8534962069243193, 0.7957381443120539]
Total Epoch List: [67]
Total Time List: [0.2323850328102708]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bda0504a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7860;  Loss pred: 0.7860; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6383 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.4961 time: 0.42s
Epoch 2/1000, LR 0.000015
Train loss: 0.7214;  Loss pred: 0.7214; Loss self: 0.0000; time: 0.35s
Val loss: 0.7036 score: 0.3953 time: 0.22s
Test loss: 0.7128 score: 0.4264 time: 0.34s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6247;  Loss pred: 0.6247; Loss self: 0.0000; time: 0.35s
Val loss: 0.7390 score: 0.3953 time: 0.23s
Test loss: 0.7519 score: 0.4109 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.4814;  Loss pred: 0.4814; Loss self: 0.0000; time: 0.34s
Val loss: 0.6926 score: 0.4341 time: 4.85s
Test loss: 0.7353 score: 0.4264 time: 1.88s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.3905;  Loss pred: 0.3905; Loss self: 0.0000; time: 0.33s
Val loss: 0.6480 score: 0.5039 time: 4.40s
Test loss: 0.7143 score: 0.4651 time: 5.78s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 2.95s
Val loss: 0.6227 score: 0.5814 time: 0.80s
Test loss: 0.7007 score: 0.5349 time: 0.22s
Epoch 7/1000, LR 0.000165
Train loss: 0.2424;  Loss pred: 0.2424; Loss self: 0.0000; time: 0.34s
Val loss: 0.6140 score: 0.6047 time: 0.22s
Test loss: 0.6909 score: 0.5194 time: 0.22s
Epoch 8/1000, LR 0.000195
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.33s
Val loss: 0.6056 score: 0.6279 time: 0.21s
Test loss: 0.6790 score: 0.5814 time: 0.21s
Epoch 9/1000, LR 0.000225
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 0.35s
Val loss: 0.5969 score: 0.6667 time: 0.32s
Test loss: 0.6653 score: 0.6124 time: 0.20s
Epoch 10/1000, LR 0.000255
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.35s
Val loss: 0.5878 score: 0.6512 time: 0.22s
Test loss: 0.6535 score: 0.6124 time: 0.28s
Epoch 11/1000, LR 0.000285
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.41s
Val loss: 0.5822 score: 0.6589 time: 0.21s
Test loss: 0.6400 score: 0.6124 time: 0.21s
Epoch 12/1000, LR 0.000285
Train loss: 0.1102;  Loss pred: 0.1102; Loss self: 0.0000; time: 0.34s
Val loss: 0.5783 score: 0.7132 time: 0.28s
Test loss: 0.6253 score: 0.6279 time: 0.22s
Epoch 13/1000, LR 0.000285
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.35s
Val loss: 0.5757 score: 0.6977 time: 0.22s
Test loss: 0.6121 score: 0.6512 time: 0.22s
Epoch 14/1000, LR 0.000285
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.35s
Val loss: 0.5742 score: 0.7132 time: 0.23s
Test loss: 0.6017 score: 0.6744 time: 0.21s
Epoch 15/1000, LR 0.000285
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.35s
Val loss: 0.5791 score: 0.6744 time: 0.35s
Test loss: 0.5952 score: 0.6899 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.33s
Val loss: 0.5876 score: 0.6744 time: 0.22s
Test loss: 0.5906 score: 0.6977 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.34s
Val loss: 0.5955 score: 0.7054 time: 0.23s
Test loss: 0.5881 score: 0.7209 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.34s
Val loss: 0.6033 score: 0.7132 time: 0.22s
Test loss: 0.5876 score: 0.7364 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.34s
Val loss: 0.6202 score: 0.7364 time: 0.22s
Test loss: 0.6051 score: 0.7597 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.34s
Val loss: 0.6237 score: 0.7364 time: 0.21s
Test loss: 0.6211 score: 0.7907 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 0.31s
Val loss: 0.6206 score: 0.7287 time: 0.21s
Test loss: 0.6303 score: 0.7907 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.31s
Val loss: 0.6087 score: 0.7287 time: 0.20s
Test loss: 0.6322 score: 0.7752 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.32s
Val loss: 0.5918 score: 0.7597 time: 0.22s
Test loss: 0.6288 score: 0.7907 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.33s
Val loss: 0.5758 score: 0.7829 time: 0.21s
Test loss: 0.6233 score: 0.8062 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.35s
Val loss: 0.5565 score: 0.7829 time: 0.21s
Test loss: 0.6099 score: 0.8217 time: 0.21s
Epoch 26/1000, LR 0.000285
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.35s
Val loss: 0.5259 score: 0.7907 time: 0.22s
Test loss: 0.5834 score: 0.8450 time: 0.23s
Epoch 27/1000, LR 0.000285
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.34s
Val loss: 0.5006 score: 0.7907 time: 0.22s
Test loss: 0.5616 score: 0.8450 time: 0.24s
Epoch 28/1000, LR 0.000285
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.34s
Val loss: 0.4720 score: 0.8062 time: 0.21s
Test loss: 0.5374 score: 0.8605 time: 0.21s
Epoch 29/1000, LR 0.000285
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.33s
Val loss: 0.4529 score: 0.8140 time: 0.35s
Test loss: 0.5297 score: 0.8372 time: 0.21s
Epoch 30/1000, LR 0.000285
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.33s
Val loss: 0.4353 score: 0.8217 time: 0.21s
Test loss: 0.5186 score: 0.8295 time: 0.33s
Epoch 31/1000, LR 0.000285
Train loss: 0.0214;  Loss pred: 0.0214; Loss self: 0.0000; time: 0.34s
Val loss: 0.4001 score: 0.8295 time: 0.22s
Test loss: 0.4724 score: 0.8450 time: 0.22s
Epoch 32/1000, LR 0.000285
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.33s
Val loss: 0.3622 score: 0.8682 time: 0.21s
Test loss: 0.4204 score: 0.8527 time: 0.22s
Epoch 33/1000, LR 0.000285
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.33s
Val loss: 0.3392 score: 0.8837 time: 0.22s
Test loss: 0.3867 score: 0.8682 time: 0.22s
Epoch 34/1000, LR 0.000285
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.33s
Val loss: 0.3241 score: 0.8837 time: 0.37s
Test loss: 0.3734 score: 0.8837 time: 0.21s
Epoch 35/1000, LR 0.000285
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.33s
Val loss: 0.3178 score: 0.8915 time: 0.22s
Test loss: 0.3796 score: 0.8682 time: 0.21s
Epoch 36/1000, LR 0.000285
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.33s
Val loss: 0.3377 score: 0.8760 time: 0.21s
Test loss: 0.4415 score: 0.8450 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.33s
Val loss: 0.3495 score: 0.8760 time: 0.28s
Test loss: 0.4722 score: 0.8295 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.32s
Val loss: 0.3519 score: 0.8682 time: 0.20s
Test loss: 0.4795 score: 0.8295 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.34s
Val loss: 0.3185 score: 0.8837 time: 0.21s
Test loss: 0.4057 score: 0.8992 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.33s
Val loss: 0.3378 score: 0.8760 time: 0.21s
Test loss: 0.4438 score: 0.8837 time: 0.35s
     INFO: Early stopping counter 5 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.33s
Val loss: 0.3549 score: 0.8760 time: 0.21s
Test loss: 0.4709 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.34s
Val loss: 0.3758 score: 0.8682 time: 0.22s
Test loss: 0.4973 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 7 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.36s
Val loss: 0.3945 score: 0.8837 time: 0.25s
Test loss: 0.5209 score: 0.8915 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.47s
Val loss: 0.4103 score: 0.8992 time: 0.22s
Test loss: 0.5325 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.37s
Val loss: 0.4259 score: 0.8992 time: 0.22s
Test loss: 0.5364 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.35s
Val loss: 0.4449 score: 0.8992 time: 0.22s
Test loss: 0.5637 score: 0.8915 time: 0.27s
     INFO: Early stopping counter 11 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.34s
Val loss: 0.4468 score: 0.8992 time: 0.30s
Test loss: 0.5529 score: 0.8992 time: 0.29s
     INFO: Early stopping counter 12 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.34s
Val loss: 0.4422 score: 0.8992 time: 0.29s
Test loss: 0.5442 score: 0.8915 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.35s
Val loss: 0.4034 score: 0.9070 time: 0.37s
Test loss: 0.4931 score: 0.8915 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.32s
Val loss: 0.3733 score: 0.9225 time: 0.24s
Test loss: 0.4503 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.35s
Val loss: 0.3780 score: 0.9225 time: 0.24s
Test loss: 0.4416 score: 0.9070 time: 0.22s
     INFO: Early stopping counter 16 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.35s
Val loss: 0.3919 score: 0.9225 time: 0.32s
Test loss: 0.4594 score: 0.9070 time: 0.26s
     INFO: Early stopping counter 17 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.41s
Val loss: 0.4125 score: 0.9147 time: 0.26s
Test loss: 0.4818 score: 0.9070 time: 0.25s
     INFO: Early stopping counter 18 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.36s
Val loss: 0.4536 score: 0.9225 time: 0.24s
Test loss: 0.5423 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 19 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.36s
Val loss: 0.4941 score: 0.9070 time: 0.24s
Test loss: 0.5873 score: 0.9070 time: 0.23s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 034,   Train_Loss: 0.0118,   Val_Loss: 0.3178,   Val_Precision: 0.9811,   Val_Recall: 0.8000,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.3178,   Test_Precision: 0.9608,   Test_Recall: 0.7656,   Test_accuracy: 0.8522,   Test_Score: 0.8682,   Test_loss: 0.3796


[0.22357910708524287, 0.24239516886882484, 0.3642447730526328, 0.23412813991308212, 0.22797992918640375, 0.22992742201313376, 0.2312316989991814, 0.21932963514700532, 0.21858455706387758, 0.23075828002765775, 0.21616878313943744, 0.24013428203761578, 0.24599635787308216, 0.24710278795100749, 0.22794162994250655, 0.22769634402357042, 0.3776605378370732, 0.23835298884660006, 0.2478047979529947, 0.22232565400190651, 0.23350003897212446, 0.22905854205600917, 0.2189835598692298, 0.24208540888503194, 0.22870979784056544, 0.22269868501462042, 0.2771703649777919, 0.22945774486288428, 0.233440559823066, 0.24295867700129747, 0.23149742395617068, 0.22545669600367546, 0.2208383148536086, 0.28655872610397637, 0.2315491798799485, 0.36736226407811046, 0.23636411898769438, 0.21508719795383513, 0.2136987680569291, 0.22428431711159647, 0.23135620588436723, 0.22881575813516974, 0.23515666206367314, 0.22804595902562141, 0.24307246785610914, 0.22970413090661168, 0.22572798002511263, 0.22194186178967357, 0.23193041211925447, 0.22827008296735585, 0.2632522680796683, 0.21958617097698152, 0.24423585389740765, 0.2207943699322641, 0.23634692002087831, 0.22525568399578333, 0.22752502607181668, 0.22082905401475728, 0.2219722520094365, 0.22247107001021504, 0.21851889207027853, 0.21625497401691973, 0.22657870198599994, 0.22330913902260363, 0.3538089918438345, 0.2996434529777616, 0.2318804159294814, 0.4247398420702666, 0.34392285090871155, 0.23834238317795098, 1.8897640148643404, 5.785503029124811, 0.22105473186820745, 0.2214994439855218, 0.21065929788164794, 0.20555413095280528, 0.28911012411117554, 0.21516157500445843, 0.2270989881362766, 0.22315962985157967, 0.21619799989275634, 0.22075085202232003, 0.23359059309586883, 0.22414065012708306, 0.21029090089723468, 0.21032428299076855, 0.20789233897812665, 0.20398874999955297, 0.21230363100767136, 0.21490445104427636, 0.21018038014881313, 0.21704530506394804, 0.2310951801482588, 0.246273058000952, 0.22028120094910264, 0.21566824382171035, 0.3310373988933861, 0.22071003983728588, 0.22754413401708007, 0.221762428060174, 0.2123332538176328, 0.21158833196386695, 0.22470934595912695, 0.22225840692408383, 0.20856687100604177, 0.21118672587908804, 0.35505646793171763, 0.21440043905749917, 0.24085702514275908, 0.21684232400730252, 0.22190482798032463, 0.22043650993146002, 0.2723625930957496, 0.29368764883838594, 0.22731107100844383, 0.20584613014943898, 0.2227208709809929, 0.22888713493011892, 0.26585738314315677, 0.2540366400498897, 0.23798727500252426, 0.24227326014079154]
[0.0017331713727538206, 0.0018790323168125956, 0.0028236028918808744, 0.0018149468210316444, 0.0017672862727628198, 0.001782383116380882, 0.0017924937906913286, 0.0017002297298217466, 0.0016944539307277332, 0.0017888238761833933, 0.0016757270010809104, 0.0018615060623070991, 0.0019069485106440478, 0.001915525487992306, 0.0017669893793992755, 0.0017650879381672126, 0.0029276010685044436, 0.00184769758795814, 0.001920967425992207, 0.0017234546821853218, 0.0018100778214893368, 0.0017756476128372805, 0.0016975469757304635, 0.0018766310766281545, 0.0017729441693067089, 0.0017263463954621738, 0.0021486074804479994, 0.001778742208239413, 0.001809616742814465, 0.0018834005969092828, 0.0017945536740788424, 0.0017477263256098874, 0.0017119249213458031, 0.0022213854736742355, 0.0017949548827902984, 0.0028477694889776006, 0.0018322799921526697, 0.0016673426197971715, 0.0016565795973405358, 0.0017386381171441586, 0.001793458960343932, 0.0017737655669393003, 0.0018229198609587066, 0.0017677981319815614, 0.0018842826965589855, 0.001780652177570633, 0.001749829302520253, 0.0017204795487571595, 0.001797910171467089, 0.0017695355268787275, 0.0020407152564315373, 0.0017022183796665234, 0.0018933011930031601, 0.001711584263040807, 0.001832146666828514, 0.0017461680929905684, 0.0017637598920295866, 0.0017118531318973433, 0.0017207151318560968, 0.0017245819380636825, 0.001693944899769601, 0.0016763951474179823, 0.0017564240464031002, 0.0017310785970744468, 0.0027427053631305003, 0.0023228174649438885, 0.0017975226041045068, 0.0032925569152733847, 0.0026660686116954384, 0.0018476153734724881, 0.01464933344856078, 0.04484886069089001, 0.0017136025726217632, 0.001717049953376138, 0.0016330178130360305, 0.0015934428756031417, 0.0022411637527998103, 0.001667919186081073, 0.0017604572723742373, 0.0017299196112525556, 0.001675953487540747, 0.0017112469149017056, 0.0018107797914408436, 0.0017375244195897911, 0.0016301620224591835, 0.0016304207983780508, 0.001611568519210284, 0.0015813081395314183, 0.0016457645814548167, 0.0016659259770874135, 0.0016293052724714195, 0.0016825217446817676, 0.0017914355050252621, 0.001909093472875597, 0.0017076062089077723, 0.001671846851331088, 0.002566181386770435, 0.0017109305413743091, 0.0017639080156362796, 0.0017190885896137518, 0.0016459942156405642, 0.0016402196276268757, 0.001741932914411837, 0.0017229333870084017, 0.0016167974496592386, 0.0016371064021634732, 0.0027523757204009117, 0.001662018907422474, 0.0018671087220368922, 0.0016809482481186242, 0.001720192464963757, 0.001708810154507442, 0.0021113379309748033, 0.002276648440607643, 0.0017621013256468514, 0.0015957064352669689, 0.0017265183796976195, 0.0017743188754272784, 0.0020609099468461766, 0.001969276279456509, 0.0018448625969187927, 0.0018780872879131127]
[576.9769889581714, 532.1888245627946, 354.15744999959054, 550.9803308901272, 565.8392844508932, 561.046607101224, 557.8819883188104, 588.1558135704643, 590.1606304342075, 559.0265276051572, 596.7559151072701, 537.1994323567379, 524.3980078215445, 522.049957710621, 565.9343579869001, 566.5440108544137, 341.5765934635511, 541.2141069605896, 520.5710344013178, 580.229936032905, 552.4624345583094, 563.1748060653291, 589.0853179893268, 532.8697858914041, 564.0335535162618, 579.258022971851, 465.4177224550541, 562.1950136269568, 552.6031984235058, 530.9544881959952, 557.2416219388408, 572.171961563284, 584.1377665171586, 450.1695054059984, 557.1170671685503, 351.15201699805334, 545.7681163811332, 599.7567555261364, 603.6534565591625, 575.162818610335, 557.5817579947466, 563.7723601352449, 548.5704673128538, 565.6754478403478, 530.7059295434633, 561.5919900563135, 571.4843148184311, 581.233296683114, 556.2013140979136, 565.1200469333858, 490.0242681326514, 587.4686890620386, 528.1779801838063, 584.2540280333008, 545.8078319303016, 572.6825521633222, 566.970597596073, 584.1622633196597, 581.1537200357635, 579.8506744902912, 590.3379738833378, 596.5180712555868, 569.3385956812956, 577.6745213591212, 364.60350916388944, 430.51165883331976, 556.3212377505438, 303.71532694278994, 375.08412034605067, 541.2381896999248, 68.26249149910946, 22.29711044149504, 583.5658839319017, 582.3942384633345, 612.3631916426231, 627.5719169546541, 446.1967577115834, 599.5494316182011, 568.0342350208546, 578.0615431464736, 596.6752701874653, 584.3692054559176, 552.2482660380789, 575.5314795725795, 613.435956808421, 613.3385939352614, 620.513486134632, 632.3878155059174, 607.6203190106472, 600.266766803366, 613.7585245048309, 594.3459590705855, 558.2115555903858, 523.8088203684113, 585.6151112495808, 598.1409117730024, 389.68406721183106, 584.4772629967478, 566.9229864230072, 581.7035876113179, 607.5355493341357, 609.6744503946909, 574.0749208689531, 580.4054919014252, 618.506665884934, 610.8338460337565, 363.3224899449192, 601.6778723359051, 535.5874503703599, 594.9023125008368, 581.3302990029487, 585.202514955938, 473.6333228941237, 439.24216939401396, 567.5042549740488, 626.6816864924754, 579.2003211544952, 563.5965518087538, 485.2225598359144, 507.8007643884204, 542.0457879465687, 532.4566150017324]
Elapsed: 0.29756598702257836~0.5220372754083873
Time per graph: 0.0023067130776944058~0.004046800584561141
Speed: 542.7122672813967~90.89053159335644
Total Time: 0.2433
best val loss: 0.3178405889017551 test_score: 0.8682

Testing...
Test loss: 0.4503 score: 0.9070 time: 0.33s
test Score 0.9070
Epoch Time List: [0.8176490690093488, 0.7852276710327715, 0.9247142921667546, 0.7980746089015156, 0.7957909472752362, 0.81363948690705, 0.9245832308661193, 0.750636235345155, 0.7583490870893002, 0.821612891042605, 0.7662683839444071, 0.8237649509683251, 1.130693505750969, 0.8393322110641748, 0.8252688201609999, 0.8340627460274845, 0.9559119797777385, 0.7895017270930111, 0.9323012290988117, 0.812568096909672, 0.8110674489289522, 0.8557941229082644, 0.7893569369334728, 0.7954422407783568, 0.9490186341572553, 0.7751659147907048, 0.8507713361177593, 0.901724579744041, 0.7956568161025643, 0.8012104779481888, 0.9331615341361612, 0.8033235969487578, 0.7899528350681067, 1.0006292047910392, 0.876933928579092, 0.9079464951064438, 0.7928618520963937, 0.7474387530237436, 0.7444294777233154, 0.7792467726394534, 0.787847087951377, 0.9177205178420991, 0.7925688473042101, 0.7882598089054227, 0.8090140928979963, 0.7913336891215295, 0.7805726390797645, 0.7869484659750015, 0.7572306888177991, 0.812423107912764, 0.8518422411289066, 0.8193880489561707, 0.8075771320145577, 0.7675648897420615, 0.7841835527215153, 0.8837106982246041, 0.7837793598882854, 0.7684130559209734, 1.0051145339384675, 0.7569330099504441, 0.7600822180975229, 0.888088540174067, 0.7871246400754899, 0.7663860430475324, 0.9111245272215456, 0.8534962069243193, 0.7957381443120539, 0.9973596683703363, 0.9074391147587448, 0.8143829039763659, 7.070852301083505, 10.514700928004459, 3.9706611670553684, 0.7724364406894892, 0.743953213095665, 0.8684734301641583, 0.8487059790641069, 0.8355570528656244, 0.8389563849195838, 0.7896718131378293, 0.7868193481117487, 0.9129766533151269, 0.7862050896510482, 0.7833982557058334, 0.7658113318029791, 0.7595994782168418, 0.7455756899435073, 0.7187343090772629, 0.7210502801463008, 0.750423468882218, 0.7418638642411679, 0.7764767780900002, 0.7947210338898003, 0.7997134171891958, 0.7727158640045673, 0.8961721831001341, 0.8694557219278067, 0.7751814268995076, 0.7623073200229555, 0.7697735568508506, 0.9069531443528831, 0.7542144430335611, 0.7636439432390034, 0.8287977871950716, 0.7261779338587075, 0.7540569875854999, 0.8966492242179811, 0.7562493288423866, 0.802443511551246, 0.8218889336567372, 0.9066256147343665, 0.8122846221085638, 0.839631489245221, 0.9306627323385328, 0.856691530905664, 0.9180679360870272, 0.7734880701173097, 0.8214896337594837, 0.9246884710155427, 0.9133298399392515, 0.8338287933729589, 0.8290957380086184]
Total Epoch List: [67, 55]
Total Time List: [0.2323850328102708, 0.24331912607885897]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75bdaa331300>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6576 score: 0.5039 time: 0.24s
Test loss: 0.6215 score: 0.4844 time: 0.22s
Epoch 2/1000, LR 0.000020
Train loss: 0.7937;  Loss pred: 0.7937; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6568 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6201 score: 0.5000 time: 0.25s
Epoch 3/1000, LR 0.000050
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6592 score: 0.5039 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6208 score: 0.5000 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6589 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6255 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.4504;  Loss pred: 0.4504; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6629 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6293 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.3976;  Loss pred: 0.3976; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6518 score: 0.5039 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6259 score: 0.5000 time: 0.21s
Epoch 7/1000, LR 0.000170
Train loss: 0.3613;  Loss pred: 0.3613; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6506 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6281 score: 0.5000 time: 0.20s
Epoch 8/1000, LR 0.000200
Train loss: 0.3358;  Loss pred: 0.3358; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6459 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6329 score: 0.5000 time: 0.20s
Epoch 9/1000, LR 0.000230
Train loss: 0.2950;  Loss pred: 0.2950; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6475 score: 0.5039 time: 0.32s
Test loss: 0.6446 score: 0.4922 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.2475;  Loss pred: 0.2475; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6390 score: 0.5039 time: 0.22s
Test loss: 0.6504 score: 0.4766 time: 0.20s
Epoch 11/1000, LR 0.000290
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.38s
Val loss: 0.6355 score: 0.4961 time: 0.21s
Test loss: 0.6660 score: 0.4375 time: 0.20s
Epoch 12/1000, LR 0.000290
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.37s
Val loss: 0.6402 score: 0.5194 time: 0.35s
Test loss: 0.6720 score: 0.4609 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.1244;  Loss pred: 0.1244; Loss self: 0.0000; time: 0.39s
Val loss: 0.6677 score: 0.5194 time: 0.26s
Test loss: 0.6809 score: 0.4922 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 0.38s
Val loss: 0.7103 score: 0.5969 time: 0.25s
Test loss: 0.6941 score: 0.5078 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.37s
Val loss: 0.6894 score: 0.6279 time: 0.37s
Test loss: 0.6655 score: 0.6250 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.0614;  Loss pred: 0.0614; Loss self: 0.0000; time: 0.43s
Val loss: 0.6646 score: 0.6667 time: 0.26s
Test loss: 0.6391 score: 0.7031 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.37s
Val loss: 0.6142 score: 0.6977 time: 0.22s
Test loss: 0.5969 score: 0.7500 time: 0.20s
Epoch 18/1000, LR 0.000290
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.42s
Val loss: 0.5624 score: 0.7674 time: 0.22s
Test loss: 0.5558 score: 0.8047 time: 0.20s
Epoch 19/1000, LR 0.000290
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.37s
Val loss: 0.5085 score: 0.7984 time: 0.25s
Test loss: 0.5042 score: 0.8516 time: 0.20s
Epoch 20/1000, LR 0.000290
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.41s
Val loss: 0.4780 score: 0.8450 time: 0.22s
Test loss: 0.4762 score: 0.8672 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.37s
Val loss: 0.4364 score: 0.8527 time: 0.22s
Test loss: 0.4450 score: 0.8828 time: 0.20s
Epoch 22/1000, LR 0.000290
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.37s
Val loss: 0.3909 score: 0.8760 time: 0.22s
Test loss: 0.4058 score: 0.8906 time: 0.20s
Epoch 23/1000, LR 0.000290
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.37s
Val loss: 0.3555 score: 0.8837 time: 0.22s
Test loss: 0.3752 score: 0.8984 time: 0.20s
Epoch 24/1000, LR 0.000290
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.37s
Val loss: 0.3186 score: 0.8915 time: 0.22s
Test loss: 0.3528 score: 0.8984 time: 0.19s
Epoch 25/1000, LR 0.000290
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.37s
Val loss: 0.3129 score: 0.8837 time: 0.22s
Test loss: 0.3447 score: 0.8906 time: 0.20s
Epoch 26/1000, LR 0.000290
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.37s
Val loss: 0.2867 score: 0.9147 time: 0.22s
Test loss: 0.3239 score: 0.9062 time: 0.21s
Epoch 27/1000, LR 0.000290
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.39s
Val loss: 0.2689 score: 0.9225 time: 0.24s
Test loss: 0.3109 score: 0.9062 time: 0.23s
Epoch 28/1000, LR 0.000290
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.41s
Val loss: 0.3509 score: 0.8527 time: 0.25s
Test loss: 0.3764 score: 0.8281 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.39s
Val loss: 0.3058 score: 0.8682 time: 0.23s
Test loss: 0.3385 score: 0.8984 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.39s
Val loss: 0.2635 score: 0.8992 time: 0.23s
Test loss: 0.3064 score: 0.9141 time: 0.20s
Epoch 31/1000, LR 0.000290
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.38s
Val loss: 0.2492 score: 0.9225 time: 0.23s
Test loss: 0.2957 score: 0.9297 time: 0.21s
Epoch 32/1000, LR 0.000290
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.39s
Val loss: 0.2485 score: 0.9302 time: 0.22s
Test loss: 0.2983 score: 0.9297 time: 0.21s
Epoch 33/1000, LR 0.000290
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.39s
Val loss: 0.2512 score: 0.9147 time: 0.23s
Test loss: 0.3033 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.39s
Val loss: 0.2504 score: 0.9302 time: 0.21s
Test loss: 0.2927 score: 0.9141 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.38s
Val loss: 0.3094 score: 0.9147 time: 0.26s
Test loss: 0.3501 score: 0.9141 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.44s
Val loss: 0.3105 score: 0.9147 time: 0.39s
Test loss: 0.3587 score: 0.9062 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.41s
Val loss: 0.3979 score: 0.9070 time: 0.23s
Test loss: 0.4329 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.39s
Val loss: 0.4842 score: 0.8760 time: 0.23s
Test loss: 0.5053 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.41s
Val loss: 0.4167 score: 0.9070 time: 0.22s
Test loss: 0.4497 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.38s
Val loss: 0.3552 score: 0.9070 time: 0.24s
Test loss: 0.3937 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.36s
Val loss: 0.3284 score: 0.9147 time: 0.22s
Test loss: 0.3669 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.36s
Val loss: 0.3703 score: 0.9070 time: 0.22s
Test loss: 0.4105 score: 0.9062 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.54s
Val loss: 0.3847 score: 0.8992 time: 0.25s
Test loss: 0.4386 score: 0.9062 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.38s
Val loss: 0.4698 score: 0.8915 time: 0.23s
Test loss: 0.5179 score: 0.8828 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.38s
Val loss: 0.2626 score: 0.9225 time: 0.23s
Test loss: 0.2821 score: 0.9375 time: 0.24s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.50s
Val loss: 0.3005 score: 0.9225 time: 0.25s
Test loss: 0.3368 score: 0.9297 time: 0.23s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.38s
Val loss: 0.2914 score: 0.9225 time: 0.25s
Test loss: 0.3298 score: 0.9375 time: 0.22s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.42s
Val loss: 0.2944 score: 0.9147 time: 0.25s
Test loss: 0.3382 score: 0.9297 time: 0.25s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.48s
Val loss: 0.3446 score: 0.9225 time: 0.25s
Test loss: 0.4057 score: 0.9141 time: 0.23s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.40s
Val loss: 0.3470 score: 0.9225 time: 0.29s
Test loss: 0.4027 score: 0.9141 time: 0.24s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.41s
Val loss: 0.3535 score: 0.9225 time: 0.25s
Test loss: 0.4069 score: 0.9219 time: 0.33s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.40s
Val loss: 0.4529 score: 0.9147 time: 0.23s
Test loss: 0.4968 score: 0.9062 time: 0.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0157,   Val_Loss: 0.2485,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2485,   Test_Precision: 0.9661,   Test_Recall: 0.8906,   Test_accuracy: 0.9268,   Test_Score: 0.9297,   Test_loss: 0.2983


[0.22357910708524287, 0.24239516886882484, 0.3642447730526328, 0.23412813991308212, 0.22797992918640375, 0.22992742201313376, 0.2312316989991814, 0.21932963514700532, 0.21858455706387758, 0.23075828002765775, 0.21616878313943744, 0.24013428203761578, 0.24599635787308216, 0.24710278795100749, 0.22794162994250655, 0.22769634402357042, 0.3776605378370732, 0.23835298884660006, 0.2478047979529947, 0.22232565400190651, 0.23350003897212446, 0.22905854205600917, 0.2189835598692298, 0.24208540888503194, 0.22870979784056544, 0.22269868501462042, 0.2771703649777919, 0.22945774486288428, 0.233440559823066, 0.24295867700129747, 0.23149742395617068, 0.22545669600367546, 0.2208383148536086, 0.28655872610397637, 0.2315491798799485, 0.36736226407811046, 0.23636411898769438, 0.21508719795383513, 0.2136987680569291, 0.22428431711159647, 0.23135620588436723, 0.22881575813516974, 0.23515666206367314, 0.22804595902562141, 0.24307246785610914, 0.22970413090661168, 0.22572798002511263, 0.22194186178967357, 0.23193041211925447, 0.22827008296735585, 0.2632522680796683, 0.21958617097698152, 0.24423585389740765, 0.2207943699322641, 0.23634692002087831, 0.22525568399578333, 0.22752502607181668, 0.22082905401475728, 0.2219722520094365, 0.22247107001021504, 0.21851889207027853, 0.21625497401691973, 0.22657870198599994, 0.22330913902260363, 0.3538089918438345, 0.2996434529777616, 0.2318804159294814, 0.4247398420702666, 0.34392285090871155, 0.23834238317795098, 1.8897640148643404, 5.785503029124811, 0.22105473186820745, 0.2214994439855218, 0.21065929788164794, 0.20555413095280528, 0.28911012411117554, 0.21516157500445843, 0.2270989881362766, 0.22315962985157967, 0.21619799989275634, 0.22075085202232003, 0.23359059309586883, 0.22414065012708306, 0.21029090089723468, 0.21032428299076855, 0.20789233897812665, 0.20398874999955297, 0.21230363100767136, 0.21490445104427636, 0.21018038014881313, 0.21704530506394804, 0.2310951801482588, 0.246273058000952, 0.22028120094910264, 0.21566824382171035, 0.3310373988933861, 0.22071003983728588, 0.22754413401708007, 0.221762428060174, 0.2123332538176328, 0.21158833196386695, 0.22470934595912695, 0.22225840692408383, 0.20856687100604177, 0.21118672587908804, 0.35505646793171763, 0.21440043905749917, 0.24085702514275908, 0.21684232400730252, 0.22190482798032463, 0.22043650993146002, 0.2723625930957496, 0.29368764883838594, 0.22731107100844383, 0.20584613014943898, 0.2227208709809929, 0.22888713493011892, 0.26585738314315677, 0.2540366400498897, 0.23798727500252426, 0.24227326014079154, 0.228751917835325, 0.2678557871840894, 0.23876731609925628, 0.20722328196279705, 0.1956381150521338, 0.21809619688428938, 0.20071540214121342, 0.20774005982093513, 0.24104468105360866, 0.20837952196598053, 0.2014479439239949, 0.20724248210899532, 0.22795566101558506, 0.20476644788868725, 0.2631366280838847, 0.22624349989928305, 0.20806150697171688, 0.20717296609655023, 0.2072563711553812, 0.21727136801928282, 0.20921020698733628, 0.20618237601593137, 0.20198010792955756, 0.19648066582158208, 0.2052176990546286, 0.2145202870015055, 0.23687606398016214, 0.21096417400985956, 0.23042925191111863, 0.20804512407630682, 0.2142492439597845, 0.21177042392082512, 0.20592433609999716, 0.19472621800377965, 0.25543152005411685, 0.23531151795759797, 0.2145240989048034, 0.21724180900491774, 0.19844776904210448, 0.2124541641678661, 0.2060126408468932, 0.22973742499016225, 0.20830107503570616, 0.208218514919281, 0.2461792710237205, 0.23114965297281742, 0.23072661412879825, 0.25751154706813395, 0.2370187370106578, 0.24714942905120552, 0.33508046506904066, 0.22580789495259523]
[0.0017331713727538206, 0.0018790323168125956, 0.0028236028918808744, 0.0018149468210316444, 0.0017672862727628198, 0.001782383116380882, 0.0017924937906913286, 0.0017002297298217466, 0.0016944539307277332, 0.0017888238761833933, 0.0016757270010809104, 0.0018615060623070991, 0.0019069485106440478, 0.001915525487992306, 0.0017669893793992755, 0.0017650879381672126, 0.0029276010685044436, 0.00184769758795814, 0.001920967425992207, 0.0017234546821853218, 0.0018100778214893368, 0.0017756476128372805, 0.0016975469757304635, 0.0018766310766281545, 0.0017729441693067089, 0.0017263463954621738, 0.0021486074804479994, 0.001778742208239413, 0.001809616742814465, 0.0018834005969092828, 0.0017945536740788424, 0.0017477263256098874, 0.0017119249213458031, 0.0022213854736742355, 0.0017949548827902984, 0.0028477694889776006, 0.0018322799921526697, 0.0016673426197971715, 0.0016565795973405358, 0.0017386381171441586, 0.001793458960343932, 0.0017737655669393003, 0.0018229198609587066, 0.0017677981319815614, 0.0018842826965589855, 0.001780652177570633, 0.001749829302520253, 0.0017204795487571595, 0.001797910171467089, 0.0017695355268787275, 0.0020407152564315373, 0.0017022183796665234, 0.0018933011930031601, 0.001711584263040807, 0.001832146666828514, 0.0017461680929905684, 0.0017637598920295866, 0.0017118531318973433, 0.0017207151318560968, 0.0017245819380636825, 0.001693944899769601, 0.0016763951474179823, 0.0017564240464031002, 0.0017310785970744468, 0.0027427053631305003, 0.0023228174649438885, 0.0017975226041045068, 0.0032925569152733847, 0.0026660686116954384, 0.0018476153734724881, 0.01464933344856078, 0.04484886069089001, 0.0017136025726217632, 0.001717049953376138, 0.0016330178130360305, 0.0015934428756031417, 0.0022411637527998103, 0.001667919186081073, 0.0017604572723742373, 0.0017299196112525556, 0.001675953487540747, 0.0017112469149017056, 0.0018107797914408436, 0.0017375244195897911, 0.0016301620224591835, 0.0016304207983780508, 0.001611568519210284, 0.0015813081395314183, 0.0016457645814548167, 0.0016659259770874135, 0.0016293052724714195, 0.0016825217446817676, 0.0017914355050252621, 0.001909093472875597, 0.0017076062089077723, 0.001671846851331088, 0.002566181386770435, 0.0017109305413743091, 0.0017639080156362796, 0.0017190885896137518, 0.0016459942156405642, 0.0016402196276268757, 0.001741932914411837, 0.0017229333870084017, 0.0016167974496592386, 0.0016371064021634732, 0.0027523757204009117, 0.001662018907422474, 0.0018671087220368922, 0.0016809482481186242, 0.001720192464963757, 0.001708810154507442, 0.0021113379309748033, 0.002276648440607643, 0.0017621013256468514, 0.0015957064352669689, 0.0017265183796976195, 0.0017743188754272784, 0.0020609099468461766, 0.001969276279456509, 0.0018448625969187927, 0.0018780872879131127, 0.0017871243580884766, 0.0020926233373756986, 0.0018653696570254397, 0.001618931890334352, 0.0015284227738447953, 0.0017038765381585108, 0.0015680890792282298, 0.0016229692173510557, 0.0018831615707313176, 0.0016279650153592229, 0.0015738120619062101, 0.001619081891476526, 0.0017809036016842583, 0.0015997378741303692, 0.0020557549069053493, 0.0017675273429631488, 0.0016254805232165381, 0.0016185387976292986, 0.0016191903996514156, 0.001697432562650647, 0.0016344547420885647, 0.0016107998126244638, 0.0015779695931996685, 0.00153500520173111, 0.001603263273864286, 0.0016759397421992617, 0.0018505942498450167, 0.0016481576094520278, 0.0018002285305556143, 0.001625352531846147, 0.0016738222184358165, 0.0016544564368814463, 0.0016087838757812278, 0.0015212985781545285, 0.001995558750422788, 0.0018383712340437341, 0.0016759695226937765, 0.0016972016328509199, 0.0015503731956414413, 0.001659798157561454, 0.001609473756616353, 0.0017948236327356426, 0.0016273521487164544, 0.0016267071478068829, 0.0019232755548728164, 0.001805856663850136, 0.0018025516728812363, 0.0020118089614697965, 0.001851708882895764, 0.001930854914462543, 0.00261781613335188, 0.0017641241793171503]
[576.9769889581714, 532.1888245627946, 354.15744999959054, 550.9803308901272, 565.8392844508932, 561.046607101224, 557.8819883188104, 588.1558135704643, 590.1606304342075, 559.0265276051572, 596.7559151072701, 537.1994323567379, 524.3980078215445, 522.049957710621, 565.9343579869001, 566.5440108544137, 341.5765934635511, 541.2141069605896, 520.5710344013178, 580.229936032905, 552.4624345583094, 563.1748060653291, 589.0853179893268, 532.8697858914041, 564.0335535162618, 579.258022971851, 465.4177224550541, 562.1950136269568, 552.6031984235058, 530.9544881959952, 557.2416219388408, 572.171961563284, 584.1377665171586, 450.1695054059984, 557.1170671685503, 351.15201699805334, 545.7681163811332, 599.7567555261364, 603.6534565591625, 575.162818610335, 557.5817579947466, 563.7723601352449, 548.5704673128538, 565.6754478403478, 530.7059295434633, 561.5919900563135, 571.4843148184311, 581.233296683114, 556.2013140979136, 565.1200469333858, 490.0242681326514, 587.4686890620386, 528.1779801838063, 584.2540280333008, 545.8078319303016, 572.6825521633222, 566.970597596073, 584.1622633196597, 581.1537200357635, 579.8506744902912, 590.3379738833378, 596.5180712555868, 569.3385956812956, 577.6745213591212, 364.60350916388944, 430.51165883331976, 556.3212377505438, 303.71532694278994, 375.08412034605067, 541.2381896999248, 68.26249149910946, 22.29711044149504, 583.5658839319017, 582.3942384633345, 612.3631916426231, 627.5719169546541, 446.1967577115834, 599.5494316182011, 568.0342350208546, 578.0615431464736, 596.6752701874653, 584.3692054559176, 552.2482660380789, 575.5314795725795, 613.435956808421, 613.3385939352614, 620.513486134632, 632.3878155059174, 607.6203190106472, 600.266766803366, 613.7585245048309, 594.3459590705855, 558.2115555903858, 523.8088203684113, 585.6151112495808, 598.1409117730024, 389.68406721183106, 584.4772629967478, 566.9229864230072, 581.7035876113179, 607.5355493341357, 609.6744503946909, 574.0749208689531, 580.4054919014252, 618.506665884934, 610.8338460337565, 363.3224899449192, 601.6778723359051, 535.5874503703599, 594.9023125008368, 581.3302990029487, 585.202514955938, 473.6333228941237, 439.24216939401396, 567.5042549740488, 626.6816864924754, 579.2003211544952, 563.5965518087538, 485.2225598359144, 507.8007643884204, 542.0457879465687, 532.4566150017324, 559.5581502059591, 477.86908524783655, 536.0867730606396, 617.6912110820633, 654.2692356542613, 586.8969832055816, 637.7188727646597, 616.1546314674774, 531.0218812566648, 614.2638143727814, 635.3998830005117, 617.6339845837244, 561.5127057153837, 625.1024096954686, 486.4393107568254, 565.762110544532, 615.2026958903064, 617.8412290546986, 617.5925945554538, 589.1250244654418, 611.824833229805, 620.8096078498468, 633.7257728599749, 651.46359039842, 623.7278781979064, 596.6801638630183, 540.3669659536378, 606.7380900134154, 555.4850303874279, 615.2511411565317, 597.4350136984667, 604.4281237679135, 621.5875327034829, 657.3331588944818, 501.1127834688583, 543.9597734568395, 596.669561384807, 589.2051837825676, 645.0059913389218, 602.4828955523016, 621.3210969667075, 557.1578074642439, 614.4951483234483, 614.7388000035496, 519.9462955094485, 553.7538056137703, 554.7691170492656, 497.06508875942944, 540.0416929664271, 517.905303246646, 381.99779856944696, 566.8535195674693]
Elapsed: 0.274889068372933~0.4386995975457688
Time per graph: 0.0021349352955015994~0.003400314780125211
Speed: 554.7665388557997~83.53298511700845
Total Time: 0.2266
best val loss: 0.24849427908716737 test_score: 0.9297

Testing...
Test loss: 0.2983 score: 0.9297 time: 0.23s
test Score 0.9297
Epoch Time List: [0.8176490690093488, 0.7852276710327715, 0.9247142921667546, 0.7980746089015156, 0.7957909472752362, 0.81363948690705, 0.9245832308661193, 0.750636235345155, 0.7583490870893002, 0.821612891042605, 0.7662683839444071, 0.8237649509683251, 1.130693505750969, 0.8393322110641748, 0.8252688201609999, 0.8340627460274845, 0.9559119797777385, 0.7895017270930111, 0.9323012290988117, 0.812568096909672, 0.8110674489289522, 0.8557941229082644, 0.7893569369334728, 0.7954422407783568, 0.9490186341572553, 0.7751659147907048, 0.8507713361177593, 0.901724579744041, 0.7956568161025643, 0.8012104779481888, 0.9331615341361612, 0.8033235969487578, 0.7899528350681067, 1.0006292047910392, 0.876933928579092, 0.9079464951064438, 0.7928618520963937, 0.7474387530237436, 0.7444294777233154, 0.7792467726394534, 0.787847087951377, 0.9177205178420991, 0.7925688473042101, 0.7882598089054227, 0.8090140928979963, 0.7913336891215295, 0.7805726390797645, 0.7869484659750015, 0.7572306888177991, 0.812423107912764, 0.8518422411289066, 0.8193880489561707, 0.8075771320145577, 0.7675648897420615, 0.7841835527215153, 0.8837106982246041, 0.7837793598882854, 0.7684130559209734, 1.0051145339384675, 0.7569330099504441, 0.7600822180975229, 0.888088540174067, 0.7871246400754899, 0.7663860430475324, 0.9111245272215456, 0.8534962069243193, 0.7957381443120539, 0.9973596683703363, 0.9074391147587448, 0.8143829039763659, 7.070852301083505, 10.514700928004459, 3.9706611670553684, 0.7724364406894892, 0.743953213095665, 0.8684734301641583, 0.8487059790641069, 0.8355570528656244, 0.8389563849195838, 0.7896718131378293, 0.7868193481117487, 0.9129766533151269, 0.7862050896510482, 0.7833982557058334, 0.7658113318029791, 0.7595994782168418, 0.7455756899435073, 0.7187343090772629, 0.7210502801463008, 0.750423468882218, 0.7418638642411679, 0.7764767780900002, 0.7947210338898003, 0.7997134171891958, 0.7727158640045673, 0.8961721831001341, 0.8694557219278067, 0.7751814268995076, 0.7623073200229555, 0.7697735568508506, 0.9069531443528831, 0.7542144430335611, 0.7636439432390034, 0.8287977871950716, 0.7261779338587075, 0.7540569875854999, 0.8966492242179811, 0.7562493288423866, 0.802443511551246, 0.8218889336567372, 0.9066256147343665, 0.8122846221085638, 0.839631489245221, 0.9306627323385328, 0.856691530905664, 0.9180679360870272, 0.7734880701173097, 0.8214896337594837, 0.9246884710155427, 0.9133298399392515, 0.8338287933729589, 0.8290957380086184, 0.8625871350523084, 0.9622763651423156, 0.9755215912591666, 0.8498575689736754, 0.7906306441873312, 0.9203978348523378, 0.7850015100557357, 0.7880857167765498, 0.9198603769764304, 0.7855351951438934, 0.7841253317892551, 0.9231103851925582, 0.875735746929422, 0.8239411264657974, 0.9945967949461192, 0.8976443218998611, 0.7934241979382932, 0.8406308402772993, 0.8205839090514928, 0.8405602716375142, 0.7907078310381621, 0.7929728289600462, 0.7868037088774145, 0.7778329346328974, 0.7880748638417572, 0.7940222539473325, 0.8575709147844464, 0.8636626251973212, 0.8390607063192874, 0.8176633098628372, 0.8192592409905046, 0.8183667047414929, 0.8135315757244825, 0.7929285301361233, 0.8891006733756512, 1.053426724858582, 0.8451351579278708, 0.8341582361608744, 0.8182892452459782, 0.82737056305632, 0.7805369710549712, 0.8099287892691791, 0.98849987401627, 0.8038186961784959, 0.8544350820593536, 0.9772991198115051, 0.8570381647441536, 0.9228085118811578, 0.9541188338771462, 0.9310238868929446, 0.9802263439632952, 0.8532075830735266]
Total Epoch List: [67, 55, 52]
Total Time List: [0.2323850328102708, 0.24331912607885897, 0.22664758516475558]
T-times Epoch Time: 0.9130175779141401 ~ 0.026722481713864772
T-times Total Epoch: 54.666666666666664 ~ 4.027681991198191
T-times Total Time: 0.2220062085479084 ~ 0.009987859609203441
T-times Inference Elapsed: 0.27432456015576134 ~ 0.001526047561130811
T-times Time Per Graph: 0.002130617325947389 ~ 1.200881128611187e-05
T-times Speed: 566.682239202918 ~ 8.685138422452416
T-times cross validation test micro f1 score:0.8775545940467236 ~ 0.04937924877590915
T-times cross validation test precision:0.9022284211434256 ~ 0.07360493680322665
T-times cross validation test recall:0.8685096153846154 ~ 0.024064355711549128
T-times cross validation test f1_score:0.8775545940467236 ~ 0.03087855317119947
