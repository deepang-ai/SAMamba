Namespace(seed=15, model='FAGNN', dataset='exchange/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c78416800>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 1.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 1.13s
Epoch 3/1000, LR 0.000045
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 1.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 1.04s
Epoch 5/1000, LR 0.000105
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 1.01s
Epoch 6/1000, LR 0.000135
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 1.12s
Epoch 7/1000, LR 0.000165
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 1.03s
Epoch 8/1000, LR 0.000195
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.96s
Epoch 9/1000, LR 0.000225
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5039 time: 0.98s
Epoch 10/1000, LR 0.000255
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5039 time: 0.96s
Epoch 11/1000, LR 0.000285
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5039 time: 0.96s
Epoch 12/1000, LR 0.000285
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4961 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5039 time: 1.10s
Epoch 13/1000, LR 0.000285
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5039 time: 0.97s
Epoch 14/1000, LR 0.000285
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4961 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5039 time: 1.07s
Epoch 15/1000, LR 0.000285
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5039 time: 1.07s
Epoch 16/1000, LR 0.000285
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5039 time: 0.97s
Epoch 17/1000, LR 0.000285
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4961 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.5039 time: 0.96s
Epoch 18/1000, LR 0.000285
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6801 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6771 score: 0.5039 time: 0.95s
Epoch 19/1000, LR 0.000285
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6774 score: 0.4961 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6738 score: 0.5039 time: 0.98s
Epoch 20/1000, LR 0.000285
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 1.19s
Val loss: 0.6742 score: 0.5116 time: 1.02s
Test loss: 0.6700 score: 0.5581 time: 0.97s
Epoch 21/1000, LR 0.000285
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 1.20s
Val loss: 0.6706 score: 0.5116 time: 1.00s
Test loss: 0.6657 score: 0.5814 time: 1.01s
Epoch 22/1000, LR 0.000285
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 1.27s
Val loss: 0.6664 score: 0.5426 time: 1.24s
Test loss: 0.6609 score: 0.6434 time: 1.12s
Epoch 23/1000, LR 0.000285
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 1.08s
Val loss: 0.6614 score: 0.5581 time: 1.01s
Test loss: 0.6550 score: 0.6434 time: 1.07s
Epoch 24/1000, LR 0.000285
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 1.10s
Val loss: 0.6554 score: 0.5736 time: 1.08s
Test loss: 0.6480 score: 0.6512 time: 1.02s
Epoch 25/1000, LR 0.000285
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 1.17s
Val loss: 0.6481 score: 0.6124 time: 1.14s
Test loss: 0.6396 score: 0.7054 time: 1.05s
Epoch 26/1000, LR 0.000285
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 1.09s
Val loss: 0.6396 score: 0.6744 time: 1.18s
Test loss: 0.6298 score: 0.7054 time: 0.98s
Epoch 27/1000, LR 0.000285
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 1.23s
Val loss: 0.6296 score: 0.7054 time: 1.11s
Test loss: 0.6185 score: 0.7674 time: 0.98s
Epoch 28/1000, LR 0.000285
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 1.21s
Val loss: 0.6181 score: 0.7442 time: 1.22s
Test loss: 0.6054 score: 0.8295 time: 1.13s
Epoch 29/1000, LR 0.000285
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 1.12s
Val loss: 0.6051 score: 0.8062 time: 1.10s
Test loss: 0.5906 score: 0.8527 time: 1.08s
Epoch 30/1000, LR 0.000285
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 1.50s
Val loss: 0.5904 score: 0.8992 time: 1.27s
Test loss: 0.5743 score: 0.8992 time: 1.10s
Epoch 31/1000, LR 0.000285
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 1.11s
Val loss: 0.5738 score: 0.9070 time: 1.10s
Test loss: 0.5561 score: 0.9147 time: 1.00s
Epoch 32/1000, LR 0.000285
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 1.09s
Val loss: 0.5555 score: 0.9070 time: 1.17s
Test loss: 0.5363 score: 0.9302 time: 1.04s
Epoch 33/1000, LR 0.000285
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 4.83s
Val loss: 0.5358 score: 0.9070 time: 1.07s
Test loss: 0.5149 score: 0.9147 time: 0.99s
Epoch 34/1000, LR 0.000285
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 1.22s
Val loss: 0.5151 score: 0.9070 time: 0.98s
Test loss: 0.4925 score: 0.9225 time: 1.07s
Epoch 35/1000, LR 0.000285
Train loss: 0.5001;  Loss pred: 0.5001; Loss self: 0.0000; time: 1.09s
Val loss: 0.4939 score: 0.8915 time: 1.00s
Test loss: 0.4694 score: 0.9302 time: 1.08s
Epoch 36/1000, LR 0.000285
Train loss: 0.4752;  Loss pred: 0.4752; Loss self: 0.0000; time: 1.08s
Val loss: 0.4725 score: 0.8915 time: 1.00s
Test loss: 0.4454 score: 0.9302 time: 1.07s
Epoch 37/1000, LR 0.000285
Train loss: 0.4555;  Loss pred: 0.4555; Loss self: 0.0000; time: 1.11s
Val loss: 0.4516 score: 0.8915 time: 0.99s
Test loss: 0.4216 score: 0.9302 time: 1.11s
Epoch 38/1000, LR 0.000284
Train loss: 0.4321;  Loss pred: 0.4321; Loss self: 0.0000; time: 1.13s
Val loss: 0.4315 score: 0.8837 time: 1.00s
Test loss: 0.3976 score: 0.9225 time: 1.10s
Epoch 39/1000, LR 0.000284
Train loss: 0.4112;  Loss pred: 0.4112; Loss self: 0.0000; time: 1.07s
Val loss: 0.4122 score: 0.8837 time: 1.00s
Test loss: 0.3750 score: 0.9302 time: 1.09s
Epoch 40/1000, LR 0.000284
Train loss: 0.3863;  Loss pred: 0.3863; Loss self: 0.0000; time: 1.07s
Val loss: 0.3938 score: 0.8837 time: 1.11s
Test loss: 0.3527 score: 0.9302 time: 0.97s
Epoch 41/1000, LR 0.000284
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 1.14s
Val loss: 0.3766 score: 0.8837 time: 1.07s
Test loss: 0.3309 score: 0.9225 time: 0.96s
Epoch 42/1000, LR 0.000284
Train loss: 0.3462;  Loss pred: 0.3462; Loss self: 0.0000; time: 1.15s
Val loss: 0.3614 score: 0.8837 time: 1.31s
Test loss: 0.3114 score: 0.9225 time: 1.01s
Epoch 43/1000, LR 0.000284
Train loss: 0.3304;  Loss pred: 0.3304; Loss self: 0.0000; time: 1.17s
Val loss: 0.3484 score: 0.8837 time: 1.20s
Test loss: 0.2937 score: 0.9302 time: 1.03s
Epoch 44/1000, LR 0.000284
Train loss: 0.3050;  Loss pred: 0.3050; Loss self: 0.0000; time: 1.27s
Val loss: 0.3373 score: 0.8760 time: 0.99s
Test loss: 0.2772 score: 0.9380 time: 1.10s
Epoch 45/1000, LR 0.000284
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 1.54s
Val loss: 0.3278 score: 0.8760 time: 1.05s
Test loss: 0.2623 score: 0.9302 time: 1.01s
Epoch 46/1000, LR 0.000284
Train loss: 0.2758;  Loss pred: 0.2758; Loss self: 0.0000; time: 1.20s
Val loss: 0.3208 score: 0.8760 time: 1.02s
Test loss: 0.2501 score: 0.9302 time: 1.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 1.11s
Val loss: 0.3145 score: 0.8760 time: 1.05s
Test loss: 0.2382 score: 0.9302 time: 1.13s
Epoch 48/1000, LR 0.000284
Train loss: 0.2494;  Loss pred: 0.2494; Loss self: 0.0000; time: 1.12s
Val loss: 0.3098 score: 0.8915 time: 1.11s
Test loss: 0.2275 score: 0.9302 time: 1.05s
Epoch 49/1000, LR 0.000284
Train loss: 0.2319;  Loss pred: 0.2319; Loss self: 0.0000; time: 1.08s
Val loss: 0.3068 score: 0.8915 time: 1.27s
Test loss: 0.2182 score: 0.9302 time: 0.99s
Epoch 50/1000, LR 0.000284
Train loss: 0.2070;  Loss pred: 0.2070; Loss self: 0.0000; time: 1.10s
Val loss: 0.3062 score: 0.8915 time: 1.22s
Test loss: 0.2120 score: 0.9302 time: 1.03s
Epoch 51/1000, LR 0.000284
Train loss: 0.1992;  Loss pred: 0.1992; Loss self: 0.0000; time: 1.24s
Val loss: 0.3075 score: 0.8992 time: 1.01s
Test loss: 0.2082 score: 0.9302 time: 0.99s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1839;  Loss pred: 0.1839; Loss self: 0.0000; time: 1.16s
Val loss: 0.3095 score: 0.8992 time: 0.99s
Test loss: 0.2047 score: 0.9302 time: 1.06s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1698;  Loss pred: 0.1698; Loss self: 0.0000; time: 1.09s
Val loss: 0.3109 score: 0.8992 time: 1.02s
Test loss: 0.2003 score: 0.9380 time: 1.06s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 1.09s
Val loss: 0.3126 score: 0.8992 time: 1.09s
Test loss: 0.1966 score: 0.9380 time: 0.95s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 1.11s
Val loss: 0.3165 score: 0.8992 time: 1.18s
Test loss: 0.1958 score: 0.9380 time: 0.98s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1453;  Loss pred: 0.1453; Loss self: 0.0000; time: 1.21s
Val loss: 0.3200 score: 0.8992 time: 0.98s
Test loss: 0.1945 score: 0.9380 time: 0.98s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1514;  Loss pred: 0.1514; Loss self: 0.0000; time: 1.25s
Val loss: 0.3238 score: 0.8992 time: 0.98s
Test loss: 0.1941 score: 0.9380 time: 0.99s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1289;  Loss pred: 0.1289; Loss self: 0.0000; time: 1.18s
Val loss: 0.3260 score: 0.9070 time: 1.03s
Test loss: 0.1919 score: 0.9302 time: 1.06s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1356;  Loss pred: 0.1356; Loss self: 0.0000; time: 1.13s
Val loss: 0.3341 score: 0.8992 time: 1.01s
Test loss: 0.1959 score: 0.9380 time: 1.09s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 1.14s
Val loss: 0.3470 score: 0.8992 time: 0.98s
Test loss: 0.2046 score: 0.9302 time: 1.07s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 1.15s
Val loss: 0.3563 score: 0.8915 time: 1.12s
Test loss: 0.2099 score: 0.9302 time: 1.02s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 1.09s
Val loss: 0.3591 score: 0.8915 time: 1.16s
Test loss: 0.2092 score: 0.9302 time: 0.97s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 1.10s
Val loss: 0.3512 score: 0.9147 time: 1.18s
Test loss: 0.1984 score: 0.9302 time: 0.95s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1067;  Loss pred: 0.1067; Loss self: 0.0000; time: 1.11s
Val loss: 0.3511 score: 0.9225 time: 1.08s
Test loss: 0.1951 score: 0.9302 time: 1.08s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 1.22s
Val loss: 0.3539 score: 0.9225 time: 1.01s
Test loss: 0.1948 score: 0.9457 time: 1.00s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1134;  Loss pred: 0.1134; Loss self: 0.0000; time: 1.30s
Val loss: 0.3588 score: 0.9225 time: 1.00s
Test loss: 0.1956 score: 0.9457 time: 1.07s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1069;  Loss pred: 0.1069; Loss self: 0.0000; time: 1.10s
Val loss: 0.3765 score: 0.9070 time: 1.13s
Test loss: 0.2091 score: 0.9225 time: 1.06s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 1.13s
Val loss: 0.3926 score: 0.8915 time: 1.02s
Test loss: 0.2212 score: 0.9302 time: 1.07s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 1.08s
Val loss: 0.4075 score: 0.8837 time: 1.08s
Test loss: 0.2318 score: 0.9302 time: 0.95s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 1.08s
Val loss: 0.4049 score: 0.8915 time: 1.10s
Test loss: 0.2253 score: 0.9302 time: 0.97s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.2070,   Val_Loss: 0.3062,   Val_Precision: 0.9630,   Val_Recall: 0.8125,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.3062,   Test_Precision: 0.9828,   Test_Recall: 0.8769,   Test_accuracy: 0.9268,   Test_Score: 0.9302,   Test_loss: 0.2120


[1.063130671158433, 1.1318616901990026, 1.1299737140070647, 1.0430995190981776, 1.0124477490317076, 1.1267243819311261, 1.0335636080708355, 0.9684562019538134, 0.9884134829044342, 0.9679312261287123, 0.9726109998300672, 1.1089736281428486, 0.9704598728567362, 1.07281416002661, 1.071435775142163, 0.9724294289480895, 0.9650109498761594, 0.9509478569962084, 0.9846369230654091, 0.9722963199019432, 1.017771836137399, 1.1250510008540004, 1.0759208949748427, 1.020191746065393, 1.059110275004059, 0.9887551900465041, 0.9821634059771895, 1.1381177380681038, 1.0807827960234135, 1.1081271299626678, 1.0133623569272459, 1.0457404619082808, 0.9964763359166682, 1.0752434299793094, 1.084426405839622, 1.0723188850097358, 1.118562453892082, 1.101099036168307, 1.0987206590361893, 0.9729251749813557, 0.9664559541270137, 1.0145321041345596, 1.034451480023563, 1.1058105430565774, 1.013644800055772, 1.0695910260546952, 1.1319445839617401, 1.051025050925091, 0.9908053691033274, 1.0377644710242748, 0.9914862578734756, 1.062704656040296, 1.0618904209695756, 0.956876145908609, 0.984929421916604, 0.9853022240567952, 0.9986524381674826, 1.0654439060017467, 1.09378322516568, 1.0712369279935956, 1.020803501829505, 0.9781342649366707, 0.9601584009360522, 1.0828025450464338, 1.0017874769400805, 1.0773686771281064, 1.0659374510869384, 1.085093806963414, 0.9581450368277729, 0.971127106109634]
[0.008241323032235914, 0.008774121629449632, 0.008759486155093525, 0.008086042783706802, 0.007848432163036493, 0.008734297534349815, 0.008012120992797174, 0.007507412418246616, 0.007662120022514994, 0.007503342838207072, 0.007539620153721451, 0.008596694791805028, 0.0075229447508274126, 0.00831638883741558, 0.008305703683272582, 0.00753821262750457, 0.007480705037799685, 0.007371688813924096, 0.007632844364848132, 0.007537180774433668, 0.007889704156103869, 0.008721325588015508, 0.008340472054068548, 0.007908463147793745, 0.008210157170574101, 0.0076647689150891795, 0.007613669813776663, 0.008822618124558944, 0.008378161209483826, 0.008590132790408277, 0.007855522146722835, 0.008106515208591325, 0.007724622759043939, 0.008335220387436507, 0.008406406246818775, 0.008312549496199502, 0.008671026774357224, 0.008535651443165171, 0.008517214411133251, 0.007542055620010509, 0.007491906621139641, 0.00786458995453147, 0.00801900372111289, 0.00857217475237657, 0.007857711628339318, 0.008291403302749576, 0.008774764216757675, 0.008147481014923186, 0.007680661775994786, 0.008044685821893603, 0.0076859399835153145, 0.008238020589459659, 0.008231708689686633, 0.007417644541927202, 0.007635111797803132, 0.007638001736874382, 0.007741491768740175, 0.008259255085284857, 0.008478939729966512, 0.008304162232508494, 0.007913205440538798, 0.007582436162299772, 0.007443088379349242, 0.008393818178654526, 0.007765794394884345, 0.008351695171535709, 0.008263081016177816, 0.008411579898941193, 0.00742748090564165, 0.00752811710162507]
[121.33974072955307, 113.97152241924485, 114.16194766384936, 123.66988732918627, 127.41398271997146, 114.49117643030242, 124.81089600356648, 133.201687117857, 130.512181623561, 133.27393157460344, 132.6326763963585, 116.3237760811597, 132.9266707548284, 120.24449788843233, 120.39919050013398, 132.65744141407131, 133.67723963811463, 135.65412556633413, 131.0127590974268, 132.67560244700888, 126.74746482431158, 114.6614685930496, 119.89729040722487, 126.4468179609554, 121.8003479378062, 130.46707749158085, 131.34270653430963, 113.34503952022648, 119.35793248619161, 116.41263579959994, 127.29898551901857, 123.35756786593025, 129.4561600214336, 119.97283257288288, 118.95689675697372, 120.30003556155665, 115.32659580262097, 117.15567425152288, 117.40927863608236, 132.5898469041795, 133.47737105776736, 127.15221083126065, 124.70377054036562, 116.65651119895307, 127.26351478634552, 120.60684584820308, 113.96317613756983, 122.73732189966054, 130.19711441081932, 124.30566241362729, 130.10770343572608, 121.38838318509121, 121.48146122479807, 134.81368571217445, 130.9738516582995, 130.92429596765442, 129.17407004654567, 121.07629437207417, 117.93927446680289, 120.42153946430348, 126.37103984146171, 131.8837348043953, 134.35283165177614, 119.13529441738437, 128.76982690383124, 119.73617085645144, 121.0202342252432, 118.88373076333436, 134.63514921195363, 132.8353406968302]
Elapsed: 1.0367396378058142~0.054893668263756634
Time per graph: 0.008036741378339643~0.00042553231212214443
Speed: 124.77630001248181~6.57433610982323
Total Time: 0.9716
best val loss: 0.306181370865467 test_score: 0.9302

Testing...
Test loss: 0.1951 score: 0.9302 time: 0.97s
test Score 0.9302
Epoch Time List: [4.1659789378754795, 3.2877563019283116, 3.4374812501482666, 3.61774112097919, 3.277830644743517, 3.747291174251586, 3.26739455293864, 3.3366983260493726, 3.2087414860725403, 3.1749150089453906, 3.1624299499671906, 3.5108352112583816, 3.0584695269353688, 3.230522735277191, 3.147791022900492, 3.134639047086239, 3.144733427092433, 3.0920771609526128, 3.1373651758767664, 3.1806733258999884, 3.2123715518973768, 3.629649024223909, 3.167626488953829, 3.1975349080748856, 3.360820314846933, 3.260718015022576, 3.315582557115704, 3.5642083338461816, 3.2947810497134924, 3.874343889998272, 3.217723141890019, 3.2963171142619103, 6.890527615090832, 3.276232295902446, 3.172624752158299, 3.153430146863684, 3.216862791683525, 3.22480682679452, 3.1705284588970244, 3.153466706862673, 3.1698755091056228, 3.4638398298993707, 3.4037535591050982, 3.3656807590741664, 3.5918995949905366, 3.2765433131717145, 3.2848015180788934, 3.2780438871122897, 3.339593442156911, 3.3478943642694503, 3.2297643069177866, 3.210738570196554, 3.172750915866345, 3.1294888567645103, 3.271078267833218, 3.169730870751664, 3.2215428811032325, 3.2630343970377, 3.2211821631062776, 3.180658738128841, 3.2791768859606236, 3.2171251547988504, 3.238767326110974, 3.269671357003972, 3.227206423180178, 3.366503225872293, 3.286996209062636, 3.220570375211537, 3.114501602947712, 3.1426010706927627]
Total Epoch List: [70]
Total Time List: [0.9715670209843665]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c78416c20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 1.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 1.23s
Epoch 3/1000, LR 0.000045
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.97s
Epoch 4/1000, LR 0.000075
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.19s
Val loss: 0.6929 score: 0.5271 time: 1.06s
Test loss: 0.6930 score: 0.5116 time: 1.08s
Epoch 5/1000, LR 0.000105
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.10s
Val loss: 0.6928 score: 0.7054 time: 1.05s
Test loss: 0.6929 score: 0.6977 time: 1.09s
Epoch 6/1000, LR 0.000135
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.06s
Val loss: 0.6926 score: 0.6977 time: 1.12s
Test loss: 0.6928 score: 0.6744 time: 1.05s
Epoch 7/1000, LR 0.000165
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.06s
Val loss: 0.6925 score: 0.6124 time: 1.07s
Test loss: 0.6926 score: 0.5891 time: 1.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.15s
Val loss: 0.6922 score: 0.5736 time: 1.05s
Test loss: 0.6925 score: 0.5271 time: 1.09s
Epoch 9/1000, LR 0.000225
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 1.16s
Test loss: 0.6922 score: 0.5039 time: 0.98s
Epoch 10/1000, LR 0.000255
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 2.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 3.57s
Test loss: 0.6919 score: 0.5039 time: 1.01s
Epoch 11/1000, LR 0.000285
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5039 time: 1.18s
Test loss: 0.6915 score: 0.5039 time: 1.00s
Epoch 12/1000, LR 0.000285
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4961 time: 0.97s
Epoch 13/1000, LR 0.000285
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5039 time: 1.06s
Test loss: 0.6903 score: 0.5039 time: 0.97s
Epoch 14/1000, LR 0.000285
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.19s
Val loss: 0.6882 score: 0.5504 time: 1.05s
Test loss: 0.6895 score: 0.5116 time: 0.97s
Epoch 15/1000, LR 0.000285
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 1.12s
Val loss: 0.6868 score: 0.5504 time: 1.02s
Test loss: 0.6884 score: 0.5194 time: 0.96s
Epoch 16/1000, LR 0.000285
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 1.14s
Val loss: 0.6851 score: 0.5504 time: 1.05s
Test loss: 0.6870 score: 0.5194 time: 1.05s
Epoch 17/1000, LR 0.000285
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 1.04s
Val loss: 0.6829 score: 0.5581 time: 1.05s
Test loss: 0.6854 score: 0.5194 time: 1.04s
Epoch 18/1000, LR 0.000285
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 1.02s
Val loss: 0.6802 score: 0.5736 time: 1.06s
Test loss: 0.6833 score: 0.5194 time: 1.05s
Epoch 19/1000, LR 0.000285
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 1.07s
Val loss: 0.6769 score: 0.5736 time: 1.04s
Test loss: 0.6808 score: 0.5194 time: 1.08s
Epoch 20/1000, LR 0.000285
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 1.04s
Val loss: 0.6730 score: 0.5736 time: 1.13s
Test loss: 0.6778 score: 0.5194 time: 1.09s
Epoch 21/1000, LR 0.000285
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 1.32s
Val loss: 0.6683 score: 0.5581 time: 1.56s
Test loss: 0.6742 score: 0.5194 time: 0.99s
Epoch 22/1000, LR 0.000285
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 1.12s
Val loss: 0.6626 score: 0.5581 time: 1.16s
Test loss: 0.6698 score: 0.5271 time: 0.98s
Epoch 23/1000, LR 0.000285
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 1.09s
Val loss: 0.6559 score: 0.5581 time: 1.34s
Test loss: 0.6645 score: 0.5194 time: 0.98s
Epoch 24/1000, LR 0.000285
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 1.21s
Val loss: 0.6481 score: 0.5736 time: 1.05s
Test loss: 0.6584 score: 0.5194 time: 0.99s
Epoch 25/1000, LR 0.000285
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 1.16s
Val loss: 0.6389 score: 0.5736 time: 1.18s
Test loss: 0.6512 score: 0.5271 time: 1.10s
Epoch 26/1000, LR 0.000285
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 1.16s
Val loss: 0.6286 score: 0.5736 time: 1.09s
Test loss: 0.6431 score: 0.5271 time: 0.95s
Epoch 27/1000, LR 0.000285
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 1.19s
Val loss: 0.6167 score: 0.5814 time: 1.19s
Test loss: 0.6336 score: 0.5271 time: 1.15s
Epoch 28/1000, LR 0.000285
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 1.32s
Val loss: 0.6033 score: 0.5969 time: 1.36s
Test loss: 0.6226 score: 0.5349 time: 1.16s
Epoch 29/1000, LR 0.000285
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 1.21s
Val loss: 0.5884 score: 0.5969 time: 1.04s
Test loss: 0.6103 score: 0.5349 time: 1.07s
Epoch 30/1000, LR 0.000285
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 1.05s
Val loss: 0.5720 score: 0.6202 time: 1.05s
Test loss: 0.5965 score: 0.5581 time: 1.07s
Epoch 31/1000, LR 0.000285
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 1.41s
Val loss: 0.5539 score: 0.6822 time: 1.22s
Test loss: 0.5813 score: 0.5426 time: 1.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.5542;  Loss pred: 0.5542; Loss self: 0.0000; time: 1.05s
Val loss: 0.5346 score: 0.7364 time: 1.08s
Test loss: 0.5646 score: 0.6512 time: 1.10s
Epoch 33/1000, LR 0.000285
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 1.07s
Val loss: 0.5139 score: 0.8217 time: 1.06s
Test loss: 0.5465 score: 0.7597 time: 1.11s
Epoch 34/1000, LR 0.000285
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 1.06s
Val loss: 0.4919 score: 0.8915 time: 1.05s
Test loss: 0.5268 score: 0.8140 time: 1.09s
Epoch 35/1000, LR 0.000285
Train loss: 0.4986;  Loss pred: 0.4986; Loss self: 0.0000; time: 1.06s
Val loss: 0.4699 score: 0.9147 time: 1.16s
Test loss: 0.5067 score: 0.8450 time: 0.99s
Epoch 36/1000, LR 0.000285
Train loss: 0.4809;  Loss pred: 0.4809; Loss self: 0.0000; time: 1.05s
Val loss: 0.4498 score: 0.9302 time: 1.16s
Test loss: 0.4880 score: 0.8760 time: 1.21s
Epoch 37/1000, LR 0.000285
Train loss: 0.4594;  Loss pred: 0.4594; Loss self: 0.0000; time: 1.04s
Val loss: 0.4309 score: 0.9380 time: 1.22s
Test loss: 0.4702 score: 0.8760 time: 0.94s
Epoch 38/1000, LR 0.000284
Train loss: 0.4457;  Loss pred: 0.4457; Loss self: 0.0000; time: 1.19s
Val loss: 0.4130 score: 0.9457 time: 1.19s
Test loss: 0.4532 score: 0.8915 time: 0.97s
Epoch 39/1000, LR 0.000284
Train loss: 0.4277;  Loss pred: 0.4277; Loss self: 0.0000; time: 1.22s
Val loss: 0.3966 score: 0.9457 time: 1.04s
Test loss: 0.4377 score: 0.8915 time: 0.99s
Epoch 40/1000, LR 0.000284
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 1.15s
Val loss: 0.3810 score: 0.9457 time: 1.08s
Test loss: 0.4232 score: 0.8915 time: 0.95s
Epoch 41/1000, LR 0.000284
Train loss: 0.3935;  Loss pred: 0.3935; Loss self: 0.0000; time: 1.15s
Val loss: 0.3670 score: 0.9457 time: 1.08s
Test loss: 0.4104 score: 0.8915 time: 1.05s
Epoch 42/1000, LR 0.000284
Train loss: 0.3776;  Loss pred: 0.3776; Loss self: 0.0000; time: 1.09s
Val loss: 0.3538 score: 0.9457 time: 1.03s
Test loss: 0.3985 score: 0.8915 time: 1.08s
Epoch 43/1000, LR 0.000284
Train loss: 0.3560;  Loss pred: 0.3560; Loss self: 0.0000; time: 1.09s
Val loss: 0.3406 score: 0.9380 time: 1.05s
Test loss: 0.3868 score: 0.8915 time: 1.07s
Epoch 44/1000, LR 0.000284
Train loss: 0.3530;  Loss pred: 0.3530; Loss self: 0.0000; time: 1.05s
Val loss: 0.3251 score: 0.9457 time: 1.18s
Test loss: 0.3733 score: 0.8992 time: 0.98s
Epoch 45/1000, LR 0.000284
Train loss: 0.3277;  Loss pred: 0.3277; Loss self: 0.0000; time: 1.07s
Val loss: 0.3095 score: 0.9457 time: 1.20s
Test loss: 0.3602 score: 0.8992 time: 1.01s
Epoch 46/1000, LR 0.000284
Train loss: 0.3122;  Loss pred: 0.3122; Loss self: 0.0000; time: 1.09s
Val loss: 0.2933 score: 0.9457 time: 1.16s
Test loss: 0.3474 score: 0.8915 time: 1.00s
Epoch 47/1000, LR 0.000284
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 1.19s
Val loss: 0.2786 score: 0.9457 time: 1.04s
Test loss: 0.3362 score: 0.8915 time: 0.96s
Epoch 48/1000, LR 0.000284
Train loss: 0.2843;  Loss pred: 0.2843; Loss self: 0.0000; time: 1.14s
Val loss: 0.2670 score: 0.9457 time: 1.08s
Test loss: 0.3275 score: 0.8915 time: 1.05s
Epoch 49/1000, LR 0.000284
Train loss: 0.2668;  Loss pred: 0.2668; Loss self: 0.0000; time: 1.11s
Val loss: 0.2561 score: 0.9457 time: 1.07s
Test loss: 0.3197 score: 0.8915 time: 1.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.2480;  Loss pred: 0.2480; Loss self: 0.0000; time: 1.05s
Val loss: 0.2443 score: 0.9457 time: 1.03s
Test loss: 0.3118 score: 0.8915 time: 1.05s
Epoch 51/1000, LR 0.000284
Train loss: 0.2398;  Loss pred: 0.2398; Loss self: 0.0000; time: 1.07s
Val loss: 0.2369 score: 0.9457 time: 1.14s
Test loss: 0.3072 score: 0.8915 time: 0.95s
Epoch 52/1000, LR 0.000284
Train loss: 0.2358;  Loss pred: 0.2358; Loss self: 0.0000; time: 1.05s
Val loss: 0.2351 score: 0.9457 time: 1.13s
Test loss: 0.3070 score: 0.8992 time: 0.94s
Epoch 53/1000, LR 0.000284
Train loss: 0.2131;  Loss pred: 0.2131; Loss self: 0.0000; time: 1.04s
Val loss: 0.2325 score: 0.9457 time: 1.18s
Test loss: 0.3068 score: 0.8915 time: 0.94s
Epoch 54/1000, LR 0.000284
Train loss: 0.2023;  Loss pred: 0.2023; Loss self: 0.0000; time: 1.04s
Val loss: 0.2193 score: 0.9380 time: 1.13s
Test loss: 0.2997 score: 0.8992 time: 0.95s
Epoch 55/1000, LR 0.000284
Train loss: 0.1997;  Loss pred: 0.1997; Loss self: 0.0000; time: 1.14s
Val loss: 0.2048 score: 0.9380 time: 1.11s
Test loss: 0.2942 score: 0.8992 time: 0.99s
Epoch 56/1000, LR 0.000284
Train loss: 0.1901;  Loss pred: 0.1901; Loss self: 0.0000; time: 1.26s
Val loss: 0.1965 score: 0.9380 time: 1.13s
Test loss: 0.2933 score: 0.9070 time: 1.01s
Epoch 57/1000, LR 0.000283
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 1.17s
Val loss: 0.2000 score: 0.9380 time: 1.10s
Test loss: 0.2972 score: 0.9070 time: 0.96s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 1.15s
Val loss: 0.2142 score: 0.9302 time: 5.68s
Test loss: 0.3060 score: 0.8915 time: 1.08s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1605;  Loss pred: 0.1605; Loss self: 0.0000; time: 1.05s
Val loss: 0.2291 score: 0.9302 time: 1.07s
Test loss: 0.3167 score: 0.8992 time: 1.05s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1602;  Loss pred: 0.1602; Loss self: 0.0000; time: 1.11s
Val loss: 0.2367 score: 0.9302 time: 1.05s
Test loss: 0.3245 score: 0.8992 time: 1.05s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1585;  Loss pred: 0.1585; Loss self: 0.0000; time: 1.10s
Val loss: 0.2401 score: 0.9380 time: 1.04s
Test loss: 0.3297 score: 0.8992 time: 1.09s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1728;  Loss pred: 0.1728; Loss self: 0.0000; time: 1.08s
Val loss: 0.2322 score: 0.9302 time: 1.12s
Test loss: 0.3273 score: 0.8992 time: 1.06s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1548;  Loss pred: 0.1548; Loss self: 0.0000; time: 1.10s
Val loss: 0.2290 score: 0.9225 time: 1.13s
Test loss: 0.3281 score: 0.8992 time: 0.99s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1518;  Loss pred: 0.1518; Loss self: 0.0000; time: 1.18s
Val loss: 0.2198 score: 0.9302 time: 1.16s
Test loss: 0.3259 score: 0.8915 time: 0.97s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1728;  Loss pred: 0.1728; Loss self: 0.0000; time: 1.06s
Val loss: 0.2048 score: 0.9302 time: 1.19s
Test loss: 0.3219 score: 0.8992 time: 0.96s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1212;  Loss pred: 0.1212; Loss self: 0.0000; time: 1.22s
Val loss: 0.2121 score: 0.9302 time: 1.12s
Test loss: 0.3266 score: 0.9070 time: 1.00s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1406;  Loss pred: 0.1406; Loss self: 0.0000; time: 1.34s
Val loss: 0.2320 score: 0.9225 time: 1.05s
Test loss: 0.3394 score: 0.9070 time: 1.05s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1352;  Loss pred: 0.1352; Loss self: 0.0000; time: 1.06s
Val loss: 0.2555 score: 0.9302 time: 1.08s
Test loss: 0.3596 score: 0.9070 time: 1.05s
     INFO: Early stopping counter 12 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 1.05s
Val loss: 0.2481 score: 0.9225 time: 1.05s
Test loss: 0.3571 score: 0.9070 time: 1.05s
     INFO: Early stopping counter 13 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.1074;  Loss pred: 0.1074; Loss self: 0.0000; time: 1.06s
Val loss: 0.2358 score: 0.9225 time: 1.08s
Test loss: 0.3512 score: 0.9070 time: 1.05s
     INFO: Early stopping counter 14 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 1.05s
Val loss: 0.2109 score: 0.9380 time: 1.14s
Test loss: 0.3400 score: 0.9147 time: 0.95s
     INFO: Early stopping counter 15 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.1077;  Loss pred: 0.1077; Loss self: 0.0000; time: 1.09s
Val loss: 0.1973 score: 0.9302 time: 1.17s
Test loss: 0.3377 score: 0.9147 time: 0.94s
     INFO: Early stopping counter 16 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.1221;  Loss pred: 0.1221; Loss self: 0.0000; time: 1.08s
Val loss: 0.2084 score: 0.9380 time: 1.20s
Test loss: 0.3446 score: 0.9147 time: 0.96s
     INFO: Early stopping counter 17 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 1.21s
Val loss: 0.2249 score: 0.9225 time: 1.05s
Test loss: 0.3562 score: 0.9147 time: 0.98s
     INFO: Early stopping counter 18 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.1066;  Loss pred: 0.1066; Loss self: 0.0000; time: 1.13s
Val loss: 0.2515 score: 0.9225 time: 1.06s
Test loss: 0.3791 score: 0.9147 time: 1.13s
     INFO: Early stopping counter 19 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 1.07s
Val loss: 0.2681 score: 0.9225 time: 1.06s
Test loss: 0.3973 score: 0.8992 time: 1.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 055,   Train_Loss: 0.1901,   Val_Loss: 0.1965,   Val_Precision: 0.9672,   Val_Recall: 0.9077,   Val_accuracy: 0.9365,   Val_Score: 0.9380,   Val_Loss: 0.1965,   Test_Precision: 0.9643,   Test_Recall: 0.8438,   Test_accuracy: 0.9000,   Test_Score: 0.9070,   Test_loss: 0.2933


[1.063130671158433, 1.1318616901990026, 1.1299737140070647, 1.0430995190981776, 1.0124477490317076, 1.1267243819311261, 1.0335636080708355, 0.9684562019538134, 0.9884134829044342, 0.9679312261287123, 0.9726109998300672, 1.1089736281428486, 0.9704598728567362, 1.07281416002661, 1.071435775142163, 0.9724294289480895, 0.9650109498761594, 0.9509478569962084, 0.9846369230654091, 0.9722963199019432, 1.017771836137399, 1.1250510008540004, 1.0759208949748427, 1.020191746065393, 1.059110275004059, 0.9887551900465041, 0.9821634059771895, 1.1381177380681038, 1.0807827960234135, 1.1081271299626678, 1.0133623569272459, 1.0457404619082808, 0.9964763359166682, 1.0752434299793094, 1.084426405839622, 1.0723188850097358, 1.118562453892082, 1.101099036168307, 1.0987206590361893, 0.9729251749813557, 0.9664559541270137, 1.0145321041345596, 1.034451480023563, 1.1058105430565774, 1.013644800055772, 1.0695910260546952, 1.1319445839617401, 1.051025050925091, 0.9908053691033274, 1.0377644710242748, 0.9914862578734756, 1.062704656040296, 1.0618904209695756, 0.956876145908609, 0.984929421916604, 0.9853022240567952, 0.9986524381674826, 1.0654439060017467, 1.09378322516568, 1.0712369279935956, 1.020803501829505, 0.9781342649366707, 0.9601584009360522, 1.0828025450464338, 1.0017874769400805, 1.0773686771281064, 1.0659374510869384, 1.085093806963414, 0.9581450368277729, 0.971127106109634, 1.171265876851976, 1.234584755031392, 0.973725930089131, 1.0858228199649602, 1.0963196828961372, 1.0589398951269686, 1.063904250971973, 1.094026334118098, 0.9807611771393567, 1.0174967339262366, 1.0071357500273734, 0.9728188118897378, 0.972883720882237, 0.9712375008966774, 0.9619193288963288, 1.0564615030307323, 1.0404493808746338, 1.0531854978762567, 1.0811734711751342, 1.1014128930401057, 0.9939156649634242, 0.9810748200397938, 0.9843799180816859, 0.998483408940956, 1.1019877879880369, 0.9594882791861892, 1.1600238280370831, 1.1655715450178832, 1.0795459949877113, 1.0775783532299101, 1.0683246459811926, 1.1040922871325165, 1.1105185439810157, 1.0905545819550753, 0.99187490507029, 1.2193036878015846, 0.9448370728641748, 0.972963685169816, 0.9994708888698369, 0.952555931173265, 1.0506268811877817, 1.0888141370378435, 1.078076816862449, 0.9811899899505079, 1.010711788898334, 1.0064305500127375, 0.9643092148471624, 1.0586546608246863, 1.0659935020375997, 1.058484423905611, 0.9592460549902171, 0.9470887549687177, 0.9459642712026834, 0.9526615161448717, 0.9936517761088908, 1.0105142940301448, 0.9673905661329627, 1.085798037936911, 1.053445324068889, 1.0579288580920547, 1.0990179961081594, 1.0649333959445357, 0.9940874299500138, 0.9755782450083643, 0.9622879358939826, 1.0072213299572468, 1.051652907859534, 1.0566066429018974, 1.053010734030977, 1.056377043016255, 0.9587492060381919, 0.9446800469886512, 0.9606549609452486, 0.9890977460891008, 1.1344759028870612, 1.0572872711345553]
[0.008241323032235914, 0.008774121629449632, 0.008759486155093525, 0.008086042783706802, 0.007848432163036493, 0.008734297534349815, 0.008012120992797174, 0.007507412418246616, 0.007662120022514994, 0.007503342838207072, 0.007539620153721451, 0.008596694791805028, 0.0075229447508274126, 0.00831638883741558, 0.008305703683272582, 0.00753821262750457, 0.007480705037799685, 0.007371688813924096, 0.007632844364848132, 0.007537180774433668, 0.007889704156103869, 0.008721325588015508, 0.008340472054068548, 0.007908463147793745, 0.008210157170574101, 0.0076647689150891795, 0.007613669813776663, 0.008822618124558944, 0.008378161209483826, 0.008590132790408277, 0.007855522146722835, 0.008106515208591325, 0.007724622759043939, 0.008335220387436507, 0.008406406246818775, 0.008312549496199502, 0.008671026774357224, 0.008535651443165171, 0.008517214411133251, 0.007542055620010509, 0.007491906621139641, 0.00786458995453147, 0.00801900372111289, 0.00857217475237657, 0.007857711628339318, 0.008291403302749576, 0.008774764216757675, 0.008147481014923186, 0.007680661775994786, 0.008044685821893603, 0.0076859399835153145, 0.008238020589459659, 0.008231708689686633, 0.007417644541927202, 0.007635111797803132, 0.007638001736874382, 0.007741491768740175, 0.008259255085284857, 0.008478939729966512, 0.008304162232508494, 0.007913205440538798, 0.007582436162299772, 0.007443088379349242, 0.008393818178654526, 0.007765794394884345, 0.008351695171535709, 0.008263081016177816, 0.008411579898941193, 0.00742748090564165, 0.00752811710162507, 0.009079580440712992, 0.00957042445760769, 0.007548263023946752, 0.008417231162519072, 0.008498602192993312, 0.00820883639633309, 0.008247319774976534, 0.008480824295489131, 0.0076027998227857115, 0.00788757158082354, 0.007807253876181189, 0.0075412310999204485, 0.007541734270404938, 0.007528972875168041, 0.007456738983692471, 0.0081896240545018, 0.008065499076547548, 0.008164228665707417, 0.008381189699032048, 0.008538084442171362, 0.007704772596615691, 0.007605231163099177, 0.007630852078152604, 0.007740181464658573, 0.008542540992155324, 0.007437893637102242, 0.008992432775481264, 0.009035438333471963, 0.0083685736045559, 0.008353320567673721, 0.00828158640295498, 0.008558854939011757, 0.008608670883573765, 0.00845391148802384, 0.007688952752482868, 0.009451966572105307, 0.007324318394295929, 0.007542354148603225, 0.0077478363478281935, 0.007384154505219108, 0.00814439442781226, 0.008440419666960026, 0.00835718462684069, 0.0076061239531047126, 0.007834975107739022, 0.007801787209401066, 0.007475265231373352, 0.008206625277710747, 0.008263515519671315, 0.008205305611671404, 0.007436015930156721, 0.007341773294331145, 0.0073330563659122746, 0.007384972993371098, 0.007702726946580548, 0.007833444139768565, 0.007499151675449323, 0.008417039053774503, 0.008166242822239449, 0.00820099889993841, 0.008519519349675654, 0.008255297642980898, 0.007706104108139641, 0.007562622054328405, 0.007459596402278936, 0.007807917286490285, 0.008152348122942124, 0.00819074916978215, 0.008162873907216876, 0.008188969325707403, 0.007432164387892961, 0.007323101139446908, 0.007446937681746113, 0.007667424388287603, 0.008794386844085745, 0.008196025357632211]
[121.33974072955307, 113.97152241924485, 114.16194766384936, 123.66988732918627, 127.41398271997146, 114.49117643030242, 124.81089600356648, 133.201687117857, 130.512181623561, 133.27393157460344, 132.6326763963585, 116.3237760811597, 132.9266707548284, 120.24449788843233, 120.39919050013398, 132.65744141407131, 133.67723963811463, 135.65412556633413, 131.0127590974268, 132.67560244700888, 126.74746482431158, 114.6614685930496, 119.89729040722487, 126.4468179609554, 121.8003479378062, 130.46707749158085, 131.34270653430963, 113.34503952022648, 119.35793248619161, 116.41263579959994, 127.29898551901857, 123.35756786593025, 129.4561600214336, 119.97283257288288, 118.95689675697372, 120.30003556155665, 115.32659580262097, 117.15567425152288, 117.40927863608236, 132.5898469041795, 133.47737105776736, 127.15221083126065, 124.70377054036562, 116.65651119895307, 127.26351478634552, 120.60684584820308, 113.96317613756983, 122.73732189966054, 130.19711441081932, 124.30566241362729, 130.10770343572608, 121.38838318509121, 121.48146122479807, 134.81368571217445, 130.9738516582995, 130.92429596765442, 129.17407004654567, 121.07629437207417, 117.93927446680289, 120.42153946430348, 126.37103984146171, 131.8837348043953, 134.35283165177614, 119.13529441738437, 128.76982690383124, 119.73617085645144, 121.0202342252432, 118.88373076333436, 134.63514921195363, 132.8353406968302, 110.13724769880149, 104.48857356635665, 132.48081006550976, 118.80391315054777, 117.66640881537576, 121.81994520523064, 121.25151288957329, 117.91306660272284, 131.53049183315127, 126.78173373807788, 128.08601024886053, 132.60434360784262, 132.59549649265315, 132.82024209413566, 134.1068799896244, 122.10572711824352, 123.98488804093346, 122.48554529105068, 119.31480325704727, 117.12228975632912, 129.78968392126828, 131.48844243578444, 131.04696431779027, 129.195937403531, 117.06118834177173, 134.4466657887829, 111.2046122520478, 110.6753168017848, 119.49467702065667, 119.71287249167374, 120.74981185285787, 116.8380591943371, 116.16195037820572, 118.28843978513868, 130.05672322242927, 105.79808893433936, 136.53147585429727, 132.5845989590917, 129.06829146956767, 135.4251186501043, 122.7838372592938, 118.47752119654596, 119.65752159983514, 131.47300861325223, 127.6328241314062, 128.1757593689573, 133.77451756534938, 121.85276726549307, 121.01387086640038, 121.87236494611224, 134.48061561359833, 136.20687535695728, 136.36878677879824, 135.41010927157353, 129.82415278837416, 127.65776868481564, 133.34841636471947, 118.80662470629312, 122.45533494015886, 121.93636558194274, 117.37751379578367, 121.13433618595865, 129.76725800313818, 132.22927085555705, 134.05550998637088, 128.07512724683426, 122.6640453669816, 122.08895416908452, 122.50587371144988, 122.11548977973675, 134.55030699119166, 136.55417028359204, 134.28338502834444, 130.4218925885403, 113.70889383521983, 122.010359456586]
Elapsed: 1.035798260312183~0.06100238473848755
Time per graph: 0.008029443878389017~0.0004728867033991284
Speed: 124.96446081914725~7.196208528038324
Total Time: 1.0580
best val loss: 0.19648049670711967 test_score: 0.9070

Testing...
Test loss: 0.4532 score: 0.8915 time: 0.95s
test Score 0.8915
Epoch Time List: [4.1659789378754795, 3.2877563019283116, 3.4374812501482666, 3.61774112097919, 3.277830644743517, 3.747291174251586, 3.26739455293864, 3.3366983260493726, 3.2087414860725403, 3.1749150089453906, 3.1624299499671906, 3.5108352112583816, 3.0584695269353688, 3.230522735277191, 3.147791022900492, 3.134639047086239, 3.144733427092433, 3.0920771609526128, 3.1373651758767664, 3.1806733258999884, 3.2123715518973768, 3.629649024223909, 3.167626488953829, 3.1975349080748856, 3.360820314846933, 3.260718015022576, 3.315582557115704, 3.5642083338461816, 3.2947810497134924, 3.874343889998272, 3.217723141890019, 3.2963171142619103, 6.890527615090832, 3.276232295902446, 3.172624752158299, 3.153430146863684, 3.216862791683525, 3.22480682679452, 3.1705284588970244, 3.153466706862673, 3.1698755091056228, 3.4638398298993707, 3.4037535591050982, 3.3656807590741664, 3.5918995949905366, 3.2765433131717145, 3.2848015180788934, 3.2780438871122897, 3.339593442156911, 3.3478943642694503, 3.2297643069177866, 3.210738570196554, 3.172750915866345, 3.1294888567645103, 3.271078267833218, 3.169730870751664, 3.2215428811032325, 3.2630343970377, 3.2211821631062776, 3.180658738128841, 3.2791768859606236, 3.2171251547988504, 3.238767326110974, 3.269671357003972, 3.227206423180178, 3.366503225872293, 3.286996209062636, 3.220570375211537, 3.114501602947712, 3.1426010706927627, 3.244092997862026, 3.6290646481793374, 3.1460583328735083, 3.336365443887189, 3.2431654648389667, 3.2317622129339725, 3.1863243642728776, 3.288858742918819, 3.1892146770842373, 7.551762596005574, 3.355106471804902, 3.2429795160423964, 3.2438240859191865, 3.2068048012442887, 3.091862980974838, 3.241695740027353, 3.1243009690660983, 3.127926020184532, 3.1832308520097286, 3.269292678916827, 3.8705331918317825, 3.2565383708570153, 3.4110655223485082, 3.250779911875725, 3.4337630001828074, 3.2048083681147546, 3.523320072097704, 3.845675452845171, 3.3320771269500256, 3.177050824975595, 3.69638844486326, 3.2304136289749295, 3.2385329538956285, 3.197368839988485, 3.2048830811399966, 3.419413761002943, 3.20646703382954, 3.346234630793333, 3.258497639093548, 3.176126355305314, 3.2841256510000676, 3.19612213014625, 3.2085093571804464, 3.2043597728479654, 3.269149581901729, 3.257375655695796, 3.1976816980168223, 3.2706798100844026, 3.2405304408166558, 3.1321769759524614, 3.166764719877392, 3.1209555258974433, 3.152196546085179, 3.1099355823826045, 3.2438340049702674, 3.394231965066865, 3.237445924896747, 7.907312656752765, 3.176335564116016, 3.2116600971203297, 3.2331078599672765, 3.2640844562556595, 3.226270640967414, 3.3064460929017514, 3.2000349829904735, 3.342331260209903, 3.4365729147102684, 3.187588046072051, 3.144639452919364, 3.1983291269280016, 3.1417448818683624, 3.2057819741312414, 3.235771635780111, 3.247754802694544, 3.323403390822932, 3.1785783800296485]
Total Epoch List: [70, 76]
Total Time List: [0.9715670209843665, 1.0579514561686665]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c78408220>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 1.02s
Epoch 2/1000, LR 0.000020
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 1.03s
Epoch 3/1000, LR 0.000050
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 1.04s
Epoch 4/1000, LR 0.000080
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.23s
Epoch 5/1000, LR 0.000110
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 1.12s
Epoch 6/1000, LR 0.000140
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 1.13s
Epoch 7/1000, LR 0.000170
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 1.02s
Epoch 8/1000, LR 0.000200
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 1.15s
Epoch 9/1000, LR 0.000230
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 1.02s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.99s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 1.03s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.09s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.11s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 1.12s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 1.13s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 1.02s
Epoch 18/1000, LR 0.000290
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 1.07s
Epoch 19/1000, LR 0.000290
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 1.06s
Epoch 20/1000, LR 0.000290
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4961 time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 1.04s
Epoch 21/1000, LR 0.000290
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.4961 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 1.15s
Epoch 22/1000, LR 0.000290
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.43s
Val loss: 0.6826 score: 0.5116 time: 1.12s
Test loss: 0.6827 score: 0.5469 time: 1.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 1.06s
Val loss: 0.6788 score: 0.5969 time: 0.93s
Test loss: 0.6790 score: 0.6406 time: 1.10s
Epoch 24/1000, LR 0.000290
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 1.06s
Val loss: 0.6745 score: 0.6589 time: 1.10s
Test loss: 0.6747 score: 0.6719 time: 0.99s
Epoch 25/1000, LR 0.000290
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 1.12s
Val loss: 0.6691 score: 0.7442 time: 1.04s
Test loss: 0.6693 score: 0.6953 time: 1.03s
Epoch 26/1000, LR 0.000290
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 1.12s
Val loss: 0.6617 score: 0.8217 time: 1.05s
Test loss: 0.6618 score: 0.7656 time: 1.02s
Epoch 27/1000, LR 0.000290
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 1.15s
Val loss: 0.6533 score: 0.8605 time: 0.94s
Test loss: 0.6531 score: 0.7891 time: 1.02s
Epoch 28/1000, LR 0.000290
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 1.30s
Val loss: 0.6449 score: 0.8450 time: 0.94s
Test loss: 0.6440 score: 0.7734 time: 1.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 1.08s
Val loss: 0.6358 score: 0.8450 time: 0.92s
Test loss: 0.6343 score: 0.7734 time: 1.01s
Epoch 30/1000, LR 0.000290
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 1.18s
Val loss: 0.6258 score: 0.8295 time: 0.93s
Test loss: 0.6237 score: 0.7734 time: 1.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 1.05s
Val loss: 0.6122 score: 0.8372 time: 0.94s
Test loss: 0.6101 score: 0.7734 time: 1.11s
Epoch 32/1000, LR 0.000290
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 1.10s
Val loss: 0.5940 score: 0.8605 time: 1.02s
Test loss: 0.5927 score: 0.7891 time: 1.02s
Epoch 33/1000, LR 0.000290
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 1.11s
Val loss: 0.5714 score: 0.8605 time: 1.28s
Test loss: 0.5718 score: 0.8047 time: 1.24s
Epoch 34/1000, LR 0.000290
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 1.21s
Val loss: 0.5427 score: 0.8992 time: 5.33s
Test loss: 0.5459 score: 0.8359 time: 1.01s
Epoch 35/1000, LR 0.000290
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 1.06s
Val loss: 0.5077 score: 0.9070 time: 0.99s
Test loss: 0.5157 score: 0.8594 time: 1.09s
Epoch 36/1000, LR 0.000290
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 1.13s
Val loss: 0.4696 score: 0.9070 time: 0.93s
Test loss: 0.4847 score: 0.8828 time: 0.98s
Epoch 37/1000, LR 0.000290
Train loss: 0.4747;  Loss pred: 0.4747; Loss self: 0.0000; time: 1.13s
Val loss: 0.4377 score: 0.8915 time: 0.93s
Test loss: 0.4609 score: 0.8906 time: 1.09s
Epoch 38/1000, LR 0.000289
Train loss: 0.4458;  Loss pred: 0.4458; Loss self: 0.0000; time: 1.07s
Val loss: 0.4099 score: 0.8915 time: 0.89s
Test loss: 0.4394 score: 0.9219 time: 1.08s
Epoch 39/1000, LR 0.000289
Train loss: 0.4161;  Loss pred: 0.4161; Loss self: 0.0000; time: 1.06s
Val loss: 0.3812 score: 0.8992 time: 0.89s
Test loss: 0.4157 score: 0.9219 time: 1.14s
Epoch 40/1000, LR 0.000289
Train loss: 0.3868;  Loss pred: 0.3868; Loss self: 0.0000; time: 1.08s
Val loss: 0.3557 score: 0.9070 time: 0.93s
Test loss: 0.3956 score: 0.8750 time: 1.15s
Epoch 41/1000, LR 0.000289
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 1.05s
Val loss: 0.3339 score: 0.9070 time: 1.02s
Test loss: 0.3796 score: 0.8672 time: 1.01s
Epoch 42/1000, LR 0.000289
Train loss: 0.3397;  Loss pred: 0.3397; Loss self: 0.0000; time: 1.08s
Val loss: 0.3141 score: 0.9147 time: 1.04s
Test loss: 0.3660 score: 0.8516 time: 1.06s
Epoch 43/1000, LR 0.000289
Train loss: 0.3175;  Loss pred: 0.3175; Loss self: 0.0000; time: 1.06s
Val loss: 0.2934 score: 0.9147 time: 1.18s
Test loss: 0.3519 score: 0.8516 time: 1.02s
Epoch 44/1000, LR 0.000289
Train loss: 0.2979;  Loss pred: 0.2979; Loss self: 0.0000; time: 1.16s
Val loss: 0.2735 score: 0.9147 time: 0.92s
Test loss: 0.3384 score: 0.8516 time: 1.00s
Epoch 45/1000, LR 0.000289
Train loss: 0.2693;  Loss pred: 0.2693; Loss self: 0.0000; time: 1.17s
Val loss: 0.2570 score: 0.9070 time: 0.92s
Test loss: 0.3264 score: 0.8672 time: 1.10s
Epoch 46/1000, LR 0.000289
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 1.04s
Val loss: 0.2441 score: 0.9070 time: 0.89s
Test loss: 0.3151 score: 0.8672 time: 0.99s
Epoch 47/1000, LR 0.000289
Train loss: 0.2299;  Loss pred: 0.2299; Loss self: 0.0000; time: 1.13s
Val loss: 0.2380 score: 0.9070 time: 0.90s
Test loss: 0.3078 score: 0.8828 time: 1.12s
Epoch 48/1000, LR 0.000289
Train loss: 0.2257;  Loss pred: 0.2257; Loss self: 0.0000; time: 1.06s
Val loss: 0.2375 score: 0.9070 time: 0.90s
Test loss: 0.3072 score: 0.9219 time: 1.09s
Epoch 49/1000, LR 0.000289
Train loss: 0.2217;  Loss pred: 0.2217; Loss self: 0.0000; time: 1.10s
Val loss: 0.2343 score: 0.9070 time: 0.89s
Test loss: 0.3069 score: 0.9219 time: 1.14s
Epoch 50/1000, LR 0.000289
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 1.14s
Val loss: 0.2294 score: 0.9070 time: 0.93s
Test loss: 0.3063 score: 0.9141 time: 1.17s
Epoch 51/1000, LR 0.000289
Train loss: 0.2005;  Loss pred: 0.2005; Loss self: 0.0000; time: 1.23s
Val loss: 0.2347 score: 0.9302 time: 1.21s
Test loss: 0.3107 score: 0.9219 time: 1.01s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1990;  Loss pred: 0.1990; Loss self: 0.0000; time: 1.23s
Val loss: 0.2695 score: 0.9147 time: 0.90s
Test loss: 0.3375 score: 0.9062 time: 1.01s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1997;  Loss pred: 0.1997; Loss self: 0.0000; time: 1.17s
Val loss: 0.2744 score: 0.9147 time: 0.94s
Test loss: 0.3411 score: 0.9062 time: 1.03s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 1.16s
Val loss: 0.2428 score: 0.9225 time: 0.93s
Test loss: 0.3158 score: 0.9219 time: 1.03s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 1.17s
Val loss: 0.2148 score: 0.9380 time: 1.02s
Test loss: 0.3060 score: 0.9062 time: 1.36s
Epoch 56/1000, LR 0.000289
Train loss: 0.1911;  Loss pred: 0.1911; Loss self: 0.0000; time: 1.05s
Val loss: 0.2135 score: 0.9302 time: 0.95s
Test loss: 0.3220 score: 0.8672 time: 1.12s
Epoch 57/1000, LR 0.000288
Train loss: 0.1875;  Loss pred: 0.1875; Loss self: 0.0000; time: 1.11s
Val loss: 0.2188 score: 0.9070 time: 0.91s
Test loss: 0.3376 score: 0.8594 time: 1.14s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.2034;  Loss pred: 0.2034; Loss self: 0.0000; time: 1.05s
Val loss: 0.2206 score: 0.9070 time: 1.00s
Test loss: 0.3433 score: 0.8594 time: 1.07s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.1997;  Loss pred: 0.1997; Loss self: 0.0000; time: 1.06s
Val loss: 0.2177 score: 0.9225 time: 0.90s
Test loss: 0.3400 score: 0.8672 time: 1.13s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.1861;  Loss pred: 0.1861; Loss self: 0.0000; time: 1.07s
Val loss: 0.2138 score: 0.9302 time: 1.03s
Test loss: 0.3342 score: 0.8672 time: 0.99s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 1.19s
Val loss: 0.2098 score: 0.9302 time: 0.94s
Test loss: 0.3246 score: 0.8828 time: 1.01s
Epoch 62/1000, LR 0.000288
Train loss: 0.1670;  Loss pred: 0.1670; Loss self: 0.0000; time: 1.19s
Val loss: 0.2098 score: 0.9302 time: 0.92s
Test loss: 0.3186 score: 0.9062 time: 1.21s
Epoch 63/1000, LR 0.000288
Train loss: 0.1762;  Loss pred: 0.1762; Loss self: 0.0000; time: 1.38s
Val loss: 0.2104 score: 0.9302 time: 1.20s
Test loss: 0.3198 score: 0.8984 time: 1.29s
     INFO: Early stopping counter 1 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 1.06s
Val loss: 0.2113 score: 0.9302 time: 0.90s
Test loss: 0.3209 score: 0.8984 time: 1.10s
     INFO: Early stopping counter 2 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 1.05s
Val loss: 0.2144 score: 0.9225 time: 0.99s
Test loss: 0.3220 score: 0.8984 time: 1.06s
     INFO: Early stopping counter 3 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 1.14s
Val loss: 0.2196 score: 0.9147 time: 0.90s
Test loss: 0.3249 score: 0.8984 time: 0.97s
     INFO: Early stopping counter 4 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 1.16s
Val loss: 0.2215 score: 0.9147 time: 0.90s
Test loss: 0.3281 score: 0.8984 time: 1.12s
     INFO: Early stopping counter 5 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.1626;  Loss pred: 0.1626; Loss self: 0.0000; time: 1.13s
Val loss: 0.2190 score: 0.9147 time: 0.91s
Test loss: 0.3300 score: 0.8984 time: 1.12s
     INFO: Early stopping counter 6 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 1.09s
Val loss: 0.2181 score: 0.9147 time: 0.94s
Test loss: 0.3324 score: 0.8984 time: 1.12s
     INFO: Early stopping counter 7 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.1404;  Loss pred: 0.1404; Loss self: 0.0000; time: 1.14s
Val loss: 0.2172 score: 0.9147 time: 1.01s
Test loss: 0.3347 score: 0.8984 time: 1.02s
     INFO: Early stopping counter 8 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 1.11s
Val loss: 0.2169 score: 0.9147 time: 1.03s
Test loss: 0.3370 score: 0.8984 time: 1.01s
     INFO: Early stopping counter 9 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 1.07s
Val loss: 0.2171 score: 0.9147 time: 1.03s
Test loss: 0.3391 score: 0.8984 time: 1.05s
     INFO: Early stopping counter 10 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.1330;  Loss pred: 0.1330; Loss self: 0.0000; time: 1.38s
Val loss: 0.2182 score: 0.9147 time: 1.19s
Test loss: 0.3416 score: 0.8984 time: 1.00s
     INFO: Early stopping counter 11 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 1.19s
Val loss: 0.2158 score: 0.9225 time: 0.93s
Test loss: 0.3444 score: 0.8984 time: 0.98s
     INFO: Early stopping counter 12 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.1402;  Loss pred: 0.1402; Loss self: 0.0000; time: 1.20s
Val loss: 0.2118 score: 0.9380 time: 0.94s
Test loss: 0.3521 score: 0.8828 time: 1.11s
     INFO: Early stopping counter 13 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.1406;  Loss pred: 0.1406; Loss self: 0.0000; time: 1.10s
Val loss: 0.2127 score: 0.9380 time: 0.91s
Test loss: 0.3572 score: 0.8828 time: 1.14s
     INFO: Early stopping counter 14 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 1.05s
Val loss: 0.2132 score: 0.9380 time: 1.00s
Test loss: 0.3565 score: 0.8828 time: 1.07s
     INFO: Early stopping counter 15 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 1.06s
Val loss: 0.2146 score: 0.9302 time: 1.09s
Test loss: 0.3548 score: 0.8828 time: 1.01s
     INFO: Early stopping counter 16 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 1.08s
Val loss: 0.2189 score: 0.9147 time: 1.03s
Test loss: 0.3532 score: 0.8828 time: 0.98s
     INFO: Early stopping counter 17 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.1623;  Loss pred: 0.1623; Loss self: 0.0000; time: 1.06s
Val loss: 0.2214 score: 0.9070 time: 0.99s
Test loss: 0.3532 score: 0.8828 time: 1.03s
     INFO: Early stopping counter 18 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 1.18s
Val loss: 0.2216 score: 0.9070 time: 0.89s
Test loss: 0.3545 score: 0.8906 time: 1.01s
     INFO: Early stopping counter 19 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 1.16s
Val loss: 0.2159 score: 0.9302 time: 0.92s
Test loss: 0.3611 score: 0.8828 time: 1.02s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 061,   Train_Loss: 0.1670,   Val_Loss: 0.2098,   Val_Precision: 0.9516,   Val_Recall: 0.9077,   Val_accuracy: 0.9291,   Val_Score: 0.9302,   Val_Loss: 0.2098,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9062,   Test_loss: 0.3186


[1.063130671158433, 1.1318616901990026, 1.1299737140070647, 1.0430995190981776, 1.0124477490317076, 1.1267243819311261, 1.0335636080708355, 0.9684562019538134, 0.9884134829044342, 0.9679312261287123, 0.9726109998300672, 1.1089736281428486, 0.9704598728567362, 1.07281416002661, 1.071435775142163, 0.9724294289480895, 0.9650109498761594, 0.9509478569962084, 0.9846369230654091, 0.9722963199019432, 1.017771836137399, 1.1250510008540004, 1.0759208949748427, 1.020191746065393, 1.059110275004059, 0.9887551900465041, 0.9821634059771895, 1.1381177380681038, 1.0807827960234135, 1.1081271299626678, 1.0133623569272459, 1.0457404619082808, 0.9964763359166682, 1.0752434299793094, 1.084426405839622, 1.0723188850097358, 1.118562453892082, 1.101099036168307, 1.0987206590361893, 0.9729251749813557, 0.9664559541270137, 1.0145321041345596, 1.034451480023563, 1.1058105430565774, 1.013644800055772, 1.0695910260546952, 1.1319445839617401, 1.051025050925091, 0.9908053691033274, 1.0377644710242748, 0.9914862578734756, 1.062704656040296, 1.0618904209695756, 0.956876145908609, 0.984929421916604, 0.9853022240567952, 0.9986524381674826, 1.0654439060017467, 1.09378322516568, 1.0712369279935956, 1.020803501829505, 0.9781342649366707, 0.9601584009360522, 1.0828025450464338, 1.0017874769400805, 1.0773686771281064, 1.0659374510869384, 1.085093806963414, 0.9581450368277729, 0.971127106109634, 1.171265876851976, 1.234584755031392, 0.973725930089131, 1.0858228199649602, 1.0963196828961372, 1.0589398951269686, 1.063904250971973, 1.094026334118098, 0.9807611771393567, 1.0174967339262366, 1.0071357500273734, 0.9728188118897378, 0.972883720882237, 0.9712375008966774, 0.9619193288963288, 1.0564615030307323, 1.0404493808746338, 1.0531854978762567, 1.0811734711751342, 1.1014128930401057, 0.9939156649634242, 0.9810748200397938, 0.9843799180816859, 0.998483408940956, 1.1019877879880369, 0.9594882791861892, 1.1600238280370831, 1.1655715450178832, 1.0795459949877113, 1.0775783532299101, 1.0683246459811926, 1.1040922871325165, 1.1105185439810157, 1.0905545819550753, 0.99187490507029, 1.2193036878015846, 0.9448370728641748, 0.972963685169816, 0.9994708888698369, 0.952555931173265, 1.0506268811877817, 1.0888141370378435, 1.078076816862449, 0.9811899899505079, 1.010711788898334, 1.0064305500127375, 0.9643092148471624, 1.0586546608246863, 1.0659935020375997, 1.058484423905611, 0.9592460549902171, 0.9470887549687177, 0.9459642712026834, 0.9526615161448717, 0.9936517761088908, 1.0105142940301448, 0.9673905661329627, 1.085798037936911, 1.053445324068889, 1.0579288580920547, 1.0990179961081594, 1.0649333959445357, 0.9940874299500138, 0.9755782450083643, 0.9622879358939826, 1.0072213299572468, 1.051652907859534, 1.0566066429018974, 1.053010734030977, 1.056377043016255, 0.9587492060381919, 0.9446800469886512, 0.9606549609452486, 0.9890977460891008, 1.1344759028870612, 1.0572872711345553, 1.0295731439255178, 1.0310271529015154, 1.0503498329780996, 1.2315851210150868, 1.1201987040694803, 1.1304369647987187, 1.026157913962379, 1.1550232521258295, 1.0245379679836333, 1.0006835290696472, 1.0079798810184002, 1.038746772101149, 1.0906703500077128, 1.1180286582093686, 1.128993626916781, 1.1345826708711684, 1.0205870000645518, 1.0798684258479625, 1.0679022688418627, 1.0462628211826086, 1.1559595058206469, 1.1505832609254867, 1.109600025927648, 0.9990868191234767, 1.0364882410503924, 1.0269373399205506, 1.0254647149704397, 1.167153351008892, 1.0194557069335133, 1.1748265731148422, 1.1172580409329385, 1.0253277239389718, 1.2487102060113102, 1.0109327861573547, 1.0906695339363068, 0.9838533611036837, 1.090604024939239, 1.0884075579233468, 1.1466087938752025, 1.150170779088512, 1.019419859861955, 1.0689954289700836, 1.0210321818012744, 1.0084301959723234, 1.1021560889203101, 0.9914246900007129, 1.1296098760794848, 1.0976131928618997, 1.148568290984258, 1.1788929670583457, 1.0138928729575127, 1.0162595121655613, 1.0355051748920232, 1.035803159000352, 1.3676723770331591, 1.1286773250903934, 1.1478902311064303, 1.0743575990200043, 1.131689118919894, 0.9985974440351129, 1.0180190841201693, 1.2200624470133334, 1.2981911979150027, 1.1081140220630914, 1.060868744039908, 0.9768932859878987, 1.1202436408493668, 1.1210205238312483, 1.1204919130541384, 1.0281666708178818, 1.014869255013764, 1.0542415601667017, 1.0046797499526292, 0.9892883088905364, 1.1116659441031516, 1.1430820480454713, 1.0730335910338908, 1.014342722017318, 0.9899760119151324, 1.036411862121895, 1.0101814561057836, 1.0213896639179438]
[0.008241323032235914, 0.008774121629449632, 0.008759486155093525, 0.008086042783706802, 0.007848432163036493, 0.008734297534349815, 0.008012120992797174, 0.007507412418246616, 0.007662120022514994, 0.007503342838207072, 0.007539620153721451, 0.008596694791805028, 0.0075229447508274126, 0.00831638883741558, 0.008305703683272582, 0.00753821262750457, 0.007480705037799685, 0.007371688813924096, 0.007632844364848132, 0.007537180774433668, 0.007889704156103869, 0.008721325588015508, 0.008340472054068548, 0.007908463147793745, 0.008210157170574101, 0.0076647689150891795, 0.007613669813776663, 0.008822618124558944, 0.008378161209483826, 0.008590132790408277, 0.007855522146722835, 0.008106515208591325, 0.007724622759043939, 0.008335220387436507, 0.008406406246818775, 0.008312549496199502, 0.008671026774357224, 0.008535651443165171, 0.008517214411133251, 0.007542055620010509, 0.007491906621139641, 0.00786458995453147, 0.00801900372111289, 0.00857217475237657, 0.007857711628339318, 0.008291403302749576, 0.008774764216757675, 0.008147481014923186, 0.007680661775994786, 0.008044685821893603, 0.0076859399835153145, 0.008238020589459659, 0.008231708689686633, 0.007417644541927202, 0.007635111797803132, 0.007638001736874382, 0.007741491768740175, 0.008259255085284857, 0.008478939729966512, 0.008304162232508494, 0.007913205440538798, 0.007582436162299772, 0.007443088379349242, 0.008393818178654526, 0.007765794394884345, 0.008351695171535709, 0.008263081016177816, 0.008411579898941193, 0.00742748090564165, 0.00752811710162507, 0.009079580440712992, 0.00957042445760769, 0.007548263023946752, 0.008417231162519072, 0.008498602192993312, 0.00820883639633309, 0.008247319774976534, 0.008480824295489131, 0.0076027998227857115, 0.00788757158082354, 0.007807253876181189, 0.0075412310999204485, 0.007541734270404938, 0.007528972875168041, 0.007456738983692471, 0.0081896240545018, 0.008065499076547548, 0.008164228665707417, 0.008381189699032048, 0.008538084442171362, 0.007704772596615691, 0.007605231163099177, 0.007630852078152604, 0.007740181464658573, 0.008542540992155324, 0.007437893637102242, 0.008992432775481264, 0.009035438333471963, 0.0083685736045559, 0.008353320567673721, 0.00828158640295498, 0.008558854939011757, 0.008608670883573765, 0.00845391148802384, 0.007688952752482868, 0.009451966572105307, 0.007324318394295929, 0.007542354148603225, 0.0077478363478281935, 0.007384154505219108, 0.00814439442781226, 0.008440419666960026, 0.00835718462684069, 0.0076061239531047126, 0.007834975107739022, 0.007801787209401066, 0.007475265231373352, 0.008206625277710747, 0.008263515519671315, 0.008205305611671404, 0.007436015930156721, 0.007341773294331145, 0.0073330563659122746, 0.007384972993371098, 0.007702726946580548, 0.007833444139768565, 0.007499151675449323, 0.008417039053774503, 0.008166242822239449, 0.00820099889993841, 0.008519519349675654, 0.008255297642980898, 0.007706104108139641, 0.007562622054328405, 0.007459596402278936, 0.007807917286490285, 0.008152348122942124, 0.00819074916978215, 0.008162873907216876, 0.008188969325707403, 0.007432164387892961, 0.007323101139446908, 0.007446937681746113, 0.007667424388287603, 0.008794386844085745, 0.008196025357632211, 0.008043540186918108, 0.008054899632043089, 0.008205858070141403, 0.009621758757930365, 0.008751552375542815, 0.00883153878748999, 0.008016858702831087, 0.009023619157233043, 0.008004202874872135, 0.007817840070856619, 0.007874842820456252, 0.008115209157040226, 0.008520862109435257, 0.008734598892260692, 0.008820262710287352, 0.008863927116181003, 0.007973335938004311, 0.008436472076937207, 0.008342986475327052, 0.00817392829048913, 0.009030933639223804, 0.008988931725980365, 0.00866875020255975, 0.007805365774402162, 0.00809756438320619, 0.008022947968129301, 0.00801144308570656, 0.00911838555475697, 0.007964497710418073, 0.009178332602459705, 0.008728578444788582, 0.008010372843273217, 0.009755548484463361, 0.007897912391854334, 0.008520855733877397, 0.007686354383622529, 0.008520343944837805, 0.008503184046276147, 0.00895788120215002, 0.008985709211629, 0.007964217655171524, 0.008351526788828778, 0.007976813920322456, 0.007878360906033777, 0.008610594444689923, 0.007745505390630569, 0.008825077156870975, 0.008575103069233592, 0.008973189773314516, 0.009210101305143326, 0.007921038069980568, 0.007939527438793448, 0.008089884178843931, 0.00809221217969025, 0.010684940445571556, 0.008817791602268699, 0.008967892430518987, 0.008393418742343783, 0.008841321241561673, 0.007801542531524319, 0.007953274094688823, 0.009531737867291667, 0.010142118733710959, 0.008657140797367902, 0.008288037062811782, 0.007631978796780459, 0.008751903444135678, 0.008757972842431627, 0.008753843070735456, 0.008032552115764702, 0.00792866605479503, 0.008236262188802357, 0.007849060546504916, 0.007728814913207316, 0.008684890188305872, 0.008930328500355245, 0.008383074929952272, 0.007924552515760297, 0.007734187593086972, 0.008096967672827304, 0.007892042625826434, 0.007979606749358936]
[121.33974072955307, 113.97152241924485, 114.16194766384936, 123.66988732918627, 127.41398271997146, 114.49117643030242, 124.81089600356648, 133.201687117857, 130.512181623561, 133.27393157460344, 132.6326763963585, 116.3237760811597, 132.9266707548284, 120.24449788843233, 120.39919050013398, 132.65744141407131, 133.67723963811463, 135.65412556633413, 131.0127590974268, 132.67560244700888, 126.74746482431158, 114.6614685930496, 119.89729040722487, 126.4468179609554, 121.8003479378062, 130.46707749158085, 131.34270653430963, 113.34503952022648, 119.35793248619161, 116.41263579959994, 127.29898551901857, 123.35756786593025, 129.4561600214336, 119.97283257288288, 118.95689675697372, 120.30003556155665, 115.32659580262097, 117.15567425152288, 117.40927863608236, 132.5898469041795, 133.47737105776736, 127.15221083126065, 124.70377054036562, 116.65651119895307, 127.26351478634552, 120.60684584820308, 113.96317613756983, 122.73732189966054, 130.19711441081932, 124.30566241362729, 130.10770343572608, 121.38838318509121, 121.48146122479807, 134.81368571217445, 130.9738516582995, 130.92429596765442, 129.17407004654567, 121.07629437207417, 117.93927446680289, 120.42153946430348, 126.37103984146171, 131.8837348043953, 134.35283165177614, 119.13529441738437, 128.76982690383124, 119.73617085645144, 121.0202342252432, 118.88373076333436, 134.63514921195363, 132.8353406968302, 110.13724769880149, 104.48857356635665, 132.48081006550976, 118.80391315054777, 117.66640881537576, 121.81994520523064, 121.25151288957329, 117.91306660272284, 131.53049183315127, 126.78173373807788, 128.08601024886053, 132.60434360784262, 132.59549649265315, 132.82024209413566, 134.1068799896244, 122.10572711824352, 123.98488804093346, 122.48554529105068, 119.31480325704727, 117.12228975632912, 129.78968392126828, 131.48844243578444, 131.04696431779027, 129.195937403531, 117.06118834177173, 134.4466657887829, 111.2046122520478, 110.6753168017848, 119.49467702065667, 119.71287249167374, 120.74981185285787, 116.8380591943371, 116.16195037820572, 118.28843978513868, 130.05672322242927, 105.79808893433936, 136.53147585429727, 132.5845989590917, 129.06829146956767, 135.4251186501043, 122.7838372592938, 118.47752119654596, 119.65752159983514, 131.47300861325223, 127.6328241314062, 128.1757593689573, 133.77451756534938, 121.85276726549307, 121.01387086640038, 121.87236494611224, 134.48061561359833, 136.20687535695728, 136.36878677879824, 135.41010927157353, 129.82415278837416, 127.65776868481564, 133.34841636471947, 118.80662470629312, 122.45533494015886, 121.93636558194274, 117.37751379578367, 121.13433618595865, 129.76725800313818, 132.22927085555705, 134.05550998637088, 128.07512724683426, 122.6640453669816, 122.08895416908452, 122.50587371144988, 122.11548977973675, 134.55030699119166, 136.55417028359204, 134.28338502834444, 130.4218925885403, 113.70889383521983, 122.010359456586, 124.3233671693945, 124.14803978710216, 121.86415990287388, 103.93110294682751, 114.2654419568591, 113.2305506506426, 124.73713671002562, 110.82027982070056, 124.93436456231436, 127.9125680413705, 126.98666154990788, 123.22541300521691, 117.35901686434845, 114.48722629794162, 113.37530783903617, 112.81681210741353, 125.4180192300157, 118.53295914221077, 119.86115558946764, 122.34019732758871, 110.73052244086124, 111.24792472388441, 115.3568826685899, 128.11699398886827, 123.49392393519382, 124.6424635897481, 124.8214571709469, 109.66853660605634, 125.55719599139768, 108.95225127622959, 114.56619268823233, 124.83813419992792, 102.50576905979148, 126.61573722080908, 117.35910467585774, 130.10068884290845, 117.36615405131232, 117.60300548098054, 111.63354117265874, 111.28782118898683, 125.56161110823675, 119.73858496600002, 125.36333553579695, 126.92995559953759, 116.13600041477869, 129.10713369455019, 113.31345689385005, 116.61667410014886, 111.44309050209912, 108.57643872402967, 126.24607925946418, 125.95208061299525, 123.61116400345091, 123.57560303593986, 93.5896652951825, 113.4070802651668, 111.50892004423032, 119.14096397396699, 113.10526703850053, 128.17977931405485, 125.73438160113174, 104.91266271930517, 98.5987273720374, 115.5115786385313, 120.65583109986024, 131.0276176896415, 114.26085838162011, 114.18167400052735, 114.23554111257157, 124.49343441387677, 126.12462084908074, 121.41429899591516, 127.40378215648839, 129.38594224725955, 115.14250362618185, 111.97796362812639, 119.28797110318723, 126.19009060905417, 129.2960621867806, 123.50302488620649, 126.70990862714487, 125.31945889192319]
Elapsed: 1.0518840425433347~0.0696734663048708
Time per graph: 0.008177674852440909~0.0005523085934179108
Speed: 122.81717622933523~7.931072176714835
Total Time: 1.0223
best val loss: 0.2098061154459336 test_score: 0.9062

Testing...
Test loss: 0.3060 score: 0.9062 time: 1.12s
test Score 0.9062
Epoch Time List: [4.1659789378754795, 3.2877563019283116, 3.4374812501482666, 3.61774112097919, 3.277830644743517, 3.747291174251586, 3.26739455293864, 3.3366983260493726, 3.2087414860725403, 3.1749150089453906, 3.1624299499671906, 3.5108352112583816, 3.0584695269353688, 3.230522735277191, 3.147791022900492, 3.134639047086239, 3.144733427092433, 3.0920771609526128, 3.1373651758767664, 3.1806733258999884, 3.2123715518973768, 3.629649024223909, 3.167626488953829, 3.1975349080748856, 3.360820314846933, 3.260718015022576, 3.315582557115704, 3.5642083338461816, 3.2947810497134924, 3.874343889998272, 3.217723141890019, 3.2963171142619103, 6.890527615090832, 3.276232295902446, 3.172624752158299, 3.153430146863684, 3.216862791683525, 3.22480682679452, 3.1705284588970244, 3.153466706862673, 3.1698755091056228, 3.4638398298993707, 3.4037535591050982, 3.3656807590741664, 3.5918995949905366, 3.2765433131717145, 3.2848015180788934, 3.2780438871122897, 3.339593442156911, 3.3478943642694503, 3.2297643069177866, 3.210738570196554, 3.172750915866345, 3.1294888567645103, 3.271078267833218, 3.169730870751664, 3.2215428811032325, 3.2630343970377, 3.2211821631062776, 3.180658738128841, 3.2791768859606236, 3.2171251547988504, 3.238767326110974, 3.269671357003972, 3.227206423180178, 3.366503225872293, 3.286996209062636, 3.220570375211537, 3.114501602947712, 3.1426010706927627, 3.244092997862026, 3.6290646481793374, 3.1460583328735083, 3.336365443887189, 3.2431654648389667, 3.2317622129339725, 3.1863243642728776, 3.288858742918819, 3.1892146770842373, 7.551762596005574, 3.355106471804902, 3.2429795160423964, 3.2438240859191865, 3.2068048012442887, 3.091862980974838, 3.241695740027353, 3.1243009690660983, 3.127926020184532, 3.1832308520097286, 3.269292678916827, 3.8705331918317825, 3.2565383708570153, 3.4110655223485082, 3.250779911875725, 3.4337630001828074, 3.2048083681147546, 3.523320072097704, 3.845675452845171, 3.3320771269500256, 3.177050824975595, 3.69638844486326, 3.2304136289749295, 3.2385329538956285, 3.197368839988485, 3.2048830811399966, 3.419413761002943, 3.20646703382954, 3.346234630793333, 3.258497639093548, 3.176126355305314, 3.2841256510000676, 3.19612213014625, 3.2085093571804464, 3.2043597728479654, 3.269149581901729, 3.257375655695796, 3.1976816980168223, 3.2706798100844026, 3.2405304408166558, 3.1321769759524614, 3.166764719877392, 3.1209555258974433, 3.152196546085179, 3.1099355823826045, 3.2438340049702674, 3.394231965066865, 3.237445924896747, 7.907312656752765, 3.176335564116016, 3.2116600971203297, 3.2331078599672765, 3.2640844562556595, 3.226270640967414, 3.3064460929017514, 3.2000349829904735, 3.342331260209903, 3.4365729147102684, 3.187588046072051, 3.144639452919364, 3.1983291269280016, 3.1417448818683624, 3.2057819741312414, 3.235771635780111, 3.247754802694544, 3.323403390822932, 3.1785783800296485, 3.283588126068935, 3.218203781871125, 3.276974755106494, 3.4079872502479702, 3.16857319092378, 3.2160638789646327, 3.201999100856483, 3.2219626072328538, 3.1030190060846508, 3.0604213047772646, 3.096811847994104, 3.0920866280794144, 3.2031460921280086, 3.1102680631447583, 3.155770583078265, 3.149663692805916, 3.138335719006136, 3.194987792056054, 3.221998370019719, 3.120534915011376, 3.303538952022791, 3.6925086681731045, 3.088503921870142, 3.156523580895737, 3.1939479731954634, 3.1933213849551976, 3.109293222660199, 3.398298482177779, 3.015494167106226, 3.2761232000775635, 3.1063967209775, 3.14204485504888, 3.6354567168746144, 7.547271895222366, 3.1334401201456785, 3.0401905053295195, 3.1448433259502053, 3.044404165353626, 3.090640758164227, 3.1546223980840296, 3.083282332168892, 3.1827859182376415, 3.2558866553008556, 3.0803928929381073, 3.1821472076699138, 2.921001149341464, 3.160115947946906, 3.059990592999384, 3.1262548838276416, 3.239174706162885, 3.4454917879775167, 3.146920967847109, 3.1353336579632014, 3.1149544278159738, 3.5518732317723334, 3.127939790021628, 3.1628672431688756, 3.1215466372668743, 3.088398908264935, 3.0912308951374143, 3.143784041982144, 3.3286908082664013, 3.8690900101792067, 3.0583214880898595, 3.098842464853078, 3.0138472630642354, 3.178119996795431, 3.15907283895649, 3.142392677022144, 3.170627382118255, 3.1444092420861125, 3.149851965950802, 3.5717160161584616, 3.1096386378630996, 3.2479554847814143, 3.1454774970188737, 3.1122879919130355, 3.165704407962039, 3.097526921192184, 3.086033917032182, 3.075823748949915, 3.092160640982911]
Total Epoch List: [70, 76, 82]
Total Time List: [0.9715670209843665, 1.0579514561686665, 1.0223089340142906]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c80061210>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 1.02s
Epoch 2/1000, LR 0.000015
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 1.02s
Epoch 3/1000, LR 0.000045
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 1.08s
Epoch 4/1000, LR 0.000075
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 1.29s
Epoch 5/1000, LR 0.000105
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 5.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 1.09s
Epoch 7/1000, LR 0.000165
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 1.00s
Epoch 8/1000, LR 0.000195
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 1.13s
Epoch 9/1000, LR 0.000225
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4961 time: 1.12s
Epoch 10/1000, LR 0.000255
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 1.15s
Val loss: 0.6909 score: 0.5194 time: 1.16s
Test loss: 0.6907 score: 0.5426 time: 1.05s
Epoch 11/1000, LR 0.000285
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 1.12s
Val loss: 0.6903 score: 0.6279 time: 1.10s
Test loss: 0.6898 score: 0.6512 time: 1.05s
Epoch 12/1000, LR 0.000285
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 1.23s
Val loss: 0.6895 score: 0.7132 time: 1.08s
Test loss: 0.6888 score: 0.6744 time: 1.03s
Epoch 13/1000, LR 0.000285
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 1.55s
Val loss: 0.6885 score: 0.7364 time: 1.14s
Test loss: 0.6877 score: 0.7519 time: 1.15s
Epoch 14/1000, LR 0.000285
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 1.22s
Val loss: 0.6874 score: 0.6822 time: 1.02s
Test loss: 0.6863 score: 0.7054 time: 1.09s
Epoch 15/1000, LR 0.000285
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.08s
Val loss: 0.6861 score: 0.6279 time: 0.98s
Test loss: 0.6847 score: 0.6512 time: 1.09s
Epoch 16/1000, LR 0.000285
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 1.07s
Val loss: 0.6844 score: 0.5426 time: 1.01s
Test loss: 0.6826 score: 0.6047 time: 1.11s
Epoch 17/1000, LR 0.000285
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 1.11s
Val loss: 0.6822 score: 0.5271 time: 1.14s
Test loss: 0.6800 score: 0.6047 time: 1.03s
Epoch 18/1000, LR 0.000285
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 1.09s
Val loss: 0.6793 score: 0.5426 time: 1.09s
Test loss: 0.6766 score: 0.6202 time: 1.00s
Epoch 19/1000, LR 0.000285
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 1.20s
Val loss: 0.6759 score: 0.5426 time: 1.00s
Test loss: 0.6726 score: 0.6124 time: 1.01s
Epoch 20/1000, LR 0.000285
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 1.15s
Val loss: 0.6718 score: 0.5426 time: 1.01s
Test loss: 0.6679 score: 0.6202 time: 0.98s
Epoch 21/1000, LR 0.000285
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 1.24s
Val loss: 0.6671 score: 0.5504 time: 1.01s
Test loss: 0.6626 score: 0.6202 time: 1.02s
Epoch 22/1000, LR 0.000285
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 1.30s
Val loss: 0.6618 score: 0.5426 time: 1.00s
Test loss: 0.6563 score: 0.6124 time: 1.02s
Epoch 23/1000, LR 0.000285
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 1.20s
Val loss: 0.6557 score: 0.5504 time: 1.05s
Test loss: 0.6493 score: 0.6047 time: 1.10s
Epoch 24/1000, LR 0.000285
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 1.12s
Val loss: 0.6485 score: 0.5426 time: 0.97s
Test loss: 0.6412 score: 0.6124 time: 1.10s
Epoch 25/1000, LR 0.000285
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 1.14s
Val loss: 0.6400 score: 0.5426 time: 1.12s
Test loss: 0.6319 score: 0.6202 time: 1.20s
Epoch 26/1000, LR 0.000285
Train loss: 0.6247;  Loss pred: 0.6247; Loss self: 0.0000; time: 1.08s
Val loss: 0.6300 score: 0.5504 time: 1.09s
Test loss: 0.6212 score: 0.6357 time: 1.03s
Epoch 27/1000, LR 0.000285
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 1.07s
Val loss: 0.6185 score: 0.5349 time: 1.10s
Test loss: 0.6090 score: 0.6357 time: 1.00s
Epoch 28/1000, LR 0.000285
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 1.24s
Val loss: 0.6056 score: 0.5504 time: 0.98s
Test loss: 0.5954 score: 0.6279 time: 1.04s
Epoch 29/1000, LR 0.000285
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 1.22s
Val loss: 0.5912 score: 0.5814 time: 1.00s
Test loss: 0.5805 score: 0.6589 time: 1.01s
Epoch 30/1000, LR 0.000285
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 1.18s
Val loss: 0.5748 score: 0.6589 time: 1.01s
Test loss: 0.5641 score: 0.7287 time: 1.46s
Epoch 31/1000, LR 0.000285
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 1.39s
Val loss: 0.5565 score: 0.7364 time: 1.20s
Test loss: 0.5461 score: 0.7597 time: 1.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 1.62s
Val loss: 0.5361 score: 0.8140 time: 1.40s
Test loss: 0.5264 score: 0.8372 time: 1.22s
Epoch 33/1000, LR 0.000285
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 1.11s
Val loss: 0.5144 score: 0.8372 time: 1.09s
Test loss: 0.5054 score: 0.8992 time: 1.02s
Epoch 34/1000, LR 0.000285
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 1.13s
Val loss: 0.4920 score: 0.8605 time: 1.11s
Test loss: 0.4835 score: 0.8992 time: 1.01s
Epoch 35/1000, LR 0.000285
Train loss: 0.4609;  Loss pred: 0.4609; Loss self: 0.0000; time: 1.24s
Val loss: 0.4709 score: 0.8760 time: 1.03s
Test loss: 0.4616 score: 0.8992 time: 1.02s
Epoch 36/1000, LR 0.000285
Train loss: 0.4327;  Loss pred: 0.4327; Loss self: 0.0000; time: 1.14s
Val loss: 0.4504 score: 0.8760 time: 1.10s
Test loss: 0.4401 score: 0.9070 time: 1.05s
Epoch 37/1000, LR 0.000285
Train loss: 0.4127;  Loss pred: 0.4127; Loss self: 0.0000; time: 1.25s
Val loss: 0.4315 score: 0.8837 time: 1.06s
Test loss: 0.4192 score: 0.9070 time: 1.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3900;  Loss pred: 0.3900; Loss self: 0.0000; time: 1.30s
Val loss: 0.4140 score: 0.8837 time: 1.03s
Test loss: 0.3987 score: 0.9070 time: 1.13s
Epoch 39/1000, LR 0.000284
Train loss: 0.3740;  Loss pred: 0.3740; Loss self: 0.0000; time: 1.24s
Val loss: 0.3984 score: 0.8837 time: 1.05s
Test loss: 0.3785 score: 0.9070 time: 1.11s
Epoch 40/1000, LR 0.000284
Train loss: 0.3476;  Loss pred: 0.3476; Loss self: 0.0000; time: 1.10s
Val loss: 0.3843 score: 0.8837 time: 1.00s
Test loss: 0.3588 score: 0.9302 time: 1.10s
Epoch 41/1000, LR 0.000284
Train loss: 0.3255;  Loss pred: 0.3255; Loss self: 0.0000; time: 1.15s
Val loss: 0.3713 score: 0.8837 time: 1.23s
Test loss: 0.3399 score: 0.9302 time: 1.32s
Epoch 42/1000, LR 0.000284
Train loss: 0.3067;  Loss pred: 0.3067; Loss self: 0.0000; time: 1.11s
Val loss: 0.3594 score: 0.8837 time: 1.03s
Test loss: 0.3219 score: 0.9302 time: 1.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.2934;  Loss pred: 0.2934; Loss self: 0.0000; time: 1.11s
Val loss: 0.3488 score: 0.8837 time: 1.15s
Test loss: 0.3045 score: 0.9302 time: 1.02s
Epoch 44/1000, LR 0.000284
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 1.13s
Val loss: 0.3384 score: 0.8915 time: 1.13s
Test loss: 0.2882 score: 0.9225 time: 1.09s
Epoch 45/1000, LR 0.000284
Train loss: 0.2651;  Loss pred: 0.2651; Loss self: 0.0000; time: 1.08s
Val loss: 0.3306 score: 0.8915 time: 1.10s
Test loss: 0.2727 score: 0.9147 time: 1.03s
Epoch 46/1000, LR 0.000284
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 1.21s
Val loss: 0.3234 score: 0.8915 time: 1.03s
Test loss: 0.2590 score: 0.9147 time: 1.02s
Epoch 47/1000, LR 0.000284
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 1.10s
Val loss: 0.3197 score: 0.8915 time: 1.13s
Test loss: 0.2457 score: 0.9147 time: 1.03s
Epoch 48/1000, LR 0.000284
Train loss: 0.2026;  Loss pred: 0.2026; Loss self: 0.0000; time: 1.12s
Val loss: 0.3204 score: 0.8915 time: 1.11s
Test loss: 0.2326 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 1.19s
Val loss: 0.3233 score: 0.8837 time: 1.24s
Test loss: 0.2211 score: 0.9147 time: 1.46s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.1730;  Loss pred: 0.1730; Loss self: 0.0000; time: 1.35s
Val loss: 0.3311 score: 0.8837 time: 1.03s
Test loss: 0.2101 score: 0.9225 time: 1.19s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 1.14s
Val loss: 0.3346 score: 0.8837 time: 1.01s
Test loss: 0.2039 score: 0.9147 time: 1.11s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.1565;  Loss pred: 0.1565; Loss self: 0.0000; time: 1.10s
Val loss: 0.3395 score: 0.8760 time: 1.02s
Test loss: 0.2002 score: 0.9147 time: 1.11s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 1.10s
Val loss: 0.3459 score: 0.8760 time: 0.97s
Test loss: 0.1986 score: 0.9147 time: 1.08s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 1.07s
Val loss: 0.3573 score: 0.8760 time: 5.37s
Test loss: 0.1946 score: 0.9147 time: 1.28s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 1.13s
Val loss: 0.3698 score: 0.8760 time: 1.08s
Test loss: 0.1901 score: 0.9147 time: 1.12s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1362;  Loss pred: 0.1362; Loss self: 0.0000; time: 1.11s
Val loss: 0.3826 score: 0.8760 time: 1.12s
Test loss: 0.1863 score: 0.9225 time: 1.00s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1357;  Loss pred: 0.1357; Loss self: 0.0000; time: 1.19s
Val loss: 0.3935 score: 0.8760 time: 1.08s
Test loss: 0.1840 score: 0.9225 time: 1.00s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 1.24s
Val loss: 0.4027 score: 0.8837 time: 1.05s
Test loss: 0.1827 score: 0.9225 time: 1.04s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 1.20s
Val loss: 0.4102 score: 0.8837 time: 1.02s
Test loss: 0.1818 score: 0.9225 time: 1.09s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 1.12s
Val loss: 0.4118 score: 0.8915 time: 1.13s
Test loss: 0.1862 score: 0.9225 time: 1.16s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 1.11s
Val loss: 0.4140 score: 0.8915 time: 0.98s
Test loss: 0.1889 score: 0.9225 time: 1.15s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 1.12s
Val loss: 0.4192 score: 0.8915 time: 1.02s
Test loss: 0.1880 score: 0.9302 time: 1.11s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 1.14s
Val loss: 0.4289 score: 0.8837 time: 0.98s
Test loss: 0.1815 score: 0.9380 time: 1.11s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1069;  Loss pred: 0.1069; Loss self: 0.0000; time: 1.11s
Val loss: 0.4428 score: 0.8760 time: 0.99s
Test loss: 0.1751 score: 0.9457 time: 1.13s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0967;  Loss pred: 0.0967; Loss self: 0.0000; time: 1.14s
Val loss: 0.4558 score: 0.8760 time: 1.11s
Test loss: 0.1731 score: 0.9535 time: 1.00s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 1.22s
Val loss: 0.4603 score: 0.8837 time: 0.99s
Test loss: 0.1735 score: 0.9535 time: 0.99s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 1.20s
Val loss: 0.4639 score: 0.8837 time: 1.02s
Test loss: 0.1741 score: 0.9535 time: 1.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.2195,   Val_Loss: 0.3197,   Val_Precision: 0.9310,   Val_Recall: 0.8438,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.3197,   Test_Precision: 0.9655,   Test_Recall: 0.8615,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.2457


[1.021574002224952, 1.0227729501202703, 1.083828069968149, 1.302200356964022, 5.157488275086507, 1.096587578067556, 1.0006103299092501, 1.1327178617939353, 1.1268768880981952, 1.057836476014927, 1.0503694820217788, 1.0351732489652932, 1.155497072963044, 1.090417972067371, 1.0901482161134481, 1.118272618856281, 1.0333068638574332, 1.0073960120789707, 1.014816174050793, 0.9905244510155171, 1.0279716649092734, 1.0223507010377944, 1.1088952419813722, 1.1065331341233104, 1.2034138669259846, 1.0395194429438561, 1.0033534499816597, 1.0425879650283605, 1.0158851048909128, 1.471680346177891, 1.1690181309822947, 1.2223316959571093, 1.0230883779004216, 1.019054327858612, 1.0209859660826623, 1.0556161189451814, 1.0626524682156742, 1.1323826119769365, 1.1188922619912773, 1.1124958631116897, 1.3278104858472943, 1.1919461819343269, 1.0245920510496944, 1.0980677101761103, 1.038895625853911, 1.0299023590050638, 1.0390352548565716, 1.0210064670536667, 1.4690098299179226, 1.1998181850649416, 1.1196443629451096, 1.1185855350922793, 1.0855567560065538, 1.286233289865777, 1.122028177138418, 1.0001092008315027, 1.003938538953662, 1.0470158529933542, 1.0973603238817304, 1.1617034878581762, 1.1592145329341292, 1.1110528940334916, 1.1186588557902724, 1.1323305540718138, 1.0029809710104018, 0.9940816969610751, 1.1229013931006193]
[0.007919178311821333, 0.007928472481552483, 0.008401767984249218, 0.010094576410573814, 0.03998052926423649, 0.008500678899748496, 0.007756669224102715, 0.008780758618557639, 0.008735479752699187, 0.008200282759805637, 0.008142399085440146, 0.00802459882918832, 0.00895734165087631, 0.008452852496646287, 0.008450761365220529, 0.008668779991133961, 0.008010130727577001, 0.007809271411464889, 0.007866792046905373, 0.007678484116399357, 0.007968772596195918, 0.00792519923285112, 0.00859608714714242, 0.008577776233514035, 0.009328789666092904, 0.00805829025537873, 0.007777933720788059, 0.008082077248281864, 0.007875078332487696, 0.011408374776572798, 0.009062156054126315, 0.009475439503543483, 0.007930917658142803, 0.00789964595239234, 0.007914619892113662, 0.00818307068949753, 0.008237616032679645, 0.00877815978276695, 0.008673583426288971, 0.008623998938850307, 0.010293104541451895, 0.009239892883211837, 0.007942574039144917, 0.00851215279206287, 0.008053454463983806, 0.007983739217093517, 0.008054536859353268, 0.00791477881436951, 0.011387673100138935, 0.00930091616329412, 0.00867941366624116, 0.008671205698389762, 0.008415168651213596, 0.00997080069663393, 0.008697892846034248, 0.007752784502569788, 0.007782469294214433, 0.008116401961188792, 0.008506669177377755, 0.009005453394249428, 0.00898615917003201, 0.00861281313204257, 0.008671774075893585, 0.008777756233114836, 0.00777504628690234, 0.007706059666364923, 0.008704661962020304]
[126.27572718084306, 126.12770017512742, 119.02256785413482, 99.06309678854134, 25.01217513632375, 117.63766303766471, 128.9213154652317, 113.88537635991456, 114.4756817381448, 121.94701442511015, 122.81392615453507, 124.61682151170528, 111.64026549128764, 118.30325921300002, 118.33253322186366, 115.35648626712813, 124.84190758051358, 128.05291906385628, 127.11661806204457, 130.23403901614452, 125.48984023930782, 126.17979316593741, 116.3319988365204, 116.58033186887327, 107.19504199292568, 124.09580299400636, 128.56885078967736, 123.73056694212937, 126.9828638877939, 87.65490436495031, 110.34901562356843, 105.53600174704667, 126.08881381756417, 126.58795166600581, 126.34845559626011, 122.20351478613509, 121.3943446784696, 113.91909292459833, 115.2926017831426, 115.95548736620243, 97.15241849267616, 108.22636286367799, 125.90376810735503, 117.47909423482703, 124.17031777756269, 125.25459221650908, 124.15363135855016, 126.34591862307903, 87.8142524119172, 107.51629005607855, 115.21515605247968, 115.32421612206701, 118.83303133274516, 100.29284812979893, 114.97037474495262, 128.98591463087018, 128.49392168413823, 123.20730352954723, 117.55482423829929, 111.04382602641256, 111.28224874258909, 116.10608342118357, 115.31665738154679, 113.92433025508439, 128.616597650946, 129.76800638655277, 114.88096888347235]
Elapsed: 1.1595612569475797~0.5019452818650172
Time per graph: 0.008988846953082014~0.0038910486966280394
Speed: 116.83575110700222~14.664451204810629
Total Time: 1.1234
best val loss: 0.3197333243235137 test_score: 0.9147

Testing...
Test loss: 0.2882 score: 0.9225 time: 1.00s
test Score 0.9225
Epoch Time List: [3.297477725893259, 3.2667788877151906, 3.3313323103357106, 3.468730659224093, 7.783946766983718, 3.853682345012203, 3.0958414503838867, 3.336175636155531, 3.2477362561039627, 3.3605838268995285, 3.262752279639244, 3.339502823073417, 3.8370817969553173, 3.3257934919092804, 3.1449284348636866, 3.188620006199926, 3.277982188621536, 3.18061429890804, 3.2129064020700753, 3.151242938125506, 3.2726090629585087, 3.309630449861288, 3.354633192066103, 3.1871223428752273, 3.463088346645236, 3.2115689248312265, 3.175978526007384, 3.264261406380683, 3.2352820539381355, 3.648784051183611, 3.7519643909763545, 4.240220817038789, 3.223491886863485, 3.2534209720324725, 3.281091834185645, 3.2854002059902996, 3.3712089511100203, 3.4539166199974716, 3.396587021648884, 3.2102619940415025, 3.7006256598979235, 3.3222255748696625, 3.274874876020476, 3.347416747128591, 3.2139628489967436, 3.269479828188196, 3.259850548580289, 3.2477970600593835, 3.8894165290985256, 3.5767957400530577, 3.2647601971402764, 3.234052960993722, 3.1416116890031844, 7.7241777391172945, 3.326898406026885, 3.225590077228844, 3.2759961700066924, 3.330360827734694, 3.312929063104093, 3.4151596089359373, 3.2471966622397304, 3.2470973029267043, 3.230730164097622, 3.2254777271300554, 3.2462758501060307, 3.1936016301624477, 3.3371768600773066]
Total Epoch List: [67]
Total Time List: [1.123403794132173]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c78410b80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 1.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 1.01s
Epoch 3/1000, LR 0.000045
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 1.10s
Epoch 4/1000, LR 0.000075
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 1.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 1.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 1.04s
Epoch 7/1000, LR 0.000165
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 1.19s
Val loss: 0.6924 score: 0.5271 time: 1.06s
Test loss: 0.6919 score: 0.5349 time: 0.99s
Epoch 8/1000, LR 0.000195
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.16s
Val loss: 0.6920 score: 0.6124 time: 1.07s
Test loss: 0.6915 score: 0.6589 time: 0.95s
Epoch 9/1000, LR 0.000225
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 1.32s
Val loss: 0.6916 score: 0.6977 time: 0.97s
Test loss: 0.6909 score: 0.7442 time: 0.98s
Epoch 10/1000, LR 0.000255
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 1.32s
Val loss: 0.6910 score: 0.5426 time: 0.95s
Test loss: 0.6901 score: 0.6202 time: 1.00s
Epoch 11/1000, LR 0.000285
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 1.64s
Val loss: 0.6902 score: 0.5349 time: 1.01s
Test loss: 0.6890 score: 0.5581 time: 0.96s
Epoch 12/1000, LR 0.000285
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 1.32s
Val loss: 0.6893 score: 0.5271 time: 0.93s
Test loss: 0.6877 score: 0.5271 time: 1.07s
Epoch 13/1000, LR 0.000285
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.22s
Val loss: 0.6881 score: 0.5271 time: 0.96s
Test loss: 0.6860 score: 0.5271 time: 1.13s
Epoch 14/1000, LR 0.000285
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 1.18s
Val loss: 0.6866 score: 0.5271 time: 1.09s
Test loss: 0.6840 score: 0.5271 time: 0.99s
Epoch 15/1000, LR 0.000285
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5039 time: 1.07s
Test loss: 0.6815 score: 0.5116 time: 0.99s
Epoch 16/1000, LR 0.000285
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5039 time: 1.21s
Test loss: 0.6783 score: 0.5039 time: 1.00s
Epoch 17/1000, LR 0.000285
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5039 time: 0.94s
Test loss: 0.6746 score: 0.5039 time: 0.96s
Epoch 18/1000, LR 0.000285
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6763 score: 0.5039 time: 0.93s
Test loss: 0.6701 score: 0.5039 time: 0.95s
Epoch 19/1000, LR 0.000285
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6722 score: 0.5039 time: 0.92s
Test loss: 0.6646 score: 0.5039 time: 1.06s
Epoch 20/1000, LR 0.000285
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6674 score: 0.5039 time: 0.98s
Test loss: 0.6583 score: 0.5039 time: 1.11s
Epoch 21/1000, LR 0.000285
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 1.22s
Val loss: 0.6617 score: 0.5349 time: 1.04s
Test loss: 0.6509 score: 0.5194 time: 1.00s
Epoch 22/1000, LR 0.000285
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 1.21s
Val loss: 0.6551 score: 0.5349 time: 1.09s
Test loss: 0.6423 score: 0.5194 time: 1.04s
Epoch 23/1000, LR 0.000285
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 1.29s
Val loss: 0.6471 score: 0.5271 time: 0.99s
Test loss: 0.6323 score: 0.5271 time: 0.97s
Epoch 24/1000, LR 0.000285
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 1.16s
Val loss: 0.6376 score: 0.5271 time: 1.22s
Test loss: 0.6208 score: 0.5271 time: 1.13s
Epoch 25/1000, LR 0.000285
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 1.32s
Val loss: 0.6264 score: 0.5349 time: 0.97s
Test loss: 0.6078 score: 0.5349 time: 1.01s
Epoch 26/1000, LR 0.000285
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 1.28s
Val loss: 0.6136 score: 0.5504 time: 0.96s
Test loss: 0.5934 score: 0.5581 time: 0.96s
Epoch 27/1000, LR 0.000285
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 1.32s
Val loss: 0.5989 score: 0.5814 time: 1.30s
Test loss: 0.5778 score: 0.5814 time: 1.28s
Epoch 28/1000, LR 0.000285
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 1.16s
Val loss: 0.5822 score: 0.5736 time: 0.97s
Test loss: 0.5607 score: 0.5814 time: 1.05s
Epoch 29/1000, LR 0.000285
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 1.18s
Val loss: 0.5635 score: 0.7054 time: 1.08s
Test loss: 0.5421 score: 0.7132 time: 1.07s
Epoch 30/1000, LR 0.000285
Train loss: 0.5130;  Loss pred: 0.5130; Loss self: 0.0000; time: 1.27s
Val loss: 0.5428 score: 0.8295 time: 1.03s
Test loss: 0.5218 score: 0.8062 time: 1.13s
Epoch 31/1000, LR 0.000285
Train loss: 0.4898;  Loss pred: 0.4898; Loss self: 0.0000; time: 1.19s
Val loss: 0.5218 score: 0.8450 time: 1.03s
Test loss: 0.5012 score: 0.8527 time: 1.07s
Epoch 32/1000, LR 0.000285
Train loss: 0.4685;  Loss pred: 0.4685; Loss self: 0.0000; time: 1.15s
Val loss: 0.5021 score: 0.8682 time: 1.08s
Test loss: 0.4818 score: 0.8837 time: 0.96s
Epoch 33/1000, LR 0.000285
Train loss: 0.4446;  Loss pred: 0.4446; Loss self: 0.0000; time: 1.30s
Val loss: 0.4837 score: 0.8682 time: 0.95s
Test loss: 0.4634 score: 0.8992 time: 0.98s
Epoch 34/1000, LR 0.000285
Train loss: 0.4227;  Loss pred: 0.4227; Loss self: 0.0000; time: 1.25s
Val loss: 0.4657 score: 0.8682 time: 0.94s
Test loss: 0.4463 score: 0.9070 time: 0.96s
Epoch 35/1000, LR 0.000285
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 1.25s
Val loss: 0.4486 score: 0.8760 time: 0.92s
Test loss: 0.4307 score: 0.9070 time: 4.96s
Epoch 36/1000, LR 0.000285
Train loss: 0.3825;  Loss pred: 0.3825; Loss self: 0.0000; time: 1.95s
Val loss: 0.4321 score: 0.8837 time: 0.92s
Test loss: 0.4173 score: 0.8915 time: 0.96s
Epoch 37/1000, LR 0.000285
Train loss: 0.3634;  Loss pred: 0.3634; Loss self: 0.0000; time: 1.29s
Val loss: 0.4172 score: 0.8915 time: 0.94s
Test loss: 0.4057 score: 0.8915 time: 1.11s
Epoch 38/1000, LR 0.000284
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 1.14s
Val loss: 0.4037 score: 0.8915 time: 0.98s
Test loss: 0.3956 score: 0.8915 time: 1.03s
Epoch 39/1000, LR 0.000284
Train loss: 0.3287;  Loss pred: 0.3287; Loss self: 0.0000; time: 1.19s
Val loss: 0.3911 score: 0.8915 time: 0.92s
Test loss: 0.3850 score: 0.8915 time: 1.14s
Epoch 40/1000, LR 0.000284
Train loss: 0.3172;  Loss pred: 0.3172; Loss self: 0.0000; time: 1.16s
Val loss: 0.3794 score: 0.8915 time: 1.02s
Test loss: 0.3754 score: 0.8915 time: 1.07s
Epoch 41/1000, LR 0.000284
Train loss: 0.2993;  Loss pred: 0.2993; Loss self: 0.0000; time: 1.15s
Val loss: 0.3685 score: 0.8915 time: 1.05s
Test loss: 0.3658 score: 0.8992 time: 0.97s
Epoch 42/1000, LR 0.000284
Train loss: 0.2869;  Loss pred: 0.2869; Loss self: 0.0000; time: 1.18s
Val loss: 0.3583 score: 0.8837 time: 1.04s
Test loss: 0.3562 score: 0.9070 time: 1.02s
Epoch 43/1000, LR 0.000284
Train loss: 0.2758;  Loss pred: 0.2758; Loss self: 0.0000; time: 1.28s
Val loss: 0.3485 score: 0.8837 time: 0.93s
Test loss: 0.3484 score: 0.9070 time: 1.01s
Epoch 44/1000, LR 0.000284
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 1.27s
Val loss: 0.3397 score: 0.8682 time: 0.96s
Test loss: 0.3406 score: 0.9070 time: 1.05s
Epoch 45/1000, LR 0.000284
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 1.21s
Val loss: 0.3322 score: 0.8682 time: 0.92s
Test loss: 0.3324 score: 0.9070 time: 1.05s
Epoch 46/1000, LR 0.000284
Train loss: 0.2307;  Loss pred: 0.2307; Loss self: 0.0000; time: 1.24s
Val loss: 0.3245 score: 0.8605 time: 0.95s
Test loss: 0.3272 score: 0.9070 time: 1.10s
Epoch 47/1000, LR 0.000284
Train loss: 0.2164;  Loss pred: 0.2164; Loss self: 0.0000; time: 1.16s
Val loss: 0.3175 score: 0.8527 time: 0.95s
Test loss: 0.3230 score: 0.9070 time: 1.07s
Epoch 48/1000, LR 0.000284
Train loss: 0.2155;  Loss pred: 0.2155; Loss self: 0.0000; time: 1.21s
Val loss: 0.3111 score: 0.8527 time: 1.04s
Test loss: 0.3209 score: 0.9070 time: 1.00s
Epoch 49/1000, LR 0.000284
Train loss: 0.2054;  Loss pred: 0.2054; Loss self: 0.0000; time: 1.30s
Val loss: 0.3063 score: 0.8527 time: 0.96s
Test loss: 0.3189 score: 0.9070 time: 0.96s
Epoch 50/1000, LR 0.000284
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 1.27s
Val loss: 0.3031 score: 0.8450 time: 0.94s
Test loss: 0.3176 score: 0.9070 time: 0.97s
Epoch 51/1000, LR 0.000284
Train loss: 0.1832;  Loss pred: 0.1832; Loss self: 0.0000; time: 1.28s
Val loss: 0.3020 score: 0.8527 time: 0.94s
Test loss: 0.3165 score: 0.9070 time: 0.96s
Epoch 52/1000, LR 0.000284
Train loss: 0.1776;  Loss pred: 0.1776; Loss self: 0.0000; time: 1.26s
Val loss: 0.3001 score: 0.8527 time: 0.94s
Test loss: 0.3189 score: 0.9070 time: 1.05s
Epoch 53/1000, LR 0.000284
Train loss: 0.1732;  Loss pred: 0.1732; Loss self: 0.0000; time: 1.16s
Val loss: 0.3008 score: 0.8527 time: 0.96s
Test loss: 0.3204 score: 0.9070 time: 1.06s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.1653;  Loss pred: 0.1653; Loss self: 0.0000; time: 1.20s
Val loss: 0.3005 score: 0.8450 time: 0.95s
Test loss: 0.3242 score: 0.9070 time: 1.06s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 1.16s
Val loss: 0.3019 score: 0.8450 time: 1.08s
Test loss: 0.3274 score: 0.9070 time: 1.03s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1627;  Loss pred: 0.1627; Loss self: 0.0000; time: 1.19s
Val loss: 0.3006 score: 0.8527 time: 1.07s
Test loss: 0.3337 score: 0.9070 time: 0.97s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 1.19s
Val loss: 0.2998 score: 0.8527 time: 1.06s
Test loss: 0.3391 score: 0.9070 time: 0.96s
Epoch 58/1000, LR 0.000283
Train loss: 0.1559;  Loss pred: 0.1559; Loss self: 0.0000; time: 1.19s
Val loss: 0.2981 score: 0.8605 time: 1.04s
Test loss: 0.3454 score: 0.9070 time: 0.96s
Epoch 59/1000, LR 0.000283
Train loss: 0.1525;  Loss pred: 0.1525; Loss self: 0.0000; time: 1.45s
Val loss: 0.2988 score: 0.8605 time: 0.98s
Test loss: 0.3485 score: 0.9070 time: 1.00s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1509;  Loss pred: 0.1509; Loss self: 0.0000; time: 1.28s
Val loss: 0.2951 score: 0.8605 time: 0.99s
Test loss: 0.3546 score: 0.9070 time: 1.08s
Epoch 61/1000, LR 0.000283
Train loss: 0.1387;  Loss pred: 0.1387; Loss self: 0.0000; time: 1.25s
Val loss: 0.2893 score: 0.8837 time: 0.95s
Test loss: 0.3609 score: 0.8992 time: 1.08s
Epoch 62/1000, LR 0.000283
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 1.21s
Val loss: 0.2853 score: 0.8915 time: 0.94s
Test loss: 0.3626 score: 0.8992 time: 1.10s
Epoch 63/1000, LR 0.000283
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 1.20s
Val loss: 0.2802 score: 0.8915 time: 1.05s
Test loss: 0.3625 score: 0.9070 time: 1.01s
Epoch 64/1000, LR 0.000283
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 1.42s
Val loss: 0.2763 score: 0.8915 time: 0.99s
Test loss: 0.3596 score: 0.9070 time: 0.99s
Epoch 65/1000, LR 0.000283
Train loss: 0.1406;  Loss pred: 0.1406; Loss self: 0.0000; time: 1.36s
Val loss: 0.2714 score: 0.8915 time: 0.95s
Test loss: 0.3553 score: 0.9070 time: 0.96s
Epoch 66/1000, LR 0.000283
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 1.30s
Val loss: 0.2640 score: 0.8915 time: 0.97s
Test loss: 0.3523 score: 0.9070 time: 1.17s
Epoch 67/1000, LR 0.000283
Train loss: 0.1115;  Loss pred: 0.1115; Loss self: 0.0000; time: 1.19s
Val loss: 0.2588 score: 0.8837 time: 1.06s
Test loss: 0.3477 score: 0.9070 time: 1.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 1.18s
Val loss: 0.2521 score: 0.8915 time: 1.09s
Test loss: 0.3445 score: 0.9070 time: 0.95s
Epoch 69/1000, LR 0.000283
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 1.18s
Val loss: 0.2444 score: 0.9147 time: 1.05s
Test loss: 0.3429 score: 0.9070 time: 0.95s
Epoch 70/1000, LR 0.000283
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 1.19s
Val loss: 0.2401 score: 0.9147 time: 1.04s
Test loss: 0.3460 score: 0.9147 time: 0.96s
Epoch 71/1000, LR 0.000282
Train loss: 0.1069;  Loss pred: 0.1069; Loss self: 0.0000; time: 1.25s
Val loss: 0.2396 score: 0.9225 time: 0.93s
Test loss: 0.3489 score: 0.9147 time: 0.94s
Epoch 72/1000, LR 0.000282
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 1.27s
Val loss: 0.2357 score: 0.9302 time: 0.94s
Test loss: 0.3391 score: 0.9147 time: 0.96s
Epoch 73/1000, LR 0.000282
Train loss: 0.0975;  Loss pred: 0.0975; Loss self: 0.0000; time: 1.27s
Val loss: 0.2340 score: 0.9147 time: 0.94s
Test loss: 0.3277 score: 0.9147 time: 0.98s
Epoch 74/1000, LR 0.000282
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 1.32s
Val loss: 0.2369 score: 0.9070 time: 0.95s
Test loss: 0.3180 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0859;  Loss pred: 0.0859; Loss self: 0.0000; time: 1.30s
Val loss: 0.2421 score: 0.8915 time: 1.00s
Test loss: 0.3110 score: 0.9147 time: 1.09s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 1.22s
Val loss: 0.2370 score: 0.8992 time: 0.99s
Test loss: 0.3123 score: 0.9147 time: 1.07s
     INFO: Early stopping counter 3 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 1.29s
Val loss: 0.2332 score: 0.9070 time: 1.30s
Test loss: 0.3180 score: 0.9225 time: 1.05s
Epoch 78/1000, LR 0.000282
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 1.58s
Val loss: 0.2327 score: 0.9225 time: 1.10s
Test loss: 0.3244 score: 0.9147 time: 0.96s
Epoch 79/1000, LR 0.000282
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 1.29s
Val loss: 0.2340 score: 0.9070 time: 0.98s
Test loss: 0.3206 score: 0.9147 time: 1.00s
     INFO: Early stopping counter 1 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 1.27s
Val loss: 0.2408 score: 0.8992 time: 1.05s
Test loss: 0.3076 score: 0.9225 time: 0.99s
     INFO: Early stopping counter 2 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 1.28s
Val loss: 0.2516 score: 0.8837 time: 0.98s
Test loss: 0.2988 score: 0.9302 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 1.31s
Val loss: 0.2555 score: 0.8837 time: 0.97s
Test loss: 0.2973 score: 0.9302 time: 1.04s
     INFO: Early stopping counter 4 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 1.46s
Val loss: 0.2550 score: 0.8915 time: 0.97s
Test loss: 0.2984 score: 0.9302 time: 1.11s
     INFO: Early stopping counter 5 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 1.20s
Val loss: 0.2516 score: 0.8915 time: 0.96s
Test loss: 0.3027 score: 0.9225 time: 1.06s
     INFO: Early stopping counter 6 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 1.21s
Val loss: 0.2502 score: 0.8915 time: 5.04s
Test loss: 0.3066 score: 0.9147 time: 1.77s
     INFO: Early stopping counter 7 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 1.14s
Val loss: 0.2496 score: 0.8992 time: 0.94s
Test loss: 0.3112 score: 0.9147 time: 1.05s
     INFO: Early stopping counter 8 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 1.15s
Val loss: 0.2584 score: 0.8915 time: 1.01s
Test loss: 0.3037 score: 0.9147 time: 0.96s
     INFO: Early stopping counter 9 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 1.15s
Val loss: 0.2671 score: 0.8915 time: 1.02s
Test loss: 0.2997 score: 0.9302 time: 0.96s
     INFO: Early stopping counter 10 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 1.16s
Val loss: 0.2719 score: 0.8915 time: 1.03s
Test loss: 0.2997 score: 0.9302 time: 0.96s
     INFO: Early stopping counter 11 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 1.28s
Val loss: 0.2781 score: 0.8915 time: 0.98s
Test loss: 0.3000 score: 0.9302 time: 0.96s
     INFO: Early stopping counter 12 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 1.27s
Val loss: 0.2842 score: 0.8915 time: 0.98s
Test loss: 0.3010 score: 0.9302 time: 0.96s
     INFO: Early stopping counter 13 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 1.29s
Val loss: 0.2963 score: 0.8915 time: 0.96s
Test loss: 0.3017 score: 0.9302 time: 1.00s
     INFO: Early stopping counter 14 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 1.26s
Val loss: 0.3054 score: 0.8915 time: 1.11s
Test loss: 0.3045 score: 0.9225 time: 1.09s
     INFO: Early stopping counter 15 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 1.18s
Val loss: 0.2958 score: 0.8915 time: 0.96s
Test loss: 0.3093 score: 0.9225 time: 1.07s
     INFO: Early stopping counter 16 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 1.21s
Val loss: 0.2920 score: 0.8915 time: 0.96s
Test loss: 0.3148 score: 0.9225 time: 1.08s
     INFO: Early stopping counter 17 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 1.20s
Val loss: 0.2932 score: 0.8915 time: 1.05s
Test loss: 0.3194 score: 0.9225 time: 1.00s
     INFO: Early stopping counter 18 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 1.18s
Val loss: 0.2996 score: 0.8915 time: 1.20s
Test loss: 0.3212 score: 0.9225 time: 1.13s
     INFO: Early stopping counter 19 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 1.22s
Val loss: 0.3041 score: 0.8915 time: 1.03s
Test loss: 0.3238 score: 0.9147 time: 0.96s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 077,   Train_Loss: 0.0647,   Val_Loss: 0.2327,   Val_Precision: 0.9508,   Val_Recall: 0.8923,   Val_accuracy: 0.9206,   Val_Score: 0.9225,   Val_Loss: 0.2327,   Test_Precision: 0.9344,   Test_Recall: 0.8906,   Test_accuracy: 0.9120,   Test_Score: 0.9147,   Test_loss: 0.3244


[1.021574002224952, 1.0227729501202703, 1.083828069968149, 1.302200356964022, 5.157488275086507, 1.096587578067556, 1.0006103299092501, 1.1327178617939353, 1.1268768880981952, 1.057836476014927, 1.0503694820217788, 1.0351732489652932, 1.155497072963044, 1.090417972067371, 1.0901482161134481, 1.118272618856281, 1.0333068638574332, 1.0073960120789707, 1.014816174050793, 0.9905244510155171, 1.0279716649092734, 1.0223507010377944, 1.1088952419813722, 1.1065331341233104, 1.2034138669259846, 1.0395194429438561, 1.0033534499816597, 1.0425879650283605, 1.0158851048909128, 1.471680346177891, 1.1690181309822947, 1.2223316959571093, 1.0230883779004216, 1.019054327858612, 1.0209859660826623, 1.0556161189451814, 1.0626524682156742, 1.1323826119769365, 1.1188922619912773, 1.1124958631116897, 1.3278104858472943, 1.1919461819343269, 1.0245920510496944, 1.0980677101761103, 1.038895625853911, 1.0299023590050638, 1.0390352548565716, 1.0210064670536667, 1.4690098299179226, 1.1998181850649416, 1.1196443629451096, 1.1185855350922793, 1.0855567560065538, 1.286233289865777, 1.122028177138418, 1.0001092008315027, 1.003938538953662, 1.0470158529933542, 1.0973603238817304, 1.1617034878581762, 1.1592145329341292, 1.1110528940334916, 1.1186588557902724, 1.1323305540718138, 1.0029809710104018, 0.9940816969610751, 1.1229013931006193, 1.072077319957316, 1.018921664217487, 1.11018578405492, 1.1772803501226008, 1.1858139249961823, 1.0489766020327806, 0.9964924270752817, 0.9605642398819327, 0.9838779189158231, 1.0125252278521657, 0.966210266109556, 1.0748366750776768, 1.1340109549928457, 0.9998607761226594, 0.99426463781856, 1.0004166630096734, 0.9669655938632786, 0.9533608499914408, 1.0678116120398045, 1.1100545260123909, 1.0022034519352019, 1.0416865639854223, 0.9777847288642079, 1.1348972930572927, 1.0197390029206872, 0.9643844410311431, 1.2901272128801793, 1.05740010808222, 1.0750538988504559, 1.134368761908263, 1.070490901824087, 0.963804999133572, 0.9862029871437699, 0.9617010969668627, 4.967912274878472, 0.9615364309865981, 1.1165528199635446, 1.039060782874003, 1.1413175710476935, 1.0795746119692922, 0.9783851290121675, 1.024548705900088, 1.0182700860314071, 1.0515165170654655, 1.060110362013802, 1.1069840011186898, 1.072740969946608, 1.003825947176665, 0.9674123011063784, 0.9771331329829991, 0.9652250360231847, 1.0565729700028896, 1.064287130953744, 1.0637795920483768, 1.0368219239171594, 0.9705157990101725, 0.9677119420375675, 0.9639936739113182, 1.000321164028719, 1.0835444550029933, 1.0884732028935105, 1.1046496899798512, 1.0111784040927887, 0.9958783600013703, 0.9697180220391601, 1.1725544098298997, 1.0604774670209736, 0.957676765974611, 0.9561533140949905, 0.9634673290420324, 0.9480152050964534, 0.9659148619975895, 0.987362511921674, 1.0209529791027308, 1.0953676318749785, 1.0716429490130395, 1.0585744308773428, 0.9694946219678968, 1.0096632780041546, 0.9901242719497532, 1.0032728288788348, 1.043171735946089, 1.1164224119856954, 1.0652962641324848, 1.7791848729830235, 1.0544608479831368, 0.9653760099317878, 0.9668132090009749, 0.9629646979738027, 0.962373228976503, 0.9682228269521147, 1.0058206419926137, 1.0981673609931022, 1.072841719025746, 1.081465768860653, 1.007790424861014, 1.1360273950267583, 0.9634329399559647]
[0.007919178311821333, 0.007928472481552483, 0.008401767984249218, 0.010094576410573814, 0.03998052926423649, 0.008500678899748496, 0.007756669224102715, 0.008780758618557639, 0.008735479752699187, 0.008200282759805637, 0.008142399085440146, 0.00802459882918832, 0.00895734165087631, 0.008452852496646287, 0.008450761365220529, 0.008668779991133961, 0.008010130727577001, 0.007809271411464889, 0.007866792046905373, 0.007678484116399357, 0.007968772596195918, 0.00792519923285112, 0.00859608714714242, 0.008577776233514035, 0.009328789666092904, 0.00805829025537873, 0.007777933720788059, 0.008082077248281864, 0.007875078332487696, 0.011408374776572798, 0.009062156054126315, 0.009475439503543483, 0.007930917658142803, 0.00789964595239234, 0.007914619892113662, 0.00818307068949753, 0.008237616032679645, 0.00877815978276695, 0.008673583426288971, 0.008623998938850307, 0.010293104541451895, 0.009239892883211837, 0.007942574039144917, 0.00851215279206287, 0.008053454463983806, 0.007983739217093517, 0.008054536859353268, 0.00791477881436951, 0.011387673100138935, 0.00930091616329412, 0.00867941366624116, 0.008671205698389762, 0.008415168651213596, 0.00997080069663393, 0.008697892846034248, 0.007752784502569788, 0.007782469294214433, 0.008116401961188792, 0.008506669177377755, 0.009005453394249428, 0.00898615917003201, 0.00861281313204257, 0.008671774075893585, 0.008777756233114836, 0.00777504628690234, 0.007706059666364923, 0.008704661962020304, 0.008310676898893922, 0.007898617552073543, 0.008606091349262946, 0.009126204264516284, 0.009192356007722344, 0.008131601566145586, 0.0077247474967076105, 0.007446234417689401, 0.007626960611750566, 0.007849032774047797, 0.007490002062864775, 0.00833206724866416, 0.008790782596843764, 0.007750858729633019, 0.007707477812546977, 0.007755167930307546, 0.007495857316769602, 0.007390394185980161, 0.0082776093956574, 0.008605073845057293, 0.007769019007249627, 0.008075089643297846, 0.007579726580342697, 0.008797653434552657, 0.007904953511013078, 0.00747584838008638, 0.01000098614635798, 0.008196900062652867, 0.008333751153879503, 0.008793556293862503, 0.00829837908390765, 0.00747135658243079, 0.007644984396463332, 0.007455047263309013, 0.03851094786727498, 0.007453770782841846, 0.008655448216771664, 0.008054734750961265, 0.008847423031377468, 0.00836879544162242, 0.007584380845055562, 0.00794223803023324, 0.007893566558383, 0.008151290829964849, 0.008217909783052727, 0.008581271326501471, 0.008315821472454325, 0.007781596489741589, 0.007499320163615337, 0.007574675449480613, 0.0074823646203347645, 0.008190488139557283, 0.00825028783685073, 0.008246353426731603, 0.008037379255171779, 0.007523378286900562, 0.007501642961531531, 0.007472819177607118, 0.007754427628129605, 0.008399569418627855, 0.00843777676661636, 0.008563175891316676, 0.00783859227978906, 0.007719987286832328, 0.007517193969295815, 0.009089569068448834, 0.008220755558302121, 0.007423850899027993, 0.00741204119453481, 0.0074687389848219566, 0.007348955078267081, 0.007487712108508446, 0.007653972960633132, 0.007914364179090937, 0.008491221952519213, 0.008307309682271623, 0.00820600334013444, 0.0075154621857976495, 0.007826847116311276, 0.007675381953098861, 0.00777730875099872, 0.008086602604233249, 0.008654437302214693, 0.008258110574670425, 0.013792130798318012, 0.008174115100644471, 0.007483534960711534, 0.007494676038767247, 0.007464842619951959, 0.007460257588965139, 0.007505603309706316, 0.007797059240252819, 0.008512925279016295, 0.008316602473067798, 0.008383455572563201, 0.0078123288748915805, 0.00880641391493611, 0.007468472402759416]
[126.27572718084306, 126.12770017512742, 119.02256785413482, 99.06309678854134, 25.01217513632375, 117.63766303766471, 128.9213154652317, 113.88537635991456, 114.4756817381448, 121.94701442511015, 122.81392615453507, 124.61682151170528, 111.64026549128764, 118.30325921300002, 118.33253322186366, 115.35648626712813, 124.84190758051358, 128.05291906385628, 127.11661806204457, 130.23403901614452, 125.48984023930782, 126.17979316593741, 116.3319988365204, 116.58033186887327, 107.19504199292568, 124.09580299400636, 128.56885078967736, 123.73056694212937, 126.9828638877939, 87.65490436495031, 110.34901562356843, 105.53600174704667, 126.08881381756417, 126.58795166600581, 126.34845559626011, 122.20351478613509, 121.3943446784696, 113.91909292459833, 115.2926017831426, 115.95548736620243, 97.15241849267616, 108.22636286367799, 125.90376810735503, 117.47909423482703, 124.17031777756269, 125.25459221650908, 124.15363135855016, 126.34591862307903, 87.8142524119172, 107.51629005607855, 115.21515605247968, 115.32421612206701, 118.83303133274516, 100.29284812979893, 114.97037474495262, 128.98591463087018, 128.49392168413823, 123.20730352954723, 117.55482423829929, 111.04382602641256, 111.28224874258909, 116.10608342118357, 115.31665738154679, 113.92433025508439, 128.616597650946, 129.76800638655277, 114.88096888347235, 120.32714208069997, 126.60443342233735, 116.19676801195507, 109.5745800790492, 108.78603909160141, 122.97700420582758, 129.45406958948666, 134.29606750284185, 131.1138277624429, 127.40423295293411, 133.51131169348704, 120.01823438958982, 113.75551482288269, 129.01796238096927, 129.7441295740227, 128.94627285786487, 133.40702173756927, 135.31077975475725, 120.80782653559616, 116.210507661639, 128.71637964418085, 123.83763452458744, 131.93088027652678, 113.66667344186513, 126.50295774754562, 133.76408257071225, 99.99013950880895, 121.99733952549339, 119.99398368578392, 113.71963362513002, 120.50546135440065, 133.84450186081898, 130.8047143252003, 134.13731190164688, 25.966642094773235, 134.1602833161898, 115.53416703046064, 124.15058110767193, 113.02726188783905, 119.49150949806638, 131.84991898869686, 125.90909466492444, 126.68544600260522, 122.67995595543147, 121.68544391448982, 116.53284949885084, 120.25270183017298, 128.50833390272692, 133.34542040913632, 132.01885766189082, 133.64759013244392, 122.09284513463106, 121.20789235175539, 121.26572173809248, 124.41866536987591, 132.91901083070147, 133.3041315253213, 133.81830554612878, 128.95858314190025, 119.05372170414884, 118.51463100522518, 116.77910306782682, 127.57392709126982, 129.53389207073693, 133.0283619239476, 110.01621666214517, 121.6433201191711, 134.70098114860178, 134.91560202570642, 133.89141085693444, 136.07376686207814, 133.5521432326009, 130.65110174066785, 126.352537913521, 117.7686798898615, 120.37591449540751, 121.86200255478045, 133.0590155705594, 127.76536773229975, 130.28667577855973, 128.57918233882452, 123.66132589185364, 115.54766243948609, 121.09307461530439, 72.50511285188455, 122.33740138075093, 133.6266891582638, 133.42804876786693, 133.96129709783963, 134.04362893302138, 133.23379330570145, 128.25348239467462, 117.46843384904632, 120.24140906558488, 119.28255494938524, 128.00280377519027, 113.55359964445356, 133.8961900201338]
Elapsed: 1.112697320614913~0.4496534119045671
Time per graph: 0.008625560624921808~0.003485685363601296
Speed: 120.98433952563333~14.425147173469318
Total Time: 0.9641
best val loss: 0.23270447866976723 test_score: 0.9147

Testing...
Test loss: 0.3391 score: 0.9147 time: 0.99s
test Score 0.9147
Epoch Time List: [3.297477725893259, 3.2667788877151906, 3.3313323103357106, 3.468730659224093, 7.783946766983718, 3.853682345012203, 3.0958414503838867, 3.336175636155531, 3.2477362561039627, 3.3605838268995285, 3.262752279639244, 3.339502823073417, 3.8370817969553173, 3.3257934919092804, 3.1449284348636866, 3.188620006199926, 3.277982188621536, 3.18061429890804, 3.2129064020700753, 3.151242938125506, 3.2726090629585087, 3.309630449861288, 3.354633192066103, 3.1871223428752273, 3.463088346645236, 3.2115689248312265, 3.175978526007384, 3.264261406380683, 3.2352820539381355, 3.648784051183611, 3.7519643909763545, 4.240220817038789, 3.223491886863485, 3.2534209720324725, 3.281091834185645, 3.2854002059902996, 3.3712089511100203, 3.4539166199974716, 3.396587021648884, 3.2102619940415025, 3.7006256598979235, 3.3222255748696625, 3.274874876020476, 3.347416747128591, 3.2139628489967436, 3.269479828188196, 3.259850548580289, 3.2477970600593835, 3.8894165290985256, 3.5767957400530577, 3.2647601971402764, 3.234052960993722, 3.1416116890031844, 7.7241777391172945, 3.326898406026885, 3.225590077228844, 3.2759961700066924, 3.330360827734694, 3.312929063104093, 3.4151596089359373, 3.2471966622397304, 3.2470973029267043, 3.230730164097622, 3.2254777271300554, 3.2462758501060307, 3.1936016301624477, 3.3371768600773066, 3.2943593009840697, 3.3171447583008558, 3.4013378261588514, 3.4844555652234703, 3.3819442000240088, 3.3025683141313493, 3.2353857420384884, 3.1874893540516496, 3.2719837550539523, 3.26599901705049, 3.6076323189772666, 3.3207508127670735, 3.3073020139709115, 3.2686967691406608, 3.248873815871775, 3.7551807740237564, 3.203547844896093, 3.1266515110619366, 3.258098397171125, 3.258534318068996, 3.263388718245551, 3.3437659956980497, 3.2571134257595986, 3.509843942243606, 3.305095309158787, 3.201325838919729, 3.8999184540007263, 3.1797086007427424, 3.3287158373277634, 3.4360167228151113, 3.293680358910933, 3.1893023459706455, 3.2328857970423996, 3.1479341650847346, 7.130141230998561, 3.826606242917478, 3.34152502566576, 3.153681157156825, 3.2479501098860055, 3.2473324770107865, 3.1768897329457104, 3.2309823299292475, 3.2269369061104953, 3.2751507179345936, 3.1895076306536794, 3.2863937409128994, 3.170838583027944, 3.2510624460410327, 3.2170262199360877, 3.1840309428516775, 3.178367129759863, 3.2521106319036335, 3.176766622113064, 3.210761565947905, 3.275479372823611, 3.2216022880747914, 3.2118823409546167, 3.1819796299096197, 3.425398090155795, 3.348635665839538, 3.2765422298107296, 3.249319963855669, 3.2632846268825233, 3.400207360740751, 3.2726620282046497, 3.43968526693061, 3.3046923191286623, 3.2163144019432366, 3.1748499991372228, 3.184461230179295, 3.130717371823266, 3.165763844270259, 3.197343456791714, 3.2854600970167667, 3.389892891049385, 3.2698798270430416, 3.6402748620603234, 3.6487189061008394, 3.2728969072923064, 3.3051071148365736, 3.257480834145099, 3.321214391849935, 3.5415532088372856, 3.2225007896777242, 8.02378314267844, 3.132884410209954, 3.1214790558442473, 3.1334300730377436, 3.1441109869629145, 3.209872535895556, 3.2112070620059967, 3.242348760832101, 3.4570489088073373, 3.2059986600652337, 3.241862124763429, 3.252892771968618, 3.51291327807121, 3.2048175332602113]
Total Epoch List: [67, 98]
Total Time List: [1.123403794132173, 0.9641003990545869]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c784091e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 1.11s
Epoch 2/1000, LR 0.000020
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 1.14s
Epoch 3/1000, LR 0.000050
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 1.03s
Epoch 4/1000, LR 0.000080
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 1.02s
Epoch 5/1000, LR 0.000110
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 1.05s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 1.01s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 1.02s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 1.07s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 1.17s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 1.13s
Epoch 11/1000, LR 0.000290
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 1.32s
Epoch 12/1000, LR 0.000290
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 1.12s
Epoch 13/1000, LR 0.000290
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 1.17s
Epoch 14/1000, LR 0.000290
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4961 time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 1.06s
Epoch 15/1000, LR 0.000290
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 1.07s
Epoch 16/1000, LR 0.000290
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 1.01s
Epoch 17/1000, LR 0.000290
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4961 time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 1.04s
Epoch 18/1000, LR 0.000290
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 1.23s
Val loss: 0.6871 score: 0.5039 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5000 time: 1.02s
Epoch 19/1000, LR 0.000290
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 1.09s
Val loss: 0.6848 score: 0.5426 time: 1.12s
Test loss: 0.6852 score: 0.5469 time: 1.09s
Epoch 20/1000, LR 0.000290
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 1.28s
Val loss: 0.6817 score: 0.6279 time: 1.01s
Test loss: 0.6824 score: 0.6172 time: 1.13s
Epoch 21/1000, LR 0.000290
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 1.08s
Val loss: 0.6778 score: 0.6822 time: 1.00s
Test loss: 0.6788 score: 0.6875 time: 1.00s
Epoch 22/1000, LR 0.000290
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 1.22s
Val loss: 0.6731 score: 0.7597 time: 1.01s
Test loss: 0.6743 score: 0.7891 time: 1.14s
Epoch 23/1000, LR 0.000290
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 1.08s
Val loss: 0.6681 score: 0.7829 time: 1.01s
Test loss: 0.6692 score: 0.8047 time: 1.13s
Epoch 24/1000, LR 0.000290
Train loss: 0.6633;  Loss pred: 0.6633; Loss self: 0.0000; time: 1.08s
Val loss: 0.6625 score: 0.7907 time: 1.07s
Test loss: 0.6634 score: 0.7969 time: 1.01s
Epoch 25/1000, LR 0.000290
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 1.06s
Val loss: 0.6554 score: 0.8062 time: 1.18s
Test loss: 0.6562 score: 0.8281 time: 1.00s
Epoch 26/1000, LR 0.000290
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 1.08s
Val loss: 0.6456 score: 0.8682 time: 1.13s
Test loss: 0.6466 score: 0.8516 time: 1.01s
Epoch 27/1000, LR 0.000290
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 1.26s
Val loss: 0.6358 score: 0.8682 time: 0.99s
Test loss: 0.6373 score: 0.8672 time: 1.29s
Epoch 28/1000, LR 0.000290
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 1.13s
Val loss: 0.6249 score: 0.8682 time: 1.01s
Test loss: 0.6262 score: 0.8672 time: 1.12s
Epoch 29/1000, LR 0.000290
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 1.10s
Val loss: 0.6117 score: 0.8760 time: 1.04s
Test loss: 0.6125 score: 0.8906 time: 1.13s
Epoch 30/1000, LR 0.000290
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 1.21s
Val loss: 0.5958 score: 0.8760 time: 1.09s
Test loss: 0.5960 score: 0.8906 time: 1.03s
Epoch 31/1000, LR 0.000290
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 1.22s
Val loss: 0.5794 score: 0.8837 time: 1.02s
Test loss: 0.5785 score: 0.8906 time: 1.04s
Epoch 32/1000, LR 0.000290
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 1.16s
Val loss: 0.5617 score: 0.8837 time: 1.15s
Test loss: 0.5597 score: 0.8906 time: 1.02s
Epoch 33/1000, LR 0.000290
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 1.24s
Val loss: 0.5421 score: 0.8837 time: 1.00s
Test loss: 0.5387 score: 0.8906 time: 1.27s
Epoch 34/1000, LR 0.000290
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 1.37s
Val loss: 0.5220 score: 0.8837 time: 1.00s
Test loss: 0.5171 score: 0.8906 time: 1.05s
Epoch 35/1000, LR 0.000290
Train loss: 0.4897;  Loss pred: 0.4897; Loss self: 0.0000; time: 1.36s
Val loss: 0.5004 score: 0.8837 time: 5.38s
Test loss: 0.4935 score: 0.8828 time: 1.19s
Epoch 36/1000, LR 0.000290
Train loss: 0.4636;  Loss pred: 0.4636; Loss self: 0.0000; time: 1.09s
Val loss: 0.4758 score: 0.8837 time: 1.04s
Test loss: 0.4658 score: 0.8906 time: 1.09s
Epoch 37/1000, LR 0.000290
Train loss: 0.4338;  Loss pred: 0.4338; Loss self: 0.0000; time: 1.09s
Val loss: 0.4541 score: 0.8760 time: 1.02s
Test loss: 0.4390 score: 0.8906 time: 1.09s
Epoch 38/1000, LR 0.000289
Train loss: 0.4014;  Loss pred: 0.4014; Loss self: 0.0000; time: 1.08s
Val loss: 0.4355 score: 0.8760 time: 0.98s
Test loss: 0.4154 score: 0.8828 time: 1.09s
Epoch 39/1000, LR 0.000289
Train loss: 0.3749;  Loss pred: 0.3749; Loss self: 0.0000; time: 1.09s
Val loss: 0.4136 score: 0.8837 time: 1.08s
Test loss: 0.3901 score: 0.8906 time: 0.99s
Epoch 40/1000, LR 0.000289
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 1.20s
Val loss: 0.3925 score: 0.8837 time: 0.99s
Test loss: 0.3670 score: 0.8906 time: 1.00s
Epoch 41/1000, LR 0.000289
Train loss: 0.3143;  Loss pred: 0.3143; Loss self: 0.0000; time: 1.18s
Val loss: 0.3756 score: 0.8915 time: 0.99s
Test loss: 0.3468 score: 0.8906 time: 1.09s
Epoch 42/1000, LR 0.000289
Train loss: 0.2919;  Loss pred: 0.2919; Loss self: 0.0000; time: 1.14s
Val loss: 0.3609 score: 0.8915 time: 0.98s
Test loss: 0.3276 score: 0.8906 time: 1.09s
Epoch 43/1000, LR 0.000289
Train loss: 0.2706;  Loss pred: 0.2706; Loss self: 0.0000; time: 1.25s
Val loss: 0.3480 score: 0.8837 time: 1.32s
Test loss: 0.3103 score: 0.8906 time: 1.43s
Epoch 44/1000, LR 0.000289
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 1.09s
Val loss: 0.3388 score: 0.8760 time: 1.01s
Test loss: 0.2971 score: 0.8906 time: 1.14s
Epoch 45/1000, LR 0.000289
Train loss: 0.2210;  Loss pred: 0.2210; Loss self: 0.0000; time: 1.10s
Val loss: 0.3350 score: 0.8760 time: 1.13s
Test loss: 0.2887 score: 0.8906 time: 1.03s
Epoch 46/1000, LR 0.000289
Train loss: 0.1942;  Loss pred: 0.1942; Loss self: 0.0000; time: 1.14s
Val loss: 0.3356 score: 0.8760 time: 1.10s
Test loss: 0.2837 score: 0.8906 time: 1.02s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1767;  Loss pred: 0.1767; Loss self: 0.0000; time: 1.14s
Val loss: 0.3386 score: 0.8837 time: 1.09s
Test loss: 0.2800 score: 0.8906 time: 1.05s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1627;  Loss pred: 0.1627; Loss self: 0.0000; time: 1.11s
Val loss: 0.3430 score: 0.8837 time: 1.13s
Test loss: 0.2759 score: 0.8906 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 1.19s
Val loss: 0.3499 score: 0.8837 time: 1.03s
Test loss: 0.2725 score: 0.8906 time: 1.00s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1460;  Loss pred: 0.1460; Loss self: 0.0000; time: 1.24s
Val loss: 0.3591 score: 0.8837 time: 1.01s
Test loss: 0.2710 score: 0.8984 time: 1.12s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1195;  Loss pred: 0.1195; Loss self: 0.0000; time: 1.20s
Val loss: 0.3675 score: 0.8682 time: 1.01s
Test loss: 0.2724 score: 0.9062 time: 1.14s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 1.11s
Val loss: 0.3739 score: 0.8760 time: 1.01s
Test loss: 0.2793 score: 0.8984 time: 1.13s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 1.10s
Val loss: 0.3867 score: 0.8760 time: 1.13s
Test loss: 0.2936 score: 0.8906 time: 1.13s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1189;  Loss pred: 0.1189; Loss self: 0.0000; time: 1.11s
Val loss: 0.4018 score: 0.8760 time: 1.00s
Test loss: 0.3057 score: 0.8906 time: 1.17s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.1062;  Loss pred: 0.1062; Loss self: 0.0000; time: 1.10s
Val loss: 0.4096 score: 0.8760 time: 1.13s
Test loss: 0.3073 score: 0.8906 time: 1.14s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 1.11s
Val loss: 0.4178 score: 0.8760 time: 1.11s
Test loss: 0.3115 score: 0.8906 time: 1.01s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 1.13s
Val loss: 0.4367 score: 0.8837 time: 1.10s
Test loss: 0.3310 score: 0.8906 time: 1.01s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 1.09s
Val loss: 0.4504 score: 0.8837 time: 1.10s
Test loss: 0.3421 score: 0.8906 time: 1.01s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 1.31s
Val loss: 0.4551 score: 0.8837 time: 1.26s
Test loss: 0.3434 score: 0.8984 time: 1.12s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 1.21s
Val loss: 0.4489 score: 0.8915 time: 1.00s
Test loss: 0.3340 score: 0.9062 time: 0.99s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 1.33s
Val loss: 0.4408 score: 0.8837 time: 1.26s
Test loss: 0.3215 score: 0.9062 time: 1.25s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 1.12s
Val loss: 0.4386 score: 0.8760 time: 1.01s
Test loss: 0.3111 score: 0.9062 time: 1.13s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 1.13s
Val loss: 0.4449 score: 0.8915 time: 1.13s
Test loss: 0.3068 score: 0.9062 time: 1.01s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0694;  Loss pred: 0.0694; Loss self: 0.0000; time: 1.12s
Val loss: 0.4525 score: 0.8837 time: 1.09s
Test loss: 0.3069 score: 0.9062 time: 1.03s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 1.23s
Val loss: 0.4542 score: 0.8837 time: 1.01s
Test loss: 0.3085 score: 0.9141 time: 1.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.2210,   Val_Loss: 0.3350,   Val_Precision: 0.9804,   Val_Recall: 0.7692,   Val_accuracy: 0.8621,   Val_Score: 0.8760,   Val_Loss: 0.3350,   Test_Precision: 0.9630,   Test_Recall: 0.8125,   Test_accuracy: 0.8814,   Test_Score: 0.8906,   Test_loss: 0.2887


[1.021574002224952, 1.0227729501202703, 1.083828069968149, 1.302200356964022, 5.157488275086507, 1.096587578067556, 1.0006103299092501, 1.1327178617939353, 1.1268768880981952, 1.057836476014927, 1.0503694820217788, 1.0351732489652932, 1.155497072963044, 1.090417972067371, 1.0901482161134481, 1.118272618856281, 1.0333068638574332, 1.0073960120789707, 1.014816174050793, 0.9905244510155171, 1.0279716649092734, 1.0223507010377944, 1.1088952419813722, 1.1065331341233104, 1.2034138669259846, 1.0395194429438561, 1.0033534499816597, 1.0425879650283605, 1.0158851048909128, 1.471680346177891, 1.1690181309822947, 1.2223316959571093, 1.0230883779004216, 1.019054327858612, 1.0209859660826623, 1.0556161189451814, 1.0626524682156742, 1.1323826119769365, 1.1188922619912773, 1.1124958631116897, 1.3278104858472943, 1.1919461819343269, 1.0245920510496944, 1.0980677101761103, 1.038895625853911, 1.0299023590050638, 1.0390352548565716, 1.0210064670536667, 1.4690098299179226, 1.1998181850649416, 1.1196443629451096, 1.1185855350922793, 1.0855567560065538, 1.286233289865777, 1.122028177138418, 1.0001092008315027, 1.003938538953662, 1.0470158529933542, 1.0973603238817304, 1.1617034878581762, 1.1592145329341292, 1.1110528940334916, 1.1186588557902724, 1.1323305540718138, 1.0029809710104018, 0.9940816969610751, 1.1229013931006193, 1.072077319957316, 1.018921664217487, 1.11018578405492, 1.1772803501226008, 1.1858139249961823, 1.0489766020327806, 0.9964924270752817, 0.9605642398819327, 0.9838779189158231, 1.0125252278521657, 0.966210266109556, 1.0748366750776768, 1.1340109549928457, 0.9998607761226594, 0.99426463781856, 1.0004166630096734, 0.9669655938632786, 0.9533608499914408, 1.0678116120398045, 1.1100545260123909, 1.0022034519352019, 1.0416865639854223, 0.9777847288642079, 1.1348972930572927, 1.0197390029206872, 0.9643844410311431, 1.2901272128801793, 1.05740010808222, 1.0750538988504559, 1.134368761908263, 1.070490901824087, 0.963804999133572, 0.9862029871437699, 0.9617010969668627, 4.967912274878472, 0.9615364309865981, 1.1165528199635446, 1.039060782874003, 1.1413175710476935, 1.0795746119692922, 0.9783851290121675, 1.024548705900088, 1.0182700860314071, 1.0515165170654655, 1.060110362013802, 1.1069840011186898, 1.072740969946608, 1.003825947176665, 0.9674123011063784, 0.9771331329829991, 0.9652250360231847, 1.0565729700028896, 1.064287130953744, 1.0637795920483768, 1.0368219239171594, 0.9705157990101725, 0.9677119420375675, 0.9639936739113182, 1.000321164028719, 1.0835444550029933, 1.0884732028935105, 1.1046496899798512, 1.0111784040927887, 0.9958783600013703, 0.9697180220391601, 1.1725544098298997, 1.0604774670209736, 0.957676765974611, 0.9561533140949905, 0.9634673290420324, 0.9480152050964534, 0.9659148619975895, 0.987362511921674, 1.0209529791027308, 1.0953676318749785, 1.0716429490130395, 1.0585744308773428, 0.9694946219678968, 1.0096632780041546, 0.9901242719497532, 1.0032728288788348, 1.043171735946089, 1.1164224119856954, 1.0652962641324848, 1.7791848729830235, 1.0544608479831368, 0.9653760099317878, 0.9668132090009749, 0.9629646979738027, 0.962373228976503, 0.9682228269521147, 1.0058206419926137, 1.0981673609931022, 1.072841719025746, 1.081465768860653, 1.007790424861014, 1.1360273950267583, 0.9634329399559647, 1.1150053821038455, 1.143872378859669, 1.0322008791845292, 1.0275143589824438, 1.0536530080717057, 1.0175943979993463, 1.0232236760202795, 1.0773312042001635, 1.1720417719334364, 1.1366102418396622, 1.3253516189288348, 1.1302633790764958, 1.1718753820750862, 1.0619193299207836, 1.071709481999278, 1.0122953429818153, 1.0435430069919676, 1.027147329878062, 1.097249094862491, 1.1341771350707859, 1.0017659021541476, 1.14899193495512, 1.1366642569191754, 1.0106293130666018, 1.008770405082032, 1.0140981299337, 1.2924220820423216, 1.1290662258397788, 1.1336009250953794, 1.0365534350275993, 1.0448901269119233, 1.0265139120165259, 1.2758772340603173, 1.059416411910206, 1.1976699789520353, 1.0962032601237297, 1.0906302260700613, 1.0960816021542996, 0.9977860960643739, 1.0049765438307077, 1.0995927229523659, 1.0982212140224874, 1.4320613839663565, 1.1493739429861307, 1.0351025061681867, 1.0247680561151356, 1.0507500278763473, 1.0076343060936779, 1.0062141329981387, 1.1292333099991083, 1.1427690761629492, 1.1374940329696983, 1.130864837905392, 1.1705732678528875, 1.149564159102738, 1.018573071109131, 1.0170842360239476, 1.015881456201896, 1.123814465943724, 0.9954843039158732, 1.2574479929171503, 1.1397839318960905, 1.012709507951513, 1.0336415430065244, 1.141922190086916]
[0.007919178311821333, 0.007928472481552483, 0.008401767984249218, 0.010094576410573814, 0.03998052926423649, 0.008500678899748496, 0.007756669224102715, 0.008780758618557639, 0.008735479752699187, 0.008200282759805637, 0.008142399085440146, 0.00802459882918832, 0.00895734165087631, 0.008452852496646287, 0.008450761365220529, 0.008668779991133961, 0.008010130727577001, 0.007809271411464889, 0.007866792046905373, 0.007678484116399357, 0.007968772596195918, 0.00792519923285112, 0.00859608714714242, 0.008577776233514035, 0.009328789666092904, 0.00805829025537873, 0.007777933720788059, 0.008082077248281864, 0.007875078332487696, 0.011408374776572798, 0.009062156054126315, 0.009475439503543483, 0.007930917658142803, 0.00789964595239234, 0.007914619892113662, 0.00818307068949753, 0.008237616032679645, 0.00877815978276695, 0.008673583426288971, 0.008623998938850307, 0.010293104541451895, 0.009239892883211837, 0.007942574039144917, 0.00851215279206287, 0.008053454463983806, 0.007983739217093517, 0.008054536859353268, 0.00791477881436951, 0.011387673100138935, 0.00930091616329412, 0.00867941366624116, 0.008671205698389762, 0.008415168651213596, 0.00997080069663393, 0.008697892846034248, 0.007752784502569788, 0.007782469294214433, 0.008116401961188792, 0.008506669177377755, 0.009005453394249428, 0.00898615917003201, 0.00861281313204257, 0.008671774075893585, 0.008777756233114836, 0.00777504628690234, 0.007706059666364923, 0.008704661962020304, 0.008310676898893922, 0.007898617552073543, 0.008606091349262946, 0.009126204264516284, 0.009192356007722344, 0.008131601566145586, 0.0077247474967076105, 0.007446234417689401, 0.007626960611750566, 0.007849032774047797, 0.007490002062864775, 0.00833206724866416, 0.008790782596843764, 0.007750858729633019, 0.007707477812546977, 0.007755167930307546, 0.007495857316769602, 0.007390394185980161, 0.0082776093956574, 0.008605073845057293, 0.007769019007249627, 0.008075089643297846, 0.007579726580342697, 0.008797653434552657, 0.007904953511013078, 0.00747584838008638, 0.01000098614635798, 0.008196900062652867, 0.008333751153879503, 0.008793556293862503, 0.00829837908390765, 0.00747135658243079, 0.007644984396463332, 0.007455047263309013, 0.03851094786727498, 0.007453770782841846, 0.008655448216771664, 0.008054734750961265, 0.008847423031377468, 0.00836879544162242, 0.007584380845055562, 0.00794223803023324, 0.007893566558383, 0.008151290829964849, 0.008217909783052727, 0.008581271326501471, 0.008315821472454325, 0.007781596489741589, 0.007499320163615337, 0.007574675449480613, 0.0074823646203347645, 0.008190488139557283, 0.00825028783685073, 0.008246353426731603, 0.008037379255171779, 0.007523378286900562, 0.007501642961531531, 0.007472819177607118, 0.007754427628129605, 0.008399569418627855, 0.00843777676661636, 0.008563175891316676, 0.00783859227978906, 0.007719987286832328, 0.007517193969295815, 0.009089569068448834, 0.008220755558302121, 0.007423850899027993, 0.00741204119453481, 0.0074687389848219566, 0.007348955078267081, 0.007487712108508446, 0.007653972960633132, 0.007914364179090937, 0.008491221952519213, 0.008307309682271623, 0.00820600334013444, 0.0075154621857976495, 0.007826847116311276, 0.007675381953098861, 0.00777730875099872, 0.008086602604233249, 0.008654437302214693, 0.008258110574670425, 0.013792130798318012, 0.008174115100644471, 0.007483534960711534, 0.007494676038767247, 0.007464842619951959, 0.007460257588965139, 0.007505603309706316, 0.007797059240252819, 0.008512925279016295, 0.008316602473067798, 0.008383455572563201, 0.0078123288748915805, 0.00880641391493611, 0.007468472402759416, 0.008710979547686293, 0.008936502959841164, 0.008064069368629134, 0.008027455929550342, 0.0082316641255602, 0.007949956234369893, 0.007993934968908434, 0.008416650032813777, 0.009156576343229972, 0.008879767514372361, 0.010354309522881522, 0.008830182649035123, 0.009155276422461611, 0.008296244765006122, 0.00837273032811936, 0.007908557367045432, 0.008152679742124747, 0.00802458851467236, 0.00857225855361321, 0.008860758867740515, 0.007826296110579278, 0.008976499491836876, 0.008880189507181058, 0.007895541508332826, 0.007881018789703376, 0.007922641640107031, 0.010097047515955637, 0.008820829889373272, 0.008856257227307651, 0.00809807371115312, 0.0081632041164994, 0.008019639937629108, 0.009967790891096229, 0.008276690718048485, 0.009356796710562776, 0.008564087969716638, 0.008520548641172354, 0.008563137516830466, 0.007795203875502921, 0.007851379248677404, 0.008590568148065358, 0.008579853234550683, 0.01118797956223716, 0.008979483929579146, 0.008086738329438958, 0.008006000438399496, 0.008208984592783963, 0.007872143016356858, 0.007861047914047958, 0.008822135234368034, 0.00892788340752304, 0.008886672132575768, 0.008834881546135875, 0.009145103655100684, 0.00898096999299014, 0.007957602118040086, 0.00794597059393709, 0.007936573876577313, 0.008779800515185343, 0.007777221124342759, 0.009823812444665236, 0.008904561967938207, 0.007911793030871195, 0.008075324554738472, 0.008921267110054032]
[126.27572718084306, 126.12770017512742, 119.02256785413482, 99.06309678854134, 25.01217513632375, 117.63766303766471, 128.9213154652317, 113.88537635991456, 114.4756817381448, 121.94701442511015, 122.81392615453507, 124.61682151170528, 111.64026549128764, 118.30325921300002, 118.33253322186366, 115.35648626712813, 124.84190758051358, 128.05291906385628, 127.11661806204457, 130.23403901614452, 125.48984023930782, 126.17979316593741, 116.3319988365204, 116.58033186887327, 107.19504199292568, 124.09580299400636, 128.56885078967736, 123.73056694212937, 126.9828638877939, 87.65490436495031, 110.34901562356843, 105.53600174704667, 126.08881381756417, 126.58795166600581, 126.34845559626011, 122.20351478613509, 121.3943446784696, 113.91909292459833, 115.2926017831426, 115.95548736620243, 97.15241849267616, 108.22636286367799, 125.90376810735503, 117.47909423482703, 124.17031777756269, 125.25459221650908, 124.15363135855016, 126.34591862307903, 87.8142524119172, 107.51629005607855, 115.21515605247968, 115.32421612206701, 118.83303133274516, 100.29284812979893, 114.97037474495262, 128.98591463087018, 128.49392168413823, 123.20730352954723, 117.55482423829929, 111.04382602641256, 111.28224874258909, 116.10608342118357, 115.31665738154679, 113.92433025508439, 128.616597650946, 129.76800638655277, 114.88096888347235, 120.32714208069997, 126.60443342233735, 116.19676801195507, 109.5745800790492, 108.78603909160141, 122.97700420582758, 129.45406958948666, 134.29606750284185, 131.1138277624429, 127.40423295293411, 133.51131169348704, 120.01823438958982, 113.75551482288269, 129.01796238096927, 129.7441295740227, 128.94627285786487, 133.40702173756927, 135.31077975475725, 120.80782653559616, 116.210507661639, 128.71637964418085, 123.83763452458744, 131.93088027652678, 113.66667344186513, 126.50295774754562, 133.76408257071225, 99.99013950880895, 121.99733952549339, 119.99398368578392, 113.71963362513002, 120.50546135440065, 133.84450186081898, 130.8047143252003, 134.13731190164688, 25.966642094773235, 134.1602833161898, 115.53416703046064, 124.15058110767193, 113.02726188783905, 119.49150949806638, 131.84991898869686, 125.90909466492444, 126.68544600260522, 122.67995595543147, 121.68544391448982, 116.53284949885084, 120.25270183017298, 128.50833390272692, 133.34542040913632, 132.01885766189082, 133.64759013244392, 122.09284513463106, 121.20789235175539, 121.26572173809248, 124.41866536987591, 132.91901083070147, 133.3041315253213, 133.81830554612878, 128.95858314190025, 119.05372170414884, 118.51463100522518, 116.77910306782682, 127.57392709126982, 129.53389207073693, 133.0283619239476, 110.01621666214517, 121.6433201191711, 134.70098114860178, 134.91560202570642, 133.89141085693444, 136.07376686207814, 133.5521432326009, 130.65110174066785, 126.352537913521, 117.7686798898615, 120.37591449540751, 121.86200255478045, 133.0590155705594, 127.76536773229975, 130.28667577855973, 128.57918233882452, 123.66132589185364, 115.54766243948609, 121.09307461530439, 72.50511285188455, 122.33740138075093, 133.6266891582638, 133.42804876786693, 133.96129709783963, 134.04362893302138, 133.23379330570145, 128.25348239467462, 117.46843384904632, 120.24140906558488, 119.28255494938524, 128.00280377519027, 113.55359964445356, 133.8961900201338, 114.79765214987884, 111.9005951761889, 124.00686976853186, 124.5724683855119, 121.4821188943913, 125.78685599258009, 125.09483801024082, 118.81211599642681, 109.2111246076556, 112.61556097965949, 96.57814437458578, 113.24794058583491, 109.22663105469911, 120.53646298118376, 119.43535272377689, 126.44531152634114, 122.65905587251494, 124.61698168966232, 116.65537078072612, 112.8571508294525, 127.77436297717377, 111.4020004022045, 112.61020940953338, 126.65375755983503, 126.88714830962074, 126.220526615475, 99.03885253781088, 113.36801781028916, 112.91451618145979, 123.48615679093463, 122.5009182336637, 124.69387750288895, 100.32313186799036, 120.82123569258904, 106.87418257907773, 116.76666605201711, 117.36333446509245, 116.77962639681361, 128.28400847123237, 127.36615673844236, 116.4067361743944, 116.5521102357608, 89.38164343589835, 111.3649746291008, 123.65925040007792, 124.90631342007683, 121.81774599492383, 127.03021247482226, 127.20950322831212, 113.35124359739359, 112.00863120113716, 112.52806282053683, 113.18770883095443, 109.34813182158409, 111.34654728615325, 125.66599651080502, 125.84994975478727, 125.99895314415633, 113.89780420072447, 128.58063105213668, 101.7934743392871, 112.30198673450789, 126.39359954160562, 123.8340320839756, 112.09170039007424]
Elapsed: 1.1077947999168511~0.38369672382756165
Time per graph: 0.008606303888510995~0.0029742701203970875
Speed: 120.00691414785511~13.122222977185462
Total Time: 1.1428
best val loss: 0.3349594632091448 test_score: 0.8906

Testing...
Test loss: 0.3468 score: 0.8906 time: 1.11s
test Score 0.8906
Epoch Time List: [3.297477725893259, 3.2667788877151906, 3.3313323103357106, 3.468730659224093, 7.783946766983718, 3.853682345012203, 3.0958414503838867, 3.336175636155531, 3.2477362561039627, 3.3605838268995285, 3.262752279639244, 3.339502823073417, 3.8370817969553173, 3.3257934919092804, 3.1449284348636866, 3.188620006199926, 3.277982188621536, 3.18061429890804, 3.2129064020700753, 3.151242938125506, 3.2726090629585087, 3.309630449861288, 3.354633192066103, 3.1871223428752273, 3.463088346645236, 3.2115689248312265, 3.175978526007384, 3.264261406380683, 3.2352820539381355, 3.648784051183611, 3.7519643909763545, 4.240220817038789, 3.223491886863485, 3.2534209720324725, 3.281091834185645, 3.2854002059902996, 3.3712089511100203, 3.4539166199974716, 3.396587021648884, 3.2102619940415025, 3.7006256598979235, 3.3222255748696625, 3.274874876020476, 3.347416747128591, 3.2139628489967436, 3.269479828188196, 3.259850548580289, 3.2477970600593835, 3.8894165290985256, 3.5767957400530577, 3.2647601971402764, 3.234052960993722, 3.1416116890031844, 7.7241777391172945, 3.326898406026885, 3.225590077228844, 3.2759961700066924, 3.330360827734694, 3.312929063104093, 3.4151596089359373, 3.2471966622397304, 3.2470973029267043, 3.230730164097622, 3.2254777271300554, 3.2462758501060307, 3.1936016301624477, 3.3371768600773066, 3.2943593009840697, 3.3171447583008558, 3.4013378261588514, 3.4844555652234703, 3.3819442000240088, 3.3025683141313493, 3.2353857420384884, 3.1874893540516496, 3.2719837550539523, 3.26599901705049, 3.6076323189772666, 3.3207508127670735, 3.3073020139709115, 3.2686967691406608, 3.248873815871775, 3.7551807740237564, 3.203547844896093, 3.1266515110619366, 3.258098397171125, 3.258534318068996, 3.263388718245551, 3.3437659956980497, 3.2571134257595986, 3.509843942243606, 3.305095309158787, 3.201325838919729, 3.8999184540007263, 3.1797086007427424, 3.3287158373277634, 3.4360167228151113, 3.293680358910933, 3.1893023459706455, 3.2328857970423996, 3.1479341650847346, 7.130141230998561, 3.826606242917478, 3.34152502566576, 3.153681157156825, 3.2479501098860055, 3.2473324770107865, 3.1768897329457104, 3.2309823299292475, 3.2269369061104953, 3.2751507179345936, 3.1895076306536794, 3.2863937409128994, 3.170838583027944, 3.2510624460410327, 3.2170262199360877, 3.1840309428516775, 3.178367129759863, 3.2521106319036335, 3.176766622113064, 3.210761565947905, 3.275479372823611, 3.2216022880747914, 3.2118823409546167, 3.1819796299096197, 3.425398090155795, 3.348635665839538, 3.2765422298107296, 3.249319963855669, 3.2632846268825233, 3.400207360740751, 3.2726620282046497, 3.43968526693061, 3.3046923191286623, 3.2163144019432366, 3.1748499991372228, 3.184461230179295, 3.130717371823266, 3.165763844270259, 3.197343456791714, 3.2854600970167667, 3.389892891049385, 3.2698798270430416, 3.6402748620603234, 3.6487189061008394, 3.2728969072923064, 3.3051071148365736, 3.257480834145099, 3.321214391849935, 3.5415532088372856, 3.2225007896777242, 8.02378314267844, 3.132884410209954, 3.1214790558442473, 3.1334300730377436, 3.1441109869629145, 3.209872535895556, 3.2112070620059967, 3.242348760832101, 3.4570489088073373, 3.2059986600652337, 3.241862124763429, 3.252892771968618, 3.51291327807121, 3.2048175332602113, 3.3336508360225707, 3.3946574579458684, 3.296862327726558, 3.2725190320052207, 3.304323879769072, 3.249173157149926, 3.2329176997300237, 3.3675702661275864, 3.43404421210289, 3.417907618917525, 3.5016914850566536, 3.7686458751559258, 3.3775911112315953, 3.363066277699545, 3.3576036158483475, 3.252923743100837, 3.7350166849792004, 3.3250699378550053, 3.300839740084484, 3.4187510297633708, 3.0695489251520485, 3.374230026267469, 3.2199586569331586, 3.1574922441504896, 3.242177584208548, 3.2189268339425325, 3.530161399161443, 3.2670424829702824, 3.2671856861561537, 3.326578058069572, 3.2843612851575017, 3.3302390249446034, 3.509262589039281, 3.422076378716156, 7.927290346007794, 3.229071795940399, 3.1944997208192945, 3.1510428129695356, 3.163670236011967, 3.190975197823718, 3.268479586346075, 3.214287659851834, 3.9919290081597865, 3.244119880022481, 3.2572855092585087, 3.2608017378952354, 3.277119510108605, 3.240051174070686, 3.225260485196486, 3.3677881942130625, 3.346044362988323, 3.2485995907336473, 3.356932203983888, 3.2765621510334313, 3.372724161017686, 3.235308607108891, 3.244759221095592, 3.2057044887915254, 3.685042340075597, 3.192679274827242, 3.846204011235386, 3.259725213982165, 3.2702079280279577, 3.2339340571779758, 3.371453659143299]
Total Epoch List: [67, 98, 65]
Total Time List: [1.123403794132173, 0.9641003990545869, 1.142791137099266]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c784099c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4961 time: 1.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5039 time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4961 time: 1.03s
Epoch 3/1000, LR 0.000045
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 1.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4961 time: 1.02s
Epoch 4/1000, LR 0.000075
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4961 time: 1.08s
Epoch 5/1000, LR 0.000105
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4961 time: 1.24s
Epoch 6/1000, LR 0.000135
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 1.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 1.05s
Epoch 7/1000, LR 0.000165
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 1.12s
Epoch 8/1000, LR 0.000195
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 1.12s
Epoch 9/1000, LR 0.000225
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 1.32s
Epoch 10/1000, LR 0.000255
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5039 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4961 time: 1.40s
Epoch 11/1000, LR 0.000285
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5039 time: 1.33s
Test loss: 0.6882 score: 0.5039 time: 1.02s
Epoch 12/1000, LR 0.000285
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 1.08s
Val loss: 0.6850 score: 0.5891 time: 1.24s
Test loss: 0.6863 score: 0.5581 time: 1.09s
Epoch 13/1000, LR 0.000285
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 1.25s
Val loss: 0.6827 score: 0.6822 time: 1.10s
Test loss: 0.6841 score: 0.6357 time: 1.06s
Epoch 14/1000, LR 0.000285
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 1.13s
Val loss: 0.6801 score: 0.7519 time: 1.14s
Test loss: 0.6816 score: 0.6899 time: 1.03s
Epoch 15/1000, LR 0.000285
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 1.15s
Val loss: 0.6771 score: 0.7597 time: 1.12s
Test loss: 0.6788 score: 0.7442 time: 1.12s
Epoch 16/1000, LR 0.000285
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 1.12s
Val loss: 0.6735 score: 0.7054 time: 1.07s
Test loss: 0.6754 score: 0.7442 time: 1.04s
Epoch 17/1000, LR 0.000285
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 1.13s
Val loss: 0.6692 score: 0.6744 time: 1.10s
Test loss: 0.6715 score: 0.6822 time: 5.38s
Epoch 18/1000, LR 0.000285
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 1.67s
Val loss: 0.6642 score: 0.6512 time: 1.18s
Test loss: 0.6670 score: 0.6202 time: 1.03s
Epoch 19/1000, LR 0.000285
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 1.02s
Val loss: 0.6583 score: 0.6202 time: 1.21s
Test loss: 0.6616 score: 0.5814 time: 1.01s
Epoch 20/1000, LR 0.000285
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 1.05s
Val loss: 0.6515 score: 0.5969 time: 1.09s
Test loss: 0.6555 score: 0.5659 time: 1.11s
Epoch 21/1000, LR 0.000285
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 1.02s
Val loss: 0.6437 score: 0.6047 time: 1.15s
Test loss: 0.6485 score: 0.5659 time: 1.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 1.02s
Val loss: 0.6347 score: 0.6124 time: 1.20s
Test loss: 0.6405 score: 0.5581 time: 0.98s
Epoch 23/1000, LR 0.000285
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 1.14s
Val loss: 0.6246 score: 0.6124 time: 1.22s
Test loss: 0.6316 score: 0.5659 time: 1.01s
Epoch 24/1000, LR 0.000285
Train loss: 0.6331;  Loss pred: 0.6331; Loss self: 0.0000; time: 1.11s
Val loss: 0.6137 score: 0.6047 time: 1.08s
Test loss: 0.6220 score: 0.5891 time: 1.12s
Epoch 25/1000, LR 0.000285
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 1.02s
Val loss: 0.6018 score: 0.6047 time: 1.10s
Test loss: 0.6115 score: 0.5891 time: 1.14s
Epoch 26/1000, LR 0.000285
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 1.09s
Val loss: 0.5890 score: 0.6124 time: 1.16s
Test loss: 0.6001 score: 0.5891 time: 1.01s
Epoch 27/1000, LR 0.000285
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 1.12s
Val loss: 0.5754 score: 0.6124 time: 1.09s
Test loss: 0.5880 score: 0.5891 time: 1.01s
Epoch 28/1000, LR 0.000285
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 1.09s
Val loss: 0.5607 score: 0.6124 time: 1.17s
Test loss: 0.5749 score: 0.5969 time: 1.04s
Epoch 29/1000, LR 0.000285
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 1.11s
Val loss: 0.5444 score: 0.6512 time: 1.09s
Test loss: 0.5605 score: 0.6202 time: 1.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 1.05s
Val loss: 0.5267 score: 0.7442 time: 1.22s
Test loss: 0.5447 score: 0.7132 time: 1.03s
Epoch 31/1000, LR 0.000285
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 1.08s
Val loss: 0.5081 score: 0.7829 time: 1.20s
Test loss: 0.5282 score: 0.7752 time: 1.03s
Epoch 32/1000, LR 0.000285
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 1.01s
Val loss: 0.4885 score: 0.8605 time: 1.19s
Test loss: 0.5110 score: 0.8295 time: 1.13s
Epoch 33/1000, LR 0.000285
Train loss: 0.4948;  Loss pred: 0.4948; Loss self: 0.0000; time: 1.00s
Val loss: 0.4694 score: 0.8760 time: 1.22s
Test loss: 0.4940 score: 0.8527 time: 1.03s
Epoch 34/1000, LR 0.000285
Train loss: 0.4729;  Loss pred: 0.4729; Loss self: 0.0000; time: 1.14s
Val loss: 0.4514 score: 0.8837 time: 1.11s
Test loss: 0.4777 score: 0.8605 time: 1.01s
Epoch 35/1000, LR 0.000285
Train loss: 0.4566;  Loss pred: 0.4566; Loss self: 0.0000; time: 1.15s
Val loss: 0.4347 score: 0.8992 time: 1.06s
Test loss: 0.4619 score: 0.8605 time: 1.21s
Epoch 36/1000, LR 0.000285
Train loss: 0.4381;  Loss pred: 0.4381; Loss self: 0.0000; time: 1.02s
Val loss: 0.4194 score: 0.9070 time: 1.09s
Test loss: 0.4466 score: 0.8605 time: 1.13s
Epoch 37/1000, LR 0.000285
Train loss: 0.4173;  Loss pred: 0.4173; Loss self: 0.0000; time: 1.01s
Val loss: 0.4057 score: 0.9070 time: 1.11s
Test loss: 0.4322 score: 0.8605 time: 1.04s
Epoch 38/1000, LR 0.000284
Train loss: 0.4038;  Loss pred: 0.4038; Loss self: 0.0000; time: 1.10s
Val loss: 0.3946 score: 0.9070 time: 1.11s
Test loss: 0.4190 score: 0.8605 time: 1.11s
Epoch 39/1000, LR 0.000284
Train loss: 0.3856;  Loss pred: 0.3856; Loss self: 0.0000; time: 1.01s
Val loss: 0.3857 score: 0.8992 time: 1.32s
Test loss: 0.4073 score: 0.8682 time: 1.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.3696;  Loss pred: 0.3696; Loss self: 0.0000; time: 1.07s
Val loss: 0.3778 score: 0.8992 time: 1.16s
Test loss: 0.3969 score: 0.8682 time: 1.03s
Epoch 41/1000, LR 0.000284
Train loss: 0.3530;  Loss pred: 0.3530; Loss self: 0.0000; time: 1.08s
Val loss: 0.3705 score: 0.8915 time: 1.05s
Test loss: 0.3871 score: 0.8682 time: 1.02s
Epoch 42/1000, LR 0.000284
Train loss: 0.3363;  Loss pred: 0.3363; Loss self: 0.0000; time: 1.00s
Val loss: 0.3631 score: 0.8915 time: 1.15s
Test loss: 0.3776 score: 0.8682 time: 1.00s
Epoch 43/1000, LR 0.000284
Train loss: 0.3173;  Loss pred: 0.3173; Loss self: 0.0000; time: 1.01s
Val loss: 0.3567 score: 0.8915 time: 1.16s
Test loss: 0.3685 score: 0.8837 time: 1.07s
Epoch 44/1000, LR 0.000284
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 1.04s
Val loss: 0.3486 score: 0.8992 time: 1.17s
Test loss: 0.3586 score: 0.8760 time: 1.02s
Epoch 45/1000, LR 0.000284
Train loss: 0.2920;  Loss pred: 0.2920; Loss self: 0.0000; time: 1.13s
Val loss: 0.3400 score: 0.9070 time: 1.07s
Test loss: 0.3484 score: 0.8760 time: 1.02s
Epoch 46/1000, LR 0.000284
Train loss: 0.2825;  Loss pred: 0.2825; Loss self: 0.0000; time: 1.11s
Val loss: 0.3306 score: 0.9070 time: 1.10s
Test loss: 0.3382 score: 0.8760 time: 1.00s
Epoch 47/1000, LR 0.000284
Train loss: 0.2603;  Loss pred: 0.2603; Loss self: 0.0000; time: 1.15s
Val loss: 0.3227 score: 0.9070 time: 1.14s
Test loss: 0.3290 score: 0.8760 time: 1.12s
Epoch 48/1000, LR 0.000284
Train loss: 0.2549;  Loss pred: 0.2549; Loss self: 0.0000; time: 1.02s
Val loss: 0.3125 score: 0.9070 time: 1.20s
Test loss: 0.3197 score: 0.8760 time: 1.28s
Epoch 49/1000, LR 0.000284
Train loss: 0.2380;  Loss pred: 0.2380; Loss self: 0.0000; time: 1.26s
Val loss: 0.3013 score: 0.9225 time: 1.21s
Test loss: 0.3112 score: 0.8915 time: 1.10s
Epoch 50/1000, LR 0.000284
Train loss: 0.2243;  Loss pred: 0.2243; Loss self: 0.0000; time: 1.04s
Val loss: 0.2910 score: 0.9147 time: 1.16s
Test loss: 0.3045 score: 0.8915 time: 1.13s
Epoch 51/1000, LR 0.000284
Train loss: 0.2171;  Loss pred: 0.2171; Loss self: 0.0000; time: 1.06s
Val loss: 0.2855 score: 0.9147 time: 1.19s
Test loss: 0.2994 score: 0.8915 time: 1.13s
Epoch 52/1000, LR 0.000284
Train loss: 0.1991;  Loss pred: 0.1991; Loss self: 0.0000; time: 1.15s
Val loss: 0.2815 score: 0.9147 time: 1.11s
Test loss: 0.2958 score: 0.8837 time: 1.01s
Epoch 53/1000, LR 0.000284
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 1.15s
Val loss: 0.2793 score: 0.9147 time: 1.11s
Test loss: 0.2938 score: 0.8760 time: 1.12s
Epoch 54/1000, LR 0.000284
Train loss: 0.1851;  Loss pred: 0.1851; Loss self: 0.0000; time: 1.30s
Val loss: 0.2817 score: 0.9147 time: 1.08s
Test loss: 0.2927 score: 0.8837 time: 1.14s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 1.04s
Val loss: 0.2894 score: 0.9225 time: 1.11s
Test loss: 0.2922 score: 0.8915 time: 1.17s
     INFO: Early stopping counter 2 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 1.06s
Val loss: 0.3028 score: 0.9225 time: 1.05s
Test loss: 0.2936 score: 0.8837 time: 1.11s
     INFO: Early stopping counter 3 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 1.03s
Val loss: 0.3218 score: 0.9147 time: 1.10s
Test loss: 0.2982 score: 0.8837 time: 1.12s
     INFO: Early stopping counter 4 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.1665;  Loss pred: 0.1665; Loss self: 0.0000; time: 1.03s
Val loss: 0.3327 score: 0.9070 time: 1.11s
Test loss: 0.3017 score: 0.8837 time: 1.11s
     INFO: Early stopping counter 5 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.1648;  Loss pred: 0.1648; Loss self: 0.0000; time: 1.03s
Val loss: 0.3534 score: 0.9147 time: 1.20s
Test loss: 0.3089 score: 0.8837 time: 1.00s
     INFO: Early stopping counter 6 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.1631;  Loss pred: 0.1631; Loss self: 0.0000; time: 1.02s
Val loss: 0.3707 score: 0.9147 time: 1.14s
Test loss: 0.3162 score: 0.8837 time: 1.00s
     INFO: Early stopping counter 7 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.1467;  Loss pred: 0.1467; Loss self: 0.0000; time: 1.13s
Val loss: 0.3803 score: 0.9147 time: 1.12s
Test loss: 0.3200 score: 0.8837 time: 1.00s
     INFO: Early stopping counter 8 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1530;  Loss pred: 0.1530; Loss self: 0.0000; time: 1.11s
Val loss: 0.3908 score: 0.9147 time: 1.08s
Test loss: 0.3241 score: 0.8837 time: 1.09s
     INFO: Early stopping counter 9 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 1.03s
Val loss: 0.3986 score: 0.9147 time: 1.08s
Test loss: 0.3267 score: 0.8837 time: 1.09s
     INFO: Early stopping counter 10 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.1388;  Loss pred: 0.1388; Loss self: 0.0000; time: 1.02s
Val loss: 0.3961 score: 0.9147 time: 1.21s
Test loss: 0.3242 score: 0.8837 time: 1.01s
     INFO: Early stopping counter 11 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 1.20s
Val loss: 0.3983 score: 0.9147 time: 1.06s
Test loss: 0.3241 score: 0.8837 time: 1.02s
     INFO: Early stopping counter 12 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1524;  Loss pred: 0.1524; Loss self: 0.0000; time: 1.10s
Val loss: 0.3942 score: 0.9147 time: 1.21s
Test loss: 0.3212 score: 0.8837 time: 5.42s
     INFO: Early stopping counter 13 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1410;  Loss pred: 0.1410; Loss self: 0.0000; time: 1.11s
Val loss: 0.3963 score: 0.9147 time: 1.05s
Test loss: 0.3208 score: 0.8837 time: 1.12s
     INFO: Early stopping counter 14 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.1492;  Loss pred: 0.1492; Loss self: 0.0000; time: 1.01s
Val loss: 0.4048 score: 0.9147 time: 1.06s
Test loss: 0.3225 score: 0.8915 time: 1.11s
     INFO: Early stopping counter 15 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 1.02s
Val loss: 0.4155 score: 0.9225 time: 1.09s
Test loss: 0.3251 score: 0.8992 time: 1.10s
     INFO: Early stopping counter 16 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.1387;  Loss pred: 0.1387; Loss self: 0.0000; time: 1.05s
Val loss: 0.4121 score: 0.9225 time: 1.09s
Test loss: 0.3218 score: 0.8992 time: 1.11s
     INFO: Early stopping counter 17 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 1.06s
Val loss: 0.4166 score: 0.9225 time: 1.17s
Test loss: 0.3219 score: 0.8992 time: 1.03s
     INFO: Early stopping counter 18 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 1.01s
Val loss: 0.4201 score: 0.9302 time: 1.23s
Test loss: 0.3217 score: 0.8992 time: 1.00s
     INFO: Early stopping counter 19 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 1.12s
Val loss: 0.4269 score: 0.9302 time: 1.11s
Test loss: 0.3230 score: 0.8992 time: 1.01s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 052,   Train_Loss: 0.1947,   Val_Loss: 0.2793,   Val_Precision: 0.9492,   Val_Recall: 0.8750,   Val_accuracy: 0.9106,   Val_Score: 0.9147,   Val_Loss: 0.2793,   Test_Precision: 0.9455,   Test_Recall: 0.8000,   Test_accuracy: 0.8667,   Test_Score: 0.8760,   Test_loss: 0.2938


[1.1669868030585349, 1.0310102780349553, 1.0290340760257095, 1.083606047090143, 1.2452423791401088, 1.0526071300264448, 1.1304891188628972, 1.1208250070922077, 1.3232748459558934, 1.404753688024357, 1.0262396030593663, 1.0954201510176063, 1.0659755850210786, 1.0302996330428869, 1.1302306840661913, 1.0445288619957864, 5.389153928030282, 1.0304573317989707, 1.0104534139391035, 1.1147393460851163, 1.0667316948529333, 0.9829727069009095, 1.0158495681826025, 1.1274712029844522, 1.1432078559882939, 1.0169651969335973, 1.0106585079338402, 1.0434961458668113, 1.189897984964773, 1.0364122069440782, 1.0326094860211015, 1.1397092759143561, 1.0307994959875941, 1.018833477050066, 1.2160717358347028, 1.1386317380238324, 1.043232630006969, 1.110483872005716, 1.1881194890011102, 1.0325840439181775, 1.0249302391894162, 1.0013716951943934, 1.0726393358781934, 1.0226306759286672, 1.0216048338916153, 1.0063876959029585, 1.1238974251318723, 1.2862480648327619, 1.108094782801345, 1.1309893031138927, 1.134779817191884, 1.0158198641147465, 1.1223067550454289, 1.1409694110043347, 1.1747372290119529, 1.1173079141881317, 1.1228790450841188, 1.1131897349841893, 1.002313629956916, 1.0061794868670404, 1.0048805710393935, 1.095259380992502, 1.0934340949170291, 1.0130563098937273, 1.0268385231029242, 5.424409226048738, 1.127567389048636, 1.1192371651995927, 1.1005312958732247, 1.1127345080021769, 1.0372081589885056, 1.0043655908666551, 1.0155386391561478]
[0.009046409326035154, 0.00799232773670508, 0.007977008341284569, 0.008400046876667774, 0.009653041698760533, 0.008159745194003448, 0.008763481541572846, 0.008688565946451222, 0.010257944542293748, 0.01088956347305703, 0.007955345760150126, 0.008491629077655863, 0.008263376628070376, 0.007986818860797572, 0.008761478171055747, 0.008097122961207646, 0.04177638703899444, 0.007988041331774966, 0.007832972201078321, 0.008641390279729584, 0.008269237944596382, 0.007619943464348136, 0.007874802854128702, 0.008740086844840716, 0.00886207640301003, 0.007883451139020134, 0.007834562077006512, 0.008089117409820242, 0.009224015387323823, 0.008034203154605258, 0.008004724697837995, 0.00883495562724307, 0.00799069376734569, 0.007897933930620667, 0.009426912680889169, 0.008826602620339786, 0.008087074651216814, 0.008608402108571441, 0.009210228596907831, 0.008004527472233933, 0.007945195652631134, 0.007762571280576693, 0.008315033611458864, 0.007927369580842381, 0.007919417316989266, 0.007801455006999678, 0.00871238314055715, 0.00997091523126172, 0.008589882037219729, 0.008767358938867384, 0.008796742768929333, 0.007874572590036794, 0.008700052364693248, 0.008844724116312671, 0.00910649014737948, 0.008661301660373114, 0.008704488721582316, 0.008629377790575111, 0.007769873100441209, 0.0077998409834654295, 0.00778977186852243, 0.008490382798391488, 0.008476233293930458, 0.007853149689098662, 0.00795998855118546, 0.04204968392285843, 0.00874083247324524, 0.008676257094570487, 0.008531250355606394, 0.008625848899241681, 0.008040373325492291, 0.007785779774160118, 0.007872392551598046]
[110.54109580494502, 125.11999419236284, 125.36028009705778, 119.04695469945892, 103.59428988361272, 122.55284647060971, 114.1098997306181, 115.09379179062827, 97.48541687635144, 91.83104561300378, 125.70163889157331, 117.76303355398711, 121.01590487876808, 125.20629520075768, 114.13599172153177, 123.50065631840853, 23.936967049510326, 125.18713392508161, 127.66546009985016, 115.72211966235753, 120.93012762481459, 131.23456947925624, 126.98730603467835, 114.41533908673932, 112.84037222476971, 126.84799872106444, 127.63955281366376, 123.62288112989846, 108.41265522759839, 124.46785085672043, 124.9262201697069, 113.1867597519613, 125.14557923450182, 126.61539192205095, 106.07926835126658, 113.29387342029274, 123.65410771242676, 116.16557723346747, 108.57493812213652, 124.92929825886601, 125.86222463493894, 128.82329370709604, 120.26409594086428, 126.14524777760363, 126.2719162247875, 128.18121736301404, 114.77915787987841, 100.29169607868184, 116.41603408137932, 114.05943420051027, 113.67844056234826, 126.99101933039994, 114.94183690873207, 113.06175148591245, 109.81179178981093, 115.45608722706949, 114.88325529339285, 115.88321015359722, 128.70223066361476, 128.20774194241395, 128.3734641884552, 117.78031965642951, 117.97693212574339, 127.33744288462356, 125.62832139388853, 23.78139159938833, 114.40557899500921, 115.25707330938705, 117.21611233022136, 115.93061873457027, 124.37233440759077, 128.4392866233972, 127.02618593339916]
Elapsed: 1.2059507399880365~0.7094609910889607
Time per graph: 0.009348455348744468~0.0054996976053407805
Speed: 116.58598944197954~17.4783791265157
Total Time: 1.0160
best val loss: 0.27926557539969454 test_score: 0.8760

Testing...
Test loss: 0.3217 score: 0.8992 time: 1.12s
test Score 0.8992
Epoch Time List: [3.250126752536744, 3.349241921212524, 3.3010398948099464, 3.3697366507258266, 3.500154238892719, 3.5185752466786653, 3.3600195108447224, 3.2505026271101087, 4.026186449918896, 4.045532207004726, 3.651824069209397, 3.404118151869625, 3.403386513935402, 3.2965790648013353, 3.3933153990656137, 3.228728477144614, 7.607430359115824, 3.882513011805713, 3.2355665918439627, 3.2445764190051705, 3.231253062840551, 3.1972750849090517, 3.3643918898887932, 3.3129324400797486, 3.255676240194589, 3.261900636134669, 3.208783364156261, 3.2987351031042635, 3.38215714530088, 3.293934019980952, 3.303090153960511, 3.330776546150446, 3.2451731278561056, 3.262796824797988, 3.4214769769459963, 3.237292880890891, 3.158879574155435, 3.322059095837176, 3.5146403051912785, 3.2593731619417667, 3.156617248430848, 3.150980557780713, 3.241230022860691, 3.2281348381657153, 3.2201549988240004, 3.213498560944572, 3.4067715818528086, 3.48981635668315, 3.5756113645620644, 3.3180751481559128, 3.3867602481041104, 3.2711902691517025, 3.373740625102073, 3.5230872039683163, 3.321380498819053, 3.2218035119585693, 3.251637303037569, 3.2518973760306835, 3.2219165682327002, 3.1603176060598344, 3.25330016692169, 3.2867898661643267, 3.199349077884108, 3.2371638279873878, 3.285034842090681, 7.725639084586874, 3.28627777681686, 3.1824437268078327, 3.209660747088492, 3.243104499997571, 3.255397008964792, 3.2376810230780393, 3.233533133054152]
Total Epoch List: [73]
Total Time List: [1.0160035521257669]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c784154b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4961 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5039 time: 1.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5039 time: 1.04s
Epoch 3/1000, LR 0.000045
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5039 time: 1.00s
Epoch 4/1000, LR 0.000075
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5039 time: 0.99s
Epoch 5/1000, LR 0.000105
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4961 time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5039 time: 0.97s
Epoch 6/1000, LR 0.000135
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5039 time: 1.07s
Epoch 7/1000, LR 0.000165
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 1.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 1.14s
Epoch 9/1000, LR 0.000225
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 1.09s
Epoch 10/1000, LR 0.000255
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5039 time: 1.14s
Epoch 11/1000, LR 0.000285
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4961 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5039 time: 1.20s
Epoch 12/1000, LR 0.000285
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 1.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4961 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5039 time: 1.00s
Epoch 13/1000, LR 0.000285
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 1.14s
Val loss: 0.6872 score: 0.5116 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5039 time: 1.10s
Epoch 14/1000, LR 0.000285
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 1.25s
Val loss: 0.6852 score: 0.5659 time: 1.05s
Test loss: 0.6872 score: 0.5891 time: 0.98s
Epoch 15/1000, LR 0.000285
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 1.21s
Val loss: 0.6825 score: 0.6589 time: 1.11s
Test loss: 0.6854 score: 0.6512 time: 1.08s
Epoch 16/1000, LR 0.000285
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 1.20s
Val loss: 0.6792 score: 0.7442 time: 1.06s
Test loss: 0.6829 score: 0.7132 time: 1.09s
Epoch 17/1000, LR 0.000285
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 1.12s
Val loss: 0.6752 score: 0.8140 time: 1.03s
Test loss: 0.6799 score: 0.7674 time: 1.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 1.15s
Val loss: 0.6703 score: 0.7984 time: 1.08s
Test loss: 0.6763 score: 0.6822 time: 1.08s
Epoch 19/1000, LR 0.000285
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 1.12s
Val loss: 0.6648 score: 0.7597 time: 1.14s
Test loss: 0.6725 score: 0.6279 time: 1.00s
Epoch 20/1000, LR 0.000285
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 1.26s
Val loss: 0.6585 score: 0.6822 time: 1.43s
Test loss: 0.6681 score: 0.5969 time: 1.00s
Epoch 21/1000, LR 0.000285
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 1.10s
Val loss: 0.6510 score: 0.6434 time: 1.12s
Test loss: 0.6628 score: 0.5736 time: 0.98s
Epoch 22/1000, LR 0.000285
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 1.24s
Val loss: 0.6422 score: 0.6202 time: 1.02s
Test loss: 0.6565 score: 0.5581 time: 1.00s
Epoch 23/1000, LR 0.000285
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 1.22s
Val loss: 0.6320 score: 0.6202 time: 1.02s
Test loss: 0.6490 score: 0.5504 time: 0.98s
Epoch 24/1000, LR 0.000285
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 1.37s
Val loss: 0.6203 score: 0.6202 time: 1.02s
Test loss: 0.6403 score: 0.5426 time: 1.00s
Epoch 25/1000, LR 0.000285
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 1.30s
Val loss: 0.6072 score: 0.6202 time: 1.04s
Test loss: 0.6303 score: 0.5581 time: 1.50s
Epoch 26/1000, LR 0.000285
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 1.27s
Val loss: 0.5928 score: 0.6202 time: 1.03s
Test loss: 0.6193 score: 0.5426 time: 1.09s
Epoch 27/1000, LR 0.000285
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 1.14s
Val loss: 0.5771 score: 0.6202 time: 1.14s
Test loss: 0.6068 score: 0.5659 time: 1.04s
Epoch 28/1000, LR 0.000285
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 1.12s
Val loss: 0.5605 score: 0.6279 time: 1.14s
Test loss: 0.5933 score: 0.5736 time: 1.02s
Epoch 29/1000, LR 0.000285
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 1.12s
Val loss: 0.5424 score: 0.7287 time: 1.20s
Test loss: 0.5776 score: 0.6357 time: 1.01s
Epoch 30/1000, LR 0.000285
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 1.24s
Val loss: 0.5228 score: 0.8295 time: 1.04s
Test loss: 0.5602 score: 0.7132 time: 1.00s
Epoch 31/1000, LR 0.000285
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 1.25s
Val loss: 0.5020 score: 0.8837 time: 1.08s
Test loss: 0.5410 score: 0.8062 time: 1.14s
Epoch 32/1000, LR 0.000285
Train loss: 0.5130;  Loss pred: 0.5130; Loss self: 0.0000; time: 1.12s
Val loss: 0.4815 score: 0.8915 time: 1.08s
Test loss: 0.5217 score: 0.8372 time: 1.07s
Epoch 33/1000, LR 0.000285
Train loss: 0.4897;  Loss pred: 0.4897; Loss self: 0.0000; time: 1.15s
Val loss: 0.4620 score: 0.9147 time: 1.10s
Test loss: 0.5040 score: 0.8450 time: 1.10s
Epoch 34/1000, LR 0.000285
Train loss: 0.4724;  Loss pred: 0.4724; Loss self: 0.0000; time: 1.16s
Val loss: 0.4435 score: 0.9070 time: 1.05s
Test loss: 0.4876 score: 0.8682 time: 1.09s
Epoch 35/1000, LR 0.000285
Train loss: 0.4500;  Loss pred: 0.4500; Loss self: 0.0000; time: 1.11s
Val loss: 0.4254 score: 0.8992 time: 1.20s
Test loss: 0.4725 score: 0.8760 time: 1.04s
Epoch 36/1000, LR 0.000285
Train loss: 0.4305;  Loss pred: 0.4305; Loss self: 0.0000; time: 1.10s
Val loss: 0.4073 score: 0.8992 time: 1.19s
Test loss: 0.4586 score: 0.8760 time: 1.20s
Epoch 37/1000, LR 0.000285
Train loss: 0.4111;  Loss pred: 0.4111; Loss self: 0.0000; time: 1.11s
Val loss: 0.3898 score: 0.9147 time: 1.13s
Test loss: 0.4461 score: 0.8760 time: 0.99s
Epoch 38/1000, LR 0.000284
Train loss: 0.3903;  Loss pred: 0.3903; Loss self: 0.0000; time: 1.11s
Val loss: 0.3733 score: 0.9070 time: 1.12s
Test loss: 0.4349 score: 0.8760 time: 0.97s
Epoch 39/1000, LR 0.000284
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 1.10s
Val loss: 0.3579 score: 0.9302 time: 5.18s
Test loss: 0.4250 score: 0.8837 time: 1.49s
Epoch 40/1000, LR 0.000284
Train loss: 0.3586;  Loss pred: 0.3586; Loss self: 0.0000; time: 1.23s
Val loss: 0.3432 score: 0.9302 time: 1.10s
Test loss: 0.4160 score: 0.8760 time: 0.99s
Epoch 41/1000, LR 0.000284
Train loss: 0.3469;  Loss pred: 0.3469; Loss self: 0.0000; time: 1.18s
Val loss: 0.3290 score: 0.9302 time: 1.07s
Test loss: 0.4080 score: 0.8760 time: 1.09s
Epoch 42/1000, LR 0.000284
Train loss: 0.3289;  Loss pred: 0.3289; Loss self: 0.0000; time: 1.11s
Val loss: 0.3152 score: 0.9302 time: 1.00s
Test loss: 0.4012 score: 0.8760 time: 1.10s
Epoch 43/1000, LR 0.000284
Train loss: 0.3065;  Loss pred: 0.3065; Loss self: 0.0000; time: 1.09s
Val loss: 0.3015 score: 0.9302 time: 1.08s
Test loss: 0.3943 score: 0.8760 time: 1.07s
Epoch 44/1000, LR 0.000284
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 1.12s
Val loss: 0.2881 score: 0.9302 time: 1.21s
Test loss: 0.3892 score: 0.8760 time: 0.96s
Epoch 45/1000, LR 0.000284
Train loss: 0.2854;  Loss pred: 0.2854; Loss self: 0.0000; time: 1.11s
Val loss: 0.2745 score: 0.9302 time: 1.10s
Test loss: 0.3829 score: 0.8760 time: 1.10s
Epoch 46/1000, LR 0.000284
Train loss: 0.2687;  Loss pred: 0.2687; Loss self: 0.0000; time: 1.10s
Val loss: 0.2610 score: 0.9380 time: 1.15s
Test loss: 0.3763 score: 0.8837 time: 0.98s
Epoch 47/1000, LR 0.000284
Train loss: 0.2548;  Loss pred: 0.2548; Loss self: 0.0000; time: 1.22s
Val loss: 0.2478 score: 0.9380 time: 1.02s
Test loss: 0.3712 score: 0.8837 time: 1.03s
Epoch 48/1000, LR 0.000284
Train loss: 0.2396;  Loss pred: 0.2396; Loss self: 0.0000; time: 1.20s
Val loss: 0.2351 score: 0.9457 time: 1.15s
Test loss: 0.3687 score: 0.8915 time: 1.33s
Epoch 49/1000, LR 0.000284
Train loss: 0.2398;  Loss pred: 0.2398; Loss self: 0.0000; time: 1.43s
Val loss: 0.2234 score: 0.9380 time: 1.20s
Test loss: 0.3684 score: 0.8837 time: 1.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.2214;  Loss pred: 0.2214; Loss self: 0.0000; time: 1.11s
Val loss: 0.2116 score: 0.9380 time: 1.05s
Test loss: 0.3655 score: 0.8915 time: 1.07s
Epoch 51/1000, LR 0.000284
Train loss: 0.2087;  Loss pred: 0.2087; Loss self: 0.0000; time: 1.15s
Val loss: 0.2010 score: 0.9380 time: 1.01s
Test loss: 0.3645 score: 0.8992 time: 1.10s
Epoch 52/1000, LR 0.000284
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 1.15s
Val loss: 0.1917 score: 0.9380 time: 1.12s
Test loss: 0.3662 score: 0.9070 time: 1.07s
Epoch 53/1000, LR 0.000284
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 1.24s
Val loss: 0.1835 score: 0.9457 time: 1.02s
Test loss: 0.3696 score: 0.9070 time: 1.01s
Epoch 54/1000, LR 0.000284
Train loss: 0.1656;  Loss pred: 0.1656; Loss self: 0.0000; time: 1.24s
Val loss: 0.1764 score: 0.9457 time: 1.02s
Test loss: 0.3735 score: 0.9225 time: 1.08s
Epoch 55/1000, LR 0.000284
Train loss: 0.1660;  Loss pred: 0.1660; Loss self: 0.0000; time: 1.14s
Val loss: 0.1706 score: 0.9457 time: 1.01s
Test loss: 0.3792 score: 0.9147 time: 1.07s
Epoch 56/1000, LR 0.000284
Train loss: 0.1604;  Loss pred: 0.1604; Loss self: 0.0000; time: 1.13s
Val loss: 0.1663 score: 0.9457 time: 1.08s
Test loss: 0.3870 score: 0.9225 time: 1.33s
Epoch 57/1000, LR 0.000283
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 1.11s
Val loss: 0.1607 score: 0.9457 time: 1.01s
Test loss: 0.3882 score: 0.9147 time: 1.07s
Epoch 58/1000, LR 0.000283
Train loss: 0.1390;  Loss pred: 0.1390; Loss self: 0.0000; time: 1.13s
Val loss: 0.1581 score: 0.9457 time: 1.15s
Test loss: 0.3930 score: 0.8992 time: 0.96s
Epoch 59/1000, LR 0.000283
Train loss: 0.1360;  Loss pred: 0.1360; Loss self: 0.0000; time: 1.11s
Val loss: 0.1554 score: 0.9535 time: 1.15s
Test loss: 0.4016 score: 0.9147 time: 1.02s
Epoch 60/1000, LR 0.000283
Train loss: 0.1352;  Loss pred: 0.1352; Loss self: 0.0000; time: 1.15s
Val loss: 0.1539 score: 0.9535 time: 1.11s
Test loss: 0.4135 score: 0.9225 time: 0.99s
Epoch 61/1000, LR 0.000283
Train loss: 0.1267;  Loss pred: 0.1267; Loss self: 0.0000; time: 1.23s
Val loss: 0.1571 score: 0.9457 time: 1.02s
Test loss: 0.4307 score: 0.9225 time: 1.10s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.1262;  Loss pred: 0.1262; Loss self: 0.0000; time: 1.33s
Val loss: 0.1534 score: 0.9380 time: 1.32s
Test loss: 0.4342 score: 0.9225 time: 1.27s
Epoch 63/1000, LR 0.000283
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 1.25s
Val loss: 0.1511 score: 0.9535 time: 1.12s
Test loss: 0.4368 score: 0.9225 time: 1.25s
Epoch 64/1000, LR 0.000283
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 1.37s
Val loss: 0.1545 score: 0.9457 time: 1.16s
Test loss: 0.4415 score: 0.9147 time: 1.09s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.1193;  Loss pred: 0.1193; Loss self: 0.0000; time: 1.14s
Val loss: 0.1576 score: 0.9457 time: 1.09s
Test loss: 0.4493 score: 0.9147 time: 1.13s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.1062;  Loss pred: 0.1062; Loss self: 0.0000; time: 1.16s
Val loss: 0.1526 score: 0.9457 time: 1.13s
Test loss: 0.4566 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 1.29s
Val loss: 0.1499 score: 0.9612 time: 1.44s
Test loss: 0.4658 score: 0.9147 time: 1.24s
Epoch 68/1000, LR 0.000283
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 1.52s
Val loss: 0.1504 score: 0.9457 time: 1.04s
Test loss: 0.4751 score: 0.9302 time: 1.20s
     INFO: Early stopping counter 1 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 1.23s
Val loss: 0.1514 score: 0.9380 time: 1.10s
Test loss: 0.4839 score: 0.9302 time: 1.25s
     INFO: Early stopping counter 2 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 1.24s
Val loss: 0.1500 score: 0.9457 time: 1.00s
Test loss: 0.4882 score: 0.9302 time: 1.07s
     INFO: Early stopping counter 3 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 1.10s
Val loss: 0.1497 score: 0.9457 time: 1.05s
Test loss: 0.4899 score: 0.9147 time: 1.07s
Epoch 72/1000, LR 0.000282
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 1.10s
Val loss: 0.1562 score: 0.9457 time: 1.01s
Test loss: 0.4960 score: 0.9070 time: 1.08s
     INFO: Early stopping counter 1 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 1.09s
Val loss: 0.1623 score: 0.9380 time: 1.12s
Test loss: 0.5039 score: 0.8992 time: 1.08s
     INFO: Early stopping counter 2 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 1.09s
Val loss: 0.1592 score: 0.9380 time: 1.13s
Test loss: 0.5089 score: 0.9070 time: 0.96s
     INFO: Early stopping counter 3 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 1.21s
Val loss: 0.1543 score: 0.9457 time: 1.01s
Test loss: 0.5138 score: 0.9070 time: 0.98s
     INFO: Early stopping counter 4 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 1.26s
Val loss: 0.1490 score: 0.9457 time: 1.04s
Test loss: 0.5178 score: 0.9147 time: 1.01s
Epoch 77/1000, LR 0.000282
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 1.26s
Val loss: 0.1472 score: 0.9535 time: 1.05s
Test loss: 0.5241 score: 0.9225 time: 1.08s
Epoch 78/1000, LR 0.000282
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 1.17s
Val loss: 0.1475 score: 0.9535 time: 1.02s
Test loss: 0.5306 score: 0.9225 time: 1.10s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 1.13s
Val loss: 0.1470 score: 0.9457 time: 1.03s
Test loss: 0.5354 score: 0.9225 time: 1.10s
Epoch 80/1000, LR 0.000282
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 1.15s
Val loss: 0.1467 score: 0.9457 time: 1.18s
Test loss: 0.5401 score: 0.9225 time: 1.04s
Epoch 81/1000, LR 0.000281
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 1.14s
Val loss: 0.1471 score: 0.9535 time: 1.16s
Test loss: 0.5445 score: 0.9147 time: 1.00s
     INFO: Early stopping counter 1 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 1.31s
Val loss: 0.1475 score: 0.9535 time: 1.32s
Test loss: 0.5492 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 2 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 1.12s
Val loss: 0.1509 score: 0.9535 time: 1.16s
Test loss: 0.5538 score: 0.9070 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.0807;  Loss pred: 0.0807; Loss self: 0.0000; time: 1.24s
Val loss: 0.1548 score: 0.9535 time: 1.01s
Test loss: 0.5585 score: 0.9070 time: 1.08s
     INFO: Early stopping counter 4 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 1.24s
Val loss: 0.1505 score: 0.9535 time: 1.03s
Test loss: 0.5602 score: 0.9070 time: 4.90s
     INFO: Early stopping counter 5 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 2.51s
Val loss: 0.1460 score: 0.9535 time: 1.03s
Test loss: 0.5627 score: 0.9147 time: 1.02s
Epoch 87/1000, LR 0.000281
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 1.21s
Val loss: 0.1463 score: 0.9457 time: 1.04s
Test loss: 0.5679 score: 0.9225 time: 0.98s
     INFO: Early stopping counter 1 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 1.22s
Val loss: 0.1479 score: 0.9380 time: 1.00s
Test loss: 0.5725 score: 0.9225 time: 1.06s
     INFO: Early stopping counter 2 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 1.09s
Val loss: 0.1464 score: 0.9457 time: 1.09s
Test loss: 0.5750 score: 0.9225 time: 0.99s
     INFO: Early stopping counter 3 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 1.09s
Val loss: 0.1448 score: 0.9457 time: 1.00s
Test loss: 0.5769 score: 0.9147 time: 1.06s
Epoch 91/1000, LR 0.000280
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 1.09s
Val loss: 0.1442 score: 0.9380 time: 1.10s
Test loss: 0.5799 score: 0.9147 time: 0.98s
Epoch 92/1000, LR 0.000280
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 1.10s
Val loss: 0.1446 score: 0.9535 time: 1.16s
Test loss: 0.5835 score: 0.9147 time: 1.01s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 1.20s
Val loss: 0.1486 score: 0.9535 time: 1.05s
Test loss: 0.5892 score: 0.9070 time: 0.99s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 1.22s
Val loss: 0.1492 score: 0.9535 time: 1.07s
Test loss: 0.5927 score: 0.9070 time: 1.02s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 1.24s
Val loss: 0.1483 score: 0.9535 time: 1.04s
Test loss: 0.5972 score: 0.9070 time: 1.13s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 1.17s
Val loss: 0.1467 score: 0.9535 time: 1.04s
Test loss: 0.6020 score: 0.9070 time: 1.09s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 1.15s
Val loss: 0.1468 score: 0.9380 time: 1.01s
Test loss: 0.6081 score: 0.9147 time: 1.09s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 1.14s
Val loss: 0.1511 score: 0.9380 time: 1.15s
Test loss: 0.6177 score: 0.9147 time: 1.00s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 1.20s
Val loss: 0.1527 score: 0.9380 time: 1.05s
Test loss: 0.6253 score: 0.9147 time: 1.02s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 1.25s
Val loss: 0.1487 score: 0.9535 time: 1.07s
Test loss: 0.6264 score: 0.9070 time: 1.01s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 1.26s
Val loss: 0.1559 score: 0.9535 time: 1.04s
Test loss: 0.6359 score: 0.9070 time: 1.13s
     INFO: Early stopping counter 10 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 1.10s
Val loss: 0.1604 score: 0.9457 time: 1.05s
Test loss: 0.6436 score: 0.9070 time: 1.11s
     INFO: Early stopping counter 11 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 1.14s
Val loss: 0.1575 score: 0.9535 time: 1.05s
Test loss: 0.6457 score: 0.9070 time: 1.09s
     INFO: Early stopping counter 12 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 1.14s
Val loss: 0.1536 score: 0.9535 time: 1.17s
Test loss: 0.6477 score: 0.9070 time: 1.01s
     INFO: Early stopping counter 13 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 1.14s
Val loss: 0.1512 score: 0.9380 time: 1.10s
Test loss: 0.6514 score: 0.9070 time: 0.96s
     INFO: Early stopping counter 14 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 1.07s
Val loss: 0.1533 score: 0.9380 time: 1.13s
Test loss: 0.6583 score: 0.9225 time: 0.99s
     INFO: Early stopping counter 15 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 1.11s
Val loss: 0.1551 score: 0.9457 time: 1.15s
Test loss: 0.6653 score: 0.9225 time: 0.98s
     INFO: Early stopping counter 16 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 1.22s
Val loss: 0.1549 score: 0.9380 time: 1.02s
Test loss: 0.6700 score: 0.9147 time: 0.98s
     INFO: Early stopping counter 17 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 1.20s
Val loss: 0.1563 score: 0.9380 time: 1.08s
Test loss: 0.6763 score: 0.9070 time: 1.08s
     INFO: Early stopping counter 18 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 1.24s
Val loss: 0.1590 score: 0.9457 time: 1.10s
Test loss: 0.6825 score: 0.9070 time: 1.12s
     INFO: Early stopping counter 19 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 1.12s
Val loss: 0.1615 score: 0.9535 time: 1.10s
Test loss: 0.6889 score: 0.9070 time: 1.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.0538,   Val_Loss: 0.1442,   Val_Precision: 0.9524,   Val_Recall: 0.9231,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1442,   Test_Precision: 0.9492,   Test_Recall: 0.8750,   Test_accuracy: 0.9106,   Test_Score: 0.9147,   Test_loss: 0.5799


[1.1669868030585349, 1.0310102780349553, 1.0290340760257095, 1.083606047090143, 1.2452423791401088, 1.0526071300264448, 1.1304891188628972, 1.1208250070922077, 1.3232748459558934, 1.404753688024357, 1.0262396030593663, 1.0954201510176063, 1.0659755850210786, 1.0302996330428869, 1.1302306840661913, 1.0445288619957864, 5.389153928030282, 1.0304573317989707, 1.0104534139391035, 1.1147393460851163, 1.0667316948529333, 0.9829727069009095, 1.0158495681826025, 1.1274712029844522, 1.1432078559882939, 1.0169651969335973, 1.0106585079338402, 1.0434961458668113, 1.189897984964773, 1.0364122069440782, 1.0326094860211015, 1.1397092759143561, 1.0307994959875941, 1.018833477050066, 1.2160717358347028, 1.1386317380238324, 1.043232630006969, 1.110483872005716, 1.1881194890011102, 1.0325840439181775, 1.0249302391894162, 1.0013716951943934, 1.0726393358781934, 1.0226306759286672, 1.0216048338916153, 1.0063876959029585, 1.1238974251318723, 1.2862480648327619, 1.108094782801345, 1.1309893031138927, 1.134779817191884, 1.0158198641147465, 1.1223067550454289, 1.1409694110043347, 1.1747372290119529, 1.1173079141881317, 1.1228790450841188, 1.1131897349841893, 1.002313629956916, 1.0061794868670404, 1.0048805710393935, 1.095259380992502, 1.0934340949170291, 1.0130563098937273, 1.0268385231029242, 5.424409226048738, 1.127567389048636, 1.1192371651995927, 1.1005312958732247, 1.1127345080021769, 1.0372081589885056, 1.0043655908666551, 1.0155386391561478, 1.0722368131391704, 1.0418615178205073, 1.0017236531712115, 0.9956707309465855, 0.9705408101435751, 1.0728246958460659, 1.1347727859392762, 1.1458313760813326, 1.100350040011108, 1.1456143849063665, 1.2055944099556655, 1.0082802060060203, 1.105565733043477, 0.9849157119169831, 1.0868982321117073, 1.0977455109823495, 1.1901369020342827, 1.0890985520090908, 1.0085097309201956, 1.008246464189142, 0.9812218560837209, 1.0068869919050485, 0.9822863219305873, 1.0123254130594432, 1.5037286221049726, 1.097269464051351, 1.041369249112904, 1.0269675781019032, 1.0116995030548424, 1.0090540279634297, 1.141607356024906, 1.0784445640165359, 1.1091676119249314, 1.0940517401322722, 1.043086526915431, 1.2000603820197284, 0.9960986340884119, 0.9795635528862476, 1.4968032049946487, 0.9924753289669752, 1.0944404599722475, 1.1003875099122524, 1.0788614009507, 0.965909545077011, 1.1031237030401826, 0.9869027119129896, 1.031772525049746, 1.3401158389169723, 1.1217843838967383, 1.072956850985065, 1.1017197868786752, 1.071367456810549, 1.015701835975051, 1.090085859876126, 1.0764850019477308, 1.336834441171959, 1.0804024350363761, 0.9685358020942658, 1.028177171945572, 0.9913730069529265, 1.1036734660156071, 1.2716556848026812, 1.2605985840782523, 1.0973238819278777, 1.1365597620606422, 1.0243435211014003, 1.2463245829567313, 1.2080683878157288, 1.2556262800935656, 1.073350528953597, 1.0720688169822097, 1.086418780963868, 1.0871573009062558, 0.9618465809617192, 0.9893437738064677, 1.018181900959462, 1.0827192838769406, 1.1045661000534892, 1.1074058171361685, 1.0412501150276512, 1.0050032390281558, 1.0270248330198228, 1.0027403940912336, 1.0850116789806634, 4.903081550030038, 1.022709206910804, 0.9861400129739195, 1.06464839191176, 0.9993928379844874, 1.0610680908430368, 0.9858189830556512, 1.0160390608943999, 0.994177893968299, 1.0241424550767988, 1.1354468611534685, 1.0959528870880604, 1.0943622409831733, 1.0071147710550576, 1.0266880479175597, 1.0185619860421866, 1.135587700176984, 1.1106306840665638, 1.090751409996301, 1.0180432708002627, 0.9660581389907748, 0.9986004079692066, 0.9831604221835732, 0.9854209108743817, 1.0825773859396577, 1.1284264249261469, 1.0990552478469908]
[0.009046409326035154, 0.00799232773670508, 0.007977008341284569, 0.008400046876667774, 0.009653041698760533, 0.008159745194003448, 0.008763481541572846, 0.008688565946451222, 0.010257944542293748, 0.01088956347305703, 0.007955345760150126, 0.008491629077655863, 0.008263376628070376, 0.007986818860797572, 0.008761478171055747, 0.008097122961207646, 0.04177638703899444, 0.007988041331774966, 0.007832972201078321, 0.008641390279729584, 0.008269237944596382, 0.007619943464348136, 0.007874802854128702, 0.008740086844840716, 0.00886207640301003, 0.007883451139020134, 0.007834562077006512, 0.008089117409820242, 0.009224015387323823, 0.008034203154605258, 0.008004724697837995, 0.00883495562724307, 0.00799069376734569, 0.007897933930620667, 0.009426912680889169, 0.008826602620339786, 0.008087074651216814, 0.008608402108571441, 0.009210228596907831, 0.008004527472233933, 0.007945195652631134, 0.007762571280576693, 0.008315033611458864, 0.007927369580842381, 0.007919417316989266, 0.007801455006999678, 0.00871238314055715, 0.00997091523126172, 0.008589882037219729, 0.008767358938867384, 0.008796742768929333, 0.007874572590036794, 0.008700052364693248, 0.008844724116312671, 0.00910649014737948, 0.008661301660373114, 0.008704488721582316, 0.008629377790575111, 0.007769873100441209, 0.0077998409834654295, 0.00778977186852243, 0.008490382798391488, 0.008476233293930458, 0.007853149689098662, 0.00795998855118546, 0.04204968392285843, 0.00874083247324524, 0.008676257094570487, 0.008531250355606394, 0.008625848899241681, 0.008040373325492291, 0.007785779774160118, 0.007872392551598046, 0.008311913280148608, 0.008076445874577575, 0.0077652996369861355, 0.007718377759275857, 0.00752357217165562, 0.00831647051043462, 0.008796688263095164, 0.008882413768072346, 0.008529845271403937, 0.008880731665940826, 0.009345693100431516, 0.007816125627953646, 0.00857027700033703, 0.007635005518736303, 0.008425567690788428, 0.008509655123894183, 0.0092258674576301, 0.008442624434178998, 0.00781790489085423, 0.00781586406348172, 0.007606370977393186, 0.007805325518643787, 0.0076146226506247075, 0.007847483822166226, 0.011656811024069555, 0.008505964837607372, 0.008072629838084528, 0.007960988977534134, 0.007842631806626685, 0.007822124247778525, 0.00884966942654966, 0.008360035379973147, 0.008598198542053731, 0.008481021241335443, 0.008085942069111868, 0.009302793659067662, 0.007721694837894666, 0.007593515913846881, 0.011603125620113557, 0.007693607201294381, 0.008484034573428275, 0.008530135735753895, 0.008363266674036434, 0.007487670892069853, 0.008551346535195214, 0.00765040861948054, 0.007998236628292604, 0.010388494875325366, 0.008696002975943707, 0.008317494968876474, 0.008540463464175778, 0.008305174083802705, 0.007873657643217449, 0.008450277983535861, 0.008344844976338998, 0.010363057683503558, 0.00837521267470059, 0.00750802947359896, 0.007970365673996681, 0.007685062069402531, 0.008555608263686877, 0.009857796006222335, 0.009772082047118234, 0.008506386681611454, 0.008810540791167769, 0.007940647450398452, 0.009661430875633576, 0.009364871223377742, 0.00973353705498888, 0.008320546736074396, 0.008310610984358214, 0.008421851015223782, 0.008427575976017487, 0.007456175046214877, 0.007669331579895098, 0.007892882953174123, 0.008393172743232098, 0.00856252790739139, 0.008584541218109833, 0.008071706318043808, 0.007790722783163999, 0.007961432814107154, 0.007773181349544447, 0.00841094324791212, 0.03800838410875998, 0.007927978348145766, 0.007644496224604027, 0.008253088309393489, 0.007747231302205329, 0.008225334037542921, 0.0076420076205864435, 0.007876271789879069, 0.007706805379599217, 0.007939088799044952, 0.008801913652352469, 0.008495758814636127, 0.008483428224675762, 0.007807091248488819, 0.007958822076880308, 0.007895829349164237, 0.008803005427728558, 0.00860954018656251, 0.00845543728679303, 0.00789180830077723, 0.007488822782874223, 0.007741088433869819, 0.007621398621578087, 0.007638921789723889, 0.008392072759222153, 0.008747491666094162, 0.00851980812284489]
[110.54109580494502, 125.11999419236284, 125.36028009705778, 119.04695469945892, 103.59428988361272, 122.55284647060971, 114.1098997306181, 115.09379179062827, 97.48541687635144, 91.83104561300378, 125.70163889157331, 117.76303355398711, 121.01590487876808, 125.20629520075768, 114.13599172153177, 123.50065631840853, 23.936967049510326, 125.18713392508161, 127.66546009985016, 115.72211966235753, 120.93012762481459, 131.23456947925624, 126.98730603467835, 114.41533908673932, 112.84037222476971, 126.84799872106444, 127.63955281366376, 123.62288112989846, 108.41265522759839, 124.46785085672043, 124.9262201697069, 113.1867597519613, 125.14557923450182, 126.61539192205095, 106.07926835126658, 113.29387342029274, 123.65410771242676, 116.16557723346747, 108.57493812213652, 124.92929825886601, 125.86222463493894, 128.82329370709604, 120.26409594086428, 126.14524777760363, 126.2719162247875, 128.18121736301404, 114.77915787987841, 100.29169607868184, 116.41603408137932, 114.05943420051027, 113.67844056234826, 126.99101933039994, 114.94183690873207, 113.06175148591245, 109.81179178981093, 115.45608722706949, 114.88325529339285, 115.88321015359722, 128.70223066361476, 128.20774194241395, 128.3734641884552, 117.78031965642951, 117.97693212574339, 127.33744288462356, 125.62832139388853, 23.78139159938833, 114.40557899500921, 115.25707330938705, 117.21611233022136, 115.93061873457027, 124.37233440759077, 128.4392866233972, 127.02618593339916, 120.30924364770576, 123.81683918018001, 128.77803133790206, 129.56090401227274, 132.91558546715478, 120.24331701114154, 113.67914493404412, 112.58201048846425, 117.2354208290825, 112.60333468189076, 107.00115970572877, 127.94062526625635, 116.68234293485199, 130.97567481071232, 118.68636472926185, 117.51357551401927, 108.39089165246644, 118.44658113081692, 127.91150748966636, 127.94490690700825, 131.46873889954742, 128.11765474885087, 131.32627129171786, 127.42938025247933, 85.78675573749553, 117.564558411846, 123.87536900085088, 125.61253417408244, 127.50821722308106, 127.8425103364983, 112.998571110456, 119.6167186559457, 116.30343206300793, 117.91032843145402, 123.67142770166254, 107.49459104956878, 129.50524735741197, 131.69130233552104, 86.18367435982421, 129.97804200762405, 117.86844942051133, 117.2314287811998, 119.5705026487409, 133.5528783802576, 116.94064740380347, 130.7119723583993, 125.0275587574697, 96.26033530373957, 114.99536083029894, 120.22850674895929, 117.08966430155064, 120.40686804509801, 127.00577613524042, 118.33930220382746, 119.83446101580118, 96.49661620545167, 119.39995303292397, 133.19073979615743, 125.46475794234888, 130.12256647625804, 116.88239680682493, 101.44255362646888, 102.33233769203747, 117.55872821555761, 113.50041089446653, 125.93431533719851, 103.5043372842454, 106.78203427973223, 102.73757569838958, 120.18440995763177, 120.32809643985819, 118.73874261042462, 118.6580818548203, 134.1170229778403, 130.38945957447808, 126.69641827107672, 119.1444559277489, 116.78794052592517, 116.48846159540983, 123.88954213616034, 128.3577952691414, 125.60553148524515, 128.64745527371568, 118.89272945079422, 26.30998458494122, 126.13556143652748, 130.8130674172448, 121.16676358131555, 129.07837148418943, 121.57560962700074, 130.85566642280585, 126.96362272376, 129.75544998802133, 125.95903954623822, 113.61165758911213, 117.70578965556827, 117.87687400846964, 128.0886783785914, 125.64673394382238, 126.64914042320946, 113.5975671274839, 116.15022153689083, 118.26709442478509, 126.71367092146858, 133.53233598835388, 129.18080041879767, 131.20951280106905, 130.90852708365605, 119.16007268896563, 114.3184855923972, 117.37353536385572]
Elapsed: 1.1486078723657713~0.5350529335831599
Time per graph: 0.0089039369950835~0.004147697159559379
Speed: 118.54251646671646~15.00268945567352
Total Time: 1.0998
best val loss: 0.1441824940112672 test_score: 0.9147

Testing...
Test loss: 0.4658 score: 0.9147 time: 1.03s
test Score 0.9147
Epoch Time List: [3.250126752536744, 3.349241921212524, 3.3010398948099464, 3.3697366507258266, 3.500154238892719, 3.5185752466786653, 3.3600195108447224, 3.2505026271101087, 4.026186449918896, 4.045532207004726, 3.651824069209397, 3.404118151869625, 3.403386513935402, 3.2965790648013353, 3.3933153990656137, 3.228728477144614, 7.607430359115824, 3.882513011805713, 3.2355665918439627, 3.2445764190051705, 3.231253062840551, 3.1972750849090517, 3.3643918898887932, 3.3129324400797486, 3.255676240194589, 3.261900636134669, 3.208783364156261, 3.2987351031042635, 3.38215714530088, 3.293934019980952, 3.303090153960511, 3.330776546150446, 3.2451731278561056, 3.262796824797988, 3.4214769769459963, 3.237292880890891, 3.158879574155435, 3.322059095837176, 3.5146403051912785, 3.2593731619417667, 3.156617248430848, 3.150980557780713, 3.241230022860691, 3.2281348381657153, 3.2201549988240004, 3.213498560944572, 3.4067715818528086, 3.48981635668315, 3.5756113645620644, 3.3180751481559128, 3.3867602481041104, 3.2711902691517025, 3.373740625102073, 3.5230872039683163, 3.321380498819053, 3.2218035119585693, 3.251637303037569, 3.2518973760306835, 3.2219165682327002, 3.1603176060598344, 3.25330016692169, 3.2867898661643267, 3.199349077884108, 3.2371638279873878, 3.285034842090681, 7.725639084586874, 3.28627777681686, 3.1824437268078327, 3.209660747088492, 3.243104499997571, 3.255397008964792, 3.2376810230780393, 3.233533133054152, 3.2001151067670435, 3.2666474629659206, 3.212385969934985, 3.230628195684403, 3.2296445718966424, 3.3111725207418203, 3.322931586066261, 3.3174488639924675, 3.4079596251249313, 3.3655037188436836, 3.5051012618932873, 3.6152734123170376, 3.4108732547611, 3.2800656768959016, 3.395371082937345, 3.3523185229860246, 3.334538124036044, 3.317234128015116, 3.2673226818442345, 3.6915807179175317, 3.1968581329565495, 3.2610722908284515, 3.2162747508846223, 3.397746730130166, 3.838129841024056, 3.393031407147646, 3.3103630882687867, 3.2798024639487267, 3.324843099107966, 3.2809803797863424, 3.460724541451782, 3.278899551834911, 3.3570585041306913, 3.296546395169571, 3.353112283628434, 3.481131297769025, 3.23431164608337, 3.2009845920838416, 7.766020548064262, 3.32065658015199, 3.3399493764154613, 3.206877558026463, 3.250198360765353, 3.2809561488684267, 3.3072770312428474, 3.237457655603066, 3.2724887202493846, 3.682663347804919, 3.749913393286988, 3.226050349883735, 3.255348323378712, 3.3355590619612485, 3.2655466778669506, 3.3526246338151395, 3.220145479775965, 3.5406906460411847, 3.2044572671875358, 3.2375290871132165, 3.2837617688346654, 3.2499784708488733, 3.346184205962345, 3.9172790909651667, 3.6223604320548475, 3.623035311931744, 3.3657443476840854, 3.306145634036511, 3.974457416916266, 3.7640652982518077, 3.577418295899406, 3.3098919254262, 3.2174782850779593, 3.195447474019602, 3.291515320772305, 3.1779033122584224, 3.2020656219683588, 3.31185926287435, 3.381813553860411, 3.290611560922116, 3.2666532788425684, 3.3723843439947814, 3.3059934079647064, 3.6516429968178272, 3.277355618774891, 3.3296422108542174, 7.171549947233871, 4.557536395732313, 3.231082840822637, 3.2774275662377477, 3.174744636984542, 3.144087244058028, 3.168588559143245, 3.2611105132382363, 3.2369114940520376, 3.3112298478372395, 3.4097163572441787, 3.303545907139778, 3.25192578881979, 3.2945043449290097, 3.2737413921859115, 3.3306266721338034, 3.4340515160001814, 3.2573772543109953, 3.273454573005438, 3.3211352471262217, 3.201779351104051, 3.1894716238602996, 3.2392486601602286, 3.2132246119435877, 3.356134324101731, 3.463943849084899, 3.308394307969138]
Total Epoch List: [73, 111]
Total Time List: [1.0160035521257669, 1.0998023699503392]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f6c800e41f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5039 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 1.18s
Epoch 2/1000, LR 0.000020
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5039 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 1.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5039 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 1.16s
Epoch 4/1000, LR 0.000080
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5039 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5000 time: 1.14s
Epoch 5/1000, LR 0.000110
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5039 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 1.07s
Epoch 6/1000, LR 0.000140
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5039 time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 1.13s
Epoch 7/1000, LR 0.000170
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5000 time: 1.03s
Epoch 8/1000, LR 0.000200
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 1.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 1.08s
Epoch 9/1000, LR 0.000230
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5039 time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 1.30s
Epoch 10/1000, LR 0.000260
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 1.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 1.01s
Epoch 11/1000, LR 0.000290
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 1.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 1.10s
Epoch 12/1000, LR 0.000290
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 1.14s
Epoch 13/1000, LR 0.000290
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 1.11s
Epoch 14/1000, LR 0.000290
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 1.04s
Epoch 15/1000, LR 0.000290
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5039 time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 1.04s
Epoch 16/1000, LR 0.000290
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5039 time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 1.08s
Epoch 17/1000, LR 0.000290
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5039 time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5000 time: 1.32s
Epoch 18/1000, LR 0.000290
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5039 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5000 time: 1.23s
Epoch 19/1000, LR 0.000290
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 1.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5039 time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.5000 time: 1.16s
Epoch 20/1000, LR 0.000290
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6775 score: 0.5039 time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.5000 time: 1.09s
Epoch 21/1000, LR 0.000290
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 1.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6722 score: 0.5039 time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6727 score: 0.5000 time: 1.11s
Epoch 22/1000, LR 0.000290
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 3.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6650 score: 0.5039 time: 4.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6654 score: 0.5000 time: 1.10s
Epoch 23/1000, LR 0.000290
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 1.13s
Val loss: 0.6570 score: 0.5116 time: 0.99s
Test loss: 0.6570 score: 0.5078 time: 1.14s
Epoch 24/1000, LR 0.000290
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 1.08s
Val loss: 0.6484 score: 0.5194 time: 1.08s
Test loss: 0.6481 score: 0.5391 time: 1.01s
Epoch 25/1000, LR 0.000290
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 1.10s
Val loss: 0.6372 score: 0.7287 time: 1.12s
Test loss: 0.6366 score: 0.6641 time: 1.05s
Epoch 26/1000, LR 0.000290
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 1.24s
Val loss: 0.6232 score: 0.8450 time: 0.99s
Test loss: 0.6226 score: 0.8125 time: 1.00s
Epoch 27/1000, LR 0.000290
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 1.22s
Val loss: 0.6072 score: 0.8837 time: 0.98s
Test loss: 0.6064 score: 0.8594 time: 1.01s
Epoch 28/1000, LR 0.000290
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 1.18s
Val loss: 0.5907 score: 0.8915 time: 0.98s
Test loss: 0.5900 score: 0.8906 time: 1.10s
Epoch 29/1000, LR 0.000290
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 1.09s
Val loss: 0.5707 score: 0.8837 time: 0.99s
Test loss: 0.5699 score: 0.9141 time: 1.11s
Epoch 30/1000, LR 0.000290
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 1.10s
Val loss: 0.5467 score: 0.8760 time: 1.02s
Test loss: 0.5458 score: 0.9219 time: 1.10s
Epoch 31/1000, LR 0.000290
Train loss: 0.5584;  Loss pred: 0.5584; Loss self: 0.0000; time: 1.11s
Val loss: 0.5173 score: 0.9225 time: 1.10s
Test loss: 0.5163 score: 0.9297 time: 1.06s
Epoch 32/1000, LR 0.000290
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 1.11s
Val loss: 0.4884 score: 0.9147 time: 1.14s
Test loss: 0.4870 score: 0.9297 time: 1.22s
Epoch 33/1000, LR 0.000290
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 1.17s
Val loss: 0.4660 score: 0.9225 time: 1.12s
Test loss: 0.4639 score: 0.9297 time: 1.02s
Epoch 34/1000, LR 0.000290
Train loss: 0.4782;  Loss pred: 0.4782; Loss self: 0.0000; time: 1.25s
Val loss: 0.4518 score: 0.8992 time: 1.01s
Test loss: 0.4484 score: 0.9297 time: 1.08s
Epoch 35/1000, LR 0.000290
Train loss: 0.4723;  Loss pred: 0.4723; Loss self: 0.0000; time: 1.22s
Val loss: 0.4421 score: 0.8760 time: 1.00s
Test loss: 0.4365 score: 0.8750 time: 1.04s
Epoch 36/1000, LR 0.000290
Train loss: 0.4578;  Loss pred: 0.4578; Loss self: 0.0000; time: 1.20s
Val loss: 0.4201 score: 0.8915 time: 1.03s
Test loss: 0.4139 score: 0.9141 time: 1.15s
Epoch 37/1000, LR 0.000290
Train loss: 0.4240;  Loss pred: 0.4240; Loss self: 0.0000; time: 1.11s
Val loss: 0.3884 score: 0.9380 time: 1.01s
Test loss: 0.3828 score: 0.9297 time: 1.11s
Epoch 38/1000, LR 0.000289
Train loss: 0.3940;  Loss pred: 0.3940; Loss self: 0.0000; time: 1.14s
Val loss: 0.3584 score: 0.9225 time: 0.99s
Test loss: 0.3523 score: 0.9375 time: 1.12s
Epoch 39/1000, LR 0.000289
Train loss: 0.3619;  Loss pred: 0.3619; Loss self: 0.0000; time: 1.14s
Val loss: 0.3313 score: 0.9225 time: 0.99s
Test loss: 0.3246 score: 0.9297 time: 1.13s
Epoch 40/1000, LR 0.000289
Train loss: 0.3333;  Loss pred: 0.3333; Loss self: 0.0000; time: 1.19s
Val loss: 0.3097 score: 0.9225 time: 1.22s
Test loss: 0.3030 score: 0.9375 time: 1.04s
Epoch 41/1000, LR 0.000289
Train loss: 0.3081;  Loss pred: 0.3081; Loss self: 0.0000; time: 1.14s
Val loss: 0.2958 score: 0.9225 time: 1.13s
Test loss: 0.2895 score: 0.9453 time: 1.01s
Epoch 42/1000, LR 0.000289
Train loss: 0.2825;  Loss pred: 0.2825; Loss self: 0.0000; time: 1.22s
Val loss: 0.2869 score: 0.9380 time: 1.08s
Test loss: 0.2799 score: 0.9375 time: 1.04s
Epoch 43/1000, LR 0.000289
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 1.13s
Val loss: 0.2703 score: 0.9380 time: 1.15s
Test loss: 0.2633 score: 0.9453 time: 1.00s
Epoch 44/1000, LR 0.000289
Train loss: 0.2530;  Loss pred: 0.2530; Loss self: 0.0000; time: 1.21s
Val loss: 0.2479 score: 0.9225 time: 1.02s
Test loss: 0.2423 score: 0.9453 time: 0.99s
Epoch 45/1000, LR 0.000289
Train loss: 0.2366;  Loss pred: 0.2366; Loss self: 0.0000; time: 1.26s
Val loss: 0.2345 score: 0.9225 time: 0.98s
Test loss: 0.2298 score: 0.9375 time: 1.11s
Epoch 46/1000, LR 0.000289
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 1.09s
Val loss: 0.2273 score: 0.9225 time: 0.97s
Test loss: 0.2227 score: 0.9297 time: 1.09s
Epoch 47/1000, LR 0.000289
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 1.11s
Val loss: 0.2293 score: 0.8992 time: 0.99s
Test loss: 0.2243 score: 0.9141 time: 1.12s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.2229;  Loss pred: 0.2229; Loss self: 0.0000; time: 1.09s
Val loss: 0.2231 score: 0.9070 time: 1.07s
Test loss: 0.2174 score: 0.9141 time: 1.00s
Epoch 49/1000, LR 0.000289
Train loss: 0.2042;  Loss pred: 0.2042; Loss self: 0.0000; time: 1.13s
Val loss: 0.2169 score: 0.9302 time: 1.08s
Test loss: 0.2069 score: 0.9531 time: 1.03s
Epoch 50/1000, LR 0.000289
Train loss: 0.1941;  Loss pred: 0.1941; Loss self: 0.0000; time: 1.24s
Val loss: 0.2368 score: 0.9380 time: 1.00s
Test loss: 0.2251 score: 0.9453 time: 1.16s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.2054;  Loss pred: 0.2054; Loss self: 0.0000; time: 1.23s
Val loss: 0.2587 score: 0.9302 time: 1.06s
Test loss: 0.2473 score: 0.9297 time: 1.00s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.2071;  Loss pred: 0.2071; Loss self: 0.0000; time: 1.23s
Val loss: 0.2592 score: 0.9302 time: 1.01s
Test loss: 0.2482 score: 0.9297 time: 1.10s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1958;  Loss pred: 0.1958; Loss self: 0.0000; time: 1.12s
Val loss: 0.2360 score: 0.9457 time: 0.99s
Test loss: 0.2251 score: 0.9453 time: 1.11s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 1.15s
Val loss: 0.2193 score: 0.9380 time: 1.02s
Test loss: 0.2078 score: 0.9453 time: 1.15s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 1.10s
Val loss: 0.2120 score: 0.9380 time: 1.13s
Test loss: 0.1993 score: 0.9453 time: 1.01s
Epoch 56/1000, LR 0.000289
Train loss: 0.1515;  Loss pred: 0.1515; Loss self: 0.0000; time: 1.12s
Val loss: 0.2109 score: 0.9225 time: 1.12s
Test loss: 0.1972 score: 0.9531 time: 1.05s
Epoch 57/1000, LR 0.000288
Train loss: 0.1730;  Loss pred: 0.1730; Loss self: 0.0000; time: 1.16s
Val loss: 0.2121 score: 0.9225 time: 1.07s
Test loss: 0.1975 score: 0.9453 time: 1.01s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 1.19s
Val loss: 0.2137 score: 0.9225 time: 1.32s
Test loss: 0.1986 score: 0.9453 time: 1.04s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 1.21s
Val loss: 0.2143 score: 0.9225 time: 1.00s
Test loss: 0.1992 score: 0.9453 time: 1.05s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 1.23s
Val loss: 0.2134 score: 0.9225 time: 1.06s
Test loss: 0.1992 score: 0.9453 time: 1.12s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 1.12s
Val loss: 0.2114 score: 0.9302 time: 0.99s
Test loss: 0.1990 score: 0.9375 time: 1.11s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 1.14s
Val loss: 0.2100 score: 0.9302 time: 1.03s
Test loss: 0.1995 score: 0.9375 time: 1.13s
Epoch 63/1000, LR 0.000288
Train loss: 0.1362;  Loss pred: 0.1362; Loss self: 0.0000; time: 1.29s
Val loss: 0.2089 score: 0.9457 time: 0.99s
Test loss: 0.2022 score: 0.9531 time: 1.10s
Epoch 64/1000, LR 0.000288
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 1.10s
Val loss: 0.2183 score: 0.9302 time: 0.97s
Test loss: 0.2190 score: 0.9453 time: 1.08s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 1.10s
Val loss: 0.2269 score: 0.9302 time: 1.11s
Test loss: 0.2306 score: 0.9453 time: 1.01s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.1433;  Loss pred: 0.1433; Loss self: 0.0000; time: 1.10s
Val loss: 0.2197 score: 0.9302 time: 1.09s
Test loss: 0.2238 score: 0.9453 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 1.10s
Val loss: 0.2130 score: 0.9457 time: 1.37s
Test loss: 0.2153 score: 0.9531 time: 1.25s
     INFO: Early stopping counter 4 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.1169;  Loss pred: 0.1169; Loss self: 0.0000; time: 1.22s
Val loss: 0.2116 score: 0.9457 time: 1.00s
Test loss: 0.2119 score: 0.9453 time: 1.02s
     INFO: Early stopping counter 5 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.1091;  Loss pred: 0.1091; Loss self: 0.0000; time: 1.25s
Val loss: 0.2125 score: 0.9457 time: 1.05s
Test loss: 0.2125 score: 0.9453 time: 1.08s
     INFO: Early stopping counter 6 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 1.23s
Val loss: 0.2146 score: 0.9457 time: 1.05s
Test loss: 0.2126 score: 0.9453 time: 1.16s
     INFO: Early stopping counter 7 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0975;  Loss pred: 0.0975; Loss self: 0.0000; time: 1.15s
Val loss: 0.2201 score: 0.9380 time: 0.98s
Test loss: 0.2148 score: 0.9453 time: 1.19s
     INFO: Early stopping counter 8 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.1079;  Loss pred: 0.1079; Loss self: 0.0000; time: 1.10s
Val loss: 0.2243 score: 0.9380 time: 1.08s
Test loss: 0.2179 score: 0.9453 time: 1.04s
     INFO: Early stopping counter 9 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.1210;  Loss pred: 0.1210; Loss self: 0.0000; time: 1.16s
Val loss: 0.2252 score: 0.9380 time: 5.26s
Test loss: 0.2195 score: 0.9453 time: 1.00s
     INFO: Early stopping counter 10 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0893;  Loss pred: 0.0893; Loss self: 0.0000; time: 1.12s
Val loss: 0.2241 score: 0.9457 time: 1.07s
Test loss: 0.2205 score: 0.9453 time: 1.01s
     INFO: Early stopping counter 11 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 1.11s
Val loss: 0.2236 score: 0.9457 time: 1.09s
Test loss: 0.2226 score: 0.9453 time: 1.03s
     INFO: Early stopping counter 12 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.1068;  Loss pred: 0.1068; Loss self: 0.0000; time: 1.18s
Val loss: 0.2236 score: 0.9457 time: 1.10s
Test loss: 0.2254 score: 0.9453 time: 0.99s
     INFO: Early stopping counter 13 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.1050;  Loss pred: 0.1050; Loss self: 0.0000; time: 1.26s
Val loss: 0.2246 score: 0.9457 time: 0.97s
Test loss: 0.2296 score: 0.9453 time: 1.02s
     INFO: Early stopping counter 14 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0877;  Loss pred: 0.0877; Loss self: 0.0000; time: 1.19s
Val loss: 0.2271 score: 0.9457 time: 0.98s
Test loss: 0.2348 score: 0.9453 time: 1.12s
     INFO: Early stopping counter 15 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 1.14s
Val loss: 0.2300 score: 0.9535 time: 1.00s
Test loss: 0.2391 score: 0.9453 time: 1.09s
     INFO: Early stopping counter 16 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0814;  Loss pred: 0.0814; Loss self: 0.0000; time: 1.11s
Val loss: 0.2318 score: 0.9535 time: 1.08s
Test loss: 0.2414 score: 0.9453 time: 1.02s
     INFO: Early stopping counter 17 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.1003;  Loss pred: 0.1003; Loss self: 0.0000; time: 1.21s
Val loss: 0.2406 score: 0.9147 time: 0.99s
Test loss: 0.2520 score: 0.9297 time: 1.03s
     INFO: Early stopping counter 18 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 1.21s
Val loss: 0.2801 score: 0.8992 time: 0.98s
Test loss: 0.2947 score: 0.8594 time: 1.00s
     INFO: Early stopping counter 19 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 1.20s
Val loss: 0.3038 score: 0.8992 time: 0.98s
Test loss: 0.3189 score: 0.8516 time: 1.00s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 062,   Train_Loss: 0.1362,   Val_Loss: 0.2089,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.2089,   Test_Precision: 0.9833,   Test_Recall: 0.9219,   Test_accuracy: 0.9516,   Test_Score: 0.9531,   Test_loss: 0.2022


[1.1669868030585349, 1.0310102780349553, 1.0290340760257095, 1.083606047090143, 1.2452423791401088, 1.0526071300264448, 1.1304891188628972, 1.1208250070922077, 1.3232748459558934, 1.404753688024357, 1.0262396030593663, 1.0954201510176063, 1.0659755850210786, 1.0302996330428869, 1.1302306840661913, 1.0445288619957864, 5.389153928030282, 1.0304573317989707, 1.0104534139391035, 1.1147393460851163, 1.0667316948529333, 0.9829727069009095, 1.0158495681826025, 1.1274712029844522, 1.1432078559882939, 1.0169651969335973, 1.0106585079338402, 1.0434961458668113, 1.189897984964773, 1.0364122069440782, 1.0326094860211015, 1.1397092759143561, 1.0307994959875941, 1.018833477050066, 1.2160717358347028, 1.1386317380238324, 1.043232630006969, 1.110483872005716, 1.1881194890011102, 1.0325840439181775, 1.0249302391894162, 1.0013716951943934, 1.0726393358781934, 1.0226306759286672, 1.0216048338916153, 1.0063876959029585, 1.1238974251318723, 1.2862480648327619, 1.108094782801345, 1.1309893031138927, 1.134779817191884, 1.0158198641147465, 1.1223067550454289, 1.1409694110043347, 1.1747372290119529, 1.1173079141881317, 1.1228790450841188, 1.1131897349841893, 1.002313629956916, 1.0061794868670404, 1.0048805710393935, 1.095259380992502, 1.0934340949170291, 1.0130563098937273, 1.0268385231029242, 5.424409226048738, 1.127567389048636, 1.1192371651995927, 1.1005312958732247, 1.1127345080021769, 1.0372081589885056, 1.0043655908666551, 1.0155386391561478, 1.0722368131391704, 1.0418615178205073, 1.0017236531712115, 0.9956707309465855, 0.9705408101435751, 1.0728246958460659, 1.1347727859392762, 1.1458313760813326, 1.100350040011108, 1.1456143849063665, 1.2055944099556655, 1.0082802060060203, 1.105565733043477, 0.9849157119169831, 1.0868982321117073, 1.0977455109823495, 1.1901369020342827, 1.0890985520090908, 1.0085097309201956, 1.008246464189142, 0.9812218560837209, 1.0068869919050485, 0.9822863219305873, 1.0123254130594432, 1.5037286221049726, 1.097269464051351, 1.041369249112904, 1.0269675781019032, 1.0116995030548424, 1.0090540279634297, 1.141607356024906, 1.0784445640165359, 1.1091676119249314, 1.0940517401322722, 1.043086526915431, 1.2000603820197284, 0.9960986340884119, 0.9795635528862476, 1.4968032049946487, 0.9924753289669752, 1.0944404599722475, 1.1003875099122524, 1.0788614009507, 0.965909545077011, 1.1031237030401826, 0.9869027119129896, 1.031772525049746, 1.3401158389169723, 1.1217843838967383, 1.072956850985065, 1.1017197868786752, 1.071367456810549, 1.015701835975051, 1.090085859876126, 1.0764850019477308, 1.336834441171959, 1.0804024350363761, 0.9685358020942658, 1.028177171945572, 0.9913730069529265, 1.1036734660156071, 1.2716556848026812, 1.2605985840782523, 1.0973238819278777, 1.1365597620606422, 1.0243435211014003, 1.2463245829567313, 1.2080683878157288, 1.2556262800935656, 1.073350528953597, 1.0720688169822097, 1.086418780963868, 1.0871573009062558, 0.9618465809617192, 0.9893437738064677, 1.018181900959462, 1.0827192838769406, 1.1045661000534892, 1.1074058171361685, 1.0412501150276512, 1.0050032390281558, 1.0270248330198228, 1.0027403940912336, 1.0850116789806634, 4.903081550030038, 1.022709206910804, 0.9861400129739195, 1.06464839191176, 0.9993928379844874, 1.0610680908430368, 0.9858189830556512, 1.0160390608943999, 0.994177893968299, 1.0241424550767988, 1.1354468611534685, 1.0959528870880604, 1.0943622409831733, 1.0071147710550576, 1.0266880479175597, 1.0185619860421866, 1.135587700176984, 1.1106306840665638, 1.090751409996301, 1.0180432708002627, 0.9660581389907748, 0.9986004079692066, 0.9831604221835732, 0.9854209108743817, 1.0825773859396577, 1.1284264249261469, 1.0990552478469908, 1.1889785949606448, 1.1651640650816262, 1.1680219192057848, 1.14580064592883, 1.0711897171568125, 1.1351875311229378, 1.0364359240047634, 1.0825279569253325, 1.3086451331619173, 1.0142634289804846, 1.106499905930832, 1.1452663699164987, 1.1153794769197702, 1.0479582659900188, 1.045878577977419, 1.0820664789061993, 1.3281419589184225, 1.2326972230803221, 1.1635170299559832, 1.0977607760578394, 1.1136058999691159, 1.103746626060456, 1.1501373040955514, 1.016103909816593, 1.053572426782921, 1.0056375148706138, 1.0142514330800623, 1.1002742820419371, 1.1125247569289058, 1.108619567938149, 1.0620340830646455, 1.2266992621589452, 1.029711436945945, 1.0875065890140831, 1.0498873519245535, 1.1523089981637895, 1.1198282749392092, 1.1277592240367085, 1.138416338013485, 1.0481045641936362, 1.011025071144104, 1.040721118915826, 1.006827670847997, 0.9969610089901835, 1.1134846119675785, 1.0967929130420089, 1.1238941790070385, 1.0020326341036707, 1.0322896901052445, 1.1704619189258665, 1.0010627799201757, 1.1104219439439476, 1.1132411379367113, 1.159789899829775, 1.0172474100254476, 1.051837480859831, 1.0185624840669334, 1.0458795330487192, 1.0573568970430642, 1.1292038969695568, 1.1189126668032259, 1.1335624048952013, 1.1092993260826916, 1.088197364937514, 1.0166911680717021, 1.0081312600523233, 1.2515047499909997, 1.029299485962838, 1.0876518690492958, 1.1616661730222404, 1.1916948650032282, 1.0428836320061237, 1.0035417641047388, 1.016791535075754, 1.0329969499725848, 0.9986230919603258, 1.0270248129963875, 1.1232106548268348, 1.0985809091944247, 1.0253616499248892, 1.0352361539844424, 1.007556410972029, 1.0023245001211762]
[0.009046409326035154, 0.00799232773670508, 0.007977008341284569, 0.008400046876667774, 0.009653041698760533, 0.008159745194003448, 0.008763481541572846, 0.008688565946451222, 0.010257944542293748, 0.01088956347305703, 0.007955345760150126, 0.008491629077655863, 0.008263376628070376, 0.007986818860797572, 0.008761478171055747, 0.008097122961207646, 0.04177638703899444, 0.007988041331774966, 0.007832972201078321, 0.008641390279729584, 0.008269237944596382, 0.007619943464348136, 0.007874802854128702, 0.008740086844840716, 0.00886207640301003, 0.007883451139020134, 0.007834562077006512, 0.008089117409820242, 0.009224015387323823, 0.008034203154605258, 0.008004724697837995, 0.00883495562724307, 0.00799069376734569, 0.007897933930620667, 0.009426912680889169, 0.008826602620339786, 0.008087074651216814, 0.008608402108571441, 0.009210228596907831, 0.008004527472233933, 0.007945195652631134, 0.007762571280576693, 0.008315033611458864, 0.007927369580842381, 0.007919417316989266, 0.007801455006999678, 0.00871238314055715, 0.00997091523126172, 0.008589882037219729, 0.008767358938867384, 0.008796742768929333, 0.007874572590036794, 0.008700052364693248, 0.008844724116312671, 0.00910649014737948, 0.008661301660373114, 0.008704488721582316, 0.008629377790575111, 0.007769873100441209, 0.0077998409834654295, 0.00778977186852243, 0.008490382798391488, 0.008476233293930458, 0.007853149689098662, 0.00795998855118546, 0.04204968392285843, 0.00874083247324524, 0.008676257094570487, 0.008531250355606394, 0.008625848899241681, 0.008040373325492291, 0.007785779774160118, 0.007872392551598046, 0.008311913280148608, 0.008076445874577575, 0.0077652996369861355, 0.007718377759275857, 0.00752357217165562, 0.00831647051043462, 0.008796688263095164, 0.008882413768072346, 0.008529845271403937, 0.008880731665940826, 0.009345693100431516, 0.007816125627953646, 0.00857027700033703, 0.007635005518736303, 0.008425567690788428, 0.008509655123894183, 0.0092258674576301, 0.008442624434178998, 0.00781790489085423, 0.00781586406348172, 0.007606370977393186, 0.007805325518643787, 0.0076146226506247075, 0.007847483822166226, 0.011656811024069555, 0.008505964837607372, 0.008072629838084528, 0.007960988977534134, 0.007842631806626685, 0.007822124247778525, 0.00884966942654966, 0.008360035379973147, 0.008598198542053731, 0.008481021241335443, 0.008085942069111868, 0.009302793659067662, 0.007721694837894666, 0.007593515913846881, 0.011603125620113557, 0.007693607201294381, 0.008484034573428275, 0.008530135735753895, 0.008363266674036434, 0.007487670892069853, 0.008551346535195214, 0.00765040861948054, 0.007998236628292604, 0.010388494875325366, 0.008696002975943707, 0.008317494968876474, 0.008540463464175778, 0.008305174083802705, 0.007873657643217449, 0.008450277983535861, 0.008344844976338998, 0.010363057683503558, 0.00837521267470059, 0.00750802947359896, 0.007970365673996681, 0.007685062069402531, 0.008555608263686877, 0.009857796006222335, 0.009772082047118234, 0.008506386681611454, 0.008810540791167769, 0.007940647450398452, 0.009661430875633576, 0.009364871223377742, 0.00973353705498888, 0.008320546736074396, 0.008310610984358214, 0.008421851015223782, 0.008427575976017487, 0.007456175046214877, 0.007669331579895098, 0.007892882953174123, 0.008393172743232098, 0.00856252790739139, 0.008584541218109833, 0.008071706318043808, 0.007790722783163999, 0.007961432814107154, 0.007773181349544447, 0.00841094324791212, 0.03800838410875998, 0.007927978348145766, 0.007644496224604027, 0.008253088309393489, 0.007747231302205329, 0.008225334037542921, 0.0076420076205864435, 0.007876271789879069, 0.007706805379599217, 0.007939088799044952, 0.008801913652352469, 0.008495758814636127, 0.008483428224675762, 0.007807091248488819, 0.007958822076880308, 0.007895829349164237, 0.008803005427728558, 0.00860954018656251, 0.00845543728679303, 0.00789180830077723, 0.007488822782874223, 0.007741088433869819, 0.007621398621578087, 0.007638921789723889, 0.008392072759222153, 0.008747491666094162, 0.00851980812284489, 0.009288895273130038, 0.009102844258450205, 0.009125171243795194, 0.008951567546318984, 0.008368669665287598, 0.008868652586897952, 0.008097155656287214, 0.00845724966347916, 0.010223790102827479, 0.007923933038910036, 0.008644530515084625, 0.008947393514972646, 0.008713902163435705, 0.008187173953047022, 0.008170926390448585, 0.008453644366454682, 0.010376109054050175, 0.009630447055315017, 0.009089976796531118, 0.00857625606295187, 0.008700046093508718, 0.008623020516097313, 0.008985447688246495, 0.007938311795442132, 0.00823103458424157, 0.00785654308492667, 0.007923839320937986, 0.008595892828452634, 0.008691599663507077, 0.008661090374516789, 0.008297141273942543, 0.00958358798561676, 0.008044620601140196, 0.008496145226672525, 0.008202244936910574, 0.009002414048154606, 0.008748658397962572, 0.008810618937786785, 0.008893877640730352, 0.008188316907762783, 0.007898633368313313, 0.008130633741529891, 0.007865841178499977, 0.007788757882735808, 0.008699098530996707, 0.008568694633140694, 0.008780423273492488, 0.007828379953934927, 0.008064763203947223, 0.009144233741608332, 0.007820802968126372, 0.00867517143706209, 0.008697196390130557, 0.009060858592420118, 0.00794724539082381, 0.00821748031921743, 0.007957519406772917, 0.008170933851943118, 0.00826060075814894, 0.008821905445074663, 0.008741505209400202, 0.00885595628824376, 0.008666400985021028, 0.008501541913574329, 0.007942899750560173, 0.007876025469158776, 0.009777380859304685, 0.008041402234084671, 0.008497280226947623, 0.009075516976736253, 0.00931011613283772, 0.008147528375047841, 0.007840170032068272, 0.007943683867779328, 0.008070288671660819, 0.007801742905940046, 0.008023631351534277, 0.008775083240834647, 0.008582663353081443, 0.008010637890038197, 0.008087782453003456, 0.007871534460718976, 0.00783066015719669]
[110.54109580494502, 125.11999419236284, 125.36028009705778, 119.04695469945892, 103.59428988361272, 122.55284647060971, 114.1098997306181, 115.09379179062827, 97.48541687635144, 91.83104561300378, 125.70163889157331, 117.76303355398711, 121.01590487876808, 125.20629520075768, 114.13599172153177, 123.50065631840853, 23.936967049510326, 125.18713392508161, 127.66546009985016, 115.72211966235753, 120.93012762481459, 131.23456947925624, 126.98730603467835, 114.41533908673932, 112.84037222476971, 126.84799872106444, 127.63955281366376, 123.62288112989846, 108.41265522759839, 124.46785085672043, 124.9262201697069, 113.1867597519613, 125.14557923450182, 126.61539192205095, 106.07926835126658, 113.29387342029274, 123.65410771242676, 116.16557723346747, 108.57493812213652, 124.92929825886601, 125.86222463493894, 128.82329370709604, 120.26409594086428, 126.14524777760363, 126.2719162247875, 128.18121736301404, 114.77915787987841, 100.29169607868184, 116.41603408137932, 114.05943420051027, 113.67844056234826, 126.99101933039994, 114.94183690873207, 113.06175148591245, 109.81179178981093, 115.45608722706949, 114.88325529339285, 115.88321015359722, 128.70223066361476, 128.20774194241395, 128.3734641884552, 117.78031965642951, 117.97693212574339, 127.33744288462356, 125.62832139388853, 23.78139159938833, 114.40557899500921, 115.25707330938705, 117.21611233022136, 115.93061873457027, 124.37233440759077, 128.4392866233972, 127.02618593339916, 120.30924364770576, 123.81683918018001, 128.77803133790206, 129.56090401227274, 132.91558546715478, 120.24331701114154, 113.67914493404412, 112.58201048846425, 117.2354208290825, 112.60333468189076, 107.00115970572877, 127.94062526625635, 116.68234293485199, 130.97567481071232, 118.68636472926185, 117.51357551401927, 108.39089165246644, 118.44658113081692, 127.91150748966636, 127.94490690700825, 131.46873889954742, 128.11765474885087, 131.32627129171786, 127.42938025247933, 85.78675573749553, 117.564558411846, 123.87536900085088, 125.61253417408244, 127.50821722308106, 127.8425103364983, 112.998571110456, 119.6167186559457, 116.30343206300793, 117.91032843145402, 123.67142770166254, 107.49459104956878, 129.50524735741197, 131.69130233552104, 86.18367435982421, 129.97804200762405, 117.86844942051133, 117.2314287811998, 119.5705026487409, 133.5528783802576, 116.94064740380347, 130.7119723583993, 125.0275587574697, 96.26033530373957, 114.99536083029894, 120.22850674895929, 117.08966430155064, 120.40686804509801, 127.00577613524042, 118.33930220382746, 119.83446101580118, 96.49661620545167, 119.39995303292397, 133.19073979615743, 125.46475794234888, 130.12256647625804, 116.88239680682493, 101.44255362646888, 102.33233769203747, 117.55872821555761, 113.50041089446653, 125.93431533719851, 103.5043372842454, 106.78203427973223, 102.73757569838958, 120.18440995763177, 120.32809643985819, 118.73874261042462, 118.6580818548203, 134.1170229778403, 130.38945957447808, 126.69641827107672, 119.1444559277489, 116.78794052592517, 116.48846159540983, 123.88954213616034, 128.3577952691414, 125.60553148524515, 128.64745527371568, 118.89272945079422, 26.30998458494122, 126.13556143652748, 130.8130674172448, 121.16676358131555, 129.07837148418943, 121.57560962700074, 130.85566642280585, 126.96362272376, 129.75544998802133, 125.95903954623822, 113.61165758911213, 117.70578965556827, 117.87687400846964, 128.0886783785914, 125.64673394382238, 126.64914042320946, 113.5975671274839, 116.15022153689083, 118.26709442478509, 126.71367092146858, 133.53233598835388, 129.18080041879767, 131.20951280106905, 130.90852708365605, 119.16007268896563, 114.3184855923972, 117.37353536385572, 107.65542840090977, 109.85577382274735, 109.58698453795769, 111.71227774639479, 119.49330538734246, 112.75670009639838, 123.50015764159457, 118.24174995309504, 97.81108472908116, 126.19995589179706, 115.68008213459474, 111.76439242630732, 114.75914937352492, 122.142268594138, 122.3851436440489, 118.29217750963699, 96.37523996624374, 103.83733945643809, 110.01128191896113, 116.60099612928406, 114.94191976133557, 115.96864441331392, 111.29106024489576, 125.9713684431192, 121.4914103160876, 127.28244333294248, 126.20144850206589, 116.33462863682682, 115.05361943885231, 115.45890375907676, 120.52343897536544, 104.3450533871886, 124.30667020620787, 117.70043629440704, 121.91784172403123, 111.08131603933379, 114.30323993823838, 113.49940419182384, 112.4368965253587, 122.12521953711494, 126.60417990935838, 122.99164269227508, 127.13198465452628, 128.3901765924131, 114.95443998441803, 116.7038904773607, 113.88972591092883, 127.74035060694148, 123.99620109249572, 109.3585343788593, 127.86410859287633, 115.27149719806081, 114.97958136656365, 110.36481695416285, 125.8297624928806, 121.69180346698201, 125.66730269597153, 122.38503188496519, 121.05657073591377, 113.35419612305046, 114.39677447365096, 112.9183531910038, 115.38815267472576, 117.62572132983428, 125.89860522027551, 126.96759347920293, 102.27687909368345, 124.35642079454156, 117.68471479011328, 110.18656045306865, 107.41004577514333, 122.73660844956899, 127.54825417175238, 125.88617782942462, 123.91130487209769, 128.17648723577213, 124.63184762455222, 113.95903292934284, 116.51394897610295, 124.83400369945217, 123.64328613075428, 127.04003329849631, 127.70315400304533]
Elapsed: 1.1301715244165855~0.44678757473674524
Time per graph: 0.008781527171469248~0.0034619425688586997
Speed: 118.36943536033903~13.114318475594192
Total Time: 1.0033
best val loss: 0.20888557336329258 test_score: 0.9531

Testing...
Test loss: 0.2391 score: 0.9453 time: 1.12s
test Score 0.9453
Epoch Time List: [3.250126752536744, 3.349241921212524, 3.3010398948099464, 3.3697366507258266, 3.500154238892719, 3.5185752466786653, 3.3600195108447224, 3.2505026271101087, 4.026186449918896, 4.045532207004726, 3.651824069209397, 3.404118151869625, 3.403386513935402, 3.2965790648013353, 3.3933153990656137, 3.228728477144614, 7.607430359115824, 3.882513011805713, 3.2355665918439627, 3.2445764190051705, 3.231253062840551, 3.1972750849090517, 3.3643918898887932, 3.3129324400797486, 3.255676240194589, 3.261900636134669, 3.208783364156261, 3.2987351031042635, 3.38215714530088, 3.293934019980952, 3.303090153960511, 3.330776546150446, 3.2451731278561056, 3.262796824797988, 3.4214769769459963, 3.237292880890891, 3.158879574155435, 3.322059095837176, 3.5146403051912785, 3.2593731619417667, 3.156617248430848, 3.150980557780713, 3.241230022860691, 3.2281348381657153, 3.2201549988240004, 3.213498560944572, 3.4067715818528086, 3.48981635668315, 3.5756113645620644, 3.3180751481559128, 3.3867602481041104, 3.2711902691517025, 3.373740625102073, 3.5230872039683163, 3.321380498819053, 3.2218035119585693, 3.251637303037569, 3.2518973760306835, 3.2219165682327002, 3.1603176060598344, 3.25330016692169, 3.2867898661643267, 3.199349077884108, 3.2371638279873878, 3.285034842090681, 7.725639084586874, 3.28627777681686, 3.1824437268078327, 3.209660747088492, 3.243104499997571, 3.255397008964792, 3.2376810230780393, 3.233533133054152, 3.2001151067670435, 3.2666474629659206, 3.212385969934985, 3.230628195684403, 3.2296445718966424, 3.3111725207418203, 3.322931586066261, 3.3174488639924675, 3.4079596251249313, 3.3655037188436836, 3.5051012618932873, 3.6152734123170376, 3.4108732547611, 3.2800656768959016, 3.395371082937345, 3.3523185229860246, 3.334538124036044, 3.317234128015116, 3.2673226818442345, 3.6915807179175317, 3.1968581329565495, 3.2610722908284515, 3.2162747508846223, 3.397746730130166, 3.838129841024056, 3.393031407147646, 3.3103630882687867, 3.2798024639487267, 3.324843099107966, 3.2809803797863424, 3.460724541451782, 3.278899551834911, 3.3570585041306913, 3.296546395169571, 3.353112283628434, 3.481131297769025, 3.23431164608337, 3.2009845920838416, 7.766020548064262, 3.32065658015199, 3.3399493764154613, 3.206877558026463, 3.250198360765353, 3.2809561488684267, 3.3072770312428474, 3.237457655603066, 3.2724887202493846, 3.682663347804919, 3.749913393286988, 3.226050349883735, 3.255348323378712, 3.3355590619612485, 3.2655466778669506, 3.3526246338151395, 3.220145479775965, 3.5406906460411847, 3.2044572671875358, 3.2375290871132165, 3.2837617688346654, 3.2499784708488733, 3.346184205962345, 3.9172790909651667, 3.6223604320548475, 3.623035311931744, 3.3657443476840854, 3.306145634036511, 3.974457416916266, 3.7640652982518077, 3.577418295899406, 3.3098919254262, 3.2174782850779593, 3.195447474019602, 3.291515320772305, 3.1779033122584224, 3.2020656219683588, 3.31185926287435, 3.381813553860411, 3.290611560922116, 3.2666532788425684, 3.3723843439947814, 3.3059934079647064, 3.6516429968178272, 3.277355618774891, 3.3296422108542174, 7.171549947233871, 4.557536395732313, 3.231082840822637, 3.2774275662377477, 3.174744636984542, 3.144087244058028, 3.168588559143245, 3.2611105132382363, 3.2369114940520376, 3.3112298478372395, 3.4097163572441787, 3.303545907139778, 3.25192578881979, 3.2945043449290097, 3.2737413921859115, 3.3306266721338034, 3.4340515160001814, 3.2573772543109953, 3.273454573005438, 3.3211352471262217, 3.201779351104051, 3.1894716238602996, 3.2392486601602286, 3.2132246119435877, 3.356134324101731, 3.463943849084899, 3.308394307969138, 3.494836586061865, 3.455499109113589, 3.3076337007805705, 3.3263283181004226, 3.345432996749878, 3.50738922203891, 3.2856206039432436, 3.4173559970222414, 3.6261598092969507, 3.4132215287536383, 3.3039623650256544, 3.2299290059600025, 3.2112774229608476, 3.2259501279331744, 3.2577801970764995, 3.3040229827165604, 3.576857553096488, 3.677490538917482, 3.4852010109461844, 3.6145062998402864, 3.2117094069253653, 8.14791230019182, 3.2625955210532993, 3.1697899841237813, 3.2602539849467576, 3.2358498938847333, 3.211258504074067, 3.261205821763724, 3.186627957969904, 3.2218053550459445, 3.272527363616973, 3.471292436821386, 3.3153996039181948, 3.3462279026862234, 3.265179652022198, 3.374493729090318, 3.233991115819663, 3.2468185890465975, 3.2655259252060205, 3.4538524460513145, 3.269353474956006, 3.340183208929375, 3.2851159521378577, 3.214678868185729, 3.356329726986587, 3.1490858469624072, 3.2126158489845693, 3.1519846038427204, 3.235612982418388, 3.4044738828670233, 3.2830326587427408, 3.338198150973767, 3.2199549155775458, 3.3278401729185134, 3.248449823819101, 3.2918298980221152, 3.2460253899917006, 3.5527643349487334, 3.2698193721007556, 3.418405947741121, 3.21981944818981, 3.297022166196257, 3.382767924107611, 3.1519894872326404, 3.2293883999809623, 3.1899717862252146, 3.719756396021694, 3.247098029125482, 3.380873978836462, 3.429093053098768, 3.3161656209267676, 3.222802235977724, 7.4162272228859365, 3.204964760923758, 3.224924077047035, 3.27164800115861, 3.251991488272324, 3.2857918539084494, 3.2336075610946864, 3.2026249549817294, 3.2276554480195045, 3.192836263915524, 3.1757040130905807]
Total Epoch List: [73, 111, 83]
Total Time List: [1.0160035521257669, 1.0998023699503392, 1.0032960961107165]
T-times Epoch Time: 3.3879457796896753 ~ 0.04923015376600255
T-times Total Epoch: 80.55555555555556 ~ 5.9773234028056725
T-times Total Time: 1.0445805288489078 ~ 0.024530309686805793
T-times Inference Elapsed: 1.0966167889589238 ~ 0.03292358223973918
T-times Time Per Graph: 0.008521835304140383 ~ 0.0002536541110885148
T-times Speed: 120.3978419125098 ~ 1.8367032235898841
T-times cross validation test micro f1 score:0.9068047028925506 ~ 0.0037022108699735034
T-times cross validation test precision:0.9595741168950628 ~ 0.0044148031408891945
T-times cross validation test recall:0.860176282051282 ~ 0.004384899650904073
T-times cross validation test f1_score:0.9068047028925506 ~ 0.003886855374174076
