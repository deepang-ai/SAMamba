Namespace(seed=15, model='I2BGNNA', dataset='exchange/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dad518a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4961 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.25s
Val loss: 0.6918 score: 0.6434 time: 0.17s
Test loss: 0.6918 score: 0.5891 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.6269;  Loss pred: 0.6269; Loss self: 0.0000; time: 0.25s
Val loss: 0.6912 score: 0.8450 time: 0.17s
Test loss: 0.6910 score: 0.8682 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5039 time: 0.20s
Epoch 18/1000, LR 0.000285
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5039 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6829 score: 0.5039 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6816 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6798 score: 0.5039 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.4889;  Loss pred: 0.4889; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.5039 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.4556;  Loss pred: 0.4556; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6743 score: 0.4961 time: 0.17s
Test loss: 0.6714 score: 0.5116 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 0.4377;  Loss pred: 0.4377; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6689 score: 0.4961 time: 0.17s
Test loss: 0.6653 score: 0.5116 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3951;  Loss pred: 0.3951; Loss self: 0.0000; time: 0.25s
Val loss: 0.6624 score: 0.5116 time: 0.17s
Test loss: 0.6580 score: 0.5504 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 0.3703;  Loss pred: 0.3703; Loss self: 0.0000; time: 0.25s
Val loss: 0.6550 score: 0.5271 time: 0.16s
Test loss: 0.6497 score: 0.5736 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.3349;  Loss pred: 0.3349; Loss self: 0.0000; time: 0.25s
Val loss: 0.6456 score: 0.5426 time: 0.16s
Test loss: 0.6391 score: 0.6047 time: 0.15s
Epoch 28/1000, LR 0.000285
Train loss: 0.2970;  Loss pred: 0.2970; Loss self: 0.0000; time: 0.24s
Val loss: 0.6337 score: 0.5814 time: 0.16s
Test loss: 0.6261 score: 0.6202 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 0.2583;  Loss pred: 0.2583; Loss self: 0.0000; time: 0.25s
Val loss: 0.6185 score: 0.6434 time: 0.16s
Test loss: 0.6096 score: 0.6744 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.2386;  Loss pred: 0.2386; Loss self: 0.0000; time: 0.25s
Val loss: 0.6012 score: 0.7364 time: 0.16s
Test loss: 0.5912 score: 0.7519 time: 0.15s
Epoch 31/1000, LR 0.000285
Train loss: 0.1980;  Loss pred: 0.1980; Loss self: 0.0000; time: 0.24s
Val loss: 0.5803 score: 0.8217 time: 0.16s
Test loss: 0.5687 score: 0.8295 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.25s
Val loss: 0.5561 score: 0.8915 time: 0.16s
Test loss: 0.5431 score: 0.8605 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.25s
Val loss: 0.5290 score: 0.9380 time: 0.18s
Test loss: 0.5147 score: 0.9147 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.1290;  Loss pred: 0.1290; Loss self: 0.0000; time: 0.24s
Val loss: 0.5011 score: 0.9457 time: 0.17s
Test loss: 0.4858 score: 0.9302 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.25s
Val loss: 0.4709 score: 0.9457 time: 0.16s
Test loss: 0.4542 score: 0.9612 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.26s
Val loss: 0.4392 score: 0.9535 time: 0.16s
Test loss: 0.4210 score: 0.9690 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.25s
Val loss: 0.4076 score: 0.9535 time: 0.16s
Test loss: 0.3881 score: 0.9690 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.25s
Val loss: 0.3786 score: 0.9457 time: 0.16s
Test loss: 0.3581 score: 0.9690 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0674;  Loss pred: 0.0674; Loss self: 0.0000; time: 0.24s
Val loss: 0.3509 score: 0.9457 time: 0.17s
Test loss: 0.3300 score: 0.9612 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.24s
Val loss: 0.3258 score: 0.9380 time: 0.18s
Test loss: 0.3046 score: 0.9535 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.25s
Val loss: 0.3000 score: 0.9380 time: 0.16s
Test loss: 0.2776 score: 0.9535 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.26s
Val loss: 0.2780 score: 0.9380 time: 0.17s
Test loss: 0.2527 score: 0.9457 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.26s
Val loss: 0.2612 score: 0.9147 time: 0.18s
Test loss: 0.2325 score: 0.9535 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.25s
Val loss: 0.2467 score: 0.9147 time: 0.16s
Test loss: 0.2132 score: 0.9457 time: 0.15s
Epoch 45/1000, LR 0.000284
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.27s
Val loss: 0.2382 score: 0.9070 time: 0.16s
Test loss: 0.1987 score: 0.9457 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.25s
Val loss: 0.2347 score: 0.9070 time: 0.15s
Test loss: 0.1893 score: 0.9457 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.26s
Val loss: 0.2346 score: 0.9070 time: 0.16s
Test loss: 0.1833 score: 0.9457 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.25s
Val loss: 0.2382 score: 0.8992 time: 0.17s
Test loss: 0.1808 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0274;  Loss pred: 0.0274; Loss self: 0.0000; time: 0.27s
Val loss: 0.2436 score: 0.9070 time: 0.18s
Test loss: 0.1814 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.27s
Val loss: 0.2499 score: 0.9070 time: 0.18s
Test loss: 0.1826 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.26s
Val loss: 0.2585 score: 0.8992 time: 0.17s
Test loss: 0.1854 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.26s
Val loss: 0.2690 score: 0.8992 time: 0.18s
Test loss: 0.1906 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.26s
Val loss: 0.2800 score: 0.8915 time: 0.17s
Test loss: 0.1966 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.25s
Val loss: 0.2942 score: 0.8837 time: 0.16s
Test loss: 0.2046 score: 0.9147 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.26s
Val loss: 0.3086 score: 0.8837 time: 0.16s
Test loss: 0.2129 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.25s
Val loss: 0.3168 score: 0.8915 time: 0.16s
Test loss: 0.2156 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.25s
Val loss: 0.3302 score: 0.8915 time: 0.17s
Test loss: 0.2215 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.25s
Val loss: 0.3418 score: 0.8915 time: 0.17s
Test loss: 0.2265 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.26s
Val loss: 0.3525 score: 0.8915 time: 0.16s
Test loss: 0.2316 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.26s
Val loss: 0.3634 score: 0.8837 time: 0.17s
Test loss: 0.2380 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.25s
Val loss: 0.3643 score: 0.8992 time: 0.16s
Test loss: 0.2368 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.25s
Val loss: 0.3734 score: 0.8915 time: 0.16s
Test loss: 0.2422 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.26s
Val loss: 0.3835 score: 0.8915 time: 0.17s
Test loss: 0.2489 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.26s
Val loss: 0.3933 score: 0.8915 time: 0.16s
Test loss: 0.2558 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.26s
Val loss: 0.4042 score: 0.8915 time: 0.15s
Test loss: 0.2640 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.25s
Val loss: 0.4137 score: 0.8915 time: 0.16s
Test loss: 0.2719 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.25s
Val loss: 0.4237 score: 0.8837 time: 0.17s
Test loss: 0.2808 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0252,   Val_Loss: 0.2346,   Val_Precision: 0.9643,   Val_Recall: 0.8438,   Val_accuracy: 0.9000,   Val_Score: 0.9070,   Val_Loss: 0.2346,   Test_Precision: 0.9833,   Test_Recall: 0.9077,   Test_accuracy: 0.9440,   Test_Score: 0.9457,   Test_loss: 0.1833


[0.17375272582285106, 0.164040912874043, 0.16506622708402574, 0.16820994392037392, 0.16372837009839714, 0.16684471094049513, 0.1709305529948324, 0.1671310910023749, 0.17108822078444064, 0.16365792113356292, 0.17745567997917533, 0.17002212698571384, 0.16345547698438168, 0.16468505514785647, 0.16610238095745444, 0.16672870912589133, 0.20815036306157708, 0.17972877598367631, 0.17334251292049885, 0.17746564908884466, 0.16714274720288813, 0.16495068091899157, 0.16346248309127986, 0.17065504705533385, 0.16935313981957734, 0.16244401596486568, 0.1591314789839089, 0.16478081000968814, 0.16034454014152288, 0.15380261605605483, 0.15261415904387832, 0.17637634393759072, 0.16027590609155595, 0.1603954890742898, 0.16099483310244977, 0.16046846914105117, 0.1686613450292498, 0.17147196014411747, 0.16438747011125088, 0.16933892597444355, 0.1600884289946407, 0.16380216809920967, 0.1696282080374658, 0.15396924503147602, 0.16141464095562696, 0.16110269096679986, 0.16445388598367572, 0.16844192403368652, 0.16714115906506777, 0.1730649450328201, 0.17534713307395577, 0.1649434280116111, 0.16369732096791267, 0.15723742684349418, 0.16103235422633588, 0.16539652808569372, 0.16538360505364835, 0.15805640304461122, 0.1586829989682883, 0.16405561193823814, 0.1659600269049406, 0.1582735620904714, 0.16077718511223793, 0.15796186891384423, 0.1617568931542337, 0.18118152697570622, 0.17313509108498693]
[0.0013469203552158998, 0.0012716349835197132, 0.001279583155690122, 0.0013039530536463095, 0.0012692121713054041, 0.0012933698522519003, 0.001325043046471569, 0.0012955898527315882, 0.0013262652773987646, 0.0012686660552989374, 0.0013756254261951576, 0.0013180009843853785, 0.0012670967208091603, 0.0012766283344795076, 0.0012876153562593367, 0.0012924706133790025, 0.0016135687059036982, 0.0013932463254548552, 0.001343740410236425, 0.001375702706115075, 0.001295680210875102, 0.0012786874489844307, 0.0012671510317153478, 0.001322907341514216, 0.0013128150373610647, 0.001259255937712137, 0.00123357735646441, 0.0012773706202301406, 0.0012429809313296348, 0.0011922683415198049, 0.001183055496464173, 0.0013672584801363622, 0.0012424488844306662, 0.0012433758842968201, 0.0012480219620344944, 0.0012439416212484586, 0.001307452287048448, 0.0013292400011171897, 0.001274321473730627, 0.0013127048525150664, 0.001240995573601866, 0.0012697842488310826, 0.0013149473491276418, 0.00119356003900369, 0.0012512762864777284, 0.0012488580695100763, 0.0012748363254548505, 0.001305751349098345, 0.0012956678997292075, 0.001341588721184652, 0.0013592801013484943, 0.00127863122489621, 0.001268971480371416, 0.001218894781732513, 0.001248312823459968, 0.0012821436285712692, 0.0012820434500282819, 0.0012252434344543505, 0.0012301007671960333, 0.001271748929753784, 0.0012865118364724078, 0.001226926837910631, 0.0012463347683119219, 0.0012245106117352265, 0.0012539294042963852, 0.0014045079610519862, 0.0013421324890309065]
[742.434395714295, 786.38918633092, 781.5045044576775, 766.8987753843206, 787.8903327656279, 773.1740447319761, 754.6924627564971, 771.8492066695535, 753.9969695665437, 788.2294917746252, 726.942073734345, 758.7247747514608, 789.2057358978926, 783.3133363812645, 776.6294453843027, 773.7119820354176, 619.7442949539221, 717.7481696738217, 744.1913574840355, 726.9012378582556, 771.7953794513852, 782.0519398968278, 789.1719100337201, 755.9108401767487, 761.7219269594405, 794.1197417078192, 810.6504182811262, 782.8581495164125, 804.5175712633704, 838.7373590120517, 845.2688846708583, 731.3905998961264, 804.8620852987727, 804.2620197395423, 801.267950741688, 803.8962463498639, 764.8462662125006, 752.3095898103634, 784.7313418273178, 761.7858638094146, 805.8046469074823, 787.5353635238141, 760.4867226535091, 837.8296586024596, 799.1840098040562, 800.7315037747222, 784.4144224892624, 765.8425937606925, 771.8027128780441, 745.3849187976016, 735.6835423456394, 782.0863283556779, 788.0397750998386, 820.4153590506145, 801.0812523965623, 779.9438204238708, 780.0047650319028, 816.1643407992141, 812.9415302125702, 786.3187273871771, 777.2956078989386, 815.0445235210021, 802.3526466764896, 816.652783909257, 797.4930618690834, 711.9931162590158, 745.0829245047596]
Elapsed: 0.16624815117067365~0.007960724364381881
Time per graph: 0.0012887453579121992~6.171104158435568e-05
Speed: 777.5811719685875~34.30369230572554
Total Time: 0.1737
best val loss: 0.23460181266820246 test_score: 0.9457

Testing...
Test loss: 0.4210 score: 0.9690 time: 0.15s
test Score 0.9690
Epoch Time List: [0.8698687679134309, 0.5728614311665297, 0.5763429428916425, 0.5819762637838721, 0.5785611851606518, 0.5774091391358525, 0.5772648637648672, 0.5788874910213053, 0.5904265521094203, 0.5761882408987731, 0.5924225079361349, 0.5924406037665904, 0.5820669201202691, 0.5795034549664706, 0.5792306498624384, 0.5783817030023783, 0.657021258957684, 0.6457256549037993, 0.593439661199227, 0.6167470060754567, 0.5836805491708219, 0.5808351233135909, 0.5797578219790012, 0.5884318521711975, 0.58912547188811, 0.5726542470511049, 0.560629605781287, 0.5663387919776142, 0.5679123636800796, 0.5556874822359532, 0.5498509551398456, 0.5865451567806304, 0.5849451450631022, 0.5662276251241565, 0.5620134931523353, 0.5730549651198089, 0.5690938180778176, 0.5724772112444043, 0.5694164759479463, 0.5864177430048585, 0.568198611959815, 0.5931966488715261, 0.6068551640491933, 0.5599356517195702, 0.5898738116957247, 0.5563114790711552, 0.5816784780472517, 0.5830242950469255, 0.6080789237748832, 0.6124434571247548, 0.6088476541917771, 0.5964665350038558, 0.5804768810048699, 0.5696809489745647, 0.5772845658939332, 0.5776450862176716, 0.5805625391658396, 0.5723252412863076, 0.575481137027964, 0.5822376699652523, 0.5686098581645638, 0.565892512910068, 0.592010980239138, 0.5733976580668241, 0.5686191020067781, 0.5865352461114526, 0.5849925049114972]
Total Epoch List: [67]
Total Time List: [0.1736806749831885]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dad51d20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.27s
Epoch 10/1000, LR 0.000255
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5039 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5039 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5584;  Loss pred: 0.5584; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.5039 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5094;  Loss pred: 0.5094; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6817 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5039 time: 0.18s
Epoch 19/1000, LR 0.000285
Train loss: 0.4815;  Loss pred: 0.4815; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4628;  Loss pred: 0.4628; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6724 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.5039 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4305;  Loss pred: 0.4305; Loss self: 0.0000; time: 0.25s
Val loss: 0.6656 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.5039 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4067;  Loss pred: 0.4067; Loss self: 0.0000; time: 0.25s
Val loss: 0.6574 score: 0.5349 time: 0.17s
Test loss: 0.6648 score: 0.5194 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3765;  Loss pred: 0.3765; Loss self: 0.0000; time: 0.25s
Val loss: 0.6475 score: 0.6047 time: 0.17s
Test loss: 0.6568 score: 0.5504 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3454;  Loss pred: 0.3454; Loss self: 0.0000; time: 0.25s
Val loss: 0.6361 score: 0.6977 time: 0.17s
Test loss: 0.6473 score: 0.6047 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.24s
Val loss: 0.6233 score: 0.7597 time: 0.18s
Test loss: 0.6365 score: 0.6589 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3029;  Loss pred: 0.3029; Loss self: 0.0000; time: 0.24s
Val loss: 0.6089 score: 0.8140 time: 0.18s
Test loss: 0.6243 score: 0.7054 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2943;  Loss pred: 0.2943; Loss self: 0.0000; time: 0.24s
Val loss: 0.5926 score: 0.8372 time: 0.16s
Test loss: 0.6104 score: 0.7984 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.23s
Val loss: 0.5747 score: 0.8682 time: 0.16s
Test loss: 0.5948 score: 0.8062 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2376;  Loss pred: 0.2376; Loss self: 0.0000; time: 0.24s
Val loss: 0.5551 score: 0.8915 time: 0.17s
Test loss: 0.5775 score: 0.8372 time: 0.18s
Epoch 30/1000, LR 0.000285
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.23s
Val loss: 0.5325 score: 0.9070 time: 0.18s
Test loss: 0.5577 score: 0.8527 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1972;  Loss pred: 0.1972; Loss self: 0.0000; time: 0.22s
Val loss: 0.5090 score: 0.9070 time: 0.17s
Test loss: 0.5366 score: 0.8837 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.24s
Val loss: 0.4835 score: 0.9070 time: 0.20s
Test loss: 0.5134 score: 0.8915 time: 0.19s
Epoch 33/1000, LR 0.000285
Train loss: 0.1683;  Loss pred: 0.1683; Loss self: 0.0000; time: 0.24s
Val loss: 0.4567 score: 0.9070 time: 0.17s
Test loss: 0.4889 score: 0.8915 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 0.24s
Val loss: 0.4289 score: 0.9070 time: 0.17s
Test loss: 0.4633 score: 0.8992 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 0.24s
Val loss: 0.4030 score: 0.9225 time: 0.17s
Test loss: 0.4389 score: 0.9070 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.24s
Val loss: 0.3784 score: 0.9225 time: 0.17s
Test loss: 0.4154 score: 0.9147 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.1166;  Loss pred: 0.1166; Loss self: 0.0000; time: 0.23s
Val loss: 0.3541 score: 0.9147 time: 0.17s
Test loss: 0.3921 score: 0.9225 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.1057;  Loss pred: 0.1057; Loss self: 0.0000; time: 0.23s
Val loss: 0.3305 score: 0.9147 time: 0.18s
Test loss: 0.3700 score: 0.9225 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.24s
Val loss: 0.3080 score: 0.9147 time: 0.17s
Test loss: 0.3491 score: 0.9302 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.24s
Val loss: 0.2887 score: 0.9225 time: 0.16s
Test loss: 0.3312 score: 0.9302 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0876;  Loss pred: 0.0876; Loss self: 0.0000; time: 0.24s
Val loss: 0.2697 score: 0.9225 time: 0.16s
Test loss: 0.3149 score: 0.9225 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0766;  Loss pred: 0.0766; Loss self: 0.0000; time: 0.24s
Val loss: 0.2526 score: 0.9225 time: 0.17s
Test loss: 0.3006 score: 0.9147 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.23s
Val loss: 0.2410 score: 0.9302 time: 0.17s
Test loss: 0.2907 score: 0.9225 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.24s
Val loss: 0.2319 score: 0.9380 time: 0.19s
Test loss: 0.2819 score: 0.9302 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.25s
Val loss: 0.2234 score: 0.9380 time: 0.18s
Test loss: 0.2744 score: 0.9302 time: 0.20s
Epoch 46/1000, LR 0.000284
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.26s
Val loss: 0.2170 score: 0.9380 time: 0.17s
Test loss: 0.2693 score: 0.9225 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.25s
Val loss: 0.2148 score: 0.9457 time: 0.18s
Test loss: 0.2669 score: 0.9225 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.25s
Val loss: 0.2101 score: 0.9457 time: 0.17s
Test loss: 0.2653 score: 0.9225 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.25s
Val loss: 0.2067 score: 0.9380 time: 0.20s
Test loss: 0.2657 score: 0.9225 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.23s
Val loss: 0.2039 score: 0.9302 time: 0.17s
Test loss: 0.2674 score: 0.9147 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.29s
Val loss: 0.2048 score: 0.9302 time: 0.17s
Test loss: 0.2739 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.25s
Val loss: 0.2080 score: 0.9302 time: 0.17s
Test loss: 0.2812 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.24s
Val loss: 0.2144 score: 0.9302 time: 0.17s
Test loss: 0.2919 score: 0.9070 time: 0.31s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.25s
Val loss: 0.2220 score: 0.9302 time: 0.17s
Test loss: 0.2998 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.24s
Val loss: 0.2336 score: 0.9225 time: 0.17s
Test loss: 0.3194 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.24s
Val loss: 0.2396 score: 0.9225 time: 0.17s
Test loss: 0.3242 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.24s
Val loss: 0.2497 score: 0.9225 time: 0.17s
Test loss: 0.3374 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.24s
Val loss: 0.2595 score: 0.9147 time: 0.17s
Test loss: 0.3518 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.25s
Val loss: 0.2709 score: 0.9147 time: 0.17s
Test loss: 0.3692 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.25s
Val loss: 0.2874 score: 0.9147 time: 0.17s
Test loss: 0.3954 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.25s
Val loss: 0.2982 score: 0.9070 time: 0.17s
Test loss: 0.4112 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.25s
Val loss: 0.3065 score: 0.9070 time: 0.17s
Test loss: 0.4256 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.25s
Val loss: 0.3120 score: 0.9147 time: 0.17s
Test loss: 0.4318 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.25s
Val loss: 0.3201 score: 0.9147 time: 0.17s
Test loss: 0.4435 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.25s
Val loss: 0.3287 score: 0.9147 time: 0.17s
Test loss: 0.4559 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.25s
Val loss: 0.3284 score: 0.9147 time: 0.17s
Test loss: 0.4552 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.25s
Val loss: 0.3287 score: 0.9147 time: 0.17s
Test loss: 0.4543 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.25s
Val loss: 0.3245 score: 0.9147 time: 0.17s
Test loss: 0.4440 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.24s
Val loss: 0.3216 score: 0.9147 time: 0.17s
Test loss: 0.4356 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.24s
Val loss: 0.3284 score: 0.9147 time: 0.16s
Test loss: 0.4445 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0498,   Val_Loss: 0.2039,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2039,   Test_Precision: 0.9818,   Test_Recall: 0.8438,   Test_accuracy: 0.9076,   Test_Score: 0.9147,   Test_loss: 0.2674


[0.17375272582285106, 0.164040912874043, 0.16506622708402574, 0.16820994392037392, 0.16372837009839714, 0.16684471094049513, 0.1709305529948324, 0.1671310910023749, 0.17108822078444064, 0.16365792113356292, 0.17745567997917533, 0.17002212698571384, 0.16345547698438168, 0.16468505514785647, 0.16610238095745444, 0.16672870912589133, 0.20815036306157708, 0.17972877598367631, 0.17334251292049885, 0.17746564908884466, 0.16714274720288813, 0.16495068091899157, 0.16346248309127986, 0.17065504705533385, 0.16935313981957734, 0.16244401596486568, 0.1591314789839089, 0.16478081000968814, 0.16034454014152288, 0.15380261605605483, 0.15261415904387832, 0.17637634393759072, 0.16027590609155595, 0.1603954890742898, 0.16099483310244977, 0.16046846914105117, 0.1686613450292498, 0.17147196014411747, 0.16438747011125088, 0.16933892597444355, 0.1600884289946407, 0.16380216809920967, 0.1696282080374658, 0.15396924503147602, 0.16141464095562696, 0.16110269096679986, 0.16445388598367572, 0.16844192403368652, 0.16714115906506777, 0.1730649450328201, 0.17534713307395577, 0.1649434280116111, 0.16369732096791267, 0.15723742684349418, 0.16103235422633588, 0.16539652808569372, 0.16538360505364835, 0.15805640304461122, 0.1586829989682883, 0.16405561193823814, 0.1659600269049406, 0.1582735620904714, 0.16077718511223793, 0.15796186891384423, 0.1617568931542337, 0.18118152697570622, 0.17313509108498693, 0.1676090930122882, 0.18740943493321538, 0.17847704095765948, 0.16820390499196947, 0.16882890998385847, 0.1672290898859501, 0.1815212108194828, 0.18264004588127136, 0.2771890698932111, 0.17325325682759285, 0.17102143191732466, 0.1651360639370978, 0.16997520485892892, 0.17265822901390493, 0.16815042309463024, 0.1666759999934584, 0.16810445999726653, 0.18043494992889464, 0.1782993588130921, 0.17511372501030564, 0.1746020431164652, 0.17715847096405923, 0.18292943900451064, 0.18177708704024553, 0.17806575703434646, 0.172691771062091, 0.16621834901161492, 0.1727059909608215, 0.1799866990186274, 0.17096704011783004, 0.16947899921797216, 0.1984959370456636, 0.16698498907499015, 0.1726553679909557, 0.18340558279305696, 0.1687190248630941, 0.16816566418856382, 0.16753067704848945, 0.16737772105261683, 0.17077329684980214, 0.17269602697342634, 0.17253062105737627, 0.168386974837631, 0.17798391403630376, 0.20285345008596778, 0.17357717198319733, 0.173806122969836, 0.17202872107736766, 0.18572724610567093, 0.1739707610104233, 0.16891120583750308, 0.17166458093561232, 0.3114665849134326, 0.16881073708646, 0.17042022105306387, 0.17092619999311864, 0.16878112009726465, 0.16791759198531508, 0.16903029708191752, 0.17402576003223658, 0.17046015709638596, 0.16940060281194746, 0.17132847290486097, 0.17639535712078214, 0.17549492209218442, 0.17362706689164042, 0.1729179120156914, 0.17214276199229062, 0.16807093704119325, 0.17074410105124116]
[0.0013469203552158998, 0.0012716349835197132, 0.001279583155690122, 0.0013039530536463095, 0.0012692121713054041, 0.0012933698522519003, 0.001325043046471569, 0.0012955898527315882, 0.0013262652773987646, 0.0012686660552989374, 0.0013756254261951576, 0.0013180009843853785, 0.0012670967208091603, 0.0012766283344795076, 0.0012876153562593367, 0.0012924706133790025, 0.0016135687059036982, 0.0013932463254548552, 0.001343740410236425, 0.001375702706115075, 0.001295680210875102, 0.0012786874489844307, 0.0012671510317153478, 0.001322907341514216, 0.0013128150373610647, 0.001259255937712137, 0.00123357735646441, 0.0012773706202301406, 0.0012429809313296348, 0.0011922683415198049, 0.001183055496464173, 0.0013672584801363622, 0.0012424488844306662, 0.0012433758842968201, 0.0012480219620344944, 0.0012439416212484586, 0.001307452287048448, 0.0013292400011171897, 0.001274321473730627, 0.0013127048525150664, 0.001240995573601866, 0.0012697842488310826, 0.0013149473491276418, 0.00119356003900369, 0.0012512762864777284, 0.0012488580695100763, 0.0012748363254548505, 0.001305751349098345, 0.0012956678997292075, 0.001341588721184652, 0.0013592801013484943, 0.00127863122489621, 0.001268971480371416, 0.001218894781732513, 0.001248312823459968, 0.0012821436285712692, 0.0012820434500282819, 0.0012252434344543505, 0.0012301007671960333, 0.001271748929753784, 0.0012865118364724078, 0.001226926837910631, 0.0012463347683119219, 0.0012245106117352265, 0.0012539294042963852, 0.0014045079610519862, 0.0013421324890309065, 0.0012992952946689008, 0.0014527863173117472, 0.001383542953160151, 0.0013039062402478254, 0.0013087512401849493, 0.001296349533999613, 0.0014071411691432775, 0.0014158143091571424, 0.002148752479792334, 0.0013430485025394795, 0.0013257475342428268, 0.001280124526644169, 0.0013176372469684413, 0.0013384358838287203, 0.0013034916518963584, 0.0012920620154531657, 0.0013031353488160197, 0.0013987205420844546, 0.0013821655721945126, 0.0013574707365139973, 0.0013535042102051566, 0.0013733214803415444, 0.0014180576667016328, 0.0014091247057383374, 0.0013803547056926082, 0.0013386958997061316, 0.0012885143334233715, 0.0013388061314792365, 0.001395245728826569, 0.0013253258923862793, 0.0013137906916121872, 0.001538728194152431, 0.0012944572796510865, 0.0013384137053562457, 0.001421748703822147, 0.001307899417543365, 0.0013036097999113475, 0.0012986874189805384, 0.001297501713586177, 0.0013238240065876134, 0.0013387288912668708, 0.001337446674863382, 0.0013053253863382246, 0.001379720263847316, 0.0015725073650075022, 0.0013455594727379639, 0.001347334286587876, 0.0013335559773439353, 0.0014397460938424104, 0.0013486105504683977, 0.0013093891925387835, 0.0013307331855473823, 0.0024144696504917254, 0.0013086103650113178, 0.001321086984907472, 0.0013250093022722374, 0.001308380775947788, 0.0013016867595760859, 0.0013103123804799807, 0.001349036899474702, 0.0013213965666386508, 0.0013131829675344766, 0.0013281276969369068, 0.00136740586915335, 0.0013604257526525923, 0.0013459462549739569, 0.0013404489303541968, 0.0013344400154441133, 0.0013028754809394825, 0.001323597682567761]
[742.434395714295, 786.38918633092, 781.5045044576775, 766.8987753843206, 787.8903327656279, 773.1740447319761, 754.6924627564971, 771.8492066695535, 753.9969695665437, 788.2294917746252, 726.942073734345, 758.7247747514608, 789.2057358978926, 783.3133363812645, 776.6294453843027, 773.7119820354176, 619.7442949539221, 717.7481696738217, 744.1913574840355, 726.9012378582556, 771.7953794513852, 782.0519398968278, 789.1719100337201, 755.9108401767487, 761.7219269594405, 794.1197417078192, 810.6504182811262, 782.8581495164125, 804.5175712633704, 838.7373590120517, 845.2688846708583, 731.3905998961264, 804.8620852987727, 804.2620197395423, 801.267950741688, 803.8962463498639, 764.8462662125006, 752.3095898103634, 784.7313418273178, 761.7858638094146, 805.8046469074823, 787.5353635238141, 760.4867226535091, 837.8296586024596, 799.1840098040562, 800.7315037747222, 784.4144224892624, 765.8425937606925, 771.8027128780441, 745.3849187976016, 735.6835423456394, 782.0863283556779, 788.0397750998386, 820.4153590506145, 801.0812523965623, 779.9438204238708, 780.0047650319028, 816.1643407992141, 812.9415302125702, 786.3187273871771, 777.2956078989386, 815.0445235210021, 802.3526466764896, 816.652783909257, 797.4930618690834, 711.9931162590158, 745.0829245047596, 769.647980796259, 688.3324740078856, 722.7820413640933, 766.926308911549, 764.0871460482304, 771.3968908637711, 710.660750981253, 706.3073127120156, 465.3863157363964, 744.5747477542083, 754.2914274180636, 781.1740023616984, 758.9342228301102, 747.1407574185824, 767.1702373737263, 773.9566584575039, 767.3799969500965, 714.9390960611345, 723.5023213696932, 736.6641306522844, 738.8229696370324, 728.1616244372005, 705.189939366835, 709.6603983506421, 724.4514731438097, 746.9956397263325, 776.0876026447946, 746.934135187376, 716.7196281912442, 754.5313992164428, 761.1562529590415, 649.8873574944953, 772.5245287890415, 747.153138075368, 703.359178251162, 764.5847888504345, 767.1007076412017, 770.0082293743893, 770.7118915751493, 755.3874193426012, 746.9772308071101, 747.6933613836592, 766.0925087845404, 724.7846003301602, 635.9270692479263, 743.1852848281657, 742.2063031829317, 749.8747836530387, 694.5669130667262, 741.5039127883741, 763.7148723223331, 751.4654409017846, 414.16962925847594, 764.169401937567, 756.9524273755823, 754.7116826162019, 764.3034951163986, 768.2339799827612, 763.1767927230374, 741.2695682300368, 756.7750857290217, 761.5085062194472, 752.9396475251019, 731.3117652618861, 735.0640033461398, 742.9717169645451, 746.0187235449265, 749.3780075736048, 767.5330564045277, 755.5165842085897]
Elapsed: 0.1717762375168883~0.017171478884168815
Time per graph: 0.0013315987404409948~0.00013311223941216133
Speed: 756.0337080257892~52.16690962031029
Total Time: 0.1714
best val loss: 0.2039468512372222 test_score: 0.9147

Testing...
Test loss: 0.2669 score: 0.9225 time: 0.17s
test Score 0.9225
Epoch Time List: [0.8698687679134309, 0.5728614311665297, 0.5763429428916425, 0.5819762637838721, 0.5785611851606518, 0.5774091391358525, 0.5772648637648672, 0.5788874910213053, 0.5904265521094203, 0.5761882408987731, 0.5924225079361349, 0.5924406037665904, 0.5820669201202691, 0.5795034549664706, 0.5792306498624384, 0.5783817030023783, 0.657021258957684, 0.6457256549037993, 0.593439661199227, 0.6167470060754567, 0.5836805491708219, 0.5808351233135909, 0.5797578219790012, 0.5884318521711975, 0.58912547188811, 0.5726542470511049, 0.560629605781287, 0.5663387919776142, 0.5679123636800796, 0.5556874822359532, 0.5498509551398456, 0.5865451567806304, 0.5849451450631022, 0.5662276251241565, 0.5620134931523353, 0.5730549651198089, 0.5690938180778176, 0.5724772112444043, 0.5694164759479463, 0.5864177430048585, 0.568198611959815, 0.5931966488715261, 0.6068551640491933, 0.5599356517195702, 0.5898738116957247, 0.5563114790711552, 0.5816784780472517, 0.5830242950469255, 0.6080789237748832, 0.6124434571247548, 0.6088476541917771, 0.5964665350038558, 0.5804768810048699, 0.5696809489745647, 0.5772845658939332, 0.5776450862176716, 0.5805625391658396, 0.5723252412863076, 0.575481137027964, 0.5822376699652523, 0.5686098581645638, 0.565892512910068, 0.592010980239138, 0.5733976580668241, 0.5686191020067781, 0.5865352461114526, 0.5849925049114972, 0.5661427511367947, 0.613021926023066, 0.5889826561324298, 0.5682347030378878, 0.5683667999692261, 0.5655624088831246, 0.5883356840349734, 0.5940116429701447, 0.6963257188908756, 0.5763749866746366, 0.5750432109925896, 0.6578238578513265, 0.5663884219247848, 0.5715601581614465, 0.63470059633255, 0.5636599028948694, 0.5683734850026667, 0.6342839279677719, 0.5918467370793223, 0.5986931547522545, 0.589085411047563, 0.5924933841452003, 0.5985625684261322, 0.5978752160444856, 0.5923515630420297, 0.5870772891212255, 0.560741594992578, 0.5584606460761279, 0.5865811619441956, 0.5778094520792365, 0.5526586452033371, 0.637524263933301, 0.5676238283049315, 0.5777220230083913, 0.5844198928680271, 0.5690428700763732, 0.5639482806436718, 0.5803022019099444, 0.5693636848591268, 0.5712903349194676, 0.5693181890528649, 0.5741904571186751, 0.5670640890020877, 0.5995784259866923, 0.6239002060610801, 0.6038739446084946, 0.5985969798639417, 0.5863928962498903, 0.6252307479735464, 0.5757407848723233, 0.6234181642066687, 0.584130119998008, 0.7116716972086579, 0.5751105623785406, 0.5729031800292432, 0.5727907042019069, 0.5757865458726883, 0.5761460969224572, 0.576797764049843, 0.5801102961413562, 0.5836858961265534, 0.582155582960695, 0.5824886218179017, 0.5880475107114762, 0.5892127589322627, 0.5898311049677432, 0.5869644731283188, 0.5856689650099725, 0.5726735407952219, 0.5707564449403435]
Total Epoch List: [67, 70]
Total Time List: [0.1736806749831885, 0.1714040080551058]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dad0ca00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.16s
Epoch 2/1000, LR 0.000020
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.14s
Epoch 4/1000, LR 0.000080
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.14s
Epoch 5/1000, LR 0.000110
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.14s
Epoch 6/1000, LR 0.000140
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.16s
Epoch 7/1000, LR 0.000170
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.15s
Epoch 8/1000, LR 0.000200
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.14s
Epoch 9/1000, LR 0.000230
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.14s
Epoch 10/1000, LR 0.000260
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.14s
Epoch 11/1000, LR 0.000290
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6523;  Loss pred: 0.6523; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.14s
Epoch 18/1000, LR 0.000290
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.14s
Epoch 19/1000, LR 0.000290
Train loss: 0.6000;  Loss pred: 0.6000; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.14s
Epoch 20/1000, LR 0.000290
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.5000 time: 0.14s
Epoch 21/1000, LR 0.000290
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.5000 time: 0.15s
Epoch 22/1000, LR 0.000290
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6696 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6736 score: 0.5000 time: 0.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.24s
Val loss: 0.6592 score: 0.5194 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6640 score: 0.5000 time: 0.15s
Epoch 24/1000, LR 0.000290
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.23s
Val loss: 0.6465 score: 0.5349 time: 0.15s
Test loss: 0.6523 score: 0.5234 time: 0.14s
Epoch 25/1000, LR 0.000290
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.23s
Val loss: 0.6297 score: 0.5814 time: 0.15s
Test loss: 0.6369 score: 0.5547 time: 0.14s
Epoch 26/1000, LR 0.000290
Train loss: 0.5014;  Loss pred: 0.5014; Loss self: 0.0000; time: 0.23s
Val loss: 0.6098 score: 0.6434 time: 0.15s
Test loss: 0.6186 score: 0.5938 time: 0.15s
Epoch 27/1000, LR 0.000290
Train loss: 0.4901;  Loss pred: 0.4901; Loss self: 0.0000; time: 0.23s
Val loss: 0.5897 score: 0.7442 time: 0.15s
Test loss: 0.5997 score: 0.6797 time: 0.14s
Epoch 28/1000, LR 0.000290
Train loss: 0.4697;  Loss pred: 0.4697; Loss self: 0.0000; time: 0.23s
Val loss: 0.5669 score: 0.8992 time: 0.17s
Test loss: 0.5782 score: 0.8438 time: 0.14s
Epoch 29/1000, LR 0.000290
Train loss: 0.4600;  Loss pred: 0.4600; Loss self: 0.0000; time: 0.23s
Val loss: 0.5427 score: 0.9302 time: 0.15s
Test loss: 0.5554 score: 0.9062 time: 0.14s
Epoch 30/1000, LR 0.000290
Train loss: 0.4413;  Loss pred: 0.4413; Loss self: 0.0000; time: 0.25s
Val loss: 0.5119 score: 0.9302 time: 0.15s
Test loss: 0.5276 score: 0.9297 time: 0.14s
Epoch 31/1000, LR 0.000290
Train loss: 0.4118;  Loss pred: 0.4118; Loss self: 0.0000; time: 0.23s
Val loss: 0.4840 score: 0.9457 time: 0.15s
Test loss: 0.5027 score: 0.9062 time: 0.14s
Epoch 32/1000, LR 0.000290
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 0.23s
Val loss: 0.4561 score: 0.9535 time: 0.15s
Test loss: 0.4784 score: 0.8906 time: 0.14s
Epoch 33/1000, LR 0.000290
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 0.23s
Val loss: 0.4260 score: 0.9380 time: 0.15s
Test loss: 0.4525 score: 0.8828 time: 0.14s
Epoch 34/1000, LR 0.000290
Train loss: 0.3342;  Loss pred: 0.3342; Loss self: 0.0000; time: 0.23s
Val loss: 0.3924 score: 0.9457 time: 0.17s
Test loss: 0.4232 score: 0.8906 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.3196;  Loss pred: 0.3196; Loss self: 0.0000; time: 0.23s
Val loss: 0.3551 score: 0.9380 time: 0.15s
Test loss: 0.3910 score: 0.9062 time: 0.24s
Epoch 36/1000, LR 0.000290
Train loss: 0.2926;  Loss pred: 0.2926; Loss self: 0.0000; time: 0.23s
Val loss: 0.3272 score: 0.9225 time: 0.16s
Test loss: 0.3672 score: 0.9141 time: 0.14s
Epoch 37/1000, LR 0.000290
Train loss: 0.2856;  Loss pred: 0.2856; Loss self: 0.0000; time: 0.22s
Val loss: 0.3069 score: 0.9302 time: 0.15s
Test loss: 0.3496 score: 0.9141 time: 0.14s
Epoch 38/1000, LR 0.000289
Train loss: 0.2587;  Loss pred: 0.2587; Loss self: 0.0000; time: 0.25s
Val loss: 0.2909 score: 0.9380 time: 0.15s
Test loss: 0.3364 score: 0.9062 time: 0.14s
Epoch 39/1000, LR 0.000289
Train loss: 0.2387;  Loss pred: 0.2387; Loss self: 0.0000; time: 0.29s
Val loss: 0.2828 score: 0.9457 time: 0.15s
Test loss: 0.3311 score: 0.8984 time: 0.14s
Epoch 40/1000, LR 0.000289
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 0.23s
Val loss: 0.2643 score: 0.9380 time: 0.15s
Test loss: 0.3148 score: 0.9062 time: 0.14s
Epoch 41/1000, LR 0.000289
Train loss: 0.2010;  Loss pred: 0.2010; Loss self: 0.0000; time: 0.25s
Val loss: 0.2499 score: 0.9380 time: 0.15s
Test loss: 0.3027 score: 0.9062 time: 0.15s
Epoch 42/1000, LR 0.000289
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.40s
Val loss: 0.2317 score: 0.9380 time: 0.16s
Test loss: 0.2860 score: 0.9062 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 0.23s
Val loss: 0.2262 score: 0.9225 time: 0.18s
Test loss: 0.2811 score: 0.9141 time: 0.15s
Epoch 44/1000, LR 0.000289
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.25s
Val loss: 0.2241 score: 0.9380 time: 0.17s
Test loss: 0.2823 score: 0.9062 time: 0.16s
Epoch 45/1000, LR 0.000289
Train loss: 0.1530;  Loss pred: 0.1530; Loss self: 0.0000; time: 0.28s
Val loss: 0.2115 score: 0.9380 time: 0.16s
Test loss: 0.2706 score: 0.9141 time: 0.15s
Epoch 46/1000, LR 0.000289
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.22s
Val loss: 0.2058 score: 0.9380 time: 0.14s
Test loss: 0.2641 score: 0.9062 time: 0.14s
Epoch 47/1000, LR 0.000289
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.24s
Val loss: 0.2088 score: 0.9380 time: 0.15s
Test loss: 0.2655 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1421;  Loss pred: 0.1421; Loss self: 0.0000; time: 0.22s
Val loss: 0.2016 score: 0.9302 time: 0.16s
Test loss: 0.2604 score: 0.9141 time: 0.15s
Epoch 49/1000, LR 0.000289
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.23s
Val loss: 0.1962 score: 0.9535 time: 0.15s
Test loss: 0.2598 score: 0.9062 time: 0.15s
Epoch 50/1000, LR 0.000289
Train loss: 0.1264;  Loss pred: 0.1264; Loss self: 0.0000; time: 0.24s
Val loss: 0.1942 score: 0.9535 time: 0.16s
Test loss: 0.2589 score: 0.9062 time: 0.15s
Epoch 51/1000, LR 0.000289
Train loss: 0.1205;  Loss pred: 0.1205; Loss self: 0.0000; time: 0.25s
Val loss: 0.2139 score: 0.9380 time: 0.16s
Test loss: 0.2894 score: 0.8594 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.24s
Val loss: 0.2036 score: 0.9380 time: 0.15s
Test loss: 0.2798 score: 0.8672 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.23s
Val loss: 0.2163 score: 0.9302 time: 0.15s
Test loss: 0.3008 score: 0.8594 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1176;  Loss pred: 0.1176; Loss self: 0.0000; time: 0.23s
Val loss: 0.1957 score: 0.9380 time: 0.15s
Test loss: 0.2707 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.23s
Val loss: 0.1896 score: 0.9457 time: 0.15s
Test loss: 0.2617 score: 0.8828 time: 0.14s
Epoch 56/1000, LR 0.000289
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.23s
Val loss: 0.1907 score: 0.9380 time: 0.15s
Test loss: 0.2670 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.23s
Val loss: 0.2098 score: 0.9302 time: 0.16s
Test loss: 0.2992 score: 0.8672 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 0.24s
Val loss: 0.2181 score: 0.9225 time: 0.16s
Test loss: 0.3162 score: 0.8672 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.24s
Val loss: 0.2119 score: 0.9225 time: 0.16s
Test loss: 0.3088 score: 0.8672 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.24s
Val loss: 0.2037 score: 0.9225 time: 0.16s
Test loss: 0.2972 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.24s
Val loss: 0.1938 score: 0.9380 time: 0.16s
Test loss: 0.2768 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.24s
Val loss: 0.1904 score: 0.9457 time: 0.16s
Test loss: 0.2738 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0768;  Loss pred: 0.0768; Loss self: 0.0000; time: 0.23s
Val loss: 0.1852 score: 0.9457 time: 0.16s
Test loss: 0.2666 score: 0.8906 time: 0.15s
Epoch 64/1000, LR 0.000288
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.24s
Val loss: 0.1747 score: 0.9302 time: 0.16s
Test loss: 0.2358 score: 0.9062 time: 0.15s
Epoch 65/1000, LR 0.000288
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.24s
Val loss: 0.1767 score: 0.9457 time: 0.16s
Test loss: 0.2469 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.24s
Val loss: 0.1719 score: 0.9302 time: 0.16s
Test loss: 0.2327 score: 0.9062 time: 0.16s
Epoch 67/1000, LR 0.000288
Train loss: 0.0628;  Loss pred: 0.0628; Loss self: 0.0000; time: 0.24s
Val loss: 0.1702 score: 0.9302 time: 0.16s
Test loss: 0.2285 score: 0.8984 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.24s
Val loss: 0.1688 score: 0.9380 time: 0.16s
Test loss: 0.2275 score: 0.8984 time: 0.15s
Epoch 69/1000, LR 0.000288
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.24s
Val loss: 0.1664 score: 0.9380 time: 0.16s
Test loss: 0.2358 score: 0.8984 time: 0.15s
Epoch 70/1000, LR 0.000287
Train loss: 0.0668;  Loss pred: 0.0668; Loss self: 0.0000; time: 0.24s
Val loss: 0.1720 score: 0.9380 time: 0.16s
Test loss: 0.2214 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.24s
Val loss: 0.1702 score: 0.9380 time: 0.16s
Test loss: 0.2525 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.25s
Val loss: 0.1851 score: 0.9457 time: 0.16s
Test loss: 0.3033 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0693;  Loss pred: 0.0693; Loss self: 0.0000; time: 0.26s
Val loss: 0.1752 score: 0.9457 time: 0.16s
Test loss: 0.2756 score: 0.8828 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.24s
Val loss: 0.1752 score: 0.9457 time: 0.16s
Test loss: 0.2782 score: 0.8828 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.24s
Val loss: 0.1736 score: 0.9457 time: 0.16s
Test loss: 0.2632 score: 0.8906 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.24s
Val loss: 0.1842 score: 0.9457 time: 0.15s
Test loss: 0.3009 score: 0.8672 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0509;  Loss pred: 0.0509; Loss self: 0.0000; time: 0.23s
Val loss: 0.1679 score: 0.9380 time: 0.15s
Test loss: 0.2505 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.23s
Val loss: 0.1760 score: 0.9457 time: 0.15s
Test loss: 0.2777 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 9 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.23s
Val loss: 0.1676 score: 0.9457 time: 0.15s
Test loss: 0.2560 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.23s
Val loss: 0.1635 score: 0.9380 time: 0.15s
Test loss: 0.2442 score: 0.8750 time: 0.14s
Epoch 81/1000, LR 0.000286
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 0.23s
Val loss: 0.1635 score: 0.9302 time: 0.15s
Test loss: 0.2256 score: 0.9062 time: 0.14s
Epoch 82/1000, LR 0.000286
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.23s
Val loss: 0.1634 score: 0.9302 time: 0.15s
Test loss: 0.2253 score: 0.9062 time: 0.14s
Epoch 83/1000, LR 0.000286
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.23s
Val loss: 0.1620 score: 0.9302 time: 0.15s
Test loss: 0.2336 score: 0.9062 time: 0.14s
Epoch 84/1000, LR 0.000286
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.23s
Val loss: 0.1691 score: 0.9457 time: 0.15s
Test loss: 0.2617 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.23s
Val loss: 0.1652 score: 0.9457 time: 0.15s
Test loss: 0.2377 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0443;  Loss pred: 0.0443; Loss self: 0.0000; time: 0.23s
Val loss: 0.1711 score: 0.9457 time: 0.15s
Test loss: 0.2557 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.23s
Val loss: 0.1826 score: 0.9457 time: 0.15s
Test loss: 0.2846 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.23s
Val loss: 0.1700 score: 0.9380 time: 0.15s
Test loss: 0.2503 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.23s
Val loss: 0.1821 score: 0.9457 time: 0.15s
Test loss: 0.2880 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.23s
Val loss: 0.1943 score: 0.9457 time: 0.15s
Test loss: 0.3201 score: 0.8828 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.23s
Val loss: 0.2000 score: 0.9302 time: 0.15s
Test loss: 0.3326 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.23s
Val loss: 0.2080 score: 0.9302 time: 0.15s
Test loss: 0.3528 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 9 of 20
Epoch 93/1000, LR 0.000285
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.23s
Val loss: 0.2372 score: 0.9147 time: 0.15s
Test loss: 0.4158 score: 0.8672 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 94/1000, LR 0.000285
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.23s
Val loss: 0.2469 score: 0.9147 time: 0.15s
Test loss: 0.4343 score: 0.8672 time: 0.14s
     INFO: Early stopping counter 11 of 20
Epoch 95/1000, LR 0.000285
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.25s
Val loss: 0.2048 score: 0.9380 time: 0.15s
Test loss: 0.3405 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 96/1000, LR 0.000285
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.23s
Val loss: 0.1844 score: 0.9457 time: 0.15s
Test loss: 0.2824 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 97/1000, LR 0.000285
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.23s
Val loss: 0.2120 score: 0.9302 time: 0.15s
Test loss: 0.3611 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 14 of 20
Epoch 98/1000, LR 0.000285
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.23s
Val loss: 0.2007 score: 0.9457 time: 0.15s
Test loss: 0.3259 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 15 of 20
Epoch 99/1000, LR 0.000284
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.23s
Val loss: 0.1916 score: 0.9457 time: 0.15s
Test loss: 0.3020 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 16 of 20
Epoch 100/1000, LR 0.000284
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.25s
Val loss: 0.1846 score: 0.9457 time: 0.15s
Test loss: 0.2746 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 101/1000, LR 0.000284
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.23s
Val loss: 0.2024 score: 0.9457 time: 0.15s
Test loss: 0.3424 score: 0.8750 time: 0.14s
     INFO: Early stopping counter 18 of 20
Epoch 102/1000, LR 0.000284
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.24s
Val loss: 0.1846 score: 0.9380 time: 0.15s
Test loss: 0.2762 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 103/1000, LR 0.000284
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.23s
Val loss: 0.1836 score: 0.9380 time: 0.15s
Test loss: 0.2693 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.0460,   Val_Loss: 0.1620,   Val_Precision: 0.9375,   Val_Recall: 0.9231,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.1620,   Test_Precision: 0.9483,   Test_Recall: 0.8594,   Test_accuracy: 0.9016,   Test_Score: 0.9062,   Test_loss: 0.2336


[0.17375272582285106, 0.164040912874043, 0.16506622708402574, 0.16820994392037392, 0.16372837009839714, 0.16684471094049513, 0.1709305529948324, 0.1671310910023749, 0.17108822078444064, 0.16365792113356292, 0.17745567997917533, 0.17002212698571384, 0.16345547698438168, 0.16468505514785647, 0.16610238095745444, 0.16672870912589133, 0.20815036306157708, 0.17972877598367631, 0.17334251292049885, 0.17746564908884466, 0.16714274720288813, 0.16495068091899157, 0.16346248309127986, 0.17065504705533385, 0.16935313981957734, 0.16244401596486568, 0.1591314789839089, 0.16478081000968814, 0.16034454014152288, 0.15380261605605483, 0.15261415904387832, 0.17637634393759072, 0.16027590609155595, 0.1603954890742898, 0.16099483310244977, 0.16046846914105117, 0.1686613450292498, 0.17147196014411747, 0.16438747011125088, 0.16933892597444355, 0.1600884289946407, 0.16380216809920967, 0.1696282080374658, 0.15396924503147602, 0.16141464095562696, 0.16110269096679986, 0.16445388598367572, 0.16844192403368652, 0.16714115906506777, 0.1730649450328201, 0.17534713307395577, 0.1649434280116111, 0.16369732096791267, 0.15723742684349418, 0.16103235422633588, 0.16539652808569372, 0.16538360505364835, 0.15805640304461122, 0.1586829989682883, 0.16405561193823814, 0.1659600269049406, 0.1582735620904714, 0.16077718511223793, 0.15796186891384423, 0.1617568931542337, 0.18118152697570622, 0.17313509108498693, 0.1676090930122882, 0.18740943493321538, 0.17847704095765948, 0.16820390499196947, 0.16882890998385847, 0.1672290898859501, 0.1815212108194828, 0.18264004588127136, 0.2771890698932111, 0.17325325682759285, 0.17102143191732466, 0.1651360639370978, 0.16997520485892892, 0.17265822901390493, 0.16815042309463024, 0.1666759999934584, 0.16810445999726653, 0.18043494992889464, 0.1782993588130921, 0.17511372501030564, 0.1746020431164652, 0.17715847096405923, 0.18292943900451064, 0.18177708704024553, 0.17806575703434646, 0.172691771062091, 0.16621834901161492, 0.1727059909608215, 0.1799866990186274, 0.17096704011783004, 0.16947899921797216, 0.1984959370456636, 0.16698498907499015, 0.1726553679909557, 0.18340558279305696, 0.1687190248630941, 0.16816566418856382, 0.16753067704848945, 0.16737772105261683, 0.17077329684980214, 0.17269602697342634, 0.17253062105737627, 0.168386974837631, 0.17798391403630376, 0.20285345008596778, 0.17357717198319733, 0.173806122969836, 0.17202872107736766, 0.18572724610567093, 0.1739707610104233, 0.16891120583750308, 0.17166458093561232, 0.3114665849134326, 0.16881073708646, 0.17042022105306387, 0.17092619999311864, 0.16878112009726465, 0.16791759198531508, 0.16903029708191752, 0.17402576003223658, 0.17046015709638596, 0.16940060281194746, 0.17132847290486097, 0.17639535712078214, 0.17549492209218442, 0.17362706689164042, 0.1729179120156914, 0.17214276199229062, 0.16807093704119325, 0.17074410105124116, 0.16289077792316675, 0.15274310484528542, 0.14436764316633344, 0.14504968095570803, 0.14802343188785017, 0.16866667103022337, 0.15016761398874223, 0.14714486710727215, 0.1487081020604819, 0.1459174680057913, 0.14922411693260074, 0.14808812201954424, 0.15304247802123427, 0.17057929886505008, 0.1474323719739914, 0.14754212903790176, 0.14739341591484845, 0.14600980305112898, 0.1449985618237406, 0.14583006198517978, 0.15253614587709308, 0.15402619913220406, 0.15599234495311975, 0.14653119910508394, 0.14843750512227416, 0.14973511383868754, 0.1470002168789506, 0.1465675209183246, 0.1447603078559041, 0.14342402596957982, 0.14629129716195166, 0.1434127960819751, 0.14493622398003936, 0.16451448807492852, 0.24547514808364213, 0.14648503391072154, 0.14643956208601594, 0.1438701949082315, 0.1458256640471518, 0.14802820491604507, 0.15708062914200127, 0.16449368698522449, 0.15196217806078494, 0.1705450927838683, 0.1568974880501628, 0.14759242697618902, 0.1485308688133955, 0.15180970798246562, 0.15269157500006258, 0.15571572608314455, 0.1557824001647532, 0.1466784100048244, 0.14749975479207933, 0.14829027699306607, 0.14885166403837502, 0.14666959387250245, 0.1567454831674695, 0.15523461485281587, 0.15742235397920012, 0.15818708296865225, 0.1601052121259272, 0.15740522393025458, 0.15692524309270084, 0.15450655785389245, 0.15969703323207796, 0.160485974047333, 0.16006200318224728, 0.1590809510089457, 0.1574351938907057, 0.15548747894354165, 0.15533292293548584, 0.15554261300712824, 0.1598217028658837, 0.15679572196677327, 0.15420616301707923, 0.1444137787912041, 0.1439404219854623, 0.14430995378643274, 0.14574623410589993, 0.14612803095951676, 0.14802094106562436, 0.14850663882680237, 0.14657211303710938, 0.14751808904111385, 0.14384745294228196, 0.14432433107867837, 0.14404302998445928, 0.1447646061424166, 0.1455142111517489, 0.14654513983987272, 0.14926240500062704, 0.1490387658122927, 0.14733271393924952, 0.14818083588033915, 0.14674785314127803, 0.14319907105527818, 0.14387869578786194, 0.14336761785671115, 0.14607861009426415, 0.14785881713032722, 0.14770538988523185, 0.14634982286952436, 0.14696603105403483]
[0.0013469203552158998, 0.0012716349835197132, 0.001279583155690122, 0.0013039530536463095, 0.0012692121713054041, 0.0012933698522519003, 0.001325043046471569, 0.0012955898527315882, 0.0013262652773987646, 0.0012686660552989374, 0.0013756254261951576, 0.0013180009843853785, 0.0012670967208091603, 0.0012766283344795076, 0.0012876153562593367, 0.0012924706133790025, 0.0016135687059036982, 0.0013932463254548552, 0.001343740410236425, 0.001375702706115075, 0.001295680210875102, 0.0012786874489844307, 0.0012671510317153478, 0.001322907341514216, 0.0013128150373610647, 0.001259255937712137, 0.00123357735646441, 0.0012773706202301406, 0.0012429809313296348, 0.0011922683415198049, 0.001183055496464173, 0.0013672584801363622, 0.0012424488844306662, 0.0012433758842968201, 0.0012480219620344944, 0.0012439416212484586, 0.001307452287048448, 0.0013292400011171897, 0.001274321473730627, 0.0013127048525150664, 0.001240995573601866, 0.0012697842488310826, 0.0013149473491276418, 0.00119356003900369, 0.0012512762864777284, 0.0012488580695100763, 0.0012748363254548505, 0.001305751349098345, 0.0012956678997292075, 0.001341588721184652, 0.0013592801013484943, 0.00127863122489621, 0.001268971480371416, 0.001218894781732513, 0.001248312823459968, 0.0012821436285712692, 0.0012820434500282819, 0.0012252434344543505, 0.0012301007671960333, 0.001271748929753784, 0.0012865118364724078, 0.001226926837910631, 0.0012463347683119219, 0.0012245106117352265, 0.0012539294042963852, 0.0014045079610519862, 0.0013421324890309065, 0.0012992952946689008, 0.0014527863173117472, 0.001383542953160151, 0.0013039062402478254, 0.0013087512401849493, 0.001296349533999613, 0.0014071411691432775, 0.0014158143091571424, 0.002148752479792334, 0.0013430485025394795, 0.0013257475342428268, 0.001280124526644169, 0.0013176372469684413, 0.0013384358838287203, 0.0013034916518963584, 0.0012920620154531657, 0.0013031353488160197, 0.0013987205420844546, 0.0013821655721945126, 0.0013574707365139973, 0.0013535042102051566, 0.0013733214803415444, 0.0014180576667016328, 0.0014091247057383374, 0.0013803547056926082, 0.0013386958997061316, 0.0012885143334233715, 0.0013388061314792365, 0.001395245728826569, 0.0013253258923862793, 0.0013137906916121872, 0.001538728194152431, 0.0012944572796510865, 0.0013384137053562457, 0.001421748703822147, 0.001307899417543365, 0.0013036097999113475, 0.0012986874189805384, 0.001297501713586177, 0.0013238240065876134, 0.0013387288912668708, 0.001337446674863382, 0.0013053253863382246, 0.001379720263847316, 0.0015725073650075022, 0.0013455594727379639, 0.001347334286587876, 0.0013335559773439353, 0.0014397460938424104, 0.0013486105504683977, 0.0013093891925387835, 0.0013307331855473823, 0.0024144696504917254, 0.0013086103650113178, 0.001321086984907472, 0.0013250093022722374, 0.001308380775947788, 0.0013016867595760859, 0.0013103123804799807, 0.001349036899474702, 0.0013213965666386508, 0.0013131829675344766, 0.0013281276969369068, 0.00136740586915335, 0.0013604257526525923, 0.0013459462549739569, 0.0013404489303541968, 0.0013344400154441133, 0.0013028754809394825, 0.001323597682567761, 0.0012725842025247402, 0.0011933055066037923, 0.00112787221223698, 0.001133200632466469, 0.0011564330616238294, 0.00131770836742362, 0.0011731844842870487, 0.0011495692742755637, 0.0011617820473475149, 0.0011399802187952446, 0.0011658134135359433, 0.0011569384532776894, 0.0011956443595408928, 0.0013326507723832037, 0.0011518154060468078, 0.0011526728831086075, 0.0011515110618347535, 0.0011407015863369452, 0.0011328012642479734, 0.001139297359259217, 0.0011916886396647897, 0.0012033296807203442, 0.001218690194946248, 0.0011447749930084683, 0.0011596680087677669, 0.0011698055768647464, 0.0011484391943668015, 0.0011450587571744109, 0.0011309399051242508, 0.0011205002028873423, 0.0011429007590777474, 0.0011204124693904305, 0.0011323142498440575, 0.001285269438085379, 0.001917774594403454, 0.001144414327427512, 0.0011440590787969995, 0.0011239858977205586, 0.0011392630003683735, 0.001156470350906602, 0.001227192415171885, 0.0012851069295720663, 0.0011872045160998823, 0.0013323835373739712, 0.0012257616253918968, 0.0011530658357514767, 0.0011603974126046523, 0.0011860133436130127, 0.001192902929687989, 0.0012165291100245668, 0.0012170500012871344, 0.0011459250781626906, 0.0011523418343131198, 0.0011585177890083287, 0.0011629036252998048, 0.0011458562021289254, 0.0012245740872458555, 0.001212770428537624, 0.001229862140462501, 0.0012358365856925957, 0.0012508219697338063, 0.001229728311955114, 0.0012259784616617253, 0.0012070824832335347, 0.001247633072125609, 0.001253796672244789, 0.0012504843998613069, 0.0012428199297573883, 0.0012299624522711383, 0.0012147459292464191, 0.0012135384604334831, 0.0012151766641181894, 0.0012486070536397165, 0.0012249665778654162, 0.0012047356485709315, 0.001128232646806282, 0.0011245345467614243, 0.0011274215139565058, 0.0011386424539523432, 0.0011416252418712247, 0.0011564136020751903, 0.0011602081158343935, 0.001145094633102417, 0.001152485070633702, 0.0011238082261115778, 0.0011275338365521748, 0.0011253361717535881, 0.0011309734854876297, 0.0011368297746230382, 0.0011448839049990056, 0.0011661125390673988, 0.0011643653579085367, 0.0011510368276503868, 0.0011576627803151496, 0.0011464676026662346, 0.0011187427426193608, 0.0011240523108426714, 0.0011200595145055559, 0.0011412391413614387, 0.0011551470088306814, 0.0011539483584783738, 0.001143357991168159, 0.001148172117609647]
[742.434395714295, 786.38918633092, 781.5045044576775, 766.8987753843206, 787.8903327656279, 773.1740447319761, 754.6924627564971, 771.8492066695535, 753.9969695665437, 788.2294917746252, 726.942073734345, 758.7247747514608, 789.2057358978926, 783.3133363812645, 776.6294453843027, 773.7119820354176, 619.7442949539221, 717.7481696738217, 744.1913574840355, 726.9012378582556, 771.7953794513852, 782.0519398968278, 789.1719100337201, 755.9108401767487, 761.7219269594405, 794.1197417078192, 810.6504182811262, 782.8581495164125, 804.5175712633704, 838.7373590120517, 845.2688846708583, 731.3905998961264, 804.8620852987727, 804.2620197395423, 801.267950741688, 803.8962463498639, 764.8462662125006, 752.3095898103634, 784.7313418273178, 761.7858638094146, 805.8046469074823, 787.5353635238141, 760.4867226535091, 837.8296586024596, 799.1840098040562, 800.7315037747222, 784.4144224892624, 765.8425937606925, 771.8027128780441, 745.3849187976016, 735.6835423456394, 782.0863283556779, 788.0397750998386, 820.4153590506145, 801.0812523965623, 779.9438204238708, 780.0047650319028, 816.1643407992141, 812.9415302125702, 786.3187273871771, 777.2956078989386, 815.0445235210021, 802.3526466764896, 816.652783909257, 797.4930618690834, 711.9931162590158, 745.0829245047596, 769.647980796259, 688.3324740078856, 722.7820413640933, 766.926308911549, 764.0871460482304, 771.3968908637711, 710.660750981253, 706.3073127120156, 465.3863157363964, 744.5747477542083, 754.2914274180636, 781.1740023616984, 758.9342228301102, 747.1407574185824, 767.1702373737263, 773.9566584575039, 767.3799969500965, 714.9390960611345, 723.5023213696932, 736.6641306522844, 738.8229696370324, 728.1616244372005, 705.189939366835, 709.6603983506421, 724.4514731438097, 746.9956397263325, 776.0876026447946, 746.934135187376, 716.7196281912442, 754.5313992164428, 761.1562529590415, 649.8873574944953, 772.5245287890415, 747.153138075368, 703.359178251162, 764.5847888504345, 767.1007076412017, 770.0082293743893, 770.7118915751493, 755.3874193426012, 746.9772308071101, 747.6933613836592, 766.0925087845404, 724.7846003301602, 635.9270692479263, 743.1852848281657, 742.2063031829317, 749.8747836530387, 694.5669130667262, 741.5039127883741, 763.7148723223331, 751.4654409017846, 414.16962925847594, 764.169401937567, 756.9524273755823, 754.7116826162019, 764.3034951163986, 768.2339799827612, 763.1767927230374, 741.2695682300368, 756.7750857290217, 761.5085062194472, 752.9396475251019, 731.3117652618861, 735.0640033461398, 742.9717169645451, 746.0187235449265, 749.3780075736048, 767.5330564045277, 755.5165842085897, 785.802619595664, 838.0083679040839, 886.6252658327641, 882.4562670984828, 864.7279580504463, 758.8932609991673, 852.3808602938574, 869.8910299513534, 860.7466454513716, 877.2082037149919, 857.7701958042953, 864.3502142805683, 836.369102584972, 750.384137182228, 868.1946731656772, 867.5488203584101, 868.4241368959633, 876.6534665838676, 882.7673763798847, 877.7339751320151, 839.145366260511, 831.0274532590056, 820.5530857201213, 873.5341059224227, 862.3157597169331, 854.8429070411421, 870.7470146483076, 873.31762997703, 884.2202803783239, 892.4585621878217, 874.9666076054934, 892.5284458357181, 883.147059341274, 778.0469762742239, 521.4377137533525, 873.809402795458, 874.0807345819235, 889.690877819729, 877.7604466015805, 864.7000757227032, 814.8681393699263, 778.1453643962489, 842.3148551398095, 750.5346410770923, 815.81930718404, 867.2531688949742, 861.7737243617073, 843.1608340540665, 838.2911761827562, 822.010744962614, 821.6589285094404, 872.6574006071511, 867.7980528200413, 863.1718990314191, 859.9164868388752, 872.7098549905877, 816.610452903722, 824.5583636185905, 813.0992629986486, 809.1684706352769, 799.4742850677743, 813.1877507236743, 815.6750149138609, 828.4438005605037, 801.5177076833069, 797.5774877513487, 799.6901041795576, 804.6217927927908, 813.0329492201081, 823.2174119079872, 824.0365119065069, 822.9256119937627, 800.892480212232, 816.348803362917, 830.0576156986881, 886.3420171635048, 889.2568066316197, 886.9797033503996, 878.2388154674005, 875.9441919494628, 864.7425092592259, 861.9143292932615, 873.2902688493871, 867.6901987547162, 889.8315359908343, 886.8913442614249, 888.6233510487099, 884.1940265017272, 879.6391705447637, 873.451007245026, 857.5501647548978, 858.8369562937066, 868.7819329302447, 863.8094071986757, 872.2444469206035, 893.8605471161821, 889.6383116283326, 892.8097007786631, 876.2405386894216, 865.690680368266, 866.5899064310173, 874.6167059875171, 870.9495594457361]
Elapsed: 0.16309660860570147~0.01796546462706354
Time per graph: 0.0012682538098283832~0.00013697470331366256
Speed: 795.5031403068458~67.63759854245005
Total Time: 0.1479
best val loss: 0.16200522285734498 test_score: 0.9062

Testing...
Test loss: 0.4784 score: 0.8906 time: 0.14s
test Score 0.8906
Epoch Time List: [0.8698687679134309, 0.5728614311665297, 0.5763429428916425, 0.5819762637838721, 0.5785611851606518, 0.5774091391358525, 0.5772648637648672, 0.5788874910213053, 0.5904265521094203, 0.5761882408987731, 0.5924225079361349, 0.5924406037665904, 0.5820669201202691, 0.5795034549664706, 0.5792306498624384, 0.5783817030023783, 0.657021258957684, 0.6457256549037993, 0.593439661199227, 0.6167470060754567, 0.5836805491708219, 0.5808351233135909, 0.5797578219790012, 0.5884318521711975, 0.58912547188811, 0.5726542470511049, 0.560629605781287, 0.5663387919776142, 0.5679123636800796, 0.5556874822359532, 0.5498509551398456, 0.5865451567806304, 0.5849451450631022, 0.5662276251241565, 0.5620134931523353, 0.5730549651198089, 0.5690938180778176, 0.5724772112444043, 0.5694164759479463, 0.5864177430048585, 0.568198611959815, 0.5931966488715261, 0.6068551640491933, 0.5599356517195702, 0.5898738116957247, 0.5563114790711552, 0.5816784780472517, 0.5830242950469255, 0.6080789237748832, 0.6124434571247548, 0.6088476541917771, 0.5964665350038558, 0.5804768810048699, 0.5696809489745647, 0.5772845658939332, 0.5776450862176716, 0.5805625391658396, 0.5723252412863076, 0.575481137027964, 0.5822376699652523, 0.5686098581645638, 0.565892512910068, 0.592010980239138, 0.5733976580668241, 0.5686191020067781, 0.5865352461114526, 0.5849925049114972, 0.5661427511367947, 0.613021926023066, 0.5889826561324298, 0.5682347030378878, 0.5683667999692261, 0.5655624088831246, 0.5883356840349734, 0.5940116429701447, 0.6963257188908756, 0.5763749866746366, 0.5750432109925896, 0.6578238578513265, 0.5663884219247848, 0.5715601581614465, 0.63470059633255, 0.5636599028948694, 0.5683734850026667, 0.6342839279677719, 0.5918467370793223, 0.5986931547522545, 0.589085411047563, 0.5924933841452003, 0.5985625684261322, 0.5978752160444856, 0.5923515630420297, 0.5870772891212255, 0.560741594992578, 0.5584606460761279, 0.5865811619441956, 0.5778094520792365, 0.5526586452033371, 0.637524263933301, 0.5676238283049315, 0.5777220230083913, 0.5844198928680271, 0.5690428700763732, 0.5639482806436718, 0.5803022019099444, 0.5693636848591268, 0.5712903349194676, 0.5693181890528649, 0.5741904571186751, 0.5670640890020877, 0.5995784259866923, 0.6239002060610801, 0.6038739446084946, 0.5985969798639417, 0.5863928962498903, 0.6252307479735464, 0.5757407848723233, 0.6234181642066687, 0.584130119998008, 0.7116716972086579, 0.5751105623785406, 0.5729031800292432, 0.5727907042019069, 0.5757865458726883, 0.5761460969224572, 0.576797764049843, 0.5801102961413562, 0.5836858961265534, 0.582155582960695, 0.5824886218179017, 0.5880475107114762, 0.5892127589322627, 0.5898311049677432, 0.5869644731283188, 0.5856689650099725, 0.5726735407952219, 0.5707564449403435, 0.6380119218956679, 0.5355687870178372, 0.5418159756809473, 0.520149148767814, 0.5231405952945352, 0.5790382630657405, 0.5438435459509492, 0.5223722921218723, 0.5357320131734014, 0.5203855158761144, 0.5808210270479321, 0.5498953950591385, 0.5570967793464661, 0.5930603260640055, 0.5458564721047878, 0.5216740989126265, 0.5264660650864244, 0.5213990609627217, 0.5382119263522327, 0.525903234956786, 0.5431462051346898, 0.5528440419584513, 0.5510709721129388, 0.5167091300245374, 0.5227011668030173, 0.5240237929392606, 0.5208999440073967, 0.5361412717029452, 0.5263960969168693, 0.5378898689523339, 0.5191146919969469, 0.515147770754993, 0.5182977868244052, 0.5527415568940341, 0.6204360567498952, 0.5400683700572699, 0.5187272899784148, 0.5348331502173096, 0.5739763749297708, 0.5236458363942802, 0.5528559223748744, 0.7253376401495188, 0.5566340717487037, 0.587642724160105, 0.5882171110715717, 0.5027677081525326, 0.5365085650701076, 0.5363006230909377, 0.5350682351272553, 0.5564776440151036, 0.5578099712729454, 0.5308216030243784, 0.5192885189317167, 0.5211973050609231, 0.520135193830356, 0.5202676730696112, 0.5488419800531119, 0.5550668297801167, 0.5534817620646209, 0.5599240048322827, 0.5594658711925149, 0.5533408802002668, 0.5503587720450014, 0.5487077618017793, 0.5570455382112414, 0.5560807110741735, 0.5537668589968234, 0.5520666209049523, 0.552247109124437, 0.5497694180812687, 0.5522694292012602, 0.5617336879950017, 0.5826039060484618, 0.5487910262309015, 0.5523925328161567, 0.5271680718287826, 0.5203926870599389, 0.5203572828322649, 0.5230886097997427, 0.517086576204747, 0.5161096593365073, 0.5227617712225765, 0.5239683499094099, 0.5227009668014944, 0.5205460079014301, 0.5199222762603313, 0.5199573070276529, 0.5206139539368451, 0.5181824550963938, 0.5194076369516551, 0.5237668640911579, 0.5263181501068175, 0.5249632999766618, 0.5264249450992793, 0.5445491261780262, 0.5194292822852731, 0.5207870500162244, 0.51869973144494, 0.5217327200807631, 0.5446390437427908, 0.5207667977083474, 0.5362635580822825, 0.517831247067079]
Total Epoch List: [67, 70, 103]
Total Time List: [0.1736806749831885, 0.1714040080551058, 0.14785082987509668]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dd751bd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6304;  Loss pred: 0.6304; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6141;  Loss pred: 0.6141; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4961 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.25s
Val loss: 0.6884 score: 0.5349 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.25s
Val loss: 0.6869 score: 0.6822 time: 0.17s
Test loss: 0.6872 score: 0.5891 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4920;  Loss pred: 0.4920; Loss self: 0.0000; time: 0.24s
Val loss: 0.6849 score: 0.7752 time: 0.17s
Test loss: 0.6852 score: 0.7364 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4699;  Loss pred: 0.4699; Loss self: 0.0000; time: 0.25s
Val loss: 0.6823 score: 0.8760 time: 0.17s
Test loss: 0.6825 score: 0.8372 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.4348;  Loss pred: 0.4348; Loss self: 0.0000; time: 0.25s
Val loss: 0.6788 score: 0.8915 time: 0.16s
Test loss: 0.6788 score: 0.8837 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.4036;  Loss pred: 0.4036; Loss self: 0.0000; time: 0.26s
Val loss: 0.6745 score: 0.8992 time: 0.18s
Test loss: 0.6745 score: 0.9070 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 0.3783;  Loss pred: 0.3783; Loss self: 0.0000; time: 0.26s
Val loss: 0.6693 score: 0.8992 time: 0.17s
Test loss: 0.6692 score: 0.9302 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3461;  Loss pred: 0.3461; Loss self: 0.0000; time: 0.29s
Val loss: 0.6628 score: 0.8915 time: 0.19s
Test loss: 0.6626 score: 0.9302 time: 0.18s
Epoch 26/1000, LR 0.000285
Train loss: 0.3084;  Loss pred: 0.3084; Loss self: 0.0000; time: 0.26s
Val loss: 0.6550 score: 0.8915 time: 0.17s
Test loss: 0.6548 score: 0.9225 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.2845;  Loss pred: 0.2845; Loss self: 0.0000; time: 0.27s
Val loss: 0.6455 score: 0.8992 time: 0.18s
Test loss: 0.6455 score: 0.9302 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2542;  Loss pred: 0.2542; Loss self: 0.0000; time: 0.26s
Val loss: 0.6342 score: 0.8992 time: 0.18s
Test loss: 0.6346 score: 0.9302 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2237;  Loss pred: 0.2237; Loss self: 0.0000; time: 0.27s
Val loss: 0.6212 score: 0.8992 time: 0.19s
Test loss: 0.6219 score: 0.9302 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.1982;  Loss pred: 0.1982; Loss self: 0.0000; time: 0.26s
Val loss: 0.6063 score: 0.8992 time: 0.17s
Test loss: 0.6074 score: 0.9302 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.25s
Val loss: 0.5901 score: 0.9147 time: 0.27s
Test loss: 0.5915 score: 0.9302 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1498;  Loss pred: 0.1498; Loss self: 0.0000; time: 0.25s
Val loss: 0.5716 score: 0.9147 time: 0.17s
Test loss: 0.5731 score: 0.9302 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.26s
Val loss: 0.5513 score: 0.9225 time: 0.17s
Test loss: 0.5522 score: 0.9302 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1099;  Loss pred: 0.1099; Loss self: 0.0000; time: 0.35s
Val loss: 0.5285 score: 0.9225 time: 0.18s
Test loss: 0.5283 score: 0.9302 time: 0.18s
Epoch 35/1000, LR 0.000285
Train loss: 0.0937;  Loss pred: 0.0937; Loss self: 0.0000; time: 0.27s
Val loss: 0.5030 score: 0.9225 time: 0.18s
Test loss: 0.5016 score: 0.9302 time: 0.18s
Epoch 36/1000, LR 0.000285
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.27s
Val loss: 0.4735 score: 0.9225 time: 0.18s
Test loss: 0.4714 score: 0.9302 time: 0.26s
Epoch 37/1000, LR 0.000285
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.26s
Val loss: 0.4419 score: 0.9225 time: 0.17s
Test loss: 0.4385 score: 0.9147 time: 0.20s
Epoch 38/1000, LR 0.000284
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.26s
Val loss: 0.4109 score: 0.9225 time: 0.17s
Test loss: 0.4068 score: 0.9147 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.26s
Val loss: 0.3792 score: 0.9302 time: 0.18s
Test loss: 0.3740 score: 0.9070 time: 0.27s
Epoch 40/1000, LR 0.000284
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.25s
Val loss: 0.3496 score: 0.9225 time: 0.17s
Test loss: 0.3443 score: 0.9147 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.26s
Val loss: 0.3223 score: 0.9380 time: 0.17s
Test loss: 0.3169 score: 0.9147 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.25s
Val loss: 0.2968 score: 0.9380 time: 0.17s
Test loss: 0.2915 score: 0.9070 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.24s
Val loss: 0.2738 score: 0.9380 time: 0.17s
Test loss: 0.2692 score: 0.8992 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.24s
Val loss: 0.2545 score: 0.9380 time: 0.17s
Test loss: 0.2506 score: 0.8992 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.25s
Val loss: 0.2378 score: 0.9380 time: 0.17s
Test loss: 0.2349 score: 0.8992 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.25s
Val loss: 0.2236 score: 0.9380 time: 0.17s
Test loss: 0.2229 score: 0.8992 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.25s
Val loss: 0.2121 score: 0.9302 time: 0.17s
Test loss: 0.2163 score: 0.8992 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.25s
Val loss: 0.2040 score: 0.9302 time: 0.17s
Test loss: 0.2140 score: 0.9070 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.26s
Val loss: 0.1988 score: 0.9302 time: 0.17s
Test loss: 0.2156 score: 0.9070 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.25s
Val loss: 0.1963 score: 0.9302 time: 0.18s
Test loss: 0.2220 score: 0.9147 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.25s
Val loss: 0.1968 score: 0.9302 time: 0.17s
Test loss: 0.2310 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.26s
Val loss: 0.1983 score: 0.9302 time: 0.17s
Test loss: 0.2407 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.26s
Val loss: 0.2000 score: 0.9302 time: 0.17s
Test loss: 0.2507 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.25s
Val loss: 0.2016 score: 0.9302 time: 0.17s
Test loss: 0.2615 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.24s
Val loss: 0.2041 score: 0.9302 time: 0.17s
Test loss: 0.2714 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.25s
Val loss: 0.2064 score: 0.9302 time: 0.17s
Test loss: 0.2793 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.25s
Val loss: 0.2082 score: 0.9302 time: 0.17s
Test loss: 0.2849 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.25s
Val loss: 0.2097 score: 0.9225 time: 0.17s
Test loss: 0.2888 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.25s
Val loss: 0.2113 score: 0.9225 time: 0.17s
Test loss: 0.2926 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.26s
Val loss: 0.2132 score: 0.9302 time: 0.17s
Test loss: 0.2988 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.25s
Val loss: 0.2163 score: 0.9302 time: 0.17s
Test loss: 0.3072 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.26s
Val loss: 0.2223 score: 0.9302 time: 0.17s
Test loss: 0.3207 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.26s
Val loss: 0.2271 score: 0.9302 time: 0.18s
Test loss: 0.3327 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.25s
Val loss: 0.2296 score: 0.9302 time: 0.18s
Test loss: 0.3402 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.24s
Val loss: 0.2298 score: 0.9302 time: 0.17s
Test loss: 0.3453 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.25s
Val loss: 0.2299 score: 0.9302 time: 0.17s
Test loss: 0.3512 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.24s
Val loss: 0.2300 score: 0.9302 time: 0.17s
Test loss: 0.3523 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.24s
Val loss: 0.2321 score: 0.9225 time: 0.17s
Test loss: 0.3491 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.24s
Val loss: 0.2364 score: 0.9225 time: 0.17s
Test loss: 0.3462 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.24s
Val loss: 0.2369 score: 0.9225 time: 0.17s
Test loss: 0.3427 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0196,   Val_Loss: 0.1963,   Val_Precision: 0.9508,   Val_Recall: 0.9062,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.1963,   Test_Precision: 0.9821,   Test_Recall: 0.8462,   Test_accuracy: 0.9091,   Test_Score: 0.9147,   Test_loss: 0.2220


[0.1656030339654535, 0.17017443315126002, 0.16972422297112644, 0.16705282707698643, 0.16640753904357553, 0.18945245211943984, 0.1804459379054606, 0.16448624897748232, 0.16584191285073757, 0.16957658994942904, 0.16734498017467558, 0.16597374388948083, 0.16985650709830225, 0.16980290808714926, 0.16654322994872928, 0.1661263678688556, 0.16417343798093498, 0.17630502115935087, 0.17123228311538696, 0.16480159503407776, 0.16462828987278044, 0.16346656996756792, 0.16940204799175262, 0.17912890994921327, 0.1821051670704037, 0.16928995889611542, 0.17792967287823558, 0.1781105101108551, 0.1917555360123515, 0.1751571379136294, 0.17066627694293857, 0.17229614802636206, 0.17556826188229024, 0.18137172190472484, 0.18446045904420316, 0.26338114799000323, 0.2011346749495715, 0.17620581784285605, 0.27724730502814054, 0.16919333301484585, 0.17132080299779773, 0.17430907883681357, 0.16757914796471596, 0.16970937605947256, 0.17180388094857335, 0.17528905905783176, 0.17323094885796309, 0.17260861699469388, 0.17388573288917542, 0.17264676000922918, 0.16905985004268587, 0.17174578201957047, 0.17430834216065705, 0.17442124197259545, 0.16962784412316978, 0.16742193698883057, 0.16527042910456657, 0.16968021309003234, 0.17162063694559038, 0.17132094991393387, 0.173185815801844, 0.1714154309593141, 0.18179996614344418, 0.1782018239609897, 0.16780850384384394, 0.1703497131820768, 0.169471190078184, 0.16663977410644293, 0.16438179602846503, 0.16683297813870013]
[0.0012837444493446009, 0.001319181652335349, 0.0013156916509389645, 0.0012949831556355537, 0.001289980922818415, 0.0014686236598406188, 0.001398805720197369, 0.0012750872013758319, 0.0012855962236491285, 0.0013145472089103026, 0.0012972479083308184, 0.001286618169685898, 0.0013167171092891648, 0.0013163016130786766, 0.0012910327903002269, 0.0012878013013089581, 0.0012726623099297285, 0.0013667055903825648, 0.0013273820396541625, 0.0012775317444502152, 0.0012761882935874453, 0.001267182712926883, 0.0013131941704787025, 0.001388596201156692, 0.0014116679617860752, 0.0013123252627605846, 0.0013792997897537642, 0.0013807016287663185, 0.0014864770233515622, 0.00135780727064829, 0.0013229943949065006, 0.0013356290544679229, 0.0013609942781572888, 0.0014059823403467042, 0.00142992603910235, 0.002041714325503901, 0.0015591835267408643, 0.0013659365724252408, 0.002149203914946826, 0.0013115762249212857, 0.0013280682402930057, 0.0013512331692776245, 0.001299063162517178, 0.0013155765586005624, 0.0013318130306090958, 0.0013588299151769904, 0.0013428755725423496, 0.0013380512945325106, 0.0013479514177455458, 0.00133834697681573, 0.0013105414731991152, 0.0013313626513144997, 0.0013512274586097446, 0.0013521026509503524, 0.0013149445280865874, 0.0012978444727816322, 0.0012811661170896633, 0.001315350489070018, 0.001330392534461941, 0.001328069379177782, 0.0013425257038902636, 0.00132880179038228, 0.0014093020631274742, 0.0013814094880696876, 0.0013008411150685576, 0.0013205404122641614, 0.0013137301556448372, 0.0012917811946235886, 0.0012742774885927523, 0.001293278900300001]
[778.9712356774256, 758.0457158646034, 760.0565066186548, 772.2108165254231, 775.2052625826046, 680.9096348812221, 714.8955609495948, 784.2600874049947, 777.849204598259, 760.7182102109157, 770.8626805856328, 777.2313679077997, 759.4645751507355, 759.7043033785519, 774.5736649860397, 776.5173082086277, 785.7543923456145, 731.6864780805371, 753.3626116114551, 782.759414272206, 783.5834296747365, 789.1521797123036, 761.5020097412305, 720.1517613018142, 708.381876666505, 762.0062101802547, 725.0055480531338, 724.2694432782829, 672.731555409648, 736.4815475782115, 755.8611010371459, 748.7108764629053, 734.756946483232, 711.2464867470582, 699.3368696382084, 489.78448527719326, 641.3613169004444, 732.0984152466796, 465.2885624511536, 762.4413899847987, 752.9733560824966, 740.0647221638325, 769.7855107078187, 760.1229996555614, 750.8561464837574, 735.9272774545502, 744.6706310300864, 747.3555042965529, 741.8664996639894, 747.1903903270704, 763.0433835557575, 751.1101494492623, 740.0678498858241, 739.5888169416205, 760.4883541780489, 770.5083474730444, 780.5389064391048, 760.2536421353536, 751.6578559307959, 752.97271037083, 744.8646957762373, 752.5576856066037, 709.5710892389064, 723.8983144652856, 768.7333898170166, 757.2657305393844, 761.1913266230503, 774.1249092044488, 784.7584289543948, 773.2284194600489]
Elapsed: 0.17500531204111341~0.0177123845873718
Time per graph: 0.0013566303259001042~0.0001373053068788512
Speed: 742.4346869653797~53.18782838388209
Total Time: 0.1674
best val loss: 0.19633680996448957 test_score: 0.9147

Testing...
Test loss: 0.3169 score: 0.9147 time: 0.16s
test Score 0.9147
Epoch Time List: [0.5803619169164449, 0.5856366232037544, 0.5843259361572564, 0.5830781981348991, 0.5824999718461186, 0.6045446319039911, 0.6213775579817593, 0.5785796199925244, 0.5724480061326176, 0.5789864638354629, 0.5744957595597953, 0.5756561388261616, 0.5781481952872127, 0.572430710773915, 0.5733379658777267, 0.5702907633967698, 0.5731216308195144, 0.5861424840986729, 0.5881437889765948, 0.5717533107381314, 0.5739075210876763, 0.5718136080540717, 0.6064810168463737, 0.6076601892709732, 0.6564280870370567, 0.5946530899964273, 0.6232895597349852, 0.6084456599783152, 0.648729955079034, 0.5940985230263323, 0.6941227652132511, 0.5951596919912845, 0.5993169962894171, 0.7112460129428655, 0.6352035140153021, 0.7089413541834801, 0.6278742318972945, 0.603398886276409, 0.7148205752018839, 0.589105969760567, 0.5966781289316714, 0.591825732961297, 0.5756831662729383, 0.5785979467909783, 0.5828904549125582, 0.5900998660363257, 0.5910145121160895, 0.5900227818638086, 0.5987259061075747, 0.5929337858688086, 0.5891345450654626, 0.5926934368908405, 0.5923525339458138, 0.5915933442302048, 0.5781651809811592, 0.5858652698807418, 0.5818386517930776, 0.5870854530949146, 0.5892537380568683, 0.5957367250230163, 0.5943335960619152, 0.593062796164304, 0.6204866750631481, 0.6079864618368447, 0.5766311669722199, 0.5774222041945904, 0.5715229192283005, 0.5742542711086571, 0.5681547462008893, 0.5708801739383489]
Total Epoch List: [70]
Total Time List: [0.16738398396410048]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dad175b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4961 time: 0.19s
Epoch 13/1000, LR 0.000285
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5969;  Loss pred: 0.5969; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4961 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4961 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.4743;  Loss pred: 0.4743; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.4474;  Loss pred: 0.4474; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4118;  Loss pred: 0.4118; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.3843;  Loss pred: 0.3843; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.3523;  Loss pred: 0.3523; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.3189;  Loss pred: 0.3189; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6999 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7022 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.2749;  Loss pred: 0.2749; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.2301;  Loss pred: 0.2301; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.5039 time: 0.16s
Test loss: 0.6913 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.2080;  Loss pred: 0.2080; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7002 score: 0.5039 time: 0.16s
Test loss: 0.6850 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.1885;  Loss pred: 0.1885; Loss self: 0.0000; time: 0.25s
Val loss: 0.6964 score: 0.5271 time: 0.16s
Test loss: 0.6778 score: 0.5194 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 0.26s
Val loss: 0.6887 score: 0.5271 time: 0.16s
Test loss: 0.6662 score: 0.5194 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 0.26s
Val loss: 0.6792 score: 0.5271 time: 0.16s
Test loss: 0.6528 score: 0.5349 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1327;  Loss pred: 0.1327; Loss self: 0.0000; time: 0.26s
Val loss: 0.6649 score: 0.5271 time: 0.16s
Test loss: 0.6345 score: 0.5426 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.25s
Val loss: 0.6461 score: 0.5581 time: 0.16s
Test loss: 0.6118 score: 0.5581 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.25s
Val loss: 0.6198 score: 0.5659 time: 0.17s
Test loss: 0.5819 score: 0.5814 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 0.27s
Val loss: 0.5887 score: 0.5659 time: 0.17s
Test loss: 0.5469 score: 0.5969 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.0798;  Loss pred: 0.0798; Loss self: 0.0000; time: 0.26s
Val loss: 0.5530 score: 0.5814 time: 0.16s
Test loss: 0.5078 score: 0.6202 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.26s
Val loss: 0.5157 score: 0.6279 time: 0.16s
Test loss: 0.4681 score: 0.6434 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.25s
Val loss: 0.4726 score: 0.6822 time: 0.16s
Test loss: 0.4231 score: 0.7209 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.25s
Val loss: 0.4290 score: 0.7287 time: 0.17s
Test loss: 0.3786 score: 0.7674 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.26s
Val loss: 0.3851 score: 0.8062 time: 0.16s
Test loss: 0.3353 score: 0.8295 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.26s
Val loss: 0.3414 score: 0.8682 time: 0.16s
Test loss: 0.2930 score: 0.9147 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.25s
Val loss: 0.3050 score: 0.9070 time: 0.17s
Test loss: 0.2586 score: 0.9302 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.25s
Val loss: 0.2766 score: 0.9225 time: 0.16s
Test loss: 0.2313 score: 0.9302 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.26s
Val loss: 0.2532 score: 0.9147 time: 0.16s
Test loss: 0.2091 score: 0.9302 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.25s
Val loss: 0.2346 score: 0.9147 time: 0.16s
Test loss: 0.1924 score: 0.9302 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 0.0272;  Loss pred: 0.0272; Loss self: 0.0000; time: 0.25s
Val loss: 0.2233 score: 0.9147 time: 0.17s
Test loss: 0.1826 score: 0.9302 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.26s
Val loss: 0.2138 score: 0.8992 time: 0.16s
Test loss: 0.1748 score: 0.9225 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.26s
Val loss: 0.2108 score: 0.8992 time: 0.16s
Test loss: 0.1724 score: 0.9302 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.25s
Val loss: 0.2126 score: 0.8837 time: 0.16s
Test loss: 0.1736 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.26s
Val loss: 0.2195 score: 0.8837 time: 0.18s
Test loss: 0.1781 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.26s
Val loss: 0.2321 score: 0.8760 time: 0.17s
Test loss: 0.1857 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.26s
Val loss: 0.2454 score: 0.8760 time: 0.17s
Test loss: 0.1939 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.26s
Val loss: 0.2575 score: 0.8760 time: 0.16s
Test loss: 0.2013 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.29s
Val loss: 0.2686 score: 0.8760 time: 0.17s
Test loss: 0.2089 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.25s
Val loss: 0.2785 score: 0.8760 time: 0.16s
Test loss: 0.2154 score: 0.9380 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.26s
Val loss: 0.2829 score: 0.8837 time: 0.17s
Test loss: 0.2194 score: 0.9380 time: 0.26s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.27s
Val loss: 0.2866 score: 0.8837 time: 0.18s
Test loss: 0.2238 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.27s
Val loss: 0.2924 score: 0.8837 time: 0.18s
Test loss: 0.2299 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.33s
Val loss: 0.2972 score: 0.8837 time: 0.18s
Test loss: 0.2346 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.27s
Val loss: 0.3007 score: 0.8837 time: 0.17s
Test loss: 0.2369 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.27s
Val loss: 0.3067 score: 0.8837 time: 0.17s
Test loss: 0.2412 score: 0.9380 time: 0.27s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.26s
Val loss: 0.3124 score: 0.8837 time: 0.18s
Test loss: 0.2461 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.27s
Val loss: 0.3183 score: 0.8837 time: 0.17s
Test loss: 0.2517 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.27s
Val loss: 0.3253 score: 0.8837 time: 0.17s
Test loss: 0.2590 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.26s
Val loss: 0.3354 score: 0.8837 time: 0.18s
Test loss: 0.2689 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.27s
Val loss: 0.3428 score: 0.8837 time: 0.16s
Test loss: 0.2746 score: 0.9380 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.26s
Val loss: 0.3508 score: 0.8837 time: 0.17s
Test loss: 0.2781 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.26s
Val loss: 0.3613 score: 0.8760 time: 0.17s
Test loss: 0.2846 score: 0.9380 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.0170,   Val_Loss: 0.2108,   Val_Precision: 0.9194,   Val_Recall: 0.8769,   Val_accuracy: 0.8976,   Val_Score: 0.8992,   Val_Loss: 0.2108,   Test_Precision: 0.9508,   Test_Recall: 0.9062,   Test_accuracy: 0.9280,   Test_Score: 0.9302,   Test_loss: 0.1724


[0.1656030339654535, 0.17017443315126002, 0.16972422297112644, 0.16705282707698643, 0.16640753904357553, 0.18945245211943984, 0.1804459379054606, 0.16448624897748232, 0.16584191285073757, 0.16957658994942904, 0.16734498017467558, 0.16597374388948083, 0.16985650709830225, 0.16980290808714926, 0.16654322994872928, 0.1661263678688556, 0.16417343798093498, 0.17630502115935087, 0.17123228311538696, 0.16480159503407776, 0.16462828987278044, 0.16346656996756792, 0.16940204799175262, 0.17912890994921327, 0.1821051670704037, 0.16928995889611542, 0.17792967287823558, 0.1781105101108551, 0.1917555360123515, 0.1751571379136294, 0.17066627694293857, 0.17229614802636206, 0.17556826188229024, 0.18137172190472484, 0.18446045904420316, 0.26338114799000323, 0.2011346749495715, 0.17620581784285605, 0.27724730502814054, 0.16919333301484585, 0.17132080299779773, 0.17430907883681357, 0.16757914796471596, 0.16970937605947256, 0.17180388094857335, 0.17528905905783176, 0.17323094885796309, 0.17260861699469388, 0.17388573288917542, 0.17264676000922918, 0.16905985004268587, 0.17174578201957047, 0.17430834216065705, 0.17442124197259545, 0.16962784412316978, 0.16742193698883057, 0.16527042910456657, 0.16968021309003234, 0.17162063694559038, 0.17132094991393387, 0.173185815801844, 0.1714154309593141, 0.18179996614344418, 0.1782018239609897, 0.16780850384384394, 0.1703497131820768, 0.169471190078184, 0.16663977410644293, 0.16438179602846503, 0.16683297813870013, 0.1643445969093591, 0.16135149309411645, 0.16459302883595228, 0.16523274290375412, 0.16286747390404344, 0.16350148478522897, 0.16200505592860281, 0.1709857580717653, 0.16761451005004346, 0.16616396303288639, 0.166737797902897, 0.1924440290313214, 0.17547689098864794, 0.16885714093223214, 0.17305082804523408, 0.16901269485242665, 0.16599569306708872, 0.16676359414123, 0.1704738351982087, 0.17209414206445217, 0.16906685894355178, 0.1844974011182785, 0.17227957816794515, 0.16701261093840003, 0.1651510470546782, 0.16334398416802287, 0.1659260841552168, 0.16834072209894657, 0.16541186487302184, 0.16308887396007776, 0.16766446083784103, 0.16723920707590878, 0.16827364405617118, 0.1673566410318017, 0.1651479760184884, 0.16475804196670651, 0.1678675040602684, 0.16816934198141098, 0.16508558811619878, 0.1620283080264926, 0.16689350921660662, 0.1654730171430856, 0.16348271700553596, 0.1703424728475511, 0.16746251587755978, 0.16515005892142653, 0.16315502091310918, 0.16668753093108535, 0.16740217595361173, 0.1732613279018551, 0.1715974099934101, 0.17600722913630307, 0.16816354705952108, 0.16284219711087644, 0.1668386571109295, 0.26089853304438293, 0.17749737901613116, 0.17614875989966094, 0.17490782402455807, 0.17549973004497588, 0.2706866869702935, 0.1747389929369092, 0.1745701681356877, 0.17851327289827168, 0.17881925706751645, 0.16646973486058414, 0.17436550580896437, 0.16421384108252823]
[0.0012837444493446009, 0.001319181652335349, 0.0013156916509389645, 0.0012949831556355537, 0.001289980922818415, 0.0014686236598406188, 0.001398805720197369, 0.0012750872013758319, 0.0012855962236491285, 0.0013145472089103026, 0.0012972479083308184, 0.001286618169685898, 0.0013167171092891648, 0.0013163016130786766, 0.0012910327903002269, 0.0012878013013089581, 0.0012726623099297285, 0.0013667055903825648, 0.0013273820396541625, 0.0012775317444502152, 0.0012761882935874453, 0.001267182712926883, 0.0013131941704787025, 0.001388596201156692, 0.0014116679617860752, 0.0013123252627605846, 0.0013792997897537642, 0.0013807016287663185, 0.0014864770233515622, 0.00135780727064829, 0.0013229943949065006, 0.0013356290544679229, 0.0013609942781572888, 0.0014059823403467042, 0.00142992603910235, 0.002041714325503901, 0.0015591835267408643, 0.0013659365724252408, 0.002149203914946826, 0.0013115762249212857, 0.0013280682402930057, 0.0013512331692776245, 0.001299063162517178, 0.0013155765586005624, 0.0013318130306090958, 0.0013588299151769904, 0.0013428755725423496, 0.0013380512945325106, 0.0013479514177455458, 0.00133834697681573, 0.0013105414731991152, 0.0013313626513144997, 0.0013512274586097446, 0.0013521026509503524, 0.0013149445280865874, 0.0012978444727816322, 0.0012811661170896633, 0.001315350489070018, 0.001330392534461941, 0.001328069379177782, 0.0013425257038902636, 0.00132880179038228, 0.0014093020631274742, 0.0013814094880696876, 0.0013008411150685576, 0.0013205404122641614, 0.0013137301556448372, 0.0012917811946235886, 0.0012742774885927523, 0.001293278900300001, 0.001273989123328365, 0.0012507867681714453, 0.0012759149522166844, 0.0012808739759980938, 0.0012625385573956855, 0.0012674533704281315, 0.0012558531467333552, 0.0013254709928043821, 0.0012993372872096393, 0.0012880927366890418, 0.0012925410690147053, 0.0014918141785373751, 0.0013602859766561855, 0.0013089700847459856, 0.0013414792871723573, 0.0013101759290885787, 0.001286788318349525, 0.001292741039854496, 0.0013215025984357263, 0.001334063116778699, 0.0013105958057639672, 0.0014302124117696008, 0.0013355006059530632, 0.001294671402623256, 0.0012802406748424666, 0.00126623243541103, 0.0012862487143815258, 0.00130496683797633, 0.001282262518395518, 0.0012642548368998277, 0.0012997245026189228, 0.0012964279618287503, 0.0013044468531486137, 0.0012973383025721061, 0.0012802168683603752, 0.0012771941237729187, 0.001301298481087352, 0.0013036383099334185, 0.0012797332412108433, 0.001256033395554206, 0.0012937481334620669, 0.0012827365670006635, 0.0012673078837638447, 0.001320484285639931, 0.0012981590378105408, 0.0012802330148947794, 0.0012647676039775906, 0.001292151402566553, 0.0012976912864621065, 0.0013431110690066287, 0.0013302124805690705, 0.0013643971250876206, 0.001303593388058303, 0.001262342613262608, 0.0012933229233405388, 0.0020224692484060694, 0.0013759486745436524, 0.0013654942627880694, 0.0013558746048415353, 0.0013604630236044642, 0.0020983464106224302, 0.0013545658367202264, 0.0013532571173309124, 0.0013838238209168347, 0.0013861957912210576, 0.0012904630609347609, 0.0013516705876663905, 0.0012729755122676608]
[778.9712356774256, 758.0457158646034, 760.0565066186548, 772.2108165254231, 775.2052625826046, 680.9096348812221, 714.8955609495948, 784.2600874049947, 777.849204598259, 760.7182102109157, 770.8626805856328, 777.2313679077997, 759.4645751507355, 759.7043033785519, 774.5736649860397, 776.5173082086277, 785.7543923456145, 731.6864780805371, 753.3626116114551, 782.759414272206, 783.5834296747365, 789.1521797123036, 761.5020097412305, 720.1517613018142, 708.381876666505, 762.0062101802547, 725.0055480531338, 724.2694432782829, 672.731555409648, 736.4815475782115, 755.8611010371459, 748.7108764629053, 734.756946483232, 711.2464867470582, 699.3368696382084, 489.78448527719326, 641.3613169004444, 732.0984152466796, 465.2885624511536, 762.4413899847987, 752.9733560824966, 740.0647221638325, 769.7855107078187, 760.1229996555614, 750.8561464837574, 735.9272774545502, 744.6706310300864, 747.3555042965529, 741.8664996639894, 747.1903903270704, 763.0433835557575, 751.1101494492623, 740.0678498858241, 739.5888169416205, 760.4883541780489, 770.5083474730444, 780.5389064391048, 760.2536421353536, 751.6578559307959, 752.97271037083, 744.8646957762373, 752.5576856066037, 709.5710892389064, 723.8983144652856, 768.7333898170166, 757.2657305393844, 761.1913266230503, 774.1249092044488, 784.7584289543948, 773.2284194600489, 784.9360576858351, 799.4967851010477, 783.7512980490359, 780.7169313599109, 792.0550181554537, 788.9836607260835, 796.2714451137349, 754.4488000331394, 769.6231069821187, 776.3416185160975, 773.6698074609673, 670.3247726070238, 735.1395347456056, 763.9593995718067, 745.4457251500723, 763.2562755870871, 777.1285966308983, 773.550130436452, 756.7143652866883, 749.5897213728944, 763.0117505351576, 699.1968408124083, 748.7828875123292, 772.3967625868661, 781.1031313491502, 789.7444197718654, 777.454615751228, 766.3029978223389, 779.8715049795651, 790.9797699110835, 769.3938199864795, 771.3502249591971, 766.6084651791264, 770.8089694240873, 781.1176564801399, 782.9663332978167, 768.4632038949358, 767.0839314710486, 781.4128505826976, 796.157175071579, 772.9479750621958, 779.5832953746946, 789.0742358755373, 757.2979177979248, 770.3216407803067, 781.1078048804957, 790.6590877684421, 773.903118484209, 770.5993023396946, 744.5400630489961, 751.7595982652304, 732.9244408483935, 767.110365210962, 792.1779630138872, 773.2020997641397, 494.44509516676766, 726.7712949624813, 732.3355558874321, 737.531329541254, 735.0438656910798, 476.56573525596804, 738.2439250212252, 738.9578722278167, 722.6353419306389, 721.3988141741005, 774.9156332112593, 739.8252274812484, 785.5610656788001]
Elapsed: 0.1734038942621724~0.01758607522825446
Time per graph: 0.0013442162345904834~0.0001363261645601121
Speed: 749.2860732918319~53.49417625720797
Total Time: 0.1648
best val loss: 0.21081432607746864 test_score: 0.9302

Testing...
Test loss: 0.2313 score: 0.9302 time: 0.16s
test Score 0.9302
Epoch Time List: [0.5803619169164449, 0.5856366232037544, 0.5843259361572564, 0.5830781981348991, 0.5824999718461186, 0.6045446319039911, 0.6213775579817593, 0.5785796199925244, 0.5724480061326176, 0.5789864638354629, 0.5744957595597953, 0.5756561388261616, 0.5781481952872127, 0.572430710773915, 0.5733379658777267, 0.5702907633967698, 0.5731216308195144, 0.5861424840986729, 0.5881437889765948, 0.5717533107381314, 0.5739075210876763, 0.5718136080540717, 0.6064810168463737, 0.6076601892709732, 0.6564280870370567, 0.5946530899964273, 0.6232895597349852, 0.6084456599783152, 0.648729955079034, 0.5940985230263323, 0.6941227652132511, 0.5951596919912845, 0.5993169962894171, 0.7112460129428655, 0.6352035140153021, 0.7089413541834801, 0.6278742318972945, 0.603398886276409, 0.7148205752018839, 0.589105969760567, 0.5966781289316714, 0.591825732961297, 0.5756831662729383, 0.5785979467909783, 0.5828904549125582, 0.5900998660363257, 0.5910145121160895, 0.5900227818638086, 0.5987259061075747, 0.5929337858688086, 0.5891345450654626, 0.5926934368908405, 0.5923525339458138, 0.5915933442302048, 0.5781651809811592, 0.5858652698807418, 0.5818386517930776, 0.5870854530949146, 0.5892537380568683, 0.5957367250230163, 0.5943335960619152, 0.593062796164304, 0.6204866750631481, 0.6079864618368447, 0.5766311669722199, 0.5774222041945904, 0.5715229192283005, 0.5742542711086571, 0.5681547462008893, 0.5708801739383489, 0.5788450767286122, 0.5772094374988228, 0.582483691861853, 0.5747541110031307, 0.5786673191469163, 0.5767863863147795, 0.5752122509293258, 0.6006325471680611, 0.5905571822077036, 0.5921309359837323, 0.6075403259601444, 0.6548862911295146, 0.636350145097822, 0.6367819830775261, 0.6028416950721294, 0.5888696250040084, 0.589294804027304, 0.5863083999138325, 0.593464540084824, 0.5948970741592348, 0.5926138667855412, 0.6272176078055054, 0.6082015361171216, 0.581276164855808, 0.5795155148953199, 0.5819513241294771, 0.5828572157770395, 0.5862661839928478, 0.5795529689639807, 0.5835194890387356, 0.5829888589214534, 0.5842927051708102, 0.5817003780975938, 0.5818709200248122, 0.5916574927978218, 0.5812945181969553, 0.5805175942368805, 0.5773959960788488, 0.5803056510630995, 0.5778286312706769, 0.5781383248977363, 0.5797938292380422, 0.576752666849643, 0.5811767547857016, 0.5776691089849919, 0.5805688800755888, 0.5792605467140675, 0.5797221302054822, 0.5775333971250802, 0.6008934800047427, 0.6033072990830988, 0.603007084922865, 0.5824755958747119, 0.6130957240238786, 0.5807942028623074, 0.6777542713098228, 0.6191890530753881, 0.6158946969080716, 0.676875727949664, 0.6154140059370548, 0.7102905081119388, 0.608139602933079, 0.6154210809618235, 0.6146698237862438, 0.6106815312523395, 0.5927173821255565, 0.6056684139184654, 0.5809664470143616]
Total Epoch List: [70, 68]
Total Time List: [0.16738398396410048, 0.16484679304994643]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756d8364c10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7043;  Loss pred: 0.7043; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7020 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.5000 time: 0.16s
Epoch 2/1000, LR 0.000020
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.7027;  Loss pred: 0.7027; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7020 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7018 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7008 score: 0.5000 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.5000 time: 0.17s
Epoch 8/1000, LR 0.000200
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000230
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5000 time: 0.17s
Epoch 10/1000, LR 0.000260
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.17s
Epoch 11/1000, LR 0.000290
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.17s
Epoch 12/1000, LR 0.000290
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.16s
Epoch 13/1000, LR 0.000290
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.16s
Epoch 14/1000, LR 0.000290
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5000 time: 0.17s
Epoch 16/1000, LR 0.000290
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.28s
Val loss: 0.6850 score: 0.8372 time: 0.17s
Test loss: 0.6850 score: 0.8359 time: 0.16s
Epoch 17/1000, LR 0.000290
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.27s
Val loss: 0.6812 score: 0.7752 time: 0.17s
Test loss: 0.6812 score: 0.7578 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.28s
Val loss: 0.6761 score: 0.7364 time: 0.20s
Test loss: 0.6761 score: 0.7031 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.26s
Val loss: 0.6703 score: 0.7829 time: 0.17s
Test loss: 0.6702 score: 0.7656 time: 0.16s
Epoch 20/1000, LR 0.000290
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.26s
Val loss: 0.6624 score: 0.8450 time: 0.16s
Test loss: 0.6621 score: 0.8203 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.26s
Val loss: 0.6530 score: 0.8837 time: 0.16s
Test loss: 0.6523 score: 0.8828 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.26s
Val loss: 0.6402 score: 0.8605 time: 0.17s
Test loss: 0.6393 score: 0.8594 time: 0.17s
Epoch 23/1000, LR 0.000290
Train loss: 0.4891;  Loss pred: 0.4891; Loss self: 0.0000; time: 0.27s
Val loss: 0.6248 score: 0.8760 time: 0.18s
Test loss: 0.6232 score: 0.9062 time: 0.17s
Epoch 24/1000, LR 0.000290
Train loss: 0.4696;  Loss pred: 0.4696; Loss self: 0.0000; time: 0.26s
Val loss: 0.6059 score: 0.8760 time: 0.16s
Test loss: 0.6035 score: 0.9062 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 0.4427;  Loss pred: 0.4427; Loss self: 0.0000; time: 0.27s
Val loss: 0.5858 score: 0.8682 time: 0.17s
Test loss: 0.5824 score: 0.9219 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.4229;  Loss pred: 0.4229; Loss self: 0.0000; time: 0.26s
Val loss: 0.5621 score: 0.8992 time: 0.17s
Test loss: 0.5572 score: 0.9141 time: 0.16s
Epoch 27/1000, LR 0.000290
Train loss: 0.4079;  Loss pred: 0.4079; Loss self: 0.0000; time: 0.27s
Val loss: 0.5413 score: 0.8760 time: 0.16s
Test loss: 0.5346 score: 0.9062 time: 0.17s
Epoch 28/1000, LR 0.000290
Train loss: 0.3869;  Loss pred: 0.3869; Loss self: 0.0000; time: 0.27s
Val loss: 0.5119 score: 0.8915 time: 0.17s
Test loss: 0.5041 score: 0.8984 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.3725;  Loss pred: 0.3725; Loss self: 0.0000; time: 0.26s
Val loss: 0.4842 score: 0.8915 time: 0.17s
Test loss: 0.4751 score: 0.9062 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.3446;  Loss pred: 0.3446; Loss self: 0.0000; time: 0.26s
Val loss: 0.4685 score: 0.8915 time: 0.17s
Test loss: 0.4579 score: 0.9141 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.3286;  Loss pred: 0.3286; Loss self: 0.0000; time: 0.26s
Val loss: 0.4501 score: 0.8837 time: 0.17s
Test loss: 0.4383 score: 0.9062 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 0.3008;  Loss pred: 0.3008; Loss self: 0.0000; time: 0.26s
Val loss: 0.4123 score: 0.8915 time: 0.16s
Test loss: 0.4002 score: 0.9062 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.2826;  Loss pred: 0.2826; Loss self: 0.0000; time: 0.26s
Val loss: 0.3843 score: 0.8915 time: 0.16s
Test loss: 0.3720 score: 0.9062 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 0.2614;  Loss pred: 0.2614; Loss self: 0.0000; time: 0.26s
Val loss: 0.3668 score: 0.8992 time: 0.16s
Test loss: 0.3537 score: 0.9062 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 0.26s
Val loss: 0.3470 score: 0.8992 time: 0.17s
Test loss: 0.3332 score: 0.9062 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 0.2315;  Loss pred: 0.2315; Loss self: 0.0000; time: 0.26s
Val loss: 0.3386 score: 0.8992 time: 0.17s
Test loss: 0.3238 score: 0.9062 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.2141;  Loss pred: 0.2141; Loss self: 0.0000; time: 0.27s
Val loss: 0.3323 score: 0.9070 time: 0.18s
Test loss: 0.3161 score: 0.9062 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 0.26s
Val loss: 0.3285 score: 0.8992 time: 0.16s
Test loss: 0.3108 score: 0.9062 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.26s
Val loss: 0.3172 score: 0.8992 time: 0.17s
Test loss: 0.2989 score: 0.9062 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 0.1778;  Loss pred: 0.1778; Loss self: 0.0000; time: 0.26s
Val loss: 0.3143 score: 0.8682 time: 0.17s
Test loss: 0.2955 score: 0.9062 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.1767;  Loss pred: 0.1767; Loss self: 0.0000; time: 0.25s
Val loss: 0.3105 score: 0.8992 time: 0.17s
Test loss: 0.2900 score: 0.9062 time: 0.16s
Epoch 42/1000, LR 0.000289
Train loss: 0.1596;  Loss pred: 0.1596; Loss self: 0.0000; time: 0.26s
Val loss: 0.3045 score: 0.8837 time: 0.16s
Test loss: 0.2826 score: 0.9062 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 0.26s
Val loss: 0.3030 score: 0.8682 time: 0.17s
Test loss: 0.2792 score: 0.9141 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 0.1531;  Loss pred: 0.1531; Loss self: 0.0000; time: 0.26s
Val loss: 0.3037 score: 0.8992 time: 0.17s
Test loss: 0.2797 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1331;  Loss pred: 0.1331; Loss self: 0.0000; time: 0.26s
Val loss: 0.3102 score: 0.8992 time: 0.16s
Test loss: 0.2865 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 0.26s
Val loss: 0.3024 score: 0.8992 time: 0.17s
Test loss: 0.2778 score: 0.9062 time: 0.17s
Epoch 47/1000, LR 0.000289
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.26s
Val loss: 0.3133 score: 0.8992 time: 0.18s
Test loss: 0.2877 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1147;  Loss pred: 0.1147; Loss self: 0.0000; time: 0.27s
Val loss: 0.3257 score: 0.8915 time: 0.18s
Test loss: 0.2984 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.27s
Val loss: 0.3195 score: 0.8992 time: 0.18s
Test loss: 0.2918 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.26s
Val loss: 0.3210 score: 0.8915 time: 0.17s
Test loss: 0.2916 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.27s
Val loss: 0.3082 score: 0.8915 time: 0.16s
Test loss: 0.2778 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.0864;  Loss pred: 0.0864; Loss self: 0.0000; time: 0.26s
Val loss: 0.3187 score: 0.8992 time: 0.16s
Test loss: 0.2885 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.25s
Val loss: 0.3259 score: 0.8992 time: 0.17s
Test loss: 0.2948 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.26s
Val loss: 0.3125 score: 0.8992 time: 0.17s
Test loss: 0.2810 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.26s
Val loss: 0.3110 score: 0.8915 time: 0.17s
Test loss: 0.2771 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0809;  Loss pred: 0.0809; Loss self: 0.0000; time: 0.27s
Val loss: 0.3123 score: 0.8915 time: 0.17s
Test loss: 0.2745 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0749;  Loss pred: 0.0749; Loss self: 0.0000; time: 0.26s
Val loss: 0.3232 score: 0.8915 time: 0.16s
Test loss: 0.2888 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.26s
Val loss: 0.3209 score: 0.8992 time: 0.17s
Test loss: 0.2840 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.27s
Val loss: 0.3291 score: 0.8915 time: 0.17s
Test loss: 0.2942 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.26s
Val loss: 0.3724 score: 0.8992 time: 0.16s
Test loss: 0.3382 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0692;  Loss pred: 0.0692; Loss self: 0.0000; time: 0.28s
Val loss: 0.3658 score: 0.9070 time: 0.18s
Test loss: 0.3333 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0737;  Loss pred: 0.0737; Loss self: 0.0000; time: 0.27s
Val loss: 0.3460 score: 0.8992 time: 0.17s
Test loss: 0.3164 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.26s
Val loss: 0.3402 score: 0.9070 time: 0.18s
Test loss: 0.3106 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.25s
Val loss: 0.3294 score: 0.8992 time: 0.16s
Test loss: 0.2991 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.25s
Val loss: 0.3513 score: 0.9070 time: 0.16s
Test loss: 0.3228 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0622;  Loss pred: 0.0622; Loss self: 0.0000; time: 0.25s
Val loss: 0.3713 score: 0.9070 time: 0.16s
Test loss: 0.3420 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.1266,   Val_Loss: 0.3024,   Val_Precision: 0.9643,   Val_Recall: 0.8308,   Val_accuracy: 0.8926,   Val_Score: 0.8992,   Val_Loss: 0.3024,   Test_Precision: 0.9643,   Test_Recall: 0.8438,   Test_accuracy: 0.9000,   Test_Score: 0.9062,   Test_loss: 0.2778


[0.1656030339654535, 0.17017443315126002, 0.16972422297112644, 0.16705282707698643, 0.16640753904357553, 0.18945245211943984, 0.1804459379054606, 0.16448624897748232, 0.16584191285073757, 0.16957658994942904, 0.16734498017467558, 0.16597374388948083, 0.16985650709830225, 0.16980290808714926, 0.16654322994872928, 0.1661263678688556, 0.16417343798093498, 0.17630502115935087, 0.17123228311538696, 0.16480159503407776, 0.16462828987278044, 0.16346656996756792, 0.16940204799175262, 0.17912890994921327, 0.1821051670704037, 0.16928995889611542, 0.17792967287823558, 0.1781105101108551, 0.1917555360123515, 0.1751571379136294, 0.17066627694293857, 0.17229614802636206, 0.17556826188229024, 0.18137172190472484, 0.18446045904420316, 0.26338114799000323, 0.2011346749495715, 0.17620581784285605, 0.27724730502814054, 0.16919333301484585, 0.17132080299779773, 0.17430907883681357, 0.16757914796471596, 0.16970937605947256, 0.17180388094857335, 0.17528905905783176, 0.17323094885796309, 0.17260861699469388, 0.17388573288917542, 0.17264676000922918, 0.16905985004268587, 0.17174578201957047, 0.17430834216065705, 0.17442124197259545, 0.16962784412316978, 0.16742193698883057, 0.16527042910456657, 0.16968021309003234, 0.17162063694559038, 0.17132094991393387, 0.173185815801844, 0.1714154309593141, 0.18179996614344418, 0.1782018239609897, 0.16780850384384394, 0.1703497131820768, 0.169471190078184, 0.16663977410644293, 0.16438179602846503, 0.16683297813870013, 0.1643445969093591, 0.16135149309411645, 0.16459302883595228, 0.16523274290375412, 0.16286747390404344, 0.16350148478522897, 0.16200505592860281, 0.1709857580717653, 0.16761451005004346, 0.16616396303288639, 0.166737797902897, 0.1924440290313214, 0.17547689098864794, 0.16885714093223214, 0.17305082804523408, 0.16901269485242665, 0.16599569306708872, 0.16676359414123, 0.1704738351982087, 0.17209414206445217, 0.16906685894355178, 0.1844974011182785, 0.17227957816794515, 0.16701261093840003, 0.1651510470546782, 0.16334398416802287, 0.1659260841552168, 0.16834072209894657, 0.16541186487302184, 0.16308887396007776, 0.16766446083784103, 0.16723920707590878, 0.16827364405617118, 0.1673566410318017, 0.1651479760184884, 0.16475804196670651, 0.1678675040602684, 0.16816934198141098, 0.16508558811619878, 0.1620283080264926, 0.16689350921660662, 0.1654730171430856, 0.16348271700553596, 0.1703424728475511, 0.16746251587755978, 0.16515005892142653, 0.16315502091310918, 0.16668753093108535, 0.16740217595361173, 0.1732613279018551, 0.1715974099934101, 0.17600722913630307, 0.16816354705952108, 0.16284219711087644, 0.1668386571109295, 0.26089853304438293, 0.17749737901613116, 0.17614875989966094, 0.17490782402455807, 0.17549973004497588, 0.2706866869702935, 0.1747389929369092, 0.1745701681356877, 0.17851327289827168, 0.17881925706751645, 0.16646973486058414, 0.17436550580896437, 0.16421384108252823, 0.16550021502189338, 0.171231604879722, 0.17067976808175445, 0.16537883481942117, 0.1714862328954041, 0.1769477678462863, 0.1719079650938511, 0.17839804594404995, 0.17699783388525248, 0.17631777003407478, 0.1735320300795138, 0.16386231407523155, 0.16608921601437032, 0.16284962091594934, 0.17167232581414282, 0.16921558999456465, 0.17142937402240932, 0.1738984731491655, 0.16337531292811036, 0.16725571500137448, 0.16766350297257304, 0.17737764585763216, 0.17822922603227198, 0.16906966199167073, 0.1678544939495623, 0.1648281319066882, 0.17193027306348085, 0.16836393508128822, 0.16506498493254185, 0.16446278314106166, 0.16304423310793936, 0.1639139719773084, 0.16772148199379444, 0.16928392881527543, 0.1679623171221465, 0.16605971101671457, 0.17387982015497983, 0.16859008697792888, 0.1653944100253284, 0.1638758690096438, 0.1686646610032767, 0.16931758588179946, 0.16479622502811253, 0.16326161194592714, 0.16568793402984738, 0.16946709691546857, 0.177799210883677, 0.1784376569557935, 0.17172231199219823, 0.16396095091477036, 0.16472442005760968, 0.1669582629110664, 0.1661805750336498, 0.1670638311188668, 0.16283521591685712, 0.17173995287157595, 0.16638735099695623, 0.17853015987202525, 0.16425715293735266, 0.16337663982994854, 0.17587368516251445, 0.17767949309200048, 0.17221444309689105, 0.16133316699415445, 0.16836300701834261, 0.1675448331516236]
[0.0012837444493446009, 0.001319181652335349, 0.0013156916509389645, 0.0012949831556355537, 0.001289980922818415, 0.0014686236598406188, 0.001398805720197369, 0.0012750872013758319, 0.0012855962236491285, 0.0013145472089103026, 0.0012972479083308184, 0.001286618169685898, 0.0013167171092891648, 0.0013163016130786766, 0.0012910327903002269, 0.0012878013013089581, 0.0012726623099297285, 0.0013667055903825648, 0.0013273820396541625, 0.0012775317444502152, 0.0012761882935874453, 0.001267182712926883, 0.0013131941704787025, 0.001388596201156692, 0.0014116679617860752, 0.0013123252627605846, 0.0013792997897537642, 0.0013807016287663185, 0.0014864770233515622, 0.00135780727064829, 0.0013229943949065006, 0.0013356290544679229, 0.0013609942781572888, 0.0014059823403467042, 0.00142992603910235, 0.002041714325503901, 0.0015591835267408643, 0.0013659365724252408, 0.002149203914946826, 0.0013115762249212857, 0.0013280682402930057, 0.0013512331692776245, 0.001299063162517178, 0.0013155765586005624, 0.0013318130306090958, 0.0013588299151769904, 0.0013428755725423496, 0.0013380512945325106, 0.0013479514177455458, 0.00133834697681573, 0.0013105414731991152, 0.0013313626513144997, 0.0013512274586097446, 0.0013521026509503524, 0.0013149445280865874, 0.0012978444727816322, 0.0012811661170896633, 0.001315350489070018, 0.001330392534461941, 0.001328069379177782, 0.0013425257038902636, 0.00132880179038228, 0.0014093020631274742, 0.0013814094880696876, 0.0013008411150685576, 0.0013205404122641614, 0.0013137301556448372, 0.0012917811946235886, 0.0012742774885927523, 0.001293278900300001, 0.001273989123328365, 0.0012507867681714453, 0.0012759149522166844, 0.0012808739759980938, 0.0012625385573956855, 0.0012674533704281315, 0.0012558531467333552, 0.0013254709928043821, 0.0012993372872096393, 0.0012880927366890418, 0.0012925410690147053, 0.0014918141785373751, 0.0013602859766561855, 0.0013089700847459856, 0.0013414792871723573, 0.0013101759290885787, 0.001286788318349525, 0.001292741039854496, 0.0013215025984357263, 0.001334063116778699, 0.0013105958057639672, 0.0014302124117696008, 0.0013355006059530632, 0.001294671402623256, 0.0012802406748424666, 0.00126623243541103, 0.0012862487143815258, 0.00130496683797633, 0.001282262518395518, 0.0012642548368998277, 0.0012997245026189228, 0.0012964279618287503, 0.0013044468531486137, 0.0012973383025721061, 0.0012802168683603752, 0.0012771941237729187, 0.001301298481087352, 0.0013036383099334185, 0.0012797332412108433, 0.001256033395554206, 0.0012937481334620669, 0.0012827365670006635, 0.0012673078837638447, 0.001320484285639931, 0.0012981590378105408, 0.0012802330148947794, 0.0012647676039775906, 0.001292151402566553, 0.0012976912864621065, 0.0013431110690066287, 0.0013302124805690705, 0.0013643971250876206, 0.001303593388058303, 0.001262342613262608, 0.0012933229233405388, 0.0020224692484060694, 0.0013759486745436524, 0.0013654942627880694, 0.0013558746048415353, 0.0013604630236044642, 0.0020983464106224302, 0.0013545658367202264, 0.0013532571173309124, 0.0013838238209168347, 0.0013861957912210576, 0.0012904630609347609, 0.0013516705876663905, 0.0012729755122676608, 0.001292970429858542, 0.0013377469131228281, 0.0013334356881387066, 0.001292022147026728, 0.0013397361944953445, 0.0013824044362991117, 0.0013430309772957116, 0.0013937347339378903, 0.001382795577228535, 0.0013774825783912092, 0.0013557189849962015, 0.0012801743287127465, 0.0012975720001122681, 0.0012722626634058543, 0.0013411900454229908, 0.0013219967968325363, 0.0013392919845500728, 0.0013585818214778556, 0.0012763696322508622, 0.0013066852734482381, 0.0013098711169732269, 0.0013857628582627513, 0.0013924158283771249, 0.0013208567343099276, 0.0013113632339809556, 0.0012877197805210017, 0.0013432052583084442, 0.0013153432428225642, 0.0012895701947854832, 0.0012848654932895442, 0.0012737830711557763, 0.0012805779060727218, 0.001310324078076519, 0.0013225306938693393, 0.0013122056025167694, 0.0012973414923180826, 0.0013584360949607799, 0.0013171100545150694, 0.001292143828322878, 0.0012802802266378421, 0.0013176926640880993, 0.0013227936397015583, 0.0012874705080321291, 0.0012754813433275558, 0.0012944369846081827, 0.0013239616946520982, 0.0013890563350287266, 0.0013940441949671367, 0.0013415805624390487, 0.0012809449290216435, 0.0012869095317000756, 0.0013043614289927064, 0.001298285742450389, 0.0013051861806161469, 0.0012721501243504463, 0.0013417183818091871, 0.0012999011796637205, 0.0013947668740001973, 0.0012832590073230676, 0.001276379998671473, 0.0013740131653321441, 0.0013881210397812538, 0.0013454253366944613, 0.0012604153671418317, 0.0013153359923308017, 0.0013089440089970594]
[778.9712356774256, 758.0457158646034, 760.0565066186548, 772.2108165254231, 775.2052625826046, 680.9096348812221, 714.8955609495948, 784.2600874049947, 777.849204598259, 760.7182102109157, 770.8626805856328, 777.2313679077997, 759.4645751507355, 759.7043033785519, 774.5736649860397, 776.5173082086277, 785.7543923456145, 731.6864780805371, 753.3626116114551, 782.759414272206, 783.5834296747365, 789.1521797123036, 761.5020097412305, 720.1517613018142, 708.381876666505, 762.0062101802547, 725.0055480531338, 724.2694432782829, 672.731555409648, 736.4815475782115, 755.8611010371459, 748.7108764629053, 734.756946483232, 711.2464867470582, 699.3368696382084, 489.78448527719326, 641.3613169004444, 732.0984152466796, 465.2885624511536, 762.4413899847987, 752.9733560824966, 740.0647221638325, 769.7855107078187, 760.1229996555614, 750.8561464837574, 735.9272774545502, 744.6706310300864, 747.3555042965529, 741.8664996639894, 747.1903903270704, 763.0433835557575, 751.1101494492623, 740.0678498858241, 739.5888169416205, 760.4883541780489, 770.5083474730444, 780.5389064391048, 760.2536421353536, 751.6578559307959, 752.97271037083, 744.8646957762373, 752.5576856066037, 709.5710892389064, 723.8983144652856, 768.7333898170166, 757.2657305393844, 761.1913266230503, 774.1249092044488, 784.7584289543948, 773.2284194600489, 784.9360576858351, 799.4967851010477, 783.7512980490359, 780.7169313599109, 792.0550181554537, 788.9836607260835, 796.2714451137349, 754.4488000331394, 769.6231069821187, 776.3416185160975, 773.6698074609673, 670.3247726070238, 735.1395347456056, 763.9593995718067, 745.4457251500723, 763.2562755870871, 777.1285966308983, 773.550130436452, 756.7143652866883, 749.5897213728944, 763.0117505351576, 699.1968408124083, 748.7828875123292, 772.3967625868661, 781.1031313491502, 789.7444197718654, 777.454615751228, 766.3029978223389, 779.8715049795651, 790.9797699110835, 769.3938199864795, 771.3502249591971, 766.6084651791264, 770.8089694240873, 781.1176564801399, 782.9663332978167, 768.4632038949358, 767.0839314710486, 781.4128505826976, 796.157175071579, 772.9479750621958, 779.5832953746946, 789.0742358755373, 757.2979177979248, 770.3216407803067, 781.1078048804957, 790.6590877684421, 773.903118484209, 770.5993023396946, 744.5400630489961, 751.7595982652304, 732.9244408483935, 767.110365210962, 792.1779630138872, 773.2020997641397, 494.44509516676766, 726.7712949624813, 732.3355558874321, 737.531329541254, 735.0438656910798, 476.56573525596804, 738.2439250212252, 738.9578722278167, 722.6353419306389, 721.3988141741005, 774.9156332112593, 739.8252274812484, 785.5610656788001, 773.4128924428732, 747.525552247851, 749.9424298414143, 773.9805407370568, 746.4156033917429, 723.3773082189599, 744.5844637281347, 717.4966481423455, 723.1726919493392, 725.9619945015355, 737.6159890560225, 781.1436126871329, 770.6701438636763, 786.0012156003969, 745.6064883665426, 756.4314848537979, 746.6631709409835, 736.0616667991385, 783.4721030118151, 765.2952247338641, 763.4338882979122, 721.6241899091167, 718.176265753536, 757.084378664612, 762.5652253222486, 776.5664666542651, 744.4878538216436, 760.2578303851118, 775.4521654141887, 778.2915840005753, 785.06303203782, 780.8974333055623, 763.169979649571, 756.1261183846641, 762.0756976513675, 770.8070742524434, 736.1406279688641, 759.237997289587, 773.9076549225464, 781.0790006701196, 758.9023049559463, 755.9758151132438, 776.7168208990498, 784.0177398370541, 772.5366409417707, 755.3088613056689, 719.9132063850523, 717.3373725239564, 745.3894518134333, 780.6736865446488, 777.0553992858745, 766.6586712643369, 770.2464621637117, 766.1742170208423, 786.0707481442846, 745.3128864878407, 769.2892472477763, 716.9656941536012, 779.2659114749111, 783.4657398587062, 727.7950642912997, 720.398273163257, 743.2593788198404, 793.3892477585703, 760.2620211342198, 763.9746185676966]
Elapsed: 0.17201231040891407~0.014857208276459075
Time per graph: 0.0013367420911092592~0.00011465233634822919
Speed: 751.9564180632342~45.78713847997019
Total Time: 0.1683
best val loss: 0.3023592797360679 test_score: 0.9062

Testing...
Test loss: 0.3161 score: 0.9062 time: 0.16s
test Score 0.9062
Epoch Time List: [0.5803619169164449, 0.5856366232037544, 0.5843259361572564, 0.5830781981348991, 0.5824999718461186, 0.6045446319039911, 0.6213775579817593, 0.5785796199925244, 0.5724480061326176, 0.5789864638354629, 0.5744957595597953, 0.5756561388261616, 0.5781481952872127, 0.572430710773915, 0.5733379658777267, 0.5702907633967698, 0.5731216308195144, 0.5861424840986729, 0.5881437889765948, 0.5717533107381314, 0.5739075210876763, 0.5718136080540717, 0.6064810168463737, 0.6076601892709732, 0.6564280870370567, 0.5946530899964273, 0.6232895597349852, 0.6084456599783152, 0.648729955079034, 0.5940985230263323, 0.6941227652132511, 0.5951596919912845, 0.5993169962894171, 0.7112460129428655, 0.6352035140153021, 0.7089413541834801, 0.6278742318972945, 0.603398886276409, 0.7148205752018839, 0.589105969760567, 0.5966781289316714, 0.591825732961297, 0.5756831662729383, 0.5785979467909783, 0.5828904549125582, 0.5900998660363257, 0.5910145121160895, 0.5900227818638086, 0.5987259061075747, 0.5929337858688086, 0.5891345450654626, 0.5926934368908405, 0.5923525339458138, 0.5915933442302048, 0.5781651809811592, 0.5858652698807418, 0.5818386517930776, 0.5870854530949146, 0.5892537380568683, 0.5957367250230163, 0.5943335960619152, 0.593062796164304, 0.6204866750631481, 0.6079864618368447, 0.5766311669722199, 0.5774222041945904, 0.5715229192283005, 0.5742542711086571, 0.5681547462008893, 0.5708801739383489, 0.5788450767286122, 0.5772094374988228, 0.582483691861853, 0.5747541110031307, 0.5786673191469163, 0.5767863863147795, 0.5752122509293258, 0.6006325471680611, 0.5905571822077036, 0.5921309359837323, 0.6075403259601444, 0.6548862911295146, 0.636350145097822, 0.6367819830775261, 0.6028416950721294, 0.5888696250040084, 0.589294804027304, 0.5863083999138325, 0.593464540084824, 0.5948970741592348, 0.5926138667855412, 0.6272176078055054, 0.6082015361171216, 0.581276164855808, 0.5795155148953199, 0.5819513241294771, 0.5828572157770395, 0.5862661839928478, 0.5795529689639807, 0.5835194890387356, 0.5829888589214534, 0.5842927051708102, 0.5817003780975938, 0.5818709200248122, 0.5916574927978218, 0.5812945181969553, 0.5805175942368805, 0.5773959960788488, 0.5803056510630995, 0.5778286312706769, 0.5781383248977363, 0.5797938292380422, 0.576752666849643, 0.5811767547857016, 0.5776691089849919, 0.5805688800755888, 0.5792605467140675, 0.5797221302054822, 0.5775333971250802, 0.6008934800047427, 0.6033072990830988, 0.603007084922865, 0.5824755958747119, 0.6130957240238786, 0.5807942028623074, 0.6777542713098228, 0.6191890530753881, 0.6158946969080716, 0.676875727949664, 0.6154140059370548, 0.7102905081119388, 0.608139602933079, 0.6154210809618235, 0.6146698237862438, 0.6106815312523395, 0.5927173821255565, 0.6056684139184654, 0.5809664470143616, 0.5919655340258032, 0.597588392207399, 0.5930754479486495, 0.5819376581348479, 0.5896768132224679, 0.6046125579159707, 0.6115397689864039, 0.6108764631208032, 0.6121837750542909, 0.6141975689679384, 0.6088834470137954, 0.5719118129927665, 0.5550957880914211, 0.5612228920217603, 0.6018522221129388, 0.609207872999832, 0.6018845429643989, 0.6458980520255864, 0.5861557729076594, 0.5862016901373863, 0.5814970240462571, 0.6113206169102341, 0.6253656100016087, 0.5848177957814187, 0.5979112279601395, 0.5892876628786325, 0.6028657008428127, 0.5975017030723393, 0.5894740109797567, 0.595636097015813, 0.5895967939868569, 0.578709919936955, 0.5838896301575005, 0.5829682538751513, 0.5894421529956162, 0.5841230158694088, 0.6151849457528442, 0.5847263769246638, 0.5859766758512706, 0.5834787078201771, 0.5838960402179509, 0.5840592780150473, 0.5855844090692699, 0.5847544437274337, 0.5858427609782666, 0.5914199680555612, 0.6176843200810254, 0.6181820689234883, 0.6183050021063536, 0.5880343120079488, 0.5884653159882873, 0.5813755600247532, 0.580905486131087, 0.5905281249433756, 0.5895359942223877, 0.6068571000359952, 0.588105863891542, 0.6027573507744819, 0.5945224892348051, 0.5815976469311863, 0.6293634478934109, 0.6117864781990647, 0.6053774661850184, 0.569686742965132, 0.5763218011707067, 0.575432657962665]
Total Epoch List: [70, 68, 66]
Total Time List: [0.16738398396410048, 0.16484679304994643, 0.16828717198222876]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dad56800>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.25s
Epoch 8/1000, LR 0.000195
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6438;  Loss pred: 0.6438; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5039 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5039 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5039 time: 0.17s
Epoch 16/1000, LR 0.000285
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6774 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5014;  Loss pred: 0.5014; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6728 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6745 score: 0.5039 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4745;  Loss pred: 0.4745; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6670 score: 0.4961 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6693 score: 0.5039 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4497;  Loss pred: 0.4497; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6592 score: 0.4961 time: 0.18s
Test loss: 0.6624 score: 0.5271 time: 0.21s
Epoch 23/1000, LR 0.000285
Train loss: 0.4247;  Loss pred: 0.4247; Loss self: 0.0000; time: 0.24s
Val loss: 0.6496 score: 0.5039 time: 0.17s
Test loss: 0.6540 score: 0.5349 time: 0.18s
Epoch 24/1000, LR 0.000285
Train loss: 0.3954;  Loss pred: 0.3954; Loss self: 0.0000; time: 0.23s
Val loss: 0.6379 score: 0.5194 time: 0.17s
Test loss: 0.6437 score: 0.5426 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3702;  Loss pred: 0.3702; Loss self: 0.0000; time: 0.23s
Val loss: 0.6240 score: 0.5736 time: 0.17s
Test loss: 0.6314 score: 0.5581 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3416;  Loss pred: 0.3416; Loss self: 0.0000; time: 0.23s
Val loss: 0.6078 score: 0.5969 time: 0.17s
Test loss: 0.6170 score: 0.5736 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.3147;  Loss pred: 0.3147; Loss self: 0.0000; time: 0.23s
Val loss: 0.5883 score: 0.6124 time: 0.17s
Test loss: 0.5999 score: 0.5891 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.23s
Val loss: 0.5658 score: 0.6202 time: 0.17s
Test loss: 0.5802 score: 0.6047 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2660;  Loss pred: 0.2660; Loss self: 0.0000; time: 0.22s
Val loss: 0.5422 score: 0.6589 time: 0.17s
Test loss: 0.5598 score: 0.6512 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.2406;  Loss pred: 0.2406; Loss self: 0.0000; time: 0.22s
Val loss: 0.5153 score: 0.7442 time: 0.17s
Test loss: 0.5365 score: 0.6977 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 0.2155;  Loss pred: 0.2155; Loss self: 0.0000; time: 0.23s
Val loss: 0.4873 score: 0.8372 time: 0.17s
Test loss: 0.5118 score: 0.8140 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1851;  Loss pred: 0.1851; Loss self: 0.0000; time: 0.23s
Val loss: 0.4584 score: 0.8837 time: 0.17s
Test loss: 0.4858 score: 0.8527 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 0.23s
Val loss: 0.4306 score: 0.8915 time: 0.17s
Test loss: 0.4600 score: 0.8837 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 0.23s
Val loss: 0.4034 score: 0.8992 time: 0.17s
Test loss: 0.4344 score: 0.8992 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.1329;  Loss pred: 0.1329; Loss self: 0.0000; time: 0.23s
Val loss: 0.3759 score: 0.8992 time: 0.17s
Test loss: 0.4089 score: 0.9147 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 0.23s
Val loss: 0.3464 score: 0.9302 time: 0.17s
Test loss: 0.3821 score: 0.9147 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.23s
Val loss: 0.3163 score: 0.9612 time: 0.17s
Test loss: 0.3556 score: 0.9147 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.23s
Val loss: 0.2873 score: 0.9612 time: 0.17s
Test loss: 0.3308 score: 0.8915 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.25s
Val loss: 0.2611 score: 0.9535 time: 0.19s
Test loss: 0.3085 score: 0.8837 time: 0.18s
Epoch 40/1000, LR 0.000284
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.25s
Val loss: 0.2364 score: 0.9457 time: 0.18s
Test loss: 0.2890 score: 0.8837 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0756;  Loss pred: 0.0756; Loss self: 0.0000; time: 0.25s
Val loss: 0.2136 score: 0.9457 time: 0.18s
Test loss: 0.2726 score: 0.8837 time: 0.18s
Epoch 42/1000, LR 0.000284
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.25s
Val loss: 0.1962 score: 0.9457 time: 0.18s
Test loss: 0.2607 score: 0.8915 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.25s
Val loss: 0.1803 score: 0.9535 time: 0.18s
Test loss: 0.2505 score: 0.8760 time: 0.18s
Epoch 44/1000, LR 0.000284
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 0.25s
Val loss: 0.1669 score: 0.9535 time: 0.18s
Test loss: 0.2433 score: 0.8760 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.25s
Val loss: 0.1526 score: 0.9535 time: 0.18s
Test loss: 0.2373 score: 0.8760 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.0536;  Loss pred: 0.0536; Loss self: 0.0000; time: 0.25s
Val loss: 0.1404 score: 0.9457 time: 0.18s
Test loss: 0.2352 score: 0.8837 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.25s
Val loss: 0.1329 score: 0.9535 time: 0.18s
Test loss: 0.2367 score: 0.8837 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.25s
Val loss: 0.1267 score: 0.9535 time: 0.18s
Test loss: 0.2418 score: 0.8837 time: 0.18s
Epoch 49/1000, LR 0.000284
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.26s
Val loss: 0.1216 score: 0.9535 time: 0.18s
Test loss: 0.2490 score: 0.8837 time: 0.18s
Epoch 50/1000, LR 0.000284
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.24s
Val loss: 0.1183 score: 0.9535 time: 0.17s
Test loss: 0.2577 score: 0.8837 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.24s
Val loss: 0.1164 score: 0.9535 time: 0.17s
Test loss: 0.2691 score: 0.8992 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.24s
Val loss: 0.1148 score: 0.9535 time: 0.17s
Test loss: 0.2799 score: 0.8992 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.24s
Val loss: 0.1142 score: 0.9535 time: 0.17s
Test loss: 0.2885 score: 0.8915 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.24s
Val loss: 0.1143 score: 0.9535 time: 0.17s
Test loss: 0.2960 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.24s
Val loss: 0.1148 score: 0.9535 time: 0.17s
Test loss: 0.3013 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.24s
Val loss: 0.1154 score: 0.9535 time: 0.18s
Test loss: 0.3084 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.25s
Val loss: 0.1163 score: 0.9535 time: 0.18s
Test loss: 0.3166 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.25s
Val loss: 0.1178 score: 0.9535 time: 0.17s
Test loss: 0.3300 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.26s
Val loss: 0.1184 score: 0.9535 time: 0.18s
Test loss: 0.3478 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.25s
Val loss: 0.1194 score: 0.9535 time: 0.18s
Test loss: 0.3603 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.26s
Val loss: 0.1219 score: 0.9535 time: 0.18s
Test loss: 0.3711 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.26s
Val loss: 0.1267 score: 0.9535 time: 0.17s
Test loss: 0.3702 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.25s
Val loss: 0.1325 score: 0.9535 time: 0.18s
Test loss: 0.3722 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.26s
Val loss: 0.1368 score: 0.9535 time: 0.18s
Test loss: 0.3788 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.26s
Val loss: 0.1402 score: 0.9535 time: 0.18s
Test loss: 0.3902 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.25s
Val loss: 0.1422 score: 0.9535 time: 0.18s
Test loss: 0.3967 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.25s
Val loss: 0.1444 score: 0.9535 time: 0.18s
Test loss: 0.4037 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.25s
Val loss: 0.1465 score: 0.9535 time: 0.18s
Test loss: 0.4024 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.25s
Val loss: 0.1487 score: 0.9535 time: 0.18s
Test loss: 0.4018 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.25s
Val loss: 0.1515 score: 0.9535 time: 0.17s
Test loss: 0.3977 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.25s
Val loss: 0.1548 score: 0.9535 time: 0.17s
Test loss: 0.3962 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.25s
Val loss: 0.1581 score: 0.9535 time: 0.18s
Test loss: 0.3970 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.25s
Val loss: 0.1616 score: 0.9535 time: 0.18s
Test loss: 0.4018 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 052,   Train_Loss: 0.0407,   Val_Loss: 0.1142,   Val_Precision: 0.9531,   Val_Recall: 0.9531,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.1142,   Test_Precision: 0.9636,   Test_Recall: 0.8154,   Test_accuracy: 0.8833,   Test_Score: 0.8915,   Test_loss: 0.2885


[0.16513680713251233, 0.1625908080022782, 0.16293188487179577, 0.1648759429808706, 0.1661781871225685, 0.16744940890930593, 0.25139125902205706, 0.16589524503797293, 0.17781809507869184, 0.17140709096565843, 0.17332719499245286, 0.17789448401890695, 0.17559570306912065, 0.17587436106987298, 0.17392624402418733, 0.17672125506214797, 0.1735569341108203, 0.17269497993402183, 0.1728251320309937, 0.16780537785962224, 0.17750837397761643, 0.21514083305373788, 0.18346873903647065, 0.17016960983164608, 0.17252500797621906, 0.16945874504745007, 0.17172094783745706, 0.1704994668252766, 0.1679374019149691, 0.1686024758964777, 0.16710170498117805, 0.16926173213869333, 0.17181788804009557, 0.1674021880608052, 0.16407865309156477, 0.16399602289311588, 0.16721480409614742, 0.17201846092939377, 0.18640703102573752, 0.18913598381914198, 0.1821067298296839, 0.1831182159949094, 0.18521307199262083, 0.18080891901627183, 0.18488386180251837, 0.18407586589455605, 0.18130424502305686, 0.17967591295018792, 0.18082816014066339, 0.17573780892416835, 0.175816236063838, 0.17366329999640584, 0.17187291802838445, 0.1760670889634639, 0.17705730791203678, 0.1766014718450606, 0.1740925470367074, 0.1851015631109476, 0.17807589611038566, 0.17446918017230928, 0.17339082807302475, 0.17902532685548067, 0.1849128280300647, 0.1756397148128599, 0.17614587605930865, 0.17763219913467765, 0.17890353105030954, 0.17394205392338336, 0.17328750505112112, 0.17576228315010667, 0.17744695488363504, 0.17667420813813806, 0.17370363208465278]
[0.0012801302878489328, 0.0012603938604827769, 0.001263037867223223, 0.001278108085123028, 0.0012882030009501433, 0.0012980574334054723, 0.001948769449783388, 0.0012860096514571546, 0.0013784348455712545, 0.00132873713926867, 0.0013436216666081616, 0.0013790270078985036, 0.001361207000535819, 0.0013633671400765347, 0.0013482654575518397, 0.0013699322097840928, 0.0013454025900063588, 0.0013387207746823398, 0.0013397297056666178, 0.0013008168826327306, 0.0013760339068032282, 0.00166775839576541, 0.0014222382871044237, 0.0013191442622608224, 0.0013374031626063492, 0.0013136336825383726, 0.001331170138274861, 0.0013217012932191985, 0.0013018403249222409, 0.0013069959371819976, 0.0012953620541176593, 0.0013121064506875452, 0.0013319216127139191, 0.0012976913803163194, 0.0012719275433454634, 0.0012712869991714408, 0.0012962387914430033, 0.00133347644131313, 0.001445015744385562, 0.001466170417202651, 0.0014116800761991, 0.001419521054224104, 0.0014357602480048126, 0.0014016195272579211, 0.0014332082310272742, 0.0014269446968570236, 0.0014054592637446268, 0.0013928365344975807, 0.0014017686832609566, 0.0013623085963113824, 0.001362916558634403, 0.001346227131755084, 0.0013323482017704221, 0.001364861154755534, 0.001372537270635944, 0.001369003657713648, 0.0013495546281915302, 0.0014348958380693614, 0.0013804333031812843, 0.0013524742649016223, 0.0013441149463025175, 0.0013877932314378346, 0.0014334327754268582, 0.0013615481768438752, 0.0013654719074365012, 0.0013769937917416872, 0.0013868490779093763, 0.0013483880149099486, 0.0013433139926443498, 0.001362498318993075, 0.0013755577897956205, 0.0013695675049468067, 0.0013465397836019595]
[781.1704867012796, 793.4027857109393, 791.7418993925263, 782.4064424909276, 776.2751672387251, 770.3819370892439, 513.1443332668999, 777.5991407738799, 725.460476578113, 752.5943020983028, 744.2571259842808, 725.1489595725163, 734.6421224739256, 733.4781443711952, 741.6937031196994, 729.9631272686145, 743.2719450876585, 746.9817596856868, 746.4192185709755, 768.747710266563, 726.7262783685164, 599.6072348003709, 703.1170578566895, 758.0672020557818, 747.7176875005937, 761.247228426475, 751.2187745557129, 756.6006064534843, 768.1433589482106, 765.1133194461883, 771.9849418324623, 762.1332853565341, 750.7949345175076, 770.5992466068816, 786.2083066223795, 786.6044415240211, 771.4627942022755, 749.9195104003924, 692.0339822492466, 682.0489543827579, 708.37579764706, 704.4629574350273, 696.4951156640799, 713.4603796198278, 697.7353174166706, 700.7980072406391, 711.5111948073514, 717.9593406922777, 713.3844634577542, 734.0480730339825, 733.7206329065124, 742.8167033718108, 750.5545462298832, 732.6752589563691, 728.5776651709176, 730.4582382709515, 740.9851954937528, 696.9146982442227, 724.4102251774463, 739.3856030766981, 743.983989428038, 720.5684372476308, 697.6260185638721, 734.4580360850993, 732.3475456022908, 726.2196866807602, 721.05899331704, 741.6262892745932, 744.4275913715996, 733.9458596462919, 726.9778175939663, 730.1575105922504, 742.6442294374887]
Elapsed: 0.17614246175101358~0.011734710001381366
Time per graph: 0.0013654454399303379~9.096674419675475e-05
Speed: 734.9986486657889~40.1145658940685
Total Time: 0.1741
best val loss: 0.11423016726508621 test_score: 0.8915

Testing...
Test loss: 0.3556 score: 0.9147 time: 0.18s
test Score 0.9147
Epoch Time List: [0.5611108120065182, 0.5528009713161737, 0.5511258579790592, 0.561831169994548, 0.6545314539689571, 0.5553772270213813, 0.6582361969631165, 0.5541657160501927, 0.5693506158422679, 0.6551428961101919, 0.5765119874849916, 0.585587803972885, 0.673010963248089, 0.5919618050102144, 0.5897446051239967, 0.5835394349414855, 0.5837383782491088, 0.5802018269896507, 0.5781370920594782, 0.5719184780027717, 0.7043282191734761, 0.6289079480338842, 0.5860825430136174, 0.5703710080124438, 0.5681690878700465, 0.5671553439460695, 0.5707890051417053, 0.5680720016825944, 0.5516806687228382, 0.5555320740677416, 0.557117328979075, 0.5627690460532904, 0.5629045451059937, 0.5595513661392033, 0.5553761690389365, 0.5561783029697835, 0.5582920969463885, 0.5627334201708436, 0.6212373720481992, 0.6110850430559367, 0.6161071427632123, 0.6138836720492691, 0.6152214000467211, 0.6141059324145317, 0.6149680779781193, 0.6132218688726425, 0.6129472611937672, 0.6113875657320023, 0.6134861761238426, 0.5856964141130447, 0.587895120959729, 0.5865757840219885, 0.5817540460266173, 0.5865511819720268, 0.5895946139935404, 0.5906168918590993, 0.5937461403664201, 0.6034877009224147, 0.6057819393463433, 0.6012193199712783, 0.6081467899493873, 0.6063306389842182, 0.6068216823041439, 0.6063891998492181, 0.6094295959919691, 0.6018798572476953, 0.6048362269066274, 0.5939806597307324, 0.5958243280183524, 0.5952026750892401, 0.599190074717626, 0.6025279865134507, 0.5977141666226089]
Total Epoch List: [73]
Total Time List: [0.1741329610813409]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756d836c7c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.18s
Epoch 2/1000, LR 0.000015
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
Epoch 5/1000, LR 0.000105
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.18s
Epoch 7/1000, LR 0.000165
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6346;  Loss pred: 0.6346; Loss self: 0.0000; time: 0.24s
Val loss: 0.6915 score: 0.7209 time: 0.16s
Test loss: 0.6917 score: 0.6589 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.24s
Val loss: 0.6904 score: 0.5891 time: 0.16s
Test loss: 0.6909 score: 0.5426 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6036;  Loss pred: 0.6036; Loss self: 0.0000; time: 0.24s
Val loss: 0.6891 score: 0.5194 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4961 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4961 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5317;  Loss pred: 0.5317; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.4961 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6826 score: 0.4961 time: 0.18s
Epoch 18/1000, LR 0.000285
Train loss: 0.4827;  Loss pred: 0.4827; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6798 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4576;  Loss pred: 0.4576; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6722 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.4961 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4284;  Loss pred: 0.4284; Loss self: 0.0000; time: 0.25s
Val loss: 0.6671 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6723 score: 0.4961 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4009;  Loss pred: 0.4009; Loss self: 0.0000; time: 0.25s
Val loss: 0.6607 score: 0.5271 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6670 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.3720;  Loss pred: 0.3720; Loss self: 0.0000; time: 0.26s
Val loss: 0.6525 score: 0.5426 time: 0.16s
Test loss: 0.6601 score: 0.5194 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3481;  Loss pred: 0.3481; Loss self: 0.0000; time: 0.25s
Val loss: 0.6423 score: 0.5504 time: 0.16s
Test loss: 0.6514 score: 0.5349 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.25s
Val loss: 0.6309 score: 0.5814 time: 0.17s
Test loss: 0.6414 score: 0.5349 time: 0.26s
Epoch 25/1000, LR 0.000285
Train loss: 0.3017;  Loss pred: 0.3017; Loss self: 0.0000; time: 0.26s
Val loss: 0.6180 score: 0.5891 time: 0.18s
Test loss: 0.6300 score: 0.5504 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2640;  Loss pred: 0.2640; Loss self: 0.0000; time: 0.26s
Val loss: 0.6030 score: 0.6202 time: 0.18s
Test loss: 0.6167 score: 0.5581 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.27s
Val loss: 0.5866 score: 0.6357 time: 0.29s
Test loss: 0.6018 score: 0.6047 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.24s
Val loss: 0.5692 score: 0.6667 time: 0.17s
Test loss: 0.5862 score: 0.6357 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 0.27s
Val loss: 0.5473 score: 0.7209 time: 0.18s
Test loss: 0.5663 score: 0.6822 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.1802;  Loss pred: 0.1802; Loss self: 0.0000; time: 0.33s
Val loss: 0.5248 score: 0.7597 time: 0.18s
Test loss: 0.5456 score: 0.7364 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1597;  Loss pred: 0.1597; Loss self: 0.0000; time: 0.24s
Val loss: 0.5002 score: 0.8295 time: 0.18s
Test loss: 0.5229 score: 0.8140 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.1461;  Loss pred: 0.1461; Loss self: 0.0000; time: 0.26s
Val loss: 0.4747 score: 0.8605 time: 0.28s
Test loss: 0.4990 score: 0.8450 time: 0.19s
Epoch 33/1000, LR 0.000285
Train loss: 0.1312;  Loss pred: 0.1312; Loss self: 0.0000; time: 0.26s
Val loss: 0.4483 score: 0.9070 time: 0.17s
Test loss: 0.4742 score: 0.8915 time: 0.19s
Epoch 34/1000, LR 0.000285
Train loss: 0.1180;  Loss pred: 0.1180; Loss self: 0.0000; time: 0.25s
Val loss: 0.4211 score: 0.9070 time: 0.20s
Test loss: 0.4490 score: 0.8915 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.25s
Val loss: 0.3942 score: 0.9147 time: 0.17s
Test loss: 0.4238 score: 0.9070 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.0946;  Loss pred: 0.0946; Loss self: 0.0000; time: 0.25s
Val loss: 0.3660 score: 0.9302 time: 0.17s
Test loss: 0.3981 score: 0.9225 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.25s
Val loss: 0.3390 score: 0.9457 time: 0.16s
Test loss: 0.3736 score: 0.9147 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.25s
Val loss: 0.3160 score: 0.9457 time: 0.17s
Test loss: 0.3535 score: 0.9070 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.25s
Val loss: 0.2944 score: 0.9380 time: 0.17s
Test loss: 0.3354 score: 0.9147 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.25s
Val loss: 0.2747 score: 0.9380 time: 0.17s
Test loss: 0.3196 score: 0.9070 time: 0.17s
Epoch 41/1000, LR 0.000284
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.25s
Val loss: 0.2571 score: 0.9457 time: 0.17s
Test loss: 0.3060 score: 0.9070 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.25s
Val loss: 0.2416 score: 0.9457 time: 0.16s
Test loss: 0.2951 score: 0.9070 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.25s
Val loss: 0.2286 score: 0.9457 time: 0.17s
Test loss: 0.2871 score: 0.9070 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.24s
Val loss: 0.2157 score: 0.9457 time: 0.17s
Test loss: 0.2810 score: 0.9070 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.25s
Val loss: 0.2051 score: 0.9457 time: 0.17s
Test loss: 0.2782 score: 0.9070 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.25s
Val loss: 0.1936 score: 0.9457 time: 0.16s
Test loss: 0.2757 score: 0.8837 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.25s
Val loss: 0.1853 score: 0.9457 time: 0.17s
Test loss: 0.2767 score: 0.8915 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.25s
Val loss: 0.1810 score: 0.9457 time: 0.16s
Test loss: 0.2826 score: 0.8915 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.24s
Val loss: 0.1806 score: 0.9380 time: 0.17s
Test loss: 0.2922 score: 0.8915 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0214;  Loss pred: 0.0214; Loss self: 0.0000; time: 0.25s
Val loss: 0.1814 score: 0.9380 time: 0.17s
Test loss: 0.3025 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.25s
Val loss: 0.1847 score: 0.9380 time: 0.16s
Test loss: 0.3148 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.25s
Val loss: 0.1889 score: 0.9380 time: 0.16s
Test loss: 0.3277 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.25s
Val loss: 0.1951 score: 0.9302 time: 0.16s
Test loss: 0.3421 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.25s
Val loss: 0.2007 score: 0.9302 time: 0.16s
Test loss: 0.3557 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.24s
Val loss: 0.1997 score: 0.9380 time: 0.17s
Test loss: 0.3638 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.24s
Val loss: 0.1991 score: 0.9380 time: 0.17s
Test loss: 0.3719 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.25s
Val loss: 0.2047 score: 0.9380 time: 0.17s
Test loss: 0.3824 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.25s
Val loss: 0.2114 score: 0.9380 time: 0.16s
Test loss: 0.3906 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.25s
Val loss: 0.2113 score: 0.9380 time: 0.16s
Test loss: 0.3914 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.24s
Val loss: 0.2065 score: 0.9380 time: 0.16s
Test loss: 0.3868 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.24s
Val loss: 0.2035 score: 0.9380 time: 0.17s
Test loss: 0.3867 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.25s
Val loss: 0.1996 score: 0.9380 time: 0.17s
Test loss: 0.3885 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.24s
Val loss: 0.1957 score: 0.9380 time: 0.16s
Test loss: 0.3907 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.24s
Val loss: 0.1930 score: 0.9380 time: 0.16s
Test loss: 0.3950 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.25s
Val loss: 0.1926 score: 0.9457 time: 0.17s
Test loss: 0.3999 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.25s
Val loss: 0.1920 score: 0.9457 time: 0.17s
Test loss: 0.4039 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.25s
Val loss: 0.1918 score: 0.9457 time: 0.16s
Test loss: 0.4077 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.25s
Val loss: 0.1920 score: 0.9457 time: 0.16s
Test loss: 0.4117 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.1936 score: 0.9457 time: 0.17s
Test loss: 0.4169 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 048,   Train_Loss: 0.0269,   Val_Loss: 0.1806,   Val_Precision: 1.0000,   Val_Recall: 0.8769,   Val_accuracy: 0.9344,   Val_Score: 0.9380,   Val_Loss: 0.1806,   Test_Precision: 0.9464,   Test_Recall: 0.8281,   Test_accuracy: 0.8833,   Test_Score: 0.8915,   Test_loss: 0.2922


[0.16513680713251233, 0.1625908080022782, 0.16293188487179577, 0.1648759429808706, 0.1661781871225685, 0.16744940890930593, 0.25139125902205706, 0.16589524503797293, 0.17781809507869184, 0.17140709096565843, 0.17332719499245286, 0.17789448401890695, 0.17559570306912065, 0.17587436106987298, 0.17392624402418733, 0.17672125506214797, 0.1735569341108203, 0.17269497993402183, 0.1728251320309937, 0.16780537785962224, 0.17750837397761643, 0.21514083305373788, 0.18346873903647065, 0.17016960983164608, 0.17252500797621906, 0.16945874504745007, 0.17172094783745706, 0.1704994668252766, 0.1679374019149691, 0.1686024758964777, 0.16710170498117805, 0.16926173213869333, 0.17181788804009557, 0.1674021880608052, 0.16407865309156477, 0.16399602289311588, 0.16721480409614742, 0.17201846092939377, 0.18640703102573752, 0.18913598381914198, 0.1821067298296839, 0.1831182159949094, 0.18521307199262083, 0.18080891901627183, 0.18488386180251837, 0.18407586589455605, 0.18130424502305686, 0.17967591295018792, 0.18082816014066339, 0.17573780892416835, 0.175816236063838, 0.17366329999640584, 0.17187291802838445, 0.1760670889634639, 0.17705730791203678, 0.1766014718450606, 0.1740925470367074, 0.1851015631109476, 0.17807589611038566, 0.17446918017230928, 0.17339082807302475, 0.17902532685548067, 0.1849128280300647, 0.1756397148128599, 0.17614587605930865, 0.17763219913467765, 0.17890353105030954, 0.17394205392338336, 0.17328750505112112, 0.17576228315010667, 0.17744695488363504, 0.17667420813813806, 0.17370363208465278, 0.18137988215312362, 0.1794392259325832, 0.17283030203543603, 0.16992356209084392, 0.1700322930701077, 0.1819217458833009, 0.17725671408697963, 0.18003421602770686, 0.16600557509809732, 0.1666803101543337, 0.16463515697978437, 0.16807918692938983, 0.16994531592354178, 0.169624756090343, 0.1672665812075138, 0.16624478297308087, 0.18061179900541902, 0.1740938441362232, 0.17214995087124407, 0.16895056096836925, 0.1713015481363982, 0.1780432779341936, 0.1714305451605469, 0.263135754968971, 0.17389090405777097, 0.18745816289447248, 0.17463224101811647, 0.1714844130910933, 0.19336803420446813, 0.1703928359784186, 0.18368527200073004, 0.1897545619867742, 0.18996305600740016, 0.17269789101555943, 0.1689623270649463, 0.17010197113268077, 0.17498920508660376, 0.17644706298597157, 0.17314821598120034, 0.17023564712144434, 0.16987817105837166, 0.17582867899909616, 0.1747259870171547, 0.17075598798692226, 0.1695457468740642, 0.16869553504511714, 0.1689377441070974, 0.17534376098774374, 0.17332946392707527, 0.16929671703837812, 0.16969976597465575, 0.17132658301852643, 0.1689866369124502, 0.17289181286469102, 0.17330609587952495, 0.17123153083957732, 0.1696149550843984, 0.17106358101591468, 0.1724497200921178, 0.17109293793328106, 0.17010576091706753, 0.16584239294752479, 0.16950745810754597, 0.1719455469865352, 0.17095459415577352, 0.16757261101156473, 0.16905324184335768, 0.17281981697306037, 0.17128820391371846]
[0.0012801302878489328, 0.0012603938604827769, 0.001263037867223223, 0.001278108085123028, 0.0012882030009501433, 0.0012980574334054723, 0.001948769449783388, 0.0012860096514571546, 0.0013784348455712545, 0.00132873713926867, 0.0013436216666081616, 0.0013790270078985036, 0.001361207000535819, 0.0013633671400765347, 0.0013482654575518397, 0.0013699322097840928, 0.0013454025900063588, 0.0013387207746823398, 0.0013397297056666178, 0.0013008168826327306, 0.0013760339068032282, 0.00166775839576541, 0.0014222382871044237, 0.0013191442622608224, 0.0013374031626063492, 0.0013136336825383726, 0.001331170138274861, 0.0013217012932191985, 0.0013018403249222409, 0.0013069959371819976, 0.0012953620541176593, 0.0013121064506875452, 0.0013319216127139191, 0.0012976913803163194, 0.0012719275433454634, 0.0012712869991714408, 0.0012962387914430033, 0.00133347644131313, 0.001445015744385562, 0.001466170417202651, 0.0014116800761991, 0.001419521054224104, 0.0014357602480048126, 0.0014016195272579211, 0.0014332082310272742, 0.0014269446968570236, 0.0014054592637446268, 0.0013928365344975807, 0.0014017686832609566, 0.0013623085963113824, 0.001362916558634403, 0.001346227131755084, 0.0013323482017704221, 0.001364861154755534, 0.001372537270635944, 0.001369003657713648, 0.0013495546281915302, 0.0014348958380693614, 0.0013804333031812843, 0.0013524742649016223, 0.0013441149463025175, 0.0013877932314378346, 0.0014334327754268582, 0.0013615481768438752, 0.0013654719074365012, 0.0013769937917416872, 0.0013868490779093763, 0.0013483880149099486, 0.0013433139926443498, 0.001362498318993075, 0.0013755577897956205, 0.0013695675049468067, 0.0013465397836019595, 0.0014060455980862295, 0.0013910017514153737, 0.0013397697832204345, 0.0013172369154328986, 0.0013180797912411449, 0.0014102460921186115, 0.0013740830549378265, 0.0013956140777341617, 0.0012868649232410645, 0.0012920954275529744, 0.0012762415269750725, 0.0013029394335611615, 0.0013174055497948976, 0.0013149205898476202, 0.0012966401643993318, 0.00128871924785334, 0.001400091465158287, 0.0013495646832265365, 0.0013344957431879385, 0.0013096942710726298, 0.0013279189778015363, 0.0013801804491022761, 0.0013289189547329217, 0.002039812054023031, 0.001347991504323806, 0.001453164053445523, 0.0013537383024660192, 0.0013293365355898706, 0.0014989770093369623, 0.001320874697507121, 0.0014239168372149615, 0.0014709655967966994, 0.0014725818295147298, 0.001338743341205887, 0.0013097854811236146, 0.0013186199312610913, 0.001356505465787626, 0.001367806689813733, 0.0013422342324124058, 0.0013196561792360028, 0.0013168850469641215, 0.0013630130154968694, 0.0013544650156368581, 0.0013236898293559865, 0.0013143081153028232, 0.0013077173259311407, 0.0013095949155588944, 0.0013592539611453002, 0.0013436392552486456, 0.0013123776514602954, 0.0013155020618190367, 0.0013281130466552436, 0.0013099739295538775, 0.0013402466113541938, 0.0013434581075932167, 0.001327376208058739, 0.0013148446130573518, 0.0013260742714411992, 0.0013368195355978124, 0.0013263018444440392, 0.001318649309434632, 0.0012855999453296495, 0.001314011303159271, 0.0013329112169498851, 0.0013252294120602599, 0.0012990124884617422, 0.001310490246847734, 0.0013396885036671346, 0.0013278155342148718]
[781.1704867012796, 793.4027857109393, 791.7418993925263, 782.4064424909276, 776.2751672387251, 770.3819370892439, 513.1443332668999, 777.5991407738799, 725.460476578113, 752.5943020983028, 744.2571259842808, 725.1489595725163, 734.6421224739256, 733.4781443711952, 741.6937031196994, 729.9631272686145, 743.2719450876585, 746.9817596856868, 746.4192185709755, 768.747710266563, 726.7262783685164, 599.6072348003709, 703.1170578566895, 758.0672020557818, 747.7176875005937, 761.247228426475, 751.2187745557129, 756.6006064534843, 768.1433589482106, 765.1133194461883, 771.9849418324623, 762.1332853565341, 750.7949345175076, 770.5992466068816, 786.2083066223795, 786.6044415240211, 771.4627942022755, 749.9195104003924, 692.0339822492466, 682.0489543827579, 708.37579764706, 704.4629574350273, 696.4951156640799, 713.4603796198278, 697.7353174166706, 700.7980072406391, 711.5111948073514, 717.9593406922777, 713.3844634577542, 734.0480730339825, 733.7206329065124, 742.8167033718108, 750.5545462298832, 732.6752589563691, 728.5776651709176, 730.4582382709515, 740.9851954937528, 696.9146982442227, 724.4102251774463, 739.3856030766981, 743.983989428038, 720.5684372476308, 697.6260185638721, 734.4580360850993, 732.3475456022908, 726.2196866807602, 721.05899331704, 741.6262892745932, 744.4275913715996, 733.9458596462919, 726.9778175939663, 730.1575105922504, 742.6442294374887, 711.2144878950592, 718.9063557845839, 746.3968903644608, 759.164876328537, 758.6794112504896, 709.0960971908816, 727.7580466526074, 716.5304620769821, 777.0823354804216, 773.9366448295872, 783.5507455788436, 767.4953833171086, 759.0676995065693, 760.5021989319409, 771.2239890881752, 775.9641998563544, 714.2390514372182, 740.979674726818, 749.3467139963509, 763.5369735419301, 753.0579927817363, 724.5429397659121, 752.4913362387657, 490.24124454394916, 741.8444380342224, 688.1535485473585, 738.695210276878, 752.2549581895505, 667.1216394721937, 757.0740827175312, 702.2882052268584, 679.825552805372, 679.0794100247298, 746.9691681896543, 763.4838028148994, 758.3686370064405, 737.1883307667848, 731.0974624171339, 745.0264460940576, 757.773134953179, 759.3677233296469, 733.6687094183487, 738.2988770144118, 755.4639899941884, 760.8565969857037, 764.6912525900541, 763.5949010791868, 735.6976904870708, 744.2473834354789, 761.9757917145954, 760.1660453631141, 752.9479531267518, 763.3739706106658, 746.1313399551098, 744.3477354061183, 753.3659213784463, 760.5461436806155, 754.1055742776638, 748.0441251577088, 753.9761813565004, 758.3517413198721, 777.846952804267, 761.0284611674989, 750.2375156601282, 754.5863311661308, 769.8155397906719, 763.0732105068388, 746.4421746269346, 753.1166598312868]
Elapsed: 0.1752656298364893~0.012007034188415056
Time per graph: 0.0013586482933061186~9.307778440631826e-05
Speed: 738.700828341829~39.62594098968689
Total Time: 0.1719
best val loss: 0.1806146790121877 test_score: 0.8915

Testing...
Test loss: 0.3736 score: 0.9147 time: 0.16s
test Score 0.9147
Epoch Time List: [0.5611108120065182, 0.5528009713161737, 0.5511258579790592, 0.561831169994548, 0.6545314539689571, 0.5553772270213813, 0.6582361969631165, 0.5541657160501927, 0.5693506158422679, 0.6551428961101919, 0.5765119874849916, 0.585587803972885, 0.673010963248089, 0.5919618050102144, 0.5897446051239967, 0.5835394349414855, 0.5837383782491088, 0.5802018269896507, 0.5781370920594782, 0.5719184780027717, 0.7043282191734761, 0.6289079480338842, 0.5860825430136174, 0.5703710080124438, 0.5681690878700465, 0.5671553439460695, 0.5707890051417053, 0.5680720016825944, 0.5516806687228382, 0.5555320740677416, 0.557117328979075, 0.5627690460532904, 0.5629045451059937, 0.5595513661392033, 0.5553761690389365, 0.5561783029697835, 0.5582920969463885, 0.5627334201708436, 0.6212373720481992, 0.6110850430559367, 0.6161071427632123, 0.6138836720492691, 0.6152214000467211, 0.6141059324145317, 0.6149680779781193, 0.6132218688726425, 0.6129472611937672, 0.6113875657320023, 0.6134861761238426, 0.5856964141130447, 0.587895120959729, 0.5865757840219885, 0.5817540460266173, 0.5865511819720268, 0.5895946139935404, 0.5906168918590993, 0.5937461403664201, 0.6034877009224147, 0.6057819393463433, 0.6012193199712783, 0.6081467899493873, 0.6063306389842182, 0.6068216823041439, 0.6063891998492181, 0.6094295959919691, 0.6018798572476953, 0.6048362269066274, 0.5939806597307324, 0.5958243280183524, 0.5952026750892401, 0.599190074717626, 0.6025279865134507, 0.5977141666226089, 0.6159767089411616, 0.5987415842246264, 0.5751202590763569, 0.5678312827367336, 0.5698179309256375, 0.5849932131823152, 0.6051828418858349, 0.6020742917899042, 0.565770179964602, 0.5658461079001427, 0.5646517928689718, 0.565255468012765, 0.5698888972401619, 0.5685202197637409, 0.5644003709312528, 0.5651132161729038, 0.5970440679229796, 0.6017000251449645, 0.5893816237803549, 0.5757993089500815, 0.5892662359401584, 0.5934899910353124, 0.5805084831081331, 0.681008392944932, 0.6084193079732358, 0.6190838189795613, 0.7327257620636374, 0.5765840578824282, 0.6378762307576835, 0.6683798530139029, 0.6061258749105036, 0.7234077951870859, 0.6161963539198041, 0.6191683812066913, 0.5791972088627517, 0.5802371867466718, 0.588502902770415, 0.5895056212320924, 0.5840925448574126, 0.5850902209058404, 0.5841694138944149, 0.5882501909509301, 0.5860522750299424, 0.5790038746781647, 0.5823789709247649, 0.5770106832496822, 0.5802501789294183, 0.5869474988430738, 0.5779579947702587, 0.587284040171653, 0.582914050668478, 0.5834205981809646, 0.5745477681048214, 0.5767530414741486, 0.5781678839121014, 0.5755154979415238, 0.5783676290884614, 0.5799775207415223, 0.5745352411177009, 0.5737728420644999, 0.5741738858632743, 0.5741239769849926, 0.5713682130444795, 0.5728718400932848, 0.578167409170419, 0.575203875079751, 0.5774573448579758, 0.5774042967241257, 0.577305750688538]
Total Epoch List: [73, 69]
Total Time List: [0.1741329610813409, 0.17187710897997022]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7756dd7d00a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.16s
Epoch 2/1000, LR 0.000020
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.15s
Epoch 3/1000, LR 0.000050
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.15s
Epoch 4/1000, LR 0.000080
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.26s
Val loss: 0.6931 score: 0.4884 time: 0.16s
Test loss: 0.6931 score: 0.5156 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.16s
Epoch 7/1000, LR 0.000170
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.15s
Epoch 8/1000, LR 0.000200
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.15s
Epoch 9/1000, LR 0.000230
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.16s
Epoch 10/1000, LR 0.000260
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.15s
Epoch 11/1000, LR 0.000290
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.15s
Epoch 12/1000, LR 0.000290
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.15s
Epoch 13/1000, LR 0.000290
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.15s
Epoch 14/1000, LR 0.000290
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.15s
Epoch 15/1000, LR 0.000290
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5039 time: 0.17s
Test loss: 0.6884 score: 0.5078 time: 0.15s
Epoch 18/1000, LR 0.000290
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5039 time: 0.17s
Test loss: 0.6863 score: 0.5078 time: 0.15s
Epoch 19/1000, LR 0.000290
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5039 time: 0.16s
Test loss: 0.6835 score: 0.5078 time: 0.15s
Epoch 20/1000, LR 0.000290
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.5039 time: 0.16s
Test loss: 0.6793 score: 0.5156 time: 0.16s
Epoch 21/1000, LR 0.000290
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.26s
Val loss: 0.6746 score: 0.5116 time: 0.16s
Test loss: 0.6734 score: 0.5312 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.26s
Val loss: 0.6668 score: 0.5271 time: 0.17s
Test loss: 0.6651 score: 0.5625 time: 0.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.26s
Val loss: 0.6570 score: 0.5969 time: 0.16s
Test loss: 0.6544 score: 0.6328 time: 0.15s
Epoch 24/1000, LR 0.000290
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.26s
Val loss: 0.6451 score: 0.8372 time: 0.16s
Test loss: 0.6415 score: 0.8750 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 0.4914;  Loss pred: 0.4914; Loss self: 0.0000; time: 0.26s
Val loss: 0.6340 score: 0.9225 time: 0.16s
Test loss: 0.6297 score: 0.9297 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 0.4772;  Loss pred: 0.4772; Loss self: 0.0000; time: 0.26s
Val loss: 0.6169 score: 0.9225 time: 0.17s
Test loss: 0.6116 score: 0.9297 time: 0.15s
Epoch 27/1000, LR 0.000290
Train loss: 0.4567;  Loss pred: 0.4567; Loss self: 0.0000; time: 0.26s
Val loss: 0.5921 score: 0.9302 time: 0.16s
Test loss: 0.5853 score: 0.9375 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.4267;  Loss pred: 0.4267; Loss self: 0.0000; time: 0.26s
Val loss: 0.5649 score: 0.9302 time: 0.17s
Test loss: 0.5565 score: 0.9375 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.4074;  Loss pred: 0.4074; Loss self: 0.0000; time: 0.27s
Val loss: 0.5345 score: 0.9147 time: 0.18s
Test loss: 0.5243 score: 0.9297 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.3867;  Loss pred: 0.3867; Loss self: 0.0000; time: 0.27s
Val loss: 0.4990 score: 0.9302 time: 0.17s
Test loss: 0.4868 score: 0.9297 time: 0.17s
Epoch 31/1000, LR 0.000290
Train loss: 0.3577;  Loss pred: 0.3577; Loss self: 0.0000; time: 0.27s
Val loss: 0.4733 score: 0.8915 time: 0.17s
Test loss: 0.4591 score: 0.9062 time: 0.17s
Epoch 32/1000, LR 0.000290
Train loss: 0.3363;  Loss pred: 0.3363; Loss self: 0.0000; time: 0.26s
Val loss: 0.4316 score: 0.9147 time: 0.18s
Test loss: 0.4173 score: 0.9219 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.27s
Val loss: 0.3938 score: 0.9380 time: 0.18s
Test loss: 0.3800 score: 0.9375 time: 0.17s
Epoch 34/1000, LR 0.000290
Train loss: 0.2936;  Loss pred: 0.2936; Loss self: 0.0000; time: 0.27s
Val loss: 0.3577 score: 0.9380 time: 0.17s
Test loss: 0.3443 score: 0.9375 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 0.2746;  Loss pred: 0.2746; Loss self: 0.0000; time: 0.27s
Val loss: 0.3297 score: 0.9302 time: 0.17s
Test loss: 0.3174 score: 0.9375 time: 0.17s
Epoch 36/1000, LR 0.000290
Train loss: 0.2523;  Loss pred: 0.2523; Loss self: 0.0000; time: 0.27s
Val loss: 0.3139 score: 0.9302 time: 0.18s
Test loss: 0.3032 score: 0.9375 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 0.2339;  Loss pred: 0.2339; Loss self: 0.0000; time: 0.27s
Val loss: 0.2948 score: 0.9302 time: 0.17s
Test loss: 0.2861 score: 0.9297 time: 0.17s
Epoch 38/1000, LR 0.000289
Train loss: 0.2235;  Loss pred: 0.2235; Loss self: 0.0000; time: 0.27s
Val loss: 0.2991 score: 0.9225 time: 0.17s
Test loss: 0.2900 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.2105;  Loss pred: 0.2105; Loss self: 0.0000; time: 0.27s
Val loss: 0.2605 score: 0.9225 time: 0.18s
Test loss: 0.2632 score: 0.9453 time: 0.17s
Epoch 40/1000, LR 0.000289
Train loss: 0.1902;  Loss pred: 0.1902; Loss self: 0.0000; time: 0.27s
Val loss: 0.2510 score: 0.9225 time: 0.18s
Test loss: 0.2552 score: 0.9375 time: 0.17s
Epoch 41/1000, LR 0.000289
Train loss: 0.1643;  Loss pred: 0.1643; Loss self: 0.0000; time: 0.27s
Val loss: 0.2443 score: 0.9302 time: 0.17s
Test loss: 0.2493 score: 0.9375 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.27s
Val loss: 0.2430 score: 0.9380 time: 0.17s
Test loss: 0.2473 score: 0.9297 time: 0.17s
Epoch 43/1000, LR 0.000289
Train loss: 0.1467;  Loss pred: 0.1467; Loss self: 0.0000; time: 0.27s
Val loss: 0.2260 score: 0.9380 time: 0.18s
Test loss: 0.2348 score: 0.9297 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 0.1379;  Loss pred: 0.1379; Loss self: 0.0000; time: 0.27s
Val loss: 0.2277 score: 0.9380 time: 0.17s
Test loss: 0.2359 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1433;  Loss pred: 0.1433; Loss self: 0.0000; time: 0.26s
Val loss: 0.2306 score: 0.9225 time: 0.16s
Test loss: 0.2383 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1329;  Loss pred: 0.1329; Loss self: 0.0000; time: 0.26s
Val loss: 0.2067 score: 0.9302 time: 0.17s
Test loss: 0.2213 score: 0.9375 time: 0.15s
Epoch 47/1000, LR 0.000289
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.26s
Val loss: 0.1967 score: 0.9302 time: 0.16s
Test loss: 0.2136 score: 0.9375 time: 0.15s
Epoch 48/1000, LR 0.000289
Train loss: 0.1212;  Loss pred: 0.1212; Loss self: 0.0000; time: 0.26s
Val loss: 0.1908 score: 0.9302 time: 0.16s
Test loss: 0.2066 score: 0.9297 time: 0.25s
Epoch 49/1000, LR 0.000289
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.25s
Val loss: 0.2079 score: 0.9302 time: 0.16s
Test loss: 0.2216 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.26s
Val loss: 0.2000 score: 0.9380 time: 0.16s
Test loss: 0.2184 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.28s
Val loss: 0.2207 score: 0.9225 time: 0.17s
Test loss: 0.2348 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.26s
Val loss: 0.2294 score: 0.9225 time: 0.16s
Test loss: 0.2436 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.0998;  Loss pred: 0.0998; Loss self: 0.0000; time: 0.27s
Val loss: 0.2093 score: 0.9302 time: 0.16s
Test loss: 0.2295 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0966;  Loss pred: 0.0966; Loss self: 0.0000; time: 0.30s
Val loss: 0.1747 score: 0.9302 time: 0.17s
Test loss: 0.2052 score: 0.9297 time: 0.15s
Epoch 55/1000, LR 0.000289
Train loss: 0.0856;  Loss pred: 0.0856; Loss self: 0.0000; time: 0.26s
Val loss: 0.1735 score: 0.9302 time: 0.16s
Test loss: 0.2008 score: 0.9297 time: 0.15s
Epoch 56/1000, LR 0.000289
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.26s
Val loss: 0.1658 score: 0.9302 time: 0.16s
Test loss: 0.1990 score: 0.9453 time: 0.15s
Epoch 57/1000, LR 0.000288
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.35s
Val loss: 0.1712 score: 0.9302 time: 0.16s
Test loss: 0.2003 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.26s
Val loss: 0.2035 score: 0.9302 time: 0.17s
Test loss: 0.2217 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.26s
Val loss: 0.2038 score: 0.9302 time: 0.16s
Test loss: 0.2229 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.26s
Val loss: 0.1960 score: 0.9302 time: 0.16s
Test loss: 0.2158 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.26s
Val loss: 0.1977 score: 0.9302 time: 0.16s
Test loss: 0.2152 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.26s
Val loss: 0.2270 score: 0.9225 time: 0.17s
Test loss: 0.2385 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.26s
Val loss: 0.2045 score: 0.9302 time: 0.16s
Test loss: 0.2217 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0627;  Loss pred: 0.0627; Loss self: 0.0000; time: 0.27s
Val loss: 0.2244 score: 0.9225 time: 0.16s
Test loss: 0.2383 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 0.27s
Val loss: 0.1925 score: 0.9302 time: 0.16s
Test loss: 0.2172 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.26s
Val loss: 0.2094 score: 0.9302 time: 0.16s
Test loss: 0.2293 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0557;  Loss pred: 0.0557; Loss self: 0.0000; time: 0.26s
Val loss: 0.2194 score: 0.9225 time: 0.17s
Test loss: 0.2397 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.26s
Val loss: 0.2194 score: 0.9225 time: 0.17s
Test loss: 0.2440 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.27s
Val loss: 0.1708 score: 0.9457 time: 0.16s
Test loss: 0.2185 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.27s
Val loss: 0.1865 score: 0.9380 time: 0.16s
Test loss: 0.2219 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.26s
Val loss: 0.1697 score: 0.9457 time: 0.16s
Test loss: 0.2170 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.26s
Val loss: 0.1788 score: 0.9457 time: 0.16s
Test loss: 0.2180 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.26s
Val loss: 0.1695 score: 0.9147 time: 0.17s
Test loss: 0.2216 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.26s
Val loss: 0.1783 score: 0.9457 time: 0.17s
Test loss: 0.2169 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.26s
Val loss: 0.1690 score: 0.9457 time: 0.16s
Test loss: 0.2133 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.26s
Val loss: 0.1653 score: 0.9457 time: 0.16s
Test loss: 0.2101 score: 0.9375 time: 0.16s
Epoch 77/1000, LR 0.000287
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.26s
Val loss: 0.1767 score: 0.9457 time: 0.16s
Test loss: 0.2099 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.26s
Val loss: 0.1669 score: 0.9457 time: 0.16s
Test loss: 0.2145 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.26s
Val loss: 0.1641 score: 0.9380 time: 0.16s
Test loss: 0.2135 score: 0.9375 time: 0.15s
Epoch 80/1000, LR 0.000287
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.26s
Val loss: 0.1642 score: 0.9302 time: 0.16s
Test loss: 0.2144 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.26s
Val loss: 0.1645 score: 0.9302 time: 0.17s
Test loss: 0.2149 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0393;  Loss pred: 0.0393; Loss self: 0.0000; time: 0.25s
Val loss: 0.1704 score: 0.9457 time: 0.16s
Test loss: 0.2156 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.26s
Val loss: 0.1729 score: 0.9225 time: 0.16s
Test loss: 0.2373 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.26s
Val loss: 0.1798 score: 0.9225 time: 0.16s
Test loss: 0.2326 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.26s
Val loss: 0.1881 score: 0.9147 time: 0.16s
Test loss: 0.2366 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.26s
Val loss: 0.1962 score: 0.9070 time: 0.16s
Test loss: 0.2380 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.26s
Val loss: 0.2059 score: 0.9070 time: 0.16s
Test loss: 0.2441 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.25s
Val loss: 0.2053 score: 0.9070 time: 0.17s
Test loss: 0.2449 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.26s
Val loss: 0.2153 score: 0.9380 time: 0.16s
Test loss: 0.2485 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.26s
Val loss: 0.2398 score: 0.9302 time: 0.16s
Test loss: 0.2661 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.26s
Val loss: 0.2497 score: 0.9302 time: 0.16s
Test loss: 0.2703 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.26s
Val loss: 0.2853 score: 0.9302 time: 0.16s
Test loss: 0.2978 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 93/1000, LR 0.000285
Train loss: 0.0393;  Loss pred: 0.0393; Loss self: 0.0000; time: 0.26s
Val loss: 0.3209 score: 0.9302 time: 0.16s
Test loss: 0.3333 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 94/1000, LR 0.000285
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.26s
Val loss: 0.2805 score: 0.9302 time: 0.16s
Test loss: 0.2930 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 95/1000, LR 0.000285
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.26s
Val loss: 0.2446 score: 0.9302 time: 0.17s
Test loss: 0.2628 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 96/1000, LR 0.000285
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.26s
Val loss: 0.2785 score: 0.9225 time: 0.17s
Test loss: 0.2957 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 97/1000, LR 0.000285
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.26s
Val loss: 0.2719 score: 0.9225 time: 0.16s
Test loss: 0.2888 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 98/1000, LR 0.000285
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.26s
Val loss: 0.2324 score: 0.9302 time: 0.16s
Test loss: 0.2564 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 99/1000, LR 0.000284
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.26s
Val loss: 0.2461 score: 0.9302 time: 0.16s
Test loss: 0.2686 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 078,   Train_Loss: 0.0386,   Val_Loss: 0.1641,   Val_Precision: 0.9524,   Val_Recall: 0.9231,   Val_accuracy: 0.9375,   Val_Score: 0.9380,   Val_Loss: 0.1641,   Test_Precision: 0.9375,   Test_Recall: 0.9375,   Test_accuracy: 0.9375,   Test_Score: 0.9375,   Test_loss: 0.2135


[0.16513680713251233, 0.1625908080022782, 0.16293188487179577, 0.1648759429808706, 0.1661781871225685, 0.16744940890930593, 0.25139125902205706, 0.16589524503797293, 0.17781809507869184, 0.17140709096565843, 0.17332719499245286, 0.17789448401890695, 0.17559570306912065, 0.17587436106987298, 0.17392624402418733, 0.17672125506214797, 0.1735569341108203, 0.17269497993402183, 0.1728251320309937, 0.16780537785962224, 0.17750837397761643, 0.21514083305373788, 0.18346873903647065, 0.17016960983164608, 0.17252500797621906, 0.16945874504745007, 0.17172094783745706, 0.1704994668252766, 0.1679374019149691, 0.1686024758964777, 0.16710170498117805, 0.16926173213869333, 0.17181788804009557, 0.1674021880608052, 0.16407865309156477, 0.16399602289311588, 0.16721480409614742, 0.17201846092939377, 0.18640703102573752, 0.18913598381914198, 0.1821067298296839, 0.1831182159949094, 0.18521307199262083, 0.18080891901627183, 0.18488386180251837, 0.18407586589455605, 0.18130424502305686, 0.17967591295018792, 0.18082816014066339, 0.17573780892416835, 0.175816236063838, 0.17366329999640584, 0.17187291802838445, 0.1760670889634639, 0.17705730791203678, 0.1766014718450606, 0.1740925470367074, 0.1851015631109476, 0.17807589611038566, 0.17446918017230928, 0.17339082807302475, 0.17902532685548067, 0.1849128280300647, 0.1756397148128599, 0.17614587605930865, 0.17763219913467765, 0.17890353105030954, 0.17394205392338336, 0.17328750505112112, 0.17576228315010667, 0.17744695488363504, 0.17667420813813806, 0.17370363208465278, 0.18137988215312362, 0.1794392259325832, 0.17283030203543603, 0.16992356209084392, 0.1700322930701077, 0.1819217458833009, 0.17725671408697963, 0.18003421602770686, 0.16600557509809732, 0.1666803101543337, 0.16463515697978437, 0.16807918692938983, 0.16994531592354178, 0.169624756090343, 0.1672665812075138, 0.16624478297308087, 0.18061179900541902, 0.1740938441362232, 0.17214995087124407, 0.16895056096836925, 0.1713015481363982, 0.1780432779341936, 0.1714305451605469, 0.263135754968971, 0.17389090405777097, 0.18745816289447248, 0.17463224101811647, 0.1714844130910933, 0.19336803420446813, 0.1703928359784186, 0.18368527200073004, 0.1897545619867742, 0.18996305600740016, 0.17269789101555943, 0.1689623270649463, 0.17010197113268077, 0.17498920508660376, 0.17644706298597157, 0.17314821598120034, 0.17023564712144434, 0.16987817105837166, 0.17582867899909616, 0.1747259870171547, 0.17075598798692226, 0.1695457468740642, 0.16869553504511714, 0.1689377441070974, 0.17534376098774374, 0.17332946392707527, 0.16929671703837812, 0.16969976597465575, 0.17132658301852643, 0.1689866369124502, 0.17289181286469102, 0.17330609587952495, 0.17123153083957732, 0.1696149550843984, 0.17106358101591468, 0.1724497200921178, 0.17109293793328106, 0.17010576091706753, 0.16584239294752479, 0.16950745810754597, 0.1719455469865352, 0.17095459415577352, 0.16757261101156473, 0.16905324184335768, 0.17281981697306037, 0.17128820391371846, 0.1602664680685848, 0.15589443687349558, 0.15918687777593732, 0.16148681193590164, 0.1606815739069134, 0.15970142395235598, 0.15619273204356432, 0.1579236718825996, 0.16183737688697875, 0.15814553201198578, 0.1548656690865755, 0.15248319087550044, 0.15416200901381671, 0.15505369217135012, 0.15781390387564898, 0.15906389779411256, 0.15749708912335336, 0.15816491493023932, 0.15913498192094266, 0.16392255597747862, 0.16159073589369655, 0.15924121299758554, 0.15776612283661962, 0.1603801059536636, 0.1619430030696094, 0.1582272609230131, 0.16349924099631608, 0.1599402748979628, 0.16880309605039656, 0.17499161418527365, 0.17073593800887465, 0.16762582096271217, 0.16988305584527552, 0.1685900401789695, 0.17056479514576495, 0.167686705943197, 0.16943338909186423, 0.17222037515603006, 0.17106455215252936, 0.17019225400872529, 0.16966083110310137, 0.1726777560543269, 0.1718630490358919, 0.16680098697543144, 0.1601755649317056, 0.15918977814726532, 0.1582454030867666, 0.2517831490840763, 0.15895454096607864, 0.1590645988471806, 0.15546403801999986, 0.15879162796773016, 0.1596657691989094, 0.15706795593723655, 0.15736857405863702, 0.15549827413633466, 0.1581096681766212, 0.16114126285538077, 0.15444416808895767, 0.15313837793655694, 0.15762400813400745, 0.15632727183401585, 0.1575829959474504, 0.1574002809356898, 0.1592534971423447, 0.16108151990920305, 0.158309058053419, 0.15872396202757955, 0.156012665014714, 0.15817019320093095, 0.15975025086663663, 0.16142650181427598, 0.15940046589821577, 0.15882335300557315, 0.16263759694993496, 0.1660088230855763, 0.15917494194582105, 0.1563602660316974, 0.15607094601728022, 0.16179761406965554, 0.16046017687767744, 0.15660065808333457, 0.15727265807799995, 0.1549019510857761, 0.15833492507226765, 0.16220848308876157, 0.16000169212929904, 0.1577232431154698, 0.15740723884664476, 0.15685950894840062, 0.15805267402902246, 0.16009755292907357, 0.15997126000002027, 0.15829048701561987, 0.15820951107889414, 0.15715334308333695, 0.1595520309638232, 0.162360172951594, 0.1590983539354056]
[0.0012801302878489328, 0.0012603938604827769, 0.001263037867223223, 0.001278108085123028, 0.0012882030009501433, 0.0012980574334054723, 0.001948769449783388, 0.0012860096514571546, 0.0013784348455712545, 0.00132873713926867, 0.0013436216666081616, 0.0013790270078985036, 0.001361207000535819, 0.0013633671400765347, 0.0013482654575518397, 0.0013699322097840928, 0.0013454025900063588, 0.0013387207746823398, 0.0013397297056666178, 0.0013008168826327306, 0.0013760339068032282, 0.00166775839576541, 0.0014222382871044237, 0.0013191442622608224, 0.0013374031626063492, 0.0013136336825383726, 0.001331170138274861, 0.0013217012932191985, 0.0013018403249222409, 0.0013069959371819976, 0.0012953620541176593, 0.0013121064506875452, 0.0013319216127139191, 0.0012976913803163194, 0.0012719275433454634, 0.0012712869991714408, 0.0012962387914430033, 0.00133347644131313, 0.001445015744385562, 0.001466170417202651, 0.0014116800761991, 0.001419521054224104, 0.0014357602480048126, 0.0014016195272579211, 0.0014332082310272742, 0.0014269446968570236, 0.0014054592637446268, 0.0013928365344975807, 0.0014017686832609566, 0.0013623085963113824, 0.001362916558634403, 0.001346227131755084, 0.0013323482017704221, 0.001364861154755534, 0.001372537270635944, 0.001369003657713648, 0.0013495546281915302, 0.0014348958380693614, 0.0013804333031812843, 0.0013524742649016223, 0.0013441149463025175, 0.0013877932314378346, 0.0014334327754268582, 0.0013615481768438752, 0.0013654719074365012, 0.0013769937917416872, 0.0013868490779093763, 0.0013483880149099486, 0.0013433139926443498, 0.001362498318993075, 0.0013755577897956205, 0.0013695675049468067, 0.0013465397836019595, 0.0014060455980862295, 0.0013910017514153737, 0.0013397697832204345, 0.0013172369154328986, 0.0013180797912411449, 0.0014102460921186115, 0.0013740830549378265, 0.0013956140777341617, 0.0012868649232410645, 0.0012920954275529744, 0.0012762415269750725, 0.0013029394335611615, 0.0013174055497948976, 0.0013149205898476202, 0.0012966401643993318, 0.00128871924785334, 0.001400091465158287, 0.0013495646832265365, 0.0013344957431879385, 0.0013096942710726298, 0.0013279189778015363, 0.0013801804491022761, 0.0013289189547329217, 0.002039812054023031, 0.001347991504323806, 0.001453164053445523, 0.0013537383024660192, 0.0013293365355898706, 0.0014989770093369623, 0.001320874697507121, 0.0014239168372149615, 0.0014709655967966994, 0.0014725818295147298, 0.001338743341205887, 0.0013097854811236146, 0.0013186199312610913, 0.001356505465787626, 0.001367806689813733, 0.0013422342324124058, 0.0013196561792360028, 0.0013168850469641215, 0.0013630130154968694, 0.0013544650156368581, 0.0013236898293559865, 0.0013143081153028232, 0.0013077173259311407, 0.0013095949155588944, 0.0013592539611453002, 0.0013436392552486456, 0.0013123776514602954, 0.0013155020618190367, 0.0013281130466552436, 0.0013099739295538775, 0.0013402466113541938, 0.0013434581075932167, 0.001327376208058739, 0.0013148446130573518, 0.0013260742714411992, 0.0013368195355978124, 0.0013263018444440392, 0.001318649309434632, 0.0012855999453296495, 0.001314011303159271, 0.0013329112169498851, 0.0013252294120602599, 0.0012990124884617422, 0.001310490246847734, 0.0013396885036671346, 0.0013278155342148718, 0.0012520817817858187, 0.0012179252880741842, 0.0012436474826245103, 0.0012616157182492316, 0.001255324796147761, 0.001247667374627781, 0.0012202557190903462, 0.0012337786865828093, 0.0012643545069295214, 0.001235511968843639, 0.0012098880397388712, 0.0011912749287148472, 0.001204390695420443, 0.0012113569700886728, 0.0012329211240285076, 0.0012426867015165044, 0.0012304460087761981, 0.0012356633978924947, 0.0012432420462573646, 0.0012806449685740517, 0.0012624276241695043, 0.001244071976543637, 0.0012325478346610907, 0.0012529695777629968, 0.0012651797114813235, 0.0012361504759610398, 0.0012773378202837193, 0.0012495333976403344, 0.0013187741878937231, 0.0013671219858224504, 0.0013338745156943332, 0.0013095767262711888, 0.001327211373791215, 0.0013171096888981992, 0.0013325374620762886, 0.0013100523901812267, 0.0013236983522801893, 0.0013454716809064848, 0.0013364418136916356, 0.0013296269844431663, 0.0013254752429929795, 0.0013490449691744288, 0.0013426800705929054, 0.0013031327107455581, 0.00125137160102895, 0.0012436701417755103, 0.001236292211615364, 0.001967055852219346, 0.0012418323512974894, 0.0012426921784935985, 0.001214562797031249, 0.001240559593497892, 0.0012473888218664797, 0.0012270934057596605, 0.0012294419848331017, 0.0012148302666901145, 0.0012352317826298531, 0.0012589161160576623, 0.0012065950631949818, 0.001196393577629351, 0.0012314375635469332, 0.0012213068112032488, 0.0012311171558394562, 0.0012296896948100766, 0.001244167946424568, 0.0012584493742906488, 0.0012367895160423359, 0.0012400309533404652, 0.0012188489454274531, 0.001235704634382273, 0.0012480488348955987, 0.001261144545424031, 0.0012453161398298107, 0.0012408074453560403, 0.0012706062261713669, 0.0012969439303560648, 0.001243554233951727, 0.0012215645783726359, 0.0012193042657600017, 0.0012640438599191839, 0.001253595131856855, 0.0012234426412760513, 0.0012286926412343746, 0.0012101714928576257, 0.001236991602127091, 0.0012672537741309498, 0.0012500132197601488, 0.001232212836839608, 0.0012297440534894122, 0.0012254649136593798, 0.001234786515851738, 0.0012507621322583873, 0.0012497754687501583, 0.0012366444298095303, 0.0012360118053038605, 0.00122776049283857, 0.0012465002419048687, 0.001268438851184328, 0.0012429558901203563]
[781.1704867012796, 793.4027857109393, 791.7418993925263, 782.4064424909276, 776.2751672387251, 770.3819370892439, 513.1443332668999, 777.5991407738799, 725.460476578113, 752.5943020983028, 744.2571259842808, 725.1489595725163, 734.6421224739256, 733.4781443711952, 741.6937031196994, 729.9631272686145, 743.2719450876585, 746.9817596856868, 746.4192185709755, 768.747710266563, 726.7262783685164, 599.6072348003709, 703.1170578566895, 758.0672020557818, 747.7176875005937, 761.247228426475, 751.2187745557129, 756.6006064534843, 768.1433589482106, 765.1133194461883, 771.9849418324623, 762.1332853565341, 750.7949345175076, 770.5992466068816, 786.2083066223795, 786.6044415240211, 771.4627942022755, 749.9195104003924, 692.0339822492466, 682.0489543827579, 708.37579764706, 704.4629574350273, 696.4951156640799, 713.4603796198278, 697.7353174166706, 700.7980072406391, 711.5111948073514, 717.9593406922777, 713.3844634577542, 734.0480730339825, 733.7206329065124, 742.8167033718108, 750.5545462298832, 732.6752589563691, 728.5776651709176, 730.4582382709515, 740.9851954937528, 696.9146982442227, 724.4102251774463, 739.3856030766981, 743.983989428038, 720.5684372476308, 697.6260185638721, 734.4580360850993, 732.3475456022908, 726.2196866807602, 721.05899331704, 741.6262892745932, 744.4275913715996, 733.9458596462919, 726.9778175939663, 730.1575105922504, 742.6442294374887, 711.2144878950592, 718.9063557845839, 746.3968903644608, 759.164876328537, 758.6794112504896, 709.0960971908816, 727.7580466526074, 716.5304620769821, 777.0823354804216, 773.9366448295872, 783.5507455788436, 767.4953833171086, 759.0676995065693, 760.5021989319409, 771.2239890881752, 775.9641998563544, 714.2390514372182, 740.979674726818, 749.3467139963509, 763.5369735419301, 753.0579927817363, 724.5429397659121, 752.4913362387657, 490.24124454394916, 741.8444380342224, 688.1535485473585, 738.695210276878, 752.2549581895505, 667.1216394721937, 757.0740827175312, 702.2882052268584, 679.825552805372, 679.0794100247298, 746.9691681896543, 763.4838028148994, 758.3686370064405, 737.1883307667848, 731.0974624171339, 745.0264460940576, 757.773134953179, 759.3677233296469, 733.6687094183487, 738.2988770144118, 755.4639899941884, 760.8565969857037, 764.6912525900541, 763.5949010791868, 735.6976904870708, 744.2473834354789, 761.9757917145954, 760.1660453631141, 752.9479531267518, 763.3739706106658, 746.1313399551098, 744.3477354061183, 753.3659213784463, 760.5461436806155, 754.1055742776638, 748.0441251577088, 753.9761813565004, 758.3517413198721, 777.846952804267, 761.0284611674989, 750.2375156601282, 754.5863311661308, 769.8155397906719, 763.0732105068388, 746.4421746269346, 753.1166598312868, 798.669874881272, 821.0684266037587, 804.0863781508784, 792.634385839548, 796.6065858562812, 801.4956713108987, 819.5003591095328, 810.5181349579761, 790.917416372798, 809.3810705338102, 826.5227584329447, 839.43678818021, 830.2953549893609, 825.5204904024276, 811.0818936515179, 804.7080561654492, 812.713433070176, 809.2818818665065, 804.3486005081501, 780.8565406800145, 792.1246183581068, 803.8120131748857, 811.3275378679047, 798.1039745476991, 790.4015460611201, 808.9630020346466, 782.8782520335006, 800.298737023306, 758.2799308478637, 731.4636223909511, 749.6957084298604, 763.6055069849483, 753.4594863691328, 759.2382080466883, 750.4479449619777, 763.3282512172392, 755.4591257724317, 743.2337775599036, 748.2555467474585, 752.0906327113912, 754.4463808633328, 741.2651341133329, 744.7790593617856, 767.3815504392277, 799.1231375058713, 804.0717280326136, 808.8702578603, 508.37397365801394, 805.2616755838109, 804.7045095368735, 823.3415369253003, 806.0878374898475, 801.6746522577383, 814.9338879226776, 813.3771355919257, 823.1602614944442, 809.5646615171801, 794.3340999808099, 828.7784613937239, 835.8453427855204, 812.0590353924895, 818.7950732992194, 812.2703800014342, 813.2132880518676, 803.7500104979825, 794.6287077012302, 808.545016778562, 806.431482461098, 820.4462117734349, 809.2548754580811, 801.250697921329, 792.930519842809, 803.0089452921276, 805.9268210734, 787.0258931543515, 771.0433555330789, 804.1466730584254, 818.6222961148698, 820.1398355452258, 791.111789478519, 797.7057142195313, 817.3656583989908, 813.8731904468603, 826.3291656611912, 808.4129255852927, 789.1079280357831, 799.991539442982, 811.5481109292857, 813.1773413845663, 816.0168347977295, 809.8565923439927, 799.512532566357, 800.1437258166485, 808.6398773122048, 809.0537612253312, 814.4911046029914, 802.2461339211827, 788.3706802786043, 804.533779475609]
Elapsed: 0.16958993920743373~0.013228886776389923
Time per graph: 0.0013186672488116591~0.00010036895180875602
Speed: 761.8920412383122~47.58619157479101
Total Time: 0.1599
best val loss: 0.16414265968483086 test_score: 0.9375

Testing...
Test loss: 0.2185 score: 0.9453 time: 0.15s
test Score 0.9453
Epoch Time List: [0.5611108120065182, 0.5528009713161737, 0.5511258579790592, 0.561831169994548, 0.6545314539689571, 0.5553772270213813, 0.6582361969631165, 0.5541657160501927, 0.5693506158422679, 0.6551428961101919, 0.5765119874849916, 0.585587803972885, 0.673010963248089, 0.5919618050102144, 0.5897446051239967, 0.5835394349414855, 0.5837383782491088, 0.5802018269896507, 0.5781370920594782, 0.5719184780027717, 0.7043282191734761, 0.6289079480338842, 0.5860825430136174, 0.5703710080124438, 0.5681690878700465, 0.5671553439460695, 0.5707890051417053, 0.5680720016825944, 0.5516806687228382, 0.5555320740677416, 0.557117328979075, 0.5627690460532904, 0.5629045451059937, 0.5595513661392033, 0.5553761690389365, 0.5561783029697835, 0.5582920969463885, 0.5627334201708436, 0.6212373720481992, 0.6110850430559367, 0.6161071427632123, 0.6138836720492691, 0.6152214000467211, 0.6141059324145317, 0.6149680779781193, 0.6132218688726425, 0.6129472611937672, 0.6113875657320023, 0.6134861761238426, 0.5856964141130447, 0.587895120959729, 0.5865757840219885, 0.5817540460266173, 0.5865511819720268, 0.5895946139935404, 0.5906168918590993, 0.5937461403664201, 0.6034877009224147, 0.6057819393463433, 0.6012193199712783, 0.6081467899493873, 0.6063306389842182, 0.6068216823041439, 0.6063891998492181, 0.6094295959919691, 0.6018798572476953, 0.6048362269066274, 0.5939806597307324, 0.5958243280183524, 0.5952026750892401, 0.599190074717626, 0.6025279865134507, 0.5977141666226089, 0.6159767089411616, 0.5987415842246264, 0.5751202590763569, 0.5678312827367336, 0.5698179309256375, 0.5849932131823152, 0.6051828418858349, 0.6020742917899042, 0.565770179964602, 0.5658461079001427, 0.5646517928689718, 0.565255468012765, 0.5698888972401619, 0.5685202197637409, 0.5644003709312528, 0.5651132161729038, 0.5970440679229796, 0.6017000251449645, 0.5893816237803549, 0.5757993089500815, 0.5892662359401584, 0.5934899910353124, 0.5805084831081331, 0.681008392944932, 0.6084193079732358, 0.6190838189795613, 0.7327257620636374, 0.5765840578824282, 0.6378762307576835, 0.6683798530139029, 0.6061258749105036, 0.7234077951870859, 0.6161963539198041, 0.6191683812066913, 0.5791972088627517, 0.5802371867466718, 0.588502902770415, 0.5895056212320924, 0.5840925448574126, 0.5850902209058404, 0.5841694138944149, 0.5882501909509301, 0.5860522750299424, 0.5790038746781647, 0.5823789709247649, 0.5770106832496822, 0.5802501789294183, 0.5869474988430738, 0.5779579947702587, 0.587284040171653, 0.582914050668478, 0.5834205981809646, 0.5745477681048214, 0.5767530414741486, 0.5781678839121014, 0.5755154979415238, 0.5783676290884614, 0.5799775207415223, 0.5745352411177009, 0.5737728420644999, 0.5741738858632743, 0.5741239769849926, 0.5713682130444795, 0.5728718400932848, 0.578167409170419, 0.575203875079751, 0.5774573448579758, 0.5774042967241257, 0.577305750688538, 0.5736562807578593, 0.5748728939797729, 0.5820037322118878, 0.579236556077376, 0.5814214828424156, 0.576245792908594, 0.5756957912817597, 0.5800732830539346, 0.5795874651521444, 0.5723480640444905, 0.5718657621182501, 0.5670889138709754, 0.5674692429602146, 0.5681960992515087, 0.5683264539111406, 0.5736464550718665, 0.5764049140270799, 0.5865151609759778, 0.5824969147797674, 0.5794208063744009, 0.5776136249769479, 0.5782580440863967, 0.5737143487203866, 0.5789380948990583, 0.5797939703334123, 0.5816759800072759, 0.5858476180583239, 0.5822097642812878, 0.6106464108452201, 0.6151220730971545, 0.607295153895393, 0.6051748089957982, 0.6080582439899445, 0.6039005280472338, 0.6090367620345205, 0.6082157439086586, 0.6056018131785095, 0.6107753231190145, 0.6113958330824971, 0.6123210748191923, 0.6083108009770513, 0.612562510650605, 0.6130520957522094, 0.6048408311326057, 0.5798620649147779, 0.5803349991329014, 0.5786445252597332, 0.6665819289628416, 0.5638546838890761, 0.5733116830233485, 0.6067000178154558, 0.57138876686804, 0.5818399200215936, 0.6136978538706899, 0.5725091211497784, 0.575900362804532, 0.6599221210926771, 0.5804846680257469, 0.5687485721427947, 0.5657175870146602, 0.5782546969130635, 0.5785879248287529, 0.578661572188139, 0.5778724821284413, 0.5820373899769038, 0.5804168153554201, 0.5828736270777881, 0.5825783431064337, 0.5822380629833788, 0.5794866841752082, 0.5804248482454568, 0.5757068481761962, 0.5786014839541167, 0.5777401390951127, 0.5847123810090125, 0.5821169989649206, 0.5795245179906487, 0.5740950650069863, 0.57318474794738, 0.5814815962221473, 0.5774514188524336, 0.5703731873072684, 0.573470122879371, 0.5745202249381691, 0.5740871978923678, 0.5764938720967621, 0.572966099018231, 0.5737648692447692, 0.5771342522930354, 0.5733023507054895, 0.5750646786764264, 0.5757959727197886, 0.5791088752448559, 0.5766224961262196, 0.5768575200345367, 0.5769240830559283, 0.578048505820334, 0.577729307115078, 0.5789256191346794]
Total Epoch List: [73, 69, 99]
Total Time List: [0.1741329610813409, 0.17187710897997022, 0.15991312684491277]
T-times Epoch Time: 0.5841297729331701 ~ 0.012123764770653188
T-times Total Epoch: 76.1111111111111 ~ 5.737035841522482
T-times Total Time: 0.16659740653509894 ~ 0.0017756583804766652
T-times Inference Elapsed: 0.1682329527406831 ~ 0.003764172672638741
T-times Time Per Graph: 0.0013078877165831006 ~ 2.8980567103975013e-05
T-times Speed: 769.7838665361309 ~ 18.633123453302737
T-times cross validation test micro f1 score:0.910495549469995 ~ 0.006409620383244362
T-times cross validation test precision:0.9620267284272371 ~ 0.009341304001083434
T-times cross validation test recall:0.8653311965811965 ~ 0.0040564890106747666
T-times cross validation test f1_score:0.910495549469995 ~ 0.006802400669629346
