Namespace(seed=60, model='GPSPerformer', dataset='exchange/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 104], edge_attr=[104, 2], x=[20, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da55470e620>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.4732;  Loss pred: 1.4732; Loss self: 0.0000; time: 0.96s
Val loss: 2.3707 score: 0.4419 time: 0.27s
Test loss: 2.6582 score: 0.3721 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 1.4729;  Loss pred: 1.4729; Loss self: 0.0000; time: 0.68s
Val loss: 1.6500 score: 0.3488 time: 0.18s
Test loss: 1.8814 score: 0.3411 time: 0.28s
Epoch 3/1000, LR 0.000045
Train loss: 1.3647;  Loss pred: 1.3647; Loss self: 0.0000; time: 0.66s
Val loss: 1.2590 score: 0.2636 time: 0.27s
Test loss: 1.4768 score: 0.2946 time: 0.24s
Epoch 4/1000, LR 0.000075
Train loss: 1.3003;  Loss pred: 1.3003; Loss self: 0.0000; time: 0.60s
Val loss: 1.0278 score: 0.3178 time: 0.23s
Test loss: 1.2287 score: 0.3411 time: 0.37s
Epoch 5/1000, LR 0.000105
Train loss: 1.1077;  Loss pred: 1.1077; Loss self: 0.0000; time: 0.63s
Val loss: 0.8878 score: 0.3488 time: 0.29s
Test loss: 1.0530 score: 0.3411 time: 0.39s
Epoch 6/1000, LR 0.000135
Train loss: 0.9725;  Loss pred: 0.9725; Loss self: 0.0000; time: 0.48s
Val loss: 0.8039 score: 0.3721 time: 0.18s
Test loss: 0.9273 score: 0.3643 time: 0.17s
Epoch 7/1000, LR 0.000165
Train loss: 0.9355;  Loss pred: 0.9355; Loss self: 0.0000; time: 0.50s
Val loss: 0.7486 score: 0.3876 time: 0.18s
Test loss: 0.8240 score: 0.3798 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.8614;  Loss pred: 0.8614; Loss self: 0.0000; time: 0.51s
Val loss: 0.7177 score: 0.4574 time: 0.18s
Test loss: 0.7509 score: 0.4419 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.7820;  Loss pred: 0.7820; Loss self: 0.0000; time: 0.51s
Val loss: 0.7081 score: 0.4729 time: 0.28s
Test loss: 0.7083 score: 0.4729 time: 0.17s
Epoch 10/1000, LR 0.000255
Train loss: 0.7628;  Loss pred: 0.7628; Loss self: 0.0000; time: 0.50s
Val loss: 0.7019 score: 0.4729 time: 0.19s
Test loss: 0.6937 score: 0.4729 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.7178;  Loss pred: 0.7178; Loss self: 0.0000; time: 0.56s
Val loss: 0.7004 score: 0.4651 time: 0.19s
Test loss: 0.6888 score: 0.4806 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 0.57s
Val loss: 0.7014 score: 0.4496 time: 0.41s
Test loss: 0.6866 score: 0.4806 time: 0.29s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.79s
Val loss: 0.7053 score: 0.4574 time: 0.23s
Test loss: 0.6886 score: 0.4806 time: 0.28s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.83s
Val loss: 0.7117 score: 0.4574 time: 0.38s
Test loss: 0.6949 score: 0.4729 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.64s
Val loss: 0.7197 score: 0.4651 time: 0.40s
Test loss: 0.7015 score: 0.4729 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.72s
Val loss: 0.7285 score: 0.4884 time: 0.36s
Test loss: 0.7080 score: 0.4729 time: 0.31s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.74s
Val loss: 0.7361 score: 0.4884 time: 0.24s
Test loss: 0.7140 score: 0.4806 time: 0.29s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.69s
Val loss: 0.7389 score: 0.4884 time: 0.22s
Test loss: 0.7152 score: 0.4806 time: 0.25s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5684;  Loss pred: 0.5684; Loss self: 0.0000; time: 0.71s
Val loss: 0.7441 score: 0.4961 time: 0.28s
Test loss: 0.7181 score: 0.4884 time: 0.20s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.57s
Val loss: 0.7479 score: 0.4961 time: 0.21s
Test loss: 0.7195 score: 0.4884 time: 0.28s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.56s
Val loss: 0.7483 score: 0.4961 time: 0.28s
Test loss: 0.7205 score: 0.5039 time: 0.23s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.88s
Val loss: 0.7474 score: 0.5039 time: 0.23s
Test loss: 0.7205 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.65s
Val loss: 0.7473 score: 0.5194 time: 0.21s
Test loss: 0.7227 score: 0.4961 time: 0.27s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.62s
Val loss: 0.7475 score: 0.5349 time: 0.30s
Test loss: 0.7246 score: 0.5116 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.61s
Val loss: 0.7494 score: 0.5349 time: 0.25s
Test loss: 0.7283 score: 0.5116 time: 0.29s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.59s
Val loss: 0.7554 score: 0.5271 time: 0.24s
Test loss: 0.7368 score: 0.5116 time: 0.28s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.4535;  Loss pred: 0.4535; Loss self: 0.0000; time: 0.52s
Val loss: 0.7670 score: 0.5271 time: 0.22s
Test loss: 0.7484 score: 0.5116 time: 0.26s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.4594;  Loss pred: 0.4594; Loss self: 0.0000; time: 0.73s
Val loss: 0.7788 score: 0.5271 time: 0.24s
Test loss: 0.7587 score: 0.5039 time: 0.42s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.4474;  Loss pred: 0.4474; Loss self: 0.0000; time: 0.65s
Val loss: 0.7893 score: 0.5349 time: 0.29s
Test loss: 0.7680 score: 0.5194 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.4189;  Loss pred: 0.4189; Loss self: 0.0000; time: 0.63s
Val loss: 0.7926 score: 0.5271 time: 0.30s
Test loss: 0.7736 score: 0.5116 time: 0.20s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.4048;  Loss pred: 0.4048; Loss self: 0.0000; time: 0.59s
Val loss: 0.7917 score: 0.5271 time: 0.27s
Test loss: 0.7732 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.7178,   Val_Loss: 0.7004,   Val_Precision: 0.4793,   Val_Recall: 0.9062,   Val_accuracy: 0.6270,   Val_Score: 0.4651,   Val_Loss: 0.7004,   Test_Precision: 0.4919,   Test_Recall: 0.9385,   Test_accuracy: 0.6455,   Test_Score: 0.4806,   Test_loss: 0.6888


[0.22950745804701, 0.28793658898212016, 0.24210316000971943, 0.37963169207796454, 0.39684269402641803, 0.1765562059590593, 0.18473096995148808, 0.18109950504731387, 0.17700714198872447, 0.18811651901341975, 0.18900389806367457, 0.29950091103091836, 0.2839567050104961, 0.1928282689768821, 0.18204248102847487, 0.3178597840014845, 0.29881244304124266, 0.24987268191762269, 0.20916973892599344, 0.2827006590086967, 0.23257746605668217, 0.21871888102032244, 0.27853258489631116, 0.19408550905063748, 0.2932660970836878, 0.2849343749694526, 0.2692053649807349, 0.42914080107584596, 0.1875568269751966, 0.20523077400866896, 0.20738892105873674]
[0.0017791275817597675, 0.0022320665812567454, 0.001876768682245887, 0.0029428813339377096, 0.003076299953693163, 0.0013686527593725526, 0.0014320230228797525, 0.00140387213214972, 0.001372148387509492, 0.0014582675892513158, 0.0014651464966176323, 0.00232171248861177, 0.0022012147675232255, 0.0014947927827665278, 0.0014111820234765495, 0.0024640293333448413, 0.0023163755274514934, 0.001936997534245137, 0.001621470844387546, 0.002191477976811602, 0.0018029260934626524, 0.001695495201707926, 0.002159167324777606, 0.001504538829849903, 0.0022733805975479675, 0.0022087936044143616, 0.0020868632944243013, 0.0033266728765569452, 0.0014539288912805938, 0.0015909362326253407, 0.0016076660547188896]
[562.073237609459, 448.01530939859276, 532.8307156123909, 339.8030319700164, 325.0658307228718, 730.6455148334641, 698.3127952712882, 712.3155856571645, 728.7841527220265, 685.7452002436717, 682.5256056705268, 430.71655293456837, 454.29460802917714, 668.9890475315402, 708.625806851215, 405.8393244217316, 431.70892981252183, 516.2629184191026, 616.7240092298515, 456.31305017945357, 554.6539060175375, 589.7981893388246, 463.1415029879632, 664.655494534337, 439.8735526636342, 452.73582737719835, 479.1880726791296, 300.6006412734469, 687.7915460633143, 628.5607050068965, 622.0197267117494]
Elapsed: 0.2499973260414516~0.06654531925373167
Time per graph: 0.001937963767763191~0.0005158551880134237
Speed: 548.9874319927313~127.98839349236589
Total Time: 0.2077
best val loss: 0.7004157545030579 test_score: 0.4806

Testing...
Test loss: 0.7246 score: 0.5116 time: 0.27s
test Score 0.5116
Epoch Time List: [1.4475273489952087, 1.1381155310664326, 1.1698298199335113, 1.2030231660464779, 1.3096011261222884, 0.8343513790750876, 0.8655512349214405, 0.8686504499055445, 0.9616319859633222, 0.8730655011022463, 0.9336248090257868, 1.27875324501656, 1.2956339820520952, 1.3979070070199668, 1.2241557959932834, 1.401193098980002, 1.2732918409164995, 1.158892210922204, 1.1933989989338443, 1.0622219608630985, 1.0643587009981275, 1.326676416094415, 1.136923737009056, 1.1033562859520316, 1.1448088319739327, 1.1080724641215056, 1.0039188610389829, 1.3949282609391958, 1.1249133518431336, 1.1381316980114207, 1.0531788499793038]
Total Epoch List: [31]
Total Time List: [0.20772093802224845]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da55470e500>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1477 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0295 score: 0.4961 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 0.8629;  Loss pred: 0.8629; Loss self: 0.0000; time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8048 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7450 score: 0.4961 time: 0.20s
Epoch 3/1000, LR 0.000045
Train loss: 0.7917;  Loss pred: 0.7917; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5039 time: 0.19s
Test loss: 0.6680 score: 0.4884 time: 0.32s
Epoch 4/1000, LR 0.000075
Train loss: 0.7725;  Loss pred: 0.7725; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.5039 time: 0.25s
Test loss: 0.6522 score: 0.4884 time: 0.27s
Epoch 5/1000, LR 0.000105
Train loss: 0.7418;  Loss pred: 0.7418; Loss self: 0.0000; time: 0.54s
Val loss: 0.6489 score: 0.5194 time: 0.35s
Test loss: 0.6526 score: 0.4961 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.73s
Val loss: 0.6444 score: 0.5271 time: 0.25s
Test loss: 0.6606 score: 0.4961 time: 0.28s
Epoch 7/1000, LR 0.000165
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.53s
Val loss: 0.6467 score: 0.5194 time: 0.33s
Test loss: 0.6817 score: 0.4574 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.63s
Val loss: 0.6501 score: 0.5194 time: 0.28s
Test loss: 0.6965 score: 0.4574 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.64s
Val loss: 0.6535 score: 0.5271 time: 0.37s
Test loss: 0.7092 score: 0.4496 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.93s
Val loss: 0.6561 score: 0.5504 time: 0.35s
Test loss: 0.7190 score: 0.4574 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5943;  Loss pred: 0.5943; Loss self: 0.0000; time: 0.67s
Val loss: 0.6600 score: 0.5194 time: 0.25s
Test loss: 0.7237 score: 0.4496 time: 0.26s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.54s
Val loss: 0.6643 score: 0.4806 time: 0.24s
Test loss: 0.7282 score: 0.4496 time: 0.37s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5739;  Loss pred: 0.5739; Loss self: 0.0000; time: 0.55s
Val loss: 0.6689 score: 0.4961 time: 0.30s
Test loss: 0.7303 score: 0.4419 time: 0.27s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.78s
Val loss: 0.6747 score: 0.4884 time: 0.20s
Test loss: 0.7303 score: 0.4341 time: 0.33s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.52s
Val loss: 0.6799 score: 0.4729 time: 0.23s
Test loss: 0.7304 score: 0.4341 time: 0.24s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.66s
Val loss: 0.6855 score: 0.4884 time: 0.24s
Test loss: 0.7306 score: 0.4574 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.4970;  Loss pred: 0.4970; Loss self: 0.0000; time: 0.69s
Val loss: 0.6877 score: 0.4651 time: 0.23s
Test loss: 0.7293 score: 0.4496 time: 0.21s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.4758;  Loss pred: 0.4758; Loss self: 0.0000; time: 0.60s
Val loss: 0.6914 score: 0.4806 time: 0.26s
Test loss: 0.7303 score: 0.4496 time: 0.25s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.4587;  Loss pred: 0.4587; Loss self: 0.0000; time: 0.84s
Val loss: 0.6983 score: 0.4729 time: 0.27s
Test loss: 0.7333 score: 0.4574 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.4553;  Loss pred: 0.4553; Loss self: 0.0000; time: 0.64s
Val loss: 0.7057 score: 0.4729 time: 0.42s
Test loss: 0.7359 score: 0.4574 time: 0.29s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4325;  Loss pred: 0.4325; Loss self: 0.0000; time: 0.72s
Val loss: 0.7096 score: 0.4496 time: 0.29s
Test loss: 0.7374 score: 0.4341 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.71s
Val loss: 0.7098 score: 0.4186 time: 0.29s
Test loss: 0.7378 score: 0.4264 time: 0.24s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.70s
Val loss: 0.7098 score: 0.4264 time: 0.28s
Test loss: 0.7386 score: 0.4264 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.3716;  Loss pred: 0.3716; Loss self: 0.0000; time: 0.73s
Val loss: 0.7077 score: 0.4419 time: 0.28s
Test loss: 0.7359 score: 0.4341 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 0.85s
Val loss: 0.7063 score: 0.4264 time: 0.28s
Test loss: 0.7322 score: 0.4496 time: 0.27s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.3274;  Loss pred: 0.3274; Loss self: 0.0000; time: 0.73s
Val loss: 0.7055 score: 0.4109 time: 0.72s
Test loss: 0.7297 score: 0.4341 time: 0.39s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6901,   Val_Loss: 0.6444,   Val_Precision: 0.5192,   Val_Recall: 0.8308,   Val_accuracy: 0.6391,   Val_Score: 0.5271,   Val_Loss: 0.6444,   Test_Precision: 0.4949,   Test_Recall: 0.7656,   Test_accuracy: 0.6012,   Test_Score: 0.4961,   Test_loss: 0.6606


[0.22950745804701, 0.28793658898212016, 0.24210316000971943, 0.37963169207796454, 0.39684269402641803, 0.1765562059590593, 0.18473096995148808, 0.18109950504731387, 0.17700714198872447, 0.18811651901341975, 0.18900389806367457, 0.29950091103091836, 0.2839567050104961, 0.1928282689768821, 0.18204248102847487, 0.3178597840014845, 0.29881244304124266, 0.24987268191762269, 0.20916973892599344, 0.2827006590086967, 0.23257746605668217, 0.21871888102032244, 0.27853258489631116, 0.19408550905063748, 0.2932660970836878, 0.2849343749694526, 0.2692053649807349, 0.42914080107584596, 0.1875568269751966, 0.20523077400866896, 0.20738892105873674, 0.22558462002780288, 0.20925447705667466, 0.3289380749920383, 0.2763901320286095, 0.19524105999153107, 0.2798424370121211, 0.2149586749728769, 0.25979458808433264, 0.24434297799598426, 0.23591693898197263, 0.26547721901442856, 0.3700173379620537, 0.2717514570103958, 0.33164610096719116, 0.2442720930557698, 0.20947151992004365, 0.21476719400379807, 0.2544604220893234, 0.20005463901907206, 0.29722138890065253, 0.21070174395572394, 0.2451685540145263, 0.20376670092809945, 0.1931789399823174, 0.27700797701254487, 0.3935844049556181]
[0.0017791275817597675, 0.0022320665812567454, 0.001876768682245887, 0.0029428813339377096, 0.003076299953693163, 0.0013686527593725526, 0.0014320230228797525, 0.00140387213214972, 0.001372148387509492, 0.0014582675892513158, 0.0014651464966176323, 0.00232171248861177, 0.0022012147675232255, 0.0014947927827665278, 0.0014111820234765495, 0.0024640293333448413, 0.0023163755274514934, 0.001936997534245137, 0.001621470844387546, 0.002191477976811602, 0.0018029260934626524, 0.001695495201707926, 0.002159167324777606, 0.001504538829849903, 0.0022733805975479675, 0.0022087936044143616, 0.0020868632944243013, 0.0033266728765569452, 0.0014539288912805938, 0.0015909362326253407, 0.0016076660547188896, 0.0017487179847116502, 0.001622127729121509, 0.0025499075580778163, 0.002142559163012477, 0.0015134965890816361, 0.0021693212171482253, 0.0016663463176192009, 0.002013911535537462, 0.001894131612371971, 0.001828813480480408, 0.0020579629380963454, 0.0028683514570701837, 0.002106600441941053, 0.002570900007497606, 0.0018935821167113938, 0.0016238102319383228, 0.0016648619690216905, 0.001972561411545143, 0.0015508111551866051, 0.002304041774423663, 0.001633346852369953, 0.0019005314264691961, 0.0015795868288999957, 0.0014975111626536232, 0.002147348659011976, 0.00305104189888076]
[562.073237609459, 448.01530939859276, 532.8307156123909, 339.8030319700164, 325.0658307228718, 730.6455148334641, 698.3127952712882, 712.3155856571645, 728.7841527220265, 685.7452002436717, 682.5256056705268, 430.71655293456837, 454.29460802917714, 668.9890475315402, 708.625806851215, 405.8393244217316, 431.70892981252183, 516.2629184191026, 616.7240092298515, 456.31305017945357, 554.6539060175375, 589.7981893388246, 463.1415029879632, 664.655494534337, 439.8735526636342, 452.73582737719835, 479.1880726791296, 300.6006412734469, 687.7915460633143, 628.5607050068965, 622.0197267117494, 571.8474955610936, 616.4742652796936, 392.17107962683355, 466.73156908021247, 660.7216740453857, 460.9736871124107, 600.1153478280278, 496.5461403611878, 527.9464180145995, 546.8026185684674, 485.91739991441193, 348.6323119627148, 474.6984668239152, 388.9688424612645, 528.0996219676555, 615.8355085657466, 600.6503954124329, 506.9550657065131, 644.823837290281, 434.01990844985517, 612.2398304738643, 526.1686210881544, 633.0769424662697, 667.774655000219, 465.69056021862497, 327.75688867689377]
Elapsed: 0.2526794523021141~0.06058463208650167
Time per graph: 0.0019587554442024348~0.000469648310748075
Speed: 537.1973604163402~114.77954923117011
Total Time: 0.3941
best val loss: 0.6443996605022934 test_score: 0.4961

Testing...
Test loss: 0.7190 score: 0.4574 time: 0.28s
test Score 0.4574
Epoch Time List: [1.4475273489952087, 1.1381155310664326, 1.1698298199335113, 1.2030231660464779, 1.3096011261222884, 0.8343513790750876, 0.8655512349214405, 0.8686504499055445, 0.9616319859633222, 0.8730655011022463, 0.9336248090257868, 1.27875324501656, 1.2956339820520952, 1.3979070070199668, 1.2241557959932834, 1.401193098980002, 1.2732918409164995, 1.158892210922204, 1.1933989989338443, 1.0622219608630985, 1.0643587009981275, 1.326676416094415, 1.136923737009056, 1.1033562859520316, 1.1448088319739327, 1.1080724641215056, 1.0039188610389829, 1.3949282609391958, 1.1249133518431336, 1.1381316980114207, 1.0531788499793038, 1.6332892660284415, 1.2612216469133273, 1.1490387569647282, 1.1020873319357634, 1.0853877241024747, 1.2575778150931, 1.0700349758844823, 1.1690626490162686, 1.2510901818750426, 1.5087090568849817, 1.1770902400603518, 1.1416043480858207, 1.1233571509364992, 1.3120967799331993, 0.9918323078891262, 1.111951831029728, 1.132962850970216, 1.10840572277084, 1.3034981389064342, 1.3579963488737121, 1.2116824388504028, 1.2436521728523076, 1.1797322370111942, 1.1992951788706705, 1.4025902668945491, 1.8373107690131292]
Total Epoch List: [31, 26]
Total Time List: [0.20772093802224845, 0.39406136493198574]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da5546436d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8914;  Loss pred: 0.8914; Loss self: 0.0000; time: 0.69s
Val loss: 0.9520 score: 0.4574 time: 0.28s
Test loss: 0.9467 score: 0.4844 time: 0.22s
Epoch 2/1000, LR 0.000020
Train loss: 0.8851;  Loss pred: 0.8851; Loss self: 0.0000; time: 0.92s
Val loss: 0.8469 score: 0.4574 time: 0.52s
Test loss: 0.8164 score: 0.4844 time: 0.40s
Epoch 3/1000, LR 0.000050
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.74s
Val loss: 0.8318 score: 0.4574 time: 0.20s
Test loss: 0.7795 score: 0.4844 time: 0.30s
Epoch 4/1000, LR 0.000080
Train loss: 0.8314;  Loss pred: 0.8314; Loss self: 0.0000; time: 0.73s
Val loss: 0.8215 score: 0.4574 time: 0.31s
Test loss: 0.7685 score: 0.4844 time: 0.32s
Epoch 5/1000, LR 0.000110
Train loss: 0.7666;  Loss pred: 0.7666; Loss self: 0.0000; time: 0.65s
Val loss: 0.8099 score: 0.4496 time: 0.29s
Test loss: 0.7549 score: 0.5078 time: 0.19s
Epoch 6/1000, LR 0.000140
Train loss: 0.7520;  Loss pred: 0.7520; Loss self: 0.0000; time: 0.63s
Val loss: 0.7913 score: 0.4651 time: 0.21s
Test loss: 0.7355 score: 0.5312 time: 0.48s
Epoch 7/1000, LR 0.000170
Train loss: 0.7104;  Loss pred: 0.7104; Loss self: 0.0000; time: 0.84s
Val loss: 0.7849 score: 0.4651 time: 0.33s
Test loss: 0.7265 score: 0.5234 time: 0.21s
Epoch 8/1000, LR 0.000200
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.60s
Val loss: 0.7703 score: 0.4806 time: 0.25s
Test loss: 0.7103 score: 0.5547 time: 0.20s
Epoch 9/1000, LR 0.000230
Train loss: 0.7039;  Loss pred: 0.7039; Loss self: 0.0000; time: 0.84s
Val loss: 0.7568 score: 0.4496 time: 0.26s
Test loss: 0.7029 score: 0.5547 time: 0.29s
Epoch 10/1000, LR 0.000260
Train loss: 0.6507;  Loss pred: 0.6507; Loss self: 0.0000; time: 0.57s
Val loss: 0.7585 score: 0.4109 time: 0.28s
Test loss: 0.7132 score: 0.4844 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 1.00s
Val loss: 0.7682 score: 0.3953 time: 0.32s
Test loss: 0.7259 score: 0.4609 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6260;  Loss pred: 0.6260; Loss self: 0.0000; time: 0.76s
Val loss: 0.7702 score: 0.4031 time: 0.23s
Test loss: 0.7302 score: 0.4453 time: 0.33s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.5885;  Loss pred: 0.5885; Loss self: 0.0000; time: 0.64s
Val loss: 0.7648 score: 0.4264 time: 0.21s
Test loss: 0.7359 score: 0.4766 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.80s
Val loss: 0.7521 score: 0.4264 time: 0.21s
Test loss: 0.7397 score: 0.4844 time: 0.19s
Epoch 15/1000, LR 0.000290
Train loss: 0.5538;  Loss pred: 0.5538; Loss self: 0.0000; time: 0.92s
Val loss: 0.7397 score: 0.4264 time: 0.27s
Test loss: 0.7337 score: 0.5000 time: 0.19s
Epoch 16/1000, LR 0.000290
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.68s
Val loss: 0.7246 score: 0.4496 time: 0.23s
Test loss: 0.7291 score: 0.5078 time: 0.30s
Epoch 17/1000, LR 0.000290
Train loss: 0.4957;  Loss pred: 0.4957; Loss self: 0.0000; time: 0.61s
Val loss: 0.7333 score: 0.4341 time: 0.20s
Test loss: 0.7414 score: 0.5078 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.4967;  Loss pred: 0.4967; Loss self: 0.0000; time: 0.95s
Val loss: 0.7069 score: 0.4806 time: 0.31s
Test loss: 0.6943 score: 0.5625 time: 0.19s
Epoch 19/1000, LR 0.000290
Train loss: 0.4778;  Loss pred: 0.4778; Loss self: 0.0000; time: 0.63s
Val loss: 0.6799 score: 0.5116 time: 0.49s
Test loss: 0.6560 score: 0.5703 time: 0.20s
Epoch 20/1000, LR 0.000290
Train loss: 0.4331;  Loss pred: 0.4331; Loss self: 0.0000; time: 0.60s
Val loss: 0.6701 score: 0.4961 time: 0.26s
Test loss: 0.6314 score: 0.5625 time: 0.18s
Epoch 21/1000, LR 0.000290
Train loss: 0.4276;  Loss pred: 0.4276; Loss self: 0.0000; time: 0.76s
Val loss: 0.6768 score: 0.5116 time: 0.20s
Test loss: 0.6473 score: 0.5547 time: 0.30s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000290
Train loss: 0.4053;  Loss pred: 0.4053; Loss self: 0.0000; time: 0.88s
Val loss: 0.6727 score: 0.5659 time: 0.22s
Test loss: 0.6828 score: 0.5312 time: 0.35s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000290
Train loss: 0.3757;  Loss pred: 0.3757; Loss self: 0.0000; time: 0.68s
Val loss: 0.6633 score: 0.6279 time: 0.41s
Test loss: 0.6849 score: 0.5859 time: 0.19s
Epoch 24/1000, LR 0.000290
Train loss: 0.3468;  Loss pred: 0.3468; Loss self: 0.0000; time: 0.67s
Val loss: 0.6488 score: 0.6512 time: 0.27s
Test loss: 0.6660 score: 0.6406 time: 0.25s
Epoch 25/1000, LR 0.000290
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.72s
Val loss: 0.6407 score: 0.6589 time: 0.20s
Test loss: 0.6544 score: 0.6328 time: 0.25s
Epoch 26/1000, LR 0.000290
Train loss: 0.3005;  Loss pred: 0.3005; Loss self: 0.0000; time: 0.73s
Val loss: 0.6320 score: 0.6434 time: 0.24s
Test loss: 0.6287 score: 0.6094 time: 0.20s
Epoch 27/1000, LR 0.000290
Train loss: 0.2746;  Loss pred: 0.2746; Loss self: 0.0000; time: 0.85s
Val loss: 0.6110 score: 0.6822 time: 0.26s
Test loss: 0.6296 score: 0.6172 time: 0.22s
Epoch 28/1000, LR 0.000290
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 0.79s
Val loss: 0.6039 score: 0.6899 time: 0.30s
Test loss: 0.6292 score: 0.6094 time: 0.20s
Epoch 29/1000, LR 0.000290
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 0.74s
Val loss: 0.6112 score: 0.6357 time: 0.21s
Test loss: 0.6289 score: 0.5859 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.1997;  Loss pred: 0.1997; Loss self: 0.0000; time: 0.69s
Val loss: 0.6069 score: 0.6977 time: 0.32s
Test loss: 0.6421 score: 0.6172 time: 0.28s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000290
Train loss: 0.1821;  Loss pred: 0.1821; Loss self: 0.0000; time: 0.74s
Val loss: 0.6801 score: 0.6357 time: 0.30s
Test loss: 0.6818 score: 0.6016 time: 0.29s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000290
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.88s
Val loss: 0.8247 score: 0.5736 time: 0.21s
Test loss: 0.7847 score: 0.5547 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000290
Train loss: 0.1392;  Loss pred: 0.1392; Loss self: 0.0000; time: 0.70s
Val loss: 0.8267 score: 0.5814 time: 0.37s
Test loss: 0.8098 score: 0.5547 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000290
Train loss: 0.1226;  Loss pred: 0.1226; Loss self: 0.0000; time: 0.70s
Val loss: 0.8432 score: 0.6124 time: 0.28s
Test loss: 0.8950 score: 0.5703 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000290
Train loss: 0.1098;  Loss pred: 0.1098; Loss self: 0.0000; time: 0.63s
Val loss: 0.7768 score: 0.6124 time: 0.20s
Test loss: 0.8579 score: 0.5625 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000290
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 1.05s
Val loss: 0.7961 score: 0.6202 time: 0.24s
Test loss: 0.8891 score: 0.5625 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000290
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.60s
Val loss: 0.8461 score: 0.6279 time: 0.21s
Test loss: 0.9482 score: 0.5703 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000289
Train loss: 0.0805;  Loss pred: 0.0805; Loss self: 0.0000; time: 0.48s
Val loss: 0.8291 score: 0.6357 time: 0.18s
Test loss: 0.9561 score: 0.5781 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000289
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.52s
Val loss: 0.8507 score: 0.5891 time: 0.19s
Test loss: 0.9532 score: 0.5703 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 40/1000, LR 0.000289
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.60s
Val loss: 0.9027 score: 0.5736 time: 0.19s
Test loss: 0.9825 score: 0.5469 time: 0.19s
     INFO: Early stopping counter 12 of 20
Epoch 41/1000, LR 0.000289
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.52s
Val loss: 0.9430 score: 0.5891 time: 0.19s
Test loss: 1.0164 score: 0.5781 time: 0.22s
     INFO: Early stopping counter 13 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.0528;  Loss pred: 0.0528; Loss self: 0.0000; time: 0.83s
Val loss: 0.9248 score: 0.6047 time: 0.17s
Test loss: 0.9920 score: 0.5547 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.53s
Val loss: 0.9347 score: 0.6202 time: 0.17s
Test loss: 0.9906 score: 0.5625 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.50s
Val loss: 0.9949 score: 0.6202 time: 0.17s
Test loss: 1.0395 score: 0.5703 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.50s
Val loss: 1.1168 score: 0.6124 time: 0.17s
Test loss: 1.1359 score: 0.5547 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.55s
Val loss: 1.1170 score: 0.5891 time: 0.18s
Test loss: 1.1638 score: 0.5391 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.51s
Val loss: 1.1011 score: 0.6202 time: 0.18s
Test loss: 1.1415 score: 0.5391 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.49s
Val loss: 1.0348 score: 0.6279 time: 0.17s
Test loss: 1.0626 score: 0.5781 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 027,   Train_Loss: 0.2403,   Val_Loss: 0.6039,   Val_Precision: 0.6984,   Val_Recall: 0.6769,   Val_accuracy: 0.6875,   Val_Score: 0.6899,   Val_Loss: 0.6039,   Test_Precision: 0.6061,   Test_Recall: 0.6250,   Test_accuracy: 0.6154,   Test_Score: 0.6094,   Test_loss: 0.6292


[0.22950745804701, 0.28793658898212016, 0.24210316000971943, 0.37963169207796454, 0.39684269402641803, 0.1765562059590593, 0.18473096995148808, 0.18109950504731387, 0.17700714198872447, 0.18811651901341975, 0.18900389806367457, 0.29950091103091836, 0.2839567050104961, 0.1928282689768821, 0.18204248102847487, 0.3178597840014845, 0.29881244304124266, 0.24987268191762269, 0.20916973892599344, 0.2827006590086967, 0.23257746605668217, 0.21871888102032244, 0.27853258489631116, 0.19408550905063748, 0.2932660970836878, 0.2849343749694526, 0.2692053649807349, 0.42914080107584596, 0.1875568269751966, 0.20523077400866896, 0.20738892105873674, 0.22558462002780288, 0.20925447705667466, 0.3289380749920383, 0.2763901320286095, 0.19524105999153107, 0.2798424370121211, 0.2149586749728769, 0.25979458808433264, 0.24434297799598426, 0.23591693898197263, 0.26547721901442856, 0.3700173379620537, 0.2717514570103958, 0.33164610096719116, 0.2442720930557698, 0.20947151992004365, 0.21476719400379807, 0.2544604220893234, 0.20005463901907206, 0.29722138890065253, 0.21070174395572394, 0.2451685540145263, 0.20376670092809945, 0.1931789399823174, 0.27700797701254487, 0.3935844049556181, 0.2200848188949749, 0.40829052997287363, 0.3034118890063837, 0.32842029398307204, 0.1948358779773116, 0.4889130980009213, 0.21120713604614139, 0.2018894290085882, 0.29102963802870363, 0.23931163700763136, 0.23719089198857546, 0.33174406201578677, 0.2046252399450168, 0.19279925792943686, 0.1946614410262555, 0.30545524100307375, 0.20803682296536863, 0.1932567290496081, 0.20448306400794536, 0.1893518449505791, 0.30408067209646106, 0.3538772319443524, 0.2003208319656551, 0.26264802902005613, 0.25148115609772503, 0.2080698919016868, 0.22078671806957573, 0.20303648395929486, 0.2121494150487706, 0.28728735307231545, 0.2909355170559138, 0.2045767189702019, 0.24341570702381432, 0.2207470569992438, 0.20550407806877047, 0.21428936696611345, 0.1902989250374958, 0.1728688960429281, 0.19042901799548417, 0.19195489399135113, 0.22323267406318337, 0.16931083402596414, 0.1626925179734826, 0.1780373880174011, 0.16194995492696762, 0.1790153149049729, 0.16752387292217463, 0.1640964230755344]
[0.0017791275817597675, 0.0022320665812567454, 0.001876768682245887, 0.0029428813339377096, 0.003076299953693163, 0.0013686527593725526, 0.0014320230228797525, 0.00140387213214972, 0.001372148387509492, 0.0014582675892513158, 0.0014651464966176323, 0.00232171248861177, 0.0022012147675232255, 0.0014947927827665278, 0.0014111820234765495, 0.0024640293333448413, 0.0023163755274514934, 0.001936997534245137, 0.001621470844387546, 0.002191477976811602, 0.0018029260934626524, 0.001695495201707926, 0.002159167324777606, 0.001504538829849903, 0.0022733805975479675, 0.0022087936044143616, 0.0020868632944243013, 0.0033266728765569452, 0.0014539288912805938, 0.0015909362326253407, 0.0016076660547188896, 0.0017487179847116502, 0.001622127729121509, 0.0025499075580778163, 0.002142559163012477, 0.0015134965890816361, 0.0021693212171482253, 0.0016663463176192009, 0.002013911535537462, 0.001894131612371971, 0.001828813480480408, 0.0020579629380963454, 0.0028683514570701837, 0.002106600441941053, 0.002570900007497606, 0.0018935821167113938, 0.0016238102319383228, 0.0016648619690216905, 0.001972561411545143, 0.0015508111551866051, 0.002304041774423663, 0.001633346852369953, 0.0019005314264691961, 0.0015795868288999957, 0.0014975111626536232, 0.002147348659011976, 0.00305104189888076, 0.0017194126476169913, 0.0031897697654130752, 0.002370405382862373, 0.0025657835467427503, 0.001522155296697747, 0.0038196335781321977, 0.0016500557503604796, 0.0015772611641295953, 0.002273669047099247, 0.00186962216412212, 0.0018530538436607458, 0.002591750484498334, 0.0015986346870704438, 0.0015062442025737255, 0.001520792508017621, 0.0023863690703365137, 0.0016252876794169424, 0.0015098181957000634, 0.0015975239375620731, 0.0014793112886763993, 0.002375630250753602, 0.002764665874565253, 0.0015650064997316804, 0.0020519377267191885, 0.001964696532013477, 0.001625546030481928, 0.0017248962349185604, 0.001586222530931991, 0.0016574173050685204, 0.0022444324458774645, 0.0022729337269993266, 0.0015982556169547024, 0.0019016852111235494, 0.0017245863828065922, 0.0016055006099122693, 0.0016741356794227613, 0.0014867103518554359, 0.0013505382503353758, 0.00148772670308972, 0.0014996476093074307, 0.00174400526611862, 0.0013227408908278449, 0.0012710352966678329, 0.001390917093885946, 0.0012652340228669345, 0.0013985571476951009, 0.0013087802572044893, 0.0012820033052776125]
[562.073237609459, 448.01530939859276, 532.8307156123909, 339.8030319700164, 325.0658307228718, 730.6455148334641, 698.3127952712882, 712.3155856571645, 728.7841527220265, 685.7452002436717, 682.5256056705268, 430.71655293456837, 454.29460802917714, 668.9890475315402, 708.625806851215, 405.8393244217316, 431.70892981252183, 516.2629184191026, 616.7240092298515, 456.31305017945357, 554.6539060175375, 589.7981893388246, 463.1415029879632, 664.655494534337, 439.8735526636342, 452.73582737719835, 479.1880726791296, 300.6006412734469, 687.7915460633143, 628.5607050068965, 622.0197267117494, 571.8474955610936, 616.4742652796936, 392.17107962683355, 466.73156908021247, 660.7216740453857, 460.9736871124107, 600.1153478280278, 496.5461403611878, 527.9464180145995, 546.8026185684674, 485.91739991441193, 348.6323119627148, 474.6984668239152, 388.9688424612645, 528.0996219676555, 615.8355085657466, 600.6503954124329, 506.9550657065131, 644.823837290281, 434.01990844985517, 612.2398304738643, 526.1686210881544, 633.0769424662697, 667.774655000219, 465.69056021862497, 327.75688867689377, 581.5939538341441, 313.50225048938603, 421.8687686206881, 389.7444900484669, 656.9631903981536, 261.80521758032097, 606.0401291177798, 634.0104116821044, 439.81774800330004, 534.8674289329199, 539.649726542473, 385.8396114831103, 625.5337808492924, 663.9029702430031, 657.551897926902, 419.04666483922585, 615.2756909833595, 662.3314004613159, 625.9687110079026, 675.9902446865941, 420.9409270162215, 361.70736189133567, 638.9749820025984, 487.3442244267737, 508.9844582639801, 615.1779040692736, 579.7450187183082, 630.4285688165369, 603.3483522477511, 445.54693630311, 439.96003408342943, 625.6821433266027, 525.8493856662966, 579.8491800524366, 622.8586858367147, 597.3231514573527, 672.6259750273384, 740.4455221847086, 672.1664657380913, 666.823321554736, 573.3927640170234, 756.0059622668385, 786.7602124202346, 718.9501116894024, 790.3676173156234, 715.0226228853465, 764.0702054414897, 780.0291901614513]
Elapsed: 0.24367947300252993~0.06402999817414472
Time per graph: 0.001895438661082299~0.0004972384924682545
Speed: 558.5898582508843~123.08209226467164
Total Time: 0.1647
best val loss: 0.6039457570674808 test_score: 0.6094

Testing...
Test loss: 0.6421 score: 0.6172 time: 0.16s
test Score 0.6172
Epoch Time List: [1.4475273489952087, 1.1381155310664326, 1.1698298199335113, 1.2030231660464779, 1.3096011261222884, 0.8343513790750876, 0.8655512349214405, 0.8686504499055445, 0.9616319859633222, 0.8730655011022463, 0.9336248090257868, 1.27875324501656, 1.2956339820520952, 1.3979070070199668, 1.2241557959932834, 1.401193098980002, 1.2732918409164995, 1.158892210922204, 1.1933989989338443, 1.0622219608630985, 1.0643587009981275, 1.326676416094415, 1.136923737009056, 1.1033562859520316, 1.1448088319739327, 1.1080724641215056, 1.0039188610389829, 1.3949282609391958, 1.1249133518431336, 1.1381316980114207, 1.0531788499793038, 1.6332892660284415, 1.2612216469133273, 1.1490387569647282, 1.1020873319357634, 1.0853877241024747, 1.2575778150931, 1.0700349758844823, 1.1690626490162686, 1.2510901818750426, 1.5087090568849817, 1.1770902400603518, 1.1416043480858207, 1.1233571509364992, 1.3120967799331993, 0.9918323078891262, 1.111951831029728, 1.132962850970216, 1.10840572277084, 1.3034981389064342, 1.3579963488737121, 1.2116824388504028, 1.2436521728523076, 1.1797322370111942, 1.1992951788706705, 1.4025902668945491, 1.8373107690131292, 1.1878605200909078, 1.842625577119179, 1.23523323610425, 1.3583078210940585, 1.132299565942958, 1.3306998080806807, 1.3734482710715383, 1.0431864680722356, 1.392397299874574, 1.088680699118413, 1.5521155808819458, 1.312736334046349, 1.0454869720852003, 1.2009116979315877, 1.3877162811113521, 1.212530139950104, 1.0133268419886008, 1.4503503689775243, 1.32028205296956, 1.0412661479786038, 1.2604351930785924, 1.4489393399562687, 1.273955367039889, 1.1869827250484377, 1.163420378929004, 1.1648806519806385, 1.3242820990271866, 1.28721573704388, 1.1600700361886993, 1.28471751709003, 1.3239898598985747, 1.288458158960566, 1.3106000401312485, 1.1925973510369658, 1.0268208049237728, 1.4982989608542994, 0.9865934510016814, 0.832645726040937, 0.8925154758617282, 0.9803529978962615, 0.9242231008829549, 1.1691210108110681, 0.8570221749832854, 0.8465024559991434, 0.8252446580445394, 0.9072931060800329, 0.8566857519326732, 0.8223929660161957]
Total Epoch List: [31, 26, 48]
Total Time List: [0.20772093802224845, 0.39406136493198574, 0.1647031910251826]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da554643640>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7711;  Loss pred: 0.7711; Loss self: 0.0000; time: 0.62s
Val loss: 0.7845 score: 0.5116 time: 0.23s
Test loss: 0.7278 score: 0.4961 time: 0.23s
Epoch 2/1000, LR 0.000015
Train loss: 0.7429;  Loss pred: 0.7429; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.25s
Test loss: 0.6554 score: 0.5116 time: 0.35s
Epoch 3/1000, LR 0.000045
Train loss: 0.7215;  Loss pred: 0.7215; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6600 score: 0.4961 time: 0.26s
Test loss: 0.6346 score: 0.5116 time: 0.31s
Epoch 4/1000, LR 0.000075
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.69s
Val loss: 0.6454 score: 0.5039 time: 0.20s
Test loss: 0.6277 score: 0.5426 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.79s
Val loss: 0.6437 score: 0.5194 time: 0.22s
Test loss: 0.6380 score: 0.5659 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.69s
Val loss: 0.6498 score: 0.5271 time: 0.31s
Test loss: 0.6544 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.69s
Val loss: 0.6676 score: 0.5426 time: 0.25s
Test loss: 0.6701 score: 0.5194 time: 0.30s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.64s
Val loss: 0.6921 score: 0.5194 time: 0.19s
Test loss: 0.6931 score: 0.5039 time: 0.33s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.63s
Val loss: 0.7078 score: 0.5039 time: 0.20s
Test loss: 0.7075 score: 0.4574 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.91s
Val loss: 0.7181 score: 0.4651 time: 0.29s
Test loss: 0.7145 score: 0.4341 time: 0.25s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.60s
Val loss: 0.7156 score: 0.4806 time: 0.26s
Test loss: 0.7035 score: 0.4961 time: 0.24s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.82s
Val loss: 0.7157 score: 0.4806 time: 0.31s
Test loss: 0.6923 score: 0.5194 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.71s
Val loss: 0.7049 score: 0.4884 time: 0.19s
Test loss: 0.6765 score: 0.5116 time: 0.28s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5303;  Loss pred: 0.5303; Loss self: 0.0000; time: 0.81s
Val loss: 0.6986 score: 0.4961 time: 0.33s
Test loss: 0.6689 score: 0.5194 time: 0.28s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.92s
Val loss: 0.6933 score: 0.5116 time: 0.35s
Test loss: 0.6639 score: 0.5194 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.84s
Val loss: 0.6956 score: 0.5271 time: 0.19s
Test loss: 0.6688 score: 0.5271 time: 0.25s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 0.66s
Val loss: 0.6998 score: 0.5349 time: 0.21s
Test loss: 0.6776 score: 0.5116 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.58s
Val loss: 0.7050 score: 0.5349 time: 0.32s
Test loss: 0.6861 score: 0.4961 time: 0.23s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.74s
Val loss: 0.7177 score: 0.5039 time: 0.22s
Test loss: 0.6982 score: 0.4961 time: 0.25s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.4973;  Loss pred: 0.4973; Loss self: 0.0000; time: 0.93s
Val loss: 0.7318 score: 0.5349 time: 0.27s
Test loss: 0.7114 score: 0.4961 time: 0.29s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4724;  Loss pred: 0.4724; Loss self: 0.0000; time: 0.75s
Val loss: 0.7311 score: 0.5349 time: 0.19s
Test loss: 0.7134 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.4732;  Loss pred: 0.4732; Loss self: 0.0000; time: 0.58s
Val loss: 0.7372 score: 0.5581 time: 0.21s
Test loss: 0.7203 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 17 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.4287;  Loss pred: 0.4287; Loss self: 0.0000; time: 0.53s
Val loss: 0.7454 score: 0.5349 time: 0.18s
Test loss: 0.7287 score: 0.4651 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4238;  Loss pred: 0.4238; Loss self: 0.0000; time: 0.54s
Val loss: 0.7600 score: 0.5039 time: 0.18s
Test loss: 0.7466 score: 0.4574 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4047;  Loss pred: 0.4047; Loss self: 0.0000; time: 0.56s
Val loss: 0.7949 score: 0.4341 time: 0.19s
Test loss: 0.7779 score: 0.3953 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 004,   Train_Loss: 0.6877,   Val_Loss: 0.6437,   Val_Precision: 0.5088,   Val_Recall: 0.9062,   Val_accuracy: 0.6517,   Val_Score: 0.5194,   Val_Loss: 0.6437,   Test_Precision: 0.5391,   Test_Recall: 0.9538,   Test_accuracy: 0.6889,   Test_Score: 0.5659,   Test_loss: 0.6380


[0.2371287209680304, 0.3580168620683253, 0.31516918702982366, 0.16618933295831084, 0.19784188200719655, 0.17650196002796292, 0.31106790399644524, 0.33200749300885946, 0.2153564359759912, 0.25610955199226737, 0.24630050791893154, 0.18140574893914163, 0.28153304394800216, 0.2828591400757432, 0.19753019092604518, 0.25215676496736705, 0.24425148498266935, 0.23375860694795847, 0.25381466397084296, 0.2909760610200465, 0.20198977598920465, 0.21438026195392013, 0.19143254100345075, 0.17488325398880988, 0.2062225400004536]
[0.0018382071392870573, 0.0027753245121575603, 0.0024431719924792534, 0.0012882894027776033, 0.0015336580000557872, 0.0013682322482787823, 0.0024113791007476376, 0.002573701496192709, 0.0016694297362479936, 0.0019853453642811425, 0.0019093062629374539, 0.0014062461158072994, 0.0021824266972713348, 0.0021927065122150638, 0.0015312417901243813, 0.001954703604398194, 0.0018934223642067393, 0.001812082224402779, 0.001967555534657697, 0.00225562838000036, 0.0015658122169705786, 0.0016618624957668226, 0.0014839731860732616, 0.0013556841394481386, 0.0015986243410887875]
[544.0083321555626, 360.3182242723002, 409.30397167218337, 776.2231047185206, 652.0358515155431, 730.8700706754913, 414.70045074619514, 388.54544766722387, 599.0069412849191, 503.69070187548044, 523.7504424573076, 711.1130752712648, 458.205538472513, 456.0573858969406, 653.0647259299075, 511.5865125280084, 528.1441789766523, 551.8513379433306, 508.2448664779231, 443.33543985638283, 638.6461857697907, 601.7345012281394, 673.8666233222838, 737.6349482166785, 625.537829180633]
Elapsed: 0.240755356666632~0.05172480222596094
Time per graph: 0.0018663205943149769~0.00040096745911597626
Speed: 560.0590675244471~115.39340198473957
Total Time: 0.2066
best val loss: 0.6437345997307652 test_score: 0.5659

Testing...
Test loss: 0.7203 score: 0.5116 time: 0.22s
test Score 0.5116
Epoch Time List: [1.08047727891244, 1.1801203248323873, 1.5338379230815917, 1.0483271301491186, 1.200642542913556, 1.1672637881711125, 1.2465601459844038, 1.158458936959505, 1.037696143030189, 1.4553013039985672, 1.1019570929929614, 1.3070393230300397, 1.174200903158635, 1.4147624570177868, 1.4551294369157404, 1.2850432831328362, 1.1168986287666485, 1.131648898939602, 1.2069831078406423, 1.482171930023469, 1.13310666102916, 0.9956009169109166, 0.9037431471515447, 0.8897450850345194, 0.9540024909656495]
Total Epoch List: [25]
Total Time List: [0.20660602499265224]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da55470e0e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7116;  Loss pred: 0.7116; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2345 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0769 score: 0.4961 time: 0.33s
Epoch 2/1000, LR 0.000015
Train loss: 0.7413;  Loss pred: 0.7413; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0270 score: 0.5039 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8899 score: 0.4961 time: 0.19s
Epoch 3/1000, LR 0.000045
Train loss: 0.7090;  Loss pred: 0.7090; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9164 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8005 score: 0.4961 time: 0.30s
Epoch 4/1000, LR 0.000075
Train loss: 0.7156;  Loss pred: 0.7156; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8647 score: 0.5039 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7649 score: 0.4961 time: 0.36s
Epoch 5/1000, LR 0.000105
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8488 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7487 score: 0.4961 time: 0.28s
Epoch 6/1000, LR 0.000135
Train loss: 0.7035;  Loss pred: 0.7035; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8435 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7407 score: 0.4961 time: 0.19s
Epoch 7/1000, LR 0.000165
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8397 score: 0.5039 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7328 score: 0.4961 time: 0.20s
Epoch 8/1000, LR 0.000195
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8240 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7192 score: 0.4961 time: 0.30s
Epoch 9/1000, LR 0.000225
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8023 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7038 score: 0.4961 time: 0.35s
Epoch 10/1000, LR 0.000255
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7849 score: 0.5039 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4961 time: 0.40s
Epoch 11/1000, LR 0.000285
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7583 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4961 time: 0.30s
Epoch 12/1000, LR 0.000285
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7327 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6800 score: 0.4961 time: 0.33s
Epoch 13/1000, LR 0.000285
Train loss: 0.6023;  Loss pred: 0.6023; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7253 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.4961 time: 0.20s
Epoch 14/1000, LR 0.000285
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7266 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4961 time: 0.33s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7300 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7051 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7336 score: 0.5039 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7124 score: 0.4961 time: 0.32s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7444 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7269 score: 0.4961 time: 0.27s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7398 score: 0.5039 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7264 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 1.08s
Val loss: 0.7175 score: 0.5116 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7152 score: 0.4961 time: 0.52s
Epoch 20/1000, LR 0.000285
Train loss: 0.4914;  Loss pred: 0.4914; Loss self: 0.0000; time: 0.95s
Val loss: 0.6969 score: 0.5194 time: 0.30s
Test loss: 0.7103 score: 0.4884 time: 0.21s
Epoch 21/1000, LR 0.000285
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.62s
Val loss: 0.6860 score: 0.5194 time: 0.25s
Test loss: 0.7083 score: 0.4884 time: 0.41s
Epoch 22/1000, LR 0.000285
Train loss: 0.4598;  Loss pred: 0.4598; Loss self: 0.0000; time: 5.26s
Val loss: 0.6728 score: 0.5116 time: 1.56s
Test loss: 0.6987 score: 0.4884 time: 5.94s
Epoch 23/1000, LR 0.000285
Train loss: 0.4559;  Loss pred: 0.4559; Loss self: 0.0000; time: 6.22s
Val loss: 0.6631 score: 0.5116 time: 2.38s
Test loss: 0.6879 score: 0.4961 time: 2.92s
Epoch 24/1000, LR 0.000285
Train loss: 0.4398;  Loss pred: 0.4398; Loss self: 0.0000; time: 7.18s
Val loss: 0.6471 score: 0.5039 time: 5.52s
Test loss: 0.6625 score: 0.4961 time: 1.60s
Epoch 25/1000, LR 0.000285
Train loss: 0.4260;  Loss pred: 0.4260; Loss self: 0.0000; time: 9.96s
Val loss: 0.6387 score: 0.4961 time: 2.47s
Test loss: 0.6459 score: 0.4961 time: 1.33s
Epoch 26/1000, LR 0.000285
Train loss: 0.4199;  Loss pred: 0.4199; Loss self: 0.0000; time: 0.85s
Val loss: 0.6166 score: 0.5039 time: 0.28s
Test loss: 0.6306 score: 0.4961 time: 0.27s
Epoch 27/1000, LR 0.000285
Train loss: 0.3996;  Loss pred: 0.3996; Loss self: 0.0000; time: 0.73s
Val loss: 0.6085 score: 0.5349 time: 0.38s
Test loss: 0.6428 score: 0.5116 time: 0.39s
Epoch 28/1000, LR 0.000285
Train loss: 0.3858;  Loss pred: 0.3858; Loss self: 0.0000; time: 0.82s
Val loss: 0.6203 score: 0.4961 time: 0.26s
Test loss: 0.6747 score: 0.4961 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.83s
Val loss: 0.6487 score: 0.5116 time: 0.44s
Test loss: 0.7237 score: 0.4651 time: 3.07s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.3670;  Loss pred: 0.3670; Loss self: 0.0000; time: 9.68s
Val loss: 0.7537 score: 0.4419 time: 7.58s
Test loss: 0.8188 score: 0.4109 time: 4.80s
     INFO: Early stopping counter 3 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.3642;  Loss pred: 0.3642; Loss self: 0.0000; time: 1.59s
Val loss: 0.8331 score: 0.4264 time: 0.29s
Test loss: 0.8844 score: 0.4109 time: 0.27s
     INFO: Early stopping counter 4 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.3487;  Loss pred: 0.3487; Loss self: 0.0000; time: 0.59s
Val loss: 0.8780 score: 0.4496 time: 0.28s
Test loss: 0.9170 score: 0.4109 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.79s
Val loss: 0.8716 score: 0.4419 time: 0.37s
Test loss: 0.9145 score: 0.4419 time: 0.37s
     INFO: Early stopping counter 6 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.3368;  Loss pred: 0.3368; Loss self: 0.0000; time: 0.71s
Val loss: 0.7785 score: 0.4496 time: 0.36s
Test loss: 0.8552 score: 0.4496 time: 0.28s
     INFO: Early stopping counter 7 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.3154;  Loss pred: 0.3154; Loss self: 0.0000; time: 0.81s
Val loss: 0.6824 score: 0.4884 time: 0.19s
Test loss: 0.7686 score: 0.4341 time: 0.43s
     INFO: Early stopping counter 8 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.3143;  Loss pred: 0.3143; Loss self: 0.0000; time: 0.81s
Val loss: 0.6627 score: 0.4961 time: 0.29s
Test loss: 0.7445 score: 0.4574 time: 0.25s
     INFO: Early stopping counter 9 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.2954;  Loss pred: 0.2954; Loss self: 0.0000; time: 0.71s
Val loss: 0.7031 score: 0.4884 time: 0.26s
Test loss: 0.8060 score: 0.4651 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.2846;  Loss pred: 0.2846; Loss self: 0.0000; time: 0.62s
Val loss: 0.7425 score: 0.5039 time: 0.31s
Test loss: 0.8668 score: 0.4419 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.2827;  Loss pred: 0.2827; Loss self: 0.0000; time: 0.66s
Val loss: 0.7336 score: 0.5194 time: 0.28s
Test loss: 0.8570 score: 0.4419 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.85s
Val loss: 0.6902 score: 0.5271 time: 0.35s
Test loss: 0.7798 score: 0.5271 time: 0.28s
     INFO: Early stopping counter 13 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.81s
Val loss: 0.6569 score: 0.5194 time: 0.27s
Test loss: 0.7153 score: 0.5581 time: 0.23s
     INFO: Early stopping counter 14 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.2464;  Loss pred: 0.2464; Loss self: 0.0000; time: 0.65s
Val loss: 0.6844 score: 0.5426 time: 0.37s
Test loss: 0.7511 score: 0.5581 time: 0.25s
     INFO: Early stopping counter 15 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.2395;  Loss pred: 0.2395; Loss self: 0.0000; time: 0.86s
Val loss: 0.7611 score: 0.5116 time: 0.39s
Test loss: 0.8764 score: 0.4961 time: 0.23s
     INFO: Early stopping counter 16 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.61s
Val loss: 0.8200 score: 0.5116 time: 0.28s
Test loss: 0.9786 score: 0.4884 time: 0.24s
     INFO: Early stopping counter 17 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.2129;  Loss pred: 0.2129; Loss self: 0.0000; time: 0.92s
Val loss: 0.8067 score: 0.5426 time: 0.26s
Test loss: 0.9509 score: 0.4961 time: 0.38s
     INFO: Early stopping counter 18 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.2033;  Loss pred: 0.2033; Loss self: 0.0000; time: 0.91s
Val loss: 0.7916 score: 0.5891 time: 0.74s
Test loss: 0.8933 score: 0.5814 time: 0.23s
     INFO: Early stopping counter 19 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.1865;  Loss pred: 0.1865; Loss self: 0.0000; time: 0.69s
Val loss: 0.7793 score: 0.5969 time: 0.42s
Test loss: 0.8529 score: 0.6124 time: 0.31s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 026,   Train_Loss: 0.3996,   Val_Loss: 0.6085,   Val_Precision: 0.5234,   Val_Recall: 0.8615,   Val_accuracy: 0.6512,   Val_Score: 0.5349,   Val_Loss: 0.6085,   Test_Precision: 0.5048,   Test_Recall: 0.8281,   Test_accuracy: 0.6272,   Test_Score: 0.5116,   Test_loss: 0.6428


[0.2371287209680304, 0.3580168620683253, 0.31516918702982366, 0.16618933295831084, 0.19784188200719655, 0.17650196002796292, 0.31106790399644524, 0.33200749300885946, 0.2153564359759912, 0.25610955199226737, 0.24630050791893154, 0.18140574893914163, 0.28153304394800216, 0.2828591400757432, 0.19753019092604518, 0.25215676496736705, 0.24425148498266935, 0.23375860694795847, 0.25381466397084296, 0.2909760610200465, 0.20198977598920465, 0.21438026195392013, 0.19143254100345075, 0.17488325398880988, 0.2062225400004536, 0.3378107550088316, 0.19758981000632048, 0.3060342730022967, 0.3674273450160399, 0.2882050590123981, 0.19275179994292557, 0.2060297499410808, 0.30268066201824695, 0.3584521219599992, 0.40608187892939895, 0.31111236999277025, 0.33155253098811954, 0.20121390698477626, 0.3396684570470825, 0.2020997789222747, 0.32628113601822406, 0.2778687849640846, 0.20147179800551385, 0.5206900680204853, 0.21938593708910048, 0.4180815489962697, 5.947200809954666, 2.9253667849116027, 1.600497044972144, 1.3352999880444258, 0.26992927701212466, 0.3929311520187184, 0.2181591490516439, 3.0740863559767604, 4.808269142988138, 0.27611631399486214, 0.23990192194469273, 0.3714196350192651, 0.2865522470092401, 0.43139739299658686, 0.25266338500659913, 0.19569578696973622, 0.19832855300046504, 0.2417719910154119, 0.2830352400196716, 0.23583085904829204, 0.2525313700316474, 0.2364227930083871, 0.2461281669093296, 0.38384892197791487, 0.2329325849423185, 0.31541534792631865]
[0.0018382071392870573, 0.0027753245121575603, 0.0024431719924792534, 0.0012882894027776033, 0.0015336580000557872, 0.0013682322482787823, 0.0024113791007476376, 0.002573701496192709, 0.0016694297362479936, 0.0019853453642811425, 0.0019093062629374539, 0.0014062461158072994, 0.0021824266972713348, 0.0021927065122150638, 0.0015312417901243813, 0.001954703604398194, 0.0018934223642067393, 0.001812082224402779, 0.001967555534657697, 0.00225562838000036, 0.0015658122169705786, 0.0016618624957668226, 0.0014839731860732616, 0.0013556841394481386, 0.0015986243410887875, 0.002618688023324276, 0.0015317039535373681, 0.0023723587054441603, 0.0028482739923724023, 0.002234147744282156, 0.0014941999995575627, 0.0015971298445045024, 0.002346361721071682, 0.002778698619844955, 0.003147921542088364, 0.002411723798393568, 0.002570174658822632, 0.001559797728564157, 0.002633088814318469, 0.0015666649528858505, 0.0025293111319242174, 0.0021540215888688733, 0.0015617968837636732, 0.004036357116437871, 0.001700666178985275, 0.0032409422402811605, 0.046102331860113685, 0.022677261898539556, 0.012406953836993365, 0.010351162698018805, 0.0020924750155978658, 0.003045977922625724, 0.00169115619419879, 0.023830126790517523, 0.037273404209210374, 0.0021404365425958306, 0.001859704821276688, 0.002879221976893528, 0.002221335248133644, 0.003344165837182844, 0.001958630891524024, 0.00151702160441656, 0.0015374306434144576, 0.0018742014807396272, 0.00219407162805947, 0.0018281461941728064, 0.0019576075196251737, 0.0018327348295223806, 0.001907970286118834, 0.0029755730385884875, 0.0018056789530412286, 0.0024450802164830902]
[544.0083321555626, 360.3182242723002, 409.30397167218337, 776.2231047185206, 652.0358515155431, 730.8700706754913, 414.70045074619514, 388.54544766722387, 599.0069412849191, 503.69070187548044, 523.7504424573076, 711.1130752712648, 458.205538472513, 456.0573858969406, 653.0647259299075, 511.5865125280084, 528.1441789766523, 551.8513379433306, 508.2448664779231, 443.33543985638283, 638.6461857697907, 601.7345012281394, 673.8666233222838, 737.6349482166785, 625.537829180633, 381.87061272406044, 652.8676756957941, 421.52141567174044, 351.0898188439637, 447.5979722286923, 669.2544507402645, 626.1231692844876, 426.1917465748879, 359.8806984169437, 317.6699249424716, 414.6411793365778, 389.0786163373382, 641.1087679429637, 379.782100232283, 638.2985705769225, 395.3645668096327, 464.24790037741604, 640.2881260655129, 247.748147934569, 588.0048726532935, 308.55224371824886, 21.69087678762664, 44.09703448653125, 80.5999613715283, 96.60750479667355, 477.90295824118994, 328.30178858879253, 591.3114373647578, 41.96368776342054, 26.828781036128085, 467.194415764946, 537.7197437782087, 347.3160485802236, 450.1796839716992, 299.0282326555938, 510.56072092373313, 659.1863933174477, 650.4358452093256, 533.5605644732306, 455.7736343751195, 547.0022053966404, 510.82762503459924, 545.6326708543019, 524.1171769158867, 336.06972069970317, 553.8083048017712, 408.9845368911298]
Elapsed: 0.5219875820039306~0.9604838153464351
Time per graph: 0.004046415364371555~0.007445610971677791
Speed: 469.5744558236035~172.83778540391498
Total Time: 0.3196
best val loss: 0.608495758023373 test_score: 0.5116

Testing...
Test loss: 0.8529 score: 0.6124 time: 0.25s
test Score 0.6124
Epoch Time List: [1.08047727891244, 1.1801203248323873, 1.5338379230815917, 1.0483271301491186, 1.200642542913556, 1.1672637881711125, 1.2465601459844038, 1.158458936959505, 1.037696143030189, 1.4553013039985672, 1.1019570929929614, 1.3070393230300397, 1.174200903158635, 1.4147624570177868, 1.4551294369157404, 1.2850432831328362, 1.1168986287666485, 1.131648898939602, 1.2069831078406423, 1.482171930023469, 1.13310666102916, 0.9956009169109166, 0.9037431471515447, 0.8897450850345194, 0.9540024909656495, 1.1531611001119018, 1.2719979940447956, 1.1647740679327399, 1.4209302071249112, 1.4600556050427258, 1.1567120578838512, 1.3285422699991614, 1.069090063101612, 1.2504503400996327, 1.5536258540814742, 1.3749740719795227, 1.2455692199291661, 1.3677147299749777, 1.566301807994023, 1.4789679300738499, 1.2891839080257341, 1.2581599310506135, 1.4626701849047095, 1.805594091070816, 1.4560371841071174, 1.2829375370638445, 12.757952664978802, 11.520802109851502, 14.298378262086771, 13.75709409697447, 1.396039982093498, 1.4992779330350459, 1.2977984060999006, 4.34884141094517, 22.060449796030298, 2.153178368927911, 1.1001347100827843, 1.525635416037403, 1.3536542870569974, 1.4255630220286548, 1.3515462139621377, 1.1618398178834468, 1.119974555913359, 1.1749498939607292, 1.4794941729633138, 1.31480993994046, 1.269807622069493, 1.4827658559661359, 1.133173708920367, 1.5608236950356513, 1.8802751230541617, 1.4227634280687198]
Total Epoch List: [25, 47]
Total Time List: [0.20660602499265224, 0.3195717759663239]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da5569fdcc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.0061;  Loss pred: 1.0061; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.9346 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.2095 score: 0.5000 time: 0.17s
Epoch 2/1000, LR 0.000020
Train loss: 0.9556;  Loss pred: 0.9556; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3564 score: 0.4961 time: 0.18s
Test loss: 1.6031 score: 0.5078 time: 0.33s
Epoch 3/1000, LR 0.000050
Train loss: 0.9442;  Loss pred: 0.9442; Loss self: 0.0000; time: 0.55s
Val loss: 1.0000 score: 0.5116 time: 0.17s
Test loss: 1.2110 score: 0.4922 time: 0.17s
Epoch 4/1000, LR 0.000080
Train loss: 0.8829;  Loss pred: 0.8829; Loss self: 0.0000; time: 0.56s
Val loss: 0.8102 score: 0.5659 time: 0.18s
Test loss: 0.9581 score: 0.4922 time: 0.17s
Epoch 5/1000, LR 0.000110
Train loss: 0.8287;  Loss pred: 0.8287; Loss self: 0.0000; time: 0.51s
Val loss: 0.6929 score: 0.6899 time: 0.18s
Test loss: 0.7739 score: 0.5938 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 0.8039;  Loss pred: 0.8039; Loss self: 0.0000; time: 0.51s
Val loss: 0.6432 score: 0.7209 time: 0.18s
Test loss: 0.6834 score: 0.6641 time: 0.19s
Epoch 7/1000, LR 0.000170
Train loss: 0.7533;  Loss pred: 0.7533; Loss self: 0.0000; time: 0.84s
Val loss: 0.6210 score: 0.7597 time: 0.19s
Test loss: 0.6386 score: 0.7109 time: 0.29s
Epoch 8/1000, LR 0.000200
Train loss: 0.7206;  Loss pred: 0.7206; Loss self: 0.0000; time: 0.96s
Val loss: 0.6188 score: 0.7209 time: 0.24s
Test loss: 0.6248 score: 0.6953 time: 0.24s
Epoch 9/1000, LR 0.000230
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 1.06s
Val loss: 0.6530 score: 0.6977 time: 0.18s
Test loss: 0.6551 score: 0.6484 time: 0.27s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.93s
Val loss: 0.6981 score: 0.6589 time: 0.40s
Test loss: 0.7047 score: 0.5703 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.59s
Val loss: 0.7863 score: 0.5426 time: 0.25s
Test loss: 0.8228 score: 0.5234 time: 0.38s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 0.76s
Val loss: 0.9231 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0021 score: 0.5000 time: 0.27s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 1.10s
Val loss: 1.0726 score: 0.4651 time: 0.21s
Test loss: 1.1758 score: 0.5078 time: 0.33s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 0.67s
Val loss: 1.1761 score: 0.4341 time: 0.20s
Test loss: 1.3067 score: 0.4844 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.86s
Val loss: 1.1725 score: 0.3721 time: 0.28s
Test loss: 1.2916 score: 0.4219 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.85s
Val loss: 1.2650 score: 0.3333 time: 0.36s
Test loss: 1.4051 score: 0.4062 time: 0.35s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 1.00s
Val loss: 1.2558 score: 0.3333 time: 0.18s
Test loss: 1.3850 score: 0.3984 time: 0.24s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.4885;  Loss pred: 0.4885; Loss self: 0.0000; time: 0.85s
Val loss: 1.3210 score: 0.3333 time: 0.23s
Test loss: 1.4721 score: 0.3906 time: 0.34s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000290
Train loss: 0.4545;  Loss pred: 0.4545; Loss self: 0.0000; time: 1.29s
Val loss: 1.4218 score: 0.3023 time: 0.28s
Test loss: 1.6288 score: 0.3750 time: 0.27s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000290
Train loss: 0.4313;  Loss pred: 0.4313; Loss self: 0.0000; time: 2.02s
Val loss: 1.1842 score: 0.3566 time: 0.31s
Test loss: 1.3550 score: 0.3672 time: 6.40s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000290
Train loss: 0.4308;  Loss pred: 0.4308; Loss self: 0.0000; time: 19.40s
Val loss: 1.1048 score: 0.3333 time: 4.02s
Test loss: 1.2458 score: 0.3594 time: 4.83s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000290
Train loss: 0.3778;  Loss pred: 0.3778; Loss self: 0.0000; time: 9.81s
Val loss: 1.0688 score: 0.3333 time: 4.16s
Test loss: 1.1854 score: 0.3438 time: 3.80s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000290
Train loss: 0.3565;  Loss pred: 0.3565; Loss self: 0.0000; time: 0.76s
Val loss: 1.0580 score: 0.3256 time: 0.19s
Test loss: 1.1475 score: 0.3281 time: 0.39s
     INFO: Early stopping counter 15 of 20
Epoch 24/1000, LR 0.000290
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.70s
Val loss: 0.9320 score: 0.3566 time: 0.39s
Test loss: 0.9614 score: 0.3828 time: 0.23s
     INFO: Early stopping counter 16 of 20
Epoch 25/1000, LR 0.000290
Train loss: 0.3137;  Loss pred: 0.3137; Loss self: 0.0000; time: 0.97s
Val loss: 1.0224 score: 0.3256 time: 1.87s
Test loss: 1.0735 score: 0.3281 time: 0.55s
     INFO: Early stopping counter 17 of 20
Epoch 26/1000, LR 0.000290
Train loss: 0.2887;  Loss pred: 0.2887; Loss self: 0.0000; time: 1.21s
Val loss: 0.7939 score: 0.4341 time: 0.23s
Test loss: 0.7807 score: 0.4375 time: 0.20s
     INFO: Early stopping counter 18 of 20
Epoch 27/1000, LR 0.000290
Train loss: 0.2705;  Loss pred: 0.2705; Loss self: 0.0000; time: 0.81s
Val loss: 0.7535 score: 0.4961 time: 0.23s
Test loss: 0.7609 score: 0.4453 time: 0.29s
     INFO: Early stopping counter 19 of 20
Epoch 28/1000, LR 0.000290
Train loss: 0.2412;  Loss pred: 0.2412; Loss self: 0.0000; time: 0.75s
Val loss: 0.8548 score: 0.4651 time: 0.25s
Test loss: 0.8971 score: 0.4688 time: 0.27s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 0.7206,   Val_Loss: 0.6188,   Val_Precision: 0.8919,   Val_Recall: 0.5077,   Val_accuracy: 0.6471,   Val_Score: 0.7209,   Val_Loss: 0.6188,   Test_Precision: 0.9310,   Test_Recall: 0.4219,   Test_accuracy: 0.5806,   Test_Score: 0.6953,   Test_loss: 0.6248


[0.2371287209680304, 0.3580168620683253, 0.31516918702982366, 0.16618933295831084, 0.19784188200719655, 0.17650196002796292, 0.31106790399644524, 0.33200749300885946, 0.2153564359759912, 0.25610955199226737, 0.24630050791893154, 0.18140574893914163, 0.28153304394800216, 0.2828591400757432, 0.19753019092604518, 0.25215676496736705, 0.24425148498266935, 0.23375860694795847, 0.25381466397084296, 0.2909760610200465, 0.20198977598920465, 0.21438026195392013, 0.19143254100345075, 0.17488325398880988, 0.2062225400004536, 0.3378107550088316, 0.19758981000632048, 0.3060342730022967, 0.3674273450160399, 0.2882050590123981, 0.19275179994292557, 0.2060297499410808, 0.30268066201824695, 0.3584521219599992, 0.40608187892939895, 0.31111236999277025, 0.33155253098811954, 0.20121390698477626, 0.3396684570470825, 0.2020997789222747, 0.32628113601822406, 0.2778687849640846, 0.20147179800551385, 0.5206900680204853, 0.21938593708910048, 0.4180815489962697, 5.947200809954666, 2.9253667849116027, 1.600497044972144, 1.3352999880444258, 0.26992927701212466, 0.3929311520187184, 0.2181591490516439, 3.0740863559767604, 4.808269142988138, 0.27611631399486214, 0.23990192194469273, 0.3714196350192651, 0.2865522470092401, 0.43139739299658686, 0.25266338500659913, 0.19569578696973622, 0.19832855300046504, 0.2417719910154119, 0.2830352400196716, 0.23583085904829204, 0.2525313700316474, 0.2364227930083871, 0.2461281669093296, 0.38384892197791487, 0.2329325849423185, 0.31541534792631865, 0.17642683896701783, 0.3380673279752955, 0.17157754092477262, 0.1783552758861333, 0.17535666597541422, 0.19929315603803843, 0.2970531089231372, 0.2433247750159353, 0.27364106802269816, 0.17772718297783285, 0.38236371404491365, 0.27063984493725, 0.33768011699430645, 0.18661125598009676, 0.18091562495101243, 0.35914746893104166, 0.23967251600697637, 0.3431424230802804, 0.272856860072352, 6.409464429016225, 4.8406411609612405, 3.807771914987825, 0.3928262130357325, 0.23181068105623126, 0.55316364497412, 0.19981954991817474, 0.2968344968976453, 0.27775971696246415]
[0.0018382071392870573, 0.0027753245121575603, 0.0024431719924792534, 0.0012882894027776033, 0.0015336580000557872, 0.0013682322482787823, 0.0024113791007476376, 0.002573701496192709, 0.0016694297362479936, 0.0019853453642811425, 0.0019093062629374539, 0.0014062461158072994, 0.0021824266972713348, 0.0021927065122150638, 0.0015312417901243813, 0.001954703604398194, 0.0018934223642067393, 0.001812082224402779, 0.001967555534657697, 0.00225562838000036, 0.0015658122169705786, 0.0016618624957668226, 0.0014839731860732616, 0.0013556841394481386, 0.0015986243410887875, 0.002618688023324276, 0.0015317039535373681, 0.0023723587054441603, 0.0028482739923724023, 0.002234147744282156, 0.0014941999995575627, 0.0015971298445045024, 0.002346361721071682, 0.002778698619844955, 0.003147921542088364, 0.002411723798393568, 0.002570174658822632, 0.001559797728564157, 0.002633088814318469, 0.0015666649528858505, 0.0025293111319242174, 0.0021540215888688733, 0.0015617968837636732, 0.004036357116437871, 0.001700666178985275, 0.0032409422402811605, 0.046102331860113685, 0.022677261898539556, 0.012406953836993365, 0.010351162698018805, 0.0020924750155978658, 0.003045977922625724, 0.00169115619419879, 0.023830126790517523, 0.037273404209210374, 0.0021404365425958306, 0.001859704821276688, 0.002879221976893528, 0.002221335248133644, 0.003344165837182844, 0.001958630891524024, 0.00151702160441656, 0.0015374306434144576, 0.0018742014807396272, 0.00219407162805947, 0.0018281461941728064, 0.0019576075196251737, 0.0018327348295223806, 0.001907970286118834, 0.0029755730385884875, 0.0018056789530412286, 0.0024450802164830902, 0.0013783346794298268, 0.002641150999806996, 0.001340449538474786, 0.0013934005928604165, 0.0013699739529329236, 0.0015569777815471753, 0.0023207274134620093, 0.0019009748048119945, 0.0021378208439273294, 0.0013884936170143192, 0.002987216515975888, 0.0021143737885722658, 0.002638125914018019, 0.001457900437344506, 0.0014134033199297846, 0.002805839601023763, 0.0018724415313045029, 0.002680800180314691, 0.00213169421931525, 0.05007394085168926, 0.03781750907000969, 0.02974821808584238, 0.00306895478934166, 0.0018110209457518067, 0.004321590976360312, 0.0015610902337357402, 0.002319019507012854, 0.002169997788769251]
[544.0083321555626, 360.3182242723002, 409.30397167218337, 776.2231047185206, 652.0358515155431, 730.8700706754913, 414.70045074619514, 388.54544766722387, 599.0069412849191, 503.69070187548044, 523.7504424573076, 711.1130752712648, 458.205538472513, 456.0573858969406, 653.0647259299075, 511.5865125280084, 528.1441789766523, 551.8513379433306, 508.2448664779231, 443.33543985638283, 638.6461857697907, 601.7345012281394, 673.8666233222838, 737.6349482166785, 625.537829180633, 381.87061272406044, 652.8676756957941, 421.52141567174044, 351.0898188439637, 447.5979722286923, 669.2544507402645, 626.1231692844876, 426.1917465748879, 359.8806984169437, 317.6699249424716, 414.6411793365778, 389.0786163373382, 641.1087679429637, 379.782100232283, 638.2985705769225, 395.3645668096327, 464.24790037741604, 640.2881260655129, 247.748147934569, 588.0048726532935, 308.55224371824886, 21.69087678762664, 44.09703448653125, 80.5999613715283, 96.60750479667355, 477.90295824118994, 328.30178858879253, 591.3114373647578, 41.96368776342054, 26.828781036128085, 467.194415764946, 537.7197437782087, 347.3160485802236, 450.1796839716992, 299.0282326555938, 510.56072092373313, 659.1863933174477, 650.4358452093256, 533.5605644732306, 455.7736343751195, 547.0022053966404, 510.82762503459924, 545.6326708543019, 524.1171769158867, 336.06972069970317, 553.8083048017712, 408.9845368911298, 725.5131971384977, 378.6228050092841, 746.0183850992538, 717.668705700181, 729.9408852694894, 642.2699230854122, 430.8993784445465, 526.0458989086388, 467.76604449366704, 720.2049672725916, 334.7597988468245, 472.9532712734069, 379.0569641450289, 685.9178956153213, 707.5121346465198, 356.3995602724872, 534.0620699132402, 373.02295312536614, 469.1104338225505, 19.9704673327357, 26.442778083261626, 33.615458818890225, 325.8438356514584, 552.1747290365388, 231.39626250381755, 640.5779617280464, 431.21672628278463, 460.82996267344856]
Elapsed: 0.5939705047779716~1.1480506007696958
Time per graph: 0.004617633482153314~0.008935617803568663
Speed: 469.2917427349275~184.04797849227495
Total Time: 0.2785
best val loss: 0.6188042256259179 test_score: 0.6953

Testing...
Test loss: 0.6386 score: 0.7109 time: 0.19s
test Score 0.7109
Epoch Time List: [1.08047727891244, 1.1801203248323873, 1.5338379230815917, 1.0483271301491186, 1.200642542913556, 1.1672637881711125, 1.2465601459844038, 1.158458936959505, 1.037696143030189, 1.4553013039985672, 1.1019570929929614, 1.3070393230300397, 1.174200903158635, 1.4147624570177868, 1.4551294369157404, 1.2850432831328362, 1.1168986287666485, 1.131648898939602, 1.2069831078406423, 1.482171930023469, 1.13310666102916, 0.9956009169109166, 0.9037431471515447, 0.8897450850345194, 0.9540024909656495, 1.1531611001119018, 1.2719979940447956, 1.1647740679327399, 1.4209302071249112, 1.4600556050427258, 1.1567120578838512, 1.3285422699991614, 1.069090063101612, 1.2504503400996327, 1.5536258540814742, 1.3749740719795227, 1.2455692199291661, 1.3677147299749777, 1.566301807994023, 1.4789679300738499, 1.2891839080257341, 1.2581599310506135, 1.4626701849047095, 1.805594091070816, 1.4560371841071174, 1.2829375370638445, 12.757952664978802, 11.520802109851502, 14.298378262086771, 13.75709409697447, 1.396039982093498, 1.4992779330350459, 1.2977984060999006, 4.34884141094517, 22.060449796030298, 2.153178368927911, 1.1001347100827843, 1.525635416037403, 1.3536542870569974, 1.4255630220286548, 1.3515462139621377, 1.1618398178834468, 1.119974555913359, 1.1749498939607292, 1.4794941729633138, 1.31480993994046, 1.269807622069493, 1.4827658559661359, 1.133173708920367, 1.5608236950356513, 1.8802751230541617, 1.4227634280687198, 0.8687458909116685, 1.0303035009419546, 0.8897444070316851, 0.9083563281456009, 0.8570424998179078, 0.8851414339151233, 1.3235724021214992, 1.4413861619541422, 1.5090926348930225, 1.4965480879181996, 1.2139911188278347, 1.2125465050339699, 1.6469455009792, 1.0527404930908233, 1.317579381982796, 1.5694369291886687, 1.4096059040166438, 1.4232202750863507, 1.840958361979574, 8.731505276053213, 28.257947873091325, 17.773902630084194, 1.3314092690125108, 1.319232914946042, 3.3946322129340842, 1.6377326528308913, 1.3266814370872453, 1.2795324579346925]
Total Epoch List: [25, 47, 28]
Total Time List: [0.20660602499265224, 0.3195717759663239, 0.27846491197124124]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da5546c0490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.0490;  Loss pred: 1.0490; Loss self: 0.0000; time: 0.64s
Val loss: 1.6674 score: 0.4419 time: 0.21s
Test loss: 1.8686 score: 0.3488 time: 0.25s
Epoch 2/1000, LR 0.000015
Train loss: 1.0567;  Loss pred: 1.0567; Loss self: 0.0000; time: 0.80s
Val loss: 0.9714 score: 0.3876 time: 0.35s
Test loss: 1.1367 score: 0.4419 time: 0.26s
Epoch 3/1000, LR 0.000045
Train loss: 0.9670;  Loss pred: 0.9670; Loss self: 0.0000; time: 0.61s
Val loss: 0.7189 score: 0.5194 time: 0.27s
Test loss: 0.8468 score: 0.4884 time: 0.37s
Epoch 4/1000, LR 0.000075
Train loss: 0.9014;  Loss pred: 0.9014; Loss self: 0.0000; time: 0.73s
Val loss: 0.6432 score: 0.5814 time: 0.31s
Test loss: 0.7169 score: 0.4961 time: 0.38s
Epoch 5/1000, LR 0.000105
Train loss: 0.8590;  Loss pred: 0.8590; Loss self: 0.0000; time: 0.65s
Val loss: 0.6378 score: 0.6124 time: 0.37s
Test loss: 0.6444 score: 0.5349 time: 0.32s
Epoch 6/1000, LR 0.000135
Train loss: 0.7340;  Loss pred: 0.7340; Loss self: 0.0000; time: 0.56s
Val loss: 0.6522 score: 0.5736 time: 0.20s
Test loss: 0.6372 score: 0.5271 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.79s
Val loss: 0.6667 score: 0.5271 time: 0.25s
Test loss: 0.6389 score: 0.5271 time: 0.31s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.69s
Val loss: 0.6844 score: 0.4729 time: 0.21s
Test loss: 0.6444 score: 0.4729 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.91s
Val loss: 0.6999 score: 0.4496 time: 0.33s
Test loss: 0.6537 score: 0.4496 time: 0.30s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.60s
Val loss: 0.7139 score: 0.4574 time: 0.41s
Test loss: 0.6666 score: 0.4264 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5078;  Loss pred: 0.5078; Loss self: 0.0000; time: 0.76s
Val loss: 0.7267 score: 0.4651 time: 0.38s
Test loss: 0.6797 score: 0.4109 time: 0.34s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.73s
Val loss: 0.7379 score: 0.4496 time: 0.27s
Test loss: 0.6922 score: 0.4109 time: 0.43s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.56s
Val loss: 0.7472 score: 0.4419 time: 0.49s
Test loss: 0.7039 score: 0.4186 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.71s
Val loss: 0.7535 score: 0.4419 time: 0.43s
Test loss: 0.7179 score: 0.4031 time: 0.31s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.4683;  Loss pred: 0.4683; Loss self: 0.0000; time: 1.27s
Val loss: 0.7605 score: 0.4419 time: 0.21s
Test loss: 0.7329 score: 0.4031 time: 0.33s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.4553;  Loss pred: 0.4553; Loss self: 0.0000; time: 0.78s
Val loss: 0.7798 score: 0.4496 time: 0.28s
Test loss: 0.7537 score: 0.3953 time: 0.58s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.4326;  Loss pred: 0.4326; Loss self: 0.0000; time: 0.86s
Val loss: 0.7967 score: 0.4496 time: 0.28s
Test loss: 0.7720 score: 0.3953 time: 0.51s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.4056;  Loss pred: 0.4056; Loss self: 0.0000; time: 1.18s
Val loss: 0.8118 score: 0.4496 time: 0.31s
Test loss: 0.7912 score: 0.3953 time: 1.94s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.3684;  Loss pred: 0.3684; Loss self: 0.0000; time: 2.55s
Val loss: 0.8232 score: 0.4574 time: 0.84s
Test loss: 0.8080 score: 0.4186 time: 0.29s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.3769;  Loss pred: 0.3769; Loss self: 0.0000; time: 1.16s
Val loss: 0.8395 score: 0.4496 time: 0.32s
Test loss: 0.8287 score: 0.4031 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.3455;  Loss pred: 0.3455; Loss self: 0.0000; time: 0.80s
Val loss: 0.8615 score: 0.4574 time: 0.31s
Test loss: 0.8568 score: 0.3953 time: 0.29s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.3391;  Loss pred: 0.3391; Loss self: 0.0000; time: 0.66s
Val loss: 0.8944 score: 0.4419 time: 0.22s
Test loss: 0.8958 score: 0.4031 time: 0.35s
     INFO: Early stopping counter 17 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.65s
Val loss: 0.9205 score: 0.4341 time: 0.32s
Test loss: 0.9299 score: 0.3953 time: 0.31s
     INFO: Early stopping counter 18 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.2787;  Loss pred: 0.2787; Loss self: 0.0000; time: 0.96s
Val loss: 0.9334 score: 0.4419 time: 0.33s
Test loss: 0.9449 score: 0.3953 time: 0.24s
     INFO: Early stopping counter 19 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.75s
Val loss: 0.9491 score: 0.4496 time: 0.28s
Test loss: 0.9599 score: 0.3876 time: 0.27s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 004,   Train_Loss: 0.8590,   Val_Loss: 0.6378,   Val_Precision: 0.5875,   Val_Recall: 0.7344,   Val_accuracy: 0.6528,   Val_Score: 0.6124,   Val_Loss: 0.6378,   Test_Precision: 0.5263,   Test_Recall: 0.7692,   Test_accuracy: 0.6250,   Test_Score: 0.5349,   Test_loss: 0.6444


[0.2562014559516683, 0.2703547530109063, 0.37667495699133724, 0.3852500549983233, 0.3291495699668303, 0.2320341810118407, 0.3147234639618546, 0.21892127802129835, 0.3043854789575562, 0.23977816407568753, 0.34668736695311964, 0.4387268429854885, 0.23039625294040889, 0.31411162100266665, 0.3311083000153303, 0.5851047420874238, 0.5173066169954836, 1.9424819039413705, 0.29750710004009306, 0.20846260502003133, 0.29954978299792856, 0.35895688203163445, 0.3193751380313188, 0.24615846807137132, 0.27373539097607136]
[0.0019860577980749484, 0.002095773279154312, 0.002919960906909591, 0.0029864345348707233, 0.0025515470540064364, 0.0017987145814871372, 0.0024397167748980977, 0.001697064170707739, 0.0023595773562601254, 0.0018587454579510661, 0.0026874989686288342, 0.003400983278957275, 0.0017860174646543324, 0.0024349738062222222, 0.0025667310078707774, 0.004535695675096309, 0.004010128813918478, 0.015057999255359462, 0.0023062565894580857, 0.0016159891862017933, 0.0023220913410692138, 0.0027826114886173215, 0.0024757762638086727, 0.0019082051788478396, 0.002121979775008305]
[503.5100191793425, 477.1508492576643, 342.4703384328434, 334.8474538194717, 391.9190901965931, 555.9525731832464, 409.88364317073984, 589.2529093834811, 423.8047111898787, 537.9972796825666, 372.09316605252553, 294.0326129173429, 559.9049392238395, 410.6820358578992, 389.6006231013457, 220.4733455753216, 249.36854809480667, 66.40988507447818, 433.60309714496066, 618.8160221235089, 430.64628092517114, 359.3746392878223, 403.91371975657637, 524.0526601042938, 471.2580260083234]
Elapsed: 0.38548569484148176~0.32976778523928835
Time per graph: 0.0029882612003215635~0.0025563394204595993
Speed: 414.8407387497617~122.42691457350297
Total Time: 0.2743
best val loss: 0.6377728119377017 test_score: 0.5349

Testing...
Test loss: 0.6444 score: 0.5349 time: 0.24s
test Score 0.5349
Epoch Time List: [1.1011148050893098, 1.4071026218589395, 1.251521908910945, 1.419748632935807, 1.34490566898603, 0.9833657959243283, 1.347534346044995, 1.1058365540811792, 1.5337701669195667, 1.2422074950300157, 1.4817271209321916, 1.4359559561125934, 1.2692145209293813, 1.452220277977176, 1.8121107410406694, 1.6402653839904815, 1.6506095909280702, 3.4168841739883646, 3.680214260937646, 1.686986057087779, 1.4026936059817672, 1.2366012838901952, 1.2757146140793338, 1.5290989259956405, 1.297546111047268]
Total Epoch List: [25]
Total Time List: [0.2742510229581967]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da5546c3ac0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7025;  Loss pred: 0.7025; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5323 score: 0.5039 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3782 score: 0.4961 time: 0.23s
Epoch 2/1000, LR 0.000015
Train loss: 0.7302;  Loss pred: 0.7302; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2052 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0916 score: 0.4961 time: 0.25s
Epoch 3/1000, LR 0.000045
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0201 score: 0.5039 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9333 score: 0.4961 time: 0.36s
Epoch 4/1000, LR 0.000075
Train loss: 0.7180;  Loss pred: 0.7180; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9018 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8360 score: 0.4961 time: 0.39s
Epoch 5/1000, LR 0.000105
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8321 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7762 score: 0.4961 time: 0.25s
Epoch 6/1000, LR 0.000135
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7793 score: 0.5039 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7329 score: 0.4961 time: 0.30s
Epoch 7/1000, LR 0.000165
Train loss: 0.6484;  Loss pred: 0.6484; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7435 score: 0.5039 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7067 score: 0.4961 time: 0.39s
Epoch 8/1000, LR 0.000195
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7247 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.31s
Epoch 9/1000, LR 0.000225
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7113 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.4961 time: 0.29s
Epoch 10/1000, LR 0.000255
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6763 score: 0.4961 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7038 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6763 score: 0.4961 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7053 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7039 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6746 score: 0.4961 time: 0.34s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6706 score: 0.4961 time: 0.31s
Epoch 15/1000, LR 0.000285
Train loss: 0.4966;  Loss pred: 0.4966; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5039 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6646 score: 0.4961 time: 0.25s
Epoch 16/1000, LR 0.000285
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6731 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6584 score: 0.4961 time: 0.27s
Epoch 17/1000, LR 0.000285
Train loss: 0.4653;  Loss pred: 0.4653; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6635 score: 0.5039 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6512 score: 0.4961 time: 0.31s
Epoch 18/1000, LR 0.000285
Train loss: 0.4749;  Loss pred: 0.4749; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6553 score: 0.5039 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6455 score: 0.4961 time: 0.93s
Epoch 19/1000, LR 0.000285
Train loss: 0.4408;  Loss pred: 0.4408; Loss self: 0.0000; time: 3.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6516 score: 0.5039 time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6472 score: 0.4961 time: 0.81s
Epoch 20/1000, LR 0.000285
Train loss: 0.4241;  Loss pred: 0.4241; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6571 score: 0.5039 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6507 score: 0.4961 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4049;  Loss pred: 0.4049; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6652 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6550 score: 0.4961 time: 0.41s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.3999;  Loss pred: 0.3999; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6708 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6584 score: 0.4961 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.3780;  Loss pred: 0.3780; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6733 score: 0.5039 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6589 score: 0.4961 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.3602;  Loss pred: 0.3602; Loss self: 0.0000; time: 0.48s
Val loss: 0.6681 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6572 score: 0.4961 time: 0.26s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.3485;  Loss pred: 0.3485; Loss self: 0.0000; time: 0.51s
Val loss: 0.6644 score: 0.4961 time: 0.24s
Test loss: 0.6509 score: 0.4884 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.71s
Val loss: 0.6653 score: 0.5039 time: 0.32s
Test loss: 0.6443 score: 0.4884 time: 0.35s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.3161;  Loss pred: 0.3161; Loss self: 0.0000; time: 0.69s
Val loss: 0.6669 score: 0.4884 time: 0.23s
Test loss: 0.6378 score: 0.5039 time: 0.27s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.3068;  Loss pred: 0.3068; Loss self: 0.0000; time: 0.54s
Val loss: 0.6663 score: 0.4961 time: 0.24s
Test loss: 0.6332 score: 0.4961 time: 0.31s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.65s
Val loss: 0.6557 score: 0.5039 time: 0.29s
Test loss: 0.6342 score: 0.5349 time: 0.43s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.2761;  Loss pred: 0.2761; Loss self: 0.0000; time: 0.59s
Val loss: 0.6603 score: 0.4806 time: 0.25s
Test loss: 0.6483 score: 0.5194 time: 0.28s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.2542;  Loss pred: 0.2542; Loss self: 0.0000; time: 0.97s
Val loss: 0.6712 score: 0.4961 time: 0.23s
Test loss: 0.6503 score: 0.5039 time: 0.25s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.2424;  Loss pred: 0.2424; Loss self: 0.0000; time: 0.65s
Val loss: 0.6946 score: 0.4884 time: 0.29s
Test loss: 0.6403 score: 0.5504 time: 0.32s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.82s
Val loss: 0.7270 score: 0.4961 time: 0.32s
Test loss: 0.6483 score: 0.5426 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.76s
Val loss: 0.7753 score: 0.5039 time: 0.30s
Test loss: 0.6881 score: 0.5426 time: 0.34s
     INFO: Early stopping counter 15 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.2015;  Loss pred: 0.2015; Loss self: 0.0000; time: 0.64s
Val loss: 0.7922 score: 0.5271 time: 0.28s
Test loss: 0.7298 score: 0.4884 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.63s
Val loss: 0.7527 score: 0.5271 time: 0.31s
Test loss: 0.7275 score: 0.5039 time: 0.34s
     INFO: Early stopping counter 17 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.1799;  Loss pred: 0.1799; Loss self: 0.0000; time: 0.65s
Val loss: 0.7094 score: 0.5349 time: 0.29s
Test loss: 0.7071 score: 0.5194 time: 0.28s
     INFO: Early stopping counter 18 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.1727;  Loss pred: 0.1727; Loss self: 0.0000; time: 0.63s
Val loss: 0.6541 score: 0.5891 time: 0.26s
Test loss: 0.6655 score: 0.5271 time: 0.27s
     INFO: Early stopping counter 19 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.1591;  Loss pred: 0.1591; Loss self: 0.0000; time: 0.65s
Val loss: 0.6882 score: 0.5426 time: 0.33s
Test loss: 0.7010 score: 0.5039 time: 0.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.4408,   Val_Loss: 0.6516,   Val_Precision: 0.5039,   Val_Recall: 1.0000,   Val_accuracy: 0.6701,   Val_Score: 0.5039,   Val_Loss: 0.6516,   Test_Precision: 0.4961,   Test_Recall: 1.0000,   Test_accuracy: 0.6632,   Test_Score: 0.4961,   Test_loss: 0.6472


[0.2562014559516683, 0.2703547530109063, 0.37667495699133724, 0.3852500549983233, 0.3291495699668303, 0.2320341810118407, 0.3147234639618546, 0.21892127802129835, 0.3043854789575562, 0.23977816407568753, 0.34668736695311964, 0.4387268429854885, 0.23039625294040889, 0.31411162100266665, 0.3311083000153303, 0.5851047420874238, 0.5173066169954836, 1.9424819039413705, 0.29750710004009306, 0.20846260502003133, 0.29954978299792856, 0.35895688203163445, 0.3193751380313188, 0.24615846807137132, 0.27373539097607136, 0.2332718869438395, 0.25991331599652767, 0.36371917405631393, 0.39779657300096005, 0.2563008959405124, 0.3057653440628201, 0.3902030859608203, 0.310687059070915, 0.29166157299187034, 0.1856199640315026, 0.19727533194236457, 0.20741215907037258, 0.3416605619713664, 0.31599949207156897, 0.2514625769108534, 0.27693165198434144, 0.31657860695850104, 0.9357704509748146, 0.820589434937574, 0.36846241005696356, 0.41230746603105217, 0.19149826001375914, 0.20066970703192055, 0.26733570091892034, 0.2109671781072393, 0.353491127025336, 0.2705555360298604, 0.31918534997384995, 0.43580342503264546, 0.2860989239998162, 0.25621126301120967, 0.3286353589501232, 0.2291289479471743, 0.3434650980634615, 0.20306879398413002, 0.3457658200059086, 0.29036691098008305, 0.2704487780574709, 0.2581208610208705]
[0.0019860577980749484, 0.002095773279154312, 0.002919960906909591, 0.0029864345348707233, 0.0025515470540064364, 0.0017987145814871372, 0.0024397167748980977, 0.001697064170707739, 0.0023595773562601254, 0.0018587454579510661, 0.0026874989686288342, 0.003400983278957275, 0.0017860174646543324, 0.0024349738062222222, 0.0025667310078707774, 0.004535695675096309, 0.004010128813918478, 0.015057999255359462, 0.0023062565894580857, 0.0016159891862017933, 0.0023220913410692138, 0.0027826114886173215, 0.0024757762638086727, 0.0019082051788478396, 0.002121979775008305, 0.001808309201115035, 0.002014831906949827, 0.002819528481056697, 0.0030836943643485276, 0.0019868286507016466, 0.002370273984983101, 0.0030248301237272893, 0.002408426814503217, 0.0022609424262935687, 0.0014389144498566093, 0.0015292661390880974, 0.0016078461943439734, 0.002648531488150127, 0.002449608465671077, 0.0019493223016345223, 0.002146756992126678, 0.002454097728360473, 0.007254034503680733, 0.006361158410368791, 0.0028562977523795626, 0.003196181907217459, 0.0014844826357655746, 0.001555579124278454, 0.002072369774565274, 0.0016354044814514675, 0.0027402412947700465, 0.002097329736665584, 0.002474305038556976, 0.003378321124284073, 0.0022178211162776447, 0.001986133821792323, 0.0025475609220939784, 0.0017761933949393357, 0.0026625201400268333, 0.0015741766975513956, 0.002680355193844253, 0.0022509062866673105, 0.0020965021554842705, 0.002000936907138531]
[503.5100191793425, 477.1508492576643, 342.4703384328434, 334.8474538194717, 391.9190901965931, 555.9525731832464, 409.88364317073984, 589.2529093834811, 423.8047111898787, 537.9972796825666, 372.09316605252553, 294.0326129173429, 559.9049392238395, 410.6820358578992, 389.6006231013457, 220.4733455753216, 249.36854809480667, 66.40988507447818, 433.60309714496066, 618.8160221235089, 430.64628092517114, 359.3746392878223, 403.91371975657637, 524.0526601042938, 471.2580260083234, 553.0027715301027, 496.3193190214363, 354.6692316529543, 324.2863532655136, 503.31466664065414, 421.8921552257299, 330.59707788408593, 415.2087968702793, 442.29343851065255, 694.9683492994681, 653.9084168804658, 621.9500369611011, 377.56772176359976, 408.22850427488504, 512.9987992039552, 465.8189090183669, 407.481734913661, 137.85432086001177, 157.204071253749, 350.10355596397704, 312.87330603488175, 673.6353635314042, 642.8474028692331, 482.5393673818527, 611.4695241097003, 364.9313664123572, 476.79674898894945, 404.1538874217399, 296.0050164597416, 450.89299252339333, 503.49074620640715, 392.5323203568556, 563.0017558049494, 375.5840134189264, 635.2527016538122, 373.08488154727263, 444.2654969348364, 476.98496153895445, 499.7658828883638]
Elapsed: 0.3458960691586981~0.23728949270404365
Time per graph: 0.00268136487719921~0.0018394534318142918
Speed: 437.19991305972377~126.4974765296857
Total Time: 0.2587
best val loss: 0.6516291406727577 test_score: 0.4961

Testing...
Test loss: 0.6655 score: 0.5271 time: 0.22s
test Score 0.5271
Epoch Time List: [1.1011148050893098, 1.4071026218589395, 1.251521908910945, 1.419748632935807, 1.34490566898603, 0.9833657959243283, 1.347534346044995, 1.1058365540811792, 1.5337701669195667, 1.2422074950300157, 1.4817271209321916, 1.4359559561125934, 1.2692145209293813, 1.452220277977176, 1.8121107410406694, 1.6402653839904815, 1.6506095909280702, 3.4168841739883646, 3.680214260937646, 1.686986057087779, 1.4026936059817672, 1.2366012838901952, 1.2757146140793338, 1.5290989259956405, 1.297546111047268, 1.076138508040458, 1.1525688630063087, 1.173368752002716, 1.4686446620617062, 1.4301768069854006, 1.2040172829292715, 1.3356331050163135, 1.2073281478369609, 1.0624188689980656, 0.9034261400811374, 0.9735827900003642, 0.8955942548345774, 1.0351393700111657, 1.292315526981838, 1.4670586970169097, 1.2013399129500613, 1.2467452900018543, 2.2577694330830127, 5.139752805931494, 1.4958562899846584, 1.7180799139896408, 1.0403898939257488, 0.981312726973556, 0.9333333559334278, 0.9569421421037987, 1.3791910021100193, 1.1922952629392967, 1.0936985401203856, 1.3755560768768191, 1.1270551509223878, 1.4504469801904634, 1.2632492219563574, 1.3637246420839801, 1.4082773530390114, 1.1205833989661187, 1.281441270140931, 1.2242263598600402, 1.1502696499228477, 1.2281286269426346]
Total Epoch List: [25, 39]
Total Time List: [0.2742510229581967, 0.2586536640301347]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7da5546c11b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9057;  Loss pred: 0.9057; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7225 score: 0.5039 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7957 score: 0.5000 time: 0.28s
Epoch 2/1000, LR 0.000020
Train loss: 0.9027;  Loss pred: 0.9027; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7385 score: 0.5039 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8205 score: 0.5000 time: 0.38s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.8519;  Loss pred: 0.8519; Loss self: 0.0000; time: 0.71s
Val loss: 0.7134 score: 0.4961 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7993 score: 0.5000 time: 0.39s
Epoch 4/1000, LR 0.000080
Train loss: 0.8142;  Loss pred: 0.8142; Loss self: 0.0000; time: 0.82s
Val loss: 0.6808 score: 0.5116 time: 0.19s
Test loss: 0.7524 score: 0.4922 time: 0.28s
Epoch 5/1000, LR 0.000110
Train loss: 0.7879;  Loss pred: 0.7879; Loss self: 0.0000; time: 0.79s
Val loss: 0.6882 score: 0.5814 time: 0.41s
Test loss: 0.7456 score: 0.4844 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.7799;  Loss pred: 0.7799; Loss self: 0.0000; time: 0.71s
Val loss: 0.6816 score: 0.6124 time: 0.27s
Test loss: 0.7322 score: 0.5156 time: 0.36s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.7816;  Loss pred: 0.7816; Loss self: 0.0000; time: 0.80s
Val loss: 0.6408 score: 0.6434 time: 0.26s
Test loss: 0.6690 score: 0.6016 time: 0.29s
Epoch 8/1000, LR 0.000200
Train loss: 0.7451;  Loss pred: 0.7451; Loss self: 0.0000; time: 0.62s
Val loss: 0.6162 score: 0.6589 time: 0.19s
Test loss: 0.6332 score: 0.6328 time: 0.22s
Epoch 9/1000, LR 0.000230
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 1.01s
Val loss: 0.6083 score: 0.6434 time: 0.32s
Test loss: 0.6244 score: 0.6406 time: 0.20s
Epoch 10/1000, LR 0.000260
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.75s
Val loss: 0.6028 score: 0.6434 time: 0.30s
Test loss: 0.6228 score: 0.6406 time: 0.45s
Epoch 11/1000, LR 0.000290
Train loss: 0.5720;  Loss pred: 0.5720; Loss self: 0.0000; time: 0.79s
Val loss: 0.6020 score: 0.5969 time: 0.26s
Test loss: 0.6215 score: 0.6250 time: 0.32s
Epoch 12/1000, LR 0.000290
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.74s
Val loss: 0.6093 score: 0.5969 time: 0.47s
Test loss: 0.6196 score: 0.6328 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.5547;  Loss pred: 0.5547; Loss self: 0.0000; time: 0.76s
Val loss: 0.6096 score: 0.5891 time: 0.32s
Test loss: 0.6252 score: 0.6172 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.60s
Val loss: 0.6136 score: 0.5581 time: 0.21s
Test loss: 0.6231 score: 0.6328 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.67s
Val loss: 0.6278 score: 0.5349 time: 0.23s
Test loss: 0.6283 score: 0.6016 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.4595;  Loss pred: 0.4595; Loss self: 0.0000; time: 0.60s
Val loss: 0.6361 score: 0.5039 time: 0.19s
Test loss: 0.6417 score: 0.5312 time: 0.32s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.4840;  Loss pred: 0.4840; Loss self: 0.0000; time: 0.60s
Val loss: 0.6023 score: 0.5891 time: 0.26s
Test loss: 0.6779 score: 0.5938 time: 0.35s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.4757;  Loss pred: 0.4757; Loss self: 0.0000; time: 0.51s
Val loss: 0.6043 score: 0.6279 time: 0.18s
Test loss: 0.7220 score: 0.6094 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000290
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.79s
Val loss: 0.6058 score: 0.5969 time: 1.59s
Test loss: 0.6648 score: 0.5938 time: 0.26s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000290
Train loss: 0.3741;  Loss pred: 0.3741; Loss self: 0.0000; time: 1.32s
Val loss: 0.6568 score: 0.5116 time: 0.20s
Test loss: 0.6399 score: 0.5156 time: 0.31s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000290
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.76s
Val loss: 0.8242 score: 0.4264 time: 0.35s
Test loss: 0.7965 score: 0.5000 time: 0.33s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000290
Train loss: 0.3403;  Loss pred: 0.3403; Loss self: 0.0000; time: 0.78s
Val loss: 0.8951 score: 0.5659 time: 0.30s
Test loss: 0.8862 score: 0.5156 time: 0.35s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000290
Train loss: 0.2793;  Loss pred: 0.2793; Loss self: 0.0000; time: 0.80s
Val loss: 0.8873 score: 0.6279 time: 0.27s
Test loss: 0.8636 score: 0.6562 time: 0.36s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000290
Train loss: 0.2838;  Loss pred: 0.2838; Loss self: 0.0000; time: 1.07s
Val loss: 1.0829 score: 0.5814 time: 0.27s
Test loss: 1.0916 score: 0.5781 time: 0.24s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000290
Train loss: 0.2323;  Loss pred: 0.2323; Loss self: 0.0000; time: 0.82s
Val loss: 1.1010 score: 0.5426 time: 0.27s
Test loss: 1.1470 score: 0.5391 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000290
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 0.69s
Val loss: 1.1161 score: 0.5504 time: 0.31s
Test loss: 1.2174 score: 0.4766 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000290
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.73s
Val loss: 1.1589 score: 0.5426 time: 0.29s
Test loss: 1.2938 score: 0.4688 time: 0.27s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000290
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.63s
Val loss: 1.0629 score: 0.5426 time: 0.31s
Test loss: 1.1834 score: 0.5312 time: 0.23s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000290
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.72s
Val loss: 1.1230 score: 0.5194 time: 0.22s
Test loss: 1.2434 score: 0.5391 time: 0.28s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000290
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.65s
Val loss: 0.9764 score: 0.5736 time: 0.21s
Test loss: 1.1110 score: 0.5625 time: 0.27s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000290
Train loss: 0.1383;  Loss pred: 0.1383; Loss self: 0.0000; time: 0.69s
Val loss: 0.8763 score: 0.6047 time: 0.35s
Test loss: 1.0262 score: 0.5625 time: 0.32s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.5720,   Val_Loss: 0.6020,   Val_Precision: 0.5802,   Val_Recall: 0.7231,   Val_accuracy: 0.6438,   Val_Score: 0.5969,   Val_Loss: 0.6020,   Test_Precision: 0.6053,   Test_Recall: 0.7188,   Test_accuracy: 0.6571,   Test_Score: 0.6250,   Test_loss: 0.6215


[0.2562014559516683, 0.2703547530109063, 0.37667495699133724, 0.3852500549983233, 0.3291495699668303, 0.2320341810118407, 0.3147234639618546, 0.21892127802129835, 0.3043854789575562, 0.23977816407568753, 0.34668736695311964, 0.4387268429854885, 0.23039625294040889, 0.31411162100266665, 0.3311083000153303, 0.5851047420874238, 0.5173066169954836, 1.9424819039413705, 0.29750710004009306, 0.20846260502003133, 0.29954978299792856, 0.35895688203163445, 0.3193751380313188, 0.24615846807137132, 0.27373539097607136, 0.2332718869438395, 0.25991331599652767, 0.36371917405631393, 0.39779657300096005, 0.2563008959405124, 0.3057653440628201, 0.3902030859608203, 0.310687059070915, 0.29166157299187034, 0.1856199640315026, 0.19727533194236457, 0.20741215907037258, 0.3416605619713664, 0.31599949207156897, 0.2514625769108534, 0.27693165198434144, 0.31657860695850104, 0.9357704509748146, 0.820589434937574, 0.36846241005696356, 0.41230746603105217, 0.19149826001375914, 0.20066970703192055, 0.26733570091892034, 0.2109671781072393, 0.353491127025336, 0.2705555360298604, 0.31918534997384995, 0.43580342503264546, 0.2860989239998162, 0.25621126301120967, 0.3286353589501232, 0.2291289479471743, 0.3434650980634615, 0.20306879398413002, 0.3457658200059086, 0.29036691098008305, 0.2704487780574709, 0.2581208610208705, 0.2816565870307386, 0.38226857502013445, 0.39275604707654566, 0.28470827091950923, 0.26729497499763966, 0.3654968219343573, 0.29790451598819345, 0.22966744098812342, 0.20580882101785392, 0.45274110697209835, 0.32520407799165696, 0.3650772030232474, 0.2617980169598013, 0.1867378120077774, 0.19616149901412427, 0.32791123003698885, 0.35315061500295997, 0.20897906203754246, 0.2686934880912304, 0.3176125759491697, 0.3367294450290501, 0.3519973459187895, 0.3630311409942806, 0.2445998089388013, 0.20513334404677153, 0.20068011502735317, 0.2793077420210466, 0.23830391908995807, 0.2797083529876545, 0.27526330295950174, 0.3265341080259532]
[0.0019860577980749484, 0.002095773279154312, 0.002919960906909591, 0.0029864345348707233, 0.0025515470540064364, 0.0017987145814871372, 0.0024397167748980977, 0.001697064170707739, 0.0023595773562601254, 0.0018587454579510661, 0.0026874989686288342, 0.003400983278957275, 0.0017860174646543324, 0.0024349738062222222, 0.0025667310078707774, 0.004535695675096309, 0.004010128813918478, 0.015057999255359462, 0.0023062565894580857, 0.0016159891862017933, 0.0023220913410692138, 0.0027826114886173215, 0.0024757762638086727, 0.0019082051788478396, 0.002121979775008305, 0.001808309201115035, 0.002014831906949827, 0.002819528481056697, 0.0030836943643485276, 0.0019868286507016466, 0.002370273984983101, 0.0030248301237272893, 0.002408426814503217, 0.0022609424262935687, 0.0014389144498566093, 0.0015292661390880974, 0.0016078461943439734, 0.002648531488150127, 0.002449608465671077, 0.0019493223016345223, 0.002146756992126678, 0.002454097728360473, 0.007254034503680733, 0.006361158410368791, 0.0028562977523795626, 0.003196181907217459, 0.0014844826357655746, 0.001555579124278454, 0.002072369774565274, 0.0016354044814514675, 0.0027402412947700465, 0.002097329736665584, 0.002474305038556976, 0.003378321124284073, 0.0022178211162776447, 0.001986133821792323, 0.0025475609220939784, 0.0017761933949393357, 0.0026625201400268333, 0.0015741766975513956, 0.002680355193844253, 0.0022509062866673105, 0.0020965021554842705, 0.002000936907138531, 0.0022004420861776453, 0.0029864732423448004, 0.003068406617785513, 0.002224283366558666, 0.00208824199216906, 0.0028554439213621663, 0.0023273790311577613, 0.0017942768827197142, 0.0016078814142019837, 0.0035370398982195184, 0.00254065685930982, 0.0028521656486191205, 0.002045297007498448, 0.0014588891563107609, 0.001532511711047846, 0.0025618064846639754, 0.0027589891797106247, 0.0016326489221683005, 0.0020991678757127374, 0.0024813482496028882, 0.002630698789289454, 0.002749979264990543, 0.002836180789017817, 0.001910936007334385, 0.0016026042503654025, 0.0015678133986511966, 0.0021820917345394264, 0.0018617493678902974, 0.002185221507716051, 0.0021504945543711074, 0.002551047718952759]
[503.5100191793425, 477.1508492576643, 342.4703384328434, 334.8474538194717, 391.9190901965931, 555.9525731832464, 409.88364317073984, 589.2529093834811, 423.8047111898787, 537.9972796825666, 372.09316605252553, 294.0326129173429, 559.9049392238395, 410.6820358578992, 389.6006231013457, 220.4733455753216, 249.36854809480667, 66.40988507447818, 433.60309714496066, 618.8160221235089, 430.64628092517114, 359.3746392878223, 403.91371975657637, 524.0526601042938, 471.2580260083234, 553.0027715301027, 496.3193190214363, 354.6692316529543, 324.2863532655136, 503.31466664065414, 421.8921552257299, 330.59707788408593, 415.2087968702793, 442.29343851065255, 694.9683492994681, 653.9084168804658, 621.9500369611011, 377.56772176359976, 408.22850427488504, 512.9987992039552, 465.8189090183669, 407.481734913661, 137.85432086001177, 157.204071253749, 350.10355596397704, 312.87330603488175, 673.6353635314042, 642.8474028692331, 482.5393673818527, 611.4695241097003, 364.9313664123572, 476.79674898894945, 404.1538874217399, 296.0050164597416, 450.89299252339333, 503.49074620640715, 392.5323203568556, 563.0017558049494, 375.5840134189264, 635.2527016538122, 373.08488154727263, 444.2654969348364, 476.98496153895445, 499.7658828883638, 454.45413277705705, 334.8431138846768, 325.90204772850666, 449.5830050409294, 478.87170344721335, 350.20824346042775, 429.6678738669168, 557.3275839591871, 621.9364134489454, 282.7222843890966, 393.5990003276767, 350.6107720230595, 488.9265453055521, 685.4530350536021, 652.5235616739633, 390.34954669152813, 362.4515845708697, 612.5015527967352, 476.37924130316003, 403.00671224203967, 380.1271373489695, 363.63910547646947, 352.5868322189379, 523.3037611735238, 623.9843678013424, 637.8310077336427, 458.27587546912633, 537.1292276216443, 457.61951201239077, 465.0093151677108, 391.99580335977174]
Elapsed: 0.32852911361321613~0.19996289139634726
Time per graph: 0.00255252125338115~0.0015495197458092961
Speed: 444.98541405470513~121.2554746364493
Total Time: 0.3271
best val loss: 0.6019554822019828 test_score: 0.6250

Testing...
Test loss: 0.6332 score: 0.6328 time: 0.24s
test Score 0.6328
Epoch Time List: [1.1011148050893098, 1.4071026218589395, 1.251521908910945, 1.419748632935807, 1.34490566898603, 0.9833657959243283, 1.347534346044995, 1.1058365540811792, 1.5337701669195667, 1.2422074950300157, 1.4817271209321916, 1.4359559561125934, 1.2692145209293813, 1.452220277977176, 1.8121107410406694, 1.6402653839904815, 1.6506095909280702, 3.4168841739883646, 3.680214260937646, 1.686986057087779, 1.4026936059817672, 1.2366012838901952, 1.2757146140793338, 1.5290989259956405, 1.297546111047268, 1.076138508040458, 1.1525688630063087, 1.173368752002716, 1.4686446620617062, 1.4301768069854006, 1.2040172829292715, 1.3356331050163135, 1.2073281478369609, 1.0624188689980656, 0.9034261400811374, 0.9735827900003642, 0.8955942548345774, 1.0351393700111657, 1.292315526981838, 1.4670586970169097, 1.2013399129500613, 1.2467452900018543, 2.2577694330830127, 5.139752805931494, 1.4958562899846584, 1.7180799139896408, 1.0403898939257488, 0.981312726973556, 0.9333333559334278, 0.9569421421037987, 1.3791910021100193, 1.1922952629392967, 1.0936985401203856, 1.3755560768768191, 1.1270551509223878, 1.4504469801904634, 1.2632492219563574, 1.3637246420839801, 1.4082773530390114, 1.1205833989661187, 1.281441270140931, 1.2242263598600402, 1.1502696499228477, 1.2281286269426346, 1.2694571620086208, 1.4404188019689173, 1.386336079100147, 1.2896171590546146, 1.4579739539185539, 1.339099774020724, 1.344409836921841, 1.0304237289819866, 1.53349170088768, 1.4945998969487846, 1.368175569921732, 1.5644150191219524, 1.3446796850766987, 0.9937526860740036, 1.0925623148214072, 1.116068591014482, 1.2154337769607082, 0.8923659860156476, 2.639718385064043, 1.8340976709732786, 1.443815384991467, 1.4220297699794173, 1.4213583809323609, 1.5802150090457872, 1.291467455914244, 1.1922527360729873, 1.2951006408547983, 1.1678669640095904, 1.215966294053942, 1.1279189590131864, 1.3637365979375318]
Total Epoch List: [25, 39, 31]
Total Time List: [0.2742510229581967, 0.2586536640301347, 0.3270647580502555]
T-times Epoch Time: 1.7096187277474086 ~ 0.5931229184912403
T-times Total Epoch: 33.333333333333336 ~ 1.360827634879543
T-times Total Time: 0.2701219613275801 ~ 0.012792875018079266
T-times Inference Elapsed: 0.38872636379790587 ~ 0.14920619610218264
T-times Time Per Graph: 0.003021864465538921 ~ 0.0011598271276546332
T-times Speed: 490.9556716801723 ~ 48.843196716903215
T-times cross validation test micro f1 score:0.6338025035798626 ~ 0.025674582360209613
T-times cross validation test precision:0.5772861539511528 ~ 0.05748667477766987
T-times cross validation test recall:0.7801014957264957 ~ 0.0387561241946876
T-times cross validation test f1_score:0.6338025035798626 ~ 0.01138067251456992
