Namespace(seed=15, model='AEtransGAT', dataset='exchange/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e9a7b7a00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.5825;  Loss pred: 3.5825; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4961 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 3.6386;  Loss pred: 3.6386; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4961 time: 0.15s
Epoch 3/1000, LR 0.000045
Train loss: 3.6258;  Loss pred: 3.6258; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4961 time: 0.15s
Epoch 4/1000, LR 0.000075
Train loss: 3.5790;  Loss pred: 3.5790; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4961 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 3.4878;  Loss pred: 3.4878; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4961 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 3.4457;  Loss pred: 3.4457; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4961 time: 0.20s
Epoch 7/1000, LR 0.000165
Train loss: 3.3762;  Loss pred: 3.3762; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.15s
Epoch 8/1000, LR 0.000195
Train loss: 3.2906;  Loss pred: 3.2906; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4961 time: 0.15s
Epoch 9/1000, LR 0.000225
Train loss: 3.1989;  Loss pred: 3.1989; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.15s
Epoch 10/1000, LR 0.000255
Train loss: 3.1123;  Loss pred: 3.1123; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 3.0142;  Loss pred: 3.0142; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.18s
Epoch 12/1000, LR 0.000285
Train loss: 2.8731;  Loss pred: 2.8731; Loss self: 0.0000; time: 0.26s
Val loss: 0.6928 score: 0.5659 time: 0.26s
Test loss: 0.6930 score: 0.4961 time: 0.20s
Epoch 13/1000, LR 0.000285
Train loss: 2.7887;  Loss pred: 2.7887; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 2.7072;  Loss pred: 2.7072; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000285
Train loss: 2.5962;  Loss pred: 2.5962; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000285
Train loss: 2.5143;  Loss pred: 2.5143; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 2.4297;  Loss pred: 2.4297; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.22s
Epoch 18/1000, LR 0.000285
Train loss: 2.3559;  Loss pred: 2.3559; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5039 time: 0.16s
Epoch 19/1000, LR 0.000285
Train loss: 2.2891;  Loss pred: 2.2891; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 2.1924;  Loss pred: 2.1924; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.19s
Epoch 21/1000, LR 0.000285
Train loss: 2.1350;  Loss pred: 2.1350; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.15s
Epoch 22/1000, LR 0.000285
Train loss: 2.0467;  Loss pred: 2.0467; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 1.9966;  Loss pred: 1.9966; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5039 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 1.9275;  Loss pred: 1.9275; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 1.8770;  Loss pred: 1.8770; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.8120;  Loss pred: 1.8120; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5039 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 1.7713;  Loss pred: 1.7713; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5039 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.6951;  Loss pred: 1.6951; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4961 time: 0.17s
Test loss: 0.6908 score: 0.5116 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.6654;  Loss pred: 1.6654; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 0.16s
Test loss: 0.6906 score: 0.5116 time: 0.15s
Epoch 30/1000, LR 0.000285
Train loss: 1.6208;  Loss pred: 1.6208; Loss self: 0.0000; time: 0.26s
Val loss: 0.6908 score: 0.5039 time: 0.15s
Test loss: 0.6904 score: 0.5426 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 1.5782;  Loss pred: 1.5782; Loss self: 0.0000; time: 0.28s
Val loss: 0.6906 score: 0.5349 time: 0.20s
Test loss: 0.6902 score: 0.5659 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 1.5428;  Loss pred: 1.5428; Loss self: 0.0000; time: 0.25s
Val loss: 0.6903 score: 0.5349 time: 0.16s
Test loss: 0.6899 score: 0.6047 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.5109;  Loss pred: 1.5109; Loss self: 0.0000; time: 0.26s
Val loss: 0.6901 score: 0.5504 time: 0.17s
Test loss: 0.6896 score: 0.6124 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 1.4684;  Loss pred: 1.4684; Loss self: 0.0000; time: 0.26s
Val loss: 0.6898 score: 0.5581 time: 0.17s
Test loss: 0.6893 score: 0.6357 time: 0.30s
Epoch 35/1000, LR 0.000285
Train loss: 1.4321;  Loss pred: 1.4321; Loss self: 0.0000; time: 0.25s
Val loss: 0.6895 score: 0.5581 time: 0.17s
Test loss: 0.6890 score: 0.6512 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 1.4081;  Loss pred: 1.4081; Loss self: 0.0000; time: 0.24s
Val loss: 0.6892 score: 0.6279 time: 0.16s
Test loss: 0.6886 score: 0.6899 time: 0.15s
Epoch 37/1000, LR 0.000285
Train loss: 1.3696;  Loss pred: 1.3696; Loss self: 0.0000; time: 0.28s
Val loss: 0.6889 score: 0.6589 time: 0.19s
Test loss: 0.6883 score: 0.7209 time: 0.15s
Epoch 38/1000, LR 0.000284
Train loss: 1.3478;  Loss pred: 1.3478; Loss self: 0.0000; time: 0.25s
Val loss: 0.6885 score: 0.8140 time: 0.17s
Test loss: 0.6879 score: 0.8217 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 1.3278;  Loss pred: 1.3278; Loss self: 0.0000; time: 0.26s
Val loss: 0.6882 score: 0.8295 time: 0.18s
Test loss: 0.6875 score: 0.8450 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 1.3005;  Loss pred: 1.3005; Loss self: 0.0000; time: 0.30s
Val loss: 0.6878 score: 0.9070 time: 0.24s
Test loss: 0.6870 score: 0.8992 time: 0.24s
Epoch 41/1000, LR 0.000284
Train loss: 1.2782;  Loss pred: 1.2782; Loss self: 0.0000; time: 0.36s
Val loss: 0.6874 score: 0.9225 time: 0.24s
Test loss: 0.6865 score: 0.9070 time: 0.23s
Epoch 42/1000, LR 0.000284
Train loss: 1.2525;  Loss pred: 1.2525; Loss self: 0.0000; time: 0.36s
Val loss: 0.6869 score: 0.9302 time: 0.25s
Test loss: 0.6860 score: 0.9302 time: 0.23s
Epoch 43/1000, LR 0.000284
Train loss: 1.2402;  Loss pred: 1.2402; Loss self: 0.0000; time: 0.33s
Val loss: 0.6865 score: 0.9380 time: 0.16s
Test loss: 0.6855 score: 0.9302 time: 0.15s
Epoch 44/1000, LR 0.000284
Train loss: 1.2220;  Loss pred: 1.2220; Loss self: 0.0000; time: 0.25s
Val loss: 0.6860 score: 0.8992 time: 0.15s
Test loss: 0.6849 score: 0.9457 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 1.2114;  Loss pred: 1.2114; Loss self: 0.0000; time: 0.26s
Val loss: 0.6854 score: 0.8992 time: 0.16s
Test loss: 0.6843 score: 0.9380 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 1.1932;  Loss pred: 1.1932; Loss self: 0.0000; time: 0.25s
Val loss: 0.6848 score: 0.8992 time: 0.16s
Test loss: 0.6837 score: 0.9302 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 1.1753;  Loss pred: 1.1753; Loss self: 0.0000; time: 0.26s
Val loss: 0.6842 score: 0.8992 time: 0.15s
Test loss: 0.6830 score: 0.9302 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 1.1649;  Loss pred: 1.1649; Loss self: 0.0000; time: 0.25s
Val loss: 0.6836 score: 0.8915 time: 0.18s
Test loss: 0.6823 score: 0.9147 time: 0.19s
Epoch 49/1000, LR 0.000284
Train loss: 1.1529;  Loss pred: 1.1529; Loss self: 0.0000; time: 0.26s
Val loss: 0.6829 score: 0.8915 time: 0.25s
Test loss: 0.6815 score: 0.9070 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.1369;  Loss pred: 1.1369; Loss self: 0.0000; time: 0.26s
Val loss: 0.6822 score: 0.8837 time: 0.16s
Test loss: 0.6806 score: 0.9070 time: 0.15s
Epoch 51/1000, LR 0.000284
Train loss: 1.1286;  Loss pred: 1.1286; Loss self: 0.0000; time: 0.25s
Val loss: 0.6813 score: 0.8760 time: 0.16s
Test loss: 0.6797 score: 0.9070 time: 0.15s
Epoch 52/1000, LR 0.000284
Train loss: 1.1157;  Loss pred: 1.1157; Loss self: 0.0000; time: 0.24s
Val loss: 0.6805 score: 0.8760 time: 0.15s
Test loss: 0.6787 score: 0.9147 time: 0.15s
Epoch 53/1000, LR 0.000284
Train loss: 1.1109;  Loss pred: 1.1109; Loss self: 0.0000; time: 0.25s
Val loss: 0.6795 score: 0.8760 time: 0.16s
Test loss: 0.6777 score: 0.9070 time: 0.16s
Epoch 54/1000, LR 0.000284
Train loss: 1.1022;  Loss pred: 1.1022; Loss self: 0.0000; time: 0.28s
Val loss: 0.6786 score: 0.8760 time: 0.24s
Test loss: 0.6766 score: 0.9070 time: 0.23s
Epoch 55/1000, LR 0.000284
Train loss: 1.0913;  Loss pred: 1.0913; Loss self: 0.0000; time: 0.25s
Val loss: 0.6776 score: 0.8760 time: 0.16s
Test loss: 0.6754 score: 0.8992 time: 0.15s
Epoch 56/1000, LR 0.000284
Train loss: 1.0859;  Loss pred: 1.0859; Loss self: 0.0000; time: 0.24s
Val loss: 0.6765 score: 0.8682 time: 0.16s
Test loss: 0.6742 score: 0.8992 time: 0.15s
Epoch 57/1000, LR 0.000283
Train loss: 1.0802;  Loss pred: 1.0802; Loss self: 0.0000; time: 0.24s
Val loss: 0.6754 score: 0.8760 time: 0.15s
Test loss: 0.6729 score: 0.9147 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 1.0708;  Loss pred: 1.0708; Loss self: 0.0000; time: 0.24s
Val loss: 0.6742 score: 0.8682 time: 0.16s
Test loss: 0.6716 score: 0.9147 time: 0.16s
Epoch 59/1000, LR 0.000283
Train loss: 1.0647;  Loss pred: 1.0647; Loss self: 0.0000; time: 0.25s
Val loss: 0.6730 score: 0.8682 time: 0.16s
Test loss: 0.6701 score: 0.9147 time: 0.22s
Epoch 60/1000, LR 0.000283
Train loss: 1.0600;  Loss pred: 1.0600; Loss self: 0.0000; time: 0.36s
Val loss: 0.6717 score: 0.8682 time: 0.24s
Test loss: 0.6686 score: 0.9147 time: 0.20s
Epoch 61/1000, LR 0.000283
Train loss: 1.0524;  Loss pred: 1.0524; Loss self: 0.0000; time: 0.26s
Val loss: 0.6703 score: 0.8682 time: 0.16s
Test loss: 0.6670 score: 0.9147 time: 0.15s
Epoch 62/1000, LR 0.000283
Train loss: 1.0468;  Loss pred: 1.0468; Loss self: 0.0000; time: 0.25s
Val loss: 0.6687 score: 0.8682 time: 0.16s
Test loss: 0.6653 score: 0.9147 time: 0.16s
Epoch 63/1000, LR 0.000283
Train loss: 1.0432;  Loss pred: 1.0432; Loss self: 0.0000; time: 0.26s
Val loss: 0.6672 score: 0.8760 time: 0.17s
Test loss: 0.6635 score: 0.9070 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 1.0348;  Loss pred: 1.0348; Loss self: 0.0000; time: 0.25s
Val loss: 0.6655 score: 0.8760 time: 0.16s
Test loss: 0.6616 score: 0.9070 time: 0.16s
Epoch 65/1000, LR 0.000283
Train loss: 1.0319;  Loss pred: 1.0319; Loss self: 0.0000; time: 0.25s
Val loss: 0.6638 score: 0.8760 time: 0.17s
Test loss: 0.6597 score: 0.9070 time: 0.16s
Epoch 66/1000, LR 0.000283
Train loss: 1.0270;  Loss pred: 1.0270; Loss self: 0.0000; time: 0.28s
Val loss: 0.6620 score: 0.8760 time: 0.17s
Test loss: 0.6576 score: 0.9070 time: 0.17s
Epoch 67/1000, LR 0.000283
Train loss: 1.0222;  Loss pred: 1.0222; Loss self: 0.0000; time: 0.35s
Val loss: 0.6601 score: 0.8760 time: 0.17s
Test loss: 0.6554 score: 0.9070 time: 0.17s
Epoch 68/1000, LR 0.000283
Train loss: 1.0161;  Loss pred: 1.0161; Loss self: 0.0000; time: 0.26s
Val loss: 0.6581 score: 0.8760 time: 0.16s
Test loss: 0.6531 score: 0.9070 time: 0.16s
Epoch 69/1000, LR 0.000283
Train loss: 1.0146;  Loss pred: 1.0146; Loss self: 0.0000; time: 0.25s
Val loss: 0.6560 score: 0.8760 time: 0.17s
Test loss: 0.6507 score: 0.9070 time: 0.15s
Epoch 70/1000, LR 0.000283
Train loss: 1.0105;  Loss pred: 1.0105; Loss self: 0.0000; time: 0.25s
Val loss: 0.6538 score: 0.8760 time: 0.16s
Test loss: 0.6482 score: 0.9070 time: 0.15s
Epoch 71/1000, LR 0.000282
Train loss: 1.0055;  Loss pred: 1.0055; Loss self: 0.0000; time: 0.27s
Val loss: 0.6515 score: 0.8760 time: 0.22s
Test loss: 0.6455 score: 0.9147 time: 0.23s
Epoch 72/1000, LR 0.000282
Train loss: 0.9999;  Loss pred: 0.9999; Loss self: 0.0000; time: 0.36s
Val loss: 0.6491 score: 0.8760 time: 0.20s
Test loss: 0.6428 score: 0.9147 time: 0.15s
Epoch 73/1000, LR 0.000282
Train loss: 0.9969;  Loss pred: 0.9969; Loss self: 0.0000; time: 0.25s
Val loss: 0.6465 score: 0.8760 time: 0.17s
Test loss: 0.6399 score: 0.9147 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 0.9920;  Loss pred: 0.9920; Loss self: 0.0000; time: 0.25s
Val loss: 0.6439 score: 0.8760 time: 0.17s
Test loss: 0.6369 score: 0.9147 time: 0.16s
Epoch 75/1000, LR 0.000282
Train loss: 0.9853;  Loss pred: 0.9853; Loss self: 0.0000; time: 0.25s
Val loss: 0.6411 score: 0.8760 time: 0.17s
Test loss: 0.6337 score: 0.9147 time: 0.16s
Epoch 76/1000, LR 0.000282
Train loss: 0.9840;  Loss pred: 0.9840; Loss self: 0.0000; time: 0.25s
Val loss: 0.6383 score: 0.8837 time: 0.24s
Test loss: 0.6304 score: 0.9147 time: 0.15s
Epoch 77/1000, LR 0.000282
Train loss: 0.9804;  Loss pred: 0.9804; Loss self: 0.0000; time: 0.26s
Val loss: 0.6354 score: 0.8760 time: 0.17s
Test loss: 0.6270 score: 0.9302 time: 0.16s
Epoch 78/1000, LR 0.000282
Train loss: 0.9758;  Loss pred: 0.9758; Loss self: 0.0000; time: 0.28s
Val loss: 0.6323 score: 0.8760 time: 0.20s
Test loss: 0.6234 score: 0.9302 time: 0.19s
Epoch 79/1000, LR 0.000282
Train loss: 0.9726;  Loss pred: 0.9726; Loss self: 0.0000; time: 0.34s
Val loss: 0.6290 score: 0.8760 time: 0.16s
Test loss: 0.6198 score: 0.9147 time: 0.16s
Epoch 80/1000, LR 0.000282
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 0.26s
Val loss: 0.6257 score: 0.8760 time: 0.16s
Test loss: 0.6160 score: 0.9147 time: 0.17s
Epoch 81/1000, LR 0.000281
Train loss: 0.9645;  Loss pred: 0.9645; Loss self: 0.0000; time: 0.26s
Val loss: 0.6222 score: 0.8760 time: 0.26s
Test loss: 0.6120 score: 0.9147 time: 0.22s
Epoch 82/1000, LR 0.000281
Train loss: 0.9601;  Loss pred: 0.9601; Loss self: 0.0000; time: 0.36s
Val loss: 0.6186 score: 0.8760 time: 0.25s
Test loss: 0.6079 score: 0.9225 time: 0.24s
Epoch 83/1000, LR 0.000281
Train loss: 0.9550;  Loss pred: 0.9550; Loss self: 0.0000; time: 0.26s
Val loss: 0.6149 score: 0.8760 time: 0.17s
Test loss: 0.6037 score: 0.9225 time: 0.25s
Epoch 84/1000, LR 0.000281
Train loss: 0.9522;  Loss pred: 0.9522; Loss self: 0.0000; time: 0.26s
Val loss: 0.6110 score: 0.8760 time: 0.17s
Test loss: 0.5992 score: 0.9225 time: 0.16s
Epoch 85/1000, LR 0.000281
Train loss: 0.9474;  Loss pred: 0.9474; Loss self: 0.0000; time: 0.24s
Val loss: 0.6071 score: 0.8760 time: 0.17s
Test loss: 0.5947 score: 0.9225 time: 0.16s
Epoch 86/1000, LR 0.000281
Train loss: 0.9441;  Loss pred: 0.9441; Loss self: 0.0000; time: 0.26s
Val loss: 0.6031 score: 0.8760 time: 0.18s
Test loss: 0.5900 score: 0.9302 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.9402;  Loss pred: 0.9402; Loss self: 0.0000; time: 0.33s
Val loss: 0.5988 score: 0.8760 time: 0.19s
Test loss: 0.5852 score: 0.9302 time: 0.17s
Epoch 88/1000, LR 0.000281
Train loss: 0.9356;  Loss pred: 0.9356; Loss self: 0.0000; time: 0.26s
Val loss: 0.5945 score: 0.8760 time: 0.17s
Test loss: 0.5802 score: 0.9225 time: 0.17s
Epoch 89/1000, LR 0.000281
Train loss: 0.9311;  Loss pred: 0.9311; Loss self: 0.0000; time: 0.25s
Val loss: 0.5900 score: 0.8760 time: 0.18s
Test loss: 0.5751 score: 0.9225 time: 0.16s
Epoch 90/1000, LR 0.000281
Train loss: 0.9270;  Loss pred: 0.9270; Loss self: 0.0000; time: 0.26s
Val loss: 0.5854 score: 0.8760 time: 0.22s
Test loss: 0.5699 score: 0.9225 time: 0.23s
Epoch 91/1000, LR 0.000280
Train loss: 0.9234;  Loss pred: 0.9234; Loss self: 0.0000; time: 0.36s
Val loss: 0.5807 score: 0.8760 time: 0.24s
Test loss: 0.5645 score: 0.9225 time: 0.23s
Epoch 92/1000, LR 0.000280
Train loss: 0.9181;  Loss pred: 0.9181; Loss self: 0.0000; time: 0.36s
Val loss: 0.5758 score: 0.8760 time: 0.24s
Test loss: 0.5589 score: 0.9302 time: 0.23s
Epoch 93/1000, LR 0.000280
Train loss: 0.9146;  Loss pred: 0.9146; Loss self: 0.0000; time: 0.37s
Val loss: 0.5708 score: 0.8760 time: 0.21s
Test loss: 0.5531 score: 0.9302 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.9103;  Loss pred: 0.9103; Loss self: 0.0000; time: 0.27s
Val loss: 0.5658 score: 0.8837 time: 0.18s
Test loss: 0.5473 score: 0.9302 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.9038;  Loss pred: 0.9038; Loss self: 0.0000; time: 0.26s
Val loss: 0.5606 score: 0.8837 time: 0.16s
Test loss: 0.5413 score: 0.9302 time: 0.15s
Epoch 96/1000, LR 0.000280
Train loss: 0.8996;  Loss pred: 0.8996; Loss self: 0.0000; time: 0.25s
Val loss: 0.5553 score: 0.8837 time: 0.17s
Test loss: 0.5352 score: 0.9302 time: 0.16s
Epoch 97/1000, LR 0.000280
Train loss: 0.8945;  Loss pred: 0.8945; Loss self: 0.0000; time: 0.32s
Val loss: 0.5499 score: 0.8837 time: 0.17s
Test loss: 0.5290 score: 0.9302 time: 0.16s
Epoch 98/1000, LR 0.000280
Train loss: 0.8916;  Loss pred: 0.8916; Loss self: 0.0000; time: 0.25s
Val loss: 0.5444 score: 0.8837 time: 0.15s
Test loss: 0.5227 score: 0.9302 time: 0.15s
Epoch 99/1000, LR 0.000279
Train loss: 0.8861;  Loss pred: 0.8861; Loss self: 0.0000; time: 0.25s
Val loss: 0.5388 score: 0.8837 time: 0.16s
Test loss: 0.5163 score: 0.9302 time: 0.15s
Epoch 100/1000, LR 0.000279
Train loss: 0.8813;  Loss pred: 0.8813; Loss self: 0.0000; time: 0.28s
Val loss: 0.5332 score: 0.8837 time: 0.17s
Test loss: 0.5097 score: 0.9302 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.8766;  Loss pred: 0.8766; Loss self: 0.0000; time: 0.25s
Val loss: 0.5273 score: 0.8837 time: 0.18s
Test loss: 0.5031 score: 0.9302 time: 0.15s
Epoch 102/1000, LR 0.000279
Train loss: 0.8717;  Loss pred: 0.8717; Loss self: 0.0000; time: 0.34s
Val loss: 0.5216 score: 0.8837 time: 0.18s
Test loss: 0.4964 score: 0.9380 time: 0.15s
Epoch 103/1000, LR 0.000279
Train loss: 0.8673;  Loss pred: 0.8673; Loss self: 0.0000; time: 0.25s
Val loss: 0.5158 score: 0.8837 time: 0.15s
Test loss: 0.4897 score: 0.9380 time: 0.15s
Epoch 104/1000, LR 0.000279
Train loss: 0.8622;  Loss pred: 0.8622; Loss self: 0.0000; time: 0.24s
Val loss: 0.5099 score: 0.8837 time: 0.16s
Test loss: 0.4829 score: 0.9380 time: 0.15s
Epoch 105/1000, LR 0.000279
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 0.24s
Val loss: 0.5040 score: 0.8837 time: 0.16s
Test loss: 0.4762 score: 0.9380 time: 0.15s
Epoch 106/1000, LR 0.000279
Train loss: 0.8504;  Loss pred: 0.8504; Loss self: 0.0000; time: 0.24s
Val loss: 0.4980 score: 0.8837 time: 0.17s
Test loss: 0.4694 score: 0.9380 time: 0.15s
Epoch 107/1000, LR 0.000278
Train loss: 0.8477;  Loss pred: 0.8477; Loss self: 0.0000; time: 0.35s
Val loss: 0.4921 score: 0.8837 time: 0.25s
Test loss: 0.4625 score: 0.9380 time: 0.24s
Epoch 108/1000, LR 0.000278
Train loss: 0.8426;  Loss pred: 0.8426; Loss self: 0.0000; time: 0.36s
Val loss: 0.4860 score: 0.8837 time: 0.23s
Test loss: 0.4557 score: 0.9302 time: 0.16s
Epoch 109/1000, LR 0.000278
Train loss: 0.8368;  Loss pred: 0.8368; Loss self: 0.0000; time: 0.25s
Val loss: 0.4799 score: 0.8837 time: 0.17s
Test loss: 0.4488 score: 0.9302 time: 0.19s
Epoch 110/1000, LR 0.000278
Train loss: 0.8334;  Loss pred: 0.8334; Loss self: 0.0000; time: 0.27s
Val loss: 0.4739 score: 0.8837 time: 0.17s
Test loss: 0.4418 score: 0.9457 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.8286;  Loss pred: 0.8286; Loss self: 0.0000; time: 0.25s
Val loss: 0.4677 score: 0.8837 time: 0.16s
Test loss: 0.4349 score: 0.9457 time: 0.15s
Epoch 112/1000, LR 0.000278
Train loss: 0.8225;  Loss pred: 0.8225; Loss self: 0.0000; time: 0.26s
Val loss: 0.4617 score: 0.8837 time: 0.16s
Test loss: 0.4280 score: 0.9457 time: 0.16s
Epoch 113/1000, LR 0.000278
Train loss: 0.8168;  Loss pred: 0.8168; Loss self: 0.0000; time: 0.26s
Val loss: 0.4557 score: 0.8837 time: 0.23s
Test loss: 0.4210 score: 0.9457 time: 0.24s
Epoch 114/1000, LR 0.000277
Train loss: 0.8120;  Loss pred: 0.8120; Loss self: 0.0000; time: 0.26s
Val loss: 0.4498 score: 0.8837 time: 0.16s
Test loss: 0.4142 score: 0.9380 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.8068;  Loss pred: 0.8068; Loss self: 0.0000; time: 0.25s
Val loss: 0.4439 score: 0.8837 time: 0.16s
Test loss: 0.4075 score: 0.9380 time: 0.15s
Epoch 116/1000, LR 0.000277
Train loss: 0.8040;  Loss pred: 0.8040; Loss self: 0.0000; time: 0.25s
Val loss: 0.4381 score: 0.8837 time: 0.16s
Test loss: 0.4009 score: 0.9380 time: 0.15s
Epoch 117/1000, LR 0.000277
Train loss: 0.7976;  Loss pred: 0.7976; Loss self: 0.0000; time: 0.26s
Val loss: 0.4324 score: 0.8837 time: 0.16s
Test loss: 0.3943 score: 0.9380 time: 0.15s
Epoch 118/1000, LR 0.000277
Train loss: 0.7907;  Loss pred: 0.7907; Loss self: 0.0000; time: 0.25s
Val loss: 0.4266 score: 0.8837 time: 0.15s
Test loss: 0.3879 score: 0.9380 time: 0.15s
Epoch 119/1000, LR 0.000277
Train loss: 0.7890;  Loss pred: 0.7890; Loss self: 0.0000; time: 0.25s
Val loss: 0.4210 score: 0.8837 time: 0.16s
Test loss: 0.3815 score: 0.9380 time: 0.15s
Epoch 120/1000, LR 0.000277
Train loss: 0.7826;  Loss pred: 0.7826; Loss self: 0.0000; time: 0.33s
Val loss: 0.4155 score: 0.8837 time: 0.16s
Test loss: 0.3753 score: 0.9380 time: 0.15s
Epoch 121/1000, LR 0.000276
Train loss: 0.7783;  Loss pred: 0.7783; Loss self: 0.0000; time: 0.26s
Val loss: 0.4100 score: 0.8837 time: 0.16s
Test loss: 0.3693 score: 0.9457 time: 0.15s
Epoch 122/1000, LR 0.000276
Train loss: 0.7743;  Loss pred: 0.7743; Loss self: 0.0000; time: 0.25s
Val loss: 0.4046 score: 0.8837 time: 0.16s
Test loss: 0.3632 score: 0.9380 time: 0.15s
Epoch 123/1000, LR 0.000276
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 0.25s
Val loss: 0.3993 score: 0.8837 time: 0.15s
Test loss: 0.3572 score: 0.9380 time: 0.16s
Epoch 124/1000, LR 0.000276
Train loss: 0.7635;  Loss pred: 0.7635; Loss self: 0.0000; time: 0.25s
Val loss: 0.3941 score: 0.8915 time: 0.15s
Test loss: 0.3513 score: 0.9380 time: 0.15s
Epoch 125/1000, LR 0.000276
Train loss: 0.7625;  Loss pred: 0.7625; Loss self: 0.0000; time: 0.26s
Val loss: 0.3890 score: 0.8915 time: 0.16s
Test loss: 0.3453 score: 0.9380 time: 0.19s
Epoch 126/1000, LR 0.000276
Train loss: 0.7573;  Loss pred: 0.7573; Loss self: 0.0000; time: 0.26s
Val loss: 0.3840 score: 0.8915 time: 0.25s
Test loss: 0.3398 score: 0.9380 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.7514;  Loss pred: 0.7514; Loss self: 0.0000; time: 0.25s
Val loss: 0.3792 score: 0.8915 time: 0.15s
Test loss: 0.3346 score: 0.9380 time: 0.16s
Epoch 128/1000, LR 0.000275
Train loss: 0.7482;  Loss pred: 0.7482; Loss self: 0.0000; time: 0.25s
Val loss: 0.3744 score: 0.8915 time: 0.16s
Test loss: 0.3293 score: 0.9380 time: 0.15s
Epoch 129/1000, LR 0.000275
Train loss: 0.7440;  Loss pred: 0.7440; Loss self: 0.0000; time: 0.25s
Val loss: 0.3697 score: 0.8915 time: 0.16s
Test loss: 0.3240 score: 0.9380 time: 0.15s
Epoch 130/1000, LR 0.000275
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.25s
Val loss: 0.3651 score: 0.8915 time: 0.16s
Test loss: 0.3188 score: 0.9380 time: 0.16s
Epoch 131/1000, LR 0.000275
Train loss: 0.7351;  Loss pred: 0.7351; Loss self: 0.0000; time: 0.25s
Val loss: 0.3606 score: 0.8992 time: 0.15s
Test loss: 0.3138 score: 0.9380 time: 0.15s
Epoch 132/1000, LR 0.000275
Train loss: 0.7324;  Loss pred: 0.7324; Loss self: 0.0000; time: 0.30s
Val loss: 0.3562 score: 0.9070 time: 0.16s
Test loss: 0.3089 score: 0.9380 time: 0.22s
Epoch 133/1000, LR 0.000274
Train loss: 0.7286;  Loss pred: 0.7286; Loss self: 0.0000; time: 0.37s
Val loss: 0.3519 score: 0.9070 time: 0.24s
Test loss: 0.3042 score: 0.9380 time: 0.23s
Epoch 134/1000, LR 0.000274
Train loss: 0.7239;  Loss pred: 0.7239; Loss self: 0.0000; time: 0.37s
Val loss: 0.3479 score: 0.9070 time: 0.21s
Test loss: 0.2999 score: 0.9380 time: 0.17s
Epoch 135/1000, LR 0.000274
Train loss: 0.7204;  Loss pred: 0.7204; Loss self: 0.0000; time: 0.25s
Val loss: 0.3441 score: 0.8915 time: 0.15s
Test loss: 0.2958 score: 0.9380 time: 0.15s
Epoch 136/1000, LR 0.000274
Train loss: 0.7156;  Loss pred: 0.7156; Loss self: 0.0000; time: 0.26s
Val loss: 0.3403 score: 0.8915 time: 0.16s
Test loss: 0.2917 score: 0.9380 time: 0.15s
Epoch 137/1000, LR 0.000274
Train loss: 0.7122;  Loss pred: 0.7122; Loss self: 0.0000; time: 0.25s
Val loss: 0.3365 score: 0.8915 time: 0.16s
Test loss: 0.2876 score: 0.9380 time: 0.15s
Epoch 138/1000, LR 0.000274
Train loss: 0.7088;  Loss pred: 0.7088; Loss self: 0.0000; time: 0.25s
Val loss: 0.3326 score: 0.9070 time: 0.17s
Test loss: 0.2833 score: 0.9380 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.7052;  Loss pred: 0.7052; Loss self: 0.0000; time: 0.28s
Val loss: 0.3288 score: 0.9070 time: 0.24s
Test loss: 0.2792 score: 0.9380 time: 0.24s
Epoch 140/1000, LR 0.000273
Train loss: 0.7002;  Loss pred: 0.7002; Loss self: 0.0000; time: 0.37s
Val loss: 0.3252 score: 0.9070 time: 0.26s
Test loss: 0.2753 score: 0.9380 time: 0.18s
Epoch 141/1000, LR 0.000273
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.27s
Val loss: 0.3217 score: 0.9070 time: 0.17s
Test loss: 0.2716 score: 0.9380 time: 0.17s
Epoch 142/1000, LR 0.000273
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.26s
Val loss: 0.3184 score: 0.9070 time: 0.17s
Test loss: 0.2680 score: 0.9380 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.27s
Val loss: 0.3146 score: 0.9070 time: 0.17s
Test loss: 0.2640 score: 0.9380 time: 0.16s
Epoch 144/1000, LR 0.000272
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.29s
Val loss: 0.3113 score: 0.9070 time: 0.26s
Test loss: 0.2605 score: 0.9380 time: 0.16s
Epoch 145/1000, LR 0.000272
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.26s
Val loss: 0.3084 score: 0.9070 time: 0.18s
Test loss: 0.2574 score: 0.9380 time: 0.17s
Epoch 146/1000, LR 0.000272
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.26s
Val loss: 0.3062 score: 0.9070 time: 0.17s
Test loss: 0.2549 score: 0.9380 time: 0.15s
Epoch 147/1000, LR 0.000272
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.24s
Val loss: 0.3043 score: 0.9070 time: 0.17s
Test loss: 0.2528 score: 0.9380 time: 0.16s
Epoch 148/1000, LR 0.000272
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.35s
Val loss: 0.3017 score: 0.9070 time: 0.24s
Test loss: 0.2499 score: 0.9380 time: 0.23s
Epoch 149/1000, LR 0.000272
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.33s
Val loss: 0.2990 score: 0.9070 time: 0.22s
Test loss: 0.2471 score: 0.9380 time: 0.16s
Epoch 150/1000, LR 0.000271
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.25s
Val loss: 0.2952 score: 0.9070 time: 0.16s
Test loss: 0.2433 score: 0.9380 time: 0.17s
Epoch 151/1000, LR 0.000271
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.25s
Val loss: 0.2919 score: 0.9070 time: 0.17s
Test loss: 0.2401 score: 0.9380 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.26s
Val loss: 0.2892 score: 0.9070 time: 0.17s
Test loss: 0.2373 score: 0.9380 time: 0.16s
Epoch 153/1000, LR 0.000271
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.27s
Val loss: 0.2872 score: 0.9070 time: 0.16s
Test loss: 0.2351 score: 0.9380 time: 0.16s
Epoch 154/1000, LR 0.000271
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.26s
Val loss: 0.2856 score: 0.9070 time: 0.17s
Test loss: 0.2332 score: 0.9380 time: 0.16s
Epoch 155/1000, LR 0.000270
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.30s
Val loss: 0.2835 score: 0.9147 time: 0.17s
Test loss: 0.2310 score: 0.9380 time: 0.24s
Epoch 156/1000, LR 0.000270
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.26s
Val loss: 0.2812 score: 0.9147 time: 0.17s
Test loss: 0.2286 score: 0.9380 time: 0.16s
Epoch 157/1000, LR 0.000270
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.26s
Val loss: 0.2790 score: 0.9147 time: 0.16s
Test loss: 0.2263 score: 0.9380 time: 0.16s
Epoch 158/1000, LR 0.000270
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.26s
Val loss: 0.2762 score: 0.9147 time: 0.17s
Test loss: 0.2236 score: 0.9380 time: 0.17s
Epoch 159/1000, LR 0.000270
Train loss: 0.6466;  Loss pred: 0.6466; Loss self: 0.0000; time: 0.32s
Val loss: 0.2735 score: 0.9147 time: 0.25s
Test loss: 0.2211 score: 0.9380 time: 0.24s
Epoch 160/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.28s
Val loss: 0.2714 score: 0.9147 time: 0.17s
Test loss: 0.2190 score: 0.9380 time: 0.16s
Epoch 161/1000, LR 0.000269
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.26s
Val loss: 0.2702 score: 0.9147 time: 0.16s
Test loss: 0.2175 score: 0.9380 time: 0.15s
Epoch 162/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.25s
Val loss: 0.2690 score: 0.9147 time: 0.16s
Test loss: 0.2161 score: 0.9380 time: 0.16s
Epoch 163/1000, LR 0.000269
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.25s
Val loss: 0.2675 score: 0.9147 time: 0.15s
Test loss: 0.2145 score: 0.9380 time: 0.16s
Epoch 164/1000, LR 0.000269
Train loss: 0.6363;  Loss pred: 0.6363; Loss self: 0.0000; time: 0.25s
Val loss: 0.2667 score: 0.9147 time: 0.17s
Test loss: 0.2134 score: 0.9380 time: 0.16s
Epoch 165/1000, LR 0.000268
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.27s
Val loss: 0.2656 score: 0.9147 time: 0.15s
Test loss: 0.2120 score: 0.9380 time: 0.22s
Epoch 166/1000, LR 0.000268
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.26s
Val loss: 0.2634 score: 0.9147 time: 0.16s
Test loss: 0.2100 score: 0.9380 time: 0.15s
Epoch 167/1000, LR 0.000268
Train loss: 0.6304;  Loss pred: 0.6304; Loss self: 0.0000; time: 0.26s
Val loss: 0.2613 score: 0.9147 time: 0.17s
Test loss: 0.2080 score: 0.9380 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.27s
Val loss: 0.2602 score: 0.9147 time: 0.16s
Test loss: 0.2067 score: 0.9380 time: 0.14s
Epoch 169/1000, LR 0.000267
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.26s
Val loss: 0.2578 score: 0.9147 time: 0.16s
Test loss: 0.2046 score: 0.9380 time: 0.15s
Epoch 170/1000, LR 0.000267
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.25s
Val loss: 0.2566 score: 0.9147 time: 0.16s
Test loss: 0.2033 score: 0.9380 time: 0.15s
Epoch 171/1000, LR 0.000267
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.31s
Val loss: 0.2554 score: 0.9147 time: 0.17s
Test loss: 0.2020 score: 0.9380 time: 0.16s
Epoch 172/1000, LR 0.000267
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.25s
Val loss: 0.2547 score: 0.9147 time: 0.19s
Test loss: 0.2010 score: 0.9380 time: 0.16s
Epoch 173/1000, LR 0.000267
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.29s
Val loss: 0.2543 score: 0.9147 time: 0.30s
Test loss: 0.2003 score: 0.9380 time: 0.15s
Epoch 174/1000, LR 0.000266
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.25s
Val loss: 0.2539 score: 0.9147 time: 0.16s
Test loss: 0.1996 score: 0.9380 time: 0.14s
Epoch 175/1000, LR 0.000266
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.24s
Val loss: 0.2520 score: 0.9147 time: 0.17s
Test loss: 0.1980 score: 0.9380 time: 0.16s
Epoch 176/1000, LR 0.000266
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.25s
Val loss: 0.2499 score: 0.9147 time: 0.21s
Test loss: 0.1962 score: 0.9380 time: 0.20s
Epoch 177/1000, LR 0.000266
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.24s
Val loss: 0.2494 score: 0.9147 time: 0.16s
Test loss: 0.1954 score: 0.9380 time: 0.15s
Epoch 178/1000, LR 0.000265
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 0.24s
Val loss: 0.2481 score: 0.9147 time: 0.15s
Test loss: 0.1942 score: 0.9380 time: 0.16s
Epoch 179/1000, LR 0.000265
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.25s
Val loss: 0.2467 score: 0.9147 time: 0.22s
Test loss: 0.1929 score: 0.9380 time: 0.27s
Epoch 180/1000, LR 0.000265
Train loss: 0.6086;  Loss pred: 0.6086; Loss self: 0.0000; time: 0.34s
Val loss: 0.2458 score: 0.9147 time: 0.19s
Test loss: 0.1919 score: 0.9380 time: 0.15s
Epoch 181/1000, LR 0.000265
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.25s
Val loss: 0.2450 score: 0.9147 time: 0.16s
Test loss: 0.1911 score: 0.9380 time: 0.21s
Epoch 182/1000, LR 0.000265
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.36s
Val loss: 0.2440 score: 0.9147 time: 0.24s
Test loss: 0.1901 score: 0.9380 time: 0.24s
Epoch 183/1000, LR 0.000264
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.37s
Val loss: 0.2418 score: 0.9147 time: 0.23s
Test loss: 0.1884 score: 0.9380 time: 0.23s
Epoch 184/1000, LR 0.000264
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.33s
Val loss: 0.2413 score: 0.9147 time: 0.17s
Test loss: 0.1877 score: 0.9380 time: 0.15s
Epoch 185/1000, LR 0.000264
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.25s
Val loss: 0.2410 score: 0.9147 time: 0.16s
Test loss: 0.1872 score: 0.9380 time: 0.15s
Epoch 186/1000, LR 0.000264
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.25s
Val loss: 0.2414 score: 0.9147 time: 0.15s
Test loss: 0.1872 score: 0.9380 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.25s
Val loss: 0.2412 score: 0.9147 time: 0.16s
Test loss: 0.1868 score: 0.9380 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.25s
Val loss: 0.2419 score: 0.9147 time: 0.16s
Test loss: 0.1870 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.25s
Val loss: 0.2418 score: 0.9147 time: 0.15s
Test loss: 0.1867 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.26s
Val loss: 0.2414 score: 0.9147 time: 0.24s
Test loss: 0.1862 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 191/1000, LR 0.000262
Train loss: 0.5946;  Loss pred: 0.5946; Loss self: 0.0000; time: 0.24s
Val loss: 0.2405 score: 0.9147 time: 0.16s
Test loss: 0.1853 score: 0.9302 time: 0.16s
Epoch 192/1000, LR 0.000262
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.25s
Val loss: 0.2394 score: 0.9147 time: 0.16s
Test loss: 0.1844 score: 0.9302 time: 0.15s
Epoch 193/1000, LR 0.000262
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 0.25s
Val loss: 0.2375 score: 0.9147 time: 0.16s
Test loss: 0.1830 score: 0.9380 time: 0.15s
Epoch 194/1000, LR 0.000262
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 0.25s
Val loss: 0.2364 score: 0.9147 time: 0.16s
Test loss: 0.1820 score: 0.9380 time: 0.16s
Epoch 195/1000, LR 0.000261
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.25s
Val loss: 0.2351 score: 0.9147 time: 0.16s
Test loss: 0.1810 score: 0.9380 time: 0.17s
Epoch 196/1000, LR 0.000261
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.26s
Val loss: 0.2350 score: 0.9147 time: 0.21s
Test loss: 0.1807 score: 0.9380 time: 0.21s
Epoch 197/1000, LR 0.000261
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.27s
Val loss: 0.2348 score: 0.9147 time: 0.17s
Test loss: 0.1804 score: 0.9380 time: 0.16s
Epoch 198/1000, LR 0.000261
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.25s
Val loss: 0.2360 score: 0.9070 time: 0.16s
Test loss: 0.1809 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 0.25s
Val loss: 0.2355 score: 0.9070 time: 0.16s
Test loss: 0.1805 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.24s
Val loss: 0.2366 score: 0.9070 time: 0.18s
Test loss: 0.1810 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.26s
Val loss: 0.2371 score: 0.9070 time: 0.17s
Test loss: 0.1812 score: 0.9302 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.33s
Val loss: 0.2357 score: 0.9070 time: 0.16s
Test loss: 0.1801 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.25s
Val loss: 0.2339 score: 0.9070 time: 0.17s
Test loss: 0.1787 score: 0.9302 time: 0.15s
Epoch 204/1000, LR 0.000259
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.25s
Val loss: 0.2338 score: 0.9070 time: 0.17s
Test loss: 0.1785 score: 0.9302 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.26s
Val loss: 0.2329 score: 0.9070 time: 0.23s
Test loss: 0.1777 score: 0.9380 time: 0.18s
Epoch 206/1000, LR 0.000259
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.26s
Val loss: 0.2327 score: 0.9070 time: 0.17s
Test loss: 0.1775 score: 0.9380 time: 0.17s
Epoch 207/1000, LR 0.000258
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.26s
Val loss: 0.2316 score: 0.9070 time: 0.17s
Test loss: 0.1766 score: 0.9380 time: 0.16s
Epoch 208/1000, LR 0.000258
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.26s
Val loss: 0.2302 score: 0.9070 time: 0.17s
Test loss: 0.1756 score: 0.9380 time: 0.16s
Epoch 209/1000, LR 0.000258
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.26s
Val loss: 0.2309 score: 0.9070 time: 0.16s
Test loss: 0.1758 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.26s
Val loss: 0.2315 score: 0.9070 time: 0.16s
Test loss: 0.1761 score: 0.9380 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.25s
Val loss: 0.2325 score: 0.9070 time: 0.21s
Test loss: 0.1766 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 212/1000, LR 0.000257
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.25s
Val loss: 0.2318 score: 0.9070 time: 0.16s
Test loss: 0.1760 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.25s
Val loss: 0.2320 score: 0.9070 time: 0.15s
Test loss: 0.1761 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.25s
Val loss: 0.2340 score: 0.9070 time: 0.16s
Test loss: 0.1773 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.26s
Val loss: 0.2328 score: 0.9070 time: 0.16s
Test loss: 0.1764 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.25s
Val loss: 0.2318 score: 0.9070 time: 0.16s
Test loss: 0.1756 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.33s
Val loss: 0.2311 score: 0.9070 time: 0.16s
Test loss: 0.1751 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.25s
Val loss: 0.2311 score: 0.9070 time: 0.16s
Test loss: 0.1749 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.25s
Val loss: 0.2317 score: 0.9070 time: 0.17s
Test loss: 0.1752 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.26s
Val loss: 0.2321 score: 0.9070 time: 0.17s
Test loss: 0.1754 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.25s
Val loss: 0.2312 score: 0.9070 time: 0.16s
Test loss: 0.1748 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 0.25s
Val loss: 0.2320 score: 0.9070 time: 0.19s
Test loss: 0.1752 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.33s
Val loss: 0.2309 score: 0.9070 time: 0.17s
Test loss: 0.1744 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.25s
Val loss: 0.2323 score: 0.9070 time: 0.16s
Test loss: 0.1753 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.27s
Val loss: 0.2336 score: 0.9070 time: 0.17s
Test loss: 0.1760 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.26s
Val loss: 0.2333 score: 0.9070 time: 0.16s
Test loss: 0.1758 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.25s
Val loss: 0.2332 score: 0.9070 time: 0.16s
Test loss: 0.1756 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.26s
Val loss: 0.2309 score: 0.9070 time: 0.22s
Test loss: 0.1741 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 207,   Train_Loss: 0.5737,   Val_Loss: 0.2302,   Val_Precision: 0.9643,   Val_Recall: 0.8438,   Val_accuracy: 0.9000,   Val_Score: 0.9070,   Val_Loss: 0.2302,   Test_Precision: 0.9831,   Test_Recall: 0.8923,   Test_accuracy: 0.9355,   Test_Score: 0.9380,   Test_loss: 0.1756


[0.1633429799694568, 0.15614532097242773, 0.1577311409637332, 0.16770406300202012, 0.1706391649786383, 0.2058770600706339, 0.1551113270688802, 0.15528662502765656, 0.15809794398956, 0.16335229808464646, 0.18260656902566552, 0.20692549110390246, 0.16007242305204272, 0.16057039005681872, 0.1674099329393357, 0.1624250439926982, 0.2282611159607768, 0.16419412195682526, 0.1558265050407499, 0.19642332289367914, 0.15770386112853885, 0.16106402687728405, 0.16513039381243289, 0.16570767085067928, 0.16767592704854906, 0.16408061399124563, 0.16593310702592134, 0.1689987180288881, 0.15415916312485933, 0.17116401996463537, 0.1551757080014795, 0.17686095391400158, 0.171754535054788, 0.30662781605497, 0.16846977709792554, 0.1585955109912902, 0.15225314302369952, 0.16379375103861094, 0.16274610487744212, 0.24087975407019258, 0.23694333783350885, 0.23075872799381614, 0.15775030595250428, 0.1620732459705323, 0.15576656605117023, 0.1621288638561964, 0.17110020900145173, 0.19222328020259738, 0.1675183018669486, 0.1572486008517444, 0.15627577109262347, 0.15468346211127937, 0.16454230016097426, 0.23612723499536514, 0.1612419649027288, 0.1564933410845697, 0.1595668150112033, 0.16275033494457603, 0.2283024019561708, 0.19984278501942754, 0.156058233929798, 0.16595723014324903, 0.1667684509884566, 0.16555132693611085, 0.16498665907420218, 0.17004937096498907, 0.17139486013911664, 0.16237166593782604, 0.15784801985137165, 0.15649966103956103, 0.2367911790497601, 0.1581813469529152, 0.1695431659463793, 0.16750274715013802, 0.16631621797569096, 0.15174328908324242, 0.16884535807184875, 0.19771736091934144, 0.15984011115506291, 0.1723744790069759, 0.22835985105484724, 0.24451613589189947, 0.2541562281548977, 0.16565385088324547, 0.16720917588099837, 0.173576092813164, 0.17278577806428075, 0.1702698660083115, 0.16691739298403263, 0.2391029680147767, 0.23687996296212077, 0.2324831911828369, 0.17574805696494877, 0.1748691969551146, 0.15655949106439948, 0.1690518530085683, 0.16250726906582713, 0.15296106692403555, 0.15526171284727752, 0.1659536729566753, 0.15777224698103964, 0.15068600699305534, 0.1502524889074266, 0.15159938903525472, 0.15497747994959354, 0.15897291991859674, 0.24933884805068374, 0.16060163197107613, 0.20753517397679389, 0.17111018812283874, 0.15180298499763012, 0.16861450299620628, 0.2403546581044793, 0.1620779091026634, 0.15737599600106478, 0.1520959788467735, 0.15623934799805284, 0.15448340703733265, 0.15666243806481361, 0.15266723604872823, 0.15643692901358008, 0.15684970002621412, 0.1612849470693618, 0.15610216511413455, 0.199371401919052, 0.16255246894434094, 0.16200057906098664, 0.14966107881627977, 0.15589978196658194, 0.16435190103948116, 0.1519422708079219, 0.22921425895765424, 0.23883025092072785, 0.1704685091972351, 0.15012488001957536, 0.15671753697097301, 0.15873603709042072, 0.1720971700269729, 0.2498951309826225, 0.18023126618936658, 0.17148291994817555, 0.17466909787617624, 0.16479278588667512, 0.1678468999452889, 0.17820204584859312, 0.159265770111233, 0.16144538996741176, 0.23045858088880777, 0.16453967499546707, 0.16962200799025595, 0.17786167794838548, 0.1608762249816209, 0.1637010988779366, 0.1662751140538603, 0.24465269688516855, 0.16412942809984088, 0.16714137489907444, 0.1724553050007671, 0.2492264430038631, 0.16196872107684612, 0.15750985499471426, 0.16604331717826426, 0.16320523107424378, 0.16299082105979323, 0.2276297581847757, 0.15736022219061852, 0.16257912782020867, 0.14675552397966385, 0.15234986413270235, 0.15615019598044455, 0.16051411000080407, 0.16799810994416475, 0.15375057398341596, 0.1485333158634603, 0.16589784203097224, 0.2077079250011593, 0.15844973200000823, 0.1674482999369502, 0.2731495089828968, 0.15703727584332228, 0.2174908600281924, 0.24107305309735239, 0.2365095519926399, 0.15826677111908793, 0.15163964498788118, 0.15823965799063444, 0.14998601889237761, 0.1562378159724176, 0.19575517787598073, 0.16225372115150094, 0.1605229249689728, 0.15789680182933807, 0.1584936089348048, 0.1597387338988483, 0.17107815598137677, 0.21372308093123138, 0.1658071039710194, 0.15958303189836442, 0.15665546106174588, 0.14989533089101315, 0.21843835688196123, 0.15782069880515337, 0.15520984400063753, 0.17343133105896413, 0.17969974200241268, 0.16986859403550625, 0.16668506199494004, 0.16771868988871574, 0.17148983012884855, 0.17740870406851172, 0.17848725686781108, 0.15871080197393894, 0.16465826705098152, 0.15789531799964607, 0.15896411216817796, 0.16003635106608272, 0.1572597420308739, 0.15345951006747782, 0.1659493560437113, 0.16638412605971098, 0.1646283350419253, 0.17556645208969712, 0.16662693489342928, 0.16646774811670184, 0.16325454600155354, 0.16724949120543897, 0.18365923292003572, 0.16824331902898848]
[0.0012662246509260217, 0.0012104288447475017, 0.0012227220229746759, 0.0013000314961396908, 0.001322784224640607, 0.0015959462020979371, 0.0012024133881308543, 0.0012037722870360972, 0.0012255654572834109, 0.0012662968843771044, 0.0014155547986485699, 0.0016040735744488562, 0.001240871496527463, 0.0012447317058668118, 0.0012977514181343854, 0.0012591088681604512, 0.0017694660151998203, 0.0012728226508281028, 0.001207957403416666, 0.0015226614177804585, 0.001222510551384022, 0.0012485583478859228, 0.0012800805721894023, 0.0012845555879897619, 0.0012998133879732485, 0.0012719427441181832, 0.0012863031552397004, 0.0013100675816192877, 0.0011950322722857312, 0.00132685286794291, 0.0012029124651277482, 0.0013710151466201673, 0.0013314305043006821, 0.0023769598143796125, 0.0013059672643250043, 0.0012294225658239552, 0.0011802569226643375, 0.0012697190002993095, 0.0012615977122282336, 0.0018672849152728108, 0.0018367700607248747, 0.0017888273487892724, 0.001222870588779103, 0.0012563817517095527, 0.001207492760086591, 0.0012568128981100496, 0.0013263582093135794, 0.0014901029473069565, 0.0012985914873406869, 0.001218981401951507, 0.0012114400859893292, 0.0011990966055137936, 0.001275521706674219, 0.0018304436821346135, 0.0012499377124242541, 0.0012131266750741838, 0.001236952054350413, 0.0012616305034463258, 0.0017697860616757426, 0.0015491688761195934, 0.001209753751393783, 0.0012864901561492174, 0.001292778689832997, 0.0012833436196597742, 0.00127896634941242, 0.001318212178023171, 0.0013286423266598189, 0.0012586950847893491, 0.001223628060863346, 0.0012131756669733413, 0.001835590535269458, 0.0012262119918830636, 0.0013142881081114675, 0.0012984709081406047, 0.001289273007563496, 0.001176304566536763, 0.0013088787447430137, 0.0015326927203049723, 0.0012390706291090148, 0.001336236271371906, 0.0017702314035259476, 0.0018954739216426315, 0.001970203319030215, 0.0012841383789398873, 0.0012961951618682044, 0.0013455511070787907, 0.001339424636157215, 0.0013199214419248955, 0.0012939332789459893, 0.0018535113799595093, 0.001836278782652099, 0.0018021952804871076, 0.001362388038487975, 0.0013555751701946868, 0.0012136394656154998, 0.0013104794806865759, 0.0012597462718281173, 0.001185744704837485, 0.0012035791693587406, 0.0012864625810594984, 0.0012230406742716252, 0.0011681085813415144, 0.0011647479760265628, 0.0011751890622887962, 0.0012013758135627406, 0.0012323482164232305, 0.0019328592872146027, 0.0012449738912486523, 0.00160879979826972, 0.0013264355668437112, 0.0011767673255630242, 0.001307089170513227, 0.0018632144039106924, 0.0012564179000206465, 0.0012199689612485641, 0.0011790385957114226, 0.0012111577364190141, 0.0011975457909870748, 0.0012144375043784, 0.0011834669461141722, 0.0012126893721982953, 0.0012158891474900319, 0.0012502709075144327, 0.0012100943032103454, 0.0015455147435585426, 0.0012600966584832632, 0.0012558184423332298, 0.0011601634016765873, 0.0012085254416014104, 0.001274045744492102, 0.0011778470605265262, 0.001776854720601971, 0.0018513972939591305, 0.0013214613116064736, 0.0011637587598416694, 0.0012148646276819613, 0.001230511915429618, 0.001334086589356379, 0.0019371715580048256, 0.0013971415983671828, 0.0013293249608385702, 0.0013540240145440017, 0.0012774634564858536, 0.0013011387592658055, 0.0013814112081286288, 0.0012346183729552945, 0.0012515146509101687, 0.0017865006270450215, 0.0012755013565540082, 0.0013148992867461702, 0.0013787726972743061, 0.0012471025192373713, 0.0012690007664956325, 0.0012889543725105451, 0.0018965325339935547, 0.001272321148060782, 0.0012956695728610422, 0.001336862829463311, 0.0019319879302625046, 0.0012555714812158615, 0.001221006627866002, 0.0012871574975059244, 0.0012651568300328976, 0.001263494736897622, 0.0017645717688742303, 0.0012198466836482054, 0.001260303316435726, 0.001137639720772588, 0.001181006698703119, 0.001210466635507322, 0.0012442954263628223, 0.0013023109297997267, 0.0011918649146001237, 0.0011514210532051186, 0.0012860297831858312, 0.001610138953497359, 0.0012282924961240947, 0.0012980488367205442, 0.002117438054130983, 0.0012173432235916455, 0.0016859756591332744, 0.001868783357343817, 0.0018334073797879062, 0.0012268741947216119, 0.0011755011239370635, 0.0012266640154312747, 0.0011626823169951753, 0.001211145860251299, 0.0015174819990386103, 0.0012577807841201623, 0.0012443637594494017, 0.001224006215731303, 0.0012286326274015876, 0.0012382847589058007, 0.0013261872556695874, 0.0016567680692343518, 0.0012853263873722433, 0.0012370777666539877, 0.0012143834190833014, 0.0011619793092326601, 0.0016933205959841955, 0.0012234162698073904, 0.00120317708527626, 0.0013444289229377065, 0.0013930212558326564, 0.0013168108064767925, 0.001292132263526667, 0.0013001448828582616, 0.0013293785281306089, 0.0013752612718489282, 0.0013836221462621013, 0.0012303162943716197, 0.0012764206748138102, 0.001223994713175551, 0.0012322799392882012, 0.0012405918687293234, 0.001219067767681193, 0.0011896086051742466, 0.001286429116617917, 0.0012897994268194649, 0.0012761886437358551, 0.0013609802487573421, 0.001291681665840537, 0.0012904476598193941, 0.0012655391162911126, 0.0012965076837630927, 0.0014237149838762459, 0.0013042117754185154]
[789.7492749558106, 826.1534780333183, 817.8473775806941, 769.2121329132384, 755.9811958535372, 626.5875370269115, 831.6607332146352, 830.7218987921535, 815.9498899525126, 789.7042252393311, 706.4367984585972, 623.4128009643136, 805.8852208294463, 803.3859789115081, 770.5635964070644, 794.2124984482023, 565.1422471016338, 785.6554087479482, 827.8437610229752, 656.7448208267288, 817.988849967704, 800.9237227024388, 781.200825733678, 778.4793506405814, 769.3412064013769, 786.1989107798114, 777.4217111468189, 763.3193997243764, 836.7974850480865, 753.6630655592982, 831.315685047624, 729.386544317329, 751.0718710213403, 420.70547173343795, 765.7159772047235, 813.3899830688426, 847.2731494279893, 787.5758335224338, 792.6456986306675, 535.5369134195034, 544.4339612141508, 559.0254423809137, 817.7480177999752, 795.9364250868056, 828.1623153817408, 795.663381163389, 753.9441404125081, 671.09457222891, 770.0651126612914, 820.3570607386358, 825.4638521255011, 833.9611632638357, 783.9929299262093, 546.3156336139374, 800.0398660350043, 824.3162239745897, 808.4387721277938, 792.6250968634284, 565.0400472999196, 645.5074171802569, 826.6145063388965, 777.3087071208123, 773.5276021058032, 779.214533567486, 781.8814001317688, 758.6032178064299, 752.6480076199142, 794.4735878327169, 817.2418008250297, 824.282935458822, 544.7838070559693, 815.5196708395623, 760.8681793803372, 770.1366228004201, 775.6309130289077, 850.1199676068308, 764.0127124200042, 652.4464993877062, 807.0564958182209, 748.3706447912133, 564.8978986635304, 527.5725445662648, 507.5618289447538, 778.7322740291773, 771.4887614290284, 743.1899054143051, 746.5892242127051, 757.6208463904178, 772.8373759847793, 539.516514876669, 544.5796190901476, 554.8788251901949, 734.0052699742097, 737.6942437329983, 823.967931442348, 763.079479486461, 793.810644542588, 843.3518580519889, 830.8551904672742, 777.3253685905306, 817.634295437921, 856.0847989418501, 858.5548295275119, 850.9269121790511, 832.3790014004439, 811.4589583311133, 517.368236071171, 803.2296958428948, 621.5813807756005, 753.9001704994437, 849.7856613426534, 765.0587446970823, 536.70688563866, 795.913525255862, 819.6929854482205, 848.1486557245464, 825.6562873112329, 835.0411379056761, 823.4264804855824, 844.9750145396348, 824.6134772231542, 822.4433963115031, 799.8266567587524, 826.3818756497149, 647.033620460652, 793.5899149226117, 796.2934499847481, 861.94754855641, 827.4546530645689, 784.9011735435369, 849.0066609776788, 562.7922127821545, 540.1325816251706, 756.7380075503839, 859.2846168015535, 823.1369793917396, 812.6699038512459, 749.5765327214954, 516.2165404854188, 715.7470661303652, 752.2615082539155, 738.5393384893343, 782.8012573845973, 768.5575369104143, 723.8974131060372, 809.9668868577659, 799.0317966096093, 559.7535118999983, 784.0054382236615, 760.5145200698868, 725.2827111944548, 801.8586961170765, 788.0215886405783, 775.8226523195405, 527.2780625040406, 785.9650855636233, 771.8017162291176, 748.0198999933704, 517.6015772852821, 796.4500746955698, 818.9963733020325, 776.9057026336416, 790.4158411522761, 791.4556118020678, 566.7097352679425, 819.7751515865025, 793.4597861950471, 879.0129086921166, 846.7352480710862, 826.1276855275628, 803.6676651003067, 767.8657816023881, 839.0212579883726, 868.4920231538064, 777.5869681048437, 621.0644105143316, 814.1383287413405, 770.3870391552064, 472.2688335788929, 821.4610149548483, 593.1283732257912, 535.1075051424599, 545.4325159941719, 815.0794957643627, 850.7010156236483, 815.2191532645691, 860.080165822415, 825.6643834727811, 658.9864002561761, 795.0511032011957, 803.62353243273, 816.9893152074668, 813.9129449255157, 807.5686895182664, 754.0413284209276, 603.5847856858646, 778.0125031467124, 808.356618278551, 823.4631536346795, 860.6005219321618, 590.5556232951727, 817.3832772041161, 831.1328500495763, 743.8102401240401, 717.8641358220093, 759.4105357287891, 773.9145815233039, 769.1450492821864, 752.2311958853543, 727.134560152036, 722.7406721564347, 812.7991188727179, 783.440772883022, 816.9969929082328, 811.5039189696032, 806.0668663129723, 820.2989419547323, 840.6126146452395, 777.3455894943106, 775.3143467166166, 783.5832146826245, 734.7645205821767, 774.184558351898, 774.9248816026804, 790.1770772053871, 771.3027948261101, 702.3877751692773, 766.7466425681556]
Elapsed: 0.17407482465639254~0.027664173190674383
Time per graph: 0.0013494172453983916~0.00021445095496646808
Speed: 755.8591991158569~94.17606287137636
Total Time: 0.1690
best val loss: 0.23022211364708667 test_score: 0.9380

Testing...
Test loss: 0.6855 score: 0.9302 time: 0.16s
test Score 0.9302
Epoch Time List: [1.174430944956839, 0.5852799040731043, 0.5726274740882218, 0.596158524043858, 0.5947930130641907, 0.6464870891068131, 0.5632938661146909, 0.5552454390563071, 0.5553074898198247, 0.5782426872756332, 0.6011429447680712, 0.7159881961997598, 0.5745558459311724, 0.5822800379246473, 0.589645538944751, 0.5750419630203396, 0.6736483769491315, 0.5935419527813792, 0.6047917427495122, 0.6200700139161199, 0.683421814115718, 0.5644348310306668, 0.5814527131151408, 0.5826187012717128, 0.581500536063686, 0.6003381358459592, 0.664601783035323, 0.5858229252044111, 0.5762148797512054, 0.5772493011318147, 0.627289857249707, 0.581787560833618, 0.5904565136879683, 0.7296749709639698, 0.5832832038868219, 0.5594859078992158, 0.6199563671834767, 0.5772430312354118, 0.5941667417064309, 0.7734850202687085, 0.8317531920038164, 0.8377597220242023, 0.6488544389139861, 0.5627321780193597, 0.5656407272908837, 0.5610004463233054, 0.5827676590997726, 0.6104064502287656, 0.672223161906004, 0.5746427110861987, 0.5560728977434337, 0.5416904569137841, 0.5634482488967478, 0.7482338191475719, 0.5641835490241647, 0.5568894068710506, 0.5505773040931672, 0.5658931189682335, 0.6387114480603486, 0.7928399608936161, 0.5653226249851286, 0.5710747041739523, 0.5867916950955987, 0.5742431553080678, 0.5791171891614795, 0.6210316480137408, 0.6866930711548775, 0.5735467351041734, 0.5729501324240118, 0.5589964131359011, 0.7157724720891565, 0.7188308159820735, 0.5880429849494249, 0.5842143178451806, 0.5833091989625245, 0.6388984601944685, 0.5885702171362936, 0.6731570037081838, 0.6581929307430983, 0.591248988872394, 0.750486524309963, 0.8468268688302487, 0.6764443798456341, 0.5969315578695387, 0.5719197031576186, 0.6057318940293044, 0.6890778630040586, 0.5901211081072688, 0.590884359087795, 0.7182504727970809, 0.8326724080834538, 0.8301889651920646, 0.7500028398353606, 0.6168864488136023, 0.5741614811122417, 0.5850393252912909, 0.6503353968728334, 0.5524739490356296, 0.5550073431804776, 0.6063376090023667, 0.5872085599694401, 0.6585303090978414, 0.5423224542755634, 0.546111777657643, 0.5566659849137068, 0.5628894320689142, 0.8393095480278134, 0.7411913895048201, 0.6038278471678495, 0.6016907801385969, 0.5527906476054341, 0.5798057108186185, 0.7279177780728787, 0.5713812841568142, 0.560779799008742, 0.5562034631147981, 0.5679180051665753, 0.5516897898633033, 0.566275330260396, 0.6405951271299273, 0.5652273760642856, 0.5556964240968227, 0.5546125818509609, 0.5496446909382939, 0.6111565819010139, 0.663951565278694, 0.5577509871218354, 0.5563793089240789, 0.5634628888219595, 0.5654441742226481, 0.5515737859532237, 0.684443267993629, 0.849644014146179, 0.7447495080996305, 0.5537510300055146, 0.5699918840546161, 0.5653237069491297, 0.5904127699322999, 0.7673012930899858, 0.8112328259740025, 0.6018884719815105, 0.5986649298574775, 0.6027025380171835, 0.7094111850019544, 0.6092476462945342, 0.580604684073478, 0.5712263602763414, 0.8155610118992627, 0.7178701930679381, 0.57501858798787, 0.5942466252017766, 0.589085791260004, 0.5916812997311354, 0.5921617140993476, 0.7099170526489615, 0.5910309941973537, 0.5848323977552354, 0.5974777899682522, 0.8071651849895716, 0.6028048421721905, 0.5646605638321489, 0.5680330998729914, 0.5534874191507697, 0.5717170101124793, 0.6409393490757793, 0.5775724344421178, 0.5873006889596581, 0.5666992699261755, 0.5635876129381359, 0.5589002310298383, 0.6348543029744178, 0.6070808491203934, 0.7428367231041193, 0.5492018610239029, 0.5709978132508695, 0.6604118573013693, 0.5472487777005881, 0.5573712368495762, 0.7384398041758686, 0.6844120419118553, 0.6201598728075624, 0.8351393102202564, 0.8347626957111061, 0.6470110339578241, 0.5618482790887356, 0.5598051177803427, 0.5478909111116081, 0.5629439868498594, 0.597299343207851, 0.6592463289853185, 0.5565982079133391, 0.5638285069726408, 0.5607192236930132, 0.5686886250041425, 0.5738048702478409, 0.681736865080893, 0.5996408858336508, 0.5717498671729118, 0.5603761710226536, 0.5627412863541394, 0.6436594552360475, 0.6523305680602789, 0.5628755099605769, 0.588419410167262, 0.6660358882509172, 0.5931780119426548, 0.5872438896913081, 0.591933943098411, 0.5874331970699131, 0.5896183832082897, 0.6398205270525068, 0.5615904049482197, 0.5615403351839632, 0.5652641160413623, 0.5751321711577475, 0.5682526440359652, 0.6396167175844312, 0.5517310439608991, 0.5835759001784027, 0.59523516590707, 0.5759977560956031, 0.6096068560145795, 0.6648427848704159, 0.5776922665536404, 0.5937667214311659, 0.5880797856952995, 0.5904208940919489, 0.6406956440769136]
Total Epoch List: [228]
Total Time List: [0.16900427895598114]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cf4faf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2801;  Loss pred: 2.2801; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 2.2775;  Loss pred: 2.2775; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.21s
Epoch 3/1000, LR 0.000045
Train loss: 2.2567;  Loss pred: 2.2567; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 2.2279;  Loss pred: 2.2279; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 2.1952;  Loss pred: 2.1952; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 2.1615;  Loss pred: 2.1615; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.15s
Epoch 7/1000, LR 0.000165
Train loss: 2.1082;  Loss pred: 2.1082; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 2.0499;  Loss pred: 2.0499; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 1.9840;  Loss pred: 1.9840; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.20s
Epoch 10/1000, LR 0.000255
Train loss: 1.9196;  Loss pred: 1.9196; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 1.8379;  Loss pred: 1.8379; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
Test loss: 0.6931 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 1.7890;  Loss pred: 1.7890; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.18s
Test loss: 0.6930 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 1.6992;  Loss pred: 1.6992; Loss self: 0.0000; time: 0.23s
Val loss: 0.6929 score: 0.5194 time: 0.16s
Test loss: 0.6930 score: 0.5039 time: 0.15s
Epoch 14/1000, LR 0.000285
Train loss: 1.6559;  Loss pred: 1.6559; Loss self: 0.0000; time: 0.24s
Val loss: 0.6928 score: 0.5426 time: 0.22s
Test loss: 0.6929 score: 0.5116 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 1.6114;  Loss pred: 1.6114; Loss self: 0.0000; time: 0.38s
Val loss: 0.6928 score: 0.5426 time: 0.25s
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 16/1000, LR 0.000285
Train loss: 1.5597;  Loss pred: 1.5597; Loss self: 0.0000; time: 0.36s
Val loss: 0.6927 score: 0.6047 time: 0.20s
Test loss: 0.6928 score: 0.5659 time: 0.22s
Epoch 17/1000, LR 0.000285
Train loss: 1.5031;  Loss pred: 1.5031; Loss self: 0.0000; time: 0.25s
Val loss: 0.6926 score: 0.5969 time: 0.16s
Test loss: 0.6927 score: 0.5581 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 1.4578;  Loss pred: 1.4578; Loss self: 0.0000; time: 0.25s
Val loss: 0.6925 score: 0.5814 time: 0.17s
Test loss: 0.6927 score: 0.5194 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 1.4236;  Loss pred: 1.4236; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.5581 time: 0.17s
Test loss: 0.6926 score: 0.5116 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 1.3862;  Loss pred: 1.3862; Loss self: 0.0000; time: 0.24s
Val loss: 0.6923 score: 0.5581 time: 0.17s
Test loss: 0.6925 score: 0.5116 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 1.3582;  Loss pred: 1.3582; Loss self: 0.0000; time: 0.34s
Val loss: 0.6921 score: 0.5581 time: 0.22s
Test loss: 0.6924 score: 0.5116 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 1.3274;  Loss pred: 1.3274; Loss self: 0.0000; time: 0.25s
Val loss: 0.6920 score: 0.5581 time: 0.17s
Test loss: 0.6923 score: 0.5116 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 1.2996;  Loss pred: 1.2996; Loss self: 0.0000; time: 0.25s
Val loss: 0.6918 score: 0.5581 time: 0.17s
Test loss: 0.6922 score: 0.5194 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 1.2767;  Loss pred: 1.2767; Loss self: 0.0000; time: 0.26s
Val loss: 0.6917 score: 0.5736 time: 0.23s
Test loss: 0.6920 score: 0.5194 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 1.2532;  Loss pred: 1.2532; Loss self: 0.0000; time: 0.25s
Val loss: 0.6915 score: 0.5891 time: 0.17s
Test loss: 0.6919 score: 0.5194 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 1.2285;  Loss pred: 1.2285; Loss self: 0.0000; time: 0.25s
Val loss: 0.6914 score: 0.5891 time: 0.17s
Test loss: 0.6918 score: 0.5194 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 1.2094;  Loss pred: 1.2094; Loss self: 0.0000; time: 0.25s
Val loss: 0.6912 score: 0.5969 time: 0.17s
Test loss: 0.6916 score: 0.5271 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.1922;  Loss pred: 1.1922; Loss self: 0.0000; time: 0.25s
Val loss: 0.6910 score: 0.5969 time: 0.16s
Test loss: 0.6915 score: 0.5271 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 1.1731;  Loss pred: 1.1731; Loss self: 0.0000; time: 0.24s
Val loss: 0.6907 score: 0.5969 time: 0.17s
Test loss: 0.6913 score: 0.5271 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 1.1651;  Loss pred: 1.1651; Loss self: 0.0000; time: 0.31s
Val loss: 0.6905 score: 0.5969 time: 0.20s
Test loss: 0.6911 score: 0.5426 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 1.1457;  Loss pred: 1.1457; Loss self: 0.0000; time: 0.24s
Val loss: 0.6902 score: 0.6047 time: 0.17s
Test loss: 0.6909 score: 0.5659 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.1367;  Loss pred: 1.1367; Loss self: 0.0000; time: 0.24s
Val loss: 0.6899 score: 0.6202 time: 0.16s
Test loss: 0.6907 score: 0.5891 time: 0.15s
Epoch 33/1000, LR 0.000285
Train loss: 1.1211;  Loss pred: 1.1211; Loss self: 0.0000; time: 0.28s
Val loss: 0.6897 score: 0.6667 time: 0.28s
Test loss: 0.6904 score: 0.6279 time: 0.15s
Epoch 34/1000, LR 0.000285
Train loss: 1.1167;  Loss pred: 1.1167; Loss self: 0.0000; time: 0.30s
Val loss: 0.6894 score: 0.7364 time: 0.16s
Test loss: 0.6902 score: 0.6589 time: 0.23s
Epoch 35/1000, LR 0.000285
Train loss: 1.1044;  Loss pred: 1.1044; Loss self: 0.0000; time: 0.37s
Val loss: 0.6890 score: 0.8450 time: 0.25s
Test loss: 0.6899 score: 0.8062 time: 0.23s
Epoch 36/1000, LR 0.000285
Train loss: 1.0978;  Loss pred: 1.0978; Loss self: 0.0000; time: 0.25s
Val loss: 0.6887 score: 0.8760 time: 0.17s
Test loss: 0.6896 score: 0.8295 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.0878;  Loss pred: 1.0878; Loss self: 0.0000; time: 0.25s
Val loss: 0.6883 score: 0.9147 time: 0.18s
Test loss: 0.6893 score: 0.8915 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.0826;  Loss pred: 1.0826; Loss self: 0.0000; time: 0.29s
Val loss: 0.6878 score: 0.9225 time: 0.17s
Test loss: 0.6889 score: 0.8760 time: 0.18s
Epoch 39/1000, LR 0.000284
Train loss: 1.0737;  Loss pred: 1.0737; Loss self: 0.0000; time: 0.27s
Val loss: 0.6874 score: 0.9225 time: 0.20s
Test loss: 0.6886 score: 0.8682 time: 0.21s
Epoch 40/1000, LR 0.000284
Train loss: 1.0661;  Loss pred: 1.0661; Loss self: 0.0000; time: 0.30s
Val loss: 0.6869 score: 0.9070 time: 0.21s
Test loss: 0.6882 score: 0.8837 time: 0.26s
Epoch 41/1000, LR 0.000284
Train loss: 1.0591;  Loss pred: 1.0591; Loss self: 0.0000; time: 0.25s
Val loss: 0.6863 score: 0.8915 time: 0.17s
Test loss: 0.6877 score: 0.8837 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 1.0572;  Loss pred: 1.0572; Loss self: 0.0000; time: 0.26s
Val loss: 0.6858 score: 0.8915 time: 0.17s
Test loss: 0.6873 score: 0.8837 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.0503;  Loss pred: 1.0503; Loss self: 0.0000; time: 0.27s
Val loss: 0.6851 score: 0.8992 time: 0.17s
Test loss: 0.6868 score: 0.8837 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 1.0465;  Loss pred: 1.0465; Loss self: 0.0000; time: 0.27s
Val loss: 0.6844 score: 0.8915 time: 0.17s
Test loss: 0.6862 score: 0.8837 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 1.0389;  Loss pred: 1.0389; Loss self: 0.0000; time: 0.34s
Val loss: 0.6837 score: 0.8915 time: 0.18s
Test loss: 0.6856 score: 0.8837 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 1.0364;  Loss pred: 1.0364; Loss self: 0.0000; time: 0.25s
Val loss: 0.6829 score: 0.9147 time: 0.18s
Test loss: 0.6850 score: 0.8837 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.0307;  Loss pred: 1.0307; Loss self: 0.0000; time: 0.25s
Val loss: 0.6820 score: 0.9147 time: 0.16s
Test loss: 0.6843 score: 0.8837 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 1.0257;  Loss pred: 1.0257; Loss self: 0.0000; time: 0.24s
Val loss: 0.6811 score: 0.8992 time: 0.16s
Test loss: 0.6836 score: 0.8837 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 1.0233;  Loss pred: 1.0233; Loss self: 0.0000; time: 0.26s
Val loss: 0.6802 score: 0.9147 time: 0.16s
Test loss: 0.6828 score: 0.8837 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.0216;  Loss pred: 1.0216; Loss self: 0.0000; time: 0.27s
Val loss: 0.6791 score: 0.9225 time: 0.21s
Test loss: 0.6820 score: 0.8837 time: 0.24s
Epoch 51/1000, LR 0.000284
Train loss: 1.0175;  Loss pred: 1.0175; Loss self: 0.0000; time: 0.31s
Val loss: 0.6781 score: 0.9225 time: 0.16s
Test loss: 0.6811 score: 0.8760 time: 0.15s
Epoch 52/1000, LR 0.000284
Train loss: 1.0141;  Loss pred: 1.0141; Loss self: 0.0000; time: 0.25s
Val loss: 0.6769 score: 0.9225 time: 0.17s
Test loss: 0.6802 score: 0.8837 time: 0.17s
Epoch 53/1000, LR 0.000284
Train loss: 1.0106;  Loss pred: 1.0106; Loss self: 0.0000; time: 0.26s
Val loss: 0.6756 score: 0.9147 time: 0.16s
Test loss: 0.6792 score: 0.8837 time: 0.16s
Epoch 54/1000, LR 0.000284
Train loss: 1.0074;  Loss pred: 1.0074; Loss self: 0.0000; time: 0.25s
Val loss: 0.6743 score: 0.9147 time: 0.19s
Test loss: 0.6781 score: 0.8837 time: 0.21s
Epoch 55/1000, LR 0.000284
Train loss: 1.0034;  Loss pred: 1.0034; Loss self: 0.0000; time: 0.25s
Val loss: 0.6729 score: 0.9147 time: 0.23s
Test loss: 0.6770 score: 0.8837 time: 0.23s
Epoch 56/1000, LR 0.000284
Train loss: 1.0020;  Loss pred: 1.0020; Loss self: 0.0000; time: 0.32s
Val loss: 0.6714 score: 0.9147 time: 0.16s
Test loss: 0.6758 score: 0.8837 time: 0.16s
Epoch 57/1000, LR 0.000283
Train loss: 0.9981;  Loss pred: 0.9981; Loss self: 0.0000; time: 0.25s
Val loss: 0.6698 score: 0.9147 time: 0.15s
Test loss: 0.6745 score: 0.8837 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 0.9948;  Loss pred: 0.9948; Loss self: 0.0000; time: 0.25s
Val loss: 0.6681 score: 0.9147 time: 0.16s
Test loss: 0.6732 score: 0.8837 time: 0.16s
Epoch 59/1000, LR 0.000283
Train loss: 0.9927;  Loss pred: 0.9927; Loss self: 0.0000; time: 0.25s
Val loss: 0.6663 score: 0.9147 time: 0.17s
Test loss: 0.6717 score: 0.8837 time: 0.16s
Epoch 60/1000, LR 0.000283
Train loss: 0.9884;  Loss pred: 0.9884; Loss self: 0.0000; time: 0.36s
Val loss: 0.6644 score: 0.9147 time: 0.22s
Test loss: 0.6702 score: 0.8837 time: 0.16s
Epoch 61/1000, LR 0.000283
Train loss: 0.9874;  Loss pred: 0.9874; Loss self: 0.0000; time: 0.25s
Val loss: 0.6623 score: 0.9147 time: 0.16s
Test loss: 0.6686 score: 0.8837 time: 0.16s
Epoch 62/1000, LR 0.000283
Train loss: 0.9839;  Loss pred: 0.9839; Loss self: 0.0000; time: 0.25s
Val loss: 0.6602 score: 0.9147 time: 0.16s
Test loss: 0.6669 score: 0.8837 time: 0.16s
Epoch 63/1000, LR 0.000283
Train loss: 0.9813;  Loss pred: 0.9813; Loss self: 0.0000; time: 0.24s
Val loss: 0.6579 score: 0.9147 time: 0.16s
Test loss: 0.6650 score: 0.8837 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 0.9794;  Loss pred: 0.9794; Loss self: 0.0000; time: 0.25s
Val loss: 0.6555 score: 0.9147 time: 0.17s
Test loss: 0.6631 score: 0.8837 time: 0.21s
Epoch 65/1000, LR 0.000283
Train loss: 0.9757;  Loss pred: 0.9757; Loss self: 0.0000; time: 0.35s
Val loss: 0.6530 score: 0.9147 time: 0.24s
Test loss: 0.6611 score: 0.8837 time: 0.24s
Epoch 66/1000, LR 0.000283
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.35s
Val loss: 0.6503 score: 0.9147 time: 0.23s
Test loss: 0.6589 score: 0.8837 time: 0.18s
Epoch 67/1000, LR 0.000283
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.31s
Val loss: 0.6474 score: 0.9147 time: 0.17s
Test loss: 0.6567 score: 0.8837 time: 0.16s
Epoch 68/1000, LR 0.000283
Train loss: 0.9682;  Loss pred: 0.9682; Loss self: 0.0000; time: 0.26s
Val loss: 0.6445 score: 0.9147 time: 0.17s
Test loss: 0.6543 score: 0.8837 time: 0.18s
Epoch 69/1000, LR 0.000283
Train loss: 0.9637;  Loss pred: 0.9637; Loss self: 0.0000; time: 0.26s
Val loss: 0.6414 score: 0.9147 time: 0.18s
Test loss: 0.6518 score: 0.8837 time: 0.18s
Epoch 70/1000, LR 0.000283
Train loss: 0.9617;  Loss pred: 0.9617; Loss self: 0.0000; time: 0.25s
Val loss: 0.6381 score: 0.9147 time: 0.19s
Test loss: 0.6492 score: 0.8837 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 0.9575;  Loss pred: 0.9575; Loss self: 0.0000; time: 0.27s
Val loss: 0.6346 score: 0.9147 time: 0.17s
Test loss: 0.6464 score: 0.8837 time: 0.16s
Epoch 72/1000, LR 0.000282
Train loss: 0.9561;  Loss pred: 0.9561; Loss self: 0.0000; time: 0.25s
Val loss: 0.6310 score: 0.9147 time: 0.20s
Test loss: 0.6435 score: 0.8837 time: 0.19s
Epoch 73/1000, LR 0.000282
Train loss: 0.9522;  Loss pred: 0.9522; Loss self: 0.0000; time: 0.33s
Val loss: 0.6273 score: 0.9147 time: 0.18s
Test loss: 0.6405 score: 0.8837 time: 0.16s
Epoch 74/1000, LR 0.000282
Train loss: 0.9489;  Loss pred: 0.9489; Loss self: 0.0000; time: 0.25s
Val loss: 0.6234 score: 0.9147 time: 0.16s
Test loss: 0.6374 score: 0.8837 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 0.9460;  Loss pred: 0.9460; Loss self: 0.0000; time: 0.25s
Val loss: 0.6193 score: 0.9147 time: 0.16s
Test loss: 0.6341 score: 0.8837 time: 0.16s
Epoch 76/1000, LR 0.000282
Train loss: 0.9429;  Loss pred: 0.9429; Loss self: 0.0000; time: 0.25s
Val loss: 0.6150 score: 0.9147 time: 0.16s
Test loss: 0.6307 score: 0.8837 time: 0.16s
Epoch 77/1000, LR 0.000282
Train loss: 0.9381;  Loss pred: 0.9381; Loss self: 0.0000; time: 0.26s
Val loss: 0.6106 score: 0.9225 time: 0.16s
Test loss: 0.6271 score: 0.8837 time: 0.16s
Epoch 78/1000, LR 0.000282
Train loss: 0.9350;  Loss pred: 0.9350; Loss self: 0.0000; time: 0.28s
Val loss: 0.6060 score: 0.9225 time: 0.22s
Test loss: 0.6235 score: 0.8837 time: 0.24s
Epoch 79/1000, LR 0.000282
Train loss: 0.9309;  Loss pred: 0.9309; Loss self: 0.0000; time: 0.36s
Val loss: 0.6013 score: 0.9070 time: 0.24s
Test loss: 0.6197 score: 0.8837 time: 0.25s
Epoch 80/1000, LR 0.000282
Train loss: 0.9271;  Loss pred: 0.9271; Loss self: 0.0000; time: 0.26s
Val loss: 0.5964 score: 0.9070 time: 0.17s
Test loss: 0.6157 score: 0.8837 time: 0.16s
Epoch 81/1000, LR 0.000281
Train loss: 0.9243;  Loss pred: 0.9243; Loss self: 0.0000; time: 0.26s
Val loss: 0.5913 score: 0.9070 time: 0.17s
Test loss: 0.6116 score: 0.8837 time: 0.17s
Epoch 82/1000, LR 0.000281
Train loss: 0.9186;  Loss pred: 0.9186; Loss self: 0.0000; time: 0.24s
Val loss: 0.5860 score: 0.9070 time: 0.16s
Test loss: 0.6073 score: 0.8837 time: 0.16s
Epoch 83/1000, LR 0.000281
Train loss: 0.9161;  Loss pred: 0.9161; Loss self: 0.0000; time: 0.25s
Val loss: 0.5806 score: 0.9147 time: 0.17s
Test loss: 0.6029 score: 0.8837 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9115;  Loss pred: 0.9115; Loss self: 0.0000; time: 0.25s
Val loss: 0.5751 score: 0.9147 time: 0.17s
Test loss: 0.5984 score: 0.8837 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.9073;  Loss pred: 0.9073; Loss self: 0.0000; time: 0.25s
Val loss: 0.5694 score: 0.9225 time: 0.17s
Test loss: 0.5938 score: 0.8837 time: 0.16s
Epoch 86/1000, LR 0.000281
Train loss: 0.9039;  Loss pred: 0.9039; Loss self: 0.0000; time: 0.33s
Val loss: 0.5636 score: 0.9225 time: 0.16s
Test loss: 0.5890 score: 0.8837 time: 0.16s
Epoch 87/1000, LR 0.000281
Train loss: 0.9000;  Loss pred: 0.9000; Loss self: 0.0000; time: 0.24s
Val loss: 0.5576 score: 0.9147 time: 0.16s
Test loss: 0.5841 score: 0.8837 time: 0.16s
Epoch 88/1000, LR 0.000281
Train loss: 0.8938;  Loss pred: 0.8938; Loss self: 0.0000; time: 0.25s
Val loss: 0.5516 score: 0.9147 time: 0.16s
Test loss: 0.5791 score: 0.8837 time: 0.16s
Epoch 89/1000, LR 0.000281
Train loss: 0.8906;  Loss pred: 0.8906; Loss self: 0.0000; time: 0.24s
Val loss: 0.5454 score: 0.9225 time: 0.16s
Test loss: 0.5741 score: 0.8837 time: 0.16s
Epoch 90/1000, LR 0.000281
Train loss: 0.8854;  Loss pred: 0.8854; Loss self: 0.0000; time: 0.24s
Val loss: 0.5392 score: 0.9225 time: 0.16s
Test loss: 0.5689 score: 0.8837 time: 0.16s
Epoch 91/1000, LR 0.000280
Train loss: 0.8821;  Loss pred: 0.8821; Loss self: 0.0000; time: 0.26s
Val loss: 0.5328 score: 0.9225 time: 0.19s
Test loss: 0.5636 score: 0.8837 time: 0.20s
Epoch 92/1000, LR 0.000280
Train loss: 0.8763;  Loss pred: 0.8763; Loss self: 0.0000; time: 0.24s
Val loss: 0.5263 score: 0.9225 time: 0.16s
Test loss: 0.5583 score: 0.8837 time: 0.16s
Epoch 93/1000, LR 0.000280
Train loss: 0.8738;  Loss pred: 0.8738; Loss self: 0.0000; time: 0.24s
Val loss: 0.5197 score: 0.9225 time: 0.16s
Test loss: 0.5529 score: 0.8837 time: 0.16s
Epoch 94/1000, LR 0.000280
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.24s
Val loss: 0.5131 score: 0.9225 time: 0.16s
Test loss: 0.5474 score: 0.8837 time: 0.16s
Epoch 95/1000, LR 0.000280
Train loss: 0.8634;  Loss pred: 0.8634; Loss self: 0.0000; time: 0.24s
Val loss: 0.5063 score: 0.9225 time: 0.17s
Test loss: 0.5418 score: 0.8837 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.8587;  Loss pred: 0.8587; Loss self: 0.0000; time: 0.26s
Val loss: 0.4996 score: 0.9225 time: 0.17s
Test loss: 0.5362 score: 0.8837 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8532;  Loss pred: 0.8532; Loss self: 0.0000; time: 0.26s
Val loss: 0.4928 score: 0.9225 time: 0.17s
Test loss: 0.5306 score: 0.8837 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.8492;  Loss pred: 0.8492; Loss self: 0.0000; time: 0.26s
Val loss: 0.4859 score: 0.9225 time: 0.18s
Test loss: 0.5249 score: 0.8837 time: 0.18s
Epoch 99/1000, LR 0.000279
Train loss: 0.8437;  Loss pred: 0.8437; Loss self: 0.0000; time: 0.27s
Val loss: 0.4789 score: 0.9225 time: 0.16s
Test loss: 0.5191 score: 0.8837 time: 0.24s
Epoch 100/1000, LR 0.000279
Train loss: 0.8393;  Loss pred: 0.8393; Loss self: 0.0000; time: 0.26s
Val loss: 0.4721 score: 0.9225 time: 0.16s
Test loss: 0.5133 score: 0.8837 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.8337;  Loss pred: 0.8337; Loss self: 0.0000; time: 0.26s
Val loss: 0.4651 score: 0.9225 time: 0.16s
Test loss: 0.5075 score: 0.8837 time: 0.16s
Epoch 102/1000, LR 0.000279
Train loss: 0.8280;  Loss pred: 0.8280; Loss self: 0.0000; time: 0.26s
Val loss: 0.4579 score: 0.9225 time: 0.16s
Test loss: 0.5016 score: 0.8837 time: 0.16s
Epoch 103/1000, LR 0.000279
Train loss: 0.8234;  Loss pred: 0.8234; Loss self: 0.0000; time: 0.27s
Val loss: 0.4508 score: 0.9225 time: 0.17s
Test loss: 0.4957 score: 0.8837 time: 0.16s
Epoch 104/1000, LR 0.000279
Train loss: 0.8189;  Loss pred: 0.8189; Loss self: 0.0000; time: 0.25s
Val loss: 0.4438 score: 0.9225 time: 0.16s
Test loss: 0.4898 score: 0.8760 time: 0.17s
Epoch 105/1000, LR 0.000279
Train loss: 0.8133;  Loss pred: 0.8133; Loss self: 0.0000; time: 0.26s
Val loss: 0.4367 score: 0.9225 time: 0.22s
Test loss: 0.4839 score: 0.8760 time: 0.25s
Epoch 106/1000, LR 0.000279
Train loss: 0.8100;  Loss pred: 0.8100; Loss self: 0.0000; time: 0.28s
Val loss: 0.4296 score: 0.9225 time: 0.18s
Test loss: 0.4780 score: 0.8760 time: 0.17s
Epoch 107/1000, LR 0.000278
Train loss: 0.8052;  Loss pred: 0.8052; Loss self: 0.0000; time: 0.27s
Val loss: 0.4227 score: 0.9225 time: 0.19s
Test loss: 0.4722 score: 0.8760 time: 0.23s
Epoch 108/1000, LR 0.000278
Train loss: 0.7997;  Loss pred: 0.7997; Loss self: 0.0000; time: 0.38s
Val loss: 0.4160 score: 0.9380 time: 0.25s
Test loss: 0.4664 score: 0.8760 time: 0.25s
Epoch 109/1000, LR 0.000278
Train loss: 0.7948;  Loss pred: 0.7948; Loss self: 0.0000; time: 0.38s
Val loss: 0.4091 score: 0.9380 time: 0.26s
Test loss: 0.4607 score: 0.8837 time: 0.25s
Epoch 110/1000, LR 0.000278
Train loss: 0.7904;  Loss pred: 0.7904; Loss self: 0.0000; time: 0.40s
Val loss: 0.4022 score: 0.9380 time: 0.18s
Test loss: 0.4549 score: 0.8837 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.7853;  Loss pred: 0.7853; Loss self: 0.0000; time: 0.27s
Val loss: 0.3955 score: 0.9380 time: 0.17s
Test loss: 0.4492 score: 0.8915 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.7799;  Loss pred: 0.7799; Loss self: 0.0000; time: 0.27s
Val loss: 0.3889 score: 0.9380 time: 0.17s
Test loss: 0.4436 score: 0.8915 time: 0.17s
Epoch 113/1000, LR 0.000278
Train loss: 0.7750;  Loss pred: 0.7750; Loss self: 0.0000; time: 0.28s
Val loss: 0.3821 score: 0.9380 time: 0.17s
Test loss: 0.4380 score: 0.8915 time: 0.17s
Epoch 114/1000, LR 0.000277
Train loss: 0.7708;  Loss pred: 0.7708; Loss self: 0.0000; time: 0.27s
Val loss: 0.3757 score: 0.9380 time: 0.18s
Test loss: 0.4324 score: 0.8915 time: 0.18s
Epoch 115/1000, LR 0.000277
Train loss: 0.7688;  Loss pred: 0.7688; Loss self: 0.0000; time: 0.27s
Val loss: 0.3694 score: 0.9457 time: 0.18s
Test loss: 0.4271 score: 0.8992 time: 0.16s
Epoch 116/1000, LR 0.000277
Train loss: 0.7624;  Loss pred: 0.7624; Loss self: 0.0000; time: 0.29s
Val loss: 0.3632 score: 0.9457 time: 0.17s
Test loss: 0.4217 score: 0.9070 time: 0.25s
Epoch 117/1000, LR 0.000277
Train loss: 0.7576;  Loss pred: 0.7576; Loss self: 0.0000; time: 0.26s
Val loss: 0.3568 score: 0.9457 time: 0.16s
Test loss: 0.4164 score: 0.9070 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.7536;  Loss pred: 0.7536; Loss self: 0.0000; time: 0.26s
Val loss: 0.3511 score: 0.9457 time: 0.17s
Test loss: 0.4113 score: 0.9070 time: 0.17s
Epoch 119/1000, LR 0.000277
Train loss: 0.7516;  Loss pred: 0.7516; Loss self: 0.0000; time: 0.28s
Val loss: 0.3449 score: 0.9457 time: 0.22s
Test loss: 0.4061 score: 0.9070 time: 0.24s
Epoch 120/1000, LR 0.000277
Train loss: 0.7451;  Loss pred: 0.7451; Loss self: 0.0000; time: 0.38s
Val loss: 0.3393 score: 0.9457 time: 0.24s
Test loss: 0.4013 score: 0.9070 time: 0.24s
Epoch 121/1000, LR 0.000276
Train loss: 0.7414;  Loss pred: 0.7414; Loss self: 0.0000; time: 0.38s
Val loss: 0.3340 score: 0.9457 time: 0.26s
Test loss: 0.3965 score: 0.9070 time: 0.25s
Epoch 122/1000, LR 0.000276
Train loss: 0.7370;  Loss pred: 0.7370; Loss self: 0.0000; time: 0.37s
Val loss: 0.3284 score: 0.9457 time: 0.19s
Test loss: 0.3917 score: 0.9070 time: 0.18s
Epoch 123/1000, LR 0.000276
Train loss: 0.7343;  Loss pred: 0.7343; Loss self: 0.0000; time: 0.25s
Val loss: 0.3237 score: 0.9457 time: 0.17s
Test loss: 0.3874 score: 0.9070 time: 0.16s
Epoch 124/1000, LR 0.000276
Train loss: 0.7290;  Loss pred: 0.7290; Loss self: 0.0000; time: 0.26s
Val loss: 0.3191 score: 0.9457 time: 0.17s
Test loss: 0.3832 score: 0.9070 time: 0.16s
Epoch 125/1000, LR 0.000276
Train loss: 0.7243;  Loss pred: 0.7243; Loss self: 0.0000; time: 0.26s
Val loss: 0.3138 score: 0.9457 time: 0.17s
Test loss: 0.3787 score: 0.9070 time: 0.16s
Epoch 126/1000, LR 0.000276
Train loss: 0.7229;  Loss pred: 0.7229; Loss self: 0.0000; time: 0.31s
Val loss: 0.3085 score: 0.9457 time: 0.17s
Test loss: 0.3741 score: 0.9070 time: 0.17s
Epoch 127/1000, LR 0.000275
Train loss: 0.7205;  Loss pred: 0.7205; Loss self: 0.0000; time: 0.28s
Val loss: 0.3036 score: 0.9457 time: 0.18s
Test loss: 0.3699 score: 0.9070 time: 0.17s
Epoch 128/1000, LR 0.000275
Train loss: 0.7133;  Loss pred: 0.7133; Loss self: 0.0000; time: 0.33s
Val loss: 0.2989 score: 0.9457 time: 0.17s
Test loss: 0.3658 score: 0.9070 time: 0.26s
Epoch 129/1000, LR 0.000275
Train loss: 0.7085;  Loss pred: 0.7085; Loss self: 0.0000; time: 0.24s
Val loss: 0.2946 score: 0.9457 time: 0.16s
Test loss: 0.3619 score: 0.9070 time: 0.15s
Epoch 130/1000, LR 0.000275
Train loss: 0.7068;  Loss pred: 0.7068; Loss self: 0.0000; time: 0.24s
Val loss: 0.2903 score: 0.9457 time: 0.16s
Test loss: 0.3580 score: 0.9070 time: 0.16s
Epoch 131/1000, LR 0.000275
Train loss: 0.7034;  Loss pred: 0.7034; Loss self: 0.0000; time: 0.26s
Val loss: 0.2866 score: 0.9457 time: 0.22s
Test loss: 0.3544 score: 0.9070 time: 0.32s
Epoch 132/1000, LR 0.000275
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.36s
Val loss: 0.2825 score: 0.9457 time: 0.25s
Test loss: 0.3507 score: 0.9070 time: 0.25s
Epoch 133/1000, LR 0.000274
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.37s
Val loss: 0.2788 score: 0.9380 time: 0.34s
Test loss: 0.3472 score: 0.9147 time: 0.25s
Epoch 134/1000, LR 0.000274
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.24s
Val loss: 0.2752 score: 0.9380 time: 0.16s
Test loss: 0.3438 score: 0.9147 time: 0.16s
Epoch 135/1000, LR 0.000274
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.24s
Val loss: 0.2715 score: 0.9380 time: 0.16s
Test loss: 0.3404 score: 0.9147 time: 0.16s
Epoch 136/1000, LR 0.000274
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.24s
Val loss: 0.2674 score: 0.9380 time: 0.16s
Test loss: 0.3369 score: 0.9147 time: 0.16s
Epoch 137/1000, LR 0.000274
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.24s
Val loss: 0.2644 score: 0.9380 time: 0.16s
Test loss: 0.3339 score: 0.9147 time: 0.16s
Epoch 138/1000, LR 0.000274
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.25s
Val loss: 0.2616 score: 0.9380 time: 0.16s
Test loss: 0.3310 score: 0.9147 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.27s
Val loss: 0.2593 score: 0.9380 time: 0.17s
Test loss: 0.3284 score: 0.9147 time: 0.25s
Epoch 140/1000, LR 0.000273
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.25s
Val loss: 0.2563 score: 0.9380 time: 0.16s
Test loss: 0.3254 score: 0.9147 time: 0.16s
Epoch 141/1000, LR 0.000273
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.24s
Val loss: 0.2531 score: 0.9380 time: 0.16s
Test loss: 0.3224 score: 0.9147 time: 0.16s
Epoch 142/1000, LR 0.000273
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 0.24s
Val loss: 0.2492 score: 0.9380 time: 0.16s
Test loss: 0.3191 score: 0.9147 time: 0.16s
Epoch 143/1000, LR 0.000273
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.24s
Val loss: 0.2462 score: 0.9380 time: 0.16s
Test loss: 0.3163 score: 0.9147 time: 0.16s
Epoch 144/1000, LR 0.000272
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.24s
Val loss: 0.2433 score: 0.9380 time: 0.16s
Test loss: 0.3136 score: 0.9147 time: 0.16s
Epoch 145/1000, LR 0.000272
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.24s
Val loss: 0.2407 score: 0.9380 time: 0.16s
Test loss: 0.3111 score: 0.9147 time: 0.16s
Epoch 146/1000, LR 0.000272
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.26s
Val loss: 0.2379 score: 0.9380 time: 0.16s
Test loss: 0.3085 score: 0.9147 time: 0.24s
Epoch 147/1000, LR 0.000272
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 0.25s
Val loss: 0.2354 score: 0.9380 time: 0.16s
Test loss: 0.3061 score: 0.9147 time: 0.16s
Epoch 148/1000, LR 0.000272
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 0.24s
Val loss: 0.2331 score: 0.9380 time: 0.16s
Test loss: 0.3037 score: 0.9147 time: 0.16s
Epoch 149/1000, LR 0.000272
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.24s
Val loss: 0.2313 score: 0.9457 time: 0.16s
Test loss: 0.3016 score: 0.9147 time: 0.16s
Epoch 150/1000, LR 0.000271
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.25s
Val loss: 0.2295 score: 0.9457 time: 0.16s
Test loss: 0.2995 score: 0.9147 time: 0.16s
Epoch 151/1000, LR 0.000271
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.24s
Val loss: 0.2278 score: 0.9457 time: 0.16s
Test loss: 0.2975 score: 0.9147 time: 0.16s
Epoch 152/1000, LR 0.000271
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.25s
Val loss: 0.2260 score: 0.9457 time: 0.16s
Test loss: 0.2955 score: 0.9147 time: 0.17s
Epoch 153/1000, LR 0.000271
Train loss: 0.6465;  Loss pred: 0.6465; Loss self: 0.0000; time: 0.32s
Val loss: 0.2241 score: 0.9457 time: 0.16s
Test loss: 0.2934 score: 0.9147 time: 0.16s
Epoch 154/1000, LR 0.000271
Train loss: 0.6433;  Loss pred: 0.6433; Loss self: 0.0000; time: 0.24s
Val loss: 0.2222 score: 0.9457 time: 0.16s
Test loss: 0.2914 score: 0.9147 time: 0.16s
Epoch 155/1000, LR 0.000270
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.26s
Val loss: 0.2194 score: 0.9457 time: 0.16s
Test loss: 0.2892 score: 0.9147 time: 0.16s
Epoch 156/1000, LR 0.000270
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.24s
Val loss: 0.2165 score: 0.9457 time: 0.16s
Test loss: 0.2872 score: 0.9147 time: 0.16s
Epoch 157/1000, LR 0.000270
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.25s
Val loss: 0.2147 score: 0.9457 time: 0.16s
Test loss: 0.2853 score: 0.9147 time: 0.18s
Epoch 158/1000, LR 0.000270
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.28s
Val loss: 0.2128 score: 0.9457 time: 0.24s
Test loss: 0.2836 score: 0.9225 time: 0.18s
Epoch 159/1000, LR 0.000270
Train loss: 0.6346;  Loss pred: 0.6346; Loss self: 0.0000; time: 0.25s
Val loss: 0.2112 score: 0.9457 time: 0.16s
Test loss: 0.2819 score: 0.9225 time: 0.16s
Epoch 160/1000, LR 0.000269
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.25s
Val loss: 0.2103 score: 0.9457 time: 0.16s
Test loss: 0.2802 score: 0.9147 time: 0.16s
Epoch 161/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.26s
Val loss: 0.2091 score: 0.9457 time: 0.16s
Test loss: 0.2786 score: 0.9147 time: 0.17s
Epoch 162/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.25s
Val loss: 0.2083 score: 0.9457 time: 0.18s
Test loss: 0.2772 score: 0.9147 time: 0.16s
Epoch 163/1000, LR 0.000269
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.25s
Val loss: 0.2076 score: 0.9457 time: 0.17s
Test loss: 0.2758 score: 0.9147 time: 0.16s
Epoch 164/1000, LR 0.000269
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.26s
Val loss: 0.2053 score: 0.9457 time: 0.16s
Test loss: 0.2741 score: 0.9147 time: 0.22s
Epoch 165/1000, LR 0.000268
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.26s
Val loss: 0.2030 score: 0.9457 time: 0.16s
Test loss: 0.2724 score: 0.9147 time: 0.16s
Epoch 166/1000, LR 0.000268
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.25s
Val loss: 0.2012 score: 0.9457 time: 0.17s
Test loss: 0.2709 score: 0.9147 time: 0.16s
Epoch 167/1000, LR 0.000268
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.28s
Val loss: 0.1993 score: 0.9457 time: 0.16s
Test loss: 0.2695 score: 0.9225 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.25s
Val loss: 0.1977 score: 0.9457 time: 0.18s
Test loss: 0.2683 score: 0.9225 time: 0.16s
Epoch 169/1000, LR 0.000267
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.25s
Val loss: 0.1966 score: 0.9457 time: 0.16s
Test loss: 0.2670 score: 0.9225 time: 0.19s
Epoch 170/1000, LR 0.000267
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.26s
Val loss: 0.1956 score: 0.9457 time: 0.21s
Test loss: 0.2656 score: 0.9225 time: 0.16s
Epoch 171/1000, LR 0.000267
Train loss: 0.6138;  Loss pred: 0.6138; Loss self: 0.0000; time: 0.25s
Val loss: 0.1955 score: 0.9457 time: 0.16s
Test loss: 0.2643 score: 0.9147 time: 0.16s
Epoch 172/1000, LR 0.000267
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.25s
Val loss: 0.1948 score: 0.9457 time: 0.16s
Test loss: 0.2632 score: 0.9147 time: 0.16s
Epoch 173/1000, LR 0.000267
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.25s
Val loss: 0.1940 score: 0.9457 time: 0.16s
Test loss: 0.2620 score: 0.9147 time: 0.16s
Epoch 174/1000, LR 0.000266
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.26s
Val loss: 0.1921 score: 0.9457 time: 0.17s
Test loss: 0.2608 score: 0.9147 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.25s
Val loss: 0.1910 score: 0.9457 time: 0.16s
Test loss: 0.2596 score: 0.9147 time: 0.16s
Epoch 176/1000, LR 0.000266
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 0.25s
Val loss: 0.1892 score: 0.9457 time: 0.16s
Test loss: 0.2585 score: 0.9225 time: 0.17s
Epoch 177/1000, LR 0.000266
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.25s
Val loss: 0.1879 score: 0.9457 time: 0.22s
Test loss: 0.2574 score: 0.9225 time: 0.20s
Epoch 178/1000, LR 0.000265
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.25s
Val loss: 0.1865 score: 0.9457 time: 0.16s
Test loss: 0.2565 score: 0.9225 time: 0.16s
Epoch 179/1000, LR 0.000265
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.24s
Val loss: 0.1857 score: 0.9457 time: 0.16s
Test loss: 0.2554 score: 0.9225 time: 0.15s
Epoch 180/1000, LR 0.000265
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.25s
Val loss: 0.1848 score: 0.9457 time: 0.16s
Test loss: 0.2544 score: 0.9225 time: 0.16s
Epoch 181/1000, LR 0.000265
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.24s
Val loss: 0.1841 score: 0.9457 time: 0.16s
Test loss: 0.2534 score: 0.9225 time: 0.16s
Epoch 182/1000, LR 0.000265
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.25s
Val loss: 0.1839 score: 0.9457 time: 0.16s
Test loss: 0.2525 score: 0.9225 time: 0.16s
Epoch 183/1000, LR 0.000264
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.27s
Val loss: 0.1833 score: 0.9457 time: 0.16s
Test loss: 0.2516 score: 0.9225 time: 0.23s
Epoch 184/1000, LR 0.000264
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.38s
Val loss: 0.1817 score: 0.9457 time: 0.17s
Test loss: 0.2507 score: 0.9225 time: 0.16s
Epoch 185/1000, LR 0.000264
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.25s
Val loss: 0.1802 score: 0.9457 time: 0.16s
Test loss: 0.2500 score: 0.9225 time: 0.16s
Epoch 186/1000, LR 0.000264
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.24s
Val loss: 0.1796 score: 0.9457 time: 0.16s
Test loss: 0.2491 score: 0.9225 time: 0.15s
Epoch 187/1000, LR 0.000263
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.24s
Val loss: 0.1797 score: 0.9457 time: 0.16s
Test loss: 0.2482 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.24s
Val loss: 0.1789 score: 0.9457 time: 0.16s
Test loss: 0.2474 score: 0.9225 time: 0.15s
Epoch 189/1000, LR 0.000263
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.25s
Val loss: 0.1775 score: 0.9457 time: 0.18s
Test loss: 0.2467 score: 0.9225 time: 0.17s
Epoch 190/1000, LR 0.000263
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.25s
Val loss: 0.1767 score: 0.9457 time: 0.16s
Test loss: 0.2460 score: 0.9225 time: 0.23s
Epoch 191/1000, LR 0.000262
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.35s
Val loss: 0.1759 score: 0.9457 time: 0.16s
Test loss: 0.2453 score: 0.9225 time: 0.16s
Epoch 192/1000, LR 0.000262
Train loss: 0.5864;  Loss pred: 0.5864; Loss self: 0.0000; time: 0.24s
Val loss: 0.1749 score: 0.9457 time: 0.16s
Test loss: 0.2447 score: 0.9225 time: 0.15s
Epoch 193/1000, LR 0.000262
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.24s
Val loss: 0.1747 score: 0.9457 time: 0.16s
Test loss: 0.2438 score: 0.9225 time: 0.16s
Epoch 194/1000, LR 0.000262
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.25s
Val loss: 0.1747 score: 0.9457 time: 0.21s
Test loss: 0.2431 score: 0.9225 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.37s
Val loss: 0.1741 score: 0.9457 time: 0.24s
Test loss: 0.2425 score: 0.9225 time: 0.24s
Epoch 196/1000, LR 0.000261
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.35s
Val loss: 0.1729 score: 0.9457 time: 0.20s
Test loss: 0.2419 score: 0.9225 time: 0.16s
Epoch 197/1000, LR 0.000261
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.24s
Val loss: 0.1720 score: 0.9457 time: 0.15s
Test loss: 0.2413 score: 0.9225 time: 0.15s
Epoch 198/1000, LR 0.000261
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.25s
Val loss: 0.1713 score: 0.9457 time: 0.15s
Test loss: 0.2407 score: 0.9225 time: 0.15s
Epoch 199/1000, LR 0.000260
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.24s
Val loss: 0.1708 score: 0.9457 time: 0.17s
Test loss: 0.2401 score: 0.9225 time: 0.16s
Epoch 200/1000, LR 0.000260
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.25s
Val loss: 0.1702 score: 0.9457 time: 0.17s
Test loss: 0.2395 score: 0.9225 time: 0.16s
Epoch 201/1000, LR 0.000260
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.24s
Val loss: 0.1697 score: 0.9457 time: 0.16s
Test loss: 0.2390 score: 0.9225 time: 0.16s
Epoch 202/1000, LR 0.000260
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.30s
Val loss: 0.1687 score: 0.9457 time: 0.17s
Test loss: 0.2385 score: 0.9225 time: 0.17s
Epoch 203/1000, LR 0.000259
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.31s
Val loss: 0.1677 score: 0.9457 time: 0.17s
Test loss: 0.2383 score: 0.9225 time: 0.17s
Epoch 204/1000, LR 0.000259
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.25s
Val loss: 0.1669 score: 0.9457 time: 0.17s
Test loss: 0.2380 score: 0.9225 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.23s
Val loss: 0.1668 score: 0.9457 time: 0.16s
Test loss: 0.2372 score: 0.9225 time: 0.15s
Epoch 206/1000, LR 0.000259
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.24s
Val loss: 0.1668 score: 0.9457 time: 0.15s
Test loss: 0.2366 score: 0.9225 time: 0.15s
Epoch 207/1000, LR 0.000258
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.24s
Val loss: 0.1669 score: 0.9457 time: 0.15s
Test loss: 0.2360 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.24s
Val loss: 0.1667 score: 0.9457 time: 0.16s
Test loss: 0.2355 score: 0.9225 time: 0.15s
Epoch 209/1000, LR 0.000258
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.23s
Val loss: 0.1666 score: 0.9457 time: 0.15s
Test loss: 0.2351 score: 0.9225 time: 0.15s
Epoch 210/1000, LR 0.000258
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.27s
Val loss: 0.1653 score: 0.9457 time: 0.16s
Test loss: 0.2348 score: 0.9225 time: 0.21s
Epoch 211/1000, LR 0.000257
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.27s
Val loss: 0.1645 score: 0.9457 time: 0.16s
Test loss: 0.2346 score: 0.9225 time: 0.16s
Epoch 212/1000, LR 0.000257
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.24s
Val loss: 0.1639 score: 0.9457 time: 0.16s
Test loss: 0.2344 score: 0.9225 time: 0.15s
Epoch 213/1000, LR 0.000257
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.24s
Val loss: 0.1640 score: 0.9457 time: 0.16s
Test loss: 0.2337 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.24s
Val loss: 0.1634 score: 0.9457 time: 0.16s
Test loss: 0.2335 score: 0.9225 time: 0.16s
Epoch 215/1000, LR 0.000256
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.25s
Val loss: 0.1630 score: 0.9457 time: 0.16s
Test loss: 0.2332 score: 0.9225 time: 0.16s
Epoch 216/1000, LR 0.000256
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.27s
Val loss: 0.1627 score: 0.9457 time: 0.16s
Test loss: 0.2328 score: 0.9225 time: 0.22s
Epoch 217/1000, LR 0.000256
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.35s
Val loss: 0.1624 score: 0.9457 time: 0.17s
Test loss: 0.2325 score: 0.9225 time: 0.16s
Epoch 218/1000, LR 0.000255
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.25s
Val loss: 0.1626 score: 0.9457 time: 0.17s
Test loss: 0.2321 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.24s
Val loss: 0.1629 score: 0.9457 time: 0.16s
Test loss: 0.2316 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.24s
Val loss: 0.1624 score: 0.9457 time: 0.16s
Test loss: 0.2314 score: 0.9225 time: 0.16s
Epoch 221/1000, LR 0.000255
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.27s
Val loss: 0.1618 score: 0.9457 time: 0.23s
Test loss: 0.2312 score: 0.9225 time: 0.18s
Epoch 222/1000, LR 0.000254
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.25s
Val loss: 0.1613 score: 0.9457 time: 0.16s
Test loss: 0.2310 score: 0.9225 time: 0.17s
Epoch 223/1000, LR 0.000254
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.24s
Val loss: 0.1607 score: 0.9457 time: 0.18s
Test loss: 0.2309 score: 0.9225 time: 0.16s
Epoch 224/1000, LR 0.000254
Train loss: 0.5567;  Loss pred: 0.5567; Loss self: 0.0000; time: 0.25s
Val loss: 0.1600 score: 0.9457 time: 0.22s
Test loss: 0.2310 score: 0.9225 time: 0.15s
Epoch 225/1000, LR 0.000253
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.24s
Val loss: 0.1602 score: 0.9457 time: 0.16s
Test loss: 0.2305 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.25s
Val loss: 0.1604 score: 0.9457 time: 0.17s
Test loss: 0.2301 score: 0.9225 time: 0.37s
     INFO: Early stopping counter 2 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.24s
Val loss: 0.1607 score: 0.9457 time: 0.18s
Test loss: 0.2297 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.31s
Val loss: 0.1604 score: 0.9457 time: 0.19s
Test loss: 0.2295 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.27s
Val loss: 0.1605 score: 0.9457 time: 0.22s
Test loss: 0.2292 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5548;  Loss pred: 0.5548; Loss self: 0.0000; time: 0.24s
Val loss: 0.1603 score: 0.9457 time: 0.16s
Test loss: 0.2289 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.25s
Val loss: 0.1599 score: 0.9457 time: 0.16s
Test loss: 0.2288 score: 0.9225 time: 0.16s
Epoch 232/1000, LR 0.000251
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.26s
Val loss: 0.1593 score: 0.9457 time: 0.19s
Test loss: 0.2288 score: 0.9225 time: 0.16s
Epoch 233/1000, LR 0.000251
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.28s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2288 score: 0.9225 time: 0.16s
Epoch 234/1000, LR 0.000251
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.25s
Val loss: 0.1584 score: 0.9457 time: 0.16s
Test loss: 0.2289 score: 0.9225 time: 0.16s
Epoch 235/1000, LR 0.000250
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.25s
Val loss: 0.1581 score: 0.9457 time: 0.25s
Test loss: 0.2287 score: 0.9225 time: 0.16s
Epoch 236/1000, LR 0.000250
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.24s
Val loss: 0.1583 score: 0.9457 time: 0.17s
Test loss: 0.2284 score: 0.9225 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.26s
Val loss: 0.1587 score: 0.9457 time: 0.17s
Test loss: 0.2281 score: 0.9225 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 238/1000, LR 0.000250
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.25s
Val loss: 0.1591 score: 0.9457 time: 0.18s
Test loss: 0.2277 score: 0.9225 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.36s
Val loss: 0.1591 score: 0.9457 time: 0.25s
Test loss: 0.2275 score: 0.9225 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.37s
Val loss: 0.1587 score: 0.9457 time: 0.25s
Test loss: 0.2275 score: 0.9225 time: 0.20s
     INFO: Early stopping counter 5 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2273 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.25s
Val loss: 0.1587 score: 0.9457 time: 0.18s
Test loss: 0.2272 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.25s
Val loss: 0.1582 score: 0.9457 time: 0.17s
Test loss: 0.2272 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.24s
Val loss: 0.1581 score: 0.9457 time: 0.17s
Test loss: 0.2271 score: 0.9302 time: 0.17s
Epoch 245/1000, LR 0.000247
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 0.26s
Val loss: 0.1579 score: 0.9457 time: 0.17s
Test loss: 0.2271 score: 0.9302 time: 0.25s
Epoch 246/1000, LR 0.000247
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.25s
Val loss: 0.1573 score: 0.9457 time: 0.17s
Test loss: 0.2274 score: 0.9225 time: 0.17s
Epoch 247/1000, LR 0.000247
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.25s
Val loss: 0.1572 score: 0.9457 time: 0.17s
Test loss: 0.2274 score: 0.9225 time: 0.17s
Epoch 248/1000, LR 0.000247
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.24s
Val loss: 0.1577 score: 0.9457 time: 0.16s
Test loss: 0.2270 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.24s
Val loss: 0.1584 score: 0.9457 time: 0.16s
Test loss: 0.2265 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.27s
Val loss: 0.1583 score: 0.9457 time: 0.20s
Test loss: 0.2265 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5389;  Loss pred: 0.5389; Loss self: 0.0000; time: 0.24s
Val loss: 0.1579 score: 0.9457 time: 0.16s
Test loss: 0.2267 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.24s
Val loss: 0.1575 score: 0.9457 time: 0.16s
Test loss: 0.2270 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.24s
Val loss: 0.1574 score: 0.9457 time: 0.16s
Test loss: 0.2272 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.25s
Val loss: 0.1573 score: 0.9457 time: 0.16s
Test loss: 0.2272 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.24s
Val loss: 0.1577 score: 0.9457 time: 0.16s
Test loss: 0.2270 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.24s
Val loss: 0.1584 score: 0.9457 time: 0.16s
Test loss: 0.2267 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.24s
Val loss: 0.1584 score: 0.9457 time: 0.16s
Test loss: 0.2267 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.25s
Val loss: 0.1589 score: 0.9457 time: 0.16s
Test loss: 0.2265 score: 0.9302 time: 0.22s
     INFO: Early stopping counter 11 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2266 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.25s
Val loss: 0.1595 score: 0.9457 time: 0.17s
Test loss: 0.2263 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.25s
Val loss: 0.1595 score: 0.9457 time: 0.17s
Test loss: 0.2264 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 262/1000, LR 0.000242
Train loss: 0.5389;  Loss pred: 0.5389; Loss self: 0.0000; time: 0.26s
Val loss: 0.1598 score: 0.9457 time: 0.17s
Test loss: 0.2263 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 0.26s
Val loss: 0.1591 score: 0.9457 time: 0.17s
Test loss: 0.2266 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 16 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.31s
Val loss: 0.1586 score: 0.9457 time: 0.25s
Test loss: 0.2270 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 17 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.32s
Val loss: 0.1581 score: 0.9457 time: 0.16s
Test loss: 0.2274 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.24s
Val loss: 0.1585 score: 0.9457 time: 0.16s
Test loss: 0.2271 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.23s
Val loss: 0.1590 score: 0.9457 time: 0.16s
Test loss: 0.2269 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 246,   Train_Loss: 0.5448,   Val_Loss: 0.1572,   Val_Precision: 0.9531,   Val_Recall: 0.9385,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1572,   Test_Precision: 0.9821,   Test_Recall: 0.8594,   Test_accuracy: 0.9167,   Test_Score: 0.9225,   Test_loss: 0.2274


[0.1633429799694568, 0.15614532097242773, 0.1577311409637332, 0.16770406300202012, 0.1706391649786383, 0.2058770600706339, 0.1551113270688802, 0.15528662502765656, 0.15809794398956, 0.16335229808464646, 0.18260656902566552, 0.20692549110390246, 0.16007242305204272, 0.16057039005681872, 0.1674099329393357, 0.1624250439926982, 0.2282611159607768, 0.16419412195682526, 0.1558265050407499, 0.19642332289367914, 0.15770386112853885, 0.16106402687728405, 0.16513039381243289, 0.16570767085067928, 0.16767592704854906, 0.16408061399124563, 0.16593310702592134, 0.1689987180288881, 0.15415916312485933, 0.17116401996463537, 0.1551757080014795, 0.17686095391400158, 0.171754535054788, 0.30662781605497, 0.16846977709792554, 0.1585955109912902, 0.15225314302369952, 0.16379375103861094, 0.16274610487744212, 0.24087975407019258, 0.23694333783350885, 0.23075872799381614, 0.15775030595250428, 0.1620732459705323, 0.15576656605117023, 0.1621288638561964, 0.17110020900145173, 0.19222328020259738, 0.1675183018669486, 0.1572486008517444, 0.15627577109262347, 0.15468346211127937, 0.16454230016097426, 0.23612723499536514, 0.1612419649027288, 0.1564933410845697, 0.1595668150112033, 0.16275033494457603, 0.2283024019561708, 0.19984278501942754, 0.156058233929798, 0.16595723014324903, 0.1667684509884566, 0.16555132693611085, 0.16498665907420218, 0.17004937096498907, 0.17139486013911664, 0.16237166593782604, 0.15784801985137165, 0.15649966103956103, 0.2367911790497601, 0.1581813469529152, 0.1695431659463793, 0.16750274715013802, 0.16631621797569096, 0.15174328908324242, 0.16884535807184875, 0.19771736091934144, 0.15984011115506291, 0.1723744790069759, 0.22835985105484724, 0.24451613589189947, 0.2541562281548977, 0.16565385088324547, 0.16720917588099837, 0.173576092813164, 0.17278577806428075, 0.1702698660083115, 0.16691739298403263, 0.2391029680147767, 0.23687996296212077, 0.2324831911828369, 0.17574805696494877, 0.1748691969551146, 0.15655949106439948, 0.1690518530085683, 0.16250726906582713, 0.15296106692403555, 0.15526171284727752, 0.1659536729566753, 0.15777224698103964, 0.15068600699305534, 0.1502524889074266, 0.15159938903525472, 0.15497747994959354, 0.15897291991859674, 0.24933884805068374, 0.16060163197107613, 0.20753517397679389, 0.17111018812283874, 0.15180298499763012, 0.16861450299620628, 0.2403546581044793, 0.1620779091026634, 0.15737599600106478, 0.1520959788467735, 0.15623934799805284, 0.15448340703733265, 0.15666243806481361, 0.15266723604872823, 0.15643692901358008, 0.15684970002621412, 0.1612849470693618, 0.15610216511413455, 0.199371401919052, 0.16255246894434094, 0.16200057906098664, 0.14966107881627977, 0.15589978196658194, 0.16435190103948116, 0.1519422708079219, 0.22921425895765424, 0.23883025092072785, 0.1704685091972351, 0.15012488001957536, 0.15671753697097301, 0.15873603709042072, 0.1720971700269729, 0.2498951309826225, 0.18023126618936658, 0.17148291994817555, 0.17466909787617624, 0.16479278588667512, 0.1678468999452889, 0.17820204584859312, 0.159265770111233, 0.16144538996741176, 0.23045858088880777, 0.16453967499546707, 0.16962200799025595, 0.17786167794838548, 0.1608762249816209, 0.1637010988779366, 0.1662751140538603, 0.24465269688516855, 0.16412942809984088, 0.16714137489907444, 0.1724553050007671, 0.2492264430038631, 0.16196872107684612, 0.15750985499471426, 0.16604331717826426, 0.16320523107424378, 0.16299082105979323, 0.2276297581847757, 0.15736022219061852, 0.16257912782020867, 0.14675552397966385, 0.15234986413270235, 0.15615019598044455, 0.16051411000080407, 0.16799810994416475, 0.15375057398341596, 0.1485333158634603, 0.16589784203097224, 0.2077079250011593, 0.15844973200000823, 0.1674482999369502, 0.2731495089828968, 0.15703727584332228, 0.2174908600281924, 0.24107305309735239, 0.2365095519926399, 0.15826677111908793, 0.15163964498788118, 0.15823965799063444, 0.14998601889237761, 0.1562378159724176, 0.19575517787598073, 0.16225372115150094, 0.1605229249689728, 0.15789680182933807, 0.1584936089348048, 0.1597387338988483, 0.17107815598137677, 0.21372308093123138, 0.1658071039710194, 0.15958303189836442, 0.15665546106174588, 0.14989533089101315, 0.21843835688196123, 0.15782069880515337, 0.15520984400063753, 0.17343133105896413, 0.17969974200241268, 0.16986859403550625, 0.16668506199494004, 0.16771868988871574, 0.17148983012884855, 0.17740870406851172, 0.17848725686781108, 0.15871080197393894, 0.16465826705098152, 0.15789531799964607, 0.15896411216817796, 0.16003635106608272, 0.1572597420308739, 0.15345951006747782, 0.1659493560437113, 0.16638412605971098, 0.1646283350419253, 0.17556645208969712, 0.16662693489342928, 0.16646774811670184, 0.16325454600155354, 0.16724949120543897, 0.18365923292003572, 0.16824331902898848, 0.16037506889551878, 0.21753326384350657, 0.16051231394521892, 0.16238144296221435, 0.15782367414794862, 0.15497286501340568, 0.16861431999132037, 0.17109942995011806, 0.20415203785523772, 0.17564825201407075, 0.1777721547987312, 0.18301862687803805, 0.1577377321664244, 0.1773533341474831, 0.2558701129164547, 0.22086331201717257, 0.17172535392455757, 0.17485612398013473, 0.1687029239255935, 0.16964234015904367, 0.17534503992646933, 0.17526198690757155, 0.17781666503287852, 0.17307649413123727, 0.1752792471088469, 0.18587489309720695, 0.16957435687072575, 0.17191556096076965, 0.1690655390266329, 0.18198603997007012, 0.17130036000162363, 0.15590016613714397, 0.15900029987096786, 0.23655748181045055, 0.23315438511781394, 0.17635147203691304, 0.17292571999132633, 0.18002695101313293, 0.2123795449733734, 0.26009495998732746, 0.17599648097530007, 0.17995330900885165, 0.17263572593219578, 0.16847493988461792, 0.18172660912387073, 0.1773084180895239, 0.16092871082946658, 0.16352331289090216, 0.1633764859288931, 0.24105715402401984, 0.159238216932863, 0.17404023394919932, 0.16174288396723568, 0.21680873190052807, 0.2369164270348847, 0.16057049995288253, 0.16189072304405272, 0.16253807791508734, 0.16520390403456986, 0.1681902960408479, 0.16543653490953147, 0.16461445786990225, 0.1631458131596446, 0.2172054371330887, 0.2411119060125202, 0.18484078790061176, 0.16874966700561345, 0.18874183506704867, 0.18860954302363098, 0.18583324900828302, 0.16438708687201142, 0.19292748300358653, 0.16894196579232812, 0.17041092016734183, 0.16486420505680144, 0.16492377896793187, 0.16491827787831426, 0.24312516395002604, 0.2531316140666604, 0.16533500398509204, 0.17017372697591782, 0.16200834210030735, 0.1770384490955621, 0.17757686600089073, 0.16510110907256603, 0.1608756510540843, 0.1650377670302987, 0.1662043600808829, 0.16211129701696336, 0.16186309978365898, 0.2029570359736681, 0.16159019898623228, 0.16376745700836182, 0.16179476492106915, 0.17733950214460492, 0.1781631289049983, 0.1789707720745355, 0.18206964014098048, 0.24246819107793272, 0.1665938028600067, 0.16879633604548872, 0.16591295110993087, 0.16640362399630249, 0.17854629992507398, 0.2594529390335083, 0.1792894108220935, 0.2342283099424094, 0.2578847180120647, 0.25558596290647984, 0.17838547891005874, 0.17556679295375943, 0.1772852868307382, 0.17022036714479327, 0.18250627000816166, 0.1681213011033833, 0.25337644992396235, 0.1736277809832245, 0.1730425471905619, 0.24302531289868057, 0.24531265697441995, 0.25794971198774874, 0.18247248395346105, 0.16622974490746856, 0.1672803598921746, 0.1693717970047146, 0.17181696509942412, 0.17738197487778962, 0.2658644018229097, 0.15944401291199028, 0.16444234806112945, 0.32776608993299305, 0.2557977519463748, 0.2553450029809028, 0.16479854099452496, 0.1635668589733541, 0.16276404401287436, 0.16539067402482033, 0.17347690905444324, 0.25894914707168937, 0.16269361996091902, 0.1646962659433484, 0.16171441203914583, 0.1641165390610695, 0.16306748893111944, 0.1651003328152001, 0.24781173188239336, 0.16011532302945852, 0.16073505603708327, 0.1609590849839151, 0.1605986470822245, 0.16600029985420406, 0.17682249494828284, 0.16331057203933597, 0.1647343470249325, 0.16061472985893488, 0.16249029990285635, 0.1859511211514473, 0.1839213480707258, 0.16419379180297256, 0.161979537922889, 0.17254779604263604, 0.16540425387211144, 0.16783905099146068, 0.22166742314584553, 0.16246345289982855, 0.16799852810800076, 0.17194091505371034, 0.1666503727901727, 0.19276583287864923, 0.16321367514319718, 0.16728879907168448, 0.16514823585748672, 0.16447300580330193, 0.17390966578386724, 0.16164435306563973, 0.1721722490619868, 0.2029468910768628, 0.16453367192298174, 0.158928785007447, 0.1596593449357897, 0.16364022390916944, 0.15969611494801939, 0.23688854509964585, 0.16064949403516948, 0.1597041708882898, 0.1594440161716193, 0.16358597809448838, 0.15800791094079614, 0.1740205381065607, 0.2364560840651393, 0.16327866190113127, 0.15889679198153317, 0.16180250118486583, 0.23493694001808763, 0.2436283810529858, 0.16108918515965343, 0.15743803093209863, 0.1571981660090387, 0.16906937398016453, 0.1687598149292171, 0.16586324595846236, 0.16963752009905875, 0.1724253660067916, 0.17034786008298397, 0.15817837789654732, 0.15792709798552096, 0.1588265358004719, 0.15803952305577695, 0.15667806495912373, 0.21817805105820298, 0.15987091301940382, 0.15919282101094723, 0.1577963470481336, 0.16119207604788244, 0.1644921510014683, 0.22361877304501832, 0.16221064212732017, 0.16535757388919592, 0.16275947890244424, 0.16147442790679634, 0.18725599790923297, 0.17646546312607825, 0.16106894402764738, 0.1571851030457765, 0.16182454186491668, 0.3714058268815279, 0.164884784957394, 0.17379621509462595, 0.17782846093177795, 0.16590273403562605, 0.1598537159152329, 0.16027329582720995, 0.1602611339185387, 0.16874718200415373, 0.16318193194456398, 0.16970682493411005, 0.16539778397418559, 0.23078342387452722, 0.24953155498951674, 0.20198461110703647, 0.17506429716013372, 0.17725185095332563, 0.17556763300672174, 0.1708742151968181, 0.25771139399148524, 0.17441275413148105, 0.17390578519552946, 0.15865115099586546, 0.18018583487719297, 0.1613080350216478, 0.16095445188693702, 0.1591844770591706, 0.1584172649309039, 0.16093753091990948, 0.15809673815965652, 0.16427419008687139, 0.20665559801273048, 0.22618633601814508, 0.17423806199803948, 0.17276679119095206, 0.17091843392699957, 0.17151305102743208, 0.1925162160769105, 0.2475918789859861, 0.16221494902856648, 0.15805739001370966, 0.15935998992063105]
[0.0012662246509260217, 0.0012104288447475017, 0.0012227220229746759, 0.0013000314961396908, 0.001322784224640607, 0.0015959462020979371, 0.0012024133881308543, 0.0012037722870360972, 0.0012255654572834109, 0.0012662968843771044, 0.0014155547986485699, 0.0016040735744488562, 0.001240871496527463, 0.0012447317058668118, 0.0012977514181343854, 0.0012591088681604512, 0.0017694660151998203, 0.0012728226508281028, 0.001207957403416666, 0.0015226614177804585, 0.001222510551384022, 0.0012485583478859228, 0.0012800805721894023, 0.0012845555879897619, 0.0012998133879732485, 0.0012719427441181832, 0.0012863031552397004, 0.0013100675816192877, 0.0011950322722857312, 0.00132685286794291, 0.0012029124651277482, 0.0013710151466201673, 0.0013314305043006821, 0.0023769598143796125, 0.0013059672643250043, 0.0012294225658239552, 0.0011802569226643375, 0.0012697190002993095, 0.0012615977122282336, 0.0018672849152728108, 0.0018367700607248747, 0.0017888273487892724, 0.001222870588779103, 0.0012563817517095527, 0.001207492760086591, 0.0012568128981100496, 0.0013263582093135794, 0.0014901029473069565, 0.0012985914873406869, 0.001218981401951507, 0.0012114400859893292, 0.0011990966055137936, 0.001275521706674219, 0.0018304436821346135, 0.0012499377124242541, 0.0012131266750741838, 0.001236952054350413, 0.0012616305034463258, 0.0017697860616757426, 0.0015491688761195934, 0.001209753751393783, 0.0012864901561492174, 0.001292778689832997, 0.0012833436196597742, 0.00127896634941242, 0.001318212178023171, 0.0013286423266598189, 0.0012586950847893491, 0.001223628060863346, 0.0012131756669733413, 0.001835590535269458, 0.0012262119918830636, 0.0013142881081114675, 0.0012984709081406047, 0.001289273007563496, 0.001176304566536763, 0.0013088787447430137, 0.0015326927203049723, 0.0012390706291090148, 0.001336236271371906, 0.0017702314035259476, 0.0018954739216426315, 0.001970203319030215, 0.0012841383789398873, 0.0012961951618682044, 0.0013455511070787907, 0.001339424636157215, 0.0013199214419248955, 0.0012939332789459893, 0.0018535113799595093, 0.001836278782652099, 0.0018021952804871076, 0.001362388038487975, 0.0013555751701946868, 0.0012136394656154998, 0.0013104794806865759, 0.0012597462718281173, 0.001185744704837485, 0.0012035791693587406, 0.0012864625810594984, 0.0012230406742716252, 0.0011681085813415144, 0.0011647479760265628, 0.0011751890622887962, 0.0012013758135627406, 0.0012323482164232305, 0.0019328592872146027, 0.0012449738912486523, 0.00160879979826972, 0.0013264355668437112, 0.0011767673255630242, 0.001307089170513227, 0.0018632144039106924, 0.0012564179000206465, 0.0012199689612485641, 0.0011790385957114226, 0.0012111577364190141, 0.0011975457909870748, 0.0012144375043784, 0.0011834669461141722, 0.0012126893721982953, 0.0012158891474900319, 0.0012502709075144327, 0.0012100943032103454, 0.0015455147435585426, 0.0012600966584832632, 0.0012558184423332298, 0.0011601634016765873, 0.0012085254416014104, 0.001274045744492102, 0.0011778470605265262, 0.001776854720601971, 0.0018513972939591305, 0.0013214613116064736, 0.0011637587598416694, 0.0012148646276819613, 0.001230511915429618, 0.001334086589356379, 0.0019371715580048256, 0.0013971415983671828, 0.0013293249608385702, 0.0013540240145440017, 0.0012774634564858536, 0.0013011387592658055, 0.0013814112081286288, 0.0012346183729552945, 0.0012515146509101687, 0.0017865006270450215, 0.0012755013565540082, 0.0013148992867461702, 0.0013787726972743061, 0.0012471025192373713, 0.0012690007664956325, 0.0012889543725105451, 0.0018965325339935547, 0.001272321148060782, 0.0012956695728610422, 0.001336862829463311, 0.0019319879302625046, 0.0012555714812158615, 0.001221006627866002, 0.0012871574975059244, 0.0012651568300328976, 0.001263494736897622, 0.0017645717688742303, 0.0012198466836482054, 0.001260303316435726, 0.001137639720772588, 0.001181006698703119, 0.001210466635507322, 0.0012442954263628223, 0.0013023109297997267, 0.0011918649146001237, 0.0011514210532051186, 0.0012860297831858312, 0.001610138953497359, 0.0012282924961240947, 0.0012980488367205442, 0.002117438054130983, 0.0012173432235916455, 0.0016859756591332744, 0.001868783357343817, 0.0018334073797879062, 0.0012268741947216119, 0.0011755011239370635, 0.0012266640154312747, 0.0011626823169951753, 0.001211145860251299, 0.0015174819990386103, 0.0012577807841201623, 0.0012443637594494017, 0.001224006215731303, 0.0012286326274015876, 0.0012382847589058007, 0.0013261872556695874, 0.0016567680692343518, 0.0012853263873722433, 0.0012370777666539877, 0.0012143834190833014, 0.0011619793092326601, 0.0016933205959841955, 0.0012234162698073904, 0.00120317708527626, 0.0013444289229377065, 0.0013930212558326564, 0.0013168108064767925, 0.001292132263526667, 0.0013001448828582616, 0.0013293785281306089, 0.0013752612718489282, 0.0013836221462621013, 0.0012303162943716197, 0.0012764206748138102, 0.001223994713175551, 0.0012322799392882012, 0.0012405918687293234, 0.001219067767681193, 0.0011896086051742466, 0.001286429116617917, 0.0012897994268194649, 0.0012761886437358551, 0.0013609802487573421, 0.001291681665840537, 0.0012904476598193941, 0.0012655391162911126, 0.0012965076837630927, 0.0014237149838762459, 0.0013042117754185154, 0.001243217588337355, 0.0016863043708798958, 0.0012442815034513094, 0.0012587708756760802, 0.001223439334480222, 0.00120134003886361, 0.0013070877518707006, 0.001326352170155954, 0.0015825739368623079, 0.001361614356698223, 0.0013780787193700093, 0.0014187490455661864, 0.0012227731175691813, 0.001374832047654908, 0.0019834892474143774, 0.0017121186978075393, 0.0013312042939888184, 0.00135547382930337, 0.0013077746040743683, 0.001315056900457703, 0.0013592638753989871, 0.0013586200535470664, 0.0013784237599447947, 0.0013416782490793587, 0.0013587538535569526, 0.001440890644164395, 0.0013145298982226803, 0.0013326787671377493, 0.0013105855738498675, 0.0014107444958920165, 0.0013279097674544467, 0.0012085284196677827, 0.00123256046411603, 0.0018337789287631826, 0.0018073983342466197, 0.0013670656747047521, 0.0013405094572971033, 0.0013955577597917282, 0.0016463530618090962, 0.0020162399999017633, 0.0013643138060100782, 0.0013949868915414856, 0.001338261441334851, 0.0013060072859272707, 0.0014087334040610135, 0.0013744838611591, 0.0012475093862749347, 0.0012676225805496291, 0.0012664843870456828, 0.001868660108713332, 0.0012344047824252946, 0.00134914910038139, 0.0012538208059475634, 0.00168068784419014, 0.001836561449882827, 0.0012447325577742833, 0.0012549668453027342, 0.001259985100116956, 0.0012806504188726346, 0.0013038007445026968, 0.0012824537589886161, 0.0012760810687589322, 0.0012646962260437567, 0.0016837630785510752, 0.0018690845427327147, 0.0014328743248109438, 0.0013081369535318872, 0.001463115000519757, 0.001462089480803341, 0.0014405678217696359, 0.0012743185028838094, 0.001495561883748733, 0.0013096276418009933, 0.0013210148850181537, 0.001278017093463577, 0.001278478906728154, 0.0012784362626225912, 0.0018846911934110545, 0.001962260574160158, 0.001281666697558853, 0.00131917617810789, 0.001255878620932615, 0.0013723910782601713, 0.0013765648527200832, 0.0012798535587020623, 0.0012470980701867, 0.0012793625351185947, 0.0012884058920998676, 0.0012566767210617316, 0.0012547527115012323, 0.001573310356385024, 0.001252637201443661, 0.001269515170607456, 0.001254222983884257, 0.0013747248228263948, 0.0013811095263953357, 0.00138737032615919, 0.001411392559232407, 0.0018795983804490908, 0.001291424828372145, 0.0013084987290347962, 0.0012861469078289216, 0.001289950573614748, 0.0013840798443804185, 0.00201126309328301, 0.001389840393969717, 0.0018157233328868945, 0.001999106341178796, 0.0019812865341587585, 0.0013828331698454166, 0.0013609828911144142, 0.0013743045490754897, 0.001319537729804599, 0.0014147772868849742, 0.0013032659000262272, 0.0019641585265423437, 0.001345951790567632, 0.0013414150945004798, 0.0018839171542533377, 0.0019016485036776741, 0.001999610170447665, 0.0014145153794841942, 0.0012886026737013066, 0.0012967469759083302, 0.001312959666703214, 0.0013319144581350707, 0.0013750540688200745, 0.0020609643552163544, 0.001236000100092948, 0.001274746884194802, 0.0025408224025813414, 0.0019829283096618197, 0.0019794186277589366, 0.0012775080697249998, 0.0012679601470802643, 0.0012617367752935998, 0.001282098248254421, 0.0013447822407321181, 0.0020073577292379022, 0.001261190852410225, 0.001276715239870918, 0.001253600093326712, 0.0012722212330315463, 0.0012640890614815461, 0.0012798475412031014, 0.0019210211773828944, 0.0012412040544919265, 0.001246008186333979, 0.001247744844836551, 0.0012449507525753836, 0.0012868240298775507, 0.0013707170151029677, 0.0012659734266615191, 0.0012770104420537403, 0.001245075425263061, 0.001259614727929119, 0.001441481559313545, 0.0014257468842691922, 0.0012728200914959114, 0.0012556553327355735, 0.0013375798142840002, 0.0012822035183884608, 0.0013010779146624858, 0.0017183521174096554, 0.001259406611626578, 0.0013023141713798508, 0.0013328753104938785, 0.001291863354962579, 0.0014943087820050327, 0.001265222287931761, 0.0012968123959045308, 0.0012802188826161762, 0.0012749845411108678, 0.0013481369440609863, 0.001253057000508835, 0.0013346685973797427, 0.0015732317137741303, 0.0012754548211083856, 0.0012320060853290465, 0.001237669340587517, 0.0012685288675129414, 0.0012379543794420107, 0.0018363453108499678, 0.0012453449150013139, 0.0012380168285913938, 0.0012360001253613898, 0.0012681083573216154, 0.0012248675266728383, 0.0013489964194307032, 0.001832992899729762, 0.0012657260612490796, 0.0012317580773762262, 0.0012542829549214405, 0.001821216589287501, 0.0018885921011859363, 0.0012487533733306468, 0.001220449852186811, 0.0012185904341785944, 0.001310615302171818, 0.001308215619606334, 0.0012857615965772276, 0.0013150195356516183, 0.0013366307442386944, 0.0013205260471549146, 0.0012261889759422272, 0.0012242410696552011, 0.0012312134558176116, 0.0012251125818277283, 0.0012145586430939825, 0.0016913027213814186, 0.0012393094032511923, 0.0012340528760538545, 0.0012232274964971597, 0.0012495509771153677, 0.0012751329534997542, 0.0017334788608140955, 0.0012574468381962804, 0.0012818416580557823, 0.001261701386840653, 0.0012517397512154754, 0.00145159688301731, 0.001367949326558746, 0.0012485964653305998, 0.0012184891708974922, 0.0012544538129063308, 0.0028791149370661078, 0.0012781766275766976, 0.0013472574813536895, 0.0013785152010215346, 0.0012860677057025276, 0.0012391760923661465, 0.001242428649823333, 0.0012423343714615402, 0.0013081176899546801, 0.001264976216624527, 0.0013155567824349615, 0.0012821533641409735, 0.0017890187897250172, 0.0019343531394536181, 0.0015657721791243137, 0.0013570875748847575, 0.0013740453562273304, 0.0013609894031528817, 0.0013246063193551791, 0.0019977627441200405, 0.0013520368537324112, 0.0013481068619808486, 0.0012298538836888796, 0.0013967894176526586, 0.0012504498838887426, 0.001247708929356101, 0.0012339881942571365, 0.0012280408134178599, 0.0012475777590690658, 0.0012255561097647793, 0.001273443334006755, 0.0016019813799436472, 0.0017533824497530626, 0.0013506826511475928, 0.0013392774510926516, 0.001324949100209299, 0.0013295585350963728, 0.0014923737680380658, 0.0019193168913642334, 0.0012574802250276472, 0.0012252510853775942, 0.0012353487590746593]
[789.7492749558106, 826.1534780333183, 817.8473775806941, 769.2121329132384, 755.9811958535372, 626.5875370269115, 831.6607332146352, 830.7218987921535, 815.9498899525126, 789.7042252393311, 706.4367984585972, 623.4128009643136, 805.8852208294463, 803.3859789115081, 770.5635964070644, 794.2124984482023, 565.1422471016338, 785.6554087479482, 827.8437610229752, 656.7448208267288, 817.988849967704, 800.9237227024388, 781.200825733678, 778.4793506405814, 769.3412064013769, 786.1989107798114, 777.4217111468189, 763.3193997243764, 836.7974850480865, 753.6630655592982, 831.315685047624, 729.386544317329, 751.0718710213403, 420.70547173343795, 765.7159772047235, 813.3899830688426, 847.2731494279893, 787.5758335224338, 792.6456986306675, 535.5369134195034, 544.4339612141508, 559.0254423809137, 817.7480177999752, 795.9364250868056, 828.1623153817408, 795.663381163389, 753.9441404125081, 671.09457222891, 770.0651126612914, 820.3570607386358, 825.4638521255011, 833.9611632638357, 783.9929299262093, 546.3156336139374, 800.0398660350043, 824.3162239745897, 808.4387721277938, 792.6250968634284, 565.0400472999196, 645.5074171802569, 826.6145063388965, 777.3087071208123, 773.5276021058032, 779.214533567486, 781.8814001317688, 758.6032178064299, 752.6480076199142, 794.4735878327169, 817.2418008250297, 824.282935458822, 544.7838070559693, 815.5196708395623, 760.8681793803372, 770.1366228004201, 775.6309130289077, 850.1199676068308, 764.0127124200042, 652.4464993877062, 807.0564958182209, 748.3706447912133, 564.8978986635304, 527.5725445662648, 507.5618289447538, 778.7322740291773, 771.4887614290284, 743.1899054143051, 746.5892242127051, 757.6208463904178, 772.8373759847793, 539.516514876669, 544.5796190901476, 554.8788251901949, 734.0052699742097, 737.6942437329983, 823.967931442348, 763.079479486461, 793.810644542588, 843.3518580519889, 830.8551904672742, 777.3253685905306, 817.634295437921, 856.0847989418501, 858.5548295275119, 850.9269121790511, 832.3790014004439, 811.4589583311133, 517.368236071171, 803.2296958428948, 621.5813807756005, 753.9001704994437, 849.7856613426534, 765.0587446970823, 536.70688563866, 795.913525255862, 819.6929854482205, 848.1486557245464, 825.6562873112329, 835.0411379056761, 823.4264804855824, 844.9750145396348, 824.6134772231542, 822.4433963115031, 799.8266567587524, 826.3818756497149, 647.033620460652, 793.5899149226117, 796.2934499847481, 861.94754855641, 827.4546530645689, 784.9011735435369, 849.0066609776788, 562.7922127821545, 540.1325816251706, 756.7380075503839, 859.2846168015535, 823.1369793917396, 812.6699038512459, 749.5765327214954, 516.2165404854188, 715.7470661303652, 752.2615082539155, 738.5393384893343, 782.8012573845973, 768.5575369104143, 723.8974131060372, 809.9668868577659, 799.0317966096093, 559.7535118999983, 784.0054382236615, 760.5145200698868, 725.2827111944548, 801.8586961170765, 788.0215886405783, 775.8226523195405, 527.2780625040406, 785.9650855636233, 771.8017162291176, 748.0198999933704, 517.6015772852821, 796.4500746955698, 818.9963733020325, 776.9057026336416, 790.4158411522761, 791.4556118020678, 566.7097352679425, 819.7751515865025, 793.4597861950471, 879.0129086921166, 846.7352480710862, 826.1276855275628, 803.6676651003067, 767.8657816023881, 839.0212579883726, 868.4920231538064, 777.5869681048437, 621.0644105143316, 814.1383287413405, 770.3870391552064, 472.2688335788929, 821.4610149548483, 593.1283732257912, 535.1075051424599, 545.4325159941719, 815.0794957643627, 850.7010156236483, 815.2191532645691, 860.080165822415, 825.6643834727811, 658.9864002561761, 795.0511032011957, 803.62353243273, 816.9893152074668, 813.9129449255157, 807.5686895182664, 754.0413284209276, 603.5847856858646, 778.0125031467124, 808.356618278551, 823.4631536346795, 860.6005219321618, 590.5556232951727, 817.3832772041161, 831.1328500495763, 743.8102401240401, 717.8641358220093, 759.4105357287891, 773.9145815233039, 769.1450492821864, 752.2311958853543, 727.134560152036, 722.7406721564347, 812.7991188727179, 783.440772883022, 816.9969929082328, 811.5039189696032, 806.0668663129723, 820.2989419547323, 840.6126146452395, 777.3455894943106, 775.3143467166166, 783.5832146826245, 734.7645205821767, 774.184558351898, 774.9248816026804, 790.1770772053871, 771.3027948261101, 702.3877751692773, 766.7466425681556, 804.364424523122, 593.0127545587814, 803.6766577549076, 794.425752393504, 817.3678676310092, 832.4037888106479, 765.0595750505677, 753.9475732771779, 631.882009874781, 734.4223385136, 725.6479517056554, 704.8462891482867, 817.8132031459406, 727.3615724231406, 504.16204741395643, 584.0716541911224, 751.1994999682593, 737.7493968392871, 764.657760507432, 760.4233700092765, 735.692324425578, 736.0409537524592, 725.4663109115658, 745.3351805369032, 735.9684738940721, 694.0151940398798, 760.7282279026573, 750.3683743290534, 763.0177074683364, 708.845579700595, 753.0632159720931, 827.4526140435274, 811.3192245843954, 545.322003822164, 553.2814659900808, 731.4937522778281, 745.985039162887, 716.5593777711065, 607.4031282823074, 495.97270168666563, 732.9692007768285, 716.8526142170282, 747.2381472805107, 765.6925124196346, 709.8575196110627, 727.545828844219, 801.597175141104, 788.8783422952355, 789.5873097438582, 535.1427984881376, 810.10703639308, 741.2079211388206, 797.5621358781483, 594.9944860116854, 544.4958022307395, 803.3854290660706, 796.8337998274138, 793.6601789236845, 780.8532174457936, 766.9883639937832, 779.755209878781, 783.6492715721909, 790.7037116163589, 593.9077847344935, 535.0212776025313, 697.8979123880539, 764.4459529256957, 683.4732742434873, 683.9526671449362, 694.1707185792687, 784.733171288794, 668.6450162085092, 763.5758196313001, 756.9937412069795, 782.4621478965372, 782.1795062377454, 782.2055969756321, 530.5909018390039, 509.6163135357276, 780.2340514149786, 758.0488615510869, 796.2552935708072, 728.6552760658684, 726.4459774808332, 781.3393909019793, 801.8615567661751, 781.639271551204, 776.1529236490697, 795.749601500637, 796.9697860254577, 635.6025026732092, 798.315744452985, 787.7022844252468, 797.3063903701216, 727.4183046640773, 724.0555371520586, 720.7880845833067, 708.5201019791795, 532.0285495037886, 774.33852751655, 764.2345978720531, 777.516156134954, 775.2235011592439, 722.5016707382576, 497.1999950377886, 719.5070774592762, 550.7446987587356, 500.2235145781882, 504.72255413808364, 723.1530323443047, 734.763094032115, 727.6407552261334, 757.8411571058926, 706.8250312399192, 767.3031266910888, 509.1238749248897, 742.9686612908084, 745.4813980398685, 530.8089040658133, 525.8595361161961, 500.09747638767215, 706.955904830566, 776.0343978859354, 771.1604642837372, 761.6380193238971, 750.7989675254272, 727.2441300130793, 485.209750216676, 809.0614231542534, 784.4694600933685, 393.573356006328, 504.3046665517352, 505.1988427188759, 782.7739203363805, 788.6683207691527, 792.5583367159169, 779.9714268087502, 743.6148171138742, 498.1673099092568, 792.901405912459, 783.2600166197631, 797.7025570780498, 786.0268120326236, 791.0835007368653, 781.3430645496768, 520.5564684936744, 805.6692985983995, 802.5629453865891, 801.4459079019441, 803.2446246820101, 777.107030007169, 729.5451861921189, 789.9059956077325, 783.0789530520668, 803.1641936782414, 793.893543658432, 693.7306922443149, 701.3867686006404, 785.6569885102355, 796.3968884848342, 747.6189378166528, 779.9073904093254, 768.5934783232495, 581.9529011943481, 794.024734163065, 767.8638703136144, 750.2577263806197, 774.0756761607861, 669.2057304637001, 790.3749479743075, 771.121561729441, 781.1164274943843, 784.3232351105377, 741.7644063574913, 798.0482927703408, 749.2496653950103, 635.634275132322, 784.0340429549578, 811.6843024626116, 807.9702447224746, 788.3147365503672, 807.7842096658965, 544.559889739442, 802.9903908179085, 807.7434626941157, 809.0614066140272, 788.5761451112193, 816.414818928496, 741.291811895257, 545.5558502967633, 790.0603697874022, 811.8477307898844, 797.2682687556995, 549.0835114736251, 529.4949604904376, 800.798637550682, 819.3700037803213, 820.6202608787603, 763.0004001501447, 764.3999849970589, 777.7491586792282, 760.4449765869678, 748.1497820623381, 757.2739683208137, 815.5349783923646, 816.8325869688745, 812.2068478661405, 816.2515142144031, 823.3443528528084, 591.2602086888515, 806.9010025879006, 810.3380490451202, 817.5094190276174, 800.2874779134943, 784.23194793561, 576.8746435883095, 795.262248569037, 780.1275560951403, 792.5805665507248, 798.8881067561936, 688.8964916495177, 731.0212305273249, 800.899271915865, 820.6884590229379, 797.1596799432498, 347.3289611074109, 782.3644858034259, 742.2486153093956, 725.4181885400758, 777.5640392538586, 806.9878092068002, 804.8751935511106, 804.9362739787624, 764.4572102947749, 790.5286967911604, 760.1344262382213, 779.9378982014272, 558.9656216823223, 516.9686856053918, 638.662516381705, 736.8721212298469, 727.7780136353475, 734.7595783504191, 754.9412873757101, 500.55994033489316, 739.6248092198198, 741.7809583215416, 813.1047218394389, 715.9275316393263, 799.712177900425, 801.4689776373285, 810.3805244279522, 814.30518356863, 801.5532440608699, 815.9561133369323, 785.2724760461901, 624.226980737552, 570.3262286792221, 740.3663615212358, 746.671273517036, 754.7459746506734, 752.1293524150971, 670.0734235731306, 521.0187043626802, 795.2411338937866, 816.1592443656746, 809.4880030065779]
Elapsed: 0.177817372661178~0.03000069459279426
Time per graph: 0.0013784292454354886~0.0002325635239751493
Speed: 741.4120887374113~96.15881133730204
Total Time: 0.1608
best val loss: 0.15719004899494407 test_score: 0.9225

Testing...
Test loss: 0.4271 score: 0.8992 time: 0.16s
test Score 0.8992
Epoch Time List: [1.174430944956839, 0.5852799040731043, 0.5726274740882218, 0.596158524043858, 0.5947930130641907, 0.6464870891068131, 0.5632938661146909, 0.5552454390563071, 0.5553074898198247, 0.5782426872756332, 0.6011429447680712, 0.7159881961997598, 0.5745558459311724, 0.5822800379246473, 0.589645538944751, 0.5750419630203396, 0.6736483769491315, 0.5935419527813792, 0.6047917427495122, 0.6200700139161199, 0.683421814115718, 0.5644348310306668, 0.5814527131151408, 0.5826187012717128, 0.581500536063686, 0.6003381358459592, 0.664601783035323, 0.5858229252044111, 0.5762148797512054, 0.5772493011318147, 0.627289857249707, 0.581787560833618, 0.5904565136879683, 0.7296749709639698, 0.5832832038868219, 0.5594859078992158, 0.6199563671834767, 0.5772430312354118, 0.5941667417064309, 0.7734850202687085, 0.8317531920038164, 0.8377597220242023, 0.6488544389139861, 0.5627321780193597, 0.5656407272908837, 0.5610004463233054, 0.5827676590997726, 0.6104064502287656, 0.672223161906004, 0.5746427110861987, 0.5560728977434337, 0.5416904569137841, 0.5634482488967478, 0.7482338191475719, 0.5641835490241647, 0.5568894068710506, 0.5505773040931672, 0.5658931189682335, 0.6387114480603486, 0.7928399608936161, 0.5653226249851286, 0.5710747041739523, 0.5867916950955987, 0.5742431553080678, 0.5791171891614795, 0.6210316480137408, 0.6866930711548775, 0.5735467351041734, 0.5729501324240118, 0.5589964131359011, 0.7157724720891565, 0.7188308159820735, 0.5880429849494249, 0.5842143178451806, 0.5833091989625245, 0.6388984601944685, 0.5885702171362936, 0.6731570037081838, 0.6581929307430983, 0.591248988872394, 0.750486524309963, 0.8468268688302487, 0.6764443798456341, 0.5969315578695387, 0.5719197031576186, 0.6057318940293044, 0.6890778630040586, 0.5901211081072688, 0.590884359087795, 0.7182504727970809, 0.8326724080834538, 0.8301889651920646, 0.7500028398353606, 0.6168864488136023, 0.5741614811122417, 0.5850393252912909, 0.6503353968728334, 0.5524739490356296, 0.5550073431804776, 0.6063376090023667, 0.5872085599694401, 0.6585303090978414, 0.5423224542755634, 0.546111777657643, 0.5566659849137068, 0.5628894320689142, 0.8393095480278134, 0.7411913895048201, 0.6038278471678495, 0.6016907801385969, 0.5527906476054341, 0.5798057108186185, 0.7279177780728787, 0.5713812841568142, 0.560779799008742, 0.5562034631147981, 0.5679180051665753, 0.5516897898633033, 0.566275330260396, 0.6405951271299273, 0.5652273760642856, 0.5556964240968227, 0.5546125818509609, 0.5496446909382939, 0.6111565819010139, 0.663951565278694, 0.5577509871218354, 0.5563793089240789, 0.5634628888219595, 0.5654441742226481, 0.5515737859532237, 0.684443267993629, 0.849644014146179, 0.7447495080996305, 0.5537510300055146, 0.5699918840546161, 0.5653237069491297, 0.5904127699322999, 0.7673012930899858, 0.8112328259740025, 0.6018884719815105, 0.5986649298574775, 0.6027025380171835, 0.7094111850019544, 0.6092476462945342, 0.580604684073478, 0.5712263602763414, 0.8155610118992627, 0.7178701930679381, 0.57501858798787, 0.5942466252017766, 0.589085791260004, 0.5916812997311354, 0.5921617140993476, 0.7099170526489615, 0.5910309941973537, 0.5848323977552354, 0.5974777899682522, 0.8071651849895716, 0.6028048421721905, 0.5646605638321489, 0.5680330998729914, 0.5534874191507697, 0.5717170101124793, 0.6409393490757793, 0.5775724344421178, 0.5873006889596581, 0.5666992699261755, 0.5635876129381359, 0.5589002310298383, 0.6348543029744178, 0.6070808491203934, 0.7428367231041193, 0.5492018610239029, 0.5709978132508695, 0.6604118573013693, 0.5472487777005881, 0.5573712368495762, 0.7384398041758686, 0.6844120419118553, 0.6201598728075624, 0.8351393102202564, 0.8347626957111061, 0.6470110339578241, 0.5618482790887356, 0.5598051177803427, 0.5478909111116081, 0.5629439868498594, 0.597299343207851, 0.6592463289853185, 0.5565982079133391, 0.5638285069726408, 0.5607192236930132, 0.5686886250041425, 0.5738048702478409, 0.681736865080893, 0.5996408858336508, 0.5717498671729118, 0.5603761710226536, 0.5627412863541394, 0.6436594552360475, 0.6523305680602789, 0.5628755099605769, 0.588419410167262, 0.6660358882509172, 0.5931780119426548, 0.5872438896913081, 0.591933943098411, 0.5874331970699131, 0.5896183832082897, 0.6398205270525068, 0.5615904049482197, 0.5615403351839632, 0.5652641160413623, 0.5751321711577475, 0.5682526440359652, 0.6396167175844312, 0.5517310439608991, 0.5835759001784027, 0.59523516590707, 0.5759977560956031, 0.6096068560145795, 0.6648427848704159, 0.5776922665536404, 0.5937667214311659, 0.5880797856952995, 0.5904208940919489, 0.6406956440769136, 0.5433058408088982, 0.6146573959849775, 0.6886862339451909, 0.5344292162917554, 0.5368188321590424, 0.5270770508795977, 0.5645450472366065, 0.5889667868614197, 0.6455133208073676, 0.6542260840069503, 0.6001218287274241, 0.6172158450353891, 0.5488508050329983, 0.6343234707601368, 0.8802544348873198, 0.7740342279430479, 0.5748501340858638, 0.5847032200545073, 0.5797163301613182, 0.5759009779430926, 0.7390030024107546, 0.589034748962149, 0.5939622248988599, 0.6576621350832283, 0.5838218871504068, 0.5978263991419226, 0.5808141089510173, 0.5802766147535294, 0.570961769670248, 0.6801857349928468, 0.5716670700348914, 0.552764531224966, 0.7150045949965715, 0.6911122759338468, 0.8484068040270358, 0.5862162061966956, 0.5953522410709411, 0.631847423966974, 0.6783251601736993, 0.7649996839463711, 0.5946180780883878, 0.6035524711478502, 0.6076895811129361, 0.6054856749251485, 0.6910822289064527, 0.5968553791753948, 0.5720568411052227, 0.559992522932589, 0.5769110308028758, 0.7154007479548454, 0.6264191090594977, 0.5913167383987457, 0.5729789638426155, 0.6544094432611018, 0.7145213629119098, 0.6367471050471067, 0.5570375649258494, 0.5684492869768292, 0.5767700800206512, 0.7407465982250869, 0.5717445269692689, 0.5632367429789156, 0.563935806741938, 0.6286893347278237, 0.8302098461426795, 0.7609569809865206, 0.6438250900246203, 0.6089727736543864, 0.616295964922756, 0.619110117899254, 0.5955979058053344, 0.6383948042057455, 0.6765195068437606, 0.5747545198537409, 0.5687824350316077, 0.57233378989622, 0.5791168073192239, 0.7332005409989506, 0.8528269652742893, 0.5855759030673653, 0.590694245416671, 0.5572972558438778, 0.5921347478870302, 0.5892388159409165, 0.577608365798369, 0.6487989507149905, 0.559218998067081, 0.5708124071825296, 0.5523093300871551, 0.5578115789685398, 0.6481247222982347, 0.5579767671879381, 0.5642095517832786, 0.5508108467329293, 0.5880312041845173, 0.6012300136499107, 0.6081357600633055, 0.6164670661091805, 0.6686402962077409, 0.5888689099811018, 0.5867930280510336, 0.5816097550559789, 0.5941988413687795, 0.5920088039711118, 0.7387392302043736, 0.6300773716066033, 0.6874719853512943, 0.8892441517673433, 0.8904990691225976, 0.7536794459447265, 0.6185891921631992, 0.6144868438132107, 0.6162908973637968, 0.6237793029285967, 0.6127991650719196, 0.7083994757849723, 0.5973125640302896, 0.5986358947120607, 0.7443693701643497, 0.8605712431017309, 0.8875442219432443, 0.736613588174805, 0.5809903799090534, 0.5837551020085812, 0.5866202351171523, 0.6470450491178781, 0.632001704769209, 0.7600011758040637, 0.5556966490112245, 0.560342279728502, 0.8025135239586234, 0.8664398689288646, 0.9584616338834167, 0.5537707803305238, 0.5558553030714393, 0.5581333001609892, 0.5556705601047724, 0.5805496531538665, 0.6992692281492054, 0.5636589750647545, 0.5591154261492193, 0.5618804630357772, 0.5623843788634986, 0.5600483238231391, 0.5623968399595469, 0.6675791691523045, 0.5683310760650784, 0.5571770437527448, 0.5567876782733947, 0.5645518761593848, 0.5665836380794644, 0.5868213009089231, 0.6351091081742197, 0.5587206066120416, 0.5703109491150826, 0.5602754740975797, 0.5898055918514729, 0.7032482351642102, 0.5759307348635048, 0.5644732739310712, 0.5876954980194569, 0.589728879975155, 0.579666590783745, 0.6389348797965795, 0.5713789069559425, 0.5900928522460163, 0.6049478042405099, 0.5943476175889373, 0.5947700499091297, 0.6291321718599647, 0.5663507278077304, 0.5701790419407189, 0.5669245643075556, 0.594210194889456, 0.5633581848815084, 0.5776345981284976, 0.6687591508962214, 0.5710011560004205, 0.5557771369349211, 0.5645009472500533, 0.5616963291540742, 0.5587433611508459, 0.6644068437162787, 0.7048904341645539, 0.5621525370515883, 0.5519593399949372, 0.5601261211559176, 0.5513729290105402, 0.5998347869608551, 0.6409898619167507, 0.6731760317925364, 0.5494971149601042, 0.5597308638971299, 0.6927095600403845, 0.8402517354115844, 0.7080070623196661, 0.5448595690540969, 0.5524842471349984, 0.5733240630943328, 0.5805915088858455, 0.5667148109059781, 0.6354539119638503, 0.6478794242721051, 0.5796315560583025, 0.5440781272482127, 0.5479035179596394, 0.545561287086457, 0.5453276745975018, 0.5394222207833081, 0.6396877376828343, 0.579335980117321, 0.5487766072619706, 0.5482910322025418, 0.5524583559017628, 0.562305532163009, 0.6514977591577917, 0.677121093031019, 0.5773346819914877, 0.5610425360500813, 0.5580393958371133, 0.6778482778463513, 0.5751045921351761, 0.5778490260709077, 0.6203715861774981, 0.5513037669006735, 0.785445804009214, 0.5853580092079937, 0.6711696072015911, 0.6644036290235817, 0.5642590757925063, 0.5700399670749903, 0.5997654667589813, 0.6071145271416754, 0.5690939268097281, 0.6542322349268943, 0.5758553768973798, 0.5867305181454867, 0.6532206002157182, 0.8538250129204243, 0.8120289882645011, 0.588288159109652, 0.5953481469769031, 0.5869355178438127, 0.5750065827742219, 0.6831301162019372, 0.5918251250404865, 0.5885955209378153, 0.5504455359186977, 0.5693725892342627, 0.6287965795490891, 0.5552675032522529, 0.5546416260767728, 0.5524798142723739, 0.5619580531492829, 0.5520848000887781, 0.5550459381192923, 0.5962758967652917, 0.6373126900289208, 0.5822645146399736, 0.5875494400970638, 0.5791808092035353, 0.5890926602296531, 0.6090007480233908, 0.802119918866083, 0.6352038809563965, 0.5490628308616579, 0.5478809520136565]
Total Epoch List: [228, 267]
Total Time List: [0.16900427895598114, 0.16075794212520123]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cf4ee00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.9144;  Loss pred: 1.9144; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4961 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.14s
Epoch 2/1000, LR 0.000020
Train loss: 1.9212;  Loss pred: 1.9212; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4961 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 1.8685;  Loss pred: 1.8685; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 1.8741;  Loss pred: 1.8741; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000140
Train loss: 1.8327;  Loss pred: 1.8327; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000170
Train loss: 1.7861;  Loss pred: 1.7861; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4961 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000200
Train loss: 1.7324;  Loss pred: 1.7324; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4961 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000230
Train loss: 1.6816;  Loss pred: 1.6816; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000260
Train loss: 1.6273;  Loss pred: 1.6273; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000290
Train loss: 1.5531;  Loss pred: 1.5531; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000290
Train loss: 1.4987;  Loss pred: 1.4987; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.4961 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000290
Train loss: 1.4450;  Loss pred: 1.4450; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000290
Train loss: 1.4094;  Loss pred: 1.4094; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000290
Train loss: 1.3613;  Loss pred: 1.3613; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000290
Train loss: 1.3236;  Loss pred: 1.3236; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000290
Train loss: 1.2863;  Loss pred: 1.2863; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000290
Train loss: 1.2588;  Loss pred: 1.2588; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000290
Train loss: 1.2304;  Loss pred: 1.2304; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000290
Train loss: 1.2043;  Loss pred: 1.2043; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000290
Train loss: 1.1837;  Loss pred: 1.1837; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 1.9144,   Val_Loss: 0.6953,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4961,   Val_Loss: 0.6953,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6948


[0.1633429799694568, 0.15614532097242773, 0.1577311409637332, 0.16770406300202012, 0.1706391649786383, 0.2058770600706339, 0.1551113270688802, 0.15528662502765656, 0.15809794398956, 0.16335229808464646, 0.18260656902566552, 0.20692549110390246, 0.16007242305204272, 0.16057039005681872, 0.1674099329393357, 0.1624250439926982, 0.2282611159607768, 0.16419412195682526, 0.1558265050407499, 0.19642332289367914, 0.15770386112853885, 0.16106402687728405, 0.16513039381243289, 0.16570767085067928, 0.16767592704854906, 0.16408061399124563, 0.16593310702592134, 0.1689987180288881, 0.15415916312485933, 0.17116401996463537, 0.1551757080014795, 0.17686095391400158, 0.171754535054788, 0.30662781605497, 0.16846977709792554, 0.1585955109912902, 0.15225314302369952, 0.16379375103861094, 0.16274610487744212, 0.24087975407019258, 0.23694333783350885, 0.23075872799381614, 0.15775030595250428, 0.1620732459705323, 0.15576656605117023, 0.1621288638561964, 0.17110020900145173, 0.19222328020259738, 0.1675183018669486, 0.1572486008517444, 0.15627577109262347, 0.15468346211127937, 0.16454230016097426, 0.23612723499536514, 0.1612419649027288, 0.1564933410845697, 0.1595668150112033, 0.16275033494457603, 0.2283024019561708, 0.19984278501942754, 0.156058233929798, 0.16595723014324903, 0.1667684509884566, 0.16555132693611085, 0.16498665907420218, 0.17004937096498907, 0.17139486013911664, 0.16237166593782604, 0.15784801985137165, 0.15649966103956103, 0.2367911790497601, 0.1581813469529152, 0.1695431659463793, 0.16750274715013802, 0.16631621797569096, 0.15174328908324242, 0.16884535807184875, 0.19771736091934144, 0.15984011115506291, 0.1723744790069759, 0.22835985105484724, 0.24451613589189947, 0.2541562281548977, 0.16565385088324547, 0.16720917588099837, 0.173576092813164, 0.17278577806428075, 0.1702698660083115, 0.16691739298403263, 0.2391029680147767, 0.23687996296212077, 0.2324831911828369, 0.17574805696494877, 0.1748691969551146, 0.15655949106439948, 0.1690518530085683, 0.16250726906582713, 0.15296106692403555, 0.15526171284727752, 0.1659536729566753, 0.15777224698103964, 0.15068600699305534, 0.1502524889074266, 0.15159938903525472, 0.15497747994959354, 0.15897291991859674, 0.24933884805068374, 0.16060163197107613, 0.20753517397679389, 0.17111018812283874, 0.15180298499763012, 0.16861450299620628, 0.2403546581044793, 0.1620779091026634, 0.15737599600106478, 0.1520959788467735, 0.15623934799805284, 0.15448340703733265, 0.15666243806481361, 0.15266723604872823, 0.15643692901358008, 0.15684970002621412, 0.1612849470693618, 0.15610216511413455, 0.199371401919052, 0.16255246894434094, 0.16200057906098664, 0.14966107881627977, 0.15589978196658194, 0.16435190103948116, 0.1519422708079219, 0.22921425895765424, 0.23883025092072785, 0.1704685091972351, 0.15012488001957536, 0.15671753697097301, 0.15873603709042072, 0.1720971700269729, 0.2498951309826225, 0.18023126618936658, 0.17148291994817555, 0.17466909787617624, 0.16479278588667512, 0.1678468999452889, 0.17820204584859312, 0.159265770111233, 0.16144538996741176, 0.23045858088880777, 0.16453967499546707, 0.16962200799025595, 0.17786167794838548, 0.1608762249816209, 0.1637010988779366, 0.1662751140538603, 0.24465269688516855, 0.16412942809984088, 0.16714137489907444, 0.1724553050007671, 0.2492264430038631, 0.16196872107684612, 0.15750985499471426, 0.16604331717826426, 0.16320523107424378, 0.16299082105979323, 0.2276297581847757, 0.15736022219061852, 0.16257912782020867, 0.14675552397966385, 0.15234986413270235, 0.15615019598044455, 0.16051411000080407, 0.16799810994416475, 0.15375057398341596, 0.1485333158634603, 0.16589784203097224, 0.2077079250011593, 0.15844973200000823, 0.1674482999369502, 0.2731495089828968, 0.15703727584332228, 0.2174908600281924, 0.24107305309735239, 0.2365095519926399, 0.15826677111908793, 0.15163964498788118, 0.15823965799063444, 0.14998601889237761, 0.1562378159724176, 0.19575517787598073, 0.16225372115150094, 0.1605229249689728, 0.15789680182933807, 0.1584936089348048, 0.1597387338988483, 0.17107815598137677, 0.21372308093123138, 0.1658071039710194, 0.15958303189836442, 0.15665546106174588, 0.14989533089101315, 0.21843835688196123, 0.15782069880515337, 0.15520984400063753, 0.17343133105896413, 0.17969974200241268, 0.16986859403550625, 0.16668506199494004, 0.16771868988871574, 0.17148983012884855, 0.17740870406851172, 0.17848725686781108, 0.15871080197393894, 0.16465826705098152, 0.15789531799964607, 0.15896411216817796, 0.16003635106608272, 0.1572597420308739, 0.15345951006747782, 0.1659493560437113, 0.16638412605971098, 0.1646283350419253, 0.17556645208969712, 0.16662693489342928, 0.16646774811670184, 0.16325454600155354, 0.16724949120543897, 0.18365923292003572, 0.16824331902898848, 0.16037506889551878, 0.21753326384350657, 0.16051231394521892, 0.16238144296221435, 0.15782367414794862, 0.15497286501340568, 0.16861431999132037, 0.17109942995011806, 0.20415203785523772, 0.17564825201407075, 0.1777721547987312, 0.18301862687803805, 0.1577377321664244, 0.1773533341474831, 0.2558701129164547, 0.22086331201717257, 0.17172535392455757, 0.17485612398013473, 0.1687029239255935, 0.16964234015904367, 0.17534503992646933, 0.17526198690757155, 0.17781666503287852, 0.17307649413123727, 0.1752792471088469, 0.18587489309720695, 0.16957435687072575, 0.17191556096076965, 0.1690655390266329, 0.18198603997007012, 0.17130036000162363, 0.15590016613714397, 0.15900029987096786, 0.23655748181045055, 0.23315438511781394, 0.17635147203691304, 0.17292571999132633, 0.18002695101313293, 0.2123795449733734, 0.26009495998732746, 0.17599648097530007, 0.17995330900885165, 0.17263572593219578, 0.16847493988461792, 0.18172660912387073, 0.1773084180895239, 0.16092871082946658, 0.16352331289090216, 0.1633764859288931, 0.24105715402401984, 0.159238216932863, 0.17404023394919932, 0.16174288396723568, 0.21680873190052807, 0.2369164270348847, 0.16057049995288253, 0.16189072304405272, 0.16253807791508734, 0.16520390403456986, 0.1681902960408479, 0.16543653490953147, 0.16461445786990225, 0.1631458131596446, 0.2172054371330887, 0.2411119060125202, 0.18484078790061176, 0.16874966700561345, 0.18874183506704867, 0.18860954302363098, 0.18583324900828302, 0.16438708687201142, 0.19292748300358653, 0.16894196579232812, 0.17041092016734183, 0.16486420505680144, 0.16492377896793187, 0.16491827787831426, 0.24312516395002604, 0.2531316140666604, 0.16533500398509204, 0.17017372697591782, 0.16200834210030735, 0.1770384490955621, 0.17757686600089073, 0.16510110907256603, 0.1608756510540843, 0.1650377670302987, 0.1662043600808829, 0.16211129701696336, 0.16186309978365898, 0.2029570359736681, 0.16159019898623228, 0.16376745700836182, 0.16179476492106915, 0.17733950214460492, 0.1781631289049983, 0.1789707720745355, 0.18206964014098048, 0.24246819107793272, 0.1665938028600067, 0.16879633604548872, 0.16591295110993087, 0.16640362399630249, 0.17854629992507398, 0.2594529390335083, 0.1792894108220935, 0.2342283099424094, 0.2578847180120647, 0.25558596290647984, 0.17838547891005874, 0.17556679295375943, 0.1772852868307382, 0.17022036714479327, 0.18250627000816166, 0.1681213011033833, 0.25337644992396235, 0.1736277809832245, 0.1730425471905619, 0.24302531289868057, 0.24531265697441995, 0.25794971198774874, 0.18247248395346105, 0.16622974490746856, 0.1672803598921746, 0.1693717970047146, 0.17181696509942412, 0.17738197487778962, 0.2658644018229097, 0.15944401291199028, 0.16444234806112945, 0.32776608993299305, 0.2557977519463748, 0.2553450029809028, 0.16479854099452496, 0.1635668589733541, 0.16276404401287436, 0.16539067402482033, 0.17347690905444324, 0.25894914707168937, 0.16269361996091902, 0.1646962659433484, 0.16171441203914583, 0.1641165390610695, 0.16306748893111944, 0.1651003328152001, 0.24781173188239336, 0.16011532302945852, 0.16073505603708327, 0.1609590849839151, 0.1605986470822245, 0.16600029985420406, 0.17682249494828284, 0.16331057203933597, 0.1647343470249325, 0.16061472985893488, 0.16249029990285635, 0.1859511211514473, 0.1839213480707258, 0.16419379180297256, 0.161979537922889, 0.17254779604263604, 0.16540425387211144, 0.16783905099146068, 0.22166742314584553, 0.16246345289982855, 0.16799852810800076, 0.17194091505371034, 0.1666503727901727, 0.19276583287864923, 0.16321367514319718, 0.16728879907168448, 0.16514823585748672, 0.16447300580330193, 0.17390966578386724, 0.16164435306563973, 0.1721722490619868, 0.2029468910768628, 0.16453367192298174, 0.158928785007447, 0.1596593449357897, 0.16364022390916944, 0.15969611494801939, 0.23688854509964585, 0.16064949403516948, 0.1597041708882898, 0.1594440161716193, 0.16358597809448838, 0.15800791094079614, 0.1740205381065607, 0.2364560840651393, 0.16327866190113127, 0.15889679198153317, 0.16180250118486583, 0.23493694001808763, 0.2436283810529858, 0.16108918515965343, 0.15743803093209863, 0.1571981660090387, 0.16906937398016453, 0.1687598149292171, 0.16586324595846236, 0.16963752009905875, 0.1724253660067916, 0.17034786008298397, 0.15817837789654732, 0.15792709798552096, 0.1588265358004719, 0.15803952305577695, 0.15667806495912373, 0.21817805105820298, 0.15987091301940382, 0.15919282101094723, 0.1577963470481336, 0.16119207604788244, 0.1644921510014683, 0.22361877304501832, 0.16221064212732017, 0.16535757388919592, 0.16275947890244424, 0.16147442790679634, 0.18725599790923297, 0.17646546312607825, 0.16106894402764738, 0.1571851030457765, 0.16182454186491668, 0.3714058268815279, 0.164884784957394, 0.17379621509462595, 0.17782846093177795, 0.16590273403562605, 0.1598537159152329, 0.16027329582720995, 0.1602611339185387, 0.16874718200415373, 0.16318193194456398, 0.16970682493411005, 0.16539778397418559, 0.23078342387452722, 0.24953155498951674, 0.20198461110703647, 0.17506429716013372, 0.17725185095332563, 0.17556763300672174, 0.1708742151968181, 0.25771139399148524, 0.17441275413148105, 0.17390578519552946, 0.15865115099586546, 0.18018583487719297, 0.1613080350216478, 0.16095445188693702, 0.1591844770591706, 0.1584172649309039, 0.16093753091990948, 0.15809673815965652, 0.16427419008687139, 0.20665559801273048, 0.22618633601814508, 0.17423806199803948, 0.17276679119095206, 0.17091843392699957, 0.17151305102743208, 0.1925162160769105, 0.2475918789859861, 0.16221494902856648, 0.15805739001370966, 0.15935998992063105, 0.14904603105969727, 0.15119345602579415, 0.1549008118454367, 0.20056381914764643, 0.21100108604878187, 0.1521552219055593, 0.15195246296934783, 0.14713891386054456, 0.15287831518799067, 0.1501455691177398, 0.1635288749821484, 0.15398170007392764, 0.16291036503389478, 0.16227175202220678, 0.1663028548937291, 0.1787113428581506, 0.20667787501588464, 0.17153385188430548, 0.17696183500811458, 0.15023547899909317, 0.20682018506340683]
[0.0012662246509260217, 0.0012104288447475017, 0.0012227220229746759, 0.0013000314961396908, 0.001322784224640607, 0.0015959462020979371, 0.0012024133881308543, 0.0012037722870360972, 0.0012255654572834109, 0.0012662968843771044, 0.0014155547986485699, 0.0016040735744488562, 0.001240871496527463, 0.0012447317058668118, 0.0012977514181343854, 0.0012591088681604512, 0.0017694660151998203, 0.0012728226508281028, 0.001207957403416666, 0.0015226614177804585, 0.001222510551384022, 0.0012485583478859228, 0.0012800805721894023, 0.0012845555879897619, 0.0012998133879732485, 0.0012719427441181832, 0.0012863031552397004, 0.0013100675816192877, 0.0011950322722857312, 0.00132685286794291, 0.0012029124651277482, 0.0013710151466201673, 0.0013314305043006821, 0.0023769598143796125, 0.0013059672643250043, 0.0012294225658239552, 0.0011802569226643375, 0.0012697190002993095, 0.0012615977122282336, 0.0018672849152728108, 0.0018367700607248747, 0.0017888273487892724, 0.001222870588779103, 0.0012563817517095527, 0.001207492760086591, 0.0012568128981100496, 0.0013263582093135794, 0.0014901029473069565, 0.0012985914873406869, 0.001218981401951507, 0.0012114400859893292, 0.0011990966055137936, 0.001275521706674219, 0.0018304436821346135, 0.0012499377124242541, 0.0012131266750741838, 0.001236952054350413, 0.0012616305034463258, 0.0017697860616757426, 0.0015491688761195934, 0.001209753751393783, 0.0012864901561492174, 0.001292778689832997, 0.0012833436196597742, 0.00127896634941242, 0.001318212178023171, 0.0013286423266598189, 0.0012586950847893491, 0.001223628060863346, 0.0012131756669733413, 0.001835590535269458, 0.0012262119918830636, 0.0013142881081114675, 0.0012984709081406047, 0.001289273007563496, 0.001176304566536763, 0.0013088787447430137, 0.0015326927203049723, 0.0012390706291090148, 0.001336236271371906, 0.0017702314035259476, 0.0018954739216426315, 0.001970203319030215, 0.0012841383789398873, 0.0012961951618682044, 0.0013455511070787907, 0.001339424636157215, 0.0013199214419248955, 0.0012939332789459893, 0.0018535113799595093, 0.001836278782652099, 0.0018021952804871076, 0.001362388038487975, 0.0013555751701946868, 0.0012136394656154998, 0.0013104794806865759, 0.0012597462718281173, 0.001185744704837485, 0.0012035791693587406, 0.0012864625810594984, 0.0012230406742716252, 0.0011681085813415144, 0.0011647479760265628, 0.0011751890622887962, 0.0012013758135627406, 0.0012323482164232305, 0.0019328592872146027, 0.0012449738912486523, 0.00160879979826972, 0.0013264355668437112, 0.0011767673255630242, 0.001307089170513227, 0.0018632144039106924, 0.0012564179000206465, 0.0012199689612485641, 0.0011790385957114226, 0.0012111577364190141, 0.0011975457909870748, 0.0012144375043784, 0.0011834669461141722, 0.0012126893721982953, 0.0012158891474900319, 0.0012502709075144327, 0.0012100943032103454, 0.0015455147435585426, 0.0012600966584832632, 0.0012558184423332298, 0.0011601634016765873, 0.0012085254416014104, 0.001274045744492102, 0.0011778470605265262, 0.001776854720601971, 0.0018513972939591305, 0.0013214613116064736, 0.0011637587598416694, 0.0012148646276819613, 0.001230511915429618, 0.001334086589356379, 0.0019371715580048256, 0.0013971415983671828, 0.0013293249608385702, 0.0013540240145440017, 0.0012774634564858536, 0.0013011387592658055, 0.0013814112081286288, 0.0012346183729552945, 0.0012515146509101687, 0.0017865006270450215, 0.0012755013565540082, 0.0013148992867461702, 0.0013787726972743061, 0.0012471025192373713, 0.0012690007664956325, 0.0012889543725105451, 0.0018965325339935547, 0.001272321148060782, 0.0012956695728610422, 0.001336862829463311, 0.0019319879302625046, 0.0012555714812158615, 0.001221006627866002, 0.0012871574975059244, 0.0012651568300328976, 0.001263494736897622, 0.0017645717688742303, 0.0012198466836482054, 0.001260303316435726, 0.001137639720772588, 0.001181006698703119, 0.001210466635507322, 0.0012442954263628223, 0.0013023109297997267, 0.0011918649146001237, 0.0011514210532051186, 0.0012860297831858312, 0.001610138953497359, 0.0012282924961240947, 0.0012980488367205442, 0.002117438054130983, 0.0012173432235916455, 0.0016859756591332744, 0.001868783357343817, 0.0018334073797879062, 0.0012268741947216119, 0.0011755011239370635, 0.0012266640154312747, 0.0011626823169951753, 0.001211145860251299, 0.0015174819990386103, 0.0012577807841201623, 0.0012443637594494017, 0.001224006215731303, 0.0012286326274015876, 0.0012382847589058007, 0.0013261872556695874, 0.0016567680692343518, 0.0012853263873722433, 0.0012370777666539877, 0.0012143834190833014, 0.0011619793092326601, 0.0016933205959841955, 0.0012234162698073904, 0.00120317708527626, 0.0013444289229377065, 0.0013930212558326564, 0.0013168108064767925, 0.001292132263526667, 0.0013001448828582616, 0.0013293785281306089, 0.0013752612718489282, 0.0013836221462621013, 0.0012303162943716197, 0.0012764206748138102, 0.001223994713175551, 0.0012322799392882012, 0.0012405918687293234, 0.001219067767681193, 0.0011896086051742466, 0.001286429116617917, 0.0012897994268194649, 0.0012761886437358551, 0.0013609802487573421, 0.001291681665840537, 0.0012904476598193941, 0.0012655391162911126, 0.0012965076837630927, 0.0014237149838762459, 0.0013042117754185154, 0.001243217588337355, 0.0016863043708798958, 0.0012442815034513094, 0.0012587708756760802, 0.001223439334480222, 0.00120134003886361, 0.0013070877518707006, 0.001326352170155954, 0.0015825739368623079, 0.001361614356698223, 0.0013780787193700093, 0.0014187490455661864, 0.0012227731175691813, 0.001374832047654908, 0.0019834892474143774, 0.0017121186978075393, 0.0013312042939888184, 0.00135547382930337, 0.0013077746040743683, 0.001315056900457703, 0.0013592638753989871, 0.0013586200535470664, 0.0013784237599447947, 0.0013416782490793587, 0.0013587538535569526, 0.001440890644164395, 0.0013145298982226803, 0.0013326787671377493, 0.0013105855738498675, 0.0014107444958920165, 0.0013279097674544467, 0.0012085284196677827, 0.00123256046411603, 0.0018337789287631826, 0.0018073983342466197, 0.0013670656747047521, 0.0013405094572971033, 0.0013955577597917282, 0.0016463530618090962, 0.0020162399999017633, 0.0013643138060100782, 0.0013949868915414856, 0.001338261441334851, 0.0013060072859272707, 0.0014087334040610135, 0.0013744838611591, 0.0012475093862749347, 0.0012676225805496291, 0.0012664843870456828, 0.001868660108713332, 0.0012344047824252946, 0.00134914910038139, 0.0012538208059475634, 0.00168068784419014, 0.001836561449882827, 0.0012447325577742833, 0.0012549668453027342, 0.001259985100116956, 0.0012806504188726346, 0.0013038007445026968, 0.0012824537589886161, 0.0012760810687589322, 0.0012646962260437567, 0.0016837630785510752, 0.0018690845427327147, 0.0014328743248109438, 0.0013081369535318872, 0.001463115000519757, 0.001462089480803341, 0.0014405678217696359, 0.0012743185028838094, 0.001495561883748733, 0.0013096276418009933, 0.0013210148850181537, 0.001278017093463577, 0.001278478906728154, 0.0012784362626225912, 0.0018846911934110545, 0.001962260574160158, 0.001281666697558853, 0.00131917617810789, 0.001255878620932615, 0.0013723910782601713, 0.0013765648527200832, 0.0012798535587020623, 0.0012470980701867, 0.0012793625351185947, 0.0012884058920998676, 0.0012566767210617316, 0.0012547527115012323, 0.001573310356385024, 0.001252637201443661, 0.001269515170607456, 0.001254222983884257, 0.0013747248228263948, 0.0013811095263953357, 0.00138737032615919, 0.001411392559232407, 0.0018795983804490908, 0.001291424828372145, 0.0013084987290347962, 0.0012861469078289216, 0.001289950573614748, 0.0013840798443804185, 0.00201126309328301, 0.001389840393969717, 0.0018157233328868945, 0.001999106341178796, 0.0019812865341587585, 0.0013828331698454166, 0.0013609828911144142, 0.0013743045490754897, 0.001319537729804599, 0.0014147772868849742, 0.0013032659000262272, 0.0019641585265423437, 0.001345951790567632, 0.0013414150945004798, 0.0018839171542533377, 0.0019016485036776741, 0.001999610170447665, 0.0014145153794841942, 0.0012886026737013066, 0.0012967469759083302, 0.001312959666703214, 0.0013319144581350707, 0.0013750540688200745, 0.0020609643552163544, 0.001236000100092948, 0.001274746884194802, 0.0025408224025813414, 0.0019829283096618197, 0.0019794186277589366, 0.0012775080697249998, 0.0012679601470802643, 0.0012617367752935998, 0.001282098248254421, 0.0013447822407321181, 0.0020073577292379022, 0.001261190852410225, 0.001276715239870918, 0.001253600093326712, 0.0012722212330315463, 0.0012640890614815461, 0.0012798475412031014, 0.0019210211773828944, 0.0012412040544919265, 0.001246008186333979, 0.001247744844836551, 0.0012449507525753836, 0.0012868240298775507, 0.0013707170151029677, 0.0012659734266615191, 0.0012770104420537403, 0.001245075425263061, 0.001259614727929119, 0.001441481559313545, 0.0014257468842691922, 0.0012728200914959114, 0.0012556553327355735, 0.0013375798142840002, 0.0012822035183884608, 0.0013010779146624858, 0.0017183521174096554, 0.001259406611626578, 0.0013023141713798508, 0.0013328753104938785, 0.001291863354962579, 0.0014943087820050327, 0.001265222287931761, 0.0012968123959045308, 0.0012802188826161762, 0.0012749845411108678, 0.0013481369440609863, 0.001253057000508835, 0.0013346685973797427, 0.0015732317137741303, 0.0012754548211083856, 0.0012320060853290465, 0.001237669340587517, 0.0012685288675129414, 0.0012379543794420107, 0.0018363453108499678, 0.0012453449150013139, 0.0012380168285913938, 0.0012360001253613898, 0.0012681083573216154, 0.0012248675266728383, 0.0013489964194307032, 0.001832992899729762, 0.0012657260612490796, 0.0012317580773762262, 0.0012542829549214405, 0.001821216589287501, 0.0018885921011859363, 0.0012487533733306468, 0.001220449852186811, 0.0012185904341785944, 0.001310615302171818, 0.001308215619606334, 0.0012857615965772276, 0.0013150195356516183, 0.0013366307442386944, 0.0013205260471549146, 0.0012261889759422272, 0.0012242410696552011, 0.0012312134558176116, 0.0012251125818277283, 0.0012145586430939825, 0.0016913027213814186, 0.0012393094032511923, 0.0012340528760538545, 0.0012232274964971597, 0.0012495509771153677, 0.0012751329534997542, 0.0017334788608140955, 0.0012574468381962804, 0.0012818416580557823, 0.001261701386840653, 0.0012517397512154754, 0.00145159688301731, 0.001367949326558746, 0.0012485964653305998, 0.0012184891708974922, 0.0012544538129063308, 0.0028791149370661078, 0.0012781766275766976, 0.0013472574813536895, 0.0013785152010215346, 0.0012860677057025276, 0.0012391760923661465, 0.001242428649823333, 0.0012423343714615402, 0.0013081176899546801, 0.001264976216624527, 0.0013155567824349615, 0.0012821533641409735, 0.0017890187897250172, 0.0019343531394536181, 0.0015657721791243137, 0.0013570875748847575, 0.0013740453562273304, 0.0013609894031528817, 0.0013246063193551791, 0.0019977627441200405, 0.0013520368537324112, 0.0013481068619808486, 0.0012298538836888796, 0.0013967894176526586, 0.0012504498838887426, 0.001247708929356101, 0.0012339881942571365, 0.0012280408134178599, 0.0012475777590690658, 0.0012255561097647793, 0.001273443334006755, 0.0016019813799436472, 0.0017533824497530626, 0.0013506826511475928, 0.0013392774510926516, 0.001324949100209299, 0.0013295585350963728, 0.0014923737680380658, 0.0019193168913642334, 0.0012574802250276472, 0.0012252510853775942, 0.0012353487590746593, 0.001164422117653885, 0.0011811988752015168, 0.0012101625925424742, 0.0015669048370909877, 0.0016484459847561084, 0.001188712671137182, 0.00118712861694803, 0.0011495227645355044, 0.001194361837406177, 0.0011730122587323422, 0.0012775693357980344, 0.0012029820318275597, 0.001272737226827303, 0.0012677480626734905, 0.0012992410538572585, 0.0013961823660793016, 0.0016146708985615987, 0.0013401082178461365, 0.0013825143360008951, 0.0011737146796804154, 0.0016157826958078658]
[789.7492749558106, 826.1534780333183, 817.8473775806941, 769.2121329132384, 755.9811958535372, 626.5875370269115, 831.6607332146352, 830.7218987921535, 815.9498899525126, 789.7042252393311, 706.4367984585972, 623.4128009643136, 805.8852208294463, 803.3859789115081, 770.5635964070644, 794.2124984482023, 565.1422471016338, 785.6554087479482, 827.8437610229752, 656.7448208267288, 817.988849967704, 800.9237227024388, 781.200825733678, 778.4793506405814, 769.3412064013769, 786.1989107798114, 777.4217111468189, 763.3193997243764, 836.7974850480865, 753.6630655592982, 831.315685047624, 729.386544317329, 751.0718710213403, 420.70547173343795, 765.7159772047235, 813.3899830688426, 847.2731494279893, 787.5758335224338, 792.6456986306675, 535.5369134195034, 544.4339612141508, 559.0254423809137, 817.7480177999752, 795.9364250868056, 828.1623153817408, 795.663381163389, 753.9441404125081, 671.09457222891, 770.0651126612914, 820.3570607386358, 825.4638521255011, 833.9611632638357, 783.9929299262093, 546.3156336139374, 800.0398660350043, 824.3162239745897, 808.4387721277938, 792.6250968634284, 565.0400472999196, 645.5074171802569, 826.6145063388965, 777.3087071208123, 773.5276021058032, 779.214533567486, 781.8814001317688, 758.6032178064299, 752.6480076199142, 794.4735878327169, 817.2418008250297, 824.282935458822, 544.7838070559693, 815.5196708395623, 760.8681793803372, 770.1366228004201, 775.6309130289077, 850.1199676068308, 764.0127124200042, 652.4464993877062, 807.0564958182209, 748.3706447912133, 564.8978986635304, 527.5725445662648, 507.5618289447538, 778.7322740291773, 771.4887614290284, 743.1899054143051, 746.5892242127051, 757.6208463904178, 772.8373759847793, 539.516514876669, 544.5796190901476, 554.8788251901949, 734.0052699742097, 737.6942437329983, 823.967931442348, 763.079479486461, 793.810644542588, 843.3518580519889, 830.8551904672742, 777.3253685905306, 817.634295437921, 856.0847989418501, 858.5548295275119, 850.9269121790511, 832.3790014004439, 811.4589583311133, 517.368236071171, 803.2296958428948, 621.5813807756005, 753.9001704994437, 849.7856613426534, 765.0587446970823, 536.70688563866, 795.913525255862, 819.6929854482205, 848.1486557245464, 825.6562873112329, 835.0411379056761, 823.4264804855824, 844.9750145396348, 824.6134772231542, 822.4433963115031, 799.8266567587524, 826.3818756497149, 647.033620460652, 793.5899149226117, 796.2934499847481, 861.94754855641, 827.4546530645689, 784.9011735435369, 849.0066609776788, 562.7922127821545, 540.1325816251706, 756.7380075503839, 859.2846168015535, 823.1369793917396, 812.6699038512459, 749.5765327214954, 516.2165404854188, 715.7470661303652, 752.2615082539155, 738.5393384893343, 782.8012573845973, 768.5575369104143, 723.8974131060372, 809.9668868577659, 799.0317966096093, 559.7535118999983, 784.0054382236615, 760.5145200698868, 725.2827111944548, 801.8586961170765, 788.0215886405783, 775.8226523195405, 527.2780625040406, 785.9650855636233, 771.8017162291176, 748.0198999933704, 517.6015772852821, 796.4500746955698, 818.9963733020325, 776.9057026336416, 790.4158411522761, 791.4556118020678, 566.7097352679425, 819.7751515865025, 793.4597861950471, 879.0129086921166, 846.7352480710862, 826.1276855275628, 803.6676651003067, 767.8657816023881, 839.0212579883726, 868.4920231538064, 777.5869681048437, 621.0644105143316, 814.1383287413405, 770.3870391552064, 472.2688335788929, 821.4610149548483, 593.1283732257912, 535.1075051424599, 545.4325159941719, 815.0794957643627, 850.7010156236483, 815.2191532645691, 860.080165822415, 825.6643834727811, 658.9864002561761, 795.0511032011957, 803.62353243273, 816.9893152074668, 813.9129449255157, 807.5686895182664, 754.0413284209276, 603.5847856858646, 778.0125031467124, 808.356618278551, 823.4631536346795, 860.6005219321618, 590.5556232951727, 817.3832772041161, 831.1328500495763, 743.8102401240401, 717.8641358220093, 759.4105357287891, 773.9145815233039, 769.1450492821864, 752.2311958853543, 727.134560152036, 722.7406721564347, 812.7991188727179, 783.440772883022, 816.9969929082328, 811.5039189696032, 806.0668663129723, 820.2989419547323, 840.6126146452395, 777.3455894943106, 775.3143467166166, 783.5832146826245, 734.7645205821767, 774.184558351898, 774.9248816026804, 790.1770772053871, 771.3027948261101, 702.3877751692773, 766.7466425681556, 804.364424523122, 593.0127545587814, 803.6766577549076, 794.425752393504, 817.3678676310092, 832.4037888106479, 765.0595750505677, 753.9475732771779, 631.882009874781, 734.4223385136, 725.6479517056554, 704.8462891482867, 817.8132031459406, 727.3615724231406, 504.16204741395643, 584.0716541911224, 751.1994999682593, 737.7493968392871, 764.657760507432, 760.4233700092765, 735.692324425578, 736.0409537524592, 725.4663109115658, 745.3351805369032, 735.9684738940721, 694.0151940398798, 760.7282279026573, 750.3683743290534, 763.0177074683364, 708.845579700595, 753.0632159720931, 827.4526140435274, 811.3192245843954, 545.322003822164, 553.2814659900808, 731.4937522778281, 745.985039162887, 716.5593777711065, 607.4031282823074, 495.97270168666563, 732.9692007768285, 716.8526142170282, 747.2381472805107, 765.6925124196346, 709.8575196110627, 727.545828844219, 801.597175141104, 788.8783422952355, 789.5873097438582, 535.1427984881376, 810.10703639308, 741.2079211388206, 797.5621358781483, 594.9944860116854, 544.4958022307395, 803.3854290660706, 796.8337998274138, 793.6601789236845, 780.8532174457936, 766.9883639937832, 779.755209878781, 783.6492715721909, 790.7037116163589, 593.9077847344935, 535.0212776025313, 697.8979123880539, 764.4459529256957, 683.4732742434873, 683.9526671449362, 694.1707185792687, 784.733171288794, 668.6450162085092, 763.5758196313001, 756.9937412069795, 782.4621478965372, 782.1795062377454, 782.2055969756321, 530.5909018390039, 509.6163135357276, 780.2340514149786, 758.0488615510869, 796.2552935708072, 728.6552760658684, 726.4459774808332, 781.3393909019793, 801.8615567661751, 781.639271551204, 776.1529236490697, 795.749601500637, 796.9697860254577, 635.6025026732092, 798.315744452985, 787.7022844252468, 797.3063903701216, 727.4183046640773, 724.0555371520586, 720.7880845833067, 708.5201019791795, 532.0285495037886, 774.33852751655, 764.2345978720531, 777.516156134954, 775.2235011592439, 722.5016707382576, 497.1999950377886, 719.5070774592762, 550.7446987587356, 500.2235145781882, 504.72255413808364, 723.1530323443047, 734.763094032115, 727.6407552261334, 757.8411571058926, 706.8250312399192, 767.3031266910888, 509.1238749248897, 742.9686612908084, 745.4813980398685, 530.8089040658133, 525.8595361161961, 500.09747638767215, 706.955904830566, 776.0343978859354, 771.1604642837372, 761.6380193238971, 750.7989675254272, 727.2441300130793, 485.209750216676, 809.0614231542534, 784.4694600933685, 393.573356006328, 504.3046665517352, 505.1988427188759, 782.7739203363805, 788.6683207691527, 792.5583367159169, 779.9714268087502, 743.6148171138742, 498.1673099092568, 792.901405912459, 783.2600166197631, 797.7025570780498, 786.0268120326236, 791.0835007368653, 781.3430645496768, 520.5564684936744, 805.6692985983995, 802.5629453865891, 801.4459079019441, 803.2446246820101, 777.107030007169, 729.5451861921189, 789.9059956077325, 783.0789530520668, 803.1641936782414, 793.893543658432, 693.7306922443149, 701.3867686006404, 785.6569885102355, 796.3968884848342, 747.6189378166528, 779.9073904093254, 768.5934783232495, 581.9529011943481, 794.024734163065, 767.8638703136144, 750.2577263806197, 774.0756761607861, 669.2057304637001, 790.3749479743075, 771.121561729441, 781.1164274943843, 784.3232351105377, 741.7644063574913, 798.0482927703408, 749.2496653950103, 635.634275132322, 784.0340429549578, 811.6843024626116, 807.9702447224746, 788.3147365503672, 807.7842096658965, 544.559889739442, 802.9903908179085, 807.7434626941157, 809.0614066140272, 788.5761451112193, 816.414818928496, 741.291811895257, 545.5558502967633, 790.0603697874022, 811.8477307898844, 797.2682687556995, 549.0835114736251, 529.4949604904376, 800.798637550682, 819.3700037803213, 820.6202608787603, 763.0004001501447, 764.3999849970589, 777.7491586792282, 760.4449765869678, 748.1497820623381, 757.2739683208137, 815.5349783923646, 816.8325869688745, 812.2068478661405, 816.2515142144031, 823.3443528528084, 591.2602086888515, 806.9010025879006, 810.3380490451202, 817.5094190276174, 800.2874779134943, 784.23194793561, 576.8746435883095, 795.262248569037, 780.1275560951403, 792.5805665507248, 798.8881067561936, 688.8964916495177, 731.0212305273249, 800.899271915865, 820.6884590229379, 797.1596799432498, 347.3289611074109, 782.3644858034259, 742.2486153093956, 725.4181885400758, 777.5640392538586, 806.9878092068002, 804.8751935511106, 804.9362739787624, 764.4572102947749, 790.5286967911604, 760.1344262382213, 779.9378982014272, 558.9656216823223, 516.9686856053918, 638.662516381705, 736.8721212298469, 727.7780136353475, 734.7595783504191, 754.9412873757101, 500.55994033489316, 739.6248092198198, 741.7809583215416, 813.1047218394389, 715.9275316393263, 799.712177900425, 801.4689776373285, 810.3805244279522, 814.30518356863, 801.5532440608699, 815.9561133369323, 785.2724760461901, 624.226980737552, 570.3262286792221, 740.3663615212358, 746.671273517036, 754.7459746506734, 752.1293524150971, 670.0734235731306, 521.0187043626802, 795.2411338937866, 816.1592443656746, 809.4880030065779, 858.7950922942207, 846.597487514028, 826.3352430181005, 638.2008507016509, 606.6319486640336, 841.2461852899658, 842.368708599481, 869.9262257795102, 837.2672072072588, 852.5060096820181, 782.736382268716, 831.2676112716404, 785.708140629165, 788.8002588551782, 769.6801121171047, 716.2388125615398, 619.32125047329, 746.2083932350113, 723.3198050536183, 851.9958191817833, 618.8951042702038]
Elapsed: 0.17740409160908238~0.029747447172343993
Time per graph: 0.0013756387595959898~0.00023050945843600958
Speed: 742.738431344353~95.97635851848842
Total Time: 0.2084
best val loss: 0.6952934445336808 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.19s
test Score 0.5000
Epoch Time List: [1.174430944956839, 0.5852799040731043, 0.5726274740882218, 0.596158524043858, 0.5947930130641907, 0.6464870891068131, 0.5632938661146909, 0.5552454390563071, 0.5553074898198247, 0.5782426872756332, 0.6011429447680712, 0.7159881961997598, 0.5745558459311724, 0.5822800379246473, 0.589645538944751, 0.5750419630203396, 0.6736483769491315, 0.5935419527813792, 0.6047917427495122, 0.6200700139161199, 0.683421814115718, 0.5644348310306668, 0.5814527131151408, 0.5826187012717128, 0.581500536063686, 0.6003381358459592, 0.664601783035323, 0.5858229252044111, 0.5762148797512054, 0.5772493011318147, 0.627289857249707, 0.581787560833618, 0.5904565136879683, 0.7296749709639698, 0.5832832038868219, 0.5594859078992158, 0.6199563671834767, 0.5772430312354118, 0.5941667417064309, 0.7734850202687085, 0.8317531920038164, 0.8377597220242023, 0.6488544389139861, 0.5627321780193597, 0.5656407272908837, 0.5610004463233054, 0.5827676590997726, 0.6104064502287656, 0.672223161906004, 0.5746427110861987, 0.5560728977434337, 0.5416904569137841, 0.5634482488967478, 0.7482338191475719, 0.5641835490241647, 0.5568894068710506, 0.5505773040931672, 0.5658931189682335, 0.6387114480603486, 0.7928399608936161, 0.5653226249851286, 0.5710747041739523, 0.5867916950955987, 0.5742431553080678, 0.5791171891614795, 0.6210316480137408, 0.6866930711548775, 0.5735467351041734, 0.5729501324240118, 0.5589964131359011, 0.7157724720891565, 0.7188308159820735, 0.5880429849494249, 0.5842143178451806, 0.5833091989625245, 0.6388984601944685, 0.5885702171362936, 0.6731570037081838, 0.6581929307430983, 0.591248988872394, 0.750486524309963, 0.8468268688302487, 0.6764443798456341, 0.5969315578695387, 0.5719197031576186, 0.6057318940293044, 0.6890778630040586, 0.5901211081072688, 0.590884359087795, 0.7182504727970809, 0.8326724080834538, 0.8301889651920646, 0.7500028398353606, 0.6168864488136023, 0.5741614811122417, 0.5850393252912909, 0.6503353968728334, 0.5524739490356296, 0.5550073431804776, 0.6063376090023667, 0.5872085599694401, 0.6585303090978414, 0.5423224542755634, 0.546111777657643, 0.5566659849137068, 0.5628894320689142, 0.8393095480278134, 0.7411913895048201, 0.6038278471678495, 0.6016907801385969, 0.5527906476054341, 0.5798057108186185, 0.7279177780728787, 0.5713812841568142, 0.560779799008742, 0.5562034631147981, 0.5679180051665753, 0.5516897898633033, 0.566275330260396, 0.6405951271299273, 0.5652273760642856, 0.5556964240968227, 0.5546125818509609, 0.5496446909382939, 0.6111565819010139, 0.663951565278694, 0.5577509871218354, 0.5563793089240789, 0.5634628888219595, 0.5654441742226481, 0.5515737859532237, 0.684443267993629, 0.849644014146179, 0.7447495080996305, 0.5537510300055146, 0.5699918840546161, 0.5653237069491297, 0.5904127699322999, 0.7673012930899858, 0.8112328259740025, 0.6018884719815105, 0.5986649298574775, 0.6027025380171835, 0.7094111850019544, 0.6092476462945342, 0.580604684073478, 0.5712263602763414, 0.8155610118992627, 0.7178701930679381, 0.57501858798787, 0.5942466252017766, 0.589085791260004, 0.5916812997311354, 0.5921617140993476, 0.7099170526489615, 0.5910309941973537, 0.5848323977552354, 0.5974777899682522, 0.8071651849895716, 0.6028048421721905, 0.5646605638321489, 0.5680330998729914, 0.5534874191507697, 0.5717170101124793, 0.6409393490757793, 0.5775724344421178, 0.5873006889596581, 0.5666992699261755, 0.5635876129381359, 0.5589002310298383, 0.6348543029744178, 0.6070808491203934, 0.7428367231041193, 0.5492018610239029, 0.5709978132508695, 0.6604118573013693, 0.5472487777005881, 0.5573712368495762, 0.7384398041758686, 0.6844120419118553, 0.6201598728075624, 0.8351393102202564, 0.8347626957111061, 0.6470110339578241, 0.5618482790887356, 0.5598051177803427, 0.5478909111116081, 0.5629439868498594, 0.597299343207851, 0.6592463289853185, 0.5565982079133391, 0.5638285069726408, 0.5607192236930132, 0.5686886250041425, 0.5738048702478409, 0.681736865080893, 0.5996408858336508, 0.5717498671729118, 0.5603761710226536, 0.5627412863541394, 0.6436594552360475, 0.6523305680602789, 0.5628755099605769, 0.588419410167262, 0.6660358882509172, 0.5931780119426548, 0.5872438896913081, 0.591933943098411, 0.5874331970699131, 0.5896183832082897, 0.6398205270525068, 0.5615904049482197, 0.5615403351839632, 0.5652641160413623, 0.5751321711577475, 0.5682526440359652, 0.6396167175844312, 0.5517310439608991, 0.5835759001784027, 0.59523516590707, 0.5759977560956031, 0.6096068560145795, 0.6648427848704159, 0.5776922665536404, 0.5937667214311659, 0.5880797856952995, 0.5904208940919489, 0.6406956440769136, 0.5433058408088982, 0.6146573959849775, 0.6886862339451909, 0.5344292162917554, 0.5368188321590424, 0.5270770508795977, 0.5645450472366065, 0.5889667868614197, 0.6455133208073676, 0.6542260840069503, 0.6001218287274241, 0.6172158450353891, 0.5488508050329983, 0.6343234707601368, 0.8802544348873198, 0.7740342279430479, 0.5748501340858638, 0.5847032200545073, 0.5797163301613182, 0.5759009779430926, 0.7390030024107546, 0.589034748962149, 0.5939622248988599, 0.6576621350832283, 0.5838218871504068, 0.5978263991419226, 0.5808141089510173, 0.5802766147535294, 0.570961769670248, 0.6801857349928468, 0.5716670700348914, 0.552764531224966, 0.7150045949965715, 0.6911122759338468, 0.8484068040270358, 0.5862162061966956, 0.5953522410709411, 0.631847423966974, 0.6783251601736993, 0.7649996839463711, 0.5946180780883878, 0.6035524711478502, 0.6076895811129361, 0.6054856749251485, 0.6910822289064527, 0.5968553791753948, 0.5720568411052227, 0.559992522932589, 0.5769110308028758, 0.7154007479548454, 0.6264191090594977, 0.5913167383987457, 0.5729789638426155, 0.6544094432611018, 0.7145213629119098, 0.6367471050471067, 0.5570375649258494, 0.5684492869768292, 0.5767700800206512, 0.7407465982250869, 0.5717445269692689, 0.5632367429789156, 0.563935806741938, 0.6286893347278237, 0.8302098461426795, 0.7609569809865206, 0.6438250900246203, 0.6089727736543864, 0.616295964922756, 0.619110117899254, 0.5955979058053344, 0.6383948042057455, 0.6765195068437606, 0.5747545198537409, 0.5687824350316077, 0.57233378989622, 0.5791168073192239, 0.7332005409989506, 0.8528269652742893, 0.5855759030673653, 0.590694245416671, 0.5572972558438778, 0.5921347478870302, 0.5892388159409165, 0.577608365798369, 0.6487989507149905, 0.559218998067081, 0.5708124071825296, 0.5523093300871551, 0.5578115789685398, 0.6481247222982347, 0.5579767671879381, 0.5642095517832786, 0.5508108467329293, 0.5880312041845173, 0.6012300136499107, 0.6081357600633055, 0.6164670661091805, 0.6686402962077409, 0.5888689099811018, 0.5867930280510336, 0.5816097550559789, 0.5941988413687795, 0.5920088039711118, 0.7387392302043736, 0.6300773716066033, 0.6874719853512943, 0.8892441517673433, 0.8904990691225976, 0.7536794459447265, 0.6185891921631992, 0.6144868438132107, 0.6162908973637968, 0.6237793029285967, 0.6127991650719196, 0.7083994757849723, 0.5973125640302896, 0.5986358947120607, 0.7443693701643497, 0.8605712431017309, 0.8875442219432443, 0.736613588174805, 0.5809903799090534, 0.5837551020085812, 0.5866202351171523, 0.6470450491178781, 0.632001704769209, 0.7600011758040637, 0.5556966490112245, 0.560342279728502, 0.8025135239586234, 0.8664398689288646, 0.9584616338834167, 0.5537707803305238, 0.5558553030714393, 0.5581333001609892, 0.5556705601047724, 0.5805496531538665, 0.6992692281492054, 0.5636589750647545, 0.5591154261492193, 0.5618804630357772, 0.5623843788634986, 0.5600483238231391, 0.5623968399595469, 0.6675791691523045, 0.5683310760650784, 0.5571770437527448, 0.5567876782733947, 0.5645518761593848, 0.5665836380794644, 0.5868213009089231, 0.6351091081742197, 0.5587206066120416, 0.5703109491150826, 0.5602754740975797, 0.5898055918514729, 0.7032482351642102, 0.5759307348635048, 0.5644732739310712, 0.5876954980194569, 0.589728879975155, 0.579666590783745, 0.6389348797965795, 0.5713789069559425, 0.5900928522460163, 0.6049478042405099, 0.5943476175889373, 0.5947700499091297, 0.6291321718599647, 0.5663507278077304, 0.5701790419407189, 0.5669245643075556, 0.594210194889456, 0.5633581848815084, 0.5776345981284976, 0.6687591508962214, 0.5710011560004205, 0.5557771369349211, 0.5645009472500533, 0.5616963291540742, 0.5587433611508459, 0.6644068437162787, 0.7048904341645539, 0.5621525370515883, 0.5519593399949372, 0.5601261211559176, 0.5513729290105402, 0.5998347869608551, 0.6409898619167507, 0.6731760317925364, 0.5494971149601042, 0.5597308638971299, 0.6927095600403845, 0.8402517354115844, 0.7080070623196661, 0.5448595690540969, 0.5524842471349984, 0.5733240630943328, 0.5805915088858455, 0.5667148109059781, 0.6354539119638503, 0.6478794242721051, 0.5796315560583025, 0.5440781272482127, 0.5479035179596394, 0.545561287086457, 0.5453276745975018, 0.5394222207833081, 0.6396877376828343, 0.579335980117321, 0.5487766072619706, 0.5482910322025418, 0.5524583559017628, 0.562305532163009, 0.6514977591577917, 0.677121093031019, 0.5773346819914877, 0.5610425360500813, 0.5580393958371133, 0.6778482778463513, 0.5751045921351761, 0.5778490260709077, 0.6203715861774981, 0.5513037669006735, 0.785445804009214, 0.5853580092079937, 0.6711696072015911, 0.6644036290235817, 0.5642590757925063, 0.5700399670749903, 0.5997654667589813, 0.6071145271416754, 0.5690939268097281, 0.6542322349268943, 0.5758553768973798, 0.5867305181454867, 0.6532206002157182, 0.8538250129204243, 0.8120289882645011, 0.588288159109652, 0.5953481469769031, 0.5869355178438127, 0.5750065827742219, 0.6831301162019372, 0.5918251250404865, 0.5885955209378153, 0.5504455359186977, 0.5693725892342627, 0.6287965795490891, 0.5552675032522529, 0.5546416260767728, 0.5524798142723739, 0.5619580531492829, 0.5520848000887781, 0.5550459381192923, 0.5962758967652917, 0.6373126900289208, 0.5822645146399736, 0.5875494400970638, 0.5791808092035353, 0.5890926602296531, 0.6090007480233908, 0.802119918866083, 0.6352038809563965, 0.5490628308616579, 0.5478809520136565, 0.5360813548322767, 0.541213909862563, 0.5666345092467964, 0.8710681309457868, 0.8880235278047621, 0.7822206672281027, 0.5384486769326031, 0.5474414608906955, 0.5482454062439501, 0.5742095133755356, 0.6015964709222317, 0.6258237797301263, 0.5744226381648332, 0.5750693760346621, 0.5900112828239799, 0.5947006060741842, 0.8230262433644384, 0.6235432981047779, 0.5925525219645351, 0.5553976390510798, 0.6833246049936861]
Total Epoch List: [228, 267, 21]
Total Time List: [0.16900427895598114, 0.16075794212520123, 0.20835461909882724]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cf4fd30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0809;  Loss pred: 2.0809; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7040 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7019 score: 0.5039 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 2.0818;  Loss pred: 2.0818; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5039 time: 0.15s
Epoch 3/1000, LR 0.000045
Train loss: 2.0863;  Loss pred: 2.0863; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.5039 time: 0.15s
Epoch 4/1000, LR 0.000075
Train loss: 2.0582;  Loss pred: 2.0582; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 2.0261;  Loss pred: 2.0261; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7017 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.5039 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 1.9947;  Loss pred: 1.9947; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 1.9608;  Loss pred: 1.9608; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5039 time: 0.15s
Epoch 8/1000, LR 0.000195
Train loss: 1.9288;  Loss pred: 1.9288; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5039 time: 0.15s
Epoch 9/1000, LR 0.000225
Train loss: 1.8762;  Loss pred: 1.8762; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 1.8240;  Loss pred: 1.8240; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5039 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 1.7783;  Loss pred: 1.7783; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5039 time: 0.24s
Epoch 12/1000, LR 0.000285
Train loss: 1.7331;  Loss pred: 1.7331; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5039 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 1.6909;  Loss pred: 1.6909; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 1.6536;  Loss pred: 1.6536; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.17s
Epoch 15/1000, LR 0.000285
Train loss: 1.6067;  Loss pred: 1.6067; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5039 time: 0.15s
Epoch 16/1000, LR 0.000285
Train loss: 1.5730;  Loss pred: 1.5730; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 1.5269;  Loss pred: 1.5269; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5039 time: 0.23s
Epoch 18/1000, LR 0.000285
Train loss: 1.4957;  Loss pred: 1.4957; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5039 time: 0.15s
Epoch 19/1000, LR 0.000285
Train loss: 1.4755;  Loss pred: 1.4755; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5039 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 1.4368;  Loss pred: 1.4368; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5039 time: 0.23s
Epoch 21/1000, LR 0.000285
Train loss: 1.4089;  Loss pred: 1.4089; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5039 time: 0.26s
Epoch 22/1000, LR 0.000285
Train loss: 1.3815;  Loss pred: 1.3815; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5039 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 1.3581;  Loss pred: 1.3581; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5039 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 1.3326;  Loss pred: 1.3326; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5039 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 1.3102;  Loss pred: 1.3102; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5039 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.2954;  Loss pred: 1.2954; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4961 time: 0.17s
Test loss: 0.6885 score: 0.5116 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 1.2734;  Loss pred: 1.2734; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4961 time: 0.16s
Test loss: 0.6881 score: 0.5194 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.2563;  Loss pred: 1.2563; Loss self: 0.0000; time: 0.33s
Val loss: 0.6879 score: 0.5116 time: 0.17s
Test loss: 0.6877 score: 0.5426 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.2358;  Loss pred: 1.2358; Loss self: 0.0000; time: 0.24s
Val loss: 0.6875 score: 0.5116 time: 0.17s
Test loss: 0.6873 score: 0.5426 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 1.2244;  Loss pred: 1.2244; Loss self: 0.0000; time: 0.24s
Val loss: 0.6870 score: 0.5116 time: 0.17s
Test loss: 0.6868 score: 0.5504 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 1.2041;  Loss pred: 1.2041; Loss self: 0.0000; time: 0.24s
Val loss: 0.6865 score: 0.5349 time: 0.17s
Test loss: 0.6863 score: 0.5814 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 1.1946;  Loss pred: 1.1946; Loss self: 0.0000; time: 0.25s
Val loss: 0.6859 score: 0.5581 time: 0.18s
Test loss: 0.6857 score: 0.6047 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.1773;  Loss pred: 1.1773; Loss self: 0.0000; time: 0.25s
Val loss: 0.6853 score: 0.5814 time: 0.17s
Test loss: 0.6851 score: 0.6279 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 1.1681;  Loss pred: 1.1681; Loss self: 0.0000; time: 0.27s
Val loss: 0.6846 score: 0.6667 time: 0.17s
Test loss: 0.6843 score: 0.6822 time: 0.22s
Epoch 35/1000, LR 0.000285
Train loss: 1.1521;  Loss pred: 1.1521; Loss self: 0.0000; time: 0.36s
Val loss: 0.6838 score: 0.8140 time: 0.27s
Test loss: 0.6836 score: 0.7984 time: 0.21s
Epoch 36/1000, LR 0.000285
Train loss: 1.1408;  Loss pred: 1.1408; Loss self: 0.0000; time: 0.34s
Val loss: 0.6831 score: 0.8217 time: 0.17s
Test loss: 0.6829 score: 0.8760 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 1.1353;  Loss pred: 1.1353; Loss self: 0.0000; time: 0.28s
Val loss: 0.6824 score: 0.8605 time: 0.18s
Test loss: 0.6822 score: 0.8837 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.1253;  Loss pred: 1.1253; Loss self: 0.0000; time: 0.26s
Val loss: 0.6816 score: 0.8837 time: 0.17s
Test loss: 0.6814 score: 0.9070 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 1.1149;  Loss pred: 1.1149; Loss self: 0.0000; time: 0.24s
Val loss: 0.6807 score: 0.8915 time: 0.16s
Test loss: 0.6806 score: 0.9380 time: 0.15s
Epoch 40/1000, LR 0.000284
Train loss: 1.1093;  Loss pred: 1.1093; Loss self: 0.0000; time: 0.24s
Val loss: 0.6798 score: 0.8915 time: 0.16s
Test loss: 0.6797 score: 0.9380 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 1.1019;  Loss pred: 1.1019; Loss self: 0.0000; time: 0.25s
Val loss: 0.6789 score: 0.8915 time: 0.19s
Test loss: 0.6788 score: 0.9457 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 1.0947;  Loss pred: 1.0947; Loss self: 0.0000; time: 0.33s
Val loss: 0.6779 score: 0.8915 time: 0.17s
Test loss: 0.6778 score: 0.9302 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 1.0839;  Loss pred: 1.0839; Loss self: 0.0000; time: 0.24s
Val loss: 0.6768 score: 0.8837 time: 0.16s
Test loss: 0.6767 score: 0.9302 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 1.0780;  Loss pred: 1.0780; Loss self: 0.0000; time: 0.24s
Val loss: 0.6756 score: 0.8760 time: 0.16s
Test loss: 0.6755 score: 0.9302 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 1.0711;  Loss pred: 1.0711; Loss self: 0.0000; time: 0.24s
Val loss: 0.6743 score: 0.8760 time: 0.15s
Test loss: 0.6742 score: 0.9302 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 1.0633;  Loss pred: 1.0633; Loss self: 0.0000; time: 0.24s
Val loss: 0.6728 score: 0.8760 time: 0.15s
Test loss: 0.6727 score: 0.9225 time: 0.15s
Epoch 47/1000, LR 0.000284
Train loss: 1.0565;  Loss pred: 1.0565; Loss self: 0.0000; time: 0.24s
Val loss: 0.6712 score: 0.8760 time: 0.15s
Test loss: 0.6710 score: 0.9225 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 1.0551;  Loss pred: 1.0551; Loss self: 0.0000; time: 0.27s
Val loss: 0.6697 score: 0.8915 time: 0.16s
Test loss: 0.6695 score: 0.9225 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 1.0443;  Loss pred: 1.0443; Loss self: 0.0000; time: 0.31s
Val loss: 0.6682 score: 0.8915 time: 0.15s
Test loss: 0.6680 score: 0.9225 time: 0.15s
Epoch 50/1000, LR 0.000284
Train loss: 1.0427;  Loss pred: 1.0427; Loss self: 0.0000; time: 0.24s
Val loss: 0.6665 score: 0.8915 time: 0.15s
Test loss: 0.6664 score: 0.9225 time: 0.15s
Epoch 51/1000, LR 0.000284
Train loss: 1.0388;  Loss pred: 1.0388; Loss self: 0.0000; time: 0.25s
Val loss: 0.6648 score: 0.8915 time: 0.16s
Test loss: 0.6646 score: 0.9225 time: 0.15s
Epoch 52/1000, LR 0.000284
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.24s
Val loss: 0.6630 score: 0.8915 time: 0.16s
Test loss: 0.6628 score: 0.9225 time: 0.15s
Epoch 53/1000, LR 0.000284
Train loss: 1.0241;  Loss pred: 1.0241; Loss self: 0.0000; time: 0.24s
Val loss: 0.6610 score: 0.8992 time: 0.16s
Test loss: 0.6609 score: 0.8992 time: 0.15s
Epoch 54/1000, LR 0.000284
Train loss: 1.0213;  Loss pred: 1.0213; Loss self: 0.0000; time: 0.24s
Val loss: 0.6590 score: 0.8992 time: 0.15s
Test loss: 0.6588 score: 0.8992 time: 0.15s
Epoch 55/1000, LR 0.000284
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.28s
Val loss: 0.6569 score: 0.8992 time: 0.17s
Test loss: 0.6567 score: 0.8992 time: 0.17s
Epoch 56/1000, LR 0.000284
Train loss: 1.0117;  Loss pred: 1.0117; Loss self: 0.0000; time: 0.34s
Val loss: 0.6547 score: 0.8992 time: 0.17s
Test loss: 0.6544 score: 0.8992 time: 0.16s
Epoch 57/1000, LR 0.000283
Train loss: 1.0067;  Loss pred: 1.0067; Loss self: 0.0000; time: 0.25s
Val loss: 0.6523 score: 0.8992 time: 0.17s
Test loss: 0.6521 score: 0.8992 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 1.0036;  Loss pred: 1.0036; Loss self: 0.0000; time: 0.25s
Val loss: 0.6499 score: 0.8992 time: 0.16s
Test loss: 0.6496 score: 0.8992 time: 0.16s
Epoch 59/1000, LR 0.000283
Train loss: 0.9979;  Loss pred: 0.9979; Loss self: 0.0000; time: 0.25s
Val loss: 0.6474 score: 0.8992 time: 0.17s
Test loss: 0.6470 score: 0.9070 time: 0.16s
Epoch 60/1000, LR 0.000283
Train loss: 0.9965;  Loss pred: 0.9965; Loss self: 0.0000; time: 0.25s
Val loss: 0.6448 score: 0.8992 time: 0.17s
Test loss: 0.6443 score: 0.9225 time: 0.16s
Epoch 61/1000, LR 0.000283
Train loss: 0.9918;  Loss pred: 0.9918; Loss self: 0.0000; time: 0.26s
Val loss: 0.6421 score: 0.8992 time: 0.17s
Test loss: 0.6415 score: 0.9225 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 0.9863;  Loss pred: 0.9863; Loss self: 0.0000; time: 0.25s
Val loss: 0.6392 score: 0.8992 time: 0.24s
Test loss: 0.6386 score: 0.9225 time: 0.23s
Epoch 63/1000, LR 0.000283
Train loss: 0.9808;  Loss pred: 0.9808; Loss self: 0.0000; time: 0.31s
Val loss: 0.6361 score: 0.8992 time: 0.16s
Test loss: 0.6355 score: 0.9070 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 0.9776;  Loss pred: 0.9776; Loss self: 0.0000; time: 0.25s
Val loss: 0.6330 score: 0.8992 time: 0.17s
Test loss: 0.6323 score: 0.9070 time: 0.17s
Epoch 65/1000, LR 0.000283
Train loss: 0.9731;  Loss pred: 0.9731; Loss self: 0.0000; time: 0.24s
Val loss: 0.6297 score: 0.8992 time: 0.17s
Test loss: 0.6289 score: 0.9070 time: 0.16s
Epoch 66/1000, LR 0.000283
Train loss: 0.9670;  Loss pred: 0.9670; Loss self: 0.0000; time: 0.24s
Val loss: 0.6263 score: 0.8992 time: 0.16s
Test loss: 0.6254 score: 0.8992 time: 0.16s
Epoch 67/1000, LR 0.000283
Train loss: 0.9641;  Loss pred: 0.9641; Loss self: 0.0000; time: 0.25s
Val loss: 0.6228 score: 0.8992 time: 0.16s
Test loss: 0.6218 score: 0.8992 time: 0.16s
Epoch 68/1000, LR 0.000283
Train loss: 0.9591;  Loss pred: 0.9591; Loss self: 0.0000; time: 0.28s
Val loss: 0.6191 score: 0.9070 time: 0.17s
Test loss: 0.6181 score: 0.8992 time: 0.21s
Epoch 69/1000, LR 0.000283
Train loss: 0.9556;  Loss pred: 0.9556; Loss self: 0.0000; time: 0.24s
Val loss: 0.6153 score: 0.9070 time: 0.15s
Test loss: 0.6142 score: 0.8992 time: 0.15s
Epoch 70/1000, LR 0.000283
Train loss: 0.9512;  Loss pred: 0.9512; Loss self: 0.0000; time: 0.25s
Val loss: 0.6114 score: 0.9070 time: 0.16s
Test loss: 0.6102 score: 0.8992 time: 0.15s
Epoch 71/1000, LR 0.000282
Train loss: 0.9461;  Loss pred: 0.9461; Loss self: 0.0000; time: 0.25s
Val loss: 0.6074 score: 0.9070 time: 0.16s
Test loss: 0.6060 score: 0.8992 time: 0.15s
Epoch 72/1000, LR 0.000282
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.24s
Val loss: 0.6032 score: 0.9070 time: 0.16s
Test loss: 0.6018 score: 0.8992 time: 0.16s
Epoch 73/1000, LR 0.000282
Train loss: 0.9389;  Loss pred: 0.9389; Loss self: 0.0000; time: 0.24s
Val loss: 0.5990 score: 0.9070 time: 0.16s
Test loss: 0.5974 score: 0.8992 time: 0.16s
Epoch 74/1000, LR 0.000282
Train loss: 0.9335;  Loss pred: 0.9335; Loss self: 0.0000; time: 0.26s
Val loss: 0.5946 score: 0.9070 time: 0.16s
Test loss: 0.5928 score: 0.8992 time: 0.21s
Epoch 75/1000, LR 0.000282
Train loss: 0.9289;  Loss pred: 0.9289; Loss self: 0.0000; time: 0.31s
Val loss: 0.5900 score: 0.9147 time: 0.16s
Test loss: 0.5882 score: 0.8992 time: 0.16s
Epoch 76/1000, LR 0.000282
Train loss: 0.9236;  Loss pred: 0.9236; Loss self: 0.0000; time: 0.25s
Val loss: 0.5856 score: 0.9147 time: 0.15s
Test loss: 0.5834 score: 0.9070 time: 0.15s
Epoch 77/1000, LR 0.000282
Train loss: 0.9180;  Loss pred: 0.9180; Loss self: 0.0000; time: 0.23s
Val loss: 0.5809 score: 0.9070 time: 0.15s
Test loss: 0.5785 score: 0.8992 time: 0.15s
Epoch 78/1000, LR 0.000282
Train loss: 0.9143;  Loss pred: 0.9143; Loss self: 0.0000; time: 0.24s
Val loss: 0.5762 score: 0.9070 time: 0.16s
Test loss: 0.5735 score: 0.8992 time: 0.15s
Epoch 79/1000, LR 0.000282
Train loss: 0.9089;  Loss pred: 0.9089; Loss self: 0.0000; time: 0.24s
Val loss: 0.5714 score: 0.9070 time: 0.16s
Test loss: 0.5684 score: 0.8992 time: 0.15s
Epoch 80/1000, LR 0.000282
Train loss: 0.9044;  Loss pred: 0.9044; Loss self: 0.0000; time: 0.24s
Val loss: 0.5665 score: 0.9070 time: 0.16s
Test loss: 0.5632 score: 0.8992 time: 0.16s
Epoch 81/1000, LR 0.000281
Train loss: 0.9011;  Loss pred: 0.9011; Loss self: 0.0000; time: 0.26s
Val loss: 0.5614 score: 0.9070 time: 0.16s
Test loss: 0.5579 score: 0.8992 time: 0.16s
Epoch 82/1000, LR 0.000281
Train loss: 0.8938;  Loss pred: 0.8938; Loss self: 0.0000; time: 0.34s
Val loss: 0.5562 score: 0.9147 time: 0.20s
Test loss: 0.5524 score: 0.9070 time: 0.16s
Epoch 83/1000, LR 0.000281
Train loss: 0.8897;  Loss pred: 0.8897; Loss self: 0.0000; time: 0.25s
Val loss: 0.5509 score: 0.9147 time: 0.17s
Test loss: 0.5468 score: 0.8992 time: 0.16s
Epoch 84/1000, LR 0.000281
Train loss: 0.8845;  Loss pred: 0.8845; Loss self: 0.0000; time: 0.26s
Val loss: 0.5455 score: 0.9147 time: 0.17s
Test loss: 0.5411 score: 0.9070 time: 0.16s
Epoch 85/1000, LR 0.000281
Train loss: 0.8799;  Loss pred: 0.8799; Loss self: 0.0000; time: 0.25s
Val loss: 0.5400 score: 0.9147 time: 0.17s
Test loss: 0.5351 score: 0.9070 time: 0.16s
Epoch 86/1000, LR 0.000281
Train loss: 0.8752;  Loss pred: 0.8752; Loss self: 0.0000; time: 0.25s
Val loss: 0.5344 score: 0.9147 time: 0.17s
Test loss: 0.5291 score: 0.9070 time: 0.16s
Epoch 87/1000, LR 0.000281
Train loss: 0.8696;  Loss pred: 0.8696; Loss self: 0.0000; time: 0.25s
Val loss: 0.5287 score: 0.9070 time: 0.19s
Test loss: 0.5229 score: 0.8992 time: 0.17s
Epoch 88/1000, LR 0.000281
Train loss: 0.8641;  Loss pred: 0.8641; Loss self: 0.0000; time: 0.30s
Val loss: 0.5230 score: 0.9070 time: 0.24s
Test loss: 0.5168 score: 0.8992 time: 0.24s
Epoch 89/1000, LR 0.000281
Train loss: 0.8600;  Loss pred: 0.8600; Loss self: 0.0000; time: 0.37s
Val loss: 0.5174 score: 0.9070 time: 0.25s
Test loss: 0.5106 score: 0.8992 time: 0.17s
Epoch 90/1000, LR 0.000281
Train loss: 0.8535;  Loss pred: 0.8535; Loss self: 0.0000; time: 0.25s
Val loss: 0.5117 score: 0.9070 time: 0.17s
Test loss: 0.5043 score: 0.8992 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.8494;  Loss pred: 0.8494; Loss self: 0.0000; time: 0.25s
Val loss: 0.5060 score: 0.9070 time: 0.17s
Test loss: 0.4981 score: 0.8992 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.8433;  Loss pred: 0.8433; Loss self: 0.0000; time: 0.25s
Val loss: 0.5003 score: 0.9070 time: 0.17s
Test loss: 0.4919 score: 0.8992 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.8386;  Loss pred: 0.8386; Loss self: 0.0000; time: 0.26s
Val loss: 0.4945 score: 0.9070 time: 0.17s
Test loss: 0.4856 score: 0.8992 time: 0.20s
Epoch 94/1000, LR 0.000280
Train loss: 0.8312;  Loss pred: 0.8312; Loss self: 0.0000; time: 0.28s
Val loss: 0.4889 score: 0.8992 time: 0.24s
Test loss: 0.4792 score: 0.9070 time: 0.23s
Epoch 95/1000, LR 0.000280
Train loss: 0.8280;  Loss pred: 0.8280; Loss self: 0.0000; time: 0.31s
Val loss: 0.4832 score: 0.8992 time: 0.17s
Test loss: 0.4729 score: 0.8992 time: 0.16s
Epoch 96/1000, LR 0.000280
Train loss: 0.8239;  Loss pred: 0.8239; Loss self: 0.0000; time: 0.25s
Val loss: 0.4775 score: 0.8992 time: 0.17s
Test loss: 0.4666 score: 0.8992 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.8190;  Loss pred: 0.8190; Loss self: 0.0000; time: 0.25s
Val loss: 0.4719 score: 0.8992 time: 0.17s
Test loss: 0.4603 score: 0.8992 time: 0.16s
Epoch 98/1000, LR 0.000280
Train loss: 0.8134;  Loss pred: 0.8134; Loss self: 0.0000; time: 0.25s
Val loss: 0.4664 score: 0.8992 time: 0.17s
Test loss: 0.4538 score: 0.9147 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.8085;  Loss pred: 0.8085; Loss self: 0.0000; time: 0.25s
Val loss: 0.4608 score: 0.8992 time: 0.17s
Test loss: 0.4475 score: 0.9147 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.8042;  Loss pred: 0.8042; Loss self: 0.0000; time: 0.26s
Val loss: 0.4552 score: 0.8992 time: 0.22s
Test loss: 0.4410 score: 0.9147 time: 0.20s
Epoch 101/1000, LR 0.000279
Train loss: 0.7994;  Loss pred: 0.7994; Loss self: 0.0000; time: 0.26s
Val loss: 0.4497 score: 0.8992 time: 0.17s
Test loss: 0.4345 score: 0.9225 time: 0.16s
Epoch 102/1000, LR 0.000279
Train loss: 0.7938;  Loss pred: 0.7938; Loss self: 0.0000; time: 0.25s
Val loss: 0.4441 score: 0.8915 time: 0.17s
Test loss: 0.4280 score: 0.9225 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.7885;  Loss pred: 0.7885; Loss self: 0.0000; time: 0.26s
Val loss: 0.4385 score: 0.8915 time: 0.17s
Test loss: 0.4216 score: 0.9225 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.7828;  Loss pred: 0.7828; Loss self: 0.0000; time: 0.26s
Val loss: 0.4331 score: 0.8915 time: 0.17s
Test loss: 0.4151 score: 0.9302 time: 0.16s
Epoch 105/1000, LR 0.000279
Train loss: 0.7776;  Loss pred: 0.7776; Loss self: 0.0000; time: 0.27s
Val loss: 0.4277 score: 0.8915 time: 0.17s
Test loss: 0.4087 score: 0.9302 time: 0.24s
Epoch 106/1000, LR 0.000279
Train loss: 0.7718;  Loss pred: 0.7718; Loss self: 0.0000; time: 0.36s
Val loss: 0.4225 score: 0.8915 time: 0.24s
Test loss: 0.4024 score: 0.9302 time: 0.18s
Epoch 107/1000, LR 0.000278
Train loss: 0.7670;  Loss pred: 0.7670; Loss self: 0.0000; time: 0.25s
Val loss: 0.4173 score: 0.8992 time: 0.17s
Test loss: 0.3961 score: 0.9302 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.7632;  Loss pred: 0.7632; Loss self: 0.0000; time: 0.26s
Val loss: 0.4121 score: 0.8992 time: 0.17s
Test loss: 0.3899 score: 0.9302 time: 0.16s
Epoch 109/1000, LR 0.000278
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 0.25s
Val loss: 0.4071 score: 0.8992 time: 0.17s
Test loss: 0.3838 score: 0.9302 time: 0.16s
Epoch 110/1000, LR 0.000278
Train loss: 0.7536;  Loss pred: 0.7536; Loss self: 0.0000; time: 0.26s
Val loss: 0.4021 score: 0.8915 time: 0.17s
Test loss: 0.3779 score: 0.9302 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.7493;  Loss pred: 0.7493; Loss self: 0.0000; time: 0.26s
Val loss: 0.3974 score: 0.8992 time: 0.17s
Test loss: 0.3718 score: 0.9302 time: 0.20s
Epoch 112/1000, LR 0.000278
Train loss: 0.7459;  Loss pred: 0.7459; Loss self: 0.0000; time: 0.26s
Val loss: 0.3927 score: 0.8992 time: 0.25s
Test loss: 0.3659 score: 0.9302 time: 0.16s
Epoch 113/1000, LR 0.000278
Train loss: 0.7402;  Loss pred: 0.7402; Loss self: 0.0000; time: 0.25s
Val loss: 0.3882 score: 0.8915 time: 0.16s
Test loss: 0.3600 score: 0.9380 time: 0.15s
Epoch 114/1000, LR 0.000277
Train loss: 0.7360;  Loss pred: 0.7360; Loss self: 0.0000; time: 0.24s
Val loss: 0.3838 score: 0.8992 time: 0.17s
Test loss: 0.3543 score: 0.9380 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.7320;  Loss pred: 0.7320; Loss self: 0.0000; time: 0.25s
Val loss: 0.3795 score: 0.8992 time: 0.17s
Test loss: 0.3487 score: 0.9380 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.7302;  Loss pred: 0.7302; Loss self: 0.0000; time: 0.25s
Val loss: 0.3752 score: 0.8992 time: 0.23s
Test loss: 0.3433 score: 0.9380 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.7249;  Loss pred: 0.7249; Loss self: 0.0000; time: 0.33s
Val loss: 0.3711 score: 0.8992 time: 0.18s
Test loss: 0.3378 score: 0.9380 time: 0.15s
Epoch 118/1000, LR 0.000277
Train loss: 0.7210;  Loss pred: 0.7210; Loss self: 0.0000; time: 0.26s
Val loss: 0.3669 score: 0.8992 time: 0.17s
Test loss: 0.3326 score: 0.9380 time: 0.16s
Epoch 119/1000, LR 0.000277
Train loss: 0.7157;  Loss pred: 0.7157; Loss self: 0.0000; time: 0.33s
Val loss: 0.3630 score: 0.8992 time: 0.17s
Test loss: 0.3273 score: 0.9380 time: 0.16s
Epoch 120/1000, LR 0.000277
Train loss: 0.7120;  Loss pred: 0.7120; Loss self: 0.0000; time: 0.25s
Val loss: 0.3591 score: 0.8992 time: 0.17s
Test loss: 0.3222 score: 0.9380 time: 0.17s
Epoch 121/1000, LR 0.000276
Train loss: 0.7099;  Loss pred: 0.7099; Loss self: 0.0000; time: 0.25s
Val loss: 0.3553 score: 0.8992 time: 0.17s
Test loss: 0.3172 score: 0.9380 time: 0.25s
Epoch 122/1000, LR 0.000276
Train loss: 0.7050;  Loss pred: 0.7050; Loss self: 0.0000; time: 0.25s
Val loss: 0.3515 score: 0.8992 time: 0.17s
Test loss: 0.3124 score: 0.9380 time: 0.17s
Epoch 123/1000, LR 0.000276
Train loss: 0.7024;  Loss pred: 0.7024; Loss self: 0.0000; time: 0.27s
Val loss: 0.3480 score: 0.8992 time: 0.17s
Test loss: 0.3075 score: 0.9380 time: 0.22s
Epoch 124/1000, LR 0.000276
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.30s
Val loss: 0.3446 score: 0.8992 time: 0.17s
Test loss: 0.3029 score: 0.9380 time: 0.16s
Epoch 125/1000, LR 0.000276
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.25s
Val loss: 0.3413 score: 0.8992 time: 0.17s
Test loss: 0.2983 score: 0.9380 time: 0.17s
Epoch 126/1000, LR 0.000276
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.25s
Val loss: 0.3380 score: 0.8992 time: 0.16s
Test loss: 0.2938 score: 0.9380 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.25s
Val loss: 0.3349 score: 0.8992 time: 0.17s
Test loss: 0.2894 score: 0.9380 time: 0.16s
Epoch 128/1000, LR 0.000275
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.25s
Val loss: 0.3319 score: 0.8992 time: 0.17s
Test loss: 0.2852 score: 0.9380 time: 0.16s
Epoch 129/1000, LR 0.000275
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.25s
Val loss: 0.3290 score: 0.8992 time: 0.17s
Test loss: 0.2811 score: 0.9380 time: 0.15s
Epoch 130/1000, LR 0.000275
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.28s
Val loss: 0.3262 score: 0.8992 time: 0.19s
Test loss: 0.2772 score: 0.9380 time: 0.15s
Epoch 131/1000, LR 0.000275
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.25s
Val loss: 0.3235 score: 0.8992 time: 0.16s
Test loss: 0.2732 score: 0.9380 time: 0.15s
Epoch 132/1000, LR 0.000275
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.25s
Val loss: 0.3209 score: 0.8992 time: 0.16s
Test loss: 0.2694 score: 0.9380 time: 0.15s
Epoch 133/1000, LR 0.000274
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.24s
Val loss: 0.3184 score: 0.8992 time: 0.15s
Test loss: 0.2658 score: 0.9380 time: 0.15s
Epoch 134/1000, LR 0.000274
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.25s
Val loss: 0.3158 score: 0.8992 time: 0.16s
Test loss: 0.2622 score: 0.9380 time: 0.15s
Epoch 135/1000, LR 0.000274
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.24s
Val loss: 0.3135 score: 0.8992 time: 0.15s
Test loss: 0.2587 score: 0.9457 time: 0.17s
Epoch 136/1000, LR 0.000274
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.25s
Val loss: 0.3112 score: 0.8992 time: 0.21s
Test loss: 0.2553 score: 0.9457 time: 0.17s
Epoch 137/1000, LR 0.000274
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.24s
Val loss: 0.3089 score: 0.8992 time: 0.16s
Test loss: 0.2521 score: 0.9457 time: 0.16s
Epoch 138/1000, LR 0.000274
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.24s
Val loss: 0.3067 score: 0.8992 time: 0.15s
Test loss: 0.2489 score: 0.9457 time: 0.15s
Epoch 139/1000, LR 0.000273
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.24s
Val loss: 0.3045 score: 0.8992 time: 0.16s
Test loss: 0.2458 score: 0.9457 time: 0.15s
Epoch 140/1000, LR 0.000273
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.25s
Val loss: 0.3025 score: 0.8992 time: 0.16s
Test loss: 0.2428 score: 0.9457 time: 0.15s
Epoch 141/1000, LR 0.000273
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.26s
Val loss: 0.3005 score: 0.8992 time: 0.21s
Test loss: 0.2399 score: 0.9457 time: 0.23s
Epoch 142/1000, LR 0.000273
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.36s
Val loss: 0.2985 score: 0.8992 time: 0.16s
Test loss: 0.2371 score: 0.9457 time: 0.15s
Epoch 143/1000, LR 0.000273
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.24s
Val loss: 0.2966 score: 0.8992 time: 0.16s
Test loss: 0.2343 score: 0.9457 time: 0.15s
Epoch 144/1000, LR 0.000272
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.25s
Val loss: 0.2948 score: 0.8992 time: 0.16s
Test loss: 0.2316 score: 0.9457 time: 0.16s
Epoch 145/1000, LR 0.000272
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.25s
Val loss: 0.2930 score: 0.8992 time: 0.16s
Test loss: 0.2290 score: 0.9457 time: 0.16s
Epoch 146/1000, LR 0.000272
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.25s
Val loss: 0.2913 score: 0.8992 time: 0.16s
Test loss: 0.2264 score: 0.9457 time: 0.18s
Epoch 147/1000, LR 0.000272
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.26s
Val loss: 0.2897 score: 0.8992 time: 0.25s
Test loss: 0.2240 score: 0.9457 time: 0.16s
Epoch 148/1000, LR 0.000272
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.25s
Val loss: 0.2880 score: 0.8992 time: 0.16s
Test loss: 0.2216 score: 0.9457 time: 0.16s
Epoch 149/1000, LR 0.000272
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.25s
Val loss: 0.2864 score: 0.8992 time: 0.17s
Test loss: 0.2193 score: 0.9457 time: 0.16s
Epoch 150/1000, LR 0.000271
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.25s
Val loss: 0.2848 score: 0.8992 time: 0.16s
Test loss: 0.2170 score: 0.9457 time: 0.16s
Epoch 151/1000, LR 0.000271
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.25s
Val loss: 0.2833 score: 0.8992 time: 0.17s
Test loss: 0.2148 score: 0.9457 time: 0.17s
Epoch 152/1000, LR 0.000271
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.25s
Val loss: 0.2819 score: 0.8992 time: 0.17s
Test loss: 0.2127 score: 0.9457 time: 0.20s
Epoch 153/1000, LR 0.000271
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.26s
Val loss: 0.2804 score: 0.8992 time: 0.25s
Test loss: 0.2107 score: 0.9457 time: 0.17s
Epoch 154/1000, LR 0.000271
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.25s
Val loss: 0.2791 score: 0.8992 time: 0.17s
Test loss: 0.2087 score: 0.9457 time: 0.16s
Epoch 155/1000, LR 0.000270
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.25s
Val loss: 0.2778 score: 0.8992 time: 0.17s
Test loss: 0.2067 score: 0.9457 time: 0.16s
Epoch 156/1000, LR 0.000270
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.25s
Val loss: 0.2764 score: 0.8992 time: 0.29s
Test loss: 0.2048 score: 0.9457 time: 0.16s
Epoch 157/1000, LR 0.000270
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.25s
Val loss: 0.2752 score: 0.8992 time: 0.17s
Test loss: 0.2030 score: 0.9457 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.25s
Val loss: 0.2740 score: 0.8992 time: 0.18s
Test loss: 0.2012 score: 0.9457 time: 0.17s
Epoch 159/1000, LR 0.000270
Train loss: 0.6155;  Loss pred: 0.6155; Loss self: 0.0000; time: 0.30s
Val loss: 0.2728 score: 0.8992 time: 0.23s
Test loss: 0.1995 score: 0.9457 time: 0.23s
Epoch 160/1000, LR 0.000269
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.32s
Val loss: 0.2716 score: 0.8992 time: 0.16s
Test loss: 0.1978 score: 0.9457 time: 0.15s
Epoch 161/1000, LR 0.000269
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.25s
Val loss: 0.2706 score: 0.8992 time: 0.17s
Test loss: 0.1962 score: 0.9457 time: 0.16s
Epoch 162/1000, LR 0.000269
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.25s
Val loss: 0.2694 score: 0.8992 time: 0.16s
Test loss: 0.1946 score: 0.9457 time: 0.16s
Epoch 163/1000, LR 0.000269
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.27s
Val loss: 0.2683 score: 0.8992 time: 0.24s
Test loss: 0.1931 score: 0.9457 time: 0.24s
Epoch 164/1000, LR 0.000269
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.37s
Val loss: 0.2673 score: 0.8992 time: 0.24s
Test loss: 0.1916 score: 0.9457 time: 0.24s
Epoch 165/1000, LR 0.000268
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.37s
Val loss: 0.2663 score: 0.8992 time: 0.25s
Test loss: 0.1901 score: 0.9457 time: 0.25s
Epoch 166/1000, LR 0.000268
Train loss: 0.6063;  Loss pred: 0.6063; Loss self: 0.0000; time: 0.26s
Val loss: 0.2653 score: 0.8992 time: 0.16s
Test loss: 0.1887 score: 0.9457 time: 0.16s
Epoch 167/1000, LR 0.000268
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.25s
Val loss: 0.2643 score: 0.8992 time: 0.17s
Test loss: 0.1873 score: 0.9457 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.25s
Val loss: 0.2634 score: 0.8992 time: 0.17s
Test loss: 0.1860 score: 0.9457 time: 0.16s
Epoch 169/1000, LR 0.000267
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.25s
Val loss: 0.2625 score: 0.8992 time: 0.17s
Test loss: 0.1847 score: 0.9457 time: 0.16s
Epoch 170/1000, LR 0.000267
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.25s
Val loss: 0.2616 score: 0.8992 time: 0.17s
Test loss: 0.1835 score: 0.9457 time: 0.18s
Epoch 171/1000, LR 0.000267
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 0.25s
Val loss: 0.2607 score: 0.8992 time: 0.25s
Test loss: 0.1822 score: 0.9457 time: 0.15s
Epoch 172/1000, LR 0.000267
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.24s
Val loss: 0.2599 score: 0.8992 time: 0.16s
Test loss: 0.1810 score: 0.9457 time: 0.16s
Epoch 173/1000, LR 0.000267
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 0.24s
Val loss: 0.2592 score: 0.8992 time: 0.16s
Test loss: 0.1799 score: 0.9457 time: 0.15s
Epoch 174/1000, LR 0.000266
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 0.24s
Val loss: 0.2584 score: 0.8992 time: 0.16s
Test loss: 0.1788 score: 0.9457 time: 0.16s
Epoch 175/1000, LR 0.000266
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.25s
Val loss: 0.2576 score: 0.8992 time: 0.17s
Test loss: 0.1776 score: 0.9457 time: 0.16s
Epoch 176/1000, LR 0.000266
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.26s
Val loss: 0.2569 score: 0.8992 time: 0.17s
Test loss: 0.1765 score: 0.9457 time: 0.24s
Epoch 177/1000, LR 0.000266
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.37s
Val loss: 0.2563 score: 0.8992 time: 0.17s
Test loss: 0.1754 score: 0.9457 time: 0.16s
Epoch 178/1000, LR 0.000265
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.26s
Val loss: 0.2556 score: 0.8992 time: 0.17s
Test loss: 0.1744 score: 0.9535 time: 0.16s
Epoch 179/1000, LR 0.000265
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 0.26s
Val loss: 0.2550 score: 0.8992 time: 0.16s
Test loss: 0.1734 score: 0.9535 time: 0.16s
Epoch 180/1000, LR 0.000265
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.25s
Val loss: 0.2543 score: 0.8992 time: 0.23s
Test loss: 0.1725 score: 0.9535 time: 0.23s
Epoch 181/1000, LR 0.000265
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.35s
Val loss: 0.2536 score: 0.8992 time: 0.23s
Test loss: 0.1715 score: 0.9535 time: 0.23s
Epoch 182/1000, LR 0.000265
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.35s
Val loss: 0.2530 score: 0.8992 time: 0.24s
Test loss: 0.1705 score: 0.9535 time: 0.23s
Epoch 183/1000, LR 0.000264
Train loss: 0.5834;  Loss pred: 0.5834; Loss self: 0.0000; time: 0.36s
Val loss: 0.2525 score: 0.8992 time: 0.18s
Test loss: 0.1696 score: 0.9535 time: 0.15s
Epoch 184/1000, LR 0.000264
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.24s
Val loss: 0.2519 score: 0.8992 time: 0.15s
Test loss: 0.1688 score: 0.9535 time: 0.16s
Epoch 185/1000, LR 0.000264
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.24s
Val loss: 0.2513 score: 0.8992 time: 0.16s
Test loss: 0.1679 score: 0.9535 time: 0.15s
Epoch 186/1000, LR 0.000264
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.24s
Val loss: 0.2506 score: 0.8992 time: 0.16s
Test loss: 0.1671 score: 0.9535 time: 0.15s
Epoch 187/1000, LR 0.000263
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.25s
Val loss: 0.2501 score: 0.8992 time: 0.17s
Test loss: 0.1664 score: 0.9535 time: 0.16s
Epoch 188/1000, LR 0.000263
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.24s
Val loss: 0.2496 score: 0.8992 time: 0.16s
Test loss: 0.1658 score: 0.9535 time: 0.16s
Epoch 189/1000, LR 0.000263
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.26s
Val loss: 0.2492 score: 0.8992 time: 0.17s
Test loss: 0.1652 score: 0.9535 time: 0.16s
Epoch 190/1000, LR 0.000263
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.31s
Val loss: 0.2488 score: 0.8992 time: 0.16s
Test loss: 0.1648 score: 0.9535 time: 0.15s
Epoch 191/1000, LR 0.000262
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.23s
Val loss: 0.2484 score: 0.8992 time: 0.17s
Test loss: 0.1641 score: 0.9535 time: 0.16s
Epoch 192/1000, LR 0.000262
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.25s
Val loss: 0.2480 score: 0.8992 time: 0.17s
Test loss: 0.1635 score: 0.9535 time: 0.16s
Epoch 193/1000, LR 0.000262
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.25s
Val loss: 0.2474 score: 0.8992 time: 0.17s
Test loss: 0.1626 score: 0.9535 time: 0.17s
Epoch 194/1000, LR 0.000262
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 0.30s
Val loss: 0.2469 score: 0.9070 time: 0.24s
Test loss: 0.1618 score: 0.9535 time: 0.24s
Epoch 195/1000, LR 0.000261
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.35s
Val loss: 0.2465 score: 0.9070 time: 0.20s
Test loss: 0.1610 score: 0.9535 time: 0.16s
Epoch 196/1000, LR 0.000261
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.25s
Val loss: 0.2460 score: 0.9147 time: 0.17s
Test loss: 0.1601 score: 0.9535 time: 0.16s
Epoch 197/1000, LR 0.000261
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.25s
Val loss: 0.2456 score: 0.9147 time: 0.17s
Test loss: 0.1594 score: 0.9535 time: 0.16s
Epoch 198/1000, LR 0.000261
Train loss: 0.5680;  Loss pred: 0.5680; Loss self: 0.0000; time: 0.25s
Val loss: 0.2453 score: 0.9147 time: 0.17s
Test loss: 0.1588 score: 0.9535 time: 0.16s
Epoch 199/1000, LR 0.000260
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.25s
Val loss: 0.2449 score: 0.9147 time: 0.17s
Test loss: 0.1583 score: 0.9535 time: 0.16s
Epoch 200/1000, LR 0.000260
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.24s
Val loss: 0.2446 score: 0.9147 time: 0.17s
Test loss: 0.1577 score: 0.9535 time: 0.17s
Epoch 201/1000, LR 0.000260
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.25s
Val loss: 0.2442 score: 0.9147 time: 0.17s
Test loss: 0.1572 score: 0.9535 time: 0.17s
Epoch 202/1000, LR 0.000260
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 0.35s
Val loss: 0.2439 score: 0.9147 time: 0.26s
Test loss: 0.1568 score: 0.9535 time: 0.17s
Epoch 203/1000, LR 0.000259
Train loss: 0.5644;  Loss pred: 0.5644; Loss self: 0.0000; time: 0.26s
Val loss: 0.2435 score: 0.9147 time: 0.17s
Test loss: 0.1564 score: 0.9535 time: 0.17s
Epoch 204/1000, LR 0.000259
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.25s
Val loss: 0.2432 score: 0.9147 time: 0.17s
Test loss: 0.1562 score: 0.9535 time: 0.16s
Epoch 205/1000, LR 0.000259
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.25s
Val loss: 0.2431 score: 0.9225 time: 0.17s
Test loss: 0.1561 score: 0.9535 time: 0.17s
Epoch 206/1000, LR 0.000259
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.25s
Val loss: 0.2429 score: 0.9225 time: 0.17s
Test loss: 0.1557 score: 0.9535 time: 0.17s
Epoch 207/1000, LR 0.000258
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.25s
Val loss: 0.2425 score: 0.9225 time: 0.17s
Test loss: 0.1552 score: 0.9535 time: 0.17s
Epoch 208/1000, LR 0.000258
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.28s
Val loss: 0.2423 score: 0.9225 time: 0.17s
Test loss: 0.1549 score: 0.9535 time: 0.24s
Epoch 209/1000, LR 0.000258
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.34s
Val loss: 0.2420 score: 0.9225 time: 0.23s
Test loss: 0.1545 score: 0.9535 time: 0.17s
Epoch 210/1000, LR 0.000258
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.25s
Val loss: 0.2417 score: 0.9225 time: 0.17s
Test loss: 0.1538 score: 0.9535 time: 0.17s
Epoch 211/1000, LR 0.000257
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.26s
Val loss: 0.2412 score: 0.9147 time: 0.19s
Test loss: 0.1532 score: 0.9535 time: 0.22s
Epoch 212/1000, LR 0.000257
Train loss: 0.5584;  Loss pred: 0.5584; Loss self: 0.0000; time: 0.35s
Val loss: 0.2410 score: 0.9147 time: 0.17s
Test loss: 0.1528 score: 0.9535 time: 0.16s
Epoch 213/1000, LR 0.000257
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.25s
Val loss: 0.2406 score: 0.9147 time: 0.17s
Test loss: 0.1522 score: 0.9535 time: 0.17s
Epoch 214/1000, LR 0.000256
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.25s
Val loss: 0.2403 score: 0.9147 time: 0.17s
Test loss: 0.1519 score: 0.9535 time: 0.25s
Epoch 215/1000, LR 0.000256
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.24s
Val loss: 0.2402 score: 0.9147 time: 0.17s
Test loss: 0.1517 score: 0.9535 time: 0.16s
Epoch 216/1000, LR 0.000256
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.25s
Val loss: 0.2399 score: 0.9147 time: 0.17s
Test loss: 0.1515 score: 0.9535 time: 0.17s
Epoch 217/1000, LR 0.000256
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.31s
Val loss: 0.2398 score: 0.9225 time: 0.24s
Test loss: 0.1513 score: 0.9535 time: 0.23s
Epoch 218/1000, LR 0.000255
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.35s
Val loss: 0.2396 score: 0.9225 time: 0.23s
Test loss: 0.1510 score: 0.9535 time: 0.17s
Epoch 219/1000, LR 0.000255
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.25s
Val loss: 0.2394 score: 0.9225 time: 0.20s
Test loss: 0.1506 score: 0.9535 time: 0.15s
Epoch 220/1000, LR 0.000255
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.23s
Val loss: 0.2391 score: 0.9225 time: 0.15s
Test loss: 0.1502 score: 0.9535 time: 0.15s
Epoch 221/1000, LR 0.000255
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.24s
Val loss: 0.2388 score: 0.9147 time: 0.16s
Test loss: 0.1497 score: 0.9535 time: 0.16s
Epoch 222/1000, LR 0.000254
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.23s
Val loss: 0.2385 score: 0.9147 time: 0.15s
Test loss: 0.1492 score: 0.9535 time: 0.15s
Epoch 223/1000, LR 0.000254
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.23s
Val loss: 0.2383 score: 0.9147 time: 0.16s
Test loss: 0.1491 score: 0.9535 time: 0.16s
Epoch 224/1000, LR 0.000254
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.24s
Val loss: 0.2383 score: 0.9225 time: 0.16s
Test loss: 0.1492 score: 0.9535 time: 0.16s
Epoch 225/1000, LR 0.000253
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.25s
Val loss: 0.2382 score: 0.9225 time: 0.20s
Test loss: 0.1491 score: 0.9535 time: 0.17s
Epoch 226/1000, LR 0.000253
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.29s
Val loss: 0.2381 score: 0.9225 time: 0.21s
Test loss: 0.1490 score: 0.9535 time: 0.16s
Epoch 227/1000, LR 0.000253
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.24s
Val loss: 0.2380 score: 0.9225 time: 0.17s
Test loss: 0.1489 score: 0.9535 time: 0.16s
Epoch 228/1000, LR 0.000253
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.24s
Val loss: 0.2377 score: 0.9225 time: 0.15s
Test loss: 0.1485 score: 0.9535 time: 0.15s
Epoch 229/1000, LR 0.000252
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.23s
Val loss: 0.2376 score: 0.9225 time: 0.16s
Test loss: 0.1483 score: 0.9535 time: 0.16s
Epoch 230/1000, LR 0.000252
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.25s
Val loss: 0.2372 score: 0.9225 time: 0.16s
Test loss: 0.1477 score: 0.9535 time: 0.15s
Epoch 231/1000, LR 0.000252
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 0.24s
Val loss: 0.2370 score: 0.9225 time: 0.16s
Test loss: 0.1473 score: 0.9535 time: 0.15s
Epoch 232/1000, LR 0.000251
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.25s
Val loss: 0.2369 score: 0.9225 time: 0.16s
Test loss: 0.1471 score: 0.9535 time: 0.15s
Epoch 233/1000, LR 0.000251
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.34s
Val loss: 0.2368 score: 0.9225 time: 0.16s
Test loss: 0.1471 score: 0.9535 time: 0.15s
Epoch 234/1000, LR 0.000251
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.24s
Val loss: 0.2369 score: 0.9225 time: 0.15s
Test loss: 0.1473 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.25s
Val loss: 0.2369 score: 0.9225 time: 0.16s
Test loss: 0.1475 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.24s
Val loss: 0.2369 score: 0.9225 time: 0.16s
Test loss: 0.1474 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.24s
Val loss: 0.2366 score: 0.9225 time: 0.16s
Test loss: 0.1470 score: 0.9535 time: 0.15s
Epoch 238/1000, LR 0.000250
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.24s
Val loss: 0.2365 score: 0.9225 time: 0.15s
Test loss: 0.1469 score: 0.9535 time: 0.15s
Epoch 239/1000, LR 0.000249
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.26s
Val loss: 0.2362 score: 0.9225 time: 0.16s
Test loss: 0.1464 score: 0.9535 time: 0.21s
Epoch 240/1000, LR 0.000249
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.30s
Val loss: 0.2361 score: 0.9225 time: 0.16s
Test loss: 0.1464 score: 0.9535 time: 0.16s
Epoch 241/1000, LR 0.000249
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.1462 score: 0.9535 time: 0.15s
Epoch 242/1000, LR 0.000248
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.24s
Val loss: 0.2361 score: 0.9225 time: 0.16s
Test loss: 0.1464 score: 0.9535 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.25s
Val loss: 0.2363 score: 0.9225 time: 0.16s
Test loss: 0.1468 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 0.24s
Val loss: 0.2363 score: 0.9225 time: 0.16s
Test loss: 0.1467 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.27s
Val loss: 0.2361 score: 0.9225 time: 0.16s
Test loss: 0.1465 score: 0.9535 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.28s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.1462 score: 0.9535 time: 0.16s
Epoch 247/1000, LR 0.000247
Train loss: 0.5331;  Loss pred: 0.5331; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.1462 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.1462 score: 0.9535 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.33s
Val loss: 0.2361 score: 0.9225 time: 0.24s
Test loss: 0.1462 score: 0.9535 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.36s
Val loss: 0.2364 score: 0.9225 time: 0.24s
Test loss: 0.1464 score: 0.9457 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5353;  Loss pred: 0.5353; Loss self: 0.0000; time: 0.36s
Val loss: 0.2363 score: 0.9225 time: 0.24s
Test loss: 0.1462 score: 0.9457 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.36s
Val loss: 0.2363 score: 0.9225 time: 0.24s
Test loss: 0.1461 score: 0.9457 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.24s
Val loss: 0.2359 score: 0.9225 time: 0.15s
Test loss: 0.1454 score: 0.9535 time: 0.16s
Epoch 254/1000, LR 0.000245
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.1455 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.24s
Val loss: 0.2358 score: 0.9225 time: 0.16s
Test loss: 0.1450 score: 0.9535 time: 0.15s
Epoch 256/1000, LR 0.000244
Train loss: 0.5288;  Loss pred: 0.5288; Loss self: 0.0000; time: 0.24s
Val loss: 0.2358 score: 0.9225 time: 0.16s
Test loss: 0.1449 score: 0.9535 time: 0.15s
Epoch 257/1000, LR 0.000244
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.24s
Val loss: 0.2357 score: 0.9225 time: 0.17s
Test loss: 0.1446 score: 0.9535 time: 0.16s
Epoch 258/1000, LR 0.000243
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.24s
Val loss: 0.2356 score: 0.9225 time: 0.17s
Test loss: 0.1444 score: 0.9535 time: 0.17s
Epoch 259/1000, LR 0.000243
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.26s
Val loss: 0.2356 score: 0.9225 time: 0.17s
Test loss: 0.1444 score: 0.9535 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.25s
Val loss: 0.2356 score: 0.9225 time: 0.16s
Test loss: 0.1444 score: 0.9535 time: 0.15s
Epoch 261/1000, LR 0.000242
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.24s
Val loss: 0.2355 score: 0.9225 time: 0.16s
Test loss: 0.1443 score: 0.9535 time: 0.15s
Epoch 262/1000, LR 0.000242
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.24s
Val loss: 0.2354 score: 0.9225 time: 0.15s
Test loss: 0.1441 score: 0.9535 time: 0.15s
Epoch 263/1000, LR 0.000242
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.23s
Val loss: 0.2355 score: 0.9225 time: 0.15s
Test loss: 0.1443 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.23s
Val loss: 0.2357 score: 0.9225 time: 0.15s
Test loss: 0.1447 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.28s
Val loss: 0.2357 score: 0.9225 time: 0.16s
Test loss: 0.1447 score: 0.9457 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.25s
Val loss: 0.2359 score: 0.9147 time: 0.16s
Test loss: 0.1451 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9147 time: 0.16s
Test loss: 0.1451 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 268/1000, LR 0.000240
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9070 time: 0.16s
Test loss: 0.1452 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 269/1000, LR 0.000240
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.24s
Val loss: 0.2362 score: 0.9070 time: 0.16s
Test loss: 0.1452 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 270/1000, LR 0.000240
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.28s
Val loss: 0.2361 score: 0.9070 time: 0.16s
Test loss: 0.1451 score: 0.9457 time: 0.23s
     INFO: Early stopping counter 8 of 20
Epoch 271/1000, LR 0.000239
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.35s
Val loss: 0.2360 score: 0.9070 time: 0.19s
Test loss: 0.1449 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 272/1000, LR 0.000239
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.23s
Val loss: 0.2361 score: 0.9070 time: 0.15s
Test loss: 0.1450 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 273/1000, LR 0.000239
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.23s
Val loss: 0.2363 score: 0.9070 time: 0.16s
Test loss: 0.1452 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 274/1000, LR 0.000238
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.24s
Val loss: 0.2363 score: 0.9070 time: 0.16s
Test loss: 0.1452 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 275/1000, LR 0.000238
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.24s
Val loss: 0.2361 score: 0.9070 time: 0.16s
Test loss: 0.1449 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 276/1000, LR 0.000238
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.24s
Val loss: 0.2358 score: 0.9070 time: 0.16s
Test loss: 0.1445 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 277/1000, LR 0.000237
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.24s
Val loss: 0.2360 score: 0.9070 time: 0.17s
Test loss: 0.1448 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 278/1000, LR 0.000237
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.29s
Val loss: 0.2360 score: 0.9070 time: 0.20s
Test loss: 0.1448 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 279/1000, LR 0.000236
Train loss: 0.5205;  Loss pred: 0.5205; Loss self: 0.0000; time: 0.24s
Val loss: 0.2364 score: 0.9070 time: 0.16s
Test loss: 0.1454 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 280/1000, LR 0.000236
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.24s
Val loss: 0.2366 score: 0.9070 time: 0.16s
Test loss: 0.1457 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 281/1000, LR 0.000236
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.23s
Val loss: 0.2368 score: 0.9070 time: 0.15s
Test loss: 0.1460 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 282/1000, LR 0.000235
Train loss: 0.5179;  Loss pred: 0.5179; Loss self: 0.0000; time: 0.23s
Val loss: 0.2370 score: 0.9070 time: 0.19s
Test loss: 0.1464 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 261,   Train_Loss: 0.5297,   Val_Loss: 0.2354,   Val_Precision: 0.9500,   Val_Recall: 0.8906,   Val_accuracy: 0.9194,   Val_Score: 0.9225,   Val_Loss: 0.2354,   Test_Precision: 0.9836,   Test_Recall: 0.9231,   Test_accuracy: 0.9524,   Test_Score: 0.9535,   Test_loss: 0.1441


[0.1761847159359604, 0.1546548930928111, 0.1587849569041282, 0.16226191190071404, 0.18021469586528838, 0.16158679011277854, 0.1592299030162394, 0.1577118900604546, 0.16152239101938903, 0.1714327740482986, 0.24523888598196208, 0.16704627103172243, 0.16452298406511545, 0.17415428115054965, 0.15610076393932104, 0.16079473705030978, 0.23738456005230546, 0.15375121706165373, 0.15569123695604503, 0.2328391510527581, 0.2646668890956789, 0.16611476591788232, 0.1655556638725102, 0.16798728005960584, 0.16675715893507004, 0.16703781904652715, 0.16067943000234663, 0.16801713895983994, 0.16800566297024488, 0.16599848913028836, 0.1683701160363853, 0.17367800092324615, 0.1672136678826064, 0.22280333191156387, 0.21284338412806392, 0.17031677812337875, 0.17187636299058795, 0.16688690893352032, 0.15493439813144505, 0.16676469310186803, 0.17436440312303603, 0.16869946592487395, 0.16115598217584193, 0.16105804685503244, 0.15485488693229854, 0.15304396115243435, 0.1566599258221686, 0.1554795578122139, 0.15735462890006602, 0.15528088784776628, 0.15677593811415136, 0.15698981704190373, 0.1561695800628513, 0.1556486680638045, 0.16999080101959407, 0.16746588191017509, 0.16547022620216012, 0.16756348893977702, 0.16509086987935007, 0.16813588701188564, 0.17873591207899153, 0.23185640387237072, 0.1652244790457189, 0.17877107695676386, 0.16707471502013505, 0.16924928594380617, 0.16485063498839736, 0.21412322390824556, 0.1561536800581962, 0.15701095294207335, 0.1568183028139174, 0.16807942185550928, 0.16000535106286407, 0.21348303696140647, 0.16164195397868752, 0.15552962222136557, 0.1551873160060495, 0.15528922295197845, 0.15861126990057528, 0.16130466503091156, 0.16118002100847661, 0.16775462916120887, 0.16885893722064793, 0.16675164294429123, 0.1672398669179529, 0.16679625306278467, 0.1709324198309332, 0.24515292490832508, 0.17225381499156356, 0.17233423609286547, 0.169909514952451, 0.17056448617950082, 0.20429906900972128, 0.23140644282102585, 0.16861508204601705, 0.16983293695375323, 0.16820121207274497, 0.1696435969788581, 0.17179657518863678, 0.20454163290560246, 0.16919332812540233, 0.169566789874807, 0.172919733915478, 0.16887591080740094, 0.24233703617937863, 0.18759063794277608, 0.17058546212501824, 0.16889236099086702, 0.16810543602332473, 0.16917433100752532, 0.2088379270862788, 0.16156752314418554, 0.1576315548736602, 0.16870742104947567, 0.17090845387429, 0.17238656803965569, 0.15310559701174498, 0.1659028958529234, 0.16369126294739544, 0.17197961686179042, 0.25874021905474365, 0.17292001307941973, 0.22514686593785882, 0.16812326502986252, 0.17325655790045857, 0.16907387203536928, 0.1674885000102222, 0.16908650984987617, 0.15707284188829362, 0.15604650508612394, 0.1555105799343437, 0.1556403269059956, 0.1556915300898254, 0.1563304818700999, 0.1725821599829942, 0.1784469799604267, 0.1602350149769336, 0.15666085807606578, 0.15737049980089068, 0.15530823194421828, 0.23051631287671626, 0.15747052477672696, 0.15738587803207338, 0.16464087110944092, 0.16640260303393006, 0.18420758796855807, 0.16828466509468853, 0.16688613314181566, 0.1661636100616306, 0.16530863801017404, 0.17013563797809184, 0.2072756551206112, 0.17055843607522547, 0.16552677401341498, 0.16564421216025949, 0.16898171696811914, 0.1704971850849688, 0.17451819707639515, 0.23470149212516844, 0.15468061808496714, 0.16875328100286424, 0.16582952486351132, 0.2452413779683411, 0.24731534905731678, 0.25722926505841315, 0.1680822018533945, 0.16897633392363787, 0.1694202129729092, 0.16873819194734097, 0.18465077807195485, 0.15841885400004685, 0.16327037778683007, 0.1580134800169617, 0.1641920479014516, 0.16838284605182707, 0.24073225702159107, 0.16650701290927827, 0.16920741088688374, 0.16270121606066823, 0.23472182103432715, 0.23467548098415136, 0.23683241987600923, 0.15765703306533396, 0.15988651989027858, 0.15925623988732696, 0.1591875208541751, 0.16826246515847743, 0.1631183698773384, 0.16467292397283018, 0.15607490204274654, 0.1667619829531759, 0.16451898496598005, 0.17463764105923474, 0.24520398792810738, 0.1678992249071598, 0.16679495107382536, 0.16648460482247174, 0.16874528815969825, 0.16857226914726198, 0.17039739899337292, 0.17122582206502557, 0.1717726420611143, 0.17041453393176198, 0.1676767449826002, 0.16957066697068512, 0.1700802471023053, 0.17198109184391797, 0.24774514697492123, 0.17160859797149897, 0.17064207000657916, 0.223316784016788, 0.16799355414696038, 0.17272847285494208, 0.25271914387121797, 0.1690982119180262, 0.17762667406350374, 0.23261798615567386, 0.17802069406025112, 0.15326821897178888, 0.15465641906484962, 0.16470201406627893, 0.15716381813399494, 0.16681200498715043, 0.1671683699823916, 0.1705365648958832, 0.1667598590720445, 0.1684489599429071, 0.15571019402705133, 0.1647240191232413, 0.15547158592380583, 0.15728697320446372, 0.15910502895712852, 0.15669275796972215, 0.15896221692673862, 0.15724002197384834, 0.1566341482102871, 0.15615573688410223, 0.15645128721371293, 0.21442424389533699, 0.1598028871230781, 0.15809707785956562, 0.1597498022019863, 0.1574920688290149, 0.15811321698129177, 0.2162758877966553, 0.16029149293899536, 0.15783051797188818, 0.1646299350541085, 0.23682046914473176, 0.23676157905720174, 0.23908881819806993, 0.20119319087825716, 0.16019623912870884, 0.16358892689459026, 0.15823833900503814, 0.1591863469220698, 0.16723872511647642, 0.1707039731554687, 0.2238825228996575, 0.15632898197509348, 0.15833012503571808, 0.15616479190066457, 0.15610537701286376, 0.1558227150235325, 0.24258922412991524, 0.15799267799593508, 0.15769893606193364, 0.15882583404891193, 0.16248247912153602, 0.23411965393461287, 0.15960331517271698, 0.15511303301900625, 0.15862507303245366, 0.15844860090874135, 0.16196936997584999, 0.15908914408646524, 0.16553416894748807, 0.16047764499671757, 0.1584020520094782, 0.1637739660218358, 0.15798120107501745, 0.1609294288791716]
[0.001365772991751631, 0.0011988751402543497, 0.0012308911387916915, 0.0012578442783001088, 0.0013970131462425455, 0.0012526107760680507, 0.00123434033345922, 0.0012225727911663149, 0.0012521115582898375, 0.0013289362329325473, 0.001901076635519086, 0.0012949323335792436, 0.001275371969497019, 0.0013500331872135633, 0.0012100834413900855, 0.0012464708298473627, 0.0018401903880023679, 0.0011918698997027421, 0.0012069088136127523, 0.0018049546593237062, 0.0020516813108192164, 0.0012877113637045142, 0.0012833772393217844, 0.0013022269772062468, 0.0012926911545354267, 0.0012948668143141638, 0.0012455769767623771, 0.001302458441549147, 0.0013023694803894952, 0.0012868099932580492, 0.0013051946979564751, 0.0013463410924282647, 0.0012962299835860961, 0.0017271576117175494, 0.001649948714171038, 0.0013202851017316182, 0.0013323749069037826, 0.0012936969684769018, 0.0012010418459801942, 0.0012927495589292095, 0.0013516620397134576, 0.0013077477978672399, 0.0012492711796576893, 0.0012485119911242826, 0.0012004254800953376, 0.0011863872957553051, 0.0012144180296292139, 0.001205267890017162, 0.0012198033248067134, 0.001203727812773382, 0.0012153173497221035, 0.0012169753259062305, 0.0012106168997120257, 0.0012065788222000349, 0.0013177581474387138, 0.0012981851310866286, 0.001282714931799691, 0.0012989417747269537, 0.0012797741851112408, 0.0013033789690843848, 0.0013855497060386941, 0.0017973364641269047, 0.0012808099150830924, 0.0013858223019904176, 0.00129515282961345, 0.001312009968556637, 0.0012779118991348632, 0.0016598699527770975, 0.0012104936438619858, 0.0012171391700935919, 0.0012156457582474217, 0.0013029412546938703, 0.0012403515586268533, 0.0016549072632667168, 0.0012530384029355622, 0.0012056559862121362, 0.0012030024496592986, 0.001203792425984329, 0.0012295447279114364, 0.001250423759929547, 0.0012494575271974932, 0.0013004234818698361, 0.001308984009462387, 0.0012926483949169862, 0.0012964330768833558, 0.0012929942097890284, 0.0013250575180692498, 0.0019004102706071712, 0.0013353008914074695, 0.0013359243107974067, 0.001317128022887217, 0.0013222053192209365, 0.0015837137132536532, 0.0017938483939614407, 0.00130709365927145, 0.0013165343949903352, 0.0013038853649049997, 0.0013150666432469621, 0.001331756396811138, 0.0015855940535318019, 0.0013115761870186227, 0.0013144712393395892, 0.0013404630536083565, 0.001309115587654271, 0.0018785816758091366, 0.0014541909918044658, 0.0013223679234497538, 0.0013092431084563334, 0.001303142914909494, 0.001311428922538956, 0.0016188986595835565, 0.0012524614197223685, 0.0012219500377803117, 0.0013078094654998115, 0.0013248717354596123, 0.0013363299848035324, 0.0011868650931143021, 0.0012860689601001812, 0.0012689245189720577, 0.0013331753245100033, 0.002005738132207315, 0.0013404652176699204, 0.0017453245421539443, 0.0013032811242625001, 0.0013430740922516168, 0.001310650170816816, 0.001298360465195521, 0.0013107481383711332, 0.0012176189293666173, 0.0012096628301249918, 0.0012055083715840596, 0.0012065141620619813, 0.0012069110859676388, 0.00121186420054341, 0.0013378462014185597, 0.0013833099221738504, 0.001242131899045997, 0.0012144252564036107, 0.0012199263550456642, 0.00120393978251332, 0.0017869481618350098, 0.0012207017424552477, 0.0012200455661401037, 0.0012762858225538055, 0.0012899426591777524, 0.0014279657982058766, 0.001304532287555725, 0.0012936909545877182, 0.0012880900004777567, 0.001281462310156388, 0.0013188809145588515, 0.0016067880241907844, 0.0013221584191877944, 0.0012831532869256976, 0.0012840636601570503, 0.001309935790450536, 0.0013216836053098356, 0.001352854240902288, 0.001819391411823011, 0.0011990745587981948, 0.0013081649690144515, 0.0012855001927403978, 0.0019010959532429544, 0.0019171732485063317, 0.0019940253105303345, 0.0013029628050650738, 0.0013098940614235494, 0.0013133349842861179, 0.0013080479995917905, 0.0014314013804027506, 0.0012280531317833089, 0.0012656618433087602, 0.0012249106978059047, 0.0012728065728794696, 0.0013052933802467216, 0.0018661415272991555, 0.0012907520380564207, 0.001311685355712277, 0.0012612497369044048, 0.001819549000266102, 0.0018191897750709407, 0.0018359102315969708, 0.0012221475431421238, 0.0012394303867463456, 0.0012345444952505966, 0.0012340117895672487, 0.001304360195026957, 0.001264483487421228, 0.0012765342943630246, 0.0012098829615716786, 0.0012927285500246193, 0.0012753409687285275, 0.0013537801632498817, 0.0019008061079698246, 0.0013015443791252698, 0.0012929841168513594, 0.0012905783319571453, 0.001308103008989909, 0.001306761776335364, 0.001320910069716069, 0.0013273319539924462, 0.0013315708686908085, 0.0013210428986958293, 0.0012998197285472885, 0.0013145012943463963, 0.0013184515279248473, 0.0013331867584799842, 0.0019205050153094669, 0.0013302992090813873, 0.0013228067442370478, 0.0017311378605952559, 0.0013022756135423286, 0.0013389804097282331, 0.0019590631307846353, 0.0013108388520777225, 0.001376950961732587, 0.0018032402027571618, 0.0013800053803120242, 0.0011881257284634798, 0.0011988869694949582, 0.0012767597989634025, 0.0012183241715813561, 0.0012931163177298483, 0.001295878837072803, 0.0013219888751618853, 0.0012927120858298024, 0.0013058058910302875, 0.0012070557676515607, 0.0012769303808003202, 0.0012052060924326034, 0.0012192788620501063, 0.0012333723174971203, 0.0012146725424009468, 0.0012322652474940979, 0.0012189148990220803, 0.0012142182031805201, 0.0012105095882488544, 0.001212800676075294, 0.001662203441049124, 0.0012387820707215357, 0.001225558743097408, 0.0012383705597053202, 0.0012208687506125185, 0.0012256838525681532, 0.001676557269741514, 0.0012425697127053904, 0.0012234923873789781, 0.0012762010469310736, 0.0018358175902692384, 0.0018353610779628042, 0.0018534016914579065, 0.0015596371385911407, 0.0012418313110752622, 0.0012681312162371337, 0.0012266537907367299, 0.0012340026893183705, 0.0012964242257091196, 0.0013232866136082845, 0.0017355234333306783, 0.0012118525734503372, 0.0012273653103544036, 0.0012105797821756943, 0.0012101192016501067, 0.0012079280234382364, 0.0018805366211621336, 0.001224749441828954, 0.0012224723725731289, 0.0012312080158830383, 0.0012595541017173335, 0.001814881038252813, 0.0012372350013388912, 0.001202426612550436, 0.0012296517289337493, 0.0012282837279747392, 0.0012555765114406975, 0.001233249178964847, 0.0012832106119960314, 0.0012440127519125392, 0.0012279228837944048, 0.0012695656280762465, 0.0012246604734497478, 0.001247514952551718]
[732.1860997686592, 834.1152188607757, 812.4195296276595, 795.0109701587483, 715.8128774161032, 798.3325859122841, 810.1493347442638, 817.9472070910527, 798.6508816880685, 752.48155270273, 526.0177213881499, 772.2411233921088, 784.084975926184, 740.7225314690053, 826.389293329432, 802.2650639345131, 543.4220320461283, 839.0177487068048, 828.5630104950574, 554.0305374621917, 487.4051319406471, 776.5715424947247, 779.1941210742225, 767.9152847419624, 773.5799819558482, 772.2801981991156, 802.8407867647777, 767.7788158911219, 767.8312606810575, 777.1155067486843, 766.1692171793877, 742.753827855315, 771.4680362765885, 578.9859554308787, 606.0794444161961, 757.4121670300235, 750.5395026718369, 772.9785447184918, 832.610456785442, 773.545032827785, 739.8299061590815, 764.673434457978, 800.4667171414362, 800.9534606868309, 833.0379657724195, 842.8950677218413, 823.4396851842854, 829.6910655985043, 819.8042911208307, 830.7525915647041, 822.8303498083538, 821.7093466996481, 826.0251448975097, 828.7896170567902, 758.8645928265892, 770.3061574607339, 779.5964443923385, 769.8574481602201, 781.3878507895342, 767.23656259583, 721.7352041876678, 556.3788527963637, 780.7559796530192, 721.5932364226843, 772.109651567884, 762.1893308479324, 782.5265581117075, 602.456836047257, 826.1092530891593, 821.5987329724218, 822.6080609549323, 767.4943105819093, 806.2230365615557, 604.2634667190006, 798.0601373886425, 829.4239911185156, 831.2535026700979, 830.7080011591783, 813.3091682631571, 799.7288855550404, 800.3473333287118, 768.9802698441994, 763.9512727208257, 773.6055712692234, 771.3471816100332, 773.3986683228575, 754.6842203930187, 526.2021656410567, 748.8948793750549, 748.5454017998256, 759.2276397004637, 756.3121895389252, 631.4272533168603, 557.4607103734404, 765.0561173691116, 759.5699769069391, 766.9385874830027, 760.4177363444885, 750.8880771246743, 630.6784499932808, 762.4414120182569, 760.7621757494029, 746.010863416285, 763.874488571206, 532.3164879532231, 687.6675798679838, 756.2191900354252, 763.8000868907018, 767.3755415149167, 762.527028963169, 617.7038902838049, 798.4277872780054, 818.3640648815013, 764.6373775233577, 754.7900473951082, 748.3181634564761, 842.5557426885198, 777.5632808384572, 788.0689395221785, 750.0888904972353, 498.5695709436904, 746.0096590482682, 572.959341284388, 767.2941634644477, 744.5605613042054, 762.9801012247117, 770.2021332337853, 762.9230747889524, 821.2750113208091, 826.6766367423824, 829.5255541742793, 828.8340339834549, 828.5614504885021, 825.1749656038949, 747.4700746167005, 722.9038004936127, 805.067481777126, 823.4347850779982, 819.7216134104817, 830.6063264330551, 559.6133236305546, 819.2009278111284, 819.6415181145499, 783.5235511736964, 775.2282575393153, 700.2968847408104, 766.558259645439, 772.9821380089084, 776.343267651404, 780.3584951928561, 758.218569213648, 622.3596298607112, 756.3390176907112, 779.3301160424071, 778.7775879255805, 763.3961964319354, 756.6107319350269, 739.1779319352605, 549.6343411877549, 833.9764968429297, 764.4295816554256, 777.9073123810464, 526.01237633175, 521.6012693579463, 501.4981478515126, 767.4816165992222, 763.4205157883022, 761.4203626377655, 764.4979391521376, 698.6160651309641, 814.2970154295009, 790.1004563634051, 816.3860449510555, 785.6653330581885, 766.1112935476497, 535.8650377644659, 774.7421429647887, 762.3779556927169, 792.8643873927674, 549.5867381717964, 549.6952619805727, 544.6889410982517, 818.2318130174483, 806.8222392264569, 810.0153569572338, 810.3650292925373, 766.6593965475413, 790.8367408097891, 783.3710417462682, 826.5262275459821, 773.5576041706169, 784.1040353286594, 738.6723687835713, 526.0925855652159, 768.3180197605486, 773.4047054152323, 774.8464198089496, 764.4657898709216, 765.2504213923092, 757.0538092838909, 753.391039063082, 750.9926985585026, 756.9776886028668, 769.3374535233631, 760.7447815387854, 758.4655020074433, 750.082457419647, 520.6963751869514, 751.7105874929678, 755.9683259528342, 577.6547453338856, 767.8866052631465, 746.8369161599351, 510.4480730028768, 762.8702783831645, 726.242275717446, 554.5572899666921, 724.6348559698342, 841.6617669690819, 834.1069887691405, 783.2326807375177, 820.7996059883008, 773.3256369044716, 771.6770822948623, 756.4360175705292, 773.5674563281367, 765.8106054422797, 828.4621363813148, 783.1280507033177, 829.7336084499766, 820.1569231821103, 810.7851828791631, 823.2671482170654, 811.5135941986302, 820.4018187014427, 823.5752003886967, 826.0983718820588, 824.5377989366467, 601.6110755785919, 807.2444892728745, 815.9543601089708, 807.5127369290478, 819.0888656117153, 815.8710730378948, 596.4603882300881, 804.7838199940876, 817.332425044545, 783.5755991618529, 544.7164278741558, 544.8519160654589, 539.548444683564, 641.1747805026783, 805.2623501126991, 788.5619304974236, 815.2259484718983, 810.3710053925189, 771.3524478864308, 755.6941857616472, 576.1950434059422, 825.1828827270969, 814.7533513972693, 826.050471620108, 826.3648726806498, 827.863896355023, 531.7631088630543, 816.493533980853, 818.0143964277438, 812.2104364978384, 793.9317561957477, 551.0003019055731, 808.2538878368588, 831.6515865188028, 813.2383962629126, 814.144140498266, 796.4468838721433, 810.8661388604131, 779.2953009050494, 803.8502808452765, 814.3833893785738, 787.6709780771906, 816.5528500998311, 801.5935985012118]
Elapsed: 0.17423844495738677~0.02531672611870913
Time per graph: 0.0013506856198247038~0.00019625369084270647
Speed: 752.8473550006466~86.50269205279974
Total Time: 0.1618
best val loss: 0.2353679313533759 test_score: 0.9535

Testing...
Test loss: 0.1561 score: 0.9535 time: 0.15s
test Score 0.9535
Epoch Time List: [0.6514348411001265, 0.5479835160076618, 0.543835316086188, 0.5608903067186475, 0.5674855799879879, 0.645805757958442, 0.5654887659475207, 0.5668850110378116, 0.5835749381221831, 0.5818482821341604, 0.8288890859112144, 0.6874047161545604, 0.5780567219480872, 0.5907871711533517, 0.5518349800258875, 0.568966040853411, 0.6502811082173139, 0.558662966825068, 0.6139067837502807, 0.6827350982930511, 0.8584778332151473, 0.650409989990294, 0.5780969799961895, 0.6574713753070682, 0.5677107872907072, 0.5731903361156583, 0.6690003371331841, 0.663868767907843, 0.5762715062592179, 0.5738839299883693, 0.5728925890289247, 0.5934950339142233, 0.5814659658353776, 0.6550932181999087, 0.8331951729487628, 0.6853766299318522, 0.6228819149546325, 0.5943403833080083, 0.5459639008622617, 0.571719435043633, 0.6070684096775949, 0.67064483393915, 0.5657713757827878, 0.5526248859241605, 0.5422062331344932, 0.5388611888047308, 0.5405601770617068, 0.5805209160316736, 0.6211097280029207, 0.546251927735284, 0.5540120000950992, 0.5487615403253585, 0.5449516321532428, 0.5425820790696889, 0.6170807520393282, 0.665134031092748, 0.5831636181101203, 0.5740805640816689, 0.5777133519295603, 0.5851070252247155, 0.5974329160526395, 0.720822888892144, 0.6321780362632126, 0.5933768870308995, 0.5753159276209772, 0.5724912760779262, 0.5662844991311431, 0.6544990579131991, 0.5486141450237483, 0.5537028987891972, 0.5585785571020097, 0.5628727211151272, 0.5581137479748577, 0.6298391248565167, 0.6273530973121524, 0.5551284332759678, 0.537341607036069, 0.5451390200760216, 0.5599424131214619, 0.5543598437216133, 0.5807774451095611, 0.6976082248147577, 0.5812663771212101, 0.5852044548373669, 0.5795763700734824, 0.5844443179666996, 0.6075666530523449, 0.7885066820308566, 0.7866816378664225, 0.5899617150425911, 0.587063561193645, 0.5853179232217371, 0.6241514489520341, 0.7469931731466204, 0.6412129218224436, 0.5863060329575092, 0.5791145809926093, 0.5875586587935686, 0.5888957346323878, 0.6857009381055832, 0.5954443216323853, 0.5863103300798684, 0.5918896098155528, 0.5891411970369518, 0.6740417440887541, 0.792027312098071, 0.586651710094884, 0.5906765698455274, 0.5857419609092176, 0.5911560789681971, 0.6275042474735528, 0.6650019958615303, 0.5570208597928286, 0.5753915419336408, 0.5933145030867308, 0.6544977549929172, 0.6514748248737305, 0.5837378248106688, 0.6537431557662785, 0.58354979660362, 0.6709226360544562, 0.5855118678882718, 0.6620401709806174, 0.6319984199944884, 0.5817320460919291, 0.5765212459955364, 0.5751727549359202, 0.583663399098441, 0.575212914030999, 0.6245843970682472, 0.5529889941681176, 0.5526393847540021, 0.5460633300244808, 0.5575012508779764, 0.5623063028324395, 0.6391850616782904, 0.5560168018564582, 0.5494875840377063, 0.5498797891195863, 0.5537669151090086, 0.697620635619387, 0.6710507250390947, 0.5500383640173823, 0.5687906160019338, 0.5749915530905128, 0.5945052767638117, 0.6695269090123475, 0.5788307979237288, 0.5828455418813974, 0.5739193938206881, 0.5818973190616816, 0.6216813160572201, 0.6805924160871655, 0.5797835097182542, 0.5806440240703523, 0.704303874168545, 0.5832063229754567, 0.6052217448595911, 0.7691395522560924, 0.6281094178557396, 0.5783505358267576, 0.5778328909073025, 0.751821924932301, 0.8561247140169144, 0.8702866218518466, 0.5877423970960081, 0.584331575781107, 0.5827174650039524, 0.588457620004192, 0.6013387490529567, 0.6478795579168946, 0.5607923429924995, 0.5526321225333959, 0.5654993038624525, 0.5839395031798631, 0.6662692399695516, 0.7014418307226151, 0.5914828500244766, 0.5778462977614254, 0.7158474458847195, 0.8182011898607016, 0.8183196177706122, 0.6863252171315253, 0.5506348090711981, 0.5511766690760851, 0.5530849087517709, 0.5821953939739615, 0.5651810998097062, 0.5909904916770756, 0.6214522290974855, 0.5646751327440143, 0.5709509910084307, 0.589108204934746, 0.7865909261163324, 0.718562857946381, 0.5791649462189525, 0.5807243043091148, 0.577527989866212, 0.5803511252161115, 0.579047636128962, 0.5897677359171212, 0.7743697501718998, 0.5936335811857134, 0.5855000899173319, 0.5868432619608939, 0.582811564905569, 0.5867910268716514, 0.698066424112767, 0.7382441158406436, 0.5824892281088978, 0.6630042020697147, 0.678390139946714, 0.5819976001512259, 0.6640927221160382, 0.5714111388660967, 0.5969323830213398, 0.7855599380563945, 0.7569130633492023, 0.6001123050227761, 0.5352803990244865, 0.5583624597638845, 0.5337138972245157, 0.5573283075354993, 0.5715679349377751, 0.6132525641005486, 0.6602767470758408, 0.5744741170201451, 0.5389897262211889, 0.55812221695669, 0.5530495762359351, 0.5503313988447189, 0.5646280799992383, 0.645521478028968, 0.5514049748890102, 0.5558957590255886, 0.5492079937830567, 0.5486940420232713, 0.5433853981085122, 0.6296349607873708, 0.6151712948922068, 0.5515101328492165, 0.5505502140149474, 0.5575964318122715, 0.5516721981111914, 0.6430764852557331, 0.596952143125236, 0.5535935340449214, 0.5585652417503297, 0.798212758032605, 0.8266647146083415, 0.8314333697780967, 0.7880301200784743, 0.5486171222291887, 0.5558237819932401, 0.5450187288224697, 0.551969330990687, 0.571517420001328, 0.5789052250329405, 0.650848297169432, 0.5596801040228456, 0.5454561747610569, 0.5433474171441048, 0.5388840732630342, 0.5404798078816384, 0.6748897910583764, 0.5552364247851074, 0.5488930230494589, 0.5455659588333219, 0.5550583966542035, 0.6693910979665816, 0.7018643422052264, 0.5368042576592416, 0.5454296569805592, 0.5495496029034257, 0.5522818409372121, 0.5534918729681522, 0.5669805146753788, 0.6440386162139475, 0.5552434488199651, 0.562889596214518, 0.5381789049133658, 0.5756728611886501]
Total Epoch List: [282]
Total Time List: [0.16181897185742855]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cfa3eb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.6776;  Loss pred: 3.6776; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.24s
Epoch 2/1000, LR 0.000015
Train loss: 3.6974;  Loss pred: 3.6974; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 3.6785;  Loss pred: 3.6785; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 3.6928;  Loss pred: 3.6928; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 3.6020;  Loss pred: 3.6020; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 3.5557;  Loss pred: 3.5557; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 3.4364;  Loss pred: 3.4364; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.21s
Epoch 8/1000, LR 0.000195
Train loss: 3.3773;  Loss pred: 3.3773; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 3.2896;  Loss pred: 3.2896; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 3.2346;  Loss pred: 3.2346; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 3.0744;  Loss pred: 3.0744; Loss self: 0.0000; time: 0.25s
Val loss: 0.6929 score: 0.5039 time: 0.16s
Test loss: 0.6929 score: 0.5116 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 3.0126;  Loss pred: 3.0126; Loss self: 0.0000; time: 0.25s
Val loss: 0.6928 score: 0.5116 time: 0.16s
Test loss: 0.6928 score: 0.5271 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 2.8839;  Loss pred: 2.8839; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 2.7771;  Loss pred: 2.7771; Loss self: 0.0000; time: 0.25s
Val loss: 0.6926 score: 0.6744 time: 0.16s
Test loss: 0.6926 score: 0.6822 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 2.7082;  Loss pred: 2.7082; Loss self: 0.0000; time: 0.26s
Val loss: 0.6925 score: 0.5891 time: 0.16s
Test loss: 0.6924 score: 0.5736 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 2.6196;  Loss pred: 2.6196; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.5504 time: 0.17s
Test loss: 0.6923 score: 0.5581 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 2.5292;  Loss pred: 2.5292; Loss self: 0.0000; time: 0.26s
Val loss: 0.6923 score: 0.5504 time: 0.16s
Test loss: 0.6922 score: 0.5659 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 2.4608;  Loss pred: 2.4608; Loss self: 0.0000; time: 0.27s
Val loss: 0.6922 score: 0.7364 time: 0.22s
Test loss: 0.6920 score: 0.7597 time: 0.23s
Epoch 19/1000, LR 0.000285
Train loss: 2.3815;  Loss pred: 2.3815; Loss self: 0.0000; time: 0.36s
Val loss: 0.6920 score: 0.8140 time: 0.25s
Test loss: 0.6918 score: 0.8527 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 2.2919;  Loss pred: 2.2919; Loss self: 0.0000; time: 0.25s
Val loss: 0.6919 score: 0.8450 time: 0.15s
Test loss: 0.6917 score: 0.8837 time: 0.15s
Epoch 21/1000, LR 0.000285
Train loss: 2.2389;  Loss pred: 2.2389; Loss self: 0.0000; time: 0.30s
Val loss: 0.6918 score: 0.7674 time: 0.16s
Test loss: 0.6915 score: 0.8140 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 2.1806;  Loss pred: 2.1806; Loss self: 0.0000; time: 0.25s
Val loss: 0.6916 score: 0.7597 time: 0.16s
Test loss: 0.6913 score: 0.8140 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 2.1032;  Loss pred: 2.1032; Loss self: 0.0000; time: 0.25s
Val loss: 0.6914 score: 0.5659 time: 0.17s
Test loss: 0.6911 score: 0.5659 time: 0.24s
Epoch 24/1000, LR 0.000285
Train loss: 2.0413;  Loss pred: 2.0413; Loss self: 0.0000; time: 0.33s
Val loss: 0.6912 score: 0.6047 time: 0.24s
Test loss: 0.6908 score: 0.6047 time: 0.23s
Epoch 25/1000, LR 0.000285
Train loss: 1.9864;  Loss pred: 1.9864; Loss self: 0.0000; time: 0.34s
Val loss: 0.6910 score: 0.5504 time: 0.17s
Test loss: 0.6906 score: 0.5659 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.9261;  Loss pred: 1.9261; Loss self: 0.0000; time: 0.34s
Val loss: 0.6908 score: 0.5349 time: 0.16s
Test loss: 0.6903 score: 0.5504 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 1.8625;  Loss pred: 1.8625; Loss self: 0.0000; time: 0.25s
Val loss: 0.6906 score: 0.5581 time: 0.17s
Test loss: 0.6900 score: 0.5891 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.8278;  Loss pred: 1.8278; Loss self: 0.0000; time: 0.25s
Val loss: 0.6904 score: 0.5039 time: 0.26s
Test loss: 0.6897 score: 0.5271 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.7684;  Loss pred: 1.7684; Loss self: 0.0000; time: 0.25s
Val loss: 0.6901 score: 0.5271 time: 0.16s
Test loss: 0.6894 score: 0.5349 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 1.7335;  Loss pred: 1.7335; Loss self: 0.0000; time: 0.25s
Val loss: 0.6898 score: 0.5581 time: 0.18s
Test loss: 0.6890 score: 0.5891 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 1.6905;  Loss pred: 1.6905; Loss self: 0.0000; time: 0.30s
Val loss: 0.6895 score: 0.6047 time: 0.20s
Test loss: 0.6887 score: 0.6512 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 1.6395;  Loss pred: 1.6395; Loss self: 0.0000; time: 0.25s
Val loss: 0.6892 score: 0.6512 time: 0.17s
Test loss: 0.6883 score: 0.6899 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 1.5958;  Loss pred: 1.5958; Loss self: 0.0000; time: 0.25s
Val loss: 0.6888 score: 0.6512 time: 0.16s
Test loss: 0.6879 score: 0.6822 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 1.5665;  Loss pred: 1.5665; Loss self: 0.0000; time: 0.25s
Val loss: 0.6884 score: 0.6899 time: 0.16s
Test loss: 0.6874 score: 0.7054 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 1.5356;  Loss pred: 1.5356; Loss self: 0.0000; time: 0.25s
Val loss: 0.6880 score: 0.7132 time: 0.16s
Test loss: 0.6869 score: 0.7519 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 1.4946;  Loss pred: 1.4946; Loss self: 0.0000; time: 0.25s
Val loss: 0.6876 score: 0.6899 time: 0.16s
Test loss: 0.6864 score: 0.7287 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 1.4680;  Loss pred: 1.4680; Loss self: 0.0000; time: 0.25s
Val loss: 0.6871 score: 0.7209 time: 0.17s
Test loss: 0.6859 score: 0.7519 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 1.4449;  Loss pred: 1.4449; Loss self: 0.0000; time: 0.33s
Val loss: 0.6866 score: 0.7287 time: 0.16s
Test loss: 0.6853 score: 0.7597 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 1.4148;  Loss pred: 1.4148; Loss self: 0.0000; time: 0.25s
Val loss: 0.6861 score: 0.7674 time: 0.16s
Test loss: 0.6847 score: 0.8140 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 1.3945;  Loss pred: 1.3945; Loss self: 0.0000; time: 0.24s
Val loss: 0.6855 score: 0.7752 time: 0.16s
Test loss: 0.6840 score: 0.8295 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 1.3619;  Loss pred: 1.3619; Loss self: 0.0000; time: 0.25s
Val loss: 0.6849 score: 0.7752 time: 0.16s
Test loss: 0.6833 score: 0.8295 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 1.3394;  Loss pred: 1.3394; Loss self: 0.0000; time: 0.29s
Val loss: 0.6842 score: 0.7829 time: 0.22s
Test loss: 0.6825 score: 0.8450 time: 0.23s
Epoch 43/1000, LR 0.000284
Train loss: 1.3093;  Loss pred: 1.3093; Loss self: 0.0000; time: 0.35s
Val loss: 0.6835 score: 0.7907 time: 0.20s
Test loss: 0.6817 score: 0.8605 time: 0.22s
Epoch 44/1000, LR 0.000284
Train loss: 1.2948;  Loss pred: 1.2948; Loss self: 0.0000; time: 0.25s
Val loss: 0.6827 score: 0.7984 time: 0.15s
Test loss: 0.6808 score: 0.8682 time: 0.15s
Epoch 45/1000, LR 0.000284
Train loss: 1.2752;  Loss pred: 1.2752; Loss self: 0.0000; time: 0.24s
Val loss: 0.6819 score: 0.7984 time: 0.15s
Test loss: 0.6799 score: 0.8682 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 1.2602;  Loss pred: 1.2602; Loss self: 0.0000; time: 0.25s
Val loss: 0.6811 score: 0.7907 time: 0.15s
Test loss: 0.6789 score: 0.8605 time: 0.15s
Epoch 47/1000, LR 0.000284
Train loss: 1.2440;  Loss pred: 1.2440; Loss self: 0.0000; time: 0.25s
Val loss: 0.6802 score: 0.7984 time: 0.16s
Test loss: 0.6778 score: 0.8682 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 1.2256;  Loss pred: 1.2256; Loss self: 0.0000; time: 0.25s
Val loss: 0.6792 score: 0.8062 time: 0.15s
Test loss: 0.6767 score: 0.8837 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 1.2141;  Loss pred: 1.2141; Loss self: 0.0000; time: 0.29s
Val loss: 0.6781 score: 0.8140 time: 0.17s
Test loss: 0.6754 score: 0.8837 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.2013;  Loss pred: 1.2013; Loss self: 0.0000; time: 0.26s
Val loss: 0.6770 score: 0.8140 time: 0.23s
Test loss: 0.6741 score: 0.8837 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 1.1900;  Loss pred: 1.1900; Loss self: 0.0000; time: 0.28s
Val loss: 0.6757 score: 0.8217 time: 0.17s
Test loss: 0.6726 score: 0.8837 time: 0.16s
Epoch 52/1000, LR 0.000284
Train loss: 1.1706;  Loss pred: 1.1706; Loss self: 0.0000; time: 0.26s
Val loss: 0.6742 score: 0.8140 time: 0.17s
Test loss: 0.6709 score: 0.8837 time: 0.16s
Epoch 53/1000, LR 0.000284
Train loss: 1.1587;  Loss pred: 1.1587; Loss self: 0.0000; time: 0.26s
Val loss: 0.6728 score: 0.8140 time: 0.17s
Test loss: 0.6692 score: 0.8837 time: 0.16s
Epoch 54/1000, LR 0.000284
Train loss: 1.1494;  Loss pred: 1.1494; Loss self: 0.0000; time: 0.26s
Val loss: 0.6714 score: 0.8140 time: 0.16s
Test loss: 0.6675 score: 0.8837 time: 0.16s
Epoch 55/1000, LR 0.000284
Train loss: 1.1389;  Loss pred: 1.1389; Loss self: 0.0000; time: 0.25s
Val loss: 0.6698 score: 0.8140 time: 0.15s
Test loss: 0.6657 score: 0.8837 time: 0.15s
Epoch 56/1000, LR 0.000284
Train loss: 1.1269;  Loss pred: 1.1269; Loss self: 0.0000; time: 0.25s
Val loss: 0.6680 score: 0.8140 time: 0.18s
Test loss: 0.6637 score: 0.8837 time: 0.15s
Epoch 57/1000, LR 0.000283
Train loss: 1.1111;  Loss pred: 1.1111; Loss self: 0.0000; time: 0.26s
Val loss: 0.6662 score: 0.8295 time: 0.24s
Test loss: 0.6615 score: 0.8837 time: 0.15s
Epoch 58/1000, LR 0.000283
Train loss: 1.1064;  Loss pred: 1.1064; Loss self: 0.0000; time: 0.26s
Val loss: 0.6642 score: 0.8295 time: 0.15s
Test loss: 0.6593 score: 0.8837 time: 0.15s
Epoch 59/1000, LR 0.000283
Train loss: 1.0956;  Loss pred: 1.0956; Loss self: 0.0000; time: 0.25s
Val loss: 0.6621 score: 0.8295 time: 0.15s
Test loss: 0.6568 score: 0.8837 time: 0.16s
Epoch 60/1000, LR 0.000283
Train loss: 1.0904;  Loss pred: 1.0904; Loss self: 0.0000; time: 0.26s
Val loss: 0.6598 score: 0.8295 time: 0.16s
Test loss: 0.6542 score: 0.8837 time: 0.16s
Epoch 61/1000, LR 0.000283
Train loss: 1.0805;  Loss pred: 1.0805; Loss self: 0.0000; time: 0.26s
Val loss: 0.6575 score: 0.8295 time: 0.15s
Test loss: 0.6516 score: 0.8837 time: 0.15s
Epoch 62/1000, LR 0.000283
Train loss: 1.0690;  Loss pred: 1.0690; Loss self: 0.0000; time: 0.26s
Val loss: 0.6551 score: 0.8295 time: 0.16s
Test loss: 0.6488 score: 0.8837 time: 0.15s
Epoch 63/1000, LR 0.000283
Train loss: 1.0645;  Loss pred: 1.0645; Loss self: 0.0000; time: 0.26s
Val loss: 0.6526 score: 0.8295 time: 0.24s
Test loss: 0.6460 score: 0.8915 time: 0.18s
Epoch 64/1000, LR 0.000283
Train loss: 1.0539;  Loss pred: 1.0539; Loss self: 0.0000; time: 0.26s
Val loss: 0.6500 score: 0.8372 time: 0.16s
Test loss: 0.6430 score: 0.8915 time: 0.15s
Epoch 65/1000, LR 0.000283
Train loss: 1.0485;  Loss pred: 1.0485; Loss self: 0.0000; time: 0.25s
Val loss: 0.6472 score: 0.8527 time: 0.16s
Test loss: 0.6398 score: 0.8915 time: 0.15s
Epoch 66/1000, LR 0.000283
Train loss: 1.0401;  Loss pred: 1.0401; Loss self: 0.0000; time: 0.25s
Val loss: 0.6443 score: 0.8527 time: 0.15s
Test loss: 0.6365 score: 0.8915 time: 0.15s
Epoch 67/1000, LR 0.000283
Train loss: 1.0333;  Loss pred: 1.0333; Loss self: 0.0000; time: 0.25s
Val loss: 0.6413 score: 0.8527 time: 0.16s
Test loss: 0.6331 score: 0.8915 time: 0.15s
Epoch 68/1000, LR 0.000283
Train loss: 1.0288;  Loss pred: 1.0288; Loss self: 0.0000; time: 0.33s
Val loss: 0.6381 score: 0.8527 time: 0.16s
Test loss: 0.6295 score: 0.8915 time: 0.15s
Epoch 69/1000, LR 0.000283
Train loss: 1.0211;  Loss pred: 1.0211; Loss self: 0.0000; time: 0.25s
Val loss: 0.6348 score: 0.8527 time: 0.15s
Test loss: 0.6257 score: 0.8915 time: 0.15s
Epoch 70/1000, LR 0.000283
Train loss: 1.0132;  Loss pred: 1.0132; Loss self: 0.0000; time: 0.25s
Val loss: 0.6314 score: 0.8527 time: 0.15s
Test loss: 0.6219 score: 0.8915 time: 0.15s
Epoch 71/1000, LR 0.000282
Train loss: 1.0084;  Loss pred: 1.0084; Loss self: 0.0000; time: 0.25s
Val loss: 0.6278 score: 0.8527 time: 0.16s
Test loss: 0.6178 score: 0.8915 time: 0.16s
Epoch 72/1000, LR 0.000282
Train loss: 1.0033;  Loss pred: 1.0033; Loss self: 0.0000; time: 0.25s
Val loss: 0.6241 score: 0.8527 time: 0.16s
Test loss: 0.6135 score: 0.8915 time: 0.15s
Epoch 73/1000, LR 0.000282
Train loss: 0.9962;  Loss pred: 0.9962; Loss self: 0.0000; time: 0.25s
Val loss: 0.6202 score: 0.8527 time: 0.17s
Test loss: 0.6091 score: 0.8915 time: 0.16s
Epoch 74/1000, LR 0.000282
Train loss: 0.9878;  Loss pred: 0.9878; Loss self: 0.0000; time: 0.26s
Val loss: 0.6163 score: 0.8527 time: 0.17s
Test loss: 0.6046 score: 0.8915 time: 0.16s
Epoch 75/1000, LR 0.000282
Train loss: 0.9814;  Loss pred: 0.9814; Loss self: 0.0000; time: 0.24s
Val loss: 0.6120 score: 0.8527 time: 0.17s
Test loss: 0.5999 score: 0.8915 time: 0.15s
Epoch 76/1000, LR 0.000282
Train loss: 0.9772;  Loss pred: 0.9772; Loss self: 0.0000; time: 0.26s
Val loss: 0.6078 score: 0.8527 time: 0.22s
Test loss: 0.5951 score: 0.8915 time: 0.15s
Epoch 77/1000, LR 0.000282
Train loss: 0.9702;  Loss pred: 0.9702; Loss self: 0.0000; time: 0.25s
Val loss: 0.6033 score: 0.8527 time: 0.15s
Test loss: 0.5901 score: 0.8915 time: 0.15s
Epoch 78/1000, LR 0.000282
Train loss: 0.9651;  Loss pred: 0.9651; Loss self: 0.0000; time: 0.25s
Val loss: 0.5987 score: 0.8450 time: 0.15s
Test loss: 0.5850 score: 0.8915 time: 0.15s
Epoch 79/1000, LR 0.000282
Train loss: 0.9592;  Loss pred: 0.9592; Loss self: 0.0000; time: 0.25s
Val loss: 0.5940 score: 0.8450 time: 0.16s
Test loss: 0.5798 score: 0.8915 time: 0.15s
Epoch 80/1000, LR 0.000282
Train loss: 0.9533;  Loss pred: 0.9533; Loss self: 0.0000; time: 0.25s
Val loss: 0.5892 score: 0.8450 time: 0.15s
Test loss: 0.5744 score: 0.8915 time: 0.15s
Epoch 81/1000, LR 0.000281
Train loss: 0.9456;  Loss pred: 0.9456; Loss self: 0.0000; time: 0.32s
Val loss: 0.5843 score: 0.8450 time: 0.16s
Test loss: 0.5689 score: 0.8915 time: 0.15s
Epoch 82/1000, LR 0.000281
Train loss: 0.9403;  Loss pred: 0.9403; Loss self: 0.0000; time: 0.25s
Val loss: 0.5793 score: 0.8450 time: 0.16s
Test loss: 0.5633 score: 0.8915 time: 0.15s
Epoch 83/1000, LR 0.000281
Train loss: 0.9342;  Loss pred: 0.9342; Loss self: 0.0000; time: 0.25s
Val loss: 0.5741 score: 0.8450 time: 0.15s
Test loss: 0.5576 score: 0.8915 time: 0.15s
Epoch 84/1000, LR 0.000281
Train loss: 0.9250;  Loss pred: 0.9250; Loss self: 0.0000; time: 0.24s
Val loss: 0.5690 score: 0.8450 time: 0.16s
Test loss: 0.5518 score: 0.8915 time: 0.15s
Epoch 85/1000, LR 0.000281
Train loss: 0.9213;  Loss pred: 0.9213; Loss self: 0.0000; time: 0.34s
Val loss: 0.5637 score: 0.8450 time: 0.23s
Test loss: 0.5459 score: 0.8915 time: 0.23s
Epoch 86/1000, LR 0.000281
Train loss: 0.9156;  Loss pred: 0.9156; Loss self: 0.0000; time: 0.36s
Val loss: 0.5584 score: 0.8450 time: 0.23s
Test loss: 0.5399 score: 0.8915 time: 0.23s
Epoch 87/1000, LR 0.000281
Train loss: 0.9081;  Loss pred: 0.9081; Loss self: 0.0000; time: 0.26s
Val loss: 0.5528 score: 0.8450 time: 0.16s
Test loss: 0.5337 score: 0.8915 time: 0.15s
Epoch 88/1000, LR 0.000281
Train loss: 0.9028;  Loss pred: 0.9028; Loss self: 0.0000; time: 0.25s
Val loss: 0.5472 score: 0.8450 time: 0.15s
Test loss: 0.5275 score: 0.8915 time: 0.15s
Epoch 89/1000, LR 0.000281
Train loss: 0.8960;  Loss pred: 0.8960; Loss self: 0.0000; time: 0.24s
Val loss: 0.5414 score: 0.8450 time: 0.17s
Test loss: 0.5212 score: 0.8915 time: 0.15s
Epoch 90/1000, LR 0.000281
Train loss: 0.8901;  Loss pred: 0.8901; Loss self: 0.0000; time: 0.30s
Val loss: 0.5357 score: 0.8450 time: 0.23s
Test loss: 0.5149 score: 0.8915 time: 0.23s
Epoch 91/1000, LR 0.000280
Train loss: 0.8828;  Loss pred: 0.8828; Loss self: 0.0000; time: 0.36s
Val loss: 0.5299 score: 0.8450 time: 0.23s
Test loss: 0.5085 score: 0.8915 time: 0.23s
Epoch 92/1000, LR 0.000280
Train loss: 0.8777;  Loss pred: 0.8777; Loss self: 0.0000; time: 0.36s
Val loss: 0.5240 score: 0.8450 time: 0.23s
Test loss: 0.5020 score: 0.8915 time: 0.24s
Epoch 93/1000, LR 0.000280
Train loss: 0.8718;  Loss pred: 0.8718; Loss self: 0.0000; time: 0.26s
Val loss: 0.5179 score: 0.8450 time: 0.15s
Test loss: 0.4955 score: 0.8915 time: 0.15s
Epoch 94/1000, LR 0.000280
Train loss: 0.8669;  Loss pred: 0.8669; Loss self: 0.0000; time: 0.25s
Val loss: 0.5121 score: 0.8450 time: 0.15s
Test loss: 0.4891 score: 0.8915 time: 0.15s
Epoch 95/1000, LR 0.000280
Train loss: 0.8597;  Loss pred: 0.8597; Loss self: 0.0000; time: 0.26s
Val loss: 0.5060 score: 0.8450 time: 0.15s
Test loss: 0.4825 score: 0.8915 time: 0.15s
Epoch 96/1000, LR 0.000280
Train loss: 0.8542;  Loss pred: 0.8542; Loss self: 0.0000; time: 0.25s
Val loss: 0.5000 score: 0.8450 time: 0.16s
Test loss: 0.4761 score: 0.8915 time: 0.16s
Epoch 97/1000, LR 0.000280
Train loss: 0.8481;  Loss pred: 0.8481; Loss self: 0.0000; time: 0.26s
Val loss: 0.4937 score: 0.8450 time: 0.16s
Test loss: 0.4695 score: 0.8915 time: 0.16s
Epoch 98/1000, LR 0.000280
Train loss: 0.8411;  Loss pred: 0.8411; Loss self: 0.0000; time: 0.26s
Val loss: 0.4878 score: 0.8450 time: 0.18s
Test loss: 0.4631 score: 0.8915 time: 0.16s
Epoch 99/1000, LR 0.000279
Train loss: 0.8374;  Loss pred: 0.8374; Loss self: 0.0000; time: 0.28s
Val loss: 0.4819 score: 0.8450 time: 0.22s
Test loss: 0.4568 score: 0.8915 time: 0.16s
Epoch 100/1000, LR 0.000279
Train loss: 0.8311;  Loss pred: 0.8311; Loss self: 0.0000; time: 0.26s
Val loss: 0.4762 score: 0.8450 time: 0.16s
Test loss: 0.4505 score: 0.8915 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.8249;  Loss pred: 0.8249; Loss self: 0.0000; time: 0.27s
Val loss: 0.4702 score: 0.8450 time: 0.16s
Test loss: 0.4442 score: 0.8915 time: 0.16s
Epoch 102/1000, LR 0.000279
Train loss: 0.8185;  Loss pred: 0.8185; Loss self: 0.0000; time: 0.26s
Val loss: 0.4641 score: 0.8450 time: 0.16s
Test loss: 0.4379 score: 0.8837 time: 0.16s
Epoch 103/1000, LR 0.000279
Train loss: 0.8132;  Loss pred: 0.8132; Loss self: 0.0000; time: 0.25s
Val loss: 0.4583 score: 0.8450 time: 0.17s
Test loss: 0.4318 score: 0.8837 time: 0.16s
Epoch 104/1000, LR 0.000279
Train loss: 0.8061;  Loss pred: 0.8061; Loss self: 0.0000; time: 0.29s
Val loss: 0.4528 score: 0.8450 time: 0.24s
Test loss: 0.4259 score: 0.8837 time: 0.23s
Epoch 105/1000, LR 0.000279
Train loss: 0.8013;  Loss pred: 0.8013; Loss self: 0.0000; time: 0.36s
Val loss: 0.4471 score: 0.8450 time: 0.24s
Test loss: 0.4200 score: 0.8837 time: 0.24s
Epoch 106/1000, LR 0.000279
Train loss: 0.7978;  Loss pred: 0.7978; Loss self: 0.0000; time: 0.36s
Val loss: 0.4417 score: 0.8450 time: 0.24s
Test loss: 0.4143 score: 0.8837 time: 0.20s
Epoch 107/1000, LR 0.000278
Train loss: 0.7905;  Loss pred: 0.7905; Loss self: 0.0000; time: 0.25s
Val loss: 0.4361 score: 0.8450 time: 0.16s
Test loss: 0.4085 score: 0.8837 time: 0.16s
Epoch 108/1000, LR 0.000278
Train loss: 0.7840;  Loss pred: 0.7840; Loss self: 0.0000; time: 0.25s
Val loss: 0.4308 score: 0.8450 time: 0.16s
Test loss: 0.4030 score: 0.8837 time: 0.16s
Epoch 109/1000, LR 0.000278
Train loss: 0.7779;  Loss pred: 0.7779; Loss self: 0.0000; time: 0.26s
Val loss: 0.4252 score: 0.8450 time: 0.16s
Test loss: 0.3973 score: 0.8915 time: 0.15s
Epoch 110/1000, LR 0.000278
Train loss: 0.7744;  Loss pred: 0.7744; Loss self: 0.0000; time: 0.24s
Val loss: 0.4200 score: 0.8450 time: 0.15s
Test loss: 0.3920 score: 0.8992 time: 0.15s
Epoch 111/1000, LR 0.000278
Train loss: 0.7689;  Loss pred: 0.7689; Loss self: 0.0000; time: 0.25s
Val loss: 0.4155 score: 0.8450 time: 0.16s
Test loss: 0.3871 score: 0.8915 time: 0.16s
Epoch 112/1000, LR 0.000278
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.26s
Val loss: 0.4105 score: 0.8450 time: 0.24s
Test loss: 0.3820 score: 0.8915 time: 0.24s
Epoch 113/1000, LR 0.000278
Train loss: 0.7592;  Loss pred: 0.7592; Loss self: 0.0000; time: 0.26s
Val loss: 0.4057 score: 0.8450 time: 0.16s
Test loss: 0.3771 score: 0.8915 time: 0.16s
Epoch 114/1000, LR 0.000277
Train loss: 0.7548;  Loss pred: 0.7548; Loss self: 0.0000; time: 0.23s
Val loss: 0.4005 score: 0.8450 time: 0.15s
Test loss: 0.3722 score: 0.8915 time: 0.15s
Epoch 115/1000, LR 0.000277
Train loss: 0.7496;  Loss pred: 0.7496; Loss self: 0.0000; time: 0.25s
Val loss: 0.3958 score: 0.8450 time: 0.15s
Test loss: 0.3675 score: 0.8915 time: 0.15s
Epoch 116/1000, LR 0.000277
Train loss: 0.7457;  Loss pred: 0.7457; Loss self: 0.0000; time: 0.26s
Val loss: 0.3913 score: 0.8527 time: 0.21s
Test loss: 0.3630 score: 0.8915 time: 0.22s
Epoch 117/1000, LR 0.000277
Train loss: 0.7405;  Loss pred: 0.7405; Loss self: 0.0000; time: 0.35s
Val loss: 0.3864 score: 0.8682 time: 0.18s
Test loss: 0.3584 score: 0.8915 time: 0.21s
Epoch 118/1000, LR 0.000277
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.32s
Val loss: 0.3821 score: 0.8682 time: 0.15s
Test loss: 0.3543 score: 0.8915 time: 0.15s
Epoch 119/1000, LR 0.000277
Train loss: 0.7312;  Loss pred: 0.7312; Loss self: 0.0000; time: 0.24s
Val loss: 0.3779 score: 0.8682 time: 0.15s
Test loss: 0.3502 score: 0.8915 time: 0.15s
Epoch 120/1000, LR 0.000277
Train loss: 0.7287;  Loss pred: 0.7287; Loss self: 0.0000; time: 0.24s
Val loss: 0.3745 score: 0.8682 time: 0.15s
Test loss: 0.3466 score: 0.8915 time: 0.22s
Epoch 121/1000, LR 0.000276
Train loss: 0.7250;  Loss pred: 0.7250; Loss self: 0.0000; time: 0.23s
Val loss: 0.3712 score: 0.8682 time: 0.15s
Test loss: 0.3431 score: 0.8915 time: 0.15s
Epoch 122/1000, LR 0.000276
Train loss: 0.7206;  Loss pred: 0.7206; Loss self: 0.0000; time: 0.23s
Val loss: 0.3671 score: 0.8682 time: 0.15s
Test loss: 0.3393 score: 0.8915 time: 0.16s
Epoch 123/1000, LR 0.000276
Train loss: 0.7146;  Loss pred: 0.7146; Loss self: 0.0000; time: 0.25s
Val loss: 0.3628 score: 0.8682 time: 0.31s
Test loss: 0.3356 score: 0.8915 time: 0.22s
Epoch 124/1000, LR 0.000276
Train loss: 0.7118;  Loss pred: 0.7118; Loss self: 0.0000; time: 0.35s
Val loss: 0.3594 score: 0.8682 time: 0.16s
Test loss: 0.3322 score: 0.8915 time: 0.15s
Epoch 125/1000, LR 0.000276
Train loss: 0.7088;  Loss pred: 0.7088; Loss self: 0.0000; time: 0.25s
Val loss: 0.3560 score: 0.8682 time: 0.15s
Test loss: 0.3290 score: 0.8915 time: 0.25s
Epoch 126/1000, LR 0.000276
Train loss: 0.7066;  Loss pred: 0.7066; Loss self: 0.0000; time: 0.25s
Val loss: 0.3522 score: 0.8682 time: 0.16s
Test loss: 0.3257 score: 0.8915 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.7009;  Loss pred: 0.7009; Loss self: 0.0000; time: 0.25s
Val loss: 0.3485 score: 0.8682 time: 0.16s
Test loss: 0.3225 score: 0.8915 time: 0.16s
Epoch 128/1000, LR 0.000275
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.26s
Val loss: 0.3444 score: 0.8682 time: 0.16s
Test loss: 0.3192 score: 0.8915 time: 0.23s
Epoch 129/1000, LR 0.000275
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.37s
Val loss: 0.3419 score: 0.8682 time: 0.24s
Test loss: 0.3167 score: 0.8915 time: 0.24s
Epoch 130/1000, LR 0.000275
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.34s
Val loss: 0.3393 score: 0.8682 time: 0.16s
Test loss: 0.3142 score: 0.8915 time: 0.16s
Epoch 131/1000, LR 0.000275
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.26s
Val loss: 0.3359 score: 0.8682 time: 0.16s
Test loss: 0.3114 score: 0.8915 time: 0.16s
Epoch 132/1000, LR 0.000275
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.26s
Val loss: 0.3332 score: 0.8760 time: 0.16s
Test loss: 0.3090 score: 0.8915 time: 0.16s
Epoch 133/1000, LR 0.000274
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.26s
Val loss: 0.3311 score: 0.8760 time: 0.16s
Test loss: 0.3070 score: 0.8915 time: 0.16s
Epoch 134/1000, LR 0.000274
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.26s
Val loss: 0.3281 score: 0.8760 time: 0.16s
Test loss: 0.3046 score: 0.8915 time: 0.16s
Epoch 135/1000, LR 0.000274
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.28s
Val loss: 0.3254 score: 0.8682 time: 0.22s
Test loss: 0.3023 score: 0.8915 time: 0.19s
Epoch 136/1000, LR 0.000274
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.26s
Val loss: 0.3220 score: 0.8682 time: 0.16s
Test loss: 0.2999 score: 0.8915 time: 0.16s
Epoch 137/1000, LR 0.000274
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.26s
Val loss: 0.3198 score: 0.8682 time: 0.16s
Test loss: 0.2980 score: 0.8915 time: 0.16s
Epoch 138/1000, LR 0.000274
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.31s
Val loss: 0.3176 score: 0.8682 time: 0.24s
Test loss: 0.2961 score: 0.8915 time: 0.24s
Epoch 139/1000, LR 0.000273
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.37s
Val loss: 0.3145 score: 0.8682 time: 0.24s
Test loss: 0.2939 score: 0.8915 time: 0.24s
Epoch 140/1000, LR 0.000273
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.39s
Val loss: 0.3116 score: 0.8682 time: 0.17s
Test loss: 0.2918 score: 0.8992 time: 0.16s
Epoch 141/1000, LR 0.000273
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.26s
Val loss: 0.3101 score: 0.8682 time: 0.17s
Test loss: 0.2904 score: 0.8992 time: 0.16s
Epoch 142/1000, LR 0.000273
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.26s
Val loss: 0.3082 score: 0.8682 time: 0.16s
Test loss: 0.2888 score: 0.8992 time: 0.16s
Epoch 143/1000, LR 0.000273
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.26s
Val loss: 0.3063 score: 0.8682 time: 0.17s
Test loss: 0.2872 score: 0.8992 time: 0.16s
Epoch 144/1000, LR 0.000272
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 0.26s
Val loss: 0.3031 score: 0.8682 time: 0.17s
Test loss: 0.2851 score: 0.8992 time: 0.18s
Epoch 145/1000, LR 0.000272
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.27s
Val loss: 0.3013 score: 0.8682 time: 0.24s
Test loss: 0.2837 score: 0.8992 time: 0.24s
Epoch 146/1000, LR 0.000272
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.29s
Val loss: 0.3002 score: 0.8682 time: 0.17s
Test loss: 0.2826 score: 0.8992 time: 0.16s
Epoch 147/1000, LR 0.000272
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.26s
Val loss: 0.2982 score: 0.8682 time: 0.16s
Test loss: 0.2811 score: 0.8992 time: 0.16s
Epoch 148/1000, LR 0.000272
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.26s
Val loss: 0.2955 score: 0.8760 time: 0.16s
Test loss: 0.2793 score: 0.8992 time: 0.17s
Epoch 149/1000, LR 0.000272
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.37s
Val loss: 0.2929 score: 0.8760 time: 0.24s
Test loss: 0.2776 score: 0.8992 time: 0.24s
Epoch 150/1000, LR 0.000271
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.33s
Val loss: 0.2918 score: 0.8760 time: 0.16s
Test loss: 0.2767 score: 0.8992 time: 0.16s
Epoch 151/1000, LR 0.000271
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.25s
Val loss: 0.2906 score: 0.8760 time: 0.15s
Test loss: 0.2757 score: 0.8992 time: 0.15s
Epoch 152/1000, LR 0.000271
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.25s
Val loss: 0.2889 score: 0.8760 time: 0.16s
Test loss: 0.2744 score: 0.8992 time: 0.18s
Epoch 153/1000, LR 0.000271
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.34s
Val loss: 0.2880 score: 0.8760 time: 0.23s
Test loss: 0.2737 score: 0.8992 time: 0.23s
Epoch 154/1000, LR 0.000271
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.36s
Val loss: 0.2858 score: 0.8760 time: 0.23s
Test loss: 0.2722 score: 0.8992 time: 0.21s
Epoch 155/1000, LR 0.000270
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 0.27s
Val loss: 0.2832 score: 0.8760 time: 0.16s
Test loss: 0.2706 score: 0.8992 time: 0.15s
Epoch 156/1000, LR 0.000270
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.24s
Val loss: 0.2801 score: 0.8760 time: 0.15s
Test loss: 0.2687 score: 0.9070 time: 0.15s
Epoch 157/1000, LR 0.000270
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.25s
Val loss: 0.2786 score: 0.8837 time: 0.16s
Test loss: 0.2677 score: 0.9070 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.29s
Val loss: 0.2769 score: 0.8760 time: 0.17s
Test loss: 0.2666 score: 0.9070 time: 0.24s
Epoch 159/1000, LR 0.000270
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.36s
Val loss: 0.2769 score: 0.8837 time: 0.24s
Test loss: 0.2663 score: 0.9070 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.37s
Val loss: 0.2758 score: 0.8837 time: 0.24s
Test loss: 0.2655 score: 0.9070 time: 0.19s
Epoch 161/1000, LR 0.000269
Train loss: 0.6219;  Loss pred: 0.6219; Loss self: 0.0000; time: 0.26s
Val loss: 0.2751 score: 0.8837 time: 0.16s
Test loss: 0.2649 score: 0.9070 time: 0.16s
Epoch 162/1000, LR 0.000269
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.26s
Val loss: 0.2733 score: 0.8837 time: 0.17s
Test loss: 0.2638 score: 0.9070 time: 0.16s
Epoch 163/1000, LR 0.000269
Train loss: 0.6185;  Loss pred: 0.6185; Loss self: 0.0000; time: 0.26s
Val loss: 0.2718 score: 0.8837 time: 0.16s
Test loss: 0.2628 score: 0.9070 time: 0.16s
Epoch 164/1000, LR 0.000269
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.26s
Val loss: 0.2706 score: 0.8837 time: 0.17s
Test loss: 0.2620 score: 0.9070 time: 0.18s
Epoch 165/1000, LR 0.000268
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.31s
Val loss: 0.2690 score: 0.8837 time: 0.19s
Test loss: 0.2610 score: 0.9070 time: 0.16s
Epoch 166/1000, LR 0.000268
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.26s
Val loss: 0.2661 score: 0.8837 time: 0.16s
Test loss: 0.2593 score: 0.9147 time: 0.16s
Epoch 167/1000, LR 0.000268
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.26s
Val loss: 0.2634 score: 0.8915 time: 0.16s
Test loss: 0.2578 score: 0.9147 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.26s
Val loss: 0.2614 score: 0.8915 time: 0.16s
Test loss: 0.2566 score: 0.9147 time: 0.16s
Epoch 169/1000, LR 0.000267
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.26s
Val loss: 0.2601 score: 0.8915 time: 0.17s
Test loss: 0.2557 score: 0.9147 time: 0.16s
Epoch 170/1000, LR 0.000267
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.26s
Val loss: 0.2589 score: 0.8915 time: 0.17s
Test loss: 0.2549 score: 0.9147 time: 0.16s
Epoch 171/1000, LR 0.000267
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.28s
Val loss: 0.2578 score: 0.8915 time: 0.17s
Test loss: 0.2542 score: 0.9147 time: 0.16s
Epoch 172/1000, LR 0.000267
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.36s
Val loss: 0.2567 score: 0.8837 time: 0.24s
Test loss: 0.2534 score: 0.9147 time: 0.19s
Epoch 173/1000, LR 0.000267
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.26s
Val loss: 0.2561 score: 0.8837 time: 0.16s
Test loss: 0.2529 score: 0.9147 time: 0.16s
Epoch 174/1000, LR 0.000266
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.26s
Val loss: 0.2565 score: 0.8837 time: 0.17s
Test loss: 0.2529 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 175/1000, LR 0.000266
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.25s
Val loss: 0.2572 score: 0.8915 time: 0.16s
Test loss: 0.2532 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.25s
Val loss: 0.2558 score: 0.8915 time: 0.16s
Test loss: 0.2523 score: 0.9147 time: 0.16s
Epoch 177/1000, LR 0.000266
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.26s
Val loss: 0.2551 score: 0.8915 time: 0.17s
Test loss: 0.2518 score: 0.9147 time: 0.16s
Epoch 178/1000, LR 0.000265
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.32s
Val loss: 0.2524 score: 0.8837 time: 0.17s
Test loss: 0.2503 score: 0.9147 time: 0.16s
Epoch 179/1000, LR 0.000265
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.26s
Val loss: 0.2495 score: 0.8837 time: 0.16s
Test loss: 0.2487 score: 0.9147 time: 0.16s
Epoch 180/1000, LR 0.000265
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.26s
Val loss: 0.2458 score: 0.8915 time: 0.16s
Test loss: 0.2467 score: 0.9147 time: 0.16s
Epoch 181/1000, LR 0.000265
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.26s
Val loss: 0.2448 score: 0.8915 time: 0.16s
Test loss: 0.2461 score: 0.9147 time: 0.16s
Epoch 182/1000, LR 0.000265
Train loss: 0.5943;  Loss pred: 0.5943; Loss self: 0.0000; time: 0.26s
Val loss: 0.2439 score: 0.8837 time: 0.17s
Test loss: 0.2455 score: 0.9147 time: 0.16s
Epoch 183/1000, LR 0.000264
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.26s
Val loss: 0.2443 score: 0.8837 time: 0.16s
Test loss: 0.2456 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.29s
Val loss: 0.2442 score: 0.8837 time: 0.17s
Test loss: 0.2455 score: 0.9147 time: 0.25s
     INFO: Early stopping counter 2 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.5897;  Loss pred: 0.5897; Loss self: 0.0000; time: 0.26s
Val loss: 0.2433 score: 0.8837 time: 0.17s
Test loss: 0.2449 score: 0.9147 time: 0.16s
Epoch 186/1000, LR 0.000264
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.26s
Val loss: 0.2418 score: 0.8837 time: 0.17s
Test loss: 0.2439 score: 0.9147 time: 0.20s
Epoch 187/1000, LR 0.000263
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.37s
Val loss: 0.2415 score: 0.8837 time: 0.25s
Test loss: 0.2437 score: 0.9147 time: 0.24s
Epoch 188/1000, LR 0.000263
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.36s
Val loss: 0.2419 score: 0.8837 time: 0.24s
Test loss: 0.2439 score: 0.9147 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.5874;  Loss pred: 0.5874; Loss self: 0.0000; time: 0.38s
Val loss: 0.2400 score: 0.8837 time: 0.17s
Test loss: 0.2428 score: 0.9147 time: 0.16s
Epoch 190/1000, LR 0.000263
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 0.26s
Val loss: 0.2398 score: 0.8837 time: 0.17s
Test loss: 0.2426 score: 0.9147 time: 0.16s
Epoch 191/1000, LR 0.000262
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.26s
Val loss: 0.2378 score: 0.8837 time: 0.16s
Test loss: 0.2415 score: 0.9147 time: 0.16s
Epoch 192/1000, LR 0.000262
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.26s
Val loss: 0.2356 score: 0.8915 time: 0.17s
Test loss: 0.2402 score: 0.9147 time: 0.17s
Epoch 193/1000, LR 0.000262
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.26s
Val loss: 0.2345 score: 0.8915 time: 0.18s
Test loss: 0.2396 score: 0.9147 time: 0.17s
Epoch 194/1000, LR 0.000262
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 0.37s
Val loss: 0.2354 score: 0.8915 time: 0.23s
Test loss: 0.2400 score: 0.9147 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.27s
Val loss: 0.2355 score: 0.8915 time: 0.17s
Test loss: 0.2400 score: 0.9147 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.27s
Val loss: 0.2336 score: 0.8915 time: 0.17s
Test loss: 0.2390 score: 0.9147 time: 0.16s
Epoch 197/1000, LR 0.000261
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 0.26s
Val loss: 0.2318 score: 0.8915 time: 0.17s
Test loss: 0.2379 score: 0.9147 time: 0.24s
Epoch 198/1000, LR 0.000261
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.37s
Val loss: 0.2314 score: 0.8915 time: 0.24s
Test loss: 0.2376 score: 0.9147 time: 0.24s
Epoch 199/1000, LR 0.000260
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.38s
Val loss: 0.2314 score: 0.8915 time: 0.24s
Test loss: 0.2376 score: 0.9147 time: 0.24s
Epoch 200/1000, LR 0.000260
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.40s
Val loss: 0.2313 score: 0.8915 time: 0.17s
Test loss: 0.2376 score: 0.9147 time: 0.17s
Epoch 201/1000, LR 0.000260
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.28s
Val loss: 0.2324 score: 0.8915 time: 0.17s
Test loss: 0.2382 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.26s
Val loss: 0.2331 score: 0.8915 time: 0.17s
Test loss: 0.2386 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.26s
Val loss: 0.2335 score: 0.8915 time: 0.17s
Test loss: 0.2389 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.31s
Val loss: 0.2301 score: 0.8915 time: 0.23s
Test loss: 0.2369 score: 0.9070 time: 0.23s
Epoch 205/1000, LR 0.000259
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.36s
Val loss: 0.2289 score: 0.8915 time: 0.24s
Test loss: 0.2362 score: 0.9070 time: 0.23s
Epoch 206/1000, LR 0.000259
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.33s
Val loss: 0.2279 score: 0.8915 time: 0.16s
Test loss: 0.2357 score: 0.9070 time: 0.15s
Epoch 207/1000, LR 0.000258
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.25s
Val loss: 0.2269 score: 0.8915 time: 0.16s
Test loss: 0.2351 score: 0.9070 time: 0.15s
Epoch 208/1000, LR 0.000258
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.25s
Val loss: 0.2252 score: 0.8915 time: 0.17s
Test loss: 0.2342 score: 0.9070 time: 0.17s
Epoch 209/1000, LR 0.000258
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.26s
Val loss: 0.2239 score: 0.8915 time: 0.27s
Test loss: 0.2334 score: 0.9070 time: 0.16s
Epoch 210/1000, LR 0.000258
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 0.24s
Val loss: 0.2244 score: 0.8915 time: 0.16s
Test loss: 0.2338 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 0.28s
Val loss: 0.2238 score: 0.8915 time: 0.17s
Test loss: 0.2334 score: 0.9070 time: 0.33s
Epoch 212/1000, LR 0.000257
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 0.37s
Val loss: 0.2241 score: 0.8915 time: 0.17s
Test loss: 0.2336 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.26s
Val loss: 0.2253 score: 0.8915 time: 0.16s
Test loss: 0.2344 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.33s
Val loss: 0.2257 score: 0.8915 time: 0.15s
Test loss: 0.2347 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.25s
Val loss: 0.2255 score: 0.8915 time: 0.16s
Test loss: 0.2346 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.25s
Val loss: 0.2250 score: 0.8915 time: 0.20s
Test loss: 0.2344 score: 0.9070 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.23s
Val loss: 0.2237 score: 0.8915 time: 0.16s
Test loss: 0.2337 score: 0.9070 time: 0.15s
Epoch 218/1000, LR 0.000255
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.25s
Val loss: 0.2221 score: 0.8915 time: 0.22s
Test loss: 0.2328 score: 0.9070 time: 0.16s
Epoch 219/1000, LR 0.000255
Train loss: 0.5569;  Loss pred: 0.5569; Loss self: 0.0000; time: 0.25s
Val loss: 0.2220 score: 0.8915 time: 0.16s
Test loss: 0.2328 score: 0.9070 time: 0.16s
Epoch 220/1000, LR 0.000255
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.26s
Val loss: 0.2218 score: 0.8915 time: 0.16s
Test loss: 0.2327 score: 0.9070 time: 0.16s
Epoch 221/1000, LR 0.000255
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.25s
Val loss: 0.2219 score: 0.8915 time: 0.16s
Test loss: 0.2329 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.26s
Val loss: 0.2206 score: 0.8915 time: 0.16s
Test loss: 0.2321 score: 0.9070 time: 0.17s
Epoch 223/1000, LR 0.000254
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.26s
Val loss: 0.2205 score: 0.8915 time: 0.24s
Test loss: 0.2322 score: 0.9070 time: 0.21s
Epoch 224/1000, LR 0.000254
Train loss: 0.5535;  Loss pred: 0.5535; Loss self: 0.0000; time: 0.32s
Val loss: 0.2201 score: 0.8915 time: 0.16s
Test loss: 0.2320 score: 0.9070 time: 0.16s
Epoch 225/1000, LR 0.000253
Train loss: 0.5535;  Loss pred: 0.5535; Loss self: 0.0000; time: 0.25s
Val loss: 0.2198 score: 0.8915 time: 0.17s
Test loss: 0.2318 score: 0.9070 time: 0.16s
Epoch 226/1000, LR 0.000253
Train loss: 0.5552;  Loss pred: 0.5552; Loss self: 0.0000; time: 0.25s
Val loss: 0.2204 score: 0.8915 time: 0.17s
Test loss: 0.2323 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.26s
Val loss: 0.2213 score: 0.8915 time: 0.22s
Test loss: 0.2330 score: 0.8992 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 0.36s
Val loss: 0.2195 score: 0.8915 time: 0.24s
Test loss: 0.2319 score: 0.8992 time: 0.24s
Epoch 229/1000, LR 0.000252
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.36s
Val loss: 0.2204 score: 0.8915 time: 0.24s
Test loss: 0.2326 score: 0.8992 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.34s
Val loss: 0.2221 score: 0.8915 time: 0.16s
Test loss: 0.2337 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.26s
Val loss: 0.2217 score: 0.8915 time: 0.16s
Test loss: 0.2336 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.26s
Val loss: 0.2202 score: 0.8915 time: 0.16s
Test loss: 0.2327 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.26s
Val loss: 0.2191 score: 0.8915 time: 0.16s
Test loss: 0.2322 score: 0.9070 time: 0.16s
Epoch 234/1000, LR 0.000251
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.26s
Val loss: 0.2171 score: 0.8915 time: 0.16s
Test loss: 0.2310 score: 0.8992 time: 0.16s
Epoch 235/1000, LR 0.000250
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.26s
Val loss: 0.2177 score: 0.8915 time: 0.16s
Test loss: 0.2315 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.30s
Val loss: 0.2178 score: 0.8915 time: 0.17s
Test loss: 0.2317 score: 0.9070 time: 0.25s
     INFO: Early stopping counter 2 of 20
Epoch 237/1000, LR 0.000250
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.26s
Val loss: 0.2164 score: 0.8915 time: 0.16s
Test loss: 0.2309 score: 0.9070 time: 0.16s
Epoch 238/1000, LR 0.000250
Train loss: 0.5454;  Loss pred: 0.5454; Loss self: 0.0000; time: 0.26s
Val loss: 0.2167 score: 0.8915 time: 0.17s
Test loss: 0.2312 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 239/1000, LR 0.000249
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.25s
Val loss: 0.2182 score: 0.8915 time: 0.16s
Test loss: 0.2323 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.26s
Val loss: 0.2197 score: 0.8915 time: 0.16s
Test loss: 0.2334 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.27s
Val loss: 0.2211 score: 0.8915 time: 0.16s
Test loss: 0.2344 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.33s
Val loss: 0.2229 score: 0.8992 time: 0.15s
Test loss: 0.2357 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.25s
Val loss: 0.2212 score: 0.8915 time: 0.16s
Test loss: 0.2348 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.25s
Val loss: 0.2190 score: 0.8915 time: 0.15s
Test loss: 0.2335 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.25s
Val loss: 0.2188 score: 0.8915 time: 0.15s
Test loss: 0.2336 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.25s
Val loss: 0.2176 score: 0.8915 time: 0.15s
Test loss: 0.2330 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.24s
Val loss: 0.2174 score: 0.8915 time: 0.15s
Test loss: 0.2330 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 248/1000, LR 0.000247
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.24s
Val loss: 0.2181 score: 0.8915 time: 0.17s
Test loss: 0.2336 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.28s
Val loss: 0.2175 score: 0.8915 time: 0.17s
Test loss: 0.2334 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 250/1000, LR 0.000246
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.34s
Val loss: 0.2194 score: 0.8915 time: 0.17s
Test loss: 0.2347 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.26s
Val loss: 0.2198 score: 0.8915 time: 0.17s
Test loss: 0.2351 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 0.26s
Val loss: 0.2211 score: 0.8992 time: 0.17s
Test loss: 0.2362 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.26s
Val loss: 0.2210 score: 0.8992 time: 0.17s
Test loss: 0.2363 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.26s
Val loss: 0.2213 score: 0.8992 time: 0.17s
Test loss: 0.2367 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.26s
Val loss: 0.2204 score: 0.8915 time: 0.17s
Test loss: 0.2362 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.27s
Val loss: 0.2191 score: 0.8915 time: 0.26s
Test loss: 0.2356 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 0.26s
Val loss: 0.2195 score: 0.8915 time: 0.17s
Test loss: 0.2360 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 236,   Train_Loss: 0.5453,   Val_Loss: 0.2164,   Val_Precision: 0.9474,   Val_Recall: 0.8308,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.2164,   Test_Precision: 0.9815,   Test_Recall: 0.8281,   Test_accuracy: 0.8983,   Test_Score: 0.9070,   Test_loss: 0.2309


[0.1761847159359604, 0.1546548930928111, 0.1587849569041282, 0.16226191190071404, 0.18021469586528838, 0.16158679011277854, 0.1592299030162394, 0.1577118900604546, 0.16152239101938903, 0.1714327740482986, 0.24523888598196208, 0.16704627103172243, 0.16452298406511545, 0.17415428115054965, 0.15610076393932104, 0.16079473705030978, 0.23738456005230546, 0.15375121706165373, 0.15569123695604503, 0.2328391510527581, 0.2646668890956789, 0.16611476591788232, 0.1655556638725102, 0.16798728005960584, 0.16675715893507004, 0.16703781904652715, 0.16067943000234663, 0.16801713895983994, 0.16800566297024488, 0.16599848913028836, 0.1683701160363853, 0.17367800092324615, 0.1672136678826064, 0.22280333191156387, 0.21284338412806392, 0.17031677812337875, 0.17187636299058795, 0.16688690893352032, 0.15493439813144505, 0.16676469310186803, 0.17436440312303603, 0.16869946592487395, 0.16115598217584193, 0.16105804685503244, 0.15485488693229854, 0.15304396115243435, 0.1566599258221686, 0.1554795578122139, 0.15735462890006602, 0.15528088784776628, 0.15677593811415136, 0.15698981704190373, 0.1561695800628513, 0.1556486680638045, 0.16999080101959407, 0.16746588191017509, 0.16547022620216012, 0.16756348893977702, 0.16509086987935007, 0.16813588701188564, 0.17873591207899153, 0.23185640387237072, 0.1652244790457189, 0.17877107695676386, 0.16707471502013505, 0.16924928594380617, 0.16485063498839736, 0.21412322390824556, 0.1561536800581962, 0.15701095294207335, 0.1568183028139174, 0.16807942185550928, 0.16000535106286407, 0.21348303696140647, 0.16164195397868752, 0.15552962222136557, 0.1551873160060495, 0.15528922295197845, 0.15861126990057528, 0.16130466503091156, 0.16118002100847661, 0.16775462916120887, 0.16885893722064793, 0.16675164294429123, 0.1672398669179529, 0.16679625306278467, 0.1709324198309332, 0.24515292490832508, 0.17225381499156356, 0.17233423609286547, 0.169909514952451, 0.17056448617950082, 0.20429906900972128, 0.23140644282102585, 0.16861508204601705, 0.16983293695375323, 0.16820121207274497, 0.1696435969788581, 0.17179657518863678, 0.20454163290560246, 0.16919332812540233, 0.169566789874807, 0.172919733915478, 0.16887591080740094, 0.24233703617937863, 0.18759063794277608, 0.17058546212501824, 0.16889236099086702, 0.16810543602332473, 0.16917433100752532, 0.2088379270862788, 0.16156752314418554, 0.1576315548736602, 0.16870742104947567, 0.17090845387429, 0.17238656803965569, 0.15310559701174498, 0.1659028958529234, 0.16369126294739544, 0.17197961686179042, 0.25874021905474365, 0.17292001307941973, 0.22514686593785882, 0.16812326502986252, 0.17325655790045857, 0.16907387203536928, 0.1674885000102222, 0.16908650984987617, 0.15707284188829362, 0.15604650508612394, 0.1555105799343437, 0.1556403269059956, 0.1556915300898254, 0.1563304818700999, 0.1725821599829942, 0.1784469799604267, 0.1602350149769336, 0.15666085807606578, 0.15737049980089068, 0.15530823194421828, 0.23051631287671626, 0.15747052477672696, 0.15738587803207338, 0.16464087110944092, 0.16640260303393006, 0.18420758796855807, 0.16828466509468853, 0.16688613314181566, 0.1661636100616306, 0.16530863801017404, 0.17013563797809184, 0.2072756551206112, 0.17055843607522547, 0.16552677401341498, 0.16564421216025949, 0.16898171696811914, 0.1704971850849688, 0.17451819707639515, 0.23470149212516844, 0.15468061808496714, 0.16875328100286424, 0.16582952486351132, 0.2452413779683411, 0.24731534905731678, 0.25722926505841315, 0.1680822018533945, 0.16897633392363787, 0.1694202129729092, 0.16873819194734097, 0.18465077807195485, 0.15841885400004685, 0.16327037778683007, 0.1580134800169617, 0.1641920479014516, 0.16838284605182707, 0.24073225702159107, 0.16650701290927827, 0.16920741088688374, 0.16270121606066823, 0.23472182103432715, 0.23467548098415136, 0.23683241987600923, 0.15765703306533396, 0.15988651989027858, 0.15925623988732696, 0.1591875208541751, 0.16826246515847743, 0.1631183698773384, 0.16467292397283018, 0.15607490204274654, 0.1667619829531759, 0.16451898496598005, 0.17463764105923474, 0.24520398792810738, 0.1678992249071598, 0.16679495107382536, 0.16648460482247174, 0.16874528815969825, 0.16857226914726198, 0.17039739899337292, 0.17122582206502557, 0.1717726420611143, 0.17041453393176198, 0.1676767449826002, 0.16957066697068512, 0.1700802471023053, 0.17198109184391797, 0.24774514697492123, 0.17160859797149897, 0.17064207000657916, 0.223316784016788, 0.16799355414696038, 0.17272847285494208, 0.25271914387121797, 0.1690982119180262, 0.17762667406350374, 0.23261798615567386, 0.17802069406025112, 0.15326821897178888, 0.15465641906484962, 0.16470201406627893, 0.15716381813399494, 0.16681200498715043, 0.1671683699823916, 0.1705365648958832, 0.1667598590720445, 0.1684489599429071, 0.15571019402705133, 0.1647240191232413, 0.15547158592380583, 0.15728697320446372, 0.15910502895712852, 0.15669275796972215, 0.15896221692673862, 0.15724002197384834, 0.1566341482102871, 0.15615573688410223, 0.15645128721371293, 0.21442424389533699, 0.1598028871230781, 0.15809707785956562, 0.1597498022019863, 0.1574920688290149, 0.15811321698129177, 0.2162758877966553, 0.16029149293899536, 0.15783051797188818, 0.1646299350541085, 0.23682046914473176, 0.23676157905720174, 0.23908881819806993, 0.20119319087825716, 0.16019623912870884, 0.16358892689459026, 0.15823833900503814, 0.1591863469220698, 0.16723872511647642, 0.1707039731554687, 0.2238825228996575, 0.15632898197509348, 0.15833012503571808, 0.15616479190066457, 0.15610537701286376, 0.1558227150235325, 0.24258922412991524, 0.15799267799593508, 0.15769893606193364, 0.15882583404891193, 0.16248247912153602, 0.23411965393461287, 0.15960331517271698, 0.15511303301900625, 0.15862507303245366, 0.15844860090874135, 0.16196936997584999, 0.15908914408646524, 0.16553416894748807, 0.16047764499671757, 0.1584020520094782, 0.1637739660218358, 0.15798120107501745, 0.1609294288791716, 0.24963408103212714, 0.16510470304638147, 0.16912259999662638, 0.1668455610051751, 0.1683214500080794, 0.1677822049241513, 0.21565099293366075, 0.16449814196676016, 0.16620515007525682, 0.16328041395172477, 0.1681322210934013, 0.18723254604265094, 0.16440063598565757, 0.16763911698944867, 0.16491808486171067, 0.16862075892277062, 0.16604061401449144, 0.23902688990347087, 0.1572276521474123, 0.153156811138615, 0.16334415622986853, 0.16620246693491936, 0.24549844209104776, 0.23898861417546868, 0.16823471314273775, 0.1674389662221074, 0.1659255779813975, 0.167211048072204, 0.16465058899484575, 0.16837147204205394, 0.1647290140390396, 0.1655403790064156, 0.1637715760152787, 0.1653494997881353, 0.1645078849978745, 0.16314410394988954, 0.1670308478642255, 0.16558869695290923, 0.16315624304115772, 0.16606758814305067, 0.16326569486409426, 0.23692546389065683, 0.2242261040955782, 0.15269680880010128, 0.1555963030550629, 0.15283555118367076, 0.1574578359723091, 0.15342774405144155, 0.1678557510022074, 0.17298787599429488, 0.16533716302365065, 0.1664405579213053, 0.16405037511140108, 0.167634645011276, 0.15447601699270308, 0.15932851401157677, 0.15766332298517227, 0.15516073605977, 0.1643465580418706, 0.1628922550007701, 0.15493113989941776, 0.15607661590911448, 0.1848291838541627, 0.15374314598739147, 0.1564022449310869, 0.15221963007934391, 0.15711520402692258, 0.15768694295547903, 0.15684198401868343, 0.1531852181069553, 0.16229493310675025, 0.159055506112054, 0.16570360003970563, 0.1688858058769256, 0.1578720931429416, 0.15617601503618062, 0.1542322940658778, 0.15446991892531514, 0.15617286902852356, 0.15943627408705652, 0.15475511900149286, 0.15890894597396255, 0.15542835602536798, 0.15313416603021324, 0.23050810093991458, 0.23865271895192564, 0.15325392899103463, 0.1532283970154822, 0.15623662201687694, 0.23049354297108948, 0.23264059890061617, 0.2407864178530872, 0.1538612220901996, 0.1552941899280995, 0.15233790292404592, 0.16654678992927074, 0.16889702808111906, 0.16703207488171756, 0.16544758202508092, 0.16369068808853626, 0.16271294304169714, 0.16185813187621534, 0.16815716680139303, 0.23908275505527854, 0.24105148599483073, 0.20395294507034123, 0.1617372438777238, 0.16186295705847442, 0.15902430983260274, 0.1501243009697646, 0.1648792859632522, 0.24196886806748807, 0.1601745099760592, 0.15163407288491726, 0.1520416741259396, 0.22850076807662845, 0.214912218041718, 0.15352580114267766, 0.1534421609248966, 0.2290843790397048, 0.15176179306581616, 0.16752813500352204, 0.22646666620858014, 0.15485038794577122, 0.25804551504552364, 0.16196759417653084, 0.163755051093176, 0.2381675790529698, 0.24111874890513718, 0.16318412288092077, 0.16637866594828665, 0.1653613750822842, 0.16239704098552465, 0.1633854398969561, 0.19753912603482604, 0.16261267592199147, 0.1640868268441409, 0.2410678940359503, 0.24110128707252443, 0.16594994999468327, 0.16765809897333384, 0.16486588888801634, 0.16844742209650576, 0.18535857019014657, 0.24119227705523372, 0.1657375511713326, 0.1642333660274744, 0.17005975008942187, 0.241093706805259, 0.16730159102007747, 0.1558912149630487, 0.18611751799471676, 0.23274136101827025, 0.21359370998106897, 0.15554518811404705, 0.15494602592661977, 0.17099798191338778, 0.24007728300057352, 0.24325647205114365, 0.19642865704372525, 0.16439776215702295, 0.1679159931372851, 0.16675720107741654, 0.18942122207954526, 0.16792821302078664, 0.16471158107742667, 0.16569202300161123, 0.1636063400655985, 0.163613063050434, 0.16455153212882578, 0.1681516559328884, 0.1970167199615389, 0.16443096683360636, 0.16677873395383358, 0.1649959480855614, 0.1660349650774151, 0.1690452869515866, 0.16537406807765365, 0.16631912696175277, 0.1644971570931375, 0.1665637199766934, 0.1669152060057968, 0.166575649054721, 0.25275734392926097, 0.1684838100336492, 0.20650596101768315, 0.2410835970658809, 0.2452835119329393, 0.16752745490521193, 0.16519116796553135, 0.1678838860243559, 0.17014243500307202, 0.1712453479412943, 0.21998390811495483, 0.16902484581805766, 0.16884981305338442, 0.2428435799665749, 0.24525876087136567, 0.24738248297944665, 0.17012784304097295, 0.16954852198250592, 0.16869311407208443, 0.16926628514192998, 0.23477193107828498, 0.23477374715730548, 0.1556712151505053, 0.1546984021551907, 0.17082575988024473, 0.16331070894375443, 0.15143458102829754, 0.33486716100014746, 0.16628118208609521, 0.1655308878980577, 0.15401695086620748, 0.155618793098256, 0.14909993414767087, 0.15315734315663576, 0.16630472498945892, 0.1629822759423405, 0.16248606285080314, 0.1634339860174805, 0.17232824908569455, 0.21556220785714686, 0.16340388194657862, 0.16716730408370495, 0.1631354158744216, 0.241091996897012, 0.24315846106037498, 0.22418635291978717, 0.1667042530607432, 0.16361410380341113, 0.1640671049244702, 0.1650441859383136, 0.16468317806720734, 0.1667199309449643, 0.25473977997899055, 0.16487939190119505, 0.16641816589981318, 0.16636257781647146, 0.15732117416337132, 0.1588465420063585, 0.15171762905083597, 0.15592920710332692, 0.1521620089188218, 0.1548619179520756, 0.1533006930258125, 0.1539220348931849, 0.16853732196614146, 0.16971066710539162, 0.1697887280024588, 0.16830030409619212, 0.16966684302315116, 0.1692621090915054, 0.16946087311953306, 0.18589046015404165, 0.1757383570075035, 0.1696217458229512]
[0.001365772991751631, 0.0011988751402543497, 0.0012308911387916915, 0.0012578442783001088, 0.0013970131462425455, 0.0012526107760680507, 0.00123434033345922, 0.0012225727911663149, 0.0012521115582898375, 0.0013289362329325473, 0.001901076635519086, 0.0012949323335792436, 0.001275371969497019, 0.0013500331872135633, 0.0012100834413900855, 0.0012464708298473627, 0.0018401903880023679, 0.0011918698997027421, 0.0012069088136127523, 0.0018049546593237062, 0.0020516813108192164, 0.0012877113637045142, 0.0012833772393217844, 0.0013022269772062468, 0.0012926911545354267, 0.0012948668143141638, 0.0012455769767623771, 0.001302458441549147, 0.0013023694803894952, 0.0012868099932580492, 0.0013051946979564751, 0.0013463410924282647, 0.0012962299835860961, 0.0017271576117175494, 0.001649948714171038, 0.0013202851017316182, 0.0013323749069037826, 0.0012936969684769018, 0.0012010418459801942, 0.0012927495589292095, 0.0013516620397134576, 0.0013077477978672399, 0.0012492711796576893, 0.0012485119911242826, 0.0012004254800953376, 0.0011863872957553051, 0.0012144180296292139, 0.001205267890017162, 0.0012198033248067134, 0.001203727812773382, 0.0012153173497221035, 0.0012169753259062305, 0.0012106168997120257, 0.0012065788222000349, 0.0013177581474387138, 0.0012981851310866286, 0.001282714931799691, 0.0012989417747269537, 0.0012797741851112408, 0.0013033789690843848, 0.0013855497060386941, 0.0017973364641269047, 0.0012808099150830924, 0.0013858223019904176, 0.00129515282961345, 0.001312009968556637, 0.0012779118991348632, 0.0016598699527770975, 0.0012104936438619858, 0.0012171391700935919, 0.0012156457582474217, 0.0013029412546938703, 0.0012403515586268533, 0.0016549072632667168, 0.0012530384029355622, 0.0012056559862121362, 0.0012030024496592986, 0.001203792425984329, 0.0012295447279114364, 0.001250423759929547, 0.0012494575271974932, 0.0013004234818698361, 0.001308984009462387, 0.0012926483949169862, 0.0012964330768833558, 0.0012929942097890284, 0.0013250575180692498, 0.0019004102706071712, 0.0013353008914074695, 0.0013359243107974067, 0.001317128022887217, 0.0013222053192209365, 0.0015837137132536532, 0.0017938483939614407, 0.00130709365927145, 0.0013165343949903352, 0.0013038853649049997, 0.0013150666432469621, 0.001331756396811138, 0.0015855940535318019, 0.0013115761870186227, 0.0013144712393395892, 0.0013404630536083565, 0.001309115587654271, 0.0018785816758091366, 0.0014541909918044658, 0.0013223679234497538, 0.0013092431084563334, 0.001303142914909494, 0.001311428922538956, 0.0016188986595835565, 0.0012524614197223685, 0.0012219500377803117, 0.0013078094654998115, 0.0013248717354596123, 0.0013363299848035324, 0.0011868650931143021, 0.0012860689601001812, 0.0012689245189720577, 0.0013331753245100033, 0.002005738132207315, 0.0013404652176699204, 0.0017453245421539443, 0.0013032811242625001, 0.0013430740922516168, 0.001310650170816816, 0.001298360465195521, 0.0013107481383711332, 0.0012176189293666173, 0.0012096628301249918, 0.0012055083715840596, 0.0012065141620619813, 0.0012069110859676388, 0.00121186420054341, 0.0013378462014185597, 0.0013833099221738504, 0.001242131899045997, 0.0012144252564036107, 0.0012199263550456642, 0.00120393978251332, 0.0017869481618350098, 0.0012207017424552477, 0.0012200455661401037, 0.0012762858225538055, 0.0012899426591777524, 0.0014279657982058766, 0.001304532287555725, 0.0012936909545877182, 0.0012880900004777567, 0.001281462310156388, 0.0013188809145588515, 0.0016067880241907844, 0.0013221584191877944, 0.0012831532869256976, 0.0012840636601570503, 0.001309935790450536, 0.0013216836053098356, 0.001352854240902288, 0.001819391411823011, 0.0011990745587981948, 0.0013081649690144515, 0.0012855001927403978, 0.0019010959532429544, 0.0019171732485063317, 0.0019940253105303345, 0.0013029628050650738, 0.0013098940614235494, 0.0013133349842861179, 0.0013080479995917905, 0.0014314013804027506, 0.0012280531317833089, 0.0012656618433087602, 0.0012249106978059047, 0.0012728065728794696, 0.0013052933802467216, 0.0018661415272991555, 0.0012907520380564207, 0.001311685355712277, 0.0012612497369044048, 0.001819549000266102, 0.0018191897750709407, 0.0018359102315969708, 0.0012221475431421238, 0.0012394303867463456, 0.0012345444952505966, 0.0012340117895672487, 0.001304360195026957, 0.001264483487421228, 0.0012765342943630246, 0.0012098829615716786, 0.0012927285500246193, 0.0012753409687285275, 0.0013537801632498817, 0.0019008061079698246, 0.0013015443791252698, 0.0012929841168513594, 0.0012905783319571453, 0.001308103008989909, 0.001306761776335364, 0.001320910069716069, 0.0013273319539924462, 0.0013315708686908085, 0.0013210428986958293, 0.0012998197285472885, 0.0013145012943463963, 0.0013184515279248473, 0.0013331867584799842, 0.0019205050153094669, 0.0013302992090813873, 0.0013228067442370478, 0.0017311378605952559, 0.0013022756135423286, 0.0013389804097282331, 0.0019590631307846353, 0.0013108388520777225, 0.001376950961732587, 0.0018032402027571618, 0.0013800053803120242, 0.0011881257284634798, 0.0011988869694949582, 0.0012767597989634025, 0.0012183241715813561, 0.0012931163177298483, 0.001295878837072803, 0.0013219888751618853, 0.0012927120858298024, 0.0013058058910302875, 0.0012070557676515607, 0.0012769303808003202, 0.0012052060924326034, 0.0012192788620501063, 0.0012333723174971203, 0.0012146725424009468, 0.0012322652474940979, 0.0012189148990220803, 0.0012142182031805201, 0.0012105095882488544, 0.001212800676075294, 0.001662203441049124, 0.0012387820707215357, 0.001225558743097408, 0.0012383705597053202, 0.0012208687506125185, 0.0012256838525681532, 0.001676557269741514, 0.0012425697127053904, 0.0012234923873789781, 0.0012762010469310736, 0.0018358175902692384, 0.0018353610779628042, 0.0018534016914579065, 0.0015596371385911407, 0.0012418313110752622, 0.0012681312162371337, 0.0012266537907367299, 0.0012340026893183705, 0.0012964242257091196, 0.0013232866136082845, 0.0017355234333306783, 0.0012118525734503372, 0.0012273653103544036, 0.0012105797821756943, 0.0012101192016501067, 0.0012079280234382364, 0.0018805366211621336, 0.001224749441828954, 0.0012224723725731289, 0.0012312080158830383, 0.0012595541017173335, 0.001814881038252813, 0.0012372350013388912, 0.001202426612550436, 0.0012296517289337493, 0.0012282837279747392, 0.0012555765114406975, 0.001233249178964847, 0.0012832106119960314, 0.0012440127519125392, 0.0012279228837944048, 0.0012695656280762465, 0.0012246604734497478, 0.001247514952551718, 0.0019351479149777298, 0.0012798814189641974, 0.001311027906950592, 0.0012933764419005823, 0.0013048174419230962, 0.0013006372474740411, 0.00167171312351675, 0.001275179395091164, 0.0012884120160872622, 0.001265739643036626, 0.001303350551111638, 0.0014514150856019452, 0.0012744235347725394, 0.0012995280386778967, 0.0012784347663698501, 0.0013071376660679893, 0.0012871365427479956, 0.001852921627158689, 0.001218819008894669, 0.0011872621018497289, 0.0012662337692237872, 0.0012883912165497626, 0.0019030886983802152, 0.0018526249160889046, 0.0013041450631219981, 0.0012979764823419179, 0.001286244790553469, 0.0012962096749783256, 0.0012763611549988043, 0.0013052052096283252, 0.0012769691010778264, 0.0012832587519877178, 0.0012695471008936333, 0.0012817790681250798, 0.0012752549224641435, 0.0012646829763557329, 0.001294812774141283, 0.0012836333097124746, 0.001264777077838432, 0.0012873456445197727, 0.001265625541582126, 0.0018366315030283475, 0.0017381868534540946, 0.0011836961922488472, 0.0012061728918997122, 0.001184771714602099, 0.0012206033796303031, 0.001189362356987919, 0.0013012073721101348, 0.0013409912867774797, 0.0012816834342918655, 0.0012902368831108938, 0.0012717083341969076, 0.0012994933721804341, 0.0011974885038194038, 0.0012351047597796649, 0.0012221963022106379, 0.0012027964035641085, 0.0012740043259059737, 0.0012627306589206985, 0.0012010165883675795, 0.0012098962473574765, 0.0014327843709625015, 0.0011918073332355928, 0.001212420503341759, 0.0011799971323980148, 0.0012179473180381597, 0.0012223794027556513, 0.0012158293334781662, 0.0011874823109066302, 0.0012581002566414748, 0.0012329884194732868, 0.0012845240313155476, 0.0013091922936195781, 0.001223814675526679, 0.0012106667832262063, 0.0011955991788052541, 0.0011974412319791872, 0.0012106423955699501, 0.0012359401092019884, 0.0011996520852828904, 0.0012318522943718026, 0.001204870976940837, 0.001187086558373746, 0.0017868845034101906, 0.0018500210771467103, 0.001188014953418873, 0.0011878170311277688, 0.0012111366047819917, 0.0017867716509386781, 0.0018034154953536136, 0.0018665613787061022, 0.0011927226518620123, 0.00120383092967519, 0.0011809139761553948, 0.0012910603870486105, 0.0013092792874505354, 0.0012948222859047872, 0.001282539395543263, 0.0012689200627018316, 0.0012613406437340863, 0.0012547142005908166, 0.0013035439286929692, 0.0018533546903509964, 0.0018686161705025637, 0.0015810305819406298, 0.0012537770843234403, 0.0012547516051044528, 0.0012327465878496336, 0.0011637542710834464, 0.0012781339997151332, 0.001875727659437892, 0.0012416628680314667, 0.0011754579293404438, 0.0011786176288832528, 0.0017713237835397554, 0.001665986186369907, 0.0011901224894781215, 0.001189474115696873, 0.0017758478995325954, 0.0011764480082621408, 0.001298667713205597, 0.0017555555520044972, 0.0012003906042307847, 0.002000352829810261, 0.0012555627455545027, 0.0012694190007222945, 0.0018462603027362, 0.001869137588411916, 0.0012649932006272928, 0.001289757100374315, 0.0012818711246688698, 0.0012588917905854624, 0.0012665537976508225, 0.0015313110545335353, 0.001260563379240244, 0.001271990905768534, 0.0018687433646197699, 0.0018690022253684065, 0.0012864337208890176, 0.0012996751858397972, 0.0012780301464187314, 0.0013057939697403548, 0.001436888141008888, 0.0018697075740715793, 0.0012847872183824232, 0.0012731268684300341, 0.0013182926363521075, 0.0018689434636066588, 0.0012969115582951742, 0.0012084590307213077, 0.0014427714573233858, 0.0018041965970408548, 0.0016557651936516973, 0.0012057766520468763, 0.001201131983927285, 0.0013255657512665719, 0.0018610642093067715, 0.0018857090856677801, 0.0015227027677808158, 0.0012744012570311857, 0.0013016743654053109, 0.0012926914812202833, 0.0014683815665081027, 0.0013017690931843926, 0.0012768339618405167, 0.0012844342868341956, 0.001268266202058903, 0.0012683183182204186, 0.001275593272316479, 0.0013035012087820805, 0.0015272613950506892, 0.0012746586576248555, 0.001292858402742896, 0.001279038357252414, 0.0012870927525381016, 0.0013104285810200513, 0.0012819695199818113, 0.0012892955578430447, 0.0012751717604119186, 0.0012911916277263053, 0.0012939163256263317, 0.0012912841011993876, 0.001959359255265589, 0.0013060760467724745, 0.0016008214032378538, 0.0018688650935339605, 0.0019014225731235604, 0.0012986624411256739, 0.0012805516896552819, 0.0013014254730570223, 0.001318933604674977, 0.0013274833173743746, 0.0017053016132942236, 0.001310270122620602, 0.0013089132794836002, 0.001882508371833914, 0.0019012307044291913, 0.0019176936665073383, 0.0013188204886897127, 0.0013143296277713637, 0.0013076985586983289, 0.001312141745286279, 0.0018199374502192633, 0.0018199515283512052, 0.0012067536058178706, 0.0011992124198076798, 0.0013242306967460832, 0.0012659744879360809, 0.001173911480839516, 0.0025958694651174222, 0.001289001411520118, 0.0012831851775043231, 0.001193929851676027, 0.001206347233319814, 0.0011558134430052006, 0.0011872662260204322, 0.0012891839146469684, 0.001263428495677058, 0.0012595818825643654, 0.0012669301241665155, 0.0013358778998891051, 0.0016710248671096657, 0.0012666967592758034, 0.0012958705742922864, 0.0012646156269335007, 0.0018689302085039688, 0.0018849493105455425, 0.0017378787048045517, 0.0012922810314786295, 0.0012683263860729544, 0.0012718380226703116, 0.0012794122940954543, 0.0012766137834667236, 0.0012924025654648395, 0.0019747269765813223, 0.0012781348209394965, 0.0012900633015489394, 0.0012896323861741974, 0.0012195439857625684, 0.0012313685426849495, 0.001176105651556868, 0.0012087535434366428, 0.001179550456735053, 0.001200479984124617, 0.001188377465316376, 0.0011931940689394178, 0.001306490867954585, 0.0013155865667084622, 0.0013161916899415412, 0.0013046535201255204, 0.0013152468451407067, 0.0013121093728023674, 0.0013136501792211865, 0.0014410113190235787, 0.001362312845019407, 0.0013148972544414822]
[732.1860997686592, 834.1152188607757, 812.4195296276595, 795.0109701587483, 715.8128774161032, 798.3325859122841, 810.1493347442638, 817.9472070910527, 798.6508816880685, 752.48155270273, 526.0177213881499, 772.2411233921088, 784.084975926184, 740.7225314690053, 826.389293329432, 802.2650639345131, 543.4220320461283, 839.0177487068048, 828.5630104950574, 554.0305374621917, 487.4051319406471, 776.5715424947247, 779.1941210742225, 767.9152847419624, 773.5799819558482, 772.2801981991156, 802.8407867647777, 767.7788158911219, 767.8312606810575, 777.1155067486843, 766.1692171793877, 742.753827855315, 771.4680362765885, 578.9859554308787, 606.0794444161961, 757.4121670300235, 750.5395026718369, 772.9785447184918, 832.610456785442, 773.545032827785, 739.8299061590815, 764.673434457978, 800.4667171414362, 800.9534606868309, 833.0379657724195, 842.8950677218413, 823.4396851842854, 829.6910655985043, 819.8042911208307, 830.7525915647041, 822.8303498083538, 821.7093466996481, 826.0251448975097, 828.7896170567902, 758.8645928265892, 770.3061574607339, 779.5964443923385, 769.8574481602201, 781.3878507895342, 767.23656259583, 721.7352041876678, 556.3788527963637, 780.7559796530192, 721.5932364226843, 772.109651567884, 762.1893308479324, 782.5265581117075, 602.456836047257, 826.1092530891593, 821.5987329724218, 822.6080609549323, 767.4943105819093, 806.2230365615557, 604.2634667190006, 798.0601373886425, 829.4239911185156, 831.2535026700979, 830.7080011591783, 813.3091682631571, 799.7288855550404, 800.3473333287118, 768.9802698441994, 763.9512727208257, 773.6055712692234, 771.3471816100332, 773.3986683228575, 754.6842203930187, 526.2021656410567, 748.8948793750549, 748.5454017998256, 759.2276397004637, 756.3121895389252, 631.4272533168603, 557.4607103734404, 765.0561173691116, 759.5699769069391, 766.9385874830027, 760.4177363444885, 750.8880771246743, 630.6784499932808, 762.4414120182569, 760.7621757494029, 746.010863416285, 763.874488571206, 532.3164879532231, 687.6675798679838, 756.2191900354252, 763.8000868907018, 767.3755415149167, 762.527028963169, 617.7038902838049, 798.4277872780054, 818.3640648815013, 764.6373775233577, 754.7900473951082, 748.3181634564761, 842.5557426885198, 777.5632808384572, 788.0689395221785, 750.0888904972353, 498.5695709436904, 746.0096590482682, 572.959341284388, 767.2941634644477, 744.5605613042054, 762.9801012247117, 770.2021332337853, 762.9230747889524, 821.2750113208091, 826.6766367423824, 829.5255541742793, 828.8340339834549, 828.5614504885021, 825.1749656038949, 747.4700746167005, 722.9038004936127, 805.067481777126, 823.4347850779982, 819.7216134104817, 830.6063264330551, 559.6133236305546, 819.2009278111284, 819.6415181145499, 783.5235511736964, 775.2282575393153, 700.2968847408104, 766.558259645439, 772.9821380089084, 776.343267651404, 780.3584951928561, 758.218569213648, 622.3596298607112, 756.3390176907112, 779.3301160424071, 778.7775879255805, 763.3961964319354, 756.6107319350269, 739.1779319352605, 549.6343411877549, 833.9764968429297, 764.4295816554256, 777.9073123810464, 526.01237633175, 521.6012693579463, 501.4981478515126, 767.4816165992222, 763.4205157883022, 761.4203626377655, 764.4979391521376, 698.6160651309641, 814.2970154295009, 790.1004563634051, 816.3860449510555, 785.6653330581885, 766.1112935476497, 535.8650377644659, 774.7421429647887, 762.3779556927169, 792.8643873927674, 549.5867381717964, 549.6952619805727, 544.6889410982517, 818.2318130174483, 806.8222392264569, 810.0153569572338, 810.3650292925373, 766.6593965475413, 790.8367408097891, 783.3710417462682, 826.5262275459821, 773.5576041706169, 784.1040353286594, 738.6723687835713, 526.0925855652159, 768.3180197605486, 773.4047054152323, 774.8464198089496, 764.4657898709216, 765.2504213923092, 757.0538092838909, 753.391039063082, 750.9926985585026, 756.9776886028668, 769.3374535233631, 760.7447815387854, 758.4655020074433, 750.082457419647, 520.6963751869514, 751.7105874929678, 755.9683259528342, 577.6547453338856, 767.8866052631465, 746.8369161599351, 510.4480730028768, 762.8702783831645, 726.242275717446, 554.5572899666921, 724.6348559698342, 841.6617669690819, 834.1069887691405, 783.2326807375177, 820.7996059883008, 773.3256369044716, 771.6770822948623, 756.4360175705292, 773.5674563281367, 765.8106054422797, 828.4621363813148, 783.1280507033177, 829.7336084499766, 820.1569231821103, 810.7851828791631, 823.2671482170654, 811.5135941986302, 820.4018187014427, 823.5752003886967, 826.0983718820588, 824.5377989366467, 601.6110755785919, 807.2444892728745, 815.9543601089708, 807.5127369290478, 819.0888656117153, 815.8710730378948, 596.4603882300881, 804.7838199940876, 817.332425044545, 783.5755991618529, 544.7164278741558, 544.8519160654589, 539.548444683564, 641.1747805026783, 805.2623501126991, 788.5619304974236, 815.2259484718983, 810.3710053925189, 771.3524478864308, 755.6941857616472, 576.1950434059422, 825.1828827270969, 814.7533513972693, 826.050471620108, 826.3648726806498, 827.863896355023, 531.7631088630543, 816.493533980853, 818.0143964277438, 812.2104364978384, 793.9317561957477, 551.0003019055731, 808.2538878368588, 831.6515865188028, 813.2383962629126, 814.144140498266, 796.4468838721433, 810.8661388604131, 779.2953009050494, 803.8502808452765, 814.3833893785738, 787.6709780771906, 816.5528500998311, 801.5935985012118, 516.7563638211647, 781.3223828261338, 762.7602697840103, 773.1701054725619, 766.390736259746, 768.8538844647832, 598.1887597414559, 784.2033864799931, 776.1492344947764, 790.0518921892245, 767.2532912554432, 688.9827795783658, 784.6684973363123, 769.510137709204, 782.2065124523536, 765.0303605802345, 776.9183507641169, 539.688233621312, 820.4663635061672, 842.2740003593323, 789.7435878787289, 776.1617644972328, 525.4615829788357, 539.7746685341522, 766.7858647611608, 770.4299835970188, 777.4569874601409, 771.4801233964879, 783.4773066255975, 766.1630467172006, 783.1043046820392, 779.2660665287019, 787.6824729827674, 780.1656501246727, 784.1569417883327, 790.7119955718592, 772.3124300061048, 779.0386806213321, 790.6531653064511, 776.7921569914014, 790.1231186832131, 544.4750339690571, 575.3121409316941, 844.8113684476322, 829.0685412644353, 844.0444582489431, 819.26694345454, 840.7866569213755, 768.5170107654136, 745.7170004460567, 780.2238628078262, 775.051475500295, 786.3438283051803, 769.530665879495, 835.0810857978913, 809.6479202123663, 818.1991699625158, 831.395901282058, 784.926691115336, 791.9345213766618, 832.6279667454044, 826.517151519472, 697.9417281947527, 839.0617947324906, 824.796345198493, 847.4596865907454, 821.0535752981303, 818.0766116851005, 822.4838572855159, 842.1178074109673, 794.8492139009026, 811.0376255011254, 778.4984754048145, 763.8297329380534, 817.1171828525765, 825.9911099032405, 836.4007083036693, 835.1140526096241, 826.007748992812, 809.1006939208985, 833.5750108450732, 811.7856374249492, 829.9643855137057, 842.3985537920285, 559.63326006328, 540.5343822040672, 841.7402467218085, 841.8805033049184, 825.6706931750305, 559.6686064918543, 554.5033868104366, 535.7445039890404, 838.4178823457872, 830.6814315443894, 846.8017317024381, 774.5571082744005, 763.7789809897837, 772.3067565995951, 779.7031447727313, 788.0717071103463, 792.8072443932269, 796.9942473984295, 767.1394710899194, 539.5621276414261, 535.1553817127946, 632.4988342556625, 797.5899484074693, 796.970488765985, 811.1967292031773, 859.287931178983, 782.3905789399837, 533.1264349429462, 805.3715913929204, 850.7322763657783, 848.4515889580796, 564.5495246507857, 600.2450729672288, 840.2496455961506, 840.7076596316967, 563.1112891274083, 850.016314343724, 770.0199133553773, 569.6202543167591, 833.0621686603455, 499.9118081057994, 796.4556160499675, 787.7619599446706, 541.6354338107021, 535.0060938262092, 790.5180830253584, 775.3397904999155, 780.1096231560078, 794.3494488393941, 789.5440382041246, 653.0351864433043, 793.2960900408764, 786.1691427705628, 535.1189569058181, 535.0448418020936, 777.3428072990256, 769.4230149926582, 782.4541563453558, 765.81759693594, 695.9483981111194, 534.8429957003086, 778.3390009584801, 785.4676739586507, 758.5569185663769, 535.0616642358004, 771.0626014579802, 827.5001258446616, 693.1104679982991, 554.2633223231579, 603.9503691912716, 829.340988069757, 832.5479742287328, 754.3948680361609, 537.3269739965019, 530.3044926709218, 656.7269864869281, 784.6822140850487, 768.241294886854, 773.5797864591894, 681.0218970386958, 768.1853911232415, 783.1871879085444, 778.5528697343841, 788.4780012087369, 788.4456020497308, 783.9489449360287, 767.1646127082189, 654.7667630705812, 784.5237578061543, 773.4799092293673, 781.837381443482, 776.9447835270886, 763.109119019359, 780.0497472156655, 775.6173469432966, 784.2080816446014, 774.4783799140081, 772.8475019556934, 774.42291674711, 510.3709272878858, 765.6522010882613, 624.6793039981723, 535.0841018219426, 525.9220197208718, 770.023039345933, 780.9134204252193, 768.3882179215534, 758.1882791184388, 753.3051352975924, 586.4065290293404, 763.2014061344487, 763.9925544911009, 531.206136961725, 525.9750948006234, 521.4597187575231, 758.2533093594335, 760.8441435621023, 764.702226937828, 762.1127851410772, 549.4694336223047, 549.46518323263, 828.6695769367564, 833.880623218005, 755.1554290783419, 789.9053334244521, 851.8529857846316, 385.22738274698486, 775.7943405358269, 779.3107476076896, 837.5701458475218, 828.9487241977954, 865.1915290065549, 842.2710745776664, 775.6845153267687, 791.4971076096479, 793.9142455463981, 789.3095135439117, 748.5714076735702, 598.4351398251049, 789.4549288747851, 771.682002692383, 790.7541063879203, 535.0654590791138, 530.5182449232971, 575.414151307219, 773.8254881415374, 788.4405867296052, 786.263645350397, 781.6088719914959, 783.3222646902952, 773.7527197188201, 506.3991183891231, 782.3900762401163, 775.1557608059471, 775.4147699148467, 819.9786245304717, 812.1045530524437, 850.263748563959, 827.2985054975479, 847.7806051366034, 833.0001442957786, 841.4834757353589, 838.0866332070019, 765.4091004597524, 760.1172171451664, 759.767750884687, 766.4870286049512, 760.3135515546657, 762.1315880582632, 761.2376687626703, 693.9570750059024, 734.0457837243356, 760.5156955208349]
Elapsed: 0.17570102110493008~0.02792934207799735
Time per graph: 0.0013620234194180627~0.00021650652773641356
Speed: 748.976869139649~93.80970457437148
Total Time: 0.1711
best val loss: 0.21639363031632217 test_score: 0.9070

Testing...
Test loss: 0.2357 score: 0.9070 time: 0.17s
test Score 0.9070
Epoch Time List: [0.6514348411001265, 0.5479835160076618, 0.543835316086188, 0.5608903067186475, 0.5674855799879879, 0.645805757958442, 0.5654887659475207, 0.5668850110378116, 0.5835749381221831, 0.5818482821341604, 0.8288890859112144, 0.6874047161545604, 0.5780567219480872, 0.5907871711533517, 0.5518349800258875, 0.568966040853411, 0.6502811082173139, 0.558662966825068, 0.6139067837502807, 0.6827350982930511, 0.8584778332151473, 0.650409989990294, 0.5780969799961895, 0.6574713753070682, 0.5677107872907072, 0.5731903361156583, 0.6690003371331841, 0.663868767907843, 0.5762715062592179, 0.5738839299883693, 0.5728925890289247, 0.5934950339142233, 0.5814659658353776, 0.6550932181999087, 0.8331951729487628, 0.6853766299318522, 0.6228819149546325, 0.5943403833080083, 0.5459639008622617, 0.571719435043633, 0.6070684096775949, 0.67064483393915, 0.5657713757827878, 0.5526248859241605, 0.5422062331344932, 0.5388611888047308, 0.5405601770617068, 0.5805209160316736, 0.6211097280029207, 0.546251927735284, 0.5540120000950992, 0.5487615403253585, 0.5449516321532428, 0.5425820790696889, 0.6170807520393282, 0.665134031092748, 0.5831636181101203, 0.5740805640816689, 0.5777133519295603, 0.5851070252247155, 0.5974329160526395, 0.720822888892144, 0.6321780362632126, 0.5933768870308995, 0.5753159276209772, 0.5724912760779262, 0.5662844991311431, 0.6544990579131991, 0.5486141450237483, 0.5537028987891972, 0.5585785571020097, 0.5628727211151272, 0.5581137479748577, 0.6298391248565167, 0.6273530973121524, 0.5551284332759678, 0.537341607036069, 0.5451390200760216, 0.5599424131214619, 0.5543598437216133, 0.5807774451095611, 0.6976082248147577, 0.5812663771212101, 0.5852044548373669, 0.5795763700734824, 0.5844443179666996, 0.6075666530523449, 0.7885066820308566, 0.7866816378664225, 0.5899617150425911, 0.587063561193645, 0.5853179232217371, 0.6241514489520341, 0.7469931731466204, 0.6412129218224436, 0.5863060329575092, 0.5791145809926093, 0.5875586587935686, 0.5888957346323878, 0.6857009381055832, 0.5954443216323853, 0.5863103300798684, 0.5918896098155528, 0.5891411970369518, 0.6740417440887541, 0.792027312098071, 0.586651710094884, 0.5906765698455274, 0.5857419609092176, 0.5911560789681971, 0.6275042474735528, 0.6650019958615303, 0.5570208597928286, 0.5753915419336408, 0.5933145030867308, 0.6544977549929172, 0.6514748248737305, 0.5837378248106688, 0.6537431557662785, 0.58354979660362, 0.6709226360544562, 0.5855118678882718, 0.6620401709806174, 0.6319984199944884, 0.5817320460919291, 0.5765212459955364, 0.5751727549359202, 0.583663399098441, 0.575212914030999, 0.6245843970682472, 0.5529889941681176, 0.5526393847540021, 0.5460633300244808, 0.5575012508779764, 0.5623063028324395, 0.6391850616782904, 0.5560168018564582, 0.5494875840377063, 0.5498797891195863, 0.5537669151090086, 0.697620635619387, 0.6710507250390947, 0.5500383640173823, 0.5687906160019338, 0.5749915530905128, 0.5945052767638117, 0.6695269090123475, 0.5788307979237288, 0.5828455418813974, 0.5739193938206881, 0.5818973190616816, 0.6216813160572201, 0.6805924160871655, 0.5797835097182542, 0.5806440240703523, 0.704303874168545, 0.5832063229754567, 0.6052217448595911, 0.7691395522560924, 0.6281094178557396, 0.5783505358267576, 0.5778328909073025, 0.751821924932301, 0.8561247140169144, 0.8702866218518466, 0.5877423970960081, 0.584331575781107, 0.5827174650039524, 0.588457620004192, 0.6013387490529567, 0.6478795579168946, 0.5607923429924995, 0.5526321225333959, 0.5654993038624525, 0.5839395031798631, 0.6662692399695516, 0.7014418307226151, 0.5914828500244766, 0.5778462977614254, 0.7158474458847195, 0.8182011898607016, 0.8183196177706122, 0.6863252171315253, 0.5506348090711981, 0.5511766690760851, 0.5530849087517709, 0.5821953939739615, 0.5651810998097062, 0.5909904916770756, 0.6214522290974855, 0.5646751327440143, 0.5709509910084307, 0.589108204934746, 0.7865909261163324, 0.718562857946381, 0.5791649462189525, 0.5807243043091148, 0.577527989866212, 0.5803511252161115, 0.579047636128962, 0.5897677359171212, 0.7743697501718998, 0.5936335811857134, 0.5855000899173319, 0.5868432619608939, 0.582811564905569, 0.5867910268716514, 0.698066424112767, 0.7382441158406436, 0.5824892281088978, 0.6630042020697147, 0.678390139946714, 0.5819976001512259, 0.6640927221160382, 0.5714111388660967, 0.5969323830213398, 0.7855599380563945, 0.7569130633492023, 0.6001123050227761, 0.5352803990244865, 0.5583624597638845, 0.5337138972245157, 0.5573283075354993, 0.5715679349377751, 0.6132525641005486, 0.6602767470758408, 0.5744741170201451, 0.5389897262211889, 0.55812221695669, 0.5530495762359351, 0.5503313988447189, 0.5646280799992383, 0.645521478028968, 0.5514049748890102, 0.5558957590255886, 0.5492079937830567, 0.5486940420232713, 0.5433853981085122, 0.6296349607873708, 0.6151712948922068, 0.5515101328492165, 0.5505502140149474, 0.5575964318122715, 0.5516721981111914, 0.6430764852557331, 0.596952143125236, 0.5535935340449214, 0.5585652417503297, 0.798212758032605, 0.8266647146083415, 0.8314333697780967, 0.7880301200784743, 0.5486171222291887, 0.5558237819932401, 0.5450187288224697, 0.551969330990687, 0.571517420001328, 0.5789052250329405, 0.650848297169432, 0.5596801040228456, 0.5454561747610569, 0.5433474171441048, 0.5388840732630342, 0.5404798078816384, 0.6748897910583764, 0.5552364247851074, 0.5488930230494589, 0.5455659588333219, 0.5550583966542035, 0.6693910979665816, 0.7018643422052264, 0.5368042576592416, 0.5454296569805592, 0.5495496029034257, 0.5522818409372121, 0.5534918729681522, 0.5669805146753788, 0.6440386162139475, 0.5552434488199651, 0.562889596214518, 0.5381789049133658, 0.5756728611886501, 0.6807274802122265, 0.5843959499616176, 0.5901080942712724, 0.5813424719963223, 0.5930975009687245, 0.597656280035153, 0.8089050508569926, 0.6621814600657672, 0.5809152869042009, 0.576172199100256, 0.5778989780228585, 0.599994502030313, 0.6533220519777387, 0.5802174219861627, 0.5783428209833801, 0.5816235761158168, 0.5820932791102678, 0.724549121921882, 0.7574351620860398, 0.5498989732004702, 0.6198216818738729, 0.571316379820928, 0.660717872902751, 0.8062451228033751, 0.6720353597775102, 0.6623916749376804, 0.5760202917736024, 0.679650858277455, 0.5740585711319, 0.5941246028523892, 0.6606450330000371, 0.5774014589842409, 0.5771647598594427, 0.5720363578293473, 0.5754935559816658, 0.5681153137702495, 0.5776335981208831, 0.6587920358870178, 0.5670429409947246, 0.567938880994916, 0.5687312260270119, 0.741788508137688, 0.7670630370266736, 0.5438444560859352, 0.542720657074824, 0.5451061497442424, 0.5577273142989725, 0.5534266538452357, 0.6203406318090856, 0.660791358910501, 0.603234649868682, 0.5854562681633979, 0.5833258589264005, 0.585320093203336, 0.550524661783129, 0.5823926208540797, 0.6466928506270051, 0.5608505825512111, 0.5623532370664179, 0.5710822788532823, 0.5615736290346831, 0.5769171821884811, 0.6766150128096342, 0.5607930987607688, 0.5540442552883178, 0.5490242652595043, 0.5576418759301305, 0.6443228190764785, 0.5588357441592962, 0.5483396057970822, 0.5678233681246638, 0.5625741840340197, 0.5769376321695745, 0.590520644094795, 0.5673778851050884, 0.6228481307625771, 0.5566889019683003, 0.5503687693271786, 0.5573633401654661, 0.5560891511850059, 0.6313047362491488, 0.5604544752277434, 0.5535481448750943, 0.5477278269827366, 0.8031589707825333, 0.8261191318742931, 0.5633641369640827, 0.5521739723626524, 0.5617218897677958, 0.7563742061611265, 0.8201960800215602, 0.8303176439367235, 0.5596285939682275, 0.5524211805313826, 0.5576768978498876, 0.5708728528115898, 0.5830703598912805, 0.603871809085831, 0.6610781161580235, 0.5813769020605832, 0.585755089763552, 0.5789614887908101, 0.5835501181427389, 0.7616803459823132, 0.8348039679694921, 0.800239983946085, 0.5692644689697772, 0.5655384652782232, 0.5757135488092899, 0.5357102581765503, 0.5762701858766377, 0.7383733899332583, 0.5770293159876019, 0.5348639970179647, 0.5462083029560745, 0.6959087559953332, 0.7407605196349323, 0.6168766871560365, 0.5434948080219328, 0.6153910181019455, 0.5330179089214653, 0.5502990940585732, 0.7776476612780243, 0.6572138308547437, 0.6542042507790029, 0.563481830060482, 0.5778834861703217, 0.6549268539529294, 0.8459754420910031, 0.6629017398227006, 0.5816641189157963, 0.5815122160129249, 0.5766320857219398, 0.5823722600471228, 0.6881094702985138, 0.5772253121249378, 0.5826270610559732, 0.7926414129324257, 0.8473923741839826, 0.7186442881356925, 0.5902395190205425, 0.5821062799077481, 0.5862884167581797, 0.6077192989178002, 0.7451740780379623, 0.6224611531943083, 0.588001930853352, 0.5870329809840769, 0.8503462220542133, 0.6604614299722016, 0.5537361919414252, 0.5912800228688866, 0.8016036788467318, 0.7988170520402491, 0.5754006078932434, 0.5448673472274095, 0.585379539988935, 0.692464197287336, 0.8431999441236258, 0.8106330600567162, 0.5836032521910965, 0.58832639711909, 0.5883104510139674, 0.6079184068366885, 0.6613830309361219, 0.586985879810527, 0.5842348842415959, 0.5785624019335955, 0.5840476367156953, 0.5815381209831685, 0.6135164890438318, 0.7910198378376663, 0.5817405229900032, 0.5897193709388375, 0.5782748369965702, 0.5787780378013849, 0.5974000887945294, 0.6528876179363579, 0.5844251881353557, 0.5834720758721232, 0.5841440150979906, 0.5869053949136287, 0.5877664957661182, 0.7037785092834383, 0.5954394908621907, 0.6304994428064674, 0.8473819890059531, 0.8473955208901316, 0.718061501160264, 0.585521134082228, 0.5879623331129551, 0.5928536460269243, 0.6014857692644, 0.8174558056052774, 0.6083881699014455, 0.6039700470864773, 0.6690604486502707, 0.8562319409102201, 0.8648251821286976, 0.7405825639143586, 0.612210385967046, 0.5924808159470558, 0.5932893448043615, 0.7703727190382779, 0.832765094935894, 0.6362756551243365, 0.560439090942964, 0.5836425716988742, 0.6819584460463375, 0.5485060778446496, 0.7749043249059469, 0.7000542108435184, 0.5840751142241061, 0.6298104557208717, 0.5571164397988468, 0.5872645941562951, 0.5400388999842107, 0.6267837840132415, 0.5672108801081777, 0.5752058271318674, 0.5736863508354872, 0.5876791020855308, 0.7114493630360812, 0.6430526867043227, 0.5811836831271648, 0.5774732772260904, 0.7160490441601723, 0.8432213880587369, 0.8221965557895601, 0.666370508261025, 0.5808187581133097, 0.5795498217921704, 0.5841713368427008, 0.5804641763679683, 0.5875575160607696, 0.717211005045101, 0.5864934169221669, 0.586860217154026, 0.576909254072234, 0.5678261239081621, 0.5823907281737775, 0.6310314980801195, 0.5596711670514196, 0.5454108028206974, 0.5481197910849005, 0.55400360096246, 0.5429114941507578, 0.5739486098755151, 0.6150992568582296, 0.6730800687801093, 0.5921797018963844, 0.5970183389727026, 0.5926933321170509, 0.5966905769892037, 0.6145398560911417, 0.6958901849575341, 0.5998946300242096]
Total Epoch List: [282, 257]
Total Time List: [0.16181897185742855, 0.17107051191851497]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cfa0520>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.0478;  Loss pred: 3.0478; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.15s
Epoch 2/1000, LR 0.000020
Train loss: 3.0235;  Loss pred: 3.0235; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 2.9734;  Loss pred: 2.9734; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 2.9656;  Loss pred: 2.9656; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 2.8840;  Loss pred: 2.8840; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.15s
Epoch 6/1000, LR 0.000140
Train loss: 2.8308;  Loss pred: 2.8308; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.15s
Epoch 7/1000, LR 0.000170
Train loss: 2.7549;  Loss pred: 2.7549; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.16s
Epoch 8/1000, LR 0.000200
Train loss: 2.6628;  Loss pred: 2.6628; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.16s
Epoch 9/1000, LR 0.000230
Train loss: 2.5506;  Loss pred: 2.5506; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.20s
Epoch 10/1000, LR 0.000260
Train loss: 2.4158;  Loss pred: 2.4158; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000290
Train loss: 2.3033;  Loss pred: 2.3033; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.16s
Epoch 12/1000, LR 0.000290
Train loss: 2.2170;  Loss pred: 2.2170; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.16s
Epoch 13/1000, LR 0.000290
Train loss: 2.1042;  Loss pred: 2.1042; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.16s
Epoch 14/1000, LR 0.000290
Train loss: 2.0226;  Loss pred: 2.0226; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000290
Train loss: 1.8993;  Loss pred: 1.8993; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 1.8218;  Loss pred: 1.8218; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 1.7445;  Loss pred: 1.7445; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.17s
Epoch 18/1000, LR 0.000290
Train loss: 1.6749;  Loss pred: 1.6749; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.17s
Epoch 19/1000, LR 0.000290
Train loss: 1.6064;  Loss pred: 1.6064; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.17s
Epoch 20/1000, LR 0.000290
Train loss: 1.5578;  Loss pred: 1.5578; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.21s
Epoch 21/1000, LR 0.000290
Train loss: 1.5016;  Loss pred: 1.5016; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.17s
Epoch 22/1000, LR 0.000290
Train loss: 1.4535;  Loss pred: 1.4535; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.16s
Epoch 23/1000, LR 0.000290
Train loss: 1.4214;  Loss pred: 1.4214; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.16s
Epoch 24/1000, LR 0.000290
Train loss: 1.3694;  Loss pred: 1.3694; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.16s
Epoch 25/1000, LR 0.000290
Train loss: 1.3382;  Loss pred: 1.3382; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.16s
Epoch 26/1000, LR 0.000290
Train loss: 1.3032;  Loss pred: 1.3032; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 1.2724;  Loss pred: 1.2724; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.21s
Epoch 28/1000, LR 0.000290
Train loss: 1.2416;  Loss pred: 1.2416; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 1.2215;  Loss pred: 1.2215; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 1.2004;  Loss pred: 1.2004; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.15s
Epoch 31/1000, LR 0.000290
Train loss: 1.1782;  Loss pred: 1.1782; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 1.1664;  Loss pred: 1.1664; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 1.1511;  Loss pred: 1.1511; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 1.1398;  Loss pred: 1.1398; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 1.1283;  Loss pred: 1.1283; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 0.16s
Epoch 36/1000, LR 0.000290
Train loss: 1.1129;  Loss pred: 1.1129; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.16s
Epoch 37/1000, LR 0.000290
Train loss: 1.1007;  Loss pred: 1.1007; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 1.0940;  Loss pred: 1.0940; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 1.0846;  Loss pred: 1.0846; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5000 time: 0.16s
Epoch 40/1000, LR 0.000289
Train loss: 1.0752;  Loss pred: 1.0752; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 1.0651;  Loss pred: 1.0651; Loss self: 0.0000; time: 0.26s
Val loss: 0.6874 score: 0.5194 time: 0.30s
Test loss: 0.6872 score: 0.5391 time: 0.17s
Epoch 42/1000, LR 0.000289
Train loss: 1.0581;  Loss pred: 1.0581; Loss self: 0.0000; time: 0.27s
Val loss: 0.6870 score: 0.6667 time: 0.17s
Test loss: 0.6868 score: 0.6406 time: 0.16s
Epoch 43/1000, LR 0.000289
Train loss: 1.0548;  Loss pred: 1.0548; Loss self: 0.0000; time: 0.28s
Val loss: 0.6865 score: 0.6512 time: 0.18s
Test loss: 0.6862 score: 0.6328 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 1.0463;  Loss pred: 1.0463; Loss self: 0.0000; time: 0.36s
Val loss: 0.6859 score: 0.5271 time: 0.17s
Test loss: 0.6856 score: 0.5469 time: 0.17s
Epoch 45/1000, LR 0.000289
Train loss: 1.0438;  Loss pred: 1.0438; Loss self: 0.0000; time: 0.40s
Val loss: 0.6854 score: 0.5039 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5000 time: 0.16s
Epoch 46/1000, LR 0.000289
Train loss: 1.0360;  Loss pred: 1.0360; Loss self: 0.0000; time: 0.30s
Val loss: 0.6848 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 0.15s
Epoch 47/1000, LR 0.000289
Train loss: 1.0334;  Loss pred: 1.0334; Loss self: 0.0000; time: 0.25s
Val loss: 0.6842 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.5000 time: 0.15s
Epoch 48/1000, LR 0.000289
Train loss: 1.0282;  Loss pred: 1.0282; Loss self: 0.0000; time: 0.26s
Val loss: 0.6835 score: 0.5194 time: 0.16s
Test loss: 0.6831 score: 0.5312 time: 0.15s
Epoch 49/1000, LR 0.000289
Train loss: 1.0242;  Loss pred: 1.0242; Loss self: 0.0000; time: 0.35s
Val loss: 0.6828 score: 0.6512 time: 0.17s
Test loss: 0.6824 score: 0.6406 time: 0.15s
Epoch 50/1000, LR 0.000289
Train loss: 1.0197;  Loss pred: 1.0197; Loss self: 0.0000; time: 0.25s
Val loss: 0.6821 score: 0.6899 time: 0.16s
Test loss: 0.6817 score: 0.7656 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 1.0157;  Loss pred: 1.0157; Loss self: 0.0000; time: 0.28s
Val loss: 0.6813 score: 0.7054 time: 0.16s
Test loss: 0.6809 score: 0.7656 time: 0.18s
Epoch 52/1000, LR 0.000289
Train loss: 1.0114;  Loss pred: 1.0114; Loss self: 0.0000; time: 0.31s
Val loss: 0.6804 score: 0.6977 time: 0.16s
Test loss: 0.6799 score: 0.7656 time: 0.16s
Epoch 53/1000, LR 0.000289
Train loss: 1.0091;  Loss pred: 1.0091; Loss self: 0.0000; time: 0.25s
Val loss: 0.6795 score: 0.7364 time: 0.17s
Test loss: 0.6789 score: 0.7734 time: 0.16s
Epoch 54/1000, LR 0.000289
Train loss: 1.0070;  Loss pred: 1.0070; Loss self: 0.0000; time: 0.26s
Val loss: 0.6786 score: 0.7984 time: 0.17s
Test loss: 0.6780 score: 0.8047 time: 0.17s
Epoch 55/1000, LR 0.000289
Train loss: 1.0028;  Loss pred: 1.0028; Loss self: 0.0000; time: 0.26s
Val loss: 0.6777 score: 0.7984 time: 0.17s
Test loss: 0.6771 score: 0.8125 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 0.9995;  Loss pred: 0.9995; Loss self: 0.0000; time: 0.24s
Val loss: 0.6769 score: 0.8450 time: 0.15s
Test loss: 0.6762 score: 0.8516 time: 0.15s
Epoch 57/1000, LR 0.000288
Train loss: 0.9969;  Loss pred: 0.9969; Loss self: 0.0000; time: 0.27s
Val loss: 0.6759 score: 0.8527 time: 0.17s
Test loss: 0.6752 score: 0.8672 time: 0.21s
Epoch 58/1000, LR 0.000288
Train loss: 0.9939;  Loss pred: 0.9939; Loss self: 0.0000; time: 0.45s
Val loss: 0.6748 score: 0.8527 time: 0.24s
Test loss: 0.6740 score: 0.8672 time: 0.21s
Epoch 59/1000, LR 0.000288
Train loss: 0.9923;  Loss pred: 0.9923; Loss self: 0.0000; time: 0.25s
Val loss: 0.6738 score: 0.8837 time: 0.15s
Test loss: 0.6730 score: 0.8984 time: 0.15s
Epoch 60/1000, LR 0.000288
Train loss: 0.9886;  Loss pred: 0.9886; Loss self: 0.0000; time: 0.25s
Val loss: 0.6728 score: 0.8837 time: 0.15s
Test loss: 0.6721 score: 0.9062 time: 0.15s
Epoch 61/1000, LR 0.000288
Train loss: 0.9880;  Loss pred: 0.9880; Loss self: 0.0000; time: 0.25s
Val loss: 0.6714 score: 0.8837 time: 0.15s
Test loss: 0.6706 score: 0.8984 time: 0.15s
Epoch 62/1000, LR 0.000288
Train loss: 0.9852;  Loss pred: 0.9852; Loss self: 0.0000; time: 0.25s
Val loss: 0.6699 score: 0.8527 time: 0.15s
Test loss: 0.6689 score: 0.8594 time: 0.15s
Epoch 63/1000, LR 0.000288
Train loss: 0.9826;  Loss pred: 0.9826; Loss self: 0.0000; time: 0.26s
Val loss: 0.6686 score: 0.7984 time: 0.16s
Test loss: 0.6674 score: 0.8047 time: 0.15s
Epoch 64/1000, LR 0.000288
Train loss: 0.9798;  Loss pred: 0.9798; Loss self: 0.0000; time: 0.25s
Val loss: 0.6676 score: 0.6977 time: 0.17s
Test loss: 0.6662 score: 0.7500 time: 0.16s
Epoch 65/1000, LR 0.000288
Train loss: 0.9776;  Loss pred: 0.9776; Loss self: 0.0000; time: 0.26s
Val loss: 0.6666 score: 0.6589 time: 0.23s
Test loss: 0.6651 score: 0.6484 time: 0.16s
Epoch 66/1000, LR 0.000288
Train loss: 0.9761;  Loss pred: 0.9761; Loss self: 0.0000; time: 0.27s
Val loss: 0.6651 score: 0.6589 time: 0.17s
Test loss: 0.6635 score: 0.6484 time: 0.17s
Epoch 67/1000, LR 0.000288
Train loss: 0.9726;  Loss pred: 0.9726; Loss self: 0.0000; time: 0.27s
Val loss: 0.6632 score: 0.6744 time: 0.17s
Test loss: 0.6615 score: 0.6797 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 0.9701;  Loss pred: 0.9701; Loss self: 0.0000; time: 0.26s
Val loss: 0.6609 score: 0.6977 time: 0.17s
Test loss: 0.6591 score: 0.7578 time: 0.16s
Epoch 69/1000, LR 0.000288
Train loss: 0.9670;  Loss pred: 0.9670; Loss self: 0.0000; time: 0.26s
Val loss: 0.6587 score: 0.7054 time: 0.16s
Test loss: 0.6570 score: 0.7656 time: 0.16s
Epoch 70/1000, LR 0.000287
Train loss: 0.9642;  Loss pred: 0.9642; Loss self: 0.0000; time: 0.26s
Val loss: 0.6568 score: 0.7054 time: 0.17s
Test loss: 0.6549 score: 0.7656 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 0.9626;  Loss pred: 0.9626; Loss self: 0.0000; time: 0.32s
Val loss: 0.6551 score: 0.6977 time: 0.17s
Test loss: 0.6530 score: 0.7422 time: 0.16s
Epoch 72/1000, LR 0.000287
Train loss: 0.9602;  Loss pred: 0.9602; Loss self: 0.0000; time: 0.33s
Val loss: 0.6533 score: 0.6977 time: 0.17s
Test loss: 0.6512 score: 0.7109 time: 0.16s
Epoch 73/1000, LR 0.000287
Train loss: 0.9581;  Loss pred: 0.9581; Loss self: 0.0000; time: 0.26s
Val loss: 0.6511 score: 0.6977 time: 0.16s
Test loss: 0.6489 score: 0.7109 time: 0.15s
Epoch 74/1000, LR 0.000287
Train loss: 0.9563;  Loss pred: 0.9563; Loss self: 0.0000; time: 0.27s
Val loss: 0.6481 score: 0.7054 time: 0.17s
Test loss: 0.6457 score: 0.7656 time: 0.21s
Epoch 75/1000, LR 0.000287
Train loss: 0.9525;  Loss pred: 0.9525; Loss self: 0.0000; time: 0.47s
Val loss: 0.6454 score: 0.7209 time: 0.25s
Test loss: 0.6429 score: 0.7656 time: 0.21s
Epoch 76/1000, LR 0.000287
Train loss: 0.9489;  Loss pred: 0.9489; Loss self: 0.0000; time: 0.47s
Val loss: 0.6427 score: 0.7442 time: 0.25s
Test loss: 0.6400 score: 0.7812 time: 0.18s
Epoch 77/1000, LR 0.000287
Train loss: 0.9461;  Loss pred: 0.9461; Loss self: 0.0000; time: 0.27s
Val loss: 0.6396 score: 0.8295 time: 0.17s
Test loss: 0.6369 score: 0.8125 time: 0.16s
Epoch 78/1000, LR 0.000287
Train loss: 0.9434;  Loss pred: 0.9434; Loss self: 0.0000; time: 0.26s
Val loss: 0.6368 score: 0.8372 time: 0.16s
Test loss: 0.6339 score: 0.8125 time: 0.16s
Epoch 79/1000, LR 0.000287
Train loss: 0.9393;  Loss pred: 0.9393; Loss self: 0.0000; time: 0.25s
Val loss: 0.6343 score: 0.8062 time: 0.16s
Test loss: 0.6312 score: 0.8047 time: 0.16s
Epoch 80/1000, LR 0.000287
Train loss: 0.9373;  Loss pred: 0.9373; Loss self: 0.0000; time: 0.40s
Val loss: 0.6323 score: 0.7442 time: 0.24s
Test loss: 0.6290 score: 0.7812 time: 0.21s
Epoch 81/1000, LR 0.000286
Train loss: 0.9360;  Loss pred: 0.9360; Loss self: 0.0000; time: 0.46s
Val loss: 0.6300 score: 0.7364 time: 0.25s
Test loss: 0.6266 score: 0.7734 time: 0.16s
Epoch 82/1000, LR 0.000286
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.26s
Val loss: 0.6264 score: 0.7752 time: 0.16s
Test loss: 0.6228 score: 0.8047 time: 0.16s
Epoch 83/1000, LR 0.000286
Train loss: 0.9301;  Loss pred: 0.9301; Loss self: 0.0000; time: 0.26s
Val loss: 0.6233 score: 0.7752 time: 0.16s
Test loss: 0.6195 score: 0.8047 time: 0.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.9275;  Loss pred: 0.9275; Loss self: 0.0000; time: 0.26s
Val loss: 0.6200 score: 0.7752 time: 0.17s
Test loss: 0.6161 score: 0.8047 time: 0.16s
Epoch 85/1000, LR 0.000286
Train loss: 0.9245;  Loss pred: 0.9245; Loss self: 0.0000; time: 0.26s
Val loss: 0.6160 score: 0.8140 time: 0.16s
Test loss: 0.6120 score: 0.8125 time: 0.18s
Epoch 86/1000, LR 0.000286
Train loss: 0.9194;  Loss pred: 0.9194; Loss self: 0.0000; time: 0.41s
Val loss: 0.6109 score: 0.8450 time: 0.24s
Test loss: 0.6068 score: 0.8594 time: 0.21s
Epoch 87/1000, LR 0.000286
Train loss: 0.9158;  Loss pred: 0.9158; Loss self: 0.0000; time: 0.34s
Val loss: 0.6066 score: 0.8527 time: 0.16s
Test loss: 0.6025 score: 0.8750 time: 0.16s
Epoch 88/1000, LR 0.000286
Train loss: 0.9116;  Loss pred: 0.9116; Loss self: 0.0000; time: 0.27s
Val loss: 0.6032 score: 0.8682 time: 0.16s
Test loss: 0.5991 score: 0.8984 time: 0.16s
Epoch 89/1000, LR 0.000286
Train loss: 0.9087;  Loss pred: 0.9087; Loss self: 0.0000; time: 0.26s
Val loss: 0.6008 score: 0.8837 time: 0.16s
Test loss: 0.5969 score: 0.8984 time: 0.16s
Epoch 90/1000, LR 0.000285
Train loss: 0.9065;  Loss pred: 0.9065; Loss self: 0.0000; time: 0.27s
Val loss: 0.5986 score: 0.8760 time: 0.16s
Test loss: 0.5949 score: 0.8984 time: 0.15s
Epoch 91/1000, LR 0.000285
Train loss: 0.9038;  Loss pred: 0.9038; Loss self: 0.0000; time: 0.26s
Val loss: 0.5951 score: 0.8837 time: 0.15s
Test loss: 0.5912 score: 0.8906 time: 0.16s
Epoch 92/1000, LR 0.000285
Train loss: 0.8999;  Loss pred: 0.8999; Loss self: 0.0000; time: 0.38s
Val loss: 0.5906 score: 0.8837 time: 0.17s
Test loss: 0.5863 score: 0.8984 time: 0.16s
Epoch 93/1000, LR 0.000285
Train loss: 0.8964;  Loss pred: 0.8964; Loss self: 0.0000; time: 0.27s
Val loss: 0.5863 score: 0.8760 time: 0.16s
Test loss: 0.5816 score: 0.8984 time: 0.16s
Epoch 94/1000, LR 0.000285
Train loss: 0.8916;  Loss pred: 0.8916; Loss self: 0.0000; time: 0.26s
Val loss: 0.5806 score: 0.8837 time: 0.16s
Test loss: 0.5752 score: 0.8984 time: 0.16s
Epoch 95/1000, LR 0.000285
Train loss: 0.8880;  Loss pred: 0.8880; Loss self: 0.0000; time: 0.26s
Val loss: 0.5756 score: 0.8837 time: 0.15s
Test loss: 0.5697 score: 0.8984 time: 0.15s
Epoch 96/1000, LR 0.000285
Train loss: 0.8831;  Loss pred: 0.8831; Loss self: 0.0000; time: 0.25s
Val loss: 0.5714 score: 0.8760 time: 0.15s
Test loss: 0.5651 score: 0.8984 time: 0.15s
Epoch 97/1000, LR 0.000285
Train loss: 0.8801;  Loss pred: 0.8801; Loss self: 0.0000; time: 0.25s
Val loss: 0.5673 score: 0.8837 time: 0.16s
Test loss: 0.5610 score: 0.9062 time: 0.15s
Epoch 98/1000, LR 0.000285
Train loss: 0.8761;  Loss pred: 0.8761; Loss self: 0.0000; time: 0.25s
Val loss: 0.5638 score: 0.8837 time: 0.15s
Test loss: 0.5575 score: 0.8984 time: 0.15s
Epoch 99/1000, LR 0.000284
Train loss: 0.8730;  Loss pred: 0.8730; Loss self: 0.0000; time: 0.26s
Val loss: 0.5599 score: 0.8837 time: 0.16s
Test loss: 0.5534 score: 0.8984 time: 0.21s
Epoch 100/1000, LR 0.000284
Train loss: 0.8695;  Loss pred: 0.8695; Loss self: 0.0000; time: 0.27s
Val loss: 0.5561 score: 0.8837 time: 0.16s
Test loss: 0.5494 score: 0.8984 time: 0.15s
Epoch 101/1000, LR 0.000284
Train loss: 0.8658;  Loss pred: 0.8658; Loss self: 0.0000; time: 0.26s
Val loss: 0.5514 score: 0.8837 time: 0.15s
Test loss: 0.5445 score: 0.8984 time: 0.15s
Epoch 102/1000, LR 0.000284
Train loss: 0.8622;  Loss pred: 0.8622; Loss self: 0.0000; time: 0.25s
Val loss: 0.5464 score: 0.8837 time: 0.15s
Test loss: 0.5392 score: 0.8984 time: 0.15s
Epoch 103/1000, LR 0.000284
Train loss: 0.8578;  Loss pred: 0.8578; Loss self: 0.0000; time: 0.25s
Val loss: 0.5413 score: 0.8837 time: 0.15s
Test loss: 0.5335 score: 0.9062 time: 0.15s
Epoch 104/1000, LR 0.000284
Train loss: 0.8544;  Loss pred: 0.8544; Loss self: 0.0000; time: 0.25s
Val loss: 0.5377 score: 0.8837 time: 0.15s
Test loss: 0.5293 score: 0.8984 time: 0.15s
Epoch 105/1000, LR 0.000284
Train loss: 0.8510;  Loss pred: 0.8510; Loss self: 0.0000; time: 0.30s
Val loss: 0.5346 score: 0.8682 time: 0.21s
Test loss: 0.5257 score: 0.8984 time: 0.20s
Epoch 106/1000, LR 0.000283
Train loss: 0.8484;  Loss pred: 0.8484; Loss self: 0.0000; time: 0.29s
Val loss: 0.5331 score: 0.8527 time: 0.15s
Test loss: 0.5235 score: 0.8828 time: 0.15s
Epoch 107/1000, LR 0.000283
Train loss: 0.8467;  Loss pred: 0.8467; Loss self: 0.0000; time: 0.26s
Val loss: 0.5297 score: 0.8527 time: 0.17s
Test loss: 0.5197 score: 0.8594 time: 0.16s
Epoch 108/1000, LR 0.000283
Train loss: 0.8423;  Loss pred: 0.8423; Loss self: 0.0000; time: 0.27s
Val loss: 0.5219 score: 0.8682 time: 0.17s
Test loss: 0.5119 score: 0.8984 time: 0.16s
Epoch 109/1000, LR 0.000283
Train loss: 0.8350;  Loss pred: 0.8350; Loss self: 0.0000; time: 0.27s
Val loss: 0.5142 score: 0.8837 time: 0.17s
Test loss: 0.5043 score: 0.9062 time: 0.16s
Epoch 110/1000, LR 0.000283
Train loss: 0.8298;  Loss pred: 0.8298; Loss self: 0.0000; time: 0.27s
Val loss: 0.5080 score: 0.8837 time: 0.16s
Test loss: 0.4981 score: 0.8984 time: 0.16s
Epoch 111/1000, LR 0.000283
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.28s
Val loss: 0.5031 score: 0.8837 time: 0.17s
Test loss: 0.4931 score: 0.8984 time: 0.16s
Epoch 112/1000, LR 0.000283
Train loss: 0.8202;  Loss pred: 0.8202; Loss self: 0.0000; time: 0.28s
Val loss: 0.4981 score: 0.8837 time: 0.25s
Test loss: 0.4877 score: 0.8984 time: 0.17s
Epoch 113/1000, LR 0.000282
Train loss: 0.8159;  Loss pred: 0.8159; Loss self: 0.0000; time: 0.27s
Val loss: 0.4926 score: 0.8837 time: 0.17s
Test loss: 0.4817 score: 0.8984 time: 0.16s
Epoch 114/1000, LR 0.000282
Train loss: 0.8111;  Loss pred: 0.8111; Loss self: 0.0000; time: 0.27s
Val loss: 0.4872 score: 0.8837 time: 0.16s
Test loss: 0.4758 score: 0.8984 time: 0.16s
Epoch 115/1000, LR 0.000282
Train loss: 0.8066;  Loss pred: 0.8066; Loss self: 0.0000; time: 0.27s
Val loss: 0.4815 score: 0.8837 time: 0.17s
Test loss: 0.4696 score: 0.8984 time: 0.16s
Epoch 116/1000, LR 0.000282
Train loss: 0.8016;  Loss pred: 0.8016; Loss self: 0.0000; time: 0.32s
Val loss: 0.4761 score: 0.8837 time: 0.22s
Test loss: 0.4636 score: 0.8984 time: 0.21s
Epoch 117/1000, LR 0.000282
Train loss: 0.7984;  Loss pred: 0.7984; Loss self: 0.0000; time: 0.47s
Val loss: 0.4709 score: 0.8837 time: 0.25s
Test loss: 0.4580 score: 0.8984 time: 0.20s
Epoch 118/1000, LR 0.000282
Train loss: 0.7931;  Loss pred: 0.7931; Loss self: 0.0000; time: 0.41s
Val loss: 0.4662 score: 0.8837 time: 0.23s
Test loss: 0.4527 score: 0.9062 time: 0.16s
Epoch 119/1000, LR 0.000282
Train loss: 0.7890;  Loss pred: 0.7890; Loss self: 0.0000; time: 0.26s
Val loss: 0.4613 score: 0.8837 time: 0.16s
Test loss: 0.4474 score: 0.9062 time: 0.15s
Epoch 120/1000, LR 0.000281
Train loss: 0.7839;  Loss pred: 0.7839; Loss self: 0.0000; time: 0.26s
Val loss: 0.4552 score: 0.8837 time: 0.16s
Test loss: 0.4409 score: 0.8984 time: 0.16s
Epoch 121/1000, LR 0.000281
Train loss: 0.7782;  Loss pred: 0.7782; Loss self: 0.0000; time: 0.27s
Val loss: 0.4491 score: 0.8837 time: 0.16s
Test loss: 0.4345 score: 0.8984 time: 0.16s
Epoch 122/1000, LR 0.000281
Train loss: 0.7756;  Loss pred: 0.7756; Loss self: 0.0000; time: 0.27s
Val loss: 0.4476 score: 0.8760 time: 0.17s
Test loss: 0.4329 score: 0.8984 time: 0.16s
Epoch 123/1000, LR 0.000281
Train loss: 0.7740;  Loss pred: 0.7740; Loss self: 0.0000; time: 0.31s
Val loss: 0.4475 score: 0.8760 time: 0.17s
Test loss: 0.4325 score: 0.8984 time: 0.19s
Epoch 124/1000, LR 0.000281
Train loss: 0.7726;  Loss pred: 0.7726; Loss self: 0.0000; time: 0.45s
Val loss: 0.4441 score: 0.8760 time: 0.16s
Test loss: 0.4288 score: 0.8984 time: 0.16s
Epoch 125/1000, LR 0.000281
Train loss: 0.7689;  Loss pred: 0.7689; Loss self: 0.0000; time: 0.26s
Val loss: 0.4408 score: 0.8760 time: 0.17s
Test loss: 0.4252 score: 0.8984 time: 0.16s
Epoch 126/1000, LR 0.000280
Train loss: 0.7669;  Loss pred: 0.7669; Loss self: 0.0000; time: 0.27s
Val loss: 0.4412 score: 0.8682 time: 0.17s
Test loss: 0.4256 score: 0.8906 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 127/1000, LR 0.000280
Train loss: 0.7659;  Loss pred: 0.7659; Loss self: 0.0000; time: 0.27s
Val loss: 0.4377 score: 0.8682 time: 0.16s
Test loss: 0.4219 score: 0.8906 time: 0.16s
Epoch 128/1000, LR 0.000280
Train loss: 0.7610;  Loss pred: 0.7610; Loss self: 0.0000; time: 0.26s
Val loss: 0.4304 score: 0.8682 time: 0.17s
Test loss: 0.4141 score: 0.8984 time: 0.16s
Epoch 129/1000, LR 0.000280
Train loss: 0.7565;  Loss pred: 0.7565; Loss self: 0.0000; time: 0.26s
Val loss: 0.4210 score: 0.8760 time: 0.16s
Test loss: 0.4042 score: 0.8984 time: 0.15s
Epoch 130/1000, LR 0.000280
Train loss: 0.7472;  Loss pred: 0.7472; Loss self: 0.0000; time: 0.28s
Val loss: 0.4114 score: 0.8992 time: 0.16s
Test loss: 0.3942 score: 0.8984 time: 0.18s
Epoch 131/1000, LR 0.000280
Train loss: 0.7412;  Loss pred: 0.7412; Loss self: 0.0000; time: 0.30s
Val loss: 0.4084 score: 0.8837 time: 0.16s
Test loss: 0.3908 score: 0.8984 time: 0.15s
Epoch 132/1000, LR 0.000279
Train loss: 0.7394;  Loss pred: 0.7394; Loss self: 0.0000; time: 0.27s
Val loss: 0.4082 score: 0.8837 time: 0.17s
Test loss: 0.3899 score: 0.9062 time: 0.17s
Epoch 133/1000, LR 0.000279
Train loss: 0.7363;  Loss pred: 0.7363; Loss self: 0.0000; time: 0.27s
Val loss: 0.4058 score: 0.8837 time: 0.17s
Test loss: 0.3871 score: 0.9062 time: 0.17s
Epoch 134/1000, LR 0.000279
Train loss: 0.7342;  Loss pred: 0.7342; Loss self: 0.0000; time: 0.28s
Val loss: 0.4010 score: 0.8837 time: 0.17s
Test loss: 0.3820 score: 0.9062 time: 0.17s
Epoch 135/1000, LR 0.000279
Train loss: 0.7289;  Loss pred: 0.7289; Loss self: 0.0000; time: 0.27s
Val loss: 0.3964 score: 0.8837 time: 0.26s
Test loss: 0.3770 score: 0.9062 time: 0.16s
Epoch 136/1000, LR 0.000279
Train loss: 0.7261;  Loss pred: 0.7261; Loss self: 0.0000; time: 0.25s
Val loss: 0.3924 score: 0.8837 time: 0.15s
Test loss: 0.3726 score: 0.9062 time: 0.15s
Epoch 137/1000, LR 0.000279
Train loss: 0.7219;  Loss pred: 0.7219; Loss self: 0.0000; time: 0.30s
Val loss: 0.3878 score: 0.8837 time: 0.16s
Test loss: 0.3676 score: 0.9062 time: 0.18s
Epoch 138/1000, LR 0.000278
Train loss: 0.7168;  Loss pred: 0.7168; Loss self: 0.0000; time: 0.32s
Val loss: 0.3820 score: 0.8837 time: 0.16s
Test loss: 0.3616 score: 0.8984 time: 0.16s
Epoch 139/1000, LR 0.000278
Train loss: 0.7131;  Loss pred: 0.7131; Loss self: 0.0000; time: 0.25s
Val loss: 0.3766 score: 0.8837 time: 0.16s
Test loss: 0.3559 score: 0.8984 time: 0.16s
Epoch 140/1000, LR 0.000278
Train loss: 0.7069;  Loss pred: 0.7069; Loss self: 0.0000; time: 0.27s
Val loss: 0.3726 score: 0.8837 time: 0.16s
Test loss: 0.3513 score: 0.8984 time: 0.16s
Epoch 141/1000, LR 0.000278
Train loss: 0.7028;  Loss pred: 0.7028; Loss self: 0.0000; time: 0.32s
Val loss: 0.3691 score: 0.8837 time: 0.17s
Test loss: 0.3474 score: 0.8984 time: 0.17s
Epoch 142/1000, LR 0.000278
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.27s
Val loss: 0.3658 score: 0.8837 time: 0.17s
Test loss: 0.3435 score: 0.8984 time: 0.22s
Epoch 143/1000, LR 0.000277
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.48s
Val loss: 0.3623 score: 0.8915 time: 0.17s
Test loss: 0.3395 score: 0.8984 time: 0.16s
Epoch 144/1000, LR 0.000277
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.26s
Val loss: 0.3590 score: 0.9070 time: 0.16s
Test loss: 0.3359 score: 0.8984 time: 0.15s
Epoch 145/1000, LR 0.000277
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.26s
Val loss: 0.3563 score: 0.9070 time: 0.16s
Test loss: 0.3328 score: 0.9062 time: 0.15s
Epoch 146/1000, LR 0.000277
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.26s
Val loss: 0.3540 score: 0.8992 time: 0.16s
Test loss: 0.3303 score: 0.9141 time: 0.16s
Epoch 147/1000, LR 0.000277
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.27s
Val loss: 0.3539 score: 0.8837 time: 0.17s
Test loss: 0.3303 score: 0.9141 time: 0.17s
Epoch 148/1000, LR 0.000277
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.46s
Val loss: 0.3533 score: 0.8760 time: 0.25s
Test loss: 0.3295 score: 0.8984 time: 0.22s
Epoch 149/1000, LR 0.000276
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.35s
Val loss: 0.3511 score: 0.8760 time: 0.17s
Test loss: 0.3271 score: 0.8984 time: 0.17s
Epoch 150/1000, LR 0.000276
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.27s
Val loss: 0.3474 score: 0.8837 time: 0.17s
Test loss: 0.3231 score: 0.9141 time: 0.16s
Epoch 151/1000, LR 0.000276
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.27s
Val loss: 0.3436 score: 0.8915 time: 0.17s
Test loss: 0.3189 score: 0.9141 time: 0.17s
Epoch 152/1000, LR 0.000276
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.27s
Val loss: 0.3412 score: 0.9147 time: 0.17s
Test loss: 0.3161 score: 0.9062 time: 0.16s
Epoch 153/1000, LR 0.000276
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.27s
Val loss: 0.3428 score: 0.8837 time: 0.17s
Test loss: 0.3172 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 154/1000, LR 0.000275
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.27s
Val loss: 0.3455 score: 0.8837 time: 0.17s
Test loss: 0.3192 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 155/1000, LR 0.000275
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.28s
Val loss: 0.3474 score: 0.8837 time: 0.23s
Test loss: 0.3204 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 156/1000, LR 0.000275
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.27s
Val loss: 0.3449 score: 0.8837 time: 0.17s
Test loss: 0.3175 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 157/1000, LR 0.000275
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.27s
Val loss: 0.3393 score: 0.8837 time: 0.17s
Test loss: 0.3120 score: 0.8984 time: 0.17s
Epoch 158/1000, LR 0.000275
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.27s
Val loss: 0.3380 score: 0.8837 time: 0.17s
Test loss: 0.3102 score: 0.8984 time: 0.17s
Epoch 159/1000, LR 0.000274
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.27s
Val loss: 0.3359 score: 0.8837 time: 0.17s
Test loss: 0.3078 score: 0.8984 time: 0.17s
Epoch 160/1000, LR 0.000274
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.27s
Val loss: 0.3343 score: 0.8837 time: 0.19s
Test loss: 0.3057 score: 0.8984 time: 0.17s
Epoch 161/1000, LR 0.000274
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.47s
Val loss: 0.3355 score: 0.8837 time: 0.25s
Test loss: 0.3062 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000274
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.34s
Val loss: 0.3355 score: 0.8837 time: 0.17s
Test loss: 0.3057 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 163/1000, LR 0.000273
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.26s
Val loss: 0.3352 score: 0.8837 time: 0.16s
Test loss: 0.3049 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 164/1000, LR 0.000273
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.26s
Val loss: 0.3356 score: 0.8837 time: 0.16s
Test loss: 0.3048 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 165/1000, LR 0.000273
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.26s
Val loss: 0.3346 score: 0.8837 time: 0.16s
Test loss: 0.3033 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 166/1000, LR 0.000273
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.26s
Val loss: 0.3326 score: 0.8837 time: 0.16s
Test loss: 0.3009 score: 0.8984 time: 0.16s
Epoch 167/1000, LR 0.000273
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 0.26s
Val loss: 0.3298 score: 0.8837 time: 0.16s
Test loss: 0.2978 score: 0.8984 time: 0.16s
Epoch 168/1000, LR 0.000272
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.26s
Val loss: 0.3264 score: 0.8837 time: 0.16s
Test loss: 0.2941 score: 0.8984 time: 0.18s
Epoch 169/1000, LR 0.000272
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.27s
Val loss: 0.3219 score: 0.8837 time: 0.24s
Test loss: 0.2893 score: 0.8984 time: 0.16s
Epoch 170/1000, LR 0.000272
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.26s
Val loss: 0.3169 score: 0.8992 time: 0.17s
Test loss: 0.2840 score: 0.9062 time: 0.16s
Epoch 171/1000, LR 0.000272
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.26s
Val loss: 0.3187 score: 0.8837 time: 0.16s
Test loss: 0.2855 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 172/1000, LR 0.000271
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.28s
Val loss: 0.3250 score: 0.8760 time: 0.17s
Test loss: 0.2919 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 173/1000, LR 0.000271
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.46s
Val loss: 0.3272 score: 0.8760 time: 0.25s
Test loss: 0.2940 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 174/1000, LR 0.000271
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.26s
Val loss: 0.3205 score: 0.8760 time: 0.17s
Test loss: 0.2865 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 175/1000, LR 0.000271
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 0.26s
Val loss: 0.3125 score: 0.8837 time: 0.15s
Test loss: 0.2779 score: 0.8984 time: 0.15s
Epoch 176/1000, LR 0.000271
Train loss: 0.6256;  Loss pred: 0.6256; Loss self: 0.0000; time: 0.25s
Val loss: 0.3098 score: 0.8837 time: 0.16s
Test loss: 0.2751 score: 0.9141 time: 0.15s
Epoch 177/1000, LR 0.000270
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.25s
Val loss: 0.3094 score: 0.9070 time: 0.15s
Test loss: 0.2750 score: 0.9062 time: 0.15s
Epoch 178/1000, LR 0.000270
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.27s
Val loss: 0.3109 score: 0.9070 time: 0.16s
Test loss: 0.2765 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 179/1000, LR 0.000270
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.33s
Val loss: 0.3127 score: 0.9070 time: 0.16s
Test loss: 0.2781 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 180/1000, LR 0.000270
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.25s
Val loss: 0.3147 score: 0.8992 time: 0.17s
Test loss: 0.2797 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 181/1000, LR 0.000269
Train loss: 0.6190;  Loss pred: 0.6190; Loss self: 0.0000; time: 0.27s
Val loss: 0.3158 score: 0.8915 time: 0.17s
Test loss: 0.2804 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 182/1000, LR 0.000269
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.27s
Val loss: 0.3157 score: 0.8915 time: 0.17s
Test loss: 0.2800 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 183/1000, LR 0.000269
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.27s
Val loss: 0.3111 score: 0.9070 time: 0.17s
Test loss: 0.2751 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 184/1000, LR 0.000269
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.27s
Val loss: 0.3044 score: 0.9070 time: 0.18s
Test loss: 0.2681 score: 0.9141 time: 0.17s
Epoch 185/1000, LR 0.000268
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.34s
Val loss: 0.3031 score: 0.8837 time: 0.17s
Test loss: 0.2662 score: 0.9141 time: 0.17s
Epoch 186/1000, LR 0.000268
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.27s
Val loss: 0.3037 score: 0.8837 time: 0.17s
Test loss: 0.2664 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 187/1000, LR 0.000268
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.27s
Val loss: 0.3068 score: 0.8837 time: 0.17s
Test loss: 0.2693 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.27s
Val loss: 0.3089 score: 0.8760 time: 0.17s
Test loss: 0.2712 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.27s
Val loss: 0.3075 score: 0.8760 time: 0.17s
Test loss: 0.2694 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.31s
Val loss: 0.3043 score: 0.8837 time: 0.23s
Test loss: 0.2658 score: 0.8984 time: 0.21s
     INFO: Early stopping counter 5 of 20
Epoch 191/1000, LR 0.000267
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.28s
Val loss: 0.3012 score: 0.8837 time: 0.17s
Test loss: 0.2623 score: 0.9141 time: 0.17s
Epoch 192/1000, LR 0.000267
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.27s
Val loss: 0.3000 score: 0.8837 time: 0.17s
Test loss: 0.2611 score: 0.9141 time: 0.17s
Epoch 193/1000, LR 0.000266
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.27s
Val loss: 0.3000 score: 0.8915 time: 0.17s
Test loss: 0.2612 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 194/1000, LR 0.000266
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.28s
Val loss: 0.2996 score: 0.8837 time: 0.17s
Test loss: 0.2606 score: 0.9141 time: 0.17s
Epoch 195/1000, LR 0.000266
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.27s
Val loss: 0.2990 score: 0.8837 time: 0.17s
Test loss: 0.2599 score: 0.9141 time: 0.17s
Epoch 196/1000, LR 0.000266
Train loss: 0.5967;  Loss pred: 0.5967; Loss self: 0.0000; time: 0.28s
Val loss: 0.2988 score: 0.8915 time: 0.17s
Test loss: 0.2594 score: 0.9141 time: 0.17s
Epoch 197/1000, LR 0.000265
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.32s
Val loss: 0.2981 score: 0.8915 time: 0.17s
Test loss: 0.2585 score: 0.9141 time: 0.20s
Epoch 198/1000, LR 0.000265
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.30s
Val loss: 0.2980 score: 0.8837 time: 0.17s
Test loss: 0.2584 score: 0.9141 time: 0.17s
Epoch 199/1000, LR 0.000265
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.28s
Val loss: 0.2987 score: 0.9070 time: 0.17s
Test loss: 0.2591 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 200/1000, LR 0.000265
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.27s
Val loss: 0.2997 score: 0.9070 time: 0.17s
Test loss: 0.2601 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 201/1000, LR 0.000264
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 0.27s
Val loss: 0.3007 score: 0.9070 time: 0.19s
Test loss: 0.2609 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 202/1000, LR 0.000264
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.43s
Val loss: 0.3012 score: 0.9070 time: 0.20s
Test loss: 0.2612 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 203/1000, LR 0.000264
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.25s
Val loss: 0.3014 score: 0.9070 time: 0.16s
Test loss: 0.2612 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 204/1000, LR 0.000264
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.26s
Val loss: 0.3014 score: 0.9070 time: 0.16s
Test loss: 0.2610 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 205/1000, LR 0.000263
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.25s
Val loss: 0.3000 score: 0.9070 time: 0.15s
Test loss: 0.2592 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.25s
Val loss: 0.2983 score: 0.9070 time: 0.16s
Test loss: 0.2571 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 207/1000, LR 0.000263
Train loss: 0.5874;  Loss pred: 0.5874; Loss self: 0.0000; time: 0.27s
Val loss: 0.3003 score: 0.9070 time: 0.19s
Test loss: 0.2593 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 208/1000, LR 0.000263
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.35s
Val loss: 0.3025 score: 0.9147 time: 0.17s
Test loss: 0.2616 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 209/1000, LR 0.000262
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 0.26s
Val loss: 0.2999 score: 0.9070 time: 0.17s
Test loss: 0.2592 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 210/1000, LR 0.000262
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.27s
Val loss: 0.2936 score: 0.8992 time: 0.17s
Test loss: 0.2534 score: 0.9141 time: 0.17s
Epoch 211/1000, LR 0.000262
Train loss: 0.5825;  Loss pred: 0.5825; Loss self: 0.0000; time: 0.26s
Val loss: 0.2915 score: 0.8915 time: 0.17s
Test loss: 0.2514 score: 0.9141 time: 0.15s
Epoch 212/1000, LR 0.000261
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 0.25s
Val loss: 0.2909 score: 0.8837 time: 0.15s
Test loss: 0.2507 score: 0.9141 time: 0.15s
Epoch 213/1000, LR 0.000261
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 0.25s
Val loss: 0.2904 score: 0.8915 time: 0.15s
Test loss: 0.2502 score: 0.9219 time: 0.15s
Epoch 214/1000, LR 0.000261
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.26s
Val loss: 0.2895 score: 0.8837 time: 0.16s
Test loss: 0.2497 score: 0.9219 time: 0.15s
Epoch 215/1000, LR 0.000261
Train loss: 0.5845;  Loss pred: 0.5845; Loss self: 0.0000; time: 0.34s
Val loss: 0.2887 score: 0.8915 time: 0.15s
Test loss: 0.2492 score: 0.9141 time: 0.15s
Epoch 216/1000, LR 0.000260
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.24s
Val loss: 0.2885 score: 0.8992 time: 0.15s
Test loss: 0.2491 score: 0.9141 time: 0.15s
Epoch 217/1000, LR 0.000260
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.25s
Val loss: 0.2888 score: 0.9070 time: 0.16s
Test loss: 0.2495 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 218/1000, LR 0.000260
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.26s
Val loss: 0.2896 score: 0.9070 time: 0.16s
Test loss: 0.2502 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 219/1000, LR 0.000260
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.26s
Val loss: 0.2902 score: 0.9070 time: 0.16s
Test loss: 0.2506 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 220/1000, LR 0.000259
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.26s
Val loss: 0.2887 score: 0.9070 time: 0.16s
Test loss: 0.2481 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 221/1000, LR 0.000259
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.27s
Val loss: 0.2905 score: 0.8837 time: 0.15s
Test loss: 0.2474 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 222/1000, LR 0.000259
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.37s
Val loss: 0.2993 score: 0.8760 time: 0.16s
Test loss: 0.2551 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 223/1000, LR 0.000258
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.25s
Val loss: 0.3005 score: 0.8760 time: 0.15s
Test loss: 0.2557 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 224/1000, LR 0.000258
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.25s
Val loss: 0.2979 score: 0.8760 time: 0.15s
Test loss: 0.2526 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 225/1000, LR 0.000258
Train loss: 0.5869;  Loss pred: 0.5869; Loss self: 0.0000; time: 0.25s
Val loss: 0.2937 score: 0.8837 time: 0.15s
Test loss: 0.2481 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 226/1000, LR 0.000258
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.26s
Val loss: 0.2902 score: 0.8837 time: 0.19s
Test loss: 0.2445 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 227/1000, LR 0.000257
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.45s
Val loss: 0.2887 score: 0.8992 time: 0.23s
Test loss: 0.2433 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 11 of 20
Epoch 228/1000, LR 0.000257
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.42s
Val loss: 0.2892 score: 0.8992 time: 0.16s
Test loss: 0.2441 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 229/1000, LR 0.000257
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 0.25s
Val loss: 0.2907 score: 0.9147 time: 0.15s
Test loss: 0.2458 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 230/1000, LR 0.000256
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.35s
Val loss: 0.2930 score: 0.9070 time: 0.16s
Test loss: 0.2483 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 231/1000, LR 0.000256
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.26s
Val loss: 0.2955 score: 0.9070 time: 0.16s
Test loss: 0.2509 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 232/1000, LR 0.000256
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.25s
Val loss: 0.2975 score: 0.9147 time: 0.15s
Test loss: 0.2530 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 233/1000, LR 0.000255
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.39s
Val loss: 0.3009 score: 0.9070 time: 0.17s
Test loss: 0.2567 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 234/1000, LR 0.000255
Train loss: 0.5750;  Loss pred: 0.5750; Loss self: 0.0000; time: 0.36s
Val loss: 0.3065 score: 0.9070 time: 0.17s
Test loss: 0.2628 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 235/1000, LR 0.000255
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.31s
Val loss: 0.3095 score: 0.9070 time: 0.21s
Test loss: 0.2659 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 236/1000, LR 0.000255
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.27s
Val loss: 0.3109 score: 0.9070 time: 0.17s
Test loss: 0.2672 score: 0.9062 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 215,   Train_Loss: 0.5820,   Val_Loss: 0.2885,   Val_Precision: 0.9483,   Val_Recall: 0.8462,   Val_accuracy: 0.8943,   Val_Score: 0.8992,   Val_Loss: 0.2885,   Test_Precision: 0.9649,   Test_Recall: 0.8594,   Test_accuracy: 0.9091,   Test_Score: 0.9141,   Test_loss: 0.2491


[0.1761847159359604, 0.1546548930928111, 0.1587849569041282, 0.16226191190071404, 0.18021469586528838, 0.16158679011277854, 0.1592299030162394, 0.1577118900604546, 0.16152239101938903, 0.1714327740482986, 0.24523888598196208, 0.16704627103172243, 0.16452298406511545, 0.17415428115054965, 0.15610076393932104, 0.16079473705030978, 0.23738456005230546, 0.15375121706165373, 0.15569123695604503, 0.2328391510527581, 0.2646668890956789, 0.16611476591788232, 0.1655556638725102, 0.16798728005960584, 0.16675715893507004, 0.16703781904652715, 0.16067943000234663, 0.16801713895983994, 0.16800566297024488, 0.16599848913028836, 0.1683701160363853, 0.17367800092324615, 0.1672136678826064, 0.22280333191156387, 0.21284338412806392, 0.17031677812337875, 0.17187636299058795, 0.16688690893352032, 0.15493439813144505, 0.16676469310186803, 0.17436440312303603, 0.16869946592487395, 0.16115598217584193, 0.16105804685503244, 0.15485488693229854, 0.15304396115243435, 0.1566599258221686, 0.1554795578122139, 0.15735462890006602, 0.15528088784776628, 0.15677593811415136, 0.15698981704190373, 0.1561695800628513, 0.1556486680638045, 0.16999080101959407, 0.16746588191017509, 0.16547022620216012, 0.16756348893977702, 0.16509086987935007, 0.16813588701188564, 0.17873591207899153, 0.23185640387237072, 0.1652244790457189, 0.17877107695676386, 0.16707471502013505, 0.16924928594380617, 0.16485063498839736, 0.21412322390824556, 0.1561536800581962, 0.15701095294207335, 0.1568183028139174, 0.16807942185550928, 0.16000535106286407, 0.21348303696140647, 0.16164195397868752, 0.15552962222136557, 0.1551873160060495, 0.15528922295197845, 0.15861126990057528, 0.16130466503091156, 0.16118002100847661, 0.16775462916120887, 0.16885893722064793, 0.16675164294429123, 0.1672398669179529, 0.16679625306278467, 0.1709324198309332, 0.24515292490832508, 0.17225381499156356, 0.17233423609286547, 0.169909514952451, 0.17056448617950082, 0.20429906900972128, 0.23140644282102585, 0.16861508204601705, 0.16983293695375323, 0.16820121207274497, 0.1696435969788581, 0.17179657518863678, 0.20454163290560246, 0.16919332812540233, 0.169566789874807, 0.172919733915478, 0.16887591080740094, 0.24233703617937863, 0.18759063794277608, 0.17058546212501824, 0.16889236099086702, 0.16810543602332473, 0.16917433100752532, 0.2088379270862788, 0.16156752314418554, 0.1576315548736602, 0.16870742104947567, 0.17090845387429, 0.17238656803965569, 0.15310559701174498, 0.1659028958529234, 0.16369126294739544, 0.17197961686179042, 0.25874021905474365, 0.17292001307941973, 0.22514686593785882, 0.16812326502986252, 0.17325655790045857, 0.16907387203536928, 0.1674885000102222, 0.16908650984987617, 0.15707284188829362, 0.15604650508612394, 0.1555105799343437, 0.1556403269059956, 0.1556915300898254, 0.1563304818700999, 0.1725821599829942, 0.1784469799604267, 0.1602350149769336, 0.15666085807606578, 0.15737049980089068, 0.15530823194421828, 0.23051631287671626, 0.15747052477672696, 0.15738587803207338, 0.16464087110944092, 0.16640260303393006, 0.18420758796855807, 0.16828466509468853, 0.16688613314181566, 0.1661636100616306, 0.16530863801017404, 0.17013563797809184, 0.2072756551206112, 0.17055843607522547, 0.16552677401341498, 0.16564421216025949, 0.16898171696811914, 0.1704971850849688, 0.17451819707639515, 0.23470149212516844, 0.15468061808496714, 0.16875328100286424, 0.16582952486351132, 0.2452413779683411, 0.24731534905731678, 0.25722926505841315, 0.1680822018533945, 0.16897633392363787, 0.1694202129729092, 0.16873819194734097, 0.18465077807195485, 0.15841885400004685, 0.16327037778683007, 0.1580134800169617, 0.1641920479014516, 0.16838284605182707, 0.24073225702159107, 0.16650701290927827, 0.16920741088688374, 0.16270121606066823, 0.23472182103432715, 0.23467548098415136, 0.23683241987600923, 0.15765703306533396, 0.15988651989027858, 0.15925623988732696, 0.1591875208541751, 0.16826246515847743, 0.1631183698773384, 0.16467292397283018, 0.15607490204274654, 0.1667619829531759, 0.16451898496598005, 0.17463764105923474, 0.24520398792810738, 0.1678992249071598, 0.16679495107382536, 0.16648460482247174, 0.16874528815969825, 0.16857226914726198, 0.17039739899337292, 0.17122582206502557, 0.1717726420611143, 0.17041453393176198, 0.1676767449826002, 0.16957066697068512, 0.1700802471023053, 0.17198109184391797, 0.24774514697492123, 0.17160859797149897, 0.17064207000657916, 0.223316784016788, 0.16799355414696038, 0.17272847285494208, 0.25271914387121797, 0.1690982119180262, 0.17762667406350374, 0.23261798615567386, 0.17802069406025112, 0.15326821897178888, 0.15465641906484962, 0.16470201406627893, 0.15716381813399494, 0.16681200498715043, 0.1671683699823916, 0.1705365648958832, 0.1667598590720445, 0.1684489599429071, 0.15571019402705133, 0.1647240191232413, 0.15547158592380583, 0.15728697320446372, 0.15910502895712852, 0.15669275796972215, 0.15896221692673862, 0.15724002197384834, 0.1566341482102871, 0.15615573688410223, 0.15645128721371293, 0.21442424389533699, 0.1598028871230781, 0.15809707785956562, 0.1597498022019863, 0.1574920688290149, 0.15811321698129177, 0.2162758877966553, 0.16029149293899536, 0.15783051797188818, 0.1646299350541085, 0.23682046914473176, 0.23676157905720174, 0.23908881819806993, 0.20119319087825716, 0.16019623912870884, 0.16358892689459026, 0.15823833900503814, 0.1591863469220698, 0.16723872511647642, 0.1707039731554687, 0.2238825228996575, 0.15632898197509348, 0.15833012503571808, 0.15616479190066457, 0.15610537701286376, 0.1558227150235325, 0.24258922412991524, 0.15799267799593508, 0.15769893606193364, 0.15882583404891193, 0.16248247912153602, 0.23411965393461287, 0.15960331517271698, 0.15511303301900625, 0.15862507303245366, 0.15844860090874135, 0.16196936997584999, 0.15908914408646524, 0.16553416894748807, 0.16047764499671757, 0.1584020520094782, 0.1637739660218358, 0.15798120107501745, 0.1609294288791716, 0.24963408103212714, 0.16510470304638147, 0.16912259999662638, 0.1668455610051751, 0.1683214500080794, 0.1677822049241513, 0.21565099293366075, 0.16449814196676016, 0.16620515007525682, 0.16328041395172477, 0.1681322210934013, 0.18723254604265094, 0.16440063598565757, 0.16763911698944867, 0.16491808486171067, 0.16862075892277062, 0.16604061401449144, 0.23902688990347087, 0.1572276521474123, 0.153156811138615, 0.16334415622986853, 0.16620246693491936, 0.24549844209104776, 0.23898861417546868, 0.16823471314273775, 0.1674389662221074, 0.1659255779813975, 0.167211048072204, 0.16465058899484575, 0.16837147204205394, 0.1647290140390396, 0.1655403790064156, 0.1637715760152787, 0.1653494997881353, 0.1645078849978745, 0.16314410394988954, 0.1670308478642255, 0.16558869695290923, 0.16315624304115772, 0.16606758814305067, 0.16326569486409426, 0.23692546389065683, 0.2242261040955782, 0.15269680880010128, 0.1555963030550629, 0.15283555118367076, 0.1574578359723091, 0.15342774405144155, 0.1678557510022074, 0.17298787599429488, 0.16533716302365065, 0.1664405579213053, 0.16405037511140108, 0.167634645011276, 0.15447601699270308, 0.15932851401157677, 0.15766332298517227, 0.15516073605977, 0.1643465580418706, 0.1628922550007701, 0.15493113989941776, 0.15607661590911448, 0.1848291838541627, 0.15374314598739147, 0.1564022449310869, 0.15221963007934391, 0.15711520402692258, 0.15768694295547903, 0.15684198401868343, 0.1531852181069553, 0.16229493310675025, 0.159055506112054, 0.16570360003970563, 0.1688858058769256, 0.1578720931429416, 0.15617601503618062, 0.1542322940658778, 0.15446991892531514, 0.15617286902852356, 0.15943627408705652, 0.15475511900149286, 0.15890894597396255, 0.15542835602536798, 0.15313416603021324, 0.23050810093991458, 0.23865271895192564, 0.15325392899103463, 0.1532283970154822, 0.15623662201687694, 0.23049354297108948, 0.23264059890061617, 0.2407864178530872, 0.1538612220901996, 0.1552941899280995, 0.15233790292404592, 0.16654678992927074, 0.16889702808111906, 0.16703207488171756, 0.16544758202508092, 0.16369068808853626, 0.16271294304169714, 0.16185813187621534, 0.16815716680139303, 0.23908275505527854, 0.24105148599483073, 0.20395294507034123, 0.1617372438777238, 0.16186295705847442, 0.15902430983260274, 0.1501243009697646, 0.1648792859632522, 0.24196886806748807, 0.1601745099760592, 0.15163407288491726, 0.1520416741259396, 0.22850076807662845, 0.214912218041718, 0.15352580114267766, 0.1534421609248966, 0.2290843790397048, 0.15176179306581616, 0.16752813500352204, 0.22646666620858014, 0.15485038794577122, 0.25804551504552364, 0.16196759417653084, 0.163755051093176, 0.2381675790529698, 0.24111874890513718, 0.16318412288092077, 0.16637866594828665, 0.1653613750822842, 0.16239704098552465, 0.1633854398969561, 0.19753912603482604, 0.16261267592199147, 0.1640868268441409, 0.2410678940359503, 0.24110128707252443, 0.16594994999468327, 0.16765809897333384, 0.16486588888801634, 0.16844742209650576, 0.18535857019014657, 0.24119227705523372, 0.1657375511713326, 0.1642333660274744, 0.17005975008942187, 0.241093706805259, 0.16730159102007747, 0.1558912149630487, 0.18611751799471676, 0.23274136101827025, 0.21359370998106897, 0.15554518811404705, 0.15494602592661977, 0.17099798191338778, 0.24007728300057352, 0.24325647205114365, 0.19642865704372525, 0.16439776215702295, 0.1679159931372851, 0.16675720107741654, 0.18942122207954526, 0.16792821302078664, 0.16471158107742667, 0.16569202300161123, 0.1636063400655985, 0.163613063050434, 0.16455153212882578, 0.1681516559328884, 0.1970167199615389, 0.16443096683360636, 0.16677873395383358, 0.1649959480855614, 0.1660349650774151, 0.1690452869515866, 0.16537406807765365, 0.16631912696175277, 0.1644971570931375, 0.1665637199766934, 0.1669152060057968, 0.166575649054721, 0.25275734392926097, 0.1684838100336492, 0.20650596101768315, 0.2410835970658809, 0.2452835119329393, 0.16752745490521193, 0.16519116796553135, 0.1678838860243559, 0.17014243500307202, 0.1712453479412943, 0.21998390811495483, 0.16902484581805766, 0.16884981305338442, 0.2428435799665749, 0.24525876087136567, 0.24738248297944665, 0.17012784304097295, 0.16954852198250592, 0.16869311407208443, 0.16926628514192998, 0.23477193107828498, 0.23477374715730548, 0.1556712151505053, 0.1546984021551907, 0.17082575988024473, 0.16331070894375443, 0.15143458102829754, 0.33486716100014746, 0.16628118208609521, 0.1655308878980577, 0.15401695086620748, 0.155618793098256, 0.14909993414767087, 0.15315734315663576, 0.16630472498945892, 0.1629822759423405, 0.16248606285080314, 0.1634339860174805, 0.17232824908569455, 0.21556220785714686, 0.16340388194657862, 0.16716730408370495, 0.1631354158744216, 0.241091996897012, 0.24315846106037498, 0.22418635291978717, 0.1667042530607432, 0.16361410380341113, 0.1640671049244702, 0.1650441859383136, 0.16468317806720734, 0.1667199309449643, 0.25473977997899055, 0.16487939190119505, 0.16641816589981318, 0.16636257781647146, 0.15732117416337132, 0.1588465420063585, 0.15171762905083597, 0.15592920710332692, 0.1521620089188218, 0.1548619179520756, 0.1533006930258125, 0.1539220348931849, 0.16853732196614146, 0.16971066710539162, 0.1697887280024588, 0.16830030409619212, 0.16966684302315116, 0.1692621090915054, 0.16946087311953306, 0.18589046015404165, 0.1757383570075035, 0.1696217458229512, 0.15781291294842958, 0.15964405494742095, 0.16283974098041654, 0.1590273198671639, 0.15871419687755406, 0.1590872728265822, 0.16380358091555536, 0.1626986030023545, 0.2089573920238763, 0.1608535940758884, 0.16162147605791688, 0.1613504490815103, 0.16123060300014913, 0.16541130491532385, 0.15836295601911843, 0.15753597090952098, 0.17320802994072437, 0.1736324441153556, 0.1738211631309241, 0.2152237470727414, 0.1783901860471815, 0.16731385979801416, 0.16426249803043902, 0.1669915490783751, 0.16466671391390264, 0.17032536794431508, 0.2153258330654353, 0.16590739809907973, 0.16693424503318965, 0.15612227702513337, 0.16502764588221908, 0.1660519710276276, 0.1682712258771062, 0.1675429088063538, 0.16659388388507068, 0.16791582596488297, 0.16581158386543393, 0.1645968919619918, 0.16022392502054572, 0.16500978218391538, 0.16973704285919666, 0.16782990493811667, 0.1692790319211781, 0.17139057395979762, 0.1632911111228168, 0.1513006950262934, 0.15628135786391795, 0.15623468696139753, 0.15900268009863794, 0.15961235109716654, 0.18748928816057742, 0.16802172106690705, 0.16771500697359443, 0.17197170504368842, 0.16634002304635942, 0.15571790002286434, 0.21107820491306484, 0.21909244009293616, 0.1581096991430968, 0.15607437188737094, 0.15537708718329668, 0.1595281190238893, 0.1563761669676751, 0.15999328717589378, 0.16721760109066963, 0.1700866089668125, 0.1602899308782071, 0.16444044699892402, 0.16513179591856897, 0.1680539189837873, 0.16896456689573824, 0.16664336016401649, 0.15842660912312567, 0.21307685202918947, 0.21950529795140028, 0.18684728210791945, 0.16901734401471913, 0.15984686720184982, 0.16388593497686088, 0.2111382228322327, 0.16431695013307035, 0.15982052101753652, 0.15987551398575306, 0.16694161109626293, 0.18128824094310403, 0.21738875494338572, 0.1647505210712552, 0.16745749907568097, 0.16409581108018756, 0.15506695513613522, 0.16787756187841296, 0.16433054697699845, 0.16467852098867297, 0.16409847093746066, 0.15506470506079495, 0.15306425304152071, 0.15524206380359828, 0.15817634295672178, 0.2127338091377169, 0.15772117394953966, 0.1569673321209848, 0.15603746101260185, 0.15693120192736387, 0.1547814980149269, 0.2023018579930067, 0.15451653907075524, 0.16631811996921897, 0.16669761296361685, 0.16637869202531874, 0.16499813389964402, 0.16769495513290167, 0.17016448895446956, 0.16570474300533533, 0.16887757903896272, 0.16902041202411056, 0.2152260688599199, 0.20488252397626638, 0.1625487138517201, 0.15806772303767502, 0.16563658299855888, 0.16730315797030926, 0.16533113596960902, 0.1920651861000806, 0.16327271587215364, 0.16753567685373127, 0.16969210910610855, 0.16615935019217432, 0.16878851200453937, 0.15747352899052203, 0.18589756917208433, 0.1591353020630777, 0.1702694317791611, 0.16948531498201191, 0.16954688797704875, 0.16529382299631834, 0.1537754451856017, 0.18799291690811515, 0.16017053602263331, 0.16135484189726412, 0.15969128301367164, 0.17482494795694947, 0.22285655001178384, 0.1688237541820854, 0.1594482990913093, 0.15722205210477114, 0.16737520205788314, 0.17192474799230695, 0.22158552194014192, 0.17202185606583953, 0.16943056881427765, 0.17038287594914436, 0.16827814397402108, 0.17215558304451406, 0.1938358531333506, 0.1738135030027479, 0.1734440380241722, 0.1707346469629556, 0.17283165990374982, 0.17341137304902077, 0.17321781511418521, 0.21905956813134253, 0.1710082539357245, 0.16050044680014253, 0.16318256594240665, 0.16029677004553378, 0.16121987509541214, 0.1617389670573175, 0.1881252119783312, 0.16911189095117152, 0.1680459319613874, 0.16812964901328087, 0.2157216491177678, 0.173341556917876, 0.1668574339710176, 0.15821913606487215, 0.15925529086962342, 0.1552647550124675, 0.15828921808861196, 0.15884657599963248, 0.1719199358485639, 0.17284175497479737, 0.17169567989185452, 0.17337808897718787, 0.17562279710546136, 0.17445546714589, 0.1745982370339334, 0.17509237094782293, 0.1746961979661137, 0.1753060871269554, 0.21178195788525045, 0.1751802188809961, 0.17402060702443123, 0.1768203128594905, 0.17303524306043983, 0.17528719804249704, 0.17399535002186894, 0.20147537905722857, 0.17522195889614522, 0.17360180895775557, 0.17273435788229108, 0.20002283691428602, 0.1544879199936986, 0.16238323599100113, 0.16184216691181064, 0.1570403620135039, 0.1702988629695028, 0.1720855210442096, 0.1718265530653298, 0.17235502018593252, 0.1710312410723418, 0.15679799509234726, 0.15329057700000703, 0.1527732431422919, 0.15602179383859038, 0.1540030729956925, 0.1545648560859263, 0.1621178900822997, 0.1660594129934907, 0.162343931151554, 0.16222693515010178, 0.15435374295338988, 0.1579631739296019, 0.1544522310141474, 0.1584082879126072, 0.15578496200032532, 0.16844119504094124, 0.20481155114248395, 0.15714748692698777, 0.15348605602048337, 0.1624304661527276, 0.16494993516243994, 0.15509655000641942, 0.17289102310314775, 0.17251691594719887, 0.17150507913902402, 0.17463794793002307]
[0.001365772991751631, 0.0011988751402543497, 0.0012308911387916915, 0.0012578442783001088, 0.0013970131462425455, 0.0012526107760680507, 0.00123434033345922, 0.0012225727911663149, 0.0012521115582898375, 0.0013289362329325473, 0.001901076635519086, 0.0012949323335792436, 0.001275371969497019, 0.0013500331872135633, 0.0012100834413900855, 0.0012464708298473627, 0.0018401903880023679, 0.0011918698997027421, 0.0012069088136127523, 0.0018049546593237062, 0.0020516813108192164, 0.0012877113637045142, 0.0012833772393217844, 0.0013022269772062468, 0.0012926911545354267, 0.0012948668143141638, 0.0012455769767623771, 0.001302458441549147, 0.0013023694803894952, 0.0012868099932580492, 0.0013051946979564751, 0.0013463410924282647, 0.0012962299835860961, 0.0017271576117175494, 0.001649948714171038, 0.0013202851017316182, 0.0013323749069037826, 0.0012936969684769018, 0.0012010418459801942, 0.0012927495589292095, 0.0013516620397134576, 0.0013077477978672399, 0.0012492711796576893, 0.0012485119911242826, 0.0012004254800953376, 0.0011863872957553051, 0.0012144180296292139, 0.001205267890017162, 0.0012198033248067134, 0.001203727812773382, 0.0012153173497221035, 0.0012169753259062305, 0.0012106168997120257, 0.0012065788222000349, 0.0013177581474387138, 0.0012981851310866286, 0.001282714931799691, 0.0012989417747269537, 0.0012797741851112408, 0.0013033789690843848, 0.0013855497060386941, 0.0017973364641269047, 0.0012808099150830924, 0.0013858223019904176, 0.00129515282961345, 0.001312009968556637, 0.0012779118991348632, 0.0016598699527770975, 0.0012104936438619858, 0.0012171391700935919, 0.0012156457582474217, 0.0013029412546938703, 0.0012403515586268533, 0.0016549072632667168, 0.0012530384029355622, 0.0012056559862121362, 0.0012030024496592986, 0.001203792425984329, 0.0012295447279114364, 0.001250423759929547, 0.0012494575271974932, 0.0013004234818698361, 0.001308984009462387, 0.0012926483949169862, 0.0012964330768833558, 0.0012929942097890284, 0.0013250575180692498, 0.0019004102706071712, 0.0013353008914074695, 0.0013359243107974067, 0.001317128022887217, 0.0013222053192209365, 0.0015837137132536532, 0.0017938483939614407, 0.00130709365927145, 0.0013165343949903352, 0.0013038853649049997, 0.0013150666432469621, 0.001331756396811138, 0.0015855940535318019, 0.0013115761870186227, 0.0013144712393395892, 0.0013404630536083565, 0.001309115587654271, 0.0018785816758091366, 0.0014541909918044658, 0.0013223679234497538, 0.0013092431084563334, 0.001303142914909494, 0.001311428922538956, 0.0016188986595835565, 0.0012524614197223685, 0.0012219500377803117, 0.0013078094654998115, 0.0013248717354596123, 0.0013363299848035324, 0.0011868650931143021, 0.0012860689601001812, 0.0012689245189720577, 0.0013331753245100033, 0.002005738132207315, 0.0013404652176699204, 0.0017453245421539443, 0.0013032811242625001, 0.0013430740922516168, 0.001310650170816816, 0.001298360465195521, 0.0013107481383711332, 0.0012176189293666173, 0.0012096628301249918, 0.0012055083715840596, 0.0012065141620619813, 0.0012069110859676388, 0.00121186420054341, 0.0013378462014185597, 0.0013833099221738504, 0.001242131899045997, 0.0012144252564036107, 0.0012199263550456642, 0.00120393978251332, 0.0017869481618350098, 0.0012207017424552477, 0.0012200455661401037, 0.0012762858225538055, 0.0012899426591777524, 0.0014279657982058766, 0.001304532287555725, 0.0012936909545877182, 0.0012880900004777567, 0.001281462310156388, 0.0013188809145588515, 0.0016067880241907844, 0.0013221584191877944, 0.0012831532869256976, 0.0012840636601570503, 0.001309935790450536, 0.0013216836053098356, 0.001352854240902288, 0.001819391411823011, 0.0011990745587981948, 0.0013081649690144515, 0.0012855001927403978, 0.0019010959532429544, 0.0019171732485063317, 0.0019940253105303345, 0.0013029628050650738, 0.0013098940614235494, 0.0013133349842861179, 0.0013080479995917905, 0.0014314013804027506, 0.0012280531317833089, 0.0012656618433087602, 0.0012249106978059047, 0.0012728065728794696, 0.0013052933802467216, 0.0018661415272991555, 0.0012907520380564207, 0.001311685355712277, 0.0012612497369044048, 0.001819549000266102, 0.0018191897750709407, 0.0018359102315969708, 0.0012221475431421238, 0.0012394303867463456, 0.0012345444952505966, 0.0012340117895672487, 0.001304360195026957, 0.001264483487421228, 0.0012765342943630246, 0.0012098829615716786, 0.0012927285500246193, 0.0012753409687285275, 0.0013537801632498817, 0.0019008061079698246, 0.0013015443791252698, 0.0012929841168513594, 0.0012905783319571453, 0.001308103008989909, 0.001306761776335364, 0.001320910069716069, 0.0013273319539924462, 0.0013315708686908085, 0.0013210428986958293, 0.0012998197285472885, 0.0013145012943463963, 0.0013184515279248473, 0.0013331867584799842, 0.0019205050153094669, 0.0013302992090813873, 0.0013228067442370478, 0.0017311378605952559, 0.0013022756135423286, 0.0013389804097282331, 0.0019590631307846353, 0.0013108388520777225, 0.001376950961732587, 0.0018032402027571618, 0.0013800053803120242, 0.0011881257284634798, 0.0011988869694949582, 0.0012767597989634025, 0.0012183241715813561, 0.0012931163177298483, 0.001295878837072803, 0.0013219888751618853, 0.0012927120858298024, 0.0013058058910302875, 0.0012070557676515607, 0.0012769303808003202, 0.0012052060924326034, 0.0012192788620501063, 0.0012333723174971203, 0.0012146725424009468, 0.0012322652474940979, 0.0012189148990220803, 0.0012142182031805201, 0.0012105095882488544, 0.001212800676075294, 0.001662203441049124, 0.0012387820707215357, 0.001225558743097408, 0.0012383705597053202, 0.0012208687506125185, 0.0012256838525681532, 0.001676557269741514, 0.0012425697127053904, 0.0012234923873789781, 0.0012762010469310736, 0.0018358175902692384, 0.0018353610779628042, 0.0018534016914579065, 0.0015596371385911407, 0.0012418313110752622, 0.0012681312162371337, 0.0012266537907367299, 0.0012340026893183705, 0.0012964242257091196, 0.0013232866136082845, 0.0017355234333306783, 0.0012118525734503372, 0.0012273653103544036, 0.0012105797821756943, 0.0012101192016501067, 0.0012079280234382364, 0.0018805366211621336, 0.001224749441828954, 0.0012224723725731289, 0.0012312080158830383, 0.0012595541017173335, 0.001814881038252813, 0.0012372350013388912, 0.001202426612550436, 0.0012296517289337493, 0.0012282837279747392, 0.0012555765114406975, 0.001233249178964847, 0.0012832106119960314, 0.0012440127519125392, 0.0012279228837944048, 0.0012695656280762465, 0.0012246604734497478, 0.001247514952551718, 0.0019351479149777298, 0.0012798814189641974, 0.001311027906950592, 0.0012933764419005823, 0.0013048174419230962, 0.0013006372474740411, 0.00167171312351675, 0.001275179395091164, 0.0012884120160872622, 0.001265739643036626, 0.001303350551111638, 0.0014514150856019452, 0.0012744235347725394, 0.0012995280386778967, 0.0012784347663698501, 0.0013071376660679893, 0.0012871365427479956, 0.001852921627158689, 0.001218819008894669, 0.0011872621018497289, 0.0012662337692237872, 0.0012883912165497626, 0.0019030886983802152, 0.0018526249160889046, 0.0013041450631219981, 0.0012979764823419179, 0.001286244790553469, 0.0012962096749783256, 0.0012763611549988043, 0.0013052052096283252, 0.0012769691010778264, 0.0012832587519877178, 0.0012695471008936333, 0.0012817790681250798, 0.0012752549224641435, 0.0012646829763557329, 0.001294812774141283, 0.0012836333097124746, 0.001264777077838432, 0.0012873456445197727, 0.001265625541582126, 0.0018366315030283475, 0.0017381868534540946, 0.0011836961922488472, 0.0012061728918997122, 0.001184771714602099, 0.0012206033796303031, 0.001189362356987919, 0.0013012073721101348, 0.0013409912867774797, 0.0012816834342918655, 0.0012902368831108938, 0.0012717083341969076, 0.0012994933721804341, 0.0011974885038194038, 0.0012351047597796649, 0.0012221963022106379, 0.0012027964035641085, 0.0012740043259059737, 0.0012627306589206985, 0.0012010165883675795, 0.0012098962473574765, 0.0014327843709625015, 0.0011918073332355928, 0.001212420503341759, 0.0011799971323980148, 0.0012179473180381597, 0.0012223794027556513, 0.0012158293334781662, 0.0011874823109066302, 0.0012581002566414748, 0.0012329884194732868, 0.0012845240313155476, 0.0013091922936195781, 0.001223814675526679, 0.0012106667832262063, 0.0011955991788052541, 0.0011974412319791872, 0.0012106423955699501, 0.0012359401092019884, 0.0011996520852828904, 0.0012318522943718026, 0.001204870976940837, 0.001187086558373746, 0.0017868845034101906, 0.0018500210771467103, 0.001188014953418873, 0.0011878170311277688, 0.0012111366047819917, 0.0017867716509386781, 0.0018034154953536136, 0.0018665613787061022, 0.0011927226518620123, 0.00120383092967519, 0.0011809139761553948, 0.0012910603870486105, 0.0013092792874505354, 0.0012948222859047872, 0.001282539395543263, 0.0012689200627018316, 0.0012613406437340863, 0.0012547142005908166, 0.0013035439286929692, 0.0018533546903509964, 0.0018686161705025637, 0.0015810305819406298, 0.0012537770843234403, 0.0012547516051044528, 0.0012327465878496336, 0.0011637542710834464, 0.0012781339997151332, 0.001875727659437892, 0.0012416628680314667, 0.0011754579293404438, 0.0011786176288832528, 0.0017713237835397554, 0.001665986186369907, 0.0011901224894781215, 0.001189474115696873, 0.0017758478995325954, 0.0011764480082621408, 0.001298667713205597, 0.0017555555520044972, 0.0012003906042307847, 0.002000352829810261, 0.0012555627455545027, 0.0012694190007222945, 0.0018462603027362, 0.001869137588411916, 0.0012649932006272928, 0.001289757100374315, 0.0012818711246688698, 0.0012588917905854624, 0.0012665537976508225, 0.0015313110545335353, 0.001260563379240244, 0.001271990905768534, 0.0018687433646197699, 0.0018690022253684065, 0.0012864337208890176, 0.0012996751858397972, 0.0012780301464187314, 0.0013057939697403548, 0.001436888141008888, 0.0018697075740715793, 0.0012847872183824232, 0.0012731268684300341, 0.0013182926363521075, 0.0018689434636066588, 0.0012969115582951742, 0.0012084590307213077, 0.0014427714573233858, 0.0018041965970408548, 0.0016557651936516973, 0.0012057766520468763, 0.001201131983927285, 0.0013255657512665719, 0.0018610642093067715, 0.0018857090856677801, 0.0015227027677808158, 0.0012744012570311857, 0.0013016743654053109, 0.0012926914812202833, 0.0014683815665081027, 0.0013017690931843926, 0.0012768339618405167, 0.0012844342868341956, 0.001268266202058903, 0.0012683183182204186, 0.001275593272316479, 0.0013035012087820805, 0.0015272613950506892, 0.0012746586576248555, 0.001292858402742896, 0.001279038357252414, 0.0012870927525381016, 0.0013104285810200513, 0.0012819695199818113, 0.0012892955578430447, 0.0012751717604119186, 0.0012911916277263053, 0.0012939163256263317, 0.0012912841011993876, 0.001959359255265589, 0.0013060760467724745, 0.0016008214032378538, 0.0018688650935339605, 0.0019014225731235604, 0.0012986624411256739, 0.0012805516896552819, 0.0013014254730570223, 0.001318933604674977, 0.0013274833173743746, 0.0017053016132942236, 0.001310270122620602, 0.0013089132794836002, 0.001882508371833914, 0.0019012307044291913, 0.0019176936665073383, 0.0013188204886897127, 0.0013143296277713637, 0.0013076985586983289, 0.001312141745286279, 0.0018199374502192633, 0.0018199515283512052, 0.0012067536058178706, 0.0011992124198076798, 0.0013242306967460832, 0.0012659744879360809, 0.001173911480839516, 0.0025958694651174222, 0.001289001411520118, 0.0012831851775043231, 0.001193929851676027, 0.001206347233319814, 0.0011558134430052006, 0.0011872662260204322, 0.0012891839146469684, 0.001263428495677058, 0.0012595818825643654, 0.0012669301241665155, 0.0013358778998891051, 0.0016710248671096657, 0.0012666967592758034, 0.0012958705742922864, 0.0012646156269335007, 0.0018689302085039688, 0.0018849493105455425, 0.0017378787048045517, 0.0012922810314786295, 0.0012683263860729544, 0.0012718380226703116, 0.0012794122940954543, 0.0012766137834667236, 0.0012924025654648395, 0.0019747269765813223, 0.0012781348209394965, 0.0012900633015489394, 0.0012896323861741974, 0.0012195439857625684, 0.0012313685426849495, 0.001176105651556868, 0.0012087535434366428, 0.001179550456735053, 0.001200479984124617, 0.001188377465316376, 0.0011931940689394178, 0.001306490867954585, 0.0013155865667084622, 0.0013161916899415412, 0.0013046535201255204, 0.0013152468451407067, 0.0013121093728023674, 0.0013136501792211865, 0.0014410113190235787, 0.001362312845019407, 0.0013148972544414822, 0.0012329133824096061, 0.0012472191792767262, 0.0012721854764095042, 0.001242400936462218, 0.001239954663105891, 0.0012428693189576734, 0.0012797154759027762, 0.0012710828359558946, 0.0016324796251865337, 0.001256668703717878, 0.0012626677817024756, 0.0012605503834492993, 0.001259614085938665, 0.0012922758196509676, 0.0012372105938993627, 0.0012307497727306327, 0.0013531877339119092, 0.0013565034696512157, 0.0013579778369603446, 0.001681435524005792, 0.0013936733284936054, 0.0013071395296719857, 0.0012833007658628048, 0.0013046214771748055, 0.0012864587024523644, 0.0013306669370649615, 0.0016822330708237132, 0.0012961515476490604, 0.0013041737893217942, 0.0012197052892588545, 0.0012892784834548365, 0.0012972810236533405, 0.001314618952164892, 0.0013089289750496391, 0.0013015147178521147, 0.0013118423903506482, 0.0012954029989487026, 0.0012859132184530608, 0.0012517494142230134, 0.0012891389233118389, 0.001326070647337474, 0.0013111711323290365, 0.001322492436884204, 0.001338988859060919, 0.0012757118056470063, 0.0011820366798929172, 0.001220948108311859, 0.0012205834918859182, 0.001242208438270609, 0.0012469714929466136, 0.001464760063754511, 0.0013126696958352113, 0.0013102734919812065, 0.0013435289456538158, 0.001299531430049683, 0.0012165460939286277, 0.001649048475883319, 0.0017116596882260637, 0.0012352320245554438, 0.0012193310303700855, 0.0012138834936195053, 0.0012463134298741352, 0.0012216888044349616, 0.0012499475560616702, 0.0013063875085208565, 0.0013288016325532226, 0.0012522650849859929, 0.0012846909921790939, 0.00129009215561382, 0.0013129212420608383, 0.001320035678872955, 0.0013019012512813788, 0.0012377078837744193, 0.0016646629064780427, 0.0017148851402453147, 0.0014597443914681207, 0.0013204480001149932, 0.0012488036500144517, 0.0012803588670067256, 0.001649517365876818, 0.001283726172914612, 0.001248597820449504, 0.0012490274530136958, 0.0013042313366895542, 0.0014163143823680002, 0.001698349647995201, 0.0012871134458691813, 0.0013082617115287576, 0.0012819985240639653, 0.0012114605870010564, 0.0013115434521751013, 0.0012838323982578004, 0.0012865509452240076, 0.0012820193041989114, 0.0012114430082874605, 0.0011958144768868806, 0.0012128286234656116, 0.001235752679349389, 0.0016619828838884132, 0.0012321966714807786, 0.0012263072821951937, 0.001219042664160952, 0.0012260250150575303, 0.0012092304532416165, 0.001580483265570365, 0.0012071604614902753, 0.0012993603122595232, 0.0013023251012782566, 0.0012998335314478027, 0.0012890479210909689, 0.0013101168369757943, 0.0013294100699567934, 0.0012945683047291823, 0.0013193560862418963, 0.0013204719689383637, 0.0016814536629681243, 0.0016006447185645811, 0.0012699118269665632, 0.0012349040862318361, 0.0012940358046762412, 0.001307055921643041, 0.0012916494997625705, 0.0015005092664068798, 0.0012755680927512003, 0.0013088724754197756, 0.001325719602391473, 0.0012981199233763618, 0.0013186602500354638, 0.0012302619452384533, 0.0014523247591569088, 0.0012432445473677944, 0.001330229935774696, 0.001324104023296968, 0.0013245850623206934, 0.001291357992158737, 0.0012013706655125134, 0.0014686946633446496, 0.0012513323126768228, 0.001260584702322376, 0.0012475881485443097, 0.0013658199059136678, 0.0017410667969670612, 0.0013189355795475421, 0.001245689836650854, 0.0012282972820685245, 0.001307618766077212, 0.001343162093689898, 0.0017311368901573587, 0.0013439207505143713, 0.0013236763188615441, 0.0013311162183526903, 0.0013146729997970397, 0.001344965492535266, 0.0015143426026043016, 0.001357917992208968, 0.0013550315470638452, 0.0013338644293980906, 0.0013502473429980455, 0.0013547763519454747, 0.001353264180579572, 0.0017114028760261135, 0.0013360019838728476, 0.0012539097406261135, 0.001274863796425052, 0.0012523185159807326, 0.0012595302741829073, 0.001263585680135293, 0.0014697282185807126, 0.0013211866480560275, 0.001312858843448339, 0.0013135128829162568, 0.001685325383732561, 0.0013542309134209063, 0.001303573702898575, 0.0012360870005068136, 0.001244181959918933, 0.0012130058985349024, 0.001236634516317281, 0.0012409888749971287, 0.0013431244988169055, 0.0013503262107406044, 0.0013413724991551135, 0.0013545163201342802, 0.0013720531023864169, 0.0013629333370772656, 0.0013640487268276047, 0.0013679091480298666, 0.0013648140466102632, 0.001369578805679339, 0.0016545465459785191, 0.001368595460007782, 0.001359535992378369, 0.0013814086942147696, 0.0013518378364096861, 0.0013694312347070081, 0.0013593386720458511, 0.0015740263988845982, 0.0013689215538761346, 0.0013562641324824654, 0.001349487170955399, 0.0015626784133928595, 0.0012069368749507703, 0.0012686190311796963, 0.0012643919289985206, 0.0012268778282304993, 0.0013304598669492407, 0.0013444181331578875, 0.001342394945822889, 0.0013465235952025978, 0.0013361815708776703, 0.001224984336658963, 0.001197582632812555, 0.0011935409620491555, 0.0012189202643639874, 0.0012031490077788476, 0.0012075379381712992, 0.0012665460162679665, 0.001297339164011646, 0.0012683119621215155, 0.0012673979308601702, 0.0012058886168233585, 0.001234087296325015, 0.0012066580547980266, 0.0012375647493172437, 0.0012170700156275416, 0.0013159468362573534, 0.0016000902433006559, 0.001227714741617092, 0.0011991098126600264, 0.0012689880168181844, 0.001288671368456562, 0.0012116917969251517, 0.0013507111179933418, 0.0013477884058374912, 0.0013398834307736252, 0.0013643589682033053]
[732.1860997686592, 834.1152188607757, 812.4195296276595, 795.0109701587483, 715.8128774161032, 798.3325859122841, 810.1493347442638, 817.9472070910527, 798.6508816880685, 752.48155270273, 526.0177213881499, 772.2411233921088, 784.084975926184, 740.7225314690053, 826.389293329432, 802.2650639345131, 543.4220320461283, 839.0177487068048, 828.5630104950574, 554.0305374621917, 487.4051319406471, 776.5715424947247, 779.1941210742225, 767.9152847419624, 773.5799819558482, 772.2801981991156, 802.8407867647777, 767.7788158911219, 767.8312606810575, 777.1155067486843, 766.1692171793877, 742.753827855315, 771.4680362765885, 578.9859554308787, 606.0794444161961, 757.4121670300235, 750.5395026718369, 772.9785447184918, 832.610456785442, 773.545032827785, 739.8299061590815, 764.673434457978, 800.4667171414362, 800.9534606868309, 833.0379657724195, 842.8950677218413, 823.4396851842854, 829.6910655985043, 819.8042911208307, 830.7525915647041, 822.8303498083538, 821.7093466996481, 826.0251448975097, 828.7896170567902, 758.8645928265892, 770.3061574607339, 779.5964443923385, 769.8574481602201, 781.3878507895342, 767.23656259583, 721.7352041876678, 556.3788527963637, 780.7559796530192, 721.5932364226843, 772.109651567884, 762.1893308479324, 782.5265581117075, 602.456836047257, 826.1092530891593, 821.5987329724218, 822.6080609549323, 767.4943105819093, 806.2230365615557, 604.2634667190006, 798.0601373886425, 829.4239911185156, 831.2535026700979, 830.7080011591783, 813.3091682631571, 799.7288855550404, 800.3473333287118, 768.9802698441994, 763.9512727208257, 773.6055712692234, 771.3471816100332, 773.3986683228575, 754.6842203930187, 526.2021656410567, 748.8948793750549, 748.5454017998256, 759.2276397004637, 756.3121895389252, 631.4272533168603, 557.4607103734404, 765.0561173691116, 759.5699769069391, 766.9385874830027, 760.4177363444885, 750.8880771246743, 630.6784499932808, 762.4414120182569, 760.7621757494029, 746.010863416285, 763.874488571206, 532.3164879532231, 687.6675798679838, 756.2191900354252, 763.8000868907018, 767.3755415149167, 762.527028963169, 617.7038902838049, 798.4277872780054, 818.3640648815013, 764.6373775233577, 754.7900473951082, 748.3181634564761, 842.5557426885198, 777.5632808384572, 788.0689395221785, 750.0888904972353, 498.5695709436904, 746.0096590482682, 572.959341284388, 767.2941634644477, 744.5605613042054, 762.9801012247117, 770.2021332337853, 762.9230747889524, 821.2750113208091, 826.6766367423824, 829.5255541742793, 828.8340339834549, 828.5614504885021, 825.1749656038949, 747.4700746167005, 722.9038004936127, 805.067481777126, 823.4347850779982, 819.7216134104817, 830.6063264330551, 559.6133236305546, 819.2009278111284, 819.6415181145499, 783.5235511736964, 775.2282575393153, 700.2968847408104, 766.558259645439, 772.9821380089084, 776.343267651404, 780.3584951928561, 758.218569213648, 622.3596298607112, 756.3390176907112, 779.3301160424071, 778.7775879255805, 763.3961964319354, 756.6107319350269, 739.1779319352605, 549.6343411877549, 833.9764968429297, 764.4295816554256, 777.9073123810464, 526.01237633175, 521.6012693579463, 501.4981478515126, 767.4816165992222, 763.4205157883022, 761.4203626377655, 764.4979391521376, 698.6160651309641, 814.2970154295009, 790.1004563634051, 816.3860449510555, 785.6653330581885, 766.1112935476497, 535.8650377644659, 774.7421429647887, 762.3779556927169, 792.8643873927674, 549.5867381717964, 549.6952619805727, 544.6889410982517, 818.2318130174483, 806.8222392264569, 810.0153569572338, 810.3650292925373, 766.6593965475413, 790.8367408097891, 783.3710417462682, 826.5262275459821, 773.5576041706169, 784.1040353286594, 738.6723687835713, 526.0925855652159, 768.3180197605486, 773.4047054152323, 774.8464198089496, 764.4657898709216, 765.2504213923092, 757.0538092838909, 753.391039063082, 750.9926985585026, 756.9776886028668, 769.3374535233631, 760.7447815387854, 758.4655020074433, 750.082457419647, 520.6963751869514, 751.7105874929678, 755.9683259528342, 577.6547453338856, 767.8866052631465, 746.8369161599351, 510.4480730028768, 762.8702783831645, 726.242275717446, 554.5572899666921, 724.6348559698342, 841.6617669690819, 834.1069887691405, 783.2326807375177, 820.7996059883008, 773.3256369044716, 771.6770822948623, 756.4360175705292, 773.5674563281367, 765.8106054422797, 828.4621363813148, 783.1280507033177, 829.7336084499766, 820.1569231821103, 810.7851828791631, 823.2671482170654, 811.5135941986302, 820.4018187014427, 823.5752003886967, 826.0983718820588, 824.5377989366467, 601.6110755785919, 807.2444892728745, 815.9543601089708, 807.5127369290478, 819.0888656117153, 815.8710730378948, 596.4603882300881, 804.7838199940876, 817.332425044545, 783.5755991618529, 544.7164278741558, 544.8519160654589, 539.548444683564, 641.1747805026783, 805.2623501126991, 788.5619304974236, 815.2259484718983, 810.3710053925189, 771.3524478864308, 755.6941857616472, 576.1950434059422, 825.1828827270969, 814.7533513972693, 826.050471620108, 826.3648726806498, 827.863896355023, 531.7631088630543, 816.493533980853, 818.0143964277438, 812.2104364978384, 793.9317561957477, 551.0003019055731, 808.2538878368588, 831.6515865188028, 813.2383962629126, 814.144140498266, 796.4468838721433, 810.8661388604131, 779.2953009050494, 803.8502808452765, 814.3833893785738, 787.6709780771906, 816.5528500998311, 801.5935985012118, 516.7563638211647, 781.3223828261338, 762.7602697840103, 773.1701054725619, 766.390736259746, 768.8538844647832, 598.1887597414559, 784.2033864799931, 776.1492344947764, 790.0518921892245, 767.2532912554432, 688.9827795783658, 784.6684973363123, 769.510137709204, 782.2065124523536, 765.0303605802345, 776.9183507641169, 539.688233621312, 820.4663635061672, 842.2740003593323, 789.7435878787289, 776.1617644972328, 525.4615829788357, 539.7746685341522, 766.7858647611608, 770.4299835970188, 777.4569874601409, 771.4801233964879, 783.4773066255975, 766.1630467172006, 783.1043046820392, 779.2660665287019, 787.6824729827674, 780.1656501246727, 784.1569417883327, 790.7119955718592, 772.3124300061048, 779.0386806213321, 790.6531653064511, 776.7921569914014, 790.1231186832131, 544.4750339690571, 575.3121409316941, 844.8113684476322, 829.0685412644353, 844.0444582489431, 819.26694345454, 840.7866569213755, 768.5170107654136, 745.7170004460567, 780.2238628078262, 775.051475500295, 786.3438283051803, 769.530665879495, 835.0810857978913, 809.6479202123663, 818.1991699625158, 831.395901282058, 784.926691115336, 791.9345213766618, 832.6279667454044, 826.517151519472, 697.9417281947527, 839.0617947324906, 824.796345198493, 847.4596865907454, 821.0535752981303, 818.0766116851005, 822.4838572855159, 842.1178074109673, 794.8492139009026, 811.0376255011254, 778.4984754048145, 763.8297329380534, 817.1171828525765, 825.9911099032405, 836.4007083036693, 835.1140526096241, 826.007748992812, 809.1006939208985, 833.5750108450732, 811.7856374249492, 829.9643855137057, 842.3985537920285, 559.63326006328, 540.5343822040672, 841.7402467218085, 841.8805033049184, 825.6706931750305, 559.6686064918543, 554.5033868104366, 535.7445039890404, 838.4178823457872, 830.6814315443894, 846.8017317024381, 774.5571082744005, 763.7789809897837, 772.3067565995951, 779.7031447727313, 788.0717071103463, 792.8072443932269, 796.9942473984295, 767.1394710899194, 539.5621276414261, 535.1553817127946, 632.4988342556625, 797.5899484074693, 796.970488765985, 811.1967292031773, 859.287931178983, 782.3905789399837, 533.1264349429462, 805.3715913929204, 850.7322763657783, 848.4515889580796, 564.5495246507857, 600.2450729672288, 840.2496455961506, 840.7076596316967, 563.1112891274083, 850.016314343724, 770.0199133553773, 569.6202543167591, 833.0621686603455, 499.9118081057994, 796.4556160499675, 787.7619599446706, 541.6354338107021, 535.0060938262092, 790.5180830253584, 775.3397904999155, 780.1096231560078, 794.3494488393941, 789.5440382041246, 653.0351864433043, 793.2960900408764, 786.1691427705628, 535.1189569058181, 535.0448418020936, 777.3428072990256, 769.4230149926582, 782.4541563453558, 765.81759693594, 695.9483981111194, 534.8429957003086, 778.3390009584801, 785.4676739586507, 758.5569185663769, 535.0616642358004, 771.0626014579802, 827.5001258446616, 693.1104679982991, 554.2633223231579, 603.9503691912716, 829.340988069757, 832.5479742287328, 754.3948680361609, 537.3269739965019, 530.3044926709218, 656.7269864869281, 784.6822140850487, 768.241294886854, 773.5797864591894, 681.0218970386958, 768.1853911232415, 783.1871879085444, 778.5528697343841, 788.4780012087369, 788.4456020497308, 783.9489449360287, 767.1646127082189, 654.7667630705812, 784.5237578061543, 773.4799092293673, 781.837381443482, 776.9447835270886, 763.109119019359, 780.0497472156655, 775.6173469432966, 784.2080816446014, 774.4783799140081, 772.8475019556934, 774.42291674711, 510.3709272878858, 765.6522010882613, 624.6793039981723, 535.0841018219426, 525.9220197208718, 770.023039345933, 780.9134204252193, 768.3882179215534, 758.1882791184388, 753.3051352975924, 586.4065290293404, 763.2014061344487, 763.9925544911009, 531.206136961725, 525.9750948006234, 521.4597187575231, 758.2533093594335, 760.8441435621023, 764.702226937828, 762.1127851410772, 549.4694336223047, 549.46518323263, 828.6695769367564, 833.880623218005, 755.1554290783419, 789.9053334244521, 851.8529857846316, 385.22738274698486, 775.7943405358269, 779.3107476076896, 837.5701458475218, 828.9487241977954, 865.1915290065549, 842.2710745776664, 775.6845153267687, 791.4971076096479, 793.9142455463981, 789.3095135439117, 748.5714076735702, 598.4351398251049, 789.4549288747851, 771.682002692383, 790.7541063879203, 535.0654590791138, 530.5182449232971, 575.414151307219, 773.8254881415374, 788.4405867296052, 786.263645350397, 781.6088719914959, 783.3222646902952, 773.7527197188201, 506.3991183891231, 782.3900762401163, 775.1557608059471, 775.4147699148467, 819.9786245304717, 812.1045530524437, 850.263748563959, 827.2985054975479, 847.7806051366034, 833.0001442957786, 841.4834757353589, 838.0866332070019, 765.4091004597524, 760.1172171451664, 759.767750884687, 766.4870286049512, 760.3135515546657, 762.1315880582632, 761.2376687626703, 693.9570750059024, 734.0457837243356, 760.5156955208349, 811.0869865372049, 801.7836933680808, 786.0489044587314, 804.8931473341742, 806.481099474361, 804.589818693606, 781.4236983377491, 786.7307870993068, 612.5650725262411, 795.7546782548823, 791.9739574345389, 793.3042686192806, 793.8939482840091, 773.8286090272054, 808.2698329055386, 812.512845548877, 738.995761592603, 737.1894155619964, 736.3890431661013, 594.729911271076, 717.5282611463051, 765.0292698675715, 779.2405542029483, 766.5058543766489, 777.3277121867256, 751.502853302788, 594.4479497780562, 771.5147212637169, 766.7689752605955, 819.8701840570382, 775.6276187285258, 770.843002993945, 760.676695215155, 763.9833933404037, 768.3355295822521, 762.2866949227838, 771.960541091506, 777.657454367713, 798.8819396578033, 775.7115869490374, 754.1076352212691, 762.6769498987501, 756.1479915575192, 746.8322034444229, 783.8761039706983, 845.997435621534, 819.0356274703989, 819.2802922927496, 805.0178771866908, 801.942951909016, 682.7056695120251, 761.8062663995079, 763.1994435665064, 744.3084893964524, 769.5081295277088, 821.9992690705791, 606.4103115369887, 584.2282825719777, 809.5645029604028, 820.1218332781089, 823.8022884867173, 802.3663839528631, 818.5390554205054, 800.0335655287779, 765.4696584876561, 752.5577749920072, 798.5529677298201, 778.3973002751418, 775.1384237540801, 761.6603098220434, 757.5552812737599, 768.1074113845143, 807.9450839001497, 600.7222219636756, 583.1294332965937, 685.0514417762289, 757.3187281232684, 800.766397494456, 781.0310263542284, 606.2379340083149, 778.9823259033261, 800.8984026898212, 800.6229147223034, 766.7351426612975, 706.0579292628905, 588.8069050919164, 776.9322923393944, 764.3730540974549, 780.0320992804084, 825.4498831658053, 762.4604418111892, 778.9178722682419, 777.2719795607356, 780.0194558106632, 825.4618609039117, 836.2501201719405, 824.5187989895375, 809.2234123469509, 601.6909137237185, 811.5587577413768, 815.4563008138674, 820.3158342192104, 815.6440429178974, 826.9722262776902, 632.7178666071608, 828.3902860481948, 769.6094690325345, 767.8574259364894, 769.3292839477398, 775.7663494415809, 763.2907018494229, 752.2133483105785, 772.4582753547295, 757.9454935842513, 757.3049814938385, 594.7234955227888, 624.7482582498229, 787.4562459889036, 809.77948907059, 772.7761445133993, 765.0782062506897, 774.2038379481577, 666.4404028604239, 783.9644199967066, 764.0163719382093, 754.3073197349534, 770.3448518061698, 758.3454494613803, 812.8350258010921, 688.5512304978615, 804.3469823513054, 751.7497337162409, 755.2276727549233, 754.953402726726, 774.3786045946254, 832.3825682670492, 680.8767165550298, 799.1482277484083, 793.2826712538232, 801.5465690074113, 732.160950115197, 574.3605022748123, 758.1871438656979, 802.7680491386102, 814.1351565281831, 764.7488900759262, 744.5117791054, 577.6550691546414, 744.0914946936125, 755.4717008611827, 751.2492043989517, 760.6454229716294, 743.5134994541722, 660.3525505260453, 736.421496539175, 737.9901982111438, 749.7013774115352, 740.6050492790694, 738.1292111897202, 738.9540152993061, 584.3159515554895, 748.5018825355081, 797.5055680647883, 784.3975198010802, 798.5188969412199, 793.9467756332639, 791.3986488774781, 680.3979044273099, 756.8953269936414, 761.6965106266963, 761.3172379244605, 593.3572292047593, 738.4265047339026, 767.1219492817626, 809.004543846821, 803.7409576852865, 824.3982994706159, 808.6463597813988, 805.8089964765508, 744.5326184436755, 740.5617931770255, 745.505070835929, 738.2709127497739, 728.8347646754316, 733.7116004106584, 733.1116406125168, 731.0427022439696, 732.7005480956633, 730.1514858825371, 604.3952057018666, 730.6761049713798, 735.5450724409307, 723.8987304683407, 739.73369665098, 730.2301675731469, 735.6518434769209, 635.3133598703489, 730.502048980437, 737.3195058765065, 741.0222353518396, 639.9269302177265, 828.543746366842, 788.2587092124056, 790.8940076769267, 815.077081833225, 751.6198157055359, 743.8162096572679, 744.9372504803341, 742.6531577781525, 748.4012815287898, 816.3369686239515, 835.0154491230999, 837.8430500475906, 820.3982075249062, 831.1522459268082, 828.1313310241867, 789.5488889907238, 770.8084576031678, 788.4495533159618, 789.0181731015689, 829.2639851218386, 810.3154476817784, 828.7351963745707, 808.0385293389241, 821.6454165822032, 759.9091182468064, 624.9647506988146, 814.5214569003577, 833.9519779107351, 788.0295059896346, 775.9930300909044, 825.2923742965404, 740.3507579663896, 741.956226710986, 746.3335817374931, 732.9449384694399]
Elapsed: 0.17389343410399893~0.02494284087949529
Time per graph: 0.001351141943624323~0.00019307076744165335
Speed: 752.1389248809978~85.0116985409984
Total Time: 0.1764
best val loss: 0.2884581249005111 test_score: 0.9141

Testing...
Test loss: 0.3161 score: 0.9062 time: 0.17s
test Score 0.9062
Epoch Time List: [0.6514348411001265, 0.5479835160076618, 0.543835316086188, 0.5608903067186475, 0.5674855799879879, 0.645805757958442, 0.5654887659475207, 0.5668850110378116, 0.5835749381221831, 0.5818482821341604, 0.8288890859112144, 0.6874047161545604, 0.5780567219480872, 0.5907871711533517, 0.5518349800258875, 0.568966040853411, 0.6502811082173139, 0.558662966825068, 0.6139067837502807, 0.6827350982930511, 0.8584778332151473, 0.650409989990294, 0.5780969799961895, 0.6574713753070682, 0.5677107872907072, 0.5731903361156583, 0.6690003371331841, 0.663868767907843, 0.5762715062592179, 0.5738839299883693, 0.5728925890289247, 0.5934950339142233, 0.5814659658353776, 0.6550932181999087, 0.8331951729487628, 0.6853766299318522, 0.6228819149546325, 0.5943403833080083, 0.5459639008622617, 0.571719435043633, 0.6070684096775949, 0.67064483393915, 0.5657713757827878, 0.5526248859241605, 0.5422062331344932, 0.5388611888047308, 0.5405601770617068, 0.5805209160316736, 0.6211097280029207, 0.546251927735284, 0.5540120000950992, 0.5487615403253585, 0.5449516321532428, 0.5425820790696889, 0.6170807520393282, 0.665134031092748, 0.5831636181101203, 0.5740805640816689, 0.5777133519295603, 0.5851070252247155, 0.5974329160526395, 0.720822888892144, 0.6321780362632126, 0.5933768870308995, 0.5753159276209772, 0.5724912760779262, 0.5662844991311431, 0.6544990579131991, 0.5486141450237483, 0.5537028987891972, 0.5585785571020097, 0.5628727211151272, 0.5581137479748577, 0.6298391248565167, 0.6273530973121524, 0.5551284332759678, 0.537341607036069, 0.5451390200760216, 0.5599424131214619, 0.5543598437216133, 0.5807774451095611, 0.6976082248147577, 0.5812663771212101, 0.5852044548373669, 0.5795763700734824, 0.5844443179666996, 0.6075666530523449, 0.7885066820308566, 0.7866816378664225, 0.5899617150425911, 0.587063561193645, 0.5853179232217371, 0.6241514489520341, 0.7469931731466204, 0.6412129218224436, 0.5863060329575092, 0.5791145809926093, 0.5875586587935686, 0.5888957346323878, 0.6857009381055832, 0.5954443216323853, 0.5863103300798684, 0.5918896098155528, 0.5891411970369518, 0.6740417440887541, 0.792027312098071, 0.586651710094884, 0.5906765698455274, 0.5857419609092176, 0.5911560789681971, 0.6275042474735528, 0.6650019958615303, 0.5570208597928286, 0.5753915419336408, 0.5933145030867308, 0.6544977549929172, 0.6514748248737305, 0.5837378248106688, 0.6537431557662785, 0.58354979660362, 0.6709226360544562, 0.5855118678882718, 0.6620401709806174, 0.6319984199944884, 0.5817320460919291, 0.5765212459955364, 0.5751727549359202, 0.583663399098441, 0.575212914030999, 0.6245843970682472, 0.5529889941681176, 0.5526393847540021, 0.5460633300244808, 0.5575012508779764, 0.5623063028324395, 0.6391850616782904, 0.5560168018564582, 0.5494875840377063, 0.5498797891195863, 0.5537669151090086, 0.697620635619387, 0.6710507250390947, 0.5500383640173823, 0.5687906160019338, 0.5749915530905128, 0.5945052767638117, 0.6695269090123475, 0.5788307979237288, 0.5828455418813974, 0.5739193938206881, 0.5818973190616816, 0.6216813160572201, 0.6805924160871655, 0.5797835097182542, 0.5806440240703523, 0.704303874168545, 0.5832063229754567, 0.6052217448595911, 0.7691395522560924, 0.6281094178557396, 0.5783505358267576, 0.5778328909073025, 0.751821924932301, 0.8561247140169144, 0.8702866218518466, 0.5877423970960081, 0.584331575781107, 0.5827174650039524, 0.588457620004192, 0.6013387490529567, 0.6478795579168946, 0.5607923429924995, 0.5526321225333959, 0.5654993038624525, 0.5839395031798631, 0.6662692399695516, 0.7014418307226151, 0.5914828500244766, 0.5778462977614254, 0.7158474458847195, 0.8182011898607016, 0.8183196177706122, 0.6863252171315253, 0.5506348090711981, 0.5511766690760851, 0.5530849087517709, 0.5821953939739615, 0.5651810998097062, 0.5909904916770756, 0.6214522290974855, 0.5646751327440143, 0.5709509910084307, 0.589108204934746, 0.7865909261163324, 0.718562857946381, 0.5791649462189525, 0.5807243043091148, 0.577527989866212, 0.5803511252161115, 0.579047636128962, 0.5897677359171212, 0.7743697501718998, 0.5936335811857134, 0.5855000899173319, 0.5868432619608939, 0.582811564905569, 0.5867910268716514, 0.698066424112767, 0.7382441158406436, 0.5824892281088978, 0.6630042020697147, 0.678390139946714, 0.5819976001512259, 0.6640927221160382, 0.5714111388660967, 0.5969323830213398, 0.7855599380563945, 0.7569130633492023, 0.6001123050227761, 0.5352803990244865, 0.5583624597638845, 0.5337138972245157, 0.5573283075354993, 0.5715679349377751, 0.6132525641005486, 0.6602767470758408, 0.5744741170201451, 0.5389897262211889, 0.55812221695669, 0.5530495762359351, 0.5503313988447189, 0.5646280799992383, 0.645521478028968, 0.5514049748890102, 0.5558957590255886, 0.5492079937830567, 0.5486940420232713, 0.5433853981085122, 0.6296349607873708, 0.6151712948922068, 0.5515101328492165, 0.5505502140149474, 0.5575964318122715, 0.5516721981111914, 0.6430764852557331, 0.596952143125236, 0.5535935340449214, 0.5585652417503297, 0.798212758032605, 0.8266647146083415, 0.8314333697780967, 0.7880301200784743, 0.5486171222291887, 0.5558237819932401, 0.5450187288224697, 0.551969330990687, 0.571517420001328, 0.5789052250329405, 0.650848297169432, 0.5596801040228456, 0.5454561747610569, 0.5433474171441048, 0.5388840732630342, 0.5404798078816384, 0.6748897910583764, 0.5552364247851074, 0.5488930230494589, 0.5455659588333219, 0.5550583966542035, 0.6693910979665816, 0.7018643422052264, 0.5368042576592416, 0.5454296569805592, 0.5495496029034257, 0.5522818409372121, 0.5534918729681522, 0.5669805146753788, 0.6440386162139475, 0.5552434488199651, 0.562889596214518, 0.5381789049133658, 0.5756728611886501, 0.6807274802122265, 0.5843959499616176, 0.5901080942712724, 0.5813424719963223, 0.5930975009687245, 0.597656280035153, 0.8089050508569926, 0.6621814600657672, 0.5809152869042009, 0.576172199100256, 0.5778989780228585, 0.599994502030313, 0.6533220519777387, 0.5802174219861627, 0.5783428209833801, 0.5816235761158168, 0.5820932791102678, 0.724549121921882, 0.7574351620860398, 0.5498989732004702, 0.6198216818738729, 0.571316379820928, 0.660717872902751, 0.8062451228033751, 0.6720353597775102, 0.6623916749376804, 0.5760202917736024, 0.679650858277455, 0.5740585711319, 0.5941246028523892, 0.6606450330000371, 0.5774014589842409, 0.5771647598594427, 0.5720363578293473, 0.5754935559816658, 0.5681153137702495, 0.5776335981208831, 0.6587920358870178, 0.5670429409947246, 0.567938880994916, 0.5687312260270119, 0.741788508137688, 0.7670630370266736, 0.5438444560859352, 0.542720657074824, 0.5451061497442424, 0.5577273142989725, 0.5534266538452357, 0.6203406318090856, 0.660791358910501, 0.603234649868682, 0.5854562681633979, 0.5833258589264005, 0.585320093203336, 0.550524661783129, 0.5823926208540797, 0.6466928506270051, 0.5608505825512111, 0.5623532370664179, 0.5710822788532823, 0.5615736290346831, 0.5769171821884811, 0.6766150128096342, 0.5607930987607688, 0.5540442552883178, 0.5490242652595043, 0.5576418759301305, 0.6443228190764785, 0.5588357441592962, 0.5483396057970822, 0.5678233681246638, 0.5625741840340197, 0.5769376321695745, 0.590520644094795, 0.5673778851050884, 0.6228481307625771, 0.5566889019683003, 0.5503687693271786, 0.5573633401654661, 0.5560891511850059, 0.6313047362491488, 0.5604544752277434, 0.5535481448750943, 0.5477278269827366, 0.8031589707825333, 0.8261191318742931, 0.5633641369640827, 0.5521739723626524, 0.5617218897677958, 0.7563742061611265, 0.8201960800215602, 0.8303176439367235, 0.5596285939682275, 0.5524211805313826, 0.5576768978498876, 0.5708728528115898, 0.5830703598912805, 0.603871809085831, 0.6610781161580235, 0.5813769020605832, 0.585755089763552, 0.5789614887908101, 0.5835501181427389, 0.7616803459823132, 0.8348039679694921, 0.800239983946085, 0.5692644689697772, 0.5655384652782232, 0.5757135488092899, 0.5357102581765503, 0.5762701858766377, 0.7383733899332583, 0.5770293159876019, 0.5348639970179647, 0.5462083029560745, 0.6959087559953332, 0.7407605196349323, 0.6168766871560365, 0.5434948080219328, 0.6153910181019455, 0.5330179089214653, 0.5502990940585732, 0.7776476612780243, 0.6572138308547437, 0.6542042507790029, 0.563481830060482, 0.5778834861703217, 0.6549268539529294, 0.8459754420910031, 0.6629017398227006, 0.5816641189157963, 0.5815122160129249, 0.5766320857219398, 0.5823722600471228, 0.6881094702985138, 0.5772253121249378, 0.5826270610559732, 0.7926414129324257, 0.8473923741839826, 0.7186442881356925, 0.5902395190205425, 0.5821062799077481, 0.5862884167581797, 0.6077192989178002, 0.7451740780379623, 0.6224611531943083, 0.588001930853352, 0.5870329809840769, 0.8503462220542133, 0.6604614299722016, 0.5537361919414252, 0.5912800228688866, 0.8016036788467318, 0.7988170520402491, 0.5754006078932434, 0.5448673472274095, 0.585379539988935, 0.692464197287336, 0.8431999441236258, 0.8106330600567162, 0.5836032521910965, 0.58832639711909, 0.5883104510139674, 0.6079184068366885, 0.6613830309361219, 0.586985879810527, 0.5842348842415959, 0.5785624019335955, 0.5840476367156953, 0.5815381209831685, 0.6135164890438318, 0.7910198378376663, 0.5817405229900032, 0.5897193709388375, 0.5782748369965702, 0.5787780378013849, 0.5974000887945294, 0.6528876179363579, 0.5844251881353557, 0.5834720758721232, 0.5841440150979906, 0.5869053949136287, 0.5877664957661182, 0.7037785092834383, 0.5954394908621907, 0.6304994428064674, 0.8473819890059531, 0.8473955208901316, 0.718061501160264, 0.585521134082228, 0.5879623331129551, 0.5928536460269243, 0.6014857692644, 0.8174558056052774, 0.6083881699014455, 0.6039700470864773, 0.6690604486502707, 0.8562319409102201, 0.8648251821286976, 0.7405825639143586, 0.612210385967046, 0.5924808159470558, 0.5932893448043615, 0.7703727190382779, 0.832765094935894, 0.6362756551243365, 0.560439090942964, 0.5836425716988742, 0.6819584460463375, 0.5485060778446496, 0.7749043249059469, 0.7000542108435184, 0.5840751142241061, 0.6298104557208717, 0.5571164397988468, 0.5872645941562951, 0.5400388999842107, 0.6267837840132415, 0.5672108801081777, 0.5752058271318674, 0.5736863508354872, 0.5876791020855308, 0.7114493630360812, 0.6430526867043227, 0.5811836831271648, 0.5774732772260904, 0.7160490441601723, 0.8432213880587369, 0.8221965557895601, 0.666370508261025, 0.5808187581133097, 0.5795498217921704, 0.5841713368427008, 0.5804641763679683, 0.5875575160607696, 0.717211005045101, 0.5864934169221669, 0.586860217154026, 0.576909254072234, 0.5678261239081621, 0.5823907281737775, 0.6310314980801195, 0.5596711670514196, 0.5454108028206974, 0.5481197910849005, 0.55400360096246, 0.5429114941507578, 0.5739486098755151, 0.6150992568582296, 0.6730800687801093, 0.5921797018963844, 0.5970183389727026, 0.5926933321170509, 0.5966905769892037, 0.6145398560911417, 0.6958901849575341, 0.5998946300242096, 0.552989904768765, 0.5656942059285939, 0.5885960129089653, 0.7081193330232054, 0.5639452389441431, 0.5581351222936064, 0.5697766849771142, 0.5695514441467822, 0.768945797579363, 0.5753936970140785, 0.5758088370785117, 0.5677427798509598, 0.5736767509952188, 0.5897768710274249, 0.7025911398231983, 0.5637537850998342, 0.5809046430513263, 0.6179161826148629, 0.6043233321979642, 0.7156237550079823, 0.7955424129031599, 0.5960180170368403, 0.580591012025252, 0.5902380859479308, 0.5886418560985476, 0.6119381829630584, 0.7013815972022712, 0.6650240870658308, 0.5866835198830813, 0.5673772150184959, 0.5796406029257923, 0.5910004312172532, 0.6109230979345739, 0.6808409669902176, 0.5864225828554481, 0.5874928929843009, 0.5838374148588628, 0.5841412132140249, 0.5584395651239902, 0.5709770158864558, 0.7331856968812644, 0.604482488008216, 0.6209514511283487, 0.6888066169340163, 0.777685227105394, 0.6077700790483505, 0.5550025270786136, 0.5700732960831374, 0.6761736520566046, 0.563054382102564, 0.6201480969320983, 0.6357232558075339, 0.5862071579322219, 0.6039157654158771, 0.593458557035774, 0.5468557218555361, 0.6433544680476189, 0.8986283240374178, 0.559327402850613, 0.5525527270510793, 0.5531374891288579, 0.5606817610096186, 0.5632453199941665, 0.5705855691339821, 0.6458255928009748, 0.6063987335655838, 0.5925456369295716, 0.5893582419957966, 0.5847483850084245, 0.593324112938717, 0.6516584397759289, 0.6601599708665162, 0.5675070411525667, 0.6483999278862029, 0.9372763668652624, 0.9052948821336031, 0.6076798946596682, 0.5752507550641894, 0.568583122221753, 0.845894519938156, 0.8704569609835744, 0.5741938608698547, 0.5665031229145825, 0.5871562487445772, 0.6012534631881863, 0.8617084629368037, 0.6631031057331711, 0.5967332657892257, 0.5825462329667062, 0.5825404268689454, 0.5729786131996661, 0.7132392229977995, 0.5925526048522443, 0.5860957952681929, 0.5658904321026057, 0.550268581835553, 0.5565011268481612, 0.5529216649010777, 0.627637597033754, 0.5842297382187098, 0.5607003930490464, 0.5567058869637549, 0.5605884748511016, 0.5592819256708026, 0.7086886551696807, 0.5928717239294201, 0.5871394788846374, 0.5973160245921463, 0.5997535670176148, 0.593864485854283, 0.6104837278835475, 0.6983926659449935, 0.5940037940163165, 0.5985103100538254, 0.6010278491303325, 0.7540049117524177, 0.9133148437831551, 0.7977777319028974, 0.5704026599414647, 0.5793960869777948, 0.5940642061177641, 0.5920136291533709, 0.6634716510307044, 0.7663813561666757, 0.5926953612361103, 0.5976301259361207, 0.594873980153352, 0.5951477130874991, 0.5740161039866507, 0.6239436061587185, 0.6148566720075905, 0.6055040862411261, 0.6043744699563831, 0.6100021309684962, 0.6889584409072995, 0.5476229689083993, 0.6406826868187636, 0.6359396758489311, 0.5679903330747038, 0.5781175547745079, 0.6559592019766569, 0.6630486201029271, 0.812621122924611, 0.575684767914936, 0.572173501830548, 0.5849145362153649, 0.6038805120624602, 0.9250745209865272, 0.6869897218421102, 0.6048362520523369, 0.6052047780249268, 0.5996921160258353, 0.6034246019553393, 0.6268014230299741, 0.6718717222101986, 0.6127077788114548, 0.6029288028366864, 0.6096366909332573, 0.6058108119759709, 0.6257388568483293, 0.9320272880140692, 0.6760998638346791, 0.5732277762144804, 0.5745905060321093, 0.5759597499854863, 0.5748741240240633, 0.5716837379150093, 0.6060670032165945, 0.6718830829486251, 0.5930648960638791, 0.591485301963985, 0.660993228899315, 0.8755148567724973, 0.5940587150398642, 0.5670260509941727, 0.5634119918104261, 0.5586855537258089, 0.5825064450036734, 0.6453982088714838, 0.5925123591441661, 0.6071999929845333, 0.604969727806747, 0.6060766412410885, 0.6257165789138526, 0.6848296748939902, 0.6104581498075277, 0.6163740698248148, 0.6123181998264045, 0.6187593450304121, 0.7434941199608147, 0.6175506531726569, 0.6154504166916013, 0.613213217118755, 0.6134224548004568, 0.613072965061292, 0.6174313169904053, 0.6947096006479114, 0.6465697619132698, 0.6148003321141005, 0.6105914819054306, 0.6645689911674708, 0.7794527350924909, 0.5651923918630928, 0.5697440488729626, 0.5559266330674291, 0.5799569478258491, 0.6280119714792818, 0.6883211040403694, 0.5970155159011483, 0.60507628088817, 0.582676975755021, 0.5432829021010548, 0.5441748781595379, 0.5718389779794961, 0.6399065011646599, 0.5455867359414697, 0.5690421722829342, 0.5804203432053328, 0.5764643133152276, 0.5774205757770687, 0.5708451638929546, 0.6788472938351333, 0.5496222318615764, 0.5558953201398253, 0.552489569876343, 0.6150847591925412, 0.8818742479197681, 0.7345429027918726, 0.5522672471124679, 0.6634731353260577, 0.5810844306834042, 0.5575657938607037, 0.7264710767194629, 0.699756225105375, 0.6836837911978364, 0.6096167487557977]
Total Epoch List: [282, 257, 236]
Total Time List: [0.16181897185742855, 0.17107051191851497, 0.17644424992613494]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cff9150>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.3711;  Loss pred: 3.3711; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4961 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 3.3698;  Loss pred: 3.3698; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4961 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 3.3204;  Loss pred: 3.3204; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4961 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 3.3139;  Loss pred: 3.3139; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4961 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 3.2593;  Loss pred: 3.2593; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 3.2393;  Loss pred: 3.2393; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4961 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 3.1451;  Loss pred: 3.1451; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 3.0699;  Loss pred: 3.0699; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4961 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 2.9991;  Loss pred: 2.9991; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 2.8950;  Loss pred: 2.8950; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 2.8037;  Loss pred: 2.8037; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4961 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 2.7001;  Loss pred: 2.7001; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4961 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 2.5763;  Loss pred: 2.5763; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 2.5209;  Loss pred: 2.5209; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 2.4203;  Loss pred: 2.4203; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 2.3406;  Loss pred: 2.3406; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 2.2731;  Loss pred: 2.2731; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.22s
Epoch 18/1000, LR 0.000285
Train loss: 2.1865;  Loss pred: 2.1865; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.16s
Epoch 19/1000, LR 0.000285
Train loss: 2.1134;  Loss pred: 2.1134; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 2.0749;  Loss pred: 2.0749; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 1.9894;  Loss pred: 1.9894; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 1.9361;  Loss pred: 1.9361; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.20s
Epoch 23/1000, LR 0.000285
Train loss: 1.8701;  Loss pred: 1.8701; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4961 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 1.8355;  Loss pred: 1.8355; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 1.7645;  Loss pred: 1.7645; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.7233;  Loss pred: 1.7233; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 1.6754;  Loss pred: 1.6754; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.6309;  Loss pred: 1.6309; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4961 time: 0.25s
Epoch 29/1000, LR 0.000285
Train loss: 1.5967;  Loss pred: 1.5967; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 1.5525;  Loss pred: 1.5525; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 1.5086;  Loss pred: 1.5086; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4961 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 1.4833;  Loss pred: 1.4833; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4961 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 1.4495;  Loss pred: 1.4495; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4961 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 1.4218;  Loss pred: 1.4218; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 1.3935;  Loss pred: 1.3935; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4961 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 1.3703;  Loss pred: 1.3703; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4961 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 1.3422;  Loss pred: 1.3422; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4961 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 1.3224;  Loss pred: 1.3224; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4961 time: 0.24s
Epoch 39/1000, LR 0.000284
Train loss: 1.3110;  Loss pred: 1.3110; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5039 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4961 time: 0.24s
Epoch 40/1000, LR 0.000284
Train loss: 1.2830;  Loss pred: 1.2830; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5039 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 1.2679;  Loss pred: 1.2679; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4961 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 1.2405;  Loss pred: 1.2405; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4961 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 1.2282;  Loss pred: 1.2282; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4961 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 1.2069;  Loss pred: 1.2069; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4961 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 1.1964;  Loss pred: 1.1964; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4961 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 1.1818;  Loss pred: 1.1818; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4961 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 1.1736;  Loss pred: 1.1736; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4961 time: 0.24s
Epoch 48/1000, LR 0.000284
Train loss: 1.1632;  Loss pred: 1.1632; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4961 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 1.1509;  Loss pred: 1.1509; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4961 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 1.1389;  Loss pred: 1.1389; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4961 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 1.1307;  Loss pred: 1.1307; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4961 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 1.1171;  Loss pred: 1.1171; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4961 time: 0.18s
Epoch 53/1000, LR 0.000284
Train loss: 1.1110;  Loss pred: 1.1110; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4961 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 1.1054;  Loss pred: 1.1054; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.4961 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 1.1012;  Loss pred: 1.1012; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4961 time: 0.16s
Epoch 56/1000, LR 0.000284
Train loss: 1.0938;  Loss pred: 1.0938; Loss self: 0.0000; time: 0.25s
Val loss: 0.6837 score: 0.5271 time: 0.23s
Test loss: 0.6858 score: 0.5039 time: 0.24s
Epoch 57/1000, LR 0.000283
Train loss: 1.0850;  Loss pred: 1.0850; Loss self: 0.0000; time: 0.36s
Val loss: 0.6830 score: 0.5349 time: 0.25s
Test loss: 0.6853 score: 0.5194 time: 0.24s
Epoch 58/1000, LR 0.000283
Train loss: 1.0789;  Loss pred: 1.0789; Loss self: 0.0000; time: 0.36s
Val loss: 0.6823 score: 0.6124 time: 0.25s
Test loss: 0.6847 score: 0.5891 time: 0.24s
Epoch 59/1000, LR 0.000283
Train loss: 1.0737;  Loss pred: 1.0737; Loss self: 0.0000; time: 0.36s
Val loss: 0.6815 score: 0.6589 time: 0.26s
Test loss: 0.6841 score: 0.6047 time: 0.17s
Epoch 60/1000, LR 0.000283
Train loss: 1.0659;  Loss pred: 1.0659; Loss self: 0.0000; time: 0.24s
Val loss: 0.6807 score: 0.7132 time: 0.16s
Test loss: 0.6834 score: 0.6202 time: 0.16s
Epoch 61/1000, LR 0.000283
Train loss: 1.0578;  Loss pred: 1.0578; Loss self: 0.0000; time: 0.24s
Val loss: 0.6799 score: 0.7132 time: 0.16s
Test loss: 0.6827 score: 0.6357 time: 0.16s
Epoch 62/1000, LR 0.000283
Train loss: 1.0538;  Loss pred: 1.0538; Loss self: 0.0000; time: 0.24s
Val loss: 0.6789 score: 0.7364 time: 0.16s
Test loss: 0.6820 score: 0.6434 time: 0.16s
Epoch 63/1000, LR 0.000283
Train loss: 1.0495;  Loss pred: 1.0495; Loss self: 0.0000; time: 0.24s
Val loss: 0.6780 score: 0.7984 time: 0.16s
Test loss: 0.6811 score: 0.7209 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 1.0423;  Loss pred: 1.0423; Loss self: 0.0000; time: 0.24s
Val loss: 0.6769 score: 0.8527 time: 0.16s
Test loss: 0.6803 score: 0.7597 time: 0.16s
Epoch 65/1000, LR 0.000283
Train loss: 1.0429;  Loss pred: 1.0429; Loss self: 0.0000; time: 0.28s
Val loss: 0.6758 score: 0.8682 time: 0.16s
Test loss: 0.6794 score: 0.7674 time: 0.24s
Epoch 66/1000, LR 0.000283
Train loss: 1.0363;  Loss pred: 1.0363; Loss self: 0.0000; time: 0.25s
Val loss: 0.6747 score: 0.8837 time: 0.16s
Test loss: 0.6785 score: 0.7829 time: 0.16s
Epoch 67/1000, LR 0.000283
Train loss: 1.0315;  Loss pred: 1.0315; Loss self: 0.0000; time: 0.24s
Val loss: 0.6735 score: 0.8837 time: 0.16s
Test loss: 0.6774 score: 0.7829 time: 0.16s
Epoch 68/1000, LR 0.000283
Train loss: 1.0298;  Loss pred: 1.0298; Loss self: 0.0000; time: 0.24s
Val loss: 0.6722 score: 0.8992 time: 0.16s
Test loss: 0.6764 score: 0.7829 time: 0.16s
Epoch 69/1000, LR 0.000283
Train loss: 1.0264;  Loss pred: 1.0264; Loss self: 0.0000; time: 0.24s
Val loss: 0.6708 score: 0.8992 time: 0.16s
Test loss: 0.6753 score: 0.7907 time: 0.16s
Epoch 70/1000, LR 0.000283
Train loss: 1.0210;  Loss pred: 1.0210; Loss self: 0.0000; time: 0.24s
Val loss: 0.6693 score: 0.8992 time: 0.16s
Test loss: 0.6741 score: 0.8062 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 1.0169;  Loss pred: 1.0169; Loss self: 0.0000; time: 0.27s
Val loss: 0.6678 score: 0.8992 time: 0.25s
Test loss: 0.6728 score: 0.8062 time: 0.24s
Epoch 72/1000, LR 0.000282
Train loss: 1.0148;  Loss pred: 1.0148; Loss self: 0.0000; time: 0.24s
Val loss: 0.6662 score: 0.8992 time: 0.16s
Test loss: 0.6715 score: 0.8217 time: 0.16s
Epoch 73/1000, LR 0.000282
Train loss: 1.0110;  Loss pred: 1.0110; Loss self: 0.0000; time: 0.23s
Val loss: 0.6645 score: 0.9070 time: 0.16s
Test loss: 0.6701 score: 0.8295 time: 0.16s
Epoch 74/1000, LR 0.000282
Train loss: 1.0082;  Loss pred: 1.0082; Loss self: 0.0000; time: 0.31s
Val loss: 0.6627 score: 0.9070 time: 0.24s
Test loss: 0.6687 score: 0.8450 time: 0.24s
Epoch 75/1000, LR 0.000282
Train loss: 1.0040;  Loss pred: 1.0040; Loss self: 0.0000; time: 0.35s
Val loss: 0.6608 score: 0.9070 time: 0.24s
Test loss: 0.6671 score: 0.8450 time: 0.23s
Epoch 76/1000, LR 0.000282
Train loss: 1.0012;  Loss pred: 1.0012; Loss self: 0.0000; time: 0.35s
Val loss: 0.6588 score: 0.9147 time: 0.24s
Test loss: 0.6655 score: 0.8527 time: 0.23s
Epoch 77/1000, LR 0.000282
Train loss: 0.9983;  Loss pred: 0.9983; Loss self: 0.0000; time: 0.35s
Val loss: 0.6568 score: 0.9147 time: 0.24s
Test loss: 0.6638 score: 0.8527 time: 0.23s
Epoch 78/1000, LR 0.000282
Train loss: 0.9953;  Loss pred: 0.9953; Loss self: 0.0000; time: 0.28s
Val loss: 0.6546 score: 0.9147 time: 0.16s
Test loss: 0.6621 score: 0.8450 time: 0.16s
Epoch 79/1000, LR 0.000282
Train loss: 0.9911;  Loss pred: 0.9911; Loss self: 0.0000; time: 0.24s
Val loss: 0.6523 score: 0.9147 time: 0.16s
Test loss: 0.6602 score: 0.8450 time: 0.16s
Epoch 80/1000, LR 0.000282
Train loss: 0.9879;  Loss pred: 0.9879; Loss self: 0.0000; time: 0.24s
Val loss: 0.6500 score: 0.9147 time: 0.16s
Test loss: 0.6583 score: 0.8450 time: 0.16s
Epoch 81/1000, LR 0.000281
Train loss: 0.9841;  Loss pred: 0.9841; Loss self: 0.0000; time: 0.23s
Val loss: 0.6475 score: 0.9147 time: 0.16s
Test loss: 0.6563 score: 0.8450 time: 0.16s
Epoch 82/1000, LR 0.000281
Train loss: 0.9817;  Loss pred: 0.9817; Loss self: 0.0000; time: 0.23s
Val loss: 0.6449 score: 0.9147 time: 0.16s
Test loss: 0.6542 score: 0.8450 time: 0.23s
Epoch 83/1000, LR 0.000281
Train loss: 0.9769;  Loss pred: 0.9769; Loss self: 0.0000; time: 0.33s
Val loss: 0.6421 score: 0.9147 time: 0.21s
Test loss: 0.6520 score: 0.8450 time: 0.17s
Epoch 84/1000, LR 0.000281
Train loss: 0.9743;  Loss pred: 0.9743; Loss self: 0.0000; time: 0.24s
Val loss: 0.6393 score: 0.9147 time: 0.18s
Test loss: 0.6497 score: 0.8450 time: 0.17s
Epoch 85/1000, LR 0.000281
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.24s
Val loss: 0.6363 score: 0.9147 time: 0.26s
Test loss: 0.6472 score: 0.8450 time: 0.17s
Epoch 86/1000, LR 0.000281
Train loss: 0.9692;  Loss pred: 0.9692; Loss self: 0.0000; time: 0.24s
Val loss: 0.6332 score: 0.9380 time: 0.17s
Test loss: 0.6447 score: 0.8450 time: 0.17s
Epoch 87/1000, LR 0.000281
Train loss: 0.9656;  Loss pred: 0.9656; Loss self: 0.0000; time: 0.25s
Val loss: 0.6300 score: 0.9380 time: 0.18s
Test loss: 0.6421 score: 0.8450 time: 0.17s
Epoch 88/1000, LR 0.000281
Train loss: 0.9624;  Loss pred: 0.9624; Loss self: 0.0000; time: 0.32s
Val loss: 0.6267 score: 0.9380 time: 0.23s
Test loss: 0.6394 score: 0.8450 time: 0.20s
Epoch 89/1000, LR 0.000281
Train loss: 0.9581;  Loss pred: 0.9581; Loss self: 0.0000; time: 0.25s
Val loss: 0.6232 score: 0.9457 time: 0.17s
Test loss: 0.6365 score: 0.8450 time: 0.17s
Epoch 90/1000, LR 0.000281
Train loss: 0.9551;  Loss pred: 0.9551; Loss self: 0.0000; time: 0.26s
Val loss: 0.6196 score: 0.9457 time: 0.17s
Test loss: 0.6336 score: 0.8450 time: 0.25s
Epoch 91/1000, LR 0.000280
Train loss: 0.9520;  Loss pred: 0.9520; Loss self: 0.0000; time: 0.23s
Val loss: 0.6158 score: 0.9457 time: 0.16s
Test loss: 0.6305 score: 0.8450 time: 0.15s
Epoch 92/1000, LR 0.000280
Train loss: 0.9488;  Loss pred: 0.9488; Loss self: 0.0000; time: 0.24s
Val loss: 0.6118 score: 0.9457 time: 0.17s
Test loss: 0.6272 score: 0.8450 time: 0.17s
Epoch 93/1000, LR 0.000280
Train loss: 0.9449;  Loss pred: 0.9449; Loss self: 0.0000; time: 0.25s
Val loss: 0.6076 score: 0.9457 time: 0.17s
Test loss: 0.6239 score: 0.8450 time: 0.17s
Epoch 94/1000, LR 0.000280
Train loss: 0.9409;  Loss pred: 0.9409; Loss self: 0.0000; time: 0.26s
Val loss: 0.6033 score: 0.9457 time: 0.21s
Test loss: 0.6204 score: 0.8450 time: 0.20s
Epoch 95/1000, LR 0.000280
Train loss: 0.9369;  Loss pred: 0.9369; Loss self: 0.0000; time: 0.25s
Val loss: 0.5988 score: 0.9457 time: 0.17s
Test loss: 0.6168 score: 0.8450 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.9337;  Loss pred: 0.9337; Loss self: 0.0000; time: 0.25s
Val loss: 0.5942 score: 0.9457 time: 0.18s
Test loss: 0.6130 score: 0.8450 time: 0.17s
Epoch 97/1000, LR 0.000280
Train loss: 0.9300;  Loss pred: 0.9300; Loss self: 0.0000; time: 0.25s
Val loss: 0.5894 score: 0.9457 time: 0.17s
Test loss: 0.6092 score: 0.8450 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.9266;  Loss pred: 0.9266; Loss self: 0.0000; time: 0.25s
Val loss: 0.5845 score: 0.9457 time: 0.17s
Test loss: 0.6052 score: 0.8450 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 0.25s
Val loss: 0.5795 score: 0.9457 time: 0.17s
Test loss: 0.6011 score: 0.8450 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.9167;  Loss pred: 0.9167; Loss self: 0.0000; time: 0.25s
Val loss: 0.5742 score: 0.9457 time: 0.17s
Test loss: 0.5969 score: 0.8450 time: 0.17s
Epoch 101/1000, LR 0.000279
Train loss: 0.9157;  Loss pred: 0.9157; Loss self: 0.0000; time: 0.28s
Val loss: 0.5686 score: 0.9457 time: 0.18s
Test loss: 0.5924 score: 0.8450 time: 0.26s
Epoch 102/1000, LR 0.000279
Train loss: 0.9102;  Loss pred: 0.9102; Loss self: 0.0000; time: 0.26s
Val loss: 0.5628 score: 0.9457 time: 0.17s
Test loss: 0.5877 score: 0.8450 time: 0.18s
Epoch 103/1000, LR 0.000279
Train loss: 0.9057;  Loss pred: 0.9057; Loss self: 0.0000; time: 0.26s
Val loss: 0.5568 score: 0.9457 time: 0.17s
Test loss: 0.5828 score: 0.8527 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.9015;  Loss pred: 0.9015; Loss self: 0.0000; time: 0.25s
Val loss: 0.5507 score: 0.9457 time: 0.17s
Test loss: 0.5778 score: 0.8527 time: 0.17s
Epoch 105/1000, LR 0.000279
Train loss: 0.8973;  Loss pred: 0.8973; Loss self: 0.0000; time: 0.26s
Val loss: 0.5445 score: 0.9457 time: 0.20s
Test loss: 0.5729 score: 0.8450 time: 0.20s
Epoch 106/1000, LR 0.000279
Train loss: 0.8933;  Loss pred: 0.8933; Loss self: 0.0000; time: 0.31s
Val loss: 0.5382 score: 0.9457 time: 0.17s
Test loss: 0.5678 score: 0.8450 time: 0.17s
Epoch 107/1000, LR 0.000278
Train loss: 0.8876;  Loss pred: 0.8876; Loss self: 0.0000; time: 0.25s
Val loss: 0.5318 score: 0.9457 time: 0.18s
Test loss: 0.5626 score: 0.8527 time: 0.17s
Epoch 108/1000, LR 0.000278
Train loss: 0.8837;  Loss pred: 0.8837; Loss self: 0.0000; time: 0.25s
Val loss: 0.5254 score: 0.9457 time: 0.17s
Test loss: 0.5574 score: 0.8527 time: 0.17s
Epoch 109/1000, LR 0.000278
Train loss: 0.8788;  Loss pred: 0.8788; Loss self: 0.0000; time: 0.26s
Val loss: 0.5190 score: 0.9457 time: 0.19s
Test loss: 0.5522 score: 0.8527 time: 0.17s
Epoch 110/1000, LR 0.000278
Train loss: 0.8750;  Loss pred: 0.8750; Loss self: 0.0000; time: 0.26s
Val loss: 0.5124 score: 0.9457 time: 0.17s
Test loss: 0.5469 score: 0.8527 time: 0.17s
Epoch 111/1000, LR 0.000278
Train loss: 0.8696;  Loss pred: 0.8696; Loss self: 0.0000; time: 0.25s
Val loss: 0.5058 score: 0.9457 time: 0.18s
Test loss: 0.5415 score: 0.8527 time: 0.17s
Epoch 112/1000, LR 0.000278
Train loss: 0.8661;  Loss pred: 0.8661; Loss self: 0.0000; time: 0.27s
Val loss: 0.4991 score: 0.9457 time: 0.20s
Test loss: 0.5361 score: 0.8527 time: 0.16s
Epoch 113/1000, LR 0.000278
Train loss: 0.8619;  Loss pred: 0.8619; Loss self: 0.0000; time: 0.24s
Val loss: 0.4924 score: 0.9380 time: 0.16s
Test loss: 0.5306 score: 0.8605 time: 0.16s
Epoch 114/1000, LR 0.000277
Train loss: 0.8571;  Loss pred: 0.8571; Loss self: 0.0000; time: 0.24s
Val loss: 0.4856 score: 0.9380 time: 0.17s
Test loss: 0.5251 score: 0.8605 time: 0.17s
Epoch 115/1000, LR 0.000277
Train loss: 0.8519;  Loss pred: 0.8519; Loss self: 0.0000; time: 0.24s
Val loss: 0.4786 score: 0.9457 time: 0.16s
Test loss: 0.5196 score: 0.8605 time: 0.16s
Epoch 116/1000, LR 0.000277
Train loss: 0.8471;  Loss pred: 0.8471; Loss self: 0.0000; time: 0.24s
Val loss: 0.4716 score: 0.9457 time: 0.16s
Test loss: 0.5140 score: 0.8605 time: 0.16s
Epoch 117/1000, LR 0.000277
Train loss: 0.8414;  Loss pred: 0.8414; Loss self: 0.0000; time: 0.24s
Val loss: 0.4645 score: 0.9380 time: 0.17s
Test loss: 0.5083 score: 0.8605 time: 0.16s
Epoch 118/1000, LR 0.000277
Train loss: 0.8368;  Loss pred: 0.8368; Loss self: 0.0000; time: 0.35s
Val loss: 0.4575 score: 0.9380 time: 0.24s
Test loss: 0.5025 score: 0.8605 time: 0.24s
Epoch 119/1000, LR 0.000277
Train loss: 0.8311;  Loss pred: 0.8311; Loss self: 0.0000; time: 0.27s
Val loss: 0.4504 score: 0.9457 time: 0.16s
Test loss: 0.4966 score: 0.8605 time: 0.16s
Epoch 120/1000, LR 0.000277
Train loss: 0.8261;  Loss pred: 0.8261; Loss self: 0.0000; time: 0.25s
Val loss: 0.4432 score: 0.9380 time: 0.16s
Test loss: 0.4907 score: 0.8605 time: 0.16s
Epoch 121/1000, LR 0.000276
Train loss: 0.8233;  Loss pred: 0.8233; Loss self: 0.0000; time: 0.24s
Val loss: 0.4359 score: 0.9380 time: 0.16s
Test loss: 0.4849 score: 0.8605 time: 0.16s
Epoch 122/1000, LR 0.000276
Train loss: 0.8177;  Loss pred: 0.8177; Loss self: 0.0000; time: 0.24s
Val loss: 0.4288 score: 0.9380 time: 0.16s
Test loss: 0.4791 score: 0.8605 time: 0.16s
Epoch 123/1000, LR 0.000276
Train loss: 0.8134;  Loss pred: 0.8134; Loss self: 0.0000; time: 0.24s
Val loss: 0.4215 score: 0.9380 time: 0.16s
Test loss: 0.4734 score: 0.8605 time: 0.16s
Epoch 124/1000, LR 0.000276
Train loss: 0.8093;  Loss pred: 0.8093; Loss self: 0.0000; time: 0.29s
Val loss: 0.4144 score: 0.9380 time: 0.17s
Test loss: 0.4678 score: 0.8605 time: 0.25s
Epoch 125/1000, LR 0.000276
Train loss: 0.8038;  Loss pred: 0.8038; Loss self: 0.0000; time: 0.25s
Val loss: 0.4074 score: 0.9380 time: 0.16s
Test loss: 0.4620 score: 0.8682 time: 0.16s
Epoch 126/1000, LR 0.000276
Train loss: 0.7984;  Loss pred: 0.7984; Loss self: 0.0000; time: 0.24s
Val loss: 0.4005 score: 0.9380 time: 0.16s
Test loss: 0.4564 score: 0.8682 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.7956;  Loss pred: 0.7956; Loss self: 0.0000; time: 0.25s
Val loss: 0.3933 score: 0.9380 time: 0.22s
Test loss: 0.4512 score: 0.8605 time: 0.23s
Epoch 128/1000, LR 0.000275
Train loss: 0.7894;  Loss pred: 0.7894; Loss self: 0.0000; time: 0.36s
Val loss: 0.3865 score: 0.9380 time: 0.25s
Test loss: 0.4458 score: 0.8682 time: 0.24s
Epoch 129/1000, LR 0.000275
Train loss: 0.7842;  Loss pred: 0.7842; Loss self: 0.0000; time: 0.36s
Val loss: 0.3800 score: 0.9380 time: 0.24s
Test loss: 0.4402 score: 0.8682 time: 0.24s
Epoch 130/1000, LR 0.000275
Train loss: 0.7791;  Loss pred: 0.7791; Loss self: 0.0000; time: 0.36s
Val loss: 0.3737 score: 0.9302 time: 0.24s
Test loss: 0.4346 score: 0.8527 time: 0.24s
Epoch 131/1000, LR 0.000275
Train loss: 0.7765;  Loss pred: 0.7765; Loss self: 0.0000; time: 0.33s
Val loss: 0.3677 score: 0.9302 time: 0.16s
Test loss: 0.4292 score: 0.8527 time: 0.16s
Epoch 132/1000, LR 0.000275
Train loss: 0.7698;  Loss pred: 0.7698; Loss self: 0.0000; time: 0.25s
Val loss: 0.3614 score: 0.9302 time: 0.16s
Test loss: 0.4240 score: 0.8527 time: 0.16s
Epoch 133/1000, LR 0.000274
Train loss: 0.7667;  Loss pred: 0.7667; Loss self: 0.0000; time: 0.24s
Val loss: 0.3552 score: 0.9302 time: 0.16s
Test loss: 0.4190 score: 0.8527 time: 0.16s
Epoch 134/1000, LR 0.000274
Train loss: 0.7623;  Loss pred: 0.7623; Loss self: 0.0000; time: 0.33s
Val loss: 0.3490 score: 0.9302 time: 0.25s
Test loss: 0.4141 score: 0.8527 time: 0.16s
Epoch 135/1000, LR 0.000274
Train loss: 0.7580;  Loss pred: 0.7580; Loss self: 0.0000; time: 0.24s
Val loss: 0.3432 score: 0.9302 time: 0.16s
Test loss: 0.4092 score: 0.8527 time: 0.16s
Epoch 136/1000, LR 0.000274
Train loss: 0.7546;  Loss pred: 0.7546; Loss self: 0.0000; time: 0.24s
Val loss: 0.3369 score: 0.9302 time: 0.16s
Test loss: 0.4046 score: 0.8527 time: 0.16s
Epoch 137/1000, LR 0.000274
Train loss: 0.7506;  Loss pred: 0.7506; Loss self: 0.0000; time: 0.24s
Val loss: 0.3312 score: 0.9302 time: 0.16s
Test loss: 0.4001 score: 0.8527 time: 0.16s
Epoch 138/1000, LR 0.000274
Train loss: 0.7457;  Loss pred: 0.7457; Loss self: 0.0000; time: 0.24s
Val loss: 0.3257 score: 0.9302 time: 0.16s
Test loss: 0.3955 score: 0.8527 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.7420;  Loss pred: 0.7420; Loss self: 0.0000; time: 0.25s
Val loss: 0.3203 score: 0.9302 time: 0.24s
Test loss: 0.3910 score: 0.8527 time: 0.24s
Epoch 140/1000, LR 0.000273
Train loss: 0.7385;  Loss pred: 0.7385; Loss self: 0.0000; time: 0.26s
Val loss: 0.3149 score: 0.9380 time: 0.17s
Test loss: 0.3867 score: 0.8527 time: 0.16s
Epoch 141/1000, LR 0.000273
Train loss: 0.7343;  Loss pred: 0.7343; Loss self: 0.0000; time: 0.25s
Val loss: 0.3101 score: 0.9380 time: 0.16s
Test loss: 0.3823 score: 0.8605 time: 0.16s
Epoch 142/1000, LR 0.000273
Train loss: 0.7300;  Loss pred: 0.7300; Loss self: 0.0000; time: 0.25s
Val loss: 0.3051 score: 0.9380 time: 0.16s
Test loss: 0.3781 score: 0.8605 time: 0.16s
Epoch 143/1000, LR 0.000273
Train loss: 0.7264;  Loss pred: 0.7264; Loss self: 0.0000; time: 0.25s
Val loss: 0.3002 score: 0.9380 time: 0.16s
Test loss: 0.3741 score: 0.8605 time: 0.16s
Epoch 144/1000, LR 0.000272
Train loss: 0.7236;  Loss pred: 0.7236; Loss self: 0.0000; time: 0.24s
Val loss: 0.2957 score: 0.9457 time: 0.17s
Test loss: 0.3701 score: 0.8605 time: 0.19s
Epoch 145/1000, LR 0.000272
Train loss: 0.7196;  Loss pred: 0.7196; Loss self: 0.0000; time: 0.25s
Val loss: 0.2907 score: 0.9380 time: 0.17s
Test loss: 0.3665 score: 0.8605 time: 0.22s
Epoch 146/1000, LR 0.000272
Train loss: 0.7173;  Loss pred: 0.7173; Loss self: 0.0000; time: 0.32s
Val loss: 0.2862 score: 0.9380 time: 0.17s
Test loss: 0.3628 score: 0.8605 time: 0.16s
Epoch 147/1000, LR 0.000272
Train loss: 0.7148;  Loss pred: 0.7148; Loss self: 0.0000; time: 0.24s
Val loss: 0.2816 score: 0.9380 time: 0.16s
Test loss: 0.3594 score: 0.8605 time: 0.16s
Epoch 148/1000, LR 0.000272
Train loss: 0.7109;  Loss pred: 0.7109; Loss self: 0.0000; time: 0.24s
Val loss: 0.2774 score: 0.9380 time: 0.16s
Test loss: 0.3559 score: 0.8605 time: 0.16s
Epoch 149/1000, LR 0.000272
Train loss: 0.7060;  Loss pred: 0.7060; Loss self: 0.0000; time: 0.24s
Val loss: 0.2736 score: 0.9380 time: 0.16s
Test loss: 0.3523 score: 0.8605 time: 0.16s
Epoch 150/1000, LR 0.000271
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.24s
Val loss: 0.2703 score: 0.9457 time: 0.17s
Test loss: 0.3487 score: 0.8605 time: 0.16s
Epoch 151/1000, LR 0.000271
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.36s
Val loss: 0.2666 score: 0.9457 time: 0.25s
Test loss: 0.3454 score: 0.8605 time: 0.16s
Epoch 152/1000, LR 0.000271
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.25s
Val loss: 0.2630 score: 0.9457 time: 0.17s
Test loss: 0.3423 score: 0.8605 time: 0.16s
Epoch 153/1000, LR 0.000271
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.24s
Val loss: 0.2594 score: 0.9457 time: 0.17s
Test loss: 0.3393 score: 0.8605 time: 0.16s
Epoch 154/1000, LR 0.000271
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.24s
Val loss: 0.2554 score: 0.9380 time: 0.17s
Test loss: 0.3366 score: 0.8605 time: 0.16s
Epoch 155/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.25s
Val loss: 0.2520 score: 0.9380 time: 0.17s
Test loss: 0.3337 score: 0.8605 time: 0.16s
Epoch 156/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.25s
Val loss: 0.2489 score: 0.9380 time: 0.17s
Test loss: 0.3309 score: 0.8605 time: 0.16s
Epoch 157/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.25s
Val loss: 0.2454 score: 0.9380 time: 0.16s
Test loss: 0.3284 score: 0.8605 time: 0.19s
Epoch 158/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.25s
Val loss: 0.2426 score: 0.9380 time: 0.22s
Test loss: 0.3257 score: 0.8605 time: 0.19s
Epoch 159/1000, LR 0.000270
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.25s
Val loss: 0.2398 score: 0.9380 time: 0.16s
Test loss: 0.3230 score: 0.8605 time: 0.16s
Epoch 160/1000, LR 0.000269
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.25s
Val loss: 0.2371 score: 0.9380 time: 0.16s
Test loss: 0.3204 score: 0.8605 time: 0.16s
Epoch 161/1000, LR 0.000269
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.24s
Val loss: 0.2349 score: 0.9457 time: 0.16s
Test loss: 0.3178 score: 0.8605 time: 0.15s
Epoch 162/1000, LR 0.000269
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.25s
Val loss: 0.2321 score: 0.9457 time: 0.16s
Test loss: 0.3155 score: 0.8605 time: 0.16s
Epoch 163/1000, LR 0.000269
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.24s
Val loss: 0.2297 score: 0.9457 time: 0.16s
Test loss: 0.3132 score: 0.8605 time: 0.15s
Epoch 164/1000, LR 0.000269
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.27s
Val loss: 0.2274 score: 0.9457 time: 0.16s
Test loss: 0.3109 score: 0.8605 time: 0.19s
Epoch 165/1000, LR 0.000268
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.33s
Val loss: 0.2245 score: 0.9457 time: 0.16s
Test loss: 0.3090 score: 0.8605 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.24s
Val loss: 0.2227 score: 0.9457 time: 0.16s
Test loss: 0.3067 score: 0.8605 time: 0.16s
Epoch 167/1000, LR 0.000268
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.24s
Val loss: 0.2202 score: 0.9457 time: 0.17s
Test loss: 0.3048 score: 0.8605 time: 0.17s
Epoch 168/1000, LR 0.000268
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.25s
Val loss: 0.2181 score: 0.9457 time: 0.17s
Test loss: 0.3028 score: 0.8605 time: 0.17s
Epoch 169/1000, LR 0.000267
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.24s
Val loss: 0.2159 score: 0.9457 time: 0.17s
Test loss: 0.3010 score: 0.8527 time: 0.17s
Epoch 170/1000, LR 0.000267
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.25s
Val loss: 0.2135 score: 0.9380 time: 0.17s
Test loss: 0.2994 score: 0.8527 time: 0.28s
Epoch 171/1000, LR 0.000267
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.29s
Val loss: 0.2121 score: 0.9457 time: 0.18s
Test loss: 0.2973 score: 0.8527 time: 0.26s
Epoch 172/1000, LR 0.000267
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.24s
Val loss: 0.2115 score: 0.9457 time: 0.17s
Test loss: 0.2950 score: 0.8682 time: 0.17s
Epoch 173/1000, LR 0.000267
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.25s
Val loss: 0.2096 score: 0.9457 time: 0.17s
Test loss: 0.2933 score: 0.8682 time: 0.17s
Epoch 174/1000, LR 0.000266
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.25s
Val loss: 0.2084 score: 0.9457 time: 0.17s
Test loss: 0.2915 score: 0.8682 time: 0.17s
Epoch 175/1000, LR 0.000266
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.24s
Val loss: 0.2067 score: 0.9457 time: 0.17s
Test loss: 0.2899 score: 0.8682 time: 0.17s
Epoch 176/1000, LR 0.000266
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.25s
Val loss: 0.2052 score: 0.9457 time: 0.17s
Test loss: 0.2883 score: 0.8682 time: 0.17s
Epoch 177/1000, LR 0.000266
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.25s
Val loss: 0.2037 score: 0.9457 time: 0.17s
Test loss: 0.2868 score: 0.8682 time: 0.17s
Epoch 178/1000, LR 0.000265
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.25s
Val loss: 0.2019 score: 0.9457 time: 0.18s
Test loss: 0.2855 score: 0.8605 time: 0.18s
Epoch 179/1000, LR 0.000265
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.26s
Val loss: 0.1999 score: 0.9457 time: 0.29s
Test loss: 0.2843 score: 0.8605 time: 0.16s
Epoch 180/1000, LR 0.000265
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.23s
Val loss: 0.1985 score: 0.9457 time: 0.16s
Test loss: 0.2830 score: 0.8605 time: 0.16s
Epoch 181/1000, LR 0.000265
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.25s
Val loss: 0.1966 score: 0.9457 time: 0.16s
Test loss: 0.2819 score: 0.8605 time: 0.20s
Epoch 182/1000, LR 0.000265
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.23s
Val loss: 0.1951 score: 0.9380 time: 0.17s
Test loss: 0.2808 score: 0.8605 time: 0.16s
Epoch 183/1000, LR 0.000264
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 0.23s
Val loss: 0.1936 score: 0.9380 time: 0.17s
Test loss: 0.2797 score: 0.8605 time: 0.17s
Epoch 184/1000, LR 0.000264
Train loss: 0.6296;  Loss pred: 0.6296; Loss self: 0.0000; time: 0.24s
Val loss: 0.1925 score: 0.9380 time: 0.17s
Test loss: 0.2784 score: 0.8605 time: 0.24s
Epoch 185/1000, LR 0.000264
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.22s
Val loss: 0.1921 score: 0.9457 time: 0.16s
Test loss: 0.2768 score: 0.8605 time: 0.15s
Epoch 186/1000, LR 0.000264
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.24s
Val loss: 0.1918 score: 0.9535 time: 0.17s
Test loss: 0.2752 score: 0.8605 time: 0.16s
Epoch 187/1000, LR 0.000263
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.30s
Val loss: 0.1915 score: 0.9535 time: 0.17s
Test loss: 0.2738 score: 0.8605 time: 0.17s
Epoch 188/1000, LR 0.000263
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.34s
Val loss: 0.1897 score: 0.9535 time: 0.17s
Test loss: 0.2730 score: 0.8605 time: 0.16s
Epoch 189/1000, LR 0.000263
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.24s
Val loss: 0.1887 score: 0.9535 time: 0.17s
Test loss: 0.2719 score: 0.8682 time: 0.17s
Epoch 190/1000, LR 0.000263
Train loss: 0.6230;  Loss pred: 0.6230; Loss self: 0.0000; time: 0.24s
Val loss: 0.1871 score: 0.9457 time: 0.17s
Test loss: 0.2713 score: 0.8682 time: 0.17s
Epoch 191/1000, LR 0.000262
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.24s
Val loss: 0.1864 score: 0.9535 time: 0.17s
Test loss: 0.2702 score: 0.8682 time: 0.17s
Epoch 192/1000, LR 0.000262
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.27s
Val loss: 0.1854 score: 0.9457 time: 0.17s
Test loss: 0.2694 score: 0.8682 time: 0.24s
Epoch 193/1000, LR 0.000262
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.35s
Val loss: 0.1855 score: 0.9535 time: 0.24s
Test loss: 0.2680 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 194/1000, LR 0.000262
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.35s
Val loss: 0.1844 score: 0.9535 time: 0.24s
Test loss: 0.2673 score: 0.8760 time: 0.25s
Epoch 195/1000, LR 0.000261
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.24s
Val loss: 0.1838 score: 0.9535 time: 0.16s
Test loss: 0.2663 score: 0.8760 time: 0.16s
Epoch 196/1000, LR 0.000261
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.23s
Val loss: 0.1833 score: 0.9535 time: 0.17s
Test loss: 0.2654 score: 0.8760 time: 0.17s
Epoch 197/1000, LR 0.000261
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.25s
Val loss: 0.1815 score: 0.9457 time: 0.17s
Test loss: 0.2651 score: 0.8760 time: 0.17s
Epoch 198/1000, LR 0.000261
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 0.25s
Val loss: 0.1803 score: 0.9457 time: 0.18s
Test loss: 0.2645 score: 0.8760 time: 0.17s
Epoch 199/1000, LR 0.000260
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.26s
Val loss: 0.1805 score: 0.9457 time: 0.18s
Test loss: 0.2633 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 0.29s
Val loss: 0.1794 score: 0.9457 time: 0.18s
Test loss: 0.2627 score: 0.8760 time: 0.26s
Epoch 201/1000, LR 0.000260
Train loss: 0.6085;  Loss pred: 0.6085; Loss self: 0.0000; time: 0.25s
Val loss: 0.1787 score: 0.9457 time: 0.18s
Test loss: 0.2621 score: 0.8760 time: 0.17s
Epoch 202/1000, LR 0.000260
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.26s
Val loss: 0.1785 score: 0.9457 time: 0.17s
Test loss: 0.2611 score: 0.8760 time: 0.17s
Epoch 203/1000, LR 0.000259
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.25s
Val loss: 0.1777 score: 0.9457 time: 0.18s
Test loss: 0.2605 score: 0.8760 time: 0.17s
Epoch 204/1000, LR 0.000259
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 0.25s
Val loss: 0.1766 score: 0.9457 time: 0.17s
Test loss: 0.2601 score: 0.8760 time: 0.17s
Epoch 205/1000, LR 0.000259
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 0.26s
Val loss: 0.1758 score: 0.9457 time: 0.18s
Test loss: 0.2595 score: 0.8760 time: 0.24s
Epoch 206/1000, LR 0.000259
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.37s
Val loss: 0.1757 score: 0.9457 time: 0.25s
Test loss: 0.2587 score: 0.8760 time: 0.25s
Epoch 207/1000, LR 0.000258
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.34s
Val loss: 0.1755 score: 0.9457 time: 0.18s
Test loss: 0.2580 score: 0.8760 time: 0.17s
Epoch 208/1000, LR 0.000258
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.25s
Val loss: 0.1751 score: 0.9457 time: 0.18s
Test loss: 0.2573 score: 0.8760 time: 0.17s
Epoch 209/1000, LR 0.000258
Train loss: 0.6013;  Loss pred: 0.6013; Loss self: 0.0000; time: 0.25s
Val loss: 0.1751 score: 0.9535 time: 0.18s
Test loss: 0.2566 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.25s
Val loss: 0.1735 score: 0.9457 time: 0.18s
Test loss: 0.2565 score: 0.8760 time: 0.17s
Epoch 211/1000, LR 0.000257
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.26s
Val loss: 0.1723 score: 0.9457 time: 0.18s
Test loss: 0.2563 score: 0.8760 time: 0.23s
Epoch 212/1000, LR 0.000257
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.37s
Val loss: 0.1720 score: 0.9457 time: 0.18s
Test loss: 0.2558 score: 0.8760 time: 0.17s
Epoch 213/1000, LR 0.000257
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.25s
Val loss: 0.1715 score: 0.9457 time: 0.18s
Test loss: 0.2553 score: 0.8760 time: 0.17s
Epoch 214/1000, LR 0.000256
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.24s
Val loss: 0.1718 score: 0.9457 time: 0.18s
Test loss: 0.2545 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.26s
Val loss: 0.1716 score: 0.9457 time: 0.24s
Test loss: 0.2539 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.5943;  Loss pred: 0.5943; Loss self: 0.0000; time: 0.34s
Val loss: 0.1713 score: 0.9457 time: 0.16s
Test loss: 0.2534 score: 0.8760 time: 0.16s
Epoch 217/1000, LR 0.000256
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.25s
Val loss: 0.1699 score: 0.9457 time: 0.18s
Test loss: 0.2534 score: 0.8760 time: 0.17s
Epoch 218/1000, LR 0.000255
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.25s
Val loss: 0.1687 score: 0.9457 time: 0.17s
Test loss: 0.2535 score: 0.8760 time: 0.17s
Epoch 219/1000, LR 0.000255
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.25s
Val loss: 0.1679 score: 0.9457 time: 0.17s
Test loss: 0.2534 score: 0.8760 time: 0.17s
Epoch 220/1000, LR 0.000255
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 0.25s
Val loss: 0.1667 score: 0.9457 time: 0.17s
Test loss: 0.2535 score: 0.8760 time: 0.17s
Epoch 221/1000, LR 0.000255
Train loss: 0.5901;  Loss pred: 0.5901; Loss self: 0.0000; time: 0.27s
Val loss: 0.1657 score: 0.9457 time: 0.18s
Test loss: 0.2538 score: 0.8682 time: 0.17s
Epoch 222/1000, LR 0.000254
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.33s
Val loss: 0.1654 score: 0.9457 time: 0.17s
Test loss: 0.2534 score: 0.8682 time: 0.17s
Epoch 223/1000, LR 0.000254
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.25s
Val loss: 0.1652 score: 0.9457 time: 0.17s
Test loss: 0.2530 score: 0.8682 time: 0.16s
Epoch 224/1000, LR 0.000254
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.25s
Val loss: 0.1656 score: 0.9457 time: 0.17s
Test loss: 0.2521 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.25s
Val loss: 0.1663 score: 0.9457 time: 0.17s
Test loss: 0.2512 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.25s
Val loss: 0.1657 score: 0.9457 time: 0.17s
Test loss: 0.2511 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.26s
Val loss: 0.1660 score: 0.9457 time: 0.17s
Test loss: 0.2505 score: 0.8760 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.5869;  Loss pred: 0.5869; Loss self: 0.0000; time: 0.26s
Val loss: 0.1662 score: 0.9457 time: 0.17s
Test loss: 0.2500 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 0.24s
Val loss: 0.1659 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.24s
Val loss: 0.1650 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8760 time: 0.16s
Epoch 231/1000, LR 0.000252
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.24s
Val loss: 0.1637 score: 0.9457 time: 0.16s
Test loss: 0.2503 score: 0.8760 time: 0.16s
Epoch 232/1000, LR 0.000251
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.24s
Val loss: 0.1645 score: 0.9457 time: 0.16s
Test loss: 0.2494 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.24s
Val loss: 0.1639 score: 0.9457 time: 0.16s
Test loss: 0.2494 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.27s
Val loss: 0.1645 score: 0.9457 time: 0.16s
Test loss: 0.2488 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.32s
Val loss: 0.1643 score: 0.9457 time: 0.16s
Test loss: 0.2487 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 236/1000, LR 0.000250
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.24s
Val loss: 0.1628 score: 0.9457 time: 0.16s
Test loss: 0.2491 score: 0.8760 time: 0.15s
Epoch 237/1000, LR 0.000250
Train loss: 0.5795;  Loss pred: 0.5795; Loss self: 0.0000; time: 0.24s
Val loss: 0.1626 score: 0.9457 time: 0.16s
Test loss: 0.2489 score: 0.8760 time: 0.16s
Epoch 238/1000, LR 0.000250
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.24s
Val loss: 0.1621 score: 0.9457 time: 0.16s
Test loss: 0.2489 score: 0.8682 time: 0.16s
Epoch 239/1000, LR 0.000249
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.24s
Val loss: 0.1615 score: 0.9457 time: 0.17s
Test loss: 0.2490 score: 0.8682 time: 0.16s
Epoch 240/1000, LR 0.000249
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.33s
Val loss: 0.1604 score: 0.9457 time: 0.16s
Test loss: 0.2495 score: 0.8682 time: 0.16s
Epoch 241/1000, LR 0.000249
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.23s
Val loss: 0.1597 score: 0.9457 time: 0.16s
Test loss: 0.2497 score: 0.8682 time: 0.16s
Epoch 242/1000, LR 0.000248
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.24s
Val loss: 0.1599 score: 0.9457 time: 0.17s
Test loss: 0.2492 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 0.25s
Val loss: 0.1603 score: 0.9457 time: 0.17s
Test loss: 0.2487 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.25s
Val loss: 0.1605 score: 0.9457 time: 0.17s
Test loss: 0.2484 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5739;  Loss pred: 0.5739; Loss self: 0.0000; time: 0.25s
Val loss: 0.1602 score: 0.9457 time: 0.17s
Test loss: 0.2483 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 246/1000, LR 0.000247
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 0.24s
Val loss: 0.1597 score: 0.9457 time: 0.17s
Test loss: 0.2484 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 247/1000, LR 0.000247
Train loss: 0.5722;  Loss pred: 0.5722; Loss self: 0.0000; time: 0.26s
Val loss: 0.1590 score: 0.9457 time: 0.17s
Test loss: 0.2486 score: 0.8682 time: 0.17s
Epoch 248/1000, LR 0.000247
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.30s
Val loss: 0.1592 score: 0.9457 time: 0.20s
Test loss: 0.2483 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 249/1000, LR 0.000246
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.25s
Val loss: 0.1590 score: 0.9457 time: 0.17s
Test loss: 0.2483 score: 0.8682 time: 0.16s
Epoch 250/1000, LR 0.000246
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2483 score: 0.8682 time: 0.16s
Epoch 251/1000, LR 0.000246
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2481 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5679;  Loss pred: 0.5679; Loss self: 0.0000; time: 0.24s
Val loss: 0.1576 score: 0.9457 time: 0.17s
Test loss: 0.2486 score: 0.8682 time: 0.17s
Epoch 253/1000, LR 0.000245
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.24s
Val loss: 0.1570 score: 0.9457 time: 0.17s
Test loss: 0.2490 score: 0.8682 time: 0.16s
Epoch 254/1000, LR 0.000245
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.28s
Val loss: 0.1571 score: 0.9457 time: 0.17s
Test loss: 0.2487 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.33s
Val loss: 0.1581 score: 0.9457 time: 0.17s
Test loss: 0.2480 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5680;  Loss pred: 0.5680; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9457 time: 0.17s
Test loss: 0.2475 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.26s
Val loss: 0.1593 score: 0.9457 time: 0.24s
Test loss: 0.2472 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 4 of 20
Epoch 258/1000, LR 0.000243
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.36s
Val loss: 0.1584 score: 0.9457 time: 0.25s
Test loss: 0.2475 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 259/1000, LR 0.000243
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.36s
Val loss: 0.1577 score: 0.9457 time: 0.25s
Test loss: 0.2478 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 6 of 20
Epoch 260/1000, LR 0.000243
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.29s
Val loss: 0.1569 score: 0.9457 time: 0.17s
Test loss: 0.2482 score: 0.8682 time: 0.16s
Epoch 261/1000, LR 0.000242
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 0.25s
Val loss: 0.1564 score: 0.9457 time: 0.17s
Test loss: 0.2484 score: 0.8682 time: 0.16s
Epoch 262/1000, LR 0.000242
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.25s
Val loss: 0.1551 score: 0.9457 time: 0.17s
Test loss: 0.2493 score: 0.8682 time: 0.18s
Epoch 263/1000, LR 0.000242
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.34s
Val loss: 0.1543 score: 0.9457 time: 0.18s
Test loss: 0.2499 score: 0.8682 time: 0.17s
Epoch 264/1000, LR 0.000241
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.24s
Val loss: 0.1538 score: 0.9457 time: 0.16s
Test loss: 0.2501 score: 0.8682 time: 0.15s
Epoch 265/1000, LR 0.000241
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.24s
Val loss: 0.1541 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.24s
Val loss: 0.1546 score: 0.9457 time: 0.16s
Test loss: 0.2493 score: 0.8682 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.24s
Val loss: 0.1556 score: 0.9457 time: 0.16s
Test loss: 0.2486 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 268/1000, LR 0.000240
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.24s
Val loss: 0.1566 score: 0.9457 time: 0.16s
Test loss: 0.2480 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 269/1000, LR 0.000240
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.27s
Val loss: 0.1582 score: 0.9457 time: 0.16s
Test loss: 0.2472 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 270/1000, LR 0.000240
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.32s
Val loss: 0.1583 score: 0.9457 time: 0.16s
Test loss: 0.2472 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 271/1000, LR 0.000239
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.24s
Val loss: 0.1566 score: 0.9457 time: 0.16s
Test loss: 0.2479 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 272/1000, LR 0.000239
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 0.24s
Val loss: 0.1556 score: 0.9457 time: 0.16s
Test loss: 0.2484 score: 0.8682 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 273/1000, LR 0.000239
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.24s
Val loss: 0.1552 score: 0.9457 time: 0.16s
Test loss: 0.2486 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 274/1000, LR 0.000238
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.24s
Val loss: 0.1554 score: 0.9457 time: 0.16s
Test loss: 0.2485 score: 0.8682 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 275/1000, LR 0.000238
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.28s
Val loss: 0.1549 score: 0.9457 time: 0.25s
Test loss: 0.2488 score: 0.8682 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 276/1000, LR 0.000238
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.23s
Val loss: 0.1537 score: 0.9457 time: 0.16s
Test loss: 0.2496 score: 0.8682 time: 0.16s
Epoch 277/1000, LR 0.000237
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.24s
Val loss: 0.1536 score: 0.9457 time: 0.16s
Test loss: 0.2496 score: 0.8682 time: 0.20s
Epoch 278/1000, LR 0.000237
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.23s
Val loss: 0.1530 score: 0.9457 time: 0.16s
Test loss: 0.2501 score: 0.8682 time: 0.15s
Epoch 279/1000, LR 0.000236
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 0.23s
Val loss: 0.1529 score: 0.9457 time: 0.16s
Test loss: 0.2503 score: 0.8682 time: 0.16s
Epoch 280/1000, LR 0.000236
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.24s
Val loss: 0.1534 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8682 time: 0.25s
     INFO: Early stopping counter 1 of 20
Epoch 281/1000, LR 0.000236
Train loss: 0.5552;  Loss pred: 0.5552; Loss self: 0.0000; time: 0.24s
Val loss: 0.1536 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 282/1000, LR 0.000235
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.27s
Val loss: 0.1534 score: 0.9457 time: 0.17s
Test loss: 0.2500 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 283/1000, LR 0.000235
Train loss: 0.5548;  Loss pred: 0.5548; Loss self: 0.0000; time: 0.35s
Val loss: 0.1548 score: 0.9457 time: 0.19s
Test loss: 0.2491 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 284/1000, LR 0.000235
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.24s
Val loss: 0.1541 score: 0.9457 time: 0.17s
Test loss: 0.2495 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 285/1000, LR 0.000234
Train loss: 0.5538;  Loss pred: 0.5538; Loss self: 0.0000; time: 0.25s
Val loss: 0.1538 score: 0.9457 time: 0.16s
Test loss: 0.2498 score: 0.8682 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 286/1000, LR 0.000234
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.27s
Val loss: 0.1529 score: 0.9457 time: 0.17s
Test loss: 0.2505 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 7 of 20
Epoch 287/1000, LR 0.000234
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.37s
Val loss: 0.1531 score: 0.9457 time: 0.16s
Test loss: 0.2504 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 288/1000, LR 0.000233
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 0.24s
Val loss: 0.1529 score: 0.9457 time: 0.16s
Test loss: 0.2505 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 289/1000, LR 0.000233
Train loss: 0.5495;  Loss pred: 0.5495; Loss self: 0.0000; time: 0.24s
Val loss: 0.1532 score: 0.9457 time: 0.16s
Test loss: 0.2504 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 290/1000, LR 0.000233
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.24s
Val loss: 0.1526 score: 0.9457 time: 0.16s
Test loss: 0.2509 score: 0.8682 time: 0.16s
Epoch 291/1000, LR 0.000232
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.24s
Val loss: 0.1522 score: 0.9457 time: 0.16s
Test loss: 0.2512 score: 0.8682 time: 0.16s
Epoch 292/1000, LR 0.000232
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.24s
Val loss: 0.1512 score: 0.9457 time: 0.16s
Test loss: 0.2521 score: 0.8682 time: 0.16s
Epoch 293/1000, LR 0.000232
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.27s
Val loss: 0.1516 score: 0.9457 time: 0.16s
Test loss: 0.2518 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 294/1000, LR 0.000231
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.32s
Val loss: 0.1513 score: 0.9457 time: 0.16s
Test loss: 0.2521 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 295/1000, LR 0.000231
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.24s
Val loss: 0.1519 score: 0.9457 time: 0.17s
Test loss: 0.2517 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 296/1000, LR 0.000231
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.24s
Val loss: 0.1525 score: 0.9457 time: 0.24s
Test loss: 0.2514 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 4 of 20
Epoch 297/1000, LR 0.000230
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.36s
Val loss: 0.1525 score: 0.9457 time: 0.24s
Test loss: 0.2515 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 298/1000, LR 0.000230
Train loss: 0.5534;  Loss pred: 0.5534; Loss self: 0.0000; time: 0.37s
Val loss: 0.1523 score: 0.9457 time: 0.24s
Test loss: 0.2517 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 299/1000, LR 0.000230
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.32s
Val loss: 0.1528 score: 0.9457 time: 0.16s
Test loss: 0.2514 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 300/1000, LR 0.000229
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.24s
Val loss: 0.1528 score: 0.9457 time: 0.18s
Test loss: 0.2515 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 301/1000, LR 0.000229
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.26s
Val loss: 0.1530 score: 0.9457 time: 0.17s
Test loss: 0.2515 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 302/1000, LR 0.000228
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.25s
Val loss: 0.1527 score: 0.9457 time: 0.17s
Test loss: 0.2518 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 303/1000, LR 0.000228
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.25s
Val loss: 0.1522 score: 0.9457 time: 0.17s
Test loss: 0.2522 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 304/1000, LR 0.000228
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.27s
Val loss: 0.1514 score: 0.9457 time: 0.17s
Test loss: 0.2530 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 12 of 20
Epoch 305/1000, LR 0.000227
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.27s
Val loss: 0.1513 score: 0.9457 time: 0.17s
Test loss: 0.2531 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 306/1000, LR 0.000227
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.25s
Val loss: 0.1511 score: 0.9457 time: 0.17s
Test loss: 0.2534 score: 0.8682 time: 0.17s
Epoch 307/1000, LR 0.000227
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.25s
Val loss: 0.1511 score: 0.9457 time: 0.17s
Test loss: 0.2535 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 308/1000, LR 0.000226
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.25s
Val loss: 0.1504 score: 0.9457 time: 0.17s
Test loss: 0.2542 score: 0.8682 time: 0.17s
Epoch 309/1000, LR 0.000226
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.25s
Val loss: 0.1506 score: 0.9457 time: 0.17s
Test loss: 0.2541 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 310/1000, LR 0.000226
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.25s
Val loss: 0.1505 score: 0.9457 time: 0.18s
Test loss: 0.2544 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 311/1000, LR 0.000225
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 0.28s
Val loss: 0.1513 score: 0.9457 time: 0.21s
Test loss: 0.2538 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 312/1000, LR 0.000225
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.25s
Val loss: 0.1514 score: 0.9457 time: 0.17s
Test loss: 0.2539 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 313/1000, LR 0.000224
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.26s
Val loss: 0.1507 score: 0.9457 time: 0.17s
Test loss: 0.2546 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 314/1000, LR 0.000224
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 0.25s
Val loss: 0.1499 score: 0.9457 time: 0.17s
Test loss: 0.2554 score: 0.8682 time: 0.17s
Epoch 315/1000, LR 0.000224
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 0.26s
Val loss: 0.1496 score: 0.9457 time: 0.18s
Test loss: 0.2558 score: 0.8682 time: 0.17s
Epoch 316/1000, LR 0.000223
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.24s
Val loss: 0.1483 score: 0.9457 time: 0.17s
Test loss: 0.2575 score: 0.8760 time: 0.17s
Epoch 317/1000, LR 0.000223
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.25s
Val loss: 0.1486 score: 0.9457 time: 0.17s
Test loss: 0.2572 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 318/1000, LR 0.000223
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.25s
Val loss: 0.1486 score: 0.9457 time: 0.17s
Test loss: 0.2572 score: 0.8760 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 319/1000, LR 0.000222
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.26s
Val loss: 0.1490 score: 0.9457 time: 0.18s
Test loss: 0.2569 score: 0.8760 time: 0.25s
     INFO: Early stopping counter 3 of 20
Epoch 320/1000, LR 0.000222
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.26s
Val loss: 0.1500 score: 0.9457 time: 0.17s
Test loss: 0.2560 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 321/1000, LR 0.000221
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 0.25s
Val loss: 0.1502 score: 0.9457 time: 0.17s
Test loss: 0.2559 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 322/1000, LR 0.000221
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.36s
Val loss: 0.1502 score: 0.9535 time: 0.25s
Test loss: 0.2560 score: 0.8682 time: 0.25s
     INFO: Early stopping counter 6 of 20
Epoch 323/1000, LR 0.000221
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.38s
Val loss: 0.1502 score: 0.9535 time: 0.25s
Test loss: 0.2561 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 7 of 20
Epoch 324/1000, LR 0.000220
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.35s
Val loss: 0.1500 score: 0.9535 time: 0.19s
Test loss: 0.2564 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 325/1000, LR 0.000220
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.25s
Val loss: 0.1502 score: 0.9535 time: 0.16s
Test loss: 0.2564 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 326/1000, LR 0.000220
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.24s
Val loss: 0.1501 score: 0.9535 time: 0.17s
Test loss: 0.2565 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 327/1000, LR 0.000219
Train loss: 0.5372;  Loss pred: 0.5372; Loss self: 0.0000; time: 0.24s
Val loss: 0.1499 score: 0.9535 time: 0.16s
Test loss: 0.2568 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 328/1000, LR 0.000219
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.23s
Val loss: 0.1484 score: 0.9457 time: 0.16s
Test loss: 0.2586 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 329/1000, LR 0.000218
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.24s
Val loss: 0.1473 score: 0.9457 time: 0.16s
Test loss: 0.2602 score: 0.8760 time: 0.18s
Epoch 330/1000, LR 0.000218
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.28s
Val loss: 0.1474 score: 0.9457 time: 0.24s
Test loss: 0.2600 score: 0.8760 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 331/1000, LR 0.000218
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.33s
Val loss: 0.1476 score: 0.9457 time: 0.16s
Test loss: 0.2600 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 332/1000, LR 0.000217
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.24s
Val loss: 0.1478 score: 0.9457 time: 0.16s
Test loss: 0.2598 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 333/1000, LR 0.000217
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.24s
Val loss: 0.1476 score: 0.9457 time: 0.16s
Test loss: 0.2602 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 334/1000, LR 0.000216
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.25s
Val loss: 0.1477 score: 0.9457 time: 0.17s
Test loss: 0.2602 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 335/1000, LR 0.000216
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.25s
Val loss: 0.1474 score: 0.9457 time: 0.16s
Test loss: 0.2607 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 336/1000, LR 0.000216
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.23s
Val loss: 0.1478 score: 0.9457 time: 0.16s
Test loss: 0.2603 score: 0.8760 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 337/1000, LR 0.000215
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.24s
Val loss: 0.1482 score: 0.9535 time: 0.21s
Test loss: 0.2598 score: 0.8760 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 338/1000, LR 0.000215
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.24s
Val loss: 0.1479 score: 0.9535 time: 0.16s
Test loss: 0.2603 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 339/1000, LR 0.000215
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.23s
Val loss: 0.1485 score: 0.9535 time: 0.16s
Test loss: 0.2597 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 340/1000, LR 0.000214
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.23s
Val loss: 0.1486 score: 0.9535 time: 0.16s
Test loss: 0.2597 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 341/1000, LR 0.000214
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.23s
Val loss: 0.1480 score: 0.9535 time: 0.16s
Test loss: 0.2605 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 342/1000, LR 0.000213
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.23s
Val loss: 0.1476 score: 0.9535 time: 0.16s
Test loss: 0.2611 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 343/1000, LR 0.000213
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.24s
Val loss: 0.1476 score: 0.9535 time: 0.17s
Test loss: 0.2612 score: 0.8760 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 344/1000, LR 0.000213
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.24s
Val loss: 0.1478 score: 0.9535 time: 0.23s
Test loss: 0.2612 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 345/1000, LR 0.000212
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.24s
Val loss: 0.1471 score: 0.9535 time: 0.17s
Test loss: 0.2621 score: 0.8760 time: 0.16s
Epoch 346/1000, LR 0.000212
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.24s
Val loss: 0.1472 score: 0.9535 time: 0.17s
Test loss: 0.2622 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 347/1000, LR 0.000211
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.24s
Val loss: 0.1467 score: 0.9535 time: 0.17s
Test loss: 0.2630 score: 0.8760 time: 0.16s
Epoch 348/1000, LR 0.000211
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 0.24s
Val loss: 0.1467 score: 0.9535 time: 0.17s
Test loss: 0.2632 score: 0.8760 time: 0.17s
Epoch 349/1000, LR 0.000211
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.24s
Val loss: 0.1467 score: 0.9535 time: 0.17s
Test loss: 0.2633 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 350/1000, LR 0.000210
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.24s
Val loss: 0.1476 score: 0.9535 time: 0.19s
Test loss: 0.2621 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 351/1000, LR 0.000210
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.31s
Val loss: 0.1481 score: 0.9535 time: 0.17s
Test loss: 0.2617 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 352/1000, LR 0.000209
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.24s
Val loss: 0.1481 score: 0.9535 time: 0.16s
Test loss: 0.2618 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 353/1000, LR 0.000209
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.23s
Val loss: 0.1481 score: 0.9535 time: 0.16s
Test loss: 0.2620 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 354/1000, LR 0.000209
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.36s
Val loss: 0.1481 score: 0.9535 time: 0.24s
Test loss: 0.2622 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 355/1000, LR 0.000208
Train loss: 0.5296;  Loss pred: 0.5296; Loss self: 0.0000; time: 0.36s
Val loss: 0.1477 score: 0.9535 time: 0.25s
Test loss: 0.2627 score: 0.8760 time: 0.23s
     INFO: Early stopping counter 7 of 20
Epoch 356/1000, LR 0.000208
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.36s
Val loss: 0.1471 score: 0.9535 time: 0.25s
Test loss: 0.2637 score: 0.8760 time: 0.25s
     INFO: Early stopping counter 8 of 20
Epoch 357/1000, LR 0.000207
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.25s
Val loss: 0.1472 score: 0.9535 time: 0.18s
Test loss: 0.2637 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 358/1000, LR 0.000207
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.25s
Val loss: 0.1470 score: 0.9535 time: 0.17s
Test loss: 0.2641 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 359/1000, LR 0.000207
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.25s
Val loss: 0.1472 score: 0.9535 time: 0.17s
Test loss: 0.2639 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 360/1000, LR 0.000206
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.25s
Val loss: 0.1471 score: 0.9535 time: 0.17s
Test loss: 0.2642 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 361/1000, LR 0.000206
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.25s
Val loss: 0.1469 score: 0.9535 time: 0.17s
Test loss: 0.2646 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 362/1000, LR 0.000205
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.27s
Val loss: 0.1464 score: 0.9535 time: 0.18s
Test loss: 0.2655 score: 0.8760 time: 0.25s
Epoch 363/1000, LR 0.000205
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.37s
Val loss: 0.1463 score: 0.9535 time: 0.17s
Test loss: 0.2659 score: 0.8760 time: 0.17s
Epoch 364/1000, LR 0.000205
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.24s
Val loss: 0.1458 score: 0.9535 time: 0.17s
Test loss: 0.2667 score: 0.8760 time: 0.17s
Epoch 365/1000, LR 0.000204
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.25s
Val loss: 0.1454 score: 0.9457 time: 0.17s
Test loss: 0.2676 score: 0.8682 time: 0.17s
Epoch 366/1000, LR 0.000204
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.24s
Val loss: 0.1457 score: 0.9535 time: 0.17s
Test loss: 0.2672 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 367/1000, LR 0.000203
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.25s
Val loss: 0.1461 score: 0.9535 time: 0.17s
Test loss: 0.2667 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 368/1000, LR 0.000203
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.24s
Val loss: 0.1465 score: 0.9535 time: 0.25s
Test loss: 0.2661 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 369/1000, LR 0.000203
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.23s
Val loss: 0.1470 score: 0.9535 time: 0.16s
Test loss: 0.2655 score: 0.8760 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 370/1000, LR 0.000202
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.25s
Val loss: 0.1475 score: 0.9535 time: 0.16s
Test loss: 0.2650 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 371/1000, LR 0.000202
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.29s
Val loss: 0.1480 score: 0.9535 time: 0.16s
Test loss: 0.2646 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 372/1000, LR 0.000201
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.23s
Val loss: 0.1479 score: 0.9535 time: 0.16s
Test loss: 0.2647 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 373/1000, LR 0.000201
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.25s
Val loss: 0.1476 score: 0.9535 time: 0.16s
Test loss: 0.2653 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 374/1000, LR 0.000200
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.34s
Val loss: 0.1462 score: 0.9535 time: 0.23s
Test loss: 0.2676 score: 0.8760 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 375/1000, LR 0.000200
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.25s
Val loss: 0.1459 score: 0.9535 time: 0.17s
Test loss: 0.2684 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 376/1000, LR 0.000200
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.27s
Val loss: 0.1455 score: 0.9535 time: 0.22s
Test loss: 0.2693 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 377/1000, LR 0.000199
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.24s
Val loss: 0.1455 score: 0.9535 time: 0.17s
Test loss: 0.2694 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 378/1000, LR 0.000199
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.25s
Val loss: 0.1454 score: 0.9535 time: 0.17s
Test loss: 0.2699 score: 0.8682 time: 0.16s
Epoch 379/1000, LR 0.000198
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.30s
Val loss: 0.1453 score: 0.9535 time: 0.16s
Test loss: 0.2701 score: 0.8682 time: 0.16s
Epoch 380/1000, LR 0.000198
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.24s
Val loss: 0.1453 score: 0.9535 time: 0.22s
Test loss: 0.2703 score: 0.8682 time: 0.19s
Epoch 381/1000, LR 0.000198
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.25s
Val loss: 0.1462 score: 0.9535 time: 0.16s
Test loss: 0.2687 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 382/1000, LR 0.000197
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.24s
Val loss: 0.1464 score: 0.9535 time: 0.16s
Test loss: 0.2685 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 383/1000, LR 0.000197
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.24s
Val loss: 0.1466 score: 0.9535 time: 0.17s
Test loss: 0.2683 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 384/1000, LR 0.000196
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.25s
Val loss: 0.1471 score: 0.9535 time: 0.17s
Test loss: 0.2677 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 385/1000, LR 0.000196
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.25s
Val loss: 0.1472 score: 0.9535 time: 0.17s
Test loss: 0.2677 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 386/1000, LR 0.000195
Train loss: 0.5192;  Loss pred: 0.5192; Loss self: 0.0000; time: 0.25s
Val loss: 0.1470 score: 0.9535 time: 0.18s
Test loss: 0.2681 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 387/1000, LR 0.000195
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.33s
Val loss: 0.1465 score: 0.9535 time: 0.17s
Test loss: 0.2691 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 388/1000, LR 0.000195
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.25s
Val loss: 0.1460 score: 0.9535 time: 0.17s
Test loss: 0.2701 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 389/1000, LR 0.000194
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.25s
Val loss: 0.1457 score: 0.9535 time: 0.17s
Test loss: 0.2708 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 390/1000, LR 0.000194
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.25s
Val loss: 0.1457 score: 0.9535 time: 0.23s
Test loss: 0.2712 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 10 of 20
Epoch 391/1000, LR 0.000193
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 0.37s
Val loss: 0.1457 score: 0.9535 time: 0.25s
Test loss: 0.2712 score: 0.8682 time: 0.25s
     INFO: Early stopping counter 11 of 20
Epoch 392/1000, LR 0.000193
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.27s
Val loss: 0.1464 score: 0.9535 time: 0.17s
Test loss: 0.2700 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 12 of 20
Epoch 393/1000, LR 0.000193
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.25s
Val loss: 0.1463 score: 0.9535 time: 0.17s
Test loss: 0.2704 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 394/1000, LR 0.000192
Train loss: 0.5244;  Loss pred: 0.5244; Loss self: 0.0000; time: 0.25s
Val loss: 0.1466 score: 0.9535 time: 0.21s
Test loss: 0.2701 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 395/1000, LR 0.000192
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.24s
Val loss: 0.1465 score: 0.9535 time: 0.17s
Test loss: 0.2704 score: 0.8682 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 396/1000, LR 0.000191
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.25s
Val loss: 0.1458 score: 0.9535 time: 0.17s
Test loss: 0.2719 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 397/1000, LR 0.000191
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.25s
Val loss: 0.1460 score: 0.9535 time: 0.18s
Test loss: 0.2716 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 398/1000, LR 0.000190
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.27s
Val loss: 0.1458 score: 0.9535 time: 0.25s
Test loss: 0.2721 score: 0.8682 time: 0.24s
     INFO: Early stopping counter 18 of 20
Epoch 399/1000, LR 0.000190
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.37s
Val loss: 0.1464 score: 0.9535 time: 0.25s
Test loss: 0.2712 score: 0.8682 time: 0.23s
     INFO: Early stopping counter 19 of 20
Epoch 400/1000, LR 0.000190
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.26s
Val loss: 0.1462 score: 0.9535 time: 0.17s
Test loss: 0.2717 score: 0.8682 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 379,   Train_Loss: 0.5237,   Val_Loss: 0.1453,   Val_Precision: 0.9531,   Val_Recall: 0.9531,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.1453,   Test_Precision: 0.9286,   Test_Recall: 0.8000,   Test_accuracy: 0.8595,   Test_Score: 0.8682,   Test_loss: 0.2703


[0.16673445515334606, 0.16456928104162216, 0.16310806293040514, 0.16728525212965906, 0.1854744099546224, 0.16258475300855935, 0.16449093399569392, 0.1610368660185486, 0.16231747902929783, 0.1652295389212668, 0.16330186813138425, 0.16176578402519226, 0.16232501482591033, 0.16171461204066873, 0.16289166919887066, 0.16123372013680637, 0.22038643597625196, 0.16740042599849403, 0.16435677302069962, 0.16419110400602221, 0.16313734301365912, 0.2006287630647421, 0.16585547616705298, 0.16315406118519604, 0.16169284307397902, 0.16263928497210145, 0.1612458899617195, 0.25024853996001184, 0.16360940993763506, 0.1635465060826391, 0.17455954803153872, 0.17379526305012405, 0.1774173858575523, 0.1780657118652016, 0.17183967703022063, 0.1667702291160822, 0.17278969404287636, 0.24313197215087712, 0.2431589609477669, 0.16836075694300234, 0.16478207614272833, 0.16209386009722948, 0.16404796205461025, 0.16202021902427077, 0.16177891404367983, 0.16372482618317008, 0.24734427803196013, 0.17053176905028522, 0.17116316105239093, 0.17020678101107478, 0.17008514003828168, 0.18675795919261873, 0.16963822301477194, 0.17098320205695927, 0.16924341884441674, 0.24730027792975307, 0.24726353911682963, 0.24728002795018256, 0.17665820382535458, 0.16417790786363184, 0.15995282703079283, 0.16107823699712753, 0.16283482708968222, 0.16100223013199866, 0.24648205284029245, 0.16518020280636847, 0.16574255004525185, 0.16364169493317604, 0.1634306670166552, 0.18196568288840353, 0.25050408602692187, 0.16633846308104694, 0.16594198206439614, 0.2410559670533985, 0.238903874065727, 0.23480591690167785, 0.23885572399012744, 0.1622869479469955, 0.16099695907905698, 0.15951298992149532, 0.1608885859604925, 0.23039979697205126, 0.17414131900295615, 0.17543383804149926, 0.1734831149224192, 0.1755000320263207, 0.1769772320985794, 0.19987277686595917, 0.17824881384149194, 0.2537408887874335, 0.1587942228652537, 0.17284859088249505, 0.170234713004902, 0.20563827897422016, 0.17523267911747098, 0.1744306841865182, 0.17912659794092178, 0.1765528260730207, 0.1783474951516837, 0.17473685811273754, 0.2670305559877306, 0.1796403299085796, 0.17467937106266618, 0.17199443420395255, 0.20603811112232506, 0.17141464701853693, 0.17427313211373985, 0.17369821411557496, 0.17368879891000688, 0.1708255368284881, 0.17318927706219256, 0.16062364005483687, 0.16172748617827892, 0.16984039498493075, 0.163166674785316, 0.16084606805816293, 0.16557894297875464, 0.24426717194728553, 0.16072220914065838, 0.1615866729989648, 0.16133025102317333, 0.16248368099331856, 0.1627005022019148, 0.24945938400924206, 0.16117874020710588, 0.16261339001357555, 0.23891933006234467, 0.24097934109158814, 0.24107169592753053, 0.2410680609755218, 0.16411066986620426, 0.16325514111667871, 0.16464215703308582, 0.16869658906944096, 0.16394772892817855, 0.16140951309353113, 0.16321560298092663, 0.17311541503295302, 0.24683682108297944, 0.16691843792796135, 0.16391977085731924, 0.166302930098027, 0.16155808698385954, 0.19306765589863062, 0.2210617740638554, 0.16319593507796526, 0.16402410087175667, 0.16452863113954663, 0.16444694600068033, 0.16804808005690575, 0.16577738290652633, 0.16188615886494517, 0.1622601628769189, 0.16385134286247194, 0.16344994003884494, 0.16524494998157024, 0.1916167119052261, 0.19729590509086847, 0.163780887844041, 0.1637106619309634, 0.15956483595073223, 0.15940011502243578, 0.15896479110233486, 0.19179784203879535, 0.17009773710742593, 0.1625336620490998, 0.17425616993568838, 0.17220639204606414, 0.1706231569405645, 0.28065608092583716, 0.2630470939911902, 0.17132195108570158, 0.17073519085533917, 0.1725540030747652, 0.17394615686498582, 0.1739957311656326, 0.17670407705008984, 0.1814715799409896, 0.1613593699876219, 0.16336485999636352, 0.2060081190429628, 0.16762209497392178, 0.17296730307862163, 0.24097197782248259, 0.15683247707784176, 0.16881524794735014, 0.1695977149065584, 0.1692938890773803, 0.17357272910885513, 0.1701673110947013, 0.17574759596027434, 0.24892814899794757, 0.2368547220248729, 0.2509191329590976, 0.15953431907109916, 0.17361875507049263, 0.17254383605904877, 0.1742519869003445, 0.17292288807220757, 0.2602621139958501, 0.17514411895535886, 0.17415130510926247, 0.17506522499024868, 0.1746231650467962, 0.248841772088781, 0.25150099699385464, 0.17336963396519423, 0.17390950908884406, 0.1745272008702159, 0.1754064450506121, 0.23269196599721909, 0.17278197989799082, 0.17422183020971715, 0.17253445112146437, 0.23262266186065972, 0.1604927929583937, 0.1731957730371505, 0.17134184786118567, 0.17323128902353346, 0.17126029008068144, 0.17884608497843146, 0.17051923903636634, 0.16904838802292943, 0.17149172793142498, 0.17118925508111715, 0.17215807596221566, 0.18798734992742538, 0.23187255300581455, 0.1581243530381471, 0.16271990607492626, 0.1616752331610769, 0.1589968439657241, 0.15896394406445324, 0.16094659105874598, 0.16214475990273058, 0.15829796390607953, 0.1598516891244799, 0.16099763801321387, 0.161884669912979, 0.16178100206889212, 0.16185220913030207, 0.16887550405226648, 0.16910325293429196, 0.16943069407716393, 0.16744819493032992, 0.169359324965626, 0.1697199430782348, 0.16995688807219267, 0.1671690831426531, 0.1681958541739732, 0.16799198207445443, 0.1701674209907651, 0.16828978690318763, 0.17307977005839348, 0.17272892384789884, 0.16932283085770905, 0.24311852105893195, 0.24518223712220788, 0.2457512009423226, 0.16916287899948657, 0.16700354497879744, 0.18392588989809155, 0.17081045894883573, 0.15663270303048193, 0.16073436895385385, 0.1584344170987606, 0.15969394287094474, 0.15999513887800276, 0.23707106802612543, 0.15953163197264075, 0.16383772296831012, 0.1586746519897133, 0.1619646770413965, 0.15816990099847317, 0.19805173994973302, 0.16098128212615848, 0.20877147209830582, 0.15866114292293787, 0.16143442783504725, 0.2575265569612384, 0.16189939505420625, 0.16271735378541052, 0.16220943885855377, 0.1684341449290514, 0.15835127118043602, 0.23612410598434508, 0.16059105680324137, 0.15998438000679016, 0.160298191010952, 0.16003166395239532, 0.16142815491184592, 0.1600618369411677, 0.1641201509628445, 0.16021832101978362, 0.1623217931482941, 0.2347525330260396, 0.23891248414292932, 0.23895561299286783, 0.16086901794187725, 0.17375208181329072, 0.17311525694094598, 0.17208570684306324, 0.17242141184397042, 0.230367325944826, 0.17220130492933095, 0.17579775606282055, 0.16982773807831109, 0.17308843811042607, 0.17223581206053495, 0.17443761392496526, 0.17192080500535667, 0.17597970296628773, 0.17497563315555453, 0.17042878991924226, 0.17136186105199158, 0.17018930078484118, 0.17208680184558034, 0.19066068599931896, 0.2576312171295285, 0.17113644001074135, 0.1758872801437974, 0.24945866386406124, 0.243104028981179, 0.1631897590123117, 0.1616576388478279, 0.16096236393786967, 0.15849255095236003, 0.16133472113870084, 0.1804954840335995, 0.24108296702615917, 0.1612322621513158, 0.16472438303753734, 0.16256156493909657, 0.1717379419133067, 0.15852960897609591, 0.1831769437994808, 0.22162948013283312, 0.15927423513494432, 0.1558688881341368, 0.15978833101689816, 0.15823188005015254, 0.1579428520053625, 0.19008617103099823, 0.1665216840337962, 0.169184128055349, 0.1677500088699162, 0.16859405091963708, 0.17007544194348156, 0.17211860581301153, 0.1743805839214474, 0.16388750518672168, 0.16252768598496914, 0.1723189519252628, 0.2389598300214857, 0.23892405210062861, 0.25364511902444065, 0.17177730496041477, 0.17345076194033027, 0.17354851216077805, 0.1717924859840423, 0.1751652779057622, 0.2507676729001105, 0.17339131888002157, 0.17571793403476477, 0.1716889429371804, 0.17099507385864854, 0.1774403378367424, 0.1647659360896796, 0.1593880879227072, 0.15944395097903907, 0.15966833592392504, 0.16064104321412742, 0.1612316770479083, 0.19079579995013773, 0.17607789696194232, 0.16891566989943385, 0.1750107710249722, 0.16356977191753685, 0.16740774596109986, 0.18981378502212465, 0.16107580298557878, 0.1601741500198841, 0.17065494507551193, 0.16975813498720527, 0.17093553021550179, 0.1761766979470849, 0.16950825601816177, 0.1713663088157773, 0.169889273121953, 0.24730916414409876, 0.256103539140895, 0.17464125994592905, 0.1708532760385424, 0.16210092208348215, 0.16917612100951374, 0.17337071802467108, 0.1767740671057254, 0.24940966512076557, 0.23081090115010738, 0.17405863315798342]
[0.0012925151562274888, 0.0012757308607877686, 0.0012644035886077917, 0.0012967849002299152, 0.0014377861236792434, 0.0012603469225469716, 0.0012751235193464645, 0.0012483477985934, 0.0012582750312348668, 0.0012808491389245489, 0.0012659059545068547, 0.0012539983257766841, 0.0012583334482628708, 0.001253601643726114, 0.0012627261178207029, 0.0012498737995101268, 0.0017084219843120307, 0.0012976777209185583, 0.0012740835117883691, 0.0012727992558606374, 0.0012646305659973576, 0.0015552617291840472, 0.0012857013656360696, 0.0012647601642263259, 0.001253432892046349, 0.001260769650946523, 0.0012499681392381357, 0.001939911162480712, 0.0012682899995165508, 0.0012678023727336364, 0.0013531747909421607, 0.0013472501011637522, 0.001375328572539165, 0.0013803543555441983, 0.0013320905196141134, 0.001292792473768079, 0.001339454992580437, 0.0018847439701618382, 0.0018849531856416038, 0.0013051221468449795, 0.0012773804352149483, 0.0012565415511413137, 0.001271689628330312, 0.0012559706901106261, 0.0012541001088657352, 0.0012691846990943417, 0.0019173975041237219, 0.0013219516980642265, 0.0013268462097084568, 0.0013194324109385641, 0.0013184894576610982, 0.0014477361177722382, 0.0013150249846106351, 0.0013254511787361184, 0.0013119644871660212, 0.0019170564180601012, 0.001916771621060695, 0.0019168994414742835, 0.0013694434405066246, 0.0012726969601831925, 0.0012399443955875414, 0.001248668503853702, 0.001262285481315366, 0.0012480793033488268, 0.0019107135879092437, 0.001280466688421461, 0.0012848259693430376, 0.0012685402707998143, 0.0012669043954779474, 0.0014105866890573918, 0.0019418921397435804, 0.001289445450240674, 0.001286371953987567, 0.001868650907390686, 0.0018519680160133875, 0.0018202009062145569, 0.001851594759613391, 0.0012580383561782596, 0.0012480384424733099, 0.001236534805592987, 0.0012471983407790115, 0.0017860449377678392, 0.0013499327054492724, 0.001359952232879839, 0.0013448303482358076, 0.0013604653645451217, 0.0013719165278959643, 0.001549401371053947, 0.0013817737507092398, 0.0019669836340111125, 0.001230962967947703, 0.0013399115572286437, 0.0013196489380224961, 0.001594095185846668, 0.001358392861375744, 0.0013521758464071177, 0.0013885782786117967, 0.0013686265587055867, 0.001382538722106075, 0.00135454928769564, 0.002070004309982408, 0.0013925606969657333, 0.0013541036516485751, 0.0013332901876275392, 0.001597194659862985, 0.0013287957133219918, 0.0013509545125096113, 0.0013464977838416664, 0.0013464247977519913, 0.0013242289676626984, 0.0013425525353658338, 0.0012451444965491231, 0.0012537014432424723, 0.0013165922091855097, 0.001264857944072217, 0.001246868744636922, 0.0012835576975097258, 0.0018935439685836089, 0.0012459085979896, 0.0012526098682090294, 0.001250622100954832, 0.001259563418552857, 0.0012612442031156184, 0.0019337936744902484, 0.0012494475985046966, 0.001260568914833919, 0.0018520878299406564, 0.0018680569076867297, 0.0018687728366475235, 0.0018687446587249753, 0.0012721757353969323, 0.0012655437295866568, 0.0012762957909541537, 0.001307725496662333, 0.0012709126273502214, 0.0012512365356087685, 0.001265237232410284, 0.0013419799614957598, 0.0019134637293254221, 0.0012939413792865222, 0.0012706958981187537, 0.0012891700007598992, 0.0012523882711927097, 0.0014966484953382218, 0.0017136571632857009, 0.0012650847680462424, 0.0012715046579205944, 0.0012754157452678034, 0.0012747825271370568, 0.0013026982950147732, 0.00128509599152346, 0.0012549314640693423, 0.0012578307199761155, 0.001270165448546294, 0.001267053798750736, 0.0012809686045082965, 0.001485400867482373, 0.0015294256208594455, 0.0012696192856127208, 0.0012690748986896388, 0.001236936712796374, 0.0012356598063754712, 0.0012322852023436812, 0.0014868049770449252, 0.001318587109359891, 0.0012599508685976729, 0.0013508230227572743, 0.0013349332716749158, 0.0013226601313222055, 0.0021756285343088153, 0.0020391247596216293, 0.0013280771401992372, 0.001323528611281699, 0.0013376279308121334, 0.0013484198206588048, 0.0013488041175630434, 0.0013697990468999211, 0.0014067564336510824, 0.00125084782936141, 0.0012663942635377018, 0.0015969621631237425, 0.001299396085069161, 0.0013408318068110203, 0.0018679998280812604, 0.0012157556362623393, 0.001308645332925195, 0.0013147109682678944, 0.0013123557292820179, 0.0013455250318515901, 0.0013191264425945837, 0.0013623844648083282, 0.0019296755736275006, 0.001836083116471883, 0.0019451095578224623, 0.001236700147837978, 0.001345881822251881, 0.0013375491167368122, 0.0013507905961267015, 0.0013404875044357176, 0.0020175357674096905, 0.0013577063484911539, 0.0013500101171260656, 0.001357094767366269, 0.0013536679460991953, 0.0019290059851843487, 0.0019496201317353073, 0.0013439506508929784, 0.001348135729370884, 0.0013529240377536116, 0.001359739884113272, 0.001803813689900923, 0.0013393951930076808, 0.0013505568233311408, 0.0013374763652826695, 0.0018032764485322458, 0.0012441301779720442, 0.0013426028917608566, 0.001328231378768881, 0.0013428782094847555, 0.0013275991479122593, 0.0013864037595227245, 0.0013218545661733826, 0.0013104526203327864, 0.0013293932397784882, 0.001327048489000908, 0.0013345587283892686, 0.0014572662785071735, 0.0017974616512078647, 0.0012257701785902875, 0.0012613946207358626, 0.0012532963810936194, 0.0012325336741529, 0.0012322786361585523, 0.0012476479927034571, 0.0012569361232769812, 0.0012271159992719343, 0.0012391603808099216, 0.0012480437055287898, 0.0012549199218060388, 0.0012541162951076909, 0.001254668287831799, 0.0013091124345136936, 0.0013108779297231934, 0.0013134162331563096, 0.0012980480227157358, 0.0013128629842296589, 0.0013156584734746882, 0.0013174952563735865, 0.0012958843654469233, 0.0013038438308059937, 0.0013022634269337552, 0.001319127294502055, 0.0013045719914975785, 0.0013417036438635153, 0.0013389839057976654, 0.0013125800841682872, 0.0018846396981312553, 0.001900637497071379, 0.0019050480693203304, 0.0013113401472828416, 0.0012946011238666468, 0.001425782092233268, 0.0013241120848746955, 0.001214207000236294, 0.0012460028601073941, 0.0012281737759593846, 0.0012379375416352305, 0.001240272394403122, 0.0018377602172567863, 0.0012366793176173702, 0.0012700598679713962, 0.0012300360619357622, 0.0012555401321038488, 0.0012261232635540555, 0.0015352848058118838, 0.0012479169157066548, 0.0016183835046380296, 0.0012299313404878906, 0.001251429673139901, 0.0019963298989243284, 0.0012550340701876453, 0.001261374835545818, 0.0012574375105314246, 0.0013056910459616387, 0.0012275292339568684, 0.001830419426235233, 0.0012448919132034214, 0.001240188992300699, 0.0012426216357438139, 0.0012405555345146924, 0.001251381045828263, 0.0012407894336524629, 0.0012722492322701123, 0.0012420024885254543, 0.0012583084740177837, 0.0018197870777212372, 0.0018520347607979017, 0.0018523690929679677, 0.0012470466507122266, 0.0013469153628937265, 0.0013419787359763254, 0.0013339977274656064, 0.0013366000918137242, 0.0017857932243784962, 0.00133489383666148, 0.0013627733028125624, 0.0013164940936303184, 0.0013417708380653183, 0.0013351613338025965, 0.0013522295653098082, 0.0013327194186461757, 0.001364183743924711, 0.0013564002570198026, 0.0013211534102266843, 0.0013283865197828806, 0.0013192969053088463, 0.0013340062158572119, 0.001477989813948209, 0.0019971412180583605, 0.0013266390698507082, 0.001363467287936414, 0.0019337880919694669, 0.001884527356443248, 0.0012650368915683077, 0.0012531599910684335, 0.001247770263084261, 0.001228624425987287, 0.0012506567530131847, 0.0013991897987100737, 0.0018688602095051098, 0.0012498624972970217, 0.0012769332018413746, 0.001260167170070516, 0.0013313018752969513, 0.0012289116974891156, 0.001419976308523107, 0.001718057985525838, 0.001234683993294142, 0.0012082859545281923, 0.001238669232689133, 0.0012266037213190118, 0.0012243631938400196, 0.0014735362095426219, 0.0012908657677038464, 0.0013115048686461162, 0.0013003876656582651, 0.0013069306272840084, 0.00131841427863164, 0.0013342527582403995, 0.0013517874722592822, 0.0012704457766412533, 0.0012599045425191406, 0.0013358058288780062, 0.001852401783112292, 0.001852124434888594, 0.001966241232747602, 0.0013316070151970136, 0.0013445795499250409, 0.0013453373035719229, 0.0013317246975507154, 0.0013578703713624976, 0.0019439354488380658, 0.001344118751007919, 0.0013621545274012774, 0.0013309220382727164, 0.0013255432082065778, 0.0013755064948584683, 0.001277255318524648, 0.0012355665730442418, 0.001235999619992551, 0.0012377390381699616, 0.0012452794047606776, 0.0012498579616116922, 0.0014790372089157964, 0.0013649449376894752, 0.00130942379767003, 0.0013566726436044358, 0.0012679827280429213, 0.0012977344648147275, 0.0014714246900939896, 0.0012486496355471222, 0.0012416600776735203, 0.0013229065509729606, 0.001315954534784537, 0.0013250816295775332, 0.0013657108367991079, 0.001314017488512882, 0.001328420998571917, 0.0013169711094725039, 0.001917125303442626, 0.0019852987530301937, 0.001353808216635109, 0.0013244440002987783, 0.0012565962952207918, 0.0013114427985233624, 0.0013439590544548146, 0.0013703416054707397, 0.0019334082567501207, 0.0017892317918612975, 0.0013492917299068481]
[773.6853182586551, 783.864395490516, 790.8867145031429, 771.1379117868381, 695.5137370786663, 793.4323336777387, 784.2377501691187, 801.0588083919956, 794.7388092240878, 780.7320703198811, 789.9480972024966, 797.44923054872, 794.7019141710807, 797.7015705145958, 791.9373693844766, 800.0807764687428, 585.3354786947984, 770.6073579595342, 784.8779069406122, 785.6698496604818, 790.7447652203035, 642.9785940432291, 777.7855937061047, 790.6637386952459, 797.8089663559127, 793.166300639653, 800.0203914073418, 515.4875230065813, 788.4632066650229, 788.7664682657119, 739.0028300067138, 742.2526813219029, 727.0989783581502, 724.4516569122243, 750.6997349472053, 773.5193546457754, 746.5723040633992, 530.5760441903059, 530.517154281271, 766.2118081571247, 782.8521342834935, 795.835202657407, 786.3553949975735, 796.196923920191, 797.3845093630086, 787.9073870915516, 521.5402637425536, 756.4572907348506, 753.6668475088205, 757.9016490042568, 758.4436827988941, 690.73361348392, 760.4418255947352, 754.4600782305299, 762.2157533852942, 521.6330571073726, 521.7105621830025, 521.6757740984588, 730.2236590582015, 785.7329995162867, 806.4877776443799, 800.8530662171354, 792.2138175573, 801.2311375701975, 523.364677117426, 780.9652598091279, 778.3155258850536, 788.3076501540628, 789.3255430870488, 708.9248805177925, 514.9616600909906, 775.5271848168147, 777.3801324726838, 535.1454335557852, 539.9661286551998, 549.3899033814263, 540.0749785059868, 794.8883236262022, 801.2573699398572, 808.7115667726349, 801.797089767928, 559.8963267126854, 740.777666889098, 735.3199442030377, 743.5882163960923, 735.042600907635, 728.907320282559, 645.4105557682393, 723.7074806832289, 508.3926387129007, 812.3721233200288, 746.317915242341, 757.7772930265131, 627.3151119698492, 736.164057124996, 739.5487818075671, 720.1610563861981, 730.6595021404344, 723.3070466747297, 738.2529444175492, 483.0907815880338, 718.1015536191073, 738.4959037534047, 750.0242702448769, 626.0977607361802, 752.5611273233251, 740.21737278359, 742.6673938867688, 742.7076519012523, 755.1564151062397, 744.8498093428491, 803.1196401473621, 797.6380703636112, 759.5366226712182, 790.602616433348, 802.009036076361, 779.0845724661494, 528.1102612832437, 802.6270960916407, 798.3331645229582, 799.6020534392559, 793.9258835802998, 792.8678661354606, 517.1182495793414, 800.3536932615433, 793.2926064036338, 539.931197556674, 535.3155976593506, 535.1105176560386, 535.1185863360752, 786.0549232123103, 790.1741967672766, 783.517431529257, 764.6864747626844, 786.8361510302574, 799.2093992951272, 790.3656123800549, 745.1676095710164, 522.6124669488994, 772.8325378630359, 786.9703533949272, 775.6928872146821, 798.4744212333223, 668.1595599199221, 583.5472937204302, 790.4608649619337, 786.4697889785081, 784.0580639766421, 784.4475263132362, 767.6374520691758, 778.1519875527093, 796.8562655663434, 795.0195396873345, 787.2990098608817, 789.2324706227625, 780.6592577527323, 673.2189416954591, 653.8402301892, 787.6376889765013, 787.9755568662911, 808.4487990814622, 809.2842340913184, 811.5004530591634, 672.5831668841555, 758.3875141062548, 793.6817418229978, 740.2894258929782, 749.1011133053255, 756.0521227780138, 459.63728836535705, 490.40648213479363, 752.968310146488, 755.5560125229213, 747.5920448168689, 741.6087962215095, 741.3974994432501, 730.0340894988672, 710.8551104362889, 799.4577569923318, 789.6435010740469, 626.1889123559115, 769.5882814259629, 745.8056968221534, 535.3319550501044, 822.5337149777499, 764.148982799423, 760.6234557527729, 761.988520099725, 743.2043078558637, 758.0774425483463, 734.0071953482591, 518.221826335372, 544.637653398581, 514.1098587369514, 808.6034450211868, 743.0072859791183, 747.6360961156156, 740.3071970351517, 745.9972559915454, 495.6541619501981, 736.53629233694, 740.7351895471897, 736.8682158731721, 738.7336036741177, 518.4017093158128, 512.9204318945585, 744.074940054947, 741.7650746981212, 739.1397980188119, 735.4347781392987, 554.3809793654057, 746.6056360516335, 740.4353395020476, 747.6767634609039, 554.5461433902369, 803.7744101907559, 744.8218726003752, 752.880873004887, 744.669168757818, 753.2394108361462, 721.2906003257336, 756.5128763710258, 763.0951203302966, 752.2228713654556, 753.5519676096144, 749.3113481839343, 686.2163866334723, 556.340102904569, 815.8136145472739, 792.7733189607451, 797.8958649249474, 811.3368591631246, 811.504777131699, 801.5081223616263, 795.5853773960141, 814.9188834578919, 806.9980411626741, 801.2539910021061, 796.863594739044, 797.3742179262013, 797.023412242376, 763.8763284465157, 762.8475370022907, 761.3732606280267, 770.3875222642619, 761.6941082292485, 760.0756732550614, 759.0160155509826, 771.6737902421726, 766.9630184021598, 767.8937911621694, 758.0769529732767, 766.5349298600637, 745.321073378351, 746.8349661785334, 761.8582759722789, 530.6053995315741, 526.1392567182656, 524.9211377415652, 762.5786506056777, 772.4386929413845, 701.3694487028198, 755.2230747101993, 823.5827991482442, 802.566376062579, 814.2170265920657, 807.7951967423725, 806.2744962418093, 544.1406286902293, 808.6170648722704, 787.3644583363227, 812.984294481786, 796.469960959629, 815.5786858666951, 651.3449466929255, 801.3353993472654, 617.9005143923916, 813.0535153314485, 799.0860545051236, 500.9192120695204, 796.7911180693969, 792.7857539406859, 795.2681478202245, 765.8779640810833, 814.6445496671055, 546.3228731443144, 803.2825897525089, 806.3287178068565, 804.7501920416954, 806.0904749348459, 799.1171061234357, 805.9385201696467, 786.0095134155987, 805.1513658295745, 794.7176869968906, 549.5148373359228, 539.9466690188772, 539.8492146064394, 801.8946199236967, 742.4371475365683, 745.1682900716554, 749.6264644317262, 748.166939479281, 559.9752459291734, 749.1232430145627, 733.7977622075132, 759.5932293493506, 745.283748633177, 748.973157537417, 739.5194023663362, 750.3454860857629, 733.0390824941466, 737.2455105524215, 756.9143691105634, 752.7929447548489, 757.9794934529167, 749.6216944966897, 676.5946494101086, 500.71571852701, 753.7845241603911, 733.4242697626315, 517.1197424127014, 530.6370303306949, 790.4907806761803, 797.9827054224805, 801.4295816989435, 813.918378023804, 799.5798987937482, 714.6993216516512, 535.085500196298, 800.0880114113517, 783.1263205921587, 793.5455102707066, 751.1444388050206, 813.7281157329508, 704.2371017021284, 582.0525316518549, 809.923839161465, 827.618657861894, 807.3180261602321, 815.2592256321083, 816.7511119504171, 678.6395838283437, 774.6738855572646, 762.4828728484349, 769.0014496513954, 765.1515536659701, 758.4869310107011, 749.4831798727485, 739.7612572401418, 787.1252897103208, 793.7109251154302, 748.6117955031965, 539.8396876512725, 539.9205264845774, 508.58459447654445, 750.9723128426506, 743.7269145256219, 743.3080145365486, 750.9059506361806, 736.4473230214105, 514.4203736794464, 743.9818834832312, 734.1310988466214, 751.3588108420008, 754.4076977716717, 727.0049278123505, 782.9288204922847, 809.3453010274932, 809.0617374186787, 807.9247475934292, 803.0326336218366, 800.0909148992415, 676.1155121533736, 732.6302859460106, 763.6946890528383, 737.0974897401775, 788.6542757119873, 770.5736628816173, 679.613443169914, 800.8651678834061, 805.3734012884468, 755.9112918932392, 759.9046726669256, 754.6704879750112, 732.2194223366898, 761.0248788482518, 752.7734062281633, 759.3180995447481, 521.6143139962093, 503.70252762899474, 738.657062139496, 755.033810243704, 795.8005318042847, 762.5189608925103, 744.0702874728995, 729.7450475178998, 517.2213351777585, 558.8990786709209, 741.1295703035529]
Elapsed: 0.1801606862276094~0.028290281067250265
Time per graph: 0.0013965944668806929~0.00021930450439728888
Speed: 730.2807879483586~91.29825433724068
Total Time: 0.1751
best val loss: 0.14529550416293993 test_score: 0.8682

Testing...
Test loss: 0.2752 score: 0.8605 time: 0.17s
test Score 0.8605
Epoch Time List: [0.7755973841995001, 0.5715205722954124, 0.5584769591223449, 0.5614219650160521, 0.5783656067214906, 0.6155834691599011, 0.5564073571003973, 0.558660996844992, 0.5558570229914039, 0.5620986700523645, 0.5813217859249562, 0.6309207328595221, 0.5656078900210559, 0.5596460532397032, 0.5592070163693279, 0.553504184121266, 0.6378331931773573, 0.5933540957048535, 0.5677692759782076, 0.5624528611078858, 0.5672363820485771, 0.595197785878554, 0.6632938722614199, 0.571598530979827, 0.5581784371752292, 0.5661037089303136, 0.5586181550752372, 0.667509201914072, 0.5720257801003754, 0.5617433458101004, 0.5829179862048477, 0.5912495763041079, 0.5941719261463732, 0.6663042260333896, 0.5905647119507194, 0.5653112919535488, 0.5735324416309595, 0.8110638167709112, 0.8411251280922443, 0.7892517161089927, 0.5707778409123421, 0.5674885248299688, 0.5651758711319417, 0.5666664321906865, 0.5594309358857572, 0.586941858753562, 0.6538559079635888, 0.5873816162347794, 0.5872467909939587, 0.5881843990646303, 0.587586629902944, 0.6043414387386292, 0.6609265480656177, 0.5823413180187345, 0.5880501219071448, 0.7219613410998136, 0.8576215959619731, 0.8513411749154329, 0.7930991072207689, 0.5636339830234647, 0.5577402701601386, 0.5556698201689869, 0.5643293308094144, 0.5549125962425023, 0.6875220818910748, 0.5747040819842368, 0.5642254890408367, 0.5624866457656026, 0.5662977257743478, 0.5713689778931439, 0.7663882789202034, 0.5682006110437214, 0.5489976811222732, 0.7894165432080626, 0.8271518624387681, 0.8162630647420883, 0.8217920821625739, 0.6007493429351598, 0.5539326486177742, 0.5496085786726326, 0.5541591949295253, 0.6128333150409162, 0.7071904812473804, 0.5872345298994333, 0.6672723889350891, 0.5788314808160067, 0.5973954449873418, 0.7422948160674423, 0.5936202490702271, 0.6802488702815026, 0.5414833128452301, 0.5768198070582002, 0.5880306896287948, 0.6682796482928097, 0.5926394197158515, 0.5997820110060275, 0.5946805828716606, 0.5952887330204248, 0.596308472333476, 0.5951835452578962, 0.7144159891176969, 0.6042460829485208, 0.5993372478988022, 0.5902742699254304, 0.6535578959155828, 0.649566964013502, 0.5966727801132947, 0.5932977860793471, 0.6119402749463916, 0.594020240008831, 0.6008137008175254, 0.6316064142156392, 0.5588254269678146, 0.5745081282220781, 0.5576676910277456, 0.561995540978387, 0.5716136188711971, 0.8355553091969341, 0.5896823131479323, 0.5644917520694435, 0.5620899929199368, 0.5659940899349749, 0.5640874139498919, 0.6980796919669956, 0.5691085828002542, 0.5598380661103874, 0.7033039007801563, 0.8393668220378458, 0.8392870649695396, 0.8371419999748468, 0.654231364140287, 0.5712516061030328, 0.5583341571036726, 0.7479637020733207, 0.5639892760664225, 0.5544634107500315, 0.5622757899109274, 0.5694899968802929, 0.7251254958100617, 0.5894532999955118, 0.5699005760252476, 0.5748510719276965, 0.5712421417701989, 0.5990380600560457, 0.6406115940771997, 0.6418615910224617, 0.5635983543470502, 0.5664167227223516, 0.5619438430294394, 0.5738787539303303, 0.7693893848918378, 0.572714862646535, 0.5676379939541221, 0.5686286299023777, 0.5708915460854769, 0.5714853131212294, 0.5997448430862278, 0.6641385932452977, 0.5700356201268733, 0.576115861069411, 0.5539940849412233, 0.563822008902207, 0.5569580621086061, 0.6154325890820473, 0.6597543458919972, 0.5565078051295131, 0.5818952068220824, 0.5893414330203086, 0.5802547840867192, 0.6972065882291645, 0.7204036589246243, 0.5831926588434726, 0.5856506058480591, 0.5845838291570544, 0.5856490680016577, 0.5876872588414699, 0.5896792982239276, 0.6018440949264914, 0.7107192501425743, 0.5533956121653318, 0.6105515700764954, 0.5661211379338056, 0.5746888322755694, 0.6516737181227654, 0.5350677068345249, 0.5729016836266965, 0.633563200943172, 0.6761251848656684, 0.5823800021316856, 0.5735965080093592, 0.5850018700584769, 0.6833115571644157, 0.8164044588338584, 0.8410908412188292, 0.5571200530976057, 0.5774670359678566, 0.5934019370470196, 0.5926426809746772, 0.6009307280182838, 0.7275966410525143, 0.5961167218629271, 0.5999345178715885, 0.5987051150295883, 0.5940521031152457, 0.6862913260702044, 0.8673943402245641, 0.6866098188329488, 0.5946250602137297, 0.5926475890446454, 0.597979661077261, 0.6654703852254897, 0.709268522914499, 0.5944798600394279, 0.5871334332041442, 0.7232648301869631, 0.6570475092157722, 0.5920855009462684, 0.586467772256583, 0.5895334919914603, 0.5872805928811431, 0.6241951850242913, 0.6711252662353218, 0.5868393362034112, 0.5837719170376658, 0.5918925758451223, 0.5880117591004819, 0.6166505941655487, 0.6666742768138647, 0.551067034015432, 0.5614066130947322, 0.5518877129070461, 0.555944656720385, 0.5552036657463759, 0.5927939550019801, 0.6341691568959504, 0.5511047439649701, 0.5551754080224782, 0.5632520359940827, 0.5626041828654706, 0.6446642859373242, 0.5483496058732271, 0.5770511988084763, 0.5779496331233531, 0.583480381872505, 0.579168704804033, 0.5775639000348747, 0.603153018746525, 0.6654929623473436, 0.5804749059025198, 0.5839402300771326, 0.5795208960771561, 0.5813210278283805, 0.5744128827936947, 0.6213273622561246, 0.6632254151627421, 0.5800286058802158, 0.7438703197985888, 0.8522536458913237, 0.8548027237411588, 0.6239087760914117, 0.5802438440732658, 0.6021364100743085, 0.6824663046281785, 0.5491016211453825, 0.5506485940422863, 0.5462566851638258, 0.5489165098406374, 0.5572183760814369, 0.6597433860879391, 0.634148753946647, 0.5548715598415583, 0.5484204050153494, 0.553975738119334, 0.5541457578074187, 0.7178778948727995, 0.5495587068144232, 0.6094880518503487, 0.5495890851598233, 0.5520400628447533, 0.6568744550459087, 0.5605107438750565, 0.6006065888796002, 0.7024782260414213, 0.5753568848595023, 0.566573899006471, 0.6649582078680396, 0.6833437101449817, 0.5585889599751681, 0.559573887148872, 0.5579374879598618, 0.5549067370593548, 0.5487464170437306, 0.5944335791282356, 0.6321499519981444, 0.5641306980978698, 0.7098724360112101, 0.8355929509270936, 0.8420459337066859, 0.6387709309346974, 0.5832411600276828, 0.5973200728185475, 0.587966870283708, 0.5877405612263829, 0.6645818760152906, 0.6100514121353626, 0.5893156619276851, 0.5879765069112182, 0.5848185468930751, 0.5887142061255872, 0.6027590003795922, 0.663023852976039, 0.5936956650111824, 0.6024591750465333, 0.5921089870389551, 0.5984022149350494, 0.5838193341623992, 0.5885331970639527, 0.6042604420799762, 0.689556498080492, 0.5951801748014987, 0.5987841070163995, 0.855895146727562, 0.865328473970294, 0.7064790651202202, 0.5668331931810826, 0.5677142380736768, 0.5546520401258022, 0.5501997121609747, 0.5800938371103257, 0.7615502302069217, 0.6529943149071187, 0.5624702572822571, 0.5593309276737273, 0.584667838877067, 0.5622086718212813, 0.5730970790609717, 0.659987036138773, 0.5539456349797547, 0.5350425380747765, 0.5368907537776977, 0.543938698945567, 0.5438304811250418, 0.5910443470347673, 0.6375722431112081, 0.5695784089621156, 0.5763746618758887, 0.5759748083073646, 0.5792086939327419, 0.5801315479911864, 0.6026290818117559, 0.6389543011318892, 0.5573943071067333, 0.5610578421037644, 0.8348937181290239, 0.8352610347792506, 0.854220069013536, 0.594773449935019, 0.5890105951111764, 0.5883497807662934, 0.5895731849595904, 0.5926206370349973, 0.6936846112366766, 0.7157735589426011, 0.5882899791467935, 0.5858090010005981, 0.5796935758553445, 0.5894813989289105, 0.6476842078845948, 0.549240366788581, 0.562006413936615, 0.6054464352782816, 0.5514756517950445, 0.5687490170821548, 0.7540259868837893, 0.5933778521139175, 0.6486072891857475, 0.5828982619568706, 0.5838135606609285, 0.6187370188999921, 0.64650285593234, 0.5654985290020704, 0.5546099930070341, 0.5812732821796089, 0.5852311819326133, 0.5835455229971558, 0.5953825989272445, 0.6634353529661894, 0.589076278032735, 0.5880122929811478, 0.722072197124362, 0.8742205209564418, 0.6087442436255515, 0.5885830188635737, 0.6224387402180582, 0.5717673238832504, 0.5894239880144596, 0.6038581726606935, 0.764710531802848, 0.8492345891427249, 0.6022522037383169]
Total Epoch List: [400]
Total Time List: [0.17514378111809492]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e8cf33760>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.2521;  Loss pred: 3.2521; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7017 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 3.2655;  Loss pred: 3.2655; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 3.2477;  Loss pred: 3.2477; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7009 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5039 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 3.2296;  Loss pred: 3.2296; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 3.1800;  Loss pred: 3.1800; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5039 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 3.1157;  Loss pred: 3.1157; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 3.0664;  Loss pred: 3.0664; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 2.9806;  Loss pred: 2.9806; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5039 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 2.9004;  Loss pred: 2.9004; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 2.8020;  Loss pred: 2.8020; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5039 time: 0.18s
Epoch 11/1000, LR 0.000285
Train loss: 2.7149;  Loss pred: 2.7149; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 2.6259;  Loss pred: 2.6259; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5039 time: 0.18s
Epoch 13/1000, LR 0.000285
Train loss: 2.5774;  Loss pred: 2.5774; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5039 time: 0.19s
Epoch 14/1000, LR 0.000285
Train loss: 2.4730;  Loss pred: 2.4730; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5039 time: 0.18s
Epoch 15/1000, LR 0.000285
Train loss: 2.4060;  Loss pred: 2.4060; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5039 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 2.3346;  Loss pred: 2.3346; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5039 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 2.2424;  Loss pred: 2.2424; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 2.1941;  Loss pred: 2.1941; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 2.1094;  Loss pred: 2.1094; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 2.0824;  Loss pred: 2.0824; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.24s
Epoch 21/1000, LR 0.000285
Train loss: 2.0228;  Loss pred: 2.0228; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 1.9690;  Loss pred: 1.9690; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 1.8893;  Loss pred: 1.8893; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 1.8557;  Loss pred: 1.8557; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 1.8209;  Loss pred: 1.8209; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5039 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 1.7577;  Loss pred: 1.7577; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.24s
Epoch 27/1000, LR 0.000285
Train loss: 1.7300;  Loss pred: 1.7300; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 1.7000;  Loss pred: 1.7000; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 1.6533;  Loss pred: 1.6533; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5039 time: 0.24s
Epoch 30/1000, LR 0.000285
Train loss: 1.6159;  Loss pred: 1.6159; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5039 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 1.5818;  Loss pred: 1.5818; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5039 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 1.5591;  Loss pred: 1.5591; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5039 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 1.5202;  Loss pred: 1.5202; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5039 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 1.4970;  Loss pred: 1.4970; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5039 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 1.4562;  Loss pred: 1.4562; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5039 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 1.4416;  Loss pred: 1.4416; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5039 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 1.4177;  Loss pred: 1.4177; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5039 time: 0.21s
Epoch 38/1000, LR 0.000284
Train loss: 1.3954;  Loss pred: 1.3954; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5039 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 1.3700;  Loss pred: 1.3700; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5039 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 1.3494;  Loss pred: 1.3494; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5039 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 1.3276;  Loss pred: 1.3276; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5039 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 1.3105;  Loss pred: 1.3105; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5039 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 1.2950;  Loss pred: 1.2950; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5039 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 1.2883;  Loss pred: 1.2883; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.5039 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 1.2629;  Loss pred: 1.2629; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5039 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 1.2507;  Loss pred: 1.2507; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5039 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 1.2347;  Loss pred: 1.2347; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.5039 time: 0.24s
Epoch 48/1000, LR 0.000284
Train loss: 1.2252;  Loss pred: 1.2252; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5039 time: 0.24s
Epoch 49/1000, LR 0.000284
Train loss: 1.2117;  Loss pred: 1.2117; Loss self: 0.0000; time: 0.37s
Val loss: 0.6844 score: 0.5116 time: 0.18s
Test loss: 0.6855 score: 0.5116 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 1.1924;  Loss pred: 1.1924; Loss self: 0.0000; time: 0.26s
Val loss: 0.6836 score: 0.5504 time: 0.16s
Test loss: 0.6849 score: 0.5426 time: 0.16s
Epoch 51/1000, LR 0.000284
Train loss: 1.1817;  Loss pred: 1.1817; Loss self: 0.0000; time: 0.25s
Val loss: 0.6829 score: 0.6124 time: 0.16s
Test loss: 0.6842 score: 0.5969 time: 0.16s
Epoch 52/1000, LR 0.000284
Train loss: 1.1771;  Loss pred: 1.1771; Loss self: 0.0000; time: 0.25s
Val loss: 0.6821 score: 0.6667 time: 0.16s
Test loss: 0.6836 score: 0.6512 time: 0.16s
Epoch 53/1000, LR 0.000284
Train loss: 1.1642;  Loss pred: 1.1642; Loss self: 0.0000; time: 0.25s
Val loss: 0.6813 score: 0.6822 time: 0.17s
Test loss: 0.6828 score: 0.6667 time: 0.17s
Epoch 54/1000, LR 0.000284
Train loss: 1.1530;  Loss pred: 1.1530; Loss self: 0.0000; time: 0.30s
Val loss: 0.6804 score: 0.7054 time: 0.16s
Test loss: 0.6821 score: 0.7054 time: 0.16s
Epoch 55/1000, LR 0.000284
Train loss: 1.1422;  Loss pred: 1.1422; Loss self: 0.0000; time: 0.25s
Val loss: 0.6794 score: 0.7519 time: 0.16s
Test loss: 0.6812 score: 0.7287 time: 0.16s
Epoch 56/1000, LR 0.000284
Train loss: 1.1350;  Loss pred: 1.1350; Loss self: 0.0000; time: 0.25s
Val loss: 0.6784 score: 0.7674 time: 0.16s
Test loss: 0.6804 score: 0.7442 time: 0.16s
Epoch 57/1000, LR 0.000283
Train loss: 1.1264;  Loss pred: 1.1264; Loss self: 0.0000; time: 0.24s
Val loss: 0.6773 score: 0.7907 time: 0.16s
Test loss: 0.6794 score: 0.7442 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 1.1166;  Loss pred: 1.1166; Loss self: 0.0000; time: 0.24s
Val loss: 0.6762 score: 0.7984 time: 0.15s
Test loss: 0.6785 score: 0.7597 time: 0.16s
Epoch 59/1000, LR 0.000283
Train loss: 1.1159;  Loss pred: 1.1159; Loss self: 0.0000; time: 0.25s
Val loss: 0.6750 score: 0.8372 time: 0.22s
Test loss: 0.6774 score: 0.7984 time: 0.21s
Epoch 60/1000, LR 0.000283
Train loss: 1.1017;  Loss pred: 1.1017; Loss self: 0.0000; time: 0.25s
Val loss: 0.6736 score: 0.8450 time: 0.17s
Test loss: 0.6763 score: 0.8140 time: 0.26s
Epoch 61/1000, LR 0.000283
Train loss: 1.0934;  Loss pred: 1.0934; Loss self: 0.0000; time: 0.26s
Val loss: 0.6722 score: 0.8527 time: 0.17s
Test loss: 0.6750 score: 0.8217 time: 0.17s
Epoch 62/1000, LR 0.000283
Train loss: 1.0924;  Loss pred: 1.0924; Loss self: 0.0000; time: 0.34s
Val loss: 0.6707 score: 0.8605 time: 0.16s
Test loss: 0.6736 score: 0.8295 time: 0.17s
Epoch 63/1000, LR 0.000283
Train loss: 1.0831;  Loss pred: 1.0831; Loss self: 0.0000; time: 0.26s
Val loss: 0.6691 score: 0.8605 time: 0.17s
Test loss: 0.6722 score: 0.8295 time: 0.17s
Epoch 64/1000, LR 0.000283
Train loss: 1.0776;  Loss pred: 1.0776; Loss self: 0.0000; time: 0.25s
Val loss: 0.6674 score: 0.8605 time: 0.24s
Test loss: 0.6708 score: 0.8295 time: 0.16s
Epoch 65/1000, LR 0.000283
Train loss: 1.0729;  Loss pred: 1.0729; Loss self: 0.0000; time: 0.24s
Val loss: 0.6657 score: 0.8682 time: 0.16s
Test loss: 0.6693 score: 0.8372 time: 0.16s
Epoch 66/1000, LR 0.000283
Train loss: 1.0656;  Loss pred: 1.0656; Loss self: 0.0000; time: 0.25s
Val loss: 0.6639 score: 0.8682 time: 0.16s
Test loss: 0.6676 score: 0.8450 time: 0.16s
Epoch 67/1000, LR 0.000283
Train loss: 1.0588;  Loss pred: 1.0588; Loss self: 0.0000; time: 0.28s
Val loss: 0.6620 score: 0.8682 time: 0.20s
Test loss: 0.6659 score: 0.8295 time: 0.23s
Epoch 68/1000, LR 0.000283
Train loss: 1.0559;  Loss pred: 1.0559; Loss self: 0.0000; time: 0.28s
Val loss: 0.6600 score: 0.8682 time: 0.17s
Test loss: 0.6641 score: 0.8450 time: 0.18s
Epoch 69/1000, LR 0.000283
Train loss: 1.0475;  Loss pred: 1.0475; Loss self: 0.0000; time: 0.26s
Val loss: 0.6579 score: 0.8682 time: 0.17s
Test loss: 0.6623 score: 0.8527 time: 0.17s
Epoch 70/1000, LR 0.000283
Train loss: 1.0452;  Loss pred: 1.0452; Loss self: 0.0000; time: 0.25s
Val loss: 0.6556 score: 0.8682 time: 0.17s
Test loss: 0.6603 score: 0.8527 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 1.0400;  Loss pred: 1.0400; Loss self: 0.0000; time: 0.26s
Val loss: 0.6533 score: 0.8682 time: 0.17s
Test loss: 0.6582 score: 0.8682 time: 0.17s
Epoch 72/1000, LR 0.000282
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.28s
Val loss: 0.6508 score: 0.8682 time: 0.17s
Test loss: 0.6560 score: 0.8682 time: 0.23s
Epoch 73/1000, LR 0.000282
Train loss: 1.0294;  Loss pred: 1.0294; Loss self: 0.0000; time: 0.26s
Val loss: 0.6482 score: 0.8682 time: 0.17s
Test loss: 0.6537 score: 0.8682 time: 0.17s
Epoch 74/1000, LR 0.000282
Train loss: 1.0239;  Loss pred: 1.0239; Loss self: 0.0000; time: 0.26s
Val loss: 0.6455 score: 0.8682 time: 0.17s
Test loss: 0.6513 score: 0.8682 time: 0.17s
Epoch 75/1000, LR 0.000282
Train loss: 1.0163;  Loss pred: 1.0163; Loss self: 0.0000; time: 0.25s
Val loss: 0.6427 score: 0.8760 time: 0.16s
Test loss: 0.6488 score: 0.8682 time: 0.16s
Epoch 76/1000, LR 0.000282
Train loss: 1.0145;  Loss pred: 1.0145; Loss self: 0.0000; time: 0.25s
Val loss: 0.6398 score: 0.8760 time: 0.16s
Test loss: 0.6462 score: 0.8682 time: 0.16s
Epoch 77/1000, LR 0.000282
Train loss: 1.0100;  Loss pred: 1.0100; Loss self: 0.0000; time: 0.25s
Val loss: 0.6367 score: 0.8915 time: 0.16s
Test loss: 0.6434 score: 0.8682 time: 0.16s
Epoch 78/1000, LR 0.000282
Train loss: 1.0042;  Loss pred: 1.0042; Loss self: 0.0000; time: 0.25s
Val loss: 0.6334 score: 0.8915 time: 0.16s
Test loss: 0.6406 score: 0.8682 time: 0.16s
Epoch 79/1000, LR 0.000282
Train loss: 1.0011;  Loss pred: 1.0011; Loss self: 0.0000; time: 0.25s
Val loss: 0.6301 score: 0.8915 time: 0.16s
Test loss: 0.6376 score: 0.8760 time: 0.16s
Epoch 80/1000, LR 0.000282
Train loss: 0.9948;  Loss pred: 0.9948; Loss self: 0.0000; time: 0.25s
Val loss: 0.6266 score: 0.8915 time: 0.16s
Test loss: 0.6345 score: 0.8760 time: 0.19s
Epoch 81/1000, LR 0.000281
Train loss: 0.9902;  Loss pred: 0.9902; Loss self: 0.0000; time: 0.25s
Val loss: 0.6230 score: 0.8915 time: 0.17s
Test loss: 0.6312 score: 0.8760 time: 0.21s
Epoch 82/1000, LR 0.000281
Train loss: 0.9870;  Loss pred: 0.9870; Loss self: 0.0000; time: 0.25s
Val loss: 0.6192 score: 0.8915 time: 0.16s
Test loss: 0.6278 score: 0.8760 time: 0.16s
Epoch 83/1000, LR 0.000281
Train loss: 0.9819;  Loss pred: 0.9819; Loss self: 0.0000; time: 0.25s
Val loss: 0.6151 score: 0.8915 time: 0.16s
Test loss: 0.6243 score: 0.8760 time: 0.16s
Epoch 84/1000, LR 0.000281
Train loss: 0.9759;  Loss pred: 0.9759; Loss self: 0.0000; time: 0.25s
Val loss: 0.6110 score: 0.8915 time: 0.16s
Test loss: 0.6205 score: 0.8760 time: 0.16s
Epoch 85/1000, LR 0.000281
Train loss: 0.9714;  Loss pred: 0.9714; Loss self: 0.0000; time: 0.25s
Val loss: 0.6067 score: 0.8915 time: 0.16s
Test loss: 0.6167 score: 0.8760 time: 0.16s
Epoch 86/1000, LR 0.000281
Train loss: 0.9678;  Loss pred: 0.9678; Loss self: 0.0000; time: 0.26s
Val loss: 0.6022 score: 0.8915 time: 0.16s
Test loss: 0.6127 score: 0.8837 time: 0.23s
Epoch 87/1000, LR 0.000281
Train loss: 0.9624;  Loss pred: 0.9624; Loss self: 0.0000; time: 0.37s
Val loss: 0.5976 score: 0.8915 time: 0.24s
Test loss: 0.6085 score: 0.8837 time: 0.25s
Epoch 88/1000, LR 0.000281
Train loss: 0.9583;  Loss pred: 0.9583; Loss self: 0.0000; time: 0.26s
Val loss: 0.5929 score: 0.8915 time: 0.16s
Test loss: 0.6043 score: 0.8837 time: 0.16s
Epoch 89/1000, LR 0.000281
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.26s
Val loss: 0.5880 score: 0.8915 time: 0.18s
Test loss: 0.6000 score: 0.8837 time: 0.17s
Epoch 90/1000, LR 0.000281
Train loss: 0.9499;  Loss pred: 0.9499; Loss self: 0.0000; time: 0.26s
Val loss: 0.5831 score: 0.8915 time: 0.17s
Test loss: 0.5955 score: 0.8760 time: 0.17s
Epoch 91/1000, LR 0.000280
Train loss: 0.9453;  Loss pred: 0.9453; Loss self: 0.0000; time: 0.26s
Val loss: 0.5780 score: 0.8915 time: 0.17s
Test loss: 0.5909 score: 0.8837 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.9419;  Loss pred: 0.9419; Loss self: 0.0000; time: 0.24s
Val loss: 0.5728 score: 0.8915 time: 0.16s
Test loss: 0.5863 score: 0.8837 time: 0.15s
Epoch 93/1000, LR 0.000280
Train loss: 0.9358;  Loss pred: 0.9358; Loss self: 0.0000; time: 0.24s
Val loss: 0.5674 score: 0.8915 time: 0.16s
Test loss: 0.5815 score: 0.8837 time: 0.18s
Epoch 94/1000, LR 0.000280
Train loss: 0.9315;  Loss pred: 0.9315; Loss self: 0.0000; time: 0.26s
Val loss: 0.5618 score: 0.8915 time: 0.23s
Test loss: 0.5764 score: 0.8837 time: 0.17s
Epoch 95/1000, LR 0.000280
Train loss: 0.9256;  Loss pred: 0.9256; Loss self: 0.0000; time: 0.26s
Val loss: 0.5561 score: 0.8915 time: 0.17s
Test loss: 0.5713 score: 0.8915 time: 0.17s
Epoch 96/1000, LR 0.000280
Train loss: 0.9208;  Loss pred: 0.9208; Loss self: 0.0000; time: 0.24s
Val loss: 0.5503 score: 0.8992 time: 0.16s
Test loss: 0.5661 score: 0.8915 time: 0.16s
Epoch 97/1000, LR 0.000280
Train loss: 0.9158;  Loss pred: 0.9158; Loss self: 0.0000; time: 0.25s
Val loss: 0.5444 score: 0.8915 time: 0.16s
Test loss: 0.5608 score: 0.8837 time: 0.17s
Epoch 98/1000, LR 0.000280
Train loss: 0.9097;  Loss pred: 0.9097; Loss self: 0.0000; time: 0.25s
Val loss: 0.5384 score: 0.8992 time: 0.17s
Test loss: 0.5554 score: 0.8837 time: 0.17s
Epoch 99/1000, LR 0.000279
Train loss: 0.9071;  Loss pred: 0.9071; Loss self: 0.0000; time: 0.25s
Val loss: 0.5323 score: 0.8992 time: 0.17s
Test loss: 0.5499 score: 0.8837 time: 0.17s
Epoch 100/1000, LR 0.000279
Train loss: 0.9003;  Loss pred: 0.9003; Loss self: 0.0000; time: 0.26s
Val loss: 0.5261 score: 0.8992 time: 0.17s
Test loss: 0.5444 score: 0.8837 time: 0.17s
Epoch 101/1000, LR 0.000279
Train loss: 0.8969;  Loss pred: 0.8969; Loss self: 0.0000; time: 0.36s
Val loss: 0.5198 score: 0.8992 time: 0.17s
Test loss: 0.5387 score: 0.8837 time: 0.17s
Epoch 102/1000, LR 0.000279
Train loss: 0.8921;  Loss pred: 0.8921; Loss self: 0.0000; time: 0.26s
Val loss: 0.5136 score: 0.8992 time: 0.17s
Test loss: 0.5331 score: 0.8837 time: 0.17s
Epoch 103/1000, LR 0.000279
Train loss: 0.8856;  Loss pred: 0.8856; Loss self: 0.0000; time: 0.26s
Val loss: 0.5071 score: 0.8992 time: 0.17s
Test loss: 0.5272 score: 0.8837 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.8809;  Loss pred: 0.8809; Loss self: 0.0000; time: 0.26s
Val loss: 0.5005 score: 0.9070 time: 0.16s
Test loss: 0.5213 score: 0.8837 time: 0.16s
Epoch 105/1000, LR 0.000279
Train loss: 0.8767;  Loss pred: 0.8767; Loss self: 0.0000; time: 0.24s
Val loss: 0.4941 score: 0.9070 time: 0.15s
Test loss: 0.5154 score: 0.8837 time: 0.16s
Epoch 106/1000, LR 0.000279
Train loss: 0.8707;  Loss pred: 0.8707; Loss self: 0.0000; time: 0.26s
Val loss: 0.4876 score: 0.9070 time: 0.16s
Test loss: 0.5095 score: 0.8837 time: 0.16s
Epoch 107/1000, LR 0.000278
Train loss: 0.8659;  Loss pred: 0.8659; Loss self: 0.0000; time: 0.36s
Val loss: 0.4811 score: 0.8992 time: 0.16s
Test loss: 0.5037 score: 0.8837 time: 0.15s
Epoch 108/1000, LR 0.000278
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 0.25s
Val loss: 0.4744 score: 0.9070 time: 0.16s
Test loss: 0.4976 score: 0.8837 time: 0.15s
Epoch 109/1000, LR 0.000278
Train loss: 0.8555;  Loss pred: 0.8555; Loss self: 0.0000; time: 0.24s
Val loss: 0.4676 score: 0.9070 time: 0.16s
Test loss: 0.4914 score: 0.8837 time: 0.16s
Epoch 110/1000, LR 0.000278
Train loss: 0.8503;  Loss pred: 0.8503; Loss self: 0.0000; time: 0.24s
Val loss: 0.4608 score: 0.9070 time: 0.16s
Test loss: 0.4852 score: 0.8760 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.8452;  Loss pred: 0.8452; Loss self: 0.0000; time: 0.24s
Val loss: 0.4539 score: 0.9070 time: 0.16s
Test loss: 0.4789 score: 0.8760 time: 0.16s
Epoch 112/1000, LR 0.000278
Train loss: 0.8405;  Loss pred: 0.8405; Loss self: 0.0000; time: 0.29s
Val loss: 0.4472 score: 0.9070 time: 0.16s
Test loss: 0.4728 score: 0.8760 time: 0.21s
Epoch 113/1000, LR 0.000278
Train loss: 0.8340;  Loss pred: 0.8340; Loss self: 0.0000; time: 0.36s
Val loss: 0.4403 score: 0.9147 time: 0.24s
Test loss: 0.4665 score: 0.8760 time: 0.22s
Epoch 114/1000, LR 0.000277
Train loss: 0.8288;  Loss pred: 0.8288; Loss self: 0.0000; time: 0.26s
Val loss: 0.4336 score: 0.9147 time: 0.16s
Test loss: 0.4604 score: 0.8682 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.8265;  Loss pred: 0.8265; Loss self: 0.0000; time: 0.24s
Val loss: 0.4269 score: 0.9147 time: 0.16s
Test loss: 0.4543 score: 0.8760 time: 0.16s
Epoch 116/1000, LR 0.000277
Train loss: 0.8184;  Loss pred: 0.8184; Loss self: 0.0000; time: 0.25s
Val loss: 0.4205 score: 0.9147 time: 0.17s
Test loss: 0.4483 score: 0.8837 time: 0.17s
Epoch 117/1000, LR 0.000277
Train loss: 0.8148;  Loss pred: 0.8148; Loss self: 0.0000; time: 0.25s
Val loss: 0.4142 score: 0.9147 time: 0.17s
Test loss: 0.4425 score: 0.8760 time: 0.17s
Epoch 118/1000, LR 0.000277
Train loss: 0.8108;  Loss pred: 0.8108; Loss self: 0.0000; time: 0.26s
Val loss: 0.4081 score: 0.9147 time: 0.18s
Test loss: 0.4367 score: 0.8760 time: 0.17s
Epoch 119/1000, LR 0.000277
Train loss: 0.8072;  Loss pred: 0.8072; Loss self: 0.0000; time: 0.33s
Val loss: 0.4018 score: 0.9147 time: 0.17s
Test loss: 0.4309 score: 0.8760 time: 0.17s
Epoch 120/1000, LR 0.000277
Train loss: 0.7997;  Loss pred: 0.7997; Loss self: 0.0000; time: 0.26s
Val loss: 0.3959 score: 0.9147 time: 0.16s
Test loss: 0.4252 score: 0.8760 time: 0.16s
Epoch 121/1000, LR 0.000276
Train loss: 0.7961;  Loss pred: 0.7961; Loss self: 0.0000; time: 0.34s
Val loss: 0.3900 score: 0.9147 time: 0.24s
Test loss: 0.4197 score: 0.8760 time: 0.24s
Epoch 122/1000, LR 0.000276
Train loss: 0.7913;  Loss pred: 0.7913; Loss self: 0.0000; time: 0.36s
Val loss: 0.3836 score: 0.9147 time: 0.23s
Test loss: 0.4138 score: 0.8837 time: 0.24s
Epoch 123/1000, LR 0.000276
Train loss: 0.7861;  Loss pred: 0.7861; Loss self: 0.0000; time: 0.36s
Val loss: 0.3773 score: 0.9147 time: 0.24s
Test loss: 0.4081 score: 0.8837 time: 0.24s
Epoch 124/1000, LR 0.000276
Train loss: 0.7831;  Loss pred: 0.7831; Loss self: 0.0000; time: 0.37s
Val loss: 0.3713 score: 0.9147 time: 0.16s
Test loss: 0.4025 score: 0.8837 time: 0.16s
Epoch 125/1000, LR 0.000276
Train loss: 0.7757;  Loss pred: 0.7757; Loss self: 0.0000; time: 0.25s
Val loss: 0.3654 score: 0.9147 time: 0.16s
Test loss: 0.3970 score: 0.8915 time: 0.15s
Epoch 126/1000, LR 0.000276
Train loss: 0.7746;  Loss pred: 0.7746; Loss self: 0.0000; time: 0.25s
Val loss: 0.3598 score: 0.9225 time: 0.16s
Test loss: 0.3917 score: 0.8915 time: 0.16s
Epoch 127/1000, LR 0.000275
Train loss: 0.7696;  Loss pred: 0.7696; Loss self: 0.0000; time: 0.25s
Val loss: 0.3545 score: 0.9225 time: 0.16s
Test loss: 0.3866 score: 0.8837 time: 0.16s
Epoch 128/1000, LR 0.000275
Train loss: 0.7648;  Loss pred: 0.7648; Loss self: 0.0000; time: 0.25s
Val loss: 0.3493 score: 0.9225 time: 0.16s
Test loss: 0.3815 score: 0.8837 time: 0.15s
Epoch 129/1000, LR 0.000275
Train loss: 0.7610;  Loss pred: 0.7610; Loss self: 0.0000; time: 0.24s
Val loss: 0.3442 score: 0.9147 time: 0.17s
Test loss: 0.3766 score: 0.8837 time: 0.16s
Epoch 130/1000, LR 0.000275
Train loss: 0.7570;  Loss pred: 0.7570; Loss self: 0.0000; time: 0.35s
Val loss: 0.3390 score: 0.9225 time: 0.23s
Test loss: 0.3716 score: 0.8837 time: 0.24s
Epoch 131/1000, LR 0.000275
Train loss: 0.7518;  Loss pred: 0.7518; Loss self: 0.0000; time: 0.25s
Val loss: 0.3340 score: 0.9225 time: 0.16s
Test loss: 0.3668 score: 0.8837 time: 0.15s
Epoch 132/1000, LR 0.000275
Train loss: 0.7508;  Loss pred: 0.7508; Loss self: 0.0000; time: 0.24s
Val loss: 0.3292 score: 0.9225 time: 0.15s
Test loss: 0.3621 score: 0.8837 time: 0.16s
Epoch 133/1000, LR 0.000274
Train loss: 0.7459;  Loss pred: 0.7459; Loss self: 0.0000; time: 0.25s
Val loss: 0.3244 score: 0.9225 time: 0.16s
Test loss: 0.3574 score: 0.8915 time: 0.15s
Epoch 134/1000, LR 0.000274
Train loss: 0.7402;  Loss pred: 0.7402; Loss self: 0.0000; time: 0.24s
Val loss: 0.3192 score: 0.9225 time: 0.15s
Test loss: 0.3524 score: 0.8992 time: 0.16s
Epoch 135/1000, LR 0.000274
Train loss: 0.7374;  Loss pred: 0.7374; Loss self: 0.0000; time: 0.26s
Val loss: 0.3142 score: 0.9302 time: 0.24s
Test loss: 0.3477 score: 0.9070 time: 0.25s
Epoch 136/1000, LR 0.000274
Train loss: 0.7335;  Loss pred: 0.7335; Loss self: 0.0000; time: 0.35s
Val loss: 0.3093 score: 0.9302 time: 0.19s
Test loss: 0.3431 score: 0.9070 time: 0.17s
Epoch 137/1000, LR 0.000274
Train loss: 0.7291;  Loss pred: 0.7291; Loss self: 0.0000; time: 0.25s
Val loss: 0.3047 score: 0.9302 time: 0.18s
Test loss: 0.3388 score: 0.9070 time: 0.17s
Epoch 138/1000, LR 0.000274
Train loss: 0.7244;  Loss pred: 0.7244; Loss self: 0.0000; time: 0.25s
Val loss: 0.3006 score: 0.9302 time: 0.17s
Test loss: 0.3345 score: 0.9070 time: 0.17s
Epoch 139/1000, LR 0.000273
Train loss: 0.7226;  Loss pred: 0.7226; Loss self: 0.0000; time: 0.26s
Val loss: 0.2966 score: 0.9302 time: 0.17s
Test loss: 0.3303 score: 0.9070 time: 0.17s
Epoch 140/1000, LR 0.000273
Train loss: 0.7183;  Loss pred: 0.7183; Loss self: 0.0000; time: 0.25s
Val loss: 0.2929 score: 0.9302 time: 0.17s
Test loss: 0.3265 score: 0.9070 time: 0.17s
Epoch 141/1000, LR 0.000273
Train loss: 0.7150;  Loss pred: 0.7150; Loss self: 0.0000; time: 0.26s
Val loss: 0.2890 score: 0.9302 time: 0.22s
Test loss: 0.3225 score: 0.9070 time: 0.18s
Epoch 142/1000, LR 0.000273
Train loss: 0.7107;  Loss pred: 0.7107; Loss self: 0.0000; time: 0.33s
Val loss: 0.2848 score: 0.9302 time: 0.17s
Test loss: 0.3184 score: 0.9070 time: 0.17s
Epoch 143/1000, LR 0.000273
Train loss: 0.7069;  Loss pred: 0.7069; Loss self: 0.0000; time: 0.25s
Val loss: 0.2809 score: 0.9302 time: 0.17s
Test loss: 0.3145 score: 0.9070 time: 0.17s
Epoch 144/1000, LR 0.000272
Train loss: 0.7045;  Loss pred: 0.7045; Loss self: 0.0000; time: 0.25s
Val loss: 0.2772 score: 0.9302 time: 0.16s
Test loss: 0.3107 score: 0.9147 time: 0.16s
Epoch 145/1000, LR 0.000272
Train loss: 0.7019;  Loss pred: 0.7019; Loss self: 0.0000; time: 0.25s
Val loss: 0.2733 score: 0.9302 time: 0.17s
Test loss: 0.3069 score: 0.9147 time: 0.17s
Epoch 146/1000, LR 0.000272
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.28s
Val loss: 0.2698 score: 0.9302 time: 0.25s
Test loss: 0.3034 score: 0.9147 time: 0.25s
Epoch 147/1000, LR 0.000272
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.36s
Val loss: 0.2664 score: 0.9302 time: 0.23s
Test loss: 0.2999 score: 0.9147 time: 0.17s
Epoch 148/1000, LR 0.000272
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.25s
Val loss: 0.2630 score: 0.9302 time: 0.16s
Test loss: 0.2965 score: 0.9147 time: 0.16s
Epoch 149/1000, LR 0.000272
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.25s
Val loss: 0.2600 score: 0.9302 time: 0.16s
Test loss: 0.2933 score: 0.9147 time: 0.15s
Epoch 150/1000, LR 0.000271
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.25s
Val loss: 0.2569 score: 0.9302 time: 0.16s
Test loss: 0.2901 score: 0.9147 time: 0.16s
Epoch 151/1000, LR 0.000271
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.24s
Val loss: 0.2538 score: 0.9302 time: 0.16s
Test loss: 0.2869 score: 0.9147 time: 0.16s
Epoch 152/1000, LR 0.000271
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.24s
Val loss: 0.2510 score: 0.9302 time: 0.16s
Test loss: 0.2839 score: 0.9147 time: 0.16s
Epoch 153/1000, LR 0.000271
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.25s
Val loss: 0.2482 score: 0.9302 time: 0.17s
Test loss: 0.2810 score: 0.9147 time: 0.16s
Epoch 154/1000, LR 0.000271
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.30s
Val loss: 0.2455 score: 0.9302 time: 0.20s
Test loss: 0.2781 score: 0.9147 time: 0.16s
Epoch 155/1000, LR 0.000270
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.25s
Val loss: 0.2430 score: 0.9302 time: 0.25s
Test loss: 0.2754 score: 0.9147 time: 0.16s
Epoch 156/1000, LR 0.000270
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.25s
Val loss: 0.2403 score: 0.9302 time: 0.17s
Test loss: 0.2726 score: 0.9147 time: 0.17s
Epoch 157/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.26s
Val loss: 0.2376 score: 0.9302 time: 0.17s
Test loss: 0.2699 score: 0.9225 time: 0.17s
Epoch 158/1000, LR 0.000270
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.31s
Val loss: 0.2351 score: 0.9380 time: 0.17s
Test loss: 0.2674 score: 0.9225 time: 0.17s
Epoch 159/1000, LR 0.000270
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.25s
Val loss: 0.2331 score: 0.9380 time: 0.17s
Test loss: 0.2650 score: 0.9225 time: 0.17s
Epoch 160/1000, LR 0.000269
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.29s
Val loss: 0.2307 score: 0.9380 time: 0.17s
Test loss: 0.2626 score: 0.9225 time: 0.29s
Epoch 161/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.25s
Val loss: 0.2281 score: 0.9457 time: 0.16s
Test loss: 0.2602 score: 0.9225 time: 0.16s
Epoch 162/1000, LR 0.000269
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.25s
Val loss: 0.2260 score: 0.9457 time: 0.17s
Test loss: 0.2580 score: 0.9225 time: 0.17s
Epoch 163/1000, LR 0.000269
Train loss: 0.6536;  Loss pred: 0.6536; Loss self: 0.0000; time: 0.35s
Val loss: 0.2240 score: 0.9457 time: 0.17s
Test loss: 0.2558 score: 0.9225 time: 0.17s
Epoch 164/1000, LR 0.000269
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.26s
Val loss: 0.2221 score: 0.9457 time: 0.17s
Test loss: 0.2537 score: 0.9302 time: 0.17s
Epoch 165/1000, LR 0.000268
Train loss: 0.6482;  Loss pred: 0.6482; Loss self: 0.0000; time: 0.27s
Val loss: 0.2205 score: 0.9457 time: 0.17s
Test loss: 0.2518 score: 0.9225 time: 0.17s
Epoch 166/1000, LR 0.000268
Train loss: 0.6490;  Loss pred: 0.6490; Loss self: 0.0000; time: 0.35s
Val loss: 0.2190 score: 0.9457 time: 0.27s
Test loss: 0.2499 score: 0.9225 time: 0.17s
Epoch 167/1000, LR 0.000268
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.25s
Val loss: 0.2180 score: 0.9380 time: 0.16s
Test loss: 0.2484 score: 0.9225 time: 0.16s
Epoch 168/1000, LR 0.000268
Train loss: 0.6437;  Loss pred: 0.6437; Loss self: 0.0000; time: 0.25s
Val loss: 0.2161 score: 0.9380 time: 0.16s
Test loss: 0.2465 score: 0.9225 time: 0.16s
Epoch 169/1000, LR 0.000267
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.25s
Val loss: 0.2141 score: 0.9457 time: 0.16s
Test loss: 0.2446 score: 0.9302 time: 0.16s
Epoch 170/1000, LR 0.000267
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 0.27s
Val loss: 0.2125 score: 0.9457 time: 0.24s
Test loss: 0.2428 score: 0.9302 time: 0.23s
Epoch 171/1000, LR 0.000267
Train loss: 0.6397;  Loss pred: 0.6397; Loss self: 0.0000; time: 0.36s
Val loss: 0.2107 score: 0.9457 time: 0.24s
Test loss: 0.2411 score: 0.9302 time: 0.24s
Epoch 172/1000, LR 0.000267
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.34s
Val loss: 0.2089 score: 0.9457 time: 0.16s
Test loss: 0.2394 score: 0.9302 time: 0.16s
Epoch 173/1000, LR 0.000267
Train loss: 0.6335;  Loss pred: 0.6335; Loss self: 0.0000; time: 0.25s
Val loss: 0.2073 score: 0.9457 time: 0.16s
Test loss: 0.2378 score: 0.9302 time: 0.16s
Epoch 174/1000, LR 0.000266
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.25s
Val loss: 0.2059 score: 0.9457 time: 0.16s
Test loss: 0.2363 score: 0.9302 time: 0.16s
Epoch 175/1000, LR 0.000266
Train loss: 0.6335;  Loss pred: 0.6335; Loss self: 0.0000; time: 0.28s
Val loss: 0.2050 score: 0.9457 time: 0.23s
Test loss: 0.2350 score: 0.9302 time: 0.24s
Epoch 176/1000, LR 0.000266
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.36s
Val loss: 0.2035 score: 0.9457 time: 0.24s
Test loss: 0.2335 score: 0.9302 time: 0.24s
Epoch 177/1000, LR 0.000266
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.39s
Val loss: 0.2029 score: 0.9457 time: 0.23s
Test loss: 0.2325 score: 0.9302 time: 0.18s
Epoch 178/1000, LR 0.000265
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.25s
Val loss: 0.2023 score: 0.9457 time: 0.16s
Test loss: 0.2315 score: 0.9302 time: 0.16s
Epoch 179/1000, LR 0.000265
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.25s
Val loss: 0.2003 score: 0.9457 time: 0.16s
Test loss: 0.2299 score: 0.9302 time: 0.16s
Epoch 180/1000, LR 0.000265
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.25s
Val loss: 0.1993 score: 0.9457 time: 0.16s
Test loss: 0.2288 score: 0.9302 time: 0.16s
Epoch 181/1000, LR 0.000265
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.25s
Val loss: 0.1979 score: 0.9457 time: 0.16s
Test loss: 0.2276 score: 0.9302 time: 0.16s
Epoch 182/1000, LR 0.000265
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 0.25s
Val loss: 0.1972 score: 0.9457 time: 0.17s
Test loss: 0.2266 score: 0.9302 time: 0.18s
Epoch 183/1000, LR 0.000264
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.26s
Val loss: 0.1959 score: 0.9457 time: 0.23s
Test loss: 0.2255 score: 0.9302 time: 0.21s
Epoch 184/1000, LR 0.000264
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 0.26s
Val loss: 0.1943 score: 0.9380 time: 0.17s
Test loss: 0.2243 score: 0.9302 time: 0.17s
Epoch 185/1000, LR 0.000264
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.26s
Val loss: 0.1931 score: 0.9380 time: 0.17s
Test loss: 0.2232 score: 0.9302 time: 0.17s
Epoch 186/1000, LR 0.000264
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.25s
Val loss: 0.1919 score: 0.9380 time: 0.16s
Test loss: 0.2223 score: 0.9302 time: 0.16s
Epoch 187/1000, LR 0.000263
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.25s
Val loss: 0.1910 score: 0.9380 time: 0.16s
Test loss: 0.2214 score: 0.9302 time: 0.16s
Epoch 188/1000, LR 0.000263
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.24s
Val loss: 0.1910 score: 0.9457 time: 0.16s
Test loss: 0.2208 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.25s
Val loss: 0.1907 score: 0.9457 time: 0.16s
Test loss: 0.2202 score: 0.9302 time: 0.25s
Epoch 190/1000, LR 0.000263
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.26s
Val loss: 0.1901 score: 0.9457 time: 0.16s
Test loss: 0.2196 score: 0.9302 time: 0.16s
Epoch 191/1000, LR 0.000262
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.25s
Val loss: 0.1903 score: 0.9457 time: 0.16s
Test loss: 0.2193 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 192/1000, LR 0.000262
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.25s
Val loss: 0.1895 score: 0.9457 time: 0.16s
Test loss: 0.2186 score: 0.9302 time: 0.16s
Epoch 193/1000, LR 0.000262
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.24s
Val loss: 0.1882 score: 0.9457 time: 0.16s
Test loss: 0.2177 score: 0.9302 time: 0.18s
Epoch 194/1000, LR 0.000262
Train loss: 0.6059;  Loss pred: 0.6059; Loss self: 0.0000; time: 0.25s
Val loss: 0.1862 score: 0.9457 time: 0.16s
Test loss: 0.2165 score: 0.9302 time: 0.24s
Epoch 195/1000, LR 0.000261
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.33s
Val loss: 0.1850 score: 0.9457 time: 0.22s
Test loss: 0.2157 score: 0.9302 time: 0.16s
Epoch 196/1000, LR 0.000261
Train loss: 0.6037;  Loss pred: 0.6037; Loss self: 0.0000; time: 0.25s
Val loss: 0.1841 score: 0.9457 time: 0.16s
Test loss: 0.2151 score: 0.9302 time: 0.16s
Epoch 197/1000, LR 0.000261
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.25s
Val loss: 0.1838 score: 0.9457 time: 0.16s
Test loss: 0.2146 score: 0.9302 time: 0.28s
Epoch 198/1000, LR 0.000261
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.25s
Val loss: 0.1840 score: 0.9535 time: 0.16s
Test loss: 0.2144 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.25s
Val loss: 0.1842 score: 0.9535 time: 0.16s
Test loss: 0.2142 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.25s
Val loss: 0.1834 score: 0.9535 time: 0.19s
Test loss: 0.2136 score: 0.9302 time: 0.16s
Epoch 201/1000, LR 0.000260
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.32s
Val loss: 0.1830 score: 0.9535 time: 0.16s
Test loss: 0.2132 score: 0.9302 time: 0.16s
Epoch 202/1000, LR 0.000260
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.26s
Val loss: 0.1826 score: 0.9535 time: 0.16s
Test loss: 0.2128 score: 0.9302 time: 0.16s
Epoch 203/1000, LR 0.000259
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.25s
Val loss: 0.1818 score: 0.9535 time: 0.16s
Test loss: 0.2123 score: 0.9302 time: 0.16s
Epoch 204/1000, LR 0.000259
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.25s
Val loss: 0.1807 score: 0.9457 time: 0.18s
Test loss: 0.2116 score: 0.9302 time: 0.22s
Epoch 205/1000, LR 0.000259
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.36s
Val loss: 0.1799 score: 0.9457 time: 0.19s
Test loss: 0.2110 score: 0.9302 time: 0.21s
Epoch 206/1000, LR 0.000259
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.26s
Val loss: 0.1792 score: 0.9457 time: 0.16s
Test loss: 0.2105 score: 0.9302 time: 0.16s
Epoch 207/1000, LR 0.000258
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.25s
Val loss: 0.1790 score: 0.9457 time: 0.16s
Test loss: 0.2103 score: 0.9302 time: 0.16s
Epoch 208/1000, LR 0.000258
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.25s
Val loss: 0.1788 score: 0.9457 time: 0.16s
Test loss: 0.2100 score: 0.9302 time: 0.16s
Epoch 209/1000, LR 0.000258
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.25s
Val loss: 0.1784 score: 0.9457 time: 0.16s
Test loss: 0.2097 score: 0.9302 time: 0.16s
Epoch 210/1000, LR 0.000258
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.30s
Val loss: 0.1782 score: 0.9535 time: 0.23s
Test loss: 0.2096 score: 0.9302 time: 0.24s
Epoch 211/1000, LR 0.000257
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 0.34s
Val loss: 0.1780 score: 0.9535 time: 0.21s
Test loss: 0.2094 score: 0.9302 time: 0.16s
Epoch 212/1000, LR 0.000257
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.25s
Val loss: 0.1775 score: 0.9535 time: 0.16s
Test loss: 0.2090 score: 0.9302 time: 0.16s
Epoch 213/1000, LR 0.000257
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.25s
Val loss: 0.1774 score: 0.9535 time: 0.16s
Test loss: 0.2089 score: 0.9302 time: 0.16s
Epoch 214/1000, LR 0.000256
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.25s
Val loss: 0.1769 score: 0.9535 time: 0.16s
Test loss: 0.2086 score: 0.9302 time: 0.16s
Epoch 215/1000, LR 0.000256
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.26s
Val loss: 0.1769 score: 0.9535 time: 0.16s
Test loss: 0.2086 score: 0.9302 time: 0.16s
Epoch 216/1000, LR 0.000256
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.25s
Val loss: 0.1763 score: 0.9535 time: 0.16s
Test loss: 0.2082 score: 0.9302 time: 0.16s
Epoch 217/1000, LR 0.000256
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.25s
Val loss: 0.1756 score: 0.9457 time: 0.19s
Test loss: 0.2078 score: 0.9302 time: 0.20s
Epoch 218/1000, LR 0.000255
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.26s
Val loss: 0.1751 score: 0.9457 time: 0.16s
Test loss: 0.2075 score: 0.9302 time: 0.16s
Epoch 219/1000, LR 0.000255
Train loss: 0.5797;  Loss pred: 0.5797; Loss self: 0.0000; time: 0.25s
Val loss: 0.1746 score: 0.9457 time: 0.17s
Test loss: 0.2073 score: 0.9302 time: 0.17s
Epoch 220/1000, LR 0.000255
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.26s
Val loss: 0.1744 score: 0.9457 time: 0.17s
Test loss: 0.2072 score: 0.9302 time: 0.17s
Epoch 221/1000, LR 0.000255
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.25s
Val loss: 0.1748 score: 0.9535 time: 0.16s
Test loss: 0.2074 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.26s
Val loss: 0.1755 score: 0.9535 time: 0.16s
Test loss: 0.2079 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.36s
Val loss: 0.1755 score: 0.9535 time: 0.24s
Test loss: 0.2079 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.36s
Val loss: 0.1750 score: 0.9535 time: 0.16s
Test loss: 0.2077 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.5785;  Loss pred: 0.5785; Loss self: 0.0000; time: 0.26s
Val loss: 0.1740 score: 0.9535 time: 0.16s
Test loss: 0.2072 score: 0.9302 time: 0.16s
Epoch 226/1000, LR 0.000253
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.25s
Val loss: 0.1729 score: 0.9535 time: 0.16s
Test loss: 0.2067 score: 0.9302 time: 0.16s
Epoch 227/1000, LR 0.000253
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.26s
Val loss: 0.1723 score: 0.9535 time: 0.16s
Test loss: 0.2064 score: 0.9302 time: 0.16s
Epoch 228/1000, LR 0.000253
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.25s
Val loss: 0.1722 score: 0.9535 time: 0.16s
Test loss: 0.2064 score: 0.9302 time: 0.16s
Epoch 229/1000, LR 0.000252
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.26s
Val loss: 0.1721 score: 0.9535 time: 0.16s
Test loss: 0.2065 score: 0.9302 time: 0.16s
Epoch 230/1000, LR 0.000252
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.26s
Val loss: 0.1722 score: 0.9535 time: 0.22s
Test loss: 0.2066 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.25s
Val loss: 0.1725 score: 0.9457 time: 0.16s
Test loss: 0.2069 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 232/1000, LR 0.000251
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.25s
Val loss: 0.1730 score: 0.9535 time: 0.17s
Test loss: 0.2073 score: 0.9302 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 233/1000, LR 0.000251
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.35s
Val loss: 0.1729 score: 0.9535 time: 0.24s
Test loss: 0.2074 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 4 of 20
Epoch 234/1000, LR 0.000251
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.37s
Val loss: 0.1725 score: 0.9535 time: 0.24s
Test loss: 0.2074 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 235/1000, LR 0.000250
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.36s
Val loss: 0.1718 score: 0.9535 time: 0.20s
Test loss: 0.2071 score: 0.9302 time: 0.16s
Epoch 236/1000, LR 0.000250
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.25s
Val loss: 0.1710 score: 0.9535 time: 0.16s
Test loss: 0.2068 score: 0.9302 time: 0.16s
Epoch 237/1000, LR 0.000250
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.25s
Val loss: 0.1707 score: 0.9535 time: 0.16s
Test loss: 0.2069 score: 0.9302 time: 0.16s
Epoch 238/1000, LR 0.000250
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.25s
Val loss: 0.1705 score: 0.9535 time: 0.16s
Test loss: 0.2068 score: 0.9302 time: 0.16s
Epoch 239/1000, LR 0.000249
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.28s
Val loss: 0.1705 score: 0.9535 time: 0.16s
Test loss: 0.2070 score: 0.9302 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 240/1000, LR 0.000249
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.37s
Val loss: 0.1708 score: 0.9535 time: 0.25s
Test loss: 0.2074 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 241/1000, LR 0.000249
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.35s
Val loss: 0.1712 score: 0.9612 time: 0.17s
Test loss: 0.2078 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 242/1000, LR 0.000248
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.26s
Val loss: 0.1711 score: 0.9612 time: 0.17s
Test loss: 0.2079 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 243/1000, LR 0.000248
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.26s
Val loss: 0.1712 score: 0.9612 time: 0.17s
Test loss: 0.2082 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 5 of 20
Epoch 244/1000, LR 0.000248
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.32s
Val loss: 0.1715 score: 0.9612 time: 0.25s
Test loss: 0.2087 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 6 of 20
Epoch 245/1000, LR 0.000247
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.37s
Val loss: 0.1703 score: 0.9535 time: 0.25s
Test loss: 0.2080 score: 0.9302 time: 0.25s
Epoch 246/1000, LR 0.000247
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.34s
Val loss: 0.1695 score: 0.9535 time: 0.18s
Test loss: 0.2077 score: 0.9302 time: 0.17s
Epoch 247/1000, LR 0.000247
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.25s
Val loss: 0.1690 score: 0.9457 time: 0.16s
Test loss: 0.2075 score: 0.9302 time: 0.16s
Epoch 248/1000, LR 0.000247
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.25s
Val loss: 0.1683 score: 0.9457 time: 0.23s
Test loss: 0.2073 score: 0.9302 time: 0.16s
Epoch 249/1000, LR 0.000246
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.25s
Val loss: 0.1680 score: 0.9457 time: 0.17s
Test loss: 0.2073 score: 0.9302 time: 0.17s
Epoch 250/1000, LR 0.000246
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.26s
Val loss: 0.1682 score: 0.9457 time: 0.17s
Test loss: 0.2076 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 251/1000, LR 0.000246
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.34s
Val loss: 0.1684 score: 0.9457 time: 0.16s
Test loss: 0.2079 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 252/1000, LR 0.000245
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.26s
Val loss: 0.1690 score: 0.9535 time: 0.22s
Test loss: 0.2084 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 253/1000, LR 0.000245
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.25s
Val loss: 0.1694 score: 0.9535 time: 0.16s
Test loss: 0.2089 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 254/1000, LR 0.000245
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.33s
Val loss: 0.1696 score: 0.9612 time: 0.17s
Test loss: 0.2093 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 255/1000, LR 0.000244
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.26s
Val loss: 0.1690 score: 0.9535 time: 0.17s
Test loss: 0.2090 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 256/1000, LR 0.000244
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.26s
Val loss: 0.1685 score: 0.9535 time: 0.17s
Test loss: 0.2089 score: 0.9302 time: 0.26s
     INFO: Early stopping counter 7 of 20
Epoch 257/1000, LR 0.000244
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.30s
Val loss: 0.1680 score: 0.9457 time: 0.23s
Test loss: 0.2087 score: 0.9302 time: 0.18s
Epoch 258/1000, LR 0.000243
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.26s
Val loss: 0.1679 score: 0.9457 time: 0.16s
Test loss: 0.2088 score: 0.9302 time: 0.16s
Epoch 259/1000, LR 0.000243
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.26s
Val loss: 0.1677 score: 0.9457 time: 0.16s
Test loss: 0.2089 score: 0.9302 time: 0.15s
Epoch 260/1000, LR 0.000243
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.25s
Val loss: 0.1679 score: 0.9457 time: 0.16s
Test loss: 0.2093 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 261/1000, LR 0.000242
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.25s
Val loss: 0.1674 score: 0.9457 time: 0.17s
Test loss: 0.2091 score: 0.9302 time: 0.17s
Epoch 262/1000, LR 0.000242
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.26s
Val loss: 0.1678 score: 0.9457 time: 0.19s
Test loss: 0.2096 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 263/1000, LR 0.000242
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.32s
Val loss: 0.1680 score: 0.9535 time: 0.16s
Test loss: 0.2100 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 264/1000, LR 0.000241
Train loss: 0.5508;  Loss pred: 0.5508; Loss self: 0.0000; time: 0.24s
Val loss: 0.1683 score: 0.9535 time: 0.16s
Test loss: 0.2104 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 265/1000, LR 0.000241
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.32s
Val loss: 0.1689 score: 0.9612 time: 0.25s
Test loss: 0.2111 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 266/1000, LR 0.000241
Train loss: 0.5508;  Loss pred: 0.5508; Loss self: 0.0000; time: 0.26s
Val loss: 0.1686 score: 0.9535 time: 0.17s
Test loss: 0.2111 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 267/1000, LR 0.000241
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.26s
Val loss: 0.1679 score: 0.9535 time: 0.17s
Test loss: 0.2108 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 268/1000, LR 0.000240
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.25s
Val loss: 0.1672 score: 0.9457 time: 0.17s
Test loss: 0.2105 score: 0.9302 time: 0.16s
Epoch 269/1000, LR 0.000240
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.26s
Val loss: 0.1668 score: 0.9457 time: 0.17s
Test loss: 0.2105 score: 0.9302 time: 0.17s
Epoch 270/1000, LR 0.000240
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.29s
Val loss: 0.1668 score: 0.9457 time: 0.17s
Test loss: 0.2106 score: 0.9302 time: 0.22s
Epoch 271/1000, LR 0.000239
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.29s
Val loss: 0.1667 score: 0.9457 time: 0.16s
Test loss: 0.2107 score: 0.9302 time: 0.16s
Epoch 272/1000, LR 0.000239
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.25s
Val loss: 0.1666 score: 0.9457 time: 0.17s
Test loss: 0.2109 score: 0.9302 time: 0.16s
Epoch 273/1000, LR 0.000239
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.25s
Val loss: 0.1672 score: 0.9457 time: 0.16s
Test loss: 0.2116 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 274/1000, LR 0.000238
Train loss: 0.5490;  Loss pred: 0.5490; Loss self: 0.0000; time: 0.28s
Val loss: 0.1672 score: 0.9457 time: 0.24s
Test loss: 0.2118 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 275/1000, LR 0.000238
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 0.36s
Val loss: 0.1671 score: 0.9457 time: 0.17s
Test loss: 0.2120 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 276/1000, LR 0.000238
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.26s
Val loss: 0.1671 score: 0.9457 time: 0.17s
Test loss: 0.2122 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 277/1000, LR 0.000237
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.26s
Val loss: 0.1670 score: 0.9457 time: 0.17s
Test loss: 0.2123 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 278/1000, LR 0.000237
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.26s
Val loss: 0.1666 score: 0.9457 time: 0.17s
Test loss: 0.2122 score: 0.9302 time: 0.16s
Epoch 279/1000, LR 0.000236
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.27s
Val loss: 0.1663 score: 0.9457 time: 0.21s
Test loss: 0.2122 score: 0.9302 time: 0.23s
Epoch 280/1000, LR 0.000236
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.37s
Val loss: 0.1663 score: 0.9457 time: 0.23s
Test loss: 0.2125 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 281/1000, LR 0.000236
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.26s
Val loss: 0.1672 score: 0.9457 time: 0.17s
Test loss: 0.2134 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 282/1000, LR 0.000235
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.26s
Val loss: 0.1675 score: 0.9535 time: 0.17s
Test loss: 0.2139 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 283/1000, LR 0.000235
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.24s
Val loss: 0.1675 score: 0.9535 time: 0.16s
Test loss: 0.2141 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 284/1000, LR 0.000235
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.24s
Val loss: 0.1672 score: 0.9457 time: 0.16s
Test loss: 0.2141 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 285/1000, LR 0.000234
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.25s
Val loss: 0.1671 score: 0.9457 time: 0.17s
Test loss: 0.2143 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 286/1000, LR 0.000234
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.26s
Val loss: 0.1666 score: 0.9457 time: 0.18s
Test loss: 0.2142 score: 0.9302 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 287/1000, LR 0.000234
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.34s
Val loss: 0.1664 score: 0.9457 time: 0.18s
Test loss: 0.2142 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 288/1000, LR 0.000233
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.26s
Val loss: 0.1666 score: 0.9457 time: 0.18s
Test loss: 0.2146 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 289/1000, LR 0.000233
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.25s
Val loss: 0.1663 score: 0.9457 time: 0.16s
Test loss: 0.2147 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 290/1000, LR 0.000233
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.26s
Val loss: 0.1667 score: 0.9457 time: 0.16s
Test loss: 0.2153 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 291/1000, LR 0.000232
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.25s
Val loss: 0.1665 score: 0.9457 time: 0.16s
Test loss: 0.2153 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 292/1000, LR 0.000232
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.27s
Val loss: 0.1665 score: 0.9457 time: 0.16s
Test loss: 0.2156 score: 0.9302 time: 0.24s
     INFO: Early stopping counter 13 of 20
Epoch 293/1000, LR 0.000232
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.36s
Val loss: 0.1667 score: 0.9457 time: 0.22s
Test loss: 0.2160 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 294/1000, LR 0.000231
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.26s
Val loss: 0.1670 score: 0.9457 time: 0.16s
Test loss: 0.2165 score: 0.9302 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 295/1000, LR 0.000231
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.26s
Val loss: 0.1670 score: 0.9457 time: 0.17s
Test loss: 0.2168 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 296/1000, LR 0.000231
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 0.25s
Val loss: 0.1675 score: 0.9457 time: 0.17s
Test loss: 0.2174 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 297/1000, LR 0.000230
Train loss: 0.5406;  Loss pred: 0.5406; Loss self: 0.0000; time: 0.26s
Val loss: 0.1670 score: 0.9457 time: 0.17s
Test loss: 0.2172 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 298/1000, LR 0.000230
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.26s
Val loss: 0.1667 score: 0.9457 time: 0.17s
Test loss: 0.2172 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 299/1000, LR 0.000230
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.25s
Val loss: 0.1666 score: 0.9457 time: 0.22s
Test loss: 0.2173 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 278,   Train_Loss: 0.5493,   Val_Loss: 0.1663,   Val_Precision: 0.9677,   Val_Recall: 0.9231,   Val_accuracy: 0.9449,   Val_Score: 0.9457,   Val_Loss: 0.1663,   Test_Precision: 0.9508,   Test_Recall: 0.9062,   Test_accuracy: 0.9280,   Test_Score: 0.9302,   Test_loss: 0.2122


[0.16673445515334606, 0.16456928104162216, 0.16310806293040514, 0.16728525212965906, 0.1854744099546224, 0.16258475300855935, 0.16449093399569392, 0.1610368660185486, 0.16231747902929783, 0.1652295389212668, 0.16330186813138425, 0.16176578402519226, 0.16232501482591033, 0.16171461204066873, 0.16289166919887066, 0.16123372013680637, 0.22038643597625196, 0.16740042599849403, 0.16435677302069962, 0.16419110400602221, 0.16313734301365912, 0.2006287630647421, 0.16585547616705298, 0.16315406118519604, 0.16169284307397902, 0.16263928497210145, 0.1612458899617195, 0.25024853996001184, 0.16360940993763506, 0.1635465060826391, 0.17455954803153872, 0.17379526305012405, 0.1774173858575523, 0.1780657118652016, 0.17183967703022063, 0.1667702291160822, 0.17278969404287636, 0.24313197215087712, 0.2431589609477669, 0.16836075694300234, 0.16478207614272833, 0.16209386009722948, 0.16404796205461025, 0.16202021902427077, 0.16177891404367983, 0.16372482618317008, 0.24734427803196013, 0.17053176905028522, 0.17116316105239093, 0.17020678101107478, 0.17008514003828168, 0.18675795919261873, 0.16963822301477194, 0.17098320205695927, 0.16924341884441674, 0.24730027792975307, 0.24726353911682963, 0.24728002795018256, 0.17665820382535458, 0.16417790786363184, 0.15995282703079283, 0.16107823699712753, 0.16283482708968222, 0.16100223013199866, 0.24648205284029245, 0.16518020280636847, 0.16574255004525185, 0.16364169493317604, 0.1634306670166552, 0.18196568288840353, 0.25050408602692187, 0.16633846308104694, 0.16594198206439614, 0.2410559670533985, 0.238903874065727, 0.23480591690167785, 0.23885572399012744, 0.1622869479469955, 0.16099695907905698, 0.15951298992149532, 0.1608885859604925, 0.23039979697205126, 0.17414131900295615, 0.17543383804149926, 0.1734831149224192, 0.1755000320263207, 0.1769772320985794, 0.19987277686595917, 0.17824881384149194, 0.2537408887874335, 0.1587942228652537, 0.17284859088249505, 0.170234713004902, 0.20563827897422016, 0.17523267911747098, 0.1744306841865182, 0.17912659794092178, 0.1765528260730207, 0.1783474951516837, 0.17473685811273754, 0.2670305559877306, 0.1796403299085796, 0.17467937106266618, 0.17199443420395255, 0.20603811112232506, 0.17141464701853693, 0.17427313211373985, 0.17369821411557496, 0.17368879891000688, 0.1708255368284881, 0.17318927706219256, 0.16062364005483687, 0.16172748617827892, 0.16984039498493075, 0.163166674785316, 0.16084606805816293, 0.16557894297875464, 0.24426717194728553, 0.16072220914065838, 0.1615866729989648, 0.16133025102317333, 0.16248368099331856, 0.1627005022019148, 0.24945938400924206, 0.16117874020710588, 0.16261339001357555, 0.23891933006234467, 0.24097934109158814, 0.24107169592753053, 0.2410680609755218, 0.16411066986620426, 0.16325514111667871, 0.16464215703308582, 0.16869658906944096, 0.16394772892817855, 0.16140951309353113, 0.16321560298092663, 0.17311541503295302, 0.24683682108297944, 0.16691843792796135, 0.16391977085731924, 0.166302930098027, 0.16155808698385954, 0.19306765589863062, 0.2210617740638554, 0.16319593507796526, 0.16402410087175667, 0.16452863113954663, 0.16444694600068033, 0.16804808005690575, 0.16577738290652633, 0.16188615886494517, 0.1622601628769189, 0.16385134286247194, 0.16344994003884494, 0.16524494998157024, 0.1916167119052261, 0.19729590509086847, 0.163780887844041, 0.1637106619309634, 0.15956483595073223, 0.15940011502243578, 0.15896479110233486, 0.19179784203879535, 0.17009773710742593, 0.1625336620490998, 0.17425616993568838, 0.17220639204606414, 0.1706231569405645, 0.28065608092583716, 0.2630470939911902, 0.17132195108570158, 0.17073519085533917, 0.1725540030747652, 0.17394615686498582, 0.1739957311656326, 0.17670407705008984, 0.1814715799409896, 0.1613593699876219, 0.16336485999636352, 0.2060081190429628, 0.16762209497392178, 0.17296730307862163, 0.24097197782248259, 0.15683247707784176, 0.16881524794735014, 0.1695977149065584, 0.1692938890773803, 0.17357272910885513, 0.1701673110947013, 0.17574759596027434, 0.24892814899794757, 0.2368547220248729, 0.2509191329590976, 0.15953431907109916, 0.17361875507049263, 0.17254383605904877, 0.1742519869003445, 0.17292288807220757, 0.2602621139958501, 0.17514411895535886, 0.17415130510926247, 0.17506522499024868, 0.1746231650467962, 0.248841772088781, 0.25150099699385464, 0.17336963396519423, 0.17390950908884406, 0.1745272008702159, 0.1754064450506121, 0.23269196599721909, 0.17278197989799082, 0.17422183020971715, 0.17253445112146437, 0.23262266186065972, 0.1604927929583937, 0.1731957730371505, 0.17134184786118567, 0.17323128902353346, 0.17126029008068144, 0.17884608497843146, 0.17051923903636634, 0.16904838802292943, 0.17149172793142498, 0.17118925508111715, 0.17215807596221566, 0.18798734992742538, 0.23187255300581455, 0.1581243530381471, 0.16271990607492626, 0.1616752331610769, 0.1589968439657241, 0.15896394406445324, 0.16094659105874598, 0.16214475990273058, 0.15829796390607953, 0.1598516891244799, 0.16099763801321387, 0.161884669912979, 0.16178100206889212, 0.16185220913030207, 0.16887550405226648, 0.16910325293429196, 0.16943069407716393, 0.16744819493032992, 0.169359324965626, 0.1697199430782348, 0.16995688807219267, 0.1671690831426531, 0.1681958541739732, 0.16799198207445443, 0.1701674209907651, 0.16828978690318763, 0.17307977005839348, 0.17272892384789884, 0.16932283085770905, 0.24311852105893195, 0.24518223712220788, 0.2457512009423226, 0.16916287899948657, 0.16700354497879744, 0.18392588989809155, 0.17081045894883573, 0.15663270303048193, 0.16073436895385385, 0.1584344170987606, 0.15969394287094474, 0.15999513887800276, 0.23707106802612543, 0.15953163197264075, 0.16383772296831012, 0.1586746519897133, 0.1619646770413965, 0.15816990099847317, 0.19805173994973302, 0.16098128212615848, 0.20877147209830582, 0.15866114292293787, 0.16143442783504725, 0.2575265569612384, 0.16189939505420625, 0.16271735378541052, 0.16220943885855377, 0.1684341449290514, 0.15835127118043602, 0.23612410598434508, 0.16059105680324137, 0.15998438000679016, 0.160298191010952, 0.16003166395239532, 0.16142815491184592, 0.1600618369411677, 0.1641201509628445, 0.16021832101978362, 0.1623217931482941, 0.2347525330260396, 0.23891248414292932, 0.23895561299286783, 0.16086901794187725, 0.17375208181329072, 0.17311525694094598, 0.17208570684306324, 0.17242141184397042, 0.230367325944826, 0.17220130492933095, 0.17579775606282055, 0.16982773807831109, 0.17308843811042607, 0.17223581206053495, 0.17443761392496526, 0.17192080500535667, 0.17597970296628773, 0.17497563315555453, 0.17042878991924226, 0.17136186105199158, 0.17018930078484118, 0.17208680184558034, 0.19066068599931896, 0.2576312171295285, 0.17113644001074135, 0.1758872801437974, 0.24945866386406124, 0.243104028981179, 0.1631897590123117, 0.1616576388478279, 0.16096236393786967, 0.15849255095236003, 0.16133472113870084, 0.1804954840335995, 0.24108296702615917, 0.1612322621513158, 0.16472438303753734, 0.16256156493909657, 0.1717379419133067, 0.15852960897609591, 0.1831769437994808, 0.22162948013283312, 0.15927423513494432, 0.1558688881341368, 0.15978833101689816, 0.15823188005015254, 0.1579428520053625, 0.19008617103099823, 0.1665216840337962, 0.169184128055349, 0.1677500088699162, 0.16859405091963708, 0.17007544194348156, 0.17211860581301153, 0.1743805839214474, 0.16388750518672168, 0.16252768598496914, 0.1723189519252628, 0.2389598300214857, 0.23892405210062861, 0.25364511902444065, 0.17177730496041477, 0.17345076194033027, 0.17354851216077805, 0.1717924859840423, 0.1751652779057622, 0.2507676729001105, 0.17339131888002157, 0.17571793403476477, 0.1716889429371804, 0.17099507385864854, 0.1774403378367424, 0.1647659360896796, 0.1593880879227072, 0.15944395097903907, 0.15966833592392504, 0.16064104321412742, 0.1612316770479083, 0.19079579995013773, 0.17607789696194232, 0.16891566989943385, 0.1750107710249722, 0.16356977191753685, 0.16740774596109986, 0.18981378502212465, 0.16107580298557878, 0.1601741500198841, 0.17065494507551193, 0.16975813498720527, 0.17093553021550179, 0.1761766979470849, 0.16950825601816177, 0.1713663088157773, 0.169889273121953, 0.24730916414409876, 0.256103539140895, 0.17464125994592905, 0.1708532760385424, 0.16210092208348215, 0.16917612100951374, 0.17337071802467108, 0.1767740671057254, 0.24940966512076557, 0.23081090115010738, 0.17405863315798342, 0.16649482608772814, 0.1688618310727179, 0.164355002110824, 0.16415038495324552, 0.16428400808945298, 0.16512308199889958, 0.16476266994141042, 0.17135234689339995, 0.16790883196517825, 0.18116962118074298, 0.17883325391449034, 0.1810827141162008, 0.1901224220637232, 0.18171323789283633, 0.1802282240241766, 0.17559347511269152, 0.17839069198817015, 0.17829548986628652, 0.1750321858562529, 0.2468140630517155, 0.16364127304404974, 0.16343534598127007, 0.16307496395893395, 0.1647448791190982, 0.16525375307537615, 0.24752583098597825, 0.16705505596473813, 0.16473806812427938, 0.2410778529010713, 0.18862594314850867, 0.16661052918061614, 0.16375418310053647, 0.16752792103216052, 0.16692447289824486, 0.16535185393877327, 0.16664017387665808, 0.2155302760656923, 0.16221859701909125, 0.16443065297789872, 0.1641415529884398, 0.16370138805359602, 0.1830015000887215, 0.1762602769304067, 0.17266172403469682, 0.16418603179045022, 0.17660178104415536, 0.24102311115711927, 0.24108548206277192, 0.16248807404190302, 0.16407914995215833, 0.16183790611103177, 0.16256956686265767, 0.17205820814706385, 0.16284006112255156, 0.16346428613178432, 0.16467402898706496, 0.16198178101330996, 0.1605974230915308, 0.21741261892020702, 0.2617416582070291, 0.1776915800292045, 0.17208029609173536, 0.17481552390381694, 0.16275116009637713, 0.16287175798788667, 0.16098585398867726, 0.23127771192230284, 0.18096888600848615, 0.17368410900235176, 0.17536227498203516, 0.17370149493217468, 0.23720170906744897, 0.17381148412823677, 0.17280369601212442, 0.16141074593178928, 0.16546714096330106, 0.16297501185908914, 0.16375897591933608, 0.1621641900856048, 0.19778229715302587, 0.21885895705781877, 0.16847763094119728, 0.16282908688299358, 0.16837679990567267, 0.166052880929783, 0.2387350860517472, 0.25732376403175294, 0.16509714303538203, 0.17749909195117652, 0.17862635990604758, 0.1736182440072298, 0.15897565498016775, 0.18826975696720183, 0.17487751808948815, 0.1712487330660224, 0.16279774601571262, 0.16994708799757063, 0.17049203114584088, 0.16979282395914197, 0.1736646539065987, 0.17157040210440755, 0.17254859814420342, 0.17497989302501082, 0.16135429311543703, 0.1617063540033996, 0.16267890809103847, 0.15888622100465, 0.15807448886334896, 0.1598067400045693, 0.1604998221155256, 0.16149828187189996, 0.21723609301261604, 0.2254325549583882, 0.16265815403312445, 0.16318596107885242, 0.1718410470057279, 0.1743388210888952, 0.17692090710625052, 0.1743997090961784, 0.16565414192155004, 0.24121814197860658, 0.24310696590691805, 0.24104341282509267, 0.16223270003683865, 0.15936532290652394, 0.16481897118501365, 0.16063545388169587, 0.15736756590195, 0.16233529103919864, 0.24675887892954051, 0.15728044090792537, 0.16079750307835639, 0.15780002204701304, 0.16606757906265557, 0.24943309812806547, 0.17221565614454448, 0.17537936894223094, 0.17829825612716377, 0.17790100304409862, 0.1783367448952049, 0.18004451296292245, 0.17627694085240364, 0.1765715009532869, 0.16466286219656467, 0.17785028787329793, 0.2514760799240321, 0.17421870585530996, 0.16244247183203697, 0.15878531197085977, 0.16133533790707588, 0.16131596895866096, 0.16570395999588072, 0.16626632399857044, 0.1631852958817035, 0.16170855308882892, 0.17173197399824858, 0.16999052790924907, 0.1728052489925176, 0.17598024220205843, 0.29934531915932894, 0.16295237699523568, 0.17316752998158336, 0.17295694816857576, 0.1716442711185664, 0.17833743407391012, 0.17030881205573678, 0.16349403886124492, 0.16174765187315643, 0.16205291007645428, 0.23893697396852076, 0.24313733400776982, 0.1670566878747195, 0.1668616971001029, 0.16194932512007654, 0.24106903793290257, 0.24315106496214867, 0.1866615079343319, 0.16470642108470201, 0.16159570892341435, 0.16192066087387502, 0.16217397898435593, 0.18667100020684302, 0.20992818707600236, 0.1719612991437316, 0.17256721202284098, 0.16307432879693806, 0.1621054532006383, 0.1641257160808891, 0.2503877968993038, 0.16504646511748433, 0.16995246685110033, 0.1632173580583185, 0.18864622386172414, 0.24009561305865645, 0.16459321207366884, 0.16520797717384994, 0.2867928429041058, 0.16461677802726626, 0.16673330683261156, 0.16530599701218307, 0.16213607299141586, 0.1648485001642257, 0.16362702008336782, 0.22274516592733562, 0.21268493309617043, 0.1634331988170743, 0.16266111796721816, 0.164705729810521, 0.1691006028559059, 0.24107574485242367, 0.1684564189054072, 0.16360006295144558, 0.1643856498412788, 0.1625266200862825, 0.16257420601323247, 0.16898503713309765, 0.20165096316486597, 0.16238805605098605, 0.1711464950349182, 0.17152575915679336, 0.1628425382077694, 0.240286833839491, 0.24096106202341616, 0.1629386639688164, 0.16256785090081394, 0.16398216295056045, 0.16300541698001325, 0.16379624791443348, 0.1681920390110463, 0.18941429909318686, 0.16135260998271406, 0.20327952387742698, 0.24102614703588188, 0.24314937484450638, 0.16887061297893524, 0.16532342205755413, 0.16175013897009194, 0.1608109879307449, 0.23871600511483848, 0.2406422810163349, 0.17303253896534443, 0.17204950214363635, 0.1837823479436338, 0.24937217496335506, 0.25149041693657637, 0.1720238020643592, 0.16527765709906816, 0.1640111079905182, 0.17719968501478434, 0.17875670501962304, 0.1840198328718543, 0.1647827229462564, 0.16082960204221308, 0.17042830912396312, 0.17309604189358652, 0.2676805721130222, 0.1852235421538353, 0.15961045399308205, 0.15867438889108598, 0.1606872829142958, 0.17381764901801944, 0.16778392414562404, 0.15818286407738924, 0.1591396329458803, 0.17828887118957937, 0.17365563288331032, 0.1717115119099617, 0.16922263707965612, 0.16971219098195434, 0.22702453681267798, 0.16417380003258586, 0.168931056978181, 0.1661837708670646, 0.24940092000178993, 0.17225202801637352, 0.17163917003199458, 0.17075011297129095, 0.167231862898916, 0.2390116211026907, 0.16484631202183664, 0.17242020508274436, 0.16966090979985893, 0.16363283386453986, 0.16660419409163296, 0.17248506797477603, 0.18009490007534623, 0.17913755285553634, 0.16950641991570592, 0.16305862902663648, 0.16401974391192198, 0.16306634014472365, 0.2397251680959016, 0.18958914908580482, 0.16457030503079295, 0.1722361040301621, 0.17342716199345887, 0.1735893569421023, 0.17439891514368355, 0.17948106909170747]
[0.0012925151562274888, 0.0012757308607877686, 0.0012644035886077917, 0.0012967849002299152, 0.0014377861236792434, 0.0012603469225469716, 0.0012751235193464645, 0.0012483477985934, 0.0012582750312348668, 0.0012808491389245489, 0.0012659059545068547, 0.0012539983257766841, 0.0012583334482628708, 0.001253601643726114, 0.0012627261178207029, 0.0012498737995101268, 0.0017084219843120307, 0.0012976777209185583, 0.0012740835117883691, 0.0012727992558606374, 0.0012646305659973576, 0.0015552617291840472, 0.0012857013656360696, 0.0012647601642263259, 0.001253432892046349, 0.001260769650946523, 0.0012499681392381357, 0.001939911162480712, 0.0012682899995165508, 0.0012678023727336364, 0.0013531747909421607, 0.0013472501011637522, 0.001375328572539165, 0.0013803543555441983, 0.0013320905196141134, 0.001292792473768079, 0.001339454992580437, 0.0018847439701618382, 0.0018849531856416038, 0.0013051221468449795, 0.0012773804352149483, 0.0012565415511413137, 0.001271689628330312, 0.0012559706901106261, 0.0012541001088657352, 0.0012691846990943417, 0.0019173975041237219, 0.0013219516980642265, 0.0013268462097084568, 0.0013194324109385641, 0.0013184894576610982, 0.0014477361177722382, 0.0013150249846106351, 0.0013254511787361184, 0.0013119644871660212, 0.0019170564180601012, 0.001916771621060695, 0.0019168994414742835, 0.0013694434405066246, 0.0012726969601831925, 0.0012399443955875414, 0.001248668503853702, 0.001262285481315366, 0.0012480793033488268, 0.0019107135879092437, 0.001280466688421461, 0.0012848259693430376, 0.0012685402707998143, 0.0012669043954779474, 0.0014105866890573918, 0.0019418921397435804, 0.001289445450240674, 0.001286371953987567, 0.001868650907390686, 0.0018519680160133875, 0.0018202009062145569, 0.001851594759613391, 0.0012580383561782596, 0.0012480384424733099, 0.001236534805592987, 0.0012471983407790115, 0.0017860449377678392, 0.0013499327054492724, 0.001359952232879839, 0.0013448303482358076, 0.0013604653645451217, 0.0013719165278959643, 0.001549401371053947, 0.0013817737507092398, 0.0019669836340111125, 0.001230962967947703, 0.0013399115572286437, 0.0013196489380224961, 0.001594095185846668, 0.001358392861375744, 0.0013521758464071177, 0.0013885782786117967, 0.0013686265587055867, 0.001382538722106075, 0.00135454928769564, 0.002070004309982408, 0.0013925606969657333, 0.0013541036516485751, 0.0013332901876275392, 0.001597194659862985, 0.0013287957133219918, 0.0013509545125096113, 0.0013464977838416664, 0.0013464247977519913, 0.0013242289676626984, 0.0013425525353658338, 0.0012451444965491231, 0.0012537014432424723, 0.0013165922091855097, 0.001264857944072217, 0.001246868744636922, 0.0012835576975097258, 0.0018935439685836089, 0.0012459085979896, 0.0012526098682090294, 0.001250622100954832, 0.001259563418552857, 0.0012612442031156184, 0.0019337936744902484, 0.0012494475985046966, 0.001260568914833919, 0.0018520878299406564, 0.0018680569076867297, 0.0018687728366475235, 0.0018687446587249753, 0.0012721757353969323, 0.0012655437295866568, 0.0012762957909541537, 0.001307725496662333, 0.0012709126273502214, 0.0012512365356087685, 0.001265237232410284, 0.0013419799614957598, 0.0019134637293254221, 0.0012939413792865222, 0.0012706958981187537, 0.0012891700007598992, 0.0012523882711927097, 0.0014966484953382218, 0.0017136571632857009, 0.0012650847680462424, 0.0012715046579205944, 0.0012754157452678034, 0.0012747825271370568, 0.0013026982950147732, 0.00128509599152346, 0.0012549314640693423, 0.0012578307199761155, 0.001270165448546294, 0.001267053798750736, 0.0012809686045082965, 0.001485400867482373, 0.0015294256208594455, 0.0012696192856127208, 0.0012690748986896388, 0.001236936712796374, 0.0012356598063754712, 0.0012322852023436812, 0.0014868049770449252, 0.001318587109359891, 0.0012599508685976729, 0.0013508230227572743, 0.0013349332716749158, 0.0013226601313222055, 0.0021756285343088153, 0.0020391247596216293, 0.0013280771401992372, 0.001323528611281699, 0.0013376279308121334, 0.0013484198206588048, 0.0013488041175630434, 0.0013697990468999211, 0.0014067564336510824, 0.00125084782936141, 0.0012663942635377018, 0.0015969621631237425, 0.001299396085069161, 0.0013408318068110203, 0.0018679998280812604, 0.0012157556362623393, 0.001308645332925195, 0.0013147109682678944, 0.0013123557292820179, 0.0013455250318515901, 0.0013191264425945837, 0.0013623844648083282, 0.0019296755736275006, 0.001836083116471883, 0.0019451095578224623, 0.001236700147837978, 0.001345881822251881, 0.0013375491167368122, 0.0013507905961267015, 0.0013404875044357176, 0.0020175357674096905, 0.0013577063484911539, 0.0013500101171260656, 0.001357094767366269, 0.0013536679460991953, 0.0019290059851843487, 0.0019496201317353073, 0.0013439506508929784, 0.001348135729370884, 0.0013529240377536116, 0.001359739884113272, 0.001803813689900923, 0.0013393951930076808, 0.0013505568233311408, 0.0013374763652826695, 0.0018032764485322458, 0.0012441301779720442, 0.0013426028917608566, 0.001328231378768881, 0.0013428782094847555, 0.0013275991479122593, 0.0013864037595227245, 0.0013218545661733826, 0.0013104526203327864, 0.0013293932397784882, 0.001327048489000908, 0.0013345587283892686, 0.0014572662785071735, 0.0017974616512078647, 0.0012257701785902875, 0.0012613946207358626, 0.0012532963810936194, 0.0012325336741529, 0.0012322786361585523, 0.0012476479927034571, 0.0012569361232769812, 0.0012271159992719343, 0.0012391603808099216, 0.0012480437055287898, 0.0012549199218060388, 0.0012541162951076909, 0.001254668287831799, 0.0013091124345136936, 0.0013108779297231934, 0.0013134162331563096, 0.0012980480227157358, 0.0013128629842296589, 0.0013156584734746882, 0.0013174952563735865, 0.0012958843654469233, 0.0013038438308059937, 0.0013022634269337552, 0.001319127294502055, 0.0013045719914975785, 0.0013417036438635153, 0.0013389839057976654, 0.0013125800841682872, 0.0018846396981312553, 0.001900637497071379, 0.0019050480693203304, 0.0013113401472828416, 0.0012946011238666468, 0.001425782092233268, 0.0013241120848746955, 0.001214207000236294, 0.0012460028601073941, 0.0012281737759593846, 0.0012379375416352305, 0.001240272394403122, 0.0018377602172567863, 0.0012366793176173702, 0.0012700598679713962, 0.0012300360619357622, 0.0012555401321038488, 0.0012261232635540555, 0.0015352848058118838, 0.0012479169157066548, 0.0016183835046380296, 0.0012299313404878906, 0.001251429673139901, 0.0019963298989243284, 0.0012550340701876453, 0.001261374835545818, 0.0012574375105314246, 0.0013056910459616387, 0.0012275292339568684, 0.001830419426235233, 0.0012448919132034214, 0.001240188992300699, 0.0012426216357438139, 0.0012405555345146924, 0.001251381045828263, 0.0012407894336524629, 0.0012722492322701123, 0.0012420024885254543, 0.0012583084740177837, 0.0018197870777212372, 0.0018520347607979017, 0.0018523690929679677, 0.0012470466507122266, 0.0013469153628937265, 0.0013419787359763254, 0.0013339977274656064, 0.0013366000918137242, 0.0017857932243784962, 0.00133489383666148, 0.0013627733028125624, 0.0013164940936303184, 0.0013417708380653183, 0.0013351613338025965, 0.0013522295653098082, 0.0013327194186461757, 0.001364183743924711, 0.0013564002570198026, 0.0013211534102266843, 0.0013283865197828806, 0.0013192969053088463, 0.0013340062158572119, 0.001477989813948209, 0.0019971412180583605, 0.0013266390698507082, 0.001363467287936414, 0.0019337880919694669, 0.001884527356443248, 0.0012650368915683077, 0.0012531599910684335, 0.001247770263084261, 0.001228624425987287, 0.0012506567530131847, 0.0013991897987100737, 0.0018688602095051098, 0.0012498624972970217, 0.0012769332018413746, 0.001260167170070516, 0.0013313018752969513, 0.0012289116974891156, 0.001419976308523107, 0.001718057985525838, 0.001234683993294142, 0.0012082859545281923, 0.001238669232689133, 0.0012266037213190118, 0.0012243631938400196, 0.0014735362095426219, 0.0012908657677038464, 0.0013115048686461162, 0.0013003876656582651, 0.0013069306272840084, 0.00131841427863164, 0.0013342527582403995, 0.0013517874722592822, 0.0012704457766412533, 0.0012599045425191406, 0.0013358058288780062, 0.001852401783112292, 0.001852124434888594, 0.001966241232747602, 0.0013316070151970136, 0.0013445795499250409, 0.0013453373035719229, 0.0013317246975507154, 0.0013578703713624976, 0.0019439354488380658, 0.001344118751007919, 0.0013621545274012774, 0.0013309220382727164, 0.0013255432082065778, 0.0013755064948584683, 0.001277255318524648, 0.0012355665730442418, 0.001235999619992551, 0.0012377390381699616, 0.0012452794047606776, 0.0012498579616116922, 0.0014790372089157964, 0.0013649449376894752, 0.00130942379767003, 0.0013566726436044358, 0.0012679827280429213, 0.0012977344648147275, 0.0014714246900939896, 0.0012486496355471222, 0.0012416600776735203, 0.0013229065509729606, 0.001315954534784537, 0.0013250816295775332, 0.0013657108367991079, 0.001314017488512882, 0.001328420998571917, 0.0013169711094725039, 0.001917125303442626, 0.0019852987530301937, 0.001353808216635109, 0.0013244440002987783, 0.0012565962952207918, 0.0013114427985233624, 0.0013439590544548146, 0.0013703416054707397, 0.0019334082567501207, 0.0017892317918612975, 0.0013492917299068481, 0.001290657566571536, 0.0013090064424241697, 0.001274069783804837, 0.001272483604288725, 0.001273519442553899, 0.0012800238914643378, 0.0012772299995458172, 0.0013283127666155036, 0.0013016188524432423, 0.001404415668067775, 0.0013863042939107778, 0.0014037419698930294, 0.0014738172253001799, 0.0014086297511072584, 0.0013971180156912915, 0.0013611897295557481, 0.0013828735813036445, 0.00138213558035881, 0.0013568386500484723, 0.0019132873104784147, 0.001268537000341471, 0.0012669406665214735, 0.0012641470074335965, 0.0012770920861945596, 0.0012810368455455515, 0.0019188048913641724, 0.0012950004338351793, 0.0012770392877851115, 0.0018688205651245838, 0.0014622166135543307, 0.001291554489772218, 0.001269412272097182, 0.0012986660545128723, 0.001293988162001898, 0.0012817973173548315, 0.0012917842936175045, 0.001670777333842576, 0.0012575085040239633, 0.0012746562246348737, 0.0012724151394452698, 0.001269003008167411, 0.001418616279757531, 0.0013663587358946254, 0.0013384629770131537, 0.0012727599363600791, 0.0013690060546058555, 0.0018683962105203043, 0.0018688797059129606, 0.0012595974731930467, 0.0012719313949779716, 0.0012545574117134245, 0.0012602292004857184, 0.0013337845592795647, 0.0012623260552135779, 0.0012671650087735218, 0.0012765428603648446, 0.0012556727210334105, 0.0012449412642754325, 0.0016853691389163334, 0.0020290051023800705, 0.001377454108753523, 0.0013339557836568633, 0.0013551591000295887, 0.0012616368999719158, 0.0012625717673479587, 0.001247952356501374, 0.0017928504800178516, 0.001402859581461133, 0.0013463884418786958, 0.0013593974804808927, 0.001346523216528486, 0.0018387729385073565, 0.0013473758459553239, 0.0013395635349777086, 0.0012512460924944905, 0.0012826910152193881, 0.0012633721849541794, 0.0012694494257312874, 0.0012570867448496495, 0.0015331961019614407, 0.0016965810624637115, 0.0013060281468309867, 0.0012622409835890975, 0.0013052465108966873, 0.0012872316351145968, 0.00185065958179649, 0.0019947578607112633, 0.0012798228142277678, 0.0013759619531098955, 0.0013847004643879658, 0.0013458778605211614, 0.0012323694184509127, 0.0014594554803659056, 0.0013556396751123112, 0.0013275095586513365, 0.0012619980311295552, 0.0013174192868028731, 0.0013216436522933402, 0.0013162234415437363, 0.0013462376271829355, 0.0013300031170884307, 0.001337586032125608, 0.0013564332792636497, 0.0012508084737630778, 0.0012535376279333302, 0.001261076806907275, 0.0012316761318189923, 0.0012253836345996044, 0.001238811938019917, 0.001244184667562214, 0.0012519246656736431, 0.0016840007210280313, 0.0017475391857239397, 0.001260915922737399, 0.0012650074502236622, 0.0013321011395792859, 0.0013514637293712806, 0.0013714799000484536, 0.0013519357294277396, 0.0012841406350507754, 0.0018699080773535394, 0.0018845501233094423, 0.001868553587791416, 0.0012576178297429353, 0.0012353901000505731, 0.0012776664432946794, 0.0012452360766022936, 0.0012199036116430232, 0.0012584131088309973, 0.0019128595265855853, 0.0012192282240924446, 0.0012464922719252433, 0.0012232559848605663, 0.001287345574129113, 0.0019335899079694998, 0.0013350050863918177, 0.0013595299918002398, 0.0013821570242415796, 0.0013790775429775087, 0.0013824553867845342, 0.0013956938989373834, 0.0013664879135845244, 0.0013687713252192781, 0.0012764562960974005, 0.0013786844021185887, 0.0019494269761552874, 0.0013505326035295345, 0.0012592439676902092, 0.001230893891246975, 0.001250661534163379, 0.0012505113872764415, 0.001284526821673494, 0.0012888862325470577, 0.0012650022936566163, 0.0012535546751072009, 0.0013312556123895238, 0.0013177560303042563, 0.0013395755735854077, 0.0013641879240469646, 0.0023205063500723174, 0.0012631967208932998, 0.0013423839533456075, 0.0013407515361905098, 0.0013305757451051658, 0.001382460729255117, 0.0013202233492692773, 0.0012673956500871698, 0.0012538577664585769, 0.0012562241091198007, 0.0018522246044071376, 0.0018847855349439521, 0.001295013084300151, 0.0012935015279077744, 0.0012554211249618337, 0.0018687522320380045, 0.001884891976450765, 0.001446988433599472, 0.0012767939618969148, 0.001252679914134995, 0.001255198921502907, 0.0012571626277857048, 0.0014470620171073102, 0.001627350287410871, 0.0013330333266955937, 0.0013377303257584572, 0.0012641420836971944, 0.0012566314201599867, 0.0012722923727200707, 0.001940990673638014, 0.0012794299621510412, 0.001317460983341863, 0.0012652508376613837, 0.0014623738283854584, 0.0018612063027802826, 0.0012759163726640994, 0.001280681993595736, 0.002223200332589967, 0.0012760990544749323, 0.0012925062545163686, 0.0012814418373037447, 0.0012568687828791926, 0.0012778953501102767, 0.0012684265122741693, 0.0017267067126150047, 0.0016487204115982204, 0.0012669240218377853, 0.0012609388989706833, 0.0012767886031823333, 0.0013108573864798905, 0.001868804223662199, 0.001305863712445017, 0.001268217542259268, 0.0012743073631106883, 0.001259896279738624, 0.001260265162893275, 0.0013099615281635477, 0.0015631857609679533, 0.0012588221399301245, 0.0013267170157745597, 0.0013296570477270802, 0.0012623452574245689, 0.001862688634414659, 0.0018679152094838462, 0.0012630904183629178, 0.001260215898455922, 0.0012711795577562827, 0.0012636078835659942, 0.001269738355925841, 0.0013038142558995836, 0.0014683278999471849, 0.0012507954262225896, 0.001575810262615713, 0.0018684197444642006, 0.0018848788747636153, 0.0013090745192165523, 0.0012815769151748382, 0.0012538770462797826, 0.001246596805664689, 0.0018505116675568874, 0.001865444038886317, 0.0013413375113592591, 0.001333717070880902, 0.0014246693639041379, 0.0019331176353748454, 0.0019495381157874137, 0.0013335178454601487, 0.001281222148054792, 0.0012714039379109938, 0.0013736409691068553, 0.0013857108916249848, 0.0014265103323399559, 0.001277385449195786, 0.001246741101102427, 0.001321149683131497, 0.0013418297821208257, 0.0020750431946745906, 0.001435841412045235, 0.0012372903410316437, 0.0012300340224115193, 0.0012456378520488045, 0.0013474236357986003, 0.0013006505747722795, 0.0012262237525379012, 0.0012336405654719404, 0.001382084272787437, 0.0013461676967698474, 0.0013310969915500907, 0.0013118033882143886, 0.0013155983797050724, 0.001759880130330837, 0.0012726651165316733, 0.0013095430773502404, 0.0012882462857911985, 0.0019333404651301544, 0.0013352870388866165, 0.0013305362017984076, 0.0013236442865991547, 0.0012963710302241551, 0.0018528032643619434, 0.0012778783877661756, 0.0013365907370755377, 0.0013152008511616972, 0.0012684715803452703, 0.0012915053805552942, 0.0013370935501920622, 0.0013960844967081103, 0.0013886632004305142, 0.001314003255160511, 0.0012640203800514456, 0.0012714708830381548, 0.0012640801561606485, 0.001858334636402338, 0.001469683326246549, 0.0012757387986883175, 0.0013351635971330395, 0.0013443966046004564, 0.0013456539297837388, 0.0013519295747572368, 0.0013913261169899804]
[773.6853182586551, 783.864395490516, 790.8867145031429, 771.1379117868381, 695.5137370786663, 793.4323336777387, 784.2377501691187, 801.0588083919956, 794.7388092240878, 780.7320703198811, 789.9480972024966, 797.44923054872, 794.7019141710807, 797.7015705145958, 791.9373693844766, 800.0807764687428, 585.3354786947984, 770.6073579595342, 784.8779069406122, 785.6698496604818, 790.7447652203035, 642.9785940432291, 777.7855937061047, 790.6637386952459, 797.8089663559127, 793.166300639653, 800.0203914073418, 515.4875230065813, 788.4632066650229, 788.7664682657119, 739.0028300067138, 742.2526813219029, 727.0989783581502, 724.4516569122243, 750.6997349472053, 773.5193546457754, 746.5723040633992, 530.5760441903059, 530.517154281271, 766.2118081571247, 782.8521342834935, 795.835202657407, 786.3553949975735, 796.196923920191, 797.3845093630086, 787.9073870915516, 521.5402637425536, 756.4572907348506, 753.6668475088205, 757.9016490042568, 758.4436827988941, 690.73361348392, 760.4418255947352, 754.4600782305299, 762.2157533852942, 521.6330571073726, 521.7105621830025, 521.6757740984588, 730.2236590582015, 785.7329995162867, 806.4877776443799, 800.8530662171354, 792.2138175573, 801.2311375701975, 523.364677117426, 780.9652598091279, 778.3155258850536, 788.3076501540628, 789.3255430870488, 708.9248805177925, 514.9616600909906, 775.5271848168147, 777.3801324726838, 535.1454335557852, 539.9661286551998, 549.3899033814263, 540.0749785059868, 794.8883236262022, 801.2573699398572, 808.7115667726349, 801.797089767928, 559.8963267126854, 740.777666889098, 735.3199442030377, 743.5882163960923, 735.042600907635, 728.907320282559, 645.4105557682393, 723.7074806832289, 508.3926387129007, 812.3721233200288, 746.317915242341, 757.7772930265131, 627.3151119698492, 736.164057124996, 739.5487818075671, 720.1610563861981, 730.6595021404344, 723.3070466747297, 738.2529444175492, 483.0907815880338, 718.1015536191073, 738.4959037534047, 750.0242702448769, 626.0977607361802, 752.5611273233251, 740.21737278359, 742.6673938867688, 742.7076519012523, 755.1564151062397, 744.8498093428491, 803.1196401473621, 797.6380703636112, 759.5366226712182, 790.602616433348, 802.009036076361, 779.0845724661494, 528.1102612832437, 802.6270960916407, 798.3331645229582, 799.6020534392559, 793.9258835802998, 792.8678661354606, 517.1182495793414, 800.3536932615433, 793.2926064036338, 539.931197556674, 535.3155976593506, 535.1105176560386, 535.1185863360752, 786.0549232123103, 790.1741967672766, 783.517431529257, 764.6864747626844, 786.8361510302574, 799.2093992951272, 790.3656123800549, 745.1676095710164, 522.6124669488994, 772.8325378630359, 786.9703533949272, 775.6928872146821, 798.4744212333223, 668.1595599199221, 583.5472937204302, 790.4608649619337, 786.4697889785081, 784.0580639766421, 784.4475263132362, 767.6374520691758, 778.1519875527093, 796.8562655663434, 795.0195396873345, 787.2990098608817, 789.2324706227625, 780.6592577527323, 673.2189416954591, 653.8402301892, 787.6376889765013, 787.9755568662911, 808.4487990814622, 809.2842340913184, 811.5004530591634, 672.5831668841555, 758.3875141062548, 793.6817418229978, 740.2894258929782, 749.1011133053255, 756.0521227780138, 459.63728836535705, 490.40648213479363, 752.968310146488, 755.5560125229213, 747.5920448168689, 741.6087962215095, 741.3974994432501, 730.0340894988672, 710.8551104362889, 799.4577569923318, 789.6435010740469, 626.1889123559115, 769.5882814259629, 745.8056968221534, 535.3319550501044, 822.5337149777499, 764.148982799423, 760.6234557527729, 761.988520099725, 743.2043078558637, 758.0774425483463, 734.0071953482591, 518.221826335372, 544.637653398581, 514.1098587369514, 808.6034450211868, 743.0072859791183, 747.6360961156156, 740.3071970351517, 745.9972559915454, 495.6541619501981, 736.53629233694, 740.7351895471897, 736.8682158731721, 738.7336036741177, 518.4017093158128, 512.9204318945585, 744.074940054947, 741.7650746981212, 739.1397980188119, 735.4347781392987, 554.3809793654057, 746.6056360516335, 740.4353395020476, 747.6767634609039, 554.5461433902369, 803.7744101907559, 744.8218726003752, 752.880873004887, 744.669168757818, 753.2394108361462, 721.2906003257336, 756.5128763710258, 763.0951203302966, 752.2228713654556, 753.5519676096144, 749.3113481839343, 686.2163866334723, 556.340102904569, 815.8136145472739, 792.7733189607451, 797.8958649249474, 811.3368591631246, 811.504777131699, 801.5081223616263, 795.5853773960141, 814.9188834578919, 806.9980411626741, 801.2539910021061, 796.863594739044, 797.3742179262013, 797.023412242376, 763.8763284465157, 762.8475370022907, 761.3732606280267, 770.3875222642619, 761.6941082292485, 760.0756732550614, 759.0160155509826, 771.6737902421726, 766.9630184021598, 767.8937911621694, 758.0769529732767, 766.5349298600637, 745.321073378351, 746.8349661785334, 761.8582759722789, 530.6053995315741, 526.1392567182656, 524.9211377415652, 762.5786506056777, 772.4386929413845, 701.3694487028198, 755.2230747101993, 823.5827991482442, 802.566376062579, 814.2170265920657, 807.7951967423725, 806.2744962418093, 544.1406286902293, 808.6170648722704, 787.3644583363227, 812.984294481786, 796.469960959629, 815.5786858666951, 651.3449466929255, 801.3353993472654, 617.9005143923916, 813.0535153314485, 799.0860545051236, 500.9192120695204, 796.7911180693969, 792.7857539406859, 795.2681478202245, 765.8779640810833, 814.6445496671055, 546.3228731443144, 803.2825897525089, 806.3287178068565, 804.7501920416954, 806.0904749348459, 799.1171061234357, 805.9385201696467, 786.0095134155987, 805.1513658295745, 794.7176869968906, 549.5148373359228, 539.9466690188772, 539.8492146064394, 801.8946199236967, 742.4371475365683, 745.1682900716554, 749.6264644317262, 748.166939479281, 559.9752459291734, 749.1232430145627, 733.7977622075132, 759.5932293493506, 745.283748633177, 748.973157537417, 739.5194023663362, 750.3454860857629, 733.0390824941466, 737.2455105524215, 756.9143691105634, 752.7929447548489, 757.9794934529167, 749.6216944966897, 676.5946494101086, 500.71571852701, 753.7845241603911, 733.4242697626315, 517.1197424127014, 530.6370303306949, 790.4907806761803, 797.9827054224805, 801.4295816989435, 813.918378023804, 799.5798987937482, 714.6993216516512, 535.085500196298, 800.0880114113517, 783.1263205921587, 793.5455102707066, 751.1444388050206, 813.7281157329508, 704.2371017021284, 582.0525316518549, 809.923839161465, 827.618657861894, 807.3180261602321, 815.2592256321083, 816.7511119504171, 678.6395838283437, 774.6738855572646, 762.4828728484349, 769.0014496513954, 765.1515536659701, 758.4869310107011, 749.4831798727485, 739.7612572401418, 787.1252897103208, 793.7109251154302, 748.6117955031965, 539.8396876512725, 539.9205264845774, 508.58459447654445, 750.9723128426506, 743.7269145256219, 743.3080145365486, 750.9059506361806, 736.4473230214105, 514.4203736794464, 743.9818834832312, 734.1310988466214, 751.3588108420008, 754.4076977716717, 727.0049278123505, 782.9288204922847, 809.3453010274932, 809.0617374186787, 807.9247475934292, 803.0326336218366, 800.0909148992415, 676.1155121533736, 732.6302859460106, 763.6946890528383, 737.0974897401775, 788.6542757119873, 770.5736628816173, 679.613443169914, 800.8651678834061, 805.3734012884468, 755.9112918932392, 759.9046726669256, 754.6704879750112, 732.2194223366898, 761.0248788482518, 752.7734062281633, 759.3180995447481, 521.6143139962093, 503.70252762899474, 738.657062139496, 755.033810243704, 795.8005318042847, 762.5189608925103, 744.0702874728995, 729.7450475178998, 517.2213351777585, 558.8990786709209, 741.1295703035529, 774.7988512990088, 763.9381805853332, 784.8863639271275, 785.8647424844157, 785.2255462975996, 781.2354180795855, 782.9443407652496, 752.834742790259, 768.2740597394701, 712.0399058035441, 721.3423520308015, 712.3816352632128, 678.5101862249743, 709.9097539392069, 715.7591475944155, 734.6514437236976, 723.1318997773412, 723.5180211049876, 737.0073073642733, 522.6606555760575, 788.309682516801, 789.3029456111874, 791.0472390629207, 783.028891033042, 780.6176719094545, 521.1577292202185, 772.2005135075303, 783.0612648843353, 535.0968512770704, 683.8932007270915, 774.2607903258984, 787.7661355423255, 770.0208968464171, 772.8045969546769, 780.1545427350716, 774.1230520767568, 598.5238007150401, 795.2232504194213, 784.5252552596687, 785.9070275098788, 788.0201966141256, 704.9122544758328, 731.8722190078789, 747.1256337859636, 785.694121438065, 730.4569593652423, 535.2183837503735, 535.0799181114191, 793.9044188974325, 786.2059258450168, 797.0938521133439, 793.506450742912, 749.7462712719839, 792.1883540863822, 789.163205325478, 783.3657850815861, 796.3858601443587, 803.2507465980809, 593.341824594572, 492.85238308517637, 725.9769989033707, 749.6500350698524, 737.9207356377314, 792.6210782375342, 792.0341844016577, 801.3126420975663, 557.7709971609249, 712.8297181093927, 742.7277068753209, 735.6200186911084, 742.65336662975, 543.8409381920536, 742.1834100721723, 746.5118106672265, 799.2032950180047, 779.6109804581134, 791.5323860294333, 787.7430795826572, 795.4900519769639, 652.2322869988287, 589.4207015064977, 765.6802821795618, 792.241745436412, 766.1388033996835, 776.8609570498741, 540.3478899286656, 501.3139788522672, 781.3581605852136, 726.7642813377499, 722.177846919404, 743.009473097932, 811.4449977645494, 685.1870532900984, 737.6591422917396, 753.2902444905445, 792.3942631708759, 759.0597845480238, 756.6336041222471, 759.749422808598, 742.8109122848886, 751.8779370902113, 747.6154624692534, 737.2275623780461, 799.4829112337908, 797.7423076231623, 792.9731119648833, 811.9017444327325, 816.0709607703805, 807.2250269063217, 803.7392085528141, 798.770107674102, 593.8239737745067, 572.2332341210064, 793.0742898614836, 790.5091782844386, 750.6937501125683, 739.9384669133619, 729.139377080678, 739.680132888632, 778.7309058718943, 534.785646476959, 530.6306198128117, 535.1733054559999, 795.1541210292844, 809.4609143776229, 782.6768913342754, 803.0605752513725, 819.7368959775054, 794.6516076337999, 522.7775412159926, 820.1909865926608, 802.2512634237765, 817.4903800809818, 776.792199465554, 517.1727447885365, 749.0608164668106, 735.5483189273644, 723.506795871274, 725.1223871291108, 723.3506481000514, 716.4894829456184, 731.8030332056389, 730.5822247845522, 783.4189098814978, 725.3291605122433, 512.9712537230951, 740.4486181130029, 794.1272903885877, 812.4177129410688, 799.5768420821728, 799.6728459849979, 778.4967842844966, 775.8636679855215, 790.5124006608711, 797.7314590721641, 751.1705420757326, 758.8658120343492, 746.5051018536228, 733.0368363278176, 430.9404281392445, 791.6423336602917, 744.9433506022714, 745.8503481124546, 751.5543580880186, 723.3478527370606, 757.4475944191444, 789.0195929986198, 797.5386257920001, 796.0363065318581, 539.8913272292273, 530.5643435075158, 772.1929701895007, 773.0953372876875, 796.5454620101293, 535.1164177122777, 530.5343820726487, 691.0905275949159, 783.2117239294538, 798.2885242400678, 796.6864716571412, 795.4420358178667, 691.0553854485165, 614.4958511612205, 750.1687917127042, 747.5348212899538, 791.0503201312097, 795.7782878552297, 785.9828616767317, 515.2008268672885, 781.5980785057982, 759.0357609402644, 790.3571135731015, 683.8196776976345, 537.285951861539, 783.7504255173174, 780.8339658093632, 449.8020197914542, 783.6382265885019, 773.6906467614589, 780.3709625277097, 795.6280031947597, 782.5366920019737, 788.3783493354252, 579.1371474345829, 606.5309757587279, 789.3133153710446, 793.0598388362114, 783.2150110891879, 762.8594920499696, 535.1015303466896, 765.7766966567001, 788.508254048078, 784.7400312895613, 793.7161305115199, 793.4838075696969, 763.3811974630379, 639.7192355314071, 794.3934002110169, 753.7402385814606, 752.0736280903433, 792.1763036842991, 536.8583785417497, 535.3562061718669, 791.708958806047, 793.5148264874683, 786.6709261475753, 791.3847428507066, 787.5638278807772, 766.9804157111605, 681.046788006936, 799.4912509554073, 634.5941663941722, 535.211642331882, 530.5380697873284, 763.8984529302995, 780.2887116327121, 797.5263627059539, 802.1839904096314, 540.3910807653735, 536.0653973822806, 745.524516783728, 749.7842097346131, 701.9172485464404, 517.2990932888017, 512.9420101622903, 749.8962262892974, 780.5047715716155, 786.5320927376317, 727.9922647110637, 721.651252107377, 701.0113963630844, 782.8490614399734, 802.0911471641972, 756.9165044415847, 745.2510097215554, 481.9176789024965, 696.4557447716907, 808.2177374521547, 812.9856424942373, 802.8015513138241, 742.1570866294869, 768.8460062957971, 815.5118492284231, 810.6088823509472, 723.5448805036789, 742.8494996570763, 751.2600556894647, 762.30935899715, 760.1103919147251, 568.2205184122464, 785.7526595254271, 763.625127951822, 776.2490845341991, 517.2394712861291, 748.902648552491, 751.5766941541002, 755.4899833166693, 771.3840996794647, 539.7227105730374, 782.5470792632098, 748.172175865891, 760.340140531931, 788.350338702745, 774.2902314274843, 747.8908262300408, 716.2890228764409, 720.1170159114026, 761.0331223097661, 791.1264848113447, 786.4906804712032, 791.0890738426501, 538.1162145995191, 680.4186875780364, 783.8595181303374, 748.9718878999344, 743.8281207926673, 743.1331175621884, 739.6835002885175, 718.7387541918769]
Elapsed: 0.1801483122110953~0.02813501217716553
Time per graph: 0.0013964985442720566~0.00021810086959043047
Speed: 730.1091206264238~90.44217304694278
Total Time: 0.1812
best val loss: 0.16626322062960427 test_score: 0.9302

Testing...
Test loss: 0.2078 score: 0.9302 time: 0.17s
test Score 0.9302
Epoch Time List: [0.7755973841995001, 0.5715205722954124, 0.5584769591223449, 0.5614219650160521, 0.5783656067214906, 0.6155834691599011, 0.5564073571003973, 0.558660996844992, 0.5558570229914039, 0.5620986700523645, 0.5813217859249562, 0.6309207328595221, 0.5656078900210559, 0.5596460532397032, 0.5592070163693279, 0.553504184121266, 0.6378331931773573, 0.5933540957048535, 0.5677692759782076, 0.5624528611078858, 0.5672363820485771, 0.595197785878554, 0.6632938722614199, 0.571598530979827, 0.5581784371752292, 0.5661037089303136, 0.5586181550752372, 0.667509201914072, 0.5720257801003754, 0.5617433458101004, 0.5829179862048477, 0.5912495763041079, 0.5941719261463732, 0.6663042260333896, 0.5905647119507194, 0.5653112919535488, 0.5735324416309595, 0.8110638167709112, 0.8411251280922443, 0.7892517161089927, 0.5707778409123421, 0.5674885248299688, 0.5651758711319417, 0.5666664321906865, 0.5594309358857572, 0.586941858753562, 0.6538559079635888, 0.5873816162347794, 0.5872467909939587, 0.5881843990646303, 0.587586629902944, 0.6043414387386292, 0.6609265480656177, 0.5823413180187345, 0.5880501219071448, 0.7219613410998136, 0.8576215959619731, 0.8513411749154329, 0.7930991072207689, 0.5636339830234647, 0.5577402701601386, 0.5556698201689869, 0.5643293308094144, 0.5549125962425023, 0.6875220818910748, 0.5747040819842368, 0.5642254890408367, 0.5624866457656026, 0.5662977257743478, 0.5713689778931439, 0.7663882789202034, 0.5682006110437214, 0.5489976811222732, 0.7894165432080626, 0.8271518624387681, 0.8162630647420883, 0.8217920821625739, 0.6007493429351598, 0.5539326486177742, 0.5496085786726326, 0.5541591949295253, 0.6128333150409162, 0.7071904812473804, 0.5872345298994333, 0.6672723889350891, 0.5788314808160067, 0.5973954449873418, 0.7422948160674423, 0.5936202490702271, 0.6802488702815026, 0.5414833128452301, 0.5768198070582002, 0.5880306896287948, 0.6682796482928097, 0.5926394197158515, 0.5997820110060275, 0.5946805828716606, 0.5952887330204248, 0.596308472333476, 0.5951835452578962, 0.7144159891176969, 0.6042460829485208, 0.5993372478988022, 0.5902742699254304, 0.6535578959155828, 0.649566964013502, 0.5966727801132947, 0.5932977860793471, 0.6119402749463916, 0.594020240008831, 0.6008137008175254, 0.6316064142156392, 0.5588254269678146, 0.5745081282220781, 0.5576676910277456, 0.561995540978387, 0.5716136188711971, 0.8355553091969341, 0.5896823131479323, 0.5644917520694435, 0.5620899929199368, 0.5659940899349749, 0.5640874139498919, 0.6980796919669956, 0.5691085828002542, 0.5598380661103874, 0.7033039007801563, 0.8393668220378458, 0.8392870649695396, 0.8371419999748468, 0.654231364140287, 0.5712516061030328, 0.5583341571036726, 0.7479637020733207, 0.5639892760664225, 0.5544634107500315, 0.5622757899109274, 0.5694899968802929, 0.7251254958100617, 0.5894532999955118, 0.5699005760252476, 0.5748510719276965, 0.5712421417701989, 0.5990380600560457, 0.6406115940771997, 0.6418615910224617, 0.5635983543470502, 0.5664167227223516, 0.5619438430294394, 0.5738787539303303, 0.7693893848918378, 0.572714862646535, 0.5676379939541221, 0.5686286299023777, 0.5708915460854769, 0.5714853131212294, 0.5997448430862278, 0.6641385932452977, 0.5700356201268733, 0.576115861069411, 0.5539940849412233, 0.563822008902207, 0.5569580621086061, 0.6154325890820473, 0.6597543458919972, 0.5565078051295131, 0.5818952068220824, 0.5893414330203086, 0.5802547840867192, 0.6972065882291645, 0.7204036589246243, 0.5831926588434726, 0.5856506058480591, 0.5845838291570544, 0.5856490680016577, 0.5876872588414699, 0.5896792982239276, 0.6018440949264914, 0.7107192501425743, 0.5533956121653318, 0.6105515700764954, 0.5661211379338056, 0.5746888322755694, 0.6516737181227654, 0.5350677068345249, 0.5729016836266965, 0.633563200943172, 0.6761251848656684, 0.5823800021316856, 0.5735965080093592, 0.5850018700584769, 0.6833115571644157, 0.8164044588338584, 0.8410908412188292, 0.5571200530976057, 0.5774670359678566, 0.5934019370470196, 0.5926426809746772, 0.6009307280182838, 0.7275966410525143, 0.5961167218629271, 0.5999345178715885, 0.5987051150295883, 0.5940521031152457, 0.6862913260702044, 0.8673943402245641, 0.6866098188329488, 0.5946250602137297, 0.5926475890446454, 0.597979661077261, 0.6654703852254897, 0.709268522914499, 0.5944798600394279, 0.5871334332041442, 0.7232648301869631, 0.6570475092157722, 0.5920855009462684, 0.586467772256583, 0.5895334919914603, 0.5872805928811431, 0.6241951850242913, 0.6711252662353218, 0.5868393362034112, 0.5837719170376658, 0.5918925758451223, 0.5880117591004819, 0.6166505941655487, 0.6666742768138647, 0.551067034015432, 0.5614066130947322, 0.5518877129070461, 0.555944656720385, 0.5552036657463759, 0.5927939550019801, 0.6341691568959504, 0.5511047439649701, 0.5551754080224782, 0.5632520359940827, 0.5626041828654706, 0.6446642859373242, 0.5483496058732271, 0.5770511988084763, 0.5779496331233531, 0.583480381872505, 0.579168704804033, 0.5775639000348747, 0.603153018746525, 0.6654929623473436, 0.5804749059025198, 0.5839402300771326, 0.5795208960771561, 0.5813210278283805, 0.5744128827936947, 0.6213273622561246, 0.6632254151627421, 0.5800286058802158, 0.7438703197985888, 0.8522536458913237, 0.8548027237411588, 0.6239087760914117, 0.5802438440732658, 0.6021364100743085, 0.6824663046281785, 0.5491016211453825, 0.5506485940422863, 0.5462566851638258, 0.5489165098406374, 0.5572183760814369, 0.6597433860879391, 0.634148753946647, 0.5548715598415583, 0.5484204050153494, 0.553975738119334, 0.5541457578074187, 0.7178778948727995, 0.5495587068144232, 0.6094880518503487, 0.5495890851598233, 0.5520400628447533, 0.6568744550459087, 0.5605107438750565, 0.6006065888796002, 0.7024782260414213, 0.5753568848595023, 0.566573899006471, 0.6649582078680396, 0.6833437101449817, 0.5585889599751681, 0.559573887148872, 0.5579374879598618, 0.5549067370593548, 0.5487464170437306, 0.5944335791282356, 0.6321499519981444, 0.5641306980978698, 0.7098724360112101, 0.8355929509270936, 0.8420459337066859, 0.6387709309346974, 0.5832411600276828, 0.5973200728185475, 0.587966870283708, 0.5877405612263829, 0.6645818760152906, 0.6100514121353626, 0.5893156619276851, 0.5879765069112182, 0.5848185468930751, 0.5887142061255872, 0.6027590003795922, 0.663023852976039, 0.5936956650111824, 0.6024591750465333, 0.5921089870389551, 0.5984022149350494, 0.5838193341623992, 0.5885331970639527, 0.6042604420799762, 0.689556498080492, 0.5951801748014987, 0.5987841070163995, 0.855895146727562, 0.865328473970294, 0.7064790651202202, 0.5668331931810826, 0.5677142380736768, 0.5546520401258022, 0.5501997121609747, 0.5800938371103257, 0.7615502302069217, 0.6529943149071187, 0.5624702572822571, 0.5593309276737273, 0.584667838877067, 0.5622086718212813, 0.5730970790609717, 0.659987036138773, 0.5539456349797547, 0.5350425380747765, 0.5368907537776977, 0.543938698945567, 0.5438304811250418, 0.5910443470347673, 0.6375722431112081, 0.5695784089621156, 0.5763746618758887, 0.5759748083073646, 0.5792086939327419, 0.5801315479911864, 0.6026290818117559, 0.6389543011318892, 0.5573943071067333, 0.5610578421037644, 0.8348937181290239, 0.8352610347792506, 0.854220069013536, 0.594773449935019, 0.5890105951111764, 0.5883497807662934, 0.5895731849595904, 0.5926206370349973, 0.6936846112366766, 0.7157735589426011, 0.5882899791467935, 0.5858090010005981, 0.5796935758553445, 0.5894813989289105, 0.6476842078845948, 0.549240366788581, 0.562006413936615, 0.6054464352782816, 0.5514756517950445, 0.5687490170821548, 0.7540259868837893, 0.5933778521139175, 0.6486072891857475, 0.5828982619568706, 0.5838135606609285, 0.6187370188999921, 0.64650285593234, 0.5654985290020704, 0.5546099930070341, 0.5812732821796089, 0.5852311819326133, 0.5835455229971558, 0.5953825989272445, 0.6634353529661894, 0.589076278032735, 0.5880122929811478, 0.722072197124362, 0.8742205209564418, 0.6087442436255515, 0.5885830188635737, 0.6224387402180582, 0.5717673238832504, 0.5894239880144596, 0.6038581726606935, 0.764710531802848, 0.8492345891427249, 0.6022522037383169, 0.5695902900770307, 0.5759673591237515, 0.643924793926999, 0.5735242618247867, 0.5800035470165312, 0.5696773829404265, 0.5740904789417982, 0.6181087750010192, 0.6562604929786175, 0.6194805030245334, 0.6081947919446975, 0.617603697348386, 0.617778732907027, 0.7061312929727137, 0.6135461058001965, 0.6018785708583891, 0.6102982671000063, 0.6047850460745394, 0.6055747889913619, 0.69675378408283, 0.5753423997666687, 0.5769154289737344, 0.5698119201697409, 0.57711176504381, 0.5685172050725669, 0.7005790330003947, 0.5797528263647109, 0.5708152442239225, 0.7221784221474081, 0.7917522599454969, 0.5800989423878491, 0.571171463932842, 0.5748765671160072, 0.5757989347912371, 0.5698474789969623, 0.5687265461310744, 0.6447796118445694, 0.6201560692861676, 0.5649256187025458, 0.5703770068939775, 0.5689509329386055, 0.5924774201121181, 0.687047426123172, 0.5915595430415124, 0.5720815958920866, 0.604394826805219, 0.8376313836779445, 0.8372098249383271, 0.7072727840859443, 0.5784047800116241, 0.5647257391829044, 0.5647053960710764, 0.5814318882767111, 0.6219406318850815, 0.5716607577633113, 0.5631513244006783, 0.5597006829921156, 0.5551632498390973, 0.6846746408846229, 0.6828498500399292, 0.6039500732440501, 0.670652219094336, 0.5987117739859968, 0.6469783848151565, 0.5544641979504377, 0.5603222479112446, 0.7034272830933332, 0.6311091519892216, 0.6005560029298067, 0.59273417503573, 0.6002940607722849, 0.6888196538202465, 0.5931902569718659, 0.5941222209949046, 0.5653513600118458, 0.5639825640246272, 0.5627433559857309, 0.5662206741981208, 0.568445652956143, 0.5970623029861599, 0.6328371369745582, 0.5711595909669995, 0.5643664796371013, 0.566700056893751, 0.5715763131156564, 0.6570497867651284, 0.8620857431087643, 0.5823266340885311, 0.6040601599961519, 0.6067164861597121, 0.6064294807147235, 0.5567307390738279, 0.5932530469726771, 0.6567656788975, 0.5903419849928468, 0.55848085321486, 0.5781422641593963, 0.5809338770341128, 0.5874077209737152, 0.604090761160478, 0.6970362800639123, 0.5960104530677199, 0.5936031760647893, 0.5735269607976079, 0.5537762669846416, 0.5754205968696624, 0.6780123903881758, 0.5558282609563321, 0.5584126459434628, 0.5553425599355251, 0.5499505810439587, 0.6612085457891226, 0.8155213510617614, 0.5715447687543929, 0.553194998530671, 0.5865303210448474, 0.5905689198989421, 0.6127524629700929, 0.676590973045677, 0.5742868101224303, 0.8113243498373777, 0.8371592359617352, 0.8330359500832856, 0.6894333553500473, 0.5588356540538371, 0.5674077321309596, 0.5680984028149396, 0.5637245341204107, 0.5702239358797669, 0.823830904206261, 0.561158302007243, 0.5488566972780973, 0.5542835278902203, 0.5537224679719657, 0.7447285000234842, 0.7112348179798573, 0.5947524271905422, 0.6001401597168297, 0.5993946348316967, 0.5956092348787934, 0.6503791580908, 0.6709188299719244, 0.5942415168974549, 0.567231927998364, 0.591286129783839, 0.7750273840501904, 0.7573144102934748, 0.5703380273189396, 0.5587789348792285, 0.5638569639995694, 0.5575008799787611, 0.5654659660067409, 0.5772581428755075, 0.6516246411483735, 0.6621223478578031, 0.5859562018886209, 0.5894331384915859, 0.6509479572996497, 0.5972565428819507, 0.7526764890644699, 0.5681344817858189, 0.583738970104605, 0.6847605637740344, 0.5907834579702467, 0.6126034241169691, 0.7921165339648724, 0.5720027778297663, 0.5632866751402617, 0.5654812522698194, 0.7367232965771109, 0.8352796752005816, 0.660122700035572, 0.5695178310852498, 0.5623886780813336, 0.7532197199761868, 0.8388396892696619, 0.8010090549942106, 0.5712054821196944, 0.5730508151464164, 0.5648004438262433, 0.5704481690190732, 0.6048995170276612, 0.6933412540238351, 0.6019199099391699, 0.5950773626100272, 0.5695176171138883, 0.5642574648372829, 0.564389152219519, 0.6624179151840508, 0.5782724409364164, 0.5752488952130079, 0.5676177300047129, 0.5819006068632007, 0.6460063278209418, 0.7168950450140983, 0.5647824581246823, 0.68767374381423, 0.5637547969818115, 0.5720744079444557, 0.5944544582162052, 0.6382358088158071, 0.5839555750135332, 0.5728332810103893, 0.6532739568501711, 0.7572318648453802, 0.5796191208064556, 0.573009432060644, 0.5691361019853503, 0.5737672948744148, 0.7722872772719711, 0.7159761202055961, 0.5660714649129659, 0.5651243589818478, 0.5652552898973227, 0.5733477238100022, 0.5731699478346854, 0.6443357071839273, 0.5767643698491156, 0.5820745250675827, 0.5986795099452138, 0.5664072108920664, 0.6571303750388324, 0.8331326369661838, 0.6831721207126975, 0.575017100200057, 0.5689384841825813, 0.5760743520222604, 0.5702139919158071, 0.592586733167991, 0.6636615407187492, 0.563293692888692, 0.6171983520034701, 0.8240932270418853, 0.8455556011758745, 0.7244387399405241, 0.5687908069230616, 0.5646315270569175, 0.5640429998748004, 0.6790482650976628, 0.8559441608376801, 0.6864445370156318, 0.6011871681548655, 0.6067788801155984, 0.805706633720547, 0.8627182869240642, 0.6808801351580769, 0.5685269772075117, 0.6389580729883164, 0.5884535012301058, 0.607280989876017, 0.6812337611336261, 0.6332013921346515, 0.5649351580068469, 0.6629346332047135, 0.5917080088984221, 0.7016130636911839, 0.7139254359062761, 0.5701944457832724, 0.569236233830452, 0.5616518391761929, 0.5940326368436217, 0.6129236042033881, 0.6310063188429922, 0.5563805301208049, 0.7425668728537858, 0.5975812708493322, 0.6047195710707456, 0.5850381623022258, 0.5902167554013431, 0.6816354498732835, 0.6082833730615675, 0.5805453290231526, 0.5787775521166623, 0.76469292701222, 0.7017018457408994, 0.599256634246558, 0.5933833990711719, 0.5886380327865481, 0.7198239408899099, 0.7538878107443452, 0.5921853638719767, 0.5957060595974326, 0.5595716841053218, 0.5597519727889448, 0.5832588721532375, 0.6087010910268873, 0.6916834290605038, 0.6051042967010289, 0.5658676533494145, 0.5754080368205905, 0.5662499971222132, 0.6695401389151812, 0.7640957310795784, 0.577547102002427, 0.5941075070295483, 0.5943375148344785, 0.5993226550053805, 0.5991288092918694, 0.6537303228396922]
Total Epoch List: [400, 299]
Total Time List: [0.17514378111809492, 0.18118192511610687]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x723e9a787a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 4.2707;  Loss pred: 4.2707; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7032 score: 0.5000 time: 0.15s
Epoch 2/1000, LR 0.000020
Train loss: 4.3056;  Loss pred: 4.3056; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7041 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7030 score: 0.5000 time: 0.22s
Epoch 3/1000, LR 0.000050
Train loss: 4.2612;  Loss pred: 4.2612; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7039 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7028 score: 0.5000 time: 0.22s
Epoch 4/1000, LR 0.000080
Train loss: 4.2386;  Loss pred: 4.2386; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.4961 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7025 score: 0.5000 time: 0.22s
Epoch 5/1000, LR 0.000110
Train loss: 4.0897;  Loss pred: 4.0897; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7023 score: 0.5000 time: 0.17s
Epoch 6/1000, LR 0.000140
Train loss: 4.0423;  Loss pred: 4.0423; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7022 score: 0.5000 time: 0.17s
Epoch 7/1000, LR 0.000170
Train loss: 3.9242;  Loss pred: 3.9242; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.17s
Epoch 8/1000, LR 0.000200
Train loss: 3.8180;  Loss pred: 3.8180; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.5000 time: 0.15s
Epoch 9/1000, LR 0.000230
Train loss: 3.6740;  Loss pred: 3.6740; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7017 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.5000 time: 0.16s
Epoch 10/1000, LR 0.000260
Train loss: 3.4977;  Loss pred: 3.4977; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.5000 time: 0.15s
Epoch 11/1000, LR 0.000290
Train loss: 3.3283;  Loss pred: 3.3283; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.5000 time: 0.15s
Epoch 12/1000, LR 0.000290
Train loss: 3.1529;  Loss pred: 3.1529; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7000 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.20s
Epoch 13/1000, LR 0.000290
Train loss: 2.9835;  Loss pred: 2.9835; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.5000 time: 0.20s
Epoch 14/1000, LR 0.000290
Train loss: 2.8636;  Loss pred: 2.8636; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.15s
Epoch 15/1000, LR 0.000290
Train loss: 2.7540;  Loss pred: 2.7540; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 2.6295;  Loss pred: 2.6295; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 2.5485;  Loss pred: 2.5485; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000290
Train loss: 2.4164;  Loss pred: 2.4164; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000290
Train loss: 2.3036;  Loss pred: 2.3036; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000290
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000290
Train loss: 2.1385;  Loss pred: 2.1385; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000290
Train loss: 2.0539;  Loss pred: 2.0539; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000290
Train loss: 1.9851;  Loss pred: 1.9851; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000290
Train loss: 1.8878;  Loss pred: 1.8878; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.14s
Epoch 25/1000, LR 0.000290
Train loss: 1.8435;  Loss pred: 1.8435; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.14s
Epoch 26/1000, LR 0.000290
Train loss: 1.7679;  Loss pred: 1.7679; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.15s
Epoch 27/1000, LR 0.000290
Train loss: 1.7118;  Loss pred: 1.7118; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.15s
Epoch 28/1000, LR 0.000290
Train loss: 1.6599;  Loss pred: 1.6599; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.15s
Epoch 29/1000, LR 0.000290
Train loss: 1.5949;  Loss pred: 1.5949; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.15s
Epoch 30/1000, LR 0.000290
Train loss: 1.5665;  Loss pred: 1.5665; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 1.5063;  Loss pred: 1.5063; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.16s
Epoch 32/1000, LR 0.000290
Train loss: 1.4769;  Loss pred: 1.4769; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.16s
Epoch 33/1000, LR 0.000290
Train loss: 1.4221;  Loss pred: 1.4221; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.16s
Epoch 34/1000, LR 0.000290
Train loss: 1.3956;  Loss pred: 1.3956; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.16s
Epoch 35/1000, LR 0.000290
Train loss: 1.3594;  Loss pred: 1.3594; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.18s
Epoch 36/1000, LR 0.000290
Train loss: 1.3300;  Loss pred: 1.3300; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.15s
Epoch 37/1000, LR 0.000290
Train loss: 1.3020;  Loss pred: 1.3020; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.15s
Epoch 38/1000, LR 0.000289
Train loss: 1.2766;  Loss pred: 1.2766; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.22s
Epoch 39/1000, LR 0.000289
Train loss: 1.2544;  Loss pred: 1.2544; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.15s
Epoch 40/1000, LR 0.000289
Train loss: 1.2322;  Loss pred: 1.2322; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.14s
Epoch 41/1000, LR 0.000289
Train loss: 1.2129;  Loss pred: 1.2129; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4961 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.20s
Epoch 42/1000, LR 0.000289
Train loss: 1.1930;  Loss pred: 1.1930; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.15s
Epoch 43/1000, LR 0.000289
Train loss: 1.1756;  Loss pred: 1.1756; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.17s
Epoch 44/1000, LR 0.000289
Train loss: 1.1606;  Loss pred: 1.1606; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
Epoch 45/1000, LR 0.000289
Train loss: 1.1454;  Loss pred: 1.1454; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.21s
Epoch 46/1000, LR 0.000289
Train loss: 1.1356;  Loss pred: 1.1356; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.16s
Epoch 47/1000, LR 0.000289
Train loss: 1.1234;  Loss pred: 1.1234; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.15s
Epoch 48/1000, LR 0.000289
Train loss: 1.1106;  Loss pred: 1.1106; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.15s
Epoch 49/1000, LR 0.000289
Train loss: 1.1036;  Loss pred: 1.1036; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.15s
Epoch 50/1000, LR 0.000289
Train loss: 1.0948;  Loss pred: 1.0948; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.16s
Epoch 51/1000, LR 0.000289
Train loss: 1.0855;  Loss pred: 1.0855; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.16s
Epoch 52/1000, LR 0.000289
Train loss: 1.0797;  Loss pred: 1.0797; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.16s
Epoch 53/1000, LR 0.000289
Train loss: 1.0726;  Loss pred: 1.0726; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.15s
Epoch 54/1000, LR 0.000289
Train loss: 1.0648;  Loss pred: 1.0648; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.16s
Epoch 55/1000, LR 0.000289
Train loss: 1.0613;  Loss pred: 1.0613; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 1.0527;  Loss pred: 1.0527; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5000 time: 0.17s
Epoch 57/1000, LR 0.000288
Train loss: 1.0493;  Loss pred: 1.0493; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.22s
Epoch 58/1000, LR 0.000288
Train loss: 1.0450;  Loss pred: 1.0450; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.17s
Epoch 59/1000, LR 0.000288
Train loss: 1.0404;  Loss pred: 1.0404; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.5000 time: 0.22s
Epoch 60/1000, LR 0.000288
Train loss: 1.0367;  Loss pred: 1.0367; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.4961 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5000 time: 0.22s
Epoch 61/1000, LR 0.000288
Train loss: 1.0309;  Loss pred: 1.0309; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.4961 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5000 time: 0.16s
Epoch 62/1000, LR 0.000288
Train loss: 1.0282;  Loss pred: 1.0282; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6857 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.5000 time: 0.16s
Epoch 63/1000, LR 0.000288
Train loss: 1.0231;  Loss pred: 1.0231; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5000 time: 0.16s
Epoch 64/1000, LR 0.000288
Train loss: 1.0234;  Loss pred: 1.0234; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5000 time: 0.16s
Epoch 65/1000, LR 0.000288
Train loss: 1.0184;  Loss pred: 1.0184; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.5000 time: 0.16s
Epoch 66/1000, LR 0.000288
Train loss: 1.0154;  Loss pred: 1.0154; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.5000 time: 0.19s
Epoch 67/1000, LR 0.000288
Train loss: 1.0114;  Loss pred: 1.0114; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6805 score: 0.5000 time: 0.16s
Epoch 68/1000, LR 0.000288
Train loss: 1.0105;  Loss pred: 1.0105; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.5000 time: 0.15s
Epoch 69/1000, LR 0.000288
Train loss: 1.0076;  Loss pred: 1.0076; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5000 time: 0.15s
Epoch 70/1000, LR 0.000287
Train loss: 1.0036;  Loss pred: 1.0036; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6793 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6783 score: 0.5000 time: 0.16s
Epoch 71/1000, LR 0.000287
Train loss: 1.0026;  Loss pred: 1.0026; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6774 score: 0.5000 time: 0.19s
Epoch 72/1000, LR 0.000287
Train loss: 0.9994;  Loss pred: 0.9994; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6763 score: 0.5000 time: 0.21s
Epoch 73/1000, LR 0.000287
Train loss: 0.9972;  Loss pred: 0.9972; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6758 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.16s
Epoch 74/1000, LR 0.000287
Train loss: 0.9944;  Loss pred: 0.9944; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6745 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6734 score: 0.5000 time: 0.15s
Epoch 75/1000, LR 0.000287
Train loss: 0.9923;  Loss pred: 0.9923; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6735 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6724 score: 0.5000 time: 0.20s
Epoch 76/1000, LR 0.000287
Train loss: 0.9901;  Loss pred: 0.9901; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6722 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6711 score: 0.5000 time: 0.20s
Epoch 77/1000, LR 0.000287
Train loss: 0.9880;  Loss pred: 0.9880; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.5000 time: 0.20s
Epoch 78/1000, LR 0.000287
Train loss: 0.9862;  Loss pred: 0.9862; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6695 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6684 score: 0.5000 time: 0.16s
Epoch 79/1000, LR 0.000287
Train loss: 0.9840;  Loss pred: 0.9840; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6685 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5000 time: 0.15s
Epoch 80/1000, LR 0.000287
Train loss: 0.9831;  Loss pred: 0.9831; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6673 score: 0.4961 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6662 score: 0.5000 time: 0.16s
Epoch 81/1000, LR 0.000286
Train loss: 0.9808;  Loss pred: 0.9808; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6658 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6647 score: 0.5000 time: 0.15s
Epoch 82/1000, LR 0.000286
Train loss: 0.9767;  Loss pred: 0.9767; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6643 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6631 score: 0.5000 time: 0.16s
Epoch 83/1000, LR 0.000286
Train loss: 0.9757;  Loss pred: 0.9757; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6630 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6618 score: 0.5000 time: 0.16s
Epoch 84/1000, LR 0.000286
Train loss: 0.9741;  Loss pred: 0.9741; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6603 score: 0.5000 time: 0.16s
Epoch 85/1000, LR 0.000286
Train loss: 0.9727;  Loss pred: 0.9727; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6591 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6577 score: 0.5000 time: 0.15s
Epoch 86/1000, LR 0.000286
Train loss: 0.9690;  Loss pred: 0.9690; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6569 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6553 score: 0.5000 time: 0.15s
Epoch 87/1000, LR 0.000286
Train loss: 0.9666;  Loss pred: 0.9666; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6550 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6531 score: 0.5000 time: 0.15s
Epoch 88/1000, LR 0.000286
Train loss: 0.9641;  Loss pred: 0.9641; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6529 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6510 score: 0.5000 time: 0.15s
Epoch 89/1000, LR 0.000286
Train loss: 0.9621;  Loss pred: 0.9621; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6508 score: 0.4961 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6489 score: 0.5000 time: 0.20s
Epoch 90/1000, LR 0.000285
Train loss: 0.9607;  Loss pred: 0.9607; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6487 score: 0.4961 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6467 score: 0.5000 time: 0.20s
Epoch 91/1000, LR 0.000285
Train loss: 0.9575;  Loss pred: 0.9575; Loss self: 0.0000; time: 0.31s
Val loss: 0.6458 score: 0.5039 time: 0.16s
Test loss: 0.6437 score: 0.5078 time: 0.15s
Epoch 92/1000, LR 0.000285
Train loss: 0.9558;  Loss pred: 0.9558; Loss self: 0.0000; time: 0.27s
Val loss: 0.6433 score: 0.5349 time: 0.16s
Test loss: 0.6410 score: 0.5156 time: 0.15s
Epoch 93/1000, LR 0.000285
Train loss: 0.9532;  Loss pred: 0.9532; Loss self: 0.0000; time: 0.26s
Val loss: 0.6411 score: 0.5891 time: 0.16s
Test loss: 0.6388 score: 0.5469 time: 0.15s
Epoch 94/1000, LR 0.000285
Train loss: 0.9515;  Loss pred: 0.9515; Loss self: 0.0000; time: 0.26s
Val loss: 0.6394 score: 0.6357 time: 0.16s
Test loss: 0.6371 score: 0.6172 time: 0.15s
Epoch 95/1000, LR 0.000285
Train loss: 0.9488;  Loss pred: 0.9488; Loss self: 0.0000; time: 0.26s
Val loss: 0.6365 score: 0.6434 time: 0.15s
Test loss: 0.6343 score: 0.6328 time: 0.15s
Epoch 96/1000, LR 0.000285
Train loss: 0.9479;  Loss pred: 0.9479; Loss self: 0.0000; time: 0.27s
Val loss: 0.6335 score: 0.6589 time: 0.17s
Test loss: 0.6313 score: 0.6406 time: 0.15s
Epoch 97/1000, LR 0.000285
Train loss: 0.9451;  Loss pred: 0.9451; Loss self: 0.0000; time: 0.40s
Val loss: 0.6313 score: 0.6977 time: 0.16s
Test loss: 0.6292 score: 0.6953 time: 0.15s
Epoch 98/1000, LR 0.000285
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.26s
Val loss: 0.6291 score: 0.7519 time: 0.15s
Test loss: 0.6271 score: 0.7578 time: 0.15s
Epoch 99/1000, LR 0.000284
Train loss: 0.9413;  Loss pred: 0.9413; Loss self: 0.0000; time: 0.27s
Val loss: 0.6265 score: 0.7752 time: 0.16s
Test loss: 0.6246 score: 0.7812 time: 0.16s
Epoch 100/1000, LR 0.000284
Train loss: 0.9393;  Loss pred: 0.9393; Loss self: 0.0000; time: 0.28s
Val loss: 0.6254 score: 0.8140 time: 0.17s
Test loss: 0.6237 score: 0.8281 time: 0.16s
Epoch 101/1000, LR 0.000284
Train loss: 0.9367;  Loss pred: 0.9367; Loss self: 0.0000; time: 0.29s
Val loss: 0.6239 score: 0.8450 time: 0.17s
Test loss: 0.6223 score: 0.8594 time: 0.16s
Epoch 102/1000, LR 0.000284
Train loss: 0.9356;  Loss pred: 0.9356; Loss self: 0.0000; time: 0.28s
Val loss: 0.6211 score: 0.8682 time: 0.17s
Test loss: 0.6197 score: 0.8750 time: 0.16s
Epoch 103/1000, LR 0.000284
Train loss: 0.9330;  Loss pred: 0.9330; Loss self: 0.0000; time: 0.33s
Val loss: 0.6194 score: 0.8760 time: 0.17s
Test loss: 0.6181 score: 0.8906 time: 0.16s
Epoch 104/1000, LR 0.000284
Train loss: 0.9315;  Loss pred: 0.9315; Loss self: 0.0000; time: 0.38s
Val loss: 0.6172 score: 0.8992 time: 0.17s
Test loss: 0.6159 score: 0.8906 time: 0.16s
Epoch 105/1000, LR 0.000284
Train loss: 0.9301;  Loss pred: 0.9301; Loss self: 0.0000; time: 0.28s
Val loss: 0.6128 score: 0.8992 time: 0.17s
Test loss: 0.6116 score: 0.8906 time: 0.16s
Epoch 106/1000, LR 0.000283
Train loss: 0.9259;  Loss pred: 0.9259; Loss self: 0.0000; time: 0.27s
Val loss: 0.6057 score: 0.8760 time: 0.17s
Test loss: 0.6044 score: 0.8906 time: 0.15s
Epoch 107/1000, LR 0.000283
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 0.26s
Val loss: 0.5990 score: 0.8217 time: 0.16s
Test loss: 0.5975 score: 0.8516 time: 0.16s
Epoch 108/1000, LR 0.000283
Train loss: 0.9169;  Loss pred: 0.9169; Loss self: 0.0000; time: 0.41s
Val loss: 0.5948 score: 0.7829 time: 0.24s
Test loss: 0.5932 score: 0.7969 time: 0.20s
Epoch 109/1000, LR 0.000283
Train loss: 0.9166;  Loss pred: 0.9166; Loss self: 0.0000; time: 0.42s
Val loss: 0.5931 score: 0.7519 time: 0.16s
Test loss: 0.5916 score: 0.7578 time: 0.15s
Epoch 110/1000, LR 0.000283
Train loss: 0.9153;  Loss pred: 0.9153; Loss self: 0.0000; time: 0.27s
Val loss: 0.5901 score: 0.7597 time: 0.16s
Test loss: 0.5886 score: 0.7578 time: 0.15s
Epoch 111/1000, LR 0.000283
Train loss: 0.9131;  Loss pred: 0.9131; Loss self: 0.0000; time: 0.26s
Val loss: 0.5873 score: 0.7519 time: 0.16s
Test loss: 0.5857 score: 0.7578 time: 0.15s
Epoch 112/1000, LR 0.000283
Train loss: 0.9112;  Loss pred: 0.9112; Loss self: 0.0000; time: 0.27s
Val loss: 0.5839 score: 0.7442 time: 0.16s
Test loss: 0.5824 score: 0.7578 time: 0.15s
Epoch 113/1000, LR 0.000282
Train loss: 0.9084;  Loss pred: 0.9084; Loss self: 0.0000; time: 0.26s
Val loss: 0.5786 score: 0.7674 time: 0.16s
Test loss: 0.5771 score: 0.7734 time: 0.15s
Epoch 114/1000, LR 0.000282
Train loss: 0.9045;  Loss pred: 0.9045; Loss self: 0.0000; time: 0.26s
Val loss: 0.5726 score: 0.8062 time: 0.16s
Test loss: 0.5711 score: 0.8125 time: 0.15s
Epoch 115/1000, LR 0.000282
Train loss: 0.9000;  Loss pred: 0.9000; Loss self: 0.0000; time: 0.27s
Val loss: 0.5680 score: 0.8140 time: 0.16s
Test loss: 0.5665 score: 0.8203 time: 0.16s
Epoch 116/1000, LR 0.000282
Train loss: 0.8965;  Loss pred: 0.8965; Loss self: 0.0000; time: 0.28s
Val loss: 0.5633 score: 0.8140 time: 0.21s
Test loss: 0.5620 score: 0.8359 time: 0.19s
Epoch 117/1000, LR 0.000282
Train loss: 0.8931;  Loss pred: 0.8931; Loss self: 0.0000; time: 0.27s
Val loss: 0.5583 score: 0.8217 time: 0.16s
Test loss: 0.5571 score: 0.8359 time: 0.15s
Epoch 118/1000, LR 0.000282
Train loss: 0.8901;  Loss pred: 0.8901; Loss self: 0.0000; time: 0.27s
Val loss: 0.5529 score: 0.8295 time: 0.16s
Test loss: 0.5517 score: 0.8516 time: 0.15s
Epoch 119/1000, LR 0.000282
Train loss: 0.8856;  Loss pred: 0.8856; Loss self: 0.0000; time: 0.26s
Val loss: 0.5477 score: 0.8295 time: 0.15s
Test loss: 0.5466 score: 0.8516 time: 0.15s
Epoch 120/1000, LR 0.000281
Train loss: 0.8830;  Loss pred: 0.8830; Loss self: 0.0000; time: 0.27s
Val loss: 0.5424 score: 0.8372 time: 0.16s
Test loss: 0.5414 score: 0.8516 time: 0.17s
Epoch 121/1000, LR 0.000281
Train loss: 0.8798;  Loss pred: 0.8798; Loss self: 0.0000; time: 0.39s
Val loss: 0.5368 score: 0.8605 time: 0.17s
Test loss: 0.5359 score: 0.8828 time: 0.16s
Epoch 122/1000, LR 0.000281
Train loss: 0.8748;  Loss pred: 0.8748; Loss self: 0.0000; time: 0.28s
Val loss: 0.5314 score: 0.8760 time: 0.17s
Test loss: 0.5307 score: 0.8906 time: 0.16s
Epoch 123/1000, LR 0.000281
Train loss: 0.8707;  Loss pred: 0.8707; Loss self: 0.0000; time: 0.27s
Val loss: 0.5268 score: 0.8915 time: 0.16s
Test loss: 0.5264 score: 0.8906 time: 0.15s
Epoch 124/1000, LR 0.000281
Train loss: 0.8683;  Loss pred: 0.8683; Loss self: 0.0000; time: 0.27s
Val loss: 0.5243 score: 0.9225 time: 0.16s
Test loss: 0.5244 score: 0.8984 time: 0.15s
Epoch 125/1000, LR 0.000281
Train loss: 0.8662;  Loss pred: 0.8662; Loss self: 0.0000; time: 0.27s
Val loss: 0.5205 score: 0.9225 time: 0.16s
Test loss: 0.5208 score: 0.8984 time: 0.15s
Epoch 126/1000, LR 0.000280
Train loss: 0.8618;  Loss pred: 0.8618; Loss self: 0.0000; time: 0.27s
Val loss: 0.5127 score: 0.9225 time: 0.17s
Test loss: 0.5131 score: 0.8984 time: 0.16s
Epoch 127/1000, LR 0.000280
Train loss: 0.8578;  Loss pred: 0.8578; Loss self: 0.0000; time: 0.32s
Val loss: 0.5054 score: 0.9147 time: 0.18s
Test loss: 0.5058 score: 0.8984 time: 0.15s
Epoch 128/1000, LR 0.000280
Train loss: 0.8524;  Loss pred: 0.8524; Loss self: 0.0000; time: 0.40s
Val loss: 0.4997 score: 0.9147 time: 0.16s
Test loss: 0.5003 score: 0.8984 time: 0.16s
Epoch 129/1000, LR 0.000280
Train loss: 0.8490;  Loss pred: 0.8490; Loss self: 0.0000; time: 0.27s
Val loss: 0.4942 score: 0.9225 time: 0.16s
Test loss: 0.4950 score: 0.8984 time: 0.25s
Epoch 130/1000, LR 0.000280
Train loss: 0.8456;  Loss pred: 0.8456; Loss self: 0.0000; time: 0.26s
Val loss: 0.4906 score: 0.9225 time: 0.16s
Test loss: 0.4919 score: 0.9062 time: 0.16s
Epoch 131/1000, LR 0.000280
Train loss: 0.8430;  Loss pred: 0.8430; Loss self: 0.0000; time: 0.28s
Val loss: 0.4893 score: 0.9380 time: 0.17s
Test loss: 0.4909 score: 0.9297 time: 0.16s
Epoch 132/1000, LR 0.000279
Train loss: 0.8424;  Loss pred: 0.8424; Loss self: 0.0000; time: 0.29s
Val loss: 0.4953 score: 0.9457 time: 0.17s
Test loss: 0.4963 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 133/1000, LR 0.000279
Train loss: 0.8449;  Loss pred: 0.8449; Loss self: 0.0000; time: 0.33s
Val loss: 0.5048 score: 0.9457 time: 0.17s
Test loss: 0.5046 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 134/1000, LR 0.000279
Train loss: 0.8498;  Loss pred: 0.8498; Loss self: 0.0000; time: 0.34s
Val loss: 0.5051 score: 0.8915 time: 0.17s
Test loss: 0.5047 score: 0.8906 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 135/1000, LR 0.000279
Train loss: 0.8481;  Loss pred: 0.8481; Loss self: 0.0000; time: 0.26s
Val loss: 0.4944 score: 0.9302 time: 0.15s
Test loss: 0.4944 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 136/1000, LR 0.000279
Train loss: 0.8415;  Loss pred: 0.8415; Loss self: 0.0000; time: 0.26s
Val loss: 0.4813 score: 0.9612 time: 0.16s
Test loss: 0.4823 score: 0.9453 time: 0.15s
Epoch 137/1000, LR 0.000279
Train loss: 0.8326;  Loss pred: 0.8326; Loss self: 0.0000; time: 0.27s
Val loss: 0.4673 score: 0.9457 time: 0.16s
Test loss: 0.4695 score: 0.9531 time: 0.15s
Epoch 138/1000, LR 0.000278
Train loss: 0.8234;  Loss pred: 0.8234; Loss self: 0.0000; time: 0.26s
Val loss: 0.4557 score: 0.9380 time: 0.16s
Test loss: 0.4587 score: 0.9531 time: 0.15s
Epoch 139/1000, LR 0.000278
Train loss: 0.8197;  Loss pred: 0.8197; Loss self: 0.0000; time: 0.27s
Val loss: 0.4451 score: 0.9457 time: 0.16s
Test loss: 0.4488 score: 0.9531 time: 0.15s
Epoch 140/1000, LR 0.000278
Train loss: 0.8089;  Loss pred: 0.8089; Loss self: 0.0000; time: 0.26s
Val loss: 0.4343 score: 0.9302 time: 0.15s
Test loss: 0.4386 score: 0.9297 time: 0.15s
Epoch 141/1000, LR 0.000278
Train loss: 0.8059;  Loss pred: 0.8059; Loss self: 0.0000; time: 0.31s
Val loss: 0.4259 score: 0.9380 time: 0.16s
Test loss: 0.4306 score: 0.9297 time: 0.15s
Epoch 142/1000, LR 0.000278
Train loss: 0.7993;  Loss pred: 0.7993; Loss self: 0.0000; time: 0.34s
Val loss: 0.4174 score: 0.9302 time: 0.16s
Test loss: 0.4224 score: 0.9219 time: 0.15s
Epoch 143/1000, LR 0.000277
Train loss: 0.7936;  Loss pred: 0.7936; Loss self: 0.0000; time: 0.26s
Val loss: 0.4102 score: 0.9225 time: 0.15s
Test loss: 0.4156 score: 0.9141 time: 0.15s
Epoch 144/1000, LR 0.000277
Train loss: 0.7899;  Loss pred: 0.7899; Loss self: 0.0000; time: 0.26s
Val loss: 0.4033 score: 0.9225 time: 0.17s
Test loss: 0.4090 score: 0.9062 time: 0.17s
Epoch 145/1000, LR 0.000277
Train loss: 0.7865;  Loss pred: 0.7865; Loss self: 0.0000; time: 0.27s
Val loss: 0.3975 score: 0.9225 time: 0.16s
Test loss: 0.4034 score: 0.8984 time: 0.16s
Epoch 146/1000, LR 0.000277
Train loss: 0.7824;  Loss pred: 0.7824; Loss self: 0.0000; time: 0.27s
Val loss: 0.3923 score: 0.9225 time: 0.19s
Test loss: 0.3985 score: 0.8984 time: 0.16s
Epoch 147/1000, LR 0.000277
Train loss: 0.7804;  Loss pred: 0.7804; Loss self: 0.0000; time: 0.35s
Val loss: 0.3855 score: 0.9225 time: 0.17s
Test loss: 0.3921 score: 0.8984 time: 0.16s
Epoch 148/1000, LR 0.000277
Train loss: 0.7769;  Loss pred: 0.7769; Loss self: 0.0000; time: 0.28s
Val loss: 0.3781 score: 0.9225 time: 0.17s
Test loss: 0.3852 score: 0.8984 time: 0.16s
Epoch 149/1000, LR 0.000276
Train loss: 0.7703;  Loss pred: 0.7703; Loss self: 0.0000; time: 0.28s
Val loss: 0.3721 score: 0.9302 time: 0.17s
Test loss: 0.3798 score: 0.9219 time: 0.16s
Epoch 150/1000, LR 0.000276
Train loss: 0.7666;  Loss pred: 0.7666; Loss self: 0.0000; time: 0.28s
Val loss: 0.3740 score: 0.9535 time: 0.17s
Test loss: 0.3819 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 151/1000, LR 0.000276
Train loss: 0.7671;  Loss pred: 0.7671; Loss self: 0.0000; time: 0.27s
Val loss: 0.3770 score: 0.9535 time: 0.16s
Test loss: 0.3845 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 152/1000, LR 0.000276
Train loss: 0.7657;  Loss pred: 0.7657; Loss self: 0.0000; time: 0.27s
Val loss: 0.3731 score: 0.9535 time: 0.19s
Test loss: 0.3809 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 153/1000, LR 0.000276
Train loss: 0.7640;  Loss pred: 0.7640; Loss self: 0.0000; time: 0.41s
Val loss: 0.3716 score: 0.9535 time: 0.16s
Test loss: 0.3792 score: 0.9531 time: 0.15s
Epoch 154/1000, LR 0.000275
Train loss: 0.7627;  Loss pred: 0.7627; Loss self: 0.0000; time: 0.27s
Val loss: 0.3743 score: 0.9457 time: 0.16s
Test loss: 0.3814 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 155/1000, LR 0.000275
Train loss: 0.7634;  Loss pred: 0.7634; Loss self: 0.0000; time: 0.26s
Val loss: 0.3684 score: 0.9457 time: 0.16s
Test loss: 0.3762 score: 0.9219 time: 0.15s
Epoch 156/1000, LR 0.000275
Train loss: 0.7584;  Loss pred: 0.7584; Loss self: 0.0000; time: 0.26s
Val loss: 0.3570 score: 0.9535 time: 0.16s
Test loss: 0.3661 score: 0.9531 time: 0.15s
Epoch 157/1000, LR 0.000275
Train loss: 0.7520;  Loss pred: 0.7520; Loss self: 0.0000; time: 0.27s
Val loss: 0.3441 score: 0.9535 time: 0.16s
Test loss: 0.3545 score: 0.9453 time: 0.15s
Epoch 158/1000, LR 0.000275
Train loss: 0.7424;  Loss pred: 0.7424; Loss self: 0.0000; time: 0.35s
Val loss: 0.3301 score: 0.9535 time: 0.21s
Test loss: 0.3418 score: 0.9531 time: 0.16s
Epoch 159/1000, LR 0.000274
Train loss: 0.7337;  Loss pred: 0.7337; Loss self: 0.0000; time: 0.27s
Val loss: 0.3207 score: 0.9457 time: 0.16s
Test loss: 0.3330 score: 0.9297 time: 0.15s
Epoch 160/1000, LR 0.000274
Train loss: 0.7312;  Loss pred: 0.7312; Loss self: 0.0000; time: 0.27s
Val loss: 0.3161 score: 0.9302 time: 0.16s
Test loss: 0.3288 score: 0.9141 time: 0.15s
Epoch 161/1000, LR 0.000274
Train loss: 0.7284;  Loss pred: 0.7284; Loss self: 0.0000; time: 0.27s
Val loss: 0.3136 score: 0.9225 time: 0.16s
Test loss: 0.3266 score: 0.9062 time: 0.15s
Epoch 162/1000, LR 0.000274
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 0.27s
Val loss: 0.3088 score: 0.9225 time: 0.16s
Test loss: 0.3225 score: 0.9062 time: 0.15s
Epoch 163/1000, LR 0.000273
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.29s
Val loss: 0.3057 score: 0.9225 time: 0.16s
Test loss: 0.3199 score: 0.8984 time: 0.17s
Epoch 164/1000, LR 0.000273
Train loss: 0.7215;  Loss pred: 0.7215; Loss self: 0.0000; time: 0.46s
Val loss: 0.2993 score: 0.9225 time: 0.24s
Test loss: 0.3141 score: 0.9062 time: 0.20s
Epoch 165/1000, LR 0.000273
Train loss: 0.7173;  Loss pred: 0.7173; Loss self: 0.0000; time: 0.33s
Val loss: 0.2923 score: 0.9535 time: 0.15s
Test loss: 0.3083 score: 0.9375 time: 0.15s
Epoch 166/1000, LR 0.000273
Train loss: 0.7099;  Loss pred: 0.7099; Loss self: 0.0000; time: 0.26s
Val loss: 0.2943 score: 0.9535 time: 0.15s
Test loss: 0.3104 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 167/1000, LR 0.000273
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.26s
Val loss: 0.2972 score: 0.9535 time: 0.15s
Test loss: 0.3133 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 168/1000, LR 0.000272
Train loss: 0.7122;  Loss pred: 0.7122; Loss self: 0.0000; time: 0.26s
Val loss: 0.2929 score: 0.9535 time: 0.16s
Test loss: 0.3096 score: 0.9609 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 169/1000, LR 0.000272
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.27s
Val loss: 0.2824 score: 0.9535 time: 0.16s
Test loss: 0.3005 score: 0.9453 time: 0.17s
Epoch 170/1000, LR 0.000272
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.37s
Val loss: 0.2722 score: 0.9380 time: 0.22s
Test loss: 0.2917 score: 0.9453 time: 0.15s
Epoch 171/1000, LR 0.000272
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.27s
Val loss: 0.2697 score: 0.9302 time: 0.16s
Test loss: 0.2898 score: 0.9219 time: 0.15s
Epoch 172/1000, LR 0.000271
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.27s
Val loss: 0.2722 score: 0.9225 time: 0.16s
Test loss: 0.2925 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000271
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.41s
Val loss: 0.2706 score: 0.9225 time: 0.23s
Test loss: 0.2916 score: 0.8984 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000271
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.47s
Val loss: 0.2634 score: 0.9302 time: 0.22s
Test loss: 0.2853 score: 0.9062 time: 0.16s
Epoch 175/1000, LR 0.000271
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.27s
Val loss: 0.2584 score: 0.9302 time: 0.16s
Test loss: 0.2812 score: 0.9062 time: 0.15s
Epoch 176/1000, LR 0.000271
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.27s
Val loss: 0.2530 score: 0.9302 time: 0.16s
Test loss: 0.2768 score: 0.9141 time: 0.15s
Epoch 177/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.27s
Val loss: 0.2455 score: 0.9457 time: 0.16s
Test loss: 0.2704 score: 0.9297 time: 0.15s
Epoch 178/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.27s
Val loss: 0.2499 score: 0.9535 time: 0.16s
Test loss: 0.2744 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 179/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.27s
Val loss: 0.2671 score: 0.9302 time: 0.16s
Test loss: 0.2894 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 180/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.28s
Val loss: 0.2652 score: 0.9225 time: 0.16s
Test loss: 0.2882 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 181/1000, LR 0.000269
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.32s
Val loss: 0.2486 score: 0.9612 time: 0.16s
Test loss: 0.2745 score: 0.9609 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 182/1000, LR 0.000269
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.28s
Val loss: 0.2355 score: 0.9612 time: 0.16s
Test loss: 0.2640 score: 0.9375 time: 0.15s
Epoch 183/1000, LR 0.000269
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.26s
Val loss: 0.2313 score: 0.9612 time: 0.16s
Test loss: 0.2610 score: 0.9453 time: 0.15s
Epoch 184/1000, LR 0.000269
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.27s
Val loss: 0.2279 score: 0.9612 time: 0.16s
Test loss: 0.2586 score: 0.9453 time: 0.15s
Epoch 185/1000, LR 0.000268
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.27s
Val loss: 0.2253 score: 0.9612 time: 0.16s
Test loss: 0.2567 score: 0.9453 time: 0.15s
Epoch 186/1000, LR 0.000268
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.27s
Val loss: 0.2240 score: 0.9612 time: 0.16s
Test loss: 0.2559 score: 0.9375 time: 0.16s
Epoch 187/1000, LR 0.000268
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.28s
Val loss: 0.2264 score: 0.9535 time: 0.20s
Test loss: 0.2582 score: 0.9453 time: 0.20s
     INFO: Early stopping counter 1 of 20
Epoch 188/1000, LR 0.000268
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.27s
Val loss: 0.2283 score: 0.9612 time: 0.15s
Test loss: 0.2600 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 189/1000, LR 0.000267
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.26s
Val loss: 0.2264 score: 0.9612 time: 0.16s
Test loss: 0.2587 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 190/1000, LR 0.000267
Train loss: 0.6547;  Loss pred: 0.6547; Loss self: 0.0000; time: 0.29s
Val loss: 0.2172 score: 0.9535 time: 0.21s
Test loss: 0.2515 score: 0.9453 time: 0.21s
Epoch 191/1000, LR 0.000267
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.47s
Val loss: 0.2109 score: 0.9612 time: 0.24s
Test loss: 0.2466 score: 0.9375 time: 0.20s
Epoch 192/1000, LR 0.000267
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.47s
Val loss: 0.2131 score: 0.9535 time: 0.24s
Test loss: 0.2485 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 193/1000, LR 0.000266
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.27s
Val loss: 0.2138 score: 0.9612 time: 0.16s
Test loss: 0.2493 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 194/1000, LR 0.000266
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.27s
Val loss: 0.2099 score: 0.9535 time: 0.17s
Test loss: 0.2462 score: 0.9453 time: 0.16s
Epoch 195/1000, LR 0.000266
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.27s
Val loss: 0.2041 score: 0.9612 time: 0.17s
Test loss: 0.2418 score: 0.9375 time: 0.16s
Epoch 196/1000, LR 0.000266
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.27s
Val loss: 0.2012 score: 0.9612 time: 0.17s
Test loss: 0.2397 score: 0.9375 time: 0.18s
Epoch 197/1000, LR 0.000265
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.41s
Val loss: 0.2008 score: 0.9535 time: 0.18s
Test loss: 0.2395 score: 0.9375 time: 0.16s
Epoch 198/1000, LR 0.000265
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.28s
Val loss: 0.2040 score: 0.9612 time: 0.16s
Test loss: 0.2421 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 199/1000, LR 0.000265
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 0.28s
Val loss: 0.2024 score: 0.9612 time: 0.17s
Test loss: 0.2409 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 200/1000, LR 0.000265
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.28s
Val loss: 0.1949 score: 0.9535 time: 0.17s
Test loss: 0.2350 score: 0.9375 time: 0.16s
Epoch 201/1000, LR 0.000264
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.28s
Val loss: 0.1895 score: 0.9612 time: 0.16s
Test loss: 0.2312 score: 0.9375 time: 0.16s
Epoch 202/1000, LR 0.000264
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.29s
Val loss: 0.1867 score: 0.9690 time: 0.16s
Test loss: 0.2296 score: 0.9453 time: 0.15s
Epoch 203/1000, LR 0.000264
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.36s
Val loss: 0.1852 score: 0.9690 time: 0.16s
Test loss: 0.2289 score: 0.9297 time: 0.15s
Epoch 204/1000, LR 0.000264
Train loss: 0.6295;  Loss pred: 0.6295; Loss self: 0.0000; time: 0.27s
Val loss: 0.1848 score: 0.9612 time: 0.16s
Test loss: 0.2293 score: 0.9297 time: 0.16s
Epoch 205/1000, LR 0.000263
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.28s
Val loss: 0.1881 score: 0.9457 time: 0.16s
Test loss: 0.2329 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 206/1000, LR 0.000263
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.28s
Val loss: 0.1906 score: 0.9380 time: 0.16s
Test loss: 0.2357 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 207/1000, LR 0.000263
Train loss: 0.6374;  Loss pred: 0.6374; Loss self: 0.0000; time: 0.27s
Val loss: 0.1894 score: 0.9380 time: 0.18s
Test loss: 0.2351 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 208/1000, LR 0.000263
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.40s
Val loss: 0.1850 score: 0.9457 time: 0.17s
Test loss: 0.2316 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 209/1000, LR 0.000262
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.28s
Val loss: 0.1778 score: 0.9535 time: 0.16s
Test loss: 0.2254 score: 0.9297 time: 0.16s
Epoch 210/1000, LR 0.000262
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.26s
Val loss: 0.1731 score: 0.9690 time: 0.15s
Test loss: 0.2207 score: 0.9453 time: 0.15s
Epoch 211/1000, LR 0.000262
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.26s
Val loss: 0.1734 score: 0.9612 time: 0.16s
Test loss: 0.2206 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 212/1000, LR 0.000261
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.26s
Val loss: 0.1747 score: 0.9535 time: 0.16s
Test loss: 0.2216 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 213/1000, LR 0.000261
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.45s
Val loss: 0.1743 score: 0.9535 time: 0.23s
Test loss: 0.2215 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 214/1000, LR 0.000261
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 0.26s
Val loss: 0.1709 score: 0.9612 time: 0.16s
Test loss: 0.2193 score: 0.9375 time: 0.15s
Epoch 215/1000, LR 0.000261
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.27s
Val loss: 0.1702 score: 0.9612 time: 0.16s
Test loss: 0.2191 score: 0.9375 time: 0.15s
Epoch 216/1000, LR 0.000260
Train loss: 0.6177;  Loss pred: 0.6177; Loss self: 0.0000; time: 0.26s
Val loss: 0.1815 score: 0.9457 time: 0.16s
Test loss: 0.2280 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 217/1000, LR 0.000260
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.26s
Val loss: 0.1941 score: 0.9225 time: 0.15s
Test loss: 0.2386 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 218/1000, LR 0.000260
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.31s
Val loss: 0.1980 score: 0.9225 time: 0.16s
Test loss: 0.2422 score: 0.8984 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 219/1000, LR 0.000260
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.30s
Val loss: 0.1901 score: 0.9302 time: 0.16s
Test loss: 0.2361 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 220/1000, LR 0.000259
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.26s
Val loss: 0.1772 score: 0.9380 time: 0.16s
Test loss: 0.2260 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 221/1000, LR 0.000259
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.27s
Val loss: 0.1692 score: 0.9535 time: 0.16s
Test loss: 0.2203 score: 0.9453 time: 0.15s
Epoch 222/1000, LR 0.000259
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.32s
Val loss: 0.1641 score: 0.9612 time: 0.16s
Test loss: 0.2171 score: 0.9375 time: 0.15s
Epoch 223/1000, LR 0.000258
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.26s
Val loss: 0.1601 score: 0.9690 time: 0.18s
Test loss: 0.2155 score: 0.9297 time: 0.16s
Epoch 224/1000, LR 0.000258
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.36s
Val loss: 0.1617 score: 0.9690 time: 0.16s
Test loss: 0.2186 score: 0.9297 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 225/1000, LR 0.000258
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.26s
Val loss: 0.1637 score: 0.9612 time: 0.16s
Test loss: 0.2211 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 226/1000, LR 0.000258
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.26s
Val loss: 0.1636 score: 0.9612 time: 0.17s
Test loss: 0.2215 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 227/1000, LR 0.000257
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.35s
Val loss: 0.1612 score: 0.9612 time: 0.16s
Test loss: 0.2197 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 228/1000, LR 0.000257
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.27s
Val loss: 0.1580 score: 0.9690 time: 0.16s
Test loss: 0.2170 score: 0.9297 time: 0.15s
Epoch 229/1000, LR 0.000257
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.28s
Val loss: 0.1557 score: 0.9690 time: 0.17s
Test loss: 0.2150 score: 0.9375 time: 0.26s
Epoch 230/1000, LR 0.000256
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.26s
Val loss: 0.1545 score: 0.9690 time: 0.16s
Test loss: 0.2140 score: 0.9375 time: 0.15s
Epoch 231/1000, LR 0.000256
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.32s
Val loss: 0.1536 score: 0.9612 time: 0.17s
Test loss: 0.2133 score: 0.9375 time: 0.19s
Epoch 232/1000, LR 0.000256
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.32s
Val loss: 0.1528 score: 0.9690 time: 0.16s
Test loss: 0.2126 score: 0.9375 time: 0.16s
Epoch 233/1000, LR 0.000255
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.27s
Val loss: 0.1520 score: 0.9690 time: 0.16s
Test loss: 0.2122 score: 0.9297 time: 0.16s
Epoch 234/1000, LR 0.000255
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.27s
Val loss: 0.1515 score: 0.9690 time: 0.16s
Test loss: 0.2121 score: 0.9297 time: 0.16s
Epoch 235/1000, LR 0.000255
Train loss: 0.5982;  Loss pred: 0.5982; Loss self: 0.0000; time: 0.28s
Val loss: 0.1511 score: 0.9690 time: 0.16s
Test loss: 0.2125 score: 0.9375 time: 0.16s
Epoch 236/1000, LR 0.000255
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.27s
Val loss: 0.1506 score: 0.9612 time: 0.16s
Test loss: 0.2126 score: 0.9375 time: 0.16s
Epoch 237/1000, LR 0.000254
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.28s
Val loss: 0.1501 score: 0.9690 time: 0.16s
Test loss: 0.2128 score: 0.9375 time: 0.16s
Epoch 238/1000, LR 0.000254
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.27s
Val loss: 0.1494 score: 0.9690 time: 0.19s
Test loss: 0.2125 score: 0.9375 time: 0.16s
Epoch 239/1000, LR 0.000254
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.41s
Val loss: 0.1483 score: 0.9690 time: 0.17s
Test loss: 0.2103 score: 0.9375 time: 0.15s
Epoch 240/1000, LR 0.000253
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.28s
Val loss: 0.1634 score: 0.9302 time: 0.16s
Test loss: 0.2202 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 241/1000, LR 0.000253
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 0.27s
Val loss: 0.1928 score: 0.9225 time: 0.16s
Test loss: 0.2449 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 242/1000, LR 0.000253
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.27s
Val loss: 0.2049 score: 0.9225 time: 0.17s
Test loss: 0.2558 score: 0.8750 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 243/1000, LR 0.000252
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.27s
Val loss: 0.1939 score: 0.9225 time: 0.16s
Test loss: 0.2466 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 244/1000, LR 0.000252
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.27s
Val loss: 0.1755 score: 0.9225 time: 0.17s
Test loss: 0.2312 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 245/1000, LR 0.000252
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 0.27s
Val loss: 0.1602 score: 0.9302 time: 0.21s
Test loss: 0.2192 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 246/1000, LR 0.000252
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.40s
Val loss: 0.1506 score: 0.9535 time: 0.17s
Test loss: 0.2125 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 247/1000, LR 0.000251
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.28s
Val loss: 0.1447 score: 0.9612 time: 0.17s
Test loss: 0.2093 score: 0.9375 time: 0.17s
Epoch 248/1000, LR 0.000251
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.27s
Val loss: 0.1431 score: 0.9612 time: 0.17s
Test loss: 0.2088 score: 0.9375 time: 0.16s
Epoch 249/1000, LR 0.000251
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.28s
Val loss: 0.1421 score: 0.9690 time: 0.17s
Test loss: 0.2088 score: 0.9375 time: 0.16s
Epoch 250/1000, LR 0.000250
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.28s
Val loss: 0.1414 score: 0.9690 time: 0.17s
Test loss: 0.2089 score: 0.9375 time: 0.16s
Epoch 251/1000, LR 0.000250
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 0.28s
Val loss: 0.1408 score: 0.9690 time: 0.17s
Test loss: 0.2091 score: 0.9297 time: 0.17s
Epoch 252/1000, LR 0.000250
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.33s
Val loss: 0.1399 score: 0.9690 time: 0.18s
Test loss: 0.2088 score: 0.9297 time: 0.16s
Epoch 253/1000, LR 0.000249
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.35s
Val loss: 0.1406 score: 0.9612 time: 0.17s
Test loss: 0.2080 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 254/1000, LR 0.000249
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.28s
Val loss: 0.1569 score: 0.9302 time: 0.18s
Test loss: 0.2194 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 255/1000, LR 0.000249
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.28s
Val loss: 0.1757 score: 0.9225 time: 0.17s
Test loss: 0.2350 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 256/1000, LR 0.000248
Train loss: nan;  Loss pred: nan; Loss self: 0.0000; time: 0.28s
Val loss: 0.1820 score: 0.9225 time: 0.17s
Test loss: 0.2407 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 257/1000, LR 0.000248
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.29s
Val loss: 0.1743 score: 0.9225 time: 0.17s
Test loss: 0.2344 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 258/1000, LR 0.000248
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.31s
Val loss: 0.1612 score: 0.9302 time: 0.18s
Test loss: 0.2235 score: 0.9219 time: 0.22s
     INFO: Early stopping counter 6 of 20
Epoch 259/1000, LR 0.000247
Train loss: 0.5916;  Loss pred: 0.5916; Loss self: 0.0000; time: 0.29s
Val loss: 0.1498 score: 0.9302 time: 0.17s
Test loss: 0.2147 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 260/1000, LR 0.000247
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.29s
Val loss: 0.1428 score: 0.9535 time: 0.17s
Test loss: 0.2099 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 261/1000, LR 0.000247
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 0.29s
Val loss: 0.1378 score: 0.9690 time: 0.17s
Test loss: 0.2070 score: 0.9453 time: 0.17s
Epoch 262/1000, LR 0.000246
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.29s
Val loss: 0.1350 score: 0.9612 time: 0.17s
Test loss: 0.2062 score: 0.9375 time: 0.17s
Epoch 263/1000, LR 0.000246
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.30s
Val loss: 0.1349 score: 0.9690 time: 0.18s
Test loss: 0.2075 score: 0.9297 time: 0.21s
Epoch 264/1000, LR 0.000246
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.50s
Val loss: 0.1359 score: 0.9612 time: 0.25s
Test loss: 0.2093 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 265/1000, LR 0.000245
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 0.28s
Val loss: 0.1368 score: 0.9690 time: 0.17s
Test loss: 0.2109 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 266/1000, LR 0.000245
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.28s
Val loss: 0.1370 score: 0.9690 time: 0.17s
Test loss: 0.2114 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 267/1000, LR 0.000245
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.42s
Val loss: 0.1361 score: 0.9690 time: 0.25s
Test loss: 0.2107 score: 0.9375 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 268/1000, LR 0.000244
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.49s
Val loss: 0.1348 score: 0.9612 time: 0.25s
Test loss: 0.2095 score: 0.9375 time: 0.21s
Epoch 269/1000, LR 0.000244
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.37s
Val loss: 0.1331 score: 0.9690 time: 0.17s
Test loss: 0.2076 score: 0.9297 time: 0.16s
Epoch 270/1000, LR 0.000244
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.29s
Val loss: 0.1322 score: 0.9690 time: 0.17s
Test loss: 0.2062 score: 0.9375 time: 0.17s
Epoch 271/1000, LR 0.000243
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 0.28s
Val loss: 0.1317 score: 0.9690 time: 0.17s
Test loss: 0.2059 score: 0.9375 time: 0.16s
Epoch 272/1000, LR 0.000243
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.39s
Val loss: 0.1311 score: 0.9690 time: 0.25s
Test loss: 0.2057 score: 0.9375 time: 0.22s
Epoch 273/1000, LR 0.000243
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.48s
Val loss: 0.1307 score: 0.9690 time: 0.25s
Test loss: 0.2064 score: 0.9375 time: 0.21s
Epoch 274/1000, LR 0.000242
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.33s
Val loss: 0.1302 score: 0.9690 time: 0.17s
Test loss: 0.2055 score: 0.9375 time: 0.16s
Epoch 275/1000, LR 0.000242
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.28s
Val loss: 0.1372 score: 0.9535 time: 0.17s
Test loss: 0.2095 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 276/1000, LR 0.000242
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.28s
Val loss: 0.1473 score: 0.9302 time: 0.17s
Test loss: 0.2175 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 277/1000, LR 0.000241
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.28s
Val loss: 0.1612 score: 0.9225 time: 0.17s
Test loss: 0.2294 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 278/1000, LR 0.000241
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.28s
Val loss: 0.1825 score: 0.9225 time: 0.17s
Test loss: 0.2483 score: 0.8906 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 279/1000, LR 0.000241
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.28s
Val loss: 0.1775 score: 0.9225 time: 0.17s
Test loss: 0.2443 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 280/1000, LR 0.000240
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.32s
Val loss: 0.1638 score: 0.9225 time: 0.18s
Test loss: 0.2328 score: 0.8984 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 281/1000, LR 0.000240
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.32s
Val loss: 0.1485 score: 0.9302 time: 0.17s
Test loss: 0.2203 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 282/1000, LR 0.000240
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.28s
Val loss: 0.1368 score: 0.9380 time: 0.17s
Test loss: 0.2115 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 283/1000, LR 0.000239
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.28s
Val loss: 0.1303 score: 0.9690 time: 0.17s
Test loss: 0.2076 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 284/1000, LR 0.000239
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.30s
Val loss: 0.1286 score: 0.9690 time: 0.18s
Test loss: 0.2081 score: 0.9297 time: 0.22s
Epoch 285/1000, LR 0.000239
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 0.29s
Val loss: 0.1287 score: 0.9690 time: 0.17s
Test loss: 0.2093 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 286/1000, LR 0.000238
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.29s
Val loss: 0.1278 score: 0.9690 time: 0.17s
Test loss: 0.2084 score: 0.9297 time: 0.16s
Epoch 287/1000, LR 0.000238
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.30s
Val loss: 0.1271 score: 0.9690 time: 0.23s
Test loss: 0.2075 score: 0.9375 time: 0.17s
Epoch 288/1000, LR 0.000237
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.28s
Val loss: 0.1297 score: 0.9690 time: 0.18s
Test loss: 0.2085 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 289/1000, LR 0.000237
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.28s
Val loss: 0.1473 score: 0.9302 time: 0.17s
Test loss: 0.2221 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 290/1000, LR 0.000237
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.28s
Val loss: 0.1514 score: 0.9225 time: 0.17s
Test loss: 0.2260 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 291/1000, LR 0.000236
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.28s
Val loss: 0.1389 score: 0.9302 time: 0.17s
Test loss: 0.2162 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 292/1000, LR 0.000236
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.28s
Val loss: 0.1311 score: 0.9612 time: 0.17s
Test loss: 0.2106 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 293/1000, LR 0.000236
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.28s
Val loss: 0.1272 score: 0.9690 time: 0.17s
Test loss: 0.2086 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 294/1000, LR 0.000235
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.29s
Val loss: 0.1262 score: 0.9690 time: 0.26s
Test loss: 0.2090 score: 0.9375 time: 0.17s
Epoch 295/1000, LR 0.000235
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.28s
Val loss: 0.1263 score: 0.9690 time: 0.17s
Test loss: 0.2102 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 296/1000, LR 0.000235
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.29s
Val loss: 0.1259 score: 0.9690 time: 0.17s
Test loss: 0.2098 score: 0.9297 time: 0.16s
Epoch 297/1000, LR 0.000234
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.29s
Val loss: 0.1279 score: 0.9690 time: 0.17s
Test loss: 0.2104 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 298/1000, LR 0.000234
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.28s
Val loss: 0.1293 score: 0.9612 time: 0.17s
Test loss: 0.2116 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 299/1000, LR 0.000234
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 0.32s
Val loss: 0.1262 score: 0.9767 time: 0.23s
Test loss: 0.2100 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 300/1000, LR 0.000233
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.28s
Val loss: 0.1251 score: 0.9690 time: 0.17s
Test loss: 0.2102 score: 0.9375 time: 0.17s
Epoch 301/1000, LR 0.000233
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.28s
Val loss: 0.1248 score: 0.9690 time: 0.17s
Test loss: 0.2110 score: 0.9297 time: 0.16s
Epoch 302/1000, LR 0.000232
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.28s
Val loss: 0.1248 score: 0.9690 time: 0.17s
Test loss: 0.2117 score: 0.9297 time: 0.17s
Epoch 303/1000, LR 0.000232
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.28s
Val loss: 0.1246 score: 0.9690 time: 0.17s
Test loss: 0.2121 score: 0.9297 time: 0.16s
Epoch 304/1000, LR 0.000232
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.28s
Val loss: 0.1241 score: 0.9690 time: 0.17s
Test loss: 0.2118 score: 0.9297 time: 0.17s
Epoch 305/1000, LR 0.000231
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 0.30s
Val loss: 0.1234 score: 0.9690 time: 0.17s
Test loss: 0.2107 score: 0.9375 time: 0.19s
Epoch 306/1000, LR 0.000231
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.32s
Val loss: 0.1237 score: 0.9767 time: 0.17s
Test loss: 0.2104 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 307/1000, LR 0.000231
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.28s
Val loss: 0.1237 score: 0.9767 time: 0.17s
Test loss: 0.2105 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 308/1000, LR 0.000230
Train loss: 0.5581;  Loss pred: 0.5581; Loss self: 0.0000; time: 0.28s
Val loss: 0.1232 score: 0.9767 time: 0.17s
Test loss: 0.2105 score: 0.9375 time: 0.16s
Epoch 309/1000, LR 0.000230
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.28s
Val loss: 0.1227 score: 0.9767 time: 0.17s
Test loss: 0.2106 score: 0.9375 time: 0.16s
Epoch 310/1000, LR 0.000229
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.28s
Val loss: 0.1225 score: 0.9690 time: 0.17s
Test loss: 0.2121 score: 0.9297 time: 0.17s
Epoch 311/1000, LR 0.000229
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.29s
Val loss: 0.1235 score: 0.9690 time: 0.23s
Test loss: 0.2145 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 312/1000, LR 0.000229
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.28s
Val loss: 0.1244 score: 0.9690 time: 0.17s
Test loss: 0.2163 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 313/1000, LR 0.000228
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.30s
Val loss: 0.1264 score: 0.9690 time: 0.18s
Test loss: 0.2195 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 314/1000, LR 0.000228
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.27s
Val loss: 0.1319 score: 0.9690 time: 0.16s
Test loss: 0.2270 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 315/1000, LR 0.000228
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 0.28s
Val loss: 0.1349 score: 0.9612 time: 0.19s
Test loss: 0.2311 score: 0.9297 time: 0.23s
     INFO: Early stopping counter 5 of 20
Epoch 316/1000, LR 0.000227
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.27s
Val loss: 0.1335 score: 0.9690 time: 0.17s
Test loss: 0.2300 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 317/1000, LR 0.000227
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.26s
Val loss: 0.1290 score: 0.9690 time: 0.16s
Test loss: 0.2249 score: 0.9375 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 318/1000, LR 0.000226
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.27s
Val loss: 0.1253 score: 0.9690 time: 0.23s
Test loss: 0.2204 score: 0.9375 time: 0.24s
     INFO: Early stopping counter 8 of 20
Epoch 319/1000, LR 0.000226
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.34s
Val loss: 0.1228 score: 0.9612 time: 0.16s
Test loss: 0.2168 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 320/1000, LR 0.000226
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.27s
Val loss: 0.1215 score: 0.9690 time: 0.16s
Test loss: 0.2135 score: 0.9297 time: 0.15s
Epoch 321/1000, LR 0.000225
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.33s
Val loss: 0.1285 score: 0.9380 time: 0.17s
Test loss: 0.2174 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 322/1000, LR 0.000225
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.26s
Val loss: 0.1424 score: 0.9302 time: 0.17s
Test loss: 0.2295 score: 0.9141 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 323/1000, LR 0.000225
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.28s
Val loss: 0.1452 score: 0.9302 time: 0.17s
Test loss: 0.2325 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 324/1000, LR 0.000224
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.27s
Val loss: 0.1299 score: 0.9380 time: 0.17s
Test loss: 0.2201 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 325/1000, LR 0.000224
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.27s
Val loss: 0.1219 score: 0.9767 time: 0.21s
Test loss: 0.2153 score: 0.9453 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 326/1000, LR 0.000223
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 0.41s
Val loss: 0.1229 score: 0.9690 time: 0.18s
Test loss: 0.2202 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 327/1000, LR 0.000223
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.28s
Val loss: 0.1290 score: 0.9690 time: 0.17s
Test loss: 0.2289 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 328/1000, LR 0.000223
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.28s
Val loss: 0.1340 score: 0.9690 time: 0.17s
Test loss: 0.2354 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 329/1000, LR 0.000222
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.28s
Val loss: 0.1372 score: 0.9612 time: 0.18s
Test loss: 0.2395 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 330/1000, LR 0.000222
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.48s
Val loss: 0.1366 score: 0.9612 time: 0.25s
Test loss: 0.2391 score: 0.9297 time: 0.22s
     INFO: Early stopping counter 10 of 20
Epoch 331/1000, LR 0.000221
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 0.28s
Val loss: 0.1335 score: 0.9690 time: 0.17s
Test loss: 0.2356 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 332/1000, LR 0.000221
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.29s
Val loss: 0.1294 score: 0.9690 time: 0.17s
Test loss: 0.2307 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 333/1000, LR 0.000221
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.27s
Val loss: 0.1257 score: 0.9690 time: 0.17s
Test loss: 0.2262 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 334/1000, LR 0.000220
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.28s
Val loss: 0.1234 score: 0.9690 time: 0.18s
Test loss: 0.2233 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 335/1000, LR 0.000220
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.27s
Val loss: 0.1216 score: 0.9612 time: 0.17s
Test loss: 0.2207 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 336/1000, LR 0.000219
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.29s
Val loss: 0.1207 score: 0.9690 time: 0.17s
Test loss: 0.2193 score: 0.9297 time: 0.16s
Epoch 337/1000, LR 0.000219
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.38s
Val loss: 0.1201 score: 0.9690 time: 0.19s
Test loss: 0.2183 score: 0.9297 time: 0.16s
Epoch 338/1000, LR 0.000219
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.26s
Val loss: 0.1199 score: 0.9690 time: 0.16s
Test loss: 0.2177 score: 0.9297 time: 0.15s
Epoch 339/1000, LR 0.000218
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.27s
Val loss: 0.1199 score: 0.9690 time: 0.17s
Test loss: 0.2174 score: 0.9297 time: 0.16s
Epoch 340/1000, LR 0.000218
Train loss: 0.5495;  Loss pred: 0.5495; Loss self: 0.0000; time: 0.27s
Val loss: 0.1200 score: 0.9690 time: 0.16s
Test loss: 0.2173 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 341/1000, LR 0.000218
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.27s
Val loss: 0.1200 score: 0.9690 time: 0.17s
Test loss: 0.2173 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 342/1000, LR 0.000217
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.32s
Val loss: 0.1199 score: 0.9690 time: 0.16s
Test loss: 0.2174 score: 0.9375 time: 0.17s
Epoch 343/1000, LR 0.000217
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.41s
Val loss: 0.1199 score: 0.9767 time: 0.16s
Test loss: 0.2174 score: 0.9375 time: 0.15s
Epoch 344/1000, LR 0.000216
Train loss: 0.5478;  Loss pred: 0.5478; Loss self: 0.0000; time: 0.26s
Val loss: 0.1197 score: 0.9767 time: 0.15s
Test loss: 0.2175 score: 0.9375 time: 0.15s
Epoch 345/1000, LR 0.000216
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.27s
Val loss: 0.1195 score: 0.9767 time: 0.16s
Test loss: 0.2176 score: 0.9375 time: 0.15s
Epoch 346/1000, LR 0.000215
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.26s
Val loss: 0.1193 score: 0.9767 time: 0.16s
Test loss: 0.2177 score: 0.9375 time: 0.18s
Epoch 347/1000, LR 0.000215
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.46s
Val loss: 0.1190 score: 0.9767 time: 0.16s
Test loss: 0.2179 score: 0.9375 time: 0.15s
Epoch 348/1000, LR 0.000215
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.26s
Val loss: 0.1187 score: 0.9690 time: 0.15s
Test loss: 0.2181 score: 0.9375 time: 0.15s
Epoch 349/1000, LR 0.000214
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.27s
Val loss: 0.1184 score: 0.9690 time: 0.15s
Test loss: 0.2183 score: 0.9375 time: 0.15s
Epoch 350/1000, LR 0.000214
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.27s
Val loss: 0.1182 score: 0.9690 time: 0.16s
Test loss: 0.2184 score: 0.9375 time: 0.15s
Epoch 351/1000, LR 0.000213
Train loss: 0.5455;  Loss pred: 0.5455; Loss self: 0.0000; time: 0.29s
Val loss: 0.1181 score: 0.9767 time: 0.16s
Test loss: 0.2185 score: 0.9375 time: 0.20s
Epoch 352/1000, LR 0.000213
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.28s
Val loss: 0.1180 score: 0.9767 time: 0.16s
Test loss: 0.2186 score: 0.9375 time: 0.15s
Epoch 353/1000, LR 0.000213
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.26s
Val loss: 0.1181 score: 0.9767 time: 0.16s
Test loss: 0.2186 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 354/1000, LR 0.000212
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 0.26s
Val loss: 0.1182 score: 0.9767 time: 0.15s
Test loss: 0.2186 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 355/1000, LR 0.000212
Train loss: 0.5474;  Loss pred: 0.5474; Loss self: 0.0000; time: 0.26s
Val loss: 0.1176 score: 0.9767 time: 0.15s
Test loss: 0.2188 score: 0.9453 time: 0.15s
Epoch 356/1000, LR 0.000211
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.26s
Val loss: 0.1168 score: 0.9690 time: 0.19s
Test loss: 0.2197 score: 0.9297 time: 0.15s
Epoch 357/1000, LR 0.000211
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.43s
Val loss: 0.1168 score: 0.9690 time: 0.23s
Test loss: 0.2208 score: 0.9297 time: 0.21s
Epoch 358/1000, LR 0.000211
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.27s
Val loss: 0.1172 score: 0.9690 time: 0.16s
Test loss: 0.2199 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 359/1000, LR 0.000210
Train loss: 0.5460;  Loss pred: 0.5460; Loss self: 0.0000; time: 0.27s
Val loss: 0.1225 score: 0.9380 time: 0.16s
Test loss: 0.2224 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 360/1000, LR 0.000210
Train loss: 0.5493;  Loss pred: 0.5493; Loss self: 0.0000; time: 0.40s
Val loss: 0.1265 score: 0.9380 time: 0.24s
Test loss: 0.2258 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 361/1000, LR 0.000209
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 0.47s
Val loss: 0.1254 score: 0.9380 time: 0.24s
Test loss: 0.2255 score: 0.9219 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 362/1000, LR 0.000209
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.36s
Val loss: 0.1234 score: 0.9380 time: 0.16s
Test loss: 0.2245 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 363/1000, LR 0.000209
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.26s
Val loss: 0.1213 score: 0.9380 time: 0.16s
Test loss: 0.2235 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 364/1000, LR 0.000208
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.26s
Val loss: 0.1196 score: 0.9457 time: 0.16s
Test loss: 0.2229 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 365/1000, LR 0.000208
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.26s
Val loss: 0.1171 score: 0.9690 time: 0.15s
Test loss: 0.2227 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 366/1000, LR 0.000207
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.27s
Val loss: 0.1181 score: 0.9612 time: 0.15s
Test loss: 0.2283 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 367/1000, LR 0.000207
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.37s
Val loss: 0.1226 score: 0.9690 time: 0.22s
Test loss: 0.2361 score: 0.9375 time: 0.26s
     INFO: Early stopping counter 10 of 20
Epoch 368/1000, LR 0.000206
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.27s
Val loss: 0.1269 score: 0.9690 time: 0.16s
Test loss: 0.2426 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 369/1000, LR 0.000206
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.27s
Val loss: 0.1288 score: 0.9690 time: 0.15s
Test loss: 0.2454 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 370/1000, LR 0.000206
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.31s
Val loss: 0.1257 score: 0.9690 time: 0.16s
Test loss: 0.2415 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 371/1000, LR 0.000205
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.46s
Val loss: 0.1193 score: 0.9690 time: 0.24s
Test loss: 0.2319 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 372/1000, LR 0.000205
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.36s
Val loss: 0.1163 score: 0.9690 time: 0.16s
Test loss: 0.2261 score: 0.9297 time: 0.15s
Epoch 373/1000, LR 0.000204
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.27s
Val loss: 0.1158 score: 0.9690 time: 0.16s
Test loss: 0.2236 score: 0.9297 time: 0.15s
Epoch 374/1000, LR 0.000204
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.26s
Val loss: 0.1164 score: 0.9767 time: 0.16s
Test loss: 0.2228 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 375/1000, LR 0.000204
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.33s
Val loss: 0.1171 score: 0.9690 time: 0.17s
Test loss: 0.2227 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 376/1000, LR 0.000203
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.28s
Val loss: 0.1177 score: 0.9612 time: 0.17s
Test loss: 0.2228 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 377/1000, LR 0.000203
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.28s
Val loss: 0.1179 score: 0.9535 time: 0.17s
Test loss: 0.2229 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 378/1000, LR 0.000202
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.28s
Val loss: 0.1167 score: 0.9767 time: 0.17s
Test loss: 0.2227 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 379/1000, LR 0.000202
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 0.27s
Val loss: 0.1152 score: 0.9690 time: 0.16s
Test loss: 0.2244 score: 0.9297 time: 0.15s
Epoch 380/1000, LR 0.000201
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.29s
Val loss: 0.1159 score: 0.9690 time: 0.17s
Test loss: 0.2275 score: 0.9297 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 381/1000, LR 0.000201
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.27s
Val loss: 0.1173 score: 0.9690 time: 0.15s
Test loss: 0.2307 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 382/1000, LR 0.000201
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.27s
Val loss: 0.1179 score: 0.9690 time: 0.15s
Test loss: 0.2320 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 383/1000, LR 0.000200
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.26s
Val loss: 0.1179 score: 0.9690 time: 0.15s
Test loss: 0.2323 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 384/1000, LR 0.000200
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.27s
Val loss: 0.1173 score: 0.9690 time: 0.16s
Test loss: 0.2315 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 385/1000, LR 0.000199
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.28s
Val loss: 0.1162 score: 0.9612 time: 0.17s
Test loss: 0.2296 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 386/1000, LR 0.000199
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.28s
Val loss: 0.1152 score: 0.9690 time: 0.23s
Test loss: 0.2275 score: 0.9297 time: 0.16s
Epoch 387/1000, LR 0.000198
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.28s
Val loss: 0.1145 score: 0.9690 time: 0.17s
Test loss: 0.2256 score: 0.9297 time: 0.16s
Epoch 388/1000, LR 0.000198
Train loss: 0.5399;  Loss pred: 0.5399; Loss self: 0.0000; time: 0.28s
Val loss: 0.1143 score: 0.9690 time: 0.17s
Test loss: 0.2243 score: 0.9297 time: 0.16s
Epoch 389/1000, LR 0.000198
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.28s
Val loss: 0.1145 score: 0.9690 time: 0.17s
Test loss: 0.2235 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 390/1000, LR 0.000197
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.27s
Val loss: 0.1148 score: 0.9767 time: 0.17s
Test loss: 0.2231 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 391/1000, LR 0.000197
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.28s
Val loss: 0.1153 score: 0.9767 time: 0.17s
Test loss: 0.2230 score: 0.9453 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 392/1000, LR 0.000196
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.27s
Val loss: 0.1155 score: 0.9767 time: 0.22s
Test loss: 0.2231 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 393/1000, LR 0.000196
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.26s
Val loss: 0.1154 score: 0.9767 time: 0.16s
Test loss: 0.2232 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 394/1000, LR 0.000195
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.27s
Val loss: 0.1148 score: 0.9767 time: 0.16s
Test loss: 0.2234 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 395/1000, LR 0.000195
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.27s
Val loss: 0.1140 score: 0.9767 time: 0.16s
Test loss: 0.2239 score: 0.9375 time: 0.15s
Epoch 396/1000, LR 0.000195
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.27s
Val loss: 0.1135 score: 0.9690 time: 0.16s
Test loss: 0.2245 score: 0.9375 time: 0.15s
Epoch 397/1000, LR 0.000194
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.28s
Val loss: 0.1133 score: 0.9690 time: 0.25s
Test loss: 0.2251 score: 0.9297 time: 0.16s
Epoch 398/1000, LR 0.000194
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.28s
Val loss: 0.1141 score: 0.9767 time: 0.17s
Test loss: 0.2243 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 399/1000, LR 0.000193
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.27s
Val loss: 0.1220 score: 0.9380 time: 0.17s
Test loss: 0.2285 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 400/1000, LR 0.000193
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.28s
Val loss: 0.1309 score: 0.9302 time: 0.24s
Test loss: 0.2360 score: 0.9062 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 401/1000, LR 0.000192
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.47s
Val loss: 0.1323 score: 0.9302 time: 0.23s
Test loss: 0.2377 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 402/1000, LR 0.000192
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.27s
Val loss: 0.1273 score: 0.9302 time: 0.17s
Test loss: 0.2339 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 403/1000, LR 0.000192
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.28s
Val loss: 0.1228 score: 0.9380 time: 0.17s
Test loss: 0.2307 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 404/1000, LR 0.000191
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.27s
Val loss: 0.1191 score: 0.9380 time: 0.17s
Test loss: 0.2285 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 405/1000, LR 0.000191
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.28s
Val loss: 0.1164 score: 0.9535 time: 0.24s
Test loss: 0.2274 score: 0.9219 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 406/1000, LR 0.000190
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.37s
Val loss: 0.1147 score: 0.9612 time: 0.16s
Test loss: 0.2273 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 407/1000, LR 0.000190
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.26s
Val loss: 0.1138 score: 0.9767 time: 0.15s
Test loss: 0.2280 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 408/1000, LR 0.000189
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.27s
Val loss: 0.1136 score: 0.9767 time: 0.16s
Test loss: 0.2289 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 409/1000, LR 0.000189
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.32s
Val loss: 0.1135 score: 0.9767 time: 0.17s
Test loss: 0.2293 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 12 of 20
Epoch 410/1000, LR 0.000188
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.47s
Val loss: 0.1135 score: 0.9767 time: 0.24s
Test loss: 0.2295 score: 0.9297 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 411/1000, LR 0.000188
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 0.47s
Val loss: 0.1134 score: 0.9767 time: 0.25s
Test loss: 0.2296 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 412/1000, LR 0.000188
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.27s
Val loss: 0.1134 score: 0.9767 time: 0.16s
Test loss: 0.2295 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 413/1000, LR 0.000187
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.26s
Val loss: 0.1133 score: 0.9767 time: 0.26s
Test loss: 0.2298 score: 0.9297 time: 0.15s
Epoch 414/1000, LR 0.000187
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.26s
Val loss: 0.1137 score: 0.9690 time: 0.15s
Test loss: 0.2332 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 415/1000, LR 0.000186
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.26s
Val loss: 0.1199 score: 0.9690 time: 0.15s
Test loss: 0.2457 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 416/1000, LR 0.000186
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.31s
Val loss: 0.1285 score: 0.9690 time: 0.16s
Test loss: 0.2587 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 417/1000, LR 0.000185
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.33s
Val loss: 0.1386 score: 0.9612 time: 0.16s
Test loss: 0.2719 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 418/1000, LR 0.000185
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.26s
Val loss: 0.1436 score: 0.9535 time: 0.15s
Test loss: 0.2780 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 419/1000, LR 0.000185
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.27s
Val loss: 0.1416 score: 0.9612 time: 0.16s
Test loss: 0.2760 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 420/1000, LR 0.000184
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.26s
Val loss: 0.1356 score: 0.9690 time: 0.15s
Test loss: 0.2691 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 421/1000, LR 0.000184
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.40s
Val loss: 0.1286 score: 0.9690 time: 0.24s
Test loss: 0.2603 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 422/1000, LR 0.000183
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.34s
Val loss: 0.1225 score: 0.9690 time: 0.16s
Test loss: 0.2520 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 423/1000, LR 0.000183
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.27s
Val loss: 0.1188 score: 0.9690 time: 0.16s
Test loss: 0.2465 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 424/1000, LR 0.000182
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.26s
Val loss: 0.1156 score: 0.9690 time: 0.16s
Test loss: 0.2411 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 425/1000, LR 0.000182
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.26s
Val loss: 0.1117 score: 0.9690 time: 0.15s
Test loss: 0.2328 score: 0.9297 time: 0.15s
Epoch 426/1000, LR 0.000181
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.26s
Val loss: 0.1131 score: 0.9612 time: 0.15s
Test loss: 0.2278 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 427/1000, LR 0.000181
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.26s
Val loss: 0.1208 score: 0.9380 time: 0.15s
Test loss: 0.2320 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 428/1000, LR 0.000181
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 0.29s
Val loss: 0.1278 score: 0.9302 time: 0.16s
Test loss: 0.2377 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 429/1000, LR 0.000180
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.27s
Val loss: 0.1301 score: 0.9302 time: 0.16s
Test loss: 0.2398 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 430/1000, LR 0.000180
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.26s
Val loss: 0.1283 score: 0.9302 time: 0.16s
Test loss: 0.2386 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 431/1000, LR 0.000179
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.26s
Val loss: 0.1305 score: 0.9302 time: 0.16s
Test loss: 0.2406 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 432/1000, LR 0.000179
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.26s
Val loss: 0.1436 score: 0.9225 time: 0.16s
Test loss: 0.2525 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 433/1000, LR 0.000178
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.27s
Val loss: 0.1503 score: 0.9225 time: 0.16s
Test loss: 0.2590 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 434/1000, LR 0.000178
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.26s
Val loss: 0.1491 score: 0.9225 time: 0.15s
Test loss: 0.2581 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 435/1000, LR 0.000177
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.30s
Val loss: 0.1488 score: 0.9225 time: 0.16s
Test loss: 0.2578 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 436/1000, LR 0.000177
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.33s
Val loss: 0.1547 score: 0.9225 time: 0.16s
Test loss: 0.2635 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 437/1000, LR 0.000176
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.26s
Val loss: 0.1524 score: 0.9225 time: 0.16s
Test loss: 0.2613 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 438/1000, LR 0.000176
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.26s
Val loss: 0.1447 score: 0.9225 time: 0.15s
Test loss: 0.2541 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 439/1000, LR 0.000176
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.26s
Val loss: 0.1360 score: 0.9302 time: 0.16s
Test loss: 0.2463 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 440/1000, LR 0.000175
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 0.29s
Val loss: 0.1275 score: 0.9302 time: 0.23s
Test loss: 0.2392 score: 0.9141 time: 0.20s
     INFO: Early stopping counter 15 of 20
Epoch 441/1000, LR 0.000175
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.43s
Val loss: 0.1208 score: 0.9380 time: 0.18s
Test loss: 0.2342 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 442/1000, LR 0.000174
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.29s
Val loss: 0.1163 score: 0.9380 time: 0.17s
Test loss: 0.2317 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 443/1000, LR 0.000174
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.27s
Val loss: 0.1134 score: 0.9612 time: 0.17s
Test loss: 0.2310 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 444/1000, LR 0.000173
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.28s
Val loss: 0.1119 score: 0.9767 time: 0.18s
Test loss: 0.2315 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 445/1000, LR 0.000173
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.27s
Val loss: 0.1115 score: 0.9767 time: 0.17s
Test loss: 0.2323 score: 0.9375 time: 0.16s
Epoch 446/1000, LR 0.000172
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.27s
Val loss: 0.1113 score: 0.9767 time: 0.17s
Test loss: 0.2327 score: 0.9375 time: 0.19s
Epoch 447/1000, LR 0.000172
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.27s
Val loss: 0.1113 score: 0.9767 time: 0.21s
Test loss: 0.2329 score: 0.9375 time: 0.17s
Epoch 448/1000, LR 0.000172
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.26s
Val loss: 0.1112 score: 0.9767 time: 0.16s
Test loss: 0.2331 score: 0.9375 time: 0.15s
Epoch 449/1000, LR 0.000171
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.26s
Val loss: 0.1113 score: 0.9767 time: 0.16s
Test loss: 0.2329 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 450/1000, LR 0.000171
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.30s
Val loss: 0.1116 score: 0.9767 time: 0.16s
Test loss: 0.2327 score: 0.9375 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 451/1000, LR 0.000170
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.26s
Val loss: 0.1136 score: 0.9535 time: 0.16s
Test loss: 0.2327 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 452/1000, LR 0.000170
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.27s
Val loss: 0.1231 score: 0.9380 time: 0.16s
Test loss: 0.2387 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 453/1000, LR 0.000169
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.26s
Val loss: 0.1307 score: 0.9302 time: 0.15s
Test loss: 0.2454 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 454/1000, LR 0.000169
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 0.26s
Val loss: 0.1297 score: 0.9302 time: 0.16s
Test loss: 0.2449 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 455/1000, LR 0.000168
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.27s
Val loss: 0.1268 score: 0.9380 time: 0.16s
Test loss: 0.2430 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 456/1000, LR 0.000168
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.30s
Val loss: 0.1223 score: 0.9380 time: 0.16s
Test loss: 0.2398 score: 0.9219 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 457/1000, LR 0.000167
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.33s
Val loss: 0.1174 score: 0.9380 time: 0.16s
Test loss: 0.2369 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 458/1000, LR 0.000167
Train loss: 0.5303;  Loss pred: 0.5303; Loss self: 0.0000; time: 0.27s
Val loss: 0.1145 score: 0.9535 time: 0.16s
Test loss: 0.2361 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 459/1000, LR 0.000167
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.27s
Val loss: 0.1131 score: 0.9612 time: 0.16s
Test loss: 0.2363 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 460/1000, LR 0.000166
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.27s
Val loss: 0.1124 score: 0.9690 time: 0.16s
Test loss: 0.2369 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 461/1000, LR 0.000166
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.26s
Val loss: 0.1121 score: 0.9767 time: 0.16s
Test loss: 0.2375 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 462/1000, LR 0.000165
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.27s
Val loss: 0.1120 score: 0.9767 time: 0.16s
Test loss: 0.2379 score: 0.9297 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 463/1000, LR 0.000165
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.31s
Val loss: 0.1118 score: 0.9767 time: 0.16s
Test loss: 0.2404 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 464/1000, LR 0.000164
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.26s
Val loss: 0.1160 score: 0.9690 time: 0.16s
Test loss: 0.2510 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 465/1000, LR 0.000164
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.27s
Val loss: 0.1218 score: 0.9690 time: 0.16s
Test loss: 0.2610 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 466/1000, LR 0.000163
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.26s
Val loss: 0.1266 score: 0.9690 time: 0.16s
Test loss: 0.2684 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 467/1000, LR 0.000163
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.27s
Val loss: 0.1308 score: 0.9690 time: 0.16s
Test loss: 0.2745 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 468/1000, LR 0.000162
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.26s
Val loss: 0.1318 score: 0.9690 time: 0.17s
Test loss: 0.2760 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 447,   Train_Loss: 0.5308,   Val_Loss: 0.1112,   Val_Precision: 0.9844,   Val_Recall: 0.9692,   Val_accuracy: 0.9767,   Val_Score: 0.9767,   Val_Loss: 0.1112,   Test_Precision: 0.9828,   Test_Recall: 0.8906,   Test_accuracy: 0.9344,   Test_Score: 0.9375,   Test_loss: 0.2331


[0.16673445515334606, 0.16456928104162216, 0.16310806293040514, 0.16728525212965906, 0.1854744099546224, 0.16258475300855935, 0.16449093399569392, 0.1610368660185486, 0.16231747902929783, 0.1652295389212668, 0.16330186813138425, 0.16176578402519226, 0.16232501482591033, 0.16171461204066873, 0.16289166919887066, 0.16123372013680637, 0.22038643597625196, 0.16740042599849403, 0.16435677302069962, 0.16419110400602221, 0.16313734301365912, 0.2006287630647421, 0.16585547616705298, 0.16315406118519604, 0.16169284307397902, 0.16263928497210145, 0.1612458899617195, 0.25024853996001184, 0.16360940993763506, 0.1635465060826391, 0.17455954803153872, 0.17379526305012405, 0.1774173858575523, 0.1780657118652016, 0.17183967703022063, 0.1667702291160822, 0.17278969404287636, 0.24313197215087712, 0.2431589609477669, 0.16836075694300234, 0.16478207614272833, 0.16209386009722948, 0.16404796205461025, 0.16202021902427077, 0.16177891404367983, 0.16372482618317008, 0.24734427803196013, 0.17053176905028522, 0.17116316105239093, 0.17020678101107478, 0.17008514003828168, 0.18675795919261873, 0.16963822301477194, 0.17098320205695927, 0.16924341884441674, 0.24730027792975307, 0.24726353911682963, 0.24728002795018256, 0.17665820382535458, 0.16417790786363184, 0.15995282703079283, 0.16107823699712753, 0.16283482708968222, 0.16100223013199866, 0.24648205284029245, 0.16518020280636847, 0.16574255004525185, 0.16364169493317604, 0.1634306670166552, 0.18196568288840353, 0.25050408602692187, 0.16633846308104694, 0.16594198206439614, 0.2410559670533985, 0.238903874065727, 0.23480591690167785, 0.23885572399012744, 0.1622869479469955, 0.16099695907905698, 0.15951298992149532, 0.1608885859604925, 0.23039979697205126, 0.17414131900295615, 0.17543383804149926, 0.1734831149224192, 0.1755000320263207, 0.1769772320985794, 0.19987277686595917, 0.17824881384149194, 0.2537408887874335, 0.1587942228652537, 0.17284859088249505, 0.170234713004902, 0.20563827897422016, 0.17523267911747098, 0.1744306841865182, 0.17912659794092178, 0.1765528260730207, 0.1783474951516837, 0.17473685811273754, 0.2670305559877306, 0.1796403299085796, 0.17467937106266618, 0.17199443420395255, 0.20603811112232506, 0.17141464701853693, 0.17427313211373985, 0.17369821411557496, 0.17368879891000688, 0.1708255368284881, 0.17318927706219256, 0.16062364005483687, 0.16172748617827892, 0.16984039498493075, 0.163166674785316, 0.16084606805816293, 0.16557894297875464, 0.24426717194728553, 0.16072220914065838, 0.1615866729989648, 0.16133025102317333, 0.16248368099331856, 0.1627005022019148, 0.24945938400924206, 0.16117874020710588, 0.16261339001357555, 0.23891933006234467, 0.24097934109158814, 0.24107169592753053, 0.2410680609755218, 0.16411066986620426, 0.16325514111667871, 0.16464215703308582, 0.16869658906944096, 0.16394772892817855, 0.16140951309353113, 0.16321560298092663, 0.17311541503295302, 0.24683682108297944, 0.16691843792796135, 0.16391977085731924, 0.166302930098027, 0.16155808698385954, 0.19306765589863062, 0.2210617740638554, 0.16319593507796526, 0.16402410087175667, 0.16452863113954663, 0.16444694600068033, 0.16804808005690575, 0.16577738290652633, 0.16188615886494517, 0.1622601628769189, 0.16385134286247194, 0.16344994003884494, 0.16524494998157024, 0.1916167119052261, 0.19729590509086847, 0.163780887844041, 0.1637106619309634, 0.15956483595073223, 0.15940011502243578, 0.15896479110233486, 0.19179784203879535, 0.17009773710742593, 0.1625336620490998, 0.17425616993568838, 0.17220639204606414, 0.1706231569405645, 0.28065608092583716, 0.2630470939911902, 0.17132195108570158, 0.17073519085533917, 0.1725540030747652, 0.17394615686498582, 0.1739957311656326, 0.17670407705008984, 0.1814715799409896, 0.1613593699876219, 0.16336485999636352, 0.2060081190429628, 0.16762209497392178, 0.17296730307862163, 0.24097197782248259, 0.15683247707784176, 0.16881524794735014, 0.1695977149065584, 0.1692938890773803, 0.17357272910885513, 0.1701673110947013, 0.17574759596027434, 0.24892814899794757, 0.2368547220248729, 0.2509191329590976, 0.15953431907109916, 0.17361875507049263, 0.17254383605904877, 0.1742519869003445, 0.17292288807220757, 0.2602621139958501, 0.17514411895535886, 0.17415130510926247, 0.17506522499024868, 0.1746231650467962, 0.248841772088781, 0.25150099699385464, 0.17336963396519423, 0.17390950908884406, 0.1745272008702159, 0.1754064450506121, 0.23269196599721909, 0.17278197989799082, 0.17422183020971715, 0.17253445112146437, 0.23262266186065972, 0.1604927929583937, 0.1731957730371505, 0.17134184786118567, 0.17323128902353346, 0.17126029008068144, 0.17884608497843146, 0.17051923903636634, 0.16904838802292943, 0.17149172793142498, 0.17118925508111715, 0.17215807596221566, 0.18798734992742538, 0.23187255300581455, 0.1581243530381471, 0.16271990607492626, 0.1616752331610769, 0.1589968439657241, 0.15896394406445324, 0.16094659105874598, 0.16214475990273058, 0.15829796390607953, 0.1598516891244799, 0.16099763801321387, 0.161884669912979, 0.16178100206889212, 0.16185220913030207, 0.16887550405226648, 0.16910325293429196, 0.16943069407716393, 0.16744819493032992, 0.169359324965626, 0.1697199430782348, 0.16995688807219267, 0.1671690831426531, 0.1681958541739732, 0.16799198207445443, 0.1701674209907651, 0.16828978690318763, 0.17307977005839348, 0.17272892384789884, 0.16932283085770905, 0.24311852105893195, 0.24518223712220788, 0.2457512009423226, 0.16916287899948657, 0.16700354497879744, 0.18392588989809155, 0.17081045894883573, 0.15663270303048193, 0.16073436895385385, 0.1584344170987606, 0.15969394287094474, 0.15999513887800276, 0.23707106802612543, 0.15953163197264075, 0.16383772296831012, 0.1586746519897133, 0.1619646770413965, 0.15816990099847317, 0.19805173994973302, 0.16098128212615848, 0.20877147209830582, 0.15866114292293787, 0.16143442783504725, 0.2575265569612384, 0.16189939505420625, 0.16271735378541052, 0.16220943885855377, 0.1684341449290514, 0.15835127118043602, 0.23612410598434508, 0.16059105680324137, 0.15998438000679016, 0.160298191010952, 0.16003166395239532, 0.16142815491184592, 0.1600618369411677, 0.1641201509628445, 0.16021832101978362, 0.1623217931482941, 0.2347525330260396, 0.23891248414292932, 0.23895561299286783, 0.16086901794187725, 0.17375208181329072, 0.17311525694094598, 0.17208570684306324, 0.17242141184397042, 0.230367325944826, 0.17220130492933095, 0.17579775606282055, 0.16982773807831109, 0.17308843811042607, 0.17223581206053495, 0.17443761392496526, 0.17192080500535667, 0.17597970296628773, 0.17497563315555453, 0.17042878991924226, 0.17136186105199158, 0.17018930078484118, 0.17208680184558034, 0.19066068599931896, 0.2576312171295285, 0.17113644001074135, 0.1758872801437974, 0.24945866386406124, 0.243104028981179, 0.1631897590123117, 0.1616576388478279, 0.16096236393786967, 0.15849255095236003, 0.16133472113870084, 0.1804954840335995, 0.24108296702615917, 0.1612322621513158, 0.16472438303753734, 0.16256156493909657, 0.1717379419133067, 0.15852960897609591, 0.1831769437994808, 0.22162948013283312, 0.15927423513494432, 0.1558688881341368, 0.15978833101689816, 0.15823188005015254, 0.1579428520053625, 0.19008617103099823, 0.1665216840337962, 0.169184128055349, 0.1677500088699162, 0.16859405091963708, 0.17007544194348156, 0.17211860581301153, 0.1743805839214474, 0.16388750518672168, 0.16252768598496914, 0.1723189519252628, 0.2389598300214857, 0.23892405210062861, 0.25364511902444065, 0.17177730496041477, 0.17345076194033027, 0.17354851216077805, 0.1717924859840423, 0.1751652779057622, 0.2507676729001105, 0.17339131888002157, 0.17571793403476477, 0.1716889429371804, 0.17099507385864854, 0.1774403378367424, 0.1647659360896796, 0.1593880879227072, 0.15944395097903907, 0.15966833592392504, 0.16064104321412742, 0.1612316770479083, 0.19079579995013773, 0.17607789696194232, 0.16891566989943385, 0.1750107710249722, 0.16356977191753685, 0.16740774596109986, 0.18981378502212465, 0.16107580298557878, 0.1601741500198841, 0.17065494507551193, 0.16975813498720527, 0.17093553021550179, 0.1761766979470849, 0.16950825601816177, 0.1713663088157773, 0.169889273121953, 0.24730916414409876, 0.256103539140895, 0.17464125994592905, 0.1708532760385424, 0.16210092208348215, 0.16917612100951374, 0.17337071802467108, 0.1767740671057254, 0.24940966512076557, 0.23081090115010738, 0.17405863315798342, 0.16649482608772814, 0.1688618310727179, 0.164355002110824, 0.16415038495324552, 0.16428400808945298, 0.16512308199889958, 0.16476266994141042, 0.17135234689339995, 0.16790883196517825, 0.18116962118074298, 0.17883325391449034, 0.1810827141162008, 0.1901224220637232, 0.18171323789283633, 0.1802282240241766, 0.17559347511269152, 0.17839069198817015, 0.17829548986628652, 0.1750321858562529, 0.2468140630517155, 0.16364127304404974, 0.16343534598127007, 0.16307496395893395, 0.1647448791190982, 0.16525375307537615, 0.24752583098597825, 0.16705505596473813, 0.16473806812427938, 0.2410778529010713, 0.18862594314850867, 0.16661052918061614, 0.16375418310053647, 0.16752792103216052, 0.16692447289824486, 0.16535185393877327, 0.16664017387665808, 0.2155302760656923, 0.16221859701909125, 0.16443065297789872, 0.1641415529884398, 0.16370138805359602, 0.1830015000887215, 0.1762602769304067, 0.17266172403469682, 0.16418603179045022, 0.17660178104415536, 0.24102311115711927, 0.24108548206277192, 0.16248807404190302, 0.16407914995215833, 0.16183790611103177, 0.16256956686265767, 0.17205820814706385, 0.16284006112255156, 0.16346428613178432, 0.16467402898706496, 0.16198178101330996, 0.1605974230915308, 0.21741261892020702, 0.2617416582070291, 0.1776915800292045, 0.17208029609173536, 0.17481552390381694, 0.16275116009637713, 0.16287175798788667, 0.16098585398867726, 0.23127771192230284, 0.18096888600848615, 0.17368410900235176, 0.17536227498203516, 0.17370149493217468, 0.23720170906744897, 0.17381148412823677, 0.17280369601212442, 0.16141074593178928, 0.16546714096330106, 0.16297501185908914, 0.16375897591933608, 0.1621641900856048, 0.19778229715302587, 0.21885895705781877, 0.16847763094119728, 0.16282908688299358, 0.16837679990567267, 0.166052880929783, 0.2387350860517472, 0.25732376403175294, 0.16509714303538203, 0.17749909195117652, 0.17862635990604758, 0.1736182440072298, 0.15897565498016775, 0.18826975696720183, 0.17487751808948815, 0.1712487330660224, 0.16279774601571262, 0.16994708799757063, 0.17049203114584088, 0.16979282395914197, 0.1736646539065987, 0.17157040210440755, 0.17254859814420342, 0.17497989302501082, 0.16135429311543703, 0.1617063540033996, 0.16267890809103847, 0.15888622100465, 0.15807448886334896, 0.1598067400045693, 0.1604998221155256, 0.16149828187189996, 0.21723609301261604, 0.2254325549583882, 0.16265815403312445, 0.16318596107885242, 0.1718410470057279, 0.1743388210888952, 0.17692090710625052, 0.1743997090961784, 0.16565414192155004, 0.24121814197860658, 0.24310696590691805, 0.24104341282509267, 0.16223270003683865, 0.15936532290652394, 0.16481897118501365, 0.16063545388169587, 0.15736756590195, 0.16233529103919864, 0.24675887892954051, 0.15728044090792537, 0.16079750307835639, 0.15780002204701304, 0.16606757906265557, 0.24943309812806547, 0.17221565614454448, 0.17537936894223094, 0.17829825612716377, 0.17790100304409862, 0.1783367448952049, 0.18004451296292245, 0.17627694085240364, 0.1765715009532869, 0.16466286219656467, 0.17785028787329793, 0.2514760799240321, 0.17421870585530996, 0.16244247183203697, 0.15878531197085977, 0.16133533790707588, 0.16131596895866096, 0.16570395999588072, 0.16626632399857044, 0.1631852958817035, 0.16170855308882892, 0.17173197399824858, 0.16999052790924907, 0.1728052489925176, 0.17598024220205843, 0.29934531915932894, 0.16295237699523568, 0.17316752998158336, 0.17295694816857576, 0.1716442711185664, 0.17833743407391012, 0.17030881205573678, 0.16349403886124492, 0.16174765187315643, 0.16205291007645428, 0.23893697396852076, 0.24313733400776982, 0.1670566878747195, 0.1668616971001029, 0.16194932512007654, 0.24106903793290257, 0.24315106496214867, 0.1866615079343319, 0.16470642108470201, 0.16159570892341435, 0.16192066087387502, 0.16217397898435593, 0.18667100020684302, 0.20992818707600236, 0.1719612991437316, 0.17256721202284098, 0.16307432879693806, 0.1621054532006383, 0.1641257160808891, 0.2503877968993038, 0.16504646511748433, 0.16995246685110033, 0.1632173580583185, 0.18864622386172414, 0.24009561305865645, 0.16459321207366884, 0.16520797717384994, 0.2867928429041058, 0.16461677802726626, 0.16673330683261156, 0.16530599701218307, 0.16213607299141586, 0.1648485001642257, 0.16362702008336782, 0.22274516592733562, 0.21268493309617043, 0.1634331988170743, 0.16266111796721816, 0.164705729810521, 0.1691006028559059, 0.24107574485242367, 0.1684564189054072, 0.16360006295144558, 0.1643856498412788, 0.1625266200862825, 0.16257420601323247, 0.16898503713309765, 0.20165096316486597, 0.16238805605098605, 0.1711464950349182, 0.17152575915679336, 0.1628425382077694, 0.240286833839491, 0.24096106202341616, 0.1629386639688164, 0.16256785090081394, 0.16398216295056045, 0.16300541698001325, 0.16379624791443348, 0.1681920390110463, 0.18941429909318686, 0.16135260998271406, 0.20327952387742698, 0.24102614703588188, 0.24314937484450638, 0.16887061297893524, 0.16532342205755413, 0.16175013897009194, 0.1608109879307449, 0.23871600511483848, 0.2406422810163349, 0.17303253896534443, 0.17204950214363635, 0.1837823479436338, 0.24937217496335506, 0.25149041693657637, 0.1720238020643592, 0.16527765709906816, 0.1640111079905182, 0.17719968501478434, 0.17875670501962304, 0.1840198328718543, 0.1647827229462564, 0.16082960204221308, 0.17042830912396312, 0.17309604189358652, 0.2676805721130222, 0.1852235421538353, 0.15961045399308205, 0.15867438889108598, 0.1606872829142958, 0.17381764901801944, 0.16778392414562404, 0.15818286407738924, 0.1591396329458803, 0.17828887118957937, 0.17365563288331032, 0.1717115119099617, 0.16922263707965612, 0.16971219098195434, 0.22702453681267798, 0.16417380003258586, 0.168931056978181, 0.1661837708670646, 0.24940092000178993, 0.17225202801637352, 0.17163917003199458, 0.17075011297129095, 0.167231862898916, 0.2390116211026907, 0.16484631202183664, 0.17242020508274436, 0.16966090979985893, 0.16363283386453986, 0.16660419409163296, 0.17248506797477603, 0.18009490007534623, 0.17913755285553634, 0.16950641991570592, 0.16305862902663648, 0.16401974391192198, 0.16306634014472365, 0.2397251680959016, 0.18958914908580482, 0.16457030503079295, 0.1722361040301621, 0.17342716199345887, 0.1735893569421023, 0.17439891514368355, 0.17948106909170747, 0.1564392219297588, 0.2217088679317385, 0.21963142114691436, 0.21967803919687867, 0.17155790608376265, 0.17157976794987917, 0.17175074107944965, 0.1532528952229768, 0.1619431539438665, 0.1565064280293882, 0.15415532304905355, 0.2092051759827882, 0.20711557194590569, 0.15891229594126344, 0.15753391291946173, 0.15621641092002392, 0.15359328989870846, 0.15241571492515504, 0.2101947779301554, 0.1536569690797478, 0.15077187400311232, 0.15619108197279274, 0.149971310980618, 0.14944882993586361, 0.14941105688922107, 0.15279091405682266, 0.15209253295324743, 0.1513867280445993, 0.15020183799788356, 0.16122661693952978, 0.16323280497454107, 0.16438298812136054, 0.16332888090983033, 0.16411427897401154, 0.18843815801665187, 0.1498603061772883, 0.15062879701144993, 0.2274928828701377, 0.15012956713326275, 0.14930173195898533, 0.20827095420099795, 0.1567461199592799, 0.17083229101262987, 0.2014807090163231, 0.21339783305302262, 0.1600217989180237, 0.15240972605533898, 0.15573239093646407, 0.15280937403440475, 0.16287282505072653, 0.16533762007020414, 0.16344306198880076, 0.15668023703619838, 0.16580419102683663, 0.16660685907118022, 0.17387232184410095, 0.22299269307404757, 0.16985028493218124, 0.21980609511956573, 0.22179129300639033, 0.16587301320396364, 0.16128182201646268, 0.16143359802663326, 0.1604943829588592, 0.1628641930874437, 0.19451354700140655, 0.16721554100513458, 0.15505207795649767, 0.15366388205438852, 0.16666284995153546, 0.1967886129859835, 0.21342782583087683, 0.16668460494838655, 0.15564024890772998, 0.20211521605961025, 0.20712120714597404, 0.20498938602395356, 0.16839558188803494, 0.15494845109060407, 0.16491370205767453, 0.15159748611040413, 0.16157243913039565, 0.1627327841706574, 0.16140012210235, 0.15244148997589946, 0.15430862200446427, 0.15240978193469346, 0.1570066548883915, 0.20510582788847387, 0.20733812008984387, 0.15601151203736663, 0.15388840110972524, 0.15431674616411328, 0.1540325980167836, 0.15663157287053764, 0.15426491317339242, 0.15209288382902741, 0.15064926305785775, 0.16144614899531007, 0.1620600090827793, 0.16331050801090896, 0.16069210995920002, 0.1644260729663074, 0.1609878959134221, 0.16023274813778698, 0.1593710919842124, 0.16339560598134995, 0.20504049002192914, 0.1500466358847916, 0.15374159417115152, 0.1529582601506263, 0.157777211163193, 0.15758641087450087, 0.15422004694119096, 0.16227707895450294, 0.1925264629535377, 0.15305164409801364, 0.1522733720485121, 0.1553941429592669, 0.17523874389007688, 0.168184416834265, 0.16921028005890548, 0.1561869529541582, 0.1564060419332236, 0.15570704685524106, 0.16326246410608292, 0.15538288583047688, 0.15991726890206337, 0.24977271095849574, 0.16054166201502085, 0.16280039306730032, 0.1616589450277388, 0.18931441497989, 0.24726936896331608, 0.15315614198334515, 0.15430923900566995, 0.15167332696728408, 0.15305865020491183, 0.15429503377526999, 0.1509669970255345, 0.15378301520831883, 0.1558162779547274, 0.15513158193789423, 0.16960160085000098, 0.160979556851089, 0.1627772729843855, 0.16633497714065015, 0.1632381258532405, 0.16758017684333026, 0.1567024781834334, 0.1580531259533018, 0.16180524602532387, 0.15462498599663377, 0.1577563090249896, 0.15341710089705884, 0.1590090529061854, 0.15742207504808903, 0.16145867807790637, 0.1578025659546256, 0.15599122387357056, 0.153844638960436, 0.15215380117297173, 0.1782863261178136, 0.20734925288707018, 0.15012734103947878, 0.15176251018419862, 0.1513134629931301, 0.1537256259471178, 0.17496285308152437, 0.15704363491386175, 0.15632525505498052, 0.16043069306761026, 0.20498006511479616, 0.16352625004947186, 0.15861924597993493, 0.15399476001039147, 0.15852230601012707, 0.15901762479916215, 0.15287777595221996, 0.18549719802103937, 0.15684323105961084, 0.1541199181228876, 0.15317231602966785, 0.15722580486908555, 0.15752986702136695, 0.16038210410624743, 0.20714857405982912, 0.15515446988865733, 0.15988492500036955, 0.21543463109992445, 0.2050700238905847, 0.16495052888058126, 0.15583638497628272, 0.16728644305840135, 0.16551725496537983, 0.1887108669616282, 0.16339496010914445, 0.1619160519912839, 0.16092860396020114, 0.16242533107288182, 0.16489047114737332, 0.15713850990869105, 0.15632302011363208, 0.161384193925187, 0.15891694789752364, 0.16001322492957115, 0.16180762299336493, 0.1633726090658456, 0.1641319189220667, 0.1505129081197083, 0.15514265513047576, 0.15334733808413148, 0.16609155503101647, 0.1526970430277288, 0.15089729987084866, 0.1544273030012846, 0.15125057892873883, 0.18260957300662994, 0.15602241386659443, 0.15290191397070885, 0.15507465996779501, 0.15429306798614562, 0.16139614791609347, 0.23544057086110115, 0.15555088897235692, 0.16982413409277797, 0.15488114184699953, 0.1575880020391196, 0.26365584088489413, 0.15932760294526815, 0.19335078983567655, 0.16265449905768037, 0.16363473306410015, 0.16389908804558218, 0.16382118500769138, 0.16116167791187763, 0.16277658101171255, 0.1666422770358622, 0.1583972650114447, 0.16015393286943436, 0.1587760541588068, 0.1586985569447279, 0.16123374900780618, 0.16200965899042785, 0.16605167998932302, 0.1597021061461419, 0.17026535607874393, 0.16908222599886358, 0.16951601090840995, 0.16604803898371756, 0.16990954196080565, 0.16664318810217083, 0.1659801029600203, 0.16698524309322238, 0.16746074403636158, 0.16698471689596772, 0.166886294959113, 0.2234906000085175, 0.17111960891634226, 0.16741955908946693, 0.17303833714686334, 0.17079391796141863, 0.21572350501082838, 0.18815535213798285, 0.16822135099209845, 0.16618380206637084, 0.2176455280277878, 0.21766121801920235, 0.16785678989253938, 0.16984005505219102, 0.1660006029997021, 0.21973196999169886, 0.21951013104990125, 0.16556282015517354, 0.16644477401860058, 0.16523367096669972, 0.1673978860490024, 0.16680756490677595, 0.16881816810928285, 0.19585965597070754, 0.16732951696030796, 0.16433846298605204, 0.16587579692713916, 0.2237750629428774, 0.1692435371223837, 0.16528627485968173, 0.17008676193654537, 0.166552587877959, 0.16628258302807808, 0.1678432310000062, 0.1661323991138488, 0.1699337000027299, 0.17427198588848114, 0.17032137396745384, 0.16961057297885418, 0.1668684638570994, 0.16998243797570467, 0.1691552058327943, 0.1708451600279659, 0.16969619411975145, 0.1660758899524808, 0.16982752899639308, 0.16713193501345813, 0.1728960070759058, 0.1949634039774537, 0.16791412490420043, 0.1694041220471263, 0.16766773001290858, 0.16813413915224373, 0.17380392202176154, 0.181511078029871, 0.16152242291718721, 0.15132701280526817, 0.1536646701861173, 0.23792208498343825, 0.17155742109753191, 0.18269530590623617, 0.24763871915638447, 0.15645852591842413, 0.15483863721601665, 0.16929739899933338, 0.17024339688941836, 0.16776094608940184, 0.16997468494810164, 0.17649599607102573, 0.17145007895305753, 0.16795479599386454, 0.17101972899399698, 0.18099723802879453, 0.22829724778421223, 0.16925060097128153, 0.16942952806130052, 0.16912742494605482, 0.1688757890369743, 0.16291826497763395, 0.1653841850347817, 0.16034395387396216, 0.1591005411464721, 0.1597292018122971, 0.159304385073483, 0.16362081188708544, 0.17783400299958885, 0.15625015110708773, 0.15624091099016368, 0.15594623098149896, 0.18643730296753347, 0.1544788631144911, 0.15312107210047543, 0.15226418594829738, 0.15198419685475528, 0.20853901910595596, 0.15187926893122494, 0.15138310589827597, 0.15419456805102527, 0.15411960589699447, 0.15726956608705223, 0.21658292412757874, 0.1513109349180013, 0.1540903551504016, 0.2029285931494087, 0.20714309415780008, 0.153356967959553, 0.1558741449844092, 0.15036280895583332, 0.15180357382632792, 0.15746639715507627, 0.2670239850413054, 0.15390156209468842, 0.15119936992414296, 0.2023113330360502, 0.20713147497735918, 0.15275346278212965, 0.15108689898625016, 0.15493189403787255, 0.16047052294015884, 0.1617567220237106, 0.16094112396240234, 0.16220127302221954, 0.1588834880385548, 0.21857642009854317, 0.15441223210655153, 0.1506700289901346, 0.15645649190992117, 0.15075331996195018, 0.16911898809485137, 0.16662045498378575, 0.16397031187079847, 0.16822650306858122, 0.16718776291236281, 0.16348699713125825, 0.18809920782223344, 0.15723000816069543, 0.152968754991889, 0.15333067602477968, 0.15145875606685877, 0.1543264859355986, 0.16685637389309704, 0.1635623900219798, 0.16193687380291522, 0.21346474182792008, 0.16807732288725674, 0.1618982821237296, 0.16214823094196618, 0.16320178797468543, 0.21127657685428858, 0.15121811418794096, 0.15404504188336432, 0.1582192329224199, 0.20829418417997658, 0.205060490174219, 0.15674842800945044, 0.15161647298373282, 0.15109261986799538, 0.14968823199160397, 0.15502054197713733, 0.1802957309409976, 0.15405611181631684, 0.15605550515465438, 0.1522581719327718, 0.1542973790783435, 0.1825320941861719, 0.1542529840953648, 0.15148913813754916, 0.15562859107740223, 0.15506136696785688, 0.15155560802668333, 0.15091335703618824, 0.2085709748789668, 0.15451723407022655, 0.15258419513702393, 0.1537876829970628, 0.1567832010332495, 0.15482767089270055, 0.1545652039349079, 0.15668691298924387, 0.15600860305130482, 0.15594327985309064, 0.15484629315324128, 0.15184854879043996, 0.20087464805692434, 0.16595759592019022, 0.1662287211511284, 0.16568542108871043, 0.1663140468299389, 0.1658389929216355, 0.1967318190727383, 0.1767928209155798, 0.15561607386916876, 0.15374868991784751, 0.19778625201433897, 0.15543264895677567, 0.15546984714455903, 0.15165049606002867, 0.1525108499918133, 0.1528805720154196, 0.18052446586079895, 0.15497745387256145, 0.1568210469558835, 0.15573850902728736, 0.15322853182442486, 0.1816518190316856, 0.1856508390046656, 0.1571724519599229, 0.15761526208370924, 0.15489203692413867, 0.15326956217177212, 0.15276749711483717, 0.15628355299122632]
[0.0012925151562274888, 0.0012757308607877686, 0.0012644035886077917, 0.0012967849002299152, 0.0014377861236792434, 0.0012603469225469716, 0.0012751235193464645, 0.0012483477985934, 0.0012582750312348668, 0.0012808491389245489, 0.0012659059545068547, 0.0012539983257766841, 0.0012583334482628708, 0.001253601643726114, 0.0012627261178207029, 0.0012498737995101268, 0.0017084219843120307, 0.0012976777209185583, 0.0012740835117883691, 0.0012727992558606374, 0.0012646305659973576, 0.0015552617291840472, 0.0012857013656360696, 0.0012647601642263259, 0.001253432892046349, 0.001260769650946523, 0.0012499681392381357, 0.001939911162480712, 0.0012682899995165508, 0.0012678023727336364, 0.0013531747909421607, 0.0013472501011637522, 0.001375328572539165, 0.0013803543555441983, 0.0013320905196141134, 0.001292792473768079, 0.001339454992580437, 0.0018847439701618382, 0.0018849531856416038, 0.0013051221468449795, 0.0012773804352149483, 0.0012565415511413137, 0.001271689628330312, 0.0012559706901106261, 0.0012541001088657352, 0.0012691846990943417, 0.0019173975041237219, 0.0013219516980642265, 0.0013268462097084568, 0.0013194324109385641, 0.0013184894576610982, 0.0014477361177722382, 0.0013150249846106351, 0.0013254511787361184, 0.0013119644871660212, 0.0019170564180601012, 0.001916771621060695, 0.0019168994414742835, 0.0013694434405066246, 0.0012726969601831925, 0.0012399443955875414, 0.001248668503853702, 0.001262285481315366, 0.0012480793033488268, 0.0019107135879092437, 0.001280466688421461, 0.0012848259693430376, 0.0012685402707998143, 0.0012669043954779474, 0.0014105866890573918, 0.0019418921397435804, 0.001289445450240674, 0.001286371953987567, 0.001868650907390686, 0.0018519680160133875, 0.0018202009062145569, 0.001851594759613391, 0.0012580383561782596, 0.0012480384424733099, 0.001236534805592987, 0.0012471983407790115, 0.0017860449377678392, 0.0013499327054492724, 0.001359952232879839, 0.0013448303482358076, 0.0013604653645451217, 0.0013719165278959643, 0.001549401371053947, 0.0013817737507092398, 0.0019669836340111125, 0.001230962967947703, 0.0013399115572286437, 0.0013196489380224961, 0.001594095185846668, 0.001358392861375744, 0.0013521758464071177, 0.0013885782786117967, 0.0013686265587055867, 0.001382538722106075, 0.00135454928769564, 0.002070004309982408, 0.0013925606969657333, 0.0013541036516485751, 0.0013332901876275392, 0.001597194659862985, 0.0013287957133219918, 0.0013509545125096113, 0.0013464977838416664, 0.0013464247977519913, 0.0013242289676626984, 0.0013425525353658338, 0.0012451444965491231, 0.0012537014432424723, 0.0013165922091855097, 0.001264857944072217, 0.001246868744636922, 0.0012835576975097258, 0.0018935439685836089, 0.0012459085979896, 0.0012526098682090294, 0.001250622100954832, 0.001259563418552857, 0.0012612442031156184, 0.0019337936744902484, 0.0012494475985046966, 0.001260568914833919, 0.0018520878299406564, 0.0018680569076867297, 0.0018687728366475235, 0.0018687446587249753, 0.0012721757353969323, 0.0012655437295866568, 0.0012762957909541537, 0.001307725496662333, 0.0012709126273502214, 0.0012512365356087685, 0.001265237232410284, 0.0013419799614957598, 0.0019134637293254221, 0.0012939413792865222, 0.0012706958981187537, 0.0012891700007598992, 0.0012523882711927097, 0.0014966484953382218, 0.0017136571632857009, 0.0012650847680462424, 0.0012715046579205944, 0.0012754157452678034, 0.0012747825271370568, 0.0013026982950147732, 0.00128509599152346, 0.0012549314640693423, 0.0012578307199761155, 0.001270165448546294, 0.001267053798750736, 0.0012809686045082965, 0.001485400867482373, 0.0015294256208594455, 0.0012696192856127208, 0.0012690748986896388, 0.001236936712796374, 0.0012356598063754712, 0.0012322852023436812, 0.0014868049770449252, 0.001318587109359891, 0.0012599508685976729, 0.0013508230227572743, 0.0013349332716749158, 0.0013226601313222055, 0.0021756285343088153, 0.0020391247596216293, 0.0013280771401992372, 0.001323528611281699, 0.0013376279308121334, 0.0013484198206588048, 0.0013488041175630434, 0.0013697990468999211, 0.0014067564336510824, 0.00125084782936141, 0.0012663942635377018, 0.0015969621631237425, 0.001299396085069161, 0.0013408318068110203, 0.0018679998280812604, 0.0012157556362623393, 0.001308645332925195, 0.0013147109682678944, 0.0013123557292820179, 0.0013455250318515901, 0.0013191264425945837, 0.0013623844648083282, 0.0019296755736275006, 0.001836083116471883, 0.0019451095578224623, 0.001236700147837978, 0.001345881822251881, 0.0013375491167368122, 0.0013507905961267015, 0.0013404875044357176, 0.0020175357674096905, 0.0013577063484911539, 0.0013500101171260656, 0.001357094767366269, 0.0013536679460991953, 0.0019290059851843487, 0.0019496201317353073, 0.0013439506508929784, 0.001348135729370884, 0.0013529240377536116, 0.001359739884113272, 0.001803813689900923, 0.0013393951930076808, 0.0013505568233311408, 0.0013374763652826695, 0.0018032764485322458, 0.0012441301779720442, 0.0013426028917608566, 0.001328231378768881, 0.0013428782094847555, 0.0013275991479122593, 0.0013864037595227245, 0.0013218545661733826, 0.0013104526203327864, 0.0013293932397784882, 0.001327048489000908, 0.0013345587283892686, 0.0014572662785071735, 0.0017974616512078647, 0.0012257701785902875, 0.0012613946207358626, 0.0012532963810936194, 0.0012325336741529, 0.0012322786361585523, 0.0012476479927034571, 0.0012569361232769812, 0.0012271159992719343, 0.0012391603808099216, 0.0012480437055287898, 0.0012549199218060388, 0.0012541162951076909, 0.001254668287831799, 0.0013091124345136936, 0.0013108779297231934, 0.0013134162331563096, 0.0012980480227157358, 0.0013128629842296589, 0.0013156584734746882, 0.0013174952563735865, 0.0012958843654469233, 0.0013038438308059937, 0.0013022634269337552, 0.001319127294502055, 0.0013045719914975785, 0.0013417036438635153, 0.0013389839057976654, 0.0013125800841682872, 0.0018846396981312553, 0.001900637497071379, 0.0019050480693203304, 0.0013113401472828416, 0.0012946011238666468, 0.001425782092233268, 0.0013241120848746955, 0.001214207000236294, 0.0012460028601073941, 0.0012281737759593846, 0.0012379375416352305, 0.001240272394403122, 0.0018377602172567863, 0.0012366793176173702, 0.0012700598679713962, 0.0012300360619357622, 0.0012555401321038488, 0.0012261232635540555, 0.0015352848058118838, 0.0012479169157066548, 0.0016183835046380296, 0.0012299313404878906, 0.001251429673139901, 0.0019963298989243284, 0.0012550340701876453, 0.001261374835545818, 0.0012574375105314246, 0.0013056910459616387, 0.0012275292339568684, 0.001830419426235233, 0.0012448919132034214, 0.001240188992300699, 0.0012426216357438139, 0.0012405555345146924, 0.001251381045828263, 0.0012407894336524629, 0.0012722492322701123, 0.0012420024885254543, 0.0012583084740177837, 0.0018197870777212372, 0.0018520347607979017, 0.0018523690929679677, 0.0012470466507122266, 0.0013469153628937265, 0.0013419787359763254, 0.0013339977274656064, 0.0013366000918137242, 0.0017857932243784962, 0.00133489383666148, 0.0013627733028125624, 0.0013164940936303184, 0.0013417708380653183, 0.0013351613338025965, 0.0013522295653098082, 0.0013327194186461757, 0.001364183743924711, 0.0013564002570198026, 0.0013211534102266843, 0.0013283865197828806, 0.0013192969053088463, 0.0013340062158572119, 0.001477989813948209, 0.0019971412180583605, 0.0013266390698507082, 0.001363467287936414, 0.0019337880919694669, 0.001884527356443248, 0.0012650368915683077, 0.0012531599910684335, 0.001247770263084261, 0.001228624425987287, 0.0012506567530131847, 0.0013991897987100737, 0.0018688602095051098, 0.0012498624972970217, 0.0012769332018413746, 0.001260167170070516, 0.0013313018752969513, 0.0012289116974891156, 0.001419976308523107, 0.001718057985525838, 0.001234683993294142, 0.0012082859545281923, 0.001238669232689133, 0.0012266037213190118, 0.0012243631938400196, 0.0014735362095426219, 0.0012908657677038464, 0.0013115048686461162, 0.0013003876656582651, 0.0013069306272840084, 0.00131841427863164, 0.0013342527582403995, 0.0013517874722592822, 0.0012704457766412533, 0.0012599045425191406, 0.0013358058288780062, 0.001852401783112292, 0.001852124434888594, 0.001966241232747602, 0.0013316070151970136, 0.0013445795499250409, 0.0013453373035719229, 0.0013317246975507154, 0.0013578703713624976, 0.0019439354488380658, 0.001344118751007919, 0.0013621545274012774, 0.0013309220382727164, 0.0013255432082065778, 0.0013755064948584683, 0.001277255318524648, 0.0012355665730442418, 0.001235999619992551, 0.0012377390381699616, 0.0012452794047606776, 0.0012498579616116922, 0.0014790372089157964, 0.0013649449376894752, 0.00130942379767003, 0.0013566726436044358, 0.0012679827280429213, 0.0012977344648147275, 0.0014714246900939896, 0.0012486496355471222, 0.0012416600776735203, 0.0013229065509729606, 0.001315954534784537, 0.0013250816295775332, 0.0013657108367991079, 0.001314017488512882, 0.001328420998571917, 0.0013169711094725039, 0.001917125303442626, 0.0019852987530301937, 0.001353808216635109, 0.0013244440002987783, 0.0012565962952207918, 0.0013114427985233624, 0.0013439590544548146, 0.0013703416054707397, 0.0019334082567501207, 0.0017892317918612975, 0.0013492917299068481, 0.001290657566571536, 0.0013090064424241697, 0.001274069783804837, 0.001272483604288725, 0.001273519442553899, 0.0012800238914643378, 0.0012772299995458172, 0.0013283127666155036, 0.0013016188524432423, 0.001404415668067775, 0.0013863042939107778, 0.0014037419698930294, 0.0014738172253001799, 0.0014086297511072584, 0.0013971180156912915, 0.0013611897295557481, 0.0013828735813036445, 0.00138213558035881, 0.0013568386500484723, 0.0019132873104784147, 0.001268537000341471, 0.0012669406665214735, 0.0012641470074335965, 0.0012770920861945596, 0.0012810368455455515, 0.0019188048913641724, 0.0012950004338351793, 0.0012770392877851115, 0.0018688205651245838, 0.0014622166135543307, 0.001291554489772218, 0.001269412272097182, 0.0012986660545128723, 0.001293988162001898, 0.0012817973173548315, 0.0012917842936175045, 0.001670777333842576, 0.0012575085040239633, 0.0012746562246348737, 0.0012724151394452698, 0.001269003008167411, 0.001418616279757531, 0.0013663587358946254, 0.0013384629770131537, 0.0012727599363600791, 0.0013690060546058555, 0.0018683962105203043, 0.0018688797059129606, 0.0012595974731930467, 0.0012719313949779716, 0.0012545574117134245, 0.0012602292004857184, 0.0013337845592795647, 0.0012623260552135779, 0.0012671650087735218, 0.0012765428603648446, 0.0012556727210334105, 0.0012449412642754325, 0.0016853691389163334, 0.0020290051023800705, 0.001377454108753523, 0.0013339557836568633, 0.0013551591000295887, 0.0012616368999719158, 0.0012625717673479587, 0.001247952356501374, 0.0017928504800178516, 0.001402859581461133, 0.0013463884418786958, 0.0013593974804808927, 0.001346523216528486, 0.0018387729385073565, 0.0013473758459553239, 0.0013395635349777086, 0.0012512460924944905, 0.0012826910152193881, 0.0012633721849541794, 0.0012694494257312874, 0.0012570867448496495, 0.0015331961019614407, 0.0016965810624637115, 0.0013060281468309867, 0.0012622409835890975, 0.0013052465108966873, 0.0012872316351145968, 0.00185065958179649, 0.0019947578607112633, 0.0012798228142277678, 0.0013759619531098955, 0.0013847004643879658, 0.0013458778605211614, 0.0012323694184509127, 0.0014594554803659056, 0.0013556396751123112, 0.0013275095586513365, 0.0012619980311295552, 0.0013174192868028731, 0.0013216436522933402, 0.0013162234415437363, 0.0013462376271829355, 0.0013300031170884307, 0.001337586032125608, 0.0013564332792636497, 0.0012508084737630778, 0.0012535376279333302, 0.001261076806907275, 0.0012316761318189923, 0.0012253836345996044, 0.001238811938019917, 0.001244184667562214, 0.0012519246656736431, 0.0016840007210280313, 0.0017475391857239397, 0.001260915922737399, 0.0012650074502236622, 0.0013321011395792859, 0.0013514637293712806, 0.0013714799000484536, 0.0013519357294277396, 0.0012841406350507754, 0.0018699080773535394, 0.0018845501233094423, 0.001868553587791416, 0.0012576178297429353, 0.0012353901000505731, 0.0012776664432946794, 0.0012452360766022936, 0.0012199036116430232, 0.0012584131088309973, 0.0019128595265855853, 0.0012192282240924446, 0.0012464922719252433, 0.0012232559848605663, 0.001287345574129113, 0.0019335899079694998, 0.0013350050863918177, 0.0013595299918002398, 0.0013821570242415796, 0.0013790775429775087, 0.0013824553867845342, 0.0013956938989373834, 0.0013664879135845244, 0.0013687713252192781, 0.0012764562960974005, 0.0013786844021185887, 0.0019494269761552874, 0.0013505326035295345, 0.0012592439676902092, 0.001230893891246975, 0.001250661534163379, 0.0012505113872764415, 0.001284526821673494, 0.0012888862325470577, 0.0012650022936566163, 0.0012535546751072009, 0.0013312556123895238, 0.0013177560303042563, 0.0013395755735854077, 0.0013641879240469646, 0.0023205063500723174, 0.0012631967208932998, 0.0013423839533456075, 0.0013407515361905098, 0.0013305757451051658, 0.001382460729255117, 0.0013202233492692773, 0.0012673956500871698, 0.0012538577664585769, 0.0012562241091198007, 0.0018522246044071376, 0.0018847855349439521, 0.001295013084300151, 0.0012935015279077744, 0.0012554211249618337, 0.0018687522320380045, 0.001884891976450765, 0.001446988433599472, 0.0012767939618969148, 0.001252679914134995, 0.001255198921502907, 0.0012571626277857048, 0.0014470620171073102, 0.001627350287410871, 0.0013330333266955937, 0.0013377303257584572, 0.0012641420836971944, 0.0012566314201599867, 0.0012722923727200707, 0.001940990673638014, 0.0012794299621510412, 0.001317460983341863, 0.0012652508376613837, 0.0014623738283854584, 0.0018612063027802826, 0.0012759163726640994, 0.001280681993595736, 0.002223200332589967, 0.0012760990544749323, 0.0012925062545163686, 0.0012814418373037447, 0.0012568687828791926, 0.0012778953501102767, 0.0012684265122741693, 0.0017267067126150047, 0.0016487204115982204, 0.0012669240218377853, 0.0012609388989706833, 0.0012767886031823333, 0.0013108573864798905, 0.001868804223662199, 0.001305863712445017, 0.001268217542259268, 0.0012743073631106883, 0.001259896279738624, 0.001260265162893275, 0.0013099615281635477, 0.0015631857609679533, 0.0012588221399301245, 0.0013267170157745597, 0.0013296570477270802, 0.0012623452574245689, 0.001862688634414659, 0.0018679152094838462, 0.0012630904183629178, 0.001260215898455922, 0.0012711795577562827, 0.0012636078835659942, 0.001269738355925841, 0.0013038142558995836, 0.0014683278999471849, 0.0012507954262225896, 0.001575810262615713, 0.0018684197444642006, 0.0018848788747636153, 0.0013090745192165523, 0.0012815769151748382, 0.0012538770462797826, 0.001246596805664689, 0.0018505116675568874, 0.001865444038886317, 0.0013413375113592591, 0.001333717070880902, 0.0014246693639041379, 0.0019331176353748454, 0.0019495381157874137, 0.0013335178454601487, 0.001281222148054792, 0.0012714039379109938, 0.0013736409691068553, 0.0013857108916249848, 0.0014265103323399559, 0.001277385449195786, 0.001246741101102427, 0.001321149683131497, 0.0013418297821208257, 0.0020750431946745906, 0.001435841412045235, 0.0012372903410316437, 0.0012300340224115193, 0.0012456378520488045, 0.0013474236357986003, 0.0013006505747722795, 0.0012262237525379012, 0.0012336405654719404, 0.001382084272787437, 0.0013461676967698474, 0.0013310969915500907, 0.0013118033882143886, 0.0013155983797050724, 0.001759880130330837, 0.0012726651165316733, 0.0013095430773502404, 0.0012882462857911985, 0.0019333404651301544, 0.0013352870388866165, 0.0013305362017984076, 0.0013236442865991547, 0.0012963710302241551, 0.0018528032643619434, 0.0012778783877661756, 0.0013365907370755377, 0.0013152008511616972, 0.0012684715803452703, 0.0012915053805552942, 0.0013370935501920622, 0.0013960844967081103, 0.0013886632004305142, 0.001314003255160511, 0.0012640203800514456, 0.0012714708830381548, 0.0012640801561606485, 0.001858334636402338, 0.001469683326246549, 0.0012757387986883175, 0.0013351635971330395, 0.0013443966046004564, 0.0013456539297837388, 0.0013519295747572368, 0.0013913261169899804, 0.0012221814213262405, 0.001732100530716707, 0.0017158704777102685, 0.0017162346812256146, 0.0013402961412793957, 0.001340466937108431, 0.0013418026646832004, 0.0011972882439295063, 0.001265180890186457, 0.0012227064689795952, 0.0012043384613207309, 0.0016344154373655329, 0.0016180904058273882, 0.0012415023120411206, 0.0012307336946832947, 0.0012204407103126869, 0.0011999475773336599, 0.0011907477728527738, 0.001642146702579339, 0.0012004450709355297, 0.001177905265649315, 0.0012202428279124433, 0.0011716508670360781, 0.0011675689838739345, 0.0011672738819470396, 0.001193679016068927, 0.0011882229136972455, 0.001182708812848432, 0.0011734518593584653, 0.0012595829448400764, 0.001275256288863602, 0.0012842420946981292, 0.0012760068821080495, 0.0012821428044844652, 0.0014721731095050927, 0.0011707836420100648, 0.0011767874766519526, 0.0017772881474229507, 0.0011728872432286153, 0.0011664197809295729, 0.0016271168296952965, 0.0012245790621818742, 0.0013346272735361708, 0.0015740680391900241, 0.0016671705707267392, 0.0012501703040470602, 0.0011907009848073358, 0.0012166593041911256, 0.0011938232346437871, 0.001272443945708801, 0.0012917001567984698, 0.001276898921787506, 0.0012240643518452998, 0.0012953452423971612, 0.0013016160864935955, 0.0013583775144070387, 0.0017421304146409966, 0.001326955351032666, 0.0017172351181216072, 0.0017327444766124245, 0.001295882915655966, 0.0012600142345036147, 0.0012611999845830724, 0.0012538623668660875, 0.001272376508495654, 0.0015196370859484887, 0.001306371414102614, 0.001211344359035138, 0.0012004990785499103, 0.0013020535152463708, 0.001537411038952996, 0.0016674048893037252, 0.00130222347615927, 0.0012159394445916405, 0.001579025125465705, 0.0016181344308279222, 0.0016014795783121372, 0.001315590483500273, 0.0012105347741453443, 0.0012883882973255822, 0.0011843553602375323, 0.001262284680706216, 0.001271349876333261, 0.0012609384539246093, 0.0011909491404367145, 0.001205536109409877, 0.0011907014213647926, 0.0012266144913155586, 0.001602389280378702, 0.0016198290632019052, 0.0012188399377919268, 0.0012022531336697284, 0.001205599579407135, 0.0012033796720061218, 0.0012236841630510753, 0.0012051946341671282, 0.0011882256549142767, 0.0011769473676395137, 0.00126129803902586, 0.0012660938209592132, 0.0012758633438352263, 0.0012554071090562502, 0.0012845786950492766, 0.0012577179368236102, 0.0012518183448264608, 0.0012450866561266594, 0.0012765281717292964, 0.0016018788282963214, 0.0011722393428499345, 0.0012011062044621212, 0.001194986407426768, 0.0012326344622124452, 0.001231143834957038, 0.0012048441167280544, 0.0012677896793320542, 0.0015041129918245133, 0.0011957159695157316, 0.0011896357191290008, 0.0012140167418692727, 0.0013690526866412256, 0.0013139407565176953, 0.001321955312960199, 0.0012202105699543608, 0.0012219222026033094, 0.0012164613035565708, 0.0012754880008287728, 0.0012139287955506006, 0.00124935366329737, 0.001951349304363248, 0.0012542317344923504, 0.0012718780708382837, 0.0012629605080292094, 0.0014790188670303905, 0.001931791945025907, 0.001196532359244884, 0.0012055409297317965, 0.001184947866931907, 0.0011957707047258737, 0.0012054299513692968, 0.0011794296642619884, 0.0012014298063149909, 0.001217314671521308, 0.0012119654838897986, 0.0013250125066406326, 0.0012576527878991328, 0.0012716974451905116, 0.0012994920089113293, 0.0012752978582284413, 0.0013092201315885177, 0.0012242381108080735, 0.0012347900465101702, 0.0012641034845728427, 0.0012080077030987013, 0.0012324711642577313, 0.0011985711007582722, 0.0012422582258295733, 0.0012298599613131955, 0.0012613959224836435, 0.0012328325465205126, 0.00121868143651227, 0.0012019112418784061, 0.0011887015716638416, 0.0013928619227954186, 0.0016199160381802358, 0.001172869851870928, 0.0011856446108140517, 0.0011821364296338288, 0.0012009814527118579, 0.0013668972896994092, 0.001226903397764545, 0.0012212910551170353, 0.0012533647895907052, 0.001601406758709345, 0.0012775488285114989, 0.0012392128592182416, 0.0012030840625811834, 0.0012384555157041177, 0.0012423251937434543, 0.0011943576246267185, 0.00144919685953937, 0.0012253377426532097, 0.0012040618603350595, 0.0011966587189817801, 0.0012283266005397309, 0.0012307020861044293, 0.001252985188330058, 0.001618348234842415, 0.0012121442960051354, 0.001249100976565387, 0.0016830830554681597, 0.001602109561645193, 0.001288676006879541, 0.0012174717576272087, 0.0013069253363937605, 0.00129310355441703, 0.0014743036481377203, 0.001276523125852691, 0.0012649691561819054, 0.0012572547184390714, 0.0012689478990068892, 0.001288206805838854, 0.0012276446086616488, 0.0012212735946377506, 0.0012608140150405234, 0.0012415386554494034, 0.0012501033197622746, 0.0012641220546356635, 0.0012763485083269188, 0.001282280616578646, 0.001175882094685221, 0.0012120519932068419, 0.0011980260787822772, 0.0012975902736798162, 0.0011929456486541312, 0.0011788851552410051, 0.001206463304697536, 0.0011816451478807721, 0.0014266372891142964, 0.001218925108332769, 0.0011945462028961629, 0.0012115207809983985, 0.0012054145936417626, 0.0012609074055944802, 0.0018393794598523527, 0.0012152413200965384, 0.0013267510475998279, 0.0012100089206796838, 0.0012311562659306219, 0.0020598112569132354, 0.0012447468980099075, 0.001510553045591223, 0.0012707382738881279, 0.0012783963520632824, 0.0012804616253561107, 0.001279853007872589, 0.001259075608686544, 0.0012716920391540043, 0.0013018927893426735, 0.0012374786329019116, 0.001251202600542456, 0.0012404379231156781, 0.0012398324761306867, 0.0012596386641234858, 0.0012657004608627176, 0.001297278749916586, 0.0012476727042667335, 0.001330198094365187, 0.0013209548906161217, 0.0013243438352219528, 0.0012972503045602934, 0.0013274182965687942, 0.0013018999070482096, 0.0012967195543751586, 0.0013045722116657998, 0.0013082870627840748, 0.0013045681007497478, 0.0013037991793680703, 0.001746020312566543, 0.001336871944658924, 0.0013079653053864604, 0.0013518620089598699, 0.001334327484073583, 0.0016853398828970967, 0.001469963688577991, 0.0013142293046257691, 0.0012983109536435222, 0.0017003556877170922, 0.0017004782657750184, 0.001311381171035464, 0.0013268754300952423, 0.0012968797109351726, 0.0017166560155601474, 0.0017149228988273535, 0.0012934595324622933, 0.001300349797020317, 0.0012908880544273416, 0.0013077959847578313, 0.0013031841008341871, 0.0013188919383537723, 0.0015301535622711526, 0.001307261851252406, 0.0012838942420785315, 0.0012959046634932747, 0.0017482426792412298, 0.0013222151337686228, 0.0012912990223412635, 0.0013288028276292607, 0.0013011920927965548, 0.00129908267990686, 0.0013112752421875484, 0.0012979093680769438, 0.0013276070312713273, 0.001361499889753759, 0.001330635734120733, 0.0013250826013972983, 0.0013036598738835892, 0.0013279877966851927, 0.0013215250455687055, 0.0013347278127184836, 0.0013257515165605582, 0.0012974678902537562, 0.001326777570284321, 0.0013057182422926417, 0.001350750055280514, 0.001523151593573857, 0.001311829100814066, 0.0013234697034931742, 0.0013099041407258483, 0.0013135479621269042, 0.001357843140795012, 0.001418055297108367, 0.0012618939290405251, 0.0011822422875411576, 0.0012005052358290413, 0.0018587662889331114, 0.001340292352324468, 0.00142730707739247, 0.0019346774934092537, 0.0012223322337376885, 0.00120967685325013, 0.001322635929682292, 0.001330026538198581, 0.0013106323913234519, 0.001327927226157044, 0.0013788749693048885, 0.001339453741820762, 0.0013121468437020667, 0.0013360916327656014, 0.0014140409220999572, 0.001783572248314158, 0.001322270320088137, 0.0013236681879789103, 0.0013213080073910533, 0.0013193421018513618, 0.0012727989451377653, 0.001292063945584232, 0.0012526871396403294, 0.0012429729777068133, 0.0012478843891585711, 0.0012445655083865859, 0.001278287592867855, 0.0013893281484342879, 0.001220704305524123, 0.0012206321171106538, 0.0012183299295429606, 0.0014565414294338552, 0.0012068661180819618, 0.0011962583757849643, 0.0011895639527210733, 0.0011873765379277756, 0.001629211086765281, 0.0011865567885251949, 0.001182680514830281, 0.001204645062898635, 0.0012040594210702693, 0.0012286684850550955, 0.0016920540947467089, 0.0011821166790468851, 0.0012038308996125124, 0.0015853796339797555, 0.0016183054231078131, 0.0011981013121840078, 0.001217766757690697, 0.0011747094449674478, 0.0011859654205181869, 0.0012302062277740333, 0.0020861248831351986, 0.0012023559538647532, 0.0011812450775323668, 0.0015805572893441422, 0.0016182146482606186, 0.0011933864279853879, 0.0011803663983300794, 0.0012104054221708793, 0.001253675960469991, 0.0012637243908102391, 0.0012573525309562683, 0.0012671974454860901, 0.0012412772503012093, 0.0017076282820198685, 0.0012063455633324338, 0.0011771096014854265, 0.0012223163430462591, 0.0011777603122027358, 0.0013212420944910264, 0.0013017223045608262, 0.001281018061490613, 0.0013142695552232908, 0.0013061543977528345, 0.001277242165087955, 0.0014695250611111987, 0.001228359438755433, 0.0011950683983741328, 0.0011978959064435912, 0.0011832715317723341, 0.0012056756713718642, 0.0013035654210398206, 0.0012778311720467173, 0.0012651318265852751, 0.0016676932955306256, 0.0013131040850566933, 0.0012648303290916374, 0.0012667830542341108, 0.00127501396855223, 0.0016505982566741295, 0.0011813915170932887, 0.0012034768897137837, 0.0012360877572064055, 0.001627298313906067, 0.001602035079486086, 0.0012245970938238315, 0.0011845036951854127, 0.001180411092718714, 0.001169439312434406, 0.0012110979841963854, 0.0014085603979765438, 0.0012035633735649753, 0.0012191836340207374, 0.0011895169682247797, 0.0012054482740495587, 0.0014260319858294679, 0.0012051014382450376, 0.0011835088916996028, 0.001215848367792205, 0.001211416929436382, 0.0011840281877084635, 0.0011790106018452207, 0.0016294607412419282, 0.001207165891173645, 0.0011920640245079994, 0.0012014662734145531, 0.0012248687580722617, 0.001209591178849223, 0.001207540655741468, 0.0012241165077284677, 0.0012188172113383189, 0.0012183068738522707, 0.0012097366652596975, 0.0011863167874253122, 0.0015693331879447214, 0.001296543718126486, 0.0012986618839931907, 0.0012944173522555502, 0.0012993284908588976, 0.0012956171322002774, 0.0015369673365057679, 0.0013811939134029672, 0.001215750577102881, 0.0012011616399831837, 0.0015452050938620232, 0.0012143175699748099, 0.0012146081808168674, 0.001184769500468974, 0.0011914910155610414, 0.0011943794688704656, 0.0014103473895374918, 0.0012107613583793864, 0.0012251644293428399, 0.0012167071017756825, 0.0011970979048783192, 0.0014191548361850437, 0.00145039717972395, 0.0012279097809368977, 0.0012313692350289784, 0.0012100940384698333, 0.0011974184544669697, 0.0011934960712096654, 0.0012209652577439556]
[773.6853182586551, 783.864395490516, 790.8867145031429, 771.1379117868381, 695.5137370786663, 793.4323336777387, 784.2377501691187, 801.0588083919956, 794.7388092240878, 780.7320703198811, 789.9480972024966, 797.44923054872, 794.7019141710807, 797.7015705145958, 791.9373693844766, 800.0807764687428, 585.3354786947984, 770.6073579595342, 784.8779069406122, 785.6698496604818, 790.7447652203035, 642.9785940432291, 777.7855937061047, 790.6637386952459, 797.8089663559127, 793.166300639653, 800.0203914073418, 515.4875230065813, 788.4632066650229, 788.7664682657119, 739.0028300067138, 742.2526813219029, 727.0989783581502, 724.4516569122243, 750.6997349472053, 773.5193546457754, 746.5723040633992, 530.5760441903059, 530.517154281271, 766.2118081571247, 782.8521342834935, 795.835202657407, 786.3553949975735, 796.196923920191, 797.3845093630086, 787.9073870915516, 521.5402637425536, 756.4572907348506, 753.6668475088205, 757.9016490042568, 758.4436827988941, 690.73361348392, 760.4418255947352, 754.4600782305299, 762.2157533852942, 521.6330571073726, 521.7105621830025, 521.6757740984588, 730.2236590582015, 785.7329995162867, 806.4877776443799, 800.8530662171354, 792.2138175573, 801.2311375701975, 523.364677117426, 780.9652598091279, 778.3155258850536, 788.3076501540628, 789.3255430870488, 708.9248805177925, 514.9616600909906, 775.5271848168147, 777.3801324726838, 535.1454335557852, 539.9661286551998, 549.3899033814263, 540.0749785059868, 794.8883236262022, 801.2573699398572, 808.7115667726349, 801.797089767928, 559.8963267126854, 740.777666889098, 735.3199442030377, 743.5882163960923, 735.042600907635, 728.907320282559, 645.4105557682393, 723.7074806832289, 508.3926387129007, 812.3721233200288, 746.317915242341, 757.7772930265131, 627.3151119698492, 736.164057124996, 739.5487818075671, 720.1610563861981, 730.6595021404344, 723.3070466747297, 738.2529444175492, 483.0907815880338, 718.1015536191073, 738.4959037534047, 750.0242702448769, 626.0977607361802, 752.5611273233251, 740.21737278359, 742.6673938867688, 742.7076519012523, 755.1564151062397, 744.8498093428491, 803.1196401473621, 797.6380703636112, 759.5366226712182, 790.602616433348, 802.009036076361, 779.0845724661494, 528.1102612832437, 802.6270960916407, 798.3331645229582, 799.6020534392559, 793.9258835802998, 792.8678661354606, 517.1182495793414, 800.3536932615433, 793.2926064036338, 539.931197556674, 535.3155976593506, 535.1105176560386, 535.1185863360752, 786.0549232123103, 790.1741967672766, 783.517431529257, 764.6864747626844, 786.8361510302574, 799.2093992951272, 790.3656123800549, 745.1676095710164, 522.6124669488994, 772.8325378630359, 786.9703533949272, 775.6928872146821, 798.4744212333223, 668.1595599199221, 583.5472937204302, 790.4608649619337, 786.4697889785081, 784.0580639766421, 784.4475263132362, 767.6374520691758, 778.1519875527093, 796.8562655663434, 795.0195396873345, 787.2990098608817, 789.2324706227625, 780.6592577527323, 673.2189416954591, 653.8402301892, 787.6376889765013, 787.9755568662911, 808.4487990814622, 809.2842340913184, 811.5004530591634, 672.5831668841555, 758.3875141062548, 793.6817418229978, 740.2894258929782, 749.1011133053255, 756.0521227780138, 459.63728836535705, 490.40648213479363, 752.968310146488, 755.5560125229213, 747.5920448168689, 741.6087962215095, 741.3974994432501, 730.0340894988672, 710.8551104362889, 799.4577569923318, 789.6435010740469, 626.1889123559115, 769.5882814259629, 745.8056968221534, 535.3319550501044, 822.5337149777499, 764.148982799423, 760.6234557527729, 761.988520099725, 743.2043078558637, 758.0774425483463, 734.0071953482591, 518.221826335372, 544.637653398581, 514.1098587369514, 808.6034450211868, 743.0072859791183, 747.6360961156156, 740.3071970351517, 745.9972559915454, 495.6541619501981, 736.53629233694, 740.7351895471897, 736.8682158731721, 738.7336036741177, 518.4017093158128, 512.9204318945585, 744.074940054947, 741.7650746981212, 739.1397980188119, 735.4347781392987, 554.3809793654057, 746.6056360516335, 740.4353395020476, 747.6767634609039, 554.5461433902369, 803.7744101907559, 744.8218726003752, 752.880873004887, 744.669168757818, 753.2394108361462, 721.2906003257336, 756.5128763710258, 763.0951203302966, 752.2228713654556, 753.5519676096144, 749.3113481839343, 686.2163866334723, 556.340102904569, 815.8136145472739, 792.7733189607451, 797.8958649249474, 811.3368591631246, 811.504777131699, 801.5081223616263, 795.5853773960141, 814.9188834578919, 806.9980411626741, 801.2539910021061, 796.863594739044, 797.3742179262013, 797.023412242376, 763.8763284465157, 762.8475370022907, 761.3732606280267, 770.3875222642619, 761.6941082292485, 760.0756732550614, 759.0160155509826, 771.6737902421726, 766.9630184021598, 767.8937911621694, 758.0769529732767, 766.5349298600637, 745.321073378351, 746.8349661785334, 761.8582759722789, 530.6053995315741, 526.1392567182656, 524.9211377415652, 762.5786506056777, 772.4386929413845, 701.3694487028198, 755.2230747101993, 823.5827991482442, 802.566376062579, 814.2170265920657, 807.7951967423725, 806.2744962418093, 544.1406286902293, 808.6170648722704, 787.3644583363227, 812.984294481786, 796.469960959629, 815.5786858666951, 651.3449466929255, 801.3353993472654, 617.9005143923916, 813.0535153314485, 799.0860545051236, 500.9192120695204, 796.7911180693969, 792.7857539406859, 795.2681478202245, 765.8779640810833, 814.6445496671055, 546.3228731443144, 803.2825897525089, 806.3287178068565, 804.7501920416954, 806.0904749348459, 799.1171061234357, 805.9385201696467, 786.0095134155987, 805.1513658295745, 794.7176869968906, 549.5148373359228, 539.9466690188772, 539.8492146064394, 801.8946199236967, 742.4371475365683, 745.1682900716554, 749.6264644317262, 748.166939479281, 559.9752459291734, 749.1232430145627, 733.7977622075132, 759.5932293493506, 745.283748633177, 748.973157537417, 739.5194023663362, 750.3454860857629, 733.0390824941466, 737.2455105524215, 756.9143691105634, 752.7929447548489, 757.9794934529167, 749.6216944966897, 676.5946494101086, 500.71571852701, 753.7845241603911, 733.4242697626315, 517.1197424127014, 530.6370303306949, 790.4907806761803, 797.9827054224805, 801.4295816989435, 813.918378023804, 799.5798987937482, 714.6993216516512, 535.085500196298, 800.0880114113517, 783.1263205921587, 793.5455102707066, 751.1444388050206, 813.7281157329508, 704.2371017021284, 582.0525316518549, 809.923839161465, 827.618657861894, 807.3180261602321, 815.2592256321083, 816.7511119504171, 678.6395838283437, 774.6738855572646, 762.4828728484349, 769.0014496513954, 765.1515536659701, 758.4869310107011, 749.4831798727485, 739.7612572401418, 787.1252897103208, 793.7109251154302, 748.6117955031965, 539.8396876512725, 539.9205264845774, 508.58459447654445, 750.9723128426506, 743.7269145256219, 743.3080145365486, 750.9059506361806, 736.4473230214105, 514.4203736794464, 743.9818834832312, 734.1310988466214, 751.3588108420008, 754.4076977716717, 727.0049278123505, 782.9288204922847, 809.3453010274932, 809.0617374186787, 807.9247475934292, 803.0326336218366, 800.0909148992415, 676.1155121533736, 732.6302859460106, 763.6946890528383, 737.0974897401775, 788.6542757119873, 770.5736628816173, 679.613443169914, 800.8651678834061, 805.3734012884468, 755.9112918932392, 759.9046726669256, 754.6704879750112, 732.2194223366898, 761.0248788482518, 752.7734062281633, 759.3180995447481, 521.6143139962093, 503.70252762899474, 738.657062139496, 755.033810243704, 795.8005318042847, 762.5189608925103, 744.0702874728995, 729.7450475178998, 517.2213351777585, 558.8990786709209, 741.1295703035529, 774.7988512990088, 763.9381805853332, 784.8863639271275, 785.8647424844157, 785.2255462975996, 781.2354180795855, 782.9443407652496, 752.834742790259, 768.2740597394701, 712.0399058035441, 721.3423520308015, 712.3816352632128, 678.5101862249743, 709.9097539392069, 715.7591475944155, 734.6514437236976, 723.1318997773412, 723.5180211049876, 737.0073073642733, 522.6606555760575, 788.309682516801, 789.3029456111874, 791.0472390629207, 783.028891033042, 780.6176719094545, 521.1577292202185, 772.2005135075303, 783.0612648843353, 535.0968512770704, 683.8932007270915, 774.2607903258984, 787.7661355423255, 770.0208968464171, 772.8045969546769, 780.1545427350716, 774.1230520767568, 598.5238007150401, 795.2232504194213, 784.5252552596687, 785.9070275098788, 788.0201966141256, 704.9122544758328, 731.8722190078789, 747.1256337859636, 785.694121438065, 730.4569593652423, 535.2183837503735, 535.0799181114191, 793.9044188974325, 786.2059258450168, 797.0938521133439, 793.506450742912, 749.7462712719839, 792.1883540863822, 789.163205325478, 783.3657850815861, 796.3858601443587, 803.2507465980809, 593.341824594572, 492.85238308517637, 725.9769989033707, 749.6500350698524, 737.9207356377314, 792.6210782375342, 792.0341844016577, 801.3126420975663, 557.7709971609249, 712.8297181093927, 742.7277068753209, 735.6200186911084, 742.65336662975, 543.8409381920536, 742.1834100721723, 746.5118106672265, 799.2032950180047, 779.6109804581134, 791.5323860294333, 787.7430795826572, 795.4900519769639, 652.2322869988287, 589.4207015064977, 765.6802821795618, 792.241745436412, 766.1388033996835, 776.8609570498741, 540.3478899286656, 501.3139788522672, 781.3581605852136, 726.7642813377499, 722.177846919404, 743.009473097932, 811.4449977645494, 685.1870532900984, 737.6591422917396, 753.2902444905445, 792.3942631708759, 759.0597845480238, 756.6336041222471, 759.749422808598, 742.8109122848886, 751.8779370902113, 747.6154624692534, 737.2275623780461, 799.4829112337908, 797.7423076231623, 792.9731119648833, 811.9017444327325, 816.0709607703805, 807.2250269063217, 803.7392085528141, 798.770107674102, 593.8239737745067, 572.2332341210064, 793.0742898614836, 790.5091782844386, 750.6937501125683, 739.9384669133619, 729.139377080678, 739.680132888632, 778.7309058718943, 534.785646476959, 530.6306198128117, 535.1733054559999, 795.1541210292844, 809.4609143776229, 782.6768913342754, 803.0605752513725, 819.7368959775054, 794.6516076337999, 522.7775412159926, 820.1909865926608, 802.2512634237765, 817.4903800809818, 776.792199465554, 517.1727447885365, 749.0608164668106, 735.5483189273644, 723.506795871274, 725.1223871291108, 723.3506481000514, 716.4894829456184, 731.8030332056389, 730.5822247845522, 783.4189098814978, 725.3291605122433, 512.9712537230951, 740.4486181130029, 794.1272903885877, 812.4177129410688, 799.5768420821728, 799.6728459849979, 778.4967842844966, 775.8636679855215, 790.5124006608711, 797.7314590721641, 751.1705420757326, 758.8658120343492, 746.5051018536228, 733.0368363278176, 430.9404281392445, 791.6423336602917, 744.9433506022714, 745.8503481124546, 751.5543580880186, 723.3478527370606, 757.4475944191444, 789.0195929986198, 797.5386257920001, 796.0363065318581, 539.8913272292273, 530.5643435075158, 772.1929701895007, 773.0953372876875, 796.5454620101293, 535.1164177122777, 530.5343820726487, 691.0905275949159, 783.2117239294538, 798.2885242400678, 796.6864716571412, 795.4420358178667, 691.0553854485165, 614.4958511612205, 750.1687917127042, 747.5348212899538, 791.0503201312097, 795.7782878552297, 785.9828616767317, 515.2008268672885, 781.5980785057982, 759.0357609402644, 790.3571135731015, 683.8196776976345, 537.285951861539, 783.7504255173174, 780.8339658093632, 449.8020197914542, 783.6382265885019, 773.6906467614589, 780.3709625277097, 795.6280031947597, 782.5366920019737, 788.3783493354252, 579.1371474345829, 606.5309757587279, 789.3133153710446, 793.0598388362114, 783.2150110891879, 762.8594920499696, 535.1015303466896, 765.7766966567001, 788.508254048078, 784.7400312895613, 793.7161305115199, 793.4838075696969, 763.3811974630379, 639.7192355314071, 794.3934002110169, 753.7402385814606, 752.0736280903433, 792.1763036842991, 536.8583785417497, 535.3562061718669, 791.708958806047, 793.5148264874683, 786.6709261475753, 791.3847428507066, 787.5638278807772, 766.9804157111605, 681.046788006936, 799.4912509554073, 634.5941663941722, 535.211642331882, 530.5380697873284, 763.8984529302995, 780.2887116327121, 797.5263627059539, 802.1839904096314, 540.3910807653735, 536.0653973822806, 745.524516783728, 749.7842097346131, 701.9172485464404, 517.2990932888017, 512.9420101622903, 749.8962262892974, 780.5047715716155, 786.5320927376317, 727.9922647110637, 721.651252107377, 701.0113963630844, 782.8490614399734, 802.0911471641972, 756.9165044415847, 745.2510097215554, 481.9176789024965, 696.4557447716907, 808.2177374521547, 812.9856424942373, 802.8015513138241, 742.1570866294869, 768.8460062957971, 815.5118492284231, 810.6088823509472, 723.5448805036789, 742.8494996570763, 751.2600556894647, 762.30935899715, 760.1103919147251, 568.2205184122464, 785.7526595254271, 763.625127951822, 776.2490845341991, 517.2394712861291, 748.902648552491, 751.5766941541002, 755.4899833166693, 771.3840996794647, 539.7227105730374, 782.5470792632098, 748.172175865891, 760.340140531931, 788.350338702745, 774.2902314274843, 747.8908262300408, 716.2890228764409, 720.1170159114026, 761.0331223097661, 791.1264848113447, 786.4906804712032, 791.0890738426501, 538.1162145995191, 680.4186875780364, 783.8595181303374, 748.9718878999344, 743.8281207926673, 743.1331175621884, 739.6835002885175, 718.7387541918769, 818.2091320901097, 577.3336952828141, 582.7945716126798, 582.670896316971, 746.1037670715355, 746.008702129674, 745.2660710255036, 835.2207624773754, 790.4008096839214, 817.8577813811241, 830.3313662368266, 611.8395465059184, 618.0124400951897, 805.4757452331497, 812.5234600465948, 819.3761413807573, 833.3697395531622, 839.8084151811736, 608.9589915622571, 833.0243708866087, 848.9647080818121, 819.5090166690604, 853.4965731982063, 856.4804425362953, 856.6969718640299, 837.7461499601804, 841.592927111988, 845.5166556099327, 852.1866423618837, 793.9135759947635, 784.1561015873235, 778.6693833883849, 783.6948327017896, 779.944321726384, 679.2679431131402, 854.128776759424, 849.771109771727, 562.6549647844046, 852.5968764459342, 857.324280974603, 614.5840186455855, 816.6071353680227, 749.2728642885014, 635.2965533272468, 599.8186493683656, 799.8910202576344, 839.8414150650991, 821.9227819614073, 837.644946907387, 785.8892357281491, 774.173475738007, 783.1473446622693, 816.9505128488395, 771.9949610880599, 768.275692330974, 736.172374317107, 574.0098396744147, 753.6048588385269, 582.3314404924628, 577.1191387405456, 771.6746535652935, 793.6418277004244, 792.8956646241794, 797.5356996313775, 785.9308886347737, 658.0518528052671, 765.4790890283911, 825.5290847241173, 832.9868950902534, 768.0175878260909, 650.4441393116427, 599.7343575126374, 767.9173492935047, 822.4093760983621, 633.3021456546285, 617.9956256714393, 624.4225736889756, 760.1149541150451, 826.0811844137356, 776.1635231209299, 844.3411779716535, 792.2143200221091, 786.5655384213594, 793.0601187453273, 839.6664190322228, 829.50646786475, 839.8411071465683, 815.2520674425492, 624.0680789899344, 617.3490911586107, 820.4522751458396, 831.7715895217708, 829.462797666004, 830.9929303799248, 817.2043327803214, 829.7414970579157, 841.5909855709554, 849.6556664259342, 792.8340242028215, 789.8308825505394, 783.7830006104061, 796.5543549866848, 778.4653473189044, 795.0908313556523, 798.8379497175604, 803.1569490198381, 783.3747990420867, 624.2669435013073, 853.068109426196, 832.5658432909515, 836.829602232344, 811.2705190840668, 812.2527779501052, 829.9828883388325, 788.7743655768348, 664.8436689500194, 836.3190134568521, 840.5934555597879, 823.7118694592778, 730.4320788802925, 761.0693214588117, 756.4552221971421, 819.5306815260604, 818.3827070737373, 822.0565644597963, 784.0136476001584, 823.771545468967, 800.4138694889158, 512.4659115433532, 797.3008276694175, 786.2388879312218, 791.7903953785958, 676.123896923522, 517.6540892899257, 835.7483959991579, 829.5031511061808, 843.9189840387004, 836.2807317889984, 829.5795196262209, 847.8674314383436, 832.3415939439578, 821.4802822923965, 825.1060061467293, 754.7098574453064, 795.1320186475846, 786.3505614342026, 769.5314731775583, 784.1305413851578, 763.8134916140258, 816.834561162238, 809.8542767057878, 791.0744746803014, 827.8092908140125, 811.3780094824859, 834.3268074521012, 804.985613463904, 813.1007037030784, 792.7725008267315, 811.1401688918353, 820.558982880619, 832.0081925826325, 841.254040406699, 717.9462541362604, 617.3159450432811, 852.609518784057, 843.4230551711529, 845.9260495929, 832.6523259305673, 731.5838633493138, 815.0600950507027, 818.80563671546, 797.8523158661229, 624.4509676017296, 782.7489467976912, 806.9638662649537, 831.1971134041355, 807.4573428916864, 804.9422204718683, 837.2701604450656, 690.0373771979135, 816.1015246577743, 830.5221126443834, 835.6601461533542, 814.1157242386485, 812.5443283884598, 798.0940312093958, 617.9139807307133, 824.9842888307114, 800.5757891164795, 594.147743779551, 624.177037538624, 775.990237004137, 821.3742895760882, 765.1546512667135, 773.3332698562027, 678.2863226738663, 783.3778955880808, 790.5331091378782, 795.3837717479695, 788.0544195570404, 776.2728744076309, 814.5679889314042, 818.8173431331871, 793.1383916031892, 805.4521666407777, 799.9338808172788, 791.0628537275327, 783.4850696937266, 779.8604978278303, 850.425399383002, 825.0471148141132, 834.7063705127696, 770.6592907513983, 838.2611572691426, 848.2590484359481, 828.8689727290986, 846.2777524990945, 700.9490132007086, 820.3949472890815, 837.137983926877, 825.4088709695206, 829.5900889824387, 793.0796468980446, 543.6616107914297, 822.8818288704668, 753.7209047688788, 826.4401880924001, 812.2445766412173, 485.4813743947427, 803.376173580985, 662.0091912155292, 786.9441100095779, 782.2300168379224, 780.9683478190053, 781.3397271786944, 794.2334781969057, 786.3539042559801, 768.11240386768, 808.0947609212293, 799.2310754201217, 806.166903933607, 806.5605791524639, 793.8784577526808, 790.0763497537065, 770.8443540482715, 801.4922475904508, 751.7677286082957, 757.0281219319901, 755.0909162743266, 770.8612566785658, 753.3420343721882, 768.1082044681103, 771.176771897963, 766.5348004945671, 764.3582425037281, 766.537215976146, 766.9892847184359, 572.7310231174007, 748.0147997683727, 764.5462734231571, 739.7204695244047, 749.4412068520758, 593.3521244871993, 680.2889131005521, 760.9022234401881, 770.2315051672671, 588.112244528441, 588.0698507747378, 762.5547949650688, 753.6502502938204, 771.0815363738751, 582.5278861552811, 583.1165941534688, 773.1204377893066, 769.0238444235906, 774.6605110879393, 764.6452593943184, 767.351289322733, 758.2122317376434, 653.529178153685, 764.9576854414917, 778.8803526224034, 771.6617033419524, 572.00296725053, 756.3065755795471, 774.4139681813534, 752.5570981694227, 768.5260351150573, 769.7739454671944, 762.6163964872392, 770.4698221584275, 753.2349380843456, 734.4840844466466, 751.5204757827935, 754.6699344972917, 767.0712430697207, 753.0189678671089, 756.7015119033629, 749.2164248553923, 754.2891616630646, 770.7319830507882, 753.7058376602686, 765.8620118871532, 740.3294163051706, 656.5334693007431, 762.2944172982915, 755.5896424078268, 763.414641506421, 761.2969064188524, 736.4620919427437, 705.191117750594, 792.4596330852825, 845.8503054224296, 832.9826227783363, 537.991250408343, 746.1058762781871, 700.6200808776817, 516.8820143960108, 818.108180737545, 826.6670535302255, 756.065957047007, 751.8646968912541, 762.9904514951129, 753.0533151985676, 725.2289165159876, 746.5730012002268, 762.1098239116428, 748.4516596590618, 707.193111861943, 560.6725496795576, 756.2750103423211, 755.4763414892409, 756.8258077649269, 757.9535274412556, 785.6700414626459, 773.9555022935265, 798.2839197080918, 804.5227192669312, 801.3562864379483, 803.493261914648, 782.2965704896556, 719.7723598467046, 819.199207764438, 819.2476553599869, 820.7957267988451, 686.5578827982196, 828.592322725301, 835.9398105311632, 840.644168573321, 842.1928243126755, 613.7940062668314, 842.7746650397816, 845.5368863023026, 830.1200335257134, 830.5237951720986, 813.889191562652, 590.9976537420894, 845.9401831689561, 830.6814522885886, 630.7637480429306, 617.9303274406558, 834.6539560808169, 821.1753143075958, 851.2743336525322, 843.1949049265429, 812.8718400405318, 479.3576875882514, 831.700460072313, 846.5643743371277, 632.6882339171353, 617.9649906610824, 837.9515440678736, 847.1945672248445, 826.1694649438085, 797.6542835080843, 791.3117822778179, 795.3218969062391, 789.1430049532694, 805.6217897793093, 585.6075414827106, 828.949871741211, 849.5385635611781, 818.1188165314047, 849.0691948429854, 756.8635635887938, 768.2130024939374, 780.6291184032051, 760.8789201771494, 765.6062726737697, 782.9368833365572, 680.4919674141781, 814.0939601629914, 836.7721892407836, 834.7970759570251, 845.1145600555221, 829.4104490490063, 767.126822988543, 782.5759942906137, 790.4314625449792, 599.6306411256637, 761.5542525380423, 790.6198776227714, 789.4011501476816, 784.3051328570882, 605.8409403720979, 846.4594383244034, 830.9258021878795, 809.0040485960554, 614.5154772511632, 624.2060569115554, 816.5951111948812, 844.235441446612, 847.1624895499817, 855.1106409432342, 825.6970229073102, 709.9447076863314, 830.8660947682239, 820.2209840219941, 840.6773730116582, 829.5669101093987, 701.2465428104255, 829.8056647051038, 844.9450671755656, 822.4709811601322, 825.4796310839533, 844.5744876525054, 848.1687937622795, 613.6999650803668, 828.3865600508049, 838.8811166520439, 832.3163305766475, 816.4139981607765, 826.7256057136402, 828.1294673146789, 816.915705070958, 820.4675735600688, 820.8112598413016, 826.6261813146973, 842.9451649001112, 637.2133130693877, 771.2813582907997, 770.0233696896915, 772.5483579600339, 769.6283172694598, 771.8329552355898, 650.6319140610099, 724.011299424433, 822.5371378255793, 832.5274190524433, 647.1632820602736, 823.5078077810767, 823.3107728020276, 844.0460356247897, 839.2845493082687, 837.2548474445128, 709.0451667570634, 825.9265899751772, 816.216971412062, 821.8904932342249, 835.3535629165156, 704.644746649484, 689.466315833796, 814.3920795524554, 812.1040964422556, 826.3820564429045, 835.1299383014349, 837.8745637482092, 819.0241234609368]
Elapsed: 0.17522008224823787~0.026010546627241976
Time per graph: 0.0013623720242960545~0.00020092711234643746
Speed: 746.8253930859155~87.72084448712782
Total Time: 0.1617
best val loss: 0.1112159386046173 test_score: 0.9375

Testing...
Test loss: 0.2100 score: 0.9453 time: 0.15s
test Score 0.9453
Epoch Time List: [0.7755973841995001, 0.5715205722954124, 0.5584769591223449, 0.5614219650160521, 0.5783656067214906, 0.6155834691599011, 0.5564073571003973, 0.558660996844992, 0.5558570229914039, 0.5620986700523645, 0.5813217859249562, 0.6309207328595221, 0.5656078900210559, 0.5596460532397032, 0.5592070163693279, 0.553504184121266, 0.6378331931773573, 0.5933540957048535, 0.5677692759782076, 0.5624528611078858, 0.5672363820485771, 0.595197785878554, 0.6632938722614199, 0.571598530979827, 0.5581784371752292, 0.5661037089303136, 0.5586181550752372, 0.667509201914072, 0.5720257801003754, 0.5617433458101004, 0.5829179862048477, 0.5912495763041079, 0.5941719261463732, 0.6663042260333896, 0.5905647119507194, 0.5653112919535488, 0.5735324416309595, 0.8110638167709112, 0.8411251280922443, 0.7892517161089927, 0.5707778409123421, 0.5674885248299688, 0.5651758711319417, 0.5666664321906865, 0.5594309358857572, 0.586941858753562, 0.6538559079635888, 0.5873816162347794, 0.5872467909939587, 0.5881843990646303, 0.587586629902944, 0.6043414387386292, 0.6609265480656177, 0.5823413180187345, 0.5880501219071448, 0.7219613410998136, 0.8576215959619731, 0.8513411749154329, 0.7930991072207689, 0.5636339830234647, 0.5577402701601386, 0.5556698201689869, 0.5643293308094144, 0.5549125962425023, 0.6875220818910748, 0.5747040819842368, 0.5642254890408367, 0.5624866457656026, 0.5662977257743478, 0.5713689778931439, 0.7663882789202034, 0.5682006110437214, 0.5489976811222732, 0.7894165432080626, 0.8271518624387681, 0.8162630647420883, 0.8217920821625739, 0.6007493429351598, 0.5539326486177742, 0.5496085786726326, 0.5541591949295253, 0.6128333150409162, 0.7071904812473804, 0.5872345298994333, 0.6672723889350891, 0.5788314808160067, 0.5973954449873418, 0.7422948160674423, 0.5936202490702271, 0.6802488702815026, 0.5414833128452301, 0.5768198070582002, 0.5880306896287948, 0.6682796482928097, 0.5926394197158515, 0.5997820110060275, 0.5946805828716606, 0.5952887330204248, 0.596308472333476, 0.5951835452578962, 0.7144159891176969, 0.6042460829485208, 0.5993372478988022, 0.5902742699254304, 0.6535578959155828, 0.649566964013502, 0.5966727801132947, 0.5932977860793471, 0.6119402749463916, 0.594020240008831, 0.6008137008175254, 0.6316064142156392, 0.5588254269678146, 0.5745081282220781, 0.5576676910277456, 0.561995540978387, 0.5716136188711971, 0.8355553091969341, 0.5896823131479323, 0.5644917520694435, 0.5620899929199368, 0.5659940899349749, 0.5640874139498919, 0.6980796919669956, 0.5691085828002542, 0.5598380661103874, 0.7033039007801563, 0.8393668220378458, 0.8392870649695396, 0.8371419999748468, 0.654231364140287, 0.5712516061030328, 0.5583341571036726, 0.7479637020733207, 0.5639892760664225, 0.5544634107500315, 0.5622757899109274, 0.5694899968802929, 0.7251254958100617, 0.5894532999955118, 0.5699005760252476, 0.5748510719276965, 0.5712421417701989, 0.5990380600560457, 0.6406115940771997, 0.6418615910224617, 0.5635983543470502, 0.5664167227223516, 0.5619438430294394, 0.5738787539303303, 0.7693893848918378, 0.572714862646535, 0.5676379939541221, 0.5686286299023777, 0.5708915460854769, 0.5714853131212294, 0.5997448430862278, 0.6641385932452977, 0.5700356201268733, 0.576115861069411, 0.5539940849412233, 0.563822008902207, 0.5569580621086061, 0.6154325890820473, 0.6597543458919972, 0.5565078051295131, 0.5818952068220824, 0.5893414330203086, 0.5802547840867192, 0.6972065882291645, 0.7204036589246243, 0.5831926588434726, 0.5856506058480591, 0.5845838291570544, 0.5856490680016577, 0.5876872588414699, 0.5896792982239276, 0.6018440949264914, 0.7107192501425743, 0.5533956121653318, 0.6105515700764954, 0.5661211379338056, 0.5746888322755694, 0.6516737181227654, 0.5350677068345249, 0.5729016836266965, 0.633563200943172, 0.6761251848656684, 0.5823800021316856, 0.5735965080093592, 0.5850018700584769, 0.6833115571644157, 0.8164044588338584, 0.8410908412188292, 0.5571200530976057, 0.5774670359678566, 0.5934019370470196, 0.5926426809746772, 0.6009307280182838, 0.7275966410525143, 0.5961167218629271, 0.5999345178715885, 0.5987051150295883, 0.5940521031152457, 0.6862913260702044, 0.8673943402245641, 0.6866098188329488, 0.5946250602137297, 0.5926475890446454, 0.597979661077261, 0.6654703852254897, 0.709268522914499, 0.5944798600394279, 0.5871334332041442, 0.7232648301869631, 0.6570475092157722, 0.5920855009462684, 0.586467772256583, 0.5895334919914603, 0.5872805928811431, 0.6241951850242913, 0.6711252662353218, 0.5868393362034112, 0.5837719170376658, 0.5918925758451223, 0.5880117591004819, 0.6166505941655487, 0.6666742768138647, 0.551067034015432, 0.5614066130947322, 0.5518877129070461, 0.555944656720385, 0.5552036657463759, 0.5927939550019801, 0.6341691568959504, 0.5511047439649701, 0.5551754080224782, 0.5632520359940827, 0.5626041828654706, 0.6446642859373242, 0.5483496058732271, 0.5770511988084763, 0.5779496331233531, 0.583480381872505, 0.579168704804033, 0.5775639000348747, 0.603153018746525, 0.6654929623473436, 0.5804749059025198, 0.5839402300771326, 0.5795208960771561, 0.5813210278283805, 0.5744128827936947, 0.6213273622561246, 0.6632254151627421, 0.5800286058802158, 0.7438703197985888, 0.8522536458913237, 0.8548027237411588, 0.6239087760914117, 0.5802438440732658, 0.6021364100743085, 0.6824663046281785, 0.5491016211453825, 0.5506485940422863, 0.5462566851638258, 0.5489165098406374, 0.5572183760814369, 0.6597433860879391, 0.634148753946647, 0.5548715598415583, 0.5484204050153494, 0.553975738119334, 0.5541457578074187, 0.7178778948727995, 0.5495587068144232, 0.6094880518503487, 0.5495890851598233, 0.5520400628447533, 0.6568744550459087, 0.5605107438750565, 0.6006065888796002, 0.7024782260414213, 0.5753568848595023, 0.566573899006471, 0.6649582078680396, 0.6833437101449817, 0.5585889599751681, 0.559573887148872, 0.5579374879598618, 0.5549067370593548, 0.5487464170437306, 0.5944335791282356, 0.6321499519981444, 0.5641306980978698, 0.7098724360112101, 0.8355929509270936, 0.8420459337066859, 0.6387709309346974, 0.5832411600276828, 0.5973200728185475, 0.587966870283708, 0.5877405612263829, 0.6645818760152906, 0.6100514121353626, 0.5893156619276851, 0.5879765069112182, 0.5848185468930751, 0.5887142061255872, 0.6027590003795922, 0.663023852976039, 0.5936956650111824, 0.6024591750465333, 0.5921089870389551, 0.5984022149350494, 0.5838193341623992, 0.5885331970639527, 0.6042604420799762, 0.689556498080492, 0.5951801748014987, 0.5987841070163995, 0.855895146727562, 0.865328473970294, 0.7064790651202202, 0.5668331931810826, 0.5677142380736768, 0.5546520401258022, 0.5501997121609747, 0.5800938371103257, 0.7615502302069217, 0.6529943149071187, 0.5624702572822571, 0.5593309276737273, 0.584667838877067, 0.5622086718212813, 0.5730970790609717, 0.659987036138773, 0.5539456349797547, 0.5350425380747765, 0.5368907537776977, 0.543938698945567, 0.5438304811250418, 0.5910443470347673, 0.6375722431112081, 0.5695784089621156, 0.5763746618758887, 0.5759748083073646, 0.5792086939327419, 0.5801315479911864, 0.6026290818117559, 0.6389543011318892, 0.5573943071067333, 0.5610578421037644, 0.8348937181290239, 0.8352610347792506, 0.854220069013536, 0.594773449935019, 0.5890105951111764, 0.5883497807662934, 0.5895731849595904, 0.5926206370349973, 0.6936846112366766, 0.7157735589426011, 0.5882899791467935, 0.5858090010005981, 0.5796935758553445, 0.5894813989289105, 0.6476842078845948, 0.549240366788581, 0.562006413936615, 0.6054464352782816, 0.5514756517950445, 0.5687490170821548, 0.7540259868837893, 0.5933778521139175, 0.6486072891857475, 0.5828982619568706, 0.5838135606609285, 0.6187370188999921, 0.64650285593234, 0.5654985290020704, 0.5546099930070341, 0.5812732821796089, 0.5852311819326133, 0.5835455229971558, 0.5953825989272445, 0.6634353529661894, 0.589076278032735, 0.5880122929811478, 0.722072197124362, 0.8742205209564418, 0.6087442436255515, 0.5885830188635737, 0.6224387402180582, 0.5717673238832504, 0.5894239880144596, 0.6038581726606935, 0.764710531802848, 0.8492345891427249, 0.6022522037383169, 0.5695902900770307, 0.5759673591237515, 0.643924793926999, 0.5735242618247867, 0.5800035470165312, 0.5696773829404265, 0.5740904789417982, 0.6181087750010192, 0.6562604929786175, 0.6194805030245334, 0.6081947919446975, 0.617603697348386, 0.617778732907027, 0.7061312929727137, 0.6135461058001965, 0.6018785708583891, 0.6102982671000063, 0.6047850460745394, 0.6055747889913619, 0.69675378408283, 0.5753423997666687, 0.5769154289737344, 0.5698119201697409, 0.57711176504381, 0.5685172050725669, 0.7005790330003947, 0.5797528263647109, 0.5708152442239225, 0.7221784221474081, 0.7917522599454969, 0.5800989423878491, 0.571171463932842, 0.5748765671160072, 0.5757989347912371, 0.5698474789969623, 0.5687265461310744, 0.6447796118445694, 0.6201560692861676, 0.5649256187025458, 0.5703770068939775, 0.5689509329386055, 0.5924774201121181, 0.687047426123172, 0.5915595430415124, 0.5720815958920866, 0.604394826805219, 0.8376313836779445, 0.8372098249383271, 0.7072727840859443, 0.5784047800116241, 0.5647257391829044, 0.5647053960710764, 0.5814318882767111, 0.6219406318850815, 0.5716607577633113, 0.5631513244006783, 0.5597006829921156, 0.5551632498390973, 0.6846746408846229, 0.6828498500399292, 0.6039500732440501, 0.670652219094336, 0.5987117739859968, 0.6469783848151565, 0.5544641979504377, 0.5603222479112446, 0.7034272830933332, 0.6311091519892216, 0.6005560029298067, 0.59273417503573, 0.6002940607722849, 0.6888196538202465, 0.5931902569718659, 0.5941222209949046, 0.5653513600118458, 0.5639825640246272, 0.5627433559857309, 0.5662206741981208, 0.568445652956143, 0.5970623029861599, 0.6328371369745582, 0.5711595909669995, 0.5643664796371013, 0.566700056893751, 0.5715763131156564, 0.6570497867651284, 0.8620857431087643, 0.5823266340885311, 0.6040601599961519, 0.6067164861597121, 0.6064294807147235, 0.5567307390738279, 0.5932530469726771, 0.6567656788975, 0.5903419849928468, 0.55848085321486, 0.5781422641593963, 0.5809338770341128, 0.5874077209737152, 0.604090761160478, 0.6970362800639123, 0.5960104530677199, 0.5936031760647893, 0.5735269607976079, 0.5537762669846416, 0.5754205968696624, 0.6780123903881758, 0.5558282609563321, 0.5584126459434628, 0.5553425599355251, 0.5499505810439587, 0.6612085457891226, 0.8155213510617614, 0.5715447687543929, 0.553194998530671, 0.5865303210448474, 0.5905689198989421, 0.6127524629700929, 0.676590973045677, 0.5742868101224303, 0.8113243498373777, 0.8371592359617352, 0.8330359500832856, 0.6894333553500473, 0.5588356540538371, 0.5674077321309596, 0.5680984028149396, 0.5637245341204107, 0.5702239358797669, 0.823830904206261, 0.561158302007243, 0.5488566972780973, 0.5542835278902203, 0.5537224679719657, 0.7447285000234842, 0.7112348179798573, 0.5947524271905422, 0.6001401597168297, 0.5993946348316967, 0.5956092348787934, 0.6503791580908, 0.6709188299719244, 0.5942415168974549, 0.567231927998364, 0.591286129783839, 0.7750273840501904, 0.7573144102934748, 0.5703380273189396, 0.5587789348792285, 0.5638569639995694, 0.5575008799787611, 0.5654659660067409, 0.5772581428755075, 0.6516246411483735, 0.6621223478578031, 0.5859562018886209, 0.5894331384915859, 0.6509479572996497, 0.5972565428819507, 0.7526764890644699, 0.5681344817858189, 0.583738970104605, 0.6847605637740344, 0.5907834579702467, 0.6126034241169691, 0.7921165339648724, 0.5720027778297663, 0.5632866751402617, 0.5654812522698194, 0.7367232965771109, 0.8352796752005816, 0.660122700035572, 0.5695178310852498, 0.5623886780813336, 0.7532197199761868, 0.8388396892696619, 0.8010090549942106, 0.5712054821196944, 0.5730508151464164, 0.5648004438262433, 0.5704481690190732, 0.6048995170276612, 0.6933412540238351, 0.6019199099391699, 0.5950773626100272, 0.5695176171138883, 0.5642574648372829, 0.564389152219519, 0.6624179151840508, 0.5782724409364164, 0.5752488952130079, 0.5676177300047129, 0.5819006068632007, 0.6460063278209418, 0.7168950450140983, 0.5647824581246823, 0.68767374381423, 0.5637547969818115, 0.5720744079444557, 0.5944544582162052, 0.6382358088158071, 0.5839555750135332, 0.5728332810103893, 0.6532739568501711, 0.7572318648453802, 0.5796191208064556, 0.573009432060644, 0.5691361019853503, 0.5737672948744148, 0.7722872772719711, 0.7159761202055961, 0.5660714649129659, 0.5651243589818478, 0.5652552898973227, 0.5733477238100022, 0.5731699478346854, 0.6443357071839273, 0.5767643698491156, 0.5820745250675827, 0.5986795099452138, 0.5664072108920664, 0.6571303750388324, 0.8331326369661838, 0.6831721207126975, 0.575017100200057, 0.5689384841825813, 0.5760743520222604, 0.5702139919158071, 0.592586733167991, 0.6636615407187492, 0.563293692888692, 0.6171983520034701, 0.8240932270418853, 0.8455556011758745, 0.7244387399405241, 0.5687908069230616, 0.5646315270569175, 0.5640429998748004, 0.6790482650976628, 0.8559441608376801, 0.6864445370156318, 0.6011871681548655, 0.6067788801155984, 0.805706633720547, 0.8627182869240642, 0.6808801351580769, 0.5685269772075117, 0.6389580729883164, 0.5884535012301058, 0.607280989876017, 0.6812337611336261, 0.6332013921346515, 0.5649351580068469, 0.6629346332047135, 0.5917080088984221, 0.7016130636911839, 0.7139254359062761, 0.5701944457832724, 0.569236233830452, 0.5616518391761929, 0.5940326368436217, 0.6129236042033881, 0.6310063188429922, 0.5563805301208049, 0.7425668728537858, 0.5975812708493322, 0.6047195710707456, 0.5850381623022258, 0.5902167554013431, 0.6816354498732835, 0.6082833730615675, 0.5805453290231526, 0.5787775521166623, 0.76469292701222, 0.7017018457408994, 0.599256634246558, 0.5933833990711719, 0.5886380327865481, 0.7198239408899099, 0.7538878107443452, 0.5921853638719767, 0.5957060595974326, 0.5595716841053218, 0.5597519727889448, 0.5832588721532375, 0.6087010910268873, 0.6916834290605038, 0.6051042967010289, 0.5658676533494145, 0.5754080368205905, 0.5662499971222132, 0.6695401389151812, 0.7640957310795784, 0.577547102002427, 0.5941075070295483, 0.5943375148344785, 0.5993226550053805, 0.5991288092918694, 0.6537303228396922, 0.6676814160309732, 0.7615259601734579, 0.9454255502205342, 0.9535431959666312, 0.8282643959391862, 0.6211785220075399, 0.6289430321194232, 0.5750516271218657, 0.6126222531311214, 0.6727421812247485, 0.5699630330782384, 0.8577312750276178, 0.9338646507821977, 0.7508479570969939, 0.6008707950823009, 0.5758527223952115, 0.5782509739510715, 0.571083745919168, 0.6471781339496374, 0.5969142739195377, 0.5592753721866757, 0.5689214027952403, 0.5705742358695716, 0.5580567501019686, 0.5606531109660864, 0.5672442489303648, 0.68149000313133, 0.56059634918347, 0.5663957779761404, 0.5945378949400038, 0.6080193209927529, 0.5998704181984067, 0.6086502098478377, 0.5800024659838527, 0.6454138348344713, 0.6454389989376068, 0.5597745671402663, 0.6373549120035022, 0.5602006067056209, 0.5613677711226046, 0.7006246082019061, 0.587237318046391, 0.6067258520051837, 0.6944259458687156, 0.897009263979271, 0.7830093572847545, 0.5837013209238648, 0.5842502941377461, 0.579844071995467, 0.5930107268504798, 0.5998627650551498, 0.6172913538757712, 0.6901784332003444, 0.6026782609988004, 0.6130605998914689, 0.6441599188838154, 0.9458472540136427, 0.6808593377936631, 0.710287936963141, 0.9535746921319515, 0.8074879536870867, 0.6085541683714837, 0.6040224598255008, 0.6084967127535492, 0.6033248472958803, 0.6614542522002012, 0.6513692250009626, 0.5796929369680583, 0.5779422561172396, 0.6125754823442549, 0.6636488309595734, 0.9368133191019297, 0.8084069082979113, 0.5775793888606131, 0.6299840172287077, 0.9117959649302065, 0.9095330049749464, 0.8951548931654543, 0.5676411758176982, 0.6137010820675641, 0.564578470075503, 0.5834459569305182, 0.5917031769640744, 0.5896038850769401, 0.5922981738112867, 0.6421705591492355, 0.5605986618902534, 0.5699457339942455, 0.8401256056968123, 0.9031213407870382, 0.6203168870415539, 0.588824252365157, 0.5704283241648227, 0.5727149769663811, 0.5703386280220002, 0.5836786767467856, 0.7070206399075687, 0.5645523129496723, 0.5925972040276974, 0.6014420632272959, 0.6125866158399731, 0.6009596346411854, 0.6621828521601856, 0.707864064257592, 0.5975460347253829, 0.5951360620092601, 0.5742874990683049, 0.8454397399909794, 0.724720117636025, 0.5742657042574137, 0.5665002090390772, 0.5873893480747938, 0.5741128167137504, 0.5713123648893088, 0.5820174822583795, 0.6792138712480664, 0.5706465549301356, 0.5687243030406535, 0.5672582830302417, 0.5980817419476807, 0.7290216970723122, 0.6193150181788951, 0.5817684610374272, 0.5803541785571724, 0.5856154789216816, 0.6006637392565608, 0.6523397306445986, 0.7170150210149586, 0.6719477828592062, 0.5853403222281486, 0.6023169809486717, 0.6165020212065428, 0.6884577139280736, 0.7503797339741141, 0.5665137930773199, 0.5704195662401617, 0.5749745538923889, 0.5626017339527607, 0.5785410180687904, 0.5635202170815319, 0.6135695578996092, 0.6507399140391499, 0.5589940929785371, 0.5944939099717885, 0.5903755312319845, 0.618294094922021, 0.6864294798579067, 0.6071684639900923, 0.6103564619552344, 0.6018577369395643, 0.5781006582546979, 0.6199721009470522, 0.7199763061944395, 0.5828315641265363, 0.570233539910987, 0.5760209131985903, 0.5780736082233489, 0.7227073241956532, 0.5753768088761717, 0.5790026800241321, 0.5758834278676659, 0.5721709502395242, 0.6312303729355335, 0.9036496679764241, 0.6251640173140913, 0.5634817231912166, 0.5611923097167164, 0.5732140026520938, 0.5963980879168957, 0.7495492971502244, 0.5818238870706409, 0.5777459570672363, 0.8497966874856502, 0.8480585729703307, 0.579091346822679, 0.5725299809128046, 0.5795747213996947, 0.5856637742836028, 0.5707582288887352, 0.6251315050758421, 0.6305171430576593, 0.5927538308314979, 0.571144143352285, 0.5845214598812163, 0.5800757990218699, 0.5837483590003103, 0.6804721052758396, 0.5700125908479095, 0.5801945091225207, 0.7134397970512509, 0.9075963851064444, 0.8712736249435693, 0.5752813029102981, 0.6049825169611722, 0.5999820209108293, 0.6290902888868004, 0.7405991230625659, 0.6010437679942697, 0.6069647097028792, 0.6017463568132371, 0.6004957319237292, 0.6030892620328814, 0.6714116488583386, 0.5903854190837592, 0.593035647412762, 0.5982873311731964, 0.6102364910766482, 0.7236678709741682, 0.6029575376305729, 0.5628163062501699, 0.5743987332098186, 0.5735034982208163, 0.8353655310347676, 0.5714127658866346, 0.5694279388990253, 0.5656583760865033, 0.5633199578151107, 0.6503499478567392, 0.608652925118804, 0.5680704219266772, 0.5739158478099853, 0.6293463059701025, 0.602867143927142, 0.7509794600773603, 0.5672059499192983, 0.5977010771166533, 0.655742006842047, 0.5786786591634154, 0.7086922340095043, 0.5744849748443812, 0.6785651498939842, 0.6404152170289308, 0.5865101381205022, 0.5962891469243914, 0.6027108458802104, 0.591273431899026, 0.6012741609010845, 0.6213006691541523, 0.7370026821736246, 0.5952822030521929, 0.5895057760644704, 0.5952944369055331, 0.5905694128014147, 0.599416728829965, 0.6437557509634644, 0.7188538932241499, 0.617281693033874, 0.6064810452517122, 0.6185439403634518, 0.6112012732774019, 0.6162706993054599, 0.6682200678624213, 0.6826701229438186, 0.6194458911195397, 0.6158637520857155, 0.6172370729036629, 0.6260971971787512, 0.7012094878591597, 0.6286852550692856, 0.618391863303259, 0.62337845005095, 0.6271232848521322, 0.6908825237769634, 0.9375559079926461, 0.6175034083425999, 0.615168800810352, 0.8864754759706557, 0.9535692138597369, 0.698738202219829, 0.624126244103536, 0.6134148680139333, 0.8536382939200848, 0.9425685810856521, 0.6616678501013666, 0.6136890831403434, 0.618118496844545, 0.6137176870834082, 0.6168892746791244, 0.6149609561543912, 0.6912613108288497, 0.6517850968521088, 0.6153937510680407, 0.617193843703717, 0.6968407470267266, 0.622494394890964, 0.6190498061478138, 0.6925464030355215, 0.6192767878528684, 0.6148875846993178, 0.6141796701122075, 0.615537840873003, 0.6181926112622023, 0.6234327477868646, 0.7086397744715214, 0.6203407649882138, 0.6193038867786527, 0.6223632839974016, 0.6188218691386282, 0.7134737651795149, 0.6184943888802081, 0.6131138319615275, 0.6162886931560934, 0.6143800062127411, 0.6229771981015801, 0.6651596389710903, 0.6530742128379643, 0.6174240917898715, 0.6181668362114578, 0.6122186733409762, 0.6197283880319446, 0.6990753849968314, 0.603303432231769, 0.6251669169869274, 0.579762201057747, 0.6978593899402767, 0.6078629214316607, 0.5977317129727453, 0.7421211588662118, 0.646760334726423, 0.5820186368655413, 0.667202371172607, 0.6014230258297175, 0.6150119889061898, 0.6075573009438813, 0.6510375479701906, 0.7634148858487606, 0.6139196120202541, 0.6175070118624717, 0.6284620189107955, 0.953979903133586, 0.6209585191681981, 0.6251492609735578, 0.6083414151798934, 0.6218790379352868, 0.5930747489910573, 0.6199645861051977, 0.719675388885662, 0.5786821059882641, 0.5899738287553191, 0.5889568272978067, 0.5993054711725563, 0.6527376621961594, 0.7183543448336422, 0.5685674301348627, 0.5775024697650224, 0.6101231900975108, 0.773710710927844, 0.5661825030110776, 0.5672431197017431, 0.5688992231152952, 0.6500220380257815, 0.5818782213609666, 0.5708759240806103, 0.5651596130337566, 0.5672272520605475, 0.6035620151087642, 0.8812598718795925, 0.5746806610841304, 0.574652899056673, 0.8374023060314357, 0.9075425169430673, 0.6615830571390688, 0.5693804470356554, 0.5688668149523437, 0.5631535807624459, 0.5746530138421804, 0.8495887548197061, 0.5766507359221578, 0.5657629871275276, 0.6659164531156421, 0.9016898521222174, 0.6626631489489228, 0.5706741388421506, 0.5677598260808736, 0.655848213005811, 0.6037189909256995, 0.5985180293209851, 0.5989517131820321, 0.591915241908282, 0.6715091760270298, 0.5720070325769484, 0.5654547808226198, 0.5673589515499771, 0.574527557939291, 0.6123573589138687, 0.6754579970147461, 0.6110910146962851, 0.6073825741186738, 0.612974951043725, 0.5980200697667897, 0.6337441098876297, 0.6448675708379596, 0.5688890470191836, 0.5728106959722936, 0.5685691589023918, 0.58053292427212, 0.6912993129808456, 0.6061016758903861, 0.5996748150791973, 0.7331987251527607, 0.8668101623188704, 0.5979198331478983, 0.6053024870343506, 0.595261154929176, 0.7332156298216432, 0.6768183531239629, 0.558970068814233, 0.5825260689016432, 0.6943244461435825, 0.9033410067204386, 0.869849347975105, 0.5701102130115032, 0.6610299427993596, 0.5563003090210259, 0.5653606399428099, 0.6454899788368493, 0.6299904298502952, 0.5674455561675131, 0.5710819754749537, 0.566541540902108, 0.8203504376579076, 0.6467346081044525, 0.5683568087406456, 0.5681054280139506, 0.5658083069138229, 0.5629251408390701, 0.5614897671621293, 0.6554156029596925, 0.5737083747517318, 0.5680895298719406, 0.565618681255728, 0.5729546078946441, 0.5745463941711932, 0.5607454024720937, 0.6084648550022393, 0.6368779060430825, 0.5707047001924366, 0.5637238351628184, 0.5623099459335208, 0.7150775538757443, 0.766515196301043, 0.62070571212098, 0.6006974980700761, 0.6158915129490197, 0.6036227738950402, 0.638708729762584, 0.6612040817271918, 0.5673422059044242, 0.5701600059401244, 0.6464470457285643, 0.5670807000715286, 0.5743588821496814, 0.5627921468112618, 0.5674912431277335, 0.5757052931003273, 0.6430123660247773, 0.6431879193987697, 0.5772036509588361, 0.5818260170053691, 0.5711308349855244, 0.5968715797644109, 0.6120295389555395, 0.6221472630277276, 0.5740033239126205, 0.5751046482473612, 0.567680757958442, 0.57323948899284, 0.5857905619777739]
Total Epoch List: [400, 299, 468]
Total Time List: [0.17514378111809492, 0.18118192511610687, 0.1616733600385487]
T-times Epoch Time: 0.6208252540187288 ~ 0.003445786703980856
T-times Total Epoch: 273.1111111111111 ~ 89.20402471128395
T-times Total Time: 0.17393884890609315 ~ 0.00401890599029302
T-times Inference Elapsed: 0.17550586932043974 ~ 0.0014473964482195761
T-times Time Per Graph: 0.0013630509091721223 ~ 1.001229782795951e-05
T-times Speed: 747.2342497704221 ~ 3.8486094912729722
T-times cross validation test micro f1 score:0.8148730939546261 ~ 0.062254418566849996
T-times cross validation test precision:0.8619270828392223 ~ 0.14656501544740183
T-times cross validation test recall:0.7732371794871796 ~ 0.13389866627863248
T-times cross validation test f1_score:0.8148730939546261 ~ 0.13974116301637865
