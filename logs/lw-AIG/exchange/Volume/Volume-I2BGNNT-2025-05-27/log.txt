Namespace(seed=15, model='I2BGNNT', dataset='exchange/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feaf949a20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.15s
Epoch 2/1000, LR 0.000015
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4961 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4961 time: 0.15s
Epoch 9/1000, LR 0.000225
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.15s
Epoch 10/1000, LR 0.000255
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.15s
Epoch 11/1000, LR 0.000285
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4961 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4961 time: 0.15s
Epoch 13/1000, LR 0.000285
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4961 time: 0.15s
Epoch 14/1000, LR 0.000285
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.24s
Val loss: 0.6919 score: 0.6279 time: 0.16s
Test loss: 0.6918 score: 0.5659 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.24s
Val loss: 0.6913 score: 0.8605 time: 0.16s
Test loss: 0.6911 score: 0.8760 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5039 time: 0.15s
Epoch 17/1000, LR 0.000285
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5039 time: 0.15s
Epoch 18/1000, LR 0.000285
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5039 time: 0.16s
Epoch 19/1000, LR 0.000285
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5039 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.5402;  Loss pred: 0.5402; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5039 time: 0.15s
Epoch 21/1000, LR 0.000285
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5039 time: 0.15s
Epoch 22/1000, LR 0.000285
Train loss: 0.4946;  Loss pred: 0.4946; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.5039 time: 0.15s
Epoch 23/1000, LR 0.000285
Train loss: 0.4613;  Loss pred: 0.4613; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6748 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6720 score: 0.5039 time: 0.15s
Epoch 24/1000, LR 0.000285
Train loss: 0.4439;  Loss pred: 0.4439; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6696 score: 0.4961 time: 0.15s
Test loss: 0.6661 score: 0.5116 time: 0.14s
Epoch 25/1000, LR 0.000285
Train loss: 0.4017;  Loss pred: 0.4017; Loss self: 0.0000; time: 0.23s
Val loss: 0.6634 score: 0.5116 time: 0.15s
Test loss: 0.6591 score: 0.5504 time: 0.15s
Epoch 26/1000, LR 0.000285
Train loss: 0.3773;  Loss pred: 0.3773; Loss self: 0.0000; time: 0.23s
Val loss: 0.6560 score: 0.5271 time: 0.15s
Test loss: 0.6508 score: 0.5659 time: 0.15s
Epoch 27/1000, LR 0.000285
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.23s
Val loss: 0.6464 score: 0.5426 time: 0.15s
Test loss: 0.6401 score: 0.6124 time: 0.15s
Epoch 28/1000, LR 0.000285
Train loss: 0.3068;  Loss pred: 0.3068; Loss self: 0.0000; time: 0.23s
Val loss: 0.6340 score: 0.5969 time: 0.15s
Test loss: 0.6267 score: 0.6357 time: 0.15s
Epoch 29/1000, LR 0.000285
Train loss: 0.2670;  Loss pred: 0.2670; Loss self: 0.0000; time: 0.23s
Val loss: 0.6189 score: 0.6589 time: 0.17s
Test loss: 0.6104 score: 0.6899 time: 0.16s
Epoch 30/1000, LR 0.000285
Train loss: 0.2469;  Loss pred: 0.2469; Loss self: 0.0000; time: 0.24s
Val loss: 0.6018 score: 0.7674 time: 0.17s
Test loss: 0.5923 score: 0.7752 time: 0.15s
Epoch 31/1000, LR 0.000285
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.24s
Val loss: 0.5808 score: 0.8837 time: 0.15s
Test loss: 0.5700 score: 0.8372 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.24s
Val loss: 0.5574 score: 0.9225 time: 0.15s
Test loss: 0.5455 score: 0.8915 time: 0.14s
Epoch 33/1000, LR 0.000285
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 0.23s
Val loss: 0.5312 score: 0.9457 time: 0.16s
Test loss: 0.5181 score: 0.9457 time: 0.15s
Epoch 34/1000, LR 0.000285
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.24s
Val loss: 0.5042 score: 0.9457 time: 0.15s
Test loss: 0.4902 score: 0.9535 time: 0.15s
Epoch 35/1000, LR 0.000285
Train loss: 0.1151;  Loss pred: 0.1151; Loss self: 0.0000; time: 0.24s
Val loss: 0.4746 score: 0.9457 time: 0.16s
Test loss: 0.4593 score: 0.9690 time: 0.15s
Epoch 36/1000, LR 0.000285
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 0.24s
Val loss: 0.4433 score: 0.9380 time: 0.16s
Test loss: 0.4265 score: 0.9612 time: 0.15s
Epoch 37/1000, LR 0.000285
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.24s
Val loss: 0.4121 score: 0.9302 time: 0.16s
Test loss: 0.3944 score: 0.9612 time: 0.15s
Epoch 38/1000, LR 0.000284
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.24s
Val loss: 0.3840 score: 0.9302 time: 0.16s
Test loss: 0.3657 score: 0.9535 time: 0.15s
Epoch 39/1000, LR 0.000284
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.24s
Val loss: 0.3572 score: 0.9302 time: 0.16s
Test loss: 0.3387 score: 0.9535 time: 0.15s
Epoch 40/1000, LR 0.000284
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.24s
Val loss: 0.3323 score: 0.9302 time: 0.16s
Test loss: 0.3137 score: 0.9535 time: 0.15s
Epoch 41/1000, LR 0.000284
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.24s
Val loss: 0.3063 score: 0.9302 time: 0.15s
Test loss: 0.2866 score: 0.9535 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.24s
Val loss: 0.2829 score: 0.9225 time: 0.16s
Test loss: 0.2608 score: 0.9457 time: 0.15s
Epoch 43/1000, LR 0.000284
Train loss: 0.0417;  Loss pred: 0.0417; Loss self: 0.0000; time: 0.24s
Val loss: 0.2644 score: 0.9302 time: 0.16s
Test loss: 0.2393 score: 0.9380 time: 0.15s
Epoch 44/1000, LR 0.000284
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.24s
Val loss: 0.2487 score: 0.9225 time: 0.15s
Test loss: 0.2189 score: 0.9380 time: 0.15s
Epoch 45/1000, LR 0.000284
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.24s
Val loss: 0.2394 score: 0.9147 time: 0.16s
Test loss: 0.2036 score: 0.9457 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.23s
Val loss: 0.2348 score: 0.9070 time: 0.15s
Test loss: 0.1936 score: 0.9457 time: 0.15s
Epoch 47/1000, LR 0.000284
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.24s
Val loss: 0.2337 score: 0.8915 time: 0.16s
Test loss: 0.1870 score: 0.9457 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.24s
Val loss: 0.2353 score: 0.8915 time: 0.16s
Test loss: 0.1839 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.24s
Val loss: 0.2355 score: 0.8915 time: 0.15s
Test loss: 0.1830 score: 0.9302 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.23s
Val loss: 0.2370 score: 0.8837 time: 0.15s
Test loss: 0.1828 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.23s
Val loss: 0.2413 score: 0.8915 time: 0.15s
Test loss: 0.1838 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.24s
Val loss: 0.2476 score: 0.8915 time: 0.15s
Test loss: 0.1872 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.23s
Val loss: 0.2580 score: 0.8915 time: 0.15s
Test loss: 0.1927 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.24s
Val loss: 0.2739 score: 0.8915 time: 0.16s
Test loss: 0.2017 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.24s
Val loss: 0.2912 score: 0.8837 time: 0.16s
Test loss: 0.2122 score: 0.9147 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.24s
Val loss: 0.3027 score: 0.8837 time: 0.16s
Test loss: 0.2176 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.24s
Val loss: 0.3196 score: 0.8837 time: 0.16s
Test loss: 0.2265 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.24s
Val loss: 0.3345 score: 0.8837 time: 0.16s
Test loss: 0.2341 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.24s
Val loss: 0.3433 score: 0.8837 time: 0.16s
Test loss: 0.2390 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.24s
Val loss: 0.3516 score: 0.8837 time: 0.15s
Test loss: 0.2441 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.24s
Val loss: 0.3483 score: 0.8915 time: 0.16s
Test loss: 0.2411 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.24s
Val loss: 0.3549 score: 0.8915 time: 0.16s
Test loss: 0.2458 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.3633 score: 0.8992 time: 0.16s
Test loss: 0.2516 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.24s
Val loss: 0.3718 score: 0.8992 time: 0.16s
Test loss: 0.2577 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.3802 score: 0.8915 time: 0.16s
Test loss: 0.2640 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.24s
Val loss: 0.3881 score: 0.8915 time: 0.16s
Test loss: 0.2702 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.24s
Val loss: 0.3988 score: 0.8837 time: 0.16s
Test loss: 0.2798 score: 0.9225 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.0261,   Val_Loss: 0.2337,   Val_Precision: 0.9630,   Val_Recall: 0.8125,   Val_accuracy: 0.8814,   Val_Score: 0.8915,   Val_Loss: 0.2337,   Test_Precision: 0.9833,   Test_Recall: 0.9077,   Test_accuracy: 0.9440,   Test_Score: 0.9457,   Test_loss: 0.1870


[0.15419887914322317, 0.15313562308438122, 0.15574973192997277, 0.1572541119530797, 0.15592341497540474, 0.16297311894595623, 0.16532097314484417, 0.15726782800629735, 0.15414192900061607, 0.1551049528643489, 0.1612031569238752, 0.1545483749359846, 0.15625845710746944, 0.16234639822505414, 0.15947941597551107, 0.15913073299452662, 0.15799084794707596, 0.16594743495807052, 0.17591885197907686, 0.15473408484831452, 0.15603062394075096, 0.1516093430109322, 0.15081099001690745, 0.14710864005610347, 0.15374028403311968, 0.1521000259090215, 0.15215042792260647, 0.15411642799153924, 0.16831961111165583, 0.156609944999218, 0.15167772118002176, 0.14887095801532269, 0.1553799759130925, 0.1567159011028707, 0.1554966999683529, 0.15564285800792277, 0.1562164609786123, 0.15916927391663194, 0.15641166595742106, 0.153916320996359, 0.16003872407600284, 0.15473792585544288, 0.15750911785289645, 0.156998303020373, 0.154370270203799, 0.15257771499454975, 0.15679770801216364, 0.156778046162799, 0.15427223406732082, 0.1586713371798396, 0.15346086188219488, 0.1529153031297028, 0.15746598201803863, 0.15491911093704402, 0.1534303210210055, 0.15916399098932743, 0.15449722902849317, 0.15650272904895246, 0.15875617414712906, 0.15780157409608364, 0.1583505398593843, 0.15868624509312212, 0.15863468288443983, 0.15398908592760563, 0.1566203199326992, 0.15570655185729265, 0.1566117680631578]
[0.0011953401483970787, 0.0011870978533672963, 0.0012073622630230448, 0.0012190241236672845, 0.0012087086432201917, 0.001263357511208963, 0.0012815579313553813, 0.0012191304496612198, 0.0011948986744233804, 0.001202363975692627, 0.0012496368753788776, 0.0011980494181084078, 0.0012113058690501506, 0.0012584992110469314, 0.0012362745424458223, 0.0012335715736009815, 0.0012247352554036895, 0.0012864142244811668, 0.001363712030845557, 0.0011994890298318956, 0.0012095397204709376, 0.0011752662248909473, 0.0011690774419915308, 0.0011403770546984766, 0.0011917851475435634, 0.0011790699682869884, 0.0011794606815705927, 0.0011947009921824748, 0.0013048031869120607, 0.0012140305813892868, 0.0011757962882172229, 0.0011540384342273076, 0.0012044959373107946, 0.0012148519465338815, 0.0012054007749484723, 0.0012065337830071533, 0.0012109803176636612, 0.0012338703404390073, 0.0012124935345536516, 0.0011931497751655736, 0.0012406102641550607, 0.0011995188050809525, 0.0012210009135883446, 0.001217041108685062, 0.0011966687612697598, 0.001182772984453874, 0.0012154861086214235, 0.0012153336911844884, 0.0011959087912195414, 0.0012300103657351907, 0.0011896190843580998, 0.0011853899467418822, 0.0012206665272716172, 0.0012009233405972405, 0.0011893823334961668, 0.001233829387514166, 0.0011976529382053734, 0.0012131994499918796, 0.0012306680166444114, 0.0012232680162487103, 0.0012275235648014286, 0.0012301259309544352, 0.0012297262239103863, 0.0011937138444000437, 0.0012141110072302263, 0.0012070275337774624, 0.0012140447136678898]
[836.5819564757153, 842.3905385418915, 828.2518268346053, 820.3283106421411, 827.3292373717468, 791.5415795826912, 780.3002701113915, 820.2567660235923, 836.8910447428253, 831.6949112051911, 800.2324672891954, 834.6901095105853, 825.5553164157894, 794.5972402860001, 808.8818184524117, 810.654218531357, 816.5029916366589, 777.3545884128554, 733.2926434475754, 833.6883248862615, 826.760777736714, 850.8710442119527, 855.3753276570744, 876.9029470383432, 839.0774142983242, 848.1260882701049, 847.8451343273101, 837.0295216489309, 766.3991090998129, 823.7024794347776, 850.4874611538615, 866.5222667991601, 830.2228085821856, 823.1455716502082, 829.5995993886328, 828.8205552832591, 825.777252870051, 810.4579283786035, 824.746665860057, 838.1177458305516, 806.054914176506, 833.6676305233184, 819.0002062006203, 821.664932157007, 835.6531334025309, 845.4707819199417, 822.7161074956068, 822.8192859735339, 836.184170015373, 813.0012785723875, 840.6052098093102, 843.60425254876, 819.224561056129, 832.6926175843016, 840.7725353213538, 810.4848288747041, 834.9664315093261, 824.266776585411, 812.5668226323456, 817.4823396974058, 814.6483119953513, 812.9249004808117, 813.1891314963719, 837.7217075023508, 823.6479152604985, 828.4815151402903, 823.6928909963992]
Elapsed: 0.15658188545242396~0.004313605967882356
Time per graph: 0.0012138130655226665~3.34388059525764e-05
Speed: 824.4508813260945~21.84410239678349
Total Time: 0.1571
best val loss: 0.23371107610621195 test_score: 0.9457

Testing...
Test loss: 0.5181 score: 0.9457 time: 0.15s
test Score 0.9457
Epoch Time List: [0.7034919501747936, 0.5501972520723939, 0.5590323482174426, 0.5452639253344387, 0.5424794270657003, 0.5476271219085902, 0.5967980760615319, 0.5418254029937088, 0.540947558125481, 0.5464117899537086, 0.5699679693207145, 0.5623902101069689, 0.5510304269846529, 0.5578973372466862, 0.5573773749638349, 0.5545182509813458, 0.5521573410369456, 0.5749077720101923, 0.5692557396832854, 0.5568229130003601, 0.5389492129907012, 0.52827985631302, 0.5314585238229483, 0.5195590700022876, 0.5326176623348147, 0.5311564919538796, 0.5276106540113688, 0.5346604862716049, 0.5640532907564193, 0.5629000831395388, 0.5380948400124907, 0.5352267078123987, 0.5400002389214933, 0.5440566609613597, 0.5436771330423653, 0.5472629121504724, 0.5486492631025612, 0.5501003880053759, 0.5523285306990147, 0.5479941663797945, 0.550890289247036, 0.5433433388825506, 0.5491836811415851, 0.5456051093060523, 0.5433400531765074, 0.5349315840285271, 0.5458787539973855, 0.5484254646580666, 0.5440200900193304, 0.5414549889974296, 0.5358345352578908, 0.5383117818273604, 0.539868788793683, 0.5459642161149532, 0.5485704569146037, 0.5460818922147155, 0.5417335582897067, 0.5438905879855156, 0.5447167402599007, 0.5457858461886644, 0.549718813970685, 0.5514735369943082, 0.5566379732917994, 0.5477859682869166, 0.5560285362880677, 0.5484820471610874, 0.5455088266171515]
Total Epoch List: [67]
Total Time List: [0.15709161292761564]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feaf949e40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.28s
Epoch 2/1000, LR 0.000015
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6610;  Loss pred: 0.6610; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5039 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5039 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5039 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5039 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.5039 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6843 score: 0.5039 time: 0.16s
Epoch 19/1000, LR 0.000285
Train loss: 0.4865;  Loss pred: 0.4865; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6781 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6812 score: 0.5039 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.4683;  Loss pred: 0.4683; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6726 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.5039 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4364;  Loss pred: 0.4364; Loss self: 0.0000; time: 0.22s
Val loss: 0.6661 score: 0.5194 time: 0.16s
Test loss: 0.6717 score: 0.5116 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.4137;  Loss pred: 0.4137; Loss self: 0.0000; time: 0.23s
Val loss: 0.6582 score: 0.6124 time: 0.18s
Test loss: 0.6654 score: 0.5504 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3806;  Loss pred: 0.3806; Loss self: 0.0000; time: 0.23s
Val loss: 0.6488 score: 0.7132 time: 0.17s
Test loss: 0.6577 score: 0.6279 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 0.23s
Val loss: 0.6376 score: 0.7829 time: 0.17s
Test loss: 0.6484 score: 0.6822 time: 0.18s
Epoch 25/1000, LR 0.000285
Train loss: 0.3281;  Loss pred: 0.3281; Loss self: 0.0000; time: 0.22s
Val loss: 0.6247 score: 0.8217 time: 0.17s
Test loss: 0.6376 score: 0.7519 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3087;  Loss pred: 0.3087; Loss self: 0.0000; time: 0.21s
Val loss: 0.6103 score: 0.8837 time: 0.16s
Test loss: 0.6253 score: 0.8140 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.22s
Val loss: 0.5939 score: 0.8992 time: 0.18s
Test loss: 0.6112 score: 0.8605 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 0.2564;  Loss pred: 0.2564; Loss self: 0.0000; time: 0.22s
Val loss: 0.5759 score: 0.8992 time: 0.19s
Test loss: 0.5955 score: 0.8837 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2426;  Loss pred: 0.2426; Loss self: 0.0000; time: 0.24s
Val loss: 0.5567 score: 0.9070 time: 0.17s
Test loss: 0.5785 score: 0.8992 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.2206;  Loss pred: 0.2206; Loss self: 0.0000; time: 0.29s
Val loss: 0.5341 score: 0.9147 time: 0.17s
Test loss: 0.5588 score: 0.8992 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.22s
Val loss: 0.5109 score: 0.9070 time: 0.17s
Test loss: 0.5380 score: 0.9070 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1803;  Loss pred: 0.1803; Loss self: 0.0000; time: 0.23s
Val loss: 0.4856 score: 0.9147 time: 0.17s
Test loss: 0.5152 score: 0.9147 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.23s
Val loss: 0.4589 score: 0.9147 time: 0.16s
Test loss: 0.4909 score: 0.9147 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 0.22s
Val loss: 0.4318 score: 0.9147 time: 0.17s
Test loss: 0.4659 score: 0.9225 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.1403;  Loss pred: 0.1403; Loss self: 0.0000; time: 0.22s
Val loss: 0.4059 score: 0.9147 time: 0.16s
Test loss: 0.4414 score: 0.9225 time: 0.15s
Epoch 36/1000, LR 0.000285
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.22s
Val loss: 0.3808 score: 0.9225 time: 0.16s
Test loss: 0.4174 score: 0.9225 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.1185;  Loss pred: 0.1185; Loss self: 0.0000; time: 0.22s
Val loss: 0.3555 score: 0.9225 time: 0.16s
Test loss: 0.3932 score: 0.9302 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.1088;  Loss pred: 0.1088; Loss self: 0.0000; time: 0.22s
Val loss: 0.3310 score: 0.9225 time: 0.17s
Test loss: 0.3697 score: 0.9302 time: 0.17s
Epoch 39/1000, LR 0.000284
Train loss: 0.1011;  Loss pred: 0.1011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3077 score: 0.9225 time: 0.17s
Test loss: 0.3476 score: 0.9302 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.24s
Val loss: 0.2880 score: 0.9302 time: 0.16s
Test loss: 0.3286 score: 0.9302 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.22s
Val loss: 0.2687 score: 0.9302 time: 0.16s
Test loss: 0.3110 score: 0.9302 time: 0.15s
Epoch 42/1000, LR 0.000284
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.22s
Val loss: 0.2512 score: 0.9302 time: 0.16s
Test loss: 0.2953 score: 0.9302 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.23s
Val loss: 0.2392 score: 0.9380 time: 0.16s
Test loss: 0.2844 score: 0.9302 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0749;  Loss pred: 0.0749; Loss self: 0.0000; time: 0.23s
Val loss: 0.2303 score: 0.9380 time: 0.17s
Test loss: 0.2753 score: 0.9225 time: 0.17s
Epoch 45/1000, LR 0.000284
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.23s
Val loss: 0.2227 score: 0.9380 time: 0.17s
Test loss: 0.2682 score: 0.9302 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.23s
Val loss: 0.2181 score: 0.9380 time: 0.17s
Test loss: 0.2638 score: 0.9302 time: 0.17s
Epoch 47/1000, LR 0.000284
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.23s
Val loss: 0.2163 score: 0.9380 time: 0.17s
Test loss: 0.2613 score: 0.9302 time: 0.17s
Epoch 48/1000, LR 0.000284
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.24s
Val loss: 0.2110 score: 0.9380 time: 0.17s
Test loss: 0.2594 score: 0.9302 time: 0.17s
Epoch 49/1000, LR 0.000284
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.24s
Val loss: 0.2078 score: 0.9380 time: 0.17s
Test loss: 0.2598 score: 0.9147 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.24s
Val loss: 0.2051 score: 0.9380 time: 0.17s
Test loss: 0.2600 score: 0.9225 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.24s
Val loss: 0.2037 score: 0.9302 time: 0.17s
Test loss: 0.2633 score: 0.9147 time: 0.18s
Epoch 52/1000, LR 0.000284
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.25s
Val loss: 0.2041 score: 0.9302 time: 0.18s
Test loss: 0.2672 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.24s
Val loss: 0.2073 score: 0.9302 time: 0.17s
Test loss: 0.2751 score: 0.9147 time: 0.29s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.24s
Val loss: 0.2129 score: 0.9302 time: 0.17s
Test loss: 0.2827 score: 0.9147 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.23s
Val loss: 0.2214 score: 0.9225 time: 0.16s
Test loss: 0.3006 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.22s
Val loss: 0.2279 score: 0.9225 time: 0.16s
Test loss: 0.3078 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.23s
Val loss: 0.2359 score: 0.9225 time: 0.16s
Test loss: 0.3188 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.23s
Val loss: 0.2433 score: 0.9225 time: 0.16s
Test loss: 0.3295 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.23s
Val loss: 0.2516 score: 0.9225 time: 0.16s
Test loss: 0.3423 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.23s
Val loss: 0.2625 score: 0.9225 time: 0.16s
Test loss: 0.3619 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.22s
Val loss: 0.2689 score: 0.9225 time: 0.16s
Test loss: 0.3724 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.22s
Val loss: 0.2728 score: 0.9225 time: 0.16s
Test loss: 0.3800 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.23s
Val loss: 0.2785 score: 0.9225 time: 0.16s
Test loss: 0.3878 score: 0.8915 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.23s
Val loss: 0.2850 score: 0.9225 time: 0.16s
Test loss: 0.4002 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.22s
Val loss: 0.2939 score: 0.9225 time: 0.16s
Test loss: 0.4182 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.22s
Val loss: 0.2991 score: 0.9225 time: 0.17s
Test loss: 0.4284 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.22s
Val loss: 0.3062 score: 0.9225 time: 0.16s
Test loss: 0.4422 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.22s
Val loss: 0.3094 score: 0.9225 time: 0.16s
Test loss: 0.4473 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.22s
Val loss: 0.3106 score: 0.9225 time: 0.16s
Test loss: 0.4481 score: 0.8760 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.22s
Val loss: 0.3134 score: 0.9225 time: 0.17s
Test loss: 0.4547 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.23s
Val loss: 0.3127 score: 0.9225 time: 0.16s
Test loss: 0.4513 score: 0.8760 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0405,   Val_Loss: 0.2037,   Val_Precision: 0.9667,   Val_Recall: 0.8923,   Val_accuracy: 0.9280,   Val_Score: 0.9302,   Val_Loss: 0.2037,   Test_Precision: 0.9818,   Test_Recall: 0.8438,   Test_accuracy: 0.9076,   Test_Score: 0.9147,   Test_loss: 0.2633


[0.15419887914322317, 0.15313562308438122, 0.15574973192997277, 0.1572541119530797, 0.15592341497540474, 0.16297311894595623, 0.16532097314484417, 0.15726782800629735, 0.15414192900061607, 0.1551049528643489, 0.1612031569238752, 0.1545483749359846, 0.15625845710746944, 0.16234639822505414, 0.15947941597551107, 0.15913073299452662, 0.15799084794707596, 0.16594743495807052, 0.17591885197907686, 0.15473408484831452, 0.15603062394075096, 0.1516093430109322, 0.15081099001690745, 0.14710864005610347, 0.15374028403311968, 0.1521000259090215, 0.15215042792260647, 0.15411642799153924, 0.16831961111165583, 0.156609944999218, 0.15167772118002176, 0.14887095801532269, 0.1553799759130925, 0.1567159011028707, 0.1554966999683529, 0.15564285800792277, 0.1562164609786123, 0.15916927391663194, 0.15641166595742106, 0.153916320996359, 0.16003872407600284, 0.15473792585544288, 0.15750911785289645, 0.156998303020373, 0.154370270203799, 0.15257771499454975, 0.15679770801216364, 0.156778046162799, 0.15427223406732082, 0.1586713371798396, 0.15346086188219488, 0.1529153031297028, 0.15746598201803863, 0.15491911093704402, 0.1534303210210055, 0.15916399098932743, 0.15449722902849317, 0.15650272904895246, 0.15875617414712906, 0.15780157409608364, 0.1583505398593843, 0.15868624509312212, 0.15863468288443983, 0.15398908592760563, 0.1566203199326992, 0.15570655185729265, 0.1566117680631578, 0.2819475249852985, 0.17297093383967876, 0.16539208311587572, 0.17045877710916102, 0.16363997105509043, 0.16282142791897058, 0.1631237780675292, 0.16703489399515092, 0.1662263770122081, 0.16450714482925832, 0.16499328892678022, 0.16519065108150244, 0.16699110297486186, 0.16780574503354728, 0.1676824139431119, 0.16680609178729355, 0.16069640195928514, 0.16366152605041862, 0.16492949984967709, 0.16668446105904877, 0.16524916607886553, 0.17224197299219668, 0.17077261209487915, 0.18011223105713725, 0.17412460618652403, 0.1616960009559989, 0.16520758089609444, 0.17465187702327967, 0.17785133118741214, 0.1728506179060787, 0.16915506403893232, 0.170209035044536, 0.16071067517623305, 0.16111916792578995, 0.15697145299054682, 0.1598125919699669, 0.16084675397723913, 0.1718525260221213, 0.16305468883365393, 0.16116520389914513, 0.159049749141559, 0.16262217494659126, 0.16114832507446408, 0.17372654587961733, 0.17505029193125665, 0.17258448689244688, 0.1736450749449432, 0.17475874605588615, 0.17478984920307994, 0.1747977149207145, 0.17998195299878716, 0.17552075209096074, 0.2991702149156481, 0.17493971507064998, 0.16279459092766047, 0.16102195903658867, 0.16347169200889766, 0.16428100410848856, 0.16475552809424698, 0.16655523283407092, 0.16470394493080676, 0.16345884301699698, 0.1655896590091288, 0.1648787420708686, 0.16448582499288023, 0.16255429689772427, 0.16316799097694457, 0.16471094102598727, 0.16413296503014863, 0.17517653689719737, 0.1700049510691315]
[0.0011953401483970787, 0.0011870978533672963, 0.0012073622630230448, 0.0012190241236672845, 0.0012087086432201917, 0.001263357511208963, 0.0012815579313553813, 0.0012191304496612198, 0.0011948986744233804, 0.001202363975692627, 0.0012496368753788776, 0.0011980494181084078, 0.0012113058690501506, 0.0012584992110469314, 0.0012362745424458223, 0.0012335715736009815, 0.0012247352554036895, 0.0012864142244811668, 0.001363712030845557, 0.0011994890298318956, 0.0012095397204709376, 0.0011752662248909473, 0.0011690774419915308, 0.0011403770546984766, 0.0011917851475435634, 0.0011790699682869884, 0.0011794606815705927, 0.0011947009921824748, 0.0013048031869120607, 0.0012140305813892868, 0.0011757962882172229, 0.0011540384342273076, 0.0012044959373107946, 0.0012148519465338815, 0.0012054007749484723, 0.0012065337830071533, 0.0012109803176636612, 0.0012338703404390073, 0.0012124935345536516, 0.0011931497751655736, 0.0012406102641550607, 0.0011995188050809525, 0.0012210009135883446, 0.001217041108685062, 0.0011966687612697598, 0.001182772984453874, 0.0012154861086214235, 0.0012153336911844884, 0.0011959087912195414, 0.0012300103657351907, 0.0011896190843580998, 0.0011853899467418822, 0.0012206665272716172, 0.0012009233405972405, 0.0011893823334961668, 0.001233829387514166, 0.0011976529382053734, 0.0012131994499918796, 0.0012306680166444114, 0.0012232680162487103, 0.0012275235648014286, 0.0012301259309544352, 0.0012297262239103863, 0.0011937138444000437, 0.0012141110072302263, 0.0012070275337774624, 0.0012140447136678898, 0.0021856397285682054, 0.001340859952245572, 0.001282109171440897, 0.0013213858690632636, 0.0012685269074038017, 0.0012621816117749658, 0.0012645254113761953, 0.001294844139497294, 0.0012885765659861094, 0.0012752491847229327, 0.0012790177436184515, 0.0012805476828023445, 0.0012945046742237354, 0.001300819728942227, 0.0012998636739776116, 0.001293070478971268, 0.0012457085423200399, 0.001268694000390842, 0.0012785232546486595, 0.0012921276051089052, 0.0012810012874330662, 0.001335209092962765, 0.0013238186984099159, 0.0013962188454041647, 0.0013498031487327446, 0.0012534573717519295, 0.0012806789216751506, 0.0013538905195603075, 0.0013786924898249004, 0.0013399272705897573, 0.0013112795661932737, 0.0013194498840661704, 0.0012458191874126594, 0.0012489857978743408, 0.0012168329689189676, 0.0012388573020927665, 0.001246874061839063, 0.0013321901242024908, 0.0012639898359197979, 0.001249342665884846, 0.001232943791795031, 0.0012606370150898547, 0.0012492118222826674, 0.0013467174099195142, 0.0013569790072190438, 0.0013378642394763325, 0.0013460858522863813, 0.0013547189616735361, 0.001354960071341705, 0.0013550210458970115, 0.0013952089379750943, 0.001360625985201246, 0.0023191489528344815, 0.0013561218222531007, 0.0012619735730826393, 0.0012482322405937107, 0.0012672224186736252, 0.0012734961558797562, 0.0012771746363895115, 0.0012911258359230304, 0.0012767747669054786, 0.0012671228140852479, 0.0012836407675126263, 0.0012781297834951054, 0.0012750839146734901, 0.0012601108286645292, 0.0012648681471080973, 0.0012768290002014516, 0.0012723485661251832, 0.001357957650365871, 0.0013178678377452054]
[836.5819564757153, 842.3905385418915, 828.2518268346053, 820.3283106421411, 827.3292373717468, 791.5415795826912, 780.3002701113915, 820.2567660235923, 836.8910447428253, 831.6949112051911, 800.2324672891954, 834.6901095105853, 825.5553164157894, 794.5972402860001, 808.8818184524117, 810.654218531357, 816.5029916366589, 777.3545884128554, 733.2926434475754, 833.6883248862615, 826.760777736714, 850.8710442119527, 855.3753276570744, 876.9029470383432, 839.0774142983242, 848.1260882701049, 847.8451343273101, 837.0295216489309, 766.3991090998129, 823.7024794347776, 850.4874611538615, 866.5222667991601, 830.2228085821856, 823.1455716502082, 829.5995993886328, 828.8205552832591, 825.777252870051, 810.4579283786035, 824.746665860057, 838.1177458305516, 806.054914176506, 833.6676305233184, 819.0002062006203, 821.664932157007, 835.6531334025309, 845.4707819199417, 822.7161074956068, 822.8192859735339, 836.184170015373, 813.0012785723875, 840.6052098093102, 843.60425254876, 819.224561056129, 832.6926175843016, 840.7725353213538, 810.4848288747041, 834.9664315093261, 824.266776585411, 812.5668226323456, 817.4823396974058, 814.6483119953513, 812.9249004808117, 813.1891314963719, 837.7217075023508, 823.6479152604985, 828.4815151402903, 823.6928909963992, 457.53194679302965, 745.7900419244194, 779.9647816855962, 756.7812123712996, 788.3159546427158, 792.2790117293279, 790.8105214838587, 772.2937220754899, 776.0501210378055, 784.1604699533802, 781.8499821362242, 780.9158639150436, 772.4962450210244, 768.7460281780618, 769.3114439762578, 773.3530509455085, 802.7559947028814, 788.2121297112886, 782.1523749091305, 773.9173716637038, 780.6393403427795, 748.9463674794545, 755.3904482548361, 716.220099228448, 740.8487681621166, 797.7933853484959, 780.8358387690041, 738.6121592200544, 725.3249055755748, 746.3091631532051, 762.6138817239882, 757.891612312158, 802.6846994360544, 800.6496164343167, 821.805478272337, 807.1954682034228, 802.0056159681926, 750.6436069691217, 791.1456022684755, 800.4209151792242, 811.0669818484665, 793.2497523315407, 800.5047520065203, 742.5462778117382, 736.9310760741782, 747.4599966820405, 742.8946662662411, 738.1604807277974, 738.0291280537755, 737.9959175010518, 716.7385276726567, 734.9558297992472, 431.1926574521194, 737.3968795359185, 792.4096203990127, 801.1329682722815, 789.1274532900694, 785.2399046380947, 782.9782799531113, 774.5178449512603, 783.2234987097229, 789.1894841479219, 779.0341544992755, 782.39315984442, 784.2621089421157, 793.5809908560219, 790.5962390517362, 783.1902313013137, 785.9481486629132, 736.3999898896498, 758.8014301274256]
Elapsed: 0.16385315850113882~0.016999782288068352
Time per graph: 0.0012701795232646423~0.0001317812580470415
Speed: 792.7900197196035~54.76660821667332
Total Time: 0.1707
best val loss: 0.2037347943211595 test_score: 0.9147

Testing...
Test loss: 0.2844 score: 0.9302 time: 0.16s
test Score 0.9302
Epoch Time List: [0.7034919501747936, 0.5501972520723939, 0.5590323482174426, 0.5452639253344387, 0.5424794270657003, 0.5476271219085902, 0.5967980760615319, 0.5418254029937088, 0.540947558125481, 0.5464117899537086, 0.5699679693207145, 0.5623902101069689, 0.5510304269846529, 0.5578973372466862, 0.5573773749638349, 0.5545182509813458, 0.5521573410369456, 0.5749077720101923, 0.5692557396832854, 0.5568229130003601, 0.5389492129907012, 0.52827985631302, 0.5314585238229483, 0.5195590700022876, 0.5326176623348147, 0.5311564919538796, 0.5276106540113688, 0.5346604862716049, 0.5640532907564193, 0.5629000831395388, 0.5380948400124907, 0.5352267078123987, 0.5400002389214933, 0.5440566609613597, 0.5436771330423653, 0.5472629121504724, 0.5486492631025612, 0.5501003880053759, 0.5523285306990147, 0.5479941663797945, 0.550890289247036, 0.5433433388825506, 0.5491836811415851, 0.5456051093060523, 0.5433400531765074, 0.5349315840285271, 0.5458787539973855, 0.5484254646580666, 0.5440200900193304, 0.5414549889974296, 0.5358345352578908, 0.5383117818273604, 0.539868788793683, 0.5459642161149532, 0.5485704569146037, 0.5460818922147155, 0.5417335582897067, 0.5438905879855156, 0.5447167402599007, 0.5457858461886644, 0.549718813970685, 0.5514735369943082, 0.5566379732917994, 0.5477859682869166, 0.5560285362880677, 0.5484820471610874, 0.5455088266171515, 0.6662610091734678, 0.6006871752906591, 0.5539901209995151, 0.6494319101329893, 0.5415425801184028, 0.5654415499884635, 0.6292525748722255, 0.5430302598979324, 0.5481568688992411, 0.5475069538224488, 0.551989114144817, 0.5560418476816267, 0.5530189729761332, 0.5528451229911298, 0.5513637829571962, 0.5530983880162239, 0.5470481009688228, 0.5390694828238338, 0.5434551960788667, 0.546666000969708, 0.5449648350477219, 0.57615887071006, 0.566328824032098, 0.578320788918063, 0.5600056771654636, 0.528307634871453, 0.560189037816599, 0.5810582099948078, 0.5854506909381598, 0.6312470622360706, 0.5566916980315, 0.5653715836815536, 0.5456566808279604, 0.543002030113712, 0.5281384759582579, 0.5348475039936602, 0.5361964369658381, 0.5564776968676597, 0.551984537160024, 0.5568837330210954, 0.536603027721867, 0.5410342409741133, 0.5441449941135943, 0.564906073268503, 0.5741975228302181, 0.5759532400406897, 0.5713343650568277, 0.5782038178294897, 0.5792081158142537, 0.5789519918616861, 0.5881293199490756, 0.5958655199501663, 0.7093341159634292, 0.5794338402338326, 0.5493742739781737, 0.542887378949672, 0.5493595160078257, 0.5480140908621252, 0.5477637620642781, 0.5515735109802336, 0.5408803573809564, 0.5443679550662637, 0.5501209541689605, 0.556190243922174, 0.5450532932300121, 0.5512835017871112, 0.5439894050359726, 0.5439122079405934, 0.5398797732777894, 0.5703359672334045, 0.5563794621266425]
Total Epoch List: [67, 71]
Total Time List: [0.15709161292761564, 0.17065712297335267]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb23f1810>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.14s
Epoch 2/1000, LR 0.000020
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
Epoch 6/1000, LR 0.000140
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
Epoch 7/1000, LR 0.000170
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
Epoch 8/1000, LR 0.000200
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
Epoch 9/1000, LR 0.000230
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.14s
Epoch 10/1000, LR 0.000260
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000290
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000290
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000290
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000290
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5039 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.14s
Epoch 19/1000, LR 0.000290
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5039 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.14s
Epoch 20/1000, LR 0.000290
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.5000 time: 0.14s
Epoch 21/1000, LR 0.000290
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6719 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.5000 time: 0.14s
Epoch 22/1000, LR 0.000290
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6593 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6644 score: 0.5000 time: 0.14s
Epoch 23/1000, LR 0.000290
Train loss: 0.4979;  Loss pred: 0.4979; Loss self: 0.0000; time: 0.21s
Val loss: 0.6476 score: 0.5116 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6541 score: 0.5000 time: 0.14s
Epoch 24/1000, LR 0.000290
Train loss: 0.4808;  Loss pred: 0.4808; Loss self: 0.0000; time: 0.21s
Val loss: 0.6294 score: 0.5426 time: 0.15s
Test loss: 0.6374 score: 0.5234 time: 0.14s
Epoch 25/1000, LR 0.000290
Train loss: 0.4622;  Loss pred: 0.4622; Loss self: 0.0000; time: 0.19s
Val loss: 0.6079 score: 0.5969 time: 0.15s
Test loss: 0.6176 score: 0.5547 time: 0.14s
Epoch 26/1000, LR 0.000290
Train loss: 0.4472;  Loss pred: 0.4472; Loss self: 0.0000; time: 0.20s
Val loss: 0.5837 score: 0.6512 time: 0.16s
Test loss: 0.5949 score: 0.5781 time: 0.15s
Epoch 27/1000, LR 0.000290
Train loss: 0.4348;  Loss pred: 0.4348; Loss self: 0.0000; time: 0.22s
Val loss: 0.5552 score: 0.7674 time: 0.15s
Test loss: 0.5682 score: 0.7266 time: 0.14s
Epoch 28/1000, LR 0.000290
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.21s
Val loss: 0.5241 score: 0.8992 time: 0.22s
Test loss: 0.5387 score: 0.8516 time: 0.14s
Epoch 29/1000, LR 0.000290
Train loss: 0.4131;  Loss pred: 0.4131; Loss self: 0.0000; time: 0.23s
Val loss: 0.4949 score: 0.9147 time: 0.15s
Test loss: 0.5124 score: 0.8906 time: 0.14s
Epoch 30/1000, LR 0.000290
Train loss: 0.3862;  Loss pred: 0.3862; Loss self: 0.0000; time: 0.20s
Val loss: 0.4604 score: 0.9302 time: 0.15s
Test loss: 0.4797 score: 0.9141 time: 0.14s
Epoch 31/1000, LR 0.000290
Train loss: 0.3660;  Loss pred: 0.3660; Loss self: 0.0000; time: 0.22s
Val loss: 0.4300 score: 0.9380 time: 0.21s
Test loss: 0.4507 score: 0.8984 time: 0.14s
Epoch 32/1000, LR 0.000290
Train loss: 0.3408;  Loss pred: 0.3408; Loss self: 0.0000; time: 0.21s
Val loss: 0.4021 score: 0.9457 time: 0.15s
Test loss: 0.4255 score: 0.8828 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 0.3175;  Loss pred: 0.3175; Loss self: 0.0000; time: 0.20s
Val loss: 0.3726 score: 0.9457 time: 0.16s
Test loss: 0.3986 score: 0.8984 time: 0.15s
Epoch 34/1000, LR 0.000290
Train loss: 0.3041;  Loss pred: 0.3041; Loss self: 0.0000; time: 0.20s
Val loss: 0.3406 score: 0.9457 time: 0.24s
Test loss: 0.3688 score: 0.8984 time: 0.14s
Epoch 35/1000, LR 0.000290
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 0.20s
Val loss: 0.3131 score: 0.9457 time: 0.14s
Test loss: 0.3442 score: 0.9141 time: 0.14s
Epoch 36/1000, LR 0.000290
Train loss: 0.2708;  Loss pred: 0.2708; Loss self: 0.0000; time: 0.19s
Val loss: 0.2912 score: 0.9457 time: 0.14s
Test loss: 0.3248 score: 0.9219 time: 0.13s
Epoch 37/1000, LR 0.000290
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.21s
Val loss: 0.2782 score: 0.9380 time: 0.24s
Test loss: 0.3130 score: 0.8906 time: 0.14s
Epoch 38/1000, LR 0.000289
Train loss: 0.2342;  Loss pred: 0.2342; Loss self: 0.0000; time: 0.19s
Val loss: 0.2735 score: 0.9302 time: 0.14s
Test loss: 0.3098 score: 0.9062 time: 0.13s
Epoch 39/1000, LR 0.000289
Train loss: 0.2134;  Loss pred: 0.2134; Loss self: 0.0000; time: 0.19s
Val loss: 0.2526 score: 0.9380 time: 0.14s
Test loss: 0.2905 score: 0.8906 time: 0.14s
Epoch 40/1000, LR 0.000289
Train loss: 0.1930;  Loss pred: 0.1930; Loss self: 0.0000; time: 0.23s
Val loss: 0.2397 score: 0.9380 time: 0.16s
Test loss: 0.2793 score: 0.8906 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.1982;  Loss pred: 0.1982; Loss self: 0.0000; time: 0.22s
Val loss: 0.2238 score: 0.9457 time: 0.15s
Test loss: 0.2643 score: 0.9141 time: 0.13s
Epoch 42/1000, LR 0.000289
Train loss: 0.1724;  Loss pred: 0.1724; Loss self: 0.0000; time: 0.19s
Val loss: 0.2163 score: 0.9457 time: 0.14s
Test loss: 0.2562 score: 0.9141 time: 0.13s
Epoch 43/1000, LR 0.000289
Train loss: 0.1588;  Loss pred: 0.1588; Loss self: 0.0000; time: 0.20s
Val loss: 0.2151 score: 0.9380 time: 0.14s
Test loss: 0.2561 score: 0.8906 time: 0.14s
Epoch 44/1000, LR 0.000289
Train loss: 0.1467;  Loss pred: 0.1467; Loss self: 0.0000; time: 0.20s
Val loss: 0.2033 score: 0.9380 time: 0.14s
Test loss: 0.2455 score: 0.8906 time: 0.13s
Epoch 45/1000, LR 0.000289
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.19s
Val loss: 0.1990 score: 0.9457 time: 0.14s
Test loss: 0.2414 score: 0.9062 time: 0.14s
Epoch 46/1000, LR 0.000289
Train loss: 0.1420;  Loss pred: 0.1420; Loss self: 0.0000; time: 0.22s
Val loss: 0.2003 score: 0.9457 time: 0.14s
Test loss: 0.2412 score: 0.9062 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1342;  Loss pred: 0.1342; Loss self: 0.0000; time: 0.20s
Val loss: 0.1945 score: 0.9457 time: 0.15s
Test loss: 0.2364 score: 0.9062 time: 0.13s
Epoch 48/1000, LR 0.000289
Train loss: 0.1252;  Loss pred: 0.1252; Loss self: 0.0000; time: 0.19s
Val loss: 0.1893 score: 0.9380 time: 0.14s
Test loss: 0.2351 score: 0.9062 time: 0.13s
Epoch 49/1000, LR 0.000289
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.19s
Val loss: 0.1911 score: 0.9380 time: 0.15s
Test loss: 0.2359 score: 0.8906 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.19s
Val loss: 0.1984 score: 0.9612 time: 0.14s
Test loss: 0.2574 score: 0.8984 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1288;  Loss pred: 0.1288; Loss self: 0.0000; time: 0.19s
Val loss: 0.1929 score: 0.9612 time: 0.15s
Test loss: 0.2549 score: 0.8984 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1238;  Loss pred: 0.1238; Loss self: 0.0000; time: 0.18s
Val loss: 0.2026 score: 0.9535 time: 0.14s
Test loss: 0.2747 score: 0.8906 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.18s
Val loss: 0.1903 score: 0.9612 time: 0.14s
Test loss: 0.2572 score: 0.8984 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.18s
Val loss: 0.1851 score: 0.9535 time: 0.14s
Test loss: 0.2501 score: 0.8906 time: 0.13s
Epoch 55/1000, LR 0.000289
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.19s
Val loss: 0.1823 score: 0.9535 time: 0.14s
Test loss: 0.2488 score: 0.8906 time: 0.14s
Epoch 56/1000, LR 0.000289
Train loss: 0.0893;  Loss pred: 0.0893; Loss self: 0.0000; time: 0.20s
Val loss: 0.2000 score: 0.9457 time: 0.15s
Test loss: 0.2792 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.19s
Val loss: 0.2013 score: 0.9380 time: 0.14s
Test loss: 0.2857 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0809;  Loss pred: 0.0809; Loss self: 0.0000; time: 0.19s
Val loss: 0.1859 score: 0.9380 time: 0.14s
Test loss: 0.2680 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.20s
Val loss: 0.1758 score: 0.9457 time: 0.14s
Test loss: 0.2550 score: 0.8906 time: 0.14s
Epoch 60/1000, LR 0.000288
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.19s
Val loss: 0.1682 score: 0.9457 time: 0.14s
Test loss: 0.2431 score: 0.8828 time: 0.14s
Epoch 61/1000, LR 0.000288
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.19s
Val loss: 0.1631 score: 0.9457 time: 0.14s
Test loss: 0.2345 score: 0.8828 time: 0.14s
Epoch 62/1000, LR 0.000288
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.19s
Val loss: 0.1595 score: 0.9380 time: 0.14s
Test loss: 0.2257 score: 0.8828 time: 0.14s
Epoch 63/1000, LR 0.000288
Train loss: 0.0766;  Loss pred: 0.0766; Loss self: 0.0000; time: 0.19s
Val loss: 0.1637 score: 0.9380 time: 0.14s
Test loss: 0.2090 score: 0.9141 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.20s
Val loss: 0.1585 score: 0.9457 time: 0.14s
Test loss: 0.2080 score: 0.8906 time: 0.14s
Epoch 65/1000, LR 0.000288
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.19s
Val loss: 0.1642 score: 0.9380 time: 0.14s
Test loss: 0.2016 score: 0.9141 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.19s
Val loss: 0.1698 score: 0.9380 time: 0.14s
Test loss: 0.2021 score: 0.9219 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.20s
Val loss: 0.1718 score: 0.9380 time: 0.15s
Test loss: 0.2010 score: 0.9219 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.20s
Val loss: 0.1675 score: 0.9380 time: 0.15s
Test loss: 0.1971 score: 0.9219 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.19s
Val loss: 0.1754 score: 0.9380 time: 0.15s
Test loss: 0.1982 score: 0.9297 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.19s
Val loss: 0.1611 score: 0.9302 time: 0.15s
Test loss: 0.1947 score: 0.9141 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 71/1000, LR 0.000287
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.22s
Val loss: 0.1602 score: 0.9457 time: 0.15s
Test loss: 0.2211 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 72/1000, LR 0.000287
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.22s
Val loss: 0.1593 score: 0.9457 time: 0.15s
Test loss: 0.2107 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 73/1000, LR 0.000287
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.21s
Val loss: 0.1596 score: 0.9457 time: 0.15s
Test loss: 0.2105 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 9 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.22s
Val loss: 0.1608 score: 0.9457 time: 0.15s
Test loss: 0.2125 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.19s
Val loss: 0.1675 score: 0.9612 time: 0.15s
Test loss: 0.2514 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 11 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.21s
Val loss: 0.1645 score: 0.9535 time: 0.15s
Test loss: 0.2400 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.21s
Val loss: 0.1771 score: 0.9612 time: 0.15s
Test loss: 0.2806 score: 0.8906 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.24s
Val loss: 0.1717 score: 0.9612 time: 0.15s
Test loss: 0.2602 score: 0.8984 time: 0.14s
     INFO: Early stopping counter 14 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0509;  Loss pred: 0.0509; Loss self: 0.0000; time: 0.20s
Val loss: 0.1698 score: 0.9612 time: 0.15s
Test loss: 0.2511 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 15 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.19s
Val loss: 0.1635 score: 0.9535 time: 0.14s
Test loss: 0.2252 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 16 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.20s
Val loss: 0.1624 score: 0.9535 time: 0.15s
Test loss: 0.2148 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.22s
Val loss: 0.1676 score: 0.9535 time: 0.15s
Test loss: 0.2319 score: 0.9141 time: 0.14s
     INFO: Early stopping counter 18 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.20s
Val loss: 0.1782 score: 0.9612 time: 0.16s
Test loss: 0.2557 score: 0.9062 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.21s
Val loss: 0.1720 score: 0.9535 time: 0.15s
Test loss: 0.2348 score: 0.9141 time: 0.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 063,   Train_Loss: 0.0707,   Val_Loss: 0.1585,   Val_Precision: 0.9531,   Val_Recall: 0.9385,   Val_accuracy: 0.9457,   Val_Score: 0.9457,   Val_Loss: 0.1585,   Test_Precision: 0.9310,   Test_Recall: 0.8438,   Test_accuracy: 0.8852,   Test_Score: 0.8906,   Test_loss: 0.2080


[0.15419887914322317, 0.15313562308438122, 0.15574973192997277, 0.1572541119530797, 0.15592341497540474, 0.16297311894595623, 0.16532097314484417, 0.15726782800629735, 0.15414192900061607, 0.1551049528643489, 0.1612031569238752, 0.1545483749359846, 0.15625845710746944, 0.16234639822505414, 0.15947941597551107, 0.15913073299452662, 0.15799084794707596, 0.16594743495807052, 0.17591885197907686, 0.15473408484831452, 0.15603062394075096, 0.1516093430109322, 0.15081099001690745, 0.14710864005610347, 0.15374028403311968, 0.1521000259090215, 0.15215042792260647, 0.15411642799153924, 0.16831961111165583, 0.156609944999218, 0.15167772118002176, 0.14887095801532269, 0.1553799759130925, 0.1567159011028707, 0.1554966999683529, 0.15564285800792277, 0.1562164609786123, 0.15916927391663194, 0.15641166595742106, 0.153916320996359, 0.16003872407600284, 0.15473792585544288, 0.15750911785289645, 0.156998303020373, 0.154370270203799, 0.15257771499454975, 0.15679770801216364, 0.156778046162799, 0.15427223406732082, 0.1586713371798396, 0.15346086188219488, 0.1529153031297028, 0.15746598201803863, 0.15491911093704402, 0.1534303210210055, 0.15916399098932743, 0.15449722902849317, 0.15650272904895246, 0.15875617414712906, 0.15780157409608364, 0.1583505398593843, 0.15868624509312212, 0.15863468288443983, 0.15398908592760563, 0.1566203199326992, 0.15570655185729265, 0.1566117680631578, 0.2819475249852985, 0.17297093383967876, 0.16539208311587572, 0.17045877710916102, 0.16363997105509043, 0.16282142791897058, 0.1631237780675292, 0.16703489399515092, 0.1662263770122081, 0.16450714482925832, 0.16499328892678022, 0.16519065108150244, 0.16699110297486186, 0.16780574503354728, 0.1676824139431119, 0.16680609178729355, 0.16069640195928514, 0.16366152605041862, 0.16492949984967709, 0.16668446105904877, 0.16524916607886553, 0.17224197299219668, 0.17077261209487915, 0.18011223105713725, 0.17412460618652403, 0.1616960009559989, 0.16520758089609444, 0.17465187702327967, 0.17785133118741214, 0.1728506179060787, 0.16915506403893232, 0.170209035044536, 0.16071067517623305, 0.16111916792578995, 0.15697145299054682, 0.1598125919699669, 0.16084675397723913, 0.1718525260221213, 0.16305468883365393, 0.16116520389914513, 0.159049749141559, 0.16262217494659126, 0.16114832507446408, 0.17372654587961733, 0.17505029193125665, 0.17258448689244688, 0.1736450749449432, 0.17475874605588615, 0.17478984920307994, 0.1747977149207145, 0.17998195299878716, 0.17552075209096074, 0.2991702149156481, 0.17493971507064998, 0.16279459092766047, 0.16102195903658867, 0.16347169200889766, 0.16428100410848856, 0.16475552809424698, 0.16655523283407092, 0.16470394493080676, 0.16345884301699698, 0.1655896590091288, 0.1648787420708686, 0.16448582499288023, 0.16255429689772427, 0.16316799097694457, 0.16471094102598727, 0.16413296503014863, 0.17517653689719737, 0.1700049510691315, 0.1402394068427384, 0.1424384000711143, 0.1398759540170431, 0.13956522685475647, 0.13954561902210116, 0.1425430600065738, 0.1414324720390141, 0.1416666880249977, 0.1414981409907341, 0.14289990696124732, 0.14351794007234275, 0.14325456297956407, 0.14126797299832106, 0.14036577893421054, 0.14127197302877903, 0.1420851389411837, 0.1407487529795617, 0.14121745200827718, 0.14186978596262634, 0.1413294340018183, 0.14194671693257987, 0.14256999385543168, 0.14193453593179584, 0.14080883306451142, 0.14247109391726553, 0.1562231571879238, 0.14265148201957345, 0.14712496707215905, 0.1482535928953439, 0.14667519996874034, 0.14853348908945918, 0.15004952112212777, 0.15140882693231106, 0.14156125695444643, 0.13991179806180298, 0.138428129022941, 0.14508578111417592, 0.13780702208168805, 0.14159510703757405, 0.16287146578542888, 0.13690568483434618, 0.13675095699727535, 0.14140125713311136, 0.13765437994152308, 0.14374141511507332, 0.13840817008167505, 0.13768063206225634, 0.13796210708096623, 0.13841560599394143, 0.137568939011544, 0.13695277785882354, 0.13740147789940238, 0.13924543582834303, 0.1375049790367484, 0.14085580105893314, 0.1414485559798777, 0.14151979377493262, 0.13962891208939254, 0.13972165901213884, 0.13956378097645938, 0.14017451205290854, 0.14107297197915614, 0.1410849061794579, 0.14105275110341609, 0.14249248802661896, 0.14248570194467902, 0.14243847108446062, 0.142741794930771, 0.14242602791637182, 0.14147192612290382, 0.14206912694498897, 0.14321189909242094, 0.14229050488211215, 0.14266904699616134, 0.1449399939738214, 0.1451696550939232, 0.14216294814832509, 0.14122232608497143, 0.1429336799774319, 0.14148623892106116, 0.14200735604390502, 0.14530965802259743, 0.14255850901827216, 0.14323334116488695]
[0.0011953401483970787, 0.0011870978533672963, 0.0012073622630230448, 0.0012190241236672845, 0.0012087086432201917, 0.001263357511208963, 0.0012815579313553813, 0.0012191304496612198, 0.0011948986744233804, 0.001202363975692627, 0.0012496368753788776, 0.0011980494181084078, 0.0012113058690501506, 0.0012584992110469314, 0.0012362745424458223, 0.0012335715736009815, 0.0012247352554036895, 0.0012864142244811668, 0.001363712030845557, 0.0011994890298318956, 0.0012095397204709376, 0.0011752662248909473, 0.0011690774419915308, 0.0011403770546984766, 0.0011917851475435634, 0.0011790699682869884, 0.0011794606815705927, 0.0011947009921824748, 0.0013048031869120607, 0.0012140305813892868, 0.0011757962882172229, 0.0011540384342273076, 0.0012044959373107946, 0.0012148519465338815, 0.0012054007749484723, 0.0012065337830071533, 0.0012109803176636612, 0.0012338703404390073, 0.0012124935345536516, 0.0011931497751655736, 0.0012406102641550607, 0.0011995188050809525, 0.0012210009135883446, 0.001217041108685062, 0.0011966687612697598, 0.001182772984453874, 0.0012154861086214235, 0.0012153336911844884, 0.0011959087912195414, 0.0012300103657351907, 0.0011896190843580998, 0.0011853899467418822, 0.0012206665272716172, 0.0012009233405972405, 0.0011893823334961668, 0.001233829387514166, 0.0011976529382053734, 0.0012131994499918796, 0.0012306680166444114, 0.0012232680162487103, 0.0012275235648014286, 0.0012301259309544352, 0.0012297262239103863, 0.0011937138444000437, 0.0012141110072302263, 0.0012070275337774624, 0.0012140447136678898, 0.0021856397285682054, 0.001340859952245572, 0.001282109171440897, 0.0013213858690632636, 0.0012685269074038017, 0.0012621816117749658, 0.0012645254113761953, 0.001294844139497294, 0.0012885765659861094, 0.0012752491847229327, 0.0012790177436184515, 0.0012805476828023445, 0.0012945046742237354, 0.001300819728942227, 0.0012998636739776116, 0.001293070478971268, 0.0012457085423200399, 0.001268694000390842, 0.0012785232546486595, 0.0012921276051089052, 0.0012810012874330662, 0.001335209092962765, 0.0013238186984099159, 0.0013962188454041647, 0.0013498031487327446, 0.0012534573717519295, 0.0012806789216751506, 0.0013538905195603075, 0.0013786924898249004, 0.0013399272705897573, 0.0013112795661932737, 0.0013194498840661704, 0.0012458191874126594, 0.0012489857978743408, 0.0012168329689189676, 0.0012388573020927665, 0.001246874061839063, 0.0013321901242024908, 0.0012639898359197979, 0.001249342665884846, 0.001232943791795031, 0.0012606370150898547, 0.0012492118222826674, 0.0013467174099195142, 0.0013569790072190438, 0.0013378642394763325, 0.0013460858522863813, 0.0013547189616735361, 0.001354960071341705, 0.0013550210458970115, 0.0013952089379750943, 0.001360625985201246, 0.0023191489528344815, 0.0013561218222531007, 0.0012619735730826393, 0.0012482322405937107, 0.0012672224186736252, 0.0012734961558797562, 0.0012771746363895115, 0.0012911258359230304, 0.0012767747669054786, 0.0012671228140852479, 0.0012836407675126263, 0.0012781297834951054, 0.0012750839146734901, 0.0012601108286645292, 0.0012648681471080973, 0.0012768290002014516, 0.0012723485661251832, 0.001357957650365871, 0.0013178678377452054, 0.0010956203659588937, 0.0011128000005555805, 0.0010927808907581493, 0.001090353334802785, 0.0010902001486101653, 0.0011136176563013578, 0.0011049411878047977, 0.0011067710001952946, 0.0011054542264901102, 0.0011164055231347447, 0.0011212339068151778, 0.0011191762732778443, 0.0011036560390493833, 0.0010966076479235198, 0.0011036872892873362, 0.0011100401479779975, 0.0010995996326528257, 0.0011032613438146655, 0.0011083577028330183, 0.0011041362031392055, 0.0011089587260357803, 0.00111382807699556, 0.001108863561967155, 0.0011000690083164955, 0.001113055421228637, 0.0012204934155306546, 0.0011144647032779176, 0.0011494138052512426, 0.0011582311944948742, 0.001145899999755784, 0.0011604178835113999, 0.0011722618837666232, 0.0011828814604086801, 0.0011059473199566128, 0.0010930609223578358, 0.0010814697579917265, 0.0011334826649544993, 0.0010766173600131879, 0.0011062117737310473, 0.0012724333264486631, 0.0010695756627683295, 0.0010683668515412137, 0.0011046973213524325, 0.001075424843293149, 0.0011229798055865103, 0.0010813138287630863, 0.0010756299379863776, 0.0010778289615700487, 0.0010813719218276674, 0.0010747573360276874, 0.0010699435770220589, 0.001073449046089081, 0.00108785496740893, 0.001074257648724597, 0.0011004359457729151, 0.0011050668435927946, 0.0011056233888666611, 0.0010908508756983792, 0.0010915754610323347, 0.001090342038878589, 0.001095113375413348, 0.0011021325935871573, 0.0011022258295270149, 0.0011019746179954382, 0.0011132225627079606, 0.0011131695464428049, 0.0011128005553473486, 0.0011151702728966484, 0.0011127033430966549, 0.0011052494228351861, 0.0011099150542577263, 0.0011188429616595386, 0.0011116445693915011, 0.0011146019296575105, 0.0011323437029204797, 0.001134137930421275, 0.0011106480324087897, 0.0011032994225388393, 0.0011166693748236867, 0.0011053612415707903, 0.001109432469093008, 0.0011352317033015424, 0.0011137383517052513, 0.0011190104778506793]
[836.5819564757153, 842.3905385418915, 828.2518268346053, 820.3283106421411, 827.3292373717468, 791.5415795826912, 780.3002701113915, 820.2567660235923, 836.8910447428253, 831.6949112051911, 800.2324672891954, 834.6901095105853, 825.5553164157894, 794.5972402860001, 808.8818184524117, 810.654218531357, 816.5029916366589, 777.3545884128554, 733.2926434475754, 833.6883248862615, 826.760777736714, 850.8710442119527, 855.3753276570744, 876.9029470383432, 839.0774142983242, 848.1260882701049, 847.8451343273101, 837.0295216489309, 766.3991090998129, 823.7024794347776, 850.4874611538615, 866.5222667991601, 830.2228085821856, 823.1455716502082, 829.5995993886328, 828.8205552832591, 825.777252870051, 810.4579283786035, 824.746665860057, 838.1177458305516, 806.054914176506, 833.6676305233184, 819.0002062006203, 821.664932157007, 835.6531334025309, 845.4707819199417, 822.7161074956068, 822.8192859735339, 836.184170015373, 813.0012785723875, 840.6052098093102, 843.60425254876, 819.224561056129, 832.6926175843016, 840.7725353213538, 810.4848288747041, 834.9664315093261, 824.266776585411, 812.5668226323456, 817.4823396974058, 814.6483119953513, 812.9249004808117, 813.1891314963719, 837.7217075023508, 823.6479152604985, 828.4815151402903, 823.6928909963992, 457.53194679302965, 745.7900419244194, 779.9647816855962, 756.7812123712996, 788.3159546427158, 792.2790117293279, 790.8105214838587, 772.2937220754899, 776.0501210378055, 784.1604699533802, 781.8499821362242, 780.9158639150436, 772.4962450210244, 768.7460281780618, 769.3114439762578, 773.3530509455085, 802.7559947028814, 788.2121297112886, 782.1523749091305, 773.9173716637038, 780.6393403427795, 748.9463674794545, 755.3904482548361, 716.220099228448, 740.8487681621166, 797.7933853484959, 780.8358387690041, 738.6121592200544, 725.3249055755748, 746.3091631532051, 762.6138817239882, 757.891612312158, 802.6846994360544, 800.6496164343167, 821.805478272337, 807.1954682034228, 802.0056159681926, 750.6436069691217, 791.1456022684755, 800.4209151792242, 811.0669818484665, 793.2497523315407, 800.5047520065203, 742.5462778117382, 736.9310760741782, 747.4599966820405, 742.8946662662411, 738.1604807277974, 738.0291280537755, 737.9959175010518, 716.7385276726567, 734.9558297992472, 431.1926574521194, 737.3968795359185, 792.4096203990127, 801.1329682722815, 789.1274532900694, 785.2399046380947, 782.9782799531113, 774.5178449512603, 783.2234987097229, 789.1894841479219, 779.0341544992755, 782.39315984442, 784.2621089421157, 793.5809908560219, 790.5962390517362, 783.1902313013137, 785.9481486629132, 736.3999898896498, 758.8014301274256, 912.7249100784959, 898.6340757555145, 915.0965289173571, 917.1338941985009, 917.2627625072731, 897.9742682253132, 905.0255443791666, 903.529275544395, 904.6055241699746, 895.7318638052858, 891.8745624099631, 893.5142960734856, 906.0793984884406, 911.9031787654854, 906.0537433983785, 900.8683170798444, 909.4219116711254, 906.4035512586471, 902.2358011713633, 905.6853648642872, 901.746815749151, 897.8046259144411, 901.8242047975425, 909.0338810020315, 898.4278598599864, 819.3407578239273, 897.2917644307187, 870.0086908921515, 863.3854836176454, 872.676499007873, 861.7585218301025, 853.0517061485234, 845.3932481573466, 904.2022001909013, 914.8620900680498, 924.6675578399763, 882.2366948506816, 928.8351062700222, 903.9860393342093, 785.895794470411, 934.950218866938, 936.0080749018107, 905.2253324700234, 929.8650726142943, 890.4879633856992, 924.8008981295457, 929.6877714951297, 927.7909906441213, 924.7512163159021, 930.4425906000407, 934.6287238652989, 931.5765882352036, 919.2401836265138, 930.875382816442, 908.7307660580174, 904.9226350406088, 904.4671178899966, 916.7155862250969, 916.1070724824383, 917.1433956893882, 913.1474625835451, 907.3318453864593, 907.2550952912419, 907.4619176066537, 898.2929680902784, 898.3357505561984, 898.6336277374169, 896.7240468152955, 898.7121376098311, 904.7731483403987, 900.9698500474581, 893.7804806107346, 899.5681061504994, 897.181292613835, 883.1240880492858, 881.7269691602241, 900.3752501421943, 906.3722681000471, 895.5202162304238, 904.6816211675152, 901.3617573474553, 880.8774429852036, 897.8769550935318, 893.6466814151136]
Elapsed: 0.15561855706042266~0.017229637476892774
Time per graph: 0.0012096014531673461~0.00013106895355146145
Speed: 833.9160342198497~69.64410507805998
Total Time: 0.1440
best val loss: 0.15854293884810552 test_score: 0.8906

Testing...
Test loss: 0.2574 score: 0.8984 time: 0.13s
test Score 0.8984
Epoch Time List: [0.7034919501747936, 0.5501972520723939, 0.5590323482174426, 0.5452639253344387, 0.5424794270657003, 0.5476271219085902, 0.5967980760615319, 0.5418254029937088, 0.540947558125481, 0.5464117899537086, 0.5699679693207145, 0.5623902101069689, 0.5510304269846529, 0.5578973372466862, 0.5573773749638349, 0.5545182509813458, 0.5521573410369456, 0.5749077720101923, 0.5692557396832854, 0.5568229130003601, 0.5389492129907012, 0.52827985631302, 0.5314585238229483, 0.5195590700022876, 0.5326176623348147, 0.5311564919538796, 0.5276106540113688, 0.5346604862716049, 0.5640532907564193, 0.5629000831395388, 0.5380948400124907, 0.5352267078123987, 0.5400002389214933, 0.5440566609613597, 0.5436771330423653, 0.5472629121504724, 0.5486492631025612, 0.5501003880053759, 0.5523285306990147, 0.5479941663797945, 0.550890289247036, 0.5433433388825506, 0.5491836811415851, 0.5456051093060523, 0.5433400531765074, 0.5349315840285271, 0.5458787539973855, 0.5484254646580666, 0.5440200900193304, 0.5414549889974296, 0.5358345352578908, 0.5383117818273604, 0.539868788793683, 0.5459642161149532, 0.5485704569146037, 0.5460818922147155, 0.5417335582897067, 0.5438905879855156, 0.5447167402599007, 0.5457858461886644, 0.549718813970685, 0.5514735369943082, 0.5566379732917994, 0.5477859682869166, 0.5560285362880677, 0.5484820471610874, 0.5455088266171515, 0.6662610091734678, 0.6006871752906591, 0.5539901209995151, 0.6494319101329893, 0.5415425801184028, 0.5654415499884635, 0.6292525748722255, 0.5430302598979324, 0.5481568688992411, 0.5475069538224488, 0.551989114144817, 0.5560418476816267, 0.5530189729761332, 0.5528451229911298, 0.5513637829571962, 0.5530983880162239, 0.5470481009688228, 0.5390694828238338, 0.5434551960788667, 0.546666000969708, 0.5449648350477219, 0.57615887071006, 0.566328824032098, 0.578320788918063, 0.5600056771654636, 0.528307634871453, 0.560189037816599, 0.5810582099948078, 0.5854506909381598, 0.6312470622360706, 0.5566916980315, 0.5653715836815536, 0.5456566808279604, 0.543002030113712, 0.5281384759582579, 0.5348475039936602, 0.5361964369658381, 0.5564776968676597, 0.551984537160024, 0.5568837330210954, 0.536603027721867, 0.5410342409741133, 0.5441449941135943, 0.564906073268503, 0.5741975228302181, 0.5759532400406897, 0.5713343650568277, 0.5782038178294897, 0.5792081158142537, 0.5789519918616861, 0.5881293199490756, 0.5958655199501663, 0.7093341159634292, 0.5794338402338326, 0.5493742739781737, 0.542887378949672, 0.5493595160078257, 0.5480140908621252, 0.5477637620642781, 0.5515735109802336, 0.5408803573809564, 0.5443679550662637, 0.5501209541689605, 0.556190243922174, 0.5450532932300121, 0.5512835017871112, 0.5439894050359726, 0.5439122079405934, 0.5398797732777894, 0.5703359672334045, 0.5563794621266425, 0.531510076019913, 0.51441809698008, 0.5150179129559547, 0.5136400230694562, 0.5169842650648206, 0.5147581603378057, 0.5159109618980438, 0.5110982009209692, 0.5131448588799685, 0.5120227530132979, 0.5159305131528527, 0.506776835070923, 0.5051452913321555, 0.5032080060336739, 0.4734725870657712, 0.48703894577920437, 0.4675173298455775, 0.49352855305187404, 0.4680972939822823, 0.46919768606312573, 0.48384766723029315, 0.486525502987206, 0.4899873530957848, 0.4897089251317084, 0.4774730121716857, 0.5097302531357855, 0.5031281090341508, 0.5749505658168346, 0.5234111100435257, 0.49034272506833076, 0.5765240287873894, 0.5105728567577899, 0.5014029399026185, 0.5782648329623044, 0.47854502126574516, 0.46410276205278933, 0.5912670928519219, 0.4607941978611052, 0.46580319106578827, 0.547196458093822, 0.4951373350340873, 0.46212498494423926, 0.4790692296810448, 0.4752140389755368, 0.46777356695383787, 0.49646674701943994, 0.4791514908429235, 0.4627640410326421, 0.46875525591894984, 0.4625880531966686, 0.46648093103431165, 0.4591433871537447, 0.4591277639847249, 0.45941924420185387, 0.46636175527237356, 0.48677076189778745, 0.47122703306376934, 0.46618256671354175, 0.4830327411182225, 0.46872249827720225, 0.46889123786240816, 0.4700800299178809, 0.46898746583610773, 0.4836470449808985, 0.471719176042825, 0.47179771307855844, 0.4878577038180083, 0.48453800310380757, 0.47183342883363366, 0.4704480010550469, 0.5025346630718559, 0.5075401929207146, 0.490210154093802, 0.5053400278557092, 0.47756943805143237, 0.5034029630478472, 0.49932697718031704, 0.523360728751868, 0.486794593045488, 0.46927517908625305, 0.48463882715441287, 0.5088613000698388, 0.502362497150898, 0.5001910889986902]
Total Epoch List: [67, 71, 84]
Total Time List: [0.15709161292761564, 0.17065712297335267, 0.14403011417016387]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb233dde0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4961 time: 0.17s
Epoch 2/1000, LR 0.000015
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4961 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4961 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4961 time: 0.18s
Epoch 5/1000, LR 0.000105
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4961 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4961 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4961 time: 0.17s
Epoch 8/1000, LR 0.000195
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4961 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4961 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4961 time: 0.17s
Epoch 11/1000, LR 0.000285
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4961 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.17s
Epoch 13/1000, LR 0.000285
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4961 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.5864;  Loss pred: 0.5864; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4961 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.25s
Val loss: 0.6904 score: 0.5349 time: 0.17s
Test loss: 0.6905 score: 0.5116 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5495;  Loss pred: 0.5495; Loss self: 0.0000; time: 0.25s
Val loss: 0.6892 score: 0.9070 time: 0.17s
Test loss: 0.6892 score: 0.9380 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.25s
Val loss: 0.6877 score: 0.5504 time: 0.17s
Test loss: 0.6875 score: 0.5891 time: 0.16s
Epoch 18/1000, LR 0.000285
Train loss: 0.5030;  Loss pred: 0.5030; Loss self: 0.0000; time: 0.26s
Val loss: 0.6861 score: 0.5116 time: 0.17s
Test loss: 0.6856 score: 0.5426 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4829;  Loss pred: 0.4829; Loss self: 0.0000; time: 0.26s
Val loss: 0.6841 score: 0.5039 time: 0.17s
Test loss: 0.6832 score: 0.5426 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.4575;  Loss pred: 0.4575; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.4961 time: 0.17s
Test loss: 0.6805 score: 0.5194 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4312;  Loss pred: 0.4312; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.4961 time: 0.17s
Test loss: 0.6779 score: 0.5194 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.4052;  Loss pred: 0.4052; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6772 score: 0.4961 time: 0.17s
Test loss: 0.6749 score: 0.5194 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.3798;  Loss pred: 0.3798; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6748 score: 0.4961 time: 0.17s
Test loss: 0.6719 score: 0.5194 time: 0.16s
Epoch 24/1000, LR 0.000285
Train loss: 0.3475;  Loss pred: 0.3475; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6721 score: 0.4961 time: 0.17s
Test loss: 0.6684 score: 0.5194 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 0.3152;  Loss pred: 0.3152; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6696 score: 0.4961 time: 0.17s
Test loss: 0.6651 score: 0.5194 time: 0.16s
Epoch 26/1000, LR 0.000285
Train loss: 0.2936;  Loss pred: 0.2936; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6665 score: 0.4961 time: 0.17s
Test loss: 0.6612 score: 0.5194 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6633 score: 0.4961 time: 0.17s
Test loss: 0.6569 score: 0.5194 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2323;  Loss pred: 0.2323; Loss self: 0.0000; time: 0.25s
Val loss: 0.6582 score: 0.5039 time: 0.17s
Test loss: 0.6507 score: 0.5426 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 0.2131;  Loss pred: 0.2131; Loss self: 0.0000; time: 0.25s
Val loss: 0.6518 score: 0.5116 time: 0.17s
Test loss: 0.6431 score: 0.5426 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.26s
Val loss: 0.6430 score: 0.5194 time: 0.17s
Test loss: 0.6328 score: 0.5504 time: 0.17s
Epoch 31/1000, LR 0.000285
Train loss: 0.1607;  Loss pred: 0.1607; Loss self: 0.0000; time: 0.25s
Val loss: 0.6305 score: 0.5194 time: 0.17s
Test loss: 0.6188 score: 0.5581 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.25s
Val loss: 0.6138 score: 0.5194 time: 0.17s
Test loss: 0.6004 score: 0.5891 time: 0.17s
Epoch 33/1000, LR 0.000285
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.25s
Val loss: 0.5966 score: 0.5659 time: 0.17s
Test loss: 0.5811 score: 0.6124 time: 0.17s
Epoch 34/1000, LR 0.000285
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.26s
Val loss: 0.5727 score: 0.5969 time: 0.17s
Test loss: 0.5550 score: 0.6434 time: 0.17s
Epoch 35/1000, LR 0.000285
Train loss: 0.0994;  Loss pred: 0.0994; Loss self: 0.0000; time: 0.25s
Val loss: 0.5494 score: 0.6202 time: 0.17s
Test loss: 0.5295 score: 0.6667 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.25s
Val loss: 0.5203 score: 0.6667 time: 0.17s
Test loss: 0.4983 score: 0.6899 time: 0.17s
Epoch 37/1000, LR 0.000285
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.25s
Val loss: 0.4879 score: 0.7209 time: 0.17s
Test loss: 0.4637 score: 0.7364 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.25s
Val loss: 0.4538 score: 0.7519 time: 0.17s
Test loss: 0.4271 score: 0.7752 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.26s
Val loss: 0.4196 score: 0.7984 time: 0.17s
Test loss: 0.3905 score: 0.8372 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.25s
Val loss: 0.3821 score: 0.8372 time: 0.17s
Test loss: 0.3511 score: 0.8682 time: 0.18s
Epoch 41/1000, LR 0.000284
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.26s
Val loss: 0.3502 score: 0.8605 time: 0.18s
Test loss: 0.3174 score: 0.9070 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.26s
Val loss: 0.3191 score: 0.8760 time: 0.18s
Test loss: 0.2845 score: 0.9225 time: 0.18s
Epoch 43/1000, LR 0.000284
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.26s
Val loss: 0.2891 score: 0.8992 time: 0.17s
Test loss: 0.2531 score: 0.9302 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.25s
Val loss: 0.2652 score: 0.8992 time: 0.17s
Test loss: 0.2295 score: 0.9302 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.25s
Val loss: 0.2443 score: 0.8992 time: 0.17s
Test loss: 0.2095 score: 0.9225 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.24s
Val loss: 0.2267 score: 0.9380 time: 0.16s
Test loss: 0.1931 score: 0.9380 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.24s
Val loss: 0.2151 score: 0.9302 time: 0.19s
Test loss: 0.1847 score: 0.9302 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.23s
Val loss: 0.2059 score: 0.9225 time: 0.16s
Test loss: 0.1795 score: 0.9302 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.23s
Val loss: 0.2006 score: 0.9380 time: 0.17s
Test loss: 0.1781 score: 0.9070 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.31s
Val loss: 0.1988 score: 0.9380 time: 0.17s
Test loss: 0.1815 score: 0.8992 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.23s
Val loss: 0.1990 score: 0.9302 time: 0.17s
Test loss: 0.1862 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.24s
Val loss: 0.2010 score: 0.9225 time: 0.17s
Test loss: 0.1946 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.28s
Val loss: 0.2039 score: 0.9225 time: 0.16s
Test loss: 0.2006 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.24s
Val loss: 0.2083 score: 0.9225 time: 0.16s
Test loss: 0.2075 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.23s
Val loss: 0.2127 score: 0.9225 time: 0.16s
Test loss: 0.2109 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.30s
Val loss: 0.2156 score: 0.9225 time: 0.18s
Test loss: 0.2211 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 6 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.25s
Val loss: 0.2181 score: 0.9225 time: 0.18s
Test loss: 0.2314 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.26s
Val loss: 0.2196 score: 0.9225 time: 0.17s
Test loss: 0.2418 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.27s
Val loss: 0.2184 score: 0.9225 time: 0.18s
Test loss: 0.2520 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.25s
Val loss: 0.2176 score: 0.9225 time: 0.17s
Test loss: 0.2572 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.25s
Val loss: 0.2193 score: 0.9225 time: 0.18s
Test loss: 0.2640 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.24s
Val loss: 0.2222 score: 0.9225 time: 0.19s
Test loss: 0.2716 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.24s
Val loss: 0.2264 score: 0.9225 time: 0.19s
Test loss: 0.2782 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.24s
Val loss: 0.2303 score: 0.9225 time: 0.16s
Test loss: 0.2836 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.23s
Val loss: 0.2341 score: 0.9225 time: 0.16s
Test loss: 0.2921 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.23s
Val loss: 0.2360 score: 0.9225 time: 0.16s
Test loss: 0.2986 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.2345 score: 0.9225 time: 0.16s
Test loss: 0.3049 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.23s
Val loss: 0.2407 score: 0.9225 time: 0.16s
Test loss: 0.3152 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.23s
Val loss: 0.2464 score: 0.9225 time: 0.16s
Test loss: 0.3237 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.23s
Val loss: 0.2480 score: 0.9225 time: 0.16s
Test loss: 0.3333 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 049,   Train_Loss: 0.0150,   Val_Loss: 0.1988,   Val_Precision: 0.9516,   Val_Recall: 0.9219,   Val_accuracy: 0.9365,   Val_Score: 0.9380,   Val_Loss: 0.1988,   Test_Precision: 0.9643,   Test_Recall: 0.8308,   Test_accuracy: 0.8926,   Test_Score: 0.8992,   Test_loss: 0.1815


[0.17579235904850066, 0.16616891091689467, 0.16873433580622077, 0.18068093108013272, 0.18446972011588514, 0.16986460983753204, 0.1704392610117793, 0.16908243112266064, 0.16764784697443247, 0.17050638305954635, 0.17103855591267347, 0.1708829398266971, 0.1664391120430082, 0.16815928602591157, 0.16876153508201241, 0.16893357504159212, 0.16721065598540008, 0.1708719851449132, 0.16961050615645945, 0.16700094495899975, 0.17072298401035368, 0.1694684731774032, 0.16826277901418507, 0.1668149777688086, 0.16956829698756337, 0.17237611790187657, 0.16972582903690636, 0.1681888489983976, 0.1697020148858428, 0.17209483683109283, 0.172021740116179, 0.17125739995390177, 0.1701131728477776, 0.17075364803895354, 0.17271410091780126, 0.17245132382959127, 0.16761941998265684, 0.16857764683663845, 0.17179759196005762, 0.1814899379387498, 0.1779744359664619, 0.17959427903406322, 0.16782247717492282, 0.1671128598973155, 0.17317479220218956, 0.16152721899561584, 0.15672574820928276, 0.15744296903721988, 0.16049625515006483, 0.17041818797588348, 0.16995830019004643, 0.16821138397790492, 0.15844632708467543, 0.16061946679838002, 0.16041616909205914, 0.18116338597610593, 0.18036656896583736, 0.17584980791434646, 0.17211682512424886, 0.1722452079411596, 0.1835792870260775, 0.18877683696337044, 0.17513156798668206, 0.16037517599761486, 0.16362825711257756, 0.16128660808317363, 0.1636486949864775, 0.16010094597004354, 0.1598263659980148, 0.15822230791673064]
[0.001362731465492253, 0.0012881310923790286, 0.0013080181070249672, 0.0014006273727142071, 0.0014299978303556988, 0.0013167799212211787, 0.0013212345814866612, 0.0013107165203307025, 0.0012995957129800966, 0.0013217549074383437, 0.0013258802783928177, 0.0013246739521449388, 0.0012902256747520015, 0.001303560356790012, 0.0013082289541241272, 0.0013095625972216443, 0.001296206635545737, 0.00132458903213111, 0.0013148101252438717, 0.0012945809686744168, 0.0013234339845763852, 0.0013137090943984744, 0.0013043626280169385, 0.0012931393625489039, 0.0013144829223842123, 0.0013362489759835394, 0.0013157041010612896, 0.0013037895271193613, 0.0013155194952390915, 0.001334068502566611, 0.0013335018613657287, 0.001327576743828696, 0.001318706766261842, 0.001323671690224446, 0.0013388689993628004, 0.0013368319676712501, 0.0012993753487027663, 0.0013068034638499106, 0.0013317642787601365, 0.0014068987437112386, 0.001379646790437689, 0.0013922037134423506, 0.001300949435464518, 0.001295448526335779, 0.0013424402496293765, 0.0012521489844621383, 0.0012149282806921144, 0.0012204881320714719, 0.0012441570166671692, 0.0013210712246192519, 0.0013175062030236157, 0.001303964216882984, 0.0012282661014315925, 0.0012451121457238762, 0.0012435361945120863, 0.001404367333148108, 0.0013981904570995144, 0.0013631768055375694, 0.0013342389544515416, 0.0013352341700865085, 0.0014230952482641667, 0.0014633863330493833, 0.001357609054160326, 0.0012432184185861618, 0.001268436101647888, 0.0012502837835904933, 0.0012685945347788953, 0.0012410926044189423, 0.0012389640775039908, 0.0012265295187343461]
[733.8202905873129, 776.3185019881138, 764.5154104742926, 713.9657695409372, 699.3017603049504, 759.4283478081913, 756.867867381123, 762.9414785644826, 769.470066738605, 756.5699165347319, 754.2159094576492, 754.9027429585822, 775.0582084736558, 767.12980322789, 764.392193619893, 763.6137456289532, 771.4819324150225, 754.9511401216391, 760.5660930048888, 772.4507189565345, 755.6100354488688, 761.203529962532, 766.6579665198856, 773.3118555983807, 760.7554141412486, 748.3635295315793, 760.0493144266766, 766.9949629135582, 760.1559715527083, 749.5866951930147, 749.9052149622296, 753.2521224468175, 758.3186994897396, 755.4743426071436, 746.8990621755554, 748.0371686069053, 769.600563069287, 765.2260096204125, 750.8836330487803, 710.7832063038978, 724.8231989020558, 718.2856864584917, 768.6693830978444, 771.9334112244001, 744.9121108191459, 798.6270103709351, 823.0938532687089, 819.3443047272825, 803.7570713371744, 756.9614577656188, 759.0097091801514, 766.8922099644847, 814.1558240795383, 803.1405070092106, 804.1583384650576, 712.0644124912421, 715.2101453148641, 733.5805567830575, 749.4909338867749, 748.9323014667989, 702.6936540050702, 683.3465486289014, 736.589076903656, 804.3638873507364, 788.3723891970991, 799.8184197256861, 788.2739303888701, 805.7416476735694, 807.1259031291639, 815.3085471859646]
Elapsed: 0.16963251058477907~0.006567768852275546
Time per graph: 0.0013149807022075897~5.0912936839345314e-05
Speed: 761.5958232315461~29.19200027762653
Total Time: 0.1587
best val loss: 0.19875606021648923 test_score: 0.8992

Testing...
Test loss: 0.1931 score: 0.9380 time: 0.16s
test Score 0.9380
Epoch Time List: [0.5947638039942831, 0.5726712427567691, 0.5779411110561341, 0.5986597770825028, 0.6263499979395419, 0.5768072491046041, 0.5815312471240759, 0.5841047447174788, 0.5818659430369735, 0.5882002140861005, 0.587894229684025, 0.6229953437577933, 0.5768196857534349, 0.5803671111352742, 0.5786438167560846, 0.5897593989502639, 0.5885763079859316, 0.5908631149213761, 0.5925244491081685, 0.5852697379887104, 0.5906268109101802, 0.5883225009310991, 0.5863647360820323, 0.5885428118053824, 0.5896384429652244, 0.5945145089644939, 0.5903863171115518, 0.5859684939496219, 0.587343490216881, 0.5917655311059207, 0.5896679221186787, 0.5866696420125663, 0.5873007269110531, 0.5902915550395846, 0.5864443930331618, 0.591356243006885, 0.5877397200092673, 0.5838204210158437, 0.5894534380640835, 0.5994851409923285, 0.6133372799959034, 0.613038441631943, 0.590622307965532, 0.5806565899401903, 0.5878265318460763, 0.5608040178194642, 0.5852622548118234, 0.5396953858435154, 0.5599909599404782, 0.6459756926633418, 0.5702726601157337, 0.5755739249289036, 0.5877885399386287, 0.5488113698083907, 0.5475070658139884, 0.6565292598679662, 0.5993187299463898, 0.608217166736722, 0.6144858910702169, 0.5876358018722385, 0.6030458197928965, 0.611588679952547, 0.5966590761672705, 0.5516297610010952, 0.5519676092080772, 0.5468440777622163, 0.5503343457821757, 0.547379887662828, 0.5474430199246854, 0.5469995348248631]
Total Epoch List: [70]
Total Time List: [0.15873849904164672]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feaf923400>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4961 time: 0.14s
Epoch 2/1000, LR 0.000015
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.4961 time: 0.15s
Epoch 3/1000, LR 0.000045
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4961 time: 0.15s
Epoch 4/1000, LR 0.000075
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4961 time: 0.15s
Epoch 5/1000, LR 0.000105
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4961 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4961 time: 0.14s
Epoch 7/1000, LR 0.000165
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4961 time: 0.14s
Epoch 8/1000, LR 0.000195
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4961 time: 0.14s
Epoch 9/1000, LR 0.000225
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4961 time: 0.15s
Epoch 10/1000, LR 0.000255
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4961 time: 0.14s
Epoch 11/1000, LR 0.000285
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4961 time: 0.14s
Epoch 12/1000, LR 0.000285
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4961 time: 0.14s
Epoch 13/1000, LR 0.000285
Train loss: 0.6229;  Loss pred: 0.6229; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4961 time: 0.14s
Epoch 14/1000, LR 0.000285
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4961 time: 0.15s
Epoch 15/1000, LR 0.000285
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.24s
Val loss: 0.6897 score: 0.5659 time: 0.17s
Test loss: 0.6888 score: 0.5814 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 0.26s
Val loss: 0.6885 score: 0.8217 time: 0.16s
Test loss: 0.6870 score: 0.8760 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.24s
Val loss: 0.6866 score: 0.6589 time: 0.16s
Test loss: 0.6845 score: 0.7287 time: 0.15s
Epoch 18/1000, LR 0.000285
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.24s
Val loss: 0.6845 score: 0.5814 time: 0.16s
Test loss: 0.6817 score: 0.6434 time: 0.15s
Epoch 19/1000, LR 0.000285
Train loss: 0.4995;  Loss pred: 0.4995; Loss self: 0.0000; time: 0.25s
Val loss: 0.6819 score: 0.5581 time: 0.16s
Test loss: 0.6782 score: 0.6124 time: 0.15s
Epoch 20/1000, LR 0.000285
Train loss: 0.4784;  Loss pred: 0.4784; Loss self: 0.0000; time: 0.24s
Val loss: 0.6786 score: 0.5659 time: 0.17s
Test loss: 0.6738 score: 0.6357 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4455;  Loss pred: 0.4455; Loss self: 0.0000; time: 0.25s
Val loss: 0.6743 score: 0.5736 time: 0.16s
Test loss: 0.6683 score: 0.6589 time: 0.15s
Epoch 22/1000, LR 0.000285
Train loss: 0.4120;  Loss pred: 0.4120; Loss self: 0.0000; time: 0.24s
Val loss: 0.6688 score: 0.6279 time: 0.16s
Test loss: 0.6613 score: 0.6744 time: 0.15s
Epoch 23/1000, LR 0.000285
Train loss: 0.3925;  Loss pred: 0.3925; Loss self: 0.0000; time: 0.24s
Val loss: 0.6618 score: 0.6744 time: 0.16s
Test loss: 0.6526 score: 0.7364 time: 0.15s
Epoch 24/1000, LR 0.000285
Train loss: 0.3639;  Loss pred: 0.3639; Loss self: 0.0000; time: 0.24s
Val loss: 0.6529 score: 0.7209 time: 0.16s
Test loss: 0.6416 score: 0.7752 time: 0.15s
Epoch 25/1000, LR 0.000285
Train loss: 0.3317;  Loss pred: 0.3317; Loss self: 0.0000; time: 0.24s
Val loss: 0.6420 score: 0.7752 time: 0.16s
Test loss: 0.6283 score: 0.8295 time: 0.14s
Epoch 26/1000, LR 0.000285
Train loss: 0.3031;  Loss pred: 0.3031; Loss self: 0.0000; time: 0.24s
Val loss: 0.6291 score: 0.7907 time: 0.18s
Test loss: 0.6127 score: 0.8295 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 0.24s
Val loss: 0.6136 score: 0.8062 time: 0.16s
Test loss: 0.5943 score: 0.8527 time: 0.14s
Epoch 28/1000, LR 0.000285
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.23s
Val loss: 0.5960 score: 0.8527 time: 0.16s
Test loss: 0.5734 score: 0.8760 time: 0.15s
Epoch 29/1000, LR 0.000285
Train loss: 0.2186;  Loss pred: 0.2186; Loss self: 0.0000; time: 0.24s
Val loss: 0.5758 score: 0.8527 time: 0.16s
Test loss: 0.5500 score: 0.8760 time: 0.15s
Epoch 30/1000, LR 0.000285
Train loss: 0.1965;  Loss pred: 0.1965; Loss self: 0.0000; time: 0.25s
Val loss: 0.5533 score: 0.8527 time: 0.16s
Test loss: 0.5239 score: 0.8837 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 0.1904;  Loss pred: 0.1904; Loss self: 0.0000; time: 0.24s
Val loss: 0.5287 score: 0.8527 time: 0.18s
Test loss: 0.4961 score: 0.8992 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.1664;  Loss pred: 0.1664; Loss self: 0.0000; time: 0.24s
Val loss: 0.5027 score: 0.8527 time: 0.16s
Test loss: 0.4669 score: 0.9070 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.24s
Val loss: 0.4769 score: 0.8450 time: 0.16s
Test loss: 0.4376 score: 0.9070 time: 0.15s
Epoch 34/1000, LR 0.000285
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.24s
Val loss: 0.4506 score: 0.8605 time: 0.16s
Test loss: 0.4082 score: 0.9070 time: 0.14s
Epoch 35/1000, LR 0.000285
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.24s
Val loss: 0.4243 score: 0.8682 time: 0.16s
Test loss: 0.3789 score: 0.9147 time: 0.14s
Epoch 36/1000, LR 0.000285
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.24s
Val loss: 0.3995 score: 0.8682 time: 0.16s
Test loss: 0.3517 score: 0.9302 time: 0.14s
Epoch 37/1000, LR 0.000285
Train loss: 0.0861;  Loss pred: 0.0861; Loss self: 0.0000; time: 0.24s
Val loss: 0.3766 score: 0.8682 time: 0.16s
Test loss: 0.3262 score: 0.9380 time: 0.14s
Epoch 38/1000, LR 0.000284
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.24s
Val loss: 0.3550 score: 0.8682 time: 0.16s
Test loss: 0.3021 score: 0.9380 time: 0.14s
Epoch 39/1000, LR 0.000284
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.24s
Val loss: 0.3358 score: 0.8682 time: 0.16s
Test loss: 0.2804 score: 0.9380 time: 0.14s
Epoch 40/1000, LR 0.000284
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.24s
Val loss: 0.3181 score: 0.8682 time: 0.16s
Test loss: 0.2605 score: 0.9380 time: 0.14s
Epoch 41/1000, LR 0.000284
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.24s
Val loss: 0.3023 score: 0.8682 time: 0.16s
Test loss: 0.2425 score: 0.9380 time: 0.14s
Epoch 42/1000, LR 0.000284
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.24s
Val loss: 0.2892 score: 0.8682 time: 0.16s
Test loss: 0.2266 score: 0.9380 time: 0.14s
Epoch 43/1000, LR 0.000284
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.24s
Val loss: 0.2777 score: 0.8682 time: 0.15s
Test loss: 0.2126 score: 0.9380 time: 0.15s
Epoch 44/1000, LR 0.000284
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.24s
Val loss: 0.2664 score: 0.8682 time: 0.15s
Test loss: 0.2003 score: 0.9380 time: 0.14s
Epoch 45/1000, LR 0.000284
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.24s
Val loss: 0.2584 score: 0.8682 time: 0.16s
Test loss: 0.1916 score: 0.9380 time: 0.15s
Epoch 46/1000, LR 0.000284
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.24s
Val loss: 0.2552 score: 0.8682 time: 0.16s
Test loss: 0.1872 score: 0.9380 time: 0.15s
Epoch 47/1000, LR 0.000284
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.24s
Val loss: 0.2549 score: 0.8682 time: 0.16s
Test loss: 0.1850 score: 0.9380 time: 0.15s
Epoch 48/1000, LR 0.000284
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.24s
Val loss: 0.2548 score: 0.8682 time: 0.16s
Test loss: 0.1837 score: 0.9380 time: 0.14s
Epoch 49/1000, LR 0.000284
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.24s
Val loss: 0.2563 score: 0.8682 time: 0.16s
Test loss: 0.1825 score: 0.9380 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.24s
Val loss: 0.2579 score: 0.8682 time: 0.16s
Test loss: 0.1819 score: 0.9380 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.25s
Val loss: 0.2579 score: 0.8682 time: 0.17s
Test loss: 0.1805 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.27s
Val loss: 0.2612 score: 0.8682 time: 0.16s
Test loss: 0.1817 score: 0.9457 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.24s
Val loss: 0.2664 score: 0.8682 time: 0.16s
Test loss: 0.1840 score: 0.9457 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.24s
Val loss: 0.2739 score: 0.8760 time: 0.16s
Test loss: 0.1882 score: 0.9457 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.24s
Val loss: 0.2840 score: 0.8760 time: 0.17s
Test loss: 0.1941 score: 0.9457 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.24s
Val loss: 0.2963 score: 0.8760 time: 0.16s
Test loss: 0.2013 score: 0.9535 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.24s
Val loss: 0.3070 score: 0.8760 time: 0.17s
Test loss: 0.2075 score: 0.9535 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.25s
Val loss: 0.3248 score: 0.8760 time: 0.17s
Test loss: 0.2148 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.24s
Val loss: 0.3410 score: 0.8760 time: 0.17s
Test loss: 0.2221 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.25s
Val loss: 0.3506 score: 0.8760 time: 0.17s
Test loss: 0.2253 score: 0.9535 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.24s
Val loss: 0.3572 score: 0.8760 time: 0.17s
Test loss: 0.2270 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.24s
Val loss: 0.3615 score: 0.8760 time: 0.17s
Test loss: 0.2288 score: 0.9535 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.24s
Val loss: 0.3666 score: 0.8760 time: 0.15s
Test loss: 0.2317 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.24s
Val loss: 0.3675 score: 0.8760 time: 0.15s
Test loss: 0.2320 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.24s
Val loss: 0.3709 score: 0.8760 time: 0.15s
Test loss: 0.2342 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.24s
Val loss: 0.3702 score: 0.8760 time: 0.15s
Test loss: 0.2332 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.23s
Val loss: 0.3704 score: 0.8760 time: 0.16s
Test loss: 0.2334 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.3728 score: 0.8760 time: 0.15s
Test loss: 0.2360 score: 0.9535 time: 0.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.0264,   Val_Loss: 0.2548,   Val_Precision: 0.9615,   Val_Recall: 0.7692,   Val_accuracy: 0.8547,   Val_Score: 0.8682,   Val_Loss: 0.2548,   Test_Precision: 0.9828,   Test_Recall: 0.8906,   Test_accuracy: 0.9344,   Test_Score: 0.9380,   Test_loss: 0.1837


[0.17579235904850066, 0.16616891091689467, 0.16873433580622077, 0.18068093108013272, 0.18446972011588514, 0.16986460983753204, 0.1704392610117793, 0.16908243112266064, 0.16764784697443247, 0.17050638305954635, 0.17103855591267347, 0.1708829398266971, 0.1664391120430082, 0.16815928602591157, 0.16876153508201241, 0.16893357504159212, 0.16721065598540008, 0.1708719851449132, 0.16961050615645945, 0.16700094495899975, 0.17072298401035368, 0.1694684731774032, 0.16826277901418507, 0.1668149777688086, 0.16956829698756337, 0.17237611790187657, 0.16972582903690636, 0.1681888489983976, 0.1697020148858428, 0.17209483683109283, 0.172021740116179, 0.17125739995390177, 0.1701131728477776, 0.17075364803895354, 0.17271410091780126, 0.17245132382959127, 0.16761941998265684, 0.16857764683663845, 0.17179759196005762, 0.1814899379387498, 0.1779744359664619, 0.17959427903406322, 0.16782247717492282, 0.1671128598973155, 0.17317479220218956, 0.16152721899561584, 0.15672574820928276, 0.15744296903721988, 0.16049625515006483, 0.17041818797588348, 0.16995830019004643, 0.16821138397790492, 0.15844632708467543, 0.16061946679838002, 0.16041616909205914, 0.18116338597610593, 0.18036656896583736, 0.17584980791434646, 0.17211682512424886, 0.1722452079411596, 0.1835792870260775, 0.18877683696337044, 0.17513156798668206, 0.16037517599761486, 0.16362825711257756, 0.16128660808317363, 0.1636486949864775, 0.16010094597004354, 0.1598263659980148, 0.15822230791673064, 0.14864952093921602, 0.1503160980064422, 0.15180860296823084, 0.15141619695350528, 0.15067239594645798, 0.14795213798061013, 0.1491881189867854, 0.14830843405798078, 0.15070471516810358, 0.14919288805685937, 0.14702222403138876, 0.1474157520569861, 0.1483563119545579, 0.1495547431986779, 0.16930185398086905, 0.1732481000944972, 0.156068965094164, 0.14989513205364347, 0.14991803606972098, 0.16499281395226717, 0.151510989991948, 0.1494535889942199, 0.14990671095438302, 0.15029826783575118, 0.1488265059888363, 0.1664700168184936, 0.1492136309389025, 0.14949395298026502, 0.15032626106403768, 0.16210531000979245, 0.1582231088541448, 0.16055749100632966, 0.1531859531532973, 0.14931279607117176, 0.14861462078988552, 0.14807161502540112, 0.14820646704174578, 0.1486832939554006, 0.147998649161309, 0.14718796289525926, 0.14738202816806734, 0.14898529602214694, 0.14972713100723922, 0.1484984161797911, 0.15118066617287695, 0.1515466459095478, 0.15031557297334075, 0.14773980900645256, 0.150014272890985, 0.14981834217905998, 0.16500689298845828, 0.1481696730479598, 0.14798320387490094, 0.15004110289737582, 0.15958807803690434, 0.1597699411213398, 0.15954530611634254, 0.1577674369327724, 0.15784707595594227, 0.15964602190069854, 0.15826681512407959, 0.15612671105191112, 0.14847271796315908, 0.14937200886197388, 0.1492345209699124, 0.1480440280865878, 0.14825289091095328, 0.14723002188839018]
[0.001362731465492253, 0.0012881310923790286, 0.0013080181070249672, 0.0014006273727142071, 0.0014299978303556988, 0.0013167799212211787, 0.0013212345814866612, 0.0013107165203307025, 0.0012995957129800966, 0.0013217549074383437, 0.0013258802783928177, 0.0013246739521449388, 0.0012902256747520015, 0.001303560356790012, 0.0013082289541241272, 0.0013095625972216443, 0.001296206635545737, 0.00132458903213111, 0.0013148101252438717, 0.0012945809686744168, 0.0013234339845763852, 0.0013137090943984744, 0.0013043626280169385, 0.0012931393625489039, 0.0013144829223842123, 0.0013362489759835394, 0.0013157041010612896, 0.0013037895271193613, 0.0013155194952390915, 0.001334068502566611, 0.0013335018613657287, 0.001327576743828696, 0.001318706766261842, 0.001323671690224446, 0.0013388689993628004, 0.0013368319676712501, 0.0012993753487027663, 0.0013068034638499106, 0.0013317642787601365, 0.0014068987437112386, 0.001379646790437689, 0.0013922037134423506, 0.001300949435464518, 0.001295448526335779, 0.0013424402496293765, 0.0012521489844621383, 0.0012149282806921144, 0.0012204881320714719, 0.0012441570166671692, 0.0013210712246192519, 0.0013175062030236157, 0.001303964216882984, 0.0012282661014315925, 0.0012451121457238762, 0.0012435361945120863, 0.001404367333148108, 0.0013981904570995144, 0.0013631768055375694, 0.0013342389544515416, 0.0013352341700865085, 0.0014230952482641667, 0.0014633863330493833, 0.001357609054160326, 0.0012432184185861618, 0.001268436101647888, 0.0012502837835904933, 0.0012685945347788953, 0.0012410926044189423, 0.0012389640775039908, 0.0012265295187343461, 0.0011523218677458606, 0.0011652410698173813, 0.0011768108757227197, 0.0011737689686318238, 0.0011680030693523875, 0.0011469157982993033, 0.0011564970464091892, 0.0011496777833951998, 0.0011682536059542913, 0.0011565340159446463, 0.0011397071630340214, 0.0011427577678836131, 0.0011500489298802937, 0.0011593390945633946, 0.001312417472719915, 0.0013430085278643194, 0.0012098369387144496, 0.0011619777678577013, 0.00116215531836993, 0.0012790140616454819, 0.001174503798387194, 0.0011585549534435652, 0.001162067526778163, 0.0011651028514399317, 0.0011536938448746998, 0.0012904652466549892, 0.0011566948134798644, 0.001158867852560194, 0.0011653198532095945, 0.0012566303101534297, 0.00122653572755151, 0.0012446317132273617, 0.0011874880089402893, 0.00115746353543544, 0.001152051323952601, 0.001147841976941094, 0.001148887341408882, 0.0011525836740728728, 0.0011472763500876668, 0.0011409919604283664, 0.001142496342388119, 0.0011549247753654803, 0.0011606754341646451, 0.0011511505130216363, 0.0011719431486269531, 0.0011747802008492078, 0.001165236999793339, 0.0011452698372593222, 0.001162901340240194, 0.0011613824975120928, 0.0012791232014609168, 0.0011486021166508512, 0.0011471566191852787, 0.001163109324785859, 0.0012371168840070103, 0.0012385266753592232, 0.0012367853187313376, 0.001223003387075755, 0.0012236207438445137, 0.0012375660612457251, 0.0012268745358455782, 0.0012102845817977606, 0.001150951302039993, 0.001157922549317627, 0.0011568567517047474, 0.001147628124702231, 0.001149247216363979, 0.0011413179991348076]
[733.8202905873129, 776.3185019881138, 764.5154104742926, 713.9657695409372, 699.3017603049504, 759.4283478081913, 756.867867381123, 762.9414785644826, 769.470066738605, 756.5699165347319, 754.2159094576492, 754.9027429585822, 775.0582084736558, 767.12980322789, 764.392193619893, 763.6137456289532, 771.4819324150225, 754.9511401216391, 760.5660930048888, 772.4507189565345, 755.6100354488688, 761.203529962532, 766.6579665198856, 773.3118555983807, 760.7554141412486, 748.3635295315793, 760.0493144266766, 766.9949629135582, 760.1559715527083, 749.5866951930147, 749.9052149622296, 753.2521224468175, 758.3186994897396, 755.4743426071436, 746.8990621755554, 748.0371686069053, 769.600563069287, 765.2260096204125, 750.8836330487803, 710.7832063038978, 724.8231989020558, 718.2856864584917, 768.6693830978444, 771.9334112244001, 744.9121108191459, 798.6270103709351, 823.0938532687089, 819.3443047272825, 803.7570713371744, 756.9614577656188, 759.0097091801514, 766.8922099644847, 814.1558240795383, 803.1405070092106, 804.1583384650576, 712.0644124912421, 715.2101453148641, 733.5805567830575, 749.4909338867749, 748.9323014667989, 702.6936540050702, 683.3465486289014, 736.589076903656, 804.3638873507364, 788.3723891970991, 799.8184197256861, 788.2739303888701, 805.7416476735694, 807.1259031291639, 815.3085471859646, 867.8130893724787, 858.1915158180288, 849.7542133827288, 851.95641282426, 856.1621336786909, 871.9035882868154, 864.6801157900944, 869.8089277213176, 855.9785263261799, 864.6524755981425, 877.4183688886309, 875.0760905804207, 869.5282209462931, 862.5604059152328, 761.9526719097646, 744.5969100361717, 826.557669054626, 860.6016635272341, 860.4701834541587, 781.8522328937308, 851.4233852399464, 863.1442099726963, 860.5351900439936, 858.2933247173125, 866.7810827304949, 774.9143207010779, 864.5322762289778, 862.9111574635366, 858.1334963492981, 795.7789907820254, 815.3044200320725, 803.4505222488463, 842.113766599123, 863.95810268338, 868.0168836307358, 871.2000607129904, 870.4073619382896, 867.615967929088, 871.629577236197, 876.4303647017518, 875.2763250950424, 865.85725869769, 861.5673000089938, 868.6961337272188, 853.2837119032596, 851.2230622180513, 858.1945133714045, 873.1566723114277, 859.9181765440677, 861.0427676860935, 781.7855221904164, 870.6235044349802, 871.7205508609709, 859.7644079451524, 808.3310582271007, 807.4109503616135, 808.5477607591382, 817.6592236518951, 817.2466877752365, 808.0376727472691, 815.0792691371549, 826.2519534989009, 868.8464909223868, 863.6156197055735, 864.4112579421758, 871.3624025722312, 870.1348028179946, 876.1799960730176]
Elapsed: 0.16109767104532785~0.010672278305486093
Time per graph: 0.0012488191553901382~8.273083957741156e-05
Speed: 804.2392938937792~52.663965682722335
Total Time: 0.1479
best val loss: 0.2547702233400918 test_score: 0.9380

Testing...
Test loss: 0.1882 score: 0.9457 time: 0.14s
test Score 0.9457
Epoch Time List: [0.5947638039942831, 0.5726712427567691, 0.5779411110561341, 0.5986597770825028, 0.6263499979395419, 0.5768072491046041, 0.5815312471240759, 0.5841047447174788, 0.5818659430369735, 0.5882002140861005, 0.587894229684025, 0.6229953437577933, 0.5768196857534349, 0.5803671111352742, 0.5786438167560846, 0.5897593989502639, 0.5885763079859316, 0.5908631149213761, 0.5925244491081685, 0.5852697379887104, 0.5906268109101802, 0.5883225009310991, 0.5863647360820323, 0.5885428118053824, 0.5896384429652244, 0.5945145089644939, 0.5903863171115518, 0.5859684939496219, 0.587343490216881, 0.5917655311059207, 0.5896679221186787, 0.5866696420125663, 0.5873007269110531, 0.5902915550395846, 0.5864443930331618, 0.591356243006885, 0.5877397200092673, 0.5838204210158437, 0.5894534380640835, 0.5994851409923285, 0.6133372799959034, 0.613038441631943, 0.590622307965532, 0.5806565899401903, 0.5878265318460763, 0.5608040178194642, 0.5852622548118234, 0.5396953858435154, 0.5599909599404782, 0.6459756926633418, 0.5702726601157337, 0.5755739249289036, 0.5877885399386287, 0.5488113698083907, 0.5475070658139884, 0.6565292598679662, 0.5993187299463898, 0.608217166736722, 0.6144858910702169, 0.5876358018722385, 0.6030458197928965, 0.611588679952547, 0.5966590761672705, 0.5516297610010952, 0.5519676092080772, 0.5468440777622163, 0.5503343457821757, 0.547379887662828, 0.5474430199246854, 0.5469995348248631, 0.5421637559775263, 0.5502092689275742, 0.5482111091259867, 0.5456907800398767, 0.5415343591012061, 0.5363543811254203, 0.53888941090554, 0.5335299097932875, 0.5406029848381877, 0.5412684492766857, 0.5413859949912876, 0.5428136999253184, 0.5396855536382645, 0.5397384034004062, 0.5759241946507245, 0.5873848751652986, 0.5546878862660378, 0.5510553508065641, 0.5509041962213814, 0.5693822789471596, 0.5535992933437228, 0.5407866667956114, 0.5388985790777951, 0.5443223770707846, 0.5432135730516165, 0.584130611969158, 0.5440693537238985, 0.5406283701304346, 0.5397459839005023, 0.5644087179098278, 0.57000538893044, 0.5519295735284686, 0.5523225758224726, 0.541534153977409, 0.5405686388257891, 0.5394574196543545, 0.5400342990178615, 0.5405828049406409, 0.5402123220264912, 0.5364389060996473, 0.5351451491005719, 0.5374878370203078, 0.5357673850376159, 0.535140483174473, 0.5397128770127892, 0.5420290499459952, 0.5426934016868472, 0.5435431147925556, 0.5399022358469665, 0.541464988142252, 0.5809292043559253, 0.5681778921280056, 0.538223986979574, 0.5392129027750343, 0.563394787022844, 0.5630108993500471, 0.5647902800701559, 0.5669822371564806, 0.5637176739983261, 0.5674543010536581, 0.5659226118586957, 0.5628762478008866, 0.5344002314377576, 0.5341469801496714, 0.5356565739493817, 0.5336465048603714, 0.5338910950813442, 0.5317327948287129]
Total Epoch List: [70, 68]
Total Time List: [0.15873849904164672, 0.14786597504280508]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb23f11e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.5000 time: 0.16s
Epoch 2/1000, LR 0.000020
Train loss: 0.7027;  Loss pred: 0.7027; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.5000 time: 0.16s
Epoch 3/1000, LR 0.000050
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.5000 time: 0.26s
Epoch 4/1000, LR 0.000080
Train loss: 0.7003;  Loss pred: 0.7003; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000110
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.16s
Epoch 6/1000, LR 0.000140
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5039 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.5000 time: 0.15s
Epoch 7/1000, LR 0.000170
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.16s
Epoch 8/1000, LR 0.000200
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.5000 time: 0.15s
Epoch 11/1000, LR 0.000290
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 0.15s
Epoch 12/1000, LR 0.000290
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5000 time: 0.15s
Epoch 13/1000, LR 0.000290
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.15s
Epoch 14/1000, LR 0.000290
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.16s
Epoch 15/1000, LR 0.000290
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.15s
Epoch 18/1000, LR 0.000290
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.15s
Epoch 19/1000, LR 0.000290
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.15s
Epoch 20/1000, LR 0.000290
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5039 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5000 time: 0.15s
Epoch 21/1000, LR 0.000290
Train loss: 0.5771;  Loss pred: 0.5771; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.5039 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5000 time: 0.16s
Epoch 22/1000, LR 0.000290
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.23s
Val loss: 0.6755 score: 0.5116 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.5000 time: 0.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.5399;  Loss pred: 0.5399; Loss self: 0.0000; time: 0.23s
Val loss: 0.6680 score: 0.5349 time: 0.16s
Test loss: 0.6704 score: 0.5234 time: 0.15s
Epoch 24/1000, LR 0.000290
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.23s
Val loss: 0.6591 score: 0.5581 time: 0.16s
Test loss: 0.6622 score: 0.5469 time: 0.15s
Epoch 25/1000, LR 0.000290
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.23s
Val loss: 0.6458 score: 0.5736 time: 0.16s
Test loss: 0.6498 score: 0.5703 time: 0.15s
Epoch 26/1000, LR 0.000290
Train loss: 0.4832;  Loss pred: 0.4832; Loss self: 0.0000; time: 0.23s
Val loss: 0.6308 score: 0.6279 time: 0.16s
Test loss: 0.6361 score: 0.5859 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.4656;  Loss pred: 0.4656; Loss self: 0.0000; time: 0.26s
Val loss: 0.6144 score: 0.6744 time: 0.16s
Test loss: 0.6210 score: 0.6406 time: 0.15s
Epoch 28/1000, LR 0.000290
Train loss: 0.4516;  Loss pred: 0.4516; Loss self: 0.0000; time: 0.23s
Val loss: 0.5917 score: 0.7597 time: 0.16s
Test loss: 0.5990 score: 0.6875 time: 0.15s
Epoch 29/1000, LR 0.000290
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.23s
Val loss: 0.5717 score: 0.7829 time: 0.16s
Test loss: 0.5801 score: 0.7344 time: 0.15s
Epoch 30/1000, LR 0.000290
Train loss: 0.4137;  Loss pred: 0.4137; Loss self: 0.0000; time: 0.23s
Val loss: 0.5451 score: 0.8527 time: 0.15s
Test loss: 0.5541 score: 0.8359 time: 0.15s
Epoch 31/1000, LR 0.000290
Train loss: 0.3903;  Loss pred: 0.3903; Loss self: 0.0000; time: 0.23s
Val loss: 0.5211 score: 0.8760 time: 0.15s
Test loss: 0.5311 score: 0.8438 time: 0.15s
Epoch 32/1000, LR 0.000290
Train loss: 0.3686;  Loss pred: 0.3686; Loss self: 0.0000; time: 0.23s
Val loss: 0.4922 score: 0.8837 time: 0.16s
Test loss: 0.5028 score: 0.8828 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.23s
Val loss: 0.4617 score: 0.8682 time: 0.16s
Test loss: 0.4717 score: 0.8984 time: 0.15s
Epoch 34/1000, LR 0.000290
Train loss: 0.3236;  Loss pred: 0.3236; Loss self: 0.0000; time: 0.23s
Val loss: 0.4331 score: 0.8837 time: 0.16s
Test loss: 0.4418 score: 0.8984 time: 0.15s
Epoch 35/1000, LR 0.000290
Train loss: 0.3012;  Loss pred: 0.3012; Loss self: 0.0000; time: 0.23s
Val loss: 0.4076 score: 0.8760 time: 0.16s
Test loss: 0.4164 score: 0.9062 time: 0.15s
Epoch 36/1000, LR 0.000290
Train loss: 0.2840;  Loss pred: 0.2840; Loss self: 0.0000; time: 0.23s
Val loss: 0.3880 score: 0.8837 time: 0.16s
Test loss: 0.3973 score: 0.9062 time: 0.15s
Epoch 37/1000, LR 0.000290
Train loss: 0.2498;  Loss pred: 0.2498; Loss self: 0.0000; time: 0.23s
Val loss: 0.3763 score: 0.8915 time: 0.17s
Test loss: 0.3832 score: 0.8984 time: 0.16s
Epoch 38/1000, LR 0.000289
Train loss: 0.2479;  Loss pred: 0.2479; Loss self: 0.0000; time: 0.23s
Val loss: 0.3576 score: 0.9070 time: 0.16s
Test loss: 0.3634 score: 0.9141 time: 0.16s
Epoch 39/1000, LR 0.000289
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 0.23s
Val loss: 0.3541 score: 0.8915 time: 0.16s
Test loss: 0.3575 score: 0.8984 time: 0.15s
Epoch 40/1000, LR 0.000289
Train loss: 0.2106;  Loss pred: 0.2106; Loss self: 0.0000; time: 0.24s
Val loss: 0.3374 score: 0.8837 time: 0.17s
Test loss: 0.3414 score: 0.9141 time: 0.16s
Epoch 41/1000, LR 0.000289
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.24s
Val loss: 0.3409 score: 0.9070 time: 0.17s
Test loss: 0.3424 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.1787;  Loss pred: 0.1787; Loss self: 0.0000; time: 0.23s
Val loss: 0.3284 score: 0.9070 time: 0.16s
Test loss: 0.3293 score: 0.8984 time: 0.15s
Epoch 43/1000, LR 0.000289
Train loss: 0.1739;  Loss pred: 0.1739; Loss self: 0.0000; time: 0.23s
Val loss: 0.3158 score: 0.8760 time: 0.16s
Test loss: 0.3175 score: 0.9297 time: 0.16s
Epoch 44/1000, LR 0.000289
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 0.23s
Val loss: 0.3063 score: 0.8760 time: 0.16s
Test loss: 0.3054 score: 0.9062 time: 0.16s
Epoch 45/1000, LR 0.000289
Train loss: 0.1553;  Loss pred: 0.1553; Loss self: 0.0000; time: 0.23s
Val loss: 0.3196 score: 0.9147 time: 0.16s
Test loss: 0.3192 score: 0.8984 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.24s
Val loss: 0.3117 score: 0.9147 time: 0.17s
Test loss: 0.3078 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.24s
Val loss: 0.2979 score: 0.8915 time: 0.17s
Test loss: 0.2900 score: 0.9219 time: 0.16s
Epoch 48/1000, LR 0.000289
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.23s
Val loss: 0.3280 score: 0.8992 time: 0.16s
Test loss: 0.3192 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000289
Train loss: 0.1315;  Loss pred: 0.1315; Loss self: 0.0000; time: 0.23s
Val loss: 0.3552 score: 0.8915 time: 0.16s
Test loss: 0.3434 score: 0.8984 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000289
Train loss: 0.1241;  Loss pred: 0.1241; Loss self: 0.0000; time: 0.23s
Val loss: 0.3279 score: 0.8992 time: 0.16s
Test loss: 0.3116 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000289
Train loss: 0.1120;  Loss pred: 0.1120; Loss self: 0.0000; time: 0.23s
Val loss: 0.3227 score: 0.9147 time: 0.16s
Test loss: 0.3044 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000289
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.23s
Val loss: 0.3477 score: 0.9147 time: 0.16s
Test loss: 0.3309 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000289
Train loss: 0.1073;  Loss pred: 0.1073; Loss self: 0.0000; time: 0.23s
Val loss: 0.3490 score: 0.9147 time: 0.16s
Test loss: 0.3299 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.23s
Val loss: 0.3430 score: 0.9147 time: 0.16s
Test loss: 0.3262 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.23s
Val loss: 0.3605 score: 0.9070 time: 0.16s
Test loss: 0.3460 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000289
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.23s
Val loss: 0.3829 score: 0.8992 time: 0.16s
Test loss: 0.3708 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.1046;  Loss pred: 0.1046; Loss self: 0.0000; time: 0.23s
Val loss: 0.3483 score: 0.9070 time: 0.16s
Test loss: 0.3325 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 0.23s
Val loss: 0.3020 score: 0.8760 time: 0.15s
Test loss: 0.2752 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.23s
Val loss: 0.3181 score: 0.9225 time: 0.15s
Test loss: 0.2921 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 60/1000, LR 0.000288
Train loss: 0.0835;  Loss pred: 0.0835; Loss self: 0.0000; time: 0.23s
Val loss: 0.3118 score: 0.8915 time: 0.16s
Test loss: 0.2848 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0841;  Loss pred: 0.0841; Loss self: 0.0000; time: 0.25s
Val loss: 0.3009 score: 0.8837 time: 0.17s
Test loss: 0.2700 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.24s
Val loss: 0.3149 score: 0.8837 time: 0.17s
Test loss: 0.2872 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.24s
Val loss: 0.3358 score: 0.9147 time: 0.17s
Test loss: 0.3097 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 64/1000, LR 0.000288
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.24s
Val loss: 0.3195 score: 0.8992 time: 0.16s
Test loss: 0.2908 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 65/1000, LR 0.000288
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.23s
Val loss: 0.3471 score: 0.9147 time: 0.16s
Test loss: 0.3226 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.23s
Val loss: 0.3739 score: 0.8992 time: 0.16s
Test loss: 0.3544 score: 0.8984 time: 0.16s
     INFO: Early stopping counter 19 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.23s
Val loss: 0.3652 score: 0.8992 time: 0.16s
Test loss: 0.3501 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 046,   Train_Loss: 0.1489,   Val_Loss: 0.2979,   Val_Precision: 0.9474,   Val_Recall: 0.8308,   Val_accuracy: 0.8852,   Val_Score: 0.8915,   Val_Loss: 0.2979,   Test_Precision: 0.9821,   Test_Recall: 0.8594,   Test_accuracy: 0.9167,   Test_Score: 0.9219,   Test_loss: 0.2900


[0.17579235904850066, 0.16616891091689467, 0.16873433580622077, 0.18068093108013272, 0.18446972011588514, 0.16986460983753204, 0.1704392610117793, 0.16908243112266064, 0.16764784697443247, 0.17050638305954635, 0.17103855591267347, 0.1708829398266971, 0.1664391120430082, 0.16815928602591157, 0.16876153508201241, 0.16893357504159212, 0.16721065598540008, 0.1708719851449132, 0.16961050615645945, 0.16700094495899975, 0.17072298401035368, 0.1694684731774032, 0.16826277901418507, 0.1668149777688086, 0.16956829698756337, 0.17237611790187657, 0.16972582903690636, 0.1681888489983976, 0.1697020148858428, 0.17209483683109283, 0.172021740116179, 0.17125739995390177, 0.1701131728477776, 0.17075364803895354, 0.17271410091780126, 0.17245132382959127, 0.16761941998265684, 0.16857764683663845, 0.17179759196005762, 0.1814899379387498, 0.1779744359664619, 0.17959427903406322, 0.16782247717492282, 0.1671128598973155, 0.17317479220218956, 0.16152721899561584, 0.15672574820928276, 0.15744296903721988, 0.16049625515006483, 0.17041818797588348, 0.16995830019004643, 0.16821138397790492, 0.15844632708467543, 0.16061946679838002, 0.16041616909205914, 0.18116338597610593, 0.18036656896583736, 0.17584980791434646, 0.17211682512424886, 0.1722452079411596, 0.1835792870260775, 0.18877683696337044, 0.17513156798668206, 0.16037517599761486, 0.16362825711257756, 0.16128660808317363, 0.1636486949864775, 0.16010094597004354, 0.1598263659980148, 0.15822230791673064, 0.14864952093921602, 0.1503160980064422, 0.15180860296823084, 0.15141619695350528, 0.15067239594645798, 0.14795213798061013, 0.1491881189867854, 0.14830843405798078, 0.15070471516810358, 0.14919288805685937, 0.14702222403138876, 0.1474157520569861, 0.1483563119545579, 0.1495547431986779, 0.16930185398086905, 0.1732481000944972, 0.156068965094164, 0.14989513205364347, 0.14991803606972098, 0.16499281395226717, 0.151510989991948, 0.1494535889942199, 0.14990671095438302, 0.15029826783575118, 0.1488265059888363, 0.1664700168184936, 0.1492136309389025, 0.14949395298026502, 0.15032626106403768, 0.16210531000979245, 0.1582231088541448, 0.16055749100632966, 0.1531859531532973, 0.14931279607117176, 0.14861462078988552, 0.14807161502540112, 0.14820646704174578, 0.1486832939554006, 0.147998649161309, 0.14718796289525926, 0.14738202816806734, 0.14898529602214694, 0.14972713100723922, 0.1484984161797911, 0.15118066617287695, 0.1515466459095478, 0.15031557297334075, 0.14773980900645256, 0.150014272890985, 0.14981834217905998, 0.16500689298845828, 0.1481696730479598, 0.14798320387490094, 0.15004110289737582, 0.15958807803690434, 0.1597699411213398, 0.15954530611634254, 0.1577674369327724, 0.15784707595594227, 0.15964602190069854, 0.15826681512407959, 0.15612671105191112, 0.14847271796315908, 0.14937200886197388, 0.1492345209699124, 0.1480440280865878, 0.14825289091095328, 0.14723002188839018, 0.16063844389282167, 0.16457372694276273, 0.2622280758805573, 0.16125054913572967, 0.15972726908512414, 0.15795707213692367, 0.16146914404816926, 0.15986965782940388, 0.17337769200094044, 0.1594034950248897, 0.15699327900074422, 0.15713594411499798, 0.1581521260086447, 0.15978816780261695, 0.15953589393757284, 0.15889523597434163, 0.1574714039452374, 0.15675469604320824, 0.15619989018887281, 0.15874999994412065, 0.16307447594590485, 0.1561490420717746, 0.15838928404264152, 0.1588784910272807, 0.15805545216426253, 0.17379166395403445, 0.15616138582117856, 0.15497383405454457, 0.155152227031067, 0.15588071709498763, 0.15829349285922945, 0.15818442008458078, 0.15689322003163397, 0.15913859009742737, 0.15835883398540318, 0.1582266071345657, 0.16883317288011312, 0.16028932202607393, 0.15907290391623974, 0.1606331409420818, 0.16055520088411868, 0.15739274001680315, 0.16003116499632597, 0.16109956987202168, 0.17072024219669402, 0.16486085206270218, 0.162186297820881, 0.15921311010606587, 0.15901190601289272, 0.15804269397631288, 0.1576103640254587, 0.15749281784519553, 0.15911365882493556, 0.15783812082372606, 0.1618330110795796, 0.15739443292841315, 0.1564233209937811, 0.1569626380223781, 0.15840815496630967, 0.16035119607113302, 0.1685166370589286, 0.16822981694713235, 0.16898087784647942, 0.1591764169279486, 0.15894571784883738, 0.1596741760149598, 0.15824766410514712]
[0.001362731465492253, 0.0012881310923790286, 0.0013080181070249672, 0.0014006273727142071, 0.0014299978303556988, 0.0013167799212211787, 0.0013212345814866612, 0.0013107165203307025, 0.0012995957129800966, 0.0013217549074383437, 0.0013258802783928177, 0.0013246739521449388, 0.0012902256747520015, 0.001303560356790012, 0.0013082289541241272, 0.0013095625972216443, 0.001296206635545737, 0.00132458903213111, 0.0013148101252438717, 0.0012945809686744168, 0.0013234339845763852, 0.0013137090943984744, 0.0013043626280169385, 0.0012931393625489039, 0.0013144829223842123, 0.0013362489759835394, 0.0013157041010612896, 0.0013037895271193613, 0.0013155194952390915, 0.001334068502566611, 0.0013335018613657287, 0.001327576743828696, 0.001318706766261842, 0.001323671690224446, 0.0013388689993628004, 0.0013368319676712501, 0.0012993753487027663, 0.0013068034638499106, 0.0013317642787601365, 0.0014068987437112386, 0.001379646790437689, 0.0013922037134423506, 0.001300949435464518, 0.001295448526335779, 0.0013424402496293765, 0.0012521489844621383, 0.0012149282806921144, 0.0012204881320714719, 0.0012441570166671692, 0.0013210712246192519, 0.0013175062030236157, 0.001303964216882984, 0.0012282661014315925, 0.0012451121457238762, 0.0012435361945120863, 0.001404367333148108, 0.0013981904570995144, 0.0013631768055375694, 0.0013342389544515416, 0.0013352341700865085, 0.0014230952482641667, 0.0014633863330493833, 0.001357609054160326, 0.0012432184185861618, 0.001268436101647888, 0.0012502837835904933, 0.0012685945347788953, 0.0012410926044189423, 0.0012389640775039908, 0.0012265295187343461, 0.0011523218677458606, 0.0011652410698173813, 0.0011768108757227197, 0.0011737689686318238, 0.0011680030693523875, 0.0011469157982993033, 0.0011564970464091892, 0.0011496777833951998, 0.0011682536059542913, 0.0011565340159446463, 0.0011397071630340214, 0.0011427577678836131, 0.0011500489298802937, 0.0011593390945633946, 0.001312417472719915, 0.0013430085278643194, 0.0012098369387144496, 0.0011619777678577013, 0.00116215531836993, 0.0012790140616454819, 0.001174503798387194, 0.0011585549534435652, 0.001162067526778163, 0.0011651028514399317, 0.0011536938448746998, 0.0012904652466549892, 0.0011566948134798644, 0.001158867852560194, 0.0011653198532095945, 0.0012566303101534297, 0.00122653572755151, 0.0012446317132273617, 0.0011874880089402893, 0.00115746353543544, 0.001152051323952601, 0.001147841976941094, 0.001148887341408882, 0.0011525836740728728, 0.0011472763500876668, 0.0011409919604283664, 0.001142496342388119, 0.0011549247753654803, 0.0011606754341646451, 0.0011511505130216363, 0.0011719431486269531, 0.0011747802008492078, 0.001165236999793339, 0.0011452698372593222, 0.001162901340240194, 0.0011613824975120928, 0.0012791232014609168, 0.0011486021166508512, 0.0011471566191852787, 0.001163109324785859, 0.0012371168840070103, 0.0012385266753592232, 0.0012367853187313376, 0.001223003387075755, 0.0012236207438445137, 0.0012375660612457251, 0.0012268745358455782, 0.0012102845817977606, 0.001150951302039993, 0.001157922549317627, 0.0011568567517047474, 0.001147628124702231, 0.001149247216363979, 0.0011413179991348076, 0.0012549878429126693, 0.0012857322417403338, 0.002048656842816854, 0.001259769915122888, 0.0012478692897275323, 0.0012340396260697162, 0.0012614776878763223, 0.0012489817017922178, 0.0013545132187573472, 0.0012453398048819508, 0.0012265099921933142, 0.0012276245633984217, 0.0012355634844425367, 0.001248345060957945, 0.0012463741713872878, 0.001241369031049544, 0.0012302453433221672, 0.0012246460628375644, 0.0012203116421005689, 0.0012402343745634425, 0.0012740193433273816, 0.001219914391185739, 0.0012374162815831369, 0.0012412382111506304, 0.001234808220033301, 0.0013577473746408941, 0.0012200108267279575, 0.0012107330785511294, 0.001212126773680211, 0.0012178181023045909, 0.00123666791296273, 0.0012358157819107873, 0.0012257282814971404, 0.0012432702351361513, 0.0012371783905109623, 0.0012361453682387946, 0.0013190091631258838, 0.0012522603283287026, 0.001242757061845623, 0.0012549464136100141, 0.0012543375069071772, 0.0012296307813812746, 0.0012502434765337966, 0.0012585903896251693, 0.001333751892161672, 0.0012879754067398608, 0.0012670804517256329, 0.0012438524227036396, 0.0012422805157257244, 0.0012347085466899443, 0.001231330968948896, 0.00123041263941559, 0.001243075459569809, 0.0012331103189353598, 0.0012643203990592156, 0.0012296440072532278, 0.0012220571952639148, 0.0012262706095498288, 0.0012375637106742943, 0.0012527437193057267, 0.0013165362270228798, 0.0013142954448994715, 0.0013201631081756204, 0.0012435657572495984, 0.001241763420694042, 0.0012474545001168735, 0.001236309875821462]
[733.8202905873129, 776.3185019881138, 764.5154104742926, 713.9657695409372, 699.3017603049504, 759.4283478081913, 756.867867381123, 762.9414785644826, 769.470066738605, 756.5699165347319, 754.2159094576492, 754.9027429585822, 775.0582084736558, 767.12980322789, 764.392193619893, 763.6137456289532, 771.4819324150225, 754.9511401216391, 760.5660930048888, 772.4507189565345, 755.6100354488688, 761.203529962532, 766.6579665198856, 773.3118555983807, 760.7554141412486, 748.3635295315793, 760.0493144266766, 766.9949629135582, 760.1559715527083, 749.5866951930147, 749.9052149622296, 753.2521224468175, 758.3186994897396, 755.4743426071436, 746.8990621755554, 748.0371686069053, 769.600563069287, 765.2260096204125, 750.8836330487803, 710.7832063038978, 724.8231989020558, 718.2856864584917, 768.6693830978444, 771.9334112244001, 744.9121108191459, 798.6270103709351, 823.0938532687089, 819.3443047272825, 803.7570713371744, 756.9614577656188, 759.0097091801514, 766.8922099644847, 814.1558240795383, 803.1405070092106, 804.1583384650576, 712.0644124912421, 715.2101453148641, 733.5805567830575, 749.4909338867749, 748.9323014667989, 702.6936540050702, 683.3465486289014, 736.589076903656, 804.3638873507364, 788.3723891970991, 799.8184197256861, 788.2739303888701, 805.7416476735694, 807.1259031291639, 815.3085471859646, 867.8130893724787, 858.1915158180288, 849.7542133827288, 851.95641282426, 856.1621336786909, 871.9035882868154, 864.6801157900944, 869.8089277213176, 855.9785263261799, 864.6524755981425, 877.4183688886309, 875.0760905804207, 869.5282209462931, 862.5604059152328, 761.9526719097646, 744.5969100361717, 826.557669054626, 860.6016635272341, 860.4701834541587, 781.8522328937308, 851.4233852399464, 863.1442099726963, 860.5351900439936, 858.2933247173125, 866.7810827304949, 774.9143207010779, 864.5322762289778, 862.9111574635366, 858.1334963492981, 795.7789907820254, 815.3044200320725, 803.4505222488463, 842.113766599123, 863.95810268338, 868.0168836307358, 871.2000607129904, 870.4073619382896, 867.615967929088, 871.629577236197, 876.4303647017518, 875.2763250950424, 865.85725869769, 861.5673000089938, 868.6961337272188, 853.2837119032596, 851.2230622180513, 858.1945133714045, 873.1566723114277, 859.9181765440677, 861.0427676860935, 781.7855221904164, 870.6235044349802, 871.7205508609709, 859.7644079451524, 808.3310582271007, 807.4109503616135, 808.5477607591382, 817.6592236518951, 817.2466877752365, 808.0376727472691, 815.0792691371549, 826.2519534989009, 868.8464909223868, 863.6156197055735, 864.4112579421758, 871.3624025722312, 870.1348028179946, 876.1799960730176, 796.8204677418432, 777.7669156421138, 488.1246966793248, 793.7957463466271, 801.3659829855628, 810.3467497108603, 792.7211155700139, 800.6522421946269, 738.2726031403492, 802.9936858035247, 815.3215272317053, 814.5812896018546, 809.347324189644, 801.0605651234188, 802.3272809696796, 805.5622260485482, 812.8459948481412, 816.5624586118796, 819.4628040085415, 806.2992128822392, 784.9174388422393, 819.7296525275143, 808.1354794528895, 805.6471280182375, 809.8423575225563, 736.5140369094704, 819.6648571406356, 825.9458816444403, 824.9962146812745, 821.1406926105029, 808.624522006287, 809.1820922159005, 815.8415001884192, 804.3303633746925, 808.29087193076, 808.9663446498658, 758.1448468713646, 798.556000999109, 804.662496558174, 796.8467730214646, 797.2335950199738, 813.2522503028716, 799.8442053642405, 794.5396756905301, 749.7646345447764, 776.4123404585902, 789.2158691566136, 803.9538949696287, 804.9711698293946, 809.9077330280411, 812.129334206247, 812.7354742348638, 804.4563926521965, 810.9574501520495, 790.9387531389218, 813.2435030800457, 818.2923056919939, 815.4806877146847, 808.0392074967549, 798.247865536458, 759.5689199235543, 760.8639319879013, 757.4821579296629, 804.1392215653362, 805.3063758643201, 801.6324442344874, 808.858700845978]
Elapsed: 0.16126045582747858~0.011500235469944773
Time per graph: 0.001253279588459957~8.965269024789712e-05
Speed: 801.3790297193069~49.74996242709414
Total Time: 0.1590
best val loss: 0.2978790790535683 test_score: 0.9219

Testing...
Test loss: 0.2921 score: 0.9219 time: 0.15s
test Score 0.9219
Epoch Time List: [0.5947638039942831, 0.5726712427567691, 0.5779411110561341, 0.5986597770825028, 0.6263499979395419, 0.5768072491046041, 0.5815312471240759, 0.5841047447174788, 0.5818659430369735, 0.5882002140861005, 0.587894229684025, 0.6229953437577933, 0.5768196857534349, 0.5803671111352742, 0.5786438167560846, 0.5897593989502639, 0.5885763079859316, 0.5908631149213761, 0.5925244491081685, 0.5852697379887104, 0.5906268109101802, 0.5883225009310991, 0.5863647360820323, 0.5885428118053824, 0.5896384429652244, 0.5945145089644939, 0.5903863171115518, 0.5859684939496219, 0.587343490216881, 0.5917655311059207, 0.5896679221186787, 0.5866696420125663, 0.5873007269110531, 0.5902915550395846, 0.5864443930331618, 0.591356243006885, 0.5877397200092673, 0.5838204210158437, 0.5894534380640835, 0.5994851409923285, 0.6133372799959034, 0.613038441631943, 0.590622307965532, 0.5806565899401903, 0.5878265318460763, 0.5608040178194642, 0.5852622548118234, 0.5396953858435154, 0.5599909599404782, 0.6459756926633418, 0.5702726601157337, 0.5755739249289036, 0.5877885399386287, 0.5488113698083907, 0.5475070658139884, 0.6565292598679662, 0.5993187299463898, 0.608217166736722, 0.6144858910702169, 0.5876358018722385, 0.6030458197928965, 0.611588679952547, 0.5966590761672705, 0.5516297610010952, 0.5519676092080772, 0.5468440777622163, 0.5503343457821757, 0.547379887662828, 0.5474430199246854, 0.5469995348248631, 0.5421637559775263, 0.5502092689275742, 0.5482111091259867, 0.5456907800398767, 0.5415343591012061, 0.5363543811254203, 0.53888941090554, 0.5335299097932875, 0.5406029848381877, 0.5412684492766857, 0.5413859949912876, 0.5428136999253184, 0.5396855536382645, 0.5397384034004062, 0.5759241946507245, 0.5873848751652986, 0.5546878862660378, 0.5510553508065641, 0.5509041962213814, 0.5693822789471596, 0.5535992933437228, 0.5407866667956114, 0.5388985790777951, 0.5443223770707846, 0.5432135730516165, 0.584130611969158, 0.5440693537238985, 0.5406283701304346, 0.5397459839005023, 0.5644087179098278, 0.57000538893044, 0.5519295735284686, 0.5523225758224726, 0.541534153977409, 0.5405686388257891, 0.5394574196543545, 0.5400342990178615, 0.5405828049406409, 0.5402123220264912, 0.5364389060996473, 0.5351451491005719, 0.5374878370203078, 0.5357673850376159, 0.535140483174473, 0.5397128770127892, 0.5420290499459952, 0.5426934016868472, 0.5435431147925556, 0.5399022358469665, 0.541464988142252, 0.5809292043559253, 0.5681778921280056, 0.538223986979574, 0.5392129027750343, 0.563394787022844, 0.5630108993500471, 0.5647902800701559, 0.5669822371564806, 0.5637176739983261, 0.5674543010536581, 0.5659226118586957, 0.5628762478008866, 0.5344002314377576, 0.5341469801496714, 0.5356565739493817, 0.5336465048603714, 0.5338910950813442, 0.5317327948287129, 0.5905305272899568, 0.5650655152276158, 0.6640858491882682, 0.5483319438062608, 0.5565384251531214, 0.6472798991017044, 0.5532136037945747, 0.5496655630413443, 0.637266888981685, 0.5508457501418889, 0.5392824180889875, 0.5416286536492407, 0.5437956650275737, 0.5431783238891512, 0.5451148299034685, 0.5418774660211056, 0.5478260049130768, 0.5434889090247452, 0.5395438799168915, 0.5439043019432575, 0.545293030096218, 0.54154237289913, 0.5433720839209855, 0.5405985950492322, 0.5459147810470313, 0.5553904487751424, 0.5674588361289352, 0.5370799677912146, 0.5356371118687093, 0.537438222207129, 0.5383359303232282, 0.5405406409408897, 0.5366549049504101, 0.5394657230935991, 0.5408374040853232, 0.5395272502209991, 0.5592157347127795, 0.5470679539721459, 0.5436031762510538, 0.5628363189753145, 0.5653592192102224, 0.5471759240608662, 0.5436509510036558, 0.5432274888735265, 0.554377470863983, 0.5698006018064916, 0.5601855358108878, 0.5431625070050359, 0.5414452769327909, 0.5378979770466685, 0.5428503940347582, 0.5372829022817314, 0.5461502999532968, 0.538619328988716, 0.5441563359927386, 0.5403604602906853, 0.5415614498779178, 0.5364723899401724, 0.5427131557371467, 0.5396370152011514, 0.5795800038613379, 0.5735075667034835, 0.5760267158038914, 0.5501136828679591, 0.5417032472323626, 0.5438954241108149, 0.5419560258742422]
Total Epoch List: [70, 68, 67]
Total Time List: [0.15873849904164672, 0.14786597504280508, 0.15899665211327374]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb23f1d50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5039 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.15s
Epoch 10/1000, LR 0.000255
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.15s
Epoch 12/1000, LR 0.000285
Train loss: 0.6495;  Loss pred: 0.6495; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.15s
Epoch 13/1000, LR 0.000285
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5039 time: 0.16s
Epoch 14/1000, LR 0.000285
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5039 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5039 time: 0.15s
Epoch 16/1000, LR 0.000285
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5039 time: 0.16s
Epoch 17/1000, LR 0.000285
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5039 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5039 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5039 time: 0.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.5084;  Loss pred: 0.5084; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.4961 time: 0.17s
Test loss: 0.6827 score: 0.5116 time: 0.16s
Epoch 21/1000, LR 0.000285
Train loss: 0.4832;  Loss pred: 0.4832; Loss self: 0.0000; time: 0.22s
Val loss: 0.6784 score: 0.5039 time: 0.17s
Test loss: 0.6795 score: 0.5271 time: 0.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.4581;  Loss pred: 0.4581; Loss self: 0.0000; time: 0.22s
Val loss: 0.6737 score: 0.5039 time: 0.17s
Test loss: 0.6753 score: 0.5349 time: 0.16s
Epoch 23/1000, LR 0.000285
Train loss: 0.4309;  Loss pred: 0.4309; Loss self: 0.0000; time: 0.22s
Val loss: 0.6683 score: 0.5039 time: 0.17s
Test loss: 0.6704 score: 0.5349 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.4036;  Loss pred: 0.4036; Loss self: 0.0000; time: 0.22s
Val loss: 0.6617 score: 0.5349 time: 0.17s
Test loss: 0.6644 score: 0.5426 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3823;  Loss pred: 0.3823; Loss self: 0.0000; time: 0.22s
Val loss: 0.6536 score: 0.5504 time: 0.17s
Test loss: 0.6571 score: 0.5504 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.22s
Val loss: 0.6451 score: 0.5736 time: 0.17s
Test loss: 0.6493 score: 0.5659 time: 0.16s
Epoch 27/1000, LR 0.000285
Train loss: 0.3163;  Loss pred: 0.3163; Loss self: 0.0000; time: 0.22s
Val loss: 0.6360 score: 0.5736 time: 0.17s
Test loss: 0.6410 score: 0.5736 time: 0.16s
Epoch 28/1000, LR 0.000285
Train loss: 0.2870;  Loss pred: 0.2870; Loss self: 0.0000; time: 0.22s
Val loss: 0.6241 score: 0.5891 time: 0.17s
Test loss: 0.6300 score: 0.5736 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 0.2571;  Loss pred: 0.2571; Loss self: 0.0000; time: 0.22s
Val loss: 0.6100 score: 0.6124 time: 0.16s
Test loss: 0.6172 score: 0.5891 time: 0.15s
Epoch 30/1000, LR 0.000285
Train loss: 0.2363;  Loss pred: 0.2363; Loss self: 0.0000; time: 0.21s
Val loss: 0.5944 score: 0.6202 time: 0.16s
Test loss: 0.6032 score: 0.5969 time: 0.15s
Epoch 31/1000, LR 0.000285
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.21s
Val loss: 0.5760 score: 0.6202 time: 0.16s
Test loss: 0.5871 score: 0.5969 time: 0.15s
Epoch 32/1000, LR 0.000285
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.21s
Val loss: 0.5550 score: 0.6512 time: 0.16s
Test loss: 0.5690 score: 0.6202 time: 0.15s
Epoch 33/1000, LR 0.000285
Train loss: 0.1732;  Loss pred: 0.1732; Loss self: 0.0000; time: 0.21s
Val loss: 0.5317 score: 0.6667 time: 0.16s
Test loss: 0.5494 score: 0.6512 time: 0.15s
Epoch 34/1000, LR 0.000285
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 0.21s
Val loss: 0.5048 score: 0.6899 time: 0.22s
Test loss: 0.5269 score: 0.6667 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.1283;  Loss pred: 0.1283; Loss self: 0.0000; time: 0.22s
Val loss: 0.4758 score: 0.7209 time: 0.17s
Test loss: 0.5026 score: 0.6899 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.22s
Val loss: 0.4429 score: 0.7752 time: 0.17s
Test loss: 0.4750 score: 0.7442 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 0.22s
Val loss: 0.4141 score: 0.7984 time: 0.24s
Test loss: 0.4507 score: 0.7674 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 0.21s
Val loss: 0.3864 score: 0.8217 time: 0.27s
Test loss: 0.4272 score: 0.7829 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.21s
Val loss: 0.3569 score: 0.8682 time: 0.16s
Test loss: 0.4014 score: 0.8140 time: 0.20s
Epoch 40/1000, LR 0.000284
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.21s
Val loss: 0.3244 score: 0.8682 time: 0.16s
Test loss: 0.3738 score: 0.8527 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.21s
Val loss: 0.2890 score: 0.8915 time: 0.16s
Test loss: 0.3439 score: 0.8915 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.21s
Val loss: 0.2560 score: 0.9457 time: 0.16s
Test loss: 0.3181 score: 0.8760 time: 0.25s
Epoch 43/1000, LR 0.000284
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.22s
Val loss: 0.2333 score: 0.9457 time: 0.16s
Test loss: 0.3012 score: 0.8605 time: 0.16s
Epoch 44/1000, LR 0.000284
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.22s
Val loss: 0.2156 score: 0.9380 time: 0.16s
Test loss: 0.2890 score: 0.8605 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.21s
Val loss: 0.2021 score: 0.9302 time: 0.16s
Test loss: 0.2808 score: 0.8682 time: 0.16s
Epoch 46/1000, LR 0.000284
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.21s
Val loss: 0.1955 score: 0.9302 time: 0.16s
Test loss: 0.2785 score: 0.8682 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.21s
Val loss: 0.1887 score: 0.9380 time: 0.16s
Test loss: 0.2764 score: 0.8682 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.21s
Val loss: 0.1821 score: 0.9302 time: 0.16s
Test loss: 0.2760 score: 0.8682 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.22s
Val loss: 0.1748 score: 0.9302 time: 0.16s
Test loss: 0.2779 score: 0.8682 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.21s
Val loss: 0.1696 score: 0.9302 time: 0.16s
Test loss: 0.2845 score: 0.8527 time: 0.16s
Epoch 51/1000, LR 0.000284
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.22s
Val loss: 0.1656 score: 0.9380 time: 0.18s
Test loss: 0.2962 score: 0.8450 time: 0.16s
Epoch 52/1000, LR 0.000284
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.22s
Val loss: 0.1643 score: 0.9380 time: 0.17s
Test loss: 0.3079 score: 0.8527 time: 0.16s
Epoch 53/1000, LR 0.000284
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.22s
Val loss: 0.1643 score: 0.9380 time: 0.16s
Test loss: 0.3224 score: 0.8527 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.22s
Val loss: 0.1657 score: 0.9380 time: 0.16s
Test loss: 0.3374 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.22s
Val loss: 0.1682 score: 0.9380 time: 0.17s
Test loss: 0.3543 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.22s
Val loss: 0.1697 score: 0.9380 time: 0.16s
Test loss: 0.3668 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.22s
Val loss: 0.1713 score: 0.9380 time: 0.16s
Test loss: 0.3767 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.21s
Val loss: 0.1706 score: 0.9380 time: 0.16s
Test loss: 0.3894 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.22s
Val loss: 0.1701 score: 0.9380 time: 0.17s
Test loss: 0.3953 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.22s
Val loss: 0.1712 score: 0.9380 time: 0.17s
Test loss: 0.3961 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.22s
Val loss: 0.1728 score: 0.9380 time: 0.16s
Test loss: 0.3889 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.22s
Val loss: 0.1763 score: 0.9380 time: 0.17s
Test loss: 0.3851 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.22s
Val loss: 0.1818 score: 0.9457 time: 0.18s
Test loss: 0.3868 score: 0.8527 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.22s
Val loss: 0.1862 score: 0.9380 time: 0.17s
Test loss: 0.3906 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.22s
Val loss: 0.1877 score: 0.9380 time: 0.16s
Test loss: 0.3969 score: 0.8450 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.22s
Val loss: 0.1881 score: 0.9380 time: 0.16s
Test loss: 0.4023 score: 0.8450 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.21s
Val loss: 0.1876 score: 0.9380 time: 0.16s
Test loss: 0.4106 score: 0.8527 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.21s
Val loss: 0.1888 score: 0.9457 time: 0.16s
Test loss: 0.4181 score: 0.8527 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.21s
Val loss: 0.1919 score: 0.9457 time: 0.16s
Test loss: 0.4234 score: 0.8450 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.21s
Val loss: 0.1960 score: 0.9457 time: 0.16s
Test loss: 0.4297 score: 0.8450 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.21s
Val loss: 0.2000 score: 0.9457 time: 0.16s
Test loss: 0.4447 score: 0.8450 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.23s
Val loss: 0.2043 score: 0.9457 time: 0.16s
Test loss: 0.4535 score: 0.8450 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.0267,   Val_Loss: 0.1643,   Val_Precision: 0.9516,   Val_Recall: 0.9219,   Val_accuracy: 0.9365,   Val_Score: 0.9380,   Val_Loss: 0.1643,   Test_Precision: 0.9107,   Test_Recall: 0.7846,   Test_accuracy: 0.8430,   Test_Score: 0.8527,   Test_loss: 0.3079


[0.1597601950634271, 0.16412508115172386, 0.15982617484405637, 0.16188101703301072, 0.16087177000008523, 0.16082345298491418, 0.16052849497646093, 0.1595297351013869, 0.15900166193023324, 0.1674083541147411, 0.15926875290460885, 0.15933969500474632, 0.16019278694875538, 0.1600113520398736, 0.1589438719674945, 0.15989737794734538, 0.16999466600827873, 0.16985684004612267, 0.16942359716631472, 0.1676013229880482, 0.16761953989043832, 0.16928906692191958, 0.1699801639188081, 0.17030102899298072, 0.1695955148898065, 0.1692629971075803, 0.16805253410711884, 0.16919020004570484, 0.15778256906196475, 0.1565149340312928, 0.1571259351912886, 0.15606522210873663, 0.15654636407271028, 0.16709696990437806, 0.16857891180552542, 0.16911047021858394, 0.1653241920284927, 0.16018256591632962, 0.2040590129327029, 0.16097278497181833, 0.1609268900938332, 0.25258219009265304, 0.1623720231000334, 0.16044907993637025, 0.16111538000404835, 0.16005097003653646, 0.1598265648353845, 0.16097859619185328, 0.16171473101712763, 0.16281472286209464, 0.16304791904985905, 0.16229780297726393, 0.160773450974375, 0.16119804489426315, 0.1620113980025053, 0.1626757080666721, 0.16559718712233007, 0.16330256895162165, 0.16624439205043018, 0.1633719028905034, 0.16584123787470162, 0.16529252915643156, 0.17936309496872127, 0.16238440503366292, 0.16709735803306103, 0.15931101399473846, 0.15839037206023932, 0.15840815613046288, 0.15879692998714745, 0.15822435589507222, 0.15840075095184147, 0.16376277594827116]
[0.0012384511245226906, 0.001272287450788557, 0.0012389625956903594, 0.00125489160490706, 0.0012470679844967848, 0.0012466934339915828, 0.0012444069378020227, 0.0012366646131890457, 0.0012325710227149864, 0.0012977391791840394, 0.0012346414953845646, 0.0012351914341453202, 0.0012418045499903519, 0.0012403980778284776, 0.0012321230385077093, 0.0012395145577313595, 0.001317788108591308, 0.0013167196902800207, 0.001313361218343525, 0.0012992350619228541, 0.001299376278220452, 0.0013123183482319349, 0.001317675689293086, 0.001320163015449463, 0.001314693913874469, 0.0013121162566479094, 0.001302732822535805, 0.0013115519383387973, 0.0012231206904028274, 0.0012132940622580836, 0.0012180305053588263, 0.0012098079233235398, 0.0012135377059900021, 0.001295325348095954, 0.0013068132698102746, 0.0013109338776634415, 0.0012815828839418037, 0.0012417253171808499, 0.0015818528134318053, 0.0012478510462931654, 0.0012474952720452188, 0.001958001473586458, 0.001258697853488631, 0.0012437913173362035, 0.0012489564341399096, 0.001240705194081678, 0.0012389656188789495, 0.0012478960945104907, 0.0012536025660242452, 0.001262129634589881, 0.0012639373569756514, 0.0012581225036997204, 0.001246305821506783, 0.00124959724724235, 0.0012559023100969402, 0.0012610520005168378, 0.0012836991249793029, 0.0012659113872218733, 0.0012887162174451952, 0.0012664488596163055, 0.0012855909912767567, 0.001281337435321175, 0.001390411588904816, 0.001258793837470255, 0.0012953283568454344, 0.0012349691007344068, 0.0012278323415522428, 0.0012279702025617277, 0.0012309839533887399, 0.0012265453945354435, 0.0012279127980762904, 0.0012694788833199314]
[807.4602058966262, 785.9859023054934, 807.1268684611034, 796.8815761374564, 801.8809017886211, 802.1218149824247, 803.595648354617, 808.6266796470003, 811.3122745635364, 770.5708635758037, 809.9517177563525, 809.591106573647, 805.2797036439989, 806.1928004198988, 811.6072573491963, 806.7674508238655, 758.8473393260334, 759.4630864731237, 761.4051534590375, 769.6836618002061, 769.6000125302735, 762.0102251464241, 758.9120814215564, 757.4822111340089, 760.6333226666804, 762.1275896350227, 767.6171066707851, 762.4555084464236, 817.580806085993, 824.2025005371589, 820.9975001450432, 826.5774927749165, 824.0370242012394, 772.0068178005909, 765.2202675789953, 762.8149802508437, 780.285077563044, 805.331087450443, 632.1700675997253, 801.377698861234, 801.6062444553717, 510.7248454559674, 794.4718402660185, 803.9933918671138, 800.6684401995552, 805.9932405942424, 807.124898998269, 801.3487696604001, 797.7009836310909, 792.3116394655784, 791.178450799808, 794.8351587856764, 802.3712821873853, 800.2578448430733, 796.2402743910967, 792.9887106877063, 778.9987393004751, 789.9447071051052, 775.9660245313291, 789.6094598742573, 777.8523704548301, 780.4345463061758, 719.2114967825239, 794.4112611876602, 772.0050246065334, 809.7368585216617, 814.4434432601654, 814.3520078205903, 812.3582742464913, 815.2979942326161, 814.3900784865587, 787.7248004195294]
Elapsed: 0.16485502335449886~0.012284954378174189
Time per graph: 0.0012779459174767354~9.523220448197047e-05
Speed: 785.6714513231013~43.05087814321907
Total Time: 0.1643
best val loss: 0.1643258718297232 test_score: 0.8527

Testing...
Test loss: 0.3181 score: 0.8760 time: 0.16s
test Score 0.8760
Epoch Time List: [0.5287089750636369, 0.5300954622216523, 0.5267736751120538, 0.5272670914418995, 0.5273532441351563, 0.5259125782176852, 0.5308875439222902, 0.5286521560046822, 0.5233327487949282, 0.5319559690542519, 0.5290399328805506, 0.5234886149410158, 0.5252309751231223, 0.5263667467515916, 0.5265775290317833, 0.5615335870534182, 0.5561851840466261, 0.5548296337947249, 0.5542434579692781, 0.5514508991036564, 0.5503680291585624, 0.5555841601453722, 0.5554322628304362, 0.5599964209832251, 0.5571405030786991, 0.5572763779200613, 0.5537625178694725, 0.5526283448562026, 0.5292878802865744, 0.5164566889870912, 0.515229739015922, 0.5200899357441813, 0.5176542403642088, 0.5921253168489784, 0.5506653008051217, 0.5525237282272428, 0.6278884562198073, 0.6364223230630159, 0.570193531922996, 0.5253394509200007, 0.5279415196273476, 0.6196260498836637, 0.5406687902286649, 0.5368745760060847, 0.5329432189464569, 0.5302486210130155, 0.530632103793323, 0.5313239071983844, 0.5351983769796789, 0.5367147459182888, 0.5562151649501175, 0.5507272491231561, 0.5415129160974175, 0.5352448699995875, 0.5442251318600029, 0.5410130098462105, 0.54279919504188, 0.5381344659253955, 0.547179396962747, 0.549336998257786, 0.5360332331620157, 0.544120276812464, 0.5755783659406006, 0.541505137225613, 0.5461704770568758, 0.5289228998590261, 0.519095096969977, 0.5214832669589669, 0.5200730916112661, 0.5221775542013347, 0.5223295202013105, 0.5462465437594801]
Total Epoch List: [72]
Total Time List: [0.16425965004600585]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb23cc340>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5039 time: 0.16s
Epoch 2/1000, LR 0.000015
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5039 time: 0.16s
Epoch 3/1000, LR 0.000045
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5039 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5039 time: 0.16s
Epoch 7/1000, LR 0.000165
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5039 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5039 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5039 time: 0.16s
Epoch 11/1000, LR 0.000285
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5039 time: 0.17s
Epoch 12/1000, LR 0.000285
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5039 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.23s
Val loss: 0.6914 score: 0.5271 time: 0.17s
Test loss: 0.6915 score: 0.5116 time: 0.17s
Epoch 14/1000, LR 0.000285
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.22s
Val loss: 0.6906 score: 0.8760 time: 0.15s
Test loss: 0.6909 score: 0.8915 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.22s
Val loss: 0.6897 score: 0.5969 time: 0.16s
Test loss: 0.6902 score: 0.5426 time: 0.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 0.22s
Val loss: 0.6887 score: 0.5426 time: 0.17s
Test loss: 0.6894 score: 0.5194 time: 0.17s
Epoch 17/1000, LR 0.000285
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.23s
Val loss: 0.6873 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4961 time: 0.17s
Epoch 18/1000, LR 0.000285
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.4961 time: 0.17s
Epoch 19/1000, LR 0.000285
Train loss: 0.4844;  Loss pred: 0.4844; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4961 time: 0.17s
Epoch 20/1000, LR 0.000285
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5039 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4961 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.4331;  Loss pred: 0.4331; Loss self: 0.0000; time: 0.23s
Val loss: 0.6762 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.4961 time: 0.17s
Epoch 22/1000, LR 0.000285
Train loss: 0.3992;  Loss pred: 0.3992; Loss self: 0.0000; time: 0.23s
Val loss: 0.6717 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.4961 time: 0.17s
Epoch 23/1000, LR 0.000285
Train loss: 0.3721;  Loss pred: 0.3721; Loss self: 0.0000; time: 0.23s
Val loss: 0.6661 score: 0.5116 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6709 score: 0.4961 time: 0.17s
Epoch 24/1000, LR 0.000285
Train loss: 0.3413;  Loss pred: 0.3413; Loss self: 0.0000; time: 0.23s
Val loss: 0.6590 score: 0.5271 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6649 score: 0.4961 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.3098;  Loss pred: 0.3098; Loss self: 0.0000; time: 0.23s
Val loss: 0.6505 score: 0.5426 time: 0.17s
Test loss: 0.6576 score: 0.5194 time: 0.17s
Epoch 26/1000, LR 0.000285
Train loss: 0.2926;  Loss pred: 0.2926; Loss self: 0.0000; time: 0.23s
Val loss: 0.6396 score: 0.5426 time: 0.17s
Test loss: 0.6481 score: 0.5271 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 0.23s
Val loss: 0.6274 score: 0.5581 time: 0.17s
Test loss: 0.6374 score: 0.5349 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.2280;  Loss pred: 0.2280; Loss self: 0.0000; time: 0.23s
Val loss: 0.6127 score: 0.5891 time: 0.17s
Test loss: 0.6245 score: 0.5426 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.23s
Val loss: 0.5963 score: 0.5969 time: 0.17s
Test loss: 0.6099 score: 0.5504 time: 0.17s
Epoch 30/1000, LR 0.000285
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.23s
Val loss: 0.5775 score: 0.6279 time: 0.17s
Test loss: 0.5931 score: 0.5736 time: 0.16s
Epoch 31/1000, LR 0.000285
Train loss: 0.1686;  Loss pred: 0.1686; Loss self: 0.0000; time: 0.24s
Val loss: 0.5563 score: 0.6434 time: 0.16s
Test loss: 0.5740 score: 0.6047 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.1478;  Loss pred: 0.1478; Loss self: 0.0000; time: 0.22s
Val loss: 0.5317 score: 0.6589 time: 0.16s
Test loss: 0.5515 score: 0.6589 time: 0.16s
Epoch 33/1000, LR 0.000285
Train loss: 0.1310;  Loss pred: 0.1310; Loss self: 0.0000; time: 0.22s
Val loss: 0.5061 score: 0.6977 time: 0.16s
Test loss: 0.5282 score: 0.6899 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.1260;  Loss pred: 0.1260; Loss self: 0.0000; time: 0.22s
Val loss: 0.4779 score: 0.7364 time: 0.18s
Test loss: 0.5022 score: 0.7287 time: 0.16s
Epoch 35/1000, LR 0.000285
Train loss: 0.1084;  Loss pred: 0.1084; Loss self: 0.0000; time: 0.24s
Val loss: 0.4518 score: 0.7907 time: 0.17s
Test loss: 0.4782 score: 0.7674 time: 0.16s
Epoch 36/1000, LR 0.000285
Train loss: 0.1007;  Loss pred: 0.1007; Loss self: 0.0000; time: 0.22s
Val loss: 0.4255 score: 0.8372 time: 0.16s
Test loss: 0.4537 score: 0.8295 time: 0.16s
Epoch 37/1000, LR 0.000285
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.22s
Val loss: 0.4009 score: 0.8372 time: 0.16s
Test loss: 0.4308 score: 0.8295 time: 0.16s
Epoch 38/1000, LR 0.000284
Train loss: 0.0812;  Loss pred: 0.0812; Loss self: 0.0000; time: 0.22s
Val loss: 0.3754 score: 0.8682 time: 0.16s
Test loss: 0.4073 score: 0.8605 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.22s
Val loss: 0.3476 score: 0.8915 time: 0.16s
Test loss: 0.3822 score: 0.8837 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.22s
Val loss: 0.3174 score: 0.8915 time: 0.16s
Test loss: 0.3553 score: 0.8915 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.22s
Val loss: 0.2892 score: 0.8992 time: 0.16s
Test loss: 0.3320 score: 0.9147 time: 0.16s
Epoch 42/1000, LR 0.000284
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.22s
Val loss: 0.2663 score: 0.9380 time: 0.16s
Test loss: 0.3145 score: 0.9225 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.22s
Val loss: 0.2468 score: 0.9457 time: 0.17s
Test loss: 0.3005 score: 0.9225 time: 0.17s
Epoch 44/1000, LR 0.000284
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.22s
Val loss: 0.2261 score: 0.9380 time: 0.16s
Test loss: 0.2869 score: 0.9070 time: 0.16s
Epoch 45/1000, LR 0.000284
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.22s
Val loss: 0.2087 score: 0.9457 time: 0.16s
Test loss: 0.2778 score: 0.9070 time: 0.17s
Epoch 46/1000, LR 0.000284
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.22s
Val loss: 0.1924 score: 0.9457 time: 0.16s
Test loss: 0.2679 score: 0.9070 time: 0.16s
Epoch 47/1000, LR 0.000284
Train loss: 0.0404;  Loss pred: 0.0404; Loss self: 0.0000; time: 0.22s
Val loss: 0.1782 score: 0.9380 time: 0.16s
Test loss: 0.2626 score: 0.8992 time: 0.16s
Epoch 48/1000, LR 0.000284
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.22s
Val loss: 0.1696 score: 0.9380 time: 0.16s
Test loss: 0.2629 score: 0.8992 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.22s
Val loss: 0.1627 score: 0.9457 time: 0.16s
Test loss: 0.2659 score: 0.9070 time: 0.16s
Epoch 50/1000, LR 0.000284
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.22s
Val loss: 0.1580 score: 0.9535 time: 0.16s
Test loss: 0.2731 score: 0.8992 time: 0.16s
Epoch 51/1000, LR 0.000284
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.22s
Val loss: 0.1558 score: 0.9457 time: 0.17s
Test loss: 0.2836 score: 0.8992 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.22s
Val loss: 0.1577 score: 0.9457 time: 0.17s
Test loss: 0.2985 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.23s
Val loss: 0.1634 score: 0.9380 time: 0.17s
Test loss: 0.3137 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.23s
Val loss: 0.1709 score: 0.9380 time: 0.17s
Test loss: 0.3285 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.23s
Val loss: 0.1786 score: 0.9380 time: 0.17s
Test loss: 0.3427 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.23s
Val loss: 0.1837 score: 0.9380 time: 0.17s
Test loss: 0.3537 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.23s
Val loss: 0.1880 score: 0.9380 time: 0.17s
Test loss: 0.3636 score: 0.8915 time: 0.17s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.23s
Val loss: 0.1895 score: 0.9380 time: 0.17s
Test loss: 0.3702 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.23s
Val loss: 0.1901 score: 0.9380 time: 0.17s
Test loss: 0.3736 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.23s
Val loss: 0.1893 score: 0.9380 time: 0.17s
Test loss: 0.3742 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.23s
Val loss: 0.1831 score: 0.9380 time: 0.17s
Test loss: 0.3707 score: 0.8992 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.1802 score: 0.9380 time: 0.27s
Test loss: 0.3726 score: 0.8992 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.22s
Val loss: 0.1792 score: 0.9380 time: 0.16s
Test loss: 0.3763 score: 0.8992 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.22s
Val loss: 0.1792 score: 0.9380 time: 0.16s
Test loss: 0.3825 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.23s
Val loss: 0.1771 score: 0.9457 time: 0.21s
Test loss: 0.3888 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.22s
Val loss: 0.1756 score: 0.9535 time: 0.16s
Test loss: 0.3972 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.22s
Val loss: 0.1759 score: 0.9535 time: 0.16s
Test loss: 0.4057 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.29s
Val loss: 0.1781 score: 0.9535 time: 0.16s
Test loss: 0.4127 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.23s
Val loss: 0.1832 score: 0.9535 time: 0.17s
Test loss: 0.4154 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.23s
Val loss: 0.1887 score: 0.9535 time: 0.17s
Test loss: 0.4130 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.31s
Val loss: 0.1931 score: 0.9457 time: 0.17s
Test loss: 0.4118 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0187,   Val_Loss: 0.1558,   Val_Precision: 1.0000,   Val_Recall: 0.8923,   Val_accuracy: 0.9431,   Val_Score: 0.9457,   Val_Loss: 0.1558,   Test_Precision: 0.9322,   Test_Recall: 0.8594,   Test_accuracy: 0.8943,   Test_Score: 0.8992,   Test_loss: 0.2836


[0.1597601950634271, 0.16412508115172386, 0.15982617484405637, 0.16188101703301072, 0.16087177000008523, 0.16082345298491418, 0.16052849497646093, 0.1595297351013869, 0.15900166193023324, 0.1674083541147411, 0.15926875290460885, 0.15933969500474632, 0.16019278694875538, 0.1600113520398736, 0.1589438719674945, 0.15989737794734538, 0.16999466600827873, 0.16985684004612267, 0.16942359716631472, 0.1676013229880482, 0.16761953989043832, 0.16928906692191958, 0.1699801639188081, 0.17030102899298072, 0.1695955148898065, 0.1692629971075803, 0.16805253410711884, 0.16919020004570484, 0.15778256906196475, 0.1565149340312928, 0.1571259351912886, 0.15606522210873663, 0.15654636407271028, 0.16709696990437806, 0.16857891180552542, 0.16911047021858394, 0.1653241920284927, 0.16018256591632962, 0.2040590129327029, 0.16097278497181833, 0.1609268900938332, 0.25258219009265304, 0.1623720231000334, 0.16044907993637025, 0.16111538000404835, 0.16005097003653646, 0.1598265648353845, 0.16097859619185328, 0.16171473101712763, 0.16281472286209464, 0.16304791904985905, 0.16229780297726393, 0.160773450974375, 0.16119804489426315, 0.1620113980025053, 0.1626757080666721, 0.16559718712233007, 0.16330256895162165, 0.16624439205043018, 0.1633719028905034, 0.16584123787470162, 0.16529252915643156, 0.17936309496872127, 0.16238440503366292, 0.16709735803306103, 0.15931101399473846, 0.15839037206023932, 0.15840815613046288, 0.15879692998714745, 0.15822435589507222, 0.15840075095184147, 0.16376277594827116, 0.1640747138299048, 0.16248160088434815, 0.167505957884714, 0.16161850607022643, 0.16124946996569633, 0.16256532608531415, 0.16252020187675953, 0.16380241909064353, 0.1615325240418315, 0.16156384604983032, 0.17227900098077953, 0.16880486416630447, 0.1696098258253187, 0.15993906394578516, 0.16245093406178057, 0.17262243013828993, 0.17271172208711505, 0.17223541415296495, 0.1723333529662341, 0.17210615100339055, 0.1725027929060161, 0.17199610103853047, 0.17155994893983006, 0.1711190789937973, 0.17126496392302215, 0.17190594621933997, 0.17315133707597852, 0.17306502093560994, 0.1723479579668492, 0.16136677912436426, 0.16173585108481348, 0.1619720479939133, 0.16207989491522312, 0.16358795785345137, 0.1626802629325539, 0.16286076279357076, 0.16264117904938757, 0.16294045001268387, 0.16227123513817787, 0.16236673900857568, 0.1627072540577501, 0.16218559793196619, 0.17208663909696043, 0.16214379016309977, 0.17646500002592802, 0.16162349213846028, 0.16190611594356596, 0.16071536694653332, 0.1608028889168054, 0.16163379908539355, 0.1728079121094197, 0.17338619311340153, 0.17315049399621785, 0.17066028900444508, 0.17169300792738795, 0.1726122999098152, 0.17194456118158996, 0.17143833613954484, 0.17202616203576326, 0.1724659949541092, 0.17584856902249157, 0.1583186718635261, 0.16122126206755638, 0.16021022689528763, 0.16696627205237746, 0.16206624801270664, 0.1619994780048728, 0.16953030391596258, 0.17052326002158225, 0.17048836103640497, 0.17174944397993386]
[0.0012384511245226906, 0.001272287450788557, 0.0012389625956903594, 0.00125489160490706, 0.0012470679844967848, 0.0012466934339915828, 0.0012444069378020227, 0.0012366646131890457, 0.0012325710227149864, 0.0012977391791840394, 0.0012346414953845646, 0.0012351914341453202, 0.0012418045499903519, 0.0012403980778284776, 0.0012321230385077093, 0.0012395145577313595, 0.001317788108591308, 0.0013167196902800207, 0.001313361218343525, 0.0012992350619228541, 0.001299376278220452, 0.0013123183482319349, 0.001317675689293086, 0.001320163015449463, 0.001314693913874469, 0.0013121162566479094, 0.001302732822535805, 0.0013115519383387973, 0.0012231206904028274, 0.0012132940622580836, 0.0012180305053588263, 0.0012098079233235398, 0.0012135377059900021, 0.001295325348095954, 0.0013068132698102746, 0.0013109338776634415, 0.0012815828839418037, 0.0012417253171808499, 0.0015818528134318053, 0.0012478510462931654, 0.0012474952720452188, 0.001958001473586458, 0.001258697853488631, 0.0012437913173362035, 0.0012489564341399096, 0.001240705194081678, 0.0012389656188789495, 0.0012478960945104907, 0.0012536025660242452, 0.001262129634589881, 0.0012639373569756514, 0.0012581225036997204, 0.001246305821506783, 0.00124959724724235, 0.0012559023100969402, 0.0012610520005168378, 0.0012836991249793029, 0.0012659113872218733, 0.0012887162174451952, 0.0012664488596163055, 0.0012855909912767567, 0.001281337435321175, 0.001390411588904816, 0.001258793837470255, 0.0012953283568454344, 0.0012349691007344068, 0.0012278323415522428, 0.0012279702025617277, 0.0012309839533887399, 0.0012265453945354435, 0.0012279127980762904, 0.0012694788833199314, 0.0012718970064333705, 0.0012595472936771175, 0.0012984957975559226, 0.0012528566362033056, 0.0012499958912069484, 0.0012601963262427454, 0.0012598465261764306, 0.0012697861945011127, 0.0012521901088514069, 0.0012524329151149638, 0.0013354961316339499, 0.0013085648384984843, 0.0013148048513590596, 0.0012398377050060866, 0.0012593095663703921, 0.0013381583731650382, 0.0013388505588148452, 0.0013351582492477903, 0.0013359174648545278, 0.0013341562093286089, 0.0013372309527598149, 0.0013333031088258176, 0.0013299220848048841, 0.0013265044883240102, 0.0013276353792482337, 0.0013326042342584493, 0.0013422584269455699, 0.0013415893095783716, 0.0013360306819135597, 0.0012509052645299555, 0.0012537662874791743, 0.0012555972712706457, 0.0012564332939164584, 0.0012681237042903206, 0.0012610873095546813, 0.0012624865332834941, 0.0012607843337161828, 0.001263104263664216, 0.0012579165514587432, 0.0012586568915393464, 0.0012612965430833342, 0.0012572526971470247, 0.0013340049542400034, 0.001256928605915502, 0.001367945736635101, 0.0012528952878950409, 0.0012550861701051625, 0.0012458555577250646, 0.0012465340226108944, 0.0012529751867084772, 0.0013395962179024784, 0.0013440790163829577, 0.0013422518914435492, 0.001322947976778644, 0.0013309535498247128, 0.0013380798442621333, 0.001332903575051085, 0.0013289793499189522, 0.0013335361398121182, 0.0013369456973186759, 0.0013631672017247408, 0.0012272765260738457, 0.0012497772253298944, 0.0012419397433743227, 0.0012943121864525385, 0.00125632750397447, 0.0012558099070145178, 0.0013141884024493223, 0.0013218857366014128, 0.0013216152018325966, 0.0013313910386041385]
[807.4602058966262, 785.9859023054934, 807.1268684611034, 796.8815761374564, 801.8809017886211, 802.1218149824247, 803.595648354617, 808.6266796470003, 811.3122745635364, 770.5708635758037, 809.9517177563525, 809.591106573647, 805.2797036439989, 806.1928004198988, 811.6072573491963, 806.7674508238655, 758.8473393260334, 759.4630864731237, 761.4051534590375, 769.6836618002061, 769.6000125302735, 762.0102251464241, 758.9120814215564, 757.4822111340089, 760.6333226666804, 762.1275896350227, 767.6171066707851, 762.4555084464236, 817.580806085993, 824.2025005371589, 820.9975001450432, 826.5774927749165, 824.0370242012394, 772.0068178005909, 765.2202675789953, 762.8149802508437, 780.285077563044, 805.331087450443, 632.1700675997253, 801.377698861234, 801.6062444553717, 510.7248454559674, 794.4718402660185, 803.9933918671138, 800.6684401995552, 805.9932405942424, 807.124898998269, 801.3487696604001, 797.7009836310909, 792.3116394655784, 791.178450799808, 794.8351587856764, 802.3712821873853, 800.2578448430733, 796.2402743910967, 792.9887106877063, 778.9987393004751, 789.9447071051052, 775.9660245313291, 789.6094598742573, 777.8523704548301, 780.4345463061758, 719.2114967825239, 794.4112611876602, 772.0050246065334, 809.7368585216617, 814.4434432601654, 814.3520078205903, 812.3582742464913, 815.2979942326161, 814.3900784865587, 787.7248004195294, 786.2271826585874, 793.9360475148209, 770.1218609118624, 798.1759214130277, 800.0026296361967, 793.527150631746, 793.7474757619475, 787.5341567978622, 798.6007818870789, 798.4459590062816, 748.7853961632387, 764.1959882916099, 760.5691437526574, 806.5571775743753, 794.0859235130092, 747.295701356171, 746.9093495283022, 748.9748878557176, 748.549237739693, 749.5374177385365, 747.8139792802223, 750.0170016708779, 751.9237490869341, 753.8609999454004, 753.2188548381744, 750.4103426149379, 745.0130168119638, 745.3845918869727, 748.4858046581144, 799.4210499831604, 797.5968168761361, 796.433715555956, 795.9037736757803, 788.5666016783665, 792.966507888437, 792.0876568870677, 793.1570636291802, 791.7002806237382, 794.9652930795368, 794.4976956960787, 792.834964532151, 795.3850504908154, 749.6224034413054, 795.5901355842209, 731.0231489589778, 798.1512977673304, 796.7580424507513, 802.6612666286953, 802.2243932864959, 798.1004018339468, 746.4935975750861, 744.0038776076525, 745.0166443233928, 755.8876218511502, 751.3410217296467, 747.3395584636708, 750.2418169759009, 752.4571394288294, 749.8859387049606, 747.9735355037676, 733.5857250194655, 814.8122927104935, 800.1426012032164, 805.1920436035183, 772.6111292676678, 795.970793313398, 796.2988621242334, 760.9259054000531, 756.4950375900222, 756.6498922026366, 751.0941346341218]
Elapsed: 0.16589064759501887~0.00950075440646565
Time per graph: 0.0012859740123644873~7.364925896485e-05
Speed: 779.6106220669275~35.38974764953076
Total Time: 0.1724
best val loss: 0.15580370121223983 test_score: 0.8992

Testing...
Test loss: 0.2731 score: 0.8992 time: 0.15s
test Score 0.8992
Epoch Time List: [0.5287089750636369, 0.5300954622216523, 0.5267736751120538, 0.5272670914418995, 0.5273532441351563, 0.5259125782176852, 0.5308875439222902, 0.5286521560046822, 0.5233327487949282, 0.5319559690542519, 0.5290399328805506, 0.5234886149410158, 0.5252309751231223, 0.5263667467515916, 0.5265775290317833, 0.5615335870534182, 0.5561851840466261, 0.5548296337947249, 0.5542434579692781, 0.5514508991036564, 0.5503680291585624, 0.5555841601453722, 0.5554322628304362, 0.5599964209832251, 0.5571405030786991, 0.5572763779200613, 0.5537625178694725, 0.5526283448562026, 0.5292878802865744, 0.5164566889870912, 0.515229739015922, 0.5200899357441813, 0.5176542403642088, 0.5921253168489784, 0.5506653008051217, 0.5525237282272428, 0.6278884562198073, 0.6364223230630159, 0.570193531922996, 0.5253394509200007, 0.5279415196273476, 0.6196260498836637, 0.5406687902286649, 0.5368745760060847, 0.5329432189464569, 0.5302486210130155, 0.530632103793323, 0.5313239071983844, 0.5351983769796789, 0.5367147459182888, 0.5562151649501175, 0.5507272491231561, 0.5415129160974175, 0.5352448699995875, 0.5442251318600029, 0.5410130098462105, 0.54279919504188, 0.5381344659253955, 0.547179396962747, 0.549336998257786, 0.5360332331620157, 0.544120276812464, 0.5755783659406006, 0.541505137225613, 0.5461704770568758, 0.5289228998590261, 0.519095096969977, 0.5214832669589669, 0.5200730916112661, 0.5221775542013347, 0.5223295202013105, 0.5462465437594801, 0.5369335820432752, 0.5368086660746485, 0.5447778538800776, 0.5374108082614839, 0.5333903548307717, 0.5335176880471408, 0.5328739781398326, 0.533420633058995, 0.5316595449112356, 0.5329904302489012, 0.5580859980545938, 0.5610692971386015, 0.5606686959508806, 0.5301270862109959, 0.5360089030582458, 0.5587480338290334, 0.5678742420859635, 0.5637465389445424, 0.5661824042908847, 0.5641333959065378, 0.5631123259663582, 0.5623391380067915, 0.5625092720147222, 0.5633731961715966, 0.564009482273832, 0.5655605618376285, 0.5676404673140496, 0.564731947844848, 0.561908780131489, 0.5580164699349552, 0.5552832318935543, 0.5344003930222243, 0.5335862042848021, 0.5552570880390704, 0.5721712580416352, 0.5343540410976857, 0.5400190181098878, 0.5363989141769707, 0.5394449320156127, 0.5363184758462012, 0.5363540379330516, 0.5335473597515374, 0.5562777160666883, 0.5380672640167177, 0.5513017510529608, 0.5392638070043176, 0.5327109971549362, 0.5319986210670322, 0.5297406939789653, 0.5304557259660214, 0.5594667096156627, 0.5615804560948163, 0.5649286892730743, 0.5653271249029785, 0.565563399111852, 0.5635227917227894, 0.5640768050216138, 0.5639470529276878, 0.566000364953652, 0.5681683297734708, 0.5721636859234422, 0.6474113271106035, 0.5308275879360735, 0.5340219670906663, 0.6048856331035495, 0.5313235758803785, 0.5384688808117062, 0.6194107900373638, 0.5588914398103952, 0.5650090468116105, 0.6484369982499629]
Total Epoch List: [72, 71]
Total Time List: [0.16425965004600585, 0.1723694079555571]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x78feb23c59f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7028;  Loss pred: 0.7028; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.14s
Epoch 2/1000, LR 0.000020
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.15s
Epoch 3/1000, LR 0.000050
Train loss: 0.7012;  Loss pred: 0.7012; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000080
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000110
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000140
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000170
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000200
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000230
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000260
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000290
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000290
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.4961 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 13/1000, LR 0.000290
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 11 of 20
Epoch 14/1000, LR 0.000290
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 0.15s
Epoch 15/1000, LR 0.000290
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.15s
Epoch 16/1000, LR 0.000290
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000290
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.14s
Epoch 18/1000, LR 0.000290
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5000 time: 0.14s
Epoch 19/1000, LR 0.000290
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.14s
Epoch 20/1000, LR 0.000290
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4961 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5000 time: 0.14s
Epoch 21/1000, LR 0.000290
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6746 score: 0.4961 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6722 score: 0.5000 time: 0.15s
Epoch 22/1000, LR 0.000290
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.23s
Val loss: 0.6658 score: 0.5736 time: 0.15s
Test loss: 0.6630 score: 0.5547 time: 0.15s
Epoch 23/1000, LR 0.000290
Train loss: 0.4914;  Loss pred: 0.4914; Loss self: 0.0000; time: 0.23s
Val loss: 0.6526 score: 0.6899 time: 0.16s
Test loss: 0.6490 score: 0.7500 time: 0.14s
Epoch 24/1000, LR 0.000290
Train loss: 0.4678;  Loss pred: 0.4678; Loss self: 0.0000; time: 0.23s
Val loss: 0.6426 score: 0.6899 time: 0.15s
Test loss: 0.6383 score: 0.7500 time: 0.14s
Epoch 25/1000, LR 0.000290
Train loss: 0.4527;  Loss pred: 0.4527; Loss self: 0.0000; time: 0.23s
Val loss: 0.6278 score: 0.7752 time: 0.15s
Test loss: 0.6223 score: 0.7891 time: 0.15s
Epoch 26/1000, LR 0.000290
Train loss: 0.4277;  Loss pred: 0.4277; Loss self: 0.0000; time: 0.24s
Val loss: 0.6068 score: 0.8295 time: 0.17s
Test loss: 0.6000 score: 0.8516 time: 0.17s
Epoch 27/1000, LR 0.000290
Train loss: 0.4162;  Loss pred: 0.4162; Loss self: 0.0000; time: 0.24s
Val loss: 0.5817 score: 0.8682 time: 0.17s
Test loss: 0.5733 score: 0.8984 time: 0.16s
Epoch 28/1000, LR 0.000290
Train loss: 0.3809;  Loss pred: 0.3809; Loss self: 0.0000; time: 0.25s
Val loss: 0.5436 score: 0.9147 time: 0.17s
Test loss: 0.5338 score: 0.9062 time: 0.16s
Epoch 29/1000, LR 0.000290
Train loss: 0.3611;  Loss pred: 0.3611; Loss self: 0.0000; time: 0.25s
Val loss: 0.5067 score: 0.9147 time: 0.17s
Test loss: 0.4963 score: 0.9141 time: 0.16s
Epoch 30/1000, LR 0.000290
Train loss: 0.3414;  Loss pred: 0.3414; Loss self: 0.0000; time: 0.25s
Val loss: 0.4712 score: 0.9225 time: 0.17s
Test loss: 0.4609 score: 0.9219 time: 0.16s
Epoch 31/1000, LR 0.000290
Train loss: 0.3210;  Loss pred: 0.3210; Loss self: 0.0000; time: 0.25s
Val loss: 0.4355 score: 0.9147 time: 0.16s
Test loss: 0.4256 score: 0.9141 time: 0.15s
Epoch 32/1000, LR 0.000290
Train loss: 0.3006;  Loss pred: 0.3006; Loss self: 0.0000; time: 0.24s
Val loss: 0.3974 score: 0.9147 time: 0.16s
Test loss: 0.3891 score: 0.9141 time: 0.15s
Epoch 33/1000, LR 0.000290
Train loss: 0.2730;  Loss pred: 0.2730; Loss self: 0.0000; time: 0.24s
Val loss: 0.3613 score: 0.9147 time: 0.16s
Test loss: 0.3559 score: 0.9141 time: 0.15s
Epoch 34/1000, LR 0.000290
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 0.24s
Val loss: 0.3270 score: 0.9302 time: 0.16s
Test loss: 0.3257 score: 0.9297 time: 0.15s
Epoch 35/1000, LR 0.000290
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 0.23s
Val loss: 0.3012 score: 0.9302 time: 0.16s
Test loss: 0.3027 score: 0.9297 time: 0.15s
Epoch 36/1000, LR 0.000290
Train loss: 0.2299;  Loss pred: 0.2299; Loss self: 0.0000; time: 0.23s
Val loss: 0.2767 score: 0.9302 time: 0.16s
Test loss: 0.2822 score: 0.9297 time: 0.15s
Epoch 37/1000, LR 0.000290
Train loss: 0.2086;  Loss pred: 0.2086; Loss self: 0.0000; time: 0.23s
Val loss: 0.2482 score: 0.9457 time: 0.16s
Test loss: 0.2587 score: 0.9453 time: 0.15s
Epoch 38/1000, LR 0.000289
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.24s
Val loss: 0.2312 score: 0.9535 time: 0.15s
Test loss: 0.2473 score: 0.9453 time: 0.15s
Epoch 39/1000, LR 0.000289
Train loss: 0.1863;  Loss pred: 0.1863; Loss self: 0.0000; time: 0.24s
Val loss: 0.2072 score: 0.9612 time: 0.16s
Test loss: 0.2304 score: 0.9453 time: 0.15s
Epoch 40/1000, LR 0.000289
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 0.24s
Val loss: 0.2001 score: 0.9612 time: 0.16s
Test loss: 0.2267 score: 0.9453 time: 0.15s
Epoch 41/1000, LR 0.000289
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.24s
Val loss: 0.2155 score: 0.9225 time: 0.16s
Test loss: 0.2440 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000289
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.24s
Val loss: 0.2236 score: 0.9147 time: 0.16s
Test loss: 0.2524 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000289
Train loss: 0.1540;  Loss pred: 0.1540; Loss self: 0.0000; time: 0.24s
Val loss: 0.2044 score: 0.9380 time: 0.16s
Test loss: 0.2361 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 44/1000, LR 0.000289
Train loss: 0.1437;  Loss pred: 0.1437; Loss self: 0.0000; time: 0.24s
Val loss: 0.2008 score: 0.9380 time: 0.16s
Test loss: 0.2349 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 45/1000, LR 0.000289
Train loss: 0.1378;  Loss pred: 0.1378; Loss self: 0.0000; time: 0.24s
Val loss: 0.2096 score: 0.9147 time: 0.16s
Test loss: 0.2458 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 46/1000, LR 0.000289
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.24s
Val loss: 0.2008 score: 0.9380 time: 0.16s
Test loss: 0.2384 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 47/1000, LR 0.000289
Train loss: 0.1255;  Loss pred: 0.1255; Loss self: 0.0000; time: 0.24s
Val loss: 0.2173 score: 0.9147 time: 0.16s
Test loss: 0.2545 score: 0.9062 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 48/1000, LR 0.000289
Train loss: 0.1197;  Loss pred: 0.1197; Loss self: 0.0000; time: 0.24s
Val loss: 0.1972 score: 0.9225 time: 0.16s
Test loss: 0.2363 score: 0.9219 time: 0.15s
Epoch 49/1000, LR 0.000289
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.24s
Val loss: 0.1861 score: 0.9380 time: 0.16s
Test loss: 0.2268 score: 0.9297 time: 0.15s
Epoch 50/1000, LR 0.000289
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.24s
Val loss: 0.1827 score: 0.9380 time: 0.16s
Test loss: 0.2236 score: 0.9297 time: 0.15s
Epoch 51/1000, LR 0.000289
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.24s
Val loss: 0.1819 score: 0.9380 time: 0.16s
Test loss: 0.2259 score: 0.9219 time: 0.15s
Epoch 52/1000, LR 0.000289
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 0.24s
Val loss: 0.1784 score: 0.9380 time: 0.16s
Test loss: 0.2247 score: 0.9219 time: 0.15s
Epoch 53/1000, LR 0.000289
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.24s
Val loss: 0.1913 score: 0.9302 time: 0.17s
Test loss: 0.2390 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000289
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.25s
Val loss: 0.2231 score: 0.9147 time: 0.17s
Test loss: 0.2652 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000289
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.25s
Val loss: 0.1769 score: 0.9380 time: 0.17s
Test loss: 0.2313 score: 0.9219 time: 0.16s
Epoch 56/1000, LR 0.000289
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.25s
Val loss: 0.2020 score: 0.9225 time: 0.17s
Test loss: 0.2478 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000288
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.25s
Val loss: 0.1993 score: 0.9225 time: 0.17s
Test loss: 0.2404 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000288
Train loss: 0.0713;  Loss pred: 0.0713; Loss self: 0.0000; time: 0.25s
Val loss: 0.2046 score: 0.9225 time: 0.17s
Test loss: 0.2394 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000288
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.25s
Val loss: 0.1739 score: 0.9380 time: 0.17s
Test loss: 0.2157 score: 0.9531 time: 0.16s
Epoch 60/1000, LR 0.000288
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.25s
Val loss: 0.1876 score: 0.9380 time: 0.17s
Test loss: 0.2254 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 61/1000, LR 0.000288
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.25s
Val loss: 0.2059 score: 0.9302 time: 0.17s
Test loss: 0.2402 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 62/1000, LR 0.000288
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.25s
Val loss: 0.1830 score: 0.9380 time: 0.17s
Test loss: 0.2257 score: 0.9297 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 63/1000, LR 0.000288
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.25s
Val loss: 0.1694 score: 0.9380 time: 0.17s
Test loss: 0.2142 score: 0.9531 time: 0.16s
Epoch 64/1000, LR 0.000288
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.24s
Val loss: 0.1628 score: 0.9380 time: 0.17s
Test loss: 0.2079 score: 0.9609 time: 0.15s
Epoch 65/1000, LR 0.000288
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.24s
Val loss: 0.1903 score: 0.9380 time: 0.17s
Test loss: 0.2318 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 66/1000, LR 0.000288
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.25s
Val loss: 0.1870 score: 0.9380 time: 0.17s
Test loss: 0.2316 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 67/1000, LR 0.000288
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.24s
Val loss: 0.2057 score: 0.9457 time: 0.16s
Test loss: 0.2480 score: 0.9219 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 68/1000, LR 0.000288
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.24s
Val loss: 0.2252 score: 0.9380 time: 0.16s
Test loss: 0.2630 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 69/1000, LR 0.000288
Train loss: 0.0566;  Loss pred: 0.0566; Loss self: 0.0000; time: 0.24s
Val loss: 0.1990 score: 0.9380 time: 0.16s
Test loss: 0.2417 score: 0.9141 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 70/1000, LR 0.000287
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.24s
Val loss: 0.1587 score: 0.9380 time: 0.16s
Test loss: 0.2121 score: 0.9531 time: 0.15s
Epoch 71/1000, LR 0.000287
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.24s
Val loss: 0.1573 score: 0.9457 time: 0.16s
Test loss: 0.2114 score: 0.9609 time: 0.15s
Epoch 72/1000, LR 0.000287
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.24s
Val loss: 0.1458 score: 0.9612 time: 0.16s
Test loss: 0.2073 score: 0.9609 time: 0.15s
Epoch 73/1000, LR 0.000287
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.24s
Val loss: 0.1468 score: 0.9612 time: 0.16s
Test loss: 0.2079 score: 0.9609 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 74/1000, LR 0.000287
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.25s
Val loss: 0.1657 score: 0.9380 time: 0.17s
Test loss: 0.2210 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 75/1000, LR 0.000287
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.25s
Val loss: 0.1582 score: 0.9535 time: 0.17s
Test loss: 0.2170 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 76/1000, LR 0.000287
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.25s
Val loss: 0.1497 score: 0.9612 time: 0.17s
Test loss: 0.2110 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 77/1000, LR 0.000287
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.25s
Val loss: 0.1485 score: 0.9612 time: 0.17s
Test loss: 0.2122 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 78/1000, LR 0.000287
Train loss: 0.0417;  Loss pred: 0.0417; Loss self: 0.0000; time: 0.25s
Val loss: 0.1677 score: 0.9380 time: 0.17s
Test loss: 0.2322 score: 0.9531 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 79/1000, LR 0.000287
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.25s
Val loss: 0.1580 score: 0.9457 time: 0.17s
Test loss: 0.2268 score: 0.9609 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 80/1000, LR 0.000287
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.25s
Val loss: 0.1775 score: 0.9380 time: 0.17s
Test loss: 0.2472 score: 0.9453 time: 0.16s
     INFO: Early stopping counter 8 of 20
Epoch 81/1000, LR 0.000286
Train loss: 0.0355;  Loss pred: 0.0355; Loss self: 0.0000; time: 0.24s
Val loss: 0.1612 score: 0.9457 time: 0.16s
Test loss: 0.2367 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 82/1000, LR 0.000286
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.24s
Val loss: 0.1912 score: 0.9380 time: 0.17s
Test loss: 0.2653 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 83/1000, LR 0.000286
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.25s
Val loss: 0.2186 score: 0.9457 time: 0.16s
Test loss: 0.2951 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 84/1000, LR 0.000286
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.25s
Val loss: 0.2361 score: 0.9457 time: 0.17s
Test loss: 0.3144 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 85/1000, LR 0.000286
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.25s
Val loss: 0.1774 score: 0.9380 time: 0.17s
Test loss: 0.2676 score: 0.9375 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 86/1000, LR 0.000286
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.24s
Val loss: 0.1999 score: 0.9380 time: 0.16s
Test loss: 0.2809 score: 0.9297 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 87/1000, LR 0.000286
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.24s
Val loss: 0.1738 score: 0.9457 time: 0.16s
Test loss: 0.2591 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 88/1000, LR 0.000286
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.25s
Val loss: 0.1911 score: 0.9380 time: 0.16s
Test loss: 0.2706 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 89/1000, LR 0.000286
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.24s
Val loss: 0.1664 score: 0.9535 time: 0.16s
Test loss: 0.2541 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 17 of 20
Epoch 90/1000, LR 0.000285
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.24s
Val loss: 0.1984 score: 0.9380 time: 0.26s
Test loss: 0.2756 score: 0.9375 time: 0.15s
     INFO: Early stopping counter 18 of 20
Epoch 91/1000, LR 0.000285
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.25s
Val loss: 0.1861 score: 0.9457 time: 0.16s
Test loss: 0.2627 score: 0.9453 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 92/1000, LR 0.000285
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.25s
Val loss: 0.1832 score: 0.9457 time: 0.16s
Test loss: 0.2555 score: 0.9531 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 071,   Train_Loss: 0.0493,   Val_Loss: 0.1458,   Val_Precision: 0.9839,   Val_Recall: 0.9385,   Val_accuracy: 0.9606,   Val_Score: 0.9612,   Val_Loss: 0.1458,   Test_Precision: 0.9836,   Test_Recall: 0.9375,   Test_accuracy: 0.9600,   Test_Score: 0.9609,   Test_loss: 0.2073


[0.1597601950634271, 0.16412508115172386, 0.15982617484405637, 0.16188101703301072, 0.16087177000008523, 0.16082345298491418, 0.16052849497646093, 0.1595297351013869, 0.15900166193023324, 0.1674083541147411, 0.15926875290460885, 0.15933969500474632, 0.16019278694875538, 0.1600113520398736, 0.1589438719674945, 0.15989737794734538, 0.16999466600827873, 0.16985684004612267, 0.16942359716631472, 0.1676013229880482, 0.16761953989043832, 0.16928906692191958, 0.1699801639188081, 0.17030102899298072, 0.1695955148898065, 0.1692629971075803, 0.16805253410711884, 0.16919020004570484, 0.15778256906196475, 0.1565149340312928, 0.1571259351912886, 0.15606522210873663, 0.15654636407271028, 0.16709696990437806, 0.16857891180552542, 0.16911047021858394, 0.1653241920284927, 0.16018256591632962, 0.2040590129327029, 0.16097278497181833, 0.1609268900938332, 0.25258219009265304, 0.1623720231000334, 0.16044907993637025, 0.16111538000404835, 0.16005097003653646, 0.1598265648353845, 0.16097859619185328, 0.16171473101712763, 0.16281472286209464, 0.16304791904985905, 0.16229780297726393, 0.160773450974375, 0.16119804489426315, 0.1620113980025053, 0.1626757080666721, 0.16559718712233007, 0.16330256895162165, 0.16624439205043018, 0.1633719028905034, 0.16584123787470162, 0.16529252915643156, 0.17936309496872127, 0.16238440503366292, 0.16709735803306103, 0.15931101399473846, 0.15839037206023932, 0.15840815613046288, 0.15879692998714745, 0.15822435589507222, 0.15840075095184147, 0.16376277594827116, 0.1640747138299048, 0.16248160088434815, 0.167505957884714, 0.16161850607022643, 0.16124946996569633, 0.16256532608531415, 0.16252020187675953, 0.16380241909064353, 0.1615325240418315, 0.16156384604983032, 0.17227900098077953, 0.16880486416630447, 0.1696098258253187, 0.15993906394578516, 0.16245093406178057, 0.17262243013828993, 0.17271172208711505, 0.17223541415296495, 0.1723333529662341, 0.17210615100339055, 0.1725027929060161, 0.17199610103853047, 0.17155994893983006, 0.1711190789937973, 0.17126496392302215, 0.17190594621933997, 0.17315133707597852, 0.17306502093560994, 0.1723479579668492, 0.16136677912436426, 0.16173585108481348, 0.1619720479939133, 0.16207989491522312, 0.16358795785345137, 0.1626802629325539, 0.16286076279357076, 0.16264117904938757, 0.16294045001268387, 0.16227123513817787, 0.16236673900857568, 0.1627072540577501, 0.16218559793196619, 0.17208663909696043, 0.16214379016309977, 0.17646500002592802, 0.16162349213846028, 0.16190611594356596, 0.16071536694653332, 0.1608028889168054, 0.16163379908539355, 0.1728079121094197, 0.17338619311340153, 0.17315049399621785, 0.17066028900444508, 0.17169300792738795, 0.1726122999098152, 0.17194456118158996, 0.17143833613954484, 0.17202616203576326, 0.1724659949541092, 0.17584856902249157, 0.1583186718635261, 0.16122126206755638, 0.16021022689528763, 0.16696627205237746, 0.16206624801270664, 0.1619994780048728, 0.16953030391596258, 0.17052326002158225, 0.17048836103640497, 0.17174944397993386, 0.14879677421413362, 0.1495593129657209, 0.15115618496201932, 0.15229364205151796, 0.15167294791899621, 0.15097620990127325, 0.14969353820197284, 0.15875939698889852, 0.1585548659786582, 0.16033986816182733, 0.1608371150214225, 0.15821159211918712, 0.14859955199062824, 0.15024342481046915, 0.14972708793357015, 0.1517194148618728, 0.1485390269663185, 0.14912840398028493, 0.14828184596262872, 0.14889719686470926, 0.14975491096265614, 0.15024079591967165, 0.14901690813712776, 0.14897564705461264, 0.1504016499966383, 0.17682056198827922, 0.16068326798267663, 0.16125336498953402, 0.163243138929829, 0.16145054111257195, 0.1508329100906849, 0.15178623306564987, 0.1517524211667478, 0.1528140758164227, 0.14975478290580213, 0.14953055093064904, 0.14962770906277, 0.15047834999859333, 0.15134701202623546, 0.15282622911036015, 0.15191629598848522, 0.153677464928478, 0.15080562885850668, 0.1525705789681524, 0.15181989409029484, 0.14980411995202303, 0.1501687471754849, 0.15141175407916307, 0.15365210990421474, 0.15565697802230716, 0.1535261480603367, 0.1518394451122731, 0.160778540186584, 0.1599056760314852, 0.16131767607294023, 0.16261924104765058, 0.16201947606168687, 0.16075566597282887, 0.161036689998582, 0.16036505112424493, 0.16189675009809434, 0.16176557494327426, 0.16169958095997572, 0.15935737290419638, 0.16154755814932287, 0.16126233502291143, 0.15290373610332608, 0.15237320796586573, 0.1528432951308787, 0.15303597715683281, 0.15456403605639935, 0.15265639405697584, 0.15246004704385996, 0.16352818603627384, 0.16481993300840259, 0.16404218086972833, 0.1628841389901936, 0.1621898200828582, 0.1638623890466988, 0.16352578601799905, 0.1501003981102258, 0.15940402704291046, 0.1599300019443035, 0.1609147358685732, 0.16126424493268132, 0.1514992848969996, 0.1581802419386804, 0.15642567304894328, 0.15813887305557728, 0.1578089608810842, 0.15850512101314962, 0.15381598006933928]
[0.0012384511245226906, 0.001272287450788557, 0.0012389625956903594, 0.00125489160490706, 0.0012470679844967848, 0.0012466934339915828, 0.0012444069378020227, 0.0012366646131890457, 0.0012325710227149864, 0.0012977391791840394, 0.0012346414953845646, 0.0012351914341453202, 0.0012418045499903519, 0.0012403980778284776, 0.0012321230385077093, 0.0012395145577313595, 0.001317788108591308, 0.0013167196902800207, 0.001313361218343525, 0.0012992350619228541, 0.001299376278220452, 0.0013123183482319349, 0.001317675689293086, 0.001320163015449463, 0.001314693913874469, 0.0013121162566479094, 0.001302732822535805, 0.0013115519383387973, 0.0012231206904028274, 0.0012132940622580836, 0.0012180305053588263, 0.0012098079233235398, 0.0012135377059900021, 0.001295325348095954, 0.0013068132698102746, 0.0013109338776634415, 0.0012815828839418037, 0.0012417253171808499, 0.0015818528134318053, 0.0012478510462931654, 0.0012474952720452188, 0.001958001473586458, 0.001258697853488631, 0.0012437913173362035, 0.0012489564341399096, 0.001240705194081678, 0.0012389656188789495, 0.0012478960945104907, 0.0012536025660242452, 0.001262129634589881, 0.0012639373569756514, 0.0012581225036997204, 0.001246305821506783, 0.00124959724724235, 0.0012559023100969402, 0.0012610520005168378, 0.0012836991249793029, 0.0012659113872218733, 0.0012887162174451952, 0.0012664488596163055, 0.0012855909912767567, 0.001281337435321175, 0.001390411588904816, 0.001258793837470255, 0.0012953283568454344, 0.0012349691007344068, 0.0012278323415522428, 0.0012279702025617277, 0.0012309839533887399, 0.0012265453945354435, 0.0012279127980762904, 0.0012694788833199314, 0.0012718970064333705, 0.0012595472936771175, 0.0012984957975559226, 0.0012528566362033056, 0.0012499958912069484, 0.0012601963262427454, 0.0012598465261764306, 0.0012697861945011127, 0.0012521901088514069, 0.0012524329151149638, 0.0013354961316339499, 0.0013085648384984843, 0.0013148048513590596, 0.0012398377050060866, 0.0012593095663703921, 0.0013381583731650382, 0.0013388505588148452, 0.0013351582492477903, 0.0013359174648545278, 0.0013341562093286089, 0.0013372309527598149, 0.0013333031088258176, 0.0013299220848048841, 0.0013265044883240102, 0.0013276353792482337, 0.0013326042342584493, 0.0013422584269455699, 0.0013415893095783716, 0.0013360306819135597, 0.0012509052645299555, 0.0012537662874791743, 0.0012555972712706457, 0.0012564332939164584, 0.0012681237042903206, 0.0012610873095546813, 0.0012624865332834941, 0.0012607843337161828, 0.001263104263664216, 0.0012579165514587432, 0.0012586568915393464, 0.0012612965430833342, 0.0012572526971470247, 0.0013340049542400034, 0.001256928605915502, 0.001367945736635101, 0.0012528952878950409, 0.0012550861701051625, 0.0012458555577250646, 0.0012465340226108944, 0.0012529751867084772, 0.0013395962179024784, 0.0013440790163829577, 0.0013422518914435492, 0.001322947976778644, 0.0013309535498247128, 0.0013380798442621333, 0.001332903575051085, 0.0013289793499189522, 0.0013335361398121182, 0.0013369456973186759, 0.0013631672017247408, 0.0012272765260738457, 0.0012497772253298944, 0.0012419397433743227, 0.0012943121864525385, 0.00125632750397447, 0.0012558099070145178, 0.0013141884024493223, 0.0013218857366014128, 0.0013216152018325966, 0.0013313910386041385, 0.001162474798547919, 0.0011684321325446945, 0.001180907695015776, 0.001189794078527484, 0.001184944905617158, 0.0011795016398536973, 0.0011694807672029128, 0.0012403077889757697, 0.0012387098904582672, 0.001252655220014276, 0.0012565399611048633, 0.0012360280634311493, 0.0011609339999267831, 0.0011737767563317902, 0.0011697428744810168, 0.0011853079286083812, 0.0011604611481743632, 0.001165065656095976, 0.001158451921583037, 0.001163259350505541, 0.001169960241895751, 0.0011737562181224348, 0.0011641945948213106, 0.0011638722426141612, 0.0011750128905987367, 0.0013814106405334314, 0.0012553380311146611, 0.0012597919139807345, 0.001275337022889289, 0.0012613323524419684, 0.0011783821100834757, 0.0011858299458253896, 0.0011855657903652173, 0.0011938599673158024, 0.0011699592414515791, 0.0011682074291456956, 0.0011689664770528907, 0.0011756121093640104, 0.0011823985314549645, 0.0011939549149246886, 0.0011868460624100408, 0.0012006051947537344, 0.0011781689754570834, 0.0011919576481886907, 0.0011860929225804284, 0.00117034468712518, 0.0011731933373084757, 0.0011829043287434615, 0.0012004071086266777, 0.0012160701407992747, 0.0011994230317213805, 0.0011862456649396336, 0.0012560823452076875, 0.0012492630939959781, 0.0012602943443198455, 0.0012704628206847701, 0.0012657771567319287, 0.0012559036404127255, 0.001258099140613922, 0.0012528519619081635, 0.001264818360141362, 0.0012637935542443302, 0.0012632779762498103, 0.0012449794758140342, 0.001262090298041585, 0.0012598619923664955, 0.001194560438307235, 0.001190415687233326, 0.0011940882432099897, 0.0011955935715377564, 0.0012075315316906199, 0.0011926280785701238, 0.001191094117530156, 0.0012775639534083894, 0.0012876557266281452, 0.0012815795380447526, 0.0012725323358608875, 0.0012671079693973297, 0.0012801749144273344, 0.0012775452032656176, 0.001172659360236139, 0.001245343961272738, 0.0012494531401898712, 0.001257146373973228, 0.0012598769135365728, 0.0011835881632578094, 0.0012357831401459407, 0.0012220755706948694, 0.0012354599457466975, 0.0012328825068834703, 0.0012383212579152314, 0.001201687344291713]
[807.4602058966262, 785.9859023054934, 807.1268684611034, 796.8815761374564, 801.8809017886211, 802.1218149824247, 803.595648354617, 808.6266796470003, 811.3122745635364, 770.5708635758037, 809.9517177563525, 809.591106573647, 805.2797036439989, 806.1928004198988, 811.6072573491963, 806.7674508238655, 758.8473393260334, 759.4630864731237, 761.4051534590375, 769.6836618002061, 769.6000125302735, 762.0102251464241, 758.9120814215564, 757.4822111340089, 760.6333226666804, 762.1275896350227, 767.6171066707851, 762.4555084464236, 817.580806085993, 824.2025005371589, 820.9975001450432, 826.5774927749165, 824.0370242012394, 772.0068178005909, 765.2202675789953, 762.8149802508437, 780.285077563044, 805.331087450443, 632.1700675997253, 801.377698861234, 801.6062444553717, 510.7248454559674, 794.4718402660185, 803.9933918671138, 800.6684401995552, 805.9932405942424, 807.124898998269, 801.3487696604001, 797.7009836310909, 792.3116394655784, 791.178450799808, 794.8351587856764, 802.3712821873853, 800.2578448430733, 796.2402743910967, 792.9887106877063, 778.9987393004751, 789.9447071051052, 775.9660245313291, 789.6094598742573, 777.8523704548301, 780.4345463061758, 719.2114967825239, 794.4112611876602, 772.0050246065334, 809.7368585216617, 814.4434432601654, 814.3520078205903, 812.3582742464913, 815.2979942326161, 814.3900784865587, 787.7248004195294, 786.2271826585874, 793.9360475148209, 770.1218609118624, 798.1759214130277, 800.0026296361967, 793.527150631746, 793.7474757619475, 787.5341567978622, 798.6007818870789, 798.4459590062816, 748.7853961632387, 764.1959882916099, 760.5691437526574, 806.5571775743753, 794.0859235130092, 747.295701356171, 746.9093495283022, 748.9748878557176, 748.549237739693, 749.5374177385365, 747.8139792802223, 750.0170016708779, 751.9237490869341, 753.8609999454004, 753.2188548381744, 750.4103426149379, 745.0130168119638, 745.3845918869727, 748.4858046581144, 799.4210499831604, 797.5968168761361, 796.433715555956, 795.9037736757803, 788.5666016783665, 792.966507888437, 792.0876568870677, 793.1570636291802, 791.7002806237382, 794.9652930795368, 794.4976956960787, 792.834964532151, 795.3850504908154, 749.6224034413054, 795.5901355842209, 731.0231489589778, 798.1512977673304, 796.7580424507513, 802.6612666286953, 802.2243932864959, 798.1004018339468, 746.4935975750861, 744.0038776076525, 745.0166443233928, 755.8876218511502, 751.3410217296467, 747.3395584636708, 750.2418169759009, 752.4571394288294, 749.8859387049606, 747.9735355037676, 733.5857250194655, 814.8122927104935, 800.1426012032164, 805.1920436035183, 772.6111292676678, 795.970793313398, 796.2988621242334, 760.9259054000531, 756.4950375900222, 756.6498922026366, 751.0941346341218, 860.2337024846724, 855.847740015613, 846.8062357631101, 840.4815741204751, 843.9210930901191, 847.81569284129, 855.0803297019875, 806.2514876454877, 807.2915278250057, 798.3042612384622, 795.8362097141023, 809.0431193156345, 861.3754098536757, 851.9507603176042, 854.8887296652035, 843.6626262797861, 861.726393488657, 858.3207261905777, 863.2209773828934, 859.6535240102818, 854.729899521752, 851.9656676235727, 858.9629297784943, 859.2008326909752, 851.0544931047033, 723.8977105416317, 796.598187272366, 793.7818848512569, 784.1064613136454, 792.812455863815, 848.6211657856556, 843.2912354089323, 843.4791288064637, 837.6191742557005, 854.7306304100738, 856.0123613760059, 855.4565247423724, 850.6207039165205, 845.7385334955385, 837.5525637524405, 842.5692528055187, 832.9132710483715, 848.7746841339454, 838.9559826388202, 843.1042635550256, 854.4491302441741, 852.3744281519587, 845.3769047089789, 833.0507148895904, 822.3209882801166, 833.734198487773, 842.9957044782024, 796.1261487475605, 800.4718980381721, 793.4654348859108, 787.1147299383443, 790.0284775101088, 796.2394309736786, 794.8499189913, 798.178899346531, 790.6273592425044, 791.2684762804774, 791.5914144000341, 803.2260928206439, 792.336334057653, 793.7377316396562, 837.1280078696235, 840.0426932579528, 837.4590451638356, 836.404630976573, 828.1357246215651, 838.484367397193, 839.5642168677591, 782.7396799448813, 776.60509662672, 780.287114700391, 785.8346478272277, 789.1987298253886, 781.1432552928393, 782.7511679773319, 852.7625616689143, 802.9910057764305, 800.3501434619922, 795.4523201936196, 793.7283311215871, 844.8884764507229, 809.2034658135118, 818.2800016462176, 809.4151521809246, 811.1072988843357, 807.5448867635077, 832.1632117956691]
Elapsed: 0.16185444295010035~0.009612109893706545
Time per graph: 0.0012583743287734527~7.229051342934025e-05
Speed: 796.8928115975701~39.39673205547216
Total Time: 0.1546
best val loss: 0.1457666219176017 test_score: 0.9609

Testing...
Test loss: 0.2304 score: 0.9453 time: 0.25s
test Score 0.9453
Epoch Time List: [0.5287089750636369, 0.5300954622216523, 0.5267736751120538, 0.5272670914418995, 0.5273532441351563, 0.5259125782176852, 0.5308875439222902, 0.5286521560046822, 0.5233327487949282, 0.5319559690542519, 0.5290399328805506, 0.5234886149410158, 0.5252309751231223, 0.5263667467515916, 0.5265775290317833, 0.5615335870534182, 0.5561851840466261, 0.5548296337947249, 0.5542434579692781, 0.5514508991036564, 0.5503680291585624, 0.5555841601453722, 0.5554322628304362, 0.5599964209832251, 0.5571405030786991, 0.5572763779200613, 0.5537625178694725, 0.5526283448562026, 0.5292878802865744, 0.5164566889870912, 0.515229739015922, 0.5200899357441813, 0.5176542403642088, 0.5921253168489784, 0.5506653008051217, 0.5525237282272428, 0.6278884562198073, 0.6364223230630159, 0.570193531922996, 0.5253394509200007, 0.5279415196273476, 0.6196260498836637, 0.5406687902286649, 0.5368745760060847, 0.5329432189464569, 0.5302486210130155, 0.530632103793323, 0.5313239071983844, 0.5351983769796789, 0.5367147459182888, 0.5562151649501175, 0.5507272491231561, 0.5415129160974175, 0.5352448699995875, 0.5442251318600029, 0.5410130098462105, 0.54279919504188, 0.5381344659253955, 0.547179396962747, 0.549336998257786, 0.5360332331620157, 0.544120276812464, 0.5755783659406006, 0.541505137225613, 0.5461704770568758, 0.5289228998590261, 0.519095096969977, 0.5214832669589669, 0.5200730916112661, 0.5221775542013347, 0.5223295202013105, 0.5462465437594801, 0.5369335820432752, 0.5368086660746485, 0.5447778538800776, 0.5374108082614839, 0.5333903548307717, 0.5335176880471408, 0.5328739781398326, 0.533420633058995, 0.5316595449112356, 0.5329904302489012, 0.5580859980545938, 0.5610692971386015, 0.5606686959508806, 0.5301270862109959, 0.5360089030582458, 0.5587480338290334, 0.5678742420859635, 0.5637465389445424, 0.5661824042908847, 0.5641333959065378, 0.5631123259663582, 0.5623391380067915, 0.5625092720147222, 0.5633731961715966, 0.564009482273832, 0.5655605618376285, 0.5676404673140496, 0.564731947844848, 0.561908780131489, 0.5580164699349552, 0.5552832318935543, 0.5344003930222243, 0.5335862042848021, 0.5552570880390704, 0.5721712580416352, 0.5343540410976857, 0.5400190181098878, 0.5363989141769707, 0.5394449320156127, 0.5363184758462012, 0.5363540379330516, 0.5335473597515374, 0.5562777160666883, 0.5380672640167177, 0.5513017510529608, 0.5392638070043176, 0.5327109971549362, 0.5319986210670322, 0.5297406939789653, 0.5304557259660214, 0.5594667096156627, 0.5615804560948163, 0.5649286892730743, 0.5653271249029785, 0.565563399111852, 0.5635227917227894, 0.5640768050216138, 0.5639470529276878, 0.566000364953652, 0.5681683297734708, 0.5721636859234422, 0.6474113271106035, 0.5308275879360735, 0.5340219670906663, 0.6048856331035495, 0.5313235758803785, 0.5384688808117062, 0.6194107900373638, 0.5588914398103952, 0.5650090468116105, 0.6484369982499629, 0.5306790901813656, 0.5334188742563128, 0.5334703731350601, 0.5366354151628911, 0.5382762118242681, 0.5419253697618842, 0.5350678241811693, 0.56455794791691, 0.5606442068237811, 0.5613244562409818, 0.5634636201430112, 0.5639489952009171, 0.5307226427830756, 0.5314479197841138, 0.5281007317826152, 0.5378365558572114, 0.5313851139508188, 0.5303852721117437, 0.5299111597705632, 0.5306965080089867, 0.5307306547183543, 0.5284698097966611, 0.5303839251864702, 0.529703076928854, 0.5321399979293346, 0.5893929258454591, 0.5661979028955102, 0.5703757528681308, 0.5727513320744038, 0.5740007981657982, 0.5553282799664885, 0.5400064431596547, 0.538502445211634, 0.5404985677450895, 0.5361242056824267, 0.5339698491152376, 0.5350785020273179, 0.5364587102085352, 0.5376242320053279, 0.5423867921344936, 0.5423776777461171, 0.5480139481369406, 0.5421359378378838, 0.5390015230514109, 0.5385442248079926, 0.5404657092876732, 0.5380998649634421, 0.5410345511045307, 0.5466127109248191, 0.5477314656600356, 0.5479947028215975, 0.5460318683180958, 0.5693210412282497, 0.5689920713193715, 0.5691806408576667, 0.5738123510964215, 0.5727385638747364, 0.572550059761852, 0.5713838897645473, 0.5695489239878953, 0.5726697428617626, 0.5714607709087431, 0.5714550307020545, 0.5676522112917155, 0.5684057432226837, 0.5720045841298997, 0.5473222588188946, 0.5453763499390334, 0.5453400160185993, 0.5473173852078617, 0.5485137570649385, 0.5453003961592913, 0.5450389008037746, 0.575436516199261, 0.5768373850733042, 0.5767566610593349, 0.576958549907431, 0.5738579521421343, 0.578352099051699, 0.5806188019923866, 0.5391864380799234, 0.5630082509014755, 0.5664000099059194, 0.5696840607561171, 0.5723735438659787, 0.5502421669661999, 0.5555672850459814, 0.5580718161072582, 0.5603650358971208, 0.658169908914715, 0.5624475961085409, 0.5534426819067448]
Total Epoch List: [72, 71, 92]
Total Time List: [0.16425965004600585, 0.1723694079555571, 0.15458849002607167]
T-times Epoch Time: 0.5483918691070505 ~ 0.012248971079949012
T-times Total Epoch: 73.55555555555554 ~ 4.094561282819618
T-times Total Time: 0.15873305825516582 ~ 0.0036383219539158555
T-times Inference Elapsed: 0.15957781861266718 ~ 0.002810103106622186
T-times Time Per Graph: 0.001240418456800252 ~ 2.1889950196355504e-05
T-times Speed: 810.7292918455755 ~ 16.497480710847338
T-times cross validation test micro f1 score:0.9086386617995963 ~ 0.00672155179214026
T-times cross validation test precision:0.9613219358833561 ~ 0.014264487334152644
T-times cross validation test recall:0.8619391025641026 ~ 0.0022118868176028783
T-times cross validation test f1_score:0.9086386617995963 ~ 0.006812587907645401
